So, I took a look at this and started working on a sample mobile app, but I quickly ran into an issue: &gt;&gt; 400kconfigure: error: C compiler cannot create executables builder for ‘/nix/store/32nngn44vclay1jn0fhymlm4ay54n381-ncurses-5.9-aarch64-apple-darwin14.drv’ failed with exit code 77 killing process 3675 killing process 3677 cannot build derivation ‘/nix/store/gjvi7ka1m7lc4djdigr5jfs91a6kkhcy-aarch64-apple-darwin14-ghc-8.2.1.drv’: 1 dependencies couldn't be built cannot build derivation ‘/nix/store/wnswkmwccyi9b2dirh11y6r04yspf4ii-frontend-0.1.0.0-aarch64-apple-darwin14.drv’: 1 dependencies couldn't be built cannot build derivation ‘/nix/store/fqm05gc3q5agyb30zsn3d0ndxznxl5xa-frontend-app.drv’: 1 dependencies couldn't be built error: build of ‘/nix/store/fqm05gc3q5agyb30zsn3d0ndxznxl5xa-frontend-app.drv’ failed
Are you thinking of building generics on top of something like this: data Universe = U1 | V1 | Universe :+: Universe | Universe :*: Universe data Rep (a :: Universe) where U :: Rep U1 L :: Rep a -&gt; Rep (a :+: b) R :: Rep b -&gt; Rep (a :+: b) (:*) :: Rep a -&gt; Rep b -&gt; Rep (a :* b) where the compiler can somehow derive a promoted type instance `TRep T :: Universe` for every ADT `T`, with conversion functions `T -&gt; Rep (TRep T)` and `Rep (TRep T) -&gt; T`? ## Inlining/optimizations. The last part of the following blogpost shows an example of unfolding a recursive function at compile-time by having its argument at the type level and matching on it using type classes: https://mpickering.github.io/posts/2017-03-20-inlining-and-specialisation.html This technique is essential to achieve performance equivalent with handwritten code. If we wrote generic implementations as simple functions over an ADT like `Rep` above, we would end up with recursive functions to handle arbitrary sums and products, which will not get inlined. ## Safety, expressiveness, extensibility Many definitions do not apply to all generic types. For example, there is no generic `Semigroup` for sum types (well, we could technically define one with `L1 x &lt;&gt; R1 y = R1 y &lt;&gt; L1 x = L1 x` but that looks like a bad idea). Typeclasses allow us to specify the types that are supported, they are precisely the types which have instances of whatever generic class we came up with. For the same reason, if we ever need to introduce a new generic constructor ([for example to carry GADT constraints](https://ryanglscott.github.io/2018/02/11/how-to-derive-generic-for-some-gadts/)), then that is just a new data type to add to `GHC.Generics`, which doesn't affect existing code. With ADTs like those above, how to exclude unsupported constructors without type classes? Adding new constructors to `Rep` may also cause problems, because functions that previously worked for all `Rep` suddenly become partial.
&gt; “expressive enough to describe itself” which reminded me of Haskell The lisp family of programming languages embodies this idea much more closely IMHO
&gt; "However, turning off a conscious sim without its consent should be considered murder, and appropriate punishment should be administered in every country.” &gt; &gt; [...] &gt; &gt; Buehrer’s research further indicates AI may one day enter into a conflict with itself for supremacy, stating intelligent systems “will probably have to, like the humans before them, go through a long period of war and conflict before evolving a universal social conscience.” Finding these kinds of quotes in a paper is a pretty good indication that this is a crackpot theory.
Yes, I would love to do exactly this for diagrams. But (a) I don't really want to do it by hand and then have to manually ensure any changes to the main library get properly reflected in the simple library, and (b) it seems an interesting yet daunting challenge to try to automate the process of generating the simple version of the library. So I've just never gotten around to it.
Thanks, that's very cool!
&gt; idea seems quite groundbreaking to me Isn't the idea of a perpetual motion machine if physically possible to build also groundbreaking? 
I'd say that it is the opposite of boring, because it reveals a quirk of the compiler :D. Even the Prelude authors didn't forget to write this in a comment lest the reader be puzzled: -- | Function composition. {-# INLINE (.) #-} -- Make sure it has TWO args only on the left, so that it inlines -- when applied to two functions, even if there is no final argument (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (.) f g = \x -&gt; f (g x)
Thanks for the info, I just read both of those papers. It's interesting to see a small seed of intuition developed into a fuller, more mature form.
Great explanation! Thanks
Great explanation! Thanks
Which error did you get exactly 
Performance tends to be crucial for the sort of stuff I write. That said, once you get the hang of the borrow checker, it’s not really unergonomic 99% of the time.
The benefits of this technique are described in the book „Growing functional programs“, or the „abstract gardener“ as it‘s referred to in the community.
I've created a PR to try help fix: https://github.com/yamadapc/stack-run/pull/15 If you like you can get a version that works will 11.4 from https://github.com/andrevdm/stack-run so long, while I see if I can get the PR accepted
It's a bounds issue on the conduit dependencies. Also changes to the cabal lib means that `unUnqualComponentName` needs to be used to get a `String` for the exe names
What if ghc could warn us about code that is guaranteed to normalize to an `error` call, if that can be detected at compile-time? That creates a new problem: How to detect an intentional error. I think we could have heuristic rules like: * Direct applications of `error` are exempted from the warning * Indirect applications of `error` in the same top-level definition are exempted * A new pragma allows specifying a function's purpose as error-throwing - so applications of it are exempted This will be a bit noisy, at least at first. But will detect so many runtime bugs at compile-time, that I am convinced that it will be worth it. 
`myconcat` of a list of lists `xss` is the list of `x`, for each `xs` in `xss` and each `x` in `xs`.
[element | list &lt;- listOfLists, element &lt;- list]
I would seriously recommend considering Elm. It is not as extensive as Haskell or even Purescript. But it is a lot faster and the best thing is it is not trying to be a general purpose language. It is focused on front end and excelling at it. If you are good with Haskell, Elm should be super smooth, maybe a bit verbose at times.
`V1` and `U1` are not typeclasses, so I'm not sure what you're saying ...
Here you go [dependent types rfc](https://github.com/luna/luna-rfcs/pull/8/files)
`xs` as in plural of `x` and `xss` as plural of `xs`: * `x :: a` (one thing) * `xs :: [a]` (many things) * `xss :: [[a]]` (many of many things)
Why? TH is exactly what we need here.
"x where xs is drawn from xxs and x is drawn from xs"? That's how I'd read it. This is slightly more concise and general and should also work - `myConcat = foldr (++) []`
Neither of these titles return relevant results for me. Could you perhaps share the author's name?
I generally agree with such a strategy. However I'm interested in having a discussion about cases where it may or may not hold up. One example is if the layer is itself sort of a feature. Such as if you have one package for the database, one for the Servant API, and one for the servant server. Any "feature" will probably affect all 3, but I don't really see an alternative way to structure it. Another example is if you are dealing with an MVC ish framework. A lot of the time features interconnect bidirectionally, such as a user page having a list of their comments, and a list of comments have the associated users attached. And oftentimes you have `n` models / pieces of state that map to `m` views. All this prevents you from really separating off a feature. Typically I then just have a module for models, and a module for views, and so on. Any thoughts on how the above should be approached? Maybe there are some clever approaches to allow for better modularity, but I'm thinking that maybe GUI / database stuff might just be tightly and bidirectionally coupled in nature. 
Wait what? In what way do I demonstrate that myself? I demonstrated how TH fails badly... and argued in favor of the type level approach. Also if you read the original thread, SPJ points out another big problem with the TH approach, namely how to deal with universally quantified numbers that won't be able to be practically resolved at compile time. Plus other people in the thread have also poked holes in the proposal such as the distaste many have for TH plus the risk of unsafePerformIO hiding at compile time. 
But it isn't. It plays extremely poorly with static length lists, heterogenous lists, statically known numbers / strings and so on. Because you get zero inference from the value of the literal. The type level approach allows the type inference algorithm to take advantage of the value of the literal, and it also doesn't need TH or risk unsafePerformIO, and it also works in cases where TH fails completely, such as SPJ's example where the number's type is not resolved at its definition site. 
"myconcat exeses is the list of ex where exes are drawn from exeses and ex is drawn from exes" "exes" is pronounced like "x-is" or the possessive "x's"; it's the plural of `x`. There's a naming convention in functional languages that if you have list where you would name the individual elements "x" (or "y", or "foo") that you name the list itself as the plural of that, "xs" (or "ys", or "foos", respectively).
Well I mean that you have to give instanciate U1 and V1 (ans the other ones) instead of just one data type.
How about the prelude, isn't that `Utils`?
This is super-useful! 2 hours on a 8-core CX51 skylake cloud instance from Hetzner is €0.128, probably well worth it to run this on every GHC pull request.
&gt; In every other language, I can just throw parenthesis somewhere and put the 'mod' 12 but Haskell is making me want to put my head through a window with it's relentless error messages I have a question. What did you tried and what was the error message? From my point of view, error message in Haskell are not perfect, but are far from "relentless". Actually, I'm usually reading them first, contrary to a lot of other languages where I try to understand the error message by looking at the error location and only reading the error message if my "brain-based compiler" fails. Note also that you are trying to write your function using something which is not available in "every other language": function composition and partial application. For example, in python, I'll write something like: def scale(root, mode): newList = [] for i in mode: newList.append((root + i) % 12) return newList Later, I'll learn about the `map` function, lambda functions and list comprehension, and I'll do something such as: # With list comprehension def scale(root, mode): return [(root + i) % 12 for i in mode] or: # with lambda functions / map def scale(root, mode): return map(lambda i: (root + i) % 12, mode) You can write the same thing in Haskell: -- With list comprehension scale root mode = [mod (root + i) 12 | i &lt;- mode] -- With lambda functions / map scale root = map (\i -&gt; mod (root + i) 12) But your current implementation (without the mod 12 thing) is different, because you use `(root +)` which is a partial application of an operator, a concept available in a few languages. And I'm not surprised that you got complexe error message in this context, because you failed at composing function, so you got error message about function composition, a concept which is not available too in many other languages, so this kind of error message in a totally different concept. Different tool, different rules. I'll be happy to see your error message and explain how to read it, I'm sure you'll find GHC errors appreciable in a (near, I hope) future.
I'm in favor of avoiding TH whenever reasonable, at least until it gets a major revamp. Running arbitrary code at compile time just has so many problems. Compile time issues, tooling issues, cross compilation issues, splicing issues, the list goes on. TH needs some pretty serious changes before I'd be want anything fundamental like literals to commit to it.
&gt; I think that refactoring to pointfree style is quite possibly the best way to get used to the less intuitive bits of Haskell syntax I wonder how much an editor plug-in which shows pointfree refactoring hints can be useful for learning purpose. I learned a lot by following hlint advices. foo a b c = (a * b + c, a * c + b, b * c + a) -- ^ Pointfree hint, why not: foo = ap (ap . (ap .) . ap (ap . ((ap . ((,,) .) . (+)) .) . (*)) (flip . ((+) .) . (*))) (flip (flip . ((+) .) . (*)))
Too bad, sounds like a good read.
The whole goal here is compile time literals with clearly determined compile-time concrete representations - like in every other programming language. It's crazy that Haskell, with all its promise of making things safer by finding errors at compile time, is the only near-mainstream language where **literals** - the simplest most basic programming construct there is - cause run-time calculation, and can throw exceptions and crash your program. Crazy! This is the number one wart of Haskell at the moment. This proposal is to fix that.
This is the best tl;dr I could make, [original](https://thenextweb.com/artificial-intelligence/2018/04/17/one-machine-to-rule-them-all-a-master-algorithm-may-emerge-sooner-than-you-think/) reduced by 83%. (I'm a bot) ***** &gt; It&amp;#039;s excusable if you didn&amp;#039;t notice it when a scientist named Daniel J. Buehrer, a retired professor from the National Chung Cheng University in Taiwan, published a white paper earlier this month proposing a new class of math that could lead to the birth of machine consciousness. &gt; If the class calculus theory is correct, that human and machine intelligence involve the same algorithm, then it is only less than a year for the theory to be testable in the OpenAI gym. &gt; Allowing machines to modify their own model of the world and themselves may create &amp;quot;Conscious&amp;quot; machines, where the measure of consciousness may be taken to be the number of uses of feedback loops between a class calculus&amp;#039;s model of the world and the results of what its robots actually caused to happen in the world. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/8d5ckk/one_machine_to_rule_them_all_a_master_algorithm/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 2.00, ~312335 tl;drs so far.") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **machine**^#1 **class**^#2 **Buehrer**^#3 **new**^#4 **math**^#5
didn't say you can't make an eDSL. That would require: 1. Have dependent types to encode array lengths and nothing more. 2. Enable serialization of functions. 3. Make heavy optimizations. 4. Probably restrict some Haskell features as described in [this comment](https://www.reddit.com/r/haskell/comments/8cvr6l/a_better_programming_language_for_data/dxifppk/) 5. Maybe tune the RTS to work better with the use cases in mind. Considering all of this, I think I would rather make a language from scratch and have complete control over it.
I've always been a fan of the [signature selections](https://ghc.haskell.org/trac/ghc/ticket/10803) proposal. It makes type level arguments much easier to read and write (:: SomeType) rather than (Proxy @SomeType) while being incredibly lightweight as extensions go. If Signature Selections was tweaked so that it instead desugared to a Proxy to allow the use of non * kinds would it not subsume a lot of these use cases?
Maybe unrelated and me demonstrating my cluelessness: if Haskell is supposed to get dependent types, doesn't there need to be a formal evaluation model at typecheck time in the first place to even get any use out of them? I mean how are you supposed to prove anything if you can't apply expressions and get a reduced result?
**Isomorphic JavaScript** Isomorphic JavaScript, also known as Universal JavaScript, describes JavaScript applications which run both on the client and the server. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Wouldn't that need relevant, dependent quantification? So either `pi` (or `foreach` as in [this proposal](https://github.com/goldfirere/ghc-proposals/blob/pi/proposals/0000-pi.rst)) or the current KnownNat / KnownSymbol hack ?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [goldfirere/ghc-proposals/.../**0000-pi.rst** (pi → 26b2391)](https://github.com/goldfirere/ghc-proposals/blob/26b2391b85252aa7e9c65e399aff65b37febe5e2/proposals/0000-pi.rst) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxkgm53.)
There’s some good advice in [this talk](https://youtu.be/PFQnNFe27kU) by Sam Newman (the guy who wrote [Building Microservices](http://shop.oreilly.com/product/0636920033158.do)) where he suggests that making the presentation part of your application very thin can help. A presentation layer that doesn’t do any business logic and just renders the output from your other feature components can help keep your components modular. That talk also has guidance on how to think about managing your data. Having a separate component that just manages creating and querying your data - like a service in front of a database - has the huge benefit of isolating your data’s database representation from your other components. That benefit is usually worth the added complexity of having an additional component for any project that is long-lived or likely to change significantly over its lifetime.
\*applause\*
How does this compare with [`ifcxt`](https://hackage.haskell.org/package/ifcxt)?
I dunno, I'm currently writing a blog post on why f x = g (h x) x can be written f' = g =&lt;&lt; h Because I've only just grasped it.
Stick with it. It will get easier eventually, I promise. (Although there will _always_ be stuff that's beyond you, it's a deep language that way.)
In list comprehensions, I read `x &lt;- xs` as 'items in collection xs may be thought of as item x' So: `[ x | x &lt;- xs , xs &lt;- xss]` Reads: "Show me a list of all x, where x may be thought of as an item in the list xs, where xs may be thought of as an item in the list xss" 
"Psuedo code" -&gt; writes completely valid Python
..by patching the compiler? 👌
I'm not understanding off the cuff how this differs from `Data.Constraint.Deferrable`.
You can only use Deferrable to defer typeclass constraints, can't you? The only instances seem to be for equalities.
Well, there is the [Big Ball Of Mud](http://wiki.c2.com/?BigBallOfMud) pattern..
The only instances out of the box are for equalities, but the plugin you have could be used to manufacture `Deferrable` instances just as well as `Emerge` instances as the types are effectively identical. One is just CPS'd and can give a message. It is morally `Either String (Dict c)`, though.
Translation: python-ish -&gt; can't be bothered to check if it is valid, don't complain if it isn't
You should check out `generics-sop`. That uses a GADT (not quite ADT) representation instead of a bunch of separate types. It's actually implemented on top of GHC Generics.
Isn't this a plugin? GHC plug-ins are not "patching the compiler."
"no instance for ...." would work just as well. you have the whole type being asked for in the plugin at that moment.
 The problem with your approach is that it is limited to what type-level programming can do. This is a pretty huge restriction -- you cannot write the `ByteString` example in my proposal, nor any other interesting instances, as `Symbol`s are currently opaque. Given a `type family Uncons :: Symbol -&gt; Maybe (TyChar, Symbol)`, we can begin to recover the functionality, but the implementation would be monstrous -- after all, we do not have higher order functions, `case`, `if`, or `let` expressions, `where` blocks, type classes, laziness, or any of the other niceties of programming in Haskell. The ByteString validator `any ((255 &lt;) . fromEnum) str` would have to look something like: type family CharToNat :: TyChar -&gt; Nat type ValidBytes (sym :: Symbol) = Valid' (Uncons sym) type family If (b :: Bool) (t :: '() -&gt; Constraint) (e :: '() -&gt; Constraint) where If True t _ = t () If False _ e = e () type family Valid' (muncons :: Maybe (TyChar, Symbol)) where Valid' Nothing = () Valid' (Just (char, rest)) = If (CharToNat char &gt; 255) DeferredError (DeferredSuccess rest) type DeferredError x = TypeError (...) type DeferredSuccess (rest :: Symbol) x = Valid' (Uncons rest) There's nothing stopping you from writing `(5 :: TypeLevelInteger 6) +. (5 :: TypeLevelInteger 4) :: TypeLevelInteger 10` at any point. It would fail at compile-time in either case. I think it would be easier to have a `constexpr` value-level language than sufficiently augmenting the type language to make this feasible. Of course, I don't care about implementation details -- in Idris, I'm sure you can write this all at the value level, promote it to the type level, and it'll Just Work. When Haskell's type-level programming is like that, I'd be happy for the implementation to do that. &gt; You will not get nice behavior for HLists, static length lists, or statically known strings / numbers. What "nice behavior" are you referring to? I demonstrated an instance for length-indexed vectors that is made safe at compile-time. I don't know what you mean by "statically known strings / numbers" -- those seem easily handled.
For me, I had started with all the haskell DT-approximations and loved em, but felt I was missing something deeper. I grabbed Type Driven Dev. With Idris, and it really pulled the concepts together. The learning curve for Idris is more or less just dependent types, so if you go that route, you'll learn far more DT stuff than Idris-only type stuff, and I find it ports back to Haskell's approximations pretty well.
Hmm, I see, thanks for sharing your insight. Might you also know the answer to [this question](https://stackoverflow.com/questions/49903536/can-idris-support-row-polymorphism) I've had for some time, and only now thought to ask?
I'm not wedded to keeping the string. It was there because it gave a little information about what went wrong. This "feels way too big" reason is why I think maybe I should get around to packaging up the "Partial" monad from https://github.com/ekmett/coda/blob/master/ref/coda-change/Coda/Syntax/Change.hs#L74 That way, the "String" only ever shows up as a bottom you can't distinguish in pure code, and so one can treat the type with the message embedded morally as `Maybe` as nobody can do anything with the information in pure code.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/coda/.../**Change.hs#L74** (master → fd4a60c)](https://github.com/ekmett/coda/blob/fd4a60c4589fe327f67bf0d25c9747ef3816e98d/ref/coda-change/Coda/Syntax/Change.hs#L74) ---- 
Template Haskell is an implementation detail that is entirely hidden away from the user. The user will deal with the type class interface that I've defined, so any UI/UX concerns are given by that interface. Another person in the GitHub thread suggested a new `constexpr`-like syntax, which I'd certainly like, and would solve the "template haskell" concerns while providing compile-time evaluation. 
The issue with handling all the cases I talked about is not that it's impossible, it's that you get zero type inference based on the value of the literal. So the `5 +. 5 :: TypeLevelInteger 10` will fail at compile time, complaining about ambiguous types. But with the type level approach it will work just fine. The exact same issue occurs with basically every other interesting statically known type: [1, 2, 3] ++. [4, 5, 6] :: StaticList 6 Int This infers just fine with my type level approach, but will complain about ambiguous types with the TH approach. Yes a better type level programming interface would be nice, no disagreements there. 
That's not entirely true. TH usage restricts portability, massively increasing compile times, inhibits no-code etc. You don't have to type $() but the UX is still terrible. 
Python equivalent: xss = [[1, 2, 3], [4, 5, 6]] myconcat_xss = [x for xs in xss for x in xs]
If module A imports module B and uses stuff from B at compile time via TH, then attempting to compile them with no-code prevents GHC from being able to compile A, IIUC, because no code was generated from B to evaluate A.
Maybe we should hold off on this proposal until we know more from DependentHaskell? That extension will surely require a revamp to GHC's ability to evaluate code at compile time. If that infrastructure is sufficiently improved, it may open new avenues to solving this problem
That approach may work, I couldn't tell you for certain. My other concern would be the one raised by SPJ. How would `f x = x * 2` work, since the type of `f` is polymorphic, there is absolutely know way to compute `2` at compile time.
What you're saying is kind of a non-sequitur. What I suggested solves the exact same problems. There are some fundamental and substantial issues with the TH based approach: Type inference based on literal value does not exist: 5 +. 5 :: CompileTimeInteger 10 Will infer fine with the type level approach, but will fail with the TH approach. TH can't help with polymorphic literals, but the type level approach can: f x :: Num a =&gt; a -&gt; a f x = x * 2 The above cannot have `2` evaluated at compile time with TH since the type is not known, but the type level approach is relatively unaffected. Then there is the usual issues with TH being far from perfect and far from portable, and with the `unsafePeformIO` risk.
What you're asking can only be done on associative operators, certainly not something that can be done in general. I'm somewhat certain that, if GHC encounters an expression `a ++ b ++ c` at compile time it will interpret it as `a ++ (b ++ c)`. Nevertheless, in your example, the complexity of `a ++ (b ++ c)` is the same as the complexity of `(a ++ b) ++ c`. The complexity of both is `O((length a) + (length b))`. The linear complexity in the left argument is due to the structure of lists itself, not the implementation or associativity of `++`. 
Does this work for instances that are generated based on other instances. For example, would this work for `((x, y), z)` where the show instance for `(,)` is given by instance (Show a, Show b) =&gt; Show (a, b) ?
Haskell is not 'supposed' to get dependent types. Some people want an optional dependent types extension in GHC; not everyone does. It's certainly not something that standard Haskell is 'supposed' to get.
Right, but in the expression (a ++ b) ++ c, the elements of a are looped through twice, once evaluating a ++ b and again evaluating (a ++ b) ++ c, where as in a ++ (b ++ c) they are only looped through once. So in a recursive function that builds expressions like (...(a ++ b) ++ ...) ++ z the time complexity can balloon in a way that it wouldn't if it built an expression like a ++ (b ++ (... ++ z)).
I feel that, if you want compile-time literals and are comfortable using template haskell, this feature adds *very* little, while greatly increasing the complexity for users who don't need it. As it is, most overloaded string instances do not fail at run-time, so there's no need for this, really. If you are comfortable using Template Haskell, we already have a solution to the compile-time literals problem: Quasi-Quoters.
I understand what you're saying. Certainly, when you start recursing to apply `++` then associativity can cause complexity to balloon out of control. However, in the case of recursion, GHC cannot simply reassociate `++`. This would require GHC to verify that a recursive function halts. Note that the infinite chain of `a ++ (b ++ (c ++ ...))` is not bottom, while `((....((a ++ b) ++c)++...))` is.
I guess sop is indeed a equivalent to a "one datatype" solution.
Maybe I should have been clearer: I’m pretty ambivalent about dependent types coming to Haskell (I think Agda covers this space better anyway). Rather, I simply meant that as far as I know, there is lots of work being done wrt dependent types in Haskell, and having a clearly-defined evaluation mechanism that takes place at typechecking time seems to me like it would be an essential part of that. I don’t really see how you can meaningfully have dependent types without it.
Sorry, I was imprecise. Actually using it is required - try adding an annotation for example. Even a tiny annotation has the negative effect. 
Does this help? http://gigiigig.github.io/tlp-step-by-step/dependent-types.html
&gt; APL) - an array programming language that uses the multi-dimentional array as it's atomic datatype. There is this work about array-oriented programming with Haskell, inspired by APL: https://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf.
that's not what patching means
Okay, that makes sense, thanks :)
The proposed extension has nothing to do with Template Haskell, except that it might provide the easiest implementation under the hood (without requiring users to write or care about TH). Would you be willing to elaborate? &gt; I feel that, if you want compile-time literals and are comfortable using template haskell, this feature adds very little, while greatly increasing the complexity for users who don't need it. I don't understand how this is true. Users that do not enable the extension will have the exact same experience as before. Users that enable the extension will have a different way of handling literals that is safer and permits more reasonable instances thanks to that compile-time safety. Users that want safer literals are now required to use `QuasiQuotes` to accomplish this goal. If you're uncomfortable enabling `QuasiQuotes`, there's simply no way to get this without allowing the full power of `TemplateHaskell` and all it brings. This extension only requires compile-time evaluation (whether that's TH, type families and a bunch of helpers for `Symbol` and a type-level `Char`, or a new `constexpr` sort of syntax backed up by `StaticPointers` and `SafeHaskell`). If the underlying implementation uses `TemplateHaskell` in a way that is not visible to the user, then any objection based on inelegance or aesthetics doesn't make sense to me. Objections related to cross-compilation, portability, or compile-time performance are fair, of course, but they don't seem like a good reason to block an optional extension. &gt; As it is, most overloaded string instances do not fail at run-time, so there's no need for this, really. Please don't generalize from your own experience so much -- I've wanted this, as have many other people. There's no "need" for anything, there is only the cost/benefit analysis. If this buys safety and expressiveness such that the implementation cost is worth it, then it should be implemented. &gt; If you are comfortable using Template Haskell, we already have a solution to the compile-time literals problem: Quasi-Quoters. You can say this about almost any feature in Haskell though. Why introduce `Generic`s, when Template Haskell has you covered? Why introduce `do` notation, when you just use a `QuasiQuote`? `TemplateHaskell` is a massive, unwieldy, and unprincipled hammer. The more things we can shift *away* from needing explicit `TemplateHaskell` support, the better. This proposal aims to do precisely that -- take a useful and common pattern for literal expressions and liberate it from `QuasiQuotation`.
Does a function calling `showAnything` itself require an `Emerge (Show c)` constraint, or does the compiler plugin discharge the constraint?
Good luck: your compiler runs at compile time, but the type might only be known at runtime! Here is a blog post in which I explain [how to construct runtime types which are bigger than any type observed at compile time](http://gelisam.blogspot.ca/2014/12/the-99-bottles-of-beers-of-type-systems_21.html).
I'm not talking about TemplateHaskell generally. I am talking about this specific use of TH as an implementation detail that is hidden from the user. I've definitely had problems with `-fno-code` and TemplateHaskell before, but I'm unable to reproduce it minimally -- if I have module A where import B (x) main = print $(lift (x * 2)) module B where x = 5 :: Integer then this compiles fine with `-fno-code`.
&gt; then this compiles fine with -fno-code. Maybe try using a function instead of a global value? Any specific use of TH is still the same problem as general TH without a major revamp of TH. Whether or not it's hidden from the user doesn't matter; the problem is in the architecture and implementation, not the user.
&gt; Would you be willing to elaborate? I should have said 'code running at compile-time', perhaps? I dunno, it seems to me that even Template Haskell is a bit of a hack. As others have correctly pointed out, we don't really have a proper notion of executing Haskell terms at compile time. The only thing that can really be reasoned about are type-level terms. &gt; I don't understand how this is true. Users that do not enable the extension will have the exact same experience as before. Users that enable the extension will have a different way of handling literals that is safer and permits more reasonable instances thanks to that compile-time safety. According to the proposal, if I want to write `IsString`, wouldn't I have to use this new interface? &gt; Please don't generalize from your own experience so much -- I've wanted this, as have many other people. There's no "need" for anything, there is only the cost/benefit analysis. If this buys safety and expressiveness such that the implementation cost is worth it, then it should be implemented. What's the issue with using Quasi Quoters? I honestly can't see why you'd not use that in this case. &gt; You can say this about almost any feature in Haskell though. Why introduce Generics, when Template Haskell has you covered? Because Generics offer you an interface to work with data types at a level where we have good enough semantics. Template Haskell does not. It seems to require access to the architecture you are writing for. I'm not sure why this is true, but it is (or at least was in the past). I think having compile time literals would be great. Others have suggested a type-level thing. I think this is a perfectly fine idea. The proposal seemed to be at the term-level though. It is not abundantly clear what it means to evaluate a term at this level. For example, I imagine the point of this is to avoid bottom, however, a perfectly valid implementation of your proposal is fromRight . compileString This doesn't buy us anything, really. It could still be bottom. You may object and say .... well the term should be evaluated to WHNF. Okay, but then there could still be bottoms inside. Then you say, well it should be evaluated to HNF. Okay, that's fine too, but then you lose the ability to express infinitely recursive data structures. For example, suppose my implementation is newtype MyInfiniteString = MyInfiniteString String instance CompileString MyInfiniteString where compileString a = MyInfiniteString (cycle a) Well, we can't compile `cycle a` to HNF. You say 'well this is okay, because this still won't cause the compiler to emit bad code -- it would just cause it to hang'. That's true, but now your extension has changed the semantics of `fromString`, because I can write instance IsString MyInfiniteString where fromString a = MyInfiniteString (cycle a) Since the default instance exists, if you choose to compile to HNF, now a previously valid (and sensible) `IsString` instance causes the compiler to hang.
I 100% agree with this sentiment. I think people throw out the term 'dependent type's to mean this kind of unification, without realizing what the term fully implies. I would love to be able to write Haskell functions at the type-level, I'm just not sure we want types to depend on values. 
[removed]
Changing `x` to `x = (*2) :: Integer -&gt; Integer` and `main = print $(lift (x 4))` doesn't break either. &gt; Any specific use of TH is still the same problem as general TH without a major revamp of TH. Whether or not it's hidden from the user doesn't matter; the problem is in the architecture and implementation, not the user. I'm claiming that the specific subset of TH in use here does not have the same problems that TH generally has. Would you be willing to elaborate on how my claim is invalid?
I love this series, especially having recently given some talks at my workplace on the subject. But damn, I can't imagine having the endurance to continue working on one series of blog posts over that many years. It's basically a Robert Jordan series!
Thank you for clarifying :) &gt; I should have said 'code running at compile-time', perhaps? I dunno, it seems to me that even Template Haskell is a bit of a hack. As others have correctly pointed out, we don't really have a proper notion of executing Haskell terms at compile time. The only thing that can really be reasoned about are type-level terms. I don't think that's necessarily true. Haskell2010 itself doesn't support *any* kind of compile-time evaluation. GHC extends Haskell with type-level programming and TemplateHaskell value-level programming that both occur at compile-time. &gt; According to the proposal, if I want to write IsString, wouldn't I have to use this new interface? Nope. This introduces a new set of type classes. The old machinery for doing this at runtime would still be present as a backwards-compatible fallback. In today's Haskell it'd be `instance {-# OVERLAPPABLE #-} (FromString s) =&gt; CompileString s where compileString = Right . fromString`, though I suppose GHC could just do what it already does (eg insert a `fromString` invocation to the literal if there's no instance of `CompileString` available.) &gt; What's the issue with using Quasi Quoters? I honestly can't see why you'd not use that in this case. I think that Chris Martin expressed this nicely [in his GitHub comment](https://github.com/ghc-proposals/ghc-proposals/pull/124#issuecomment-382438824). I do use QuasiQuoters in this case now, because that is the way to do this now. I would prefer not to :) &gt; Because Generics offer you an interface to work with data types at a level where we have good enough semantics. Template Haskell does not. That is precisely what I am trying to move the needle on here :) There's a principled, reasonable feature here, that we currently use `QuasiQuotes` for. If we get this feature in, then we can drop a use case for `QuasiQuotes` and use it less than we do now. In the same way that introducing `Generic` allowed us to stop using TemplateHaskell to derive instances, introducing `CompileTimeLiterals` will allow us to stop using `QuasiQuotes`. Correct me if I'm wrong, but you're saying "Why not use QuasiQuoters?" and also "I oppose this proposal because a potential implementation uses QuasiQuoters." If you didn't mind quasiquoters, then would you have issue with that implementation detail? &gt; I think having compile time literals would be great. Fantastic! We are on the same page then :D What remains is an implementation detail. &gt; The proposal seemed to be at the term-level though. It is not abundantly clear what it means to evaluate a term at this level. The proposal doesn't particularly care how this is done -- an implementation that `singleton`-izes the class method and applies it to a `Proxy literal` instead of a real literal is fine by me (though I am not likely smart enough to implement it). An implementation that does this entirely through `DependentTypes` sounds cool too. An implementation that gives GHC a `constexpr`-like concept (possibly a pure+safe variant of TemplateHaskell?) is cool too. These implementations all feel very far away, though. An implementation based on `QuasiQuotes` internally can be implemented now (possibly even by me?), and later swapped out for a better one when the time comes. I do have a preference for the simpler, value-level programming, because it is *much* easier and more flexible than Haskell's type-level programming currently. Perhaps DependentTypes changes that, but it looks like we're at least 2 years out before serious effort are going to be put into it. &gt; For example, I imagine the point of this is to avoid bottom at run-time, however, a perfectly valid implementation of your proposal is `fromRight . compileString`. Do you mean `instance (CompileString s) =&gt; IsString s where fromString = fromRight . compileString`? That would indeed defeat the purpose and isn't part of my proposal at all. I don't believe that the proposal would have any problem implementing your `InfiniteString` type.
But if the type level solution solves all the same problems and doesn't have *any* of the problems hat TH does, how is that not just obviously the better option?
&gt; I'm somewhat certain that, if GHC encounters an expression `a ++ b ++ c` at compile time it will interpret it as `a ++ (b ++ c)`. Yes, `(++)` is right-associative (as is `(&lt;&gt;)`).
What does [ANN] mean in this context? I saw it on another post the other day.
&gt; I don't believe that the proposal would have any problem implementing your InfiniteString type. May I suggest a much more general, but much lighter-weight extension? Why not just introduce a pragma "{-# EVAL ... -#}". This would have the effect of evaluating at compile time the expression the pragma applies to. You could provide options for how the evaluation is to take place. Calls to `error` or `throw` are converted to compile time exceptions during evaluation. Then, if you have a partial `IsString` instance... x :: MyComplicatedDataStructure x = {-# EVAL whnf #-} "String representation" This has the advantage of providing compile time literals, while also providing compile time error messages. Better yet, it doesn't require any new syntax and is 100% backwards compatible.
Well, it doesn't *exist* now, for starters. Integers and lists seem to be totally fine. Rational literals aren't supported at the type level at all right now. `Symbol`s are entirely opaque and don't support any level of parsing. They'd need type-level characters and either an `Uncons :: Symbol -&gt; Maybe (Char, Symbol)` type family or a `ToList :: Symbol -&gt; '[Char]` family. And then, all of your parsing and validating needs to happen using type families, which are vastly less pleasant to work with than ordinary functions. Error reporting, likewise, must use the much more verbose `TypeError` machinery instead of a mere `String`. [I wrote elsewhere](https://www.reddit.com/r/haskell/comments/8d1fz7/ghc_proposal_compiletime_literal_values/dxkqac0/) on the difference between the type and value level validation. A middle ground is using `singleton`-style lifting to generate type families from a value level function declaration -- the definition is value-level (and GHC complains if it can't lift it for some reason), and the evaluation happens at the type level. This would involve porting `singleton`'s code generator into GHC itself. The question is then: is the TH solution so bad that we shouldn't implement it at all, and instead wait until all of this other stuff is done? That doesn't seem clear to me.
I do this in [`closed-world`](http://github.com/tathougies/closed-world), but I forgot how it worked, haha!
Announcement... usually used for new packages or features in packages. 
&gt; Just run the expression The difficulty behind "run the expression" is precisely why this isn't a trivial extension to implement. This was also suggested [in this GitHub comment](https://github.com/ghc-proposals/ghc-proposals/pull/124#issuecomment-382462662).
Or we do it with a TH thing *now*, reap the rewards for those that can use TH and don't mind it, and replace it with types later :P I'd rather have something now that can eventually be improved than waiting forever on the perfect thing.
I mean run the expression as in 'compile this expression separately and run it to ensure it terminates'. Not 'evaluate this expression until its in normal form, and then convert the STG implementation of the normal form back into an expression'. This is the equivalent of using the TH machinery to simply evaluate an expression, and completely ignores the complexity of having to turn the resulting thunk back into code.
I don't see how this relevant to the proposal.
Well we're not exactly clamoring for this feature *now*. I think tysonzero's examples, especially the hlist one, show how insufficient it is not to represent this stuff at the type level. We won't just be able to swap out a backend for free once we get dependent types. The TH solution just won't have the same type signatures, and thus won't be a drop out for the dependently typed solution. I really don't think we should be adding complicated features to the compiler that generate new tooling problems/breakages if we know on day zero that we're going to deprecate it in a few year's time unless it's a really critical feature (which this is not). Also, we still can't just refuse to support these literals in cross compilation, and I still don't see how the TH method solves SPJ's polymorphism issue. These are complete and total deal breakers that can't be overlooked.
The syntax `'&lt;Data constructor&gt;` is called a 'lifted data constructor'. It allows you to embed data constructors at the type level. It is not standard Haskell, but is part of the `DataKinds` extension. To work with it, enable the `DataKinds` extension, by adding `{-# LANGUAGE DataKinds #-}` to the top of the file or by giving the `-XDataKinds` option to GHC. For reference, Ghc tells you that this is the problem: &gt; (Perhaps you intended to use DataKinds)
If you do need to use the Local constructor as a type here then you should enable DataKinds like the error suggests: Put `{-# Language DataKinds #-}` at the top of the file. I'm not familiar with the library though so this could be incorrect, but it it works here then it's almost certainly the right solution.
This is super cool! I'm impressed with how straightforward the plugin code is.
Is that part of the list fusion framework?
You are very kind, my friend. Writing this many blog posts is made easier by the fact that I usually only do one a year 🤣
Thank you.
Thank you.
[This article](http://neilmitchell.blogspot.ca/2008/02/adding-data-files-using-cabal.html) demos embedding/packaging non-source files with a package. Perhaps then use `getDataFile` with `persistManyFile`? `persistManyFile lowerCaseSettings =&lt;&lt; runQ . mapM . getDataFile $ ["Data/Foo/A/Entity", "Data/Bar/B/Entity"]`
This worked! Thank you! Twidge is installed and tested in our production environment (Which is going to be used for something really cool. . .thank you!) I will look into doing a pull request with hoath and possibly reaching out to package maintainers as well, since Twidge is in package repositories as a binary for Debian, Ubuntu and Redhat/Fedora/CentOS and what I did was update the character limit to match Twitter's new limit of 280.
https://github.com/alexeyzab/cards-with-comrades https://github.com/thoughtbot/carnival are some of the more worked examples I'm aware of.
this post is marketing noise. 
I'll try that out. Thank you!
Wouldn't bother me if the page weren't such a shameless plug for their business. - "Explore FPComplete's Software Solutions for Advanced Data." - "Get our blog posts in real time. Subscribe now." - "I Want a Cloud Readiness Assessment." I'm all for evangelism. But don't use it to sell to me.
oh no
There's [snowdrift.coop](https://git.snowdrift.coop/sd/snowdrift) too.
I like this a lot.
Compiler optimisations will only work in a certain set of cases where the compiler can inline enough to “see” the badly associated `++`. A more robust approach is to use a type like https://hackage.haskell.org/package/dlist-0.8.0.4#readme More general than that is some of the libraries that /u/edwardkmett has written to handle Monoidal operations such as https://hackage.haskell.org/package/reducers - I’m pretty sure there is a YouTube video of him on this topic but I can’t immediately find it. 
Most of your solution is implementing Vector and Matrix. You could move that to another module and your solution would be fairly simple.
A cheeky alternative to `Partial` would be `Mayhaps` (owing to its middle ground between Either and Maybe); an even cheekier one would be `Meh` (which I find amusing)
Clearly its the `WellActually` monad. =)
There is a typo in LANGUAGE.
Nice! This looks quite similar to [Foreman](https://github.com/theforeman/foreman) and its Haskell clone [Houseman](https://github.com/fujimura/houseman).
Nice! This looks quite similar to Foreman and its Haskell clone Houseman.
I don't understand. The first example doesn't occur with a TH-based approach, since you don't need to put value in the type? Also, of course you can't make this work with polymorphic literals. The entire point is you want to be safe at compile-time, and you can't be if the fromIntegral function is partial!
&gt; I don't understand. The first example doesn't occur with a TH-based approach, since you don't need to put value in the type? Ok I think you are completely missing the point of that example. It's not about NEEDING to put the value in the type. It's about IF YOU WANT to have the value in the type (say for compile time guaranteed safe indexing or similar). With my approach: 5 +. 5 :: CompileTimeInteger 10 5 + 5 :: Integer [1, 2, 3] ++ [4, 5, 6] :: [Int] [1, 2, 3] ++. [4, 5, 6] :: SList 6 Double ["foo", 3, 6] ++. ["baz"] :: HList [String, Int, Double, ByteString] Every single one of the above works verbatim and with no extra type annotations needed. With the TH approach examples 1, 4 and 5 will all fail with an ambiguity error. &gt; Also, of course you can't make this work with polymorphic literals. The entire point is you want to be safe at compile-time, and you can't be if the fromIntegral function is partial! But that's the thing, with my approach polymorphic literals are ALSO safe. So if you do: f x = (-2) * x f 5 :: Nat My approach will still catch that error at compile time, whereas the TH approach will not, as it can't know what types `f` is bad for without enumerating them exhaustively, and `f 5` looks sane at the call site since `5` is a positive number. Whereas my approach will now constraint `f` with `(Num a, fromInteger (-2) a) =&gt; a -&gt; a` and thus will not typecheck with `Nat`, only with `Int` and friends. &gt; I think we must be thinking of two entirely different problems that need solving. What specific problem do you think the type level approach is trying to solve? The type level approach does everything the TH approach does, with the huge benefit of working with polymorphism and having good inference, and without the downsides of TH (portability, hidden unsafePeformIO, lacking polish etc.).
It looks like you are misusing typeclass trying to translate an OOP approach to Haskell. It is a common anti pattern and even if it is doable (as you did) it makes things complicated as you realized. You could try removing the typeclass and just uses normal datatype s.
My first thought was to just use type family, then I saw your comment about not being able to use type families because of type synonyms. I'm just not sure as you are using type families anyway for the lookup, how your initial solution will work with types synonyms ? (It feels that you should try to stick to a direct type family though)
Hi there, in order to not reinvent the wheel, have a look at this: https://wiki.haskell.org/Leksah. I have no experience with it though.
When you installed nix on your cluster, did you use the binaries or did you build from source? The default binary installer script seems to expect sudo, but the source depends on libraries that we don’t have (e.g. OpenSSL). I have ideas for working around either issue, but it would be helpful to know which route you took.
[removed]
If you also like reading about architecture, [here](http://www.aosabook.org/en/posa/warp.html) you can find a description of the internals of the Warp server.
I am sure that it's not possible to make a good IDE (especially for such complicated language as Haskell) for a single person. Even if project is an open-source, I don't think community can help much (unless it's Hacktoberfect). JetBrains company (which makes Intellij IDEA) has hundreds of developers with 10-15 developers are working on a single IDE. Developing IDEs is an extremely difficult task. You need to develop nice GUI (to give your users nice UX), you should think a lot about performance (including multithreading programming), and so on. There're people who are already working on IDEs for Haskell. Maybe instead of starting new project it's better to help somebody else? Personally, I think, that the chances the new IDE without company behind it will succeed are almost zero. All Haskell tooling is broken even if there're a lot of amazing people working on it. It's extremely hard to develop Haskell tooling. See recent news. Maybe you can help this person who implements new approach for GUI programming in Haskell during his PhD thesis. A lot of people there requested Haskell IDE. * https://www.reddit.com/r/haskell/comments/7l203h/what_haskell_programslibs_need_a_gui/ Or maybe you could just fix some of the existing bugs in Leksah. Community will benefit from this! * https://github.com/leksah/leksah/issues
That's an awesome read.
I don't understand what is this for? As in, genuinely curious; in what situations might we need to defer instance lookup to runtime?
I believe it would be more beneficial for all to help with haskell-ide-engine as it provides IDE functionatility as a language server which aren't tied to any specific IDE or editors.
It is more for performing checks against "is this thing Showable? If so do this, else do that." It pretty blatantly violates the open world assumption, but that may sometimes be just what you want.
You don't need to install nix on the cluster! You build everything on your own computer, and follow the steps on the post. That way you ship not only the binary but also all its dependencies. PRoot is there to fix the dependencies location.
It is indeed nice to know the tradeoffs of this method. As you assumed, my executable was just a glorified calculator, so this was not a problem. I will keep on my notes to test someday the difference when using the custom store!
If I remember well, Yi can be embedded in other applications, so it is possibly a good base for an IDE.
Thanks, I really like the way you worded this. 
Thank you!
Brilliant
Thank you, nice and simple explanation. It is for an exam, my final year in Comp sci. Wish me luck
I didn't mind until they used the phrase "Elite Community". I'm having a hard enough time convincing my boss that Haskell is great for DevOps without confirming this stereotype.
IDEs as a single integrated solution are something more and more languages are moving away from. They probably still make sense for big companies like MS that "control" a language and most of the ecosystem and even then there's VS Code which is a much more light weight solution. It's a much better idea to develop various components separately. An editor for the actual text, a LSP server to get information from the compiler, a LSP client to somehow display that information and allow the user to interact with it. All of those components exist already for pretty much every editor. Why do you think a dedicated IDE would do a better job?
This seems very similar to how Haskell is compiled to Core. 
I am also learning. Here is my attempt to make your types and typeclasses cleaner: newtype Vector = Vector {getV :: [Float]} deriving (Eq, Show) type Row = Vector type Col = Vector newtype Matrix = Matrix {getM :: [Row]} deriving (Eq, Show) class Vec v where scale :: Float -&gt; v -&gt; v vecSum :: v -&gt; v -&gt; v vecLen :: v -&gt; Int vecNegate :: v -&gt; v instance Vec Float where scale = (*) vecSum = (+) vecLen = const 1 vecNegate = (0.0 -) instance Vec Vector where scale f = Vector.(map (f *)).getV vecSum a b = Vector $ zipWith (+) (getV a) (getV b) vecLen = length.getV vecNegate = Vector.(map (0.0 -)).getV
Hey man - sorry I have just seen this! You know what, most of our feedback about bugs come from firefox mobile and we've have numerous tickets lined up for fixes here! I will pass this on to the tech team now though so thanks for the heads up :)
Stack build is failing with the error message "File name does not match module name" but I can't determine why. This is the library file: src/Parsers/System/Uname.hs `module Parsers.System.Uname where` `onefunction = "hi"` This is the package.yaml file `library: source-dirs: - src/Parsers/System dependencies: - hspec - hspec-megaparsec - megaparsec - QuickCheck executables: parserapp-linux-exe: main: Main.hs source-dirs: app ghc-options: - -threaded - -rtsopts - -with-rtsopts=-N - -Wall dependencies: - configurator - parserapp-linux tests: parserapp-linux-test: main: Spec.hs source-dirs: - test - test/Parsers/System ghc-options: - -threaded - -rtsopts - -with-rtsopts=-N - -Wall dependencies: - hspec - parserapp-linux` This is the error message `File name does not match module name:` `Saw: ‘Parsers.System.Uname’` `Expected: ‘Uname’` `|` `1 | module Parsers.System.Uname where` `| ^^^^^^^^^^^^^^^^^^^^` If I change the module name to `module Uname where` the code builds with `stack build`, however, I read that the module name should follow the directory layout, as listed above. What are some common reasons this error comes up?
Can someone please think of the poor developers like me that store their files in a VM but use a Windows editor!
I really like both the approach and the explanation style of this article, so I'll be referring to it often in the future.
&gt; will not work since Foo may be a type synonym for e.g. an application of some other type family and so could not appear on the LHS of a `type instance`. I don't understand, type instance Map String = () -- type synonym on the left seems to just work.
Windows machines at work but we develop for and build on Ubuntu (hence the VMs). Not a fan of terminal editors.
&gt; “I just want to” is not a compelling reason Not true. Who are you to scold OP about wanting to work on something for the hell of it? I appreciate the arguments about the best ways of helping the community but it isn't mandatory. 
There is a kind of recurrence relation between the distributions of real data in database and the fake data generators. You observe the real data for outliers, you modify your logic to prevent those and migrate your outliers in real data, then that ends up changing the distribution of your data over time. That change, prompts you to update the distribution model you use in your fake generators... It would be interesting to automate that loop, having a complementary library that builds histograms and learns simple distributions and outputs them for generating fake instances.
&gt; My ideal use-case would be an ide server running on the VM which my editor talks to over the network. That's one of intended use cases for LSP servers, actually. Though i haven't yet seen editor plugins or LSP servers that communicate over anything except stdio. 
Oh, thanks for bringing to my attention that the the GitHub references are incorrect. I'll fix that and upload a new version. Meanwhile: All references to &lt;https://github.com/emilaxelsson/raw-feldspar&gt; should be changed to &lt;https://github.com/Feldspar/raw-feldspar&gt;
this!
Type synonyms are fine on the left, but applications of type families are not. For example: type Foo' = Lookup Foo M type instance Map Foo' = Bar fails.
AFAIK Lookup simply compares the key types for equality (e.g. via `~`) which is totally allowed with applied type families.
I see. That situation sounds quite tricky.
I think this [Build yourself a Haskell web framework](https://github.com/cbaatz/build-a-haskell-web-framework) is very useful for learning web development in Haskell.
Thanks for the example! Getting this working with the new subdict generation stuff in 0.1.2 should be as easy as lifting instances of `Show` through `Emerge`, eg, if we have: instance (Show a, Show b) =&gt; Show (a, b) we should be able to lift a instance (Emerge (Show a), Emerge (Show b)) =&gt; Emerge (Show (a, b)) which shouldn't be very hard. The neat thing is the derivation of this doesn't need to provide an implementation of this instance, the subdict generation stuff should find it no problem. I 'll look into the implementation tomorrow :)
 MS made both VSCode and LSP. So, centralized ecosystems don't work for them either.
Yeah, that's why I mentioned VS Code.
Thanks!! I'll do that.
Yeah.. You have a point.
I'm sure you are right but could you give me an example of type synonyms not working with direct type family +and working with lookup)?
Look at other examples of linear algebra apis in haskell. Some examples in order of complexity: Subhask, htensor, numhask, hmatrix.
Ok. So the problem the value level approach is supposed to solve is a shorthand "smart constructor" syntax that lets you use literals for things where the range of literals that are syntactically valid as literals is _smaller_ than the range of literals that you actually can use for the structures in question. And it looks like the type approach, if I understand it, is to have the value both as a literal, and also in the type itself, and then we can use the constraints of the type to check that the literal is of a valid form and throw a type error otherwise? So the idea is the conversion still happens on the value level as before, but there's also a constraint checking that relies on singletons generated by the literal value that occurs at the type level too?
Sure, I will look into them. Thank you.
Sure! Consider this trivial type family type family Switch a where Switch Int = Double Switch Double = Int Switch a = a type Foo = Switch Int -- aka Double type Map = '[ '(Char, Bool), '(Foo, [Int]) ] If you load this up in GHCi, you'll see that `:kind! Lookup Double Map` and `:kind! Lookup Foo Map` yield `[Int]`. On the other hand: type family MapFamily a where MapFamily Char = Bool MapFamily Foo = [Int] fails to compile with Illegal type synonym family application in instance: Foo
Also https://github.com/yi-editor/yi/blob/master/README.md
It's not intended for you, and is labelled up front as a non-technical post. Part of FP Complete's mission is to communicate Haskell's value to business people.
The source directory should simply be `src` if you want the module name to retain the qualifications of all the higher level directories. If the tool included the entire directory structure in the module name then your module name would have instead been `Src.Parsers.System.Uname` or perhaps `Home.MyUserName.Project.Src.Parsers.System.Uname`.
`at @SomeType val`
ghcid builds now?
This doesn't discuss auth, but I think it is a good introductory video on snap: https://vimeo.com/59109358
Good reminder to finally read this series, really enjoyed it! I am looking forward to reading about recursion scheme fusion. I wonder if it'd be possible to get partial loopification. That is, a recursion scheme in general might not be tail recursive but constructor specialization might be able to split the function into pieces of which some might be join points?
Seconded. This stereotype hurts the community and the commercial prospects of the language. Also I fear association with the block chain will hurt us in the long run as that hype bubble continues to deflate and people remember databases are actually pretty good at this stuff already.
Ooh, that sounds like an interesting project.
Hey Carter, why do you think that? Do you disagree with the statements? If yes, which ones? If somebody set out to build a cryptocurrency, which language would you recommend them? I would be happy to get some feedback. I don't think I oversold Haskell here.
I'm working on a command line util to rapidly scaffold out fullstack haskell apps. It is extremely raw right now - and I don't intend it for public use for a week or two - but you could probably use it if you wanted: https://github.com/smaccoun/create-haskstar-app And here's the back-end i'm scaffolding that includes auth and db stuff etc.: https://github.com/smaccoun/haskstar-haskell There's a pair programming channel opened recently, if you ever want to talk through it i'd also be happy to do that just to help get you started!
That's on the right track. But actually the value starts out solely on the type level. You can then use KnownNat or KnownInteger etc. to go from type level to value level. Now in some situations you will have some stuff on the value level right off the bat. Such as with list literals, the types of the elements are stored solely on the type level, but the values are stored on the value level. This way you can do all your logic in a compile time correct way on the type level, and eventually swap over to a value once it has been validated. Or you can just leave it in the type system if you want to do some arithmetic with types rather than values; and convert to a value later.
I see. But you could define. `MapFamily Double = [Int]` still be able to use `MapFamily Foo` couldn't you? Ok it's not ideal but should solve the performance problem.
If your types can admit one type parameter (as if we're saying "a Vector of `a`s"), they can be made instances of Functor: newtype Vector a = Vector { getV :: [a] } deriving (Eq, Show) instance Functor Vector where fmap f (Vector v) = Vector $ map f v (where `fmap` is a generalization of the usual `map` for lists, and it has an infix synonym written `&lt;$&gt;`) Then all the mapping functions can be shortened like so : scale f v = (f *) &lt;$&gt; v vecNegate v = negate &lt;$&gt; v etc. This polymorphic version is handy (and in fact it's how the real Vector is implemented : https://hackage.haskell.org/package/vector-0.12.0.1/docs/src/Data.Vector.html#Vector ) because the parameter `a` can be anything from Double to Float to large arbitrary datatypes, and it's only constrained by the functions that use it. 
You are correct, and that is likely what I'll end up doing if there is no better solution. It's just a bit inconvenient for my use case since the type families I'm using expand to much more complicated types than just `Double`.
Some examples of how Haskell forces you to write code that is explicit, as compared to other languages: * If you want to look into any form of container (like `Maybe`), you need to do so explicitly. * If you want to execute IO, you need to do so explicitly, you cannot do it from within pure expressions. * If you want to discriminate a sum type (`union`), you need to do so explicitly, you cannot just assume it to have some contents. * If you want to unpack structurally equivalent but nominally different data types, you need to do so explicitly, you cannot just cast them. Because you have to do these things explicitly, Haskell is hard to misunderstand. There's no implicit stuff going on that you thought is happening but isn't. If you design your types cleanly, the exaustiveness checker for `case` statements can tell you whether you have covered your corner cases or not.
I see. have you considered TH ?
I did a bunch of linear algebra in a previous project and came up with [this typeclass hierarchy](https://github.com/george-steel/maxent-learner/blob/master/maxent-learner-hw/src/Text/PhonotacticLearner/Util/Ring.hs). Without using a `Nat` type parameter to control the length of your vector type, it's probably best to use fixed products and a treat variable-length lists a polynomials like I did.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [george-steel/maxent-learner/.../**Ring.hs** (master → dfa2938)](https://github.com/george-steel/maxent-learner/blob/dfa29387338418e55f681886a7b3e5ca46bba459/maxent-learner-hw/src/Text/PhonotacticLearner/Util/Ring.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxn9yjs.)
Nevermind; the plugin definitely needs to give an instance for the dictionary. I'm going to hold off on this feature until 8.6 :)
I’m sorry, I can see how that could sound scolding or discouraging and that was not my intent. I meant to offer guidance from my own experience and what I’ve seen in other people &amp; projects. I have seen many “I just want to” projects fail because they weren’t followed by a *because*—because I have a clear idea of how to do it better than existing tools, because I want to use it as a learning exercise, because I can’t stop thinking about it, &amp;c.
TBH I'm not entirely sure what the open world assumption means here. Mind elaborating on what it is and why this violates it?
exactly. This post does nothing to educate / share knowhow to either newcomers or experts. It just does repeated calls to action for folks to be an FPCo customer. this is a community forum, not an advertising page. 
That clears things up nicely. Thanks,
Yes, you've rediscovered \[free monads\]\([https://hackage.haskell.org/package/free\-5.0.1/docs/Control\-Monad\-Free.html#t:Free](https://hackage.haskell.org/package/free-5.0.1/docs/Control-Monad-Free.html#t:Free)\). Note the definition.
I do think haskell is a fine tool for building programming languages that can safely run on top of a distributed data base. But the calls to action on the page are all generic sales funnel ones, not domain specific. I agree with the intent of what you wrote, just not the way the calls to action appear.
As in, does the `master` branch of `ghcid` compile? Yes.
I kind of figured something like this would be the case. I'm learning scheme, and was inspired to implement schemes lists in Haskell, which led me to abstract it and end up with this. 
Would love to have your help on Leksah! Please give [try it Leksah out](https://github.com/leksah/leksah#getting-leksah). You will most likely encounter a bug or missing feature and that is often a good place to start. File an issue and I will do my best to point you in the right direction in the code to fix it. There are lots of things to do and we desperately need more contributors. There is a [Contributing.md](https://github.com/leksah/leksah/blob/master/Contributing.md) and #leksah IRC channel. Feel free to file an issue with any questions you have as that is a good way to make sure the answers can be found by others. Lately I have been focusing on improving the support for using Nix. It is getting pretty good and I am thinking we should do an official release of 0.17 soon. Leksah currently uses Haskell GI and Gtk, but I am keen to explore adding a Reflex-DOM based UI as well. As a first step I think we should consider migrating the internal workings of Leksah to reflex. Next we would add a reflex-dom based window to the app to contain the new UI. Once it is working nicely we might add an option to select between the two UIs. Adding support for HaRe, brittany and stylish-haskell might also be an interesting project and would have the added benefit of allowing us to use them to clean up some of the cuft in the Leksah source. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [leksah/leksah/.../**Contributing.md** (master → 364cd77)](https://github.com/leksah/leksah/blob/364cd77fffab41a4a06738eeea8b8c4b084c4b5b/Contributing.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxnflez.)
The open world assumption is central to the way GHC compiles incrementally. It doesn't have a whole program optimization pass like JHC where it buiilds the entire codebase all at once. Instead, each module is built incrementally. This means that at no point do you actually know "all" instance statements in the program. There could always be another module introducing an orphan or something. This means that there isn't a clear time to ever dispatch the `Emerge p` constraints. There may not be a `Show a` instance _now_ for some `a`, but you might wind up in a context where it becomes available. e.g. calling a function that does something fancy with rank-n types and higher order functions to dispatch it, where that is handled in a module that hasn't been written yet wherein an orphan exists. This means its possible to make positive statements about instances that exist, but in general GHC can never talk about negative assertions like "something that isn't an instance of `Show`". Now, one could envision a world where you could make up "negative assertions" like `no instance Show (a -&gt; b)` stating that `(a -&gt; b)` shall never pick up an instance of `Show`, and that any attempt to define it would be an error. _Now_ its safe to talk about discharging those negative assertions, as there exists no world in which someone can write the instance later as an orphan. So to talk about when the plugin can soundly discharge `Emerge p` or `Deferred p` constraints, you need some dodge around this "people can write instances later" problem. Here's an effective and sound way one might do that. If you say that the plugin only ever tries to discharge `Embed p` constraints negatively when `p` contains no type variables, but can freely discharge `Embed p` constraints positively when `p` is dischargeable in scope without incurring any extra obligations (even when it does have type variables) then the whole system is sound as long as you pretend orphan instances do not exist. In this setting the constraint would just pass along the `Emerge p` constraint whenever p had type variables and it couldn't discharge the constraint yet. It'd continue to infect the type signature of the function or use side of `emerge` just like `Show` infects th use site of `show`, and removing it from the signature requires an instance. Why? Because if you are in a context where you can tallk about `Ord Foo`, then both the class `Ord` and data type `Foo` must be in scope, and so the instance must be in scope or not exist at that point, unless the instance is an orphan. *tl;dr* orphans and the fact that we never know if we have all the code make discharging `Deferred` constraints hard. If you pretend orphans don't exist, then there is a consistent way to view the problem.
I think the "elite community" section was the most troublesome to me. Language like that drives lots of people away from haskell who feel like haskellers are smug or are going to look down on them, or that they're not "up to the standards" or whatever. And it dually _attracts_ people that I'm least interested in building a programming language ecosystem with -- people that get into the language because they want to feel elite and smarter and better than others. There was a fair amount of discussion over this stuff here: https://twitter.com/argumatronic/status/986997729988677634
Congratulations on discovering it. I must mention the [cofree comonad](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Comonad-Cofree.html#t:Cofree) data Cofree f a = a :&lt; f (Cofree f a) The similarities become clearer with a change of notation Free f a = a + f (Free f a) Cofree f a = a * f (Cofree f a) It's good to contrast this with `Fix`, which only has the recursive component: newtype Fix f = In (f (Fix f)) If we pick `Free` with no variables (`a ~ Void`), we get `Fix` Fix f = Free f Void `Cofree` with dummy values (`a ~ ()`) is also `Fix` Fix f = Cofree f () This has a deep reason, [more information](https://stackoverflow.com/a/43203104/165806).
The story seems very similar to that of [How to Architect a Query Compiler](https://dl.acm.org/citation.cfm?id=2915244), published in the database community but with a strong PL slant.
He seems like a really humble and nice guy.
From the title I was hoping for something I've wanted but not yet badly enough to fully work it out on my own I have these two actual functions in a production codebase right now, for working with the same data at two depths (disclaimer: I did not choose the names): ``` weightLift :: (Functor g, Functor f) =&gt; (a -&gt; b) -&gt; f (g a) -&gt; f (g b) weightLift f = getCompose . fmap f . Compose heavyLift :: (Functor f, Functor g, Functor h) =&gt; (a -&gt; b) -&gt; f (g (h a)) -&gt; f (g (h b)) heavyLift f = getCompose . getCompose . fmap f . Compose . Compose ``` It seems like there should be a way to create a `Compose` variant that understands how deeply nested it is (via KnownNat?) and provides the ability to `fmap` at arbitrary depth.
Looks like you went with Elm for the front end! That's awesome! I've been reading about Elm as well. Will be keeping a close eye on the progress of your project :) 
Can confirm. A really wonderful human being!
s/forces/guides/
Just write `fmap . fmap`, `fmap . fmap . fmap`, etc. --- Here's how you can give the number of functors to go through as a single number, though that seems quite overkill. {-# LANGUAGE AllowAmbiguousTypes #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE UndecidableInstances #-} import GHC.TypeNats class Functorial (n :: Nat) a b s t where maps :: (a -&gt; b) -&gt; (s -&gt; t) instance {-# OVERLAPPING #-} (a ~ a', b ~ b') =&gt; Functorial 0 a b a' b' where maps = id instance (s ~ f s', t ~ f t', Functorial (n-1) a b s' t', Functor f) =&gt; Functorial n a b s t where maps = fmap . maps @(n-1) main = do print $ maps @3 (+ 1) [Just (Right @() (0 :: Int))] print $ maps @4 (replicate 3) [Just (Just (Just (Just (0 :: Int))))] 
&gt; This has a deep reason, The deepness stops at another "coincidence" though. In Hask, the final object of F-coalgebras just happens to be the same as the initial object of F-algebgras. Is there deep reason for *that* coincidence?
`Free`/`Cofree` preserving initial/final objects are the real attraction. I found prerequisites for the two fixed points to coincide (section **3**, [*Theory and Practice of Fusion*](https://www.cs.ox.ac.uk/ralf.hinze/publications/IFL10.pdf)) including being a *algebraically compact* category, maybe there is a deeper connection
Have you never seen Haskell code that abuses type class constraints and/or `Data.Reflection` to *avoid* being explicit about where values come from?
I've found and then lost a reference reference for when greatest and least fixpoints coincide a few times now. This time 'round we have that it occurs in settings of continuous functions between complete partial orders (source: http://www.cs.ox.ac.uk/jeremy.gibbons/publications/adt.pdf) and that it occurs in a setting with general recursion (source: https://kodu.ut.ee/~varmo/papers/thesis.pdf). Neither of these is really better than your source. Note that it turns out the _definition_ of algebraically compact categories is categories where the algebras and coalgebras coincide, so it really doesn't move the bar much. And it turns out that as late as '99 there was no full characterization theorem for when this happens! (Source, a paper on algebraically compact functors from Michael Barr that does some work towards such a thing: https://pdfs.semanticscholar.org/e2c6/5380c69f196d79dcd2935cf15e147d9b33b7.pdf) So, I think there still _is_ no full characterization theorem. We just have a bunch of examples of categories where these things coincide, most of which come from doing something to CPO! This latter fact makes sense in light of Fiore's thesis, which puts a lot of time to exploring algebraically compact categories, and demonstrates that they are sufficient for solving domain equations: https://www.era.lib.ed.ac.uk/handle/1842/406 This is all rather funny, in that these days people _complain_ that the two coincide so we can't distinguish things, but for most of the history of this stuff the search was for ever-more settings where the two coincided :-)
Hint: `Void` is zero and `()` is one 0 + a = a 1 × a = a
That's an interesting problem! I tried to take a stab at it a handful of ways, but ran into issues each time. I did a similar thing before where I defined a typeclass whose instances are functions of form a-&gt;a a-&gt;a-&gt;a a-&gt;a-&gt;...-&gt;a And so on. The strategy that worked for me there didn't work so well here. Perhaps you could make separate post to cast a wider net and see what people can come up with. 
&gt;But it is a lot faster and the best thing is it is not trying to be a general purpose language. It is focused on front end and excelling at it. Yeah, I had my hands on it. But I am really in a doubt now. The DOM diffing and immutable state in Elm, was it a inspiration from Haskell which later got into React UI development ? Elm seems best solution for UI development, but is it really industrially strong?
I've long been sold on the merits of using Fix + some base functor being a superior approach from a mathematical standpoint. In fact, I'll go further and say the same for sums and products, too. But the problem is that all major languages are setup to push you in the opposite direction. They give you all this nice syntactic sugar for declaring inductive data, but wait, just kidding, you're not supposed to use the pretty syntax, you're supposed to use this obtuse mathematical thing with pattern synonyms. Semantically, I really like these concepts broken up into their primitive pieces (sum/product/fixed point), but syntactically, I want to use the pretty stuff that comes with the language. In practice, I almost always go with the latter, but I always feel a little bit guilty about it.
/u/bgamari Is there no Fedora release like there was for 8.4.1? I tried the debian9 version but that wants libtinfo.so.5 whereas Fedora has libtinfo.so.6. Also the listing at https://downloads.haskell.org/~ghc/ does not show the 8.4.2 subdirectory despite it existing.
Judging by that blog post alone, FP Complete's mission seems to be spreading insipid bullshit to business people. The quote below is not only nonsensical, it's a gross abuse of language and honest communication. &gt; One of the biggest strengths of Haskell for cryptocurrencies is its advertisability 
&gt; Just write `fmap . fmap`, `fmap . fmap . fmap`, etc. This is the correct answer. `fmap . fmap . fmap` *is* the thing that understands how deeply nested your composition is, and the only different between it and `deepFmap @3` is syntax. 
The "murder" part is a philosophical question that shouldn't be dismissed so quickly.
You should look raichoos talk "the free and the furious" for a generic solution using the codensity-monad to swap the parens in such cases. It boils down to a changed type &amp; no code changes.
the reason I am interested in developing Haskell IDE that. When I started to learn to learn Haskell, it was difficult for me to setup correct environment for Haskell development. it's my personal opinion.So for the new people who want to learn Haskell and to make existing Haskell development easy, having a good Haskell ide would be beneficial.
Definitely, but actually building dictionaries in plugins seems pretty tricky until /u/nomeata's patch lands in 8.6 when we can write arbitrary `CoreExpr`s as witnesses. I'm getting around this at the moment by unsafely coercing other dictionaries of the same shape into `Emerge`s: class AlwaysFails where alwaysFail :: Maybe (Dict c) instance AlwaysFails where alwaysFail = Nothing class Succeed s where succeed :: Maybe (Dict c) instance c =&gt; Succeed c where succeed = Just Dict I spent a few hours yesterday playing with `Sub` in an attempt to make an analogous instance that I could use to lift `a :- b` into `Emerge a :- Emerge b`. Managed to get something that folds `strengthen` on `Emerge` dicts into a `Maybe (() :- c)`, but it was pretty clear that it wasn't going to coerce nicely. Getting this working would be great, in that it would solve /u/gelisam's polymorphic recursion problem, but I'm not sure I have the tooling available to solve it yet. What I'd really really like is to be able to write this instance: instance (a =&gt; b, Emerge a) =&gt; Emerge b where emerge = case emerge @a of Just Dict -&gt; Just Dict Nothing -&gt; Nothing and let the usual instance resolution handle it, but alas constrains must be monotypes.
If the socket isn't HTTP, I'd recommend using the network package together with aeson. I have found that creating a socket and then feeding it to io-streams is especially easy if you are comfortable with using a streaming library.
If the events are served over HTTP(S), you can use `req`, which is flexible and batteries-included : https://hackage.haskell.org/package/req
I recently just wrote an admin dashboard for one of our systems in Elm. Considering I have never written or read Elm before it was much easier than I thought. I say give it a go and don't give up if you get stuck.
Great work! I think it is much better than a REPL in the sense that you: 1. Don't have to edit (or navigate) a line of text. 2. Have default values. 3. Have values stored in the GUI. 4. Have the ability to provide instances for user types. (Although I guess you use `Generic` instances for that.)
The `Traversable` constraint on `adi` seems unnecessarily strong. With `Functor` it still compiles.
You can use conduit and aeson to do this trivially. But it requires you to know conduit.
I've been working on this: https://github.com/Southern-Exposure-Seed-Exchange/southernexposure.com But I haven't looked at too many other codebases so I couldn't tell you how "well-architected" it is.
I don't disagree, but I'm not the sole owner of this codebase and a coworker found those fmap chains hard to parse mentally so he wrote those functions. After seeing them it just became a back-of-the-mind curiosity how/whether they could be generalized. And thanks to /u/Syrak, that itch is nicely scratched. :)
For the same reason `Float` and `Double` have an `Eq` instance: &gt; nan = 0/0 &gt; eqTest x = x == x &gt; eqTest nan False You can't use floating point numbers and expect it to be completely sounds mathematically unfortunately. If you want something better to reason about, have a look at [`Rational`](https://hackage.haskell.org/package/base-4.11.0.0/docs/Prelude.html#t:Rational) Hope this helps! :-)
I'll check it out. Thanks for the tip!
&gt; Out of the "expert Haskell programmers," how many expert application programmers are there that would hack the Haskell compiler to support code that their team writes? Quite a few. Many contributions to GHC do not come from people who are paid to work on the compiler as their main job, but people who need these features for their projects. [There will even be a dedicated track on ZuriHac](https://mail.haskell.org/pipermail/ghc-devs/2018-April/015598.html) this year to show that working on GHC is not black magic, further lower the barrier and get more such contributions. I invite you to come and be convinced. &gt; And, does it matter? Of course this matters! You can build much better systems if you yourself can get compiler support for what you need. For example, say you want to build reliable distributed systems. You can do a lot better if you can [add `StaticPointers` to the compiler](https://ocharles.org.uk/blog/guest-posts/2014-12-23-static-pointers.html). Similarly, you can improve the stability of production software if you yourself can go ahead and fix RTS bugs, like [this](https://ghc.haswithkell.org/trac/ghc/ticket/8684) and [this](https://ghc.haskell.org/trac/ghc/ticket/13497). All of the 3 Haskell shops I've worked for have use this to great effect.
You can always abuse features and do obscure stuff if you really want to. But you can easily identify when that is done. Obscure code becomes _obviously obscure_ in Haskell very easily, and by default it is one of the most explicit languages out there.
I'm surprised to see SPJ mention simplicity as a core tenet of his work. Haskell is many things, but I would hardly call it simple (with the GHC extensions). GHC seems to prefer features over simplicity, which is of course a perfectly defensible position.
This paper describes a type system that fits my idea well: http://www.ccs.neu.edu/home/shivers/papers/rank-polymorphism.pdf It's type system supports rank polymorphism while still being sub-turing.
Can you elaborate? It looks quite reasonable to me. Are you perhaps seeing the CDN staleness noted above?
Hi Ben, Thank you for great release. Maybe it is the following: https://ghc.haskell.org/trac/ghc/blog/ghc-8.4.2-released#Howtogetit ``` - ​https://www.haskell.org/ghc/download_ghc_8_4_1.html + https://www.haskell.org/ghc/download_ghc_8_4_2.html ``` 
The `==` example is interesting and helpful, thanks. I'm currently reading "Haskell Programming from first principles" and its language usage around Monoids (specifically section 15.8) basically seems to come down to, "don't make Monoids that don't obey their laws, they are useless because you can't reason about them." As a C++ programmer the alarm bell*s* of undefined behaviour went off.. :p Since there is basically no reason to create programs that use Monoid instances unless you want to take make use of their laws aren't Monoids that don't obey their laws 'less than ideal'? It's clear that `Float` and `Double` were not designed with consistency and provability in mind, which makes it tricky in these kind of circumstances, wouldn't it be better to force people to turn `Double`s into `Rational`s before trying to use them Monoidally, instead of introducing a 'bad' Monoid? I'm sure way more knowledgeable people have thought about this, I'm just curious what their reasoning is.
I dislike `DefaultSignatures`. They clutter class definitions with ad-hoc implementations, so that if we forget to implement a method when we should, we might be surprised by a type error, instead of a missing method error. `DerivingVia` is a much more principled alternative, although in this case it would not be simpler than just writing `toList = F.toList` for each instance, unless is combined with a way to derive the rest of the class (`fromList`). Why does `IsList` have `toList` anyway? We only need `fromList` to overload literals. Similarly, `fromInteger` should be its own typeclass. 
I've made good experience with the [socket](https://hackage.haskell.org/package/socket) compared to the network library, if you're looking for something low level as that. :)
`NaN` is not though the reason for the breaking of the monoid laws: as the example in the post illustrates. Floating point addition isn't really addition. It is addition with rounding. If you leave out NaN, floating point types can be modeled , neatly, as sets of rationals. Then, each operations on reals is uniformally lifted to floating points as "apply operation, then round to nearest member of the set." As such, floating point math has a distinctively non "algebraic" character, even though it can be reasoned about on its own terms.
Related GHC tickets: * https://ghc.haskell.org/trac/ghc/ticket/9883 * https://ghc.haskell.org/trac/ghc/ticket/7495
That's a good question! The answer is convenience. One of the initial reasons that type classes were invented was to provide numeric operators that worked over `Int` and `Float` in a principled, extensible way. OCaml has `+ : int -&gt; int -&gt; int` and `+. : float -&gt; float -&gt; float` to avoid the problem. So now we can make `instance Num Double` and `instance Num Int` and everyone is happy, along with `instance Ord Double`, `instance Eq Double`, etc... Now, `Double` doesn't really follow any laws, at all. You might want `Eq` to have some laws, and `Double` fails all of them. Same for `Ord`. Same for `Num`. So we either have to say: These classes do not actually have the laws you might want. Or, `Double` is a type that has a non-lawful instance for convenience, because programmers expect it.
I find words like "advertisability" distasteful. How is "advertisability" and intrinsic strength of Haskell? Advertising, to me, is mostly spin, generally bullshit. 
My only wish is for intero to be faster. :) Nothing wrong with trying to build your own IDE. If it's something you're passionate about you're bound to have a few good ideas. Would be nice if there were more developers focused on this. Who knows, if you focus on a particular user it might even bring more of those people into Haskell... and that's a win too. I will parrot others that writing an IDE is not a simple task; you would probably be looking at losing the next 3-4 years of your spare time. :)
Somehow that is not really a fair comparison. If you need arbitrary precision you simply need Rational (or maybe even a more complicated type) rather than doubles or floats. Fast code that is incorrect is useless.
This is really great! I want interactive documentation examples so much!!!...
Fixed. Thanks!
Great! I think that constraint was due to an earlier formulation that was later simplified. I’ll make that change. 
I think you did it wrong? I just tried and still got the old page. Then I ran `curl -X PURGE https://downloads.haskell.org/~ghc` and it worked, for future reference :-)
This is perhaps not the right thread for this conversation, but the way I see it is that the core of Haskell is in itself a *very* simple language. Extensions and other concepts certainly make it a lot more difficult, but you can write equivalent code without using \(most of\) them too, perhaps at the expense of simplicity or speed. Which is why the importance afforded to simplicity isn't all that surprising. Just my $0.02. 
Floating point code isn't incorrect. It is correct according to the floating point spec, which is very well defined, and mathematically is entirely sufficient for many sorts of calculations of interest and importance.
The Eq instance for Double doesn't fail any laws of Eq. Eq needs only two laws: reflexivity and substitutability. For Double, if a == b, does b == a? Yes, so that's satisfied. Also, if a == b, and there is some expression with a in it, can we substitute b in its place and get the same result? Yes, so substitutability is satisfied. The Monoid instance, however, does fail the monoid laws.
I was mistaken. I just tested whether nan == nan, and that's false. So reflexivity doesn't hold. My apologies.
For floating point numbers specifically, you have to assume that everyone understands their limitations just to get any work done. When I see Anything Float/Double it's more or less implied that that this more or less works give or take some rounding error. If there were similar nonsense with Integer, then I would be concerned.
Also -0.0 == 0.0, even though they are not substitutable for each other. The only way to get a valid instance of Eq would be to use bitwise equality, like the identicalIEEE function from Numeric.IEEE.
Additionally, here are packaged binary distributions of GHC 8.4.2 and Cabal 2.2 (and others) optimised for Debian &amp; Ubuntu ---- ### Ubuntu Ubuntu users can use my [GHC ppa](https://launchpad.net/~hvr/+archive/ubuntu/ghc) to conveniently install binary packages specifically built for the currently non-EOL'ed Ubuntu releases - Ubuntu 14.04 LTS (Trusty), - Ubuntu 16.04 LTS (Xenial), - Ubuntu 17.10 (Artful) - Ubuntu 18.04 LTS (Bionic) Installing is simply a matter of sudo apt-add-repository ppa:hvr/ghc sudo apt update sudo apt install ghc-8.4.2-prof ghc-8.4.2-htmldocs cabal-install-2.2 and add `/opt/ghc/bin` to your `$PATH`, and you should be ready to go. Please refer to the [PPA description](https://launchpad.net/~hvr/+archive/ubuntu/ghc) for additional information on how to manage multiple GHC versions installed side-by-side. #### Ubuntu on Windows Subsystem for Linux (WSL) For the benefit of WSL users these packages use build-settings optimized for WSL (see also https://mail.haskell.org/pipermail/haskell-cafe/2018-February/128591.html). The instructions are the same as the ones described above for the standard Ubuntu packages. The following distributions are supported currently: - Ubuntu 14.04 LTS (Trusty) - Ubuntu 16.04 LTS (Xenial) ---- ### Debian 9 (Stretch) The instructions are similar to the ones for Ubuntu, except for how to setup the repository: Invoke sudo apt edit-sources and add the line deb http://downloads.haskell.org/debian stretch main and then proceed as for Ubuntu, i.e. sudo apt update sudo apt install ghc-8.4.2-prof ghc-8.4.2-htmldocs cabal-install-2.2 and add `/opt/ghc/bin` to your `$PATH` if desired. The symlinks for the default `ghc`/`cabal` executables are controlled like in Ubuntu via [`update-alternatives(1)`](http://manpages.ubuntu.com/manpages/xenial/man1/update-alternatives.1.html). For more details, please consult http://downloads.haskell.org/debian/
Bah, indeed my history claims I ran `curl -XPURGE https://downloads.haskell.org/%7Eghc/`. Thanks for taking care of this. I've added the additional purge to my release upload script.
I'm looking forward to a new version of the Haskell Platform that can target x86 again too... Yes, I'm still using that and I like it. :X
This has nothing to do with encoding. Pandoc-citeproc is looking for locale files, and it can't find one for "C". Setting LANG should be enough; I don't know why gitlab isn't letting you do that. You can force the locale by adding a `lang` field to the pandoc metadata. Using pandoc by itself, you'd just add this to the YAML metadata section, or use `-M` on the command line, but I don't know how it works with hakyll. 
Also `DefaultSignatures` doesn't work well with `{-# MINIMAL #-}` pragma.
So, I believe that the first error has nothing to do with encoding, but the hGetContents error seemed like an encoding one. I'm using pandoc as a library, not as a command line tool, and indirectly at that (since Hakyll uses it). Do you know how I'd apply your config in this case?
`Excusable`
We'll likely have an HP "real soon now" -- there's a stack RC in the works that will move it to handling Cabal 2.2 features, so once that ships we'll produce new platform builds.
GHC Haskell is indeed quite feature\-rich. But the core of it \(System FC\) is actually extremely simple. All language features and extensions are desugared to this very simple core \(this actually amazes me\), which makes sure that we can always understand them in simple terms. Also, they are mostly orthogonal w.r.t. one another. It may not be the simplest of designs, but it gets pretty close.
Re: #1, part of what I value in GHC for production use is that I don't have to debug JVM-style performance problems such as are attendant to JIT. Then for #3, from experience I can say that I don't want to use Java libraries and could only see that making cleanup work for me. It was a tremendous problem when I used Clojure in production and I don't think it would be less of a problem as a Haskell user. I'd rather use and write Haskell libraries when one is needed.
How are JVM performance problems different from GHC performance problems?
Floating point code is incorrect *if you need arbitrary precision*, which is rare. Separately, and importantly, *particular* floating point code *may* be incorrect for giving up *too much* precision, and transformations of the code that you've promised (by way of `Monoid` instance) *should* be safe may make the difference between "correct" and "incorrect".
Not a troll question, but isn't Backpack just a bad version of \(Scala\-style\) OOP / cake pattern?
Point #3 is less about libraries, and more about existing proprietary codebases. The usual reply I get to "let's do Haskell" is "how do we integrate it with our existing code?" – and "We don't" is not an answer. "Microservices" is not an answer either.
This series is really great and very well-written, I feel like the text comes alive nicely (it's nice to see a text on functional programming with references to Lewis Caroll) and its a nice friendly pace as well. I've gotten as far as chapter IV.
I just recently migrated to 8.2.1 Any must have features in 8.4.2 that would be beneficial for me to consider migrating again? I'm exclusively writing yesod web apps and services that interact with sql server and are deployed on linux. 
I believe Eta Lang(https://eta-lang.org/) is effectively attempting to answer all the 3 points that you raised(JITing, JVM ecosystem and runtime reflection). In fact Eta converts STG to straight JVM bytecode rather than incurring the compilation overhead of going through the Truffle AST route. In fact, there is another approach possible. If you are aware of the Sulong project (https://github.com/graalvm/sulong/), you can utilize the GHC's LLVM backend to produce LLVM bitcode(which I guess is work in progress: https://ghc.haskell.org/trac/ghc/ticket/12470) and utilize Sulong to interpret the LLVM bitcode to run on Graal VM.
AFAIK Eta has no ETA \(sorry\) on first\-class modules...
Thanks!
Unfortunately, it is proprietary software.
Nothing that I am aware of, either. Except an open issue in their tracker(https://github.com/typelead/eta/issues/589)
Backpack is second\-class though – it's compile\-time and static only. [First\-class modules](https://people.mpi-sws.org/~rossberg/1ml/1ml-extended.pdf) allow dynamic parameterization by both types and values \(think OOP with dynamic mixins and associated types\)
&gt; You can always abuse features and do obscure stuff if you really want to. Sure. &gt; But you can easily identify when that is done. Less sure about that. I recently started working on a large complex code base that does abuse features of the language. Superficially on the surface, the code looks fine, apart from a lack of tests. Its only when I dug deeper that I found that this code is highly and tightly coupled, that's the tight coupling means its difficult to modify and its difficult to add anything other that integration level tests. 
I'm integrating haskell code with legacy asp (pre dotnet) and dot net applications just fine. Thanks to microservcies. Deployments are dead simple and automatic, integration is also very simple (http rest). And I have no need for yet another proprietary system. Why introduce more complexity when simple and proven solutions exist?
Technically, `Product Double` is never explicitly declared a monoid, we just have the declaration that `Num a =&gt; Monoid (Product a)`. This is quite desirable because for most kinds of numbers in use this actually is true - though not all, octonions for example are not associative in multiplication at all, instead of just close with tiny errors.
In your system, do you have integration tests that spin\-up both production applications and test real code integration?
We use drone.io to for continuous integration into test env. and building/shipping production docker containers.
a
&gt; If you leave out NaN, floating point types can be modeled , neatly, as sets of rationals. Then, each operations on reals is uniformally lifted to floating points as "apply operation, then round to nearest member of the set." Almost. For transcendental functions, it's "apply operation, then round arbitrarily up or down, to any number within one ulp". There are Good Reasons (tm) from numerical analysis why it's not feasible to require rounding to nearest.
[This](https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values/1573715#1573715) SO answer has some discussion on the matter. TL;DR: in the beginning, there was no other way to check whether a value was NaN.
&gt; There are Good Reasons (tm) from numerical analysis why it's not feasible to require rounding to nearest. In general, sure. And my original statement may have been too fast. But all the IEEE 754 operations are defined in terms of rounding to nearest (subject to a rounding rule for ties), which is actually all I meant to be referring to.
It just returns completely incorrect answer that is not useful. 
&gt; become the leader in the pure FP niche Isn't Haskell already the leader in that niche? Scala is bigger in the FP niche, but it certainly isn't pure.
This. And with tools like [automated canary analysis](https://medium.com/netflix-techblog/automated-canary-analysis-at-netflix-with-kayenta-3260bc7acc69), building drop in replacements to parts of a legacy/monolith ecosystem safe and practical. 
This is the first time that I’ve heard of GraalVM. Do you know who uses it in production? And which has a bigger user base, Haskell or GraalVM?
It’s bad but I sometimes use Cairo and render a square for each pixel. It works...
Does `identicalIEEE` consider `NaN` with different payloads equal? Should it?
I throw this in every project's `package.yaml`: ghc-options: - -Wall - -Wcompat - -Wincomplete-record-updates - -Wincomplete-uni-patterns - -Wredundant-constraints 
Sounds like you're describing exactly what a fragment shader does. It looks like there are a few DSLs that compile down to GLSL: - https://hackage.haskell.org/package/hylogen - https://hackage.haskell.org/package/ixshader - https://github.com/jaredloomis/andromeda
Thanks for the correction; It slipped my mind to mention the rounding behavior in my haste to get out the door this morning and that's an oversight on my part. I'd argue that floating point numbers behave close enough to algebraically that the Monoid instance is still justifiable (well, if we can justify the NaN behavior as well). Everything about floating point numbers screams "with caveats", after all :)
HLint and Weeder and my standard set. I don't turn on all warnings. Some I feel make the code actively worse. Turning all dials up to 11 is not the best spot. 
Wcompat is an important flag and imho it should be used much more widely
I'm also using hvr's [packunused](http://hackage.haskell.org/package/packunused) for detecting redundant Cabal package dependencies.
Do you perhaps have an example of one of your configured hlint yamls? 
https://github.com/ndmitchell/hlint/blob/master/.hlint.yaml https://github.com/ndmitchell/shake/blob/master/.hlint.yaml However all the features around banning/allowing and general code style rules are way more useful in multi person projects with more limited PR review. In companies I've worked at the config file was much longer and more prescriptive (eg ban unsafePerformIO without sign off)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/hlint/.../**.hlint.yaml** (master → 2c6b36b)](https://github.com/ndmitchell/hlint/blob/2c6b36b23b9e9ef44b2f0ff946812cafbb62d586/.hlint.yaml) * [ndmitchell/shake/.../**.hlint.yaml** (master → 87d32dd)](https://github.com/ndmitchell/shake/blob/87d32ddbe4e7c74c4f4d28dd0079683abeb770c9/.hlint.yaml) ---- 
Interesting thanks! This is what I've been using - error: {lhs: foldl x, rhs: foldl' x} - error: {lhs: modifyTVar x, rhs: modifyTVar' x} - ignore: {name: Use String} The string one because I use Protolude. I like some of the ones you disable, I'll think about those :)
That was a weird read. Are you the github OP? Why would someone start making proposals to change a language they're not even familiar with? 
Do you suggest the proposal is unacceptable because of the background of the author?
Not *because* of his background, no. I'm not convinced that it's a good proposal, because there was only one example, which could be implemented in many other ways without changing the language. And separately from that, I'm confused about why he would make the proposal, given that he doesn't know the language very well. Not because I don't think newcomers should be allowed to speak or something like that, but... why a ghc proposal? It doesn't even seem like he has a use case. Why go to that amount of trouble for something you don't even know will be useful to you, or anyone? 
Not a direct answer, but some FYI: We have setup [danger](https://github.com/danger/danger) along with hlint in our CI environment to publish linter warnings as comments on all PRs. No PR is reviewed till all linter warnings are dealt with.
Thanks. How about code coverage? Do you have automated process to make sure it’s always above certain number?
I am curious how to create an `Int -&gt; [Int]` function, which allows Python style `yield`, without changing the type of `&gt;&gt;=`? One of the approach is creating `Int -&gt; Eff[Int]` instead, provide an interpreter to convert it back to `[Int]`. The approach is not as simple as Python.
I've used Gloss-raster with Repa for that before.
Could you please figure out which part of the proposal is Scala-specific thinking?
Is there any good place to share Haskell learning videos? I recorded a beginner/intermediate refactoring session - really, my first time refactoring any sort of complex code. I figured it might be helpful to someone out there to follow along with a developer who doesn't know what he's doing and is actually figuring it out as he goes along. The content is refactoring a decently long Yesod route handler - with duplicated code and deeply nested pattern matching on `Maybe`/`Either` - into a somewhat shorter, more "Haskelly" handler, with (most) duplicated code factored out, and trimmed down to just one level of pattern matching, the rest replaced by the `foo &lt;$&gt; bar &lt;*&gt; baz` pattern. It's totally not high quality or anything, there's tons of bumbling around taking the shotgun approach to fixing type errors that I should probably edit out.
I don't understand what you're trying to get out of it. In Python, the yield keyword is used to make things you can iterate over lazily, but haskell lists already do this by default. Why do you want a yield keyword? 
What exactly do you mean when you say "python style yield"? 
There was a state monad example which does exactly what you wanted. You could also use the writer monad, or simply `iterate`.
Wouldn't @specialized be able to handle that, at least for primitives? "Bad" only as in Backpacks can't be parameterized by values and instantiated dynamically at runtime, unlike cakes , I just wanted to confirm whether my intuition was correct about them being basically the same, except with Backpack being static.
It's likely impossible to support both first\-class modules and globally coherent type classes, which is why I said 'Haskell dialect', not 'Haskell on GraalVM'...
Twitter uses it for all their code [https://www.youtube.com/watch?v=G\-vlQaPMAxg](https://www.youtube.com/watch?v=G-vlQaPMAxg) – since experimental versions. Haskell probably has more users right now, but since it's an official replacement for JVM by Oracle that was released just now, I expect that to change soon.
Have you looked at lambdachine ? It is an effort for a Haskell JIT built on LuaJIT, which iirc is a lot faster than GraalVM and some optimisations were introduced specific to Haskell in the paper.
I've been poking at it a bit for a personal project, but while I'm not going to say it'd be impossible to use, it seems to me that it's a bit of a difficult fit for Haskell at the level of maturity of GHC. Here's a brain dump: I'm 100% on board with the idea that a JIT could help with many of GHCs issues that come from heavy static inlining that might be better done online on demand. In concrete terms, we have a ton of type information that is hard to convey through the funny dynamic tree model that Truffle uses, which is really aimed at "dynamic" scripting languages. What you can get out of Truffle might be "good enough" to write some real applications though. Purescript also has the benefit that its feeding Truffle a strict language. That said, if we look at a project to use GraalVM as mostly being about exposing more of the guts of the hotspot compiler and less about using Truffle itself as an intermediate compilation target I could see that you might get some benefit there out of having a cross-platform runtime. There are a lot of little efforts inside of the Valhalla project, and the surrounding current java research efforts that might be relevant in the long run. All the work on fibers starts to make things more competitive with GHC's light weight threads. The current work on adding value types gives a comparable story to a lot of what we do in kind #, etc. There are still problems like that the java garbage collector isn't set up to handle things like http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html which remains sort of a looming specter over efforts like eta / frege. (I vaguely recall that for Ben Gamari wrote a patch for GHC that allowed you to turn off selector forwarding for comparison and with it turned on couldn't finish compiling a full stage2 build!)
Scala isn't even much of an FP out of the box. Sure, you can map over containers, but so can C++ and Java. You need third-party libraries to do anything Haskell-like, which means many libraries ignore the FP aspect alltogether, and others don't even agree on which of the competing FP libraries to use (e.g. scalaz vs cats). Even when ecosystem fragmentation doesn't get in the way, you don't have to attempt very complicated things to feel the pain of the foot not fitting the shoe.
The key is that this _doesn't_ work just for primitives and that is what matters. I can take `Text`, which is made up of a bunch of things and apply this and it "just works". The fact that it can compile separately at compile time like a c++ template is the very thing that provides this benefit. The cake solution can't offer this feature and can only hope that the JIT will be smart enough to make up some of the difference. Like generics vs. templates or typeclasses vs. modules they are two different things that offer slightly overlapping functionality.
Here's how: import Control.Monad.Writer.Lazy import Data.Bits yield :: a -&gt; Writer [a] () yield a = tell [a] xsRNG :: Word -&gt; [Word] xsRNG = execWriter . fix (\this seed -&gt; do let tmp1 = seed `xor` (shiftL seed 13) let tmp2 = tmp1 `xor` (shiftR tmp1 17) let tmp3 = tmp2 `xor` (shiftL tmp2 5) yield tmp3 this tmp3) In fact, you could even simplifiy this a bit with a scary looking `yieldable` function: -- truly terrifying signature which I trust GHCi to be correct yieldable :: ((a1 -&gt; Writer c a2) -&gt; a1 -&gt; Writer c a2) -&gt; a1 -&gt; c yieldable x = execWriter . (fix x) xsRNG2 :: Word -&gt; [Word] xsRNG2 = yieldable (\this seed -&gt; do let tmp1 = seed `xor` (shiftL seed 13) let tmp2 = tmp1 `xor` (shiftR tmp1 17) let tmp3 = tmp2 `xor` (shiftL tmp2 5) yield tmp3 this tmp3) But in reality, this is incredibly bad practice. If you didn't need the effects of the Monad, why are you using it? The yield keyword in other languages allows you to only partially evaluate an expression, which happens automatically in haskell as *part of the language*. xsRNG3 :: Word -&gt; [Word] xsRNG3 = tail . iterate (\seed -&gt; let tmp1 = seed `xor` (shiftL seed 13) tmp2 = tmp1 `xor` (shiftR tmp1 17) tmp3 = tmp2 `xor` (shiftL tmp2 5) in tmp3) This creates an infinite list, which will only be evaluated as required, so why do you need yield to begin with?
I want to state the obvious here: `(&gt;&gt;=) :: m a -&gt; (a -&gt; d) -&gt; d` will completely break `IO`. A function with this type signature would allow unsafe IO anywhere in the program, since you could unwrap the `IO` monad anywhere.
No, it won't. `d` is constrained by type class instances, not anywhere
I've mostly wanted to play with using Truffle to execute SPMD code at this point. e.g. writing code that sits atop https://software.intel.com/en-us/articles/vector-api-developer-program-for-java via truffle from something closer to https://ispc.github.io/ Unfortunately, all that stuff was done in a couple of separate JDK 9 projects, so its not entirely clear to me what works where right now, as I'm not 100% up to date on the status of the Java ecosystem.
SalesHorse
The yielding alone is already built into the language - all you have to do is write a function that returns a list. Lists are lazy, so elements will be evaluates as you iterate over them, exactly like Python's generators / iterators. If you want to intersperse this with effects, things get a bit trickier. Depending on the desired effects, you would use State, Writer, Reader, Cont, or even some sort of Chan driven from a separate thread. Either way, this does not require breaking the Monad typeclass.
&gt; Also, if we're feeling a bit more pedantic, the monoid in Haskell isn't really as abstract and general as the Monoid of category theory/abstract algebra in the first place (it's more "inspired by"). Can you explain what you mean by that? I thought Monoids in Haskell were exactly the same as the ones in abstract algebra. It has the same operations as it would in abstract algebra. I guess you could argue about the fact that Monoids in AA is triple of a set, an operation and a constant and that you can have multiple monoidal operations on the same set, which can't be neatly expressed in Haskell without the use of newtype. But that seems more a matter of how nice the definition is rather than it not corresponding to the AA definition.
I just have HLint as a CI test, so it fails like normal tests. 
I didn't read your proposal thoroughly, but have you tried working this out using RebindableSyntax first?
Nice, I threw something similar together that watches, builds, &amp; runs a servant backend and an Elm/Webpack frontend: https://github.com/Southern-Exposure-Seed-Exchange/southernexposure.com/blob/master/manage.hs Looks something like this: https://i.imgur.com/W5Av450.png Been thinking of the best way to abstract this out so I can use it in multiple projects. I'd love to have another `watch-ui` command that sticks it in a Brick TUI that lets you re-run specific commands, split the display into the output log &amp; a ghci or elm REPl, etc. I like how yours pads wrapped lines so they appear after the tag: [SOMETAG] long text... wrapped text in mine SOMETAG | long text... wrapped text in yours Not exactly sure what you mean by aggregating the output - but you might want to checkout the [typed-process](https://www.stackage.org/lts-11.5/package/typed-process) library. Are you just trying to merge the output &amp; error handles? 
I use hspec. I don't use quickcheck mainly because I have the feeling that even though it shpild be quick at finding error, it should be slow at finding that there is no errors at all. Therefore for big project you should end up with slow test. I am probably wrong but I prefer to write test for corner cases (and usually wait for a bug to actually write test). 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Southern-Exposure-Seed-Exchange/southernexposure.com/.../**manage.hs** (master → 1d9d0f2)](https://github.com/Southern-Exposure-Seed-Exchange/southernexposure.com/blob/1d9d0f2d4a52d5384b64c617cc11489f888389c5/manage.hs) ---- 
Even though i love the idea if do test (and use to use it lot when writing python) I've given up because it's to easy to write tests s which are actually never called (slight typo or wrong formatting) however, I'm sure if I was writing the test first, I wouldnt have this problem ;-)
Quick check is very fast typically 
Dear all new Haskell users, this is a comment to bookmark and cherish. By the way, merging `-Wincomplete-record-updates` and `-Wincomplete-uni-patterns` into `-Wall` is apparently accepted by the committee. https://github.com/ghc-proposals/ghc-proposals/pull/71
As the author of ixshader I can’t recommend it ... yet. It’s still rather cumbersome and over complicated for writing non-trivial shaders.
What about making it easier to start using the existing infrastructure? One of the reasons stack took over so quickly was that it was straightforward to adopt and when it wasn't, they fixed it.
QuickCheck is a stochastic testing tool so you have some amount of control over how fast it is. You can ask it to generate as many test data as you want.
You probably shouldn't use `QuickCheck` because `hedgehog` is better in almost everything. Property-based testing can give you huge win and helps you find bugs you never would've thought personally. For example, during writing parser for small eDSL some random unicode string was generated with unicode symbol and unicode feature I've never heard about. So I had no chance to catch this bug by unit tests only.
You probably shouldn't use QuickCheck because hedgehog is better in almost everything. Property-based testing can give you huge win and helps you find bugs you never would've thought personally. For example, during writing parser for small eDSL some random unicode string was generated with unicode symbol and unicode feature I've never heard about. So I had no chance to catch this bug by unit tests only.
You are very keen, `Dsl` can be seen as both an ad hoc polymorphic delimited continuation and an adaptive monad. That means the `Dsl` adapter to both `Cont` and `Monad` are almost zero cost. On the other hand, `Monad` instance for `Cont` is less efficient. 
[animateField](https://hackage.haskell.org/package/gloss-raster-1.12.0.0/docs/Graphics-Gloss-Raster-Field.html#v:animateField) looks like basically what I need. animateField :: Display -- ^ Display mode. -&gt; (Int, Int) -- ^ Number of pixels to draw per point. -&gt; (Float -&gt; Point -&gt; Color) -- ^ Function to compute the color at a particular point. -- -- It is passed the time in seconds since the program started, -- and a point between (-1, -1) and (+1, +1). -&gt; IO ()
What does it do?
Conduit also gives you a monad that works very similarly to Python's yield, with side effects.
I've used gloss + friday with freenect (xbox connect, depth information) to make a full system called [mellow](https://github.com/TomMD/mellow) which would capture a frame of depth pixels, apply your transformation, and render. The underlying notion of Friday + JuicyPixels + Gloss isn't hard. 1. Generate an image using pixel data with Friday. [`frame :: RGBA`](https://hackage.haskell.org/package/friday-0.2.3.1/docs/Vision-Image-RGBA-Type.html#t:RGBA). 2. Convert the RGBA image to a Gloss picture. I patched Gloss a bit to make this a zero copy operation. toPicture :: RGBA -&gt; Picture toPicture = fromImageRGBA8 . toJuicyRGBA 3. Return this picture as the frame for `animate` or `playIO` or whatever operation in Gloss you've opted to use. You can see mellow for [an example](https://github.com/TomMD/mellow/blob/master/src/Mellow.hs) of composed pipeline.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [TomMD/mellow/.../**Mellow.hs** (master → d6bae93)](https://github.com/TomMD/mellow/blob/d6bae93406065497eee46533b66613e1d998d741/src/Mellow.hs) ---- 
Yes. I use quickcheck via Haskell Testing Framework (HTF).
http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using-warnings.html
&gt; They are for different purposes actually. Quickcheck is for small functions I know, but still. But for big project there are lots of small functions and I feel trying a hundred a thousands of test for each functions can take ages, but as I said earlier, I am probably wrong and it is premature optimisation. 
Qquickcheck might be fast at finding bugs (when there is one) but obliviously slower when there is none. This is fine when you are developping and need to be sure there is no bug in what you are writing, but is a bit different when you are testing for regression and have a lots of test which should in theory all pass. It seems that the best way is use quickcheck to find bug and corner case and then "freeze" them as normal test. However, I haven't use quickcheck so I am probably doing some premature optimisation.
Sounds great. Why not post it here?
This is really arrogant. 
That's a bit facile—it's like saying _all_ code is correct according to the perfectly well-defined operational semantics of the language as specified by the compiler. We care about correctness *compared to what the programmer expects*, and most programmers use floating point numbers to write code that cares about *numbers*, not IEEE-754.
Then most programmers need to learn numerical analysis. ¯\_(ツ)_/¯
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
I try to write most of my tests as property tests. A lot of what I've written recently is optimized versions of fairly simple algorithms. So a property test that proves the optimized version is working for smaller inputs same as the naive version. Then usually a few unit tests for the obvious cases eg. the null case. And any time a property test finds an error I usually add it as a unit test for future reference. I'm also personally a fan of tasty.
Have you tried a debugger, e.g. lldb or gdb?
I really wish GitHub would get rid of their emoji voting garbage, or at least allow developers to disable it. It's manipulative and adds no substantive value to technical discussions.
In this case the votes are probably pretty appropriate though.
Yeah, my typical testing strategy is to write QuickCheck properties when I can and `hspec` tests where properties are hard to express. I have not ever had a performance issue with pure tests. They always come from tests that need to touch database or HTTP or similar. Don't worry about perf. Solve it when it becomes a problem.
I think you can change the number of iterations that they run. I'd keep it high when writing a function and then add commonly broken edge cases to my concrete tests. Then I can keep in minimal tests for the particular things I know can break but also get quick checking when I need it.
[duplicated](https://redd.it/8djcxq)
Since you're asking, I'll complain a bit about "Haskell attracts [...] experienced engineers. [...] Most Haskellers chose Haskell after a long journey in search for better tools and improved productivity. " I don't see any evidence of this, and if anything, I'd suspect it's less true for Haskell than other languages that are on the decline after wide popularity in previous generations (see C++). The Haskell community has an unusually high number of CS graduate students, who are rarely known for their software engineering experience, and certainly not for making decisions based on a long search for the best tools. Even outside of graduate students, historically, a common reason for learning Haskell has been its being taught in a computer science degree program. There's no problem there, and these are great people to work with and a valuable voice in the Haskell community; but it seems false to claim a well of deeply experienced software engineers as a unique feature of the Haskell community.
IMHO it's a noise in `+RTS -B` and should be removed.
[removed]
What instance? Could you show me?
Following the instructions here, http://downloads.haskell.org/debian/. The key signing thing hangs. ``` root@x:~$ wget keyserver.ubuntu.com --2018-04-22 17:59:33-- http://keyserver.ubuntu.com/ ... 2018-04-22 17:59:34 (61.2 MB/s) - ‘index.html’ saved [1417/1417] root@devel06:~$ ^C root@devel06:~$ apt-key ^C root@devel06:~$ apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA3CBA3FFE22B574 Executing: /tmp/apt-key-gpghome.zg27KQaRoT/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys BA3CBA3FFE22B574 ^[[2;5~gpg: keyserver receive failed: Connection timed out ```
The example in the github PR about java classes. If you added IO to that like the author wanted, you would need unsafeIO, and your program would break. 
Could someone with 8.4 check whether the error message for this code suggests that the user turns on `NoMonomorphismRestriction`? If it doesn't then I will file an issue on the trac. type family F a b data D a = D (F a ()) d = D ()
Before emoji reactions, people would just reply with an emoji, which is arguably worse. For example : https://github.com/rails/rails/issues/16731 (there are also emoji reactions that were added afterwards). 
Could you create a runnable example?
No, because it wouldn't work, that's the whole point. This, and the entire proposal doesn't work for good reason. But the dsl has no laws, so nothing is stopping you from making code that doesnt work if you use it. 
Arghh. I tested turning off my local firewall, but not the one on my router. Thank you, forcing port 80, works perfectly.
With the current ghc-8.4.2 release and `TypeFamilies` enabled: tmp.hs:5:7: error: • Couldn't match expected type ‘F a0 ()’ with actual type ‘()’ The type variable ‘a0’ is ambiguous • In the first argument of ‘D’, namely ‘()’ In the expression: D () In an equation for ‘d’: d = D () • Relevant bindings include d :: D a0 (bound at tmp.hs:5:1) | 5 | d = D () | ^^ Failed, no modules loaded. 
Thanks very much. Sounds like this could do with a helpful suggestion of `NoMonoMorphismRestriction`.
&gt; Don't worry about perf. Solve it when it becomes a problem. I think you are right and probably should give it a go.
Ok ,but how do you know where to stop ? It's fine where you know there is a bug and try to find it : You just have to increase the dept until you find something, but when everything is fine. How do you know how much you can actually prune the search tree whilst still being sure that you are covering enough ?
&gt; I'm also personally a fan of tasty. For information, what does it brings over, let's say hspec ? 
With recent time library, &gt;= 1.9, you can just do: import Data.Time import Data.Time.Clock.POSIX main :: IO () main = do pt &lt;- getPOSIXTime let start = 1524183107 :: POSIXTime duration = pt - start myFormatSpec t | t &gt;= 2 * nominalDay = "%d days %02Hh %02Mm" | t &gt;= nominalDay = "%d day %02Hh %02Mm" | otherwise = "%02Hh %02Mm" myFormat t = formatTime defaultTimeLocale (myFormatSpec t) t putStrLn $ myFormat duration Unfortunately, out-of-the-box stackage currently provides an older version of time, where `NominalDiffTime` still does not have a `FormatTime` instance. You can get this if you specify `time &gt;= 1.9` in your cabal file, and, if you are using stack, also add `time-1.9.1` to your `extra-libs`. If you stick with an older version of time then you are right, you have to do some kind of hack. Personally I wouldn't go so far as resorting to `printf`. I would compute the number of whole days, subtract that off and print it separately (as you did), and then add the remaining `NominalDiffTime` to midnight of some day (any day) and use `formatTime` on the resulting `UTCTime`.
&gt; The key is that this doesn't work just for primitives and that is what matters. Yes, but @specialized can be extended to work for value types, esp. after Project Valhalla provides unboxed multiple fields structs. &gt; The fact that it can compile separately at compile time like a c++ template is the very thing that provides this benefit. The cake solution can't offer this feature and can only hope that the JIT will be smart enough to make up some of the difference. Unless I'm missing something, I think programmer can partially instantiate his cake, as in `class MyCakes extends CakeX with CakeY...` and get a class file with everything precompiled. &gt; Like generics vs. templates or typeclasses vs. modules they are two different things that offer slightly overlapping functionality. Functionality may only be slightly overlapping, but conceptually to me they seem very close. I have a question though: Is there any difference between 1ML first-class modules and Scala's version of OOP? Does Scala have full-featured first-class modules or is it still missing something? 
There's some more general "fallacy" here that comes up most often with floating point (because it surprises people most, perhaps?) but I think is the case for all number types: it goes along the lines of every numerical data type does not behave like "numbers". For example, in `Int`, you can't even count properly - the most caveman like of operations: &gt; 9223372036854775807 + 1 :: Int -9223372036854775808 But people are happy to say, for whatever reason, "oh yeah `Int`, of course that's the case - it's `Int`" 
I agree, type classes approach has its own benefits. But it doesn't scale well. For example, you have multipackage project and tests in one package require `Arbitrary` instances for types in another package. So you end up moving `Arbitrary` instances to library. And things can become confusing. Also, type class approach means that once you need more than one standard `Arbitrary` instance for some type, you end up creating `newtype`s which is a slight overhead comparing to just creating function which generates you proper type. Regarding tuples: if you need to generate tuples or lists, you can use standard combinators like this: * https://hackage.haskell.org/package/hedgehog-0.5.3/docs/Hedgehog-Gen.html#v:list But usually in tests you don't often have tuples, you have custom data type and writing generator for your custom data type with further explicit call of this generator like: genMyType :: Gen MyType genMyType = MyType &lt;$&gt; int64 &lt;*&gt; string &lt;*&gt; bool prop_myType = property $ do myType &lt;- forAll genMyType is not much harder than writing instance Arbitrary MyType where arbitrary = MyType &lt;$&gt; arbitrary &lt;*&gt; arbitrary &lt;*&gt; arbitrary Functions just give you more flexibility and it's just feels more pleasant and convenient to work with.
We're in a similar situation (except we use PostgreSQL on RDS). We find that it's a bad idea to get too far behind, but you also don't want to be on the very bleeding edge for production. What works best for us is to stick to stackage LTS, and to update to latest LTS about once per month. To accomplish that, we do this: Whenever a new GHC version (first or second version component bump) is released, we start gradually investigating what needs to be done to get our systems to compile and run properly on that new version. By the time that version makes it to LTS, it's usually at least a month or two and at least one third-version-component later, and we switch to it on HEAD up to about another month after that. We have a fast dev cycle, so it soon starts going out to active customers. For our less active customers or customers with longer test-and-roll-out procedures it can take quite a while. So we can have a fairly wide range of LTS versions live at any given time. But LTS works well and we're able to support that.
Me neither, but terminal emulators can do something (e.g. play a sound alert, of course, but also flash the background colour, post a desktop notification, or whatever else) when told to beep.
I was wondering if I could omit hours if it was zero.
It is lirerally noise.
I'm not sure it is worse... having it as a button in the interface invites drive-by reactions, and the reward mechanism of seeing a counter go up when one reacts further intensifies that.
Next challenge, write an algorithms with an allocation pattern which plays the star wars theme using `-B`.
Pinging you - I posted some actual code for this, but I accidentally answered myself instead of you.
It should probably suggest that OR giving an explicit type signature.
To avoid depending on `pipes`? 
I see. I was meaning using tuples to pull multiple values at once but Monads ( or applicatives) work as well. Thanks
That's good. Thanks.
It's faster and not a long-lived program
One of the developers of Hasql gave a talk a few months ago where he mentioned using the binary interface of postgres directly to improve performance. https://www.youtube.com/watch?v=L8-iC4_E5n4 (skip to the 9 minute mark for the relevant bit) I don't know if that's already implemented but you could check it out or get in contact with them. 
I also keep gauging Lang's, it's a pest.
I'm perhaps unreasonably excited that this may be a final, perfect solution to adding this feature to the language. And that it was added on the library level instead of as a rigid, compiler feature. I guess we needed the DataKinds and OverloadedLabels building blocks for it. Does anyone else see this as such a nice solution as I do? Or has there been something else that I might've missed? And/or do you see any flaw with this approach?
You can use weeder to detect unused dependencies: https://hackage.haskell.org/package/weeder
This quite interesting indeed ! Looking at the different bwaybif invoking a function, am I right to suppose that currying works as expected ?
Ticket submitted: https://ghc.haskell.org/trac/ghc/ticket/15077#ticket
What kind of conflict do you have in mind? If you have nix it's easy to load it up if you want to try: nix-shell -p "haskell.packages.ghc822.ghcWithPackages (p: with p; [named])" --run 'ghci'
The implementation has a comment `Do not read further to avoid emotional trauma.` but it actually seems quite straightforward and comparatively non-hacky. Biggest complaint would be that default parameters aren't a thing. Which is an improvement over having undefined as default for some things with lens but still.
Yeah, the lack of default values might indeed be missed by some, but then currying would be hard to support, wouldn't it? Besides, we could still use `Maybe` for explicit defaults, couldn't we? 
Syntax should be ignored, in my opinion, as we can just layer some syntactic sugar on top to make it as pretty as we want. I’m not sure if it’s feasible, but allowing libraries to define new syntax via some GHC extension — as opposed to defining syntax in GHC — seems like a better solution. It’s probably really hard to create an extension that can accommodate all desired syntax, though. 
I can't imagine ever wanting named parameters unless they were optional parameters. To me the whole idea behind named parameters is for the optional ones, and the mandatory ones don't get names. Normal for lisp and for the command line.
At some point I'm hoping to add the postgres system catalog to [Squeal](https://github.com/morphismtech/squeal/issues/13). Not going lower level than libpq though.
I respectfully disagree. I’ve always thought position seemed like an odd way to pass parameters between code. For example, I know all the arguments of a fold, but I can never remember the order. Also, a lot of documentation uses named parameters to explain what the function does (e.g. [`foldr`](https://www.stackage.org/haddock/lts-11.5/base-4.10.1.0/Prelude.html#v:foldr)), but in an unsafe way, since the programmer has to make sure the parameter names in the definition and Haddock snippet match up. Side question: does anyone know a way to make this fail with a type error?: replace :: Text `Named` "needle" -&gt; Text `Named` "replacement" -&gt; Text `Named` "haystack" -&gt; Text replace (Named replacement) (Named needle) (Named haystack) = ... 
That's a fair point that /u/Tarmen has raised as well. I wonder if we could easily support those too by using `Maybe`s, and having a function called `call` which fills in any remaining parameters with `Nothing`? If there is anything else left to be filled it's a type error. pair :: Maybe Int `Named` "x" -&gt; Maybe Int `Named` "y" -&gt; (Int, Int) pair (Named x) (Named y) = (x ||| 0, y ||| 0) main = do (pair ! #x (J 1) ! #y (J 2)) `shouldBe` (1, 2) (call $ pair) `shouldBe` (0, 0) (call $ pair ! #y (J 3)) `shouldBe` (0, 3) (call $ pair ! #y (J 3)) `shouldBe` (0, 3) (call $ pair ! #x (J 4) ! #y (J 5)) `shouldBe` (4, 5) pattern J a = Just a pattern N = Nothing (|||) = flip fromMaybe call :: _ call = _
What is this `#param value` syntax? How is a `String` literal automatically promoted to the type level, without it being a promoted `String`? What is this `Apply` constraint? Does that mean that somehow, just by writing a type signature, I have triggered the creation of `O(n^2)` typeclass instances? As a quite experienced Haskell developer, I feel I have no intuition whatsoever about what this is doing behind the scenes, what it is doing to my code both in terms of semantics and in terms of performance, what kinds of errors to expect and how to interpret them. All I see is a simple usage recipe (very nicely written by the way) for some vaguely Python-like code that looks very out of place in my Haskell program. And how could this possible work without a huge pile of scary language extensions? This sure doesn't look like valid Haskell 98 to me. Yet no required extensions are mentioned. I would love to use something like this if I knew what it was and felt confident that I am still in control of my own code. Couldn't you add at least some *hint* of the mechanisms it is using, the basic approach to how they are being used, and links to where those mechanisms are documented?
Thinking about it some more, I can think of two ways to do defaults with this: Let users apply multiple arguments from an hlist: fooDefaults = #arg1 "abc" &amp;: #arg2 42 &amp;: HNil foo ! fooDefaults Add type level defaults bar :: Default Int "count" 5 -&gt; String bar (Default count) = replicate count 'z' class Default lifted where retrieve = val instance (KnownNat nat) =&gt; Default (nat :: Nat) where retrieve = natVal (Proxy :: Proxy nat) runWithDefaults bar
How do you mean your side question? `replace 1 1 1` will fail with a type error, but I don't think that's what you were asking for.
If the type signature marks a parameter as optional with a default, I wouldn't expect to be able to curry with respect to that parameter. If that's possible.
This is a cool idea, but why have named parameters instead of simply passing a record? You can use record fields in any order, it’s type safe and idiomatic.
You can click view source on hackage, the implementation is pretty short https://hackage.haskell.org/package/named-0.1.0.0/docs/src/Named.html#named . The #foo syntax is from -XOverloadedLabels and is translated to `fromLabel @"foo"`. fromLabel is defined in GHC.TypeLits. http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overloaded-labels .
Lazy I/O is faster than pipes?
Yes, especially if you do any parsing
It's quite ceremonial and pollutes the namespace, like newtypes.
Squeal was one of my motivations for this question. It's been interesting seeing people use type level approaches to reduce any need for template Haskell boilerplate. I am curious if removing SQL DDL as an intermediary has any benefits in simplicity.
That's great ! Do you have to have a type signature or can you thing s like f (#a x) (#b y) = ... Or equivalent ?
:-( Is there anywhere I can read on why?
The only problem with this in my experience is that for every function `foo`, it requires a new data type `FooConfig`, which sucks. The record field names polluting the namespace doesn't bother me because of the combination of `-XDuplicateRecordFields`, `-XNamedFieldPuns`, `generic-lens`, and `generic-lens-labels`.
Even though you are raising some fair point I found your comment unnecessarily harsh. Named parameters is about safety and has nothing to do with dynamic language. I've been beaten a few time by the regex operator : of `"aaa" =~ "a"` and `"a" =~ "aaa"` which one is true ? If there is a elegant solution which can disambiguate things like this, I'll embrace them regardless the fact it might you like Python.
They are cases (like the replace function or regex matching) when the type signature doesn't give enough information nor give enough safety. You might not have encountered them or you might be lucky to remember the parameters order, but if you work in a team, you might want to make sure that people don't make mistake. This is a way to do it.
Maybe a pattern synonym with a Proxy type in it or something.
You probably can stick all the default in a record ;-)
I have another question, if you’re up for it. I see from the Tweag GitHub repo that you’ve added support for linearity in Core by adding a `varWeight :: Rig` field for the value-level variable constructor (`Id`): https://github.com/tweag/ghc/blob/3f5d5442ac09a87563243535ad584fe1c3e80fbf/compiler/basicTypes/Var.hs#L240 I’m curious to know whether you’ve considered how to approach altering code generation based on the linearity of arguments (in addition to supporting type checking). Specifically, should information about linearity be transferred to STG? As I understand it, STG assumes the presence of some sort of memory management, so in order to generate code differently for linear variables, the information on linearity would have to be present in the STG representation as well. Could you share your thoughts on this? 
Not that I know of
Interesting, do you think it would simplify things to directly use DML on the system catalog instead of DDL? In Squeal I think of DDL as a `Category` which I hoped would provide a simple and familiar API. [Here](https://www.youtube.com/watch?v=3Tfcmq2IA6k&amp;t=4760s)'s a lecture I listed to to try to understand the system catalog. Good luck with your project!
I guess you could use `Named replacement :: Named Text "replacement"` with `ScopedTypeVariables`. Maybe one could extend patterns in a way that would allow types in it. E.g. in conjunction with `PatternSynonyms` one could wirte something like: pattern NamedP :: Symbol -&gt; Type -&gt; Type pattern NamedP name x = (Named x :: Named name x)
If you are in Linux you can just use `time`. This seem more suitable for a more general programming subreddit.
Sounds like I'd still have to open up the docs, because I'd just forget what the parameters are named.
I started a new career in Haskell, with no previous experience in functional programming (my background was C++ compilers) when I was age 40. So yes, you can.
The pipes monad is a recursive ADT, and [`(&gt;&gt;=)` being recursive]( https://hackage.haskell.org/package/pipes-4.3.9/docs/src/Pipes-Internal.html#line-95) means that you keep pattern matching on it at runtime. This is also why the [`Free` monad](https://hackage.haskell.org/package/free-5.0.1/docs/src/Control-Monad-Free.html#line-226) is slow (`(&lt;$&gt;)` is the recursive bit). In fact pipes's `Proxy` is a free monad.
Well said. I had a coworker write a bug (that got through code review) using a sub string check with the arguments around the wrong way. That was a bad few minutes of down time.
Yeah, it turning out that defaults are not perfectly supported by this method out of the box, I agree. Although, do you think row types would be a solution to all of the use-cases people bring up in this thread? In other news, [it seems dependent types can give rise to full row types too](https://stackoverflow.com/questions/49903536/can-idris-support-row-polymorphism). So we might only have to wait as long as DependentHaskell.
In my experience regarding records, Haskellers are capable of bringing up any and every possible use for records that stalls any advancement in the direction of getting proper records into the language. Hugs had row types and there have been library implementations of extensible records in Haskell for ages. SPJ made a proposal for it some years ago. I've implemented row types on top of basic HM without any fancy dependent types or type classes, it's a straight-forward thing to add. Dependent Haskell still won't give us `{x:3}.x` or `{a::Int}` syntax which is all most people wish to have. I think this is more of a perfect being the enemy of good thing. People are afraid if you add something then you'll have to commit to it forever. And they know people would immediately start using this wholesale because **everybody wants proper record support right now**. I'm not optimistic. 
The C code is likely a mixture of cbits files and packages that include their C dependencies completely to avoid distribution issues.
Formatting package has this built in https://hackage.haskell.org/package/formatting-6.3.4/docs/Formatting-Time.html#g:5 Example use https://github.com/chrisdone/formatting#3-years-ago-time-spans
But in this case Int is still a Monoid even when it overflows. So this is specifically a problem with floats. 
The behaviour isn't undefined like in c++, and the operations will still compute in the order you specify, but for any program you could afford to write in c++ with `ffast-math`, then the monoid is acceptable.
If only we had anonymous records :(.
[removed]
Mostly just syntax differences. I'm not a huge fan of the rspec inspired syntax. I also love golden tests for integration testing. 
We should repeat the following to ourselves until it sinks in: Monads are not DSLs; Monads are *not* DSLs; Monads are *not* DSLs. Now that we've let that sunk in. Let's remind ourselves what a monad is. A monad is a data structure along with an operation to inject an arbitrary value into that data structure as well as a way to collapse two levels of that data structure. And once more, for final effect: It is an accident (albeit, a convenient one) that monads can be used to express sequencing of operations.
Can you explain the rational for wanting to use the Postgres catalog over just using SQL DDL? As far as I understand it, the DDL commands in postgres provide much safer semantics than just modifying system catalogs willy nilly. Is there any advantage? We use regular Postgres DDL statements in [`beam-migrate`](http://hackage.haskell.org/package/beam-migrate), without any issue.
I always wonder what people think of that practice. For example, I'm sometimes tempted to make a `saltine-complete` that includes `libsodium` so you don't have the external dependency (at the obvious cost).
You already [asked elsewhere](https://stackoverflow.com/questions/49950803/how-to-get-haskell-code-execution-time-runtime). Could you say why the [linked answer](https://stackoverflow.com/a/5968689/216164) doesn't suffice? I know closed questions, and other ways these communities are managed, make them seem unfriendly and unapproachable but if you give more details then more help usually appears.
&gt; It is an accident (albeit, a convenient one) that monads can be used to express sequencing of operations. Agreed. That's why this proposal introduced another type class for sequential operations.
I used to use it to get a sense of how much garbage I was generating in a game I was writing, so I could see what actions generated more garbage, and which collections tended to result in visible hiccups, in real time while testing. 
I think people should just list the deps in `pkgconfig-depends`. It doesn't automate the *installation* of those deps (except it does with Nix), but at least it makes it obvious what needs to be installed and avoids potentially trying to link two versions of the same lib. Heck, we could even give tools like Stack / cabal-install some opt-in apt / brew integration in order to handle it automatically for a large percentage of users. (Hey, that might actually prove to be a good way to integrate GHC installation; the way Stack does it is a bit error prone, so delegating to the system package manager for that would be great).
Yeah the concept is simple enough. But nobody wants named parameters for `writeFile`. When the arguments are mandatory, you just learn them. They're there in every call, after all. Easy to learn.
How hard would it be to add proper row types to GHC? You called this a 'simple problem'. So, if it's truly simple and someone makes a fork of GHC 8.4 that just works, that maintains compatibility with the libraries that I need, I wouldn't mind being stuck on that fork of GHC 8.4 for a few years for all my projects, because I find that the problem is really annoying and damaging to productivity and everything I've seen (e.g. `makeFields`, `labels`) is just a workaround with some limitations and not a true solution, or difficult to comprehend (e.g. `vinyl`).
Yeah I don't think bundling is a great idea. I understand why people do it, but we have nix these days.
Does `QuickCheck` have an equivalent to `hedgehog`'s [recursive](https://hackage.haskell.org/package/hedgehog-0.5.3/docs/Hedgehog-Gen.html#v:recursive)? If the answer is no, then that alone is enough reason to move.
I'd be interested to see SPJ's proposal. Is there a link?
&gt; Something that every single effect system so far (except for MTL) fails at is encoding the Continuation monad. This is incredibly important, and neither your solution, nor extensible effects can handle it. Why do you say that? In fact the `Dsl` instance for `Cont` is more straightforward than the `Monad` instance for `Cont`. instance Dsl (Cont r) a r where (&gt;&gt;=) = runCont 
`fmap . fmap . fmap` is very easy to parse mentally when you view `fmap` as an operation over the function and not the functor. So, you give `fmap` a function `a -&gt; b` and `fmap` makes it a function, say `[a] -&gt; [b]`. In this view, `fmap` is not a curried multi-argument function, but a single argument function over functions. Then the `fmap . fmap . ...` chain simply becomes, "given a function; lift it, and then lift it and then lift it etc."
I tried to use nix, tried to like it (admittedly briefly). I think my objection can be boiled down to this: It doesn't distinguish between a package installed for nix's use and a package installed for the user's use. If I `nix-env -i inkscape` then I expect to be able to use `inkscape` as I would if it were installed by any package manager. If I `nix-env -i clang` I similarly expect to be able to use it as I always have. In the clang example, and most developer tools, I can not just use the tool but instead I must use a nix shell which has paths or environmental variables set that enable the tool to find data files (ex: header files), which is infuriating since I'm not thinking or working in with nix in an idiomatic manner - I want to use it as I've used virtually every other os/packaging system for twenty years.
This isn't consistent with my experience with Nix. `nix-env -i clang` installs a Clang that I can use outside any nix-shell just fine. `inkscape` too. It's rare that a Nix package isn't `nix-env` installable.
Nix won't work for Haskellers on Windows. I think a proper solution is: do the bundling, but also include Cabal flag to explicitly switch between bundled/external version.
Just an experiment, I don't know if it will offer any advantage. I am curious if it will help yield a simpler target language than SQL DDL or remove an entire layer of generation/evaluation
Surely there has to be a pure, clean representation for pipes that is as fast as lazy I/O? I remember the original iteratees advertising how much faster they were than lazy I/O.
You've got your chances and I can tell you that learning Haskell will open you some doors that were closed before, not only in terms of job oppportunities but also with the mere act of designing programs. Some Haskell-like languages are used for web development, such as PureScript and Elm. Check them out :) Also two advices for you to start would be 1) Read the [haskellbook.com](Haskell Book) 2) You don't need to get Monads to do useful stuff
The SO post was made in 2011, it seems unlikely they are the same person.
Bundling is often the best alternative for making sure things are seamless across many oses, including just different linux distros, as well as mac and windows. For mac and windows, assuming everyone uses nix or brew or choco or whatever is sorta bad form, especially if you anticipate less seasoned users. And even on linux distros, packages and versions of packages may vary widely between different systems. If something is small and self-contained enough, it works reliably a _lot_ more places if you just bundle...
It may be related to the strange way you handle `isFileOverwritten`. Try: do ... isFileOverwritten &lt;- writeToOriginalFile ... case isFileOverwritten of True -&gt; ... False -&gt; ...
Also, I can give you more feedback on your code later, if you'd like, but it'll have to wait until after work.
&gt; Nix won't work for Haskellers on Windows. Why not? Have you tried running it in a WSL shell? WSL is a Linux-compatible kernel API exposed by the Windows kernel as an equal peer to the Win32 API. Both Linux ELF binaries and NT binaries run fine there, and interact with each as part of the same unified platform. So if you can run Nix on Linux you can run it on Windows, and a GHC Nix package could allow you to choose to build either an NT binary or an ELF binary.
I vaguely remembered seeing mentions of this around previously. After a bit of searching now, I've found these: - http://web.archive.org/web/20160322051608/http://research.microsoft.com/en-us/um/people/simonpj/Haskell/records.html - https://www.microsoft.com/en-us/research/publication/lightweight-extensible-records-for-haskell/ - https://wiki.haskell.org/Extensible_record Or did you think of something else, /u/Chrisdone2? 
To GHC specifically I’m not really qualified to say, but afaik the main things you have to do are: * Add syntax for row types and functions taking &amp; returning rows—should be generally uncontroversial, doesn’t invalidate any existing code. * Add a “row” kind (and a “label” kind, but that could be handled by existing stuff like `Symbol` maybe?). * Use row unification to unify rows, or the equivalent rules with bidirectional typing. * Handle polymorphic label offsets in codegen—i.e., where is the actual field `x` in this value of which I only know its type is “record containing `x` of type `T`? (Might be able to reuse existing typeclass dictionary passing machinery to pass offsets, or reduce to `Has` typeclasses; might be more efficient to take a more specialised route.) 
I agree. Besides, parameters in Haskell functions already have names, but they don't appear in the type signature and therefore unfortunately also not in the Haddock generated API docs.
We could even define `!.` to be able to fill in optional parameters for us neatly without having to warp `Just`s at call-site. main = hspec $ specify "call" $ do ( pair !. #x 1 !. #y 2) `shouldBe` (1, 2) (call $ pair ) `shouldBe` (0, 0) (call $ pair !. #y 3 ) `shouldBe` (0, 3) (call $ pair !. #y 3 ) `shouldBe` (0, 3) (call $ pair !. #x 4 !. #y 5) `shouldBe` (4, 5) let triplet :: Maybe Int `Named` "x" -&gt; Maybe Int `Named` "y" -&gt; Int `Named` "z" -&gt; (Int, Int, Int) triplet (Named x) (Named y) (Named z) = (x ||| 0, y ||| 0, z) ( triplet !. #x 1 !. #y 2 ! #z 3) `shouldBe` (1, 2, 3) (call $ triplet !. #x 1 !. #y 2 ! #z 3) `shouldBe` (1, 2, 3) (call $ triplet !. #x 1 ! #z 3) `shouldBe` (1, 0, 3) (call $ triplet ! #z 3) `shouldBe` (0, 0, 3) (call $ triplet ) `shouldPass` isTypeError "#z is required" ( triplet !. #x 1 ! #z 3 == (1, 0, 3)) `shouldPass` isTypeError ("Couldn't match expected type `(Int, Int, Int)' " &lt;&gt; "with actual type `Maybe Int `Named` \"y\" -&gt; (Int, Int, Int)'") Also, maybe we can also call `call` to be `fillDefaults`, `fillDefs`, `fill`, or something more descriptive. I also wonder how difficult it would be to implement it. Anyone has an idea? cc /u/MelissaClick /u/Tarmen 
Ping /u/int_index, is `named` your library? If so you may be interested in the conversation in this thread. Also, I'm curious what you have to say about my parent comments, as you may be uniquely equipped to answer.
It's very interesting but there are a number of unfinished sentences that 
Building an NT binary through Nix + WSL sounds interesting. Has anyone ever attempted that? And even succeeded? A cross compiler is necessary for this, right? What are the steps to try? Is there a write up somewhere that I could read?
/u/tom-md, I second what ElvishJerrico said, `nix-env -i ...` should make the installed package available outside of `nix-shell`. It does for me too. Perhaps you are missing something like a `source path/to/nix.sh` in your `~/.bashrc`?
Yaay, finally we can have `-XBlockArguments`! Also, `-XNumericUnderscores` looks rather neat too. I've been wishing for something like that a few times. Somehow [this proposal](https://github.com/ghc-proposals/ghc-proposals/blob/master/proposals/0009-numeric-underscores.rst) flew under my radar. `threadDelay $ 2 * 1000 * 1000`, anyone? So then, as I understand, we'll be able to write `threadDelay 2_000_000`.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc-proposals/ghc-proposals/.../**0009-numeric-underscores.rst** (master → 615256d)](https://github.com/ghc-proposals/ghc-proposals/blob/615256d7daffe73e4fc591d5f663a638ebe10f81/proposals/0009-numeric-underscores.rst) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxtlsto.)
&gt; Does anyone else see this as such a nice solution as I do? I don't, I'm afraid. It's definitely a clever little thing, but if you were to present this as a serious solution to a Smalltalk, Objective-C, or Swift programmer, or any number of others, they'd think you were crazy. (Of course, if you showed them Haskell's record system, they'd think you were crazy for putting up with that too.) Compared to languages that have this built-in with nice syntax, IDE and tool support, et cetera, this is both baroque in its mechanism and primitive in its functionality. In my opinion, languages should either have named arguments built-in or they should offer some sort of structurally typed records. Until Haskell gets one or the other, I'll continue declaring record types and using newtypes for function parameters as is appropriate.
You can get good error messages in parsec, too. I don't think the question was about "I want to switch to a whole different parsing framework." It was "How do I get good error messages in Parsec?"
Right, when `blockA &lt;|&gt; blockB &lt;|&gt; blockC`is inside `many`it's expected to fail eventually. That's not an error, it just terminates the `many`. Then parsec looks for the next thing to parse. But the next thing (after discarding any newlines) is `eof` and that's not what it found, so that's where the error happened.
Text based, no high-performance requirements seems like a good match for MegaParsec. 2 is probably the right approach, although there’s 1000 variations to consider. One variation is to make the STM a series of messages in a queue. So the parsing/logic thread just sends updates as needed. Another variation is to do everything in the logic queue and just have a single shared mutable bit for repaint required. Without actually implementing it myself I’d hesitate to recommend a particular variation.
As someone who writes Objective-C and Swift (amongst other things) for a living, I can say that named parameters are truly a wonderful thing when used liberally in an environment with appropriate tool support. The IDE does all the typing and you, the programmer, get to benefit from code that's much simpler to read. I will take `split(list: xs, at: 42)` over `splitAt(42, xs)` any day.
There's already an implementation of row types as a GHC plugin: https://github.com/nfrisby/coxswain It has a few bugs, but they're the characteristic plugin bugs (weird type variables in error messages and so on).
Glad to hear that it is trivial for you! I don't think it would be quite trivial for me. :) I'm okay if these go into `named-defaults`, or `Named.Defaults`, are you willing to add them?
Yes, this is the conclusion I came to. My question is how can this be changed to convert this is in to error we need it to be?
&gt; if you were to present this as a serious solution to a Smalltalk, Objective-C, or Swift programmer, or any number of others, they'd think you were crazy This is hardly a counterargument. Some people might think that using `Maybe a` is crazy-complicated when using `null` is so much simpler. ("All that unnecessary wrapping and unwrapping, yuck!") I personally take delight in when the compiler gives flexible building blocks with which even these kinds of features can be constructed on the library level. But that being said I also strongly prefer when I can use features more than when I can't. And people did raise an objection regarding one limitation I concur with: optional named parameters. My hope is that with one of the 1-3 proposed solutions in this thread we can solve that too. Potentially sooner than in ~6 months of the next GHC release. &gt; built-in with nice syntax To me this syntax looks quite close to being as minimal as it possibly can: &gt; (replace ! #needle "," ! #replacement ";" ! #haystack "........,.,....") == "........;.;...." Do those other languages provide a nicer, more succinct syntax? &gt; primitive in its functionality Primitive, how? The optional named parameters is the only limitation I/we've found. Do you see any other feature that you may miss? I see all other use-cases well supported, and even that limitation may be surpassable. int_index, the creator of the package [says that it's trivial to support](https://www.reddit.com/r/haskell/comments/8e3vf3/package_worth_highlighting_named_adds_support_for/dxtmgou/?context=100). &gt; they should offer some sort of structurally typed records No argument from me here, I agree that this would still be nice to have in Haskell. &gt; IDE and tool support Bring it on. I don't see why tools couldn't support this feature being implemented as it is.
`many` succeeds if the last iteration fails without consuming any input. You can keep the error messages from that last failure (from all three branches) , if you fail immediately after that without consuming anything. However, if you fail after consuming (as `ignoreNewLines` probably does), the previous errors are discarded. To avoid that, you can wrap the last statements in a `try`. do blocks &lt;- many ... try (ignoreNewLines *&gt; eof) pure blocks Also try to fail after consuming input inside `many` (so the subsequent statement are not even executed). In either of `blockA`, `blockB`, or `blockC`, an initial `try` should only span until a point where it is certain that the consumed input does not correspond to any other block. blockA = do try (...) -- After this point, we are sure this is not blockB/blockC -- so any subsequent failure is proper to parsing a blockA etc If the three blocks share a common header format, it is also preferable to parse it before branching out with `(&lt;|&gt;)` or `case`. Then if you fail after `blockHeader` has consumed any input (which can be in the middle of `blockHeader` or in the block-specific parsing after it), that will cause `many` to fail immediately. blocks &lt;- many $ do bh &lt;- blockHeader blockA bh &lt;|&gt; blockB bh &lt;|&gt; blockC bh
Wow! Any compile-time and run-time perf benchmarks?
[removed]
Right - this reddit thread is about float not being a monoid. What I'm saying is there's a broader pattern: for every rule R you might have an intuition that numbers have, there exists T a numeric type that R does not hold for. The word "number" is such a weak, overloaded (insinuating?) thing, that then leads to expectations about what `Num` means... 
In practice, that instance doesn't cause me any problems. But I would strongly prefer if it wasn't needed :P
I'm sorry if you felt that my comment sounds "harsh", that was not my intention at all. The classic semantics of Haskell functions and function application - since Haskell 98 and even before - is perhaps the strongest feature of Haskell, far stronger than Python functions, and one of the main reasons I switched from Python to Haskell. That doesn't mean it can't be improved, and this looks really cool. But for a change as fundamental and far-reaching as this to the way we write Haskell, providing a nice simple usage recipe and then saying "go read the source code" just doesn't cut it. I would expect to find something like a blog post that explains in detail: * Usage: What is the new way to write Haskell compared to the old way, and what are the advantages and disadvantages of each. * Implementation: What machinery is this using under the hood with what required extensions, and what is the basic idea of how it is implemented.
I think it's that april is `TODO`
... that sounds like a pretty legit use case. Nice!
The solution for that is better types; I don't think that's what this feature is for. On the contrary, if this feature encourages people to use more "stringly types", that is a disadvantage. In my opinion, this feature adds: * Flexible currying. It removes the burden of carefully crafting the parameter order of various functions to make currying work out. * Makes functions with many parameters less painful. In real-life applications you sometimes do need functions that depend on many parameters. To make it readable, you artificially group parameters together into an auxiliary record type. This feature reduces the threshold where that is needed. On the flip side, it could discourage gathering parameters into a type where needed for other reasons. * More readable code at function call sites. It reduces (but does not eliminate) the burden of crafting intermediate variable names to make it clear what is happening in function calls without being overly verbose. On the flip side, it could discourage the use of meaningful variable names where needed for other reasons. Overall - I could see this being a great feature. But don't let it discourage you from fixing stringly types.
Yes, and that's great, but you can't cover all the cases with custom type errors and you certainly don't have enough context to cover all the errors that GHC can print for regular function application. For example try removing the `!` from some function call. The error doesn't make any sense. For something as core as parameter passing, I expect a parse error on something like that. Of course, you as the library author can't do anything about it. Or for example, `fmap (replace ! #needle "test" ! #replacement "test2") ["test"]`. I'd expect this to work because the named parameters should be an opt-in feature. It doesn't and the error message talks about `IsString` instances for `Named Text "haystack"`.
Thanks those are good pointers. The basic machinery is built from the extensions [overloaded labels](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overloaded-labels), [type-level literals](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#type-level-literals), and, under the hood, [visible type application](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application). Are you *sure* that the extra lambdas will always be optimized away? Does it need to be tested? That's a really important detail. What about the apparent need to generate `O(n^2)` instances of an `Apply` type class? Am I just imagining that? This feature uses `UndecidableInstances`, which is a scary extension.
Thanks, I'll look into that
&gt; We do not want people to have to read library documentation to understand something as basic as syntax. Why not? Seem to me like, in a lot of cases, Haskell supports almost everything imaginable, but with horrible syntax (evidenced by custom syntax like e.g. "ifThenElse", do-notation (which, later on, was extended to support `Applicative` instead of just `Monad`)). This way there'd be competition wrt. syntax (different libraries could offer different syntax, but with the same functionality).
How would it look like? I can't imagine 😶
I mean I'll look into that in a few weeks when I have the background to implement it ha. ... in the meantime, if you're bored, would you mind giving an example in code? It would help with my learning
Sure! So, the `iterate` function takes another function as a parameter and it takes an initial value. Then it produces an infinite list where each element is your function applied one more time on the last. Something like this `iterate f a == [a, f a, f (f a), f (f (f a))...` The next piece is `takeWhile (&lt;= b)` which takes that infinite list and gives you a new one with the same elements, but this new list stops when you get to an element that's bigger than `b`. Combining those two pieces, you can imagine the original syntax in your question being equivalent to `[a, b..c] == takeWhile (&lt;= c) (iterate (+ (b - a)) a)`. In other words, start with `a`, then add `b - a` to each consecutive element. Finally, take a prefix of that inifinte list of all the elements less than `c`. Now, what if we instead want each element to be double the last? &gt; takeWhile (&lt;= 100) $ iterate (*2) 1 [1,2,4,8,16,32,64] You can use any stepping function you want. Hope this helps!
Try `takeWhile (&lt;= 6) (iterate succ 1)`.
You may [find `unfoldr` a little easier](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html#v:unfoldr).
Mainly because you would make it even more difficult for people to read your code, including those new to the language who might not be committed enough to stay. Just look at e.g. the backlash Yesod got for its relatively simple quasi quotation based routing and web template syntax systems.
/u/yang_bo I'm sorry that you got roasted in this thread. I hope the unfriendly response to your suggestion hasn't put you off Haskell for good - though I fully understand why it might have done! - because it's a beautiful language and I personally continue to learn new things every day from Haskell and its ecosystem. Haskellers as a species really _are_ receptive to new ideas - that's why we're Haskellers in the first place! - but you have to approach it in the right way. I personally can see some merit in your idea, but I can also understand why some people were upset by the way in which you presented it. I think the main reason that you managed to get people's backs up is that you proposed a sweeping change to one of the language's core features out of the blue, without prior discussion. The politically controversial title - thank you for changing it - and your defensive tone in the comments certainly didn't help. On the whole I get the impression that people felt _disrespected_ by the manner in which you brought up your idea and that's why they reacted badly. It may well be the case that some (not all) of the people responding hadn't taken the time to fully understand your idea, but by that point it was too late - they were reacting emotionally because that's what people do when they feel insulted. Next time it would be more appropriate to float your idea for discussion in an informal forum, such as here on Reddit or in IRC, before writing an entire GHC proposal. You would certainly have got some criticism, but it may have been more measured in its tone, and hopefully you would've learned something from it! (Of course you still have a responsibility not to escalate the argument by responding defensively.) I would be angry and upset if I was in your position, and I might find this comment hard to read, but I hope that you're able to read it with the compassion and respect with which it was intended.
You can actually make it look even nicer, like so: type a (:::) b = Named a b replace :: "needle" ::: Text -&gt; "replacement" ::: Text -&gt; "haystack" ::: Text -&gt; Text replace (Named replacement) (Named needle) (Named haystack) = ... 
Sorry if this was mentioned but I didn't see it anywhere: it looks to me like you're using `many` when you should actually be using `many1`?
Oops sorry, I tried not to use `$` but I guess one slipped by. `takeWhile (&lt;= 100) $ iterate (*2) 1` is actually exactly the same as `takeWhile (&lt;= 100) (iterate (*2) 1)`. The `$` just lets you get rid of some parentheses
The syntax [a, b .. c] isn't intended to express all possible lists. It's just for arithmetic sequences. Those are common enough that I'm glad to have to special syntax for them. You can express any sequence with a closed form expression by using a list comprehension, such as: [ n^2 | n &lt;- [1 .. 10] ]. If there is no closed form, then you have to turn to more complicated techniques, such as in other responses here. Some will prefer the more applicative style even for simple lists, but it's nice to be able to just write the math you intended in the simple cases.
I'd really love a `threadDelay` API which takes a `TimeDelta` something instead of an integer of micro second (or is this milli, or nano ?). I'm always wrong with my threadDelay by a factor of 1000. Or at least a `Float` in second, I think we don't really care about the "precision" in `threadDelay`.
https://github.com/ghc-proposals/ghc-proposals/blob/3d7fab718306b2486c1af338bc2a9e2e4ff8029c/proposals/0010-block-arguments.rst
It's a draft (for the Haskell Communities and Activities Report). http://mail.haskell.org/pipermail/ghc-devs/2018-April/015654.html
Yes, Hakyll respects the locale set by the environment. This is really a system misconfiguration issue and not a Hakyll/Pandoc/Haskell issue. I don't really know much about Docker or GitLab but I suspect it's a matter of generating the right locale using `locale-gen`, installing some package which contains the missing locale, or as a last resort using something else like `en_US.UTF-8`.
Excellent thank you. It appears at a glance that this approach might lend me to specify step by its index? replacing `n^2` with an arbitrary function? I'm batting out of my league at the moment so forgive if that's nonsensical, and no I don't have a use case I'm aiming for, just exploring. I can't actually try to implement any of this until later.
Looks like [this known issue](https://github.com/plow-technologies/ble/issues/20)
Great, thank you. I'll take a look when I get home. !RemindMe 6 hours
I will be messaging you on [**2018-04-23 20:48:38 UTC**](http://www.wolframalpha.com/input/?i=2018-04-23 20:48:38 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/8e3vf3/package_worth_highlighting_named_adds_support_for/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/8e3vf3/package_worth_highlighting_named_adds_support_for/]%0A%0ARemindMe! 6 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Yup, I agree. I also like to define this alias locally. IIRC I've also suggested something similar in a GitHub issue.
Yep, and the deadline for contributions to the next HCAR is next week. I'm still looking for more material :)
Ryan Scott has been `TODO` 10/10
Yes, you can replace n^2 with any expression you want (it doesn't even have to involve n, but if it doesn't, all the elements of the resulting list will be the same). You can also pull elements from multiple lists: [x * y | x &lt;- someList, y &lt;- someOtherList] That will give you x*y for all possible combinations of x and y. 
That describes my whole life, really
 I don't have the same experience as you or /u/catscatscats instead I experience what /u/tjr_22 mentions. For example: % ~/.nix-profile/bin/clang x.c -o xclang &amp;&amp; ./xclang x.c:2:10: fatal error: 'stdio.h' file not found #include &lt;stdio.h&gt; ^~~~~~~~~ 1 error generated. N.B. % cat x.c #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; int main() { printf(stderr, "test\n"); return (int) ((uint32_t) 0); } 
How did you install it and what version of nixpkgs are you on?
&gt; might lend me to specify step by its index? List comprehensions allow you to express *elements* of the list by their index. So you don't think in terms of a "step" any more. If you really meant that you want to give the step (i.e., difference between consecutive elements) in terms of the index, then you might be interested in making a list of these steps, and then adding them up to get a sequence of partial sums. partialSums :: Num a =&gt; [a] -&gt; [a] partialSums = sumFrom 0 where sumFrom y (x:xs) = let yy = x + y in yy : sumFrom yy xs sumFrom _ [] = []
I wonder, what criteria are at play when something is decided to be included into `base`? For instance, why now? Why `Functor/Contravariant` but not `Profunctor`?
No, it does not consider those equal, and I don't think it should. Those are not identical in the binary form.
There are a few, somewhat stalled, native postgresql wire protocols implemented in Haskell, e.g.: https://github.com/postgres-haskell/postgres-wire Google for “haskell postgresql wire” to tease out the others.
(&gt;&gt;=) = runCont? So, there's no way to chain two `Cont`s together? That's remarkably unuseful.
installed via: nix-env -i clang and as for the version: % nix-env --version nix-env (Nix) 1.11.15 This is on OS-X but I can SSH into my nix machine (vm) and get the exact same result: [tommd@nixos-plain:~]$ nix-env -i clang replacing old ‘clang-3.9.1’ installing ‘clang-3.9.1’ [tommd@nixos-plain:~]$ vi x.c [tommd@nixos-plain:~]$ clang x.c x.c:1:10: fatal error: 'stdio.h' file not found #include &lt;stdio.h&gt; ^ 1 error generated. and: $ nixos-version 17.03.1858.3d04a557b7 (Gorilla)
People thought I want to replace `Monad`. It's not true. `Monad` is good and useful, just like other type classes. However, it does not mean do notation has to use `Monad`. As /u/travis_athougies pointed out, `Monad`s are not DSLs, unfortunately the Haskell community use do notation + `Monad` for DSLs in many ways including `Free`, `Freer` or `Eff`, which are quite inconvenient. It could be simpler if do notation is used with another type class for DSL only.
Wow, fascinating. There is a bunch of good stuff here for me to get into, thanks.
`parse error on input ‘=’ Perhaps you need a 'let' in a 'do' block? e.g. 'let x = 5' instead of 'x = 5' | 68 | sumFrom _ [] = [] | ^ `
seems like the last one is on lts-8.24: https://www.stackage.org/lts-8.24/package/fay-0.23.1.16 and this should still work you can probably clone and compile it from source as well: https://github.com/faylang/fay/wiki/Installing-and-running no clue if it will work with stack as is but it should be no big deal OT: have you considered looking into purescript?
I have to point out that you just only read half of the post...
Yes. typeOf is your algebra. fold/cata should be able to apply that to Fix ExprF.
a catamorphism would only produce the end result: \``Typing Type`. I want to get the original tree annotated with intermediate results.
It looks you can use a paramorphism according to http://blog.sumtypeofway.com/recursion-schemes-part-iii-folds-in-context/
Try updating your nixpkgs. nix-channel --update nixpkgs Maybe Clang was broken at one time, but that was definitely undesirable, and at least on the version I'm using, it works fine.
You might want to look at [this old article of mine](http://comonad.com/reader/2009/incremental-folds/), which talks about annotating ASTs and the like.
Histomorphism, then?
(I had been assuming we would also get `QuantifiedConstraints` in 8.6; is this not the case, or is the writeup merely incomplete?)
You can do whatever you want with `do` notation (remove the Monad restriction, for example) by turning on `RebindableSyntax`.
How about `scanl` / `scanl'`? partialSums = scanl (+) 0
The problem seems to be how to lift the algebra ExprF (Typing Type) -&gt; Typing Type into the algebra ExprF (Typing (Fix (Compose (Type,) ExprF))) -&gt; Typing (Fix (Compose (Type,) ExprF)) 
I'm trying to move from lts-8.24 to higher I manage to use fay-0.23.2 but fay-0.24 breaks yesod-fay. I have also some problem using lts-10 and 11 ... Purescript is probably a good idea, but I have quite a few stuff written in Fay already, some I'm kind of stuck with Fay for the moment.
I understand what you mean but I'm not sure that Named is THE new way and everybody will use it for every functions. However, there are cases, which are ambiguous and this approach will be beneficial. People are already using the newtype trick (or equivalent). Named is not really different, only neater. Default values used in conjunction with lenses provides already a similar approach. That doesn't mean everybody is using it, even though some packages (like diagram) chose to use it. That's an API design which makes sense for that particular case.
Oh I see the issue now. Try using `nix-env -iA clang` instead of `-i`. Basically, `-i` is pointing to the unconfigured clang. But `-iA` tells it to use a concrete attrpath instead of the magic that `-i` does by default, and the derivation at the concrete `clang` attrpath is properly configured. I'd consider this a bug in nixpkgs. clang-wrapper ought to be what's installed with `nix-env -i clang`, but right now it's the unwrapped one.
maybe you can get in touch with the yesod-fay maintainer(s) or try to fix it yourself (it's probably not that hard)
Sure! I didn't remember that this pattern had a name.
Thanks for walking through this with me. I needed just one more tweak - the incantation that worked for me is: nix-env -f "&lt;nixpkgs&gt;" -iA clang With that done I get a clang that I can use directly. I suspect my issues with other dev tools can be similarly tackled. Thanks again!
I don't understand the what the chain you mean. Does it mean chain of types or sequential continuation operations?
I'm trying to follow the proto-lens tutorial at https://github.com/google/proto-lens/tree/master/proto-lens-tutorial/person, but when I trigger the build it can't find the protoc binary even though it's installed: &gt; osg-proto-0.0.0: build (lib) &gt; ========== &gt; Error: couldn't find the executable "protoc" in your $PATH. &gt; Follow the installation instructions at https://google.github.io/proto-lens/installing-protoc.html . &gt; ========== &gt; Missing executable "protoc" &gt; CallStack (from HasCallStack): &gt; error, called at src/Data/ProtoLens/Setup.hs:297:13 in proto-lens-protoc-0.2.2.3-HsIJaKE8qa7Dx26MSJkOJQ:Data.ProtoLens.Setup &gt; &gt; -- While building custom Setup.hs for package osg-proto-0.0.0 using: &gt; /home/pi/personal/projects/osg/proto/.stack-work/dist/x86_64-linux-nix/Cabal-2.0.1.0/setup/setup --builddir=.stack-work/dist/x86_64-linux-nix/Cabal-2.0.1.0 build lib:osg-proto --ghc-options " -ddump-hi -ddump-to-file -fdiagnostics-color=always" &gt; Process exited with code: ExitFailure 1 protoc is on my path: &gt; which protoc &gt; /home/pi/.nix-profile/bin/protoc I'm using NixOS, triggering the build with *stack build --nix*. I don't even know where to start looking... 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [google/proto-lens/.../**person** (master → e9461a9)](https://github.com/google/proto-lens/tree/e9461a9ded98c2b80286b4a9996772247cf0c567/proto-lens-tutorial/person) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxuh57g.)
Yes please
Given Monads are not DSLs, and Free Monad is commonly used to build DSLs Do you find any problem here?
That proposal is quite nice. Aside from the overloading of `.`, I quite like it. Also, the proposed type level rows should probably be replaced with DataKinds-style lifted value records. I would not be opposed to that implementation at all.
I've just been trying to fix it, but it seems out of my reach ...
Use Maybe most of the time. People implement lots of things for Maybe that you can get for free by using Maybe. If your type is semantically wrong without a Nothing variant (maybe someone else could name an example where this is the case), then add a Nothing variant. Don’t add a Nothing variant because of a specific use of your type.
It really depends on the level of refactoring involved. If NoStuff really makes sense, by using Maybe, you will probably find yourself changing all type signature to use maybe, then in that case you might be better using NoStuff. If by using NoStuff, you find yourself adding the NoStuff case to lots of function (and not really knowing what to do with it), then Maybe is probably a better idea. In fact, Maybe is probably pretty much always a better idea.
Use `Maybe`. Always. Semantically, `Maybe` is one of the most fundamental types in Haskell's type system. Despite the apparent simplicity of `Maybe`, there is such a huge amount of very commonly used machinery built around it that if you "roll your own" you are almost guaranteed to re-invent the wheel in a way that will confuse your readers. That machinery includes: * Instances: `Functor`, `Applicative`, 'Alternative`, `Monad`, `Monoid`, `Foldable`, `Traversable`, `NFData`, `Typeable`, `Generic` * Wrappers: `First`, `Last` * Combinators: `maybe`, `fromMaybe`, `listToMaybe`, `maybeToList`, `mapMaybe`, `catMaybes`, `isJust`, `isNothing` and much more.
Use `Maybe`. Always. Semantically, `Maybe` is one of the most fundamental types in Haskell's type system. Despite the apparent simplicity of `Maybe`, there is such a huge amount of very commonly used machinery built around it that if you "roll your own" you are almost guaranteed to re-invent the wheel in a way that will confuse your readers. That machinery includes: * Instances: `Functor`, `Applicative`, 'Alternative`, `Monad`, `Monoid`, `Foldable`, `Traversable`, `NFData`, `Typeable`, `Generic` * Wrappers: `First`, `Last` * Combinators: `maybe`, `fromMaybe`, `listToMaybe`, `maybeToList`, `mapMaybe`, `catMaybes`, `isJust`, `isNothing` and much more.
It depends. If all you really want to do is to sometimes return "no value" from some functions, you should use `Maybe`. If absence of a value actually _means_ something in the domain you're modeling, I'd add it as a separate constructor (although in that case you really should be able to tell right away rather than 'one day').
I'm glad this extension got picked up instead of continuing to languish for years, thank you /u/takano-akio. This will be fun to play with.
If you use Maybe, you get a bunch of stuff for free and your code becomes usable by more existing code as well. Almost anytime more information can be exposed in a type, it should be.
I think you might be looking for `ana` to annotate the tree. You'd unfold your "fixed" datatype (`Fix ExprF`) into `EnvT`.
I’ve actually got a trac issue open to deprecate and remove the First and Last from Data.Monoid. Just a heads up.
So if there are different reasons why you may not have one of those things, then yes, you want different constructors or types for their absence. But that's not `Maybe`. If you are only trying to express "one of those, or maybe not", then use `Maybe`.
No. Free monads are particular type of Monad best suited for building abstract syntax trees. However, Free monads are only one kind of monad. I said that monads are not DSLs. That doesn't mean many DSLs aren't monads. Do notation is convenient for both, not just DSLs. Your solution was to redefine `&gt;&gt;=` to make it particular to DSLs, but as it is right now, it can be used to structure arbitrary monads as well as DSLs, which is really quite nice. Moreover, it allows us to do so while providing laws which can be reasoned about and used for optimization. It is overall a more powerful feature than what you've described.
Heh, thanks. I used to use `First` all the time, until I discovered the `Alternative` instance. Now I haven't used `First` or `Last` in years. But why remove them? Could you please give a link to the trac issue?
Can you explain why or provide a link to the trac ticket? Aside: I'm sad that the `Monoid` instance for `Map` is not `Monoid m =&gt; Map k m`. You could then express the current behaviour by using `First`. There's probably too much legacy code to ever hope for that to change though.
&gt; parse error on input ‘=’ Perhaps you need a 'let' in a 'do' block? e.g. 'let x = 5' instead of 'x = 5' | 68 | sumFrom _ [] = [] | ^ Use `&gt;` instead of indentation to quote non-code. At a glance it appears you are using an old version of ghc(i). Try `let partialSums = scanl (+) 0` 
@andrewthad you scared a little, so it's worth mentioning [your email to the libraries list](https://mail.haskell.org/pipermail/libraries/2018-April/028712.html) The types `Data.Semigroup.First` and `Data.Semigroup.Last` are (a) not going away, (b) can be used in place of `Data.Monoid.First` and `Data.Monoid.Last` by wrapping the former in a `Maybe` to get usable `Monoid` instances. Here's the [trac ticket #15028](https://ghc.haskell.org/trac/ghc/ticket/15028)
This is a misrepresentation of boolean blindness. Boolean blindness isn't when you compress &gt;2 meanings into two constructors. It's when you throw away evidence and context. 
That’s actually being discussed on the libraries mailing list currently 
In a sense, yes, but that's an English language vague sense (or a confusion based on object-oriented programming) and not a Haskell-language sense. Haskell has a pretty complex type system. As a result, more vague English words for something like "type" have been given special meanings. Off the top of my head, "type", "class", "kind", "sort", "category" and "family" at least. Consequences... 1. It's a bad idea to try to learn the Haskell type-system all in one go. 2. It's a bad idea to be dismissive and vague about the meanings of some seemingly vague words - it just creates a lot of confusion a bit later. A class is a name for an, errr..., errrm... - a set of types that provide a common interface. It's how Haskell provides "ad-hoc polymorphism" where different types provide the same functions, and the implementations of those functions may vary depending on which type you're referring to. Those implementations are provided when you declare an "instance" of the class for that type. `1 :: Int` is an example of a value. `Int` is an example of a type. `1` is an example of a polymorphic value - it doesn't know its type until type inference determines which type applies. `Num` is an example of a class. There are no values of type `Num` because `Num` isn't a type. But there are types that instance `Num` (such as `Int`), and those types have values. One consequence is that you cannot write `1 :: Num`. one = (1 :: Num) -- Illegal - Num is not a type, this isn't OOP one :: Num a =&gt; a -- Fine - the type of the nullary function called one one = 1 -- Fine - the value of that nullary function In the valid nullary function above, `a` is a type variable representing some unknown type. It might be `Int`, or it might not. It must however be some type that behaves like numbers - that instances `Num` - because that's what the `Num a =&gt;` constraint specifies. Because of that, the polymorphic literal `1` can be used, so the value of `one` (of any type that instances `Num`) is the same thing as the `1` of that type. Haskell also has subclasses - classes that are related but more specialized (generally providing more functions and applying to fewer types) but subclasses are still classes, not types. This means that Haskell doesn't have some abilities that OOP classes have - they can't be treated like types. But that clear division of labor allows Haskell classes (especially going beyond Haskell 98) to have a LOT of abilities that OOP classes don't have, in particular for defining classes that aren't really a class of types any more - they provide more sophisticated interfaces in which multiple types each play specific roles. Though I'm now going to be lynched for using the word "role" in a vague general English sense - yes, that has a meaning in the Haskell type system too, but don't worry, it's a dark-corners-of-the-compiler-internals kind of issue and you don't need to know it, certainly not yet.
I'd be happy as a clam if it went through, but it'll need a lot of care.
It is pretty easy to switch between parsing frameworks nowadays. All the functions mostly have the same names.
An example would be a cons list, which needs a nil element at the end. Maybe makes sense most of the time though. 
Hi there! Is there a way to write a `zipLatest` function with pipes, without using `pipes-concurrency`? What I mean by `zipLatest` is something like this: &gt;&gt;&gt; toList $ zipLatest [each [1,2,3,4,5], each ['a', 'b', 'c']] [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'c'), (5, 'c')]
Actually no, the post was made the same day as this question... and also by someone named Pedro. See the first link.
It looks like you have a basic misunderstanding of `IO`. A value of type `IO a` is a value representing an action that 1. may do some I/O and 2. returns a value of type `a` when it’s executed. In particular, an `IO String` value like `getLine` does *not* “contain” a `String`, it specifies how to *get* a `String` using side effects. The only things you can do with `IO` actions are *create* them with standard functions like `pure`, `putStrLn`, `readFile`, &amp;c., and *connect* them to other `IO` actions using the `&gt;&gt;=` operator—or `do` notation, which desugars to `&gt;&gt;=`. In particular, you *cannot* execute `IO` actions in a pure function, so the type of your function `parseAndTestFile` is wrong: it says it’s a pure function which takes and returns `ParseAndTestInformationOutput`, but it needs to do I/O when it calls `writeToOriginalFile`, so it should have a type that includes `IO`, such as `ParseAndTestInformationOutput -&gt; IO ParseAndTestInformationOutput`. However, when you write this: let isFileOverwritten = writeToOriginalFile (nextBeforeStatement ++ nextAfterStatement) fileName You are *not* calling `writeToOriginalFile`—you are making an action of type `IO Bool` and naming it `isFileOverwritten`. If you want to *execute* `writeToOriginalFile` and get its result, you need to use a “bind” statement: isFileOverwritten &lt;- writeToOriginalFile (nextBeforeStatement ++ nextAfterStatement) fileName In addition, when you write: case isFileOverwritten of returnIOBool -&gt; ... _ -&gt; ... This doesn’t do what you think—you’re not comparing the result of `writeToOriginalFile` with the top-level definition `returnIOBool`. You’re attempting to pattern-match on `IO Bool` values, which is not possible (from ordinary code); and the compiler only allows this because your pattern *always* matches because you’re creating a *new* variable called `returnIOBool` which *shadows* your the top-level definition of `returnIOBool`. It’s just as if you wrote: case x of y -&gt; y + 1 -- Always matches, defining a local variable y = x. _ -&gt; 0 -- Never matches because the previous pattern always matches. Enabling all warnings with `-Wall` would have helped you catch this. You’re passing around `IO` actions as arguments often when you don’t need to. The main reason to have a function with a type like: IO String -&gt; IO String Is when you want to control when &amp; how the action is executed, or “wrap” it with other actions. For example: main :: IO () main = do result1 &lt;- hello putStrLn $ "The result of ‘hello’ was: " ++ show result1 putStr "\n" result2 &lt;- wrapped hello putStrLn $ "The result of ‘wrapped hello’ was: " ++ show result2 hello :: IO String hello = do putStrLn "hello" return "result" wrapped :: IO a -&gt; IO a wrapped action = do putStrLn "begin wrapping" result &lt;- action putStrLn "end wrapping" return ("wrapped " ++ result) This outputs: hello The result of ‘hello’ was: "result" begin wrapping hello end wrapping The result of ‘wrapped hello’ was: "wrapped result" Likewise, I don’t think `ParseAndTestInformationOutput` needs to contain an `IO String`. There are other minor issues, e.g., `if x then True else False` is exactly equivalent to just `x`, and `case y of { True -&gt; …; False -&gt; … }` is the same as `if y then … else …`, but the issues with `IO` seem to be the core of your problem. For now I would suggest keeping `IO` to the *results* of your functions, like so: param1 -&gt; param2 -&gt; … -&gt; paramN -&gt; IO result And following error messages as needed to keep them that way. For example, if the compiler says “couldn’t match expected type `Int` with actual type `IO Int`”, that’s a good sign you need to use a bind statement `&lt;-` to run the `IO Int` to obtain an `Int`. 
Btw Aeson JSON type has Null constructor as opposed to handling it with Maybe. Same goes for hdbc that has its own SqlNull. Then they have code that translates those into Maybes. I think this is the right approach because both aeson and hdbc are "wire" formats. In other words they are used to store what came into your program from outside or what you are going to send. Therefore they must match the data that is transferred one to one.
It's still in consideration by the steering committee but will likely pass. https://mail.haskell.org/pipermail/ghc-steering-committee/2018-April/000470.html
I've probably used it as much as anyone else has (with the exception of the author himself!) and it tends to work fine for basic stuff with concrete record types. I tried writing an extensible-effects library with it and had all manner of troubles. I don't think it's production-ready, honestly.
&gt; This is hardly a counterargument. Some people might think that using &gt; `Maybe a` is crazy-complicated when using `null` is so much simpler. Fair enough. I think what I meant to say is that, compared to languages with solid approaches to named arguments, I find this pretty awkward. &gt; Do those other languages provide a nicer, more succinct syntax? I think Swift handles this a good deal more nicely, yes. &gt; Primitive, how? I mostly meant in terms of tool support and type error reporting. If I have a type in a parameter name in a language that has this stuff built in, my IDE will chirp up and say "You wrote `foo` but I think you meant `food`. Shall I correct this for you? [Y/n]". With this approach in Haskell though, I'll just get some type error. &gt; Bring it on. I don't see why tools couldn't support this feature being implemented as it is. Something is surely possible, but it'll be a good deal more complicated than dealing with something built into the language.
I think the proposal for [Deriving Via](https://github.com/ghc-proposals/ghc-proposals/pull/120) should be sufficient for your Result newtype. It would be a `deriving ... via Either` where each class name would have to be listed, but that doesn't seem like a huge burden. (It looks like a marvellous extension overall, so it'll hopefully get through the proposal process without too much fuss.)
Yeah, I had `DerivingVia` in mind when I wrote that, it’s just that having to list all the classes means you don’t benefit from instances added in the future for the original type. Not such a big deal.
[Just uploaded it.](https://www.youtube.com/watch?v=dzt1u2Q1FlI&amp;feature=youtu.be) It turned out to be way longer than I wanted (1.75 hours) so sorry about that :(
I think reddit did something weird, because I couldn't see this message last night, even though it's older than that... I'll see when I can find some time, maybe after work today :)
What's wrong with list a = Cons (a, Maybe list a)
Maybe is just as likely to fall victim to the same disadvantages of Bool or Int. Boolean blindness applies to any overly general structure that trades away context for convenience.
Aside from all the other arguments: Maybe a is a boxed value which cannot be unpacked (but can be eliminated sometimes thanks to inlining and deforestation). Whereas your own type can have more reliable performance characteristics.
Why would you want to do that. Then you wouldn’t be able to have generic math operators between ie Int, Word, Integer, Rational, Float and Double.
To make that match current lists (which can be empty) you'd have to use `Maybe (List a)`, and then you can't write (the right) a `Functor` instance for it.
A very basic library like Control.Monad.State is now defined using Monad transformers and uses multi-param classes and functional dependencies. You've got as much chance of writing Haskell programs using the simple core as you have of writing C++ ones using just the safe modern parts of that language.
This is an easy choice, ask this question before append another constructor into your data type: *Are you going to have this constructor to be one of your data type, to be pattern matched for the rest of your life, will you care her/him, love her/him, honor and keep her/him in sickness and in health, and forsaking all others, be faithful to her/him as long as you both shall live in haskell?* If you do, then do it now!
Let's say we'd like to have a more fine-grained typeclass hierarchy, for example.
It would probably have to go through the generic representation coercability instead of the regular coercabiloty since you can't really coerce between sum types.
I imagine that you do not want to break compatibility with existing libraries. A possibility would be to define your alternative hierarchy, and define some kind of overlappable instance `Num a =&gt; Class a` and a overlapping one `Class1 a, Class2 a =&gt; Num a`, but this seems... unhealthy.
You can't change the type of literals, so if you're ok with never using those then maybe? Although I'd find it difficult to write constant free code.
Is this for the status of a dbus service? I'm fairly new to the dbus world.
Does RebindableSyntax handle this?
Yep
btw, I just ran into this whole mess and trying your suggestion I only get another wrong version bound when building tagged-0.8.5: transformers-compat &gt;=0.5 &amp;&amp; &lt;1 &amp;&amp; ==0.4.0.4 Is there no better way to fix this? I get that you thankfully have fixed future snapshots to no longer be effected by this Hackage misfeature, but I'd really like to keep my old code working for my users. Is there any way to just tell Cabal invoked from stack to ignore all version bounds and just build like before? Can I just tell stack to ignore revision and fetch the original one?
I mean of course you would be able to, just because you don't want to use `Num` doesn't mean you don't want to use something else. If I could change anything in the existing hierarchy right now it would be `Num`. Quite frequently I end up with types that have two fundamental operations that distribute and thus would fit perfectly into a ring-like hierarchy, but that don't have a sane implementation of `abs` or `signum` or `fromInteger`. Such as Sets, Filters and lattice-like / partially ordered things. I would love it if a `Semiring` class was made and `+` and `*` were defined within `Semiring` instead of within `Num`. Well I'd personally go a lot further than that, but that alone would be just swell.
&gt; Perhaps someone here will come up with some case where it is absolutely clearly better to roll your own. A good example is List. `[]` is `Nothing` and `[a]` isomorphic to `Maybe (NonEmpty a)`. This may stands for pretty much for every `Monoid` ?
https://hackage.haskell.org/package/numeric-prelude
Why would you see this as unhealthy? Too much trouble only for re-obtaining Num?
You can't probably hack `OverloadedStrings`, can't you? I guess it might only work on the RHS.
Rust explicitly addressed this: it has Eq and Ord typeclasses, which ints and rationals adhere to; and it has PartialEq and PartialOrd typeclasses for types like floating-point numbers. https://doc.rust-lang.org/std/cmp/trait.PartialEq.html So the real answer is the original Haskell designers thought it was _easier_ to ignore floating-point weirdness, and there's enough baked in legacy not to eliminate what was evidently a mistake. For example in the Rust type-system, the type-checker will prevent you using a floating point number as a key in a map. The Haskell type-system won't catch this bug.
&gt; This is of course a recursive structure, but doesn't support cata and friends unless you parameterize it over the recursive bit and Fix the result: What about makeBaseFunctor ''Tree
Good example. The purpose of `Maybe` is to represent the "maybe not there" intent, as the name communicates. Isomorphism by itself truly is not a sufficient criterion.
Polymorphic variants aren't strictly necessary. You could also explicitly create the `AppCmd` type. The `Which` variant just helps with automatically creating the `AsFacet` instance. I think the `Control.Lens.TH` `ClassyPrisms` could probably be used instead.
Please note, I've only tested the the gist with data-diverse &gt;= 3 and data-diverse-lens &gt;= 3.1
That's not haskell 98 though
Haskell-interactive-mode for emacs by default just displays the type when there is no known instance for Show. This is probably even more confusing for beginners, but it feels a lot nicer than the error message. We could probably do something in-between with displaying the type and a (suppressible) message about how to make the value be showable. An automatic recursive "deriving instance Show" that also derives Show for any parts of the data type that is missing Show as well would be really nice for this.
Sorry, I probably should have clarified that better.
By the way, [Bifunctor is in base now](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Bifunctor.html), no need for the bifunctors library. &gt; So far so good, but the `Functor` instance for these `Fix`ed structures (which is used in `cata`) maps over the structure, rather than the values I wouldn't say that RTree is the `Fix`ed structure, I would say that `RTree` is the base `Functor` from which you construct the `Fix`ed structure `Tree`. Since `Tree` is a type synonym for `Fix (RTree a)`, we won't be able to give it any instances other than `Fix`'s instances, so let's wrap it in a `newtype`: newtype NTree a = NTree { unNTree :: Fix (RTree a) } We can now use `cata` to define `NTree`'s `Functor` instance: instance Functor NTree where fmap f = NTree . cata go . unNTree where go (RLeaf x) = Fix (RLeaf (f x)) go (RNode l r) = Fix (RNode l r) The advantage of this approach is that you get to reuse all the existing definitions which use `Functor`, you don't have to rewrite everything in terms of `Bifunctor` and `vmap`.
I'm reasonably sure this is just reinventing freer-monads?
What are the advantages to other approaches?
What about my edit?
My understanding is \(and I might be wrong\) that this is about a DBus API \(originating at Ubuntu\), that allows \(desktop\) applications to display a status \(tray\) icon. So it is not for DBus services, it just uses DBus to enable \(desktop\) applications to communicate with the tray implementation \(such as gtk\-sni\-tray\). The traditional way to implement status icons is to use the old XEmbed API, but it is considered deprecated \(at least by the [Gnome folks](https://developer.gnome.org/gtk3/stable/GtkStatusIcon.html#GtkStatusIcon.description)\) and does not work in Gnome under wayland.
&gt; freer-effects I had no idea that this approach is the same as [freer-effects](https://hackage.haskell.org/package/freer-effects). I'll have to read that paper properly.
I'm not familiar with the freer-effects package. Would you be able to clarify how my approach is the same? For example, I don't think I'm using a `data Eff effs a` effects monad.
It was the 90s, and I think people were generally not expecting Haskell to be _that_ serious of a floating point language someday. I'm inclined to cut some slack. The whole Num hierarchy is high on the list of "things Haskell's successor language will do a lot differently but is too hard to fix in Haskell today". It's quite a list.
I think you just add additional type classes and make them instances. The Haskell num is already rather fine grained and the low level code works. I'd say just use it as a base. https://msdnshared.blob.core.windows.net/media/MSDNBlogsFS/prod.evol.blogs.msdn.com/CommunityServer.Components.PostAttachments/00/09/47/71/68/Haskell%20Class%20Hierarchy.png But in answer to your question, yes you can replace prelude. 
I wouldn't go so far as to say these are the advantages, but some distinguishing features are: * The commands just consist of plain data types, with continuations that return a polymorphic variant of the data type. Contrast this with free monads, where the the type argument into the command data type is the free monad. * Because it is simpler data type, maybe it will be possible to serialize the commands? * Since interpreters are just functions, you can vary the interpreters more easily than using newtypes and instances. * Since it doesn't use MTL-style interpreter, it doesn't suffer from the n^2 instances problem. * Since it doesn't use the free monad, it doesn't suffer from the free monad quadratic bind complexity.
So Google is throwing like 100K our way? That's awesome! 
IMHO, my approach is different enough not be a reinvention. I'm not saying it's better than `freer-effects` though! :) I guess my approach shows that the `freer-effects` approach can be broken down into a polymorphic variant + provide a Monadic way of composing effects with return values; amongst other things. My approach kept those ideas separate.
Is `data Eff effs a` still a type of Free monad (with a Pure value | or Freer bit)? My category-theory-fu is not strong enough to really know. I don't think I'm intentionally using a Free monad in my approach. Instead, I'm using `DList cmd`, and the continuation in the last arg of commands that "return" to chain commands together.
Oh. Arbitrary status icons. Nice
The title and structure of Oleg's paper ["Freer monads, more extensible effects"](http://okmij.org/ftp/Haskell/extensible/more.pdf) certainly points to this distinction being known. Oleg's site has a few more references http://okmij.org/ftp/Haskell/extensible/index.html and you'll probably find interesting a lot of other topics he writes about.
I am looking forward to the new- prefix disappearing from cabal commands. 
SIMD! Go go go!
&gt; Your AppCmd is an ad-hoc fixpoint construction. Who *are* you u/Syrak? (You don't have to answer :) How are you able to succinctly describe what I'm doing? You've given me heaps of reading to do!
\&gt; Tooling improvements FUCK YEAH
I've been hearing good things about new-cabal and I'd like to give it a try. Is there some "Cabal for Stack users" guide somewhere to get me up to speed?
I believe that amount is well below their accounting noise floor .. 
I've spent years on making Haskell resources for people and still wouldn't use Haskell for a game engine. Fortunately, that's not what I do for a living. What I do as a software developer is the same stuff that 99% of the industry does: networked services and the clients that talk to them. For that, Haskell and Haskell-alikes like PureScript are just fine.
Perhaps this approach will help: http://newartisans.com/2018/04/win-for-recursion-schemes/
It's just a waste to use it since `fmap` works in strictly more situations, so why bother with `map`?
This is Template Haskell, right? I haven't looked into it in any detail yet...I'm still trying to get a decent hold on the vanilla language :P Thank you for the suggestion though, when I get to TH I'll take a look!
Haskell is perfectly viable for commercial games w/ CPU/GPU intensive graphics - it's been done! See http://rumpus.land/ - VR no less. For the most part a dev's decision about why *not* to use Haskell usually comes down to either tooling or the possibility of hitting GC pauses. In practice the former is very legitimate and the latter is a bit like premature optimization. If you're off to build a AAA game Haskell is an obvious "no" due to a lack of tooling and a very small hiring pool. On the other hand if you're an independent studio or sole developer with Haskell experience you may find the experience surprisingly pleasant and maybe addictive, but be prepared to write a *lot* of Haskell. Please release and maintain your supporting libraries at that point :)
Is there anything you can share about what you're working on? Is any of it in the OSS space?
It's actually quite good. A lot of applications have both XEMBED and SNI support, and they will use the SNI support if its there, and otherwise fallback on XEMBED. There's also https://github.com/KDE/plasma-workspace/tree/master/xembed-sni-proxy which will display your XEMBED tray icons in as SNI icons. I haven't been able to get menus to work with this, but that might be an issue with my SNI implementation.
&gt; Bifunctor is in base now Ah! Awesome! Come to think of it, that's probably why I could import it in repl.it :P &gt; I wouldn't say that RTree is the `Fix`ed structure, I would say that `RTree` is the base `Functor` from which you construct the `Fix`ed structure `Tree`. Derp, yeah, that was terrible wording...thanks for clarifying! &gt; We can now use `cata` to define `NTree`'s `Functor` instance Ahh, this is cool too...when I started playing around with this my `newtype` wrappers got a bit out of control, which is what made me want to try flatten everything down into instances over the one type. Using a `newtype` for instances over the values makes a lot more sense than a completely breaking API change :P
You can may be try out [jenga](https://hackage.haskell.org/package/jenga) which if I am not mistaken is built on top of cabal-install. I have not had a reason to use it though. cabal new-* commands are awesome to say the least. There indeed are some rough edges each but I hope this GSoc will finish all those little pending things. Also, what is most important to me is that the only tool that supports backpack, a feature that I am itching to use in a big way, is cabal-install. 
Thanks you so much! I hope that this soon works
This issue has not been solved. I have successfully connected 2 bluetooth devices, but the client still could not find the service.
&gt; &gt; Would the Haddock improvement make it easier to produce documentation for entries in the reexported-modules section of Cabal files? Currently they seem to be ignored. Seems quite plausible to me, but I'm far from a haddock expert right now. In fact, so far it seemed like I would do hardly any work on haddock proper, and instead touch mostly ghc, haddock-library and ghci. But the project is currently a bit [in flux](https://github.com/haskell/haddock/issues/805), so maybe I'll even end up wrangling haddock itself.
The simpler error messages can be nice. At times I also like the fact that I can tell it's working on lists immediately.
When reading your post, I got the impression that the techniques described there are able to lift "simple" algebras to algebras that work on "decorated trees" of some kind. But here what seems to change is the output of the algebra, not the source datatype. Would the techniques still be applicable?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [prikhi/xmonad-config/.../**Theme.hs** (master → 26ebae3)](https://github.com/prikhi/xmonad-config/blob/26ebae3fa7b2bba8254e37c3e8c919aa011e4749/src/Theme.hs) * [prikhi/xmonad-config/.../**StatusBar.hs#L122** (master → 26ebae3)](https://github.com/prikhi/xmonad-config/blob/26ebae3fa7b2bba8254e37c3e8c919aa011e4749/src/StatusBar.hs#L122) * [prikhi/xmonad-config/.../**StatusBar.hs#L144-L151** (master → 26ebae3)](https://github.com/prikhi/xmonad-config/blob/26ebae3fa7b2bba8254e37c3e8c919aa011e4749/src/StatusBar.hs#L144-L151) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxwere5.)
Last time I tried learning property testing I had much less Haskell experience and it was hard to wrap my head around. I went ahead &amp; took a stab at it again, I think I may have gotten it this time, but all comments, feedback, tips, etc. are appreciated :) https://github.com/prikhi/crypto-portfolio/blob/master/test/QuantityQueueSpec.hs
The biggest issue if you're switcying from stack to cabal is that cabal doesn't sandbox GHC, instead just relying on your system GHC so you'll need some extra tool to manage your GHC versions.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [prikhi/crypto-portfolio/.../**QuantityQueueSpec.hs** (master → 2b4722d)](https://github.com/prikhi/crypto-portfolio/blob/2b4722daa969b766434840bc53c888ad835234c6/test/QuantityQueueSpec.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dxwfzdj.)
That's fair. If map was generalized I wouldn't be opposed to `map @[]` in places so you can see more at a glance if it's ever a little ambiguous. 
Does this give a start? ``` module ComposeAlg where import Control.Monad import Control.Monad.Trans.Except import Control.Monad.Trans.Reader import Data.Fix import Data.Functor.Compose algComp :: Functor f =&gt; (f a -&gt; a) -&gt; (f b -&gt; b) -&gt; f (a, b) -&gt; (a, b) algComp f g x = (f (fmap fst x), g (fmap snd x)) data ExprF a = Cons1 a a | Cons2 a type Typing m a = ReaderT () (ExceptT () m) a data Type = Type typeOf :: ExprF (Typing m Type) -&gt; Typing m Type typeOf = undefined identityAlg :: ExprF (ExprF a) -&gt; ExprF a identityAlg = undefined result :: Fix ExprF -&gt; Typing m (Fix (Compose ((,) Type) ExprF)) result = cataM $ algComp typeOf identityAlg ```
Note that `result` need a bit more juggling before it will work, but I think your definition of `Typing` should allow it.
Thanks for the example!
&gt; Does taffybar support even distribution of space for window titles? I'm not 100% sure if I understand what you are asking here, but alignment and spacing are very configurable in taffybar. If you are asking about Workspace Icon tiling, then the answer is definitely yes. The best approach is to use the [minIcons](https://github.com/travitch/taffybar/blob/bbdae99dbcf382cd44fc9c0a2ed90af66c083875/src/System/Taffybar/WorkspaceHUD.hs#L222) setting in workspaceHUD (if you are using icons), or set the [minWidgetSize](https://github.com/travitch/taffybar/blob/bbdae99dbcf382cd44fc9c0a2ed90af66c083875/src/System/Taffybar/WorkspaceHUD.hs#L219). You can also provide a completely custom widget builder with [this](https://github.com/travitch/taffybar/blob/bbdae99dbcf382cd44fc9c0a2ed90af66c083875/src/System/Taffybar/WorkspaceHUD.hs#L215) which will definitely get you what you want with enough tweaking. &gt; Does taffybar adjust the [tray] spacing when icons are added &amp; removed? Yes, although you could obviously configure this by setting a min size for the tray gtk-sni-tray also has [settings to control how icons are arranged in the tray](https://github.com/IvanMalison/gtk-sni-tray/blob/5dd9fc18f25c20305ca4b14ad9816bfc810c7490/src/StatusNotifier/Tray.hs#L98). &gt; I actually want to couple my WM w/ taffybar. Mostly because I want: Fair enough, although I really don't generally think that is wise. You can get the things you want easily enough without needing to have your WM actually start taffybar (you can simply have a share haskell lib that both xmonad and taffybar import, or you can import one config from the other which is what I do) &gt; to be able to clone &amp; build my xmonad config &amp; have a statusbar, systray, &amp; notifications without any extra work If you set up stack builds for taffybar and xmonad there really isn't much work to doing either. In fact, you could make a single stack build that produces the executables for BOTH taffybar and xmonad. &gt; Can I give it different configs for different monitors? I find it useful to have my portrait monitor be just the workspace list, window list, &amp; time. Yes, but you'll have to use the advanced config system rather than the simple one. Seems like you are a competent haskeller, so it shouldn't be an issue for you. Basically, you'll want to set [this parameter](https://github.com/travitch/taffybar/blob/bbdae99dbcf382cd44fc9c0a2ed90af66c083875/src/System/Taffybar/Context.hs#L84) to an appropriate value. &gt; Does it support system trays on multiple(but not all) screens? Yes (for the SNI tray), see above (just pass the tray icon in for the screens you want it to appear on). XEMBED trays can only be displayed on one monitor though.
I see. The book specifies "(run with ghci +RTS -M20m to not throttle your computer)", so I just assumed that the code would have filled the whole memory without the -M20m.
&gt; and the latter is a bit like premature optimization I don't think this is a premature optimization at all! A premature optimization is something like writing a bitshift instead of a multiplication, making the code more complicated without first measuring to see if the code being tweaked is a bottleneck or even measuring if the tweak actually improves anything. In this case, however, GC pauses are an intrinsic property of the language, not something which can be tweaked if it ends up causing problems. Consider the following scenario: you write a small game in Haskell and conclude that Haskell is fine for writing games. You then embark on a bigger project using the same tech, and 80% of the way through, you have a performance issue and your measurements indicate that the problem is the GC pauses. You research the topic, and learn that the duration of GC pauses is proportional to the amount of live memory, not the amount of generated garbage. So you have to reduce the amount of live memory you're using, that is, you have to make a smaller game! That's not really a "tweak" at all, and it would really be quite unfortunate to learn about this so late during development. Fortunately, ghc now has [compact regions](https://www.reddit.com/r/haskell/comments/86y9dq/streaming_audio_in_realtime_hard_problem/dw9jmwc/) which can be used to fix this kind of problem, but ghc didn't always have those, and so in the past Haskell really was ill-suited for bigger games. Might still be, who knows, I only write small games.
Hmm, `allow-newer: true` in your stack.yaml or `--allow-newer` should ignore all bounds. I know that's confusing, it seems like it would only ignore upper, but it ignores both. You could use a tarball reference for tagged, perhaps pulled off github, assuming it's up there. Ugly, but at least it's not mutable.
My reasons for exploring native GUIs require the absence of GC.
Yes it is Template Haskell. It's a way to generate code that you could also have written by hand. Given the original `Tree` type, it derives the functor you call `RTree` (that's necessary to write algebras in the first place) and an isomorphism between `Tree a` and `RTree a (Tree a)`, in the `Recursive` and `Corecursive` type classes. Essentially you get an implementation of `cata` for `Tree` directly, that is equivalent to this: data Tree a = Leaf a | Node (Tree a) (Tree a) data RTree a b = RLeaf a | Node b b instance Recursive (Tree a) where cata :: (RTree a b -&gt; b) -&gt; Tree a -&gt; b cata f (Leaf a) = f (RLeaf a) cata f (Node l r) = f (RNode (cata f l) (cata f r))
The approach I have settled on for `recursion-schemes` is to only define the base functor, then make a `newtype` for the "real" type and add pattern synonyms for the "real" constructors like [this](https://github.com/ohua-dev/ohua-core/blob/1a5b3892e0e78c249f35b5db2f2768b1ff2edee4/src/Ohua/Types.hs#L462). This means not only that `embed` and `project` are extremely cheap (they are NoOps as they only unwrap the `newtype` which is free) but also that the "real" type is its very own type and thus I can implement `Functor`, `Foldable`, `Traversable`, all the good stuff on it while also having the derived `Functor`, `Foldable` and `Traversable` on the base functor. PS: The coolest thing in my opinion is then that for the "real" type I can use `cata` and friends to implement `Foldable` and stuff (tough I didn't do that in the code that I linked)
Hi, If a package has been dropped from stackage it's almost always because it has some outdated dependencies and it's best to send a PR or contact the maintainer of the package. But that's me! I think the only thing missing to get fay into LTS 11 is to add support for haskell-src-exts 1.20.*. I haven't had much time so I'd appreciate any help with this! It compiles with this in a stack.yaml so you may be able to use that in the meantime: extra-deps: - spoon-0.3.1 - haskell-src-exts-1.19.1 - fay-0.24.0.0 
Equivalence between lambda-calculus and SKI. Here is a translation I have in some old notes (2010). I don't remember why it works, but maybe you'd like to try to figure it out? Translating SKI to lambda-calculus is easy: S f g x = f x (g x) K x y = x I x = x -- and actually I = (S K _) works Recall that lambda-calc has grammar var ::= x,y,z, ... &lt;infinite set of symbols&gt; exp ::= \ var . exp | (exp) | exp exp | var We can define a translation `[exp]` from lambda-calculus to SKI by [e1 e2] =&gt; [e1] [e2] [\ x. e] =&gt; [e]_x [(e)] =&gt; ([e]) [x] =&gt; x where the lambda translation `[exp]_var` is [e]_x =&gt; case e of e1 e2 -&gt; s [e1]_x [e2]_x \ y . e' -&gt; [ [e']_x ]_y (e) -&gt; ([e]_x) y -&gt; if y == x then I else (K y) To see that recursion halts, note that `[exp]` and `[exp]_var` are decreasing in `(lambda, length)`. 
I guess that's fair. I just think that's a very specific subset of app development. The majority of native GUI apps I use are in GC'd languages or would have worked fine in a GC'd language. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jaspervdj/talks/.../**slides.md#smell-types-modules-3** (master → dbcaed1)](https://github.com/jaspervdj/talks/blob/dbcaed1cac46477840512c46fbbf1b1c93299815/2017-haskell-exchange-getting-things-done/slides.md#smell-types-modules-3) ---- 
Oh stream? Tell me more. 
&gt; Mo (ext . fmap join) (fmap join wr) === Mo (ext . fmap join . fmap join) wr &gt; Mo (ext . fmap join) (fmap pure wr) === Mo (ext . fmap join . fmap pure) wr These look like free theorems, especially if you generalize them to get something like &gt; Mo g (fmap f wr) === Mo (g . fmap f) wr I'd be willing to bet if you write down the free theorem for the type forall x. (forall r. (w (m r) -&gt; a) -&gt; w r -&gt; x) -&gt; x that would lead somewhere interesting.
The free theorem looks like it should imply that phi (Mo x (fmap f y)) = phi (Mo (x . fmap (fmap f)) y) for any phi :: Mo m w a -&gt; b f :: r -&gt; s x :: w (m s) -&gt; a y :: w r but I haven't checked it. That basically tells you that Mo x (fmap f y) and Mo (x . fmap (fmap f)) y are substitutable in all contexts, so equal for all intents and purposes.
No. I don't have to bankrupt my small business "running into" an issue I can predict from the outset to be a more-than-slim risk. And it would bankrupt an indie to spend 80% of their game budget determining that Haskell cannot meet their requirements. Compact regions are not a panacea - large blocks of immutable memory without sharing would not be an easy medium to develop on, and likely it would be absurd to try to retrofit such an architecture into a large game. It would be interesting to see what kind of engine could be built around that architecture to start with - and perhaps if someone takes the time to prove that out then someone whose livelihood is at stake could consider using it.
I've thought about this some more, and I will say that the above points are advantages compared to other approaches. I believe this approach of using standard ContT and State to compose the commands is simpler and more flexible than `freer-effect`way of encoding the "container" inside the `data Eff effs a` monad (This is the "Add a new constructor for return" in Syrak's comment above) At the end, you get plain `DList` of recursive commands, which can be walked in a straight forward way. For example, with my approach, you could even use different containers (instead of DList) for different use cases. Eg. you could use a Map to pre-batch commands.
That seems only possible by messing with the internals of pipes (via pattern matching on the `Proxy` values. It might be better to zip the sources before or while converting them into pipes.
Does this approach avoid some of the potential GC issues by essentially delegating management of the memory-heavy resources to Unity?
&gt; No. I know that saying "No." as an opening to expressing your disagreement is popular on the Linux kernel mailing lists and other macho newsgroups but here it seems a bit out of place.
I don't know very much about macho newsgroups, I just thought that was an appropriate answer to calling all reference to pause times "cargo culting". Sorry if you disagree. Maybe neither belong here?
I think I see what's going on here. If we fix `w ~ Identity` for simplicity, we have ``` exists r. (r, m r -&gt; a) ~ (forall r. r -&gt; m r) -&gt; a ``` So do we have a `Traced` comonad? The `Monoid` would be `forall r. r -&gt; m r`, presumably under Kleisli composition. I'm not sure if the transformer version degenerates in a similar way though.
Worth showing that `I` is derivable from `S` and `K` I = S K K
\&gt; Significantly improved Windows support with a new I/O manager, long file path compatibility and dynamic linking support \(Tamar Christina\). It only took 6 years to merge since [joeyadams'](https://github.com/joeyadams/haskell-iocp) first attempt...
Could you define the recursion yourself to more accurately express what you want to convey, something like: foo = [] &lt;$ eof &lt;|&gt; (:) &lt;$&gt; (blockA &lt;|&gt; blockB &lt;|&gt; blockC) &lt;*&gt; foo
`many1` won't help I don't think, because once you successfully parse the first block you are back to `many` basically.
I think case in point was especially surprising for me, given that this was a pixel art adventure game, not some mega intense RTS or 3d shooter. I never really felt like it was adequately explained why GC pauses were just absolutely an unavoidable evil... When developing a game that could've run on a console in 199X. The phrasing on a lot of the responses out of Chucklefish left it kind of ambiguous as to whether or not this was an actual, real life problem they ran into early on, or whether this was an anticipated result given what they already knew, or just a fear that they could run into this problem in theory and 'have no escape.'
Could you link the book? That's interesting. It's important to note that `[1 .. 10^6]` evalutes to `enumFromTo 1 (10 ^ 6)`, which is defined lazily. So `enumFromTo` will lazily produce the list, and `foldl'` consumes it strictly, so this ends up never allocating anything when rewrite and fusion happens. However, when you do `let xs = [1..10^6] :: [Int]`, you define a reference to `xs`. If you do `:sprint xs`, you'll get `_` back because it's not evaluated at all. When you start doing `foldl' (+) 0 xs`, you start evaluating the list. Since you have a reference to the list, you can't garbage collect, and it starts taking up space in memory. Let's analyze the space usage. The value `[1 .. 10^6]` is a list of 1,000,000 cons cells and ints. An `Int` is a heap allocated number; so it consists of a word pointer (64 bit = 8 byte) and then the payload (64 bit = 8 byte). So we get 16MB just from the numbers, not 4. Then we must pay for the list constructors: 1 word for the empty list, and [3 words per element](https://wiki.haskell.org/GHC/Memory_Footprint), for a total of 3MB. Still, 19MB ought not to exhaust any but the most meager of heaps. 
&gt; taking around 4 MB Nit pick - it's more like 28MB - you didn't account for the memory taken up by the list itself. value header (one word) v data [] a = a : [a] | [] ^ ^ pointer (one word each) A 32-bit `Int` takes four bytes, but when you put them in a list each is stored in a cons cell which takes another three words each (24 bytes total on a 64-bit machine). 24 + 4 = 28 bytes(!) for each list element, times a million is 28MB. In an unboxed `Array`, of course, it would be much closer to 4MB.
I'm not sure gloss is really a legit example. Nobody is building games on top of gloss, and performance wasn't really a core design goal - It's pretty far removed from the underlying APIs and delegates a lot of rendering logic to the GHC runtime that, say, the SDL2 bindings don't. Not saying this invalidates your larger point or that GC pauses aren't a thing with lower level libraries, but it does seem like a poor case example to trot out as an example of GC pauses being a real and intractable issue affecting even minor projects.
http://i0.kym-cdn.com/photos/images/newsfeed/001/185/750/a53.jpg
&gt; Linux kernel mailing lists and other macho newsgroups but here it seems a bit out of place. The only thing out of place is you ignoring his entire post and just quoting "No" to complain about it.
It’s rather easy to construction a simple test case that incurs a space or time leak, or incurs a long GC pause. Those are easily remedied with with some profiling. I’m definitely not saying all refs to GC pauses are cargo culted. *Most* are. It’s keeping good developers from trying and it’s keeping good libraries from being written. Again, games have been written in Haskell without long GC pauses. Rumpus, Nikki and the Robots. 
[abstract](http://www.lambdadays.org/lambdadays2018/jarek-ratajski): &gt; ### Jarek Ratajski &gt; #### Software Anarchitect &gt; #### JVM Developer since 1999 &gt; ### Beauty and the beast Haskell on JVM &gt; After 20 years of evolution Java Virtual Machine became is a real masterpiece of engineering. There are however lot of issues arising when someone tries to use JVM for functional laungagues. &gt; &gt; Developers of Eta tried it once again in a little bit different way. Did the succeed? We will analyze how it performs on a sample system, where the business logic is written in Haskell and middleware components and libraries in Java. [slides](http://www.lambdadays.org/static/upload/media/1520323372815660jarekratajskibeautyandthebeasthaskellonjvm.pdf)
Woah!
`cabal new-build -w ghc-7.10.3` (or whichever version you want) should work just fine. No need for extra tools.
Thank you! We will definitely consider doing that to improve the situation. Your architecture seems very reasonable and we are doing something very similar just with multiple repos at the moment. I have a few remaining questions: Even if they are in a single repo, if they are deployed separately it is still possible to deploy different versions of the git repo to different targets accidentally. Do you do any runtime version checks or similar to mitigate this risk? If I build different parts of the megarepo with different build tools (e.g. Reflex platform or nix for ghcjs, but perhaps something else for non ghcjs), then I could have different underlying versions of libraries, so I am still curious about the compatibility of different versions of servant / persistent on the client vs the server. Is this encapsulated by PVP or not?
I got the impression it was anticipated. There's always a way out by using the C FFI. 
Just to understand your proposal, let’s say you have the following commits. ``` commit1 "Release 1.1.0.0" commit2 "Typo in comment" commit3 "Breaking change" commit4 "Typo in comment" commit5 "Another breaking change" commit6 "Updating Changelog for release" ``` according to your suggestion, the commits would have the following version numbers. ``` commit1 1.1.0.0 "Release 1.1.0.0" commit2 1.1.0.0-1 "Typo in comment" commit3 1.1+ "Breaking change" commit4 1.1+ "Typo in comment" commit5 1.1+ "Another breaking change" commit6 1.2 "Updating Changelog for release" ``` But why not simply always keep the version number that the package would have if you would upload it now: ``` commit1 1.1.0.0 "Release 1.1.0.0" commit2 1.1.0.1 "Typo in comment" commit3 1.2 "Breaking change" commit4 1.2 "Typo in comment" commit5 1.2 "Another breaking change" commit6 1.2 "Updating Changelog for release" ```
Your first example got the numbering correct. That is what I was envisioning. That approach would have the same problems that we have now. A library depending on 1.2 that was given commit3 might break in nasty ways.
How about I reserve even numbers for GitHub versions (and ask people not to assume GitHub versions to be stable). commit1 1.1.0.0 "Release 1.1.0.0" commit2 1.1.0.1 "Typo in comment" commit3 1.2 "Breaking change" commit4 1.2 "Typo in comment" commit5 1.2 "Another breaking change" commit6 1.3 "Updating Changelog for release" Would this achieve some of what you want?
Just use Cabal! (e.g: https://github.com/futurice/haskell-mega-repo) /r/phadej
I don't think so. `study` exists, which is not possible with a `Traced` comonad (forall r. w r -&gt; w (m r)) -&gt; a admittedly, though, it it probably possible to have a `Monoid` for `(forall r. w r -&gt; w (m r))` that would make this `Traced` comonad act like `Mo`.
I am not Chris, but I think he was referring to the tone of your message, rather than its content. He may have referred to the "macho" stuff to point out that the tone of your critique may be perceived as harsh and thus detract a sensitive person from participating in the discussion.
That would work somewhat, although yes as you said it does take up some real version numbers, and also it is slightly misleading to have two different 1.2 versions be incompatible yet indistinguishable by cabal, even with the aforementioned warnings.
*Feel* is a delicate thing. Even a regular hitch once a minute makes the whole experience feel bad. We tend to strive to have 0 hitches ever. That doesnt always happen, but there is a lot of engineering done to try and reach that. GC unfortunately introduces those for you. As a gamedev, Id easily take a 5% speed reduction if I could get a wait-free GC. Even 10%. Its not just GC of course. Every game Ive worked on has had hitching issues to fix. Loading hitches, AI hitches, VFX hitches, etc. Everything comes down to making sure your algorithms tick at 16.6666666ms. Usually for gameplay and AI I got like 3 or 5 ms / tick with the rest being taken up by VFX and character skinning. Nowadays Im in server land where I have more memory than I know what to do with. I actually use immutable objects and save off snapshots of every action. 
https://github.com/jonascarpay/apecs was introduced to the Haskell community recently, and is a pretty neat implementation. Beats my naive attempt at an ECS from a couple years ago.
What is the best way to communicate that version in communication with a persistent server? And is there an elegant way to do it when communicating with an api (I know some ways, but they aren't as pretty as I'd like)? Also at what point should the version be rejected, since perhaps minor deviations are ok.
Would that work with cabal?
I'm trying to write a rank 2 (I think) type that returns the same existentially quantified type that gets passed in. Specifically: data NodeSequence p o a = NodeSequence { runNodes :: forall g. RandomGen g =&gt; g -&gt; p -&gt; (a, g, Status, p, [o]) } Here I want to say this is a function that takes a (RandomGen g) and returns the same g. 
I'll definitely be making it OS but I still have a long ways to go before a stable release.
Ah, I'm not quite sure what you mean. My core libraries won't be managing any state and its interface will not share memory so I don't think this is an issue for me.
`False`.
The moment we begin to criticize tone in such an emotionally distanced form of communication as online forum, well, get your shit together because it isn't going to change
Thanks! I dug in a bit more and updated my rather old stack install, now `allow-newer` works as you described. Maybe there was a bug that has been fixed / change of behavior. Also digging through the various bug reports this has caused I've seen that the ability to explicitly reference revisions has been added to stack at some point. `- semigroupoids-5.0.1@rev:0` also works, a bit nicer than the git reference. Maybe the better option as it does not change any packages like `allow-newer` would. But hey, at least the git people understood allowing people to edit history might not be the best of ideas ;-) The strange thing is after adding `allow-newer` I can remove it again and it still builds. Guess the stack package database mixes up revisions and doesn't rebuild something it should?
Thank you, your explanation of how monads relate to "&lt;-" and the return objects finally clicked!
&gt; Guess the stack package database mixes up revisions and doesn't rebuild something it should? Yes, because the ghc pkg database does not know which revision is installed. Revisions are gnarly...
Are you compelling a sensitive person to distance themyselves from their emotional life?
Indeed. In fact, my version doesn't compile, since it can't infer an applicative for the second `pure` because it doesn't have to be `K`. So `pure &lt;*&gt; undefined` is sort of better :P
For the purposes of this translation, you can think of S as “apply a function to an argument in an environment”, K as “ignore environment”, and I as “access environment”. When translating a lambda, the “environment” in question is just its parameter. So with an application [e1 e2]^(x), you translate it as S; with a lambda [λy.e′]^(x), you translate the body in the outer environment x, then translate the result in the inner environment y to get y “out of the way” so x is accessible; group expressions [(e)]^(x) are self-explanatory; and with a variable [y]^(x), if y=x then you access the environment with I, otherwise you ignore it with Ky.
Isn't boolean blindness when you throw away evidence and context by compressing &gt;2 meanings into two constructors, such as `True` or `False`?
I actually think `Maybe (NonEmpty a)` would have been a perfectly good representation for lists. But you're right, lists are so ubiquitous and fundamental that they need to be an exception, for a lot of reasons. For one thing, as you say, it's worth it to have `[]` as an explicitly typed `Nothing` for lists. And even the slight verbosity in the type name would have been extremely painful - although the magical type level `[a]` notation could just as well have been an alias for `Maybe (NonEmpty a)`. Also, there is a lot of list-specific optimization code in the compiler; I'm not sure how that all would have worked out.
So this is similar to Haxl but using -XArrows instead of -XApplicativeDo?
Not for *every* function - but for a lot of them. If I could be convinced that this is production-ready, I would use it quite ubiquitously in our dozens-of-thousands LOC code base. It would be worth some widespread refactoring because it would clean up a lot of things.
Happy cake day!
Compelling how? If you seek emotional satisfaction through an internet forum you are very lost
&gt; Maybe that makes it viable for vector to add simd to their generalized stream fusion. This paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/haskell-beats-C.pdf actually was the starting point of integrating SIMD ops to GHC and they actually benchmarked with the vector library. &gt; Kinda wish fusion was factored out from text/bytestring/vector/bunch of other libraries into a common implementation so that improvements could be shared. There probably are huge issues with this that aren't obvious from outside but I can dream. This sounds like a nice idea because at the heart of it all these libraries utilize a very similar stream type and do fusion using that. 
I have had it in my to-read list, I completely forgot. Thank you for the link!
Hey, cheers :D
You can leverage arrows to get a lot more static analysis, especially if you're willing to heavily bend the laws. I'm not sure if they're taking advantage of this though. When I've tried in the past, I found that you can't get around the Arrow notation desugarer's pervasive use of tuple swapping, which acts like an opaque blocker to most useful static analysis. So I'm *guessing* the automatic parallelism here isn't nearly as good as one would hope; or as good as you can get by just not using the arrow notation.
On the contrary, this is the first time I've ever seen the confused "reaction" being used legitimately and not just as a passive aggressive replacement for thumbs down.
Call me oblivious to tooling improvements, but I am most excited by the cmov and lookup table support in Cmm.
It works for lts-10.10 but with lts-11 there is a problem (from memory) with rtraverse .
Thanks! Yikes. Someone should have come up with another pair of type classes which do the conversion bijectively.
Intentionally ignorant.. Okay bro 
Yeah, I'm not trying to put a value judgement on the decision. My complaint is that it's not clear exactly why it was made - it'd be pretty thick to accuse anyone of poor judgement without hearing why they made a choice. I just meant that the circumstances didn't seem to be such that should force untenable GC pause lengths, so it wasn't obvious at what point the call to drop Haskell was made, or what the struggle was like before they abandoned it, what solutions they tried or didn't, etc.
Probably not worth spending much more time on supporting revisions now that snapshots have them fixed. Maybe for external dependencies not in Stackage? Yeah, revisions look like a poor idea &amp; implementation to me. Personally, the fact that such a feature was required and is even, IMHO, abused like in the case of `semigroupoids` here just tells me that fundamentally this business of constantly guessing and adjusting version ranges is just broken. I could never remember to bump the min when using new features and my upper bound guesses were frequently wrong. It's just not workable and caused me &amp; users of my stuff endless grief. Hopefully this will be last time that this nonsense intrudes on my shiny new world of snapshots.
I've been watching this repo for awhile and still can't see which use case it was built for. Is it a build system? Will it hold up in distributed data processing? It's looks like it could do both but I'm unclear where it would excel and where it would struggle. 
I suppose it makes a lot more sense if you see it as a convenient way to generate and consume JSON as opposed to a convenient way to serialise and deserialise Haskell data structures.
&gt; Nobody is building games on top of gloss [I do...](https://github.com/gelisam/ludum-dare-31) &gt; It's pretty far removed from the underlying APIs and delegates a lot of rendering logic to the GHC runtime that, say, the SDL2 bindings don't. I was curious about this, so I wrote the same toy application (just a square moving along a circular path) in SDL2 and in gloss, and then ran it with [`-B`](https://www.reddit.com/r/haskell/comments/8e0x5t/has_anyone_here_used_the_b_rts_option_in_the_wild/) in order to compare the two frameworks. Gloss is using the trick I linked above, to call `performGC` on every frame to avoid long pauses, so I patched my copy of gloss to remove those calls. To my surprise, the SDL2 implementation performed garbage collection more often than gloss, indicating that it is SDL2 which is allocating more memory! I didn't see any visual hiccup when the GC happened though. I think it's because the programs keep so little memory from frame to frame that the GC completes extremely quickly. For fun, I tried artificially allocating some memory to see the cutoff at which I would start noticing hiccups... and to my surprise, I never did! I think what happened is that even though I had a lot of live memory, very little of it changed every frame, so the GCs I was hearing were all minor GCs, never major GCs. So one good news seems to be: under some circumstances (when minor GCs don't promote any newly-allocated memory to the older generation?), it seems to be possible to avoid major GCs altogether, thereby side-stepping the problem of long GC pauses. I think those circumstances might be difficult to achieve in a game though, because the state of the world does change from frame to frame. I then added a button to force a GC, and with both gloss and SDL2, I couldn't see any hiccup when the amount of memory was about 1 million data constructors, and it was quite noticeable when the amount of memory was 10 million data constructors. This gives an estimate for how big your game needs to get before GC pauses even starts to become an issue. I then added the performGC-on-every-frame trick, and with both gloss and SDL2, I could hit 60fps with 1 million data constructors but only ~30fps with 2 million. Of course, my toy example is barely doing any work per frame, so the real limit is probably much lower than 1 million data constructors. I then stored my 1 million data constructors in a compact region and I could hit 400fps again, yay!
Started watching the video. I don't have a good answer to your question. You might have a better feel for it. If DDL wasn't your target language, what would a language you dreamed up look like, seems to be the question to ponder. 
I [just used it](https://www.reddit.com/r/haskell/comments/8ekkk3/rust_instead_of_haskell_in_gamedev/dxxw2jz/), thanks for the tip!
Thank you, im\_not\_afraid, for voting on FatFingerHelperBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
already one person has contacted me saying that they found a real purpose.
Well, it says something about [reproducible research](https://en.wikipedia.org/wiki/Reproducibility#Reproducible_research).
You mean in the tags? If I saw that before it's clarity was dulled when I read about cached outputs and tried to understand how caching would work on very large datasets and workflows. 
Haskell doesn't have any built\-in way to do this, no. It doesn't "know" that the math operators are special at all, and so doesn't have any knowledge of how to invert them, and thus to do equational manipulation. Idris also doesn't provide this out\-of\-the\-box. This sort of reasoning really requires a computer algebra system, with a set of heuristic rules, and also richer representation of equations \-\- not least because not all functions are continuously invertible! If you have an equation in only two variables and you construct it using only constructions with inverses \(bijections\) then a package like [https://hackage.haskell.org/package/invertible](https://hackage.haskell.org/package/invertible) could build up "both directions" simultaneously. It doesn't seem obvious how to generalize this for an arbitrary number of variables, however.
Thanks. Do you happen to know what you'd call this feature? Invertible?
I think the search term is "Symbolic computation" (https://reference.wolfram.com/language/tutorial/SymbolicComputation.html). Haskell doesn't support this.
Thank you.
Spark for Haskell?
Not in Haskell, no, but try Prolog and Mercury. In those languages, equations are constraints, you can ask which values would satisfy the equations, and so you can both ask which output would be produced by given inputs and which inputs would lead to a given output. Another keyword to search for is "symbolic manipulation", which is the kind of thing Maple and Wolfram Alpha do in order to manipulate mathematical equations. I'm sure you can find libraries in Haskell which provide similar functionalities, but they would do so by manipulating equations as values like you would do in any language, not by manipulating the equations which are used to define Haskell functions.
No knowing as much as I should about either of them, this feels a bit like [pachyderm](http://pachyderm.io/)
Thanks. I noticed the focus on category theory in Haskell and was hoping that through category theory, it might be possible.
[`subhask`](https://github.com/mikeizbicki/subhask) is one such attempt, and there's [`noether`](https://github.com/mrkgnao/noether).
[removed]
Well, you have to define what a schema is and at its simplest, it's like a depth-2 (or 3 with multiple schemas) forest whose nodes are aliases and whose leaves are types, or a list of aliased lists of aliased types. The nesting structure is very interesting because you can make it a sort of 2-level language where the same constructs work at the schema and table level. At each level, you need to be able to create, drop, alter or rename constituents and at each level you should have a category for composing such expressions (including a no-op identity which DDL lacks). That's to me what just falls out. In a way, solving this requires the same sort of thinking that nailing a good record system requires. 
Well in the two variable case you're calculating the inverse of a function. \([https://en.wikipedia.org/wiki/Inverse\_function](https://en.wikipedia.org/wiki/Inverse_function)\). In the multivariate case you're just "solving for a variable" as far as I know \-\- i.e. there's no particular fancier term I'm aware of.
**Inverse function** In mathematics, an inverse function (or anti-function) is a function that "reverses" another function: if the function f applied to an input x gives a result of y, then applying its inverse function g to y gives the result x, and vice versa. I.e., f(x) = y if and only if g(y) = x. As a simple example, consider the real-valued function of a real variable given by f(x) = 5x − 7. Thinking of this as a step-by-step procedure (namely, take a number x, multiply it by 5, then subtract 7 from the result), to reverse this and get x back from some output value, say y, we should undo each step in reverse order. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thanks for the more in depth analysis. There aren't any huge surprises here, but it's pretty valuable to get a better ballpark for what Haskell can and can't handle before noticeable pausing, and this data can definitely inform architecture decisions for avoiding the issue. This definitely beats the heck out of 'Haskell is bad for all videogames forever because GC is the devil.'
Rather than puling packages from github with nix, it seems to me that it would be appropriate to integrate pinning to source repos as part of the cabal file description itself. See this ghc proposal: [https://github.com/ghc\-proposals/ghc\-proposals/pull/115](https://github.com/ghc-proposals/ghc-proposals/pull/115)
Mafia is sort of a nice middle ground that supports submodule based git references 
We had our very first mentor student chat a few days ago. There’s some simple stuff that should happen quickly and then some much more ambitious stuff after the first 2-3 weeks. We’re gonna see about adding a notion of const expr Singleton types and I think we will also need to figure out support for unboxed floating point single and double precision literals. There’s a lot more to be done than just that. But some good work will be done. The student Abhiroop is also interested in auto vectorization optimization, though my perspective is that this summers work is about making a nice foundation for that and hand written codes per platform, so that adventure is out of scope for the summer :)
Good news / bad news, I’m the mentor :)
I'm not saying `Mo` is `TracedT`, but that fixing `w ~ Identity`, we get something isomorphic to `Traced`. Following the same argument without that restriction fails, since we don't have exists r. (w r, w (m r) -&gt; a) ~ (forall r. w r -&gt; w (m r)) -&gt; a in general (`w ~ Const Void` is a trivial counterexample, I think)
Definitely far, far less that the overall cost to the company for a new SWE.
Ah, I missed that comment :) Thanks!
This seems very nice! In the article you say that you are able to identify if an output has already been created by knowing the hash of the input and the process that is going to be run. In Nix land, this is done through the derivation language, where hashing happens. How do you describe processes in `funflow`, so that you can calculate output hashes? Is this method Haskell specific or it has a neutral representation like .drv?
I agree. Use of JSON for faithful data type serialize/deserialize is only one use, and by far not the most common one. At least for us, in the web app development space. For us, JSON is almost always a one-directional REST API access format. There the use of `Maybe` for fields that can be `null`is convenient and natural. In the unusual case where I'm using JSON as a faithful serialization, it would never cross my mind to rely on the default representation of `Maybe`. I understand that does complicate automatic deriving though. Perhaps the solution is to have two `FromJSON/ToJSON` pairs of classes, one for simple one-directional marshaling as it is now, and a new one for faithful data type serialization.
For a long time my opinion has been that `Arrow` has little use nowadays, given that `Applicative` is essentially equivalent and so widely used. People tend to find `Applicative` far easier to grok and learn how to use. But here, the metaphor is explicitly an arrow-like thing. So I'm not sure. It would be interesting to think about the design of an analogous library with an `Applicative` interface.
I understand this point, what I am curious about is how they deal with the nix-style hashes (made a small edit to make it more clear). In nix, you have the `drv` that *describes* the building process, I would like to know how this is done in this more general case.
What are the prerequisites of reading Purely Functional Data Structures by Okasaki and Pearls of Functional Programming by Bird? Can I read these books immediately after finishing the Haskell from First Principles? Should I read one before the other?
 with-compiler: ghc-7.10.3
&gt; given that `Applicative` is essentially equivalent What do you mean by that? `Applicative` is strictly weaker than `Arrow` after all.
I was hoping that I was missing something obvious. Thanks anyway. 
TIL. 👍🏾
Yes, you can pass in the compiler, but you will need an extra tool to get and manage that compiler unless you want to manually build it yourself.
Is it possible to use something else than local disk for storage? For example docker images, a key/value store, or S3? In the case where workflows are distributed using redis, how do you share storage in the cluster?
The rest of Aeson is careful to manually implement the sum-types as tagged types. It is just the `Maybe` instance that is the culprit here. There is an encoding, we use it for everything else, its just not the obvious one that users would expect for `Maybe`.
You should have specific environments each with its own database instance, so for example a branch of code shouldn't be able to upgrade a prod database. For the main deployment branch, you should have a linear history and a single environment, and a single database. You need to keep your packages as close to each other as possible between ghcjs and ghc.
Here's an (admittedly rambling) set of thoughts on the topic. (I'll focus entirely on the ToJSON side for now.) Using type family magic just to rule out `a` in `Maybe a` being a `Maybe` in its own right is problematic. `Maybe` isn't the only JSON type that can serialize out to `Null`. Requiring something like a more restricted like instance ToNonNullJSON a =&gt; ToJSON (Maybe a) could work, but would require a ton of instances, and still restricts things so you can't serialize `Maybe (Maybe a)`. A slightly more clever class ToJSON a where type EmitsNull a :: Bool type EmitsNull = False instance (ToJSON a, EmitsNull a ~ False) =&gt; ToJSON (Maybe a) where type EmitsNull (Maybe a) = True would require a lot less change, as you'd only have to markup the handful of `ToJSON` instances out there that do emit `Null`, while the rest of your code would go through unmolested, and it would handle the cases where your `Maybe` contains some other Null-producing type. All of these focus on mitigating it by making it unrepresentable. I'm far more partial to finding a solution that lets us properly encode `Maybe (Maybe a)`, though, as it would mean that JSON serialization would have fewer rough edges, and the types would remain simple, even if the code gets more complicated. We could also try to push the issue around a bit, e.g. catch this on the back-swing by making the `Maybe` instance a bit smarter, e.g. by having `ToJson a =&gt; ToJSON (Maybe a)` check if output of the 'a' is `Null`, then we could try to encode another way. The problem is there isn't 'another way' left open in the grammar of json for us to exploit to mark the `Just Nothing` unless we change the encoding of `Just (Just a)`. So, we need more information in there to avoid emitting 'null', when we're recursing into serializing `a` in `Maybe a`. There exists a "heavy" encoding of Maybe, like we encode Either, wherein we use a tagged type. We don't want to use it in general, but we could use it _inside_ of Maybe. class ToJSON a where toJSON ... toEncoding ... toNonNullJSON ... toNonNullEncoding ... instance ToJSON a =&gt; ToJSON (Maybe a) where toJSON (Just a) = toNonNullJSON a toJSON Nothing = Null toNonNullJSON (Just a) = .. heavy tagged encoding of Just a toNonNullEncoding Nothing = .. heavy tagged encoding of Null Most of the time `toJSON = toNonNullJSON`, but the cases where you can produce `Null` as an output would change. The downside here is it'd move the common logic to the new combinators, making it brittle and hard to CPP around the version divide. Adding a parameter for whether we're producing a 'heavy' output would work, but would mean you'd be adding a parameter to all the calls, breaking user code, with similar consequences. On the other hand for most code it'd be an extra _ for an ignored argument. Finally, there is the option to flip the script and have toJSON be the thing that we expect to never emit `Null`, and add members for `toJSONWithNull`, etc. default to calling toJSON. This has the same flavor as the `EmitsNull` checks above, where the costs are borne by the few instances that actually emit `Null`. This has the benefit that user's 'd still define toJSON and toEncoding, and only define toNonNullJSON and toNonNullEncoding with non-default values when it can emit `Null`. These are all the possible solutions I can think of off the top of my head. 
Ok thanks! Do you also prefer the monorepo approach?
In theory we can fix the class we have just by handling the `Maybe a` case a bit differently. e.g. adding combinators for (de)serializing to `Value` without a top level `Null` to the existing (To|From)JSON, and using that when serializing the contents under Maybe, so that that will use the heavy tagged encoding in those positions as needed. This would preserve bijection and existing instances wouldn't change at all unless they were for a type that can serialize out as a `Null`. The only things that would have changed output are cases where the bijection property was failing already, and so the From/ToJSON instances were already kinda bogus.
The issue for me is that I can't use Store or Binary since the communication I am doing is over HTTP between my client and my server. So I kind of need to use something like JSON.
It goes further than that. For example Gloss uses `Up` and `Down` for keystate rather than `True` and `False` to avoid Boolean blindness, that way I you have a function that deals with multiple 2 state inputs, you don't confuse the state of the key for whether or not the player is alive, since they would have different types. 
Agreed, although I'd say `Semigroup m =&gt;` instead. 
[`Arrow` is not equivalent to `Applicative`](https://stackoverflow.com/a/24668518/2751851). A quick look at [the code](https://github.com/tweag/funflow/blob/8382d3363b143ed066366d22add7fe5004b6971a/funflow/src/Control/Arrow/Free.hs) suggests the arrows involved here cannot be simplified to applicatives, or monads. (On a tangential note, it is also possible to talk about applicatives in general using arrow-like imagery if you switch to [the static arrow p resentation](https://duplode.github.io/posts/applicative-archery.html).)
Yes
Do you put everything in this repo, even external non-Haskell projects that rely on the provided interface (i.e the servant api / server)? Or just the Haskell parts that have a direct dependency + an implied runtime dependency? We have some pretty large non-Haskell external tools that interact with our api and aren't sure if they should go in that same repo or not.
I definitely see Aeson as a way to work with JSON through Haskell rather than a way to serialize/deserialize Haskell types. JSON is pretty awkward for algebraic data types (no native way to work with sum types) and I prefer not using it unless I was working with external tools or ecosystems that were already obsessed with JSON. If I was working in a purely Haskell world, I would love something like Jane Street's [sexplib] for human readable serialization and deserialization. I've used it in OCaml and it's much nicer than JSON—more flexible, less verbose and a better fit for algebraic data types. I'm tempted to implement my own take on this in Haskell except I suspect nobody else will use it because JSON is the default :(. (Same way nobody would use a better version control system because everyone's already using Git...) [sexplib]: https://github.com/janestreet/sexplib
An extra tool, like... a [browser](https://www.haskell.org/ghc/)? What exactly does "manage" mean, even? You just install it and move on with your life. There's no management. Everybody sells "stack downloads GHC for me" like that's a killer feature godsend save the world thing, but it saves, like... literally no more than a few minutes per dev machine at most.
Actually, now that I think about it, a prototype for an s-expression based library might be a fun project at BayHac...
I think `Maybe` should have a regular encoding like any other ADT. Special `null` business leads to nonsense (in the literal sense) like this. 
It may surprise you but the internet is "real life" too.
I don't disagree - I think the topic is more nuanced and lies somewhere in between the extremes of "pause times are cargo cult" and "Haskell would bankrupt my business". But your tone made me think twice about engaging the topic this time, even if the topic interests me.
Yeah. Both of my mentors are pretty awesome :D You should still reach out to him; he's super cool about helping and mentoring others, especially if you blog about Haskell. Hope I get to meet you in the future, fellow Haskellino!
Thanks for the explanation! arrows are a part of medium/advanced Haskell that I haven't actually touched at all, so I am pretty confident that I don't understand all the benefits.
I didn't ignore his post any more than you. Someone wrote a comment in a way I didn't like so I told them. If that's out of place then so is your own comment.
I'd agree that a platformer style 2D game shouldn't present a challenge but we don't have more info. Speculating, it may be the typical response to Haskell which is reasoning about performance in general requires lots of science and diligent stats to keep control of. That the GC sets a limit on the working set is one simple fact, but often the reason and origin of allocation number and size in Haskell is an amalgamation of all the libraries being used, and the compiler version. Code that was fine can suddenly blow up if you upgrade packages. It's all manageable, but it's a price to pay that isn't really there with Rust or C to the same degree.
I'm not surprised either, I'm tempted to bring some of this into Haskell-perf somehow, as a link to share with people when Haskell's GC and games comes up.
&gt; still wouldn't use Haskell for a game engine. Depends on the type of game engine. 
There's a compiler I've heard of that does this. The glasgow-something-or-other-compiler.
&gt; Any attempt to represent this would be a standard by convention, and have no strong connection to JSON as a format or as a concept. A JSON encoding of tagged enums is no more an accurate representation of sum types than Nothing is an accurate representation of null, undefined, and also a missing property. You're absolutely correct, but this is just the same basic concept from the other angle. If a user wants that degree of correspondance between what their serializing and parsing the should pull back raw Aeson `Value` types or define their own instances. The set of concepts behind the generically derived instances is a convenience mechanism first and foremost, and in my opinion this is critical to having a useful design.
A convenient way to serialise and deserialize Haskell data structures exists already, it's called `read` and `show`. If there are specific reasons read and show don't fit whatever this usecase is we should focus on trying to resolve that.
[removed]
This seems to be a "freer" version. Dunno if it matters or if it's of any use. data Mo m w a where Mo :: (forall b. (b -&gt; m r) -&gt; w b -&gt; a) -&gt; w r -&gt; Mo m w a instance Functor (Mo m w) where fmap f (Mo ext wr) = Mo ((f .) . ext) wr instance Monad m =&gt; Comonad (Mo m w) where extract (Mo ext wr) = ext pure wr duplicate (Mo ext wr) = Mo (\bmr -&gt; Mo (\q -&gt; ext (q &gt;=&gt; bmr))) wr extend f (Mo ext wr) = Mo (\bmr -&gt; f . Mo (\q -&gt; ext (q &gt;=&gt; bmr))) wr instance Applicative m =&gt; ComonadTrans (Mo m) where lower (Mo ext wr) = extend (ext pure) wr liftMo :: (Monad m, Comonad w) =&gt; w (m a) -&gt; Mo m w (m a) liftMo = Mo (\q -&gt; join . q . extract) runMo :: Mo m w a -&gt; (forall r. w r -&gt; w (m r)) -&gt; a runMo (Mo ext wr) f = ext id (f wr)
Late response, do you have a code example of what you mean?
Other than Maybe, there is a notion that there is an injection into a subset of valid JSON syntax and a mapping back that accepts that JSON document and gives back what you put in. This doesn't require that the target language model sum types the same way we do, just that there is an injection we can read back. The current instance for `Maybe` fails the obvious injectivity test. Wanting to spew out data from Haskell into a format and expecting to get it back without random changes is a reasonable ask. It does require a few things to change in the concept of ToJSON/FromJSON though.
Anything that successfully gets the ball rolling on this is a big win in my book. Good luck!
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://hackage.haskell.org/package/ad) - Previous text "AD" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Just a note, AD doesn't do symbolic differentiation, it does automatic differentiation, which is a different beast. Automatic differentiation can be used to mimic something like symbolic differentiation (and vice versa), but the difference between ad's automatic differentiation and CAS-style symbolic differentiation is still significant.
&gt; https://hackage.haskell.org/package/ad CTRL+F "symbolic" is what I was referring too.
[removed]
As well as [`numhask`](https://github.com/tonyday567/numhask). Of the three, this is actively developed.
I like it better, because this avoids the copious amount of fmaps that's required by my version (as well as dropping the `Functor` constraint on `w`). I'm fairly certain our versions are isomorphic to one another, and if so this is simply a straight-up improvement.
&gt; no native way to work with sum types What do you mean? An object with a single constructor seems a perfectly reasonable way to represent sum types. Indeed, that's what Aeson uses. Did you have a particular objection to that approach? toJSON (Left () :: Either () ()) Object (fromList [("Left",Array [])]) Prelude GHC.Generics Data.Aeson&gt; toJSON (Right () :: Either () ()) Object (fromList [("Right",Array [])])
&gt; JSON is not capable of representing sum types meaningfully I'm not sure what you mean. Aeson *does* represent sum types meaningfully in JSON. See also https://www.reddit.com/r/haskell/comments/8et35p/aeson_fromjsontojson_not_inverses/dxzm3d9/
It has almost made me want to fork Aeson. I've wasted hours tracking down at least one bug caused by that, and spent effort on annoying contortions (like sticking Identity into my types) avoiding it in other cases where it would otherwise have hit me again. The single most important thing I care about if I'm serialising stuff is that it round-trips so I don't just randomly lose data. If you've been relying on automatically generated Aeson instances to encode and decode all the communication between your frontend and backend, and a few layers up in your software architecture you start getting bugs where stuff is just disappearing, it can take a while before you even suspect that this kind of thing is taking place. I would never have expected it before I ran into it. Why Maybe doesn't just have the TH or Generics derived instance, I couldn't tell you. It's frankly ridiculous that the library goes out of its way to be wrong.
The problem is not so much with cases where you're explicitly writing Maybe (Maybe a) anywhere, as with cases where, e.g. thanks to type family instances or just parameter substitution, something like Maybe (f a) turns into Maybe (Maybe a) by accident, because f happens to be Maybe, and you then suddenly have a really hard to understand bug where data is just silently being lost.
I'm in the web application development space too, and have found that if I'm taking apart or producing stuff with arbitrary structure that I don't control, I don't typically use FromJSON and ToJSON all that much -- I just use something like lens-aeson, or the Value type directly. Maybe it's just been that the external APIs I've typically needed to access have been stuff that usually I don't want to be bothered with trying to fully represent -- and since I don't control the other end and it can change unexpectedly, that can become a bit of a fool's errand anyway. The one thing I *do* need FromJSON and ToJSON for are the ability to have automatically-generated instances for data types that represent the communication protocol between my frontend and backend. I could theoretically use Binary for that, but web browsers are not good at helping you diagnose issues with binary data, while they'll let you inspect JSON tree structures being shoved over a websocket a little more conveniently.
In my experience, even the relatively nice encodings get awkward for people to read and edit when you encode things like trees.
Maybe would end up being encoded just like any of your other algebraic data types, which honestly isn't all that hard to deal with. If you want nulls in your JSON output, you can produce that directly using Value. If you're targeting some external API's convention for how JSON requests are constructed, you're probably already producing aeson Values directly, and not trying to get aeson's ToJSON instances to somehow magically line up with the outside world. Pretty much the same goes for if you're trying to follow some artisanal design of your own specification. It's not typically worth even getting involved in ToJSON instances in that case. The one place where ToJSON is obviously a win is when you're automatically generating the instances (so the generated code can recursively use toJSON and not have to know what the correct function to apply is). Having silent-data-loss surprises waiting for you in there if ever two Maybes should find themselves next to each other is not a good time.
That sounds a whole lot better than what we're currently doing to me. It would be really nice not having to worry that my data types might, via type families or what have you, end up with two Maybes next to each other, and quietly begin losing data via JSON serialisation.
&gt; It has almost made me want to fork Aeson Not sure what degree of "fork" you are considering but I think just new `FromJSON`/`ToJSON` classes would be sufficient, plus enough wrappers around them to expose an equivalent API to what we have now.
Yeah, that's pretty much the whole thing. The trouble is, everything else in existence is already using FromJSON and ToJSON, and usually in a way which is totally acceptable. By adding a new class, we also need to get new instances into a lot of places. I somewhat like [Ed's approach here](https://www.reddit.com/r/haskell/comments/8et35p/aeson_fromjsontojson_not_inverses/dxypz4f/) which would maintain compatibility in any case where things aren't already broken, while fixing Maybe.
we put everything in there.. haskell, react, react-native, forked third-party code, static web sites, machine learning code (python), etc.
`proc` notation I could take or leave. To me, the nice thing about arrows is that they enable sane point-free code—basically a concatenative language, albeit a kinda verbose one—but arrow notation goes and makes it pointful *and* the desugaring is inefficient. There are two main places I’ve used arrow combinators recently: 1. To write point-free code like `map (this &amp;&amp;&amp; that)` 2. To write [optparse-applicative parsers](https://github.com/evincarofautumn/Ward/blob/7ddc2eccc80174423d87bb722faf2e78aa9e59b3/src/Args.hs#L33) I was thinking recently that it’d be nice to have `proc` notation for linear algebra, though, i.e., `data Matrix a m n` &amp; `instance Arrow (Matrix a)`, but I dunno if `arr`/`***` make sense. Anyone know if it’s possible and has been done?
Looks like a replacement for Luigi with better semantics
https://elvishjerricco.github.io/2017/03/10/profunctors-arrows-and-static-analysis.html :P This didn't delve very deep into the realm of law breaking things though, so I presume there could be a lot more to add
Looks elegent. two things: 1. I don't understand the use of `Cofree`, `CofreeF`, `project` 2. correct me if i'm wrong, but does `typeOf` run for each node in the AST? Does that mean that the performance is O(#nodes^2)? Does Haskell make that O(n) automatically with memoization?
Funflow's cache is stored on the filesystem. However, it does work on shared filesystems. Supporting other storage backends is something that we discussed, but it's not something we're currently working on.
That is true. However, where parallelism is required it is still possible to get it by explicit use of \`\(\*\*\*\)\` or \`\(&amp;&amp;&amp;\)\`. In practice it hasn't been much of a problem so far. With long running external tasks it's typically fairly clear where you'd want parallelism. Of course an arrow notation desugarer that does that for you would be even better.
Similar to monad transformers you can define arrow transformers that allow to abstract over that. E.g. a reader arrow transformer allows to keep common arguments in scope without having to pass them around explicitly all the time.
It works for as you wrote it. What flags did you use? What error did you get? Prelude System.Random&gt; type Status = Bool Prelude System.Random&gt; data NodeSequence p o a = NodeSequence { runNodes :: forall g. RandomGen g =&gt; g -&gt; p -&gt; (a, g, Status, p, [o]) }
Me too, in particular it looks like steps can be arbitrary Haskell functions. How do they find out if they have changed?
Do you own both the client and the server? If so you can POST whatever data you want between them, binary data included.
Steps of arbitrary Haskell functions (e.g. `arr`) are not cached. However, you can use `step'`, or `stepIO'` to tell Funflow to cache such functions. Those add a `ContentHashable` constraint on the input, so we can identify the input by its hash.
I do, although not all of the clients are written in Haskell.
I see, thank you for your reply. With your hints in mind, would these answers be closer? i) f :: a -&gt; b -&gt; b ii) f :: Num =&gt; a -&gt; b iii) f :: a -&gt; a -&gt; a iv) f :: a -&gt; [b] -&gt; c
I am not sure what cleanroom means, but: partition :: (a -&gt; Bool) -&gt; [a] -&gt; ([a],[a]) -- [ \ p xs -&gt; -- a tuple (trueList, falseList) where trueList is a list of the -- elements x in xs for which (p x) is True, and -- falseList is a list of the elements x in xs for which -- (p x) is False ] partition _ [] = ([],[]) -- Case A partition p xs | p first = (first:oldTrue, oldFalse) -- Case B | otherwise = (oldTrue, first:oldFalse) -- Case C where first = head xs reTail = partition p (tail xs) oldTrue = fst reTail oldFalse = snd reTail Why use partial functions instead of pattern matching? Here's what I'd write: partition _ [] = ([],[]) -- Case A partition p (first:xs) | p first = (first:oldTrue, oldFalse) -- Case B | otherwise = (oldTrue, first:oldFalse) -- Case C where (oldTrue, oldFalse) = partition p (tail xs) 
The room has to be clean but not what you do in there!
For external REST APIs - I highly recommend a data type. It's not any more of a "fool's errand" than maintaining *ad hoc* manual JSON code, and it's a much better architecture for interfacing with Haskell. For a REST API controlled by us, a data type is a *must*. Generating the data type automatically using something like [swagger2](https://hackage.haskell.org/package/swagger2) or [highjson](https://hackage.haskell.org/package/highjson) is an option, but so far we have not found any such library that is worth the trouble for us. In any case, since those libraries are for API specification and not faithful serialization, you would have to check whether the proposed changes would break them. To be clear - I strongly oppose changing the current behavior of `FromJSON/ToJSON` for `Maybe`, unless we provide two sets of classes for the two different use cases. If we do provide two sets of classes, I vote to leave the current ones as they are without the breaking change and provide the new ones for the other use case. But if I am outvoted that wouldn't be the end of the world.
I understand the problem. Here's another idea: Use generics to find the nested `Maybe`s and fix them (wrap them in a newtype or whatever). Then we could use the same `FromJSON/ToJSON` instances for both use cases.
An early version of the idea of "contract-based programming" was the [Eiffel programming language](https://en.wikipedia.org/wiki/Eiffel_%28programming_language%29), introduced in 1985. Types are a more mathematically sound way of doing "contracts". But we still need more work on making type-level programming less awkward, so you can more easily do the same kinds of calculation that you do in contracts.
**Eiffel (programming language)** Eiffel is an object-oriented programming language designed by Bertrand Meyer (an object-orientation proponent and author of Object-Oriented Software Construction) and Eiffel Software. Meyer conceived the language in 1985 with the goal of increasing the reliability of commercial software development; the first version becoming available in 1986. In 2005, Eiffel became an ISO-standardized language. The design of the language is closely connected with the Eiffel programming method. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; [The segment list problem](http://infohost.nmt.edu/~shipman/soft/cleanhaskell/web/seglist.html) The Haskell library that does this is [Ranged-sets](https://hackage.haskell.org/package/Ranged-sets).
It is true that, when a person shares a traumatic experience, the pain they suffered may break through upon the listener even if they are trying to speak calmly. Then again, if you were unintentionally being rude in a conversation, would you not consider someone pointing that out doing you a favour too? And would you not prefer them doing so in a polite and gentle fashion? Please notice how Chris chose to withdraw from the conversation. Who benefits?
i think the second definition of "fac" sort of overrides the first one, because I happened to get this error, when I'm writing down the pattern for x in a different order: Prelude&gt; fac x = x * fac (x-1) Prelude&gt; fac 0 = 1 Prelude&gt; fac 8 *** Exception: &lt;interactive&gt;:2:1-9: Non-exhaustive patterns in function fac can somebody explain to me, what I got wrong about haskells pattern matching? I thought it worked like that, but I seem to be wrong 
I think you're right about the override. To do this you'll need a multiline statement so try: :{ fac 0 = 1 fac x = x * (fac (x-1)) :}
I'm pretty sure those aren't two patterns for the same definition; rather, they're two distinct definitions, with the second completely overriding the first.
It does work as you think, and you are right the second fac is overriding the first due to ghci's line by line compilation, it will work if you put it in a source file or use ghci's multiline syntax: Prelude λ&gt; :{ Prelude| fac 0 = 1 Prelude| fac x = x * (fac (x-1)) Prelude| :} Prelude λ&gt; fac 8 40320 Prelude λ&gt; 
I mean that it's not a built-in feature of Haskell/GHC as it is in Wolfram Alpha. I'm not familiar with AD but I gave it a look and I should have written "Haskell doesn't have built-in support for it but with AD it seems to be pretty close".
Woah, thank you! What a handy little trick, good to know, it works now!
thank you for sharing that trick, asking this question quickly taught me more, than I imagined it would :)
looks the same on my GHCI now, thank you for easing the struggle of learning a new language for me!
I use [hspec-wai](https://hackage.haskell.org/package/hspec-wai) for writing automated test against my HTTP API.
can you give an example how you would use this, and where this would be useful?
&gt; `fmap` has signature `a -&gt; b -&gt; f a -&gt; f b` Not quite, it's fmap :: (a -&gt; b) -&gt; f a -&gt; f b &gt; while `go :: RTree a (Fix (RTree a)) -&gt; Fix (RTree a)`. Not quite, it's go :: RTree a (Fix (RTree b)) -&gt; Fix (RTree b) &gt; Am I missing something? I am giving a `Functor` instance for `NTree`, so my `fmap` implementation must have this type: fmap :: (a -&gt; b) -&gt; NTree a -&gt; NTree b fmap f = NTree . cata go . unNTree The `unNTree` goes from `NTree a` to `Fix (RTree a)`, and the `NTree` goes from `Fix (RTree b)` to `NTree b`, so the middle part, `cata go`, must go from `Fix (RTree a)` to `Fix (RTree b)`. This is a bit confusing because `Fix (RTree _)` appears twice with two different roles, so let's look at the type of `cata`: cata :: (g r -&gt; r) -&gt; Fix g -&gt; r To specialize `Fix g -&gt; r` to `Fix (RTree a) -&gt; Fix (RTree b)`, I must instantiate `g` to `RTree a` and `r` to `Fix (RTree b)`. So the role of the `Fix (RTree a)` is to be deconstructed by `cata`, while the role of the `Fix (RTree b)` is to be constructed by `cata`, and thus also by the `g r -&gt; r` function, `go`.
This is the downside of removing `let` from top level declarations in GHCi. I never did like that idea.
Oh no, this is really bad for consistency :( Prelude&gt; f True = 1; f False = 0 Prelude&gt; f True 1 Prelude&gt; f True = 1 Prelude&gt; f False = 0 Prelude&gt; f True *** Exception: &lt;interactive&gt;:11:1-11: Non-exhaustive patterns in function f 
Well to be fair the most annoying issue for me isn’t directly related to aeson, but I’ve found for UTCTime it is very annoying that ‘show . read /= id’
You could use an Either of tuples, but if you want better names than `Left` and `Right` for your constructors, you have to define the datatype separately. This is because Haskell's type system is "nominal", meaning that a constructor is only considered to have a given type if the constructor and the type both refer to the same type definition; it is not sufficient for the constructor to merely have the same name as one of those expected by your function. The other possibility is a "structural" type system, in which a value is considered to have a given type if the value has structure required by the type. Typescript supports ADTs and structural types, so its [discriminated union](http://www.typescriptlang.org/docs/handbook/advanced-types.html#discriminated-unions) could be written like this, without having to declare the types beforehand: function area( s: {kind: "square", size: number} | {kind: "rectangle", width: number, height: number} | {kind: "circle", radius: number} ) { switch (s.kind) { case "square": return s.size * s.size; case "rectangle": return s.height * s.width; case "circle": return Math.PI * s.radius ** 2; } } Which I believe is what you're asking for? There are libraries in Haskell which try to emulate structural types in Haskell, for example here is the same example using [composite](https://github.com/ConferHealth/composite#readme): {-# LANGUAGE DataKinds, TypeApplications, TypeOperators #-} import Composite.Record import Composite.CoRecord -- | -- &gt;&gt;&gt; area (field (Val @"square" 4.0)) -- 16.0 -- &gt;&gt;&gt; area (field (Val @"rectangle" (3.0, 4.0))) -- 12.0 -- &gt;&gt;&gt; area (field (Val @"circle" 1.0)) -- 3.141592653589793 area :: Field '[ "square" :-&gt; Double , "rectangle" :-&gt; (Double, Double) , "circle" :-&gt; Double ] -&gt; Double area = foldField $ Case (\(Val side) -&gt; side * side) :&amp; Case (\(Val (w,h)) -&gt; w * h) :&amp; Case (\(Val r) -&gt; pi * r * r) :&amp; RNil It's using quite advanced magic under the hood, but at least for simple cases like this one, it works!
&gt; UTCTime it is very annoying that ‘show . read /= id’ Why's that?
A more general solution, one that attempts to rewrite the types in error message using in-scope type synonyms, would be really cool. Lens and pipes both suffer from this (relatively mind) inconvenience.
How would `let` fix this? Wouldn’t you still have to use multiple lines, and so wouldn’t the second declaration still override the first? (Sincere question; I don’t have a ton of experience with this so this is merely my (probabkybincorrect
I never knew about :{ in ghci!
For simple definitions you fan also use fib 0 = 1; fib n = fin (n-1) + fib (n-2)
:}
It's more obvious that let fac 0 = 1 let fac x = ... is overwriting the old definition.
Ohh I see what you mean. Yes, that’s a good point!
I usually delete all that stuff out of the template, but it's sometimes nice to keep around as a reference.
&gt; if you were unintentionally being rude in a conversation, Who judge these things? Look, if we have some kind of absolute measure of "unintentional rudeness", we could have used it to some advantage. But there is no such measure. And frankly. It is not important. I would rather have a large number of rude/blunt/whatever expressions of wisdom scattered over the world than a sea of utterly useless noise from people correcting each other about others tone of expression. 
Ok, how about this. Generalize the original `typeOf` algebra to: typeOf' :: (a -&gt; Type) -&gt; (Type -&gt; ExprF a -&gt; b) -&gt; ExprF (Typing a) -&gt; Typing b typeOf' = undefined Where the `a -&gt; Type` means: "we are not giving you a `Type` directly, but an `a` instead. But here's a function to get the `Type` anyway." and the `Type -&gt; ExprF a -&gt; b` means: "You are going to eventually build an `ExprF a` at one point, do you mean passing it along with your result to get the `b`? The original algebra can be recovered this way: typeOf1 :: ExprF (Typing Type) -&gt; Typing Type typeOf1 = typeOf' id const And the tree-annotating algebra this way: typeOf2 :: ExprF (Typing (Fix (Compose ((,) Type) ExprF))) -&gt; Typing (Fix (Compose ((,) Type) ExprF)) typeOf2 = typeOf' (\(Fix (Compose (x,_))) -&gt; x) (\x f -&gt; Fix (Compose (x,f))) 
Easier way: fac 0 = 1; fac x = x * fac (x-1)
https://www.reddit.com/r/haskell/comments/8f4reg/explaining_the_recent_hackage_downtime/
https://www.reddit.com/r/haskell/comments/8f4reg/explaining_the_recent_hackage_downtime/ Still not sure if it's a good story :-)
&gt; in the Recursive and Corecursive type classes Where do you find those? I have searched for them before, even on Hayoo, but it never returned anything relevant.
There are unboxed sums, which have magic syntax.
Thank you for all the hard work @gbaz1 - I think this happens to sysadmins at least once in their lifetime :) But the important part is not not to fail, but how failure is handled and we're impressed :)
What a rough day you had there. I'm glad you were able to handle it so well. Thanks for the info on what happened, too. I appreciate it, and hope some additional admins are found. 
Here's a simple example (not real but for illustration). Let's say part of an API is this: data Item c = Item { name :: Name , price :: Price , code :: c , altCode :: Maybe c } where `c` can be various types. One of the types we might use for `c` could be `Maybe SKU`. If the SKU or alternate SKU doesn't exist, in JSON we expect them to be `null`, not `{"tag": null}`. That's the normal semantics of JSON-as-API, it's not broken. `Maybe` means "might not be present", and if it's not present, it is `null` in JSON. This kind of thing comes up all the time. It would be awful to have to thread manual joins through every field of this nature for every endpoint of every API, when anyway what you always want when something is missing is just `null`. The mapping is not injective because of the nature of JSON's primitive type system; JSON doesn't have `Maybe`, it has `null`. The closest thing we have in Haskell is `Maybe`, so it makes perfect sense to encode it this way. That is, unless you were perchance planning on using JSON in a totally different way, for faithful serialization. We should do our best to support both use cases.
Took me a while, but I found this: (show . read) "2018-04-26 19:00:00.0" == "2018-04-26 19:00:00" Not sure why that's annoying. Also, seems to me that `read . show == id`.
I tried that but saw after that a lot of errors. 
There was [this recent thread](https://www.reddit.com/r/haskell/comments/8e3vf3/package_worth_highlighting_named_adds_support_for/) that described something new that might do what you are looking for. I'm still not sure what the costs are of using this though.
This is not meant as a personal criticism pointed at any individual, but rather a criticism of some of the practices I don't quite understand mentioned in the post: - 2 week old backups strike me as unacceptable for a critical project like Hackage. Why was this span chosen, what made it more difficult to create more granular backups, and how will this be addressed going forward? (You mention snapshots, but it's unclear why this wasn't in place at first, and what potential blockers for implementing this would be on top of existing infrastructure) - `acid-state` is mentioned as a boon here, but some other comments mention scripted replays on top of some mirror that are a little confusing to me. I don't understand why any of this is necessary versus a standard SQL store using any of the industry's tooling around backup, replication, and restoration - Your take away at the end is that this is a social, not a technical weakness, but it seems that there are technical weaknesses if something like deleting a box resulted in significant downtime such as this. If Rackspace destroyed/lost access to the machine (for whatever reason), we would be in the same situation irrespective of any social issues.
Take a look at this: https://hackage.haskell.org/package/named
They belong to the obscurely named [*recursion-schemes*](https://hackage.haskell.org/package/recursion-schemes-5.0.2/docs/Data-Functor-Foldable.html#t:Recursive). I don't know why Hayoo doesn't index them, but [Stackage's hoogle does](https://www.stackage.org/lts-11.6/hoogle?q=Recursive). haskell.org also has a hoogle indexing Stackage, that should give the same results: https://hoogle.haskell.org/ not to be confused with https://haskell.org/hoogle which I found much less useful although it's the one linked everywhere and it apparently has better type search (that I don't use much).
To be clear, I said there are social _and_ technical weaknesses. We could use help with both!
"whoops" - sysadmin proverb
Hmm, I wonder why I was having so much trouble getting it to parse.
Isn't that the syntax that blaze uses to specify tag properties? That's awesome!
the use of `!` to specify named params look like blaze and will fit right into the problem (inconvenience) I'm trying to solve! I think I'm gonna go with this one
i too recall those discussions, and its perhaps worth revisiting them! (edit: eg it would be nice to be able to have some sort of sqlite powered local mirror of hackage on my dev machine so i can do stuff offline :) )
To me that particular instance of ToJSON looks like a trash fire. If you insist that it is the way you want to write ToJSON instances then yes, there is nothing to be done, and everybody who wants injectivity needs to go off and work off a completely different class. I personally think the pattern smells off. Moving to Item c d readily fixes the problem. Having a guarantee about injectivity would would be a pattern that'd allow folks to reason about ToJSON/FromJSON. Right now you can only shrug and hope something works, and not actually know anything about the behavior of the code involved without instance by instance reasoning.
You basically have to run each monad in your stack. Using your own example as a base, you have an `Either` stacked on top of `State`, so we first have to remove the state from the stack (using `evalState` is a way to achieve that) and then you will receive the `Either`, which you can pattern match. newtype Env = Env String f :: (Show a) =&gt; a -&gt; StateT Env (Either String) a f a = do Env env &lt;- get -- As we are on the State level, we can just use `get` here result &lt;- if env == "Success" then lift $ Right a -- We must lift our computation to the upper Either here else lift $ Left "Error" return result g :: (Show a) =&gt; a -&gt; String -- Just a note: `String` is not a monad, so we can't use do notation here g a = case evalStateT (f a) (Env "Success") of -- We run our state monad and pattern match on its result Left err -&gt; "Error: " ++ err Right result -&gt; show a
&gt; which first means securing hosting elsewhere, something we’ve only just started to investigate — leads welcome! How much resources are we talking about? SimSpace is a Haskell shop in the business of spinning up virtual networks containing hundreds of VMs, on our own hardware, so it sounds like we might be able to help.
The only reliable way to use JSON from serialization/deserialization is the hand write the instances and use a round-trip property test to validate them.
Just use stack + vscode + HIE.
The `proc` desugaring really completely hides any structure behind opaque functions, but as you've noted, it would be law-breaking to use that structure in the first place, even if `proc` exposed it to you. I'm not really that strong on the category theory side, but I feel there should be other abstractions between `Arrow` and `Applicative`. Something like `Category` + `(***)` + `(&amp;&amp;&amp;)`, along with something like `swap :: a (b,c) d -&gt; a (c,b) d` and not `Profunctor`. So, this would just be an abstraction for describing a dependency graph of computations, and the instances would have full access to the structure. The interesting part is what sugar would support this, and could we define sensible laws without removing our ability to use that structure. I guess if there were such an abstraction, it would have the same friction with `Arrow` that we see between `Monad` and `Applicative` where some things have multiple `Applicative` instances but not all of them are consistent with the `Monad` instance.
I think you're ultimately asking for cartesian categories (possibly closed ones; i.e. higher order functions but with arrows). IIRC, Conal Elliott's [Compiling to Categories](http://conal.net/papers/compiling-to-categories/) talks about a way of desugarring Haskell syntax into this with a Core plugin, so that you can use ordinary Haskell notation and get category polymorphic functions that you can heavily statically analyze.
To add to the technical analysis: \&gt; even if we were to decide that losing all that data was ok ... then hackage\-security would accurately detect the loss of this data as a potential rollback, and force a redownload of the proper, signed 01\-index.tar. But this in turn would cause another problem, because all but the latest versions of cabal don’t handle re\-initializations of index\-files following resets properly. * Cabal/Hackage is a client/server architecture, but the server code is no longer compatible with client versions in use. Sounds like API versioning is needed.
According to \[my home\-grown repo check tool\]\([http://repocheck.com/#https&amp;#37;3A&amp;#37;2F&amp;#37;2Fgithub.com&amp;#37;2Fhaskell&amp;#37;2Fhackage\-server](http://repocheck.com/#https%3A%2F%2Fgithub.com%2Fhaskell%2Fhackage-server)\) the human processes are doing well.
The only thing that's going to improve the situation is people with knowledge of better practices volunteering to do the work.
You disagree that it's a downside or you disagree that I never did like that idea?
Lmao
You can run a local mirror of hackage on your dev machine anyway -- it is actually really easy to setup these days!
That sentence was a bit condensed. It isn't about api versioning -- the hackage-security "api" is fixed and hasn't moved and doesn't plan to. Rather, the problem is that there is code for _incremental_ fetching of tarballs that works correctly, so when you "cabal update" you only get the most recent bit of the new 01-index.tgz. Also, if things don't matchup, you are instead forced to redownload the whole thing. Prior versions of cabal just had a bug in this redownload logic. (Note that this nonetheless doesn't weaken us to rollback attacks because there's still both incremental and whole-tarball signing of the contents with trusted keys).
There should be implementations of [Protobuf](https://developers.google.com/protocol-buffers) for most programming languages.
Speaking of new infrastructure, is there any real possibility of moving to a distributed system like ipfs? Though obviously there would still need to be a centralized system somewhere to manage the metadata (at least until one of the World Computer flavors gets up and running to a useful degree).
foo.bar.baz = “qux” Wow So powerful
I agree, but that's not something you'd typically encode into JSON.
One advantage `acid-state` might bring to the table is that it seems like it should be fairly simple to cut the transaction log into multiple smaller files. This again means that backup can be done every 60 seconds using `docker commit &amp;&amp; docker push` to docker hub. Doing this with a traditional SQL databases is a huge PITA.
Fwiw, there was a GSOC project idea to improve the CDN-compatibility of Hackage; unfortunately we failed to attract students for it: https://summer.haskell.org/ideas.html#hackage-cdn-aware
Is there a way to have the quasiquoter just output text? That way I can use Haskell modules to store the tables rather than external text files that are annoying to deal with outside the original package (i.e in downstream dependencies).
This seems like a good use case for the`iterate` function. It has the signature: `iterate :: (a -&gt; a) -&gt; a -&gt; [a]` It takes a rule as its first argument and a starting value, then continuously applies the rule to the previous value. Since you want a list of lists as output we can specialize it: `iterate :: ([Integer] -&gt; [Integer]) -&gt; [Integer] -&gt; [[Integer]]` First write a function that contains your rule for what to do with a single number: `rule :: Integer -&gt; [Integer]` Then we make it work on every number in a list by using the list monad which is just `concatMap`: rule' :: [Integer] -&gt; [Integer] rule' = concatMap rule Then put it together: take 3 $ iterate rule' [1] &gt; [[1],[2,3,5],[4,6,10,6,9,15,10,15,25]] I will let you figure out the rule and the rest on your own.
Thank you, you explained that very well and I'm glad, that I learned something new! Though I didn't specified the requirements just right, the result is not supposed to be a list of lists, but a sorted list of these numbers without duplicates. I'm not sure, if I can apply the iterate full function though here, it seems like a step in the right direction, but my understanding of lazy evaluation is too limited, to apply it just now. (at least I think that would help me here). Would you point me a little further? 
It's probably not wise to enumerate all such numbers or at least do it in a scheme that is not exponential. It seems to me you have to solve it mathematically first. Basically, what you're looking for are numbers of the form *2^i × 3^j × 5^k* for *i,j,k in N_0*. The hard part is to find out how they overlap and compare to each other.
I think you are asking if it's possible to ask `iterate` to apply the function infinitely often and then to sort the resulting infinite list? Haskell does have a good support for infinite lists, but you have to traverse its elements from left to right. For example, `iterate rule' [1]` returns an infinite list of sublists, and `take 3` traverses this list from left to right, stopping after 3 sublists. That works, because this traversal terminates after examining a finite number of sublists. But you can't traverse the entire list to check if any of the sub-will ever contain the number 7, because that would require examining an infinite number of sublists, and that would never terminate. You could, however, produce an infinite list of booleans indicating whose Nth entry indicates whether any of the first N sublists contain the number 7! The trick is that even though you are producing an infinite number of booleans, each of them takes a finite number of steps to compute. Anyway, I'm telling you this in order to help with your understanding of infinite lists in Haskell, but this doesn't really help you with your problem. In fact, Haskell can't help you yet; you still need to solve the mathematical trick of this challenge, that is, you need to figure out a strategy to determine in a finite amount of time whether a given number will ever be part of a sublist or not. That's the fun part of the puzzle, and it has nothing to do with Haskell so I don't want to spoil it for you! Once you find a strategy, come back and we can help you to implement this strategy in Haskell.
I'm writing a blog post about it. From `yesod-minimal` to yesod devel at this commit: https://gitlab.com/ibnuda/Cirkeltrek/commit/f3d72f0a11ec786374a8dc44803cbf49854d5684
Under the assumption that the problem could be solved that way: You could eliminate duplicates in each step. E.g.: last $ iterate (fmap head . group . sort . concatMap rule) [1] !! 600
Thank you for pointing that out. I now have an solution on Mind, that might work for mutable lists and that's something! 
Nix supports "fixed output derivations" that led you specify an expected SHA256 hash of the output. If you specify such a hash then it uniquely determines the store path and the build product is checked against it. Two different derivations with the same fixed output hash will generate the same store path.
Sure, but I think the issue is more about incidental duplication. Sounds like funflow doesn't require you to know the hash up front, and still deduplicates identical paths by defaulting *everything* to content addressable storage. Of course, Nix has optimize-store to basically get the same effect with hard links :P
&gt; you have to express code in a totally different, less direct way that does not look like "normal" Haskell Can you give an example? That's not my experience at all.
[Regular number](https://en.wikipedia.org/wiki/Regular_number)
**Regular number** Regular numbers are numbers that evenly divide powers of 60 (or, equivalently powers of 30). As an example, 602 = 3600 = 48 × 75, so both 48 and 75 are divisors of a power of 60. Thus, they are regular numbers. Equivalently, they are the numbers whose only prime divisors are 2, 3, and 5. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
 :{ print "Have you ever heard the tragedy of Darth Plagueis the Wise?" print "I would think not. It is not a story the Jedi would tell you." :}
but can i easily query it? :) 
How do you *automatically* reload ghci after a save?
I'm glad you asked. Your data forms a [rose tree](https://en.wikipedia.org/wiki/Rose_tree) known in Haskell as [`Data.Tree`](https://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Tree.html). The actual tree can be constructed as import Data.Tree tree :: Tree Int tree = go 1 where go n = Node n $ map (go . (n *)) [2,3,5] And here is how we can flatten it and remove duplicates: mergeOrderedUnique :: Ord a =&gt; [a] -&gt; [a] -&gt; [a] mergeOrderedUnique [] ys = ys mergeOrderedUnique xs [] = xs mergeOrderedUnique (x:xs) (y:ys) = case x `compare` y of LT -&gt; x : mergeOrderedUnique xs (y:ys) EQ -&gt; x : mergeOrderedUnique xs ys GT -&gt; y : mergeOrderedUnique (x:xs) ys mergeOrderedUniqueLeftBiased :: Ord a =&gt; [a] -&gt; [a] -&gt; [a] mergeOrderedUniqueLeftBiased [] ys = ys mergeOrderedUniqueLeftBiased (x:xs) ys = x : mergeOrderedUnique xs ys concatOrderedTree :: Ord a =&gt; Tree a -&gt; [a] concatOrderedTree (Node x ts) = x : foldr (mergeOrderedUniqueLeftBiased . concatOrderedTree) [] ts What's cool about this solution is that it's lazy enough to work not only for infinitely deep trees, but also for infinitely wide, so it's possible to flatten a tree like tree :: Tree Int tree = go 1 where go n = Node n $ map (go . (n *)) [2..] which is a really silly way to get all numbers greater than zero.
**Rose tree** In computing, a multi-way tree or rose tree is a tree data structure with a variable and unbounded number of branches per node. The name rose tree for this structure is prevalent in the functional programming community, e.g., in the context of the Bird–Meertens formalism. It was coined by Lambert Meertens to evoke the similarly-named, and similarly-structured, common rhododendron. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
What's your stack version? And can you paste the whole output of running `stack install foo`?
fizz
You have a couple options here. The simplest one, by far, is to brute Force this. If your input list isn't giant, you can use `concatMap` to build a flat list of your multiples and the original numbers, sort that, and remove the duplicates, and select the 600th item. Haskell is pretty fast all on it's own, that's a decent beginner excercise, and it'll probably get the job done. The other two approaches I can think of : If you really want to learn a lot about how data structures and algorithms in Haskell work, build a tree and implement a lazy fold over the tree to build your list. You'll come up with several different possible right answers and learn a bit. If you want to write the code you'd "actually write for this on a real project", it'd be a strict fold over the input list with a `Set` or `IntMap` from the containers package as your accumulator. Containers already has all the fancy tree structures and algorithms you likely are going to want for day to day tasks, so learning that library should be a focus early in your time as a haskeller. My recommendation for maximum learnings is to write the brute force solution first, the containers solution second, and if you're feeling confident, use the source code of the containers package as inspiration for writing your own fancy tree implementation.
Well, depends what kinds of queries. the use of acid-state means that your "queries" are just manipulations of plain old data structures once you load up the db. So if you're comfortable looking up things in maps, etc., then yes :-)
What's the purpose of `mergeOrderedUniqueLeftBiased `?
I have been working on a simple web app with Scotty and it is really nice, I'm very pleased with it. Yesterday I was reading about Warp, which Scotty uses under the hood and how it has [performance on par with nginx](http://www.aosabook.org/en/posa/warp.html). I usually deploy my web applications using cheap VPS that run nginx + my web app. If I were to deploy an app using warp should I still use nginx as a reverse proxy or just let warp take full control of the server?
TAPL and ATAPL are the go-to sources on this, I'd think. Both by Pierce. I followed Pierce precisely to do type checking for a simple lambda calculus in Agda. It's still on the front page of /r/Agda for reference.
This is impressive. Perhaps there could be a static analysis tool which points out where your program is allocating?
Learn a unification algorithm from some undergrad-level discrete math textbook, like Robinson's Resolution Algorithm. Type checkers do unification, so it's helpful to be able to unify by hand to be able to grasp the intuitions. After that something like Hindley-Milner isn't as hard to grasp.
I would probably advise against jumping straight into inference and unification if the OP is new to typing systems. Instead try to write a type checker with no inference then learn about unification and write an inference engine. TAPL by Pierce is very didactic and great for this.
Great work! It's interesting that all that's needed is `State` and `BangPatterns`. A little profiling skills go a long way. 
That's pretty cool. I wasn't planning on sharing it just yet, but I'm been working on a shmup game with sdl2 for a couple of months now, and while it's still a work in progress the basic engine is in place and I can confidently say that you can definitely do these kind of games with Haskell and GC is not a problem. No IORefs or other mutable state required. [animated gif](https://streamable.com/ujqrs) (which also has low frame rate). This report isn't for the session in question, but it's typical: 379,546,184 bytes allocated in the heap 19,501,952 bytes copied during GC 223,184 bytes maximum residency (5 sample(s)) 30,768 bytes maximum slop 4 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 354 colls, 0 par 0.040s 0.047s 0.0001s 0.0007s Gen 1 5 colls, 0 par 0.001s 0.001s 0.0002s 0.0002s INIT time 0.000s ( 0.000s elapsed) MUT time 1.386s ( 37.171s elapsed) GC time 0.041s ( 0.048s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 1.427s ( 37.219s elapsed) %GC time 2.9% (0.1% elapsed) Alloc rate 273,815,977 bytes per MUT second Productivity 97.1% of total user, 99.9% of total elapsed Note that while my computer is fairly powerful, I have also tested this on my Macbook air 2014 with fairly similar results.
Or you can lazily multiply the result by 2, 3, 5, then pick the next element from each.
buzz
You could try `inspect (mkObligation 'f NoAllocation)` from [`inspection-testing`](https://hackage.haskell.org/package/inspection-testing-0.2.0.1/docs/Test-Inspection.html).
Using a standard RDBMS also opens the option of a hosted/managed solution like RDS. Someone else handling backup, restore, patching etc would be an ideal situation for hackage since it already is starved for ops resources.
Is there a mailing list or other place where these infrastructure discussions take place? I'd like to check it out after reading this.
I was speaking from experience having tried both ways. If OP already has familiarity with basic undergrad computer science then it is likely his textbook had unification in it but it wasn't covered in class. TAPL is a very difficult and dense book, at least for me.
I think for the GC-less Haskell the [ASAP Memory Management](http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.html) can be the generic solution.
Have you looked at Typing Haskell in Haskell? I feel like it's really accessible and, if you follow along with your own implementation, teaches you a lot of the basics. Other resources will make more sense afterwards, in my experience.
TAPL can be really hard to grok if it's your first exposure to the notation. I recommend [Type Theory and Formal Proof](https://www.amazon.com/Type-Theory-Formal-Proof-Introduction/dp/110703650X) as a first step; the first 100 pages will get you introduced to all of the notation and concepts you need for typing and understanding extensions to the lambda calculus. TAPL takes over from there with more details on implementing type system features in languages. 
You can find heap allocations by looking for let bindings in STG. It also got easier to spot non-allocating loops thanks to join points. You can cause indirect allocations by forcing thunks/cafs, though, so it isn't always trivial to point to a single point that caused the allocation.
I usually recommend TAPL, but I also usually recommend How to Prove It before TAPL so that the maths isn't as intimidating and the text isn't as dense. Perhaps even Software Foundations in between, which covers a good chunk of TAPL but where you are using the proofs to get comfortable with Coq. Although I'd still recommend How to Prove It and TAPL, mostly to get used to proofs and propositions being used as a tool of communication in a heap of different contexts - sometimes they are the end goal, sometimes they are motivating some definitions that were set up earlier, etc... 
I'd be curious how much work it would actually be to switch to a SQL setup using `persistent` or similar.
I couldn't find the chance to read Conal Elliott's work completely, let alone invest the mental effort to fully grok the implications, but I'm definitely sure it includes and goes far beyond what I'm asking. Advancements on the category theory front excite me far more than the dependent types front. Then again, I don't understand it enough to say how likely his plugin is to make it into mainline GHC. You're right that I'm probably asking for not-necessarily-closed Cartesian categories, since you sometimes want to interpret the structure in a way that doesn't play well with "higher order functions". F.x. if you wanted to generate C code that doesn't allocate heap memory, you couldn't support closures.
I'm impressed. I had the idea of merging sub-results, but I couldn't tie the knot.
With `Applicative` you build up computations in a way that the pieces don't have data dependencies, so, as the instance author, you can decide if you could, say, run them in parallel if the side-effects allow it (and you have full knowledge of what side-effects will happen at this point). With `Arrow`, you build up computations in a way that they might have data dependencies, but you still get a list of what side-effects will take place statically. Especially if you obey the `Arrow` laws while defining your instance, you have nothing but an ordered list of side-effects. With `Monad`, you not only have data dependencies, but you can even decide on the following side-effects based on data, but in return, the instance author has no static information whatsoever, only the next side-effect to perform. This looks like a very nice spectrum to me. There are probably nice things between them, but none of them seems redundant.
Thanks for the offer. I'll try to remember to reach out to you about this...
A fair amount of admin work tends to be coordinated on the #haskell-infrastructure channel on freenode. Idling there and asking questions is a good way to get informed. Other discussion often occurs on tickets in various repos (i.e. hackage, etc). There was an -infrastructure mailinglist back in the day, but it sort of died off some years ago. The haskell-community@h.org list is pretty inert, but could also be used as a place for discussion and coordination as appropriate.
Great explanation.
Looks similar to: https://hackage.haskell.org/package/Workflow With steps and so on
&gt; Single compilation unit: Programs consist of one standalone, self-contained unit; modules and name spaces are not supported &gt; No mutual recursion: We could compile code with mutually recursive functions using a trampo- line. However, trampolines impair performance. Moreover, the trampolines parameters carry information from all of the mutually recursive functions. As such, it creates a choke point in the aliasing graph: many values alias through this single variable. As a result, trampolines can limit the precision of asap’s analyses. &gt; No higher-order No, it can not. 
When working with sequences, I sometimes consult [OEIS](https://oeis.org/), the On-line Encyclopedia of Integer Sequences. It's just what it says on the tin. Sequences often have useful information included in their entry, such as formulas for producing them, code to compute them in various (usually mathy) languages, etc. This sequence looks to be [A051037](https://oeis.org/A051037). Actually there's not much in there that would be terribly useful to me in solving this problem. Some might consider it "cheating" on a programming exercise.
Well, the single module thing is workable, and lowering higher order code to first order code is no problem (we do this with Cmm already). But yea the lack of mutual recursion is a major problem... Interestingly, it did say it allowed for self recursion... I wonder what the difference is, and whether lowering multiple mutually recursive definitions to a large self recursive definition would avoid the problem entirely.
Haskell beginner here. What's the reason behind the `Mk` prefix for your constructor?
It is perfectly possible, but requires whole program analysis. I.e. it fits very well into the [GRIN](https://github.com/grin-tech/grin) compilation approach, as it does whole program analysis and optimization and maps higher-order (surface) languages to the first order intermediate language. Interestingly the ASAP's μL intermediate language is almost identical to GRIN.
I know about `ProfuctProfunctor` of course, but I don't think it's the same thing, is it? A `ProductProfunctor` is something that's `Applicative` and a `Profunctor`, but it's not necessarily `Divisible` in the contravariant argument.
Hmmm, it's not quite `Divisible`, actually, I can't implement `conquer` in terms of `propure`! newtype Wrap p a b = Wrap (p a b) newtype Flip p a b = Flip (p b a) instance Profunctor p =&gt; Functor (Wrap p a) where fmap f (Wrap p) = Wrap (rmap f p) instance Proapplicative p =&gt; Applicative (Wrap p a) where pure = Wrap . propure Wrap f &lt;*&gt; Wrap a = Wrap (proliftA2 (\a -&gt; (a, a)) id f a) instance Profunctor p =&gt; Contravariant (Flip p a) where contramap f (Flip p) = Flip (lmap f p) instance Proapplicative p =&gt; Divisible (Flip p a) where conquer = Flip (propure undefined) divide f (Flip p) (Flip q) = Flip (proliftA2 f const p q) 
Okay, shit, you're right, they really are completely equivalent! newtype Wrap p a b = Wrap (p a b) instance Profunctor p =&gt; Functor (Wrap p a) where fmap f (Wrap p) = Wrap (rmap f p) instance Proapplicative p =&gt; Applicative (Wrap p a) where pure = Wrap . propure Wrap f &lt;*&gt; Wrap a = Wrap (proliftA2 (\a -&gt; (a, a)) id f a) instance Profunctor p =&gt; Profunctor (Wrap p) where dimap f g (Wrap p) = Wrap (dimap f g p) instance Proapplicative p =&gt; ProductProfunctor (Wrap p) where purePP = pure (****) = (&lt;*&gt;) instance ProductProfunctor p =&gt; Proapplicative (Wrap p) where propure = Wrap . purePP proliftA2 i o (Wrap p) (Wrap q) = Wrap (dimap i (uncurry o) (p ***! q)) 
No need to remember anything: it turns out I have a colleague who is friends with other members of the infrastructure team, and they'll work out the details together.
Damn, this answer is mindblowing. Very elegant.
But do you want `conquer`? It's not just that you can't write it using `Proapplicative`'s methods, you also can't write it for `Flip (-&gt;)`, which is suspicious for a `Profunctor`-style abstraction. `conquer` is the identity for `divide`; for example, `Predicate`'s definitions for `divide` and `conquer` are basically `(&amp;&amp;)` and `True`. But your definition of `divide` is too asymmetric for it to have an identity! You're using `const` to combine the results, instead of using something like `(&amp;&amp;)`. Choosing `const` thus seems ill-advised, but the types don't give us any choice, as we don't know anything about the `a`s we have to combine. I would be inclined to blame the types. The idea of a `Profunctor`-style abstraction which uses `Divisible` and `Applicative` instead of `Contravariant` and `Functor` is really appealing, but maybe it's too simplistic. Let's take a closer look at the relationship between `Profunctor` and `Contravariant`: instance Profunctor p =&gt; Contravariant (Flip p a) where contramap f (Flip p) = Flip (dimap f id) We can implement `contramap` in terms of `dimap` because `id` allows us to leave the second type parameter alone. For implementing `divide` in terms of `proliftA2`, we don't have this luxury, which is why you had to resort to `const`, which typechecks but is clearly not what we want. My intuition is to require a `Monoid` constraint on the `a`: instance (Proapplicative p, Monoid a) =&gt; Divisible (Flip p a) where conquer = Flip (propure mempty) divide f (Flip p) (Flip q) = Flip (proliftA2 f mappend p q) This definition makes a lot more sense, but breaks the symmetry between the `Divisible` and `Applicative` sides a little bit. Why isn't there a "contravariant Monoid" constraint on the `a` in the `Applicative` instance for `Wrap p a`? Well, there is! Let's tweak the definition of `Monoid` a little but to make it clearer what a contravariant version would look like: class Monoid a where mempty :: () -&gt; a mappend :: (a, a) -&gt; a class ContraMonoid a where drop :: a -&gt; () dup :: a -&gt; (a, a) You even already used `dup` in your `Applicative` instance for `Wrap p a`. You're not using `drop`, but semantically the `p a b` constructed by `propure b` is clearly dropping the `b`. Perhaps we should think of `propure :: b -&gt; p a b` as a simplification of the more accurate and symmetric type `propure :: (a -&gt; ()) -&gt; (() -&gt; b) -&gt; p a b`? Anyway, we don't have to add a `ContraMonoid a` constraint because we can implement those methods for any `a`, but I for one am satisfied because I feel like the balance is restored :)
&gt; I think this example illustrates is Haskell’s powerful facilities for abstraction. With only a couple changes, I was able to transform the naive solution into something much more powerful. This example would have been even more impressive if you had used the more abstract [`(&lt;|&gt;)`](https://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Applicative.html#v:-60--124--62-) instead of hardcoding `(++)`: you would only have had to change the type of `produceAll` from `Regex -&gt; [String]` to `Regex -&gt; Logic String`! That's because the instances of `Alternative` and `Monad` for `Logic` use `interleave` and `(&gt;&gt;-)` for `(&lt;|&gt;)` and `(&gt;&gt;=)`.
I still feel like there's some imbalance. I mean like you said, ContraMonoid can be implemented for any type, whereas Monoid cannot. Moreover, they don't really look like inverses in anything except the degenerate case -- I mean if I have a monoid like list append, then I give you `[ 1 ]` and `[ 2 ]` and get `[ 1 , 2 ]`, but the ContraMonoid operation gives... `[ 1 , 2 ] x [ 1 , 2 ]` o_O. It may be a valid ContraMonoid (whatever that is), but it's not an *inverse* of the monoid I gave. Anyway I may be just missing something or lost, but at least from what I can grasp off the top of my head, I'm not sure the symmetry is as pretty as the type signatures make it seem?
Yes, it should.
 Version 1.6.1, Git revision f25811329bbc40b0c21053a8160c56f923e1201b (5435 commits) x86_64 hpack-0.20.0 C:\Program Files\Git\hoogle&gt;stack install hoogle appar-0.1.4: using precompiled package async-2.1.1.1: using precompiled package auto-update-0.1.4: using precompiled package base-compat-0.9.3: using precompiled package Warning: Retry number 0 after a total delay of 0 us If you see this warning and stack fails to download, but running the command again solves the problem, please report here: https://github.com/commercialhaskell/stack/issues/3510 base64-bytestring-1.0.0.1: using precompiled package Warning: Retry number 1 after a total delay of 100000 us If you see this warning and stack fails to download, but running the command again solves the problem, please report here: https://github.com/commercialhaskell/stack/issues/3510 Warning: Retry number 2 after a total delay of 200000 us If you see this warning and stack fails to download, but running the command again solves the problem, please report here: https://github.com/commercialhaskell/stack/issues/3510 basement-0.0.7: using precompiled package Warning: Retry number 3 after a total delay of 300000 us If you see this warning and stack fails to download, but running the command again solves the problem, please report here: https://github.com/commercialhaskell/stack/issues/3510 byteable-0.1.1: using precompiled package Progress: 8/111HttpExceptionRequest Request { host = "s3.amazonaws.com" port = 443 secure = True requestHeaders = [] path = "/hackage.fpcomplete.com/package/base-orphans-0.7.tar.gz" queryString = "" method = "GET" proxy = Nothing rawBody = False redirectCount = 10 responseTimeout = ResponseTimeoutDefault requestVersion = HTTP/1.1 } (ConnectionFailure Network.BSD.getProtocolByName: does not exist (no such protocol name: tcp)) 
Yes, the `Divisible`ness comes for free when `p a` is `Applicative`, just like the `Divisible`ness of `Contravariant c` comes for free when `c a` is a `Monoid`.
You may first discover that there is no module called `Weigh` in the project, but there is a dependency called `weigh` in the cabal file. (One way to do so is by using [repository wide search](https://github.com/haskell-game/sdl2/search?utf8=%E2%9C%93&amp;q=Weigh).) You may then search for a package with a name like this [on Hackage](https://hackage.haskell.org/packages/search?terms=weigh). I get one match. It is then a matter of proceeding to the "Home page" link. It will get you back to GitHub, and there in the source tree you will find [the source you require](https://github.com/fpco/weigh/blob/master/src/Weigh.hs).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/weigh/.../**Weigh.hs** (master → 80e3357)](https://github.com/fpco/weigh/blob/80e3357fac0609132efb929836c9061341fbd049/src/Weigh.hs) ---- 