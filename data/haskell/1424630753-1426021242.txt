 seq (const undefined) =/= seq undefined But that breaks our nice domain-theoretic semantics in a number of ways. You can find a further discussion in the thread following this message: https://mail.haskell.org/pipermail/libraries/2011-March/016049.html
People are doing deep, interesting work with Javascript. For an example of how deep the rabbit hole could go (although I don't entirely endorse this viewpoint), check out [The Birth and Death of Javascript](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript).
Your assessment in #1 is pretty accurate. If both yield an answer it should be the same answer, but you introduce more bottoms by re-associating. Fast and loose reasoning would say the two operations are morally the same, but that exploiting the natural tree structure allows more programs to terminate.
I'll pass that on the to the rest of the program committee and see what is going on.
Thanks chreekat! I've taken to heart your advice about both Haskell and JavaScript. Hope to meet you soon in #haskell.
Why do we need a callback to work asynchronously? I Thought haskell threads were cheap. Why isnt the type just (Request -&gt; IO Response)
Thanks!
You know I hadn't considered C# earlier but watching Eric Meijer's video tutorial on Functional Programming, I was stuck by the fact that MS does seem to have good things going on. Your advice here only seems to reinforce that notion. I will definitely look at both C# and Python as far as getting back into the swing of things employment wise. Thanks again!
Duly noted. Thanks!
Thanks for your detailed response. Your conclusions are correct and the advice sensible. Somewhere along the line there were more than just logical reasons for me to have gotten off track. I'm trying to put things back together from a positive place and not just out of dread and foreboding if you know what I mean... Thanks again!
See my reply to /u/sclv at the top. Thanks!
RealFrac and Integral won't exist on the same type. 
Ultimately the problem here is that (/) and `quot` are fundamentally different operations with very different semantics. C/C++ happens to try to pretend they are the same. We don't. (Note `quot` is different than `div`. That said, yes, I'd like a nicer numeric tower. The constraints that we do have captured in our numeric tower are precisely the properties that the language designers needed to distinguish between the different data types they packaged in the report and no more.
Hi! Yeah, I'm really happy to have been there! Will you be at ICFP this year?
I don't understand that issue. Every instance of every typeclass is already different operation. The "+" of an Int is different from the "+" of a Double which is different from the "+" of Linear's 3D vectors. And, semantically, why would you consider "/" different from "div"? They both implement our intuitive concept of "division" under their respective sets and can be used interchangeably in a ton of formulas.
I'd say it's an intermediate topic, not that the pre-requisite is "intermediateness". When I've given the talk to mixed audiences a few times in the past the reactions ranged the room from "lots of questions, some confused looks" to one guy just nodding along. I'll be taking advantage of existential types and GADT notation to explore free applicatives, though I'll motivate them in a way that hopefully can be followed broadly even without tracking all of the details. I'll also assume familiarity with writing monad instances for types and probably skip over those details.
My pleasure. I really hope it helps! I'm technically in charge of a group of 20 programmers, and do a lot of hiring; this puts me in place so see a bit how the start of a programmers career may go. Myself I choose to work at a smaller company, just because I want to make a difference (not merely optimize on paycheck-size), and this way I get to choose a lot of the tools we're using. After playing with Haskell in my sparetime for years, I finally managed to do some Haskell work professionally last year. Some things take time. 
&gt; ... I started reading about prisms. "First class patterns" they're supposed to be. [My god. It all makes sense now.](https://i.warosu.org/data/lit/img/0056/42/1414417183737.gif)
Oh, I wondered what "thunks" were for. Thanks for explaining, I'm still learning virtual-dom.
The slides are in English so I figured that was accessible enough for everyone to enjoy. I can more or less understand German myself (even if Raichoo speaks reaaally fast) but wouldn't be able to write anything. Bitte schön.
&gt; So what do I do now? I read the type of every single numeric function you used in order to fix the plumbing. Look at the five different errors. They tell you exactly what to do add (Integral a) to the context of ... etc. etc..
Yes, it is ⊥ on infinite (and partial) things, and on finite things, you'd turn all the booleans into bits and add them up.
hscolour for Haddock with both hover-over-types and click-to-follow-definition functionality, as discussed here: http://www.reddit.com/r/haskell/comments/2wni22/looking_for_hscolour_with_gotodefinition/
They are actually very different concepts, semantically.... and especially with respect to equations reasoning. For example, you might expect x / y * y = x. you should be able to make that deduction and simplification.... it's essential for doing any reasonable mathematical manipulation. With integers and div, this simply isn't the case. So we have two options: (1) surrender the ability to do that sort of equational reasoning ever, or (2) define a different div function that is not a multiplicative identity. Personally I really really like being able to see things like x / y * y and being able to rewrite it as x. 
I have had this type packaged up on Hackage for a while now: https://hackage.haskell.org/package/fmlist-0.9/docs/Data-FMList.html
&gt; I'm interested in React's diff approach because it's a clever way to bridge between the two worlds. It may be an implementation detail to Facebook, but to me, it's fundamental and I want to explore it further. Right, I think we're in agreement that theoretically it's possible to preserve all the state without diffing, given enough time, effort and suitable access to the underlying GUI API. On the other hand practical concerns suggest that diffing provides great benefits. This is indeed an interesting problem to work on. Luckily, my approach is completely agnostic to diffing. It can happen or not depending on your desires and domain. &gt; Do you mean that you provide an abstract representation of a button, allowing the GUI frontend to be changed without changing the user's code? That's not what I meant, although I suppose that would be possible. What I meant was that my API is suitably general to be used with "any" graphics toolkit, in the same way that reactive-banana can connect up to "any" graphics toolkit. &gt; &gt; I have an implementation in HTML/Javascript and another in WXWidgets. &gt; &gt; Your implementation is much further along than I thought! It must be way too late to have these design discussions. I've only implemented four HTML widgets and two WXWidgets just as a proof-of-concept, so not really very far at all. Regarding simultaneous events: suppose you are implementing a calculator, and you have an event for numeric keypresses, operator keypresses and the "clear" keypress. We also want a behaviour which shows what is on the screen numericPressed :: Event Number operatorPressed :: Event Operator clearPressed :: Event () screen :: Behaviour Screen The obvious thing to do is to turn the events into functions which update the screen numericUpdate = fmap numericHandle numericPressed :: Event (Screen -&gt; Screen) operatorUpdate = fmap operatorUpdate operatorPressed :: Event (Screen -&gt; Screen) clearPressed = fmap (const clearScreen) clearPressed :: Event (Screen -&gt; Screen) and then take the union of these events and fold them to update the behaviour screen = accumB startScreen (unions [numericUpdate, operatorUpdate, clearPressed]) Now what happens if I receive a clear event and a numeric event at the same time? The simultaneous events are processed in the order they occur in the `unions` so the numeric update appears on the screen before being immediately cleared. Is this the way round we wanted, or is it the other order? I would argue neither! It's actually meaningless to our domain to receive simultaneous events. In the spirit of making illegal states unrepresentable I want an approach where simultaneous events are impossible. I don't know of any FRP approach which takes that point of view. Now onto dynamic event switching. I don't really know very much about this at all, but it seems to me that if you dynamically create widgets then you need to dynamically create events for them and this ends up looking very imperative to me. I'm far from experienced in this matter though, so this is really just a basic impression. Anyway, I'll email you a link to my repository and you can see what you think.
It works fine. You may find it hard to satisfy those constraints, but that's another issue!
&gt; For example, why "Float" is not an instance of "Integral", when, in reality, Floats can absolutely always be used where Integers can. What is the 2.5th element of the list [0, 1, 2, 3, 4]?
I present a proposed addition to the universe of [nerdy obscure t-shirts](https://i.imgur.com/b0wi5s6.png)
In Haskell, a numeric constant like 10 is equivalent[0] to: fromInteger (10 :: Integer) ...where `fromInteger` comes from the `Num` type class: ghci&gt; :i Num class Num a where (+) :: a -&gt; a -&gt; a ... fromInteger :: Integer -&gt; a That's why you can use an integer literal (or a definition like your original `foo`) in place of any numeric type. OTOH, `True` isn't desugared at all; it's always a boolean. If you wanted the same effect, you'd have to do something like: class Value a where true :: a instance Value Bool where true = True instance Value Int where true = 1 foo :: Value a =&gt; a foo = true [0]: I realize this looks like a circular definition, but you get the idea!
Yeah, `floor` / `ceil` have a rather interesting type. I think many times I want these instead: floorf :: RealFrac a =&gt; a -&gt; a floorf = fromInteger . floor ceilf :: RealFrac a =&gt; a -&gt; a ceilf = fromInteger . ceil But then, I fear those might be inefficient if the intermediate Integer is reified.
I have considered similar things for Francium, but I'm really hesistant - the whole point of declarative programming is that I *shouldn't* have to annotate my source tree as to where I think the performance is slow. It doesn't scale - data loads change, as does browser performance. It's akin to query hints in SQL statements and "cuts" in logic programming. Some people deem them a necessary evil, but I still sit firmly on the other side of the fence.
So what about functions like par, seq, deepseq, pseq etcetera? Or how about foldl' ?. Don't those all fall under the same problem? Yes they should be used with caution but they can be very effective when used. I won't tell you to annonate your code with `memo` all over the place but for a library like Ohm it would be nice if `Component`s annonate their render function with it. To the user of the library it's not visible that memo is used, but as a library author it makes sense to say that if your component state is the same you probably don't need to rerender the component. 
In the case of `foo = true`, you've called `true` explicitly, but in the case of `fromInteger`, what is calling it? It's reminding me of Scala's implicits.
You can think of it like the compiler inserting a call to fromInteger there. You can do similar things with Strings with the OverloadedStrings extension in GHC. However, they can do this because specific compiler support was added. This can't be done for every typeclass.
Although an extended base wouldn't be a bad idea, it'd still be great to bless a few libraries for GUI, database connections, XML parsing (specifically, writing your own in attoparsec isn't the same), and such. The Haskell platform as it is is too small, if it blessed more functionality, there would be less need to install new libraries and deal wit the ensuing dependency hell. Package maintainers can then strive to keep a lower bound of the version in the platform where possible.
Fine, let x be a member of a group G. What is x repeated 2.5 times?
Yea, then why don't we go all in and use dependent types so that the only way to access a list of N elements we need an element of a corresponding finite ring type? I can live with either accepting that types are a burden and using reals for the practicality, or building a proper, well-designed numeric tower and have actual type-safety. But Haskell tries to be a middle-ground, except there is no such a thing as half safety - you either accept invalid arguments or you don't. So we get no productivity, a false sense of safety and ton of undocumented, ad-hoc typeclasses to figure out. What RealFrac even is... ugh.
However unpopular and impractical, there are hidden AI gems in haskell. I'm expecting greatness one day from [hlearn](https://github.com/mikeizbicki/HLearn). And this [post](https://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2/) showcases how deep thoughts can be thunk best in haskell. [Moo](https://hackage.haskell.org/package/moo) looks like a well-designed GA library.
/u/tailcalled chose that example especially because x repeated -3 times is a meaningful concept!
Not the op, but [Kevin Murphy's Machine learning book] (http://www.amazon.com/dp/0262018020/ref=wl_it_dp_o_pC_nS_ttl?_encoding=UTF8&amp;colid=Y7L9SD3HO8WW&amp;coliid=I2JCL1LNLMCQB2) is generally considered a good overview of modern AI.
Isn't that what it says in the blog post, or was it updated? &gt; Wai corresponds most closely to Ring or Rack. Am I missing something?
It's actually literally how integer literals are read in Haskell. When you write the character 5 in a source code, how do you think it is translated into haskell data? there has to be a process...the process is that it calls `fromInteger` on the corresponding integer that it can be parsed as. There could be any process, but this one is chosen :) What process were you expecting? It isn't that ghc inserts a "fromIntegral"... it's that a fromIntegral is involved in the process of taking the string from the source code into haskell data. 
I would nominate Haddock support for Markdown as a tractable project having the biggest bang for the community buck. This is *not* to be confused with a concept of replacing or rengineering haddock functionality to use markdown syntax which has well [documented flaws](http://fuuzetsu.co.uk/blog/posts/2013-08-30-why-Markdown-in-Haddock-can%27t-happen.html). What I mean is the addition of functionality that would allow comment sections to be marked as Markdown, subsequently processed separately by haddock as such (for example, by passing through pandoc) and otherwise not interfering with haddock methods. The major benefit of this is to enable the integration of blogging, documentation and library documentation. At the moment, quality explanations of libraries are often found in haykll blogs and written in markdown. Inclusion of these in code bases via haddock involves complete rewrites including formatting, loss of functionality like tables and so on (the list is large).
"Artificial Intelligence: A Modern Approach" by Russell and Norvig. 
X^-3 has meaning in every group. It is exactly the inverse of X (wrt the group operation) repeated 3 times.
Logically speaking, a typeclass constraint represents a `forall` quantifier. The type `(Value v) =&gt; v` is equivalent to `v, for any type v which has a Value instance.` The reason that works for the `Num` constraint you've given is that integer literals are actually `Num` literals which are silently coerced as needed. What you want is an *existential* quantifier; i.e., `v, for some type v which has a Value instance`. Unfortunately, while there are Haskell extensions for existential types, it is not possible to express existential typeclass constraints. You'd need a dependently-typed language like Idris or Agda for this.
I didn't notice that sentence in my initial read through, but it is possible that it's always been there. That said, given it's the only mention of Rack in the article, I think my observations still hold. It seems somewhat irresponsible to mention Rack like that in passing and then compare WAI with Sinatra. That said, I don't think the author was meaning to be misleading, so I didn't down-vote. *note: I'm not a Rubist, nor have I ever used WAI directly for anything (though I've played around a bit with Scotty and friends).*
Murphy is good for ML, Bishop is good for NNs in particular, and there are a bunch of canonical papers from the past few years that give an overview of recent developments.
In my experience, not really. With CuDNN and CuBLAS, computational kernels that you still need to write are either: 1) So simple that they are impossible to write incorrectly, in which case Accelerate doesn't help. 2) Sophisticated and performance critical, where Accelerate doesn't help either, since you need to avoid warp divergence, optimize register usage, etc, and so hand-writing them is the only option. There's also the issue of interoperating with existing code.
Any type: There needs to be a way to get a value for all type fulfilling the constraints. In OP's example, this means that there needs to be a way to get both an `Int` and a `Bool` from `foo`. For some type: It's enough if there exists one type fulfilling the constraints that `foo` can return, so the definition of `foo = True ` suffices, since `Bool` is in the `Value` type class
GHC is a product of Microsoft Research ;)
Because `Point` is not a ring. It doesn't have a sensible `(*)`. It's an Abelian group so you can make it a `Monoid` but I don't see a point to it. And actually, the numerical hierarchy in Haskell _does_ correspond (albeit imperfectly) with mathematical concepts. `Num` ~ ring `Integral` ~ Euclidean domain `Fractional` ~ field The analogy is strained in a number of ways but I'd say it's "morally correct".
This is a side-project I've been working on (and off) for a while. I use jmacro and lucid to render a web page (served via happstack) with some d3js javascript tucked in there to create dynamic charts in the browser. Data is served to the page via websockets and the process is managed using [tekmo's](/u/tekmo) mvc library leading to a nice separation of code logic and browser view. Any feedback, questions or ideas?
Haha, I didn't realize it's Raichoo. I know him from my local hackerspace. He's the reason I became interested in Haskell in the first place. :D Funny coincidence. 
Really? AIMA has been updated several times. Last edition dates back to 2009, not exactly AI-winter...
To actually answer your question: I'm sure some people have trouble working with numeric formulas, but personally I find that they translate very easily to Haskell.
&gt; If the all of the input must be consumed in order to use any of the result, then a tail-recursive implementation with a strict accumulator (such as foldl') should be used. That's the definition I use myself. It doesn't rely on alien concepts for developpers used to classic strict evaluation. Plus, if you program and don't know if your algorithm has to go through all the data before finding a (partial) answer, then you don't exactly know what you're doing (so it's a question you _should_ be able to answer, lazy evaluation or not). But it requires lots of getting used to. I think it's normal to make errors like _sum = foldr (+) 0_ at first. I also say "lazy evaluation is about evaluating expressions _inwards_, i.e. starting by the outside". But that remains quite abstract.
&gt; the idealist in me that abhors everything about the 10-12 years I spent as a java developer and all its unnecessary, over-engineered complexity (language + ecosystem) If you realized that you're certainly not worthless. Be aware that learning Haskell will only fuel that hatred. All in all, it's blue pill vs. red pill all over again... Now, more seriously, you should try Ruby. To me, it's the language that best mixes blissful idealism with pragmatic money-getting. Ruby devs gave me the feeling they are interested in getting sh*t done but without forgetting the art of building nice code and abstractions. That's something you don't get e.g from Python communities, as the "simplicity as a spearhead" nature of the language (1) directly conflicts with the power needed to construct nice abstractions and (2) doesn't incite developpers to do so. To me Python is, in all aspects, tomorrow's Java. **tl;dr** Learn Haskell and Ruby (and Clojure, if you really have spare time).
Based on Haskell job postings, ICFP sponsorships, etc. I'd actually think that *most* proprietary Haskell code is either in machine learning, quantitative analysis, and whatever "big data" those touch. The QA probably uses the numeric typeclasses quite a bit, though I doubt they'd touch Float/Double unless they absolutely had to. The ML certainly has some numeric processing, too. But, it's also possible that Haskell is used for DSLs and higher-level organization, and all the math is done with FFI calls to Numpy, for all I know. Also, keep in mind that the vast majority of practical CS in use today was theoretical CS in the past. :)
Note that while prisms are first-class, the exhaustiveness checker doesn't know enough about them to check them. For sufficiently polymorphic sum types, you can do the checking via a library that's been posted here before, but otherwise "buyer beware".
That's very nice. But no, I still think this is begging the question. In spirit, it's still not what I think of as being a monad. Just because you can illustrate something that looks like the monad laws on a single concrete type doesn't make it an interesting monad. To me, the essence of a monad is that it exhibits that pattern on *every* type. When you only have one type to begin with, that's a pretty vacuous statement. So, yeah, technically it's a monad I guess, but a very trivial and uninteresting one. So then - why does that derivation of yours seem to look so beautiful and interesting? Because in reality, you are thinking of a much more interesting type system. This derivation is a type-erased version of the one you would do in the more interesting type system. Our minds subconsciously recreate the richer picture from the projection, similarly to the way that we can look at a relatively uninteresting and asymmetric jumble of two dimensional lines and angles and feel as if we are looking at a beautiful symmetric three dimensional cube.
I suggest the [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell) as among the best ways to go through the whole process of understanding the language from ground up. I learned partly by *improving* the book for others whenever I got confused — I worked to figure out my confusion and make the book clearer. With all the community help, I think it's the best intro and overall resource out there already.
As other people have mentioned, it's inserted by the compiler. Every numeric literal has fromInteger called on it. It's possible to do this for other types, but unsurprisingly it requires extending the compiler. For example, there's the overloaded strings extension, so a string literal always has fromString called on it, and there's also an overloaded lists extension (I'm not sure if it's in a stable release of ghc yet, though) that calls fromList on list literals. It's part of the definition of how to interpret a literal. It's convenient, but not really magic.
I'm holding out hope that someone will build a library intended for practical use (I have never been able to get my head around [compdata](https://hackage.haskell.org/package/compdata), but that very well may be my own fault) based around [DTalC](http://journals.cambridge.org/action/displayFulltext?type=1&amp;fid=1899160&amp;jid=JFP&amp;volumeId=18&amp;issueId=04&amp;aid=1899152) and provides an example of a non-trivial multi-pass translation (at least one of the passes eliminates a constructor, and that's known to the type system) for a non-trivial language (i.e. one with lambdas, distinct value and expression types (which must depend on each other because of said lambdas)). In this case, prisms and exhaustiveness checking would play very nicely together
The closest I've seen on Hackage is [monad-coroutine](https://hackage.haskell.org/package/monad-coroutine).
I think I want something like helper k = do i &lt;- State.get State.put (i+1) let i' = (fromIntegral (i^2)) :: Float if i' &gt; 100 then do k (Left i') k' &lt;- callCC (k . Right) helper k' else do helper k blah escape = forever $ do f &lt;- callCC (helper 1) case f of Left i' -&gt; do lift $ State.put i' return undefined Right f' -&gt; do i' &lt;- lift State.get if i' &gt; 1000 then escape i' else do callCC f' --??? What I really want to do is like that but with a more complicated monad transformer stack.
Thanks, I will definitely take a look at the wiki site again. When I started a few weeks ago, the haskell.org and wiki site both were experiencing some technical issues. I was also working offline in a bid to keep distractions down. I see that the wikibooks page has an on-demand downloadable PDF version... I'm assuming it should be up to date on all the changes in the html version, am I right?
Thanks. You are the only one that suggested Ruby here and a couple of years ago I was actually interested in going the RoR route until someone I respected gave a withering assessment of it and my plans fell by the wayside. Will check it out now...
That clears it up. Thanks.
&gt; I'm gonna assume they'll have a basic understanding. :) I would not have them do anything more than desugar "do" notation, or some nested "do" notations.
Please don't drop the support for haste! Ghcjs's codegen is unusable. 
Here's a paper that discusses quotienting in the context of agda: http://www.duplavis.com/venanzio/publications/Universal_Algebra_TPHOLs_1999.pdf
how does this perform relative to something like https://hackage.haskell.org/package/json-builder ?
&gt; How is it different than Haste's haste-inst? Full sandbox support, js-extras in the cabal file to manage js files, and generated files being put in sane places without having to resort to a Makefile. Update: I didn't notie luite answered this question. See his answer for a more thorough reply.
Great question! I can try to benchmark tonight. Data.BufferBuilder tends to outperform ByteString.Builder by about a factor of two in my tests, so I suppose I would expect Data.BufferBuilder.Json to outperform json-builder, but we should get some concrete numbers on that! 
And the colored girls go, do do do do do do do do do (Lou Reed) 
&gt; Not exactly sure what you mean by "Can I use Haskell modules". In ghcjs, there's not yet a simple way of including a Haskell file that doesn't define a main function. There are times where I'd like to write utility functions in haskell to be easily called from js, which is why I asked. &gt; Can you clarify this point too? You should have to write very little boilerplate. Sorry. It was late when I posted those questions, and I mistook your haste diff for a gist of multiple files. I was originally concerned about the start up tax to play with this, but see now that it's just a matter of having your haste branch installed, the minimal js boiler plate, and the relevant haskell model written, which is not taxing in the slightest (nevermind the ugly marshalling, which you admit to). (Fixed malformed reply)
I saw that people here are suggesting Python. Seeing your grievances against Java, I can tell you that, IMHO, not too long it will be until you resent Python in a similar way. For somebody interested in the Haskell ways, Ruby is by far a better catch. Concerning RoR, I actually got interested in Ruby for lighter, more EDSL-oriented frameworks, like Sinatra. RoR is kind of a big animal, but I suppose it's a no-brainer if you intend to value your Ruby skills on a résumé. The problem with Ruby is, even if it's a general purpose language, companies seem to use it exclusively for web development (Python's fault for hoarding all the workforce for general purpose libraries development).
I read your link and I'm guilty. The problem I was trying to solve is to write a lisp interpreter which can evaluate an expression and return different kinds of values (Int, Bool, etc). My solution is to define `Value` as a type instead of a type class. {-# LANGUAGE FlexibleInstances #-} data Value = IntValue Int | BoolValue Bool | StringValue String Then I can use a typeclass to handle the evaluation. class Eval a where eval :: a -&gt; Value data Expression = Atom String | List Expression instance Eval Expression where eval (Atom s) = StringValue s eval (List e) = ....
I think +1s are a great metric - they are basically a people-who-downloaded-this-and-liked-it-enough-to-mention-it counter. Often, when I'm trying to find a library for something, this metric is good enough for me to get going and avoid going down the wrong path. On the subject of stealing ideas though, I consider the MetaCPAN project a shining example of good metadata. See the sidebar for [Catalyst](https://metacpan.org/pod/Catalyst) - sparklines, inline issue counters, build machine reports, ratings, +1 aggregates, and even a direct link to IRC. There's a lot we could learn from, right there!
Nice! Speaking of type-level JSON tricks, here's a [gist](https://gist.github.com/danidiaz/2bf98df3799c33ee5e9f) about using reflection to dynamically generate a FromJSON instance at runtime. It's a slight adaptation of the [Monoid example](https://github.com/ekmett/reflection/blob/master/examples/Monoid.hs) from the reflection repo. It could be useful if one needs to invoke a function that has a FromJSON constraint, but some detail about the conversion method can't be known until runtime.
Do we really need to convert lazy bytestring to strict one in both Aeson and BufferBuilder? The network package already implement lazy bytestring version of socket write, why don't we use that? 
Some ideas: * use the state monad to compute the sum of a list * use the probability monad to compute the probability that two six-sided dice yield the same number * use the list monad to find pythagorean triples * use a parser monad to parse a simple (real world) config file
We have logs. There is very little activity. Only a few of the users have logged in in recorded history. Most of the activity is hitting old user websites and such, though that does happen. A very few people use it regularly.
A friend and I both separately wrote a ray tracer in haskell using repa. Really nice exercise and you can sort of pick where to stop or what sorts of things you want to do. Start with fromFunction :: sh -&gt; (sh -&gt; a) -&gt; Array D sh a and writeImageToBMP :: FilePath -&gt; Array U DIM2 (Word8, Word8, Word8) -&gt; IO () from the Repa-io package. Repa is a really cool tool, I've been able to quickly write studies that use as many cpu cores as I want to throw at it in very little time. Really illustrates what haskell is good at.
Our particular use case happens to want the output data in a single buffer anyway, but you're right, there are situations where applications can benefit from a lazy bytestring, writev(), etc.
I stand corrected, it seems.
I am not very familiar with UndecidableInstances. Why was it necessary to use this at the end?
same email in its original html formatting: http://permalink.gmane.org/gmane.comp.lang.haskell.libraries/24110
In my opinion, this suvey had quite a positive impact on the Haskell community. One of the effects is that now everybody is informed and has a deeper knowledge of this important change.
That is more or less what I am asking :) [this is the nat binary encoding I had in mind, though](http://repository.readscheme.org/ftp/papers/topps/D-456.pdf) On my better dreams there would be a huge repository with encoding alternatives, algorithms, pros and cons, complexity analysis, all explained like I'm not a PHD on algorithmic information theory.
Thus ends the airing of grievances. The feats of strength are sure to be memorable.
Correct. It was required specifically so I could do: instance (s ~ TypeKey a, KnownSymbol s, FromJSON a) =&gt; FromJSON (Message a) The reason this pops up is that `Message a` wraps `Payload s a` (I believe it "quantifies over" `s`, but I'm not sure if this is 100% correct for GADTs) and we need to convince GHC that when we deserialize the Payload that `s` is a `KnownSymbol`, because the constraint `s ~ TypeKey a, KnownSymbol s` is present in the constructor for `Message`. As a side note, the reason I say "it's not the same as quantifying over `s`" is because I was not able to do this with a `newtype` wrapper: -- | this did not work newtype Message a = Message { unMsg :: forall s. (s ~ TypeKey a, KnownSymbol s) =&gt; Payload s a } I'd love for someone to chime in on this if they know how to get that to work or what the differences are between GADTs and quantification.
A Festivus poll?
Actually, [here is the original formatting](http://imgur.com/xcqF6hD).
I always thought it was a bit of a stretch to abbreviate to FTP, but I figured that was just needless bikeshedding.
I've been using (the excellent) `criterion` a ton and have a small wishlist of things easily doable in a summer. Mainly around making the reports better, in order from most to least important to me: - Replace bar graphs in overview with filled version of the KDE contour plots shown in details section. Or at least error bars, but those I think would be strictly less useful. Maybe one could toggle between different representations. - allow dynamic showing/hiding/filtering in summary, so that if you get a really slow benchmark you can hide it and have the scale re-adjust to the others - In the KDE plot, use the gray vertical lines to indicate scale by e.g. marking a line every `mean / 20` or something. So a KDE plot with lots of vertical gray lines close together indicates we're "zoomed out" and the timings have quite a spread. As is the gray lines don't serve much function (you can mouseover parts of the plot to get a sample) - Often I'm testing several different benchmarks against several different alternate implementations. It would be nice to be able to toggle between grouping by benchmark (so that we can compare implementations), and grouping by implementation. - fixing some layout issues (e.g. keys overlapping graphs) - figure out a way to lighten the dependencies
Marketing was aghast at "burning bridges".
Heh. Good one. 
Well, you could use a regular data type if you wanted: data Message a = Message { unMsg :: Payload (TypeKey a) a } But you can't capture the `KnownSymbol` constraint that way, which you kinda want. `newtype`s have no runtime representation, so they can't grab extra information (like a class constraint, or a quantifier, etc.) at all, for various reasons. Incidentally, you can drop the `s` variable, which I think is clearer: instance (KnownSymbol (TypeKey a), FromJSON a) =&gt; FromJSON (Message a) where This still requires `UndecidableInstances` though. I don't think you can avoid that. Anyway, on the subject of terminology: A "quantifier" in this case just means `forall`, which introduces new type variables the way a lambda introduces new term-level variables. This can also include constraints, which must be satisfied before using the quantified type. Quantifiers attached to a GADT constructor, including the implicit `forall` associated with new type variables, apply (somewhat confusingly) to *pattern matching* on that constructor, so even if your `newtype` example worked (and it does, if you use `data` instead) it would mean something different -- `forall s. (s ~ TypeKey a, KnownSymbol s) =&gt; Payload s a` means that you have a value of type `Payload s a` for all types `s` such that you can satisfy the given constraints, whereas the constructor `Message :: (s ~ TypeKey a, KnownSymbol s) -&gt; Payload s a -&gt; Message a` means that pattern matching on that constructor gives you a value of type `Payload s a` for some specific but unknown type `s` which satisfies the given constraints. Now, in your case the constraint `s ~ TypeKey a` means that the type `s` can be determined just from `a` so the only difference is that the `forall`'d version requires the `KnownSymbol` constraint be satisfied at the point of use, not construction; whereas the GADT version requires the `KnownSymbol` constraint be satisfied at construction, not the point of use, and in fact brings that constraint *into* scope when used.
The easiest way to avoid most warnings all in one go, in a backwards compatible manner, without CPP, is to add an explicit import Prelude at the bottom of your import list. It is a bit of a hack and exploits the fact that GHC does the unused-import warning check top-to-bottom, and you usually use _something_ out of the Prelude. The alternative is to do more with CPP. Then when you are done supporting both new and old versions, you'll be able to strip away most of those imports entirely.
I was wondering whether you ran your pure-Haskell version through a profiler to help identify the slow parts of your code. 
Code that takes more than 5 micro seconds should use safe ffi calls always. Code that takes less than 2 microseconds will have a perf win by using unsafe ffi because safe calls have a roughly 200ns overhead last I checked. Code that takes less than 40ns shouldn't be written in c unless you need to twiddle with registers we can't call from haskell code currently. 
Re `js-sources`: this is a bit out there, but I'm considering distributing modules on npm. Distributing code intended to be run in the browser through hackage has always felt a little weird to me. I expect everything on there to run natively. Then I put up something like my own [react-haskell](http://hackage.haskell.org/package/react-haskell) and it feels somewhat broken that it's useless outside the browser. Hackage has no way of telling me that. On the other hand, I could imagine putting packages on npm. Then *all* javascript developers could use them. You'd have to upload an npm package for all the haskell packages you rely on, but it's a one-time cost - only one runtime would be linked, for example. This is probably a very bad idea: * Whose job is it to upload all of those npm packages? Especially different versions... Seems like a lot of work. * Is it annoying for us Haskell people to upload a bunch of packages to npm which aren't really useful on their own? But the benefits... * Webpack takes care of linking and optimizing our code. We rely on the much larger JS community to work on this stuff instead of doing it ourselves. * I see this making it much easier to use Haskell from JS and vice-versa. It's important for this to be easy. * It strikes me as much nicer to be able to specify `"react": "^0.13"` in a `package.json` than upload a file to hackage. (still working on understanding that code you linked, btw)
And rotating gifs
Sounds like a mixup between two of Norvigs books: http://aima.cs.berkeley.edu/ - which is new and updated a few years back. I feel it's a good intro on a bunch of topics; just the right amount of theory for me. And then there's: http://norvig.com/paip.html - which is older and makes a decent Common Lisp intro.
I lol'd.
I took a quick peek at `Data.BufferBuilder.Json`, which is definitely very much in the spirit of `json-builder`. There's a few tricks in `json-builder` that you should consider adopting: First, you probably want to make the `mappend` of `ObjectBuilder`s lazier, for a variety of reasons. Compare the last two implementations of mappend in [this blog post](http://blog.melding-monads.com/2012/02/24/implementing-json-builder-or-how-the-commabuilder-got-its-spots/), you are using the equivalent of the penultimate definition. And at least in the context of `json-builder` and an older version of GHC and `blaze-builder`, there really wasn't any difference in speed. Second, I would introduce a `ArrayBuilder` using the same technique, as it really simplifies a lot of code. See for example, my code for [`Vector`](https://github.com/lpsmith/json-builder/blob/2d424ef2245fc3b60e9194922f00e3023f96b875/src/Data/Json/Builder/Implementation.hs#L316) and [`HashMap`](https://github.com/lpsmith/json-builder/blob/2d424ef2245fc3b60e9194922f00e3023f96b875/src/Data/Json/Builder/Implementation.hs#L330). Also notable is the fact that my code for [lists](https://github.com/lpsmith/json-builder/blob/2d424ef2245fc3b60e9194922f00e3023f96b875/src/Data/Json/Builder/Implementation.hs#L309) is a [good consumer](https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/rewrite-rules.html#idp25099648), which means that `json-builder` can serialize things such as `toJson [1..n]` without creating a list at all, when compiled with optimization and assuming the right things get inlined. And again, at least in the context of `json-builder` when I last worked on it, this cost nothing in terms of performance. And, the first enhancement goes hand in hand with the second; not having the correct strictness properties on `mappend` greatly reduces the advantages of the second.
I just noticed that my reaction to Comic Sans changed from "Yuck, that font again!" years ago to "Oh, cool, slides by SPJ!".
It is, to a limited degree. In fact, that's how I usually code. I'll write a few signatures down ("I'll need an Int -&gt; Int -&gt; Text, a Text -&gt; FilePath -&gt; IO FooHandle") then fill out the definitions. Of course, I'm not very religious about test-first coding even in other languages so I could be biased. Other thing is that sometimes the types don't cover everything. It's nice when you can say "it type checks, so it's probably going to work" but if you're writing an XML parser, say, "it type checks" probably means you have *a* XML parser. It's a good idea to test to see if you have the right one.
If the OP is looking for videos of the technique he describes, this may be interesting: https://www.youtube.com/watch?v=52VsgyexS8Q 
I'll just throw in my two cents: if you find yourself stuck with some abstract concepts, try using some concrete examples first. For example if you can't wrap your head around what a monad "is", look at how the instances work for, let's say, Maybe, Either, List ([]), parsing monads, etc. I remember trying to learn https://wiki.haskell.org/Typeclassopedia , lenses, etcs, staring at the code in disbelief, trying to figure out what the types mean. But then after I wrote more, however simple, code, I noticed it somehow "clicked" in meantime, and now I can hardly see what all the fuss was about. Maybe it could work the same for you.
I don't think npm would do a good job with Haskell packages. The problem is that GHC (and GHCJS, Haste) inlines things across modules and packages. It's also very particular about the exact interfaces being used. This is why packages are configured against dependencies like `somedependency-x.y.z-hash`, where the hash is the installedpackageid hash derived from the package itself and its dependencies. Everything has to match exactly for the package to be usable. From GHC 7.10 on it's even more complicated. `somepackage-x.y.z` can be linked multiple times in the same program, with a different hash. For example you might have one `aeson-0.8.0.2` linked against `text-1.2.0.4` and another `aeson-0.8.0.2` linked against `text-1.1.1.3` in the same program. Also keep in mind that the GHCJS linker keeps track of dependencies on a per-function basis (actually per group of mututally recursive bindings, but that gives the same result), much finer grained than per module. Does webpack also do this? With the flags added for GHCJS you can also specify that packages are only buildable to JS, for example: if !os(ghcjs) buildable: false I do agree that packaging some Haskell code to use from JS should be made as easy as possible, but it seems to me that at the moment the only realistic/reliable option is to package all compiled Haskell code (including dependencies) for a library in a single package. An exception could possibly the the RTS or some other support code not written in Haskell.
Is plan FTP "plan foldable" ? I cannot find the wiki pages anymore linked from the ML thread, only refs to "plan list" and "plan foldable" with a quick summary.
He has that effect on a lot of people in his hackerspace, myself included. ;)
I don't understand how do you intend the functions to interlock if none of have implementations, and thus don't mention any other function. Can you elaborate?
 f :: A -&gt; B f = undefined g :: B -&gt; C g = undefined main :: IO () main = print (g . f)
Yes, Plan FTP refers to the Foldable/Traversable Proposal.
It's not completely clear to me, is this the same server as code.haskell.org? I still use the latter for hosting my Haskell projects.
I know that technique as TDD, type driven development. It is possible to a certain degree in Haskell. I like to see type holes as a tool to support writing the function bodies after (preliminary) finishing the type signatures.
Just to clarify, nobody is proposing to move the official mailing lists like haskell-cafe to some proprietary hoster, right?
Yes, this is roughly how I usually work in Haskell, note that Typed Holes are a powerful feature here. When you write a name starting with an underscore AND that is not in scope, GHC will treat it as a typed hole and print the line it appears on AND reports it's type. The biggest problem with using ``undefined`` is that you can forget to remove all of them and have your code crash. Typed holes solve this problem, at the cost of not being able to run the code due to typed holes being an error. HOWEVER! In 7.10 there is a new flag, -fdefer-typed-holes which will typed hole errors into warnings, delaying the error until runtime. This runtime error will print it's location and type. This means you can now both run the code with typed holes and still guarantee you don't forget one because leaving out -fdefer-typed-holes will turn them into errors. 
The workaround is not going to work for the people using import lists. Example: $ cat Test.hs module Test where import Data.Monoid (Monoid) import Prelude f :: Monoid a =&gt; a f = undefined $ ghc-7.10.0.20150123 -Wall /tmp/Test.hs [1 of 1] Compiling Test ( /tmp/Test.hs, /tmp/Test.o ) /tmp/Test.hs:3:1: Warning: The import of ‘Data.Monoid’ is redundant except perhaps to import instances from ‘Data.Monoid’ To import instances alone, use: import Data.Monoid() 
I haven't been there for years, maybe I should visit the space next Friday. :D
Yes, it did. See the [release notes for 7.10.1-rc2](https://downloads.haskell.org/~ghc/7.10.1-rc2/docs/html/users_guide/release-7-10-1.html) and the [user manual section](https://downloads.haskell.org/~ghc/7.10.1-rc2/docs/html/users_guide/partial-type-signatures.html).
I'll stand corrected on this point.
It was also very interesting to see the sheer number of responses, and that the *majority* of those were from professional Haskell programmers. That's amazing!
Overloading ETLA's (extended three letter abbreviations) is even worse. (sorry, obligatory) 
I should fix this in the article. Do you have any links that explain how continuation passing style helps with resource handling?
&gt; integer literals are actually Num literals which are silently coerced as needed. They're not coerced, they're simply instanciated. There is no magic here apart from recognizing "10" as a (Num a) =&gt; a, the rest is standard Haskell type machinery. &gt; it is not possible to express existential typeclass constraints. You'd need a dependently-typed language like Idris or Agda for this. @OP A workaround to do so: {-# LANGUAGE ExistentialQuantification #-} data SomeValue = forall a . (Value a) =&gt; SV a Therefore the SV data constructor will wrap any value whose type is an instance of Value. Your function can then do: f :: SomeValue f = SV True Note that it will be *impossible* to know afterwards what is the actual type of the value inside SV. You'll just know it is an instance of Value (so, if Value comes with methods, that these methods will be appliable).
Yes.
I guess it's possible to `import Prelude hiding (Monoid)`? Another option is to make a prelude-compat package that exports a `Prelude.Compat`, for the sane people who use closed imports? :)
http://stackoverflow.com/a/8359609/153269
I haven't edited the post (yet). But I tend to add an "EDIT" section at the bottom of posts if I did. And yeah, it's a bit confusing. I'm working on a "from scratch" routing post next.
My personal website is on community.haskell.org. Guess I should probably migrate it away.
That's needlessly tedious. import Data.Functor ((&lt;$&gt;)) is better. When the FTP dust has settled, I am thinking of submitting a proposal and patch for map = fmap, and putting (&lt;$&gt;) in Prelude.
well, that's much more extensive the just exporting (&lt;$&gt;) from Prelude by default... IMO, this should go for GHC-7.10.1.
We had to write a parser for a subset of the LaTeX .bib file format. Not terribly hard, lots of fun bonuses like comments and validations
`Set` is the most canonical counter-example. It has internal shape invariants on where they `a`'s can lie in the structure internally. You can also make a data type like data Repeat a = Repeat !Int a where you choose the action of Foldable to be to fold over `a &lt;&gt; ... n times ... &lt;&gt; a`. This can be both `Foldable` and `Functor`, but not `Traversable`. (To make it `Traversable` you'd have to make `traverse f (Repeat n a) = Repeat n &lt;$&gt; f a`, but this would force you into a different `Foldable` instance w/ `foldMap f (Repeat _ a) = f a`)
We still have an RC3 coming to get this right. We actually just realized that we'd had our wires cross on adding `(&lt;$&gt;)` (and possibly `(&lt;$)`) to the `Prelude` as part of the AMP a few days ago. (I mentioned in previous threads on the topic that it was in as a canonical example of where the AMP warnings had been a little bit off, and some folks reached out and pointed out to me that I was mistaken!) I've put out a poll to the libraries@ mailing list and included a question to try to get a sense of whether folks want `(&lt;$)` to come along as well, but deferring `(&lt;$&gt;)` to 7.12 would mostly just breaks AMP for at least one release. [libraries@ discussion](https://mail.haskell.org/pipermail/libraries/2015-February/025060.html) Without it the AMP is actually quite difficult to _use_.
Not speaking about `Control.Monad` (I mean, every single time I am hit by the need to import `forM_` or something similar)
We decided to be quite minimalist in what new symbols we introduced to the Prelude, so `forM_` and `for_` remain tucked away for now to try to balance the demands of generality with the clutter of having symbols that collide with more people's code start popping up in scope.
I'm a bit out of the loop. What is "FTP/BBB/AMP"?
I haven't ever seen `forM_` colliding with anything, on the other hand it is (`forM` also) a very basic function used very often (almost always, in my code). Another similar function is `sort` and maybe some other very basic list functions, all needing explicit import. Well, of course I can live with manually importing things, but I find these at least as annoying as `&lt;$&gt;` (actually probably a bit more annoying) 
FTP = foldable/Traversable proposal AMP = Applicative/Monad proposal Not sure about BBB
Actually, BBP - Burning Bridges Proposal, the same as FTP.
You could at least still do `pure f &lt;*&gt; x &lt;*&gt; y &lt;*&gt; z`... Not that I'm suggesting one should!
Thanks for taking a look and providing feedback. One thing to note: code that goes through the Monoid instance for ObjectBuilder is not in the hot path. We saw fairly significant wins in our benchmarks by writing the first element, then, for all remaining elements, writing a comma followed by the element: https://github.com/chadaustin/buffer-builder/blob/master/Data/BufferBuilder/Json.hs#L233 That said, for the Monoid case, I'll look into seeing if we can construct a benchmark that benefits from the increased laziness. In Data.BufferBuilder.Json, the non-specialized array implementations (e.g. ToJson [a] and ToJson (Vector a)) use the same code as objects for inserting commas. Thanks! It seems like the main advantage of reducing strictness is giving GHC an opportunity to completely elide the temporary ObjectBuilder when streaming a large, not-known-at-compile-time data structure into the output buffer. 
I have one too, but right.
As probably the most active admin for {community,projects,code}.haskell.org currently, I hereby endorse this post by Gershom. It accurately describes the current situation, and presents reasonable proposals for the way forward.
No doubt.
It turns out this doesn't quite satisfy the monad laws when you take simplification into account. :-( https://gist.github.com/DRMacIver/db7664e7bd5ee77d8cd7#comment-1400046 This is some mix of fixable and a relatively harmless violation, but still annoying.
That's a really nice deck!
Whoa I did not realize this was being worked on. Awesome! What's great about it is that it can be just a development aid, so even if you target older versions of GHC, you can still reap the benefits of this new feature.
I think haskell community really need a more powerful emacs mode (or IDE). With the purely functional and fully typed behaviour of haskell it is easier to create a fancy automatic code generator with type holes and all good stuff and it is possible to create a lambda calculus wrapper to show the user that ´fmap id = id´ for example and to give the user a better intuition about his functions. Something like this Bret Victor's talk "Inventing on principle" would also be very helpful. This would really increase the level of programming in haskell.
QuickCheck's `Gen` monad doesn't technically obey the monad laws either. It chooses to obey the monad laws only when you consider any random data source to be the same as any other. I think that's a defensible position to take, so long as it's documented.
At this point, the best official email contact remains "admin@h.o". I know you have a lot of projects, and migration is some work, so we'll make sure to check in how far you are underway, and to ensure you've found another hosting solution that you feel is appropriate for your needs before anything drastic happens :-)
Ah, interesting. I had not realised that. Well this is definitely fixable up to that level of obeying the laws
For clarity, btw, here are the tracs currently listed on c.h.o as well: http://trac.haskell.org/
Lack of use can be considered a directive. Let folks know what's going to happen. Give them time to move out, then freeze it.
There's a name collision with the [`sodium` package](http://hackage.haskell.org/package/sodium), a cross-language FRP framework.
Yes, I know that now. Sorry! Didn't know about it when chose the name, and I already had a logo, so...
Back in my phone company days we use to refer to them as ADTAs (Another Damn Telecom Acronym).
Thanks everyone for all the enlightening answers, I have gained a better understanding of Haskell now.
Big Beautiful Bifunctors 
Thanks for explaining this more thoroughly. This all makes sense, however, I'm going to sit with this for a bit and play around when I have some time! I may follow up in a few days. Thanks again! :-)
Yes, [they are](https://hackage.haskell.org/package/base-4.8.0.0/candidate/docs/Data-Bifunctor.html).
Slightly off topic - not a GSoC question. Are their any easy bugs to fix or enhancements that a beginner to the project could cut their teeth on? There's an issue tag but there are only two and they both seem to have ground to a halt a bit. That seems to be true of a lot of the issues :/ Cheers 
This keeps happening. Can't we fix it? I mean, whenever we move things about, we break imports. Why can't we record the trail in some way, so that the imports made redundant are harmless? It's a problem related to the "Santa broke my program" problem: quite often I wish such-and-such a function or such-and-such an instance were in such-and-such a library, but it isn't, so I write it myself; sometime later, it appears and my code breaks. I'm usually behind the version curve; my uni sysadmins are further behind. I bring my code up to date on my own machine, then it falls over when I give it to my students. It's something of a nuisance. I wish I could write things like import Your.Module where wish :: You were -&gt; Here wish (You what) = what `ever` I miss so that `Your.Module.wish` is in scope, defined as I have given it if `Your.Module` goes not give it. It should clearly be an error if the types aren't compatible. It should give a warning if the code is redundant, and there should be a way to inhibit the warning by marking the code with a upper version bound at which it is needed. Similarly, when we move stuff from one place to another, mightn't we document that at one or both ends of the move: "this used to be there"; "if you were looking for this, it's there now". The behaviour of `import` currently treats the libraries as existing in the moment, not changing over time. It would be good to accept that change happens, so that we can work in ways which are deliberately robust and enjoy progress more positively. Just now, we can't say "such-and-such used to be here, but it's been moved to there"; we can't say "use this definition only if you can't get it from this module". Every time somebody says "yes it's wrong, but I'd rather you didn't break my workaround", a kitten forgets to love bootlaces.
That's just hilarious, but I hope to add other front-end languages in the future.
I like his monoid explanations. It's nice to see *why* these concepts matter for software. I've always wondered, as a mathematician, why isn't `List` a subcategory of types, and `map` an endofunctor on this subcategory? Or is it just an abuse of language to say this in the succinct catchphrase "`List` is a functor"?
`List` is a functor is a slight abuse of notation: a functor is a pair of a function on objects and a function on morphism. For endofunctors defined in haskell though, the function on morphisms is unique (there is at most one law obeying function with the type of `fmap`), so talking about the "`List` functor" is very reasonable. The category whose objects are of the form `[a]` and where the morphisms are any function is a full subcategory of Hask and so the pair `[], map` is a functor from Hask to this subcategory. Indeed, this is true for any endofunctor.
The issue is that with streaming responses, the response body may want to use resources that were acquired earlier. Here's an example demonstrating this case. Without `ResourceT` or some other such mechanism, the only solution we could find was the CPS style. {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE ScopedTypeVariables #-} import Blaze.ByteString.Builder (fromByteString) import Control.Exception (IOException, handle) import qualified Data.ByteString as S import Data.Function (fix) import Network.HTTP.Types (status200, status404) import Network.Wai (responseLBS, responseStream) import Network.Wai.Handler.Warp (run) import qualified System.IO as IO main :: IO () main = run 3000 $ \_req send -&gt; handle (\(_ :: IOException) -&gt; send response404) $ IO.withBinaryFile "input" IO.ReadMode $ \h -&gt; send $ responseHandle h where response404 = responseLBS status404 [] "not found" responseHandle h = responseStream status200 [] $ \send _flush -&gt; fix $ \loop -&gt; do bs &lt;- S.hGetSome h 4096 if S.null bs then return () else do send $ fromByteString bs loop 
Wouldn't &lt;&lt;$&gt;&gt; be a more sensible name for it? It's $ nested inside two functors.
Does the hiding work for earlier GHCs that don't export `Monoid`?
I had success using the capability number as they key used for the stripe. Helps avoid ping-ponging cache lines between CPUs.
What about (&amp;) and (&lt;&amp;&gt;), will those be in RC3 / added to Prelude?
Have you seen the gl package /u/ekmett mentioned? I've heard it's one of the most complete OpenGL bindings there are.
I'm so delighted it looks like it's coming in. As you see I was keen already: https://www.reddit.com/r/haskell/comments/2ttvsj/major_prelude_changes_proposed/co3mvvf?context=1
I don't think this discussion is productive. I like the name and it's too late for a change, I also have a logo with metallic sodium that I bought for $10 :) I am sorry for the confusion it may cause, though.
Oh. I thought CPS was supposed to restrict the user, but it's the opposite. Thanks for explaining. But then shouldn't there be a non-CPSed version for 90% of uses that don't require this?
+1 for "for = flip fmap" ;) We do that in our own Prelude and then provide generalized forM_ and mapM_
The former has been added to Data.Function, after a committee vote that I abstained from several months ago. The latter we decided not to do. The obvious fixity for the `(&lt;&amp;&gt;)` operator is wrong for most of its usecases., and even pulling in `(&amp;)` anywhere was contentious when it was first raised.
Perhaps consider changing the name of your project to "Weird grey blob that costed $10"? That way you could keep using the same logo. All jokes aside, your attitude of "oh well, too late to change now" is more than a little concerning to me, and I have to ask myself if you apply this same philosophy elsewhere in your project as well (particularly the code).
interesting. could you explain more?
"There is no better way to understand the patterns underlying software than studying Category Theory." --kenbot
/u/yitz had put forth a proposal previously (when `lens` was fairly new) that I had rather solidly backed, but when it proved contentious I unilaterally withdrew it. When it kept coming back up as a repeated `libraries@` and `haskell-cafe@` proposal, I offered to put it to the committee, just so we wouldn't have to deal with it as a persistent low level noise source. After all, we now had a body that could make such a contended decision. But I wanted to not just ram it through, given my role in the previous proposal, so I stepped back and decided not to vote, rather than suffer the appearance of abusing my opinion. By then I largely didn't care about this issue, but the rest of the committee gave a reasonably strong consensus to go ahead with it. The committee was much more lukewarm on (&lt;&amp;&gt;) so we left it off -- at least only Dan Doel spoke up in favor of it. https://groups.google.com/d/msg/haskell-core-libraries/pLBHW0cBoRg/b8_fB-_bvg4J
It's also a popular (unrelated to haskell) encryption library https://github.com/jedisct1/libsodium
Fine, you've convinced me. I will think of another name. Is Kalium ok (based on your Potassium suggestion)?
It was more of a poor communication of intent. The AMP spec itself kind of left things like this open to interpretation and was more of a "broad strokes" writeup. Austin implemented the most conservative possible interpretation of the proposal as a first cut and we didn't revisit it, and because all the code that I was personally porting forward had redundant imports for backwards compatibility I failed to notice. Hence the actual discussion we're having on the `libraries@` list about whether we want to include it, but given responses so far the impression seems favorable.
This was actually a change in behavior some time around GHC 7.6, so it really depends on how far back you support. Prior to 7.6 or so hiding a non-existent export was an error, but this broke old versions of cabal when `catch` was removed from the Prelude, because it was generating code with an explicit hiding clause for it. I may be slightly off on the timing, but that is my rough recollection.
I have just added [Silly representation tricks](https://ghc.haskell.org/trac/summer-of-code/ticket/1669). The idea is to observe certain data-type shapes (constructor arities/strictness) and find an efficient machine-memory representation for them. E.g. lists with a certain number of `head`s could be laid out by GC to occupy a C-array-like layout in memory. Clearly this necessitates the introduction of *administrative* constructors with specific pointer-tagging schemes.
&gt; I also have a logo with metallic sodium that I bought for $10 :) That's a mighty dangerous logo you've got there...
I know what it stands for and I don't think you understand how these methods work.
Well, that's certainly a much simpler embedding of Hoare Logic than [Integrating Dependent and Linear Types](http://www.cs.bham.ac.uk/~krishnan/dlnl-paper.pdf). I'm curious to know how strong the analogy is. What can you express with MonadIx Hoare clauses?
How have you been benchmarking `ByteString.Builder`? My attempts at adapting `bench/Bench.hs` in the `buffer-builder` repository seem to suggest that `ByteString.Builder` is a bit faster than `BufferBuilder` when compiled with optimizations, even when being forced to produce a strict bytestring in the end.
Thanks!
The point of WAI is to be an interface between servers and applications. To have a "non-CPSed version" would essentially mean having two different interfaces, meaning that every server would need to support two different ways applications need to be written. Given that WAI is intended as a lower level interface for nicer abstractions to be built on top of, I don't think that would be a good trade-off.
Haha thats dope, go with that man :) glad you're so amicable. Looking through your code this is really nicely coded. 
Now I want backpack even more
I've just been running "cabal bench" with GHC 7.8 on a fairly stock Ubuntu AMD64 machine. What OS are you testing on?
Yeah, let's question the quality of his work because he is reluctant to change the name of his project.
A lovely example of indexed monads is the implementation of session types. Type safe communication over channels, using index monads to encode the protocol progression. A nice paper about this idea: [Haskell Session Types with (Almost) No Class, by Riccardo Pucella Jesse A. Tov](http://www.eecs.harvard.edu/~tov/pubs/haskell-session-types/session08.pdf) (pdf)
I'm also curious because I haven't seen any code so far with MonadIx, aside from the curiosity that [Free is one.](http://blog.sigfpe.com/2014/04/the-monad-called-free.html)
I thought this was particularly good.
Weird, I never really use join. 
A LANGUAGE pragma to allow "do", "case", etc. to appear as non-infix function arguments would be great: {-# LANGUAGE SyntaxArguments #-} main = forever do ... This would allow us to avoid a big chunk of `$` noise in code.
Unilateral actions are always easier than those requiring multilateral cooperation. Regardless of whether this is a good proposal or not I heartily recommend people use their own alternative preludes to support their own preferred way of working! 
Sure! I will add more examples from the test suite.
Here you go: https://int-index.github.io/kalium/examples.html
cool
Very nice, only one correction: categories are not necessarily isomorphic to their dual.
I suppose you're referring to the following paper: [Lightweight Semiformal Time Complexity Analysis for Purely Functional Data Structures](http://www.cse.chalmers.se/~nad/publications/danielsson-popl2008.html)
Mine too ;) : https://hackage.haskell.org/package/TCache
In your examples for-loops are translated using `traverse_`; would it be possible to use `for_` instead? I think it'd be more readable.
I have a half-finished upgraded version of the `indexed` package that I haven't shipped to date because it required us to kill `Any` as a data type and degrade it to a special `type family`. That only just now happens with 7.10. (It also ideally requires some typechecking rules to be added to GHC around kinds that are inhabited by a single type constructor, but that is another matter.) I have a few McBride style indexed monads and comonads in there: https://github.com/ekmett/indexed/blob/master/src/Indexed/Thrist.hs https://github.com/ekmett/indexed/blob/master/src/Indexed/Comonad/Store.hs#L30 https://github.com/ekmett/indexed/blob/master/src/Indexed/Coproduct.hs#L46 https://github.com/ekmett/indexed/blob/master/src/Indexed/Product.hs#L53 https://github.com/ekmett/indexed/blob/master/src/Indexed/Monad/Free.hs#L52
map is a functor between list monoids, where the relevant functor laws are: map (xs ++ ys) = map xs ++ map ys map [] = [] In this case the domain and codomain are list monoids, meaning that they are both a category with one anonymous object with (++) as composition and [] as identity. This is also known as a monoid homomorphism.
They're translated as `foldM` and then turned into `traverse_`, it is absolutely possible to turn them into `for_` instead.
Fair enough. I've been waiting to play with it for a while - much easier to let you do the hard work ;)
Thanks! This is not what I thought was best, but if that's the decision, there's no reason to be upset about it. I'll adjust my programming style to be the best it can in light of the new types. We'll use whatever hours we can get from management to develop tools and techniques to minimize the transition damage to our shop; the scope of that damage is hard to estimate until we jump in, and that won't be for quite a while. If the damage is great enough that management decides to invest in a move to a different language (I doubt it) - I'll go back to being a hobbyist Haskeller like I was when I started out.
Beer - now why didn't I think of that?
Yes, yes, yes. I was thinking about very similar things. When new developers are approaching Haskell they are not used to crazy high polymorphism, as it's in Haskell (compared to "standard" languages). Having specialized versions of the typeclass instances will be very handy. One more thing is providing a specialized version of types for functions, e.g. along with map :: Traversable t =&gt; (a -&gt; b) -&gt; t a -&gt; t b show also map :: (a -&gt; b) -&gt; [a] -&gt; [b]
Your proof is correct, but it doesn't assert what you think it does. In this case your proof is relying on the fact that `F` is a covariant functor, but the functor from a category to its dual is a contravariant functor. When `F` is a contravariant functor then you have: F(i) : F(G(X)) -&gt; F(O) ... and then your proof reduces to "contravariant functors transform initial objects into terminal objects".
I'm not sure how exactly Haddock would know what the "current data type" is. Are you imagining something where Haddock parses `SPECIALIZE` pragmas (e.g., the [class instances for `Ratio`](http://hackage.haskell.org/package/base-4.7.0.2/docs/src/GHC-Real.html#Ratio)) and marks them up?
slide 63 and generative functors gave me the creeps.... It's a nice talk which clarified a lot for me as well. It's good to see how Backpack compares :-) 
I really like this idea. The functions that come from type classes could either be displayed "inline" together with the list of instances ([as shown here](http://imgur.com/jz43fGy)), or together with the other functions.
When Haddock provides an instance list for a type, it could also provide names and signatures of everything that instance provides, specialized to the type being documented.
I stand corrected: I didn't know you could put a typeclass constraint inside an existential type. That's pretty much exactly what /u/joseph wanted.
~~So for example, when you view the Haddock for `Foldable` at the moment, it looks like~~ ~~class Foldable t where~~ ~~fold :: Monoid m =&gt; t m -&gt; m~~ ~~...~~ ~~Are you envisioning something like where you can click on the `Foldable []` instance and have the Haddock be replaced with something like this?~~ ~~fold :: Monoid m =&gt; [m] -&gt; m~~ ~~...~~ Never mind, tobiasgw has a [visual](http://imgur.com/jz43fGy) of what you're talking about. I agree that this is a good idea, although it wouldn't allow users to view type-specialized signatures for functions outside of the typeclass itself (e.g., [`foldrM`](http://hackage.haskell.org/package/base-4.8.0.0/candidate/docs/Data-Foldable.html#v:foldrM). 
Based on their CircuitHub's github page + the pages of their team members, it looks like they're using some mix of Yesod/Wai/ClassyPrelude/Halycon/Redis/MySQL.
What kind of experience are you looking for? Entry-Level or several years of professional work experience? What's your stance on non-professional (aka hobbyist) experience? Any other technologies that one should know about, like redis, cassandra, rabbitmq, postgresql, etc.?
@saldaoud Sure, where should we start? (Sorry it took so long to reply, lost track of this post and hadn't logged into reddit for a while).
Do some stuff with SBV. You can have "symbolic" values in a Haskell program and run an external SMT solver to compute possible value assignments.
Aye there's a special circle of hell for people like you.
Maybe they could all be displayed? My illustration is limited to Traversable because I was lazy :P 
`map` is a monoid homomorphism, which is of course just a functor between monoid categories. When we say that `List` is a functor, we mean that it is an endofunctor of the category **Hask** of Haskell types and functions. `List` maps objects (types), and `map` maps morphisms (functions).
Derived functions don't need to be in the same module as either the type class declaration or the instance definition!
We're making progress on the alex/happy thing, and the Cabal thing can safely be ignored, so things are getting better. GHC uses its own copy of the MSYS gcc compiler, which is the biggest piece of MSYS that GHC uses.
It's not possible to find all of them. Your example was the list of instances accompanying a datatype. Even with an inventory of the whole module containing both the datatype and the instance, we might not find all the derived functions.
First of all: code.haskell.org is down :-( I wanted to tell you how many darcs repositories I host at code.haskell.org. There must be about 100. I would be very sad if it goes way. I like the shell access, I can simply place files for the web browser there, I can easily put new darcs repositories publically or privately there right from my command line. The code.haskell.org URL looks very professional, too. In principle I could collaborate with other developers, actually no one wants to collaborate. Thus I put all my new repositories to my public_html directory. I would not be sad if the Trac goes away. I don't like the web interface, I almost never look for new tickets and do not get e-mails for newly created tickets. Thus I manage ToDo lists in plain text files and fix all bugs promptly when reported by e-mail. My prefered solution would be a darcsit/gitit based issue tracker with command-line and web support, offline and online access. I would not be sad if the mailing lists go away. There is almost no traffic, most traffic is caused by spam and I cannot delete spam from the archives. Maintaining the archives as static pages would be nice, but I guess there is not much information in them. I also do not like to subscribe to a mailing list for every project where I want to discuss only a single issue. Googlegroups is not an option for me, since it requires JavaScript. Mailing lists served by a repository hosting platform without or very simple per-project subscription for users of the hosting platform would be nice. I started using darcs in its early days and managed my first darcs repositories at darcs.haskell.org. Then I had to move to code.haskell.org and hoped this would be the appropriate and lasting place for the increasing number of my projects. Next shock was the attack at code.haskell.org. I had to recover all my packages and look for possible manipulations in each of them, but fortunately found none. Now most things are recovered, many new packages added, many new links set to code.haskell.org. Now it shall come to an end? I may give hub.darcs.net a try. But I guess once I finished the transition to hub.darcs.net, it will be closed, because the Darcs developers give up the competition with Git! 
Ah, that's right. I guess we could show the ones we find and leave the rest out
As I could see, it doesn't work for LaTeX text. It would be amazing if it would have a `--tex` command to handle TeX files. Do you plan to implement it?
This seems tricky to implement directly. Maybe you can convert to plain text with Pandoc and then run the tool?
Indeed. I also use pandoc for basically everything and LaTeX only for templating and my CV. With that said, I wouldn't mind giving a go at implementing a --tex option, but I might need some guidance from some experienced LaTeX-ers. For now, I would suggest /u/guiraldelli and others interested in using it with TeX play with the --nopunct and --blacklist features. The blacklist should run before and after punctuation is filtered, so careful selection of your words (i.e. LaTeX commands) could get the job done.
Would it be possible to find all derived functions for the modules that are included in Stackage? Or am I forgetting something?
I have no insight into neither stackage nor haddock, but my guess is that this should be possible. 
Could you elaborate on why it is not the case? C and D are isomorphic if there exists a covariant functor F : C -&gt; D which has an inverse G : D -&gt; C. To the definition of isomorphism, it does not matter whether D is a dual category or not. Forget about contravariant functors, you do not need them in the argument (indeed, you do not need them at all... just embrace dual categories).
`[[a]] -&gt; [a]` is common enough. Other than that I've mainly encountered it via people code-golfing things like `(a -&gt; a -&gt; b) -&gt; a -&gt; b`.
I usually use `mconcat` for that.
oh, that's really clever!
[It's been said before]( https://www.reddit.com/r/haskell/comments/2i2uus/specializing_type_signatures_in_haskell_re/), looks like no one has made a proposal or an attempt to implement it yet. 
You have never dealt with nested Maybe's?
Having worked on a variety of hardware projects for fun and for business, this sounds like an amazing business model. I wish I knew about these guys earlier; I'm getting too old to spend weekends sitting at my desk breathing solder fumes and clearing shorts.
I don't want to alienate or go too much off topic, but some of these posts are getting really uncomfortable to read.
Chennai South
what? and what does this have to do with haskell?
No problems! I'm not a native english speaker so I don't really understand what kind of connotation this word has, I hope I didn't offend anyone! Did you spot that word in any other of my posts? I don't remember using it much. Do you guys want me to delete this thread? I guess it doesn't have a lot to do with Haskell (a shame there is not a /r/functional_programming thing...). I posted it since someone could be on my place someday (googling for a div implementation) which proved to be very hard to find.
No, thank you for pointing it out! If you find anything else inconvenient please let me know!
How do I go about evaluating this. Could you post this as a Haskell file?
&gt; a shame there is not a /r/functional_programming thing... There is /r/functionalprogramming, if you don't mind the lack of an underscore ;-)
&gt; (a shame there is not a /r/functional_programming[1] thing...). /r/functionalprogramming is a thing ;)
We started CircuitHub due to frustrations getting things made, I definitely relate to that feeling!
Not very often. I normally use do notation or the bind operator to deal with maybes instead so they don't end up nested. 
This will be remote work, but we like to get together for every so often to regroup. It's a slightly unusual lifestyle, but everyone currently in the company loves to travel so it works out well. I do hope that we will attract nomads like ourselves, that love seeing new places and exploring other cultures, but we consider each person's situation individually. We don't necessarily want to exclude a great candidate who has a more settled down life.
`termWinsock` is a C function, and the body of that C function installs an `atexit` handler - it doesn't call the terminate stuff directly. In reality, now that I had refactored it, I merged the two functions together so there is just `initWinsock` which includes the `atexit` call (see https://github.com/haskell/network/blob/master/cbits/initWinSock.c#L15)
Yep ... actually I also mentioned this some time ago deep in a reddit thread. What would be the right place to make a proposal like this?
makes much more sense now, thanks!
I think I got it fixed. Mind giving it another go?
No, still broken. When doing a cabal unpack on the package those files are not even in there.
I was thinking that maybe the examples here are in untyped lambda calculus(they have to be, right?), and he is going to translate it into simply typed lambda calculus(with integers as a base type) for the application he has in mind.
I just updated it, adding lines about how to sample from a Spline :)
I think you have it backwards with paradigms and languages. Paradigms are attempts to capture similarities in languages, they are descriptive, not presciptive. Languages are developed and then, when an idea catches on so multiple languages use it (and it is central enough and different enough from existing languages) then it is declared a paradigm.
Yes this would be awesome!
I don't suppose I could get in on this email chain? I'll probably mostly just lurk, but it is relevant to my interests.
ESL can be hard to get right, especially when it comes to connotations. In Québec, the slurs starting in F, N, B... are used in friendly slang, but God forbid you use church words.
You can do this with lenses. Just generate prisms for your type using the makePrisms Template Haskell function and then you just write: has _Red has _Green has _Blue This works even if the constructors have fields. If you want a more light-weight dependency you can do the same thing with makeTraversals from lens-family-th and then just define "has" in terms of "nullOf" from lens-family-core: has l x = not (nullOf l x) The benefit of going through a lens intermediate is that you get a lot of other useful functions for free.
Amazing demo! People often ask about real-world applications in Haskell. Well there it is.
I'm confused exactly what this does. Is it just checking for the use of the same words close together?
I think it looks rather beautiful, actually. Inscrutable, but beautifully so.
Oh, are you also in Québec? In Montréal, by any chance? We have [presentations about Haskell](http://www.meetup.com/Haskellers-Montreal-Meetup/) every two months, if you're interested.
It's a nice outcome, but probably not worth a publication all by itself. A blog post (or this reddit post) seems to me like the right level of announcement for this.
Is there something about division that makes you think it should run forever, or have potentially two different correct answers?
Tiiiiiight.
Yes! Although only one line-shaped pen-tip exists at this moment.
Thanks :)
Previously it wasn't thread safe, as there were C variables that were accessed non-atomically, although the chances of it going wrong were low, and if you did get a different number of calls to WSACleanup it probably didn't matter anyway, since it was during exit. Now it is thread safe as in reality I merged the `initWinsock` and `termWinsock` into one call, which does one check of the variable, and thus for each WSAStartup there is one WSACleanup registered.
I assume that you're probably already aware of this: http://hackage.haskell.org/package/MonadRandom-0.3.0.1/docs/Control-Monad-Random.html I figured it would be worth mentioning just in case.
I registered a PayPal account and created a button for donations (see the project page). Thanks for the advice!
What about (if I'm not misunderstanding): random g = let (x, g2) = random g (y, g3) = random g2 in Position (y, x) Edit: /u/tomejaguar beat me by 14s.
This is an active area of research. Google "gradual typing" and "optional typing", different ideas each connected to what you mentioned. Also, Javascript doesn't always simply work. It can be incredibly buggy if poorly written, and hard to understand, particularly tracing through callback hell. 
Huh, you're right. For some reason I thought this worked for me for the other day, but I guess I'm mistaken.
No, it looks like I was mistaken. Sorry about that.
I would really like to see another run at improving haddock. I think the last effort veered off from the popular desire to improve haddock into something very specific that got stuck. Your suggestion of marking individual blocks as markdown sounds good, and it’s just a small step to a file-wide toggle.
Too late. I've already updated my code in 1000 places to use this new syntax. It will take hours to undo, so someone has make a GHC extension for it.
I guess that the dynamic nature of JavaScript is where it shines and is the reason of its success. The fact that browsers can implements stuff using prefixes and developers can query the capabilities is a true winner, this makes the environment go forward in a very fast way. Maybe we could get a lot more "standards" if it were all type-checked, but then we would need the standards to be set *before* any progress can be made, and that's too slow for the internet. JavaScript is not well designed, the semi-colon and stuff. But these are minor problems compared to all other stuff it solved for the internet. Hot code loading, reloading, code swaping, reflection, code and environment inspection, all-async mode, etc. It may not be the beauty of the languages, standards and mathematical background, but it solved *real* problems and it's working pretty well so far. The initiatives to make JavaScript stricter have failed so far, why? How do I do static type checking in a dynamic environment? The browser cannot halt just because something didn't type check.
In this particular case, besides /u/tomejaguars suggestion, you are actually dealing with the state monad, only you don't know it! You can imagine something like this -- strandom is the State monad version of random strandom = state random randomPos = runState $ do x &lt;- strandom y &lt;- strandom return $ Position (y, x) The state monad plumbing will take care of the rest, and the above code is actually precisely equivalent to the code /u/tomejaguar suggested. One reason it's a very good idea to do it the state monad way instead of manually threading around generators is that it's much harder to make mistakes when you're in the state monad. The state monad takes care of threading generators so you can't accidentally use the wrong one.
Do you have an elevator pitch beyond it is a Haskell version of xournal. I'm curious why I'd want to use this software. Thanks, it looks like you've put a lot of great work into it.
A Haskell-like type system does not preclude the ability of browser developers to add new APIs. And, a type-system would only enhance the ability for developers to query the API. There is nothing that would require browser developers to register their extensions with any standards committee before adding them. I am not surprised that initiatives to make JavaScript stricter have failed -- you can not simply bolt a type-system onto JavaScript as an after thought and get something really great. You suggest that a browser can not halt simply because something did not type-check. But, in fact, JavaScript does in many cases halt when it runs into a type error. The biggest difference is that the browser does not halt until it actually runs the line that has the type error. But we can already do that in Haskell with the [-fdefer-type-errors](https://www.haskell.org/platform/doc/2014.2.0.0/ghc/users_guide/defer-type-errors.html) flag. There seems to be an implicit suggestion that in a dynamic language broken code would somehow work better than in a strongly typed language. But, calling non-existent functions or using values as the wrong type is obviously going to fail even in a dynamically typed language. So I don't see how being dynamically typed magically solves anything. Hot code loading, reloading, and swapping are all quite possible in Haskell as well. Admitted the support libraries for that are underdeveloped at the moment -- but it is not a technical limitation of the language. It's also not clear that all-async mode is better performing or easier to understand than things like STM and lightweight threads. Since, until recently, Javascript has faced ZERO competition in the web browser, it is really difficult to suggest that it is a great solution. Only that programmers are smart enough to work around it. The plethora of languages that now compile to Javascript is, perhaps, an indication that people are not very satisfied with Javascript. There are languages new languages like TypeScript that are designed specifically to target JavaScript as well has code generators for other languages like ML and Haskell with target JavaScript.
According to the [Cabal User Guide](https://www.haskell.org/cabal/users-guide/developing-packages.html), section "Modules included in the Package" you need to add an other-modules field with entries for those modules. Or at least that is my best guess on what is wrong. I never uploaded anything to Hackage myself so far.
How would you query if an object has a property in a statically typed language? Using a Dictionary/Map is not the ideal solution. So if you want to use a strict type, you have to follow that strict interface. I may be missing something, but Haskell (by itself) has no run-time type information, neither code inspection. Erlang has no static type enforcement exactly because its dynamic nature of code reloading/swaping/etc. This is really hard to do in static typing systems. Lightweight threads are a huge win compared to all-async, the thing is that JavaScript solved it in a way that is confortable for the programmers and the browsers. It's way easier to reason about monothread running code and browsers are more safe this way. Callback hell can be daunting, tough. As I said, I'm not proposing or stating that JavaScript is pure gold, it's just a language that passed the trials of time. People are disatisfied with JavaScript and I see their point, that's why I'm looking after some works on how to join the good parts of these words.
Seems that way, but every time I add it, `cabal check` fails. I'm updating my system + cabal and will try this again.
You could just add a library to your package and add them as exposed-modules in there?
Wow, this looks seriously awesome.. Sorry for dumb question, but ... what is the hardware used in this demo? I was thinking using something like this to put some hand-made drawings to blog posts etc. but I have no idea where to start. I need a device for drawing that is 1) not very expensive 2) very well supported in Linux.
Okay I'll give that a try. Going to get some lunch and will havebit up in 45 minutes or so.
Not *every* time. The [Klop fixed-point combinator](https://en.wikipedia.org/wiki/Jan_Willem_Klop) is a huge wall-of-text, but it's hard to say that it's *wrong*.
Here's the ticket: https://ghc.haskell.org/trac/ghc/ticket/3919 "Milestone set to bottom" - I like that comment. Can anyone share whether the ticket makes sense still? In particular, that TH doesn't care for ViewPatterns is still a blockage?
Personally, I rarely find it worthwhile to use TH or some extra library dependency just to avoid writing isRed Red = True isRed _ = False
It might be possible for technical writers to use this tool for the opposite purpose. Technical writers are *required* to use the same word over and over again for the same meaning, for preciseness. If the percentage of repeated words is not high enough, it is probably bad technical writing.
Why would a very experienced Haskeller have a lot of trouble with F#? Why would a C++ programmer take time understanding foreach, the C++ language has at least four ways of doing that (one standard one is even called for_each). For that matter, a complex language like C++ supports a number of different "paradigms", whether by actual syntax and semantics or by convention. You can write a very imperative program in Haskell. Are you sure you aren't "putting the cart before the horse"? 
I'm adding less than 10 instructions on a bunch of slow paths on one OS to eliminate about half the errors related to the network library. I consider that a very good trade-off! I'm sure with one well chosen INLINE pragma or compiler flag tweak somewhere we could make gains far greater than that - the cost really is sufficiently negligible I'm not sure we could ever measure it.
Yes, although someone more type theoretic than I might have something to say about `let` generalization.
We can easily query if a type has a constructor in Haskell using the Data class: Prelude Data.Data&gt; let dt = dataTypeOf (undefined :: Either Int Int) in (readConstr dt "Right", readConstr dt "Left", readConstr dt "Bork") (Just Right,Just Left,Nothing) What is currently missing is the ability to avoid type-checking a block of code if a symbol/constructor/etc does not exist. We'd like to be able to do something like: ifExists ''Module.someFunction then &lt;&lt;code that uses Module.someFunction&gt;&gt; fi In theory, it should be possible to do that using Template Haskell. Though I would expect some ugliness as a result. GHC already has the ability to do dynamic recompilation and reloading via the GHC API. It also has the ability to defer type checking errors to runtime. It also has some introspection abilities via Template Haskell, as well as the Data, Typeable, and Generics libraries. What is definitely true is that these pieces are not well integrated. Tying those pieces together is quite possibly where a master thesis might lie. I think your insistence that a 'dynamic' system which supports hot code swapping, etc, must have a 'dynamic' type system is simply not true. It is true that certain errors can not happen until runtime and therefore must be handled at runtime. But that can easily happen with in the context of a strongly typed system. In fact, in a strongly typed system it is likely that potential points of runtime failure could be identified at compile time and the developer can add runtime checks for those potential failures. In a dynamic system you can write: module.potentially_non_existent_symbol and not even realize it is not always there. In a strongly type system, you would have already provided proof that the symbol exists such as the `ifExists` operator shown above. 
Are there common functors where `pure f &lt;*&gt; x` is significantly more expensive than `fmap f x`?
Isn't it: strandom = state random ?
&gt; But can you even have church encoded integers in STLC, wouldn't each and every integer end up with a different type? In that case, what is the type of the division function? Why is that? If we're talking about naturals, they have type `(a -&gt; a) -&gt; (a -&gt; a)` (for any type `a`). If you want full integers, tuple that with a boolean.
Thanks! ReactNative looks interesting. 
Nice, I can use this in my sound synthesis library to generate envelopes. +1
For me, the biggest feature that I love is linking notes. You can make a link to other hoodle document and web URL as well by simple drag and drop. I am using hoodle for my everyday work (theoretical physics) for calculation, note-taking during seminar, memo, etc... so already have about ~1100 documents.. navigating notes by simple link is really helpful. In my personal sync web service, I do that even on web browser.. There are many small features that are quite customized to my purpose.. maybe that's useful to others. 
 Sure, even just `[]` pays a reasonable price there. It'd probably be more interesting to categorize the ones that get away scot-free. GHC probably won't be smart enough to figure out that the left list is a singleton and that it can unroll the entire (&lt;*&gt;), so you build up a messy result that is way more complex internally than it needs to be. Even State is going to wind up doing an extra pattern match in the result that probably won't fuse away.
No, Nat division... church encoded nats on the STLC have the type `(t → t) → t → t`, except for zero, which has the type `t → t1 → t1`... no? I guess I'm misunderstanding something here, nevermind anyway.
It doesn't. That's why I didn't say `forall a. (a -&gt; a) -&gt; (a -&gt; a)`. Nevertheless, any church numeral can be assigned type `(a -&gt; a) -&gt; (a -&gt; a)` for any type `a`. The "for any" here is quantification in the meta-language and doesn't belong to STLC. This is called a type scheme.
The PureScript community has no shortage of fun projects, and we have developers able and willing to mentor any interested students. Any contributions will have a very real impact on development of production PureScript code. The community has been voting on project ideas, and I've moved some of the best over to the wiki [here](https://github.com/purescript/purescript/wiki/GSOC-2015-Projects).
Damn nevermind. Yea I somehow missed `t → t1 → t1`could be `(t → t) → t → t`. So, could there be a better than that gigantic division implementation, right? I wonder if there is any way to figure it out, I'm really out of ideas and couldn't find it at all. Maybe genprog? 
Wadler hints at this in his [monad paper](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf). He writes an expression evaluator in the Identity monad, and then fiddles with the monad to show how easy it is to make the evaluator throw exceptions, carry state, have output as a side effect, etc.
Okay, I'm really sorry about the wait. I live in the 3rd world (Mississippi) and am having some electrical issues at my house today. I uploaded v.0.2.0.4 and the tarball actually includes the module files. My main concerns with uploading it as a library were that 1) I'm a novice and the code may not really be library-quality. 2) Who the hell would use this as a library? haha Thanks for the hint, hope it works for you this time!
This is really cool, thank you for that insight. I was already thinking of a whitelist feature. If you think it would be useful, I'm sure it wouldn't be a problem to add it.
Hmm, I think that giant implementation is *also* using untyped lambda calculus. And it's integer division, not nat division. How does your implementation of division work, by the way? I'm stuck at making a well-typed implementation for pred :(
It works by using cyclic lists with a "rotate" operation that transforms [1,2,3] into [3,1,2]. So I just keep rotating the list and calling succ on head. I do that "divisor" times on a zeroed list with length "dividend", and then I get the head. I = (λ (a) a) T = (λ (a b) a) F = (λ (a b) b) app = (λ (f t) (t f)) not = (λ (p a b) (p b a)) succ = (λ (n f x) (f (n f x))) t2 = (λ (a b t) (t a b)) t2_1 = (app T) t2_2 = (app F) cycle_append = (λ (head tail last) (t2 head (tail last))) cycle_rotate = (λ (cycle last) (t2_2 (cycle (cycle_append (t2_1 (cycle n0)) I last)))) cycle_head = (λ (cycle) (t2_1 (cycle I))) cycle_tail = (λ (cycle last) (t2_2 (cycle last))) cycle_apply = (λ (f cycle) (cycle_append (f (cycle_head cycle)) (cycle_tail cycle))) cycle_replicate = (λ (times value) (times (cycle_append value) I)) cycle_succ = (λ (cycle) (cycle_rotate (cycle_apply succ cycle))) cycle_num = (λ (n) (cycle_replicate n n0)) cycle = (cycle_replicate n3 n0) div = (λ (a b) (cycle_head (a cycle_succ (cycle_num b)))) I also have a "half" combinator that halves a number and is simpler, since it doesn't use the cyclic lists: half = (λ (((0 (λ ((0 (λ (λ (λ ((0 (λ (λ (1 ((4 1) 0))))) 1))))) (λ (λ (λ ((0 1) 2))))))) (λ ((0 (λ (λ 0))) (λ (λ 0))))) (λ (λ 1)))) I wonder if this one is typeable? 
No, roche means that Church numerals are well typed in the STLC as `(T -&gt; T) -&gt; T -&gt; T`, no matter which particular type `T` you pick. `T` could be a primitive type (e.g., `Bool`, if that's a primitive type in your implementation), or it could be a more complex type built up from the typing rules (e.g., ` Bool -&gt; Bool`). Or whatever. Any type works. When working in the STLC, you sometimes introduce so-called "abstract types" for this sort of thing; you won't ever need any values of type `T` when working with Church numerals typed as above, so for theoretical purposes you can just say, "OK, we have this abstract type `T` that we'll use here." You can think of `T` as being uninhabited, or as comprising some number of unspecified lambda terms, if you want. It doesn't matter because you won't ever use the values. So with abstract types, you can work with a version of the STLC that doesn't have any primitive types—types with values provided by the implementation—at all if you want. Then it's lambda terms all the way down (at the value level), just like in the untyped calculus.
Yes, this is the process of concluding theory from practice. The other way around will be applying the theory into practice. When you know what is OOP, and what is good or bad in OOP, you tell others what OOP should be like. This is why a paradigm is come up with. Isn't it?
Yes, yes it is. Thank you.
I'm trying to share :). Duncan suggested I raise the ticket to see if platform can take the best ideas from MinGHC. 
Duh! How did I not think of this?! Thank you.
I think the derive library might be what you are thinking of. 
Huh, interesting. So how does the desugaring process work?
There is a race condition if two threads call `initWinsock` concurrently... and I'm not sure that `atexit` works properly if this code is being executed from a library used from a non-Haskell application, or a dynamically loaded (and unloaded) Haskell module.
Hey that's me.
What relationship to wxHaskell do you envisage?
Edit: Yeah, whatever. I can live with it.
Another way of saying this is, "There are plenty of types of church numerals". I'm saying: for any type a, for any church numeral c, c has type `(a -&gt; a) -&gt; (a -&gt; a)`. You seem to be interpreting this as: for any numeral c there exists a type a such that c has type `(a -&gt; a) -&gt; (a -&gt; a)`. The latter is also true, but is a far weaker version of the former. Also, what ericpashman said.
This doesn't convey Haskell as lousy, he's just telling the truth. The lack of global mutable state is supposed to be a feature.
But what is the state you are threading around in this case? Does your code compile? I can't mentally make it type check given what I guess are the signatures involved.
`state (random g)` still doesn't type-check in my head. Once you make it compile, you'll probably arrive at something close to randomPos = runState $ do x &lt;- state random y &lt;- state random return (Position (y, x)) which is what I suggest in my comment further down.
Sure, I wasn't really trying to provide working code (I know... not great..) but provide a hint as to how this could be done with a normal monad - no need to override syntax or anything like that.
GHC now supports typechecker plugins, something that, I think, could be very nice for better "aftermarket" type error messages.
Thank you! That is actually the very one that I was trying to remember the name of when I found `is`.
TIL `do do do do do do do 42` is valid Haskell.
`Enum` instance is a bit ugly for `Float`, see here: http://stackoverflow.com/questions/7290438/haskell-ranges-and-floats
Here's an old discussion I found when I first noticed this behavior: http://code.haskell.org/~dons/haskell-1990-2000/msg07279.html specifically: http://code.haskell.org/~dons/haskell-1990-2000/msg07289.html tl/dr: to be sure that the `To` part of `enumFromThenTo` is included (using the generic `numericEnumFromThenTo`)
Where exactly is the race condition? In the Haskell, the C in this module, or in the underlying WinSock? I can see 2 race conditions, but in neither case I see a way to mismatch calls to WSAStartup/WSACleanup, or to cause actual harm. Agreed, atexit for freeing resources is mostly pointless, as the resources get closed at exit anyway, and not if a Haskell dll is unloaded etc. But that's the way the network library has always worked, so I didn't want to change it. If I'd been writing it from scratch I would have skipped WSACleanup entirely, it probably only saves 2Kb or some other irrelevant amount anyway.
Clever! And how do you compile it down to STLC? It sounds like it should work. Meanwhile, still stuck on pred, I'm slowly realizing that the fact that the STLC doesn't have polymorphism like in System F means that we must be careful not to use the same value twice, as it is likely that it will be needed at a different type. For example, suppose I want to implement the following Haskell function in STLC: foo :: Bool -&gt; ((Bool, Bool), Int) foo b = ( if b then (True, False) else (False, True) , if b then 0 else 6 ) (ignore the fact that we could easily implement a version of foo which only uses `b` once) Even though `foo` is not itself polymorphic, the church encoding of `Bool` is: foo :: (forall a. a -&gt; a -&gt; a) -&gt; ((Bool, Bool), Int) foo b = ( b (True, False) (False, True) , b 0 6 ) And that's a problem if we want to translate this expression to STLC, because our boolean argument can either have type `((Bool, Bool) -&gt; (Bool, Bool) -&gt; (Bool, Bool))`, or type `(Int -&gt; Int -&gt; Int)`, but not both. So this is not a proper STLC translation of foo: foo = (λ (b) (t2 (b (t2 T F) (t2 F T)) (b n0 n6))) Instead, it seems we have to do each if operation one at a time. Each time except the last, instead of returning our results directly in each branch, we need to return a pair consisting of the result and a fresh copy of the bool, to be used in our next if operation. Like this: foo = (λ (b) (b (t2 (t2 T F) T) (t2 (t2 T F) F)) (λ (tfAndBool) tfAndBool (λ (tf b') (t2 tf (b' n0 n6)))) 
A bug by definition is behaviour that deviates from documentation, where a program does something it says it won't. A standard always does what it says it does, it can't have a bug. It can only have a design flaw.
I think it's s bug in the standard. :)
&gt; […] and that made me ask why we only have do notation for monads. That’s not really true. `do` also works for *arrow* do notation: proc x -&gt; do a &lt;- f -&lt; x b &lt;- g -&lt; x returnA -&lt; a + b Since a function is an arrow, you can use that, I guess.
I don't know -- does wxHaskell have a GSoC worth of work to be done on it? Then maybe that's the right thing :-)
I'm interested in contributing to the numerical haskell tooling after the GSOC timeline. I unfortunately cannot join the program this year as I'm working. Apart from linear algebra tools, what else do you have in mind? Any statistical packages à la scikit or HLearn? I started a "typesafe" linear algebra library in C++, but I'm not sure about what sort of requirements you have in terms of performance for the Haskell one. How does hblas compare to existing projects like [hmatrix](https://hackage.haskell.org/package/hmatrix)? Can you please detail some of the issues with the API and performance? 
What are your problems with the OpenGL support? I think Haskell's OpenGL support is excellent, it is OpenGL itself whose design is very poor (and in particular, not a good fit for a functional language).
Misread the code my first scan through, looks like it's fine.
&gt; List is a functor That's sort of like saying "the Integers are a monoid", at least until you look deeper. `List` maps types to types, so it's only the object mapping half of a functor. It also needs the `(a -&gt; b) -&gt; List a -&gt; List b` part that maps morphisms, and `map` provides that. But, just like there are at *least* four law-abiding Monoid instances for the Integers, maybe there's a lot of law-abiding Functor instances for List? `const $ const []` also makes a functor with `List`. It's rather useless, but it does preserve composition. Due to parametricity, we know that (ignoring _|_s) those are the only two such functions that can be paired with the List type constructor to result in a functor! So, as long as you are comfortable with excluding "silly" morphism maps (or empty Lists!), then yes. "List is a functor".
&gt; The first answer was basically a screed about how lousy Haskell is for not having static initializers, what a bad language, every other language has solved this problem, etc. etc. There's no such thing as global mutable state. Haskell gets this right, and we only need to emulate linker-scope mutable state to make up for the fact that other languages do this aspect of API design so badly. In this particular case I'd argue that Windows is closer to doing it right, except that intialising winsock ought to return a context object, and then every other network resource allocation (e.g. making a socket) ought to take that networking context object. Unix fudges this by putting the context into the kernel directly and thus making it appear like a process-local mutable. One way to look at it is, how would a Haskell-OS do it, or something sensible like a L4 microkernel.
No, that would lead to code of unknowable bug-ed-ness. The way to have bug-free code is to standardise on whatever the code does.
Robert Harper has a book called *Practical Foundations for Programming Languages*. You can find a preview [here](http://www.cs.cmu.edu/~rwh/plbook/book.pdf).
"Write Yourself A Scheme": http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours
Very glad that you appreciate it.
Also known as "reference implementation".
&gt; A bug by definition is behaviour that deviates from documentation That's an *interesting* definition. I prefer the one from [wiktionary](https://en.wiktionary.org/wiki/bug#Noun): "4. A problem that needs fixing, especially in computing." &gt; it can't have a bug. It can only have a design flaw. I would consider design flaws to be bugs. I do consider the standard buggy, in particular here. But, I'd also consider it a defect to *not* follow the standard if your compiler claims to be "a Haskell compiler".
&gt; For endofunctors defined in haskell though, the function on morphisms is unique... What do you mean by "unique"? In general, for a family of endofunctors which have "uniquely determined" the map on morphisms, it consists of a single endofunctor. We can see this easily with the identity functor, for example. &gt; The category whose objects are of the form `[a]` and where the morphisms are any function is a full subcategory of **Hask** and so the pair `[], map` is a functor from **Hask** to this subcategory. Indeed, this is true for any endofunctor. This I understand, and appreciate your clarification on this. I suppose I just find it mildly counter-intuitive that the `functor` typeclass really refers to only *part* of a functor (i.e., how it behaves on morphisms) and that *the subcategory* is an instance of `functor`.
I wonder if those restrictions mean something about the nature of STLC programs. 
Got me thinking. Why isn't (..) a function? 1..n = [1,2,3,4..n]. No need for sugar 
To support the three different syntaxes: `a .. b`, `a, c .. b` and `a ..`.
Around here, people consider most of PHP to be broken beyond repair.
Agreed, but that in no way answers the quaestion.
How does this relate to the cloudhaskell (typed channels)?
There's also docopt port: https://github.com/docopt/docopt.hs
a less ambiguous definition isn't always a correct or more accurate or more accepted one :) 
presumably layouting rules help a lot with if/then/else clauses. writing branches with do notation and multi-line stuff etc. is tough when you don't have layouting from if/then/else and case switches to help
The `if then else` syntax actually pretty much has no layouting rules, just like a normal infix. Like this: if True then 1 else 2 Is legal. There is no rule that says the `then` and `else` need to be indented deeper. Likewise. If we were to define `(x ? y) z = if x then y else z` then: (x ? y $ z) Would also be completely legal.
Been in `base` since forever, been the gnu standard for forever... and both for good reason. It may not be the most idiomatic library, but there's not a lot of nonsense and your command lines will be dead predictable to anybody who hasn't been under a rock for the last 30 years. Strongly recommended.
Sure, that would be (the proven part of) [the Church-Turing Thesis](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis): &gt; * [...] A function on the natural numbers is called λ-computable if the corresponding function on the Church numerals can be represented by a term of the λ-calculus. &gt; &gt; * [...] a function on the natural numbers is called Turing computable if some Turing machine computes the corresponding function on encoded natural numbers. &gt; &gt; **Church and Turing proved that [...] a function is λ-computable if and only if it is Turing computable**
It's always annoying when hackage doesn't document the things.
I've used [parseargs](https://hackage.haskell.org/package/parseargs) for a couple of projects. It's worked well for me. I haven't tried the others so I can't provide a comparison.
CmdArgs is excellent if you are building a fairly large interface and want good maintainable code. I always feel like there is too much boiler plate using it for small one-offs though.
I am not sure at all how this paper disagrees with me, it seems to _agree_ with me that exceptions screw up the notion of a single bottom value. It also seems to date from a time _before_ Haskell got exceptions where the theory of a singular bottom value does apply. &gt; No lazy functional programming language has so far supported exceptions, for two apparently persuasive reasons. Firstly, lazy evaluation scrambles control flow. &gt; Evaluation is demand-driven; that is, an expression is evaluated only when its value is required [14]. As a result, programs don't have a readily-predictable control flow; the only productive way to think about an expression is to consider the value it computes, not the way in which the value is computed. Since exceptions are typically explained in terms of changes in control flow, exceptions and lazy evaluation do not appear very compatible
Is it really that vague? you argued for one definition being better than another for a concept that already has widely accepted meaning...simply on the fact that it was more specific. I can make an even more specific definition than the one you presented.... would that make it even better?
Cool. Note that the hblas and numerical repos are different :) On the hblas repo, the pull requests by archblob are good examples of high quality contribs. 
Wonderful. This sort of service is coming just in time as parts are getting so complicated and there's becoming less and less incentive to produce hand-solderable parts.
The paper is what introduced to Haskell exceptions precisely the semantics that you appear to be complaining about. You're quoting from the section where they _introduce_ the problem. What matters is if you understand how they _solve_ it.
The answer to this question varies hugely from person to person. I've seen people pick up Haskell very quickly, and I've seen others take much longer but still get to a pretty solid level competency. So in that sense this question is unanswerable without knowing more about you. But in a different sense, compared to other languages, I would say that effective web development in Haskell demands a fairly significant amount of Haskell knowledge. If you've only finished chapter 2 in Learn You a Haskell, then you won't have enough knowledge to do substantial web development. Should that stop you? Not necessarily. Some people can look at a few examples and blindly copy the patterns they see there and get some basic web development things working. But Haskell is so different from other languages that you're not going to have a clue about what's really going on under the hood that way. You need to have a basic familiarity with monads because web servers are usually doing lots of monadic operations. Most of the web frameworks probably have some kind of monad transformer stack, so some familiarity with monad transformers is helpful. The list could go on and on. In short, I would say that pretty much *everything* covered in Learn You a Haskell is needed for real-world web development. And from what I have observed, even then you will still be missing some things. But by that point you will have a much better ability to help yourself and understand the answers you get to the questions that you do have.
Interesting! I hadn't seen this. I'd love to build something that takes the best of both and is general enough for people to really embrace the idea and start defining syntaxes/grammars on top of all of the libraries (binary would be really useful). There are some crusty things in roundtrip and partial-isomorphisms that could be left behind.
When learning other languages like say Python or Lua, it's mostly syntax and standard library differences. Most of the concepts and "ways to do stuff" are very similar. Whereas in Haskell you basically have to re-learn how to do even basic things, because it works so differently. Also I'd recommend learning Haskell from scratch rather than jumping right into something like yesod. After getting over the basic syntax barrier Haskell code can be deceptively easy to read. As to whether or not you should finish your book first, I doubt that it matters either way.
Yes, the paper is indeed a great read! However, 1. he does not show how to combine monads (i.e., to have both exception handling and state, for example). 2. he only points out that the program written using the Identity monad can be simplified to the original program Today we have monad transformers and what I was suggesting was that every program will be written with Identity monad by default (it could even be "hidden" be the compiler, that is, you write as usual but the identity is always there thus allowing you to use the `do` notation). Which is why one can then naturally stack up monad transformers on top, without much problem. Again, maybe even these transformers could be hidden as well - the compiler could add them when you use their methods (e.g., raise an exception). This is all speculation as I haven't written any code this way :-) Just a mental exercise. 
I think writing code with type `Identity a` is too restrictive. The best way to get code reuse across different monads is to use typeclasses like MonadState, MonadReader, etc. If you don't need anything, just make it completely polymorphic in the choice of monad. There's basically no advantage to giving something type `Identity a` instead of `Monad m =&gt; m a`. You can always instatiate the latter and run with `runIdentity`.
&gt; Well, the idea is that we treat the single lowest point of our domain lattice -- bottom -- as potentially containing a set of information. The paper makes this clear enough. See section 4.1, on page 7. It does, but I don't see how this contradicts my contention that exceptions destroy the idea of modelling partial functions as simply returning a single special unique value whose propagation defines strictness. If anything, it affirms it firmly that the old model of the single unique value called "bottom" no longer holds. I also don't buy the idea of "Okay, so catching exceptions can only happen in the IO Monad so it's. 'okay'.", I mean, define 'okay'. At any rate, this added complexity to the language does add a fundamentally new dimension to it. Does it make the language type-unsafe? No, not that I know at least. But it does remove certain assumptions that could be used to reason about programs that no longer exist. Even within the IO Monad, without exceptions you can always just replace a pure expression with its one deterministic value. _Even if_ that value is "bottom" to reason about it. Now that a "class" of Bottom-values has been introduced, for which the term "Bottom" is strictly speaking a misnomer. In the event a function evaluates to one of those. You cannot just reason about the program any more by replacing an expression with its value, they become distinct. So depending on what you call "okay", I wouldn't say it's "okay". I also just on a general level object to the existence of exceptions. The old system of the Maybe and Either monads worked fine and exceptions have many of the same difficulties as gotos, but that's another matter altogither. &gt; As the paper notes in the conclusion, btw, the semantics it describes for getException and raise were in fact implemented in GHC 4.0. It has remained in GHC, with a few tweaks along the way, ever since. Hmm, that's possible, I never used it, when I speak of Haskell exceptions I mean the de facto standard of Control.Exception
There are many people in the Yesod community who have learned both Haskell and Yesod at the same time, and have become quite proficient at both. In many cases, they've even become contributors. If your plan is to spend 6+ hours a day doing this, I think you'll have no problem picking it up. When you're ready to dig into Yesod, I'd recommend going through [the book](http://www.yesodweb.com/book). You should also pay attention to [the community page](http://www.yesodweb.com/page/community), which will give you great avenues for being in touch with the rest of the community. My biggest advice is not to become intimidated. Many people have made the journey from Ruby (and others) to Haskell. If you email the Yesod mailing list, I'm sure they'll be happy to share their experiences. And a side note to the comments in this thread: if you're allergic to Template Haskell and metaprogramming, and/or you want to understand what's going on under the surface, there's [an entire chapter devoted to that](http://www.yesodweb.com/book/yesod-for-haskellers). This myth that you can't use Yesod without TH, or that somehow it's impossible to understand what's going on, needs to be put to bed. __EDIT__ Just to be clear: I think 6+ hours is a very generous amount of time to devote to learning the toolset, and will get you there quickly. I think many people have devoted far less time (1-2 hours a day) and gotten proficient in a decent time period also. Given that I never actually learnt Yesod (for obvious reasons), I'm about the worst person to judge how long it takes to learn it, but people on the mailing list will be well equipped to answer such questions.
&gt; So depending on what you call "okay", I wouldn't say it's "okay". Luckily the paper gives a very clear discussion of what is "okay" and for what reasons, and the description agrees with the "gold standard" of what "okay" should mean in terms of a sound denotational semantics. &gt; Even within the IO Monad, without exceptions you can always just replace a pure expression with its one deterministic value. We still can do that. And when we do so we obviously aren't "within the IO Monad" since that definitionally means we're not working just with a "pure expression" anymore. The rule introduced by the paper is simple -- we define the deterministic denotation of an expression which has multiple bottoms to be the set-union of all of them. When we _observe_ the bottom -- which we can _only_ do through an action with a result in `IO` -- we only provide a mechanism to, nondeterministically observe _an element_ of that set. It is exactly that "imprecise" trick that _does_ preserve our ability to reason about programs by replacing like-for-like. With regards to the way exceptions function today, I maintain that they have essentially the same semantics as described in the paper and implemented in GHC 4.0. Since then we have added an extensible hierarchy, fundamentally as a library on top, and also a whole bunch of other work to further account for asynchronous exceptions. But the imprecise semantics are still what the paper describes, and I don't know why you think otherwise.
Anything in particular? I know that IsoFunctor can be expressed as a CFunctor from ekmett's categories package. Beyond that I've been wanting to find a nicer representation of the applicative and anything else.
That is more a fault of C++ with its name mangling issues and the fact that half of C++ features (e.g. templates or subtyping) are not usable from any other language because they rely on having a C++ compiler available. 
An example/tutorial would be nice.
I'm actually really surprised to hear that (and appreciative of the feedback). One of the biggest complaints I get about Yesod documentation is how little it recommends the scaffolding. The majority of the book doesn't touch the scaffolding at all, for example. I'm happy to make these changes, but I'd appreciate recommendations on where to do it. Are you thinking about the quickstart guide, the output of `yesod init`, or something else? Also, we recently added a "minimal" scaffolding that may help address this concern as well.
Your `Iso` looks very much like `Iso Maybe`, and `IsoFunctor f` like `Semifunctor f (Iso Maybe) (-&gt;)` were my main observations. EDIT: Discussion below is quite interesting! But `Iso Maybe` is completely wrong due to my inability to read. It's vaguely like `Iso (Kleisli Maybe)` but that's getting a bit awful.
I just want to thank everybody involved in one way or another with the haskell emacs tooling. It is one of the best language experiences I had in emacs.
One of the authors here. I'll try to answer the questions: It is a different approach to writing distributed programs than those based on explicit message passing like Cloud Haskell, Erlang, or MPI. We're not arguing that those are a bad way to write programs (they are popular and successful after all), just that they use a somewhat low-level language construct, at times reminiscent of programming with unstructured control-flow. We propose to use a model based on RPCs that are tightly integrated in the language, or ``native''. This idea is similar to how RPyC and Obliq work. What's called symmetry in the RPyC project, for instance, looks like what we are after in supporting higher-order RPCs. It's possible that our formalisation, with some modifications, could act as a core language also for them. Since symmetry is not trivial to get right (from our experience) it brings peace of mind to have formalised at least part of it. The focus here is expressivity and correctness. To be as general as possible we show how to do higher-order remote calls without sending code at all (if necessary --- we also have `ubiquitous' functions, which may for example be standard library functions, that can freely move between nodes). This is because it's not possible in general to transport a piece of compiled code between nodes because they may not be binary compatible or not have the same resources available locally. Instead a function call is reduced to a communication protocol when required.
Any good resources on setting up a solid haskell dev environment in EMACS with evil-mode?
&gt; Your `Iso` looks very much like `Iso` Maybe Are you sure? Looking at the definition for [`Iso`](https://hackage.haskell.org/package/semigroupoids-4.3/docs/Data-Isomorphism.html#t:Iso), it doesn't look like `Iso Maybe` is well-kinded, since the `k` is applied to two type arguments. Or maybe you're saying that the type `Iso a b` from [partial-isomorphisms](http://hackage.haskell.org/package/partial-isomorphisms) corresponds to the type `Iso (-&gt;) (Maybe a) (Maybe b)` from semigroupoids? In which case, the two definitions do look similar, but I'm concerned that their meaning might be very different. In partial-isomorphism, the `Maybe` indicates that the conversion may fail, so multiple values of type `Maybe a` can map to `Nothing :: Maybe b`. If `Iso (-&gt;) (Maybe a) (Maybe b)` represents a total isomorphism between `Maybe a` and `Maybe b` (does it? the documentation doesn't state any laws), then I would expect exactly one value from `Maybe a` to map to `Nothing :: Maybe b`. This discussion makes me realize that having a `Maybe` on both sides isn't ideal for representing parsing and pretty-printing, because pretty-printing shouldn't fail. Is there an official name for a "half-partial isomorphism", that is, an isomorphism which may only fail in one direction? -- parse . unparse = Just data HalfPartialIsomorphism a b = HalfPartialIsomorphism { parse :: a -&gt; Maybe b , unparse :: b -&gt; a }
Edit: now that I've read your post till the end, you want something like this: getFirst [] = Nothing getFirst xs = foo (head xs) (tail xs) foo m [] = Just m foo m (x : xs) | x &gt; m = foo x xs | otherwise = foo m xs But rather than do recursion by hand, it's better to use a fold: getFirst [] = Nothing getFirst xs = Just $ foldl' max (head xs) (tail xs) Or if you insist on using your `trans` function, this is easy to rewrite to getFirst [] = Nothing getFirst xs = Just $ foldl' (\x y -&gt; if trans y &gt; trans x then y else x) (head xs) (tail xs)
I'm one of the guys working on the Haskell layer for spacemacs, and it seems to be working decently so far. If you have any comments or suggestions please do tell, we need all the feedback we can get!
You can use [argmax](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/list-extras/Data-List-Extras-Argmax.html#v:argmax) from list-extras: argmax trans ['a', 'b', 'c', 'd']
Note: `argmax` has the same type as `maximumBy . comparing`, but it looks like `argmax` has different semantics: it returns the *first* maximum element instead of the *last*, which is just what OP wanted. (And arguably maybe more sane? It seems like that's what a lot of other languages' standard libraries do.)
That looks really awesome. I'll check it out
I hadn't known about `maximumBy`, and would have suggested doing this instead: getBiggestTrans values = snd . maximum . zip (map trans values) $ values edit: It looks like `maximum` takes the last item among ties, and since you want the first, we'll need to add another field to break ties the way we want: getBiggestTrans values = (\(_,_,x) -&gt; x) . maximum . zip3 (map trans values) [0,-1..] $ values I could swear there was a pre-made function to get the last item of a triple, but I can't find it on Hoogle right now.
Isn’t this basically `find` from `Data.List`?
I'd rather see a list of differences from other *n* similar libraries
Err, Why not just use List as a monad to get the same effect? I mean that's just what using List for recursive descent parsers does, no? If you need state and things, there's always ListT (or one of many implementations thereof).
/u/levischuck, done ;)
Updated the description.
Thanks!
I love the idea of having monthly project updates in this format. I might steal that for some of the OSS projects I'm involved in. :)
The notion of monoids as categories of a single object with arrows as monoid elements doesn't jive with intuition for me. I can see how it fits all the definitions, but what is the object? It kills me. Take the integers with (+) and 0 for example. So 0, 1, 2... whatever are all elements of the monoid that can be (+)'d together to form new arrows. But what are we using these arrows for if not to go between objects? All we can do is go back to our one object and then what's the point? In what is likely a similar vein, I didn't quite draw the analogy between functors and monoids as the embodiment of strongly- and weakly-typed composability. I kind of see it for functors, actually. But how are monoids modeling weakly-typed composition?
is there a readme or something?
Yep, we initially built one of these on top of "prism" and it does indeed work fine when your pretty printer can't fail. This isn't always the case, though. The other issue with prisms that is that we wanted nice error messages back out. This seems to be surmountable but partial-isos has a bunch of annotation related code built in and we end up getting really nice errors. Also, if you read the paper (Invertible Syntax Description), you'll find some more neat things like being able to build fold/unfolds out of partial isos.
To be honest I'm not sure how this could have been avoided. A few thoughts that come to mind: - "Multiple ways to setup a Yesod project" tutorial that users who want more context can click on - "What is all this scaffolding for?" tutorial that links to a tutorial giving a tl;dr and an in depth look at what each part of the scaffolding exists for. When I was evaluating (and when I evaluate new web frameworks) the one I can understand the soonest definitely gets precedence, so I usually lean towards the minimal side. After using the bare bones version without so much scaffolding I sometimes see more value in using the scaffolded version and switch to it. I haven't put enough time into Yesod yet to have an informed opinion, so as I said above I lean towards the minimal scaffolding by default.
&gt; For instance I know there are a few "single file" yesod app gists floating around, seeing what that looks like would have helped me with Yesod in the beginning I think First of all apologies if the Yesod tutorial already does this, I haven't checked it out in some time. I would do so now, but I'm a couple hours late for lunch ;) Maybe changing the current tutorial to use a single file Yesod app in the beginning then showing how things get unwieldy and refactoring into the scaffolded version would be helpful?
I do not know how to use the list monad for backtracking. I know how to use it for indeterminism, but using it for backtracking would be something really interesting. Can you elaborate it a bit more? This problem (undoing IO actions) is not similar to the one of the example here that I think is what you have in your mind: http://www.randomhacks.net/2007/03/12/monads-in-15-minutes/ Here a combination of posible answers are filtered against the guard condition. Really in that example there is no backtracking, but indeterminism. It is a list monad that compute all the responses "at the same time". Only the ones that pass the guard condition are shown. As always to remove the magic and understand what is going on, the best thing is to desugar everything. In this case substitution the bind by `concatMap`. You will see that it is ordinary list processing going on there. Lazy evaluation in fact makes the backtracking simulation a bit more real, since the list is created on demand, and the guard forces the evaluation of each element of the list by executing the processing of 'choose', but this is no different from how any other monad execute lazy statements. That is not privative of the List monad. Laziness assures that every pure computation implement "backtracking in some way", since it forces the execution of unevaluated expressions coming from statements above them. Is the indeterministic nature of the List monad the effect that permits to simulate the execution of different alternatives. To summarize, indeterminism can simulate backtracking in some problems like this, where a tree of combinations are explored. A deterministic program would need either execute explicit loops or do true backtracking to solve the problem of the above link, but in the case of my article, there is no tree of alternatives to explore and the list monad can do nothing. The backtracking on my article is different; While in the case of the list it explores alternatives simulating being restarted from the same point again and again choosing of another alternative each time (it is not that, but it simulates this behavior), in my case the backtracking does his job _while_ it is going back (by undoing or compensating actions) This is beyond what any indeterminism monad can do. However my monad can reverse the execution flow and go forward (with retry) then backward (with undo), then forward etc and can simulate an indeterministic effect if `onUndo` feed the flow with different values each time. In this case it would be backtracking simulating indeterminism, while the list monad execute indeterminism simulating backtracking. That is closely related, but not the same than exploring a tree deep first versus doing it breath first. Laziness complicates the analogy in the case of the list monad. It would be a good exercise to do the list example with my monad. I will do it. 
&gt; I believe this rules out type inference. It doesn't. &gt; Are there other drawbacks? * The typeclasses need to be written explicitly. * There is no guarantee that the operation is always the same, so, say, maps might not work correctly because they get incompatible orders when they are constructed.
I think I would have had a much easier time with Yesod if I'd tried to use Scotty first, then moved on to the big guns! However I actually prefer Scotty these days for its simplicity.
If you have some time, [here](http://youtu.be/hIZxTQP1ifo)'s /u/ekmett giving a talk on type classes. If I recall correctly, a rather large chunk is devoted to alternative approaches and the specific tradeoffs involved. There's also a [reddit discussion](http://www.reddit.com/r/haskell/comments/2w4ctt/boston_haskell_edward_kmett_type_classes_vs_the/) of the same talk with some further elaboration.
&gt; [...] it does indeed work fine when your pretty printer can't fail. This isn't always the case, though. Do you have a concrete example of a situation in which you would want a pretty-printer to fail? &gt; [...] neat things like being able to build fold/unfolds out of partial isos. Interesting, I see the fact that the isomorphisms are partial in both directions is used to implement `iterate`, which repeatedly applies each half of the isomorphism repeatedly until it fails. I wonder if it would be possible to use `trace` as a looping primitive instead? trace :: Iso (Either a loop) (Either b loop) -&gt; Iso a b I first heard of `trace` on page 6 of this famous paper on an [invertible language based on the rational numbers](http://www.cs.indiana.edu/~sabry/papers/rational.pdf).
Something that wasn't mentioned in the post but that I personally found very helpful when I was trying to learn about type inference: one way to look at let polymorphism is that it acts as if you did textual replacement and copy pasted the function definition in multiple places. For example, if you write let singleton x = [x] in (singleton 10, singleton "hello") With non-polymorphic type inference this won't type check because the x variable will try to be both int and string at the same time. One workaround is to inline the definition of "singleton" so that you end up with two distinct "x" variables that don't conflict with each other: ((\x -&gt; [x]) 10, (\x -&gt; [x]) "hello") The abstraction step with let polymorphism ends up serving a very similar purpose to this so but I found the inlining analogy much easier to understand (it also extends to how "flow-based" type inference algorithms get implemented). That said, naive inlining would be super inneficient so we do the generalization step instead. If you squint at it you can kind of see it as using dynamic programming to convert the exponential naive algorithm into something efficient.
Last time I checked it lacked an ability to handle commands in usage display. E.g., usage does not automatically list commands, it does not provide usage display, etc. It is somewhat minor, but a little annoying. Other than that it is excellent.
It’s [Halcyon](https://halcyon.sh/), not Halycon. Maybe I should have given naming the project more than 5 minutes thought…
Your gallery page is broken. http://ianwookim.org/hoodle/gallery.html Great looking demo video though!
Nah it's fine; was just a typo on my part.
See also, http://hackage.haskell.org/package/boomerang With a simple getting started here: http://hackage.haskell.org/package/boomerang-1.4.5/docs/Text-Boomerang.html And a more detailed example that is specific to web-routing here: http://happstack.com/docs/crashcourse/index.html#web-routes-boomerang The basic idea is the same (since it was copied from early prototypes of Zwaluw/JsonGrammar). The primary advantage of boomerang is that it is more general purpose, abstracted over the token type, provides error messages with positional information, etc. I'd love to see a 'boomerang 2.0' that borrows some ideas from the JsonGrammar implementation. I don't really care what it is called or who implements it, as long as I can build web-routes routing on top of it. 
cofree, technically. ;)
Your idea about substituting reals for ints because *some* reals would work doesn't even pass muster in the comparatively unsophisticated polymorphism used in OO languages like Java. See http://en.wikipedia.org/wiki/Liskov_substitution_principle. The examples people are giving attempt to point that out.
I think you really can talk about lack of interest though. The major frameworks like Qt work fine from Haskell, but not in a _satisfying_ way. No one is choosing Haskell because they want to build GUIs imperatively in `IO`.
Well, get your friends to show interest in this and it'll be made! I need 50+ votes to get this going, and it'll be made! I've never seen one of these be more than ~$10 a key with a 25 key minimum. I am planning on buying ~20 for myself anyway, so as long as we can get by the Interest Check period, you can have as many of these as you care to!
lens exports |&gt; as &amp; I like |&gt; better but lens is a de facto standard and I think |&gt; was already claimed in Haskell. F# also has composition operators - &lt;&lt; is equivalent to . 
I'm a big fan of left-to-right piping operators and mentioned this at https://github.com/Gabriel439/Haskell-Turtle-Library/issues/28 where it was revealed to me that `&amp;` is going into GHC 7.10 but not into the prelude, just `Data.Function`.
I see no images at the link.
There really isn't any meaningful interpretation of that in standard haskell... so... and, TH uses dollar signs as syntax, so it would be a good first guess. 
&gt; I started working on Haskell projects (I am a product manager working for FP Complete) with developers and a pattern is emerging which is there isn't enough tutorials and samples teaching how to build an end-to-end Haskell application! I am creating a step by step Haskell tutorial with practical applications initially focusing Scientific, Engineering, Statistical and Financial Applications and design patterns in functional programming. The idea is to provide fast understanding by examples of functional programming to beginners in FP. It still a work in progress. It is available in: [Functional Programming Step by Step tutorial](https://github.com/caiorss/Functional-Programming)
`(|&gt;)` and `(&lt;|)` are sort of implicitly claimed as cons and snoc for `Data.Sequence`.
I used Control.Arrow in the past for left-to-right composition, but at this point I'm already used to right-to-left and for me it really doesn't make any difference. Other than an aesthetic one. *Main&gt; :m +Control.Arrow *Main Control.Arrow&gt; :{ *Main Control.Arrow| let infixr 0 # *Main Control.Arrow| (#) a f = f a *Main Control.Arrow| :} *Main Control.Arrow&gt; [1..10] # Prelude.map (+1) &gt;&gt;&gt; Prelude.map (+1) &gt;&gt;&gt; Prelude.filter (&gt; 3) [4,5,6,7,8,9,10,11,12] **edit**: actually instead of all that, you can just do let (#) = flip ($) [1..10] # Prelude.map (+1) # Prelude.map (+1) # Prelude.filter (&gt; 3) 
In that case `&amp;` might as well not be there. Personally I quite like `|&gt;` and I'd rather define it locally where I need it rather than import Data.Function ((&amp;)) It's not worth the trouble. But I suppose it can go to `Data.Function` first and then make it's way to `Prelude`
why not the idiomatic take 3 . reverse . filter even $ [1..10] ? There are a lot of advantages here... first of all, it matches actual function application ordering in math and almost all programming languages. f(g(x)) turns to f . g $ x. Second, it is much easier to refractor out units as separate named functions. if I wanted to define takelast3 = take 3 . reverse I can just drop it in to the original version take 3 . reverse . filter even $ [1..10] (take 3 . reverse) . filter even $ [1..10] takelast3 . filter even $ [1..10] this works because `.` is associative (like +) and `$` is not (like -). associativity is a beautiful thing, and works will with haskell's evaluation-by-substitution well here. It helps us see compositions of functions as functions themselves, and helps us parse them in our minds as units of meaning. The third main reason is that it reflects more the evaluation model of Haskell and helps us reason about evaluation and laziness. In a chain of compositions, Haskell evaluation is "driven" from left to right. For example, take: map negate . take 3 . filter even $ [1..] How would you get the first element from that list? Where does it "start"? It starts with `map`... it asks for the first element of the result of map. and the first element of the result of map requires the first element of the result of `take`. And the first element of the result of take depends on the first element of the result of filter. And the first element of the result of filter is acquired by going through that infinite list until it finds an even number, and returns that to take, to map. See, in evaluation, map is the king, and take is subordinate to it; the "flow of control" flows left to right. Also over the years people have more or less come to the conclusion that new operators are seldom worth the extra overhead, and adding three new operators to haskell's already operator-filled soup in base, heh. 
The operators |&gt; ?&gt;&gt; |&gt;&gt; are good for interactively development in the ghci shell where you can add the next operation at the end of the line. It can be seen as the old unix Pipes and they are widely used in F# and they seem to be faster to perform refactoring and try new things.
It almost does when you [use a better Hoogle](https://www.fpcomplete.com/hoogle?q=%28a%2C+b%2C+c%29+-%3E+c&amp;env=ghc-7.8-stable-14.09).
It does in [Hayoo](http://hayoo.fh-wedel.de/?query=%28a%2Cb%2Cc%29+-%3E+c)
&gt; The operators |&gt; ?&gt;&gt; |&gt;&gt; are good for interactively development in the ghci shell where you can add the next operation at the end of the line. If you want to have those operators in _your_ GHCi, simply put them into your [`.ghci` file](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci-dot-files.html): $ cat ~/.ghci let (|&gt;) x f = f x let (|&gt;&gt;) x f = map f x let (?&gt;&gt;) x f = filter f x $ ghci GHCi, version 7.8.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; [1..10] ?&gt;&gt; even |&gt;&gt; (+1) [3,5,7,9,11]
&gt; The types mean such guarantees would still hold, don't they? Map would have an extra type parameter for the Ord instance it uses. That requires some method of specifying it at type level. Whether that is acceptable depends on stuff.
There's a project `servant` is going to have to tackle at some point that might be of just the right size, and is (in my opinion) very interesting. It involves automatically rewriting a type into a canonical, trie form, so that: type API = "b" :&gt; Get () :&lt;|&gt; "a" :&gt; "b" :&gt; Get () :&lt;|&gt; "a :&gt; "c" :&gt; Get () becomes type API' = "a" :&gt; ("b" :&gt; Get () :&lt;|&gt; "c" :&gt; Get ()) :&lt;|&gt; "b" :&gt; Get () And also rewriting the term-level in lock-step. The most obvious advantage is log n routing, which we get pretty much immediately from that, but there are also other things - automatic HATEOAS support, for instance, and API versioning checks that can tell whether a change is a backwards-compatible addition to an API.
I disagree. It's nice being able to know what an operator does just by looking at it. If you want to use an established operator for something else, you need a good enough reason to outweigh the benefits of the familiarity of the existing meaning.
A place in `base` gives some level of canonicity, which I think is a good thing. I don't personally think &amp; is the best name for that operator, but I also don't have a good enough alternative to go against base so I will call it &amp; in my code too, to help with interaction with others.
Why not options? https://hackage.haskell.org/package/options
Indeed, especially because of packages which use a wide scope of the operator namespace; the desirability of operators is inversely proportional to its character count, so the true namespace of desirable operators is a rather small one.
&gt; It's nice being able to know what an operator does just by looking at it. This is true of regular words, too. Why are operators special?
For me, the fewer operators the better. 
shameless self-promotion: [fixplate](http://hackage.haskell.org/package/fixplate) is a uniplate-style generic programming library grown out from exactly the same idea.
One downside of the F# operators is that `&lt;|` has the wrong associativity. It should be right-associative, like Haskell's `$`, but it is left-associative. Of course we could change this, but we already have `$` anyway.
not quite :) x |&gt; f |&gt; g |&gt; h if I wanted to pull out the f and g, I can't do fg = f |&gt; g and have x |&gt; fg |&gt; h that's because x |&gt; (f |&gt; g) |&gt; h is not the same as the original
You are welcome! You are also welcome to be involved in this particular OSS project :)
And yet another demonstration of this technique (rediscovering the cofree comonad) that only shows the trivial case where your AST is originally represented as a single inductive type, instead of a set of mutually-inductive ones...
Sure, that’s why I took care to ask people to give feedback if they encounter any issues. What do you mean with “Raw API is not a joke.” ? :)
I just want to mention that what you are describing looks a lot like explicit dictionary passing, a la http://www.haskellforall.com/2012/05/scrap-your-type-classes.html. Your example would look like this in Haskell: data Monoid a = { op :: a -&gt; a -&gt; a, id :: a } reduce :: [a] -&gt; Monoid a -&gt; a reduce xs m = foldr (op m) (zero m) xs intSumMonoid :: Monoid Int intSumMonoid = Monoid { op = (+), id = 0 } intProductMonoid :: Monoid Int intProductMonoid = Monoid { op = (*), id = 1 }
We went throught different ideas with /u/tel. Here is [the original gist](https://gist.github.com/gallais/53790d37bf29fe1e19ab) with comments linking other attempts. [This other thread](https://gist.github.com/tel/99e666308d270a3d1d8c) is relevant.
So what happens if we throw in mutually recursive datatypes? How do we fold over those? I've been wanting to write a compiler as a catamorphism but my language is too complex to write as one datatype. I've been following http://foswiki.cs.uu.nl/foswiki/pub/TC/CourseMaterials/TC-07-handout.pdf to write my own custom folds over custom algebras but it seems like it could be generalized.
Is there a good article / intro to Fix?
I can't vote without logging in. I can't log in without registering (come on -- get with the OpenID/OAuth program). I'm not going to register because they ask for too much information up front. (You don't need my address until I am buying a physical product from you and you have to ship it.) If this is successful though, I am willing to purchase some caps.
what am i missing?
I believe there's a separate subreddit for code reviews. If not, I'd stil rather this be on /r/haskellquestions, and include a question. I'm pretty sure this: parseJObj = (\k v -&gt; JObj k v) &lt;$&gt; (spaces *&gt; (char '{') *&gt; spaces *&gt; parseJObjKey &lt;* spaces &lt;* (char ':')) &lt;*&gt; (spaces *&gt; parseJValue &lt;* spaces &lt;* (char '}')) won't read this: { "foo": "bar" , "bar": "foo" } If you are just doing this for your own enrichment, I suggest reading the [relevant RFC](https://tools.ietf.org/html/rfc7159) again. If you actually need a JSON parser, I suggest using [Aeson](https://hackage.haskell.org/package/aeson), unless you have a particular issue with Aeson.
whoops. i have been thinking of &gt;&gt;, which behaves as i was describing and expecting, at least in f#. thanks for pointing out what i was missing and for /u/mstksg pointing out my original mistake.
This lovely library was inspiration for my blog post [A category for correct by construction serializers and deserializers](https://ocharles.org.uk/blog/posts/2014-06-10-reversible-serialization.html). Love this library!
Compare: &gt; 1 `f` $ 2 &lt;interactive&gt;:2:7: parse error on input `$' Perhaps you intended to use TemplateHaskell &gt; 1 `f` + 2 &lt;interactive&gt;:3:7: parse error on input `+' Note the absence of a message along the lines of "Perhaps you meant to write ``(1 `f`) + 2``". So it's not that GHC thinks that TH is more likely than a typo, but rather, that in addition to the error it usually gives in case of a typo, it helpfully points out that one reason for getting this type error is forgetting to enable the TemplateHaskell extension. In this case, turning on TemplateHaskell would still not allow the example code to parse, but there's no way to know that without re-running the parser with the flag on. That is, the non-TH parser doesn't distinguish between the invalid input above and the invalid input below, which does look like it would need the TemplateHaskell flag. &gt; 1 `f` $(generateCode) &lt;interactive&gt;:2:7: parse error on input `$' Perhaps you intended to use TemplateHaskell
Perhaps because it's actually necessary? Avoiding orphan instances turns out to be more important than having small dependency lists.
A couple things are going on here: - Suppose I want to instantiate type class C for concrete type A. If I want to avoid orphan instances, I need to put this into the same package as either A or C -- so one of those packages will need to depend on the other. - Many of these category-theory-esque abstractions are defined in terms of other, simpler abstractions. If those basic abstractions live in a separate package, we'll have to pull that in too. The first point is a real basic language issue. I view the second one overall as a good thing -- though our tooling has a bit of catching up to do. Also note that the `free` package defines a whole lot more than the free monad -- if you look through the source, you'll see where a lot of those dependencies come from.
I'm sure /u/edwardkmett will comment on this thread soon, as he is the author of the package and probably can describe his motives best. I guess the reason for all those dependencies is to avoid orphan instances, as mentioned in the other comments. Let *C* be a type class which has an instance *I* for a type *T*, then *I* is called *orphan* if it is neither defined in the module of *C* nor in the module of *T*. As far as I remember, the issue of orphan instances is somewhat controversial in the Haskell community: * On the one hand, as you have already noticed, if one does not use orphan instances, the dependencies can grow very big, only to support instances for types in other modules, which in practice would have been only necessary if the instances were actually used in the client code. * On the other hand, if the instances are neither contained in the module of the type class nor in the module of the instance type, then it can happen that two independent libraries each define their own instance for that type and class. In this case it is hard/impossible to use them together, because in the client code, using both libraries, there are now two instances from which the compiler could choose. I'm not sure if those are the only arguments, but they are the ones I remember out of my head. Hope I could help :)
`free` needs `semigroups` so it can provide the `Functor f =&gt; Semigroup (Free f a)` instance. `semigroups` needs `text` so it can provide the `Semigroup Text` instance.
Other suggestions that I received from ghc were generally useful, but maybe it should refrain from the suggestion if it relies on unvalidated assumption. But again, since I never used TH, maybe I'm just wrong and everyone else agrees it's a sensible suggestion.
https://mail.haskell.org/pipermail/ghc-devs/2015-March/008437.html
Wait till you'll need `lens`
This is pretty much it. I get battered by folks who want more minimal dependencies, but ultimately the packages I write have to work together or they are useless for _me_ to build on top and orphan instances are a horrible horrible thing. I used to solve this just by building monolithic packages, but then some folks convinced me to split them up into lots of smaller pieces. This proved near unmaintainable, so I found ways to cut the number of packages involved in half, while avoiding orphans. So, let's see what happens to bring in those packages: * Installed prelude-extras-0.4 A form of these were adopted by `transformers` 0.5. I could in theory switch now that `transformers-compat` exists. I lose some functionality in the process though, as the `transformers` versions are missing the very nice default definitions that make the current versions write themselves, so I've been slow moving on this front. Those definitions sadly won't merge into `transformers` as `transformers` clings tightly to Haskell98/2010. * Installed mtl-2.1.3.1 Folks find it useful to be able to lift monad transformer instances over the free and free completely-iterative monad transformers. * Installed transformers-compat-0.4.0.3 This enables support for older versions of `transformers`. Since transformers ships as a boot lib, you can't really upgrade it. Not if you like things like `doctest`. This lets me provide a wider support window. * Installed distributive-0.4.4 Distributive is a form of "cotraversable". The combinators therein are quite useful, and it has minimal dependencies on its own. * Installed semigroups-0.16.1 * Installed semigroupoids-4.3 There are a number of key semigroup/semigroupoid instances involved. These let you talk about containers that always have at least one member or ways to smash together lots of data types. * Installed comonad-4.2.2 The package also supplies cofree comonads. * Installed profunctors-4.3.2 This lets me supply the `_Pure` and `_Free` prisms without incurring a lens dependency. * Installed bifunctors-4.2 Several of the constituent parts are bifunctors. * Installed text-1.2.0.4 * Installed hashable-1.2.3.1 * Installed nats-1 * Installed unordered-containers-0.2.5.1 * Installed void-0.7 * Installed contravariant-1.2.0.1 * Installed tagged-0.7.3 These are brought in transitively. Tagged should probably be set up as a free monad in its own right though. A large part of this comes from the fact that `semigroups` has to depend on other packages to get instances out to them. Inevitably I get folks arguing for different layerings, where different packages sit on the bottom of the heap, because the part they want is "so simple," but if it doesn't all work together it is useless to me. Occasionally, that case is compelling enough to encourage me to throw in the 48 hours of work that any such inversion causes me to do, and deal with the ripple effects across the ecosystem. If Haskell had a better way to say "when you depend on package X and package Y you also get these instances" than using orphans? We could have smaller packages that felt more independent. This would come at the expense of then having to maintain that inter-package space. Haskell is not currently that language.
There is no fluff to that FFI
The huge dependency graphs caused by the desire for non-orphan instances is an issue for modularity. A few times recently I have wondered if it would be worth teaching cabal a means to have intersection dependencies to hold such instances. I.e the package holding the class (packageC), could specify a dependency on a packageCDinst (exporting just a module) but only in the presence of packageD (which defines a data type). Thinking about this a minute one mechanism might be: * A GHC pragma which could be applied to an import specifying the intersection package it comes from. It would be an error if the import brought in anything other than instances. GHC would import the instance if cabal has provided it, but ignore it otherwise. Non-supporting compilers can treat it as a normal dependency without issue (apart from bringing in the large dependency graph). * Cabal would need support for declaring the intersection dependencies. It would also need a mechanism to go back and install packageCDinst (specified by packageC) when packageD is later installed. * GHC would need a means to ensure that if both packages are being used then the relevant intersection package is being linked in. Some kind of extra information in the .hi file and a check at compile/link time. This would create extra packages for the instances, but avoid some of the big dependency graphs and lesson the pressure for Orphan Instances since there would be less reason for a maintainer to decline to include an instance. Without further thought, this feels a little like backpack. In fact, I have a feeling that if backpack lands there will be a profusion of new packages to provide interfaces. The extra packages this scheme creates might feel less out of place at that point. edit: Cabal would also get a flag to ask it to install the relevant dependencies rather than delay. So I could still use 'cabal install lens' as a shortcut to get a whole bunch of stuff :-)
Thanks! I fixed it.
`base` could use some more attention. There are about [50](https://ghc.haskell.org/trac/ghc/query?status=infoneeded&amp;status=new&amp;type=bug&amp;component=Core+Libraries&amp;col=id&amp;col=summary&amp;col=component&amp;col=status&amp;col=owner&amp;col=priority&amp;col=milestone&amp;order=priority) open bug reports (some are for other core libraries). * Pro: everyone benefits * Con: requires a lot of design discussion and library proposals
What I took away from this talk was a very different community model for F# than prior -- and the scope of effort taken in trying to turn a project gestated in a large company into a more "open source" world. I don't know what pointers, if any, we can get towards how we might structure things differently in Haskell, but it is certainly worth thinking about...
Open sourcing shouldn't be incentive based. It's a philosophical choice.
Does it also avoid assuming / implying axiom K or the UIP axiom? I love that it handles guards and laziness, but maybe those would be easier to squeeze into [pattern matching without K](http://people.cs.kuleuven.be/~jesper.cockx/Without-K/Pattern-matching-without-K.pdf), which handles GADTs well already. They already overlap in one way that I can see; axiom K and \_|_ : EQ Zero (Succ n) are incompatible -- axiom K / UIP says that all members of EQ are Refl (and therefore not \_|_). I'm not deeply knowledgeable of either, but it seems like the without-K process should be able to at least extend the abilities of the oracle used by this paper to any enum-like type, if not more. EDIT: bottoms, not underlined pipes.
Why did you make `MyAlternative` instead of using `Alternative`?
Everything is incentive-based. But, that warm feeling of satisfaction working on / releasing an open-source project that long ago flickered out at work might not be enough incensive for everyone. There's [an incentive-reward exchange protocol](http://www.logarithmic.net/pfh/rspp) for that case.
Thanks. It makes sense. I suppose, then, this all reduces to the lack of a package management feature to isolate the interdependencies between different packages. There is already a package naming convention that has been widely adopted. Suppose there is also a packaging convention where packages are created with the core functionality in one package with only dependencies on functionality actually used, and an ancillary package containing the tentacles that reach into all these otherwise unnecessary packages. Users can choose to install the core functionality, or "everything". This may get us to the *high cohesion*, *loose coupling* ideals. As it is right now, we have *high cohesion*, *tight coupling*. Not the best. At this point, I think I'm back to cutting-and-pasting just the Free monad definition to avoid the excess 500MB of packages installed for no useful purpose. 
A few years ago, when I was a student, I tried to get involved in GSOC. I picked up [an idea](https://ghc.haskell.org/trac/summer-of-code/ticket/1621) and [wrote a proposal](http://www.google-melange.com/gsoc/proposal/public/google/gsoc2012/danburton/5707702298738688) shortly before the deadline. My proposal was declined and... that was that. As I recall, information about what I (an aspiring student) could do to get involved and write a successful proposal, was very scattered. I didn't personally know anyone who had ever really been involved in GSOC, let alone the Haskell sector of it. I'd like to see a clearer documented path for aspiring students to take. Right now all there is in the way of documentation is "go look at this list of ideas, and submit a proposal." The process is very opaque; it's difficult for students to gain insight into what will make for a successful proposal and a successful project. Maybe it would be helpful to have somewhere on the wiki to display short bios for students and mentors. That way mentors can familiarize themselves with the interested students, and vice versa. Maybe we can organize a google hangout where former GSOC students can give some insight to aspiring new students on the process.
It's not an opinion. Orphan instances can cause silent, incorrect runtime behavior and was previously one of the ways to subvert the type system (I believe *that* has been fixed). Large dependency lists can, at worst, cause loud, compile-time failures or deployment difficulty. The former is far worse than the later.
IIRC, /u/edwardkmett did the monolithic approach before and there were still similar complaints.
&gt; The first point is a real basic language issue. Do you have a better way to ensure coherence of instances across module boundaries?
Excellent points -- I think a "what do I do as a student" page on the wiki would be a good place to start -- explaining how to write a good proposal, etc. That said, the GSOC faq isn't a bad place to start: https://www.google-melange.com/gsoc/document/show/gsoc_program/google/gsoc2015/help_page#5._What_should_a_student_proposal_look
Because blaze-markup isn't a monad! :-) &gt; Has anyone used lucid with yesod? The haskell.org homepage is written in yesod and lucid.
it's really weird that this was downvoted to oblivion.... I can see people possibly disagreeing to the opinion expressed here, but disagreement isn't why one should down vote something. *shrug*
Not a problem. I just got my confirmation that I don't have a future in stand up comedy on the topic of functional programming.
&gt; Haskell is not currently that language. I like the optimism in the word "currently". Do you think it would be worthwhile to work towards improving this part of the language? The changes I've heard of which did make it in (such as BBP) all seem to have been proposed so many years earlier, so I wonder if it's because there are a lot of sources of latency in the process, or if such a change would really take years of hard work.
I'm profoundly unqualified to comment on that -- I just wanted to point out that it's a logical consequence of the language definition and the avoidance of orphan instances, not a tooling or cultural thing. (It's true that I've never found the arguments against orphan instances super compelling, but I'm not sure I want to revive that debate...)
At least I learned about [DisambiguateRecordFields](https://downloads.haskell.org/~ghc/7.10.1-rc2/docs/html/users_guide/syntax-extns.html#disambiguate-fields), which helps with the record problem a bit... EDIT: I traced this extension back to [GHC 7.4.2](Record field disambiguation)...
&gt; it's important for a library to have few dependencies Why? That seems like an assertion without support.
First of all, sorry for (accidentally) misleading phrasing. What I meant wasn't that it's universally important for libraries to have as few dependencies as possible, but rather that it's important for some library authors to ship libraries with few dependencies. As for *why* an author might think it's important for a library to have few dependencies, here are some reasons: * If you are a newbie (or even not exactly a newbie), you don't really know how to fix cabal-related things when they go wrong, so you often resort to just Reinstalling The World. If it takes too long, it leads to annoyance. So, if you want to minimise pain of newbies/short-tempered people who want to use your library, you'd want it to have few dependencies. * Since Cabal got sandboxes, some people (a lot of them?) have been using them for every little project. I like lenses, but I hate waiting half an hour for `lens` to install; once I finish my `microlens` library (which is intended to be modular and compatible with `lens`), I'm not going to ever use `lens` for a personal project again. So, if you want your library to be widely used (and I think authors are partly motivated by this), you don't want to have your users pay the cost of having to wait half an hour to install a library, because there always will be people for whom it outweighs the benefits of your library. * The more dependencies there are, the more likely it is something will go wrong; one of your dependencies might end up having wrong version constraints, or Cabal's solver will give up, or a wrong build plan will be chosen. Again, if your library doesn't install (even if it's not your fault), your library won't get used. * When 90% of people follow the “it's completely fine to depend on what 90% of people use (i.e. vector, mtl, etc.)” rule, it makes lives of the other 10% really miserable. Among those are users of old versions of GHC (who can't move on for some reason or another), brave souls who try out JHC, and so on. I really don't think actively discouraging people from not going with the mainstream (by which I mean 2 latest versions of GHC) would help Haskell in the long run. (Consider the fact that currently no library which depends on `mtl` will compile with JHC.) * Humans being humans, there are people who would *never* depend on `free` with all its dependencies when they can copy 6 lines of `Free` into their code instead and avoid the imaginary burden of dependencies. Bye, code reuse. It doesn't matter whether they're irrational, wrong, whatever, they're still people who write code and by putting the canonical implementation of `Free` into a library which has a dozen dependencies you've discouraged code reuse. I would like to note once again that it has little to do with what is right and what is wrong (in regard to some ideal world you want to live in and some ideal version of Haskell you want to use), and more to do with how real-world people behave (mostly irrationally). * If you define your userbase as “smart people who can clearly see that dependencies are totally alright and that orphan instances are the true enemy”, then there's no argument, and the rational thing is to do what's more convenient for you and your users, that is, always provide instances for `Text`, `Bytestring`, `Vector`, make your types instances of all classes that can possibly fit, etc. Provide lenses for all your datatypes, while you're at it, so that users won't have to declare them by themselves. (Actually, I think there are Aeson lenses in `lens`, but I'm not sure.) * If you don't entirely not-care about less rational people (or just people who disagree with you), the rational thing might turn out to be sacrificing some convenience and safety in order to get more people using your library. Really, it all depends on your goals.
I accidentally wrote a longer version of your comment.
I'm pretty sure they cut the branch for 7.10 a while ago.
How will microlens be different from lens-family? 
It seems to be exactly the existential typeclass anti-pattern. Why do you think it isn't? 
right, 7.12 or 8.0 or whatever, unless I misunderstand what you mean by "cut".
&gt; Suppose there is also a packaging convention where packages are created with the core functionality in one package with only dependencies on functionality actually used, and an ancillary package containing the tentacles that reach into all these otherwise unnecessary packages. This won't work for dependencies pulled in for writing instances as you can't move the instances to another package without creating orphan instances in that package.
What do you think about MFlow?
there is absolutely no valid reason why it should break
The Python orgs actually require this for all proposals and I think it is a really good stance.
I can't wait to look into this one. The [older paper](https://www.mpi-sws.org/~neelk/bidir.pdf) gave my understanding of type systems a huge boost (it was pretty much my introduction to System F based inference). 
I doubt an implementation could be simple! You can't serialize arbitrary functions in haskell, and I think the reasons are quite involved. by the way, have you taken a look at the static pointers that's being introduced in GHC 7.10? They allow for a limited form of this.
why are you not recommending MFlow?
~~That would require semigroup be in base. I'd be all for that, but that brings in its own can of worms, probably as big as the FTP.~~ I apparently have no idea what I'm talking about.
One thing I've always wanted is a little dropdown that displays the current version of the package documentation that one is reading, and allows one to switch between versions without having to go to the index or fiddle with urls. ReadTheDocs is an example [0], but of course there are many others. Doesn't really involve writing Haskell, though. [0] http://docs.python-requests.org/en/latest/api/ , bottom right, as an arbitrary example.
Thanks for this, it's a pretty elegant way of doing the same thing. I'll also add a footnote about the anti-pattern. Because yeah, the code currently exemplifies it. I promise that it won't by the time I've finished :).
Actually, thinking a bit more. If one could use package IDs (not package keys) in CPP, that could be done: In a cabal file (only depend on text if it is already required): #if PACKAGE(text) &lt;depend on text&gt; #endif in haskell code: #if PACKAGE(text) &lt;semigroup instance for text&gt; #endif The `PACKAGE(&lt;packageX&gt;)` would be true if 1) the `packageX` is already installed, or 2) `packageX` is on cabal's install plan.
Yes.
Are you complaining specifically about this tutorial or in general? Several libraries have some support, including compdata and multirec have implementations using the idea of higher-order functors that you experimented with.
I've wondered if the SAT/SMT solver integration work that Ivor is doing for type-nats would be useful here? I took a quick scan through the old (current) code at ICFP last year it seemed like it would be easy to reimagine as a SAT problem.
Thankyou. I'll add a note to the blog post. I'm learning a lot writing them. :)
I have vague ideas in that direction, but so far they introduce almost as many headaches as they solve and it would pretty much be a new language at that point. There are a number of points in that design space, though, so there may well be ones I haven't explored, and tools like backpack are going to change the design surface in ways I haven't anticipated, so I'm happy to let that proceed slowly.
We don't have it as a hard requirement, but how much experience they have with Haskell and with the project in question is something that we definitely consider when evaluating student proposals, as it correlates heavily with success.
I don't think it would be too hard. My idea is: let the compiler store the abstract syntax trees during compilation. For each function call, an function gets the real parameters and the AST that lead to that invocation (parameters substituted with the according ASTs). Normal functions will evaluate the parameters, the serialization functions only the AST. Of course such a scheme might get a slight performance hit (i.e. it will be as slow as molasses). So an optimization would be to remove most of the unneeded ASTs. Perhaps this could be made slick enough, so the runtime penalty becomes tolerable.
Great news, congratulations! That said, what's a good place to talk about ideas? The mailing list has zero topics up, and the IRC seems rather dead, too.
Okay, no worries... maybe Olle will email you in a few days with a link to his implementation again, ha. :) I have a somewhat related question - do you know if it is possible / easy to implement one or both typechecking algorithms using a representation of terms / types where polymorphic recursion is used to enforce proper variable scoping, a la [bound](https://hackage.haskell.org/package/bound-1.0.4/docs/Bound.html). I am asking because I have an implementation of the algorithm from the older paper that I am using for [this project](http://pchiusano.github.io/2015-02-23/unison-update3.html), but I am finding that enforcing scoping via the types is very nice for a lot of the other code I'm writing. Here's a really simple representation along the lines of what I am thinking: data Expr t a -- annotations in `t`, vars in `a` = Var a | App (Expr t a) (Expr t a) | Ann (Expr t a) t -- `Var Nothing` means bound by nearest enclosing lambda, etc | Lam (Expr t (Maybe a)) -- using bound, this would be `Lam (Scope () (Expr t) a)` instead instance Monad Expr where ... instance Traversable Expr where ... The AST for `Type` could be done similarly. I really like this kind of representation for various reasons, but I wasn't sure how to adapt your algorithm. The ordered context seems like it would need to become something fancier than a regular list, and I'm guessing other stuff would change as well. If this question makes sense and you could even sketch out a little bit how it might work, either with the old or the new algo, it would be super interesting to see! Also, I'd be curious if you have any opinion on whether it is worth it to enforce well-formedness of scoping via the types like is done with bound and in my little snippet above.
Has this been posted on the mailing list? At the current rate of votes it doesn't look like it will happen unfortunately.
I didn't, though if anyone is willing to do that for me, I'd be appreciative (I don't want my first post to the mailing list to be something like this :) ). We have another ~40+ days after only 2-3 days, getting to 50 might be difficult, however I'm still hopeful! I only posted on keyboard forums and here though.
Well, one issue is that Edward Kmett will have to maintain around 1000 packages instead of 100, complete with 1000 Travis CI instances and stuff. It's doable, but probably requires some tooling which we don't have readily.
I usually think of the anti-pattern as applying primarily to functions that have the type in both a negative and a positive position. Like `withAttr :: w -&gt; Attr -&gt; w`. But, I think that's just because *I've* been tempted to use the anti-pattern and I may have mentally justified it by restricting my type variable to being only used in a negative position. So, sure, I'm willing to call the article an example of the ExTC anti-pattern.
&gt; It doesn't matter whether they're irrational, wrong, whatever It does. If they are provably wrong, we can ignore them for now and wait until they correct their errors in the (potentially far) future. There's no reason to coddle provably wrong ideas, right ideas will win out in the long run. That said, things are rarely -- if ever -- that clear.
The point of category theory is that it's about capturing structure in an abstract way. The point of a monoid (or as a special case a group) is that it's a set of things which can be composed associatively and this matches the use of morphisms. The morphisms don't have to be functions and the objects don't have to be sets because the objects are basically 'atoms' of the category, morphisms between them are what matters. If you really want to define the object * of the monoid category you could take the underlying set of the monoid (which is just Hom (*,*)) and see the morphisms as composition with a certain element of the monoid. So for example the natural numbers {0,1,...} using + form a monoid. Take the set {0,1,...} to be the object and take as morphisms the endomorphisms a: x -&gt; a + x for a an element of {0,1,...}. So the morphisms of a monoid shift the object by some amount. This 'internal' description is usually not necessary if you want to use the monoid as a category, so the abstract notion suffices.
The main place folks have been chatting about ideas so far has been over in [the brainstorming thread](http://www.reddit.com/r/haskell/comments/2xp9v2/haskellorg_has_been_accepted_into_gsoc_2015/) mentioned in the self-post above, and there has been a fair bit of communication directly with would-be mentors scattered across the different sub-communities, darcs, #yesod, #snapframework, #haskell-lens, #happstack, and direct outreach to mentors: I know for instance that I've had a few students reach out to me individually as a potential mentor. If you have a particular area in mind I can maybe point you to a potential mentor in that space or if you have a particular area you'd want to mentor a project in. The tricky thing with the summer of code is that the proposals are ultimately written by the students, so while we can help shape the direction of the project into something that benefits the community, the barrier to entry is rather high. This has always been a bit problematic.
Is there an explanation of how to write how to write an entry in the Trac? Or is it just a case of sign up for an account and create a ticket? (I don't want to violate Trac etiquette.)
Well, before I wrote json_parser, I had completed the Parser homework from Prof. Yorgey's UPenn class (www.cis.upenn.edu/~cis194/spring13). Implementing `Alternative` was a homework question, as I recall.
&gt; if your utility function assigns some weight to the present, despite the fact that the future is in a sense “infinitely longer” than the present and thus should also be “infinitely more valuable” That would be irrational: http://lesswrong.com/lw/hw/scope_insensitivity/
We actually already do this. It's just the origin dependent namespace is normally hidden. Right now, it only shows up in compile-time errors like ["Couldn't match expected type `S8.ByteString` with actual type `bytestring-0.10.0.2:Data.ByteString.Internal.ByteString`"](http://www.yesodweb.com/blog/2014/09/woes-multiple-package-versions). IIRC, there was some problems with getting GHC to actually compile against two versions of the same name, but I think they may have already been addressed. Cabal has supported multiple version of the same package for a while, and it currently growing (or recently grew) support for multiple flavors of the same version of the same package -- i.e. semigroups-0.16.1 compiled against base-4.6.0.1 and semigroups-0.16.1 compiled against base-4.7.0.2 -- by further appending some hash value to the send of the package identifier. If there's not a lot of work left, finishing it might be a good GSoC, yes. These changes feed into the larger body of work needed to implement backpack, IIRC.
That's handled by `Reactive.Threepenny.newEvent`. You'll have to call the handler in IO once the computation is done.
It is just "Sign up for an account and create tickets" -- No real etiquette surrounding this trac. It has rotted for a while and we're just trying to clean it up this year. Some folks had problems earlier with the trac emails being sent with invalid return addresses (trac@localhost), but it should all just work now.
Interesting. Do you have more experience with that pattern? Dos and don'ts?
I'm still in the process of getting my feet wet with OCaml, but since ML-family languages are all based around the notion of signatures+structures then the "abstract interface" and "concrete implementation" fit that same bill. Perhaps the most interesting missing thing is ML's notion of a "functor" which is a structure which has had some substructure factored out against a given signature. It's function abstraction at the structure level: someFn (arg : SomeType) = ... arg ... arg ... structure SomeMod (Mod : SomeSignature) = struct ... Mod ... Mod ... Mod ... end In other words, ML functors let you explicitly talk about when a module depends upon another one and allows multiple or merely more specific combinations to be expressed. I'm not sure exactly how to pull this out in Haskell since you'd need to explicitly name the module chosen in your "concrete implementation". It'd be more doable when you try the "typeclass dictionary passing" method of achieving this kind of modularity in Haskell; see Oleg's Finally Tagless Interpreters for
do you know of any examples that I could look at that use newEvent?
No worries! Cheers!
Is this even really a pattern? It seems like just a newtype wapper around a function type, and using a pair of higher-order functions.
https://twitter.com/reddit_haskell 
What we have here is buzzwords being abused
I have a blog post on that: http://haroldcarr.com/posts/2015-02-06-how-to-connect-threepenny-gui-to-external-events.html
While I personally also prefer the looks of `|&gt;` as opposed to `&amp;`, I much prefer writing `&amp;` on a German keyboard, because it's just Shift+6 as opposed to AltGr+&lt;, Shift+&lt; for `|&gt;`.
and he explicitly calls that out in his follow up comments in that he's exploring a point of view not definitions.
"IT'S JUST A PRANK"
Haskell uses resizable heap-allocated stacks. [Some more info here](https://ghc.haskell.org/trac/ghc/blog/stack-chunks). Here's a ticket about [dynamic stack overflow checks](https://ghc.haskell.org/trac/ghc/ticket/8703). What are you trying to assert/validate? Having a tail recursive function in Haskell doesn't have the same implications or meaning as it does in other languages. You'll want to root around for Don Stewart's posts about performance in Haskell to get a proper run down on some practical examples.
The paper itself cites ['Definitional reflection and the completion'](http://ls.informatik.uni-tuebingen.de/psh/forschung/publikationen/StAndrews1993.pdf) by Peter Schroeder-Heister and an [email from Jean-Yves Girard to the TYPES mailing list titled 'A FIXPOINT THEOREM IN LINEAR LOGIC '](http://www.seas.upenn.edu/~sweirich/types/archive/1992/msg00030.html).
Cool. What about other profiles like DSA and SA?
Hrm, what's this "mgu" function/notation in (w) or "from /- gamma(theta) derive /- gamma, not(t = u) if theta = mgu(t,u)"? It seems to have something to do with saying that all values in a type match one of the definitional constructors of the type, along with a substitution that witnesses that fact, but I don't want to be missing something deep here. Section 6 tickles against higher inductive types, even. You simply use a larger definition of equality, but we might lose the proofs of "occurs check failure" and "unification failure" if we have a larger definition of equality. Very cool connection. Thanks for the links!
I agree. And Haskell is an imperative language too, because when you type Haskell you are actually using your brain to send imperative commands to make your fingers write functions!!! HA!!!
A *unifier* of two terms is a substitution that makes both terms equal. The *most general* unifier, `mgu(t, u)`, is a unifier such that for any other unifier `θ` there exists a (unique?) unifier `ρ` such that `θ = mgu(t, u) ∘ ρ`.
GHC is getting a similar feature called Backpack. https://ghc.haskell.org/trac/ghc/wiki/Backpack
GHC does not have a call stack, so "tail recursive" is not the best way to analyze function application. Because GHC uses normal-order evaluation, there is a sense in which all function applications are "tail calls", and in that sense it's trivial to check.
Can you get the instantiated types displayed like that? e.g. in the example, `concatMap` in `hscolour` shows the type as `(a -&gt; [b]) -&gt; [a] -&gt; [b]` but that is something you might as well have looked up in its documentation; can this tool instead display concatMap :: (Either Anchor (TokenType, String) -&gt; String) -&gt; [Either Anchor (TokenType, String)] -&gt; String ?
Why does a module not just correspond to something of the form Service1.Handle -&gt; ... -&gt; ServiceN.Handle -&gt; X.Handle ?
Can't most patterns be described as "just" a combination of some features that happen to be particularly useful in some situations?
The article is very hard for me to follow because of those different font sizes and inconsistent space interleaving between paragraphs.
Great post, thanks! I have medium side code base (~70 modules, excluding tests), where I use type classes to manage dependencies. It is a pain. Now I'm slowly switching to an approach, similar too the service pattern you described, it works much better. I'm still using monad transformers inside modules but I avoid them in interfaces. Few weeks ago I asked #haskell how people do dependency injection in Haskell. The answer was something like "you don't need dependency injection because it solves an issue that doesn't exist in Haskell". My impression is the opposite: [Designing large programs in Haskell is not that different from doing it in other languages](http://stackoverflow.com/questions/3077866/large-scale-design-in-haskell/3078764#3078764) 
I'm pretty sure I don't give imperative commands to my fingers. It's declarative. "that should be a newtype not data" instead of "type the following: cawnewtype".
Until Backpack is there, I'm wondering if package-qualified imports could be used to some extent (See section 7.3.25 [there](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html)).
Vim user spotted. Deploy high five!
I have to agree, the CSS needs to be adjusted. Crucially, `&lt;pre&gt;` needs to have a margin-bottom of 1.404em and `&lt;code&gt;` needs a font-size of like 95%. Those two little changes help a lot!
&gt; One reason for factoring our code using the service pattern was that we wanted to reduce compile times (and reload times in GHCi). Aha! I consider this a very legitimate developer concern. Turnaround time has substantial effects on productivity and experimentation.
Exactly! He's saying there are two ways of thinking about it, and we all have our "language implementors" hat on by default, but its interesting to think from the other side too.
There are some posts on the web making a point in favor of using "value-level polymorphism" instead of type classes (links below). Does the Service Pattern advocate this? Links: * [Haskell's Type Classes: We Can Do Better](http://degoes.net/articles/principled-typeclasses/) * [Scrap your type classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) 
I have i and a on my home row, but i is not under a finger, but one movement away, a is right under my pinky (afaict without typing on a keyboard).
Sounds coherent with my Dvorak experiences, so you might very well be correct. :) I have the i key under my ring finger so it's easier for me.
I'd say the service pattern follows the advice given in the two posts that you link to. One criterion that I use to decide whether I should introduce a type-class is whether I'm actually exploiting the assumption that there is at most one class instance per type. If I'm not exploiting this assumption, then I should not use a type class and thereby make my code usable in more contexts.
I think package qualified imports solve a different problem from Backpack. One will have to use package qualified imports when developing an application that requires two libraries with clashing module namespaces. However this is independent of the service pattern. 
Isn't the difference that you can also introduce new types in a ML-style functor, whereas we can only introduce new values using function composition? 
Well, I guess I don't see that, but sure...
Maybe, but your pattern doesn't have any free variables, and if it did then Service1.Handle Int -&gt; X.Handle would be equivalent to introducing a new variable, or Service1.Handle t -&gt; X.Handle [t] if you want a polymorphic example. 
[This is my favorite introduction to the current OpenGL pipeline](https://www.youtube.com/playlist?list=PLEETnX-uPtBXT9T-hD0Bj31DSnwio-ywh). It's the exact opposite of the way you learn in university -- it is completely example-driven and practical, and easy to follow if you have even a cursory knowledge of C++.
Sorry for disappointing! There are several reasons why there are only trivial case in the post: * this post is not about "cofree comonads" per se, it's about using it for a particular problem: adding source positions to syntax tree. Everything that precedes that is just an introduction to keep the post self-contained. * I haven't even thought about mutually recursive datatypes, because I just don't usually use them.
Here's an interesting variation after giving it a little more thought. Now we use impure modules to generate the handles and the types all in one go via anonymous modules. open Better let main = let config_file = parse_arguments in let module Init = struct module Monitor = Services.Ekg.Make (struct let config = Services.Ekg.parse_c config_file end) module Logger = Services.Logger.Default.Make (Monitor) (struct let config = Services.Logger.parse_c config_file end) module DB = Services.Postgres.Make (Monitor) (Logger) (struct let config = Services.Postgres.parse_c config_file end) module LRUCache = Services.LRUCache.Make (Monitor) (struct let config = Services.LRUCache.parse_c config_file end) module Storage = Services.Storage.Make (Logger) (DB) (struct let config = Services.Storage.parse_c config_file end) module Server = Services.Snap.Make (Monitor) (Logger) (Storage) (struct let config = Services.Snap.parse_c end) end in let open Init in (* Each module has its own already-initialized handle called [h]. *) Server.run snap_c Monitor.h Logger.h DB.h LRUCache.h 
[obligatory xkcd reference](http://xkcd.com/138/)
[Image](http://imgs.xkcd.com/comics/pointers.png) **Title:** Pointers **Title-text:** Every computer, at the unreachable memory address 0x-1, stores a secret. I found it, and it is that all humans ar-- SEGMENTATION FAULT. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/138#Explanation) **Stats:** This comic has been referenced 49 times, representing 0.0903% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cp42wl4)
I'm not sure. It gives a few things for free, like nicely moduled code, and url namespacing, which is nice. A friend of mine is using this pattern to alter the number of servers that deliver the most accessed pieces of an application, which seems reasonable. What are your thoughts?
It doesn't appear that the new GADT instructions have been pushed to https://github.com/AccelerateHS/accelerate-llvm yet... and the paper alone is too light on specifics for me to do a comparison.
&gt; as it does not support separate compilation. What? That's news to me. Indeed it seems that most of the point of backpack is to have modular separate compilation?
My recommendation is that snaplets be used for reusable components and not just code organization. If you're just trying to organize your app code, Haskell provides plenty of ways to achieve those goals and snaplets tend to impose unnecessary boilerplate. Snaplets were intended to be used for standalone components that can be reused across multiple applications.
If you're a commercial user of Haskell and interested in supporting ICFP and the Haskell Symposium, please do get in touch: http://anil.recoil.org/2015/02/18/icfp15-call-for-sponsorships.html
I was about to go on a rant about how you can use the types to figure out what a function does, how some Haskellers (guilty!) find this is sufficient documentation and how this leads to some of our libraries having documentation which looks sparse to novices, but wait a minute. This is Parsec we're talking about, a very well-known package. There's no way this is one of those libraries with sparse documentation. And indeed, the [documentation for many](http://hackage.haskell.org/package/parsec-3.1.8/docs/Text-Parsec-Prim.html#v:many) explains that "many p applies the parser p zero or more times. Returns a list of the returned values of p". And if you go to [parsec's github homepage](https://github.com/aslatter/parsec), there are links to tons of tutorials. So are you saying that despite our efforts, we still have work to do in order to make the documentation for Parsec understandable enough for novices like you? Or are you asking about a theoretical package other than Parsec, with much sparser documentation, like the ones I was about to rant about?
I don't have any tips per say. Trial and error sometimes works well for small libraries which is a great way to discover the benefits and issues within libraries. However, It would be great if you (and everyone) encountered a library with limited documentation that once you/they manage to grasp it that you feedback to the community by helping to write of examples and documentation. In this way we can hopefully eliminate (or atleast reduce) the issue.
I don't see why this is more useful than using type-classes idiomatically.
I drink a *lot*. That said, Parsec has pretty decent documentation, the only problem I usually have with it is actually finding which module something is in.
You may want to look at some of the internals of [quine](https://github.com/ekmett/quine). The Quine.GL modules provide some nice abstractions around gluing together bits of external GLSL, [handling includes](https://github.com/ekmett/quine/blob/master/src/Quine/GL/Shader.hs#L190), [serializing data structures into STD140 format for uniform blocks](https://github.com/ekmett/quine/blob/master/src/Quine/GL/Block.hs#L806), [upload/download screenshots and textures](https://github.com/ekmett/quine/blob/master/src/Quine/Image.hs), etc. It isn't a complete project yet, but there is some useful code in there you're welcome to steal.
Huh well if you share my deep sense of masochism [here's](http://jozefg.bitbucket.org/posts/2014-12-27-folds.html) [how](http://jozefg.bitbucket.org/posts/2014-12-25-operational.html) [I](http://jozefg.bitbucket.org/posts/2014-11-26-conc-supply.html) [read](http://jozefg.bitbucket.org/posts/2014-07-15-reading-extensible-effects.html) [libraries](http://jozefg.bitbucket.org/posts/2014-07-10-reading-logict.html). YMMV but I think it's kinda fun and for small libraries actually not so bad. In general though most libraries either have someone in the know you can bother here or irc (or twitter as a last resort). Types are a useful signpost for understanding though. I would probably say that if a library has more than 300 downloads there are docs somewhere. For lower use packages you may have to actually read them or nag someone to explain it to you :/
only really useful if you already know the name or signature of the function, with parsec the issue is more of "where do functions like this live?". It's not a killer problem, it's just annoying, I think the only time it ever really slowed me down was trying to figure out where the `parse` entry functions were.
I think parsec's documentation is amazing, *once* you understand how the peices fit together. To get to that point it took me a ton of trial and error with some really bad code (even after going through RWH's CSV parser examples). Whether there is a more efficient way to learn it, I have no idea.
Too easy! :-)
These are great. However, I am not sure they will completely solve issue of maintaining consistency of package database during reinstalls. Since it is impossible to know in advance what other packages and/or reinstalls will be needed in the future, it is most important that cabal and ghc-pkg can maintain the consistency of a sandbox or user DB.
A lightweight alternative is manually writing out a few example types in the doc, just like the pipes package does with Proxy/Producer/Consumer.
The Parsec home page ( http://www.cs.uu.nl/~daan/parsec.html ) on Hackage wouldn't load for me, and I ran across a few links to documentation from that site which were also broken. That didn't help my opinion of Parsec documentation any. As I mentioned, I was already working my way through Write Yourself a Scheme, so I had a decent tutorial. I try to seek the most official source of documentation I can, so I didn't look for another tutorial, but looked to the official documentation. The gloss documentation ( https://hackage.haskell.org/package/gloss-1.9.2.1/docs/Graphics-Gloss.html ) at least starts with a paragraph which says things like "for functionality related to X look in module Y". Whereas Parsec documentation ( https://hackage.haskell.org/package/parsec-3.1.8/docs/Text-Parsec.html ) starts with "This library has all you need! If you want to do something advanced and unusual, then customize these functions. Here's a monad transformer data type..." and the list of functions begins. Continuing to use Parsec as an example: I don't want to memorize every single Parsec function, but I would like to feel like I have a comprehensive understanding of the library. One effective way to give me this feeling would be a paragraph or two at the root module explaining how things are organized. Then I know where to look when I want a specific functionality. My intention is not to be critical of Parsec documentation, but to give my perspective as a beginner approaching Parsec's documentation, in response to the parent comment.
Sorry, the code is still on a branch, but it is public: https://github.com/tmcdonell/accelerate-llvm/tree/dev/typed
Yes, it makes sense to describe it as write-only. For our application, program generation, that was exactly what we needed, but it is not what you want if you are interested in transforming the LLVM code. (We rather leave that to LLVM and do our optimisations on the functional AST.)
Sorry, I didn't mean to imply I'm disappointed, more like... frustrated! See, when I think AST, I am immediately thinking of mutually inductive types. And there are all these posts on using the cofree comonad to annotate ASTs, but they mostly just contain the kernel of the idea (that, let's be honest, everyone can easily stumble upon by themselves), without showing how it'd be applicable to a non-toy example.
Yep, words mean something different if you define them differently. Well done.
Of course. Right now if you install package A-5.0, which has package B==2.0 as dependency, you cannot install package C-1.0, which has got package B&lt;=1.0 as dependency (in the same sandbox / on the same system if you're installing packages system wide). This is very common problem source when you get error about unresolved dependencies.
&gt; My intention is not to be critical of Parsec documentation, but to give my perspective as a beginner approaching Parsec's documentation, in response to the parent comment. And this information will be very useful to us in our quest to improve our documentation, so thank you for sharing it. 
How about HASKELL JVM backend? :D
I wouldn't say that. I'd say a pattern is a way of working around the limitations of your language, by (say) adding multiple dispatch to Java (visitor) or something like that in a way that your language doesn't allow to be a library. I wouldn't call this a Haskell pattern. I'd call something like Oleg's finally tagless approach a Haskell pattern, especially if you aren't using the helper TH, since it adds extensible records to Haskell. I wouldn't call HList a pattern, but rather just a library even though it added hetrogenous lists to Haskell.
i like that tutorial. go with it
Slightly off topic, but how did you get that fancy vim editor (plugin? Mac app?)
I wrote this for when I needed to upload lens documentation. It detects the package from the cabal file in the current directory and has been used to upload docs for a number of packages. https://github.com/ekmett/lens/blob/master/scripts/hackage-docs.sh
&gt; With this point cleared up (if I haven't misinterpreted everything), is my argument still fallacious? I don't believe so. I'm a little suspicious of the implications you draw toward the end, and I think we *see* more libraries than applications because the commerical users are more willing to contribute libraries than they are applications.
&gt; I don't know what you are arguing for. I honestly had to look back through the thread context. I am advocating ignoring provably incorrect ideas and proposals motivated by those ideas. I also said that we shouldn't think too much of now, because the future is so much bigger. But, that's hard for me to do. I'm nearing the midpoint of my life expectancy and I'm still a bit flighty with my utility function.
Aw, poop! I was hoping to learn of a shiny new toy! :)
I generalize whenever I already know or am made aware of the generalization and would either directly benefit from the generalization or I can conceive of a scenario where users of my API might benefit. Generalizing without clear benefit like buying shares in something you don't expect to improve, even if you don't have to add another dependency; adding a dependency *just* to generalize to doubling-down on a bad bet. `Foldable` I use a lot, not so much with `ListLike`, possibly because I'm not as familiar with it. I almost never use `MonadIO`, because it usually doesn't buy you that much. Writing an `a -&gt; IO b` with `MonadIO` will, at best, save each user of your code a single call to lift. Writing a HOF that takes a `a -&gt; IO b` as an argument using `MonadIO` would be a real win, but I think that's a little more rare.
&gt; commerical users This whole debate would make much more sense if we agreed that * the maximum amount of utility would be achieved if Haskell was used more widely in industry * and therefore optimising for non-industrial uses is a waste of time (I'm assuming that not particularly caring about build times, bandwidth, compatibility windows, choice of OS, etc. is mainly a trait of commercial users.) However, even then I don't see how my concerns about “casual users” don't apply. Having more casual users would clearly be better for Haskell's adoption in industry, and once you agree with it, having to *care* about casual users' needs (one of which is not-depending-on-million-line-libraries) seems to follow naturally.
That's pretty cool.
Yes, write feedback! And try asking the authors of the library for help, too.
Hm, what do dependencies have to do with these? Okay, well, “Being able to statically eliminate more bugs.” does require depending on things like `semigroups` and so on more often than an average Haskeller does, but it's merely a quirk of Haskell's ecosystem that * We don't have good array/text/container types in base, so they are all provided by outside packages, and those are bigger than they would've been if they were a part of base. An example: containers has `Sequence` and `Graph` in addition to `Map`, but `Map` is used much, much more often than either `Sequence` or `Graph`. If `Map` was in base, a *lot* of packages would suddenly lose the containers dependency. * Many packages providing fine-grained type classes are written by Edward Kmett, and Edward's libraries all depend on each other for reasons not entirely clear to me. So, I see 2 ways to achieve the “Being able to statically eliminate more bugs.” goal: 1. Reshape Haskell's ecosystem to make most common dependencies lightweight. 2. Never be reluctant to depend on heavy-weight packages if it helps to eliminate bugs. I think that if your goal is to reduce the amount of bugs *in general*, as opposed to “only in your libraries”, you should be advocating for #1, not for #2. Moreover, #2 might actually hurt this goal if your more safe library won't get used as much (due to a million-line-library dependency).
doesn't that just mean it might be merged back at any time later or never?
Yep. It’s all about learning how to make those logical connections based on the types you have available. While types are sometimes underspecified (easy to read, but not enforcing enough invariants) or overspecified (enforcing lots of invariants, but hard to read), most of the time the types and a couple of examples are enough to put things together. It’s unfortunate that you often have to read the source, but also fortunate that the source is almost always available because of how Hackage is set up. As an aside, it’s “to pore over”, not “to pour over”. English is silly. 
The “functional” in “functional programmer” actually refers to “functional alcoholic”.
That sounds like something that is a fair bit bigger than a single student summer project.
&gt; Wait... in this scenario I'm /u/bos? ...I have spent more than a year thinking this. Every single time I noticed your comments on Reddit. *facepalms*
[Image](http://imgs.xkcd.com/comics/compiling.png) **Title:** Compiling **Title-text:** 'Are you stealing those LCDs?' 'Yeah, but I'm doing it while my code compiles.' [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/303#Explanation) **Stats:** This comic has been referenced 352 times, representing 0.6475% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cp4yqiv)
&gt; People that would say adding an orphan instances is better than adding a dependency are the "provably wrong" people. ;) Okay, just in case it's a misunderstanding: I say that adding an orphan instance *can* be better than adding a dependency. Am I provably wrong?
&gt; a package containing only type classes would, in some sense, actually have the worst power-to-weight ratio (all interface, no instance) even if they were very lightweight I admit I don't like this backwards reasoning at all: * power-to-weight ratio is important * a package with only interface and no implementation would have 0 power-to-weight ratio * therefore, a package with only interface and no implementation would be nearly useless What I conclude from it is *not* that interface-only packages are useless, but that the power-to-weight ratio is clearly not always a valid metric. It's the same kind of fallacy as “more freedom is better” – limiting freedom lets us have cool things, such as eliminating bugs, automatic parallelisation, proving properties of our programs, etc. In the same way interface-only packages enable us to have much better dependency control, *as well as other things*; I already said in another comment that having an interface-only category theory package would let us have: * an usable subset of lens which would work with JHC * more wide adoption of lenses * prisms and isos in wide use (which *directly* fulfills the “statically eliminating bugs” goal) I get it – if everyone wasn't being silly and we all were fine with depending on lens as we ought to, we would have all those things as well. (Except for JHC support, fine, but that could be solved separately.) But given that we most likely are going to have a bright future with lenses and statically eliminated bugs *either way*, I'm advocating to choose the way which would a) bring about this future sooner, b) cause less pain to people in the process.
Ah wait, I think I understand. If you have module Service1 where data Handle = ... module Service1Impl service1 :: Service1.Handle module Service2 where data Handle = ... mkService2 :: Service1.Handle -&gt; Service2.Handle module Main where service :: Service2.Handle service = Service2.mkService2 Service1Impl.service1 then when you change module `Service1Impl` you only have to recompile modules `Service1Impl` and `Main`. If you didn't use this "modular" pattern but instead just imported the definition of `service1` into `Service2` then you would have to compile `Service2` as well.
In old C++ you'd use `typedef`. It's been `using` for years now unless one is stuck with an old compiler. Which some unfortunately are.
Yeah. I also assume that my Interface class, being simple as it is, also has some other, well known, name. But since I redefined it, I thought I'd redefine the operator to something that looks better to me.
If be more than willing to help out in what ways I can, myself being a casual user of Clay! 
I always liked Clay but I found myself not caring about CSS enough to want it in a DSL, the overhead/layer turns out to be a worse trade-off than e.g. using an HTML DSL. I don't know whether this is why you don't have a use for Clay anymore or you're just not doing web dev. --- Relatedly, the [css](https://hackage.haskell.org/package/css) namespace is also up for grabs. Anyone is welcome to update or replace it entirely.
For most projects, I have a policy of making everyone who submits a pull request that gets accepted a collaborator.
&gt; fortunate that the source is almost always available That's one of the things I really miss about documentation for libraries in other languages. So silly to make me search for their Github and then navigate their code structure when they could have just provided a link.
But with a legacy API, though, no? That bothered me a few years ago when I was trying to learn Parsec. All tutorials were really good and I understood perfectly the code in them... only when I imported Parsec I got a different API so all I knew was useless. (Much later I figured out that the legacy API was still exported in a compatibility module somewhere, but by that point I had learned the new API the hard way (by following the types) instead.)
I don't have any *large* open source projects, but the few smaller ones I did this with had relatively good success. If they stop contributing, they're probably no longer using it, which is ok too.
&gt; Use low-level OpenGL bindings such as OpenGLRaw and gl. For some mysterious reason, this seems to be a very popular opinion these days, but I don't see any reason not to use the medium-level OpenGL bindings [OpenGL](http://hackage.haskell.org/package/OpenGL), instead of [OpenGLRaw](http://hackage.haskell.org/package/OpenGLRaw), the latter being certainly much more inconvenient, unsafe, and anti-Haskell-ish. Especially for a school project there is zero reasons to go for the low-level binding. To partially answer the OP's question, shaders are more-or-less language-independent in the sense that they have their own (c-like) language. Basically you give your shader source code to the driver, which compiles it down to something the hardware can execute. As the parent comment said, it is possible to generate the shader source code from some very-high-level description, but this is pretty advanced software engineering. LambdaCube does this (there was another one which I cannot find at the moment)
You might also want to look into existential types, which actually are vtable-dispatching types. (As I learned from the comments in A Route To Learning The Haskell Type System :) ) While we're talking C#: The IEnumerable interface is _awful_ (and it's awful in C#, not just in Haskell). It just about works in a single threaded environment, but two calls to get the next element each time with no way to synchronize between them is problematic in quite a lot of contexts.
I attempt to understand existential quantification every now and then but it hasn't yet clicked. Got anything I could read?
I find clay to be a wonderful tool. I hope it finds a maintainer.
On the unrelated: please stop propagating this 'C and Fortran are dead or dying, as they should'. Even in jokes, it stopped being funny years ago. It assumes that little things and places like 'CERN' and almost any supercomputer / HPC science-oriented place will gladly go from decades of well researched and stable C/C++/Fortran libraries and tools to run everything through some freaky API on server that runs Node. Because, you know, people making superconductors and research nanoparticles (among metric tons of other stuff, my flatmate works on purely ceramic jet turbines) are fringe and unimportant compared to making smartphone apps or Firefox add-ons.
On a somewhat related note, I'd personally love to see them both killed by Rust.
There often *are* tutorials for libraries, scattered about the web. **Googling** the package name or module name can often be useful. Playing around with the library in **ghci** can be extremely helpful. Never under estimate the learning benefits of playing around. Learn to use `:t`. This helps you to learn which values can be plugged into which functions. Just reading the types doesn't always grant this intuition; it's much more intuitive to see whether an actual expression does or doesn't typecheck. Use `(undefined :: SomeType)` when you don't want to bother creating an actual value of `SomeType` and you just want to play around and figure out what holes it can fill. **StackOverflow** is a pretty good place to ask for help when you've got a specific problem and you want to use a specific library to solve it. Often just formulating a good question can help you gain insights about the problem at hand. Of course **reddit** is also a great place to ask for help. Other people-resources include the **mailing lists** and the #haskell **irc channel**. The Haskell community is notoriously helpful. Most libraries have a **maintainer email** listed on Hackage as well. Even if they don't have the time to help you personally, then can almost certainly point you to the resources you are looking for. And finally, "use the **source**, Luke." Maybe not the best learning tool, but source reading is a valuable skill nonetheless.
As SP Jones has said in one of his talks, C/C++/Fortran are immortal, as is Ruby, Java, Javascript, etc. 
This is one of the variants I've played with. It may even be better. FWIW- You aren't alone in wanting it. When I brought up the previous idea, what you gave above is what Duncan wanted as well.
Fortran won't die until either a new low level language implements basic matrices, or scientists learn to use abstract data structures. Fortran isn't dying basicly.
They moved to [better.io](http://www.better.io/), which explains the "new" website at better.com at least.
We aren't in a time shell. This is cloud cuckoo land!
Speaking as one who sometimes says that, my problem is that I feel _sorry_ for those people, having to work in such a hostile language that makes them worry about all sorts of things they shouldn't ought to have to worry about. It isn't that I think they aren't doing work important enough to be worth worrying about, it is that I think the work they are doing is _too_ important to be stuck in C. "People making superconductors and research nanoparticles" shouldn't be fighting with memory corruption errors when we _know_ how to avoid them. They shouldn't be worried about manual memory allocation, or fighting with pointer segfaults. It's just an atrocity that that is the best thing the programming community appears to have to offer them. I'm a professional programmer whose life it is to know how to deal with that sort of thing, and _my_ technique is to avoid C as much as I possibly can. I have a lot of other options, and I take them. They should be given the courtesy of the same option without having to know as much as I do about the programming language field.
I would love help with [IHaskell](https://github.com/gibiansky/IHaskell). IHaskell is a tool for interactive computing -- if you haven't used it (or it's ilk, IPython and friends), imagine it as an REPL that can also do graphics, animations, interactive widgets, and so on. You can take a look at the [demo notebook](http://gibiansky.github.io/IHaskell/demo.html) exported to HTML, although it's a little bit outdated. I've been pretty swamped with other stuff lately, so can only dedicate a few hours a week to IHaskell, but am very happy to mentor others and get contributions. It's a pretty young project with a lot of open directions, so it's easy to find something that different people are interested in. If you're interested in helping out, just shoot me an email (andrew dot gibiansky on google's mail). (We're going through a bit of a rough patch right now as I'm scrambling to get IPython 3.0 and GHC 7.10 support out the door, so getting everything set up *might* be a little tricker than it usually is, but I'm happy to help with that too.)
I am looking for contributions to my package "abt", which provides an alternative to Bound and Unbound using abstract binding trees (à la PFPL). In particular, some of the things that would be interesting to add would be: 1. generic and style-able pretty printing of abts 2. generic parsing of abts 3. general purpose utilities for dealing with contexts 4. making the interface a bit easier to understand 5. better documentation and tutorials I'm sure there are a number of other things that would be useful or could be improved. please get in touch with me if you are interested in helping; I am happy to help you get up to speed too. EDIT: The project is on GitHub here: https://github.com/jonsterling/hs-abt, and on Hackage here: http://hackage.haskell.org/package/abt.
I think the GSOC projects for better C++ FFI compatibility would cover this, modulo the QObject and cmake yaks.
While I intend to do this myself sooner or later, if anyone likes both Diagrams and HsQML, and wouldn't mind having a look at my backend https://github.com/marcinmrotek/diagrams-hsqml to find out why isn't it rendering Charts properly (among others I guess, but this is what I tested it on), then I'd be grateful.
I'm not sure I agree with the sentiment (though I do agree with the two comments above it). Fortran _is_ fading; it will never die completely, but it's already mostly relegated to specific domains and maintaining old large codebases. Starting completely new projects in Fortran requires serious justification. C is alive and kicking, but there _are_ decent arguments for why C should die. It was a great language for its time, and its time has not yet passed, but that's because it doesn't yet have a successor. I'm very sympathetic to the sentiment expressed in [these](https://www.mail-archive.com/bitc-dev@coyotos.org/msg03516.html) [two](https://www.mail-archive.com/bitc-dev@coyotos.org/msg03550.html) emails: they muse about a Pigovian tax on C (analogous to a carbon tax), since C code is the source of many of the worst security vulnerabilities (made by experienced programmers; I don't want to think about SQL injection vulnerabilities in amateur devs' code), and this makes everyone worse-off. It's true that most of the derision is unwarranted, but that's not to say that there aren't legitimate and serious criticisms.
[Haste](http://haste-lang.org) could always use more help. Lots of things need to be done, but it's pretty hard for me to justify spending time on them unless they more or less directly contribute to a publication or two.
[PureScript](http://github.com/purescript) has a very welcoming developer community, and we are always looking for new contributors. Anyone who is interested can join us on IRC in #purescript, or take a look at the [issues for the next milestone](https://github.com/purescript/purescript/issues?q=is%3Aopen+is%3Aissue+milestone%3A0.7.0). Documentation is another area where we could use some help.
I really like "construct, combine, eliminate" as a conceptual template for functional programming.
They are still in the picture. Regarding location, Gleb Peregud at Google stepped up to offer to host it / organize this time around.
&gt;&gt; It's just an atrocity that that is the best thing the programming community appears to have to offer them. I think you are rushing too quickly to fit my opinion into a prepared slot in your head. My point is not that C doesn't have good things, or that I've got a drop-in replacement ready to go. My point is that it's an atrocity that the only way to get those good things is also unnecessarily bound to the bad things it brings with it, and I explicitly already pinned the blame on the programming community for not giving these people better options. And I'm not really into fashionable self-flaggelation, so when I say that, I'm serious, and I've put some thought into it. The programming community has put up with C's crap for far too long out of misguided ideas about being "real programmers", or grossly inflated beliefs about their ability to handle its sharp corners, or incredibly immature judgments about speed vs. safety as if they are always in conflict with each other and "speed" should automatically win. These people who would not consider themselves "programmers" are caught in the crossfire, and should probably be even angrier than I am. Also, /r/haskell this may be, but a casual perusal of the rest of my posts here would show me very, _very_ firmly on the practical side of things relative to most people here. My complaints about C are not "it's not close to the glory of Haskell", but rather more that it just _begs_ bad programming practices. You could get [some of the idea of that statement here](http://www.jerf.org/iri/post/2942), which is focused on security but it trivially generalizes to just in general whether a language encourages bad practices or not. C is bad whether or not Haskell exists. In 2020, I hope to be able to recommend Rust but we certainly aren't there yet... and note how many years I'm allocating for it to reach that point. I didn't say 2016 for a reason. Perhaps it's worth just pointing out in general that computer programming languages are poor things to incorporate into your personal identity, or your conception of anybody else's identity. Criticisms of C are not criticisms of its users.
You're welcome to write some docs on Haskell advanced topics https://github.com/commercialhaskell/haskelldocumentation
Oh, come on. It's cute.
Leave it to Dan to write the ultimate "man bites dog" article about Haskell. Great work! :)
You've pretty much described Ada, from what I can tell. It's like C, but with a stronger type system and less of that manual heap management and undefined behaviour at every turn. Sure, there are tradeoffs to this – writing an Ada compiler is considerably harder than writing a C compiler, but how often are we writing compilers this day and age, anyway?
I suppose the challenge of such a derivation mechanism is that not all types have a single obvious correct applicative instance. The canonical example is the zippy list versus the non-deterministic list. It feels more like a place for TH than a language pragma, imo.
It would be a good idea to have a place in which people can create roadmaps for any project where they can announce task slots, status, needed skills etc for the people that may want to participate. Something like roadmaps.haskell.org? In the meantime, this is my roadmap for MFlow/hplayground/Others. We welcome people to contribute in any way! from development to suggestions, documentation...: https://github.com/agocorona/MFlow/wiki/Roadmap I know, it is ambitious, heterodox and challenging, but I want to change how things are done out there in the industry .. or die in the effort ;)
Yet another coup by Manuel and his crack research team. Great work!
I'm not a huge deriving code fan, so I just am leaning toward TH ever since Aeson and Lens began to popularize its use in that way. Never been quite sure why.
Hackage is not only for fully usable code. It is a general collaboration tool - a way to make it easier for collaborators, or for people who just want to try it the way it is now. So, please upload! Because of that, it is important to communicate the status of the code to potential users via the package description and version number. It was originally intended for Hackage to grow features that would provide standard ways to communicate that information, which would then also allow for some automated tool support. But such features are unfortunately still lacking.
&gt; Would you mind linking it sometime? Sounds like actually sensible point of view. [Here](http://youtu.be/ILSFJXeNSHw?t=52s) is the relevant part from his "being lazy with class" talk.
You can use `proc` from `turtle`, which works with `less` correctly. Just invoke it like this: proc "less" ["bigfile"] empty That in turn just a wrapper around `createProcess` from `System.Process`, which you can also use to achieve the same effect.
This reminds me a little bit of `Align` from the [these](https://hackage.haskell.org/package/these-0.3/docs/Data-Align.html#v:alignWith) package: alignWith :: (These a b -&gt; c) -&gt; f a -&gt; f b -&gt; f c 
How about looking at the GHC source code?
Here's a bit of POSIX magic that allows you to pipe the standard output of *any* IO-computation to less. import System.Process import System.Posix.IO import Control.Exception less :: IO a -&gt; IO a less a = do -- make a copy of stdout stdout_copy &lt;- dup stdOutput -- launch less that reads from a pipe and writes to the terminal (Just pipe_handle, Nothing, Nothing, pid) &lt;- createProcess (proc "less" []) { std_in = CreatePipe } -- close stdout (remember, we still have a copy of it as stdout_copy) closeFd stdOutput -- obtain an fd for our end of the pipe pipe_fd &lt;- handleToFd pipe_handle -- make the pipe our new stdout dupTo pipe_fd stdOutput -- close pipe_fd, so that less can observe the EOF closeFd pipe_fd -- run the computation, and restore the normal output afterwards a `finally` closeFd stdOutput `finally` waitForProcess pid `finally` do dupTo stdout_copy stdOutput closeFd stdout_copy test = less $ mapM_ print [1..300] 
&gt; Hackage is not only for fully usable code. It is a general collaboration tool - a way to make it easier for collaborators, or for people who just want to try it the way it is now. So, please upload! Really? Interesting! I always assumed that projects were supposed to incubate on GitHub or otherwise and then only promoted to hackage once deemed ready.
It has only worked for me once, but it got me the help of the amazing Anthony Cowley, so I cannot complain! :)
&gt; Unfortunately, most library authors expect everybody to be at their level and implicitly assume same level of expertise/knowledge which is not the case. That's called the "[curse of knowledge](http://en.wikipedia.org/wiki/Curse_of_knowledge)": it's very difficult, once you know something, to imagine yourself in a state of not knowing that thing or even to remember how you were thinking before you learned of it. That's why we can't just go over our documentation and improve it: we need a lot of feedback from people who don't understand the existing documentation in order to even have a clue about which parts need to be improved.
Actually I just realised it's FTP(relude), so yeah, the question is bad. But I still think it would be worth the consideration/discussion. 
The typical zippy `Applicative` is sort of a structural intersection; given `f (a -&gt; b)` and `f a`, `(&lt;*&gt;)` lines up the `f`s and, wherever they have the same shape, combines them into one structure using function application. `pure` produces a maximal structure (which is the identity for intersection) containing copies of a single value. The intersection-like behavior is (I think) required by the types and laws. The `fzip :: f a -&gt; f b -&gt; f (a, b)` and `funit :: f ()` formulation of `Applicative` probably makes this clearer (and corresponds more directly to `Align`). `Align` is a structural union in the same sense. Each location in the result structure may correspond to locations in either or both of the inputs, which is where `These` comes in. The most obvious practical example of this is probably wanting to zip two lists with padding, e.g. `malign` which zips with `mappend` and pads with `mempty` and has an *excellent* name.
I think "it builds, ship it" is an acceptable readiness level for hackage. Though if the code flat out doesn't work in significant ways, a notice to that effect up front is appreciated. ;]
Certainly true--and is of course how it's implemented (i.e., `alignWith (mergeThese mappend)`), on account of I was thinking before typing when I wrote it, but not when I wrote my earlier comment. :] But it's a trivial convenience function, not even part of the class, so favoring the type class in `base` (and soon `Prelude`, I think!) seemed more helpful to me. At whatever point `Semigroup` is added to `base` as a superclass of `Monoid` I suppose I'll have to mourn the name, but under those hypothetical circumstances I think I won't mind too much, all considered.
NB: 'Zippy' exists in some fashion as MonadZip in Control.Monad.Zip in base. It is used for parallel list comprehensions when you turn on monad comprehensions.
Well, okay. I can *instantiate* the class with the obvious type-tetris solution.
And it's fun with `State s`, and the like. But with `Maybe` it solves the Halting Problem in a pleasingly direct manner.
Truly "Zippy" Applicatives are reader monads under the hood. They are isomorphic to a function from some representation 'x'. if there exists an `x` such that for all `a`. `f a` is isomorphic to `(x -&gt; a)`, we say `f` is a corepresentable functor. Lots of Applicatives come from this process. e.g. the infinite stream comonad can effectively be thought of as `(Natural -&gt; a)`, etc. [Edited: The original version of this said `Conat`] What you have there is a bit different, it could have extra data inside of it that gets smashed together monoidally when you zip. That said, A more 'match' like behavior that is appropriate to unification would be to fail when the extra stuff doesn't fit. FWIW- I have the class you give there (modified to Alternative) [on my github account](https://github.com/ekmett/unification/blob/master/src/Control/Unification/Class.hs#L21 ), complete with GHC Generics support: class Traversable f =&gt; Unified f where unified :: Alternative m =&gt; (a -&gt; b -&gt; m c) -&gt; f a -&gt; f b -&gt; m (f c) default unified :: (Alternative m, Generic1 f, GUnified (Rep1 f)) =&gt; (a -&gt; b -&gt; m c) -&gt; f a -&gt; f b -&gt; m (f c) unified f as bs = to1 &lt;$&gt; gunified f (from1 as) (from1 bs) That said, Conor makes a pretty solid case that a more fundamental operation would be zipWith :: (a -&gt; b -&gt; c) -&gt; t a -&gt; t b -&gt; Maybe (t c) instead, because it can handle the (partially) exponential/naperian/corepresentable/function space/whatever you want to call it case as well. It doesn't compose as well and doesn't have a good generics story, but it handles more cases. At least for me, the "zippy" monad for a representable functor is a bit boring. I use it to glue together libraries like `linear`, but I want it maybe 20% of the time, not 95-100% of the time like when I `DeriveFunctor, DeriveFoldable or DeriveTraversable`. I also have code supplying all sorts of default definitions for a real Representable functor. http://hackage.haskell.org/package/adjunctions-4.2/docs/Data-Functor-Rep.html
Ah, so it does. It's one of those things I tend to overlook.
What sort of generalizations were you thinking of for those functions you mentioned? How could they take advantage of Foldabe/Travesersable, besides foldableToMaybe? 
None of these ever include APL even though APL is a very relevant language in the industry and weird enough to warrant jokes about it. I think it's just not hipster enough and people just don't know it enough to joke about it.
Sounds interesting... could you give a bit more context? :) What are abstract binding trees, how does this differ from Bound (you mean the Haskell library bound or something else?), and can you compare / contrast abt with other libraries or approaches for doing similar things?
Firstly, `Stream x` is iso to `Nat -&gt; x`, for *inductive* numbers. There is no element in position infinity, nor can you wait that long for the projection. In general, `Log` takes limits to colimits, e.g. `Log` of a container product is a sum type and `Log` of a coinductive container is an inductive type. In that respect, it resembles derivatives, and that's because the logarithm is the shape of the derivative. `Log f = D f ()` when it exists. Secondly, I'm very happy to note that functors which happen to be both `Traversable` and `Matchable` give you a unification algorithm for the induced first-order syntax. The combination is highly valuable. But the two are separable, exactly because `Traversable` is a condition only on positions and `Matchable` is a condtion only on shapes. There are non-Naperian examples of containers which are `Matchable` but not `Traversable`. data Foo x = MkFoo (Maybe (Integer -&gt; x)) springs to mind: there are but two shapes, even though the element position set is either empty or infinite. Crucially, though, `Matchable` is not closed under composition. To ensure that `Matchable (Compose f g)` you need either the boring possibility that `(Matchable f, Naperian g)` or the fun case where `(Matchable f, Traversable f, Naperian g)`. The shapes for `(Compose f g)` are `f`-structures of `g`-shapes, so deciding shape equality can require visiting each `f`-position.
It will also vomit a massive amount of generated code before compiling.
Your HasPostgres instance is missing a method. You need to provide a way to run a function within a modified version of your `App` type. I.e you need to implement a `local` function for your reader monad. You can do it easily with the lenses you already have.
I don't know, but empirically, it's not, or it would be seeing more use, especially outside of high-assurance domains (IIRC, Ada is known for being used by NASA).
 let catMaybes' = Data.Foldable.foldr (\a b -&gt; if isJust a then fromJust a:b else b) [] :t catMaybes' catMaybes' :: Foldable t =&gt; t (Maybe a) -&gt; [a] let mapMaybe' f = Data.Foldable.foldr (mapMaybeFB (mappend) f) mempty :t mapMaybe' mapMaybe' :: (Monoid b, Foldable t) =&gt; (a -&gt; Maybe b) -&gt; t a -&gt; b Not sure about either of them, to be honest, but the first one was exactly something I had to do and the reason I got to post this question. I actually did something like that and slapped a fromList after that to get back in the structure I've used. edit: let mapMaybeFB cons f x next = case f x of Nothing -&gt; next; Just r -&gt; cons r next -- From the Data.Maybe source
Aye, when `f ()` admits multiple monoids, `f` is applicative in multiple interesting ways. But if `f ()` has only one value, then it has only one monoid structure. Do we then get a canonical `Applicative` instance? Worth a look-see.
&gt; Do you really want to subject thousands of people to programming in a language without GC, without closures, without sensible metaprogramming or a decent type system? Yes (not necessarily to all of the above), as long as building these features on top of assembly stays far from trivial.
I see you already mentioned the `derive` tool -- I was going to suggest it to you! /u/ndmitchell might have some suggestions as to how to handle the cases you suggest. Regardless, it seems to me it might make sense to "teach" `derive` to be smarter, rather than try to reinvent the _whole_ wheel.
It is. import System.Process import System.Posix.IO import System.Posix.Types import Data.IORef import System.IO.Unsafe {-# NOINLINE pid_ref #-} pid_ref :: IORef (ProcessHandle, Fd) pid_ref = unsafePerformIO $ newIORef undefined browseLess :: String -&gt; IO String browseLess mod_ = do -- make a copy of stdout stdout_copy &lt;- dup stdOutput -- launch less that reads from a pipe and writes to the terminal (Just pipe_handle, Nothing, Nothing, pid) &lt;- createProcess (proc "less" []) { std_in = CreatePipe } closeFd stdOutput -- obtain an fd for our end of the pipe pipe_fd &lt;- handleToFd pipe_handle -- make the pipe our new stdout dupTo pipe_fd stdOutput -- close pipe_fd, so that less can observe the EOF closeFd pipe_fd writeIORef pid_ref (pid, stdout_copy) return $ unlines [ ":browse " ++ mod_ , "finish" ] finish = do closeFd stdOutput (pid, stdout_copy) &lt;- readIORef pid_ref waitForProcess pid dupTo stdout_copy stdOutput closeFd stdout_copy Now put in your .ghci: :load path/to/less.hs :def browseLess browseLess And try it: &gt; :browseLess Prelude
Same to u.
FWIW- `Naperian` also implies a whole lot of other structure, it's basically saying `f` is isomorphic to the reader monad for some environment that is `Log f`, so you can demand a whole lot of instances if you want: http://hackage.haskell.org/package/adjunctions-4.2/docs/Data-Functor-Rep.html e.g. Monad f, MonadReader (Log f) f, MonadZip f, etc. not just Applicative.
`f ()` having only one value is really strong. It seems like any such type ought to be isomorphic to `Fin n -&gt; a` for some `n`.
Wait, why can't we say that the element at position infinity is \_|\_? Doesn't that work perfectly well?
Yes. ;) Specifically orphan instances can cause silent issues at runtime, not just loud, nearly blocking issue at compile time. I'm assuming of course, that you think bugs are a bad thing.
&gt; But then I don't even know what reasoning rule you are following, because it's clearly not consequentialism. Even consequentialism can't blame me for *all* of that time. Not if the amount of blame is proportial to the amount of effect. There are a lot of mitigating factors between adding a dependency and wasted time, even if our lizard brain "robs" us of what we believe should be conscious choices. Plus, I think you vastly overestimate the amount of time consumed -- the amount of wasted time per year should decrease in the future, not increase, because of advanced in technology AND because of improvements in the ecosystem.
Fixed. Thanks!
&gt; essentially encouraging people to get rid of the “trust judgement ...” heuristic I think more people *should* get rid of the trust-authority heuristic, particularly when if they are applying it to unsubstantiated claims of single, famous individuals.
Yep. In any case, your original point is well taken.
Wait, what? No, I was posing all of that as serious questions. Honestly, it is not some trolling attempt or some stupid bias on my part. But I have to say that I have tried Python, Julia, Common Lisp, Java and for the past five years I likely experimented with most of the mainstream languages. At this point, C and Fortran are still my firs choices. But, seriously, it is not me being biased or too lazy to really learn something. I will however grant that I am unable to see most of the problems you attribute to C. For starters, I don't have to care about C string vulnerabilities, about 100% of programs I ever got require basic ability to add to avoid all the fun you describe that comes from pointers. Race conditions? I remember literally two times when it came into play. We really have different needs. To a point where I simply don't get a lot of said problems. It does not mean that I am not genuinely curious about alternatives. EDIT: To add some more I have also tried: D, Sage (Mathematica fork-alike in Python), Go and likely that is it. By tried I mean that I literally tried how it would work when ran on computational grid where I worked. D is closest to my needs, and I would love to see it getting as replacement. As far as garbage collection goes: In most cases, scientists don't have all that much to remember. From my perspective you are making a lot of fuss resulting from *having to* write`free` a few times and watch it in up to five relevant places throughout the code.
If what you mean to say is that the routes DSL is compiled to Haskell code at compile time using TH, then yes, that's how it works. It's not all that much generated code though. The main thing in the usual Yesod workflow that results in a large amount of TH-generated code is the use of shakespearean templates.
See the [pearl](http://www.cs.tufts.edu/~nr/cs257/archive/chris-okasaki/breadth-first.pdf) by Okasaki
Tmux is great. One annoyance however is that if a lot of text is generated, you have to manually scroll upwards to find the beginning, unlike with less.
Long your learning will be young padawan. Strength you must find. Strong the force is on Haskell. Go there and stay now you must.
As circlejerky as it sounds, I feel the same. Which language do you haskellers recommend for people wanting to open up to other languages?
Depends on which parts of Haskell you like. Elm is interesting for webby stuff. Clojure is interesting for immutability and access to Java ecosystem. Prolog is interesting for declarativeness. Scala is interesting because relatively popular. Ada is interesting for its strong type system and low-level stuff. Try a bunch of them and see what you like.
[JuicyPixels](https://github.com/Twinside/Juicy.Pixels) could benefit from new image file loader, and [rasterific-svg](http://hackage.haskell.org/package/rasterific-svg) and [svg-tree](https://github.com/Twinside/svg-tree) could have a look for svg filters effect. [FontyFruity](https://github.com/Twinside/FontyFruity) need much more love to implement hinting bytecode from true type font and/or expand support for open type fonts.
&gt; guess it's the end of my programmer life... Welcome to mathematics. You still won't be employable.
Yeah, sure.
I'll tell you tomorrow.
C (this is not a joke)
Also, I'd be a bit wary about underestimating the vocabulary that Haskell gives us. There might well be a way to talk about productivity of the computation coming out which justifies allowing infinite data in. It's worth thinking about (e.g., making use of equipment like `Clock c`, being an `Applicative c` with a Loeb-style `fixC :: (c x -&gt; x) -&gt; x` to demand "enough today to keep going").
I think the community would benefit the most from contributions to shared tools like [cabal](https://github.com/haskell/cabal) or [hackage-server](https://github.com/haskell/hackage-server). Alternatively, our [rest suite](https://github.com/silkapp/rest) always welcomes new contributions.
Anybody like Haskell and in china, welcome to join , let's talk #Haskell
## Using (non-Haskell) external libraries First of all, many hardware libraries have a C interface. For example if you want to get data from a serial port, you would have something like VendorHardwareStatus initDevice(DeviceID, Handle*); VendorHardwareStatus status(Handle*); VendorHardwareStatus aquireData(Handle*, size_t length, void* destination); VendorHardwareStatus deinitDevice(Handle*); If you want to use that library together with Haskell, you're bound to know at least enough C to use pointers, functions, basic data types, memory allocation and aliasing (funny enough, you also learn a little bit about C-style object oriented programming). Then, you can easily create your Haskell interface: data Handle data SerialMonad initDevice :: DeviceID -&gt; SerialMonad (Either ErrorStatus Handle) deinitDevice :: Handle -&gt; SerialMonad (Maybe ErrorStatus) aquireData :: Handle -&gt; Size -&gt; SerialMonad (Either ErrorStatus ByteString) aquireData' :: Handle -&gt; ByteString -&gt; SerialMonad (Maybe ErrorStatus) runSerial :: SerialMonad a -&gt; IO a If you don't know C, you need to wait till someone writes that library for you. If you know C, you can cobble the missing parts together quickly. ## There's not only one paradigm Given that it's possible to write imperative, procedural, object oriented code in C, you learn quite a lot if you try to realize all those programming paradigms. Sure generic programming (Generics in Java, templates in C++, others: ??) is messy in C. But there are some gems which do this quite elegantly. Have a look at PETSc, which uses an object oriented paradigm quite well (tl;dr: matrixes basically behave like data Mat = { matMult:: Maybe (Mat -&gt; Mat -&gt; PETSCResult Mat) , matAdd :: Maybe (Mat -&gt; Mat -&gt; PETSCResult Mat) , matValue :: Maybe (Mat -&gt; Int -&gt; Int -&gt; PETSCResult PETScReal , … } which is actually quite cool). ## You'll need to learn it anyway. C's syntax is pervasive: C++, C#, D, Java, JavaScript and others are heavily influenced by its syntax. If you learn any of them, it's not that hard to jump back to C, especially if you've learned C++. **Now to something completely different:** By the way, I've just noticed that there is a bunch of "Haskell for C programmers" tutorials/articles, but not the other way round. Anyone feel like writing one?
Well, to each it's own I guess. I'm just a student, and I don't (yet?) have computer cluster privilieges on my university, but I do have experience writing a desktop data analysis program for actual scientists to use, and what I found out: * if a program is to be used by people who don't program, it definitely needs a GUI * if it needs to load many data sets at once, it needs to store and access them in some sensible manner (and I personally prefer the functional way of strongly typed containers, rather than the "panta rei" inevitable in dynamically or weakly typed languages) * for an interactive program that does heavy number crunching, safe concurrency is a huge boon The scientific programs I've seen either are written in low level languages, give up those two, and focus on number crunching, or are written in propertiary packages like Matlab or LabView. The first approach at it's best results in venerable libraries like LAPACK. At it's worst, in decades old Basic program that noone, including it's creator, can port to modern hardware (unfortunately it's a true story). The second approach... well, the packages are usually easy enough to use, but also are one trick ponies. Why would I pay for a Matlab or LabView, if in Haskell I can have BOTH matrix manipulation AND streaming, together with concurrency, powerful type system, and many other adventages, not to mention all that for free as in beer and free as in no vendor lock-in?
The relevant slide has a graph of number-of-users over time, and the caption indicates the "threshold of immortality". I'm prone to flights of fancy and false memory syndrome, but I could swear I've heard him insert an extra word when giving the talk and say "threshold of regrettable immortality". Cue jokes about COBOL programmers in cryogenic suspension awaiting the year 9999, etc.
Thank you! Yeah, I'm reading a book on graphic shaders, this one: http://www.amazon.co.uk/Graphics-Shaders-Theory-Practice-Edition/dp/1568814348 The first approach is how I'm doing the rendering in direct mode, so I guess that's how I'd like to approach it. The whole point of the project is to have the functional API to enable a more functional way of writing programs. I've had a dash at LambdaCube and even GPipe which might be possibilities. I figured it'd make sense to get as much information before I get too deep in the shaders and realise there's a much better way of doing it and it was because I didn't explore the possibilities. Thank you very much, I really appreciate it! 
Ah, yeah, I'm using the medium ones, I didn't know there was a difference! Aye, it's essentially just so they can called a bit nicer. Thanks!
That's really great, thanks! Especially with how you deal with the compilation/use of shaders! 
I had a likewise mindframe! Mine's just on bitbucket at the moment whilst I implement little bits of the whole thing and thought I could release it there with a bit more substantial usage!
&gt; Ada is interesting for its strong type system and low-level stuff. Also, rust.
That's cool! I might speak to my supervisor and see if it's worth putting what I currently have up just for the direct mode rendering. 
You can also run ghci in an Emacs buffer, which gives scrollback and searchability, but I haven't found a workaround for this same issue.
 Someone agreeing with the OP? What a *circlejerk*!
If you have some web development under your belt you could try us http://www.reddit.com/r/haskell/comments/2x4vgw/circuithub_is_hiring_haskellers_worldwide_to_fix/. We are still talking to candidates at the moment. (Having some open source Haskell also helps :))
God forbid that someone has an opinion which is popular.
&gt; I don't even want to touch other languages You call *that* popular? Of course it is necessary to have familiarity with other languages, and everybody here knows that.
&gt; By the way, I've just noticed that there is a bunch of "Haskell for C programmers" tutorials/articles, but not the other way round. Anyone feel like writing one? I'd be more than interested in reading it if there was one.
Now I'm confused. What do you mean by *circlejerk*, if it's not having a popular opinion? I wouldn't be surprised if it is common to have a fear of being spoiled by what one thinks is a better language, and then feeling let down when they have to get back to those other languages. And feeling disappointed is not the same as actually quitting programming and becoming a Haskell hermit, for fucks sake. Most people are more pragmatic than that. But that doesn't mean that they might sometimes secretly want to. :)
Generalizing `catMaybes` to me feels more naturally to me as: catMaybes :: Foldable t =&gt; t (Maybe a) -&gt; t a if we are making it polymorphic over all Foldable, then it is sort of weird that it'd return a list. Even returning a polymorphic type liie fizruk's example would make sense as a "generalization". but the above function is impossible, heh. Any other type signature I think might be a bit surprising or unintuitive... perhaps :) Your version is also `catMaybes . toList`, btw. :) The second one also has a type signature I wouldn't expect from a generalization of *map* Maybe... I would think of something like mapMaybe :: Foldable t =&gt; (a -&gt; Maybe b) -&gt; t a -&gt; t b having a monoid constraint on the return type of the maybe producing function feels like basically another function to me... Besides anything would be weird if `catMaybes` was not `mapMaybe id` :D
And resurrected by... Well, these analogies don't really work with the HPMOR characters, do they? ;)
For me it's more when you push an already popular opinion to the extreme so that others will validate what you say and make you feel like you're smart. So &gt; Haskell is a beautiful language would be a popular opinion, whereas &gt; Haskell is so beautiful I don't want to use any other language anymore is circlejerk-worthy.
How are you supposed to have a discussion around "Haskell is beautiful"? It's probably been discussed before, and it's not a very interesting sentiment. The other statement is more interesting because it shows a real dilemma - the fear of spoiling yourself by using something that you can't go back to using in other contexts. It doesn't have to do with Haskell - you can spoil yourself on expensive chocolate so that you now always have to buy expensive chocolate when you have a craving, and so on.
Agreed, there aren't many entry level jobs, guess it's because we have more experienced pigeons than pigeon holes, so they don't need child pigeons.
I agree with the sentiment, but C leaves too much room for errors with memory management for me to agree fully. I'd say another language that is fairly simple, fast and does not allow for memory-management errors would be a good thing. No, it won't prevent all errors, leaks etc, memory management is too easy to screw up in c and c++, and having a compiler help you with that would be very useful I think. Especially for people just trying to get shit done
&gt; How are you supposed to have a discussion around "Haskell is beautiful"? To be honest, I don't know. I just wanted to point out that I felt the same as OP, and since it sounded circlejerky to my ears, I just decided to point it out.
Sorry about that. We will be getting back to everybody I promise. We are pretty much doing this on a first come first served basis, the logistics of arranging schedules etc is time consuming (complicated by varied time zones) and it's just a case of limited hours in the day. No reflection on the application.
I'm after the creation of the first truly composable GUI toolkit ever. where widgets can be combined by means of monadic, applicative, alternative and monoidal combinators. I would mentor anyone interested in or outside of GSOC. Perhaps it is too late and this is too little detailed, but the project can be achieved in a month work and would show how Haskell can solve a longstanding problem: the non composability of GUIs. That achievement could attract much attention from the software industry. The idea start from well tested stuff. It has been tested in the Web browser as a layer on top of Haste.DOM. The Javascript Document Object Model of the Web browser has a similar architecture and interface that a GUI toolkit. The idea is to translate hplayground, the Haskell package that implement composable widgets in the Web Browser on top of a Haskell GUI Toolkit such is GTK or FLTK. the hplayground widgets can be composed like formelts, but instead of generating HTML, generate instead a Haste.DOM computation. When executed, this DOM produces the rendering of the widgets. The same mechanism can be used to generate the rendering of the widgets under a GUI toolkit. The widgets will reshape themselves while compute anything else like any monadic/applicative computation. Events will execute only the portion of the computation that is in its scope. The result will be the possibility of more dynamic interfaces with much less code. and, at last, full composability of applications and libraries with GUI interfaces.
I would say there's actually a shortage of experienced pigeons in this case :) From what I've heard from, for example, the Facebook Haskell experience is that their approach is: Have highly experienced haskell people build the foundation which can then be easily and safely used by the non-haskell programmers they have. If you have a ton of programmers it's preferable to train your current programmers to learn Haskell, which solves all the need for hiring beginners. You can simply train beginners. But you still need experienced people to do the training and initial design. This is why I think there's high demand for experienced haskellers and less so for beginners.
F#, getting into the .NET world means jobs. 
If you have more specific questions, feel free to ask! Also, if you have some time, personally I found that it is very useful to read through the official OpenGL specification and also GLSL specification (which is OpenGL shaders language), which you can find [here](https://www.opengl.org/registry/) There are several version, the current one seems to be version 4.5, which I'm not familiar with, but seems to be rather different from say the (very) old version 2.0 (the way OpenGL evolved is that they tried to be very forward-looking when they originally defined it, but the real world happened to go into the opposite direction, so it became pretty outdated fast...). However, the old versions still work, too.
Show us what you've made in Haskell. I'm sure you've made great products in Haskell since you love it so much. Otherwise, I'd call this masturbation. 
&gt; Prolog is interesting for declarativeness. I could never get on board with Prolog being declarative compared to Haskell. Cuts everywhere, man.
Show me what you've made in Node.js with mongodb. ;) Don't bother replying unless it's webscale.
I once tried to generate test cases for a tiny web app (written in dart) in a functional style, it's doable, but I wouldn't call it pleasant, with Haskell, you write only what's needed, but if you do it in dart, there is too much noise in the language grammar that makes your code less beautiful (because it's not intended to be used that way), as you said, even for those do transfer well, even I have the luxury to use it, I may don't want to use it, after all, no one use plate to drink water, though it's doable.
Yes, I would recommend C over C++. C++ is not a simple language. I'd say that seeing how things are done in C is worthwhile. Sure, a C++ programmer can read C, but like many things, being able to read is quite different to being able to write. I think learning C is just as enlightening as learning Haskell. It forces you to get intimate with your machine in ways other languages (including C++) actively discourage or even forbid. 
Could you expand on what this does, exactly? I haven't needed it so far, but I see it in the `setLocalPostgresState` function in the typeclass definition of `HasPostgres`.
Well it implies that there will need to be work to get it caught up to `HEAD`before it could be merged. Aside from that, you're right. It doesn't mean much of anything.
Yeah, that's what we're going to do. The check will be moved from Cabal to the hackage-server, with it turned on in the central community instance and off by default for people running their own instances.
Which is a perfectly legitimate answer, as far as I'm concerned!
Oh the issue tracker's I've seen rot...
Sure. First, it makes sense to distinguish ABTs in general from the various ABT implementations that have proliferated. "Abstract binding trees" were first used, if I recall correctly, in Nuprl for the representation of its uniform syntax. An ABT consists of one of the following: 1. A variable `x` 2. A binding of a variable into an ABT, `x. E` 3. An application of an 'operator' to a list of ABTs conforming to its arity, `o{x_1...x_n.M; y_1...y_n.N; ...}`. The idea is that each language contains a set of operators, and each operator has an arity. An arity is a list of valences, and a valence is simply a number which counts how many variables are to be bound in a particular slot. So an example operator might be `Pi` whose arity would be `[0,1]`; then an abstract binding tree involving `Pi` might be `Pi{Unit{}; x. Unit{}}`. (Arities may also be generalized to involve sorts, to separate different syntactic categories; in papers and books this is very useful, but leads to a lot of complexity in an implementation.). Alpha equivalence and substitution are defined generically on ABTs, as are a number of other useful operations. So the point of ABTs is that once you have implemented them in general, you can define a language by giving a set of its "op codes" and explaining their arities: then, there is no need to implement substitution or alpha equivalence, since this comes for free from the underlying ABT structure. A great place to learn about ABTs is Bob Harper's *Practical Foundations for Programming Languages*, where he uses them as the basis for his entire book. ------------------------------------------------------------------------------ Typically, an ABT implementation is done in a nominal manner, by assuming a type of variables that can be generated freely (or in a monad), and has decidable equality. Then, the primary interface is via a "view" type parameterized over some interpretation `phi`, which gives you one layer of syntax: it is either a named variable, or it is the binding of (named) variable into a piece of syntax in `phi`, or it is an application of an op code to a list of arguments in `phi` with the appropriate valences. Then, there is a concrete term representation `Tm`, which usually is locally nameless, and you have injections and projections between `Tm` and `View Tm` (i.e. letting `phi` be `Tm`). The conversion from `Tm` to `View Tm` lets you convert whatever binding style is implemented in `Tm` to a nominal one which is suitable for programming with as a human being, and the conversion from `View Tm` to `Tm` will convert the nominal representation into whatever you have chosen to use for `Tm` (probably locally nameless). So this takes care of what you usually use `unbind` for in `Unbound` from the Penn folks. ------------------------------------------------------------------------------- In practice, using the `abt` library is similar to using `Unbound`, except that it does not use any code generation, and is a lot easier to understand in my opinion. There may be some cases where `Unbound` gives you a bit finer control over binding too, but since `abt` is based on a nominal interface, it seems to suffice perfectly. On the other hand, whilst `Bound` is extremely elegant, it is simply not practical to use it for serious work in type checkers. Why? Try going under a binder and adding a variable to a context, and you will know what I mean. I have seen tricks for making this work (by Francesco Mazzoli and perhaps others), but it is extremely complicated. I am sure that in the use-cases that it was intended for, `Bound` is a wonderful solution; but it is clear that when it was written, the most basic tasks of writing type checkers with hypothetico-general judgements were not even being considered. Finally, `abt` requires a level of boilerplate somewhere in between `Bound` and `Unbound`. You do *not* need to write any tricky `Monad` instances for anything. All the kit is built-in from the start. The only thing you have got to do is define the arities of your operators, and their string representation (so they can be shown); you also have to demonstrate a decidable equality on op codes. Also, `abt` is very light on dependencies. Currently, it has a `profunctors` dependency, but I will be removing this shortly, since the prisms which I provide can be moved into a separate package. I have found them useful, but they are by no means necessary for successfully using `abt`.
Languages experience network effects; a language which is "good" but no one uses may be good from a language design perspective, but not from a practical perspective. Lack of users means lack of documentation, lack of libraries, lack of potential hires, etc. At this point, it's somewhere between risky and ridiculous to decided to port even a medium sized project from C to Ada.
For those that don't know, /u/passwordissame is a pretty funny troll. I recommend going through his/her comment history for some a good laugh.
My usual recommendation is to switch to iterative deepening rather than keep all the pointers around.
How does Ada compare to ATS? I was under the impression that the latter is also good for low-levels stuff, and I've used neither.
I usually just go with using GHC.Generics to supply a default definition for the members of the class, and then glue things together from that. It isn't perfect, you wind up having to write a number of cases by hand, you still have to write the instance declaration itself, but it removes at least some of the boilerplate.
As far as I can see they didn't release 1.0 yet. They just changed the version in the cabal file already. Their is no github tag for it and the [changelog](https://github.com/snapframework/snap/blob/master/changelog.md) says “UNRELEASED”.
For my day job I write Python. I know, I know, Python is about un-Haskelly as it gets, but Haskell made me an infinitely better Python programmer. I got much more disciplined about isolating side-effects and limiting mutability, and Python is great for using closures, list comprehensions and other nifty functional structures. So overall Haskell was a total win, and I enjoy writing Python more now.
I was afraid of that. It sounds like in order to make this work, I'd have to implement (at least a subset of) the GHC typechecker in Template Haskell.
&gt; F#, pretty much OCaml, but running on .NET, might be worth investigating if you're in a Windows/.NET shop. &gt; The syntax is a little nicer at least. 
Snap 1.0 is pretty much finished. There are a few more things we would like to do before release, but it's going a little slower than we would like because the core developers have been busy with other things. That said, it should be usable today. It hasn't had enough battle testing for me to be able to say that it's ready for mission-critical applications, but it should be pretty stable for just about anything less critical. Also, I recently discussed a similar question [here](http://www.reddit.com/r/haskell/comments/2wfap0/web_development_using_haskell/coqrq1q) with more details about the Snap ecosystem as a whole.
First, I think "i" is a horrible name. "upCast" is probably better or, as is, even "[slice](http://stackoverflow.com/questions/274626/what-is-object-slicing)". As written, it also only works for pure-functional objects. You'd want a lens to handle mutable objects, if you did that, then "upCast" would make more sense. Most of the time I don't need nominal subtyping in Haskell, but a typeclass like this might make it nicer if you do need it. Global confluence disallows incorrect public multiple inheritance, it seems.
As a side note, if your project is looking for contributors, make it easy for them to contribute. I've been staring at the GHC wiki for twenty minutes and their instructions basically amount to: * here is how to get the sources * here's another way to get the sources * here's someone's workflow for getting the sources * here's how to build the source * here's ten paragraphs explaining how we've made actually submitting patches impossible for anyone without a deep knowledge of git * that's it, good luck figuring out the rest from random links on the wiki!
&gt;&gt;you can not solve any problem without going to the deep of it and discover something that truly solves the problem in the deeper sense &gt;I've recently been putting effort into not trying to make problems solve themselves, instead going for the first solution that enters my mind. If I have time left over later, I can refactor. My understanding of this is a bit different. You see, basic Haskell lets me see patterns that I would not normally see in other languages and without much effort. This is due to the fact that I usually make types for more things and thus a little domain language emerges. When something goes wrong the domain language stops making sense or just seems unnatural (and often won't type-check). When I go back to JS or Ruby for example I can't quite get that feeling. I know somethings wrong but I can't tell what it is without digging for an inordinate amount of time so I just code a "quick and dirty patch". It's not about an unwillingness to deliver a quick patch, it is about an unwillingness to program in languages where the quick patch is too often not the correct fix. In Haskell many times easy quick things happen to be in the set of right things - and that is why I like Haskell and languages like it. And so my take away for agocorona's statement is that you start to want what is cheap in Haskell-like languages only to find that they are not even moderately cheap in some other more mainstream languages.
I find that, if I try hard enough, I can spread just a little of the beauty I see in Haskell code to the Java and C code I write for work. Beauty created is even more satisfying than beauty observed.
&gt; `int* pointer = nullptr;` &gt; `int&amp; reference = *pointer; // a null reference!` That's undefined behaviour.
Yes! Please people, work on Cabal. :) If I didn't already have my hands full with Haskell projects... https://github.com/haskell/cabal/issues (590 open issues. Not that that's a bad thing.)
Hos do you evaluate that in this case? For example when mocking the executable is different so there is still one instance within the executable. 
This definitely deeply resonates inside of my tiny brain. I've had a 40k SLOC Java optimization project given to me, and I had to speed the thing up. It was filled with hierarchies of classes, interfaces, a mess, I've managed to speed it up around 5 times. Then, adding new features was a hell. We've rewritten the project in C++, concentrating on composability and templates, I was amazed how simple design, present in the simplest forms in Haskell, used in the context of C++ (that was still impure and mutable), ended up being highly composable. (I don't need to mention the speed comparison) Ease to develop new features, or create new optimization algorithms by just composing, I was amazed that something so simple, worked so well. (we could just by a single change create an algorithm that went from single thread, to many threads, from many threads to many computers, or define a many thread algorithm, that would eventually be distributed across the server farm) Haskell gives one a view of the programming world, unique, different, and powerful.
&gt; *references can be null* Not exactly—this is undefined behaviour, so the compiler is perfectly within its rights to optimise out `if (&amp;x == nullptr) { … }`, making all of their work for naught. Clang even has a warning about this: `-Wtautological-undefined-compare`. 
Interesting. I followed about 10% of that, but I look forward to digging in at some point. I hope you don't mind if I ping you with questions. :) Is there an implementation of, like, the simply typed lambda calculus or some other simple language in abt that I could take a look at?
This could be more easily understood like so: ``` a :: Api request response ``` Then you can sub in the constraint and imagine this: ``` a :: (JsTypeable request, JsTypeable response) =&gt; Api request response ``` This puts a JsTypeable constraint on the request and response types of the api. Some2 is just a generic way of doing this.
With the implementation from [this comment](http://www.reddit.com/r/haskell/comments/2y2pe5/shouldnt_ftp_propagate_changes_over_the_entire/cp6o8hw) I was able to have that work as well. &gt; mapMaybe' Just [[1,2],[3]] [[1,2],[3]] 
This might be a good starting point too: https://hackage.haskell.org/package/basic-sop-0.1.0.3/docs/Generics-SOP-Show.html
Non-monadic signals were definitely my biggest issue with Elm. The reason they're absent is that without linear types, there is no way to ensure you haven't written a space leak. That being said, Elm *does* support Yale's 'arrowized FRP' in EvanCZ's automaton library. I think it's not fair to call Elm a "toy language." I used it for my last front-end project, and it is so much more ergonomic than Haskell in the browser, and it's light-years ahead of js, which nobody calls a toy language. 
And as usual, contributors needed for: - [hledger](http://hledger.org) - command-line and web-based accounting - [darcsden](http://hub.darcs.net/simon/darcsden) - UI for darcs repos, runs hub.darcs.net - [rss2irc](http://hub.darcs.net/simon/rss2irc) - RSS feed announcer, runs hackagebot - [shelltestrunner](http://joyful.com/shelltestrunner) - tool for testing command-line programs - [FunGEn](http://joyful.com/fungen/) - minimal-dependency cross-platform game engine 
Lots of great talks, but 1 of those is a rambling news segment hosted by me (Nick Partridge) at the Brisbane Functional Programming Group. I'm not sure it's like the others :) But I'll take the opportunity to plug our group and videos! We are located in Brisbane, Australia, and have a regular attendance of around 40 people each month, with about 500 on the mailing list. We started over 5 years ago as a general FP group in a city with limited academic FP presence. We were (and are) just a bunch of keen professionals who wanted a place to talk about the ideas we were learning. Over the last year, we have shamelessly re-presented the CIS194 course content by Brent Yorgey. These are the first 3 videos in the link above. The full effort is available here: https://github.com/bfpg/cis194-yorgey-lectures In general the videos from each month pop up here: https://vimeo.com/channels/bfg
I'm not sure how useful arrowized FRP is, or if it solves the problem of nesting dynamic elements, but I would love to see an example if that is the case. The main reason JavaScript + HTML + CSS is still way ahead of Elm is because elm provides a very small subset of the features that these 3 provide and so writing a complete web application of moderate complexity with Elm is simply not an option. I have seen it used to write small widgets and things like that but I have yet to see it used on a serious project. I would agree that it is quite comfortable for the things it can do and it would be a great language if it had a good way to write nested dynamic elements, and had better support for common HTML elements and CSS styles.
Hmph. My attempt at iterative deepening ended up using the "copy shape" technique anyways. -- traverse a specific level of the given forest bflevel :: Applicative f =&gt; (a -&gt; f b) -&gt; [Tree a] -&gt; Int -&gt; Maybe (f [b]) bflevel _ [] _ = Nothing bflevel k ts 0 = Just $ traverse k (map root ts) bflevel k ts n = bflevel k (concatMap children ts) (n-1) takeWhileIsJust :: [Maybe a] -&gt; [a] takeWhileIsJust [] = [] takeWhileIsJust (Nothing:_) = [] takeWhileIsJust (Just x:mxs) = x : takeWhileIsJust mxs btraverse :: forall f a b. Applicative f =&gt; (a -&gt; f b) -&gt; Tree a -&gt; f (Tree b) btraverse k t0 = fmap ((\[t] -&gt; t) . restoreShape [t0]) fbs where bs :: [f [b]] bs = takeWhileIsJust $ map (bflevel k [t0]) [0..] fbs :: f [[b]] fbs = sequenceA bs restoreShape :: [Tree a] -&gt; [[b]] -&gt; [Tree b] restoreShape = undefined -- blarg I guess my problem is that the structure information is lost at `concatMap children`, so it has to be put back somehow.
Our mission is Learning Haskell Using Haskell Share Haskell.
Hi Paul, I have an implementation of Intensional Type Theory with singleton types on my GitHub (http://github.com/jonsterling/tt-singletons); might be a little trickier than STLC, but it shows the technique in action. In the abt package, ~~there is also a silly little tutorial module that gives a small step evaluator for the untyped lambda calculus as well.~~ EDIT: It actually turns out that I wrote a type checker and evalutor for a very small dependently typed lambda calculus. The code is here: http://hackage.haskell.org/package/abt-0.1.1.0/docs/src/Abt-Tutorial.html#step Please ping me at any time if you have questions! Either here or my email is fine. JMS
While Rust might be a nice language it is just not quite at the point in its lifecycle where it has a significant library ecosystem with ready-made solutions for most common problems. That takes time after the language itself stabilizes.
Also see http://www.dohaskell.com/type/videos
Reminds me of Allison queues. [Leon Smith](http://www.melding-monads.com/files/CorecQueues.pdf) has a good writeup on the subject and on packaging it up into a somewhat easier to think about [package](https://hackage.haskell.org/package/control-monad-queue-0.2) using `Cont`.
Gnu grep shenanigans! Lets use perl and a regex cause that fixes everything always! (I'm excused because I used be more of a sysadmin, regexes in perl were my go to for a long time) Does that grep catch the http channel 9 videos up top? I punted to a character class. curl -L https://raw.githubusercontent.com/drKraken/haskell-must-watch/master/README.md | perl -ne 'm|\((http[s]{0,1}[:]//[\w./?=-]+(?=\)))|g &amp;&amp; print $1 . "\n";' | sort | uniq | xargs youtube-dl 
&gt; catMaybes :: Foldable f =&gt; f (Maybe a) -&gt; [a] On a side note, why not catMaybes :: Foldable f =&gt; f (Maybe a) -&gt; f a And similar for `mapMaybes`. Or am I misunderstanding something? I am definitely not as well-versed in the ideas behind FTP as I should be. EDIT: Ooh, I've just seen /u/mstkg's post further down. Ignore me!
Only saw this post now, and it is already booked out. Wow.
This is a very good point. I think it might be a little overkill, since you're probably not going to dynamically generate a progress-bar configuration, but it does make sense and wouldn't take much to implement.
No, Heist does not depend on any of the other Snap projects and is not blocked by them. You don't need the HeistConfig constructor. Use emptyHeistConfig and modify it as needed. You also don't have to use the lens package to use Heist's lenses. They work with lens-family and any other lens packages compatible with Van Laarhoven lenses.
Could this list (and suggestions fr the comments) serve a input for the videos on the homepage of haskell.org?
This is also a very good point. I've added a HTTP download example using `http-conduit` [here](https://github.com/yamadapc/haskell-ascii-progress/blob/b067fa8856bd463d899316fd164502be53fcb517/bin/DownloadExample.hs). I had left the completion `Async` intentionally open so that people would be able to `wait` for completion, but I decided to add a `complete :: ProgressBar -&gt; IO ()` function to force a progress bar's completion as well. *EDIT:* I think this is what you meant, but maybe I misunderstood you. Do you have any ideas for a better (hopefully simple) example I could use in the README?
$ curl http://code.haskell.org/~thielema/darcs-scripts/darcs-hub-put export pkg=$1 ssh hub.darcs.net init $pkg "`fgrep -i Synopsis: *.cabal | perl -p -e 's/Synopsis: +(.+)/\1/i'`" darcs push hub.darcs.net:$pkg To be called like: mypkg$ darcs-hub-put `basename $PWD`
Cool! I'm personally a big fan of the `vty` library and `vty-ui` widgets -- perhaps this library could either utilize or add to those? They're really great for making complex command-line interfaces, but I don't know whether they're any good for small things. That said, if you can't output multiple progress bars, you may want to rethink the interface. Right now it seems like I could do: main = do let fmt = currentProgress % "/" % totalProgress % " [" % barProgress % "]" pg &lt;- newProgressBar def { pgFormat = fmt } pg' &lt;- newProgressBar def { pgFormat = fmt } forever $ tick pg &gt;&gt; tick pg' -- or something Instead, perhaps you could not expose `newProgressBar`, and instead expose an interface that makes it clear that only one progress bar could exist. A potential might look like this: import System.Console.AsciiProgress as Progress main = do Progress.setFormat fmt Progress.tickN where fmt = currentProgress % "/" % totalProgress % " [" % barProgress % "]" That said, that is a very imperative-style interface, but I don't know if there's a way around that, since ultimately you can only have a single progress bar (due to the fact that there's literally just one space in the command line). Maybe another option would be to expose some low-level primitives, perhaps something like `renderProgressBar :: ProgressBar -&gt; IO ()` which prints out a progress bar and `clearProgressBar :: IO ()` which clears whichever progress bar was printed last (I don't know if such primitives could actually be made). That way, the user could decide how they actually manage things, and it's clear what the API can and can't do. As an aside, please don't take all these musings about library design as criticism! This seems like a great library that's already pretty solid, and I don't mean to rag on it at all -- I just like thinking about ways in which we can use the Haskell language to very precisely express what a library / API does (I think that this enables the sort of "learn-library-by-looking-at-the-types" exploration that Haskell is great at and that I really miss in many other languages). 
Yes, `Alternative` does not save `Foldable` structure and it's not meant that way. If you want to save structure you have to introduce a new class or multiple ad-hoc functions. Also, your example works just fine with my definition: &gt;&gt;&gt; let afoldMap f = F.asum . fmap f &gt;&gt;&gt; let catMaybes = afoldMap (maybe empty pure) &gt;&gt;&gt; let mapMaybe f = catMaybes . fmap f &gt;&gt;&gt; mapMaybe Just [[1, 2], [3]] :: [[Int]] [[1,2],[3]] Note however, that with `mapMaybe`/`catMaybes` implemented that way there is no way for a compiler to figure out result type. We have to provide an explicit type signature to disambiguate the `Alternative g`.
When you say "nesting dynamic elements," what exactly are you referring to? Do you mean like hover animations, where you want a simple, declarative way to describe the element without being bothered with the signal graph? Because you can totally [do that in Elm](https://github.com/identicalsnowflake/elm-dynamic-style) and even with [static typing] (https://github.com/identicalsnowflake/elm-typed-styles) to some extent. If you're referring to have elements which are sometimes displayed and sometimes not displayed, then yeah, you need to have a static place in your signal graph, whether you're actively using that signal or not, which is kind of annoying. In any case, I was thoroughly impressed with Elm on the project I worked on, and will absolutely use it over the standard platform whenever I have the option to.
It is not the end of programming life. What is more important is the transferable skill in functional programming that Haskell gives: * Pure Functions / Referential Transparency / Side effects * Currying ( Can be implemented in OO languages (no Java) with some tricks ) * Function Composition * High Order Functions * High Order Functions idioms: map, flodl, foldr, reduce, filter, zipWith, last, head, take, takeWhile ... * Lazy Evaluation / Eager(aka Strict) Evaluation / Infinite lists ( Python Generators, Yield in F# ...) * Recursion (Not all languages supports recursions like Haskell does, but can be implemented with for loops). * Pattern Matching: In python the library: [Patterns](https://pypi.python.org/pypi/patterns/0.3) * Factoring -&gt; Divide the code in many reusable functions Other skill that I learned in Haskell is how to purify impure languages like Python, for example: instead write: lst = [1, 2, 3, 4] lst.reverse() x = lst create: def reverse(alist): newalist = alist.copy() newalist.reverse() return newalist now there is a pure function. x = reverse(lst) It is also important to notice that as Haskell uses lazy evaluation by default, in impure languages generally use eager/strict evaluation by default like Python. Higher Order functions needs to be implemented in two ways in impure languages, for Lists (Strict evaluation) and for Generators ( Lazy evaluation). Besides python and other languages, Javascript can also be programmed in FP style with [underscore js]( http://underscorejs.org/). This library is a nice example of implementation of Haskell features in impure languages.
STM with persistence. STM are amazing but you can't as of today use any kind of IO inside a transaction. This is because we consider every IO to be non-revocable. There is however well known techniques whose can be used to do revocable persistence on disk. We could build a such library on an existing transactional persistence framework (about any existing database). This kind of library exists in Scala, for example : http://activate-framework.org/
 sort | uniq -&gt; sort -u
So you introduce a type class, then an existential over it, then you use typeable to compare these existentials... You should stop and rethink your approach. Use plain old datatypes and functions. See also https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/
It's also irresponsible to knowingly work with suboptimal tools, no?
No, I mean nesting Elm's (Signal Element) elements... you end up with (Singal (Signal (Signal ... Element))) which are unusable in Elm. This means there is no (reasonable) way to write complicated UI's such as is often done with frameworks such as AngularJS and ReactJS - complex webapps may have 10 nesting (routes -&gt; inner routers -&gt; tabs -&gt; dynamically hidden element based on state -&gt; data-and-input-driven dropdown -&gt; etc). 
Certainly. I read through the book and it felt like it was giving a basic description on how certain things worked yet not tying them all together and giving a larger example. At the end of the book it was as if I'd understood all the words but not the overall meaning, I didn't know how I'd be able to combine all these parts to create something bigger. Now, this may be a case of my lack of experience with Haskell or maybe the tutorials I've used in other languages spoon-feed more. The book was well written, I'm just not sure I was it's target audience.
D isn't purely functional. You can try to annotate purity in D, but that is not the default (is it even done by most libraries?)
Python has `reversed` built-in, so that's not a great example :)
I'll give that a gander, it sounds like something towards what I'm after :) Thanks
Ryan Trinkle has a great step by step tutorial for writing a URL shortener. He gave a [presentation](https://vimeo.com/59109358) on it at the NY Haskell meetup awhile back and the [git repo](https://github.com/ryantrinkle/memoise) is structured as an incremental set of lessons.
From first glance this looks to be the sort of thing I'm after (ideally it'd be in text and have a login feature but we can't have everything we want). Thanks.
Note that a lack of orphan instances is important here. If you have two instances for a type (e.g. HttpVersion), and you create two RequestConditionBox values that use the same internal value (e.g. http11) but *different instances* and combine them into a single list, then `nub` will drop the second one even though it's not really the same condition. In effect the `Eq` instance that we've defined *assumes* coherence.
While I agree *in principle*, I'm not sure how to provide an `Eq` instance without closing the set of valid condtions, and I believe the goal is to have an open set of conditions. A request condition behaves very much like a `Request -&gt; Bool`. In Haskell (and most languages), we can't inspect under the `(-&gt;)` to determine equivalence. If we want an inspectable form we could use a (properly typed) AST for a DSL. (There may be other ways, but this is the first I came up with.) At that point, you can (at least) define equality based on alpha-equivalence. But, along the way you have to in some sense fixed the number of "primitives", closing the set.
Where's the `equalRC3` that you use in the definition of `equalRC4`?
Following the pattern [here](http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/), we should be able to show bad behavior. I don't really feel like rewriting all the `Eq` machinery but I think it should be relatively obvious that you can set up conditions so that `isMatch req (MkAnd cs) /= isMatch req (MkAnd (nub cs))` or `isMatch req (MkOr cs) /= isMatch req (MkOr (nub cs))`
The [default project template](https://github.com/snapframework/snap/blob/0.13-stable/project_template/default/src/Site.hs#L30) generated by `snap init` has an example of auth.
I assume that'll be auth specific to the snapstack framework? If so, are there any tutorials of this nature for Snap?
I can understand the sentiment but I encourage you to give it a bit more time. The Yesod scaffold could be the larger example you are looking for: https://github.com/yesodweb/yesod-scaffold/tree/postgres I highly recommend the minimal scaffold site as well: https://github.com/yesodweb/yesod-scaffold/tree/minimal There are also some cookbook and other examples in the wiki: https://github.com/yesodweb/yesod/wiki If you're up for helping us improve the documentation, I would be happy to offer some guidance. Feel free to ask question on the yesodweb mailing list or in irc (#yesod). EDIT: Cookbook: https://github.com/yesodweb/yesod/wiki/Cookbook 
From the article: &gt; Eq isn't actually what we wanted in the first place. Really we want to be unifying to [RC "GET",RC HTTP/1.1]. To do that, we're going to have to rip up everything we've done so far and start again. Is seems like maybe these first two posts in the series are a sort of "unteaching" -- presenting a flawed method with the intent to show that way is bad, as a means of motivating a more correct method. I.e. teaching something only for it to be unlearned later. I don't like "unteaching". I do understand it can be valuable if "everyone" makes the same first mistakes, and you a teaching in a group setting. Instead of waiting for the students to make the mistake, you make it part of the curriculum so that you can address it quickly, as a group and put it aside. But, that only works if well if every student that sees the flawed method sticks with the class *at least* long enough to have the flaws made obvious, if not long enough to learn the more correct method. You don't have that type of guarantee in an unsupervised, unstructured setting (like blog posts). I can't definitively say "unteaching" does more harm than good when this precondition is violated, but it does have higher potential to harm than simply covering the more correct method even if there appears to be a lack of motivation. But, that's just my opinion.
Okay, I'll give it another shot. I'll spend some time looking at the scaffold and the cookbook. I'd be delighted to help with documentation when I know a bit more as it's an area of myself I want to develop. Thanks
Nono, I'm just relating a prior experience which might explain an excess of skepticism :) I was thinking specifically of things like http://www.mozillascience.org/code-review-for-science-what-we-learned/ in the context of my own experience with such programmers. I'm glad to hear that you're doing testing, and I'd be very interested in what kind of testing you're doing (specifically). My experience is that testing code that may run for months can quickly become a prickly issue unless you can do some sort of "inductive proof" style testing which has tests for low-level primitives and has tests for higher-level combinators (thus completing the induction). I have a hard time understanding how this level of "inductive" programming would even be realized in C code in any realistic way (because of the difficulty of building such an abstraction tower), but maybe I'm missing something. I'd love to see if you have concrete examples! EDIT: Of course, in parallel/distributed settings this gets even more "interesting" in that floating point addition, subtration and multiplication isn't actually commutative. So how do you prove you're doing the right things? (A favorite of mine is actually doing everything twice, but forcing a different order and then comparing the final results to appropriate orders of magnitude. Can you afford that sort of overhead? Can you afford *not* to do it?)
Having actually worked (fo' reelz') with O'Caml modules I can definitely say that it's one of the things I miss most in Haskell. (That, and polymorphic variants and row types, but I think we can emulate those semi-reasonably these days.) I don't know if the generative/applicative thing has been resolved, but... I think it's interesting that "generative" is kind of complementary to the Haskell type class coherence requirement in that it explicitly *doesn't* promise coherence unless you can absolutely prove the providence of a given module (via type aliases).
Cheers, I'll be looking at them.
Wow, dohaskell.com is a great resource!
Any idea how long those things might take or if they're things people could jump in and help with? Would it be possible to do a beta release or something on Hackage? I'd like to play with it but I'm not sure about pulling all the deps individually off github, or how to get docs if I do that.
It really looks like OP wants newtype RequestConditionBox = RequestConditionBox (Wai.Request -&gt; Bool) Personally I'd also go without `RequestCondition` and just have isMatchHttpVersion :: HttpVersion -&gt; Wai.Request -&gt; Bool isMatchMethod :: Method -&gt; Wai.Request -&gt; Bool rather than using `RequestCondition` merely for overloading the name `isMatch`.
Does hackage support now Apache2 licenses? I remember having problems with that in the past and having to set the field in cabal to OtherLicense.
[Oui oui](http://hackage.haskell.org/package/Cabal-1.22.1.1/docs/Distribution-License.html#v:Apache)
i moved on from trying to learn haskell to trying to learn f#. since f# is a .net language, you will have a much greater chance of using it in industry than haskell. furthermore, f# can actually do graphics and UIs in a functional-esque way, whereas that situation in haskell is really quite hopeless. f# doesn't have haskell's clean syntax or typeclasses, but it has a lot of neat features and it's much cleaner than any other language than haskell. so i say take a look at it.
Announcement posts like this should really have at least one line mentioning what the software actually does (I know it is mentioned on the Hackage page but still, many people won't click those links unless they know it is interesting to them).
I will, cheers.
&gt; Yes it is. Oh come on, no it's not, and it was never indended to be. [F*](https://www.fstar-lang.org/) is more "purely functional" than D, and noone calls it such. (btw. IO is not a keyword in Haskell, and it doesn't make it impure, so it's hardly opposite of pure. It could be argued unsafePerformIO does)
&gt; I don't know if the generative/applicative thing has been resolved, but... It's solved in the last version of OCaml. Basically, you mark generative functors explicitely by adding a dummy functor argument `()`(yes, it's a weird piece of syntax), the rest is applicative.
Sure! Might want to add a quick warning that it's a 20GB download too.
What about an idea like having a submodule Example (eg Data.Maybe.Example/Data.Either.Example/etc) in which you'd have a section called generalizations, and an example section where people could try to capture common usage patterns for the functions in that module, or examples for the default instances for a like (like how Traversable works for Maybe/Either a/(,) a/[]/etc) without the need to go into the source to find out. That would make even some doc pages like Data.List smaller, which has a lot of examples inlined. Then you could link from the function name from one to the other. Don't know if haddock can be instructed to make that linking easily, and if maintainers would be interested to invest that additional effort for those modules. 
Nice!
Might be a good idea to just get Snap 1.0 on hackage so people can start using it and providing feedback. That might help get its engines lit again.
One comment (from bss03) I found really useful that I haven't incorporated yet was to use the reader monad. Because I'm not massively comfortable with using mtl (yet) I've avoided doing it so far, especially when the straight function code would be pretty similar. However, I've realized that if I'm to capture path components, isMatch is going to need to log the match under certain circumstances. So, in practice I'm doing to need isMatch to be an m Bool where non-capturing conditions could be restricted to a MonadReader and capturing conditions are restricted to a Reader/Writer stack of some kind. However, I haven't got a fully worked out version of that yet, so I haven't blogged about it yet.
Sounds correct. I'll add a footnote.
This is why the gl library is there to offer real raw OpenGL binding.
Pretty much. =) I have been building up more mid-level bindings in `quine` with an eye towards splintering them off eventually. e.g. https://github.com/ekmett/quine/blob/master/src/Quine/GL/Block.hs#L83 provides GHC.Generics support for serializing a large number of different Haskell structures to uniform buffers.
So, I cloned the repos from GitHub and I was trying to install them into a shared sandbox. Installed `snap-core-1.0.0.0` no problem. Then tried `snap-server-1.0.0.0`. It requires `io-streams-haproxy`, which failed to install (GHC 7.8.3, Cabal 1.20, HP 2014.2.0.0, Windows). So, I guess, things like that (compatibility across GHC versions, OSs, etc) are holding the 1.0.0.0 release... I submitted an [issue](https://github.com/snapframework/io-streams-haproxy/issues/5).
Hum, didn't know about Quine. Cool but impossible for me to understand right now ;P Like in this : "newtype Offset (a :: *) (b :: *) = Offset Int" is a and b some kinds of phantom types? The explicit kind is there because of PolyKinds right? And why is Offset a Category? Is it just a fancy way of implanting and addition operation on it? (And Num would not be a good idea for this) Anyway, nice stuff!
`Offset` there is a bit of work in progress. Basically it describes how to get down to a part of a whole in a larger structure. The idea is that I can construct a way to work with offsets so that if i have a structure like (pseudocode): struct { float foo; mat4 bar[20]; } baz; i could use the offset of the array `bar` in `baz` to find the start of the matrix, compute the offset of the nth element, then find an offset for the particular member of the matrix i am interested in, without having to serialize/deserialize the entire structure. I'm trying to explore how to build these up in a composable manner such that I get the ability to dereference parts of a larger C-like structure without having to parse the whole thing around it.
That is the general idea.
For the raw bindings, you should check out [gl](https://hackage.haskell.org/package/gl), from /u/edwardkmett 
But Vulkan is almost here. 
Hopefully this works out better than the last time Khronos tried to throw everything out and start over.
There's lots of jobs in Scala, more than Haskell I'd guess. It's not as nice, but better than C... 
I'm going to be "that guy" and recommend Java. It has been proven by many years of enterprise use. It gets updated but stays backward compatible. It is extremely well documented and stable. It isn't going anywhere (immortal like C). There tons of libraries to do whatever you want and you can interface with existing C code. It is extremely fast and beats C in speed for certain benchmarks and its general library (though huge) will get you through most tasks easily.
Have you looked at [the happstack book](http://happstack.com/docs/crashcourse/index.html)?
I was actually working with quite a few languages (I mentioned all of the tried on the grid somewhere here. Python, Java, Go, Common Lisp and D) and while I do admit that it fitted a lot of the needs, it can get quite complex. Mostly because I have almost no training in OOP and, I think you can guess, it shows with a lot of redundant code. Honestly, I gave up after one of my friends cut away most of my library and made it smaller, faster and generally better... and I couldn't understand most of his arguments. Almost a year of putting a lot of extra effort and it is frustrating that I had so little to show for it. Before anything else: yes, I know that I should learn that stuff and I'm not blaming other people. I'm saying that it is difficult, possibly a worthy investment. I'm going through [Cave of Programming](https://www.caveofprogramming.com/) courses in my own pace at the moment, to get firmer grip on the basics.
Correct me if I'm wrong, but I think the "closed" here is the "rows" or "routes", whilst the algorithm allows for arbitrary "columns" or "properties/guards" to be added. So it definitely wouldn't work if you were dynamically adding routes at runtime, but compile-time extensibility should be OK.
&gt; D isn't purely functional. [Nope.](http://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Pure) D is not [referentially transparent](http://en.wikipedia.org/wiki/Referential_transparency_%28computer_science%29) -- Haskell without foreign imports is. EDIT: Replied to the wrong comment. Leaving this here though, maybe it'll get seen my /u/MintyGrindy.
The problem is I'm one of those persons who wants to experiment with things as I'm reading about them. Unless the introductions use Yesod examples, I'm going to have to learn another framework just to be able to learn Yesod, and that's not really sustainable.
I'm looking for something rather more complex. Thank you though.
&gt; D is not referentially transparent It is.
&gt; How do you think this works? Roughly: first, `putStrLn` gets *evaluated* and returns an `IO ()`. Then, `a` gets *evaluated* and returns an `IO ()`. Then `main` gets *evaluated* and returns an `IO ()`. The resulting action then gets *executed*, which means that some (but not all) of the aforementioned functions get executed too and produce side effects.
Nope, it works the same in D.
Uh, I don't think khronos has ever done that before?
That's always the way things work with new major releases for any package. There will be a few early adopters that are working with the new version before the official hackage release, but most people will wait. With Snap 1.0, even though the changes under the hood are significant the majority of the user-level API will remain the same, so upgrading should be fairly straightforward.
this is obviously the first version but I think you should at least write that ($=!) is strict. I had guessed at that but I had to check the source code to use it. 
No...at least not to my satisfaction. The Snap Framework has always tried to maintain a pretty high self-imposed bar for the quality of hackage releases, especially major ones. We want our users to be confident that when we upload something to hackage we will continue to support it well into the future.
Hmm. IANAL, but I believe that if you are only using it in library form, you're good to go with pretty much whatever license. Worst case, as your program is unlikely to be a derivative work, you have to say that ascii-progress is licensed under the GPL and that you can get the source code from ...
GL 3.x was never supposed to actually "throw everything out and start over", though, it was just an effort to deprecate old stuff. Which it did.
You're right, with Qt I'm referring to hsqml and the style it requires. I think it's a good option, but it isn't very Haskelly feeling.
here is a link to the [f# mvc for wpf github page](http://fsprojects.github.io/FSharp.Desktop.UI/), and here is a link to the [series](https://github.com/dmitry-a-morozov/fsharp-wpf-mvc-series/wiki) with detailed descriptions. he also has a chapter in the f# deep dives book. it's a very good book. i am still a beginner at f#, but i find it to be very helpful to see the real-world use cases of the language.
It went badly in that it was a complete PR nightmare. They started work on the Longs Peak release, and when the deadline came and passed they waited something like 8 months before they said anything to anyone about it. As this was basically the first thing Khronos had done with OpenGL, a lot of us were quite upset. Regarding "throwing everything out and starting over": There was also supposed to be a whole shift in the API to a more functional style in which many of the properties of an object became immutable upon creation. This object model that was proposed for Longs Peak was never included in any of the later OpenGL specifications. This is the "start over" component I was referring to. OpenGL adopted some of the ideas behind Longs Peak, in that they deprecated the whole fixed pipeline, but they left that whole functional object model thing on the floor. I put a project on hold 10 months explicitly to exploit that structure that never materialized, before abandoning it after they came back from the Longs Peak media blackout saying "oh, we're not doing that because the CAD folks said no." That is a bit sarcastically paraphrased, I'll admit ;) Further, when you factor in the delay between DirectX 10 (2006) and OpenGL 3.3 (2010), there was a 4-5 year window in which OpenGL just couldn't compete on DirectX 10 class hardware, and a large part of it was they didn't even get the crippled OpenGL 3.0 out the door until 2008, in part due to an extra 8 months wasted on that media blackout. I realize Vulkan is a very different scenario. Longs Peak was basically a product of the guy at nVidia who'd made the nice portable OpenGL driver lots of folks loved on Windows. Vulkan on the other hand appears to have a wide support base across nVidia, AMD, etc, and it has the benefit that it has some competition from Apple's work on Metal kicking them into high gear.
Today I wanted to try this. I guess I'm still too inexperienced - could you provide some examples?
I was only starting to get into graphics back then (2008), but that still feels somewhat overdramatized to me... The comittee made a decision that ended up being a compromise of what the various users wanted, and in the end it turned into something most people ended up liking (people still call GL 3.x "modern GL" and it is still what is taught in courses that have ambition to teach "modern GL" and "a modern perspective of the graphics pipeline",) albeit, as you said, way behind schedule. Apple(OSX) and intel(linux) even dropped support for the compatibility mode. I don't know what the dropped ideas behind longs peak were, and whether they would've genuinely made the API better or not, but relying on a standard before it was published is obviously something you do at your own peril. I agree though that the delays made GL non-competitive with DX for quite a while, and I'm sure quite a few users were lost to DX. But comittees are known for these kind of things, look e.g. how long it took for C++11 to appear and gain adoption, or C11. I think overall khronos is still doing quite an outstanding job compared to many of those other comittees, and probably has a better track record... But yeah, Vulkan appears to have so incredibly wide-spread support and push behind it, that it'll most certainly be a different thing. Literally every single important and not-so-important GPU vendor from both the desktop and mobile/embedded world seem to be behind it.
&gt; relying on a standard before it was published is obviously something you do at your own peril. Sure. That same reason is why I'm not stopping work on anything I'm doing just because Vulkan is coming. I learned that lesson, overly dramatically or not. =)
Ditto. I might even wait until WebVulkan becomes a thing (my hobbyist engine supports iOS, android, OSX, linux, windows and the browser (using emscripten), so I'm using a very conservative intersection of WebGL, GL ES 2.0 and GL 2.1), although I'll certainly crank out a PBR or two with it once it becomes available, just for fun.
https://hackage.haskell.org/package/rmonad-0.8.0.2/docs/Control-RMonad.html That's effectively what the rmonad package that was linked in the post does.
Yes. The problem with that encoding is that it is quite hard to work with though when it comes to polymorphic recursion. We can also tweak such a construction to have more category theoretically sound foundations by defining a Category where knowledge something is an object in the category is a constraint on the objects: https://github.com/ekmett/hask/blob/master/src/Hask/Category.hs#L69 We can define "one Functor to rule them all": https://github.com/ekmett/hask/blob/master/src/Hask/Category.hs#L88 This latter Functor is suitable for Functors between any two arbitrary categories, so now you can map to or from functions, natural transformations, monad homomorphisms, monoid homomorphisms, the category of constraints themselves, etc. not just functions with constrained end-points. 
I haven't had time to look over it in detail but I noticed you used Show for the user facing output of your Peg data type, you should avoid that as Show is designed to produce output you can then read again with Read and also useful for debug output in general. Often it makes more sense to just make your own class for output designed to be seen by the user and just use derived Show instances for simple serialization and debug output. You should also (not just in Haskell) avoid comments like this one in Keyboard.hs which just describe what the code is doing, not why or how (useful in case of complex algorithms). '\n' -&gt; return Nothing -- Return Nothing on newline Also in Keyboard.hs you should consider putting some of the nested cases in functions (possibly defined locally with a where clause) to make the whole thing easier to read. I am also not sure why you handle any other key pressed as a special case. If you just want to ignore any key but newline and arrow keys you could just call getKey recursively if any ignored key is pressed. From the description I would expect Nothing to be returned if another key is pressed though. I assume you wanted to write everything yourself but for a real project I would use a library for colored terminal output, something like ansi-terminal (haven't tested it but usually established libraries on Hackage handle a lot of special cases you don't handle in your code like cross-platform support). In Board.hs the underlneNth function has quite a few things to improve. You do not need to pattern match on a parameter in a separate case, you can do that in the function clause directly. But doing things manually for every version seems like a waste in general so I would probably use something more like underlineNth :: Int -&gt; Board -&gt; String underlineNth n (Board p1 p2 p3 p4) | n &gt;= 0 &amp;&amp; n &lt; 4 = concat [ "[ " , insertAt n (show Underline) $ map show [p1,p2,p3,p4]) , " ]" ] where insertAt :: Int -&gt; a -&gt; [a] -&gt; [a] insertAt n x = uncurry (++) . second (x++) . splitAt n underlineNth _ b = show b (I haven't compiled this so it might be slightly broken but i think you get the general idea). I am out of time now but I hope this helps. And please don't let this discourage you from learning more Haskell, just trying to help.
The `modifyAt` is handling `[]` specially, but it isn't inductively recursive, so it is wrong for indices out of the bounds of the list anyway. A better `modifyAt`: modifyAt f ix l = case splitAt ix l of (prefix, x:suffix) -&gt; prefix ++ f x : suffix (prefix, []) -&gt; prefix With `lens`, you can write: `modifyAt f i = ix i %~ f`
Right you are, thanks for the correction. : )
I'm asking because I'm interested in the history of abstract binding trees/binding signatures. Previously, I thought that it first appeared in 'An Illative Theory of Relations' (published 1990) by Plotkin. You as well as Licata &amp; Harper seem to suggest that such a notion has been used in the Nuprl community even earlier so I'd be very interested in a better reference.
Sorry, `lens`? What's that?
Thank you for the advice! &amp;nbsp; I felt that Mastermind.Keyboard was definitely the most awkward part of the whole program. I don't remember what my original thought process was there, but yeah, just recursively calling getKey until a newline or arrow is probably a good way to go. I do like the local where clause to simplify nested `case`s. &amp;nbsp; As for `underlineNth`, yes, it's ugly brute force, but I'm having trouble understanding your solution there. I should probably read up on `uncurry`, I don't think I've seen that before.
I made a library similar to this: http://hackage.haskell.org/package/monad-statevar My library is not confined to the IO monad, so you can use $= inside of (say) an STM transaction.
All good comments - I'm not sure I can address all your points! I had no experience making Cabal packages, so I just let it use the default for base. I hadn't realized that could be an issue. I see what you're thinking for `Mastermind.Colors`, but I might just leave it. A giant series of `show X` vs. a giant `where` clause. In `Board`, I'm not sure why I made a huge lambda at 66! `min` is much better. That's a good catch. Not sure how to simplify 78 with `uncurry`, but I'll look into it. I like your idea about using SMs in `getPegs`, but /u/matchi pointed out there is a `randomRs` function that is even simpler. I also don't know much about SMs, I'll make sure to read up on them. Thanks for all the advice! You've pointed out a some things I wasn't even considering, and now I have some new tools to look into :)
IANAL either, but if you use a GPL-licensed library in your code, your code also has to be GPL (or more lenient). There is the more lenient LGPL, which is the GPL with an exception that allows linking (so e.g. you can write proprietary code that uses a DLL if it's under the LGPL, but not if it's under the GPL). But even then, the LGPL linking exception does not apply to static linking, which GHC uses when compiling libraries together. Note that this restriction (namely, that code using GPL libraries has to be GPL-compatible) has not gotten much testing in court cases, though a quick google suggests that in a recent case, *Versata Software vs Ameriprise*, it was ruled that this restriction is valid. So, when you license your library under the GPL, you are in fact prohibiting people from using your library in proprietary software.
http://hackage.haskell.org/package/vivid
If you are interested, I also finally managed to pull the same trick embodied in the `control-monad-queue` package with the last algorithm mentioned in the original post, in [Fun with the Lazy State Monad](http://blog.melding-monads.com/2009/12/30/fun-with-the-lazy-state-monad/), although I would definitely recommend Twan van Laarhoven's suggestion in the first comment to that post.
What would you like to try with it? The readme show how to use it with gchi, does that work? Mainly you will use the parseTomlDoc function, incase of success it returns the parsed values with have ToJSON instances specified. So you can either use the parsed Toml values, or convert to Json and use your Aeson fu (tutorials for using the amazing Aeson lib are just a gg search away). I hope you manage! Lemme know in case you need more support... 
Yep, I'm interested in knowing that as well. I wrote some audio stuff in hsc3 last year and would like to know what the differences are. 
My project [PostgREST](https://github.com/begriffs/postgrest) has many open issues and I am happy to guide people who want to make contributions. It's the second most popular Haskell repo on Github (right after Pandoc) and has interesting potential for research into declaratively defining APIs on the web.
Have a look at some tutorials: * https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial * https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/basic-lensing * http://www.haskellforall.com/2012/01/haskell-for-mainstream-programmers_28.html * http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html * https://www.fpcomplete.com/user/tel/lenses-from-scratch * http://twanvl.nl/blog/haskell/cps-functional-references - the source There are two libraries that I know of implementing lenses this way: * [lens](https://hackage.haskell.org/package/lens) - the big one * [lens-family](https://hackage.haskell.org/package/lens-family) - the small one, with much less features
nice - looks awesome - congrats as a side note: you might want to insert a .gitignore like this: https://github.com/github/gitignore/blob/master/Haskell.gitignore in the future - you don't really need all the stuff in the /dist folder on your git ;)
Hi, first of all, I'm going to assume that you're trying to use Nix (the package manager), and not NixOS (the operative system based on that package manager), since you run on a mac. For the installation process you can refer to: https://nixos.org/nix/download.html Basically, it boils down to: 1) creating the folder /nix with your user, and running the command at the top of the page I linked. 2) adding in your `.bashrc` the five lines posted here: http://permalink.gmane.org/gmane.linux.distributions.nixos/15161 They contain useful environment variable detailing where your system expects to find the compiler, libraries ecc. 3) moving to the `unstable` channel, to get the current haskell goodies: $ nix-channel --add https://nixos.org/channels/nixos-unstable nixos $ nix-channel --remove nixpkgs $ nix-channel --update So, you have the package manager now, let's switch to haskell things. The installation process if you're used to a standard package manager would be to first install ghc, then install cabal-install, then the libraries. However, this approach gets in the way when you are trying to do functional builds (as Nix does), because in order to function correctly, ghc is required to know about the installed libraries; but from a functional perspective, for ghc to do so, the libraries should be an input in the construction of ghc; that means ghc can't be built on its own. The nix way of doing this installation, (or, at least, one of them), is declaring an environment which contains ghc AND the libraries you'll need. So, I prepared for you a gist with the basic installation https://gist.github.com/meditans/ab0403725a91eee5b939 you should put this file under `~/.nixpkgs/config.nix`. This configuration file instructs nix to build current `ghc`, `cabal-install` and some tools so that you have your ide covered. Note that `ghci-ng` has to be built from source for a Hackage bug. Make sure to correct the source path or comment the line inside `haskellEnv`. If you want a new package, just write its name at the heading `# other packages here`. Now, for the installation command, issue: $ nix-env -iA nixos.pkgs.haskellEnv --option extra-binary-caches http://hydra.nixos.org --option extra-binary-caches http://hydra.cryp.to I know it's a bit long, but the infrastructure that permits this is fairly new, and so you have to specify manually the binary caches, if you want them. If you want to read more about this new infrastructure, which is called HaskellNG, go here: http://lists.science.uu.nl/pipermail/nix-dev/2015-January/015591.html As always, if you need more information, don't esitate to ask :)
Agreed that if it's due, that's not worth the trouble. But for future reference: if you continue to have trouble kicking the curlies-and-semicolons habit, you might consider this style: else do { foo ; bar ; baz } That style is popular for other kinds of delimited lists. People find it easier to read (once you get used to it) and maintain than the traditional delimiter-at-EOL style.
Regarding the `--option extra-binary-caches`: I think it should also be possible to specify the caches globally instead by editing `/etc/nix/nix.conf` and changing the `binary-caches` variable: binary-caches = https://cache.nixos.org/ http://hydra.nixos.org/ http://hydra.cryp.to/ If NixOS is used instead of plain nix, the `nix.conf` is generated from `/etc/nixos/configuration.nix`, where the following can be added: nix.binaryCaches = [ https://cache.nixos.org/ http://hydra.nixos.org/ http://hydra.cryp.to/ ]; 
&gt; People find it easier to read (once you get used to it) and maintain than the traditional delimiter-at-EOL style. I agree that the typical Haskell style is nicer than *delimiter* at EOL, but it's arguably harder to maintain than *terminator* at EOL (deleting the first statement in a block in this style doesn't require any additional changes).
Hakyll is a good and simple option.
The big question up for discussion is: how long should the support timeframe be on LTS 2. LTS 1 had three months, but that was just to test the processes. I think we're ready for something longer, the question is what we should target this time around.
I guess you haven't seem the chapter ["Blog: i18n, authentication, authorization, and database"](http://www.yesodweb.com/book/blog-example-advanced) of the [Haskell and Yesod book](http://www.yesodweb.com/book). Still, the [advice](https://www.reddit.com/r/haskell/comments/2yfn3d/is_there_a_nice_blogging_application_written_in/cp91xfw) of /u/guaraqe, [Hakyll](http://jaspervdj.be/hakyll/), is maybe the easiest and straightforward option.
Inference for `Hask` works pretty well, actually, the kinds do most of the work for you, and here `F` determines `Dom F` and `Cod F`, which puts a lid on most of the worst problems folks experience in practice.
This looks cool, but is it possible that you store user passwords in plaintext?
If you want to increase the support window gradually, why not double it for now and see how that goes?
This isn't really a Haskell talk -- I think I mention Haskell only a few times and talk about lenses for a couple of minutes, tops. That said, I've had a lot of folks ask me when the video would be available and, well, now they are.
Is Vector immutable and structure-sharing?
&gt; supports TVar Superficially. You can't use them like TVars though -- combining multiple get/set/update operations into a single atomic action. Use of STM is unlikely to be available to foreign libraries though, that the package is intended for that use, so I'm not sure this is a big loss. I would like additional documentation on why, for IORefs, the code is using non-atomic write, but atomic modify? Why not use the atomic variants for both, or neither?
Why a typeclass? Why not just a record type? You should be able to avoid both the Storable and Element constraints on the record type, and just have them on the "constructors" that convert from a "concrete type" (e.g. Vector Int) to you store interface (e.g. "NcStore Int") and "eliminators" that go the other way. It seems like that would be quite a bit simpler to being with AND wouldn't force use of ConstraintKinds or associated types. Are there any laws for this typeclass?
No, the passwords are NOT stored in plaintext. The backends take care of that. See https://github.com/agrafix/users/blob/master/users-postgresql-simple/src/Web/Users/Postgresql.hs#L182-L188 for example.
Halide is definitely a great DSL success story. I think being able to add annotations would be a good idea, and would be useful for other things as well such as profiling information (at the moment the generated functions have very generic names such as 'map' and 'fold', so it can be difficult to trace these back to the source program). In terms of optimisations, ideally these would be backend agnostic (even the sledgehammer approach of INLINE / NOINLINE would be useful in many cases) but we'd have to look at some real applications and see what kinds of optimisation pragmas are useful. Halide certainly has an advantage here of being in a very narrowly focused domain (2D image processing) whereas Accelerate's domain is much broader (dense array computations), but I'd certainly like to look into this.
Vector provides both mutable and immutable implementations. I think the immutable implementation only shares structure if you compute slices.
&gt; It turns out, Fix (Info i ExprF) is usually written as a Cofree ExprF i &gt; (:&lt;) is a constructor for Cofree value, analogous to Info from the last post A link to the last post somewhere would be good. I didn't find one. &gt; because it seems like it would give us better error positions later for some reasons I don't completely understand. Not exactly helpful. :/ &gt; The only problem is that we have to locally replace current stream of s-expression to those expressions found in the current compound "token". Why is this not just a nested parse?
The goal is to *not* support both versions. This is a trade-off we've discussed in the past. Supporting multiple GHC versions is definitely nice from a user perspective, but puts a heavy burden on package maintainers. I believe it's unlikely that GHC 7.10 will have enough support for the April 1 cut-off, so what will likely happen is that Stackage Nightly will switch over to GHC 7.10 sometime in April or May, while LTS sticks with GHC 7.8 until ~~2.0~~ 3.0.
Same here - games stuff. I have a file with more than 100 of these one-liners in it, pretending at expressions and currying, pulling from my playings in Haskell and Clojure: ident = lambda x: x const = lambda x: lambda _: x juxt = lambda *fs: lambda x: [f(x) for f in fs] comp = lambda *fs: lambda x: reduce(lambda a, b: b(a), reversed(fs), x) cmap = lambda f: lambda xs: map(f, xs) mapcat = lambda f: lambda coll: list(chain(*map(f, coll))) flip = lambda f: lambda a: lambda b: f(b)(a) curry2 = lambda f: lambda a: lambda b: f(a,b) uncurry2 = lambda f: lambda a, b: f(a)(b) curryPair = lambda f: lambda (a,b): f(a)(b) onargs = lambda f: lambda args: f(*args) Most are IO/domain-specific stuff to my job, but it's really nice to be able to write something like: map(comp(locAt,wpos), sel()) ...instead of spending several minutes doing the same old thing I've done for years by hand, or banging out the same old several-line for loop full of statements yet again. I just wish I had types (me of a few years ago would never have believed I feel that way now).
Although I'm only beginning by exploration of Haskell, and there's something of a hump to get over, I am impressed by what I have found so far.
If it would be licensed as BSD3: you would both get the source and be able to use this library in some bigger commercial application. With Haskell we need to look for licences which will allow to do full program optimizations... i.e. dynamically rewriting libraries during compilation (not just linking - but fusing) We need the commercial users to start adoption of Haskell, but keep its spine in academia; so it would be nice to have open-source libraries compatible with commercial needs and have big players giving some nice libraries or just sponsoring community developers. 
&gt;I have enough stereotypes associated with being a scientist when it comes to coding, with constant condescension that 'real programming is not project Euler' or 'Decade of C programming? Nice. Here, have my copy of K&amp;R, you might find it enlightening' etc. Actually, I don't want to crap on C because of my weenie-hood. I'm just goddamn tired of writing numerical, matrix-processing code in languages in which *numerical matrices are not even a thing* and *NaNs propagate out*. That latter issue already makes SciPy and NumPy a pain in the damned ass. What I'd *really* love would be numerical programming in Idris. That would be *lovely*, to just *prove* once and for all that my code does the right calculations on the right data.
&gt; I would like additional documentation on why, for IORefs, the code is using non-atomic write, but atomic modify? Why not use the atomic variants for both, or neither? Oh, right, atomicity! That explains why modifiable variables need a separate typeclass: with only typeclasses for readable and writable, the only update functions we can build out of these two primitives are non-atomic ones. So the (currently undocumented) guarantee for `HasUpdate` must be something like "*even* in the presence of concurrent mutation from other threads (but still in the absence of a thrown exception), `($~ id)` should restore the previous state". What guarantees does `atomicWriteIORef` offer? With such a name, I would have thought that it would prevent prevent inconsistent results such as getting the high word from one write and the low word from another, but since the documentation mentions a "barrier to reordering" instead, I guess that the basic `writeIORef` already has this property and that `atomicWriteIORef` provides a stronger property. I'm not familiar enough with these low-level barriers to understand what this guarantee means in practice. Any idea?
i was thinking the same (not that im any authority on support windows). is there any idea for the maximum size of the support window?
So a coconsultant is a genius? New word for "genius": "sultant"!
You seem to be confusing referential transparency and controllable side effects. &gt; In D, functions have side effects as part of ordinary evaluation Nope, evaluating D functions doesn't produce any side effects. Executing *results of their evaluation* (thanks for correction) does.
interesting!
This makes me sad. I think ideally packages would support the three last GHC versions. Two would be ok, but only one? That's kind of limiting...
Evaluating D functions does produce side effects. D doesn't distinguish evaluation and execution as Haskell does.
Thanks, that clarifies a lot. Supporting multiple versions is a huge pain and is best avoided. &gt; LTS sticks with GHC 7.8 until 2.0. I'm guessing that should be "until 3.0"?
Mentioned 9 times going through a textbook. I have a pile of 200+ textbooks that I started and never finished, how does one find the time? Nice views on problem solving.
I agree, and try to do so for my own packages. The problem is enforcing that on all package maintainers, which many find too onerous. It's a tough line to walk between "easy for package users" and "easy for package authors." I think this is a good balance for now.
&gt; I'm bringing up the discussion now so that people with feelings one way or the other have a chance to discuss it. For sure. I guess, that was my contribution to the discussion :) Obviously more important to see how others feel.
Oh wow! The example even uses getUrl. That's pretty neat.
I think that's generally worked out poorly in the past with similar efforts. For example, the last Haskell Platform was significantly delayed by waiting for GHC 7.8 to be ready. I'd much rather that LTS has a release schedule and incorporates new versions of GHC when they're ready to be added.
Fair enough :)
SPJ works at MS Research, yes. However, MSR is really very separate from MS proper. They are not involved with products and do pure academic research. It's a pretty highly esteemed place. In academic circles MSR is as reputable as CS departments at top tier universities. As such there's a big difference between MS using haskell in production and MSR's funding of GHC work. At least from the perspective of industry adoption. 
Thanks! I use youtube-dl all the time, not sure why I didn't think of that... no spare cycles at the time I guess :P
If you haven't encountered it yet, "Parallel and Concurrent Programming in Haskell" is a great resource: http://chimera.labs.oreilly.com/books/1230000000929 Also this presentation: http://www.infoq.com/presentations/Concurrent-Haskell 
Well, Linux distributions like RHEL support things for 10 years but only properly support (more than security fixes on critical issues) them for about half that time. I don't think it makes sense to go anywhere near that length of time of course but it might make sense to scale support down over time like that too, e.g. full support for a while and then limit support to critical bugs or security holes.
I find no difference in difficulty of removing the top item: position the cursor immediately before `foo` (inside the brace) and press `Shift+Down`, it should select to the same point on the next line. I think the only more-difficult shift is moving an arbitrary entry to the top (other than the second, which is equivalent to moving the top entry down), and EOL-termination has no advantage there.
It'd be cool if you could integrate this work with cabal-install-bin [0] so that the symlink method is attempted when installation fails. A command-line flag to cabal-install-bin to direct it to use the symlink method would be great too. [0] https://github.com/quchen/cabal-install-bin/blob/master/cabal-install-bin
Yeah you're right. I used to have a bunch of nested ifs everywhere before I optimized and prettied it up, so some things got changed to guards while others stayed ifs. I'll probably go through this later tonight and fix the things you guys have pointed out.
Hah! We've all been there before :)
Concurrently is truly the bee's knees. It recently gained a (sequential) monad instance as well. Sometimes, when running concurrent computations, I want to return information about an error (say, if one page load fails, all the other page loads should be cancelled, but I want to know the return code of the failed one). This can be done with Concurrently by shoving the error information into an exception, but I wanted to use an Either instead, so I copied the Concurrently code, modified it slightly, and put it in a package called [conceit] (http://hackage.haskell.org/package/conceit).
I'm normally surprised at what that thing manages to download when i throw stuff at it.
Sorry, you're right... the issue inverts, it's moving an arbitrary item to the *bottom* that's more difficult with EOL termination. Also, is there some sort of ritual I can perform to ward off the evil of those freakish vi incantations? \*shudder\*
/u/edwardkmett : not sure if you're able to share, but I'd love an example of a common problem and/or solution you keep in your head. Mostly just curious. I like the dialectical approach to learning you outlined in the talk. 
Sure, given (a,b) is the product constraint of two constraints a, and b. we can make that even nicely act like abifunctor where we can partially apply it with some trickery class (a, b) =&gt; a &amp; b instance (a, b) =&gt; a &amp; b for which you'll need to turn on flags until GHC stops kvetching, then `(&amp;) :: Constraint -&gt; Constraint -&gt; Constraint`.
[clckwrks](http://www.clckwrks.com/) has blogging support with an atom feed and a media gallery. Post are written in markdown. It's themeable and can be extended via additional plugins. However, I must apologize that the example-dot-org does not build this week. clckwrks is in the midst of being upgraded to happstack-authenticate-2 and there has been a lot of yak shaving during the process of updating the starting code. I'm committing all the changes now and hope to have the online docs updated soon. clckwrks is far enough along that it can host clckwrks.com, happstack.com, and even some commercial sites. However the blogging and media plugins could definitely use a lot of easy contributions. For example, it should be switched from the markdown script to the pandoc script. (Alas, probably not the pandoc library due to GPL infection). My focus has been more on getting the internals right and less on making the plugins slick. Though, the two are intertwined. Adding features to the plugins makes it clearer where there are deficiencies in the core. Some high priority internal tasks after the update to authentication is making it fully support I18N (it has fully unicode support, but much of the interface itself can not be translated yet), replacing all the javascript with ghcjs, and migrating from classic forms to SPA and virtualdom technology. From a blogging point of view, that should lead to a nicer user experience, but not negatively affect existing content.
But you actually can install things.
I collect tools that have a broad range of applicability. Hrmm, off the top of my head, solutions in search of problems: * [Succinct](http://github.com/ekmett/succinct) data structures are amazingly useful tools, that fit into a weird gap in the space of what folks know how to do in big data and give you all sorts of things like efficient medians, etc. that that whole community has given up on. * "Monads are monoids in the category of endofunctors" is a throwaway line for many, but it gives deep insight once you work through what a monoidal category is. [(You can even turn it into code!)](https://github.com/ekmett/hask/blob/master/src/Hask/Tensor/Compose.hs#L134) Given that you can find Day convolution, build up that "Applicatives are monoids in the category of endofunctors equipped with Day convolution" then you can keep going. * Fritz Henglein's work on discrimination gives you linear time versions of almost everything. I want to get this packaged up some time soon. I've started [here](https://github.com/ekmett/discrimination). Using Day convolution we can take his 'initial' encoding and make it final, giving us access to all the instances. * For that matter, thinking in terms of initial and final encodings is quite useful. You can start with a syntax tree with no semantics, then figure out your semantics and eventually throw away the syntax tree. * [Automatic differentiation](http://github.com/ekmett/ad). This pretty much makes how you derive any notion of back-propagation for neural networks, etc. trivial. * Hamiltonian Monte Carlo or hybrid monte carlo gives a tool for solving large classes of probabilistic programming problems / models. It lets you exploit knowing the derivative. If only I had a way to compute that derivative automatically... =) * Tanh-sinh quadrature gives me the closest thing to a 1-dimensional "universal integrator" I know of. Folks have used my [`integration`](http://github.com/ekmett/integration) package without knowing how it works at all, and it almost always "just worked". * In general I try to find solutions that are in a category theoretic sense "universal", which means that any other solution will necessarily factor through the one I gave. This lets me see how the universal version of the thing relates to any particular incarnation someone else comes up with. * [Concurrent revisions](http://research.microsoft.com/en-us/projects/revisions/default.aspx). I have a half-finished [repository](http://github.com/ekmett/revisions) for working with these in Haskell using sparks rather than full IO actions, but I've never gotten around to finishing it up. They can provide a pretty amazing parallelism/incremental computation benefit once you add the `record` functionality from the later paper! * Every analysis trick in Okasaki's Purely Functional Data Structures, especially the "data structures from number systems" trick. * "Everything in (1-)category theory is a Kan extension". So understanding Kan extensions gives you a way to move results around. When you're looking for a result, you can often find a way to express it as a kan extension. e.g. we just found out to today that you can express the store comonad, as CoYoneda of a very simple GADT, giving us a way to write it out as a simple left kan extension. * [`bound`](http://github.com/ekmett/bound) lets me avoid thinking about name capture when writing compilers. * There are lots of special purpose tools that are useful to have like [`hyperloglog`](http://github.com/ekmett/hyperloglog), etc. that I tend to implement in Haskell as a library, so that when I need them I can just reach out and grab them. Many of these solutions can be offloaded from my brain and packaged up in libraries, making room for more solutions. =) * [`speculation`](http://github.com/ekmett/speculation) lets me opportunistically parallelize many algorithms that seem inherently sequential. * Valiant's algorithm can take advantage of sparse matrix multiplication to do efficient context-free grammar parsing. Parsing combinators using these could parse much more efficiently than you'd expect. [paper](http://www.cse.chalmers.se/~bernardy/PP.pdf) Similarly, some problems in search of solutions: * Good representations for probabilistic programming. How to represent measures nicely? Stick to Hamiltonian Monte Carlo? Algebraic simplification? * Realtime raytracing. [Signed distance fields?](http://github.com/ekmett/quine) * Taylor models are useful for high energy physics and a bunch of other spaces, they avoid a lot of the problems with loss of precision due to interval arithmetic, while still providing a known enclosing interval when run. There is a fair bit of research on these, but how to implement them nicely in Haskell? * [Type error slicing](http://www.macs.hw.ac.uk/ultra/skalpel/). This gives error messages in terms of parts of the code users wrote, rather than made up types the compiler generated! Potentially a good use of concurrent revisions? * How to build a "nice" computer algebra system tooling using Haskell. Lots of these aren't really full fledged problems so much as intermediate problems. They are problems that when solved, provide me with a better ability to provide solutions to other problems.
(I'm the author of Vivid) Vivid and hsc3 do similar things, but they feel very different to use. Vivid has a lot more emphasis on being user-friendly and fast to develop with. I'd personally only use Vivid for a livecoding performance, for example. On the other hand, hsc3 is more featureful at the moment. If you like, give both a spin -- you'll notice a difference right away. It may very well be a personal preference thing though, so it's likely we'll end up with a situation like with Snap and Yesod, where each is popular with people whose aesthetics fit with it.
btw. see also clckwrks, it is --unlike LambdaCms-- it tries to do point and click installation/configuration/etc.
Two students where graduating on [LambdaCms](http://lambdacms.org/) this winter. In order to complete their studies they put a significant effort into research. One of them was did a security comparison between LambdaCms and WordPress. To do so he took the list of WP's CVE entries and for each of them analyzed the LambdaCms stack (Haskell/Warp/Yesod/lambdacms-core) to see "if, how and in what layer of the stack" that particular vulnerability would be dealt with. He did not manage (yet) to publish this research. The main findings where that pretty much all vulnerabilities found in WP over the years, where mitigated by LambdaCms and underlying technologies. Anyway. To answer your question. I'm not aware of any security research done on Haskell web frameworks, except for what I just told you. There are no CVE entries for Haskell web frameworks, indeed. That is probably due to (1) the high level of "security awareness" in the Haskell community, (2) the compiled/strongly-typed nature of the language which prevents some categories of vulnerabilities, and (3) the small installed base. Also I don't believe security research directly relates to CVE entries. It is also possible to research how a type system can be used to mitigate particular classes of vulnerabilities (e.g.: buffer overflows, SQL-injections and XSS). This is more the Haskell approach. If CVE entries exist for a piece of software it does not tell me that it is safer then a piece of software that does not have documented vulnerabilities. I prefer to use other metrics to analyze the security of some software.
You might also be interested in a piece of example code I wrote a couple hours ago, [a dead simple concurrent url downloader](https://github.com/codygman/haskell-concurrent-downloader-example/blob/master/src/Main.hs) which uses mapConcurrently. Here is the code: import System.Log.Logger import System.IO import Network.Wreq import Control.Lens (view) import qualified Data.ByteString.Lazy as BS import Control.Concurrent.Async downloadUrl :: String -&gt; IO () downloadUrl url = do updateGlobalLogger "DownloaderApp" (setLevel INFO) infoM "DownloaderApp" ("Downloading: " ++ url) response &lt;- get url BS.writeFile fileName (view responseBody response) infoM "DownloaderApp" ("Downloaded " ++ url ++ " to " ++ fileName) where fileName = "/tmp/lastDownloadedFile" main = do mapConcurrently downloadUrl [ "http://www.google.com" , "http://www.reddit.com" , "http://www.github.com" ]
Constraint kinds will be much nicer to use in &gt;= 7.10 code, if only because importing a module exports something thats constraint kinded will result enabling constraint kinds extension in the client module. this is not the case for ghc &lt;= 7.8
&gt; Did you mean to say the opposite? Nope. Obviously I cannot speak for all of Haskell. But I've used some Haskell web frameworks and got a little more in-depth with Yesod on the topic of security: I must say that Yesod is doing really well on the topic of security. &gt; there are no CVE entries due to the high level of security awareness Well I suggested "security awareness" just as one of a bunch of reasons. But then I think that it really is a factor. Consider this... If one programming community is a group of mostly self-taught coders, copy-paste happy, believing in just-make-it-work; and another one is a group of mostly language geeks, CS degree holding, believing in it-should-not-just-work-but-also-be-correct. In which community will you likely find more security awareness? To know what I think contributes to the absence of CVE entries on Yesod et al, re-read my post: I've numbered them for you. I stand by them. On the topic your link: A buildbox being compromised (which runs a lot besides Haskell code; e.g.: Linux + services) is some entirely other story then the security offered by Haskell web frameworks (topic of this thread).
Sound to me like something FPComplete could sell. Free support on LTS (backed by FPC + community) for X months, possibly extended by FPC support for another Y months if you become a subscriber to such service. Just an idea.
&gt; Consider this... If one programming community is a group of mostly self-taught coders, copy-paste happy, believing in just-make-it-work; and another one is a group of mostly language geeks, CS degree holding, believing in it-should-not-just-work-but-also-be-correct. In which community will you likely find more security awareness? In the second one... unless the first community's language of choice (aka PHP) is used almost exclusively for web programming, while a big chunk of the second community's attention is given to areas where security is a non-issue. But, more importantly, you still didn't explain how more security awareness can result in a worse process for handling security issues. &gt; On the topic your link: A buildbox being compromised (which runs a lot besides Haskell code; e.g.: Linux + services) is some entirely other story then the security offered by Haskell web frameworks (topic of this thread). How often such things happen, how they are handled, the community's reaction -- all of this is very relevant whenever the topic of security is raised. Somehow I don't get the feeling that security is really high up on the list of importance for most Haskellers.
I agree with you that the process should be more in line with the rest of IT, i.e. getting CVEs. I am only aware of DOS vulnabilities, not code injections or anything else. If you look at most templating libraries or database libraries you will see why. It's hard using them in a way that you are vulnerable.
There are two security issues in Haskell which I've been aware of: * The aeson library could parse short strings with exponents, and by converting them to Integer (unbounded arbitrary precision) take up a lot of memory, leading to denial of service. Fixed in recent aeson versions. * GHC would take command line arguments and treat them as RTS options, certain RTS options could be used to trigger writing to files. When run as a CGI binary you have some control over the command line, which could write to arbitrary files. Newer GHC versions default to only taking safe RTS options unless you include the flag -rtsopts, plus very few people use CGI nowadays.
The investigation is basically concluded. The problem is that the people who would be centrally involved with wrapping it up and moving forward on a new deb box have been preoccupied, for example with the GHC release. In the meantime, we've felt no pressure to act quickly, since as has been noted, the service has been very peripheral, and had few users.
&gt; An easy-to-overlook part of the code is the line: `let m1 = map calc [3.5, 3.2, 3.6]` The really cool bit is that Haskell is LAZY, so it will only try to perform the mapping as it needs to. If the mapping had been eager, as in most languages, then the program would have taken over 10 seconds to execute (being the cumulative delay time of 10.3). Actually, the map produced a (Fractional a) =&gt; [IO a], which even if you had forced to be strictly evaluated would still only have produced 3 IO-value "instructions" to wait for three-ish seconds over the course of just a few nanoseconds. The fact you waited in parallel is because you ran `mapM async` over that list of instructions. Had you instead run `mapM calc [3.5, 3.2, 3.6]`, it would have waited in sequence, as that is the definition of mapM. Laziness has no user-visible practical effect here.
Sounds good. I found hsc3 rather uncomfortable to use to be honest. I'm interested in creating something similar in other languages, can you give any tips for learning about interacting with supercollider?
Thanks for doing this concrete writeup! It made me realize many things in my toolbox could and should be formalized into generic libraries.
I only read the abstract, but isn't this community generally opposed to viewing monads as "imperative-statement wrappers"?
&gt; not syntactically a function &gt; having no type signature Okay, so these are easy enough to recognize. &gt; overloaded What's this mean? Principal type includes a typeclass constraint? Isn't there also a set of defaulting rules, at least for the REPL?
I seriously cannot deal with it. Every time I click a link on that site I don't even bother checking the content.
Not an expert, but if you have a do block with every line prefixed with liftIO (including RHS of "&lt;-" construct), you can move the liftIO in front of the do. That might or might not make it more readable, depending on context... In particular, at Just 'r' -&gt; do liftIO $ setCursorPosition ((snd $ fieldSize f) + 2) 0 liftIO $ putStr "\nEnter the number of iterations to run: " can be Just 'r' -&gt; liftIO $ do setCursorPosition ((snd $ fieldSize f) + 2) 0 putStr "\nEnter the number of iterations to run: " and so on. That is, you should make the monad for the do block to be IO, not (InputT IO). But one could argue that parallelism with the other blocks where InputT is necessary is desirable. Another thing I noticed is that in the same case statement, you're matching on 'w', 'a', 's', 'd', but then essentially throwing away the match by calling to moveCur with the matched character. I don't know much about the optimizer, and I suspect it can probably detect this and fix it, but perhaps this isn't the best style... The first thing that comes to mind to fix that is a way over-engineered solution involving lenses and ReaderT, but there's probably a better tweak. Even breaking it out into four separate functions could be better, though, and at the very least you should pattern match on the character instead of the if/else tree. It's a neat project, though!
Minor nitpick: They are generalized modulo the type-variables that have type-class constraints.
Doesn't this violate the principle that the `Applicative` instance must agree with the `Monad` instance, if both exist? &gt; let getConc n = Concurrently (threadDelay n &gt;&gt; getLine) &gt; runConcurrently $ liftA2 (++) (getConc 1000) (getConc 100) hello world "worldhello" &gt; runConcurrently $ liftM2 (++) (getConc 1000) (getConc 100) hello world "helloworld" 
They might not have everything you need, but the FP Complete case studies were assembled towards the end of helping people in situations such as yours make a case to higher-ups: https://www.fpcomplete.com/page/case-studies
* Ask Ben Ford (@boothead and /u/b00thead) about hiring numbers for Haskell developers * Ask Jonathan Fischoff (@jfischoff and /u/jfischoff) about training Haskell developers * Ask both of them about team output
The defaulting rules only apply once you've already decided the type has to be monomorphic. (Which can also happen for other reasons, e.g. `x^2` where the type of `2` doesn't occur in the type of the whole.)
Here's a current example: https://github.com/yesodweb/wai/issues/297 [SSL Labs](https://www.ssllabs.com/ssltest/) reported that a Haskell-based web service was vulnerable to a certain MITM attack involving insecure renegotiation. It turned out that the test was inaccurate - there is in fact no vulnerability - but there is nevertheless work going on to ensure that Haskell-based sites will pass this test in the future. Now that commercial use of Haskell has vastly increased, it may be time for us to think about how we can become more directly involved in more formal security review processes together with other programming languages.
Same, I can only think of the unlikely situation where two similar apps, one in Haskell one in X, are developed and maintained side by side. Any other situation (and probably this) will be incomparable without making significant assumptions, right? Hopefully OP will be able to use numbers that are indicative rather than proper measurements.
I wonder if there's some sort of standard benchmark for that? Some specification document which each researcher wanting numbers for a given language or technique X could give to a team of X programmers who have not heard about the specification before, and would then be able to compare the results with the other researchers who used the same benchmark.
[This may be old](http://haskell.cs.yale.edu/wp-content/uploads/2011/03/HaskellVsAda-NSWC.pdf), but it sure is some concrete data.
I like mumus, but I LOVE lambdas.
Papers are much more accessible than textbooks, and you can carry those on your tablet.
do you have any metrics in mind? what measures "software productivity" besides the non-numerical analysis of other humans? (honest question) also, I think selection bias and sample size and coincidence and many other factors plague studies of this kind, if any exist. (though if you know of good ones for software engineering generally, i'd be curious to read then). personal experience and professional anecdotes say much more than ~~fetishizing~~ privileging raw blind empiricism/numbers (in this case). 
I'm referring to ordinary code inside a "do" block in Haskell, or an ordinary function in D. This demonstrated non-referential-transparency in D, because it has no mechanism to create transparent references to arbitrary expressions.
The closest thing is the old ARPA challenge which compared several languages implementing the same spec: http://www.cs.yale.edu/publications/techreports/tr1049.pdf My favorite excerpt: &gt; In conducting the independent design review at Intermetrics, there was a signiﬁcance sense of disbelief. We quote from [CHJ93]: “It is signiﬁcant that Mr. Domanski, Mr. Banowetz and Dr. Brosgol were all surprised and suspicious when we told them that Haskell prototype P1 (see appendix B) is a complete tested executable program. We provided them with a copy of P1 without explaining that it was a program, and based on preconceptions from their past experience, they had studied P1 under the assumption that it was a mixture of requirements speciﬁcation and top level design. They were convinced it was incomplete because it did not address issues such as data structure design and execution order.