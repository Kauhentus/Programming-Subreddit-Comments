I feel exactly the same.
Yep, unless and if are just an illusion. 
I always add the parens as well. Old habit.
Yeah, it's great. I love AST macros
The first thing you should do is try writing it single-threaded and compare performance. I would be very surprised if it wasn't faster in this case. Stream (instead of Enum) causes functions to be executed per item. That means that this code: |&gt; Stream.map(fn(elem) -&gt; spawn(fn -&gt; send(me, {self, elem, to_dvorak(elem, map)}) end) end) |&gt; Stream.map(fn(pid) -&gt; receive do {pid, orig, dvorak} -&gt; {orig, dvorak} end end) is going to repeatedly spawn one process and wait for it to return, getting the overhead of spawning processes without the benefits of concurrency. By using Enum instead of Stream, then all of the processes get spawned and then you get the results. As for the complexity of this bit of code, you should really be using [Tasks](http://elixir-lang.org/docs/stable/elixir/Task.html) to do this sort of thing. |&gt; Enum.map(fn word -&gt; Task.async(&amp;to_dvorak(&amp;1, map)) end) |&gt; Task.yield_many(optional_timeout) |&gt; Enum.map(fn {_, {:ok, result}} -&gt; result {_, {:error, reason}} -&gt; raise_or_handle(reason) end) I would actually throw the entire cleaning up of the `yield_many` response into a named function, but that is much more straightforward. Every time you spawn a new process, you are sending a deep copy of your map. While this would increase complexity of the code quite a bit, this is the sort of thing you would normally save in an [ETS](http://www.erlang.org/doc/man/ets.html) table. Then the map would not need to be copied over and over. While concurrency can allow parallelism, BEAM concurrency is not designed or optimized for the sort of parallelism you are trying to use it for. It is generally not a good idea to spawn a new process to do something tiny and then return. the overhead of spawning a process, copying data to it, managing a large number of processes, copying the result back, and matching against the mailbox is often worse than just doing it in a single process. Batching work and using process pools can improve things.
i have been watching Elixir from 0.x something and its a really well though out language, big salute to Jose Valim, and a big thumbs up for Phoenix (to all its contributors and people who give support online) Now one thing i want to say is something that does not quite yet exist. I really wish there was a "phoenix light" version, something like express with Plug middleware you could just bolt on. maybe something like Flask in Python. phoenix i great, but offers alot of thing i will never use, and it makes things more complex for noobs like myself. The way webapps are usually made today does not require any view layers, or such. just a rest/graph etc backend. Supereasy to share the backend with other frontends. Its a shame that phoenix has a builtin javascript pipeline, i always build my own anyway, the js world is a mess so i maily use makefiles and a superminimal pipeline (10-30 lines of code) and utilize unix stuff. i know there are many microframeworks out there, but most are not maintained, or has near zero users. Phonenix could take charge, and have a separare install (or project) that would be basically just the thing most people want, raw power and fast response elixir provides. so basically something like flask, with a easy plugin system. other than that, no bad parts at all. love the language, and i wish i could use it more often.
Just reading into memory? I can run the following code in &lt;2 seconds on a [SCOWL](http://app.aspell.net/create) dictionary with 676583 words. File.stream!("words.txt") |&gt; Enum.count Something doesn't line up here.
I guess I look at that as more of an opportunity. :)
And fault tolerance and distributed processing.
&gt; Deployment setup sucks. It's getting better, but there's no capistrano-like fire and forget solution. Be prepared to do a fair bit of DevOps work to get a deploy script that works for you. You've been using it in production for a few months. Would you mind doing a post on how you deploy your Phoenix app sometime?
Enum.stream! doesn't actually read the whole into memory here, as far as I know. If I use File.read!("words.txt") |&gt; Enum.count in a module function it throws an error in iex after printing a bunch of words. When I did the same in an init function in a Phoenix project it worked, but like I said it took several minutes.
That code took &lt;3 seconds to throw an error that strings don't work in `Enum.count`. It took 5 seconds to do: File.read!("words.txt") |&gt; String.split("\n") |&gt; Enum.count 
You can also put the following code into a .exs file and run `$ iex program.exs`. But as it has been said, it still lauches the VM. qwerty_to_dvorak = %{ ?a =&gt; ?a, ?b =&gt; ?x, ?c =&gt; ?j, ?d =&gt; ?e, ?f =&gt; ?u, ?g =&gt; ?i, ?h =&gt; ?d, ?i =&gt; ?c, ?j =&gt; ?h, ?k =&gt; ?t, ?l =&gt; ?n, ?m =&gt; ?m, ?n =&gt; ?b, ?o =&gt; ?r, ?p =&gt; ?l, ?r =&gt; ?p, ?s =&gt; ?o, ?t =&gt; ?y, ?u =&gt; ?g, ?v =&gt; ?k, ?x =&gt; ?q, ?y =&gt; ?f } to_dvorak = fn(word) -&gt; dv_word = word |&gt; String.downcase |&gt; String.to_char_list |&gt; Enum.map(fn(c) -&gt; Map.get(qwerty_to_dvorak, c, c) end) |&gt; to_string {word, dv_word} end full_dictionary = File.read!("words.txt") |&gt; String.split("\n") index = for w &lt;- full_dictionary, do: {w, :y}, into: %{} full_dictionary |&gt; Stream.filter(fn(word) -&gt; !String.contains?(word, ["e", "E", "q", "Q", "w", "W", "z", "Z"]) end) |&gt; Stream.map(to_dvorak) |&gt; Stream.filter(&amp;(Map.has_key?(index, elem(&amp;1, 1)))) |&gt; Stream.each(fn({word, dv_word}) -&gt; IO.puts("qwerty: #{word} | dvorak: #{dv_word}") end) |&gt; Stream.run System.halt(0) I find it harder to read than your ruby version, but it is still ok. Btw the index has no downcase keys in you version so if the file to read has uppercase words it could fail ? (Actually I'm not sure what this program does !)
Maybe you've been used to ruby for a long time ? I hope I'll feel like you in a couple of years.
&gt; Deployment setup sucks. It's getting better, but there's no capistrano-like fire and forget solution. Be prepared to do a fair bit of DevOps work to get a deploy script that works for you. You know that Capistrano is Language independent, right? right? We've been using Elixir with Capistrano for a while now. I don't see how this is a problem.
In my talks, I usually mention three things **Processing speed** Raw CPU power is something where Erlang doesn't shine. If you have some parts in the system that need to crunch a lot of numbers, you may want to do it in another language, and there are ways of integrating that into your Elixir system. If most of your system is heavily CPU based, Elixir is perhaps not the best tool. That being said, for "typical" systems which perform many concurrent I/O and CPU tasks, Erlang will be fine, if not better than throughput oriented platforms. Erlang was built with strong focus on latency, so it will keep the predictable performance, even in the times of overload or increased faults. **Learning curve** I don't think learning Elixir is hard, but there's quite a lot of new things to learn, so it may seem daunting. You need to learn new language, functional programming, some aspects of BEAM, OTP framework, and you'll likely want to do something with Phoenix. That's quite a lot of new things, none of which is IMO hard, but it still takes time. I definitely think it's worth taking the effort to learn this, since Erlang/Elixir/Phoenix stack is really fantastic. When it comes to backend development, I think it's the best stack available today, mostly due to Erlang/BEAM, with Elixir/Phoenix improving the developer's experience. **Ecosystem** The amount of libraries is obviously not as big as that of e.g. JavaScript, and Ruby, but there are already quite a lot of various packages available, and you can also reach for the Erlang ones. Occasionally you may stumble upon some generic problem which is not solved, and then you may need to roll your sleeves and work a bit more, but in my experience this doesn't happen very often. I also mentioned some thoughts on this issue [here](https://www.reddit.com/r/elixir/comments/44sbry/phoenix_is_rails_5/czt9owv).
Yes, Capistrano is language independent. Getting it to work with `mix` and `exrm` requires writing a lot of cap tasks yourself, however. In addition, there's several deploy issues that aren't present on a run of the mill Rails app. You need a build server (which can be your dev machine, but probably shouldn't). You need to keep track of versions so that you can generate the right appup files. You can't run two copies of an application (i.e. staging and production) on the same machine without tinkering with the vm.args files or you'll get module name collisions. Hot upgrades are awesome, but they don't always work if you're upgrading very low level dependencies, so you have to have a cold upgrade fallback. Mix tasks aren't included in compiled builds, so you'll have to write your own script (in Erlang, not Elixir) to kick off the migration tool, which is not hard but is very poorly documented. You can use a traditional Capistrano clone/symlink/restart workflow and just run `mix phoenix.server` on your production machine, but you miss out on one of the best features of BEAM, namely hot upgrades.
Coming from Django, I didn't notice any downsides to the Elixir dev setup. I've never used Capistrano so what's so impressive about it?
I'm curious why did you choose Elixir for such tasks? Based on the fact that Elixir requires Beam VM to be installed, I would assume it wouldn't fit devops use cases (but I guess another question here would be what those devops use cases actually are). I also perceive Elixir as a great language for long-lived applications (daemons, webservers, etc.), a different world than small scripts and CLIs.
If the rails setup is like Phoenix, I can see why so many people tend to just build monoliths. Having directories labeled by function: controllers/models/views would lean someone towards not thinking too deeply about the separation of concerns. In Django, everything is a separate app so while you can (I certainly have sometimes) tangle all your apps with different models/views together, it feels messy to do so from the get-go so things tend to get factored out earlier. I'm not sure if Umbrella projects are a good fix for this.
Just use Plug. It comes with a router, Plug.Router IIRC.
Are you saying you found a Quality Without A Name? :) there's a good talk by Dave Thomas &amp; Bruce Tate about it: https://www.youtube.com/watch?v=fklep3sUSWo
I feel like if you have five layers of nested parenthesis, your code is trying to tell you something about refactoring.... For anything I'd use nested calls, I tend to just use pipes in Elixir.
They should compare it to Erlang's syntax with 3 different types of line terminators, 2 of them context dependant. 
Its actually also valid Ruby syntax, however I've yet to see it be used. Its cosmetic, and I think you'll find that you will use it very infrequently. More often than not, you'll be passing in anonymous functions rather than executing it.
I've been doing Elixir for several years now, never written any Erlang. Though I will say reading source code is frequent enough, but its not that hard to read once you understand Elixir.
This is not a wart. Ruby has the same syntax, and it has a reason. The reason for using the hashrocket =&gt; syntax is when either you have a variable as a key name or you have a non-atom key (string or whatever else you can think of). Both have their purpose. In Ruby however, you still see code doing `{ :key =&gt; :value }` even though most everyone dropped 1.8.7 support a long time ago (generated code often still does this).
Oo I'll give that a watch. I agree with Dave Thomas though. My brain just clicked with both Ruby and Elixir so far. It's a strange phenomenon. But I mean. When you know, you know. You know?
&gt; The reason for using the hashrocket =&gt; syntax is when either you have a variable as a key name or you have a non-atom key (string or whatever else you can think of). That's exactly what I said. So I'm unclear where you think I'm misunderstanding.
I've done a bit. for scripting you have 2 choices: an .exs file using only the standard library, or an full mix project that builds an escript. I create .exs scripts for my own use where previously I would have used python, mostly as practice. For use by others I will still use python or shell. The only real complaint I have is OptionParser is weak. I build an ArgumentParser lib (https://github.com/jisaacstone/ex_argument_parser) so if you go the escript route you can use that. I've also used docopt-erl (https://github.com/plux/docopt-erl) - interface uses charlists instead of strings but it works all right. 2nd issue is compared to python the stdlib is quite small (even no json parser). Consequently the number of things you can do with just an .exs file is also much lower.
I built a type of financial calculator for work recently, in Ruby, in elixir, and in JavaScript. In Ruby, it takes several seconds to run a scenario. Using puma server and threading, I was able to run several scenarios concurrently, but can't put this into production as it locks up the threads available to the rest of the app. In elixir, a single scenario runs in the 100ms range. Using tasks I can run around 100-200 scenarios concurrently before maxing out the RAM on my dev computer. Since assembly and sorting of the results map is outside the tasks, 100-200 scenarios results in ~200ms to get result. In JavaScript, the scenario runs in &lt;10ms. The JavaScript code is 10x faster, but I also can't run 100 scenarios easily in js. The scenarios are math intensive cascading amortizations, so this highlights the benefits and drawbacks of elixir. Unfortunately, I can't post the code, since it's company stuff, but I'm a fairly good dev in each of these languages and have optimized the code in each case. The one disadvantage the elixir implementation has in terms of speed is that it uses the decimal library, while the Ruby and js versions use floating point math. Im sure this has some impact, but I found it relatively minor in benchmark testing I've done.
In Ruby it's not cosmetic, though it is not the only way to do it. Most often you'll think of this when working with a proc, which you can call with: 1. `my_proc.call` 2. `my_proc[]` 3. `my_proc.()` And, well, maybe other things, I dunno. At this point we're discussing Ruby, not Elixir, so I'll stop here and apologize for intruding.
&gt; You would prefer to write %Point{:x =&gt; 10, :y =&gt; 20} instead of %Point{x: 10, y: 20} only for consistency purposes? I think there are a lot of options available here. 1. Structs are not Maps with an additional hidden field, and thus can be sensibly written in a different structure than Maps. I feel a lot of people will be comfortable with the distinction since in other languages structured data types (structs, records, frames, etc.) are not implemented in the same way as unstructured data types. 2. Maps only accept string keys. This has the benefit of allowing for certain optimizations that would be more difficult to implement if the base Map can handle keys of any type. It also allows for an easy conversion of structs to maps and vice versa since you're only converting atom-key to string-key. 3. Basically what we have today. Structs are maps, but are defined in different ways. Structs are defined as `%Module{atom: value}` and Maps as `%{key =&gt; value}` &gt; Regarding Map.fetch/Map.get, I honestly feel the standard is to do map[key] but, even though, I am not sure we could unify them. I thought the use case for Map.fetch/Map.get is that sometimes in the midst of a series of pipes you want to just get a single element from the `Map (e.g A |&gt; B |&gt; Map.get(key) ) `, so you need a function. Using the accessor [] in the same way won't work. ---- &gt; Btw, do you have more examples of where it is ill-defined? Not quite, though I have been noting anything that's 'odd' in my mind down so I can remember it. For example, ranges `1...2` never got support for skipping elements. It was noted in the mailing list and on github that it would *eventually* happen, but it hasn't. So we have to use Enum.take_every. Coming from Python, I was pretty surprised to see first class range support exist but be so limited. Also, ranges can only be integers. I'm wondering if Ranges are implemented as a protocol because if they are, then a library can add support for string ranges like A..Z without the standard library having to deal with the question of "what is a string range, if we are in UTF-8 anyways?" ---- Question. Since you seem to be familiar with Elixir historically. Did the character sigil operator really change from `%C{}` to `~c()` at some point. I've seen examples of the previous in old blog posts, and they're talking about sigils and not Maps/Structs. 
Agree with this, so much
I haven't tried to run it this way- I'll try it out and see how it handles 100 scenarios. The js version is still a work in progress this week :-)
You'll want to limit it to 2 x physical cpus if you have HT.
I would phrase that a different way: if you want to really *understand* Elixir, then forget about Elixir (at first); just learn Erlang, separately, as if Elixir didn't exist. Then, already knowing Erlang well, learn Elixir. This makes things *way* easier. Coming to Elixir directly from e.g. Ruby gives you all sorts of invalid assumptions; coming from Erlang lets you [read the language backwards](http://slatestarcodex.com/2013/04/11/read-history-of-philosophy-backwards/) to see exactly what parts of Erlang Elixir chooses to keep or change, and it's *in those considerations* that you can see and understand the philosophy of the language. Specifically, I would advise anyone who wants to learn Elixir to read the [Learn You Some Erlang](http://learnyousomeerlang.com/) book, which is nominally "about" Erlang, but is actually a guide through the Erlang Runtime System and the OTP framework, with Erlang's use as the code-sample language being mostly incidental after the first two chapters. After reading that—really reading it, with plenty of experimentation—start in on the Elixir docs.
I have a decently large and very very busy Phoenix app in elixir right now. To do a from scratch setup just follow the Readme, first git clone it, then run a command to setup a new PostgreSQL Docker if not using an existing one, then copy the prod.cfg.secret.template to prod.cfg.secret, edit it to include a new secret, as well as the PostgreSQL details if not using the prior Docker command, then run the Docker build command and run it, oh and I have a script too do all that too, seems easy, easier than most servers I run from others for sure. Dialyzer does need help though.
IO.stream(:stdio, :line) is a wonderful construct for use in .exs files.
Capistrano is not unlike [fabric](http://www.fabfile.org/) (if you're familiar with that). It's basically a Makefile-like system with facilities for running tasks on other machines. As long as you're fine with what I call the "Cthulhu-deployment model" (wherein a host-who-shall-not-be-named extends its many tentacles into your Cloud to do your bidding), it's great. I find that it tends to lend itself to weak deploy reliability and weak security (because you deploy everything from one machine and that machine has to have access everywhere). That's probably good enough for 80% of projects, though (which has made it understandably popular in Ruby-land).
I find the argument that structs are different so they can have a nicer syntax to be a bit too convenient. I don't find maps with only string keys to be useful as associative arrays. I like option number 3 the best. Map.get(map, key) is subtly different than map[key] because it allows for Map.get(map, key, :some_default). This is a fairly useful pattern that Python also makes use of to great effect. Ranges are iterated using the [Range.Iterator protocol](http://elixir-lang.org/docs/v1.0/elixir/Range.Iterator.html).
Well, in Elixir atoms for module name have an uppercase first letter. Is that a problem for you ? If you use appropriate syntax colors it is ok.
This is pretty amazing. Doing it this way improved the character conversion by about 3x. It definitely isn't ... succint... But it's definitely faster than using String methods, you're right. The macro version needs to be wrapped in a quote block. I had some trouble doing it that way, but I think that's the way to go. It's much more DRY obviously. Thanks!
I believe that maps have O(1) so my proposal may not change speed. I just found it would be more correct use of the available datatypes.
I would like to counter #1, which I think is actually quite wrong. Elixir/Erlang is actually the best platform I've ever seen in terms of runtime introspection and debugging. You can toggle tracing of function calls globally, on a module level, on a process level, on a function level, on a function with certain arguments level. You can run commands and query the system during runtime. You can update code during runtime if you have a bug you want to fix but can't take the system down, and quite a bit more. It has a runtime debugger in the form of IEx.pry (at least that's what I've heard, I haven't actually had a need to try it myself), but having spent many years in Visual Studio, Elixir definitely lacks the capabilities the VS debugger provides. I also think Elixir/Erlang are great for text processing. Binary pattern matching has made writing parsers one of my favorite things to do in the language. Is it as fast as other languages? No, but it's actually quite competitive in my experience, and definitely less error prone.
Looking back at my original post, I think I may have overreached. I should not have called Elixirs' take on Map creation a 'wart'. It's a valid linguistic approach that values optimizing two common use cases, at the exepnse of creating syntax overlap. It is not the approach I would take or support, but it does have benefits even in retrospect so it was my mistake to judge it so harshly. However for the sake of completing this discussion, I'll leave my response to your arguments below. ----- I guess what it comes down to is that I don't feel the hashrocket is a large syntactic burden and due to that people can (and do) define even 'simple' atom maps via the hashrocket. I'll see `%{:pkg =&gt; "foo"}` almost as much as I see `{pkg: "foo"}` simply because the difference is only 2 characters per K/V, and that won't discourage someone from writing it in their preferred way. And thus, we now have the case of something as basic as map definition becoming a code style. ---- &gt; What I believe that he assumes that you don't appreciate is that these two ways to create a map are two very common but entirely different use cases. I suppose so. When I look at your example, I can't help but think "I wish there were a Package struct to get default values for the things left undefined". &gt; Python does this (slightly more convolutedly) with .... dict(a=1, b=2, c=3) Elixir also has a `Map.new([{:b, 1}, {:a, 2}, {:c, 3}])` It's necessary to take care of the use-case of programmatically creating a map. I'll note that it's not the same as having two different functionally incompatible ways of creating a map. A fourth option that I didn't list (due to parsing complexity), is that %{key: value} could be modified to create maps where key is any type as well. However, it would look awkward and be hard to read if the key became a very complex type. 
Thank you for replying, and thanks for your great work on exrm. It's a great tool, but it's full of surprises if you're coming at it from a ruby/python/node background. Do you mind answering a couple of follow-up questions for me? * What's the minimum set of files I need to add to the git repo to allow anyone who clones it to be able to generate an upgrade? * We do continuous deployment, and manually managing versions is tedious and error prone. Is there a way to reliably create an upgrade from the code running on the target server to the code in the master branch without having to manually bump the version every time we want to deploy? * Currently our migrations are done with the command `bin/myapp rpc Elixir.Ecto.Migrator run "['Elixir.MyApp.Repo', &lt;&lt;\"$MIGRATIONS_DIR\"&gt;&gt;, up, [{all, true}]]."`. Is there a better way to kick off this process? * What's the best way to run the same build artifact in both staging and production? Right now, I'm using the `$RELEASE_CONFIG_DIR` env variable and maintaining two `sys.config` and `vm.args` files, but that feels like it's fighting the system. What should we be doing? Thanks again.
&gt; Do you mind answering a couple of follow-up questions for me? No problem :) &gt; What's the minimum set of files I need to add to the git repo to allow anyone who clones it to be able to generate an upgrade? Relative to the `rel` directory: - `lib/*` where the wildcard represents the set of applications required for the last released version of the app - `releases/RELEASES` - `releases/start_erl.data` - `releases/&lt;version&gt;` where `version` is the last released version of the app - `erts-*`, if you are bundling ERTS in the release. This is only needed if a.) you want to bundle ERTS, use `{include_erts, false}.` in relx.config if you just want your release to use the system Erlang, b.) it's only needed if you want hot upgrades to also handle upgrades of Erlang (this is in conjunction with bundling ERTS of course, if you aren't bundling, this won't be a concern for your app &gt; We do continuous deployment, and manually managing versions is tedious and error prone. Is there a way to reliably create an upgrade from the code running on the target server to the code in the master branch without having to manually bump the version every time we want to deploy? What I've recommended for this is generating version numbers in mix.exs based on the current git revision, as described [here](https://gist.github.com/jeffweiss/9df547a4e472e3cf5bd3). Note that José's concern in that thread is in regards to packages deployed to hex - for your own apps, it's irrelevant. This approach makes it easy to automate the release process, since you don't have to care about manually bumping versions, and it works naturally with all of the tooling, with the added benefit of being able to log in to a node and see exactly what git revision is running (assuming you didn't have a dirty working directory when you built the release). Any variation on this scheme should work just fine. &gt; Currently our migrations are done with the command bin/myapp rpc Elixir.Ecto.Migrator run "['Elixir.MyApp.Repo', &lt;&lt;\"$MIGRATIONS_DIR\"&gt;&gt;, up, [{all, true}]].". Is there a better way to kick off this process? I've gisted up the way I handle this [here](https://gist.github.com/bitwalker/ae5be60faf924d2e5082). I find it to be the simplest, but I also have a simple setup, the more complex you get, the more likely you'll want to build an escript which will execute these for you, as described [here](https://github.com/bitwalker/exrm/issues/67#issuecomment-183457937). &gt; What's the best way to run the same build artifact in both staging and production? Right now, I'm using the $RELEASE_CONFIG_DIR env variable and maintaining two sys.config and vm.args files, but that feels like it's fighting the system. What should we be doing? This really depends on how you deploy and provision systems, how you maintain configuration, etc. What you are doing is actually completely fine, and the reason why those variables exist actually. As alternatives, I know some folks use chef/puppet/ansible/etc for deploying to different environments, and applying the relevant configuration. My environment is AWS, so all of my configuration is applied via environment variables when the container is spun up, and is automated for me. I have a thin config layer which allows me to use the `{:system, "VAR"}` convention, with a `{:system, "VAR", default_value}` variant, so everything in my config file uses that for things which very by environment. I'm sure there are other options as well, but I couldn't speak to them as I'm only familiar with what's been described to me (and what I myself use). Hopefully that helps! Feel free to open issues on the tracker [here](https://github.com/bitwalker/exrm/issues) (assuming you haven't already), and I'll be glad to walk through things in more detail. There is a log of things already, and is itself a wealth of information for issues people have run into, solutions they've found, etc. Many of the open ones are discussions which I always appreciate input into.
Yeah, I can relate to the lack of any IDE debugging, it's certainly not up to par with other compiled languages I've used. It's worth mentioning that, outside the context of debugging, and depending on your editor, there is some great support for IDE-ish things I rely on, like go to definition, go to tests for a module/function, open help for a module/function, expand macros, etc. I haven't missed an interactive debugger enough for it to become an annoyance - but it is something I miss from time to time. I've been working with Elixir since ~2012-2013, and I have yet to see anyone try and build their own binary protocol in it just because they can, seems like most people are just using the capabilities to easily integrate with existing protocols/standards. For my part, beyond the normal usage of matching on arguments which are binaries, I've used it for handwritten parsers for date/time strings, zoneinfo/tzdata files, and a parser combinator library. I haven't had any real need beyond those yet, but having such a powerful mechanism for doing that kind of thing is one of my favorite features of the language for sure.
 I didn't implement the whole thing in floating point, but in doing some benchmarking of floating point math vs decimal math, I saw very little difference, ~5-10%. The initial pass on the elixir was much slower, as I was using the Timex library for dates. I switched it to initially calculating integer days from 1970 with Erlang calendar and using/comparing integers. This was an order of magnitude faster
You're welcome :-)
Thank you for the considered reply! I've learnt a lot in my last few days since being here. I think the thing I wrote that I most take issue with now, now I know a tiny bit more, is what I said about macros. As I learn more maybe I'll end up disagreeing with all of it, it's early days. Would still really like a compiler warning about data rebinding though. I'm putting Learn You Some Erlang on my reading list - I've tried halfheartedly before, but it's slowly dawning on me that I'm going to have to make the effort.
Here is what José has to say about rebinding. It's at least worth a read. http://blog.plataformatec.com.br/2016/01/comparing-elixir-and-erlang-variables/ 
Good to know. Thank you. :)
The maps are really hash-based tries. Lookups are O(n log n), where the base of the log is the number of bits in the trie/tree. The reason this is done is because of immutability. You can't really do normal O(1) lookup hashmaps immutability with good performance because you have to generate the entire map every time.
What do you wish existed? I'd love a new project to write with Elixir and I'd love to help the community grow.
I don't get why pattern matching using function definitions get so much praise. In the fib example in this blog post the Ruby version is at least as readable as the Elixir version. So much forking — really? There is just as much "forking" in the Elixir example. I find it cumbersome to have to write out function signatures for each case, often it is easier and at least as clear to just use the case statement inside a single function. Am I missing something here?
I think the Fib examples are very contrived, they're just the examples that everyone uses. Something like the subscription example speaks to me more. One of a handful of main problems I see when deciding why some ruby code ending up going horribly wrong is the developer didn't stick to 'Tell, don't ask'. Having explicitly different functions to handle different cases makes it easy for me to reason about what the function is meant to do.
&gt; At DockYard, our goal is to build the fastest and most robust applications for our clients both on the front-end and back-end the page stayed empty for 9 seconds while the 1.8mb ajax request for the actual content was loading. for a simple blog post.
I think Heroku and DigitalOcean are the big ones who currently offer hosting for Phoenix. I use AWS to host my app [Chow Chow](https://itunes.apple.com/us/app/chow-chow/id1076800869?mt=8). Just using EXRM releases built on Ubuntu. I should also note that I've heard that it may be a slight challenge to get EPMD to run correctly on PaaS platforms. Which is what allows you to connect two Nodes together. EDIT: Make a mistake Digital Ocean is not a PaaS platform
Just as a question how much time does it take you to deploy a release to Heroku? It usually takes me about 15 minutes to launch a new release with EXRM. I'm just wondering what the experience is like on Heroku. With a multi server setup do you have to upgrade each machine individually or will Heroku push the update to all machines?
I wrote a blog post on using Dokku to host a Phoenix app. http://ashleyconnor.co.uk/2016/03/06/deploying-a-phoenix-application-to-dokku.html
Neat! Lovely when math and music sound so sweet together. Can we improve this by using the list prepending syntax instead of Enum.reverse()? Then you don't need the List.flatten() call either since each element in the list is a string. I am not at my terminal to try this atm though. Will report back post haste!
Yeah, list appending is expensive, while prepending is cheap. Will play with it a bit more. Again, nice work! :D
How would you feel about having to edit one massive handle_{call,cast,info} function when you wanted to customize your GenServer? That would require all the boilerplate from "use GenServer" to be visible and you'd have to go in and poke the guts of this whenever you want to add what it is that **you** want the GenServer to do. Add, on top of that, that adding a feature is much easier if you can simply add a new function, handling a slightly different pattern instead of inserting lines in a case statement. Once a case statement grows too big you've put much more strain on the reader of the code, whereas each separate function will have a much higher likelyhood of being easily parsed on its own.
I use Docker to run tests on my Gitlab CI server and I use Dokku to deploy using HashNukes buildpack. Easy peasy.
I use exrm and deploy using a simple Ansible playbook.
I'm working on two setups. We're currently migrating towards Dockerized deployments so I'm working on a proof of concept for stuffing what comes out of exrm in Docker with Jenkins and an internal tool. We're also currently working with a Chef managed environment and I'm also working on a setup involving Jenkins, exrm and a nice cookbook for deployment. exrm ftw. Ansible seems legit. I've actually met very few people running Chef and a lot of Ansible / SaltStack folks.
I believe most of the optimizations on are the BEAM side of things. When doing a production build you usually want to use EXRM https://exrm.readme.io/ What you do want to keep in mind is what Erlang you have installed. Hipe Erlang is typically faster because it will compile your program into native code. Whereas non hipe Erlang versions will be interpreted. This is not always true though so you want to build both versions and test. 
i agree, especially the via part was new to me and very helpful as it is exactly what i was looking for. if i wasn't browsing reddit instead of actually implementing i'd have implemented it myself
Phoenix multiplexes each connection into another Erlang-lighweight thread, so this is automatic (and one of the big pluses of Phoenix)
Be careful of Hipe compiled code, it is not ALWAYS faster. There is a performance penalty between switching between bytecode and hipe, and if you do this frequently enough (like in a loop), you'll notice performance go down instead of up. The most frequent cause of this is Elixir standard library code that you may call inside your hipe compiled loop.
* BEAM is a very light requirement * Devops are using Ruby these times * BEAM has almost instantaneous startup times * Elixir is great for data manipulation, which is bread&amp;butter of many Devops activities 
Why are you even coming from Django? It's a perfect framework. Please turn back.
While Elixir/Erlang's performance is pretty solid for non-hardcore number crunching, the advantages of using Elixir/Phoenix are in the scalability and reliability features of the language in concert with OTP. I'm assuming you'd want high availability and getting the most out of the hardware you're running on and this is where Elixir/Erlang really shines. I've had a decent amount of experience in Ruby and I was able to be conversational with it in a couple of weeks. I've also noticed some great productivity gains with Elixir (and functional programming in general). It does require a shift in thinking if you've been an OO dev your whole career. All that said, I've been pushing Elixir/Phoenix hard and can't wait to use it in more production environments. The availability of engineers will come. Functional programming is the future! :) Good luck!
At work, we're building out our app to include real-time functionality for 5000 concurrent users----that's not really what Django is built for, even with the new Django-Channels addition. Our tests in Tornado/Twisted showed it dying with 7+ processes. Our tests with Elixir, has a single box handling 5000 concurrent connections with a 2% CPU usage, and only ever spiking to 25-30% during heavy broadcast periods. That answer your question? :)
Definitely, I wrote a 4chan thread image ripper in elixir and it is blazingly fast. the ease of setup with concurrency in elixir is amazing.
Definitely, I'm from a big company and we use it for a super duper concurrent scalable always online server, and it works great (6 months in prod)
As others mentioned, Elixir can be used for other purposes, such as CLI tools or crawlers. However, in my opinion the sweet spot of Elixir and Erlang are systems which need to provide continuous service, even in periods of increased load or unexpected failures. Many such systems will need to expose some HTTP interface, so it's understandable that Phoenix is used a lot. Notice that this doesn't mean that Elixir lives within Phoenix, but rather that Phoenix is frequently used within Elixir/Erlang/OTP based systems. As [Chris mentions here](https://dockyard.com/blog/2015/11/18/phoenix-is-not-rails): There is no such thing as a "Phoenix application". Your Phoenix projects are first and foremost Elixir applications, which relies on Phoenix to provide part of its functionality. Phoenix is written as OTP compliant, so it integrates nicely with the rest of your Elixir system. To oversimplify: Phoenix is just a library which you can use in your Elixir systems to expose HTTP interface. In a more involved application, there will still be a lot of stuff you're doing behind the scene which is not strictly related to HTTP. Such tasks will likely be implemented with "vanilla" Elixir and OTP and/or other 3rd party dependencies. 
Hey Chris, Thanks for the response. As someone who went from Erlang to Elixir to Phoenix. I felt that things I really liked about Erlang (spawn, gen_server, fsm, etc. aka the stuff that was different from Ruby/Java/JS) were not obvious parts of Phoenix. Though coming from RoR, there was a lot that was familiar and comforting. I'm excited about the future of Elixir and Phoenix. I will check out the book again, especially that chapter. Thanks.
I'm going through the phoenix book and abdolutely loving it! I havent been this excited about a language/framework in a long time. Thanks for your efforts Chris
Yes. Here's a datapoint: I've been a professional Elixir (mostly) dev for nearly a year now, at [Ably](https://ably.io), and I've still never actually tried out Phoenix. (It's on my bucket-list of things I want to try at some point, but I haven't gotten around to it yet).
I just made a name server with monitoring built in and it has nothing to really do with Phoenix. I'll be making a front-end for it with Phoenix and Elm, though. As many people have noted, I think you'll find that a lot of apps work like this; you have the backend functionality and it is/can be pure Elixir, but then you want to serve that functionality via HTTP(S) and naturally you'll want to use Phoenix since it's extremely good.
if it makes you feel better, only about half the companies that I know are using Elixir, are using Phoenix. 
What was the trading system written in before? Wouldn't you be concerned with the lower execution speed, assuming the old system was in C++ or Java? 
You're missing the link. (Also, it's not Apple Music, it's whatever Apple calls their podcasts app.)
It's interesting listening to Chris squirm on some of the misconceptions that Rubiests often have about Elixir/Phoenix. I think he handled it quite well, considering.
This is allowable repetition, seeing as you're basically writing an API wrapper.
Many components, some in C, some in Java, some in Fortran, all talking the same binary format. Performance was not a huge concern, but even if it were, an erlang based system would probably be better, because the existing system contained a giant monolith, and it could have easily been sharded by instrument type. 
Actually, I'd say Java is probably the most common language. C++ is very rare indeed, usually it'd be C in that case. Speed is actual not that critical in a lot of markets. The equities market, for sure, and the treasury market, to some extent. But many markets are quite slow moving. There are plenty of exceptions though, and the banks are surprisingly adventurous in terms of technology, so you occasionally work happening in more esoteric languages. (I know of one group at a big bank who are building their own language based on Haskell)
Building a good software product requires a lot of developer time, and if you're not a developer, that time requires a lot of money. Are you rich? Do you have a rich uncle? Are you sitting on a couple million in seed money? In my opinion, what you really want is a good relationship with a developer that has a track record of building minimum viable products for entrepreneurs. You need someone who can help you pair down the scope of what you want to build to a manageable size. If that person knows Elixir, great. If that person knows PHP, that's fine, too. Another route is to learn to code yourself. This is hard. Very hard. But it might be easier than raising money for a product which does't yet exist. Ask yourself these questions... * If I spend money on a developer, and don't get what I want (likely), would I be willing to learn how to code to do it myself? * If I have to learn to code (because I ran out of money), am I willing to spend years of my life getting to the point where I can do it well enough to build a solid software product? Good luck!
To add one thing about the language, where Elixir shines is not the syntax (either you're a fan of the ruby'ish syntax or you aren't) but the concurrency and reliability. I'd suggest not coming to a conclusion about Elixir until you've investigated what it provides over other languages and concurrency paradigms. Wish you well on your learning!
For me [`vim-polyglot`](https://github.com/sheerun/vim-polyglot) and [`lexima.vim`](https://github.com/cohama/lexima.vim) does the job.
Programming Elixir is a great book. Great at teaching the basics of Elixir syntax and functional programming, and pretty good at the other stuff. After that I found *Elixir in Action* to be a little better at helping me understand some of the more OTP aspects of things. I basically skipped all the syntax-related parts of *Elixir in Action* since I learned those from *Programming Elixir*. I felt it did a great job of helping me understand how processes and GenServers work.
And tooling! Although being a young language, Elixir has all the Erlang backpack at disposal. `:observer` comes to mind; that thing is incredible.
Oh, I should have clarified. I was specifically talking about the algorithmic trading market, where 1 ms is the difference between making money and losing it. But yeah, your absolutely correct about Java ruling other financial markets
&gt; Even Jose and Chris avoid making syntax a selling point because people's preferences are so wildly different. That's a good point. I don't love Ruby's syntax of def/end for object oriented style programming. I much prefered Smalltalk syntax, and Python as a 2nd place. I do however like Elixir's syntax when it comes to functional programming, and when combined with macros it brings me almost to the level of Smalltalk when it comes to writing a good DSL. The only place it falls short is the reliance on Kernel special forms for some items, but that's a fair trade off.
This makes it harder for other people to read your code. Everyone knows what the match operator does, but we'd have to study the definition of your macro to determine what your code does.
When you say embedded, do you mean elixir running on top of small footprint linux, or there there a close to the metal version of the Erlang VM that's suitable for micro controllers and such?
I mean [Nerves](http://nerves-project.org/) in particular. I don't know enough about the project to really go in depth about it, but hop into their Slack channel (#nerves on the Elixir Lang chat) if you're interested. It's a really cool project, so definitely check it out
After playing a lot with ORMs for about a decade, I've got some fairly strong opinions about this kind of pattern: I firmly believe that converting data/strings to package/class/object methods/functions is a recipe for pain. For example, what will you do if one of the URLs is "movie/#{id}/module_info"? I submit this alternative: def get(id, attribute, options \\ %{}) do TMDb.get("movie/#{id}/#{validate_attribute attribute}", options) end @supported_attributes ~w( alternative_titles credits images keywords release_dates videos translations similar reviews lists changes ) defp validate_attribute(attribute) do true = Enum.member?(@supported_attributes, attribute) attribute end If you still want to make all those functions, though, I like sasajuric's macro answer. If you don't want to use code generation, at minimum, you could abstract the TMDb.get("movie/#{id}/#{attribute}", options) part out into its own function, which you could call for each attribute: def release_dates(id, options \\ %{}), do: get_url(id, "release_dates", options) 
Why not build something? Projects are great for learning. :)
Some thoughts here...but most of it boils down to the fact that parsing out every piece of information into a struct is just not idiomatic. You're repeating information everywhere. You should just have a simple function that takes an IP address and spits it out in the new format or extracts information about it. And using an async tasks here is probably way more wasteful than not. Plus, you're still eagerly evaluating everything.
I haven't read the code yet, what is the overhead? Have you benchmarked it?
I haven't benchmarked it, but there are no big surprises -- it's O(n) on the number of keys in the struct, as you'd expect. ExConstructor's goals are correctness, ease of use, and no atom allocations (as atoms are not GC'ed). Anyway, look at the code, it's a quick read: https://github.com/appcues/exconstructor/blob/master/lib/exconstructor.ex
I need to know where to find the song from the beginning (it's a weird version of Cemetery Gates, but it sounds great... Kazzo maybe?)!
I just meant that it's best to do the least computation for the common case. You should probably parse to some internal representation that requires the least parsing or generating...which might just be a binary of some kind. Or you could just use what Erlang likes to, which is the 4-tuple.
Solve a problem in your life. :)
Query language of Ecto is plain terrible. After 1 month of using Phoenix I still have to consult docs every time I try to write query (with ActiveRecords I almost never needed to read docs, it's intuitive). In the end I just stop using it and wrote my own layer that allows to write plain SQL, instead of Ecto.Query.
I've found the direct opposite, writing composable queries in Ecto is an absolute pleasure, and its going to be even better with Ecto 2.0 with the easier use of pipes. Maybe ActiveRecords queries are intuitive to you because that is what you are used to?
There's elixirforum.com, which is growing steadily I think. It's not SUPER active yet, but certainly not dead. :-) I'm likewise interested in Elixir's applicability to bioinformatics, would be very interested in hearing of any findings! 
Wording is important to avoid confusion -- macros are just functions that run at a specific time, but in this case, it helps to distinguish between what the macro **expands** to and what it **returns**. I don't think it will return the AST when called, no -- not in the sense that `x = ControlFlow.unless(...)` will set `x` to be an ast data structure, but rather x will have the value of whatever `block` evaluates to. Ie, you're just doing normal quote(unquote(...)) inside a macro -- the macro expands in the place you've called it, before the program runs. This example can show why one might think an AST is being returned. iex(1)&gt; quote do "hey you" end "hey you" iex(2)&gt; quote do 2+2 end {:+, [context: Elixir, import: Kernel], [2, 2]} A quoted and a normal string literal have the same form, so your example usage doesn't tell you what's happening. Trying it with `2+2` returns 4, not the AST shown above: iex(5)&gt; ControlFlow.unless 2 == 5, do: 2 + 2 4
Atoms are represented as integers internally. When you first introduce an atom, it gets inserted into the atom table in the beam vm. When you spin up 1k processes each with `:ok`, they all have a reference to that same atom. This reference is the integer registered in the atom table. So, it's the atom table entries that are not gc'd. The actual atoms you pass around are just represented as integers and presumably copied. They aren't gc'd because there isn't a good reason to. They're meant to be used as named constants, in a sense. You should never take user-provided data and use that to generate atoms.
Thanks, I will come back to it and update if i find anything.
Thanks, I started watching it and it sounds very interesting. However I am doing genetics so this is a different story :). Will watch the whole things a bit later.
I know that quote sometimes return value, for string, int, etc. Now I am confuse, what does it return on my above example? Ast or value? Consider following sample: iex(3)&gt; quote do: if 5 == 5, do: "Hello Foo" {:if, [context: Elixir, import: Kernel], [{:==, [context: Elixir, import: Kernel], [5, 5]}, [do:"Hello Foo"]]} Thanks
That's pretty impressive, what test did you use for that?
What did you test here? Was there any dynamic stuff there? Or just serving the static page? Looks like you are comparing apples to oranges. 
https://en.wikipedia.org/wiki/ApacheBench
A few things: 1. The phoenix app was not run in prod mode, so this is benchmarking dev 2. If this is the stock phoenix app, it's doing crsf token generation, more expensive logging, serving static files, fetching the session, etc. Is the express app doing these things? 3. `ab` in my experience is not able to stress the a Phoenix project beyond 40% CPU usage, so its results aren't accurate. You should use `wrk` instead.
I'm a big fan of Elixir and Phoenix, but this is not a fair comparison. Node isn't being run in clustered mode, so it's only using a single core.
Comments, corrections - let me know! I'd like this to be the go-to article for explaining Behaviours to new people.
Searching for the string "foo": Returning boolean: `"foo" in Map.values(your_map)` Returning key: `{result, _} = Enum.find(your_map, fn {k, _} -&gt; k == "foo" end)` There might be a better way to do it, but that's one solution that works.
[Here's](https://github.com/mroth/phoenix-showdown) a more properly done benchmark (done on a prod-tier machine, using multiple node instances, not done at the same time as the other benchmark). Scroll down for the results.
Yes, that was my read too. So, for any non-OTP services, just create an abstraction layer in elixir that handles the communication with that service, and access the service in the rest of the system via that layer. No point adding an additional layer for OTP to OTP services. In terms of docker and container-isation, any benefits to using this in the OTP portion of the system or does it get in the way? I'm attracted by the simplicity of firing up containers via a simple interface in EC2/Google Cloud to manage a cluster, but my understanding of the way containerisation works is that my elixir nodes would be hidden from each other, and fighting it would be to not embracing the OTP way of doing things, not to mention I'd lose the benefits of hot code updates, and possibly other stuff that my research hasn't turned up. Is this an accurate assessment? If so, how do people manage their erlang/elixir clusters in Google cloud/AWS? Edit: deleted double post. New Reddit app...
You're right that OTP servers can be stateless. They absolutely can be designed to just not hold state internally and instead stick it all in a database or wherever you want it. But the fact that you respawn a stateful server with its intended initial state is not what makes it stateless. That is best described as a stateful server which - upon crashing - can be restarted with a known good state. I'm sure there's a way to design this to play nice with Docker, but I am not sure what that design is.
This is really useful thanks. In reality I think the OTP portion should be able to run stateless. As i say, I'm new to this so maybe getting my terms mixed up. Essentially I'm looking to create a backend for a mobile app, plus a web interface for the content management / admin. I plan to expose a rest interface for the mobile / web apps via phoenix, build most of the services with elixir / OTP, and for the FB use what's offered by AWS/Google cloud. With that in mind is docker/LING recommended for this kind of setup?
Ok thanks. Really useful feedback.
First question: is this for a fun project or for something important (i.e. work)? I'm going to answer the rest of this assuming its for something work-related or important. It's generally not efficient to start most projects using a microservice style architecture. The most common path is to build out an application as a monolith and then at some point start breaking up the monolith into services. Focusing on microservices at the start will slow you down and it is frequently difficult to know where to draw boundaries at the beginning. For more on this: http://martinfowler.com/bliki/MonolithFirst.html That said, if this is for something important, I think using docker containers is a good idea for these reasons: - For an important project, docker deployed services communicating over HTTP APIs is a great way to hedge your technological bets. Today you are sure that Erlang/Elixir is the best tool for the job, but in a year you may have a different view. If you use OTP for all inter-node communication, then what happens when you want a new component to be deployed/written in something that is unrelated to OTP? If your system components use primarily HTTP for inter-module communication, you can hedge against this pretty well. - Docker is a great way to isolate and codify your application's dependencies from the underlying operating system. Even if you only ever use Elixir/Erlang this is super useful for setting up reproducible dev/CI/prod pipelines. Docker compose will make it easy for you to string together a few Elixir/Erlang apps, a persistence layer that is environment-appropriate, and any other extra services your app relies on (want to drop in ElasticSearch? Some NLP service? etc.) and smoothly work in dev, CI and prod.
Someone in the twitter thread linked to the thread on [this gist](https://gist.github.com/wojtekmach/c54d8e25de4cccbcf6450087084f362c) which may have been the catalyst.
&gt; From Scott: You're a nice guy. I get that. But if that becomes your identity, then you wont be able to do the things that a community leader needs to do that can dirty our nice guy image. Then &gt; From Jose: Fuck you and your condescending tone. This is not OK. And this is not welcome. Well looks like Scott got what he wanted: no more mr. nice guy.
"Hey I'm just asking tough questions!" (See his Twitter bio) and/or "I'm just speaking truth to power!" is one way assholes justify their rude behavior. And judging from his inability to express himself concisely, this dude sounds like a narcissist too. I have no dog in this fight -- I don't use elixir anymore -- but his comments &amp;amp; tone irritate me. The thing is this guy knows he's being rude but he excuses himself by saying he's just "asking tough questions." Most adults, most reasonable people that have a normal brain, a normal set of social skills, a normal level of respect for their fellow people and professionals, would just stop having the conversation. But it says things about this guy that he just keeps pushing... FOR YEARS. Like keeps provoking Jose, keeps pestering him, for years, because it's way more important to him that he gets his opinion out to boost his personal brand. Annoying as hell. Can you imagine working with this guy?
Scott missed his calling, dude would've been perfect as a writer for **The Walking Dead** with that type of "taking out the trash" leadership mentality.
Scott Belware: &gt; Nonetheless, given a choice of damaging a relationship with a community celebrity and holding them to account in pursuit of a better outcome, I'll take the hit on my own personal brand knowing that suffering the one is not worth sacrificing the other. How can you type something like this with a straight face? What is wrong with this guy? What a windbag.
I think it's also good to be clear that this was not the first time. * [October 2015](https://twitter.com/apotonick/status/659966637236428800) * [Jan 2016](https://twitter.com/josevalim/status/692423263918645248) 
Mr. Genius? Exactly what has Bellware done but speak in vague confident opinion pieces? Anyone can go around saying X is too much like Y and I learned years ago *^points ^nose ^into ^the ^air* that was a bad architectural decision. As far as I can tell Scott has had success as a manager, so he probably doesn't even write code anymore and he probably thinks his processes are the best thign since sliced bread because he's been able to use them with success in a total of maybe 4-5 places, tops.
I like how he thinks he has a brand. As if anyone actually knows his name outside of his employer and coworkers. Talk about trumpeting up unwanted advice from a serious position of inexperience.
Can you explain in a little more detail when you say it's not an ORM? Is that because it's not mapping data into objects, but rather into models / structs? I guess I had always thought of Ecto as Elixir's ORM lib - so I'm wondering if there is something I should know about here.
using single core. and this comparison makes nonsense :) but I noticed it afterwards. https://tmux.github.io/ for windows
Thanks. Ah and yes, then its not a great comparison---multicore phoenix + as many native threads as the VM thought it could allow.
Right, because it's not an ORM... you can't expect to: 1. Change the state of the struct and be able to save it back as is. You have to use a changeset. 2. You can't simply mutate the data via functions available on the object---since there is no object. That adds indirection, which is what an ORM is supposed to help you avoid by mapping database transitions to your object and exposing a simple interface. By being simply a repository pattern, it wouldn't count as an ORM even if Elixir were an object oriented language.
Please everyone. There are disagreements—and I certainly don't agree with the viewpoints spewed by the person who allegedly pulled the last straw—but please don't contact this person. Either he is thinking about what he has done by now, or he will never realise the harm in it. Forming an angry mob will solve nothing, besides wasting time and feeding the troll—if he is one of the people who feed on this kind of drama you will only do him a service. So, just calm down. Many of us will sure miss José on Twitter, as some of us miss Stephen Fry and many others, but remember: he is not gone, and I am sure he will still be active and within reach on Github, Slack, Email, and IRC. Love.
Scott Bellware is Donald Trump with a mask
I don't know José's reason to quit twitter, but I do have a feeling that he is a busy man. This could just as well be a realisation that he want to spend time or focus on something else, like his family—imagine having the number of followers that he has and also have opinions; I could imagine that one could use all day on Twitter debating. I guess the choice of word «indefinitely» means that he could come back at some point. Never say never. He might be back next week, next year, or never. The thing about blocking people: I don't think it scales well when you have that kind of audience. Block one and two others will take over.
Scott's problem is not a lack of experience, but lack of humility, compromise, and nuance, all of which are just as deadly to design.
This is the absolute best policy with Scott. Few who know him IRL humor his negativity. On a technical level he can write good code—eventually. But the trolling that comes with it isn't worth it.
Only José knows exactly, but I think I remember him expressing something in the lines of that.
you can block people (for example, Scott has blocked me as a result of the lengthy argument we had about his negativity on that thread) but its trivial on twitter to harass someone around a block if they're determined to do so. 
Oh yeah? and what experience does he have leading an open source community or a language community? He isn't a core developer for any team I can find and his open source profile is non-existent according to the 5 minutes I spent googling his name. It appears he's done nothing other than jump a few jobs as a manager for developers, so he's probably used to acting like he understands programming more than he really does in order to impress the tier of middle management above him. I mean holy crap, just read his blog and how many words he's able to string together without saying a damn thing! http://blog.scottbellware.com/ The guy wreaks of middle management buzzwords and jargon. The only trigger word I can't find in the first couple of blogs he wrote was synergy, LOL edit2: god I've got to put down this quote from his blog &gt;Here's how to get a clear, honest look at software development productivity: The moment you're forced to divert to non-value-added work, write down the time. The moment you're back on track doing value-added work, write down that time. At the end of the day, add up all the non-value-added time and subtract it from your total work time. LOLOL so you mean all I have to do to measure productive time is write down time that isn't productive and subtract it from total time?!?! Holy shit, how revolutionary!
Hah. Yeah. Whoever he works for now has a known liability.
okay? So what's his experience...I'm extremely experienced, too, but not in running an open source project or core language design. Bill Gates could be vouching for this guy for all I care, but luckily this is a meritocracy where it's super easy to say put up or shut up...does this guy have relevant experience in the open source world? If so, show it, if not, well I've looked around a good bit and I'm fairly sure I already know the answer to this question. The beautiful thing about open source is that every can gather round and add their experience to benefit the community. If Scott had experience he could've been a value add (that seems to be a nice manager term he would understand). Instead he's done nothing but stir up shit and bring someone down who actually contributes a shit ton to a growing community and just about anyone would call him a value subtraction. I'll quit being long winded, because I think it's abundantly obvious Scott does nothing for any communities outside of his middle management duties that I'm sure have made him a lot of money. Luckily for elixir and open source communities you can have all the money you want and it doesn't give you a strong influence to stir shit around just because you're feeling bored/superior/w.e. scotts deal is.
Awesome thanks for this
Damn, the dude is a professional troll. Wrote a bunch of words, threw in some architecture buzzwords and then shit all over the place. Fuck that guy. I won't pay any attention to him if he can't produce anything valuable for the OSS community. Jose is amazing for creating Elixir and attending meetups/confs/podcasts all on his own. I started learning Erlang &amp; Elixir because I wanted to see a different approach to building systems than what C bases OOP languages often lead to. So far I'm having fun. Haters gonna hate. 
Ok sorry about it.
Thanks these are great. 
Great episode. Robotics are surely an area where tons of fun can be had with Elixir.
Wow, good work!
I have a hard time believing you tried all that hard in any of those languages...I can handle 5k connections on a pretty darn small ruby server... With that said, elixir is still a good choice. Outside of performance reasons I find elixir code (outside of macros which are generally discouraged) to be very explicit, expressive and still terse.
&gt; I have a hard time believing you tried all that hard in any of those languages...I can handle 5k connections on a pretty darn small ruby server... I, personally? No. I only wrote the Python implementation. The other spike tests were up to other people. Our basic usage was a websocket connection initializing per page. Since we hadn't built an SPA, that meant a connection would be open anytime someone clicked on any link. I'm not sure if the system would have been able easily handle 5000 open connections if it didn't have to burn CPU on teardowns/connection initializations many times per second, but in practice... it didn't handle it. Granted, I think my Python code *would* have worked *if* I could have used more than 1 CPU core, and if I had more time to put a keen eye to optimization but that wasn't the constraints I was dealing with. The Elixir implementation on the other hand actually worked, despite no one on the team knowing the language well and it being similarly constrained to a 1 CPU / 4GB RAM virtual machine. Any case, I'm not trying to put down Python or... any other language, it's just that when it comes to concurrency on the Erlang VM, you really are standing on the shoulders of 20+ years of work and that's something that I'm glad to be able to take advantage of.
I am glad someone found this helpful.
I'm prototyping creating a live media server (rtmp and hls + dash if it looks to be working well). I started learning Erlang for it because the BEAM VM seems perfect for what I need. No stop the world garbage collection combined with the well designed preemptive scheduler (required to keep the live streams in sync and with minimal latency), hot code reloading (so I don't have to disconnect long running connections to do an upgrade), built in fault tolerance and supervision structure (so one bug doesn't risk bringing all (or even one) connections down and brings things back up reliably), and the extremely simple process communication system plus OTP architecture (though honestly I only learned the true value of these after I put effort into learning Erlang). I created a simple IRC server to test my Erlang abilities after going through the Learn You Some Erlang book and it was a great experience. Then I found Elixir, and even though the tooling (IDE-wise) isn't nearly as good the syntax and wrappers are so much more pleasant to work with and easier to work with, so I don't have to worry about stupid things like the exact complicated format of supervisor child specifications, exact functions implemented by gen_server, application definitions, etc.... The mix tool is great and makes life a lot easier too. It just makes more sense imo.
Hi /u/SulfurousAsh, Yeah, I had written a lot of my arc code a while back. And then did the refactoring a few weeks ago. I just checked the github repo for Arc and see the changes you mentioned. Looks interesting and I will definitely want to check that out soon. There have certainly been some big changes for Ecto 2. I think a lot of projects are going to be updated to support it (mine included). &gt; I mostly have used a separate module because my original use case had multiple different models each with their own attached avatar, thus prompting a single place to keep that code. Interesting, in that case do you do different things in functions like store/2 depending on the type of scope?
Is anyone else having trouble with the new "with" statement not indenting correctly with alchemist? It should be something like: with name = "/etc/passwd", stat = File.stat!(name), owner = stat.uid, do: IO.puts "#{name} is owned by user ##{owner}" But instead it ends up flat like: with name = "/etc/passwd", stat = File.stat!(name), owner = stat.uid, do: IO.puts "#{name} is owned by user ##{owner}" And with alchemist not liking me to indent things on my own, this is a little annoying. 
I'm pretty sure he was being sarcastic about 'Mr. Genius', I would go have gone with 'Mr. Arrogant Prick' 
Yes. The responses so far have helped me think about maps in new ways, but you are correct this is re: a nested map. Sorry for not being explicit. I'm actually just trying to solve a challenge. It's something I've come across that seems like it'd be easy in OOP but my mind is having trouble breaking down the solution once a json is structured as a nested map https://www.reddit.com/r/dailyprogrammer/comments/3j3pvm/20150831_challenge_230_easy_json_treasure_hunt/?sort=confidence for the curious 
Great question What ruster_unsafe provides are unsafe bindings directly to the C NIF API. That means you need to deal with pointers, manual memory management and all other things you would expect in a normal C API. Rustler actually uses ruster_unsafe as a dependency, and provides safe abstractions that allow you to take full advantage of the safety characteristics of the Rust language. What this means in practice is that (unless there is a bug in Rustler) it should not be possible to crash the Erlang VM from your NIF code. If you do something bad in a Rust NIF, the worst you can do is cause a Rust panic, which will make the NIF return safely with an error. No segfaults.
Are you going to open source this or otherwise offer it for a fee? A live media server written in Elixir would be an amazing contribution to the community.
Why did you submit this twice?
Yeah I use postgres. I haven't needed to do any special caching because Elixir is _so fast!_
its a cross post to two subreddits, not two on one subreddit.
If I'm successful at implementing this I really really really want to open source it. The existing media servers out there (both open source and commercial) are not great for various reasons (imho). Unfortunately my company deals with live streaming and this product could lower the barriers for other companies to compete with us, so I will need to talk to my business partners and make sure everyone is ok with me doing that. However having that conversation with them is something I 100% want to have with them once this project starts moving away from POC towards an actual workable product.
The main thing that puts me off LFE is the documentation. I try to dip into the language every now and again but I've found the low quality or entirely absent documentation makes it a comparatively frustrating experience.
Elixir, mainly because I never liked lisp syntax. Lisp-likes are everywhere though on every VM, so if you're thinking you like Lisp then learn it well enough and you can use it on JVM, Erlang VM, and bare metal.
http://learnyousomeerlang.com/what-is-otp
If you wanted to know what all of those Erlang greybeards were afraid of at the Erlang Factory conference as you wandered around singing the praises of this ruby-like language for the Erlang VM, this is it: that such a question would ever get asked in the first place... 
Of course Phoenix is going to attract new programmers into the language. Nothing wrong with progression. 
I for sure prefer Lisp syntax to Ruby-like. However, Elixir has better momentum with lot of people coming from Ruby (that was one of its objectives) and Phoenix is a great RoR clone. For myself alone I would choose LFE. For a regular web-dev team of people more confident with mainstream languages (Ruby, Node, etc..) I would rather go with Elixir. Don't forget Erlang which is a great middle-ground between those two ;-)
Thank you. That means a lot. I have a few tasks on pipeline that will improve the tool a lot. If you have time to spare I would love to get some feedback on the codes as I am still fairly new with elixir. Also I would love some info on best practices to add tests to such a project. Thank you for trying out the tool and have a great week ahead.
I think you read the title of the post, but not the actual question.
Ah got it; thank you.
I think your idea can be expressed even more concise: def dedup([h,h|t]), do: dedup([h|t]) def dedup([h|t]), do: [h|dedup(t)] def dedup([]), do: [] 
In that case you would be blown away with the amount of interesting mechanism in syntax / type system in Haskell.
Be careful with `++` and `--`: [1,2,3,4] -- [1,2] -- [3,4] = [3, 4]
Sounds like any functional language could make you exciting.
What is thing? An ecto model? A generic struct ? Is it referenced in app.other schema? Maybe a simple pin operator (\^) could solve your problem, but given that I'm from my phone and I cannot understand completely the issue at hand I cannot help you further (for now) :(
As people are afraid of developers using garbage collectors without understanding how memory allocation works. As users of low-level languages are afraid of VMs abstracting away system calls, etc, etc. If that's the Erlang developer prerogative, are they ready for introducing courses about operating systems before they can teach others about Erlang? Don't be fooled, we are all blub developers to someone else.
I think you mean Perl ;)
Just in case you don't know already I'll give an overview of distributed elixir/erlang An BEAM runtime (app?) can have many processes running that communicate with each other via message passing. But they can also connect to other BEAM runtimes on the same machine or on different machines in the same network. What that means is that you can write several 'services' for example and connect them all together, and you don't need to set up any REST system to communicate between them. You can just use [rpc](http://erlang.org/doc/man/rpc.html). For me it is one of the coolest features - I can create 'microservices' without paying the HTTP/REST overhead.
Just listened to Joe Armstrong (one of the originators of Erlang) suggest that Erlang is the only proper object oriented language. I guess Elixir has an even better claim to that. 
because you don't manage the sockets with ruby, you use redis or something else suited for that problem and pakyow deals with the concurrent communications using primitives from concurrent ruby which are actually really good given it's still a ruby runtime. Like seriously, though....5k connections are almost nothing. I have no idea what kind of business model would be able to drag 5k people to the same place at the same time and not afford a VPS with a few gigs of ram.
You could just construct it manually: iex(1)&gt; quote do: @foo 1 {:@, [context: Elixir, import: Kernel], [{:foo, [], [1]}]} So you can just copy the AST and put your values in iex(2)&gt; Macro.to_string {:@, [context: Elixir, import: Kernel], [{:foo, [], [1]}]} "@foo(1)"
I really can't tell what you're trying to do with this package. 
It extracts the primary readable content of a web page. it finds ads, navs, header and footer and eliminates those. It can help you to find title, author, main images and the article user really want to read.
Keep in mind that this isn't strictly equivalent. @foo :bar expands to `Module.put_attribute(__MODULE__, :foo, :bar)` in the module body which is then run when the module body is evaluated. The version where you call `Module.put_attribute` inside the macro itself means that the attribute is set on the module earlier, during the macro expansion phase. Macro expansion happens before function call evaluation in a module body. You would need to do defmacro on_error(error_handler) do quote do Module.put_attribute(unquote(__CALLER__.module), :error_handler, error_handler) end end
Take a look at this project https://github.com/bigardone/phoenix-battleship Uses React in the frontend, but the backend is using Phoenix 
Thanks! I'm pretty new to elixir stuff but this looks exactly like what I was looking for.
Are you talking about a backend for the Lambda, or the Lambda itself? AWS Lambdas only support JS, Java and JVM based languages, and Python, so not sure what you're trying to accomplish in Elixir...
Redis is managing the sockets, not emulating the socket. They don't have direct access to redis, sorry if I was being unclear. Redis serves as a pubsub, managing the sockets and information of the client connecting to them. This is a fairly typical setup for many websocket setups in rubyland, including actioncable.
It need not run on Lambda. That's just how Amazon is getting people up to speed quickly. The skill can be any http webservice which sends and receives the proper JSON. When you configure a new skill, you can specify your server's endpoint.
Thanks for the answer! I decided to try my hand at writing a custom type for Ecto and using the ltree extension from Postgres.
Nice. Had to look up ltree, looks pretty cool for the application. Let us know how it goes, and share some code if you can =)
my mistake! that's cool to know, I wonder what OP wants to know then; elixir+phoenix api server should be ez pz
Looks like the author did put some effort into making the exercises at least somewhat resemble koans. There have been several "programming koans" published lately that did not. That's why I am compelled to push this reminder: &gt; [If this is the case, we should appeal to the author's sense of decency. The concept of 'koan' is far too unique and beautiful to be watered down.](https://news.ycombinator.com/item?id=9798177)
Here is a gist with some example code. https://gist.github.com/amokan/57a64fc5d96e6ed5b9d1156e0afa72ee Be sure to note that I simplified it a lot and you'll need your own scraping/crawling module as well as parsing. Note that the code has comments where you'll need to implement logic. As far as your HTTP timeout goes, with my PhantomJS solution - I set the global timeout in Phantom via a js file that I load when that Port kicks off. As the other person on this thread states, you can do something as a `handle_cast` to avoid the timeout - but for this scenario, I need to know which process to notify when I finished so I do some tricks and do a `handle_call`, store the `from` in state and return a `{:noreply, state}`. When my crawl is done or times out on the phantom side, I trap for a `handle_info` and then do a `GenServer.reply` with the pid I stored in state from that initial request. Hope that makes sense. I've seen a number of people using this pattern for potentially long-running GenServer tasks, so its not unique to this scenario. I'm sure my Elixir could be tightened up or improved, but maybe this will help.
Thank you for your feedback! We'll put in some effort over the coming days to make the koans more story-like. Now that we have a general structure in place, making it more koan-like should be easier.
&gt; I have heard that Elixir does all this concurrency for you so you don't need to "spin" up multiple instances of your app. That's partially true right? I will only need one app running per physical server? Yeah, basically, the BEAM virtual machine (erlang's VM) has these tiny processes, which are different from the OS processes, and these tiny processes are independent of each other by default, and each are garbage collected separately. In Phoenix, the web framework built by Chris McCord, there is something called the Supervisor, and basically all that does is makes sure that the processes that need to be running, are in fact running. So for example, if i have a processes in my supervision tree interfacing with different services, such as Redis, MySQL, or even an outside service such as an API, i can have my supervisor look to see if each is running. If one dies, the supervisor is alerted and will spin up a new, identical processes immediately. Tl;dr Elixir is cool
Clutch
I am interested too!!
I am a newbie who would love to do some supervised grunt work on this project. I got the syntax down and completed advent of code with elixir, but I still didn't do any actual projects yet, but I would love to help! 
I might have a neat idea that I was toying with while thinking of an example: With inspiration from: https://www.youtube.com/watch?v=xoNRtWl4fZU What a about a simple image processing server that applies different filters (grayscale, blur, etc.) to images that you link to or upload. You could eventually hook this up to a html5 form or whatever for uploading but could also just focus on the processing part for the mean time. So the goal is that you as a user could upload a number of images in a 'batch' and these would get processed by a few workers. When the work is finished, you could store the result somewhere or could maybe send an email to the user (Mailgun and an elixir mailgun (https://github.com/chrismccord/mailgun) clients are available). So you would require something like a master that handles the requests puts these in a buffer somewhere and use process pooling to process these images. Some considerations: - For workers look into https://github.com/devinus/poolboy and https://github.com/thestonefox/elixir_poolboy_example - ImageMagick or something of the sort could be used to do the image processing see: https://github.com/route/mogrify - The server could be simple GenServer that is started through mix and communication is done via a socket or http, or you could skip the entire genserver part and just make the program callable through a command line. Or make a seperate command line client, that makes a call to the GenServer using different nodes. - Maybe you should have a way or handling errors, maybe the workers could log their results to a separate Logger (which is already a GenSever I think?) or Custom error handler? - What will you do with the request when the server crashes, do you know what images to process? - An extension would be to present the results in a web page (maybe serve these using Plug and Cowboy) - Another extension would be to be able to download images from an url resource The nice OTP part would be to be able to scale the service so that in can process multiple requests from multiple users. Something that you can achieve with a simple GenServer for processing the requests and multiple workers, I think. Now don't get me wrong, this is not a very useful service, but I thought it would be neat way to learn the platform :) 
Ooh, nice. How are you doing it?
We've just finished a proof of concept voice based banking application using Alexa skills and used Phoenix - what an awesome framework. There's going to be a video soon showing how we built it and a smart mirror to render some user feedback. Take a look at this https://hex.pm/packages/alexa
This is a really common issue in SQL, and there are quite a few ways to encode tree-like data in SQL so that ancestors or children can be queried efficiently. See the original Joe Celko articles about Nested Data from the late 90s/early 00's for an easy-to-read-and-visualize example; portable SQL is totally capable of querying nested data efficiently (there are more tradeoffs in speed when both updates and queries of nested data need to be fast). Usually there's a library in your target language that provides an implementation of Nested Sets or Materialized Path that handles edge cases. In this case, it looks like someone is building this into a library already for Elixir, though they caution that it may not be ready for production yet: https://hex.pm/packages/nested_set
yeah, i see what your saying, and its a fair point. Its just that we can learn so much from doing this project, and also, we can take advantage of the distributed nature of BEAM
Im new at this too
You mean like a scikit-learn equivalent? I am actually planning on working on a Naive Bayes classifier implementation. After that, we can move on to Decision Trees, ANNs, and others. It also seems reasonable to write a separate library for some basic statistics similar to https://github.com/simple-statistics/simple-statistics, since most classifiers rely on some commonalities like distribution functions, mean, z-score, etc. I am also planning to write this. I'm not sure Elixir/Erlang is a good fit for number crunching, but writing an ML library can be fun and full of learning.
Yep this is exactly how action cable works and Ruby being the one holding the socket open was what I was referring to not being able to handle more than 250 connections (at least in my MacBook Pro). After hitting that limit I saw all kinds of irrational errors such as 404 no such route /cable, something about this thread thought it owned the database connection but turns out that wasn't true some other thread owned it and a few others I can't recall at the moment. However this thread has digressed so I won't be responding anymore in this topic.
People have made machine learning libraries in languages as slow at computation as Elixir before.
Don't know why that benchmark is poor for Phoenix, but here's one showing nice perf: https://github.com/mroth/phoenix-showdown
It's a fair question. Chris McCord mentioned that they set up the test really poorly and weren't given a chance to address it. Here is the [comment](https://www.reddit.com/r/elixir/comments/48ke69/any_reason_why_elixirphoenix_did_so_badly_in/d0ko6ai): &gt; We don't know what caused the errors and unfortunately we didn't have a chance to collaborate with them on a true run. A few months ago they added Phoenix in a preview, but it was a very poor implementation. They were testing JSON benchmarks through the :browser pipeline, complete with crsf token generation. They had a dev DB pool size of 10, where other frameworks were given of pool size of 100. And they also had heavy IO logging, where other frameworks did no logging. We sent a PR to address these issues, and I was hoping to see true results in the latest runs, but no preview was provided this time and we weren't able to work with them on the errors. tldr; these results are not representative of the framework.
Thats pretty cool, looking forward for the videos and other tags :)
Thanks. Contributions are welcome :)
&gt; Read on for a tale of technical subtlety and engineering that culminates in the in-memory rendering of the Template of Doom, a 30 gigabyte monster from the bottom of the sea. By the end of this post, I promise you won’t look at any web server the same way. I'm not usually one to be swayed by fancy intros, but damn - this was effective.
[This is a *REALLY* good overview](http://www.slideshare.net/billkarwin/models-for-hierarchical-data) of the four standard hierarchical data models with detailed queries of each that I recommend you go through. It's all SQL up to slide 69, and after that it's PHP implementations of those models, so you can ignore the rest. Those slides should at least help you model your data structure appropriately in SQL, but as far as translating that to Ecto queries, I cant really help you as I'm just learning Elixir myself.
Try :"gsx$name", this is an atom. Atoms in Elixir start with :. They can also be wrapped in quotes when they contain "tricky" characters. Perhaps you are confusing Elixir atoms with Erlang ones? Let me know if it works :)
Thanks for the write-up. Was very interesting, indeed!
As I programmed mostly in Erlang for past couple of years I may now suffer blub paradox myself. Can you give me one or two examples where multiple dispatch can't be easily replaced with some type of composition? Like in this question for example: http://stackoverflow.com/questions/25129110/handling-records-with-shared-substructure-in-elm#25309783
NICE! I missed that somehow in the changelog. I'm probably going to be putting everything under `lib` if possible.
The reasoning from the [guide](http://www.phoenixframework.org/docs/adding-pages) still makes sense: &gt; The distinction between web and lib provides a convention for the different ways that we handle state inside of our application. The 'web' directory contains anything whose state lasts for the duration of a web request. The lib directory contains both shared modules and anything that needs to manage state outside of the duration of a web request.
We're exploring directory changes now that lib code is being reloaded, but now it's safe to say that anything non-phoenix/web related - controllers, views, channels, can go in lib.
Elixir has access to all Erlangs tools. Find a project and start contributing, because this is a mature ecosystem you're not really going to find missing gaps for common tools, just bugs and feature requests.
I disagree with the other comment. There is tons of work to be done essentially porting various Ruby libraries over to elixir. It's sometimes a bit tedious but IMO adds the the attractiveness of the ecosystem as a whole. There is another side of the ecosystem porting Erlang libraries to elixir, but you probably need more than a passing understanding of otp and Erlang to start doing that.
Wouldn't wrapping Erlang libraries in Elixir be better? You get the syntax with no additional code that can have bugs. Only downside I can see might be stack traces or generally errors.
One library I found missing from the Elixir ecosystem was something like [Mechanize](https://github.com/sparklemotion/mechanize) for acting like a browser client, saving your cookies, interacting with forms, etc.
I'm getting my bachelors degree in physics soon and I love elixir. How can I help?
Kinda. Though I've noticed that the community would rather reimplement things than wrap an erlang library. Example being ExAws vs Erlcloud. Or Conquerer vs Poolboy. I think this is because unlike say wrapping a C library in Python or other, you don't really buy yourself any speed gains. You just save time on implementing the library. If the library isn't very complex, you might as well re-write it. On the other hand, lots of Erlang stuff does exist for complex functionality but its not really documented online, and the API docs are pretty anemic leading one to just want to duplicate the code rather than figure out what the erlang is doing.
Unicode character support is a bit lacking. A lot of other languages provide convenient wrapper functions over the full unicode database so that you can do things like check if a character is alphanumeric, i.e. Char.alphanumeric?(c). I recently ran into this limitation when trying to strip all the non-alphanumeric characters from a string, which was dead slow using an `[^[:alnum:]]` unicode regex. This problem isn't insurmountable though, and I might help implement it when I get some time. Here's the equivalent function in Rust's standard library: http://doc.rust-lang.org/std/primitive.char.html#method.is_alphanumeric and the generated table that backs these functions: https://github.com/rust-lang/rust/blob/master/src/librustc_unicode/tables.rs
These changes are so necessary. I have struggled with error handling with the `with` special form and I really dislike Timex. Having date and time in the stdlib is going to be a huge improvements.
Probably have to see how it's done in erlang and wrap that I guess
I found Timex to be exceedingly slow, and have replaced it with Erlang calendar math which gave exponential speed increases. To note, this is not doing timezone specific tasks where Timex would need to call the tz data process- this is simply comparing date structs. It seems Timex made process calls for almost everything which is probably the reason for its speed problem. Super happy the STD library isn't implementing time zone process, but rather seems to be wrapping Erlang date time functions with convenient elixir syntax and structs.
Nothing really, this still uses four strucs. it just organizes the creations of them.
I am interested in this as well, as I've contributed on the Hex package [Math](https://hex.pm/packages/math) in the past, as well as writing my own rational number implementation [Ratio](https://hex.pm/packages/ratio).
The common opinion right now is that 'you should not be afraid to call Erlang code from Elixir', which means that there are many Erlang libraries that you should just call directly. Most Erlang packages that have been wrapped, were wrapped because they had weird parameter orders which made them hard to pipe. However, some of these are quite old and Erlang documentation is *all over the place*, which makes it very hard for new users to get into. So my personal opinion would be that: Yes, wrapping Erlang libraries is a good thing.
As for porting Ruby gems: I'd love to see an implementation of [Ancestry](https://github.com/stefankroes/ancestry) in Elixir. Ancestry is a gem that allows storing tree-structures in a RDBS by storing the path back to the root node in a string field, allowing for *O(1)* (single-query) lookup of ancestors, children, etc.
Im thinking about a port of Plutus... I already have a port of apartment, and both of these things are kinda in my field (fintech saas)
Have you checked out [math](https://github.com/folz/math) and [statistics](https://hex.pm/packages/statistics)? What specifically do you think are missing? Do you mean numpy/scipy like libraries?
The thing about not having a way to inspect individual characters is true, yeah. Apart from that I think the unicode coverage is very good, and I'll bet there's compile-time metaprogramming like you say. I suspect unicode character support is missing from Elixir because it's missing from Erlang too. In the Erlang docs on unicode support they argue that lowercasing, uppercasing and similar options are very difficult to get right. This is a valid argument and it might be better if unicode character support lands in a separate Elixir package. http://erlang.org/doc/apps/stdlib/unicode_usage.html Two of the examples they give as being tricky are uppercasing `ß` and lowercasing `Σ`. Rust gets the first one right, and makes the pragmatic choice to dodge the context-sensitivity in the second case. https://play.rust-lang.org/?gist=4b84dc84a8d8aa4b51dc268f5d5a09bd&amp;version=stable&amp;backtrace=0 Haskell just returns the same character in the first case (arguably wrong), and does the same thing as Rust in the second. Try: putStrLn [toUpper 'ß'] putStrLn [toLower 'Σ'] On https://tryhaskell.org/ Edit: formatting
&gt; when trying to strip all the non-alphanumeric characters from a string, which was dead slow using an [\^[:alnum:]] unicode regex What's the actual code you used? I wonder if it would go faster if you used elixir pattern matching just to extract one unicode character at a time and then use [:alnum:] to test it (or some other single-character test) and accumulate the valid values (via something like map_reduce) into a new string recursively (hopefully taking advantage of TCO)
Actually, because lowercasing/uppercasing are difficult to get right that is why they need to be in the standard library rather than allowing a proliferation of implementations. What is difficult to implement and discern should be in the standard. What is easy to implement or opine over should be outside the standard. ---- As an example. Elixir appears to get the 'tricky' cases correct. iex(1)&gt; String.upcase("ß") "SS" iex(2)&gt; String.downcase("Σ") "σ" Edit: formatting
Here's the actual code: https://github.com/michaelsproul/elixir-word-freq/blob/master/Words.ex In the `load!` version of the function I'm using a hacky single character check that works for `æ` and A-Z, prompted by the presence of *Phædrus* in the text I was analysing (see `data/Zen.txt`). I'll try to optimise an actual solution along the lines of what you're saying when I get time.
Single query != O(1)
From my spot tests, even simple regex is about 4x slower in Elixir than in say Python. I'm not sure why this is true, since they're both using PCRE.
Just some superficial criticisms: Your word_freq function is not tailcall-optimized. In order to make it so (and it's possible to do), the recursive call has to be the last thing the function does. Your "alphabetic?" and "numeric?" functions could either take binary arguments via &lt;&lt;argument::binary&gt;&gt; (which is probably faster) or could be macro'd out somehow to define it for every possible valid character (which is pretty much how Elixir itself does UTF-8, since the Erlang argument pattern matcher is extremely well optimized) 
I'm going to see if I can use my github account to suggest changes to this that are more performant, because I have a bit of free time now
[Created a pull request here](https://github.com/michaelsproul/elixir-word-freq/pull/1) with the initial changes I propose. Didn't get to test whether it is more performant, but the style I used usually is... want to look at the regex problem next. for single file code I tend to use that technique at the bottom for easy test running or benchmarking of that file
I've noticed the same thing. I started using phoenix just yesterday, and I had a strangely hard time looking for documentation on `link_to`
Yup, built right into Elixir! [Supervisor Documentation](http://elixir-lang.org/docs/stable/elixir/Supervisor.html)
If you want something performant, you may need to hand code a solution. It seems there's no String.alphanumeric? sort of function yet (which if written correctly would be much faster than Regex), so the Regex solution may be the only sensible one at this time (it itself could probably be optimized somehow...) When I mentioned this problem on the Slack elixir channel, someone created a bug report/feature request to add String.alphabetic? and String.numeric? to the String module. Elixir itself uses pattern matching (and not Regex) to work with Unicode, so maybe its source code would also potentially be helpful here.
I also discovered that if you use this as your regular expression, it runs 10x faster (14 secs vs. 140 secs): punctuation = ~r/[[:punct:]]/u And if you take off the "u" (unicode) switch, it runs in... under a second :O I'm not entirely sure why all that is, though. Actually considering digging into some source to see what the heck is going on :O I pushed my most recent edits to the pull request on your github project.
This reminds me a whole lot more of prolog than erlang. 
Oh I didn't mean to denigrate their work (and I sure hope I didn't). It's simply that when people talk about the Elixir ecosystem... they usually leave out Erlang completely due to: 1. Erlang modules tend not to have online documentation, or websites backing them. Example: Poolboy, widely used but the only documentation is the github readme that doesn't go over all the features. Lot of other erlang modules are the same way; requiring you to download it to get its docs via the erlang documentation system. 2. Erlang modules have this trait of using unit tests as examples, instead of showing examples in documentation. I guess its a philosophical thing. 3. Even if all of the above is done, an Erlang module won't (a) take advantage of Elixir meta-programming features or (b) use Elixir's types. Structs are extremely valuable as are Exceptions. Erlang is my goto for OTP support (workers, pooling/etc.), and networking (e.g. cowboy) or math just because Elixir doesn't really add much in the way of type support for math. Adding onto what you say here... &gt; Elixir's a special snowflake in that it doesn't attempt to leverage the average developer's familiarity with its VM, like JVM languages do. It's instead leveraging the language's features to evangelize the VM and ecosystem. I think this may be a matter of perspective. If you're not previously an Erlang (or Java developer) can't be attracted by a language leveraging familiarity with the VM. That said, Elixir's semantics make calling in/out of Erlang seamless which is something I've never seen from a JVM language.
I've never used prolog so I'll take your word for it. Since erlang is heavily based on prolog syntax I guess it makes sense :)
Our deployment process at work is complete shit, so I made a wrapper around it that kicks off the deploy in parallel to each box and spins up a web interface to monitor the state of the deploy. It also can integrate with our HipChat queue and merge my PR's and all that jazz for me I also really like the idea of bots like homu which integrate with Github. We wanted to have something that would automatically assign revieiwers to PR's, so I'm building a bot that pulls down the comments, parses callouts/commands, maintains a sync state so that it doesn't rerun commands, and processes commands (e.g. calls out reviewers). Both have been just about the right amount of code to do something interesting (~300-600 lines)
*Cheer!* We reinvented the wheel! *YAAAAAAAHHHHHHHH* ...
Can the elixir version run with a #! Line like ruby? 
&gt; It might be because moving the filter logic into a guard means it runs in the macro space, exposing it to compile-time optimizations. Again, this is a cool facility of Erlang/Elixir that should be used- It's been heavily optimized, and IMHO results in cooler code. Now you also know why I avoid case statements. Not only very procedural, but slow. Can you explain this a bit more? Or link to somewhere there does?
So Erlang invented pattern matching? (: https://youtu.be/eVgEkJGM_wM?t=1439
It's a bit hard to describe but it's a style often used in functional programming, and Erlang's BEAM VM is highly optimized to run fastest when you use that style. Basically, you move as much of what you would have formerly called "procedural logic" (if-thens, case statements, etc.) into your function heads and their guards, and the BEAM VM will "naturally" run your code faster, because it will pattern-match on them (which also, neatly, eliminates a lot of that logic from your code, which means... fewer bugs!) If you look at my latest pull request for [Words.ex](https://github.com/pmarreck/elixir-word-freq/blob/patch-1/Words.ex) and compare it to the original code (I left it commented out in there), you can hopefully see what I'm referring to. A minor but significant difference. Instead of defining a function once and determining its behavior via logic inside the function, you define a version of the function *given these arguments which fall within these constraints* and do the specific behavior there. The VM will try each function head until it finds one which matches, or hits a catch-all (usually denoted with an underscore i.e. ignored argument). That way you can define a function given all possible inputs, and it runs *really fast.* Docs about guards are contained [here](http://elixir-lang.org/getting-started/case-cond-and-if.html) (although I don't think that's the best treatment of them... I learned a lot from Dave Thomas' Elixir book on this stuff). Basically, only certain comparisons can be made in guards. You can define your own via macros, however. Macros are run in a precompilation step, explaining how that works is best left to other people, not me :O
[Here's another writeup of pattern matching and guards](https://github.com/cromwellryan/sweetelixir/blob/master/advanced/pattern_matching.md) Once you get used to it, you won't want to go without it! (at least in my case)
[Virding's First Rule of Programming](http://rvirding.blogspot.com/2008/01/virdings-first-rule-of-programming.html) "Any sufficiently complicated concurrent program in another language contains an ad hoc informally-specified bug-ridden slow implementation of half of Erlang." You'll be seeing the relevance of this a lot in the coming years. ;)
&gt;and also I don't want to test this by passing "a or b" to the test and asserting that the answer is either "a" or "b", because that sounds kind of brittle to me. I think this is exactly what you should be doing. Why would you want a deterministic way to test that the output of your function is random? You would have to actually break your implementation in order to satisfy that test. To be clear you should have a group of allowed responses, not a bunch of conditionals for each new response you want to add.
Protocols are definitely one form of polymorphism. It is based on Clojure protocols and there is a good description here: http://clojure.org/reference/protocols There is also a more exploratory definition of polymorphism here: https://wiki.haskell.org/Polymorphism Protocols would definitely solve the problem above as you could write: `Getter.get(obj, key)`. Maybe you are hinting at the fact that Elixir and Erlang requires all calls to be qualified (although you could get away by importing them). This is however a namespace issue and it is not related to the polymorphism. On the other hand, languages like Julia have multimethods which can be implemented for any namespace without the ceremony of protocols / type classes. But again this is a namespace issue and not a polymorphism one. They are all polymorphic with different constraints. Edit: both Elixir and Erlang also provide polymorphism on function clauses but that's even less dynamic than protocols so I did not mention it the first time around.
Since the question dictates all of the possible answers, couldn't you could do something like this: * In the test pass it "vim or emacs?" * Assert that the bot response is one of ["vim", "emacs"], or whatever the other possible answers are. 
If you want something faster, consider using Calendar (https://github.com/lau/calendar). It will be using the structs in Elixir once 1.3 is released. See these benchmarks: https://github.com/benwilson512/time-bench The standard library will mainly have the structs and just a limited set of functionality.
 from u in "users", join: a in "activities", on: a.user_id == u.id, where: a.start_at &gt; type(^start_at, Ecto.DateTime) and a.end_at &lt; type(^end_at, Ecto.DateTime), group_by: a.user_id, select: %{user_id: a.user_id, interval: a.start_at - a.end_at, count: count(u.id)} Sometimes I look at Ecto and think "what's not to love? It's SQL with compile time guarantees, and your own languages data-model and macro ability!" Other times, I think "It's SQL. I don't want to write SQL, no matter the guarantees". Ecto, especially 2.0, is pretty great but I do think that eventually frameworks like Phoenix and others will want to implement their own data layer so you're not just left writing SQL with compile-time guarantees to query data. On the other hand, with composable queries you can go a pretty long way to defining your own data layer with some functions. Users.get_with(Activities), where Activities.time_between: (start_at, end_at) # ==&gt; {User, Activity} It'd just be nice to have it in some sort of 'standardized' conventional way.
I recently learned the basics of elixir for a new job. They gave me a simple coding test to implement in Elixir to see how I liked the language: https://bitbucket.org/mikebuhot/creditcard_test/ Just building this simple app I learned about Mix, EScript, ExUnit, capture_io, doctest, dialyzer, OptionParser, Stream, Enum and pattern matching. I based the structure of the app on the CLI example app in the Programming Elixir book (https://pragprog.com/book/elixir/programming-elixir). Now I'm trying to write a simple REST API in Plug and Ecto before taking on Phoenix proper.
Maybe I'm being nitpicky, but I think that leaves open the possibility of a test that fails only sometimes, and that's something I don't want in a test suite. If I hardcode a third value, the test would be green 66% of the time.
It's worth noting that the function can be broken and still pass that test. def random do "emacs" end Always passes the following test: test "random" do assert random() in ["emacs", "vim"] end I would probably run the function a bunch of times and see whether I see all values (please tell me if this is stupid): test "random" do results = for _ &lt;- 1..100, do: random() |&gt; Enum.uniq # Easy comparison without depending on order by using a set |&gt; Enum.into(%MapSet{}) assert results == MapSet.new(["emacs", "vim"]) end
When it comes down too it I think you're really just testing the stdlib's random function when really you're only concerned with your function. I would want to know that my function could accept a group of something and give me back something in that group - I wouldn't write a super complicated test that no matter what would fail eventually (because it's random, that's what random is supposed to do). If you give it a group of something, run it through random and get back the same thing each time - that's the stdlib's random functions fault - not your code, hence you aren't responsible and _shouldn't_ test that code. If it gets to the point that you don't trust your stdlib is enough for the task at hand, pick a new language for that projects needs. Just my two cents, if it's worth even that much :p
I think that you should get comfortable with the language, and learn the ins and outs of Elixir and OTP, such as how to metaprogram in Elixir before you move on to Phoenix. I made the mistake of not doing that, and I had a hard time picking up Phoenix.
I do not agree, protocols are not polymorphism. When you say X have featue Y it usually means that this feature is accessible and easy to use. Protocols in Elixir is more like special feature that shoudl be used carefully and sparsely, it's considered more like a **special case** than default practice.
If you want an excuse to learn elixir and phoenix, then go for it. 5$ droplet with pheonix will easily handle 1000 visits a day. One of the first things I do with a new language or framework is building something like a blogging engine in it. But if all you want is a blogging platform, just use github pages with a static site generator (e.g. Jekyll) 
Phoenix will use an order of magnitude less memory. A stock Phoenix application uses 15mb of memory and folks are getting 500 reqs/sec through little raspi's fwiw
Raspberry Pi devices.
I'd say it depends on which one you want to learn more. Obviously, you don't need to write your own blogging software, so building a blog is primarily a learning experience. Rails or Phoenix will both be able to easily handle the limited traffic. Just pick which language/framework you are more excited about to build the blog with.
They all perform fine. You shouldn't be picking the web framework based on performance. The web server is rarely the bottle neck for a web app, its almost always the database. And even if it is the framework, you can easily remedy it with a load balancer or caching layers.
Compositional query construction sounds really cool. I can imagine building up a library of predicates and aggregations that can be pieced together without repetition. Do ecto migrations support defining views? I think just throwing a rankings view in the DB would be a good fit here, since the structure of the query doesn't change. 
As simple as git deploys on heroku are, IMO building exrm releases is just as easy to get running on on droplet. 
How are you determining the response format? If you are including a `format` param then it would be best to simply match that in the function signature and create two separate functions. def show(conn, %{"id" =&gt; id, "format" =&gt; "html"} = params) do # html render end def show(conn, %{"id" =&gt; id, "format" =&gt; "json") = params) do # json render end A general rule of thumb I have is to try and avoid `if/else` statements as much as I can. Not to the point of where it doesn't make sense, but if the conditional branching can be represented as separate functions then I head in that direction. All this being said, it might actually be best for you to capture this in the router as separate pipelines. Then delegate the actions to their own controllers rather than keep everything in one.
OPs link no longer works, updated link: http://geoffreylessel.com/2016/f1-standings-with-ecto/
*Well there are industries where the application server is the bottleneck and it can't be solved by load balancing because of latency concerns.* That is valid, but I think the reality is the vast majority of people never reach the scale that Phoenix/Elixir/Erlang allow for. Having said that, I love what Elixir brings to the table and use it whenever I can and it fits the problem I'm trying to solve. So, I'd agree with Infintie_3ntropy and say it shouldn't be your main focal point on a decision to use or not use a framework.
On further thought, my comment was more about the location/city as opposed to the venue itself. Also, let me preface by saying, this is just my own opinion--I'm not trying to belittle anyone to whom this venue/location is appealing. Anyway, here is a short list of my general sentiments: * Florida is pretty remote. It's far from both the west coast and NE tech scenes/general population centers. The average programmer is much more likely to have friends/family in a place like LA, NY, Boston, SF, etc. Moreover, Orlando is also somewhat remote within Florida itself. * Outside of theme parks, there is nothing of interest in Orlando. I can see the draw if you have a family and kids; however, I do not. * It's in August, so the weather is going to be fairly intolerable--had it been scheduled in a winter month, it would have been much more appealing. * The venue is at Disney World, so I will undoubtedly be thrust into throngs of vacationing families. **tldr**; Beside the conference and theme parks, there is virtually no reason to go to Orlando in August. Moreover, it's really far away from everything.
If I were to write a plug, then I'd just do one in lib/plugs/ ? Is that correct?
Yep, I'm using format param. My router has 2 pipelines, both for browser and api as well. I'm going to try your suggestion on separating it on another controller.
Many thanks for your response. In some cases in my app, they might be the same, but not always. On the other controllers, they might not have any html at all, just API.
you will?
As far as I know, you should write plugs in your web/ folder. You can create folders like web/auth/ and put it there.
&gt; Given enough test runs you will have a failure though 1/2^100 ≈ 7.8e-31 that requires a loooot of runs. The probability of cosmic rays corrupting data isn't as low as you think, btw: https://en.wikipedia.org/wiki/Cosmic_ray#Effect_on_electronics &gt; just trusting your stdlib. With your testing approach you need to trust that the stdlib works AND that no one changes the method to always return the same result.
&gt;that requires a loooot of runs No, it requires exactly one run, minimum. &gt;isn't as low as you think Not sure how you managed to surmise my understanding of cosmic rays as being on the low side when I'm the one who brought it up, but maybe you're making the point that your tests coverage works during runtime to prove the absence of cosmic rays xD
Hey I am working through this right now, just thought you should know there is a minor typo in the code. You are using the cell {1, 0} twice in the active_cells list. I assume you meant one cell to be {0, 1}. # test/game_of_life/cell_test.exs test "alive cell with more than 3 neighbours dies" do alive_cell = {1, 1} alive_cells = [alive_cell, {0, 0}, {1, 0}, {2, 0}, {1, 0}] refute GameOfLife.Cell.keep_alive?(alive_cells, alive_cell) end
You should get comfortable with Elixir first. Otherwise you won't understand some of the key paradigm differences between Phoenix and Ye Average Web Framework, e.g. "Phoenix is just another OTP app".
In your tests you're using the receive block and then placing an assertion inside that. It's more idiomatic to use `assert_received`. You get better error messages that way too.
Thanks! 
Elixir doesn't have objects or methods, instead we regular old functions with various data structures. If you wish to have a function manipulate a data structure you must pass in that structure as an argument, like so. defmodule MyApp.MyModel do use MyApp.Web, :model import Ecto.Query def my_func(model) do do_something_to(model) end end Then you can use this function like so model = MyRepo.one!(MyApp.MyModel) # Get the record MyApp.MyModel.my_func(model) 
Note this will iterate through the list each time.
&gt; def my_func(model) do what's the "model" here, is it MyModel (module or class or whatever) or an instance of it? By instance I mean something returned by MyRepo.one!()
It's neither. Modules are just a container for functions, they are not classes or objects. They cannot be instantiated. Instead that function returns a data structure called a struct that contains all the information from that row in the database. It's basically a map with some extra niceness. Before continuing with more advanced things like Phoenix and Ecto, I recommend spending some time learning Elixir, and how as an objectless functional language it differs from an OO programming language. The official "Getting Started" guide is very good. http://elixir-lang.org/getting-started/introduction.html
That is basically correct, elixir will expand the `in` guard to be a bunch of `or ==` guards. That's why your list has to be compile time resolvable to use it in an `in` guard.
Yes, so it will iterate over each element thanks to the chain of `or`s. If you have a case for each one it will use a more efficient method of comparison.
Its clear the OP is not getting this idea so lemme try. If you're from Ruby (since you seem to use the terms instance/class methods) all modules in Elixir are like modules with `extend self` in Ruby and there are no classes. You can't instantiate a Ruby module and you can't instantiate an Elixir one either. All data is like primitives in Java, no methods co-exist with the data. You can't instantiate a Java primitive (by using new) they're just there. They're separated on purpose. In Elixir, there is only data and code. Data is owned by the process (read mini-thread) and the code is always global. Therefore, you can execute any public method from any location in the codebase without having a reference to anything (though functions can require parameters which you may not have the data for and will crash otherwise).
Could you generate the more efficient version using an elixir macro?
Because that's just how this language works.
So your %conn{} struct most likely has this info already. If not, I suggest writing a plug you can add to your controller that, for example, reads content type requested and drops that info into the %conn{} struct. That way your controller can still fetch the correct data and send it to your view which pattern matches its render method on the requested content type and renders thusly.
Had you seen this [SO - Phoenix Controller With Multiple Render](http://stackoverflow.com/questions/30624403/phoenix-controller-with-multiple-render/30625444#30625444)? I prefer the variant using render in conjunction with an :atom.
Thanks for the response, yeah FP is totally different to me. I think that's what is causing my brain to go haywire. In curiosity, what exactly do you mean "kept separate". For example one our RPG objects looks like this: RPG: { user_id: 31, character_id: 47, username: 'test123', character_name: '123123', level: 1, gender: 0, cc: 0, stamina: 100, staminaMax: 100, invslots: null, last_difficulty: 0, health: 55, chealth: 55, experience: 251, galactic_experience: 0, galactic_level: 1, cexperience: 0, equipped: {}, waypoints: [ 0 ], statpoints: 0, skillpoints: 0, kills: 12, min_dmg: 0, max_dmg: 0, critchance: '', crit_multiplier: '', str: 0, dex: 0, intel: 0, vita: 0, defense: 0, last_planet: 0, completed_quests: [], active_quests: [], active_skills: [], skills: {}, Inventory: { Items: {} }, in_game: 'cio2syq100000mchcqai1sllx', in_game_name: 'hey55', xferto: 'joingame' } } Which is stored inside a global variable called "users". (That has a property `RPG`) In Elixir, this data would not be stored inside the global users map, it would be assigned to a key/name, then, right? So think of a Redis Key / Value pair type system when thinking FP / `maps`? 
The problem with Javascript Objects it is that they aren't just a datastructure that get's passed along they use pass by reference so if you change things in the object it often gets changed where you have to object stored so it makes it really hard to debug and using something like immutable.js is advisable. The reason that Elixir has different structures to deal with data is that they have different strengths. Keyword lists are really good for options since Elixir allows you to use them optional parameters if they are the last parameter. https://www.djm.org.uk/posts/elixir-keyword-lists-function-parameters/ http://stackoverflow.com/questions/28180366/what-is-the-benefit-of-keyword-lists
no :)))))
Use recursion, depreciating a counter (learning Elixir, so my code may be incorrect :) ) call_me(50) def call_me(0) // finished end def call_me (counter // 1) call_me(counter - 1) end 
From reading the prompt and your description you most likely want to create a range from 0 to 50 and use reduce over that range.
ok that makes sense thanks! i guess i could just pass in whatever variable holds the number of trials say if t = 50 so that everything changes with whatever user input is given for that round. 
Yeah, but assuming it were equally easy to write an app in Elixir and Rails, the extra performance of Elixir(for certain apps) could translate to reduced server costs. There's a lot of factors that would go into that so obviously, case-by-case YMMV yadayada. Also, if your app benefits from a distributed architecture, orchestrating it in Elixir is just going to be easier(I say this as someone who knows Sidekiq, Cells, drb etc. pretty well). So, even if you're not operating at a large scale there can be benefits.
1. First, you should know that every function returns the last line thats executed, just like in Ruby. Also, its very typical to return data in the format `{:ok, data}` or `{:error, reason}`, since the language is dynamically typed (i.e. we don't know what type `data` or `reason` is) this provides as a way of tagging the data to know what it contains. It's a commonly used pattern. In the case of Phoenix, thats how Phoenix knows you want to allow the join, sort of like returning true/false to a `can_join?` function but instead you return additional data like the socket. 2. So the `leave` function you gave as an example is just returning the socket that it was given. Since data is immutable, the calling function cannot tell if you've modified the parameter (which is a good thing!) so if it needs to see changes then it typically requires you return it. (BTW, the current version of Phoenix doesn't look like this, is this an older version?) 3. Not sure since this looks like a very old version of Phoenix. Phoenix now has a terminate/2 function. Here's the docs for it: https://hexdocs.pm/phoenix/1.1.4/Phoenix.Channel.html#c:terminate/2
Using the for syntax with a range is probably the easiest way to call a function n times. This may not be well suited for your problem though. for _ &lt;- 0..50 do IO.inspect "call" end
Yes, all your modules that are in the main project directory or external dependencies as specified in your `mix.exs` file will be imported and made available throughout your project. You can find out more about how to customize paths (for builds, compilation, load paths, etc.) [here](http://elixir-lang.org/docs/stable/mix/Mix.Project.html).
To be fair... &gt; Applications are also useful if you need to perform extra configuration, and initialize your library accordingly. I think config will work even if you don't point it at an OTP app, the top level of the config has to be a valid module, but it does not have to be an app. Oh, mix *will* complain, but it doesn't seem to actually have any failure cases. --- I do think it's better practice, at least for internally made apps, to.... a) Add the app directly to your supervision tree b) Inject configuration when you add the application to the supervision tree by passing a variable to start_server It's only if the OTP app is totally disconnected from your own, like in the case of a library, that you can just have it run its own supervision.
Well WSL would solve your issue I believe as its similar to running a Linux VM in Windows but with native performance and some other great things.
You can also use the id: field when starting a worker in order to give it an identifier that's specific to that supervisor. Since its not globally accessible, you can pretty much pick whatever you want---it's kind of like using the supervisor as a process registry.
That'd be great, I hope they can solve most of the issues before they do an official release. Elixir, obviously, would have run in a VM on my windows system but I had just spent the last few months progressively moving away from VM usage on the Mac, and getting native performance where it matters---compile times.
A note needs to be added in: all applications that need to be packaged into the release MUST be in applications, not just in the deps list.
On a related note, please don't make libraries as otp apps unnecessarily. What I mean is, simple api wrappers and helper modules should not be otp apps in my opinion. Only things that need to hold state should be, in my opinion.
I've never seen any serious mention of adding static type checking to the compiler.
The music is obnoxious. The idea of the video news series seems good though.
That is awesome. It's both useful and good looking. Thanks for your work.
Amazing, very helpful. Note: 'Gaurd Clauses' should be 'Guard Clauses'.
Thanks! This is going to be very useful! Btw. `case` can optionally take `when`guard, might be useful to mention it.
It wouldn't hurt to also emphasize that processes are often not about concurrency at all. The primary reason they exist the way they do is to offer isolation. The concurrency and distribution benefits were just a side-effect. One of the benefits of node child processes is that you can isolate another instance of the VM, and anything that happens there doesn't directly affect the other processes. That process can crash and be restarted without harming the other process. Imagine extending that through your entire application, because your VM supported it. And the entire ecosystem of libraries works this way, not just some small subset of libraries. If you wanted to do this with node processes, you could, but you could only launch a few hundreds of workers on a large box, if that many. Because this abstraction is so cheap in Elixir/Erlang, we can literally launch a process for every incoming connection, providing the same kind of isolation that is in PHP and Rails and Django (if 1 request throws an uncaught exception, the process manager starts another worker), whereas if you handle multiple connections in 1 OS process in node, multiple connections fail. But you have the same kind of performance characteristics that node has in Erlang/Elixir, with some other benefits (work-stealing schedulers help prevent unbalanced workers that node's workers often have, pre-empting scheduling, no one can block the CPU forever, etc).
Koans are like a self guided tutorial. I haven't checked out the Elixir koans specifically, but there will be several files in a numbered progression. Each will contain a general statement about what a snippet of code should do. The code is usually partial and incomplete, giving you the task of using a certain concept to complete the code. They're usually wrapped in a test suite so you can know if you completed the code snippet correctly. Here's an example from [elixir-koans](https://github.com/dojo-toulouse/elixir-koans): think "Can reach tuple element with index" do a_tuple = {:foo, :bar} assert elem(a_tuple, 0) == __? end
Love me a good cheatsheet. This should be first page of any elixir guide.
A few things I have noticed: - `is_function` appeared twice in `Built in Type Checks` - You can also use `/`, `|`, `"`, `'`, `"""`, `'''` for sigil delimiters - Here doc should starts with a new line - No map example - No `receive ... after` 
An example of starting a worker with the id: field?
Awesome, I"ll let you know what I think.
What's the deal with that? String module operates on string, if you want to do operations per-character you have to cast it to char list. It's annoying. 
Makes sense, thanks
There are also [Elixir Sips](http://elixirsips.com/) and [Elixir School](http://elixirschool.com/). Both are great resources for getting started.
&gt; There are also Elixir Sips and Elixir School. Both are great resources for getting started. Thanks!
Link is not working for me? Is it dead, or just my corporate firewall? Anyone have a corporate-firewall-friendly mirror, or do I have to wait until I get home to see this awesomeness?:-)
We are a good looking community, I must say!
Not to be confused with the original Awesome Elixir GitHub repo: https://github.com/h4cc/awesome-elixir
Yes and of another process using the supervisor to get that info. 
Here is one example in public code I have found. [Link](https://github.com/KaySackey/magpie-server/blob/master/web/models/room/supervisor.ex). The that whole section is the supervisor for a module that keeps information about a chatroom live in memory---both the username list and the message history. In the init/1 function of the supervisor, the children are declared using the id parameter. children = [ worker(Room.Abstract.MapServer, [], restart: :permanent, id: :users), worker(Room.MessageListServer, [], restart: :permanent, id: :messages), ] The later to get them, you call `get_child/2`. For example to get the GenServer that tracks user list state from its supervisor you'd call `get_child(supervisor, :users)`. The :users atom only has to be unique to the supervisor, so each supervisor can handle a different room, and to get the users list from each... its always the same call. ` user_list = some_specific_chat_room |&gt; get_child(:users) ` 
Thanks for sharing, Im still watching the video, but I can't contain myself of being Happy for trying Elixir this week.
The talk from Lance Halvorsen about Phoenix decoupling so interesting !
Noticed a few errors in the sigils section: ~c[ #{baz} ] #=&gt; ‘Sparky’ # Actual Result ~c[ #{baz} ] #=&gt; ' Sparky ' ~w(one two #{dog})a#=&gt; [:one, :two, :Sparky] # `dog` doesn't exist, should be ~w(one two #{baz})a#=&gt; [:one, :two, :Sparky] 
Do you have a link to this talk by any chance? Also OP, great write up! Have to find some time to catch up and watch these talks.
Thanks! Every couple of days new video is posted on the ElixirConf.eu website and links from the blog post will take you to talk subpage where links to videos and slides will show up. As jmiven said you can also subscribe to youtube channel or follow @ElixirConfEU on Twitter :)
This seems reasonable, but ultimately seems to come down to "elixir and erlang might be good for some applications" without really identifying when they'll beat traditional approaches. That said, nice to see a practical overview of the tooling in this area.
Yeah, the Elixir deployment tooling needs a lot of work. Also needs a lot of documentation.
I didn't mention this in the slides, but did bring it up. My use case involved tens of thousands of concurrent websocket connections, which I *really* didn't want to drop every time I upgraded. A long-lived Erlang cluster was the easiest way to get this done. If this were a simple web API, I'd have had a lot more freedom to choose among deployment patterns. Just one example. :)
It's actually not quite as bad as it may seem. I meant that 90% figure in the slides. Also, in that 10% that failed for me, keep in mind I was/am an Elixir noob and probably/definitely screwed some stuff up myself. Most of the information on hot upgrades I found out there when I looked into this stuff six months ago was vague, no-details pronouncements of "don't". Much of my reason for giving this talk was to add gory details to the public record, so I went relatively heavy on them, slide-for-slide. Really, it's pretty nice most of the time. 
Did you consider python, lisp, or other languages known to be able to hot swap code?
Which software did you use to create this ?
Indeed, it will be live soon. For now we just have the slides.
The Elixir Slack is very good. https://elixir-lang.slack.com/
The IRC channel is great, very helpful. 
That was a fantastic talk yesterday at Empex. 
Not yet, since I feel that the discussion is more "meta" on Elixir than actually hands-on. Not complaining, I like reading discussions here, I just wanted to know good resources to strictly discuss code.
This is new, but it can be helpful: [Elixir Forum](http://elixirforum.com/)
Thanks for the extra details. Any chance the talk itself will be available? Based on the slides or looks like you have a ton of great information. I'm semi evaluating elixir for a performance &amp;amp; reliability-critical API for a large company and I've not yet seen anything describing deployments in the depth that you have. 
Stack Overflow. It helps the community best if we post questions and answers there. It is easy to find and refine/improve questions as well as answers. IRC and Slack might be helpful, but answering the same questions over and over might turn people away from the chats; ideally they should be used for discussion and design of the language. Elixir Forum has some features for QA style posts, but I think that stack overflow is more intuitive as the place for finding answers; especially for newcomers who might have used SO from their other languages.
nice crt monitor shots p.s.: is there a video of the talk?
Was there a talk that went with this? If so, will a video be available at some point?
to those asking, this talk and all the others at EmpEx will be available in a couple weeks. follow us at @empexco for the latest news.
Hot upgrades were just one part of the picture. Immutability was a huge draw, as was an easy-to-use actor model of concurrency. And I should specifically call out the Phoenix Channels websocket library, which shaved weeks of development time off the schedule compared to other platforms.
Thank you! It was a pleasure.
Please add a readme and a license to the repo. Right now it's just some unusable code :)
Awesome! Thanks for sharing :)
The first thing I did was to do as many small problems with Elixir as I could. Advent of code style stuff. I forced myself to stay away from OTP and agents and everything the kept state. You can actually accomplish a great deal just using the core language and staying inside the functional programming paradigm. Every time you solve a problem, go to the [docs](http://elixir-lang.org/docs/stable/elixir/Kernel.html) and try to find a new way to do it. Use as much of the standard libraries as you can to get a feel for what the language is capable of. When you feel ready to dive into the world of OTP (which you may be at already), I recommend three books: * [Elixir in Action](https://www.manning.com/books/elixir-in-action) * [Reactive Design Patterns](https://www.manning.com/books/reactive-design-patterns) * [Designing for Scalability with Erlang/OTP](http://shop.oreilly.com/product/0636920024149.do) The first book will walk you through the basics of the language and build you up to OTP. Sasa did an amazing job structuring this book so that there is a natural progression up to each new concept. By the time you get to OTP, you know both why it's needed and how it works under the hood. The second book isn't actually an Elixir or Erlang book. It's a book about building highly scalable systems written by the maintainers of Akka. However, they learned everything they know from OTP and almost everything in the book applies to writing Elixir/Erlang applications. If you want a good place to learn about the basic design of actor-based systems, I really couldn't recommend a better book. The final book is an Erlang book that brings all of the concepts together. You get a whirlwind tour of Erlang itself and how to build actor-based systems from an Erlang/OTP perspective. It'll mostly cover familiar concepts that you learned in the previous two books (but with unique Erlang incite) until you get to the final four chapters. Those final four chapters are worth the price of the book on their own. They give a high level overview of everything you should consider in a distributed system. Since even a single node Erlang system has many distributed qualities, these concepts are incredibly important to building Elixir/Erlang applications. Once you get to the point of building multi-node distributed Erlang systems, the advice is invaluable. There are many other fantastic resources to learn from in the community. Pretty much all of the Elixir and Erlang books are worthwhile in one way or another. I also highly recommend subscribing to the newsletters in the sidebar and hopping on the elixir/phoenix slack to talk to people. The three books I recommend cover everything you need to go from zero to awesome. Definitely read them in that order and take your time playing with the examples. Some of these concepts are deeper than you'll realize and there's no substitute for hands on experience. 
The talks will be posted in "a few short weeks", even though there is no such thing right after a conference that good :) Depending on the workload you need to serve, you can possibly get away with a much more 2016-y deployment (in which case, when the videos come out, you should watch Monica Hirst talk about deploying Elixir with AWS CodeDeploy). My talk is about what you can do when ephemeral servers aren't an easy option. Either way can be suitable for high performance and availability.
Awesome I'll keep an eye out for the videos. I'm actually looking at hosting in aws so that sounds like a perfect talk. Thanks again for the information!
that's a really nice talk. I'm surprised channels weren't featured a bit more though, that was one of the main attractions for me coming from rails
Thanks! re: channels: I wanted the primary topic I addressed to be the value prop of comparing how we build Rails apps with how we build Phoenix apps. I felt that greater productivity through performance was a good hook to focus on.
Not at all! You can get started today by reading the excellent [official guide](http://elixir-lang.org/getting-started/introduction.html) or one of [the numerous books](http://elixir-lang.org/learning.html).
No! :) As /u/jmiven said, we have great getting starting guides and first-class books to get you up to speed. While we drew inspiration from Rails in some areas, learning it first would make as little sense as learning Ruby first before Elixir since it has some syntactical inspirations. The elixir-lang irc and slack channels are also a great place for help as you're getting started. Jump in!
When I started, I had no knowledge about functional programming. The language is great, and the official guide is amazing. Also check out https://elixirschool.com/ Check out the IRC or Slack channel, a lot of helpful people on both ends :) 
I am a huge fan of the budgeting program [YNAB](https://www.youneedabudget.com/), but the built-in reports don't capture the level of detail I want to see. I'm using Elixir to build a reporting library for my budget information (which includes all my transactions, budget categories, and metadata). Once I have the core functionality built, I'm going to put a simple API around it using Phoenix and write a JavaScript frontend for displaying the data. Since I don't want to send any data over the wire, my plan is to build it using [Electron](http://electron.atom.io/) and run the Phoenix server locally as an internal API.
I'm working on a SGF parser - a tool to parse SGF files, plain text game records for games like Go and Chess. I'm also working on a MUD server with Elixir/Phoenix. And by "working on" I mean "there's no code" :D
A messenger bot for my university: - The places on campus that serves hot food is so far away from the CS building. - I hate checking when the gym closes. So I'm developing this bot to improve some of my own and hopefully others pain. Would be awesome if I could get some others at the university interested in Elixir😁 
Sure but it is in no way a prerequisite, which is what the OP was asking.
Looks great. Thank you! 
Cool thread! I'm about to release a coding statistics website made with Elixir and Phoenix ([will look somewhat like this](https://files.nytsoi.net/aSzX.png)). Currently it has a plugin for the Atom editor that sends the typed amounts to the service and some people are already testing it. If you'd like to test, send me a PM. :) It's inspired by Codeivate and WakaTime, but it's currently very simple and bare bones (I tinker on it on my free time).
This is gold, thank you so much! EDIT: Thank you for the actual gold, kind stranger ❤️
I'm working on a tool I call [clashcallerbot](https://github.com/nickve28/clashofclansbot). Basically it's for an online game in which you can war with clan mates. Since coordination is required for serious play, and the game does not offer ways to do so, I made this integration into our slack that basically is a wrapper around a website (which is used for coordination). This way we dont need to constantly share links and enter the link on our browsers/devices every time. It's the first time I'm working with elixir, so it's a starter learning project. Enjoying it so far!
I really love this article. I'm a very big TDD advocate myself, and seeing someone else's take on doing TDD in elixir is very eyeopening. Thanks for taking the time to write this. 
Thanks! I'm not usually a big TDD advocate. I use it when it make sense but not all them time. Here, though, it was a no brainer. I really just wanted a relaxing experience and knew roughly how I wanted to approach it. Still, it was surprising how naturally this flowed from test to test. It was perfect for calming down post conference.
test
I'm glad you liked it.
The second argument to controller functions is a combination of URL path params, query params, JSON body etc.. The framework abstracts the details by pulling them all into the single map. They're available separately in the conn, you should be able to inspect it to see them.
Wonderful. hope it is extracted into hex library.
Great idea! I'm working on a few things: - [Find a Player](https://findaplayer.com) - a sports startup which helps people find players and games to play (UK only for now). About 40% of the backend is written in Elixir. - [SMS Blitz](https://github.com/johnhamelink/sms_blitz) - an SMS sending library we used to migrate from one provider to another seamlessly (pull requests accepted!) - [Exrm_deb](https://github.com/johnhamelink/exrm_deb) - An adapter for exrm which creates debian packages ready for you to install on your machine, or put in your private APT repository.
Hello. Both Programming Elixir and Programming Phoenix are very nice books. I would also recommend Elixir in Action from Manning Publications. If you don't have any experience with FP I'd recommend reading Programming Elixir and Elixir in Action first.
Definitely if it only takes a few days! My biggest concern was that it would take me a month to read both haha
The biggest mistake people made when they"learned Rails" was to not study Ruby first. Here the mistake would be significantly magnified since Phoenix is just another Elixir app that happens to have behavior for an MVC framework. Learn Elixir first, then read the Phoenix book.
Both: Elixir first, then Phoenix.
I disagree entirely. You don't study something to become a master, you study something to see if it interests you, or else you end up mastering something you never want to use. Learn the stuff that interests you first and keep going if you find your interest growing - order only matters if you're on a deadline trying to min/max efficiency of learning your tools.
There are three good books, imo: 1. Programming Elixir - You should get this. It can be roughly divided into two seconds, the first on syntax and functional programming. The second section covers OTP and some slightly more advanced things, which you'll eventually need to know but not in the beginning. So you might just read the first half of this book and then move on and come back to the second half later. 2. Programming Phoenix - Good book. A lot of the information is available in the Phoenix guides on the website though, so you may not feel the need for this one. I'd say learn some Elixir then start going over the Phoenix guides, and if you need more then buy this. 3. Elixir in Action - This covers a similar space as Programming Elixir. In my opinion, Programming Elixir is the better book for learning the syntax and functional programming stuff, but this book is better for understanding processes and OTP basics. So depending on your experience, get whichever one you think will benefit you more.
Personally I liked Elixir in Action best. Programming Elixir felt too exploratory for me while Elixir in Action felt more thorough. I'll agree with others though that Programming Elixir is a good intro to FP, I just felt like Elixir in Action was a better intro to Erlang/Elixir
Yesterday I got a proxy for a Minecraft server working that just captures and then forwards the packages in both directions. Someone proficient in the language probably could have built this in thirty minutes, but using tasks and :gen_tcp taught me a lot. Next thing is doing it again with a proper supervision tree. Inspired from that I am also going to try to create a protobuf serializer for Phoenix. 
You won't understand how Phoenix works if you don't know how Elixir works.
I am working on Elixir flycheck syntax checker for emacs. It uses *mix compile* instead of *elxiirc* so it works well even with phoenix and projects that rely heavily on macros. https://github.com/tomekowal/elixir-flycheck-mix-compile/ It is still very early version so you have to install it manually, but after fixing the issues I'll try to push it on melpa.
I don't need to understand how something works to use it. Things are designed to operate that way on purpose. I watch a couple of guides and boom, I can use phoenix channels for websockets well before I had any idea how they were implemented.
I've got both of them and they are both worth reading because the crossover is pretty small. Looks like there are people who strongly suggest getting one or another first. But I think it depends on your learning style. For me personally, I like to work on some real project or mindlessly follow a tutorial before work on my own projects. The retention is much better for me if I did my own project, because emotional connection probably. With Programming Elixir, you can work on some simpler projects, but it might or might not necessarily be the project that will make you interested. If you are mostly interested in a fully fledged web app, then I'd say start with Programming Phoenix. This also mirrors u/tewls's comment, some people like to start building from bottom-up, understanding the small pieces first before they embark on the complex project, some like to see the big rough picture first and then pieces will emerge from the journey. I'm usually the later. Hope that helps.
How will a protobuf serialiser work with Phoenix? I guess you would need macros of some sort to let a user import their own proto definitions? Is there an extra cost to: 1. deserialise the Phoenix preamble (i.e. channel join, send message etc), 2. deserialise the actual message body according to a user specified schema?
Thanks for this explanation. Is it a common pattern to spawn a new process and assume the state from that process? http://elixir-lang.org/docs/stable/elixir/Agent.html#start_link/2 "Starts an agent linked to the **current process** with the given function." The analogy might be in an OO lang like ruby, you might instantiate a new object with state. Whereas in elixir, you spawn a new process and use an Agent to manage the state in that process. As you say, this feels like a lot of overhead to manage state. 
Awesome, I am interested to see where it goes. I think message pack definitely looks like it would be easier to start with. 
That seems really interesting. I wonder if it could be adapted monitor distributed systems as well
You have to explicitly say what state that new process has and erlang will copy it for you. All you have to do is use the local variables in your agent's start function and that will do what you want. foo = :bar Agent.start_link(fn -&gt; foo end) # Now :bar is the state the agent has I would say that processes are used a bit less than actual objects in terms of storing state, though they can be thought of as analogous, typically state is managed in tail recursive function calls within a process. Processes don't hold the state in the same way an object in an OO language holds it, but that's just semantics and possibly too much info 😆
Read Programming Phoenix and just get started with a project. Study things as you go along. Refactor when you learn new Elixir features.
Cool, so it runs long after macro expansion :)
&gt;The real advantage of Agents is when you include them as part of a supervision tree. It's a not-uncommon structure to create an rest-for-one supervisor with an Agent-like state holder followed by a process which uses that state-holder. If the worker process crashes, it can be respawned and the state process is left untouched, allowing the new process to pick up where the old one left off. Just wanted to highlight that valuable nugget. Thanks.
So I'm no monad expert, but from my perspective with is an alternative to full blown monads. There's already a lot of convention around {:ok, _}/{:error, _}, so through convention it could replicate this easily throughout your own code. If this becomes common, like node's last parameter is callback and first parameter is the error (now being replaced by async/await &amp; Promises), you could have it everywhere. On top of that, you could probably make a lightweight library to consume these like monads.
I think it's very specifically one monad, probably the Either/Error monad (because things can return non-matching values which are then returned from the entire expression). I don't see how this could replace the List monad (which would involve running subsequent lines in the with statement potentially multiple times) or the Cont monad (if you can do the Cont monad, you can do any monad)
You're right that this would't replace all types of monads. The most commonly talked about one for use in other languages seems to be the Option/Maybe monad, which is fairly close to Error/Try monads.
Strange, i've tried it an it seems to work but you can also find it on: https://medium.com/@olafura Thanks for the heads up ;)
Looks pretty neat. Will give it a go for sure. Have you tried benchmarking benchee?
I don't know of any blog posts, but José covered a bunch of the history in this talk: https://youtu.be/aZXc11eOEpI
Could you use the Wayback Machine for the old posts? https://archive.org/web/
Build tool as in the task runner built into VSC, although I guess I could just use mix from the command line.
Same as /u/zerovap, I just use the command line. Especially since I'm trying to transition to WSL.
This sounds like it would make a great backend for a niche phone app! I would use it!
Thanks for the feedback, I really appreciate it! :) I was actually thinking about making just an API for the thing and partner up with somebody to make a phone app out of it ( or just prototype it in Ionic or something ). So I might as well do that now and just push back on the web app version.
Excellent post. I've been meaning to learn more about elixir processes. 
I was just looking for a resource to refresh my understanding of GenServers and Supervisors and this was perfect. Thanks!
First idea came up is to use list comprehensions. Enum.sum(for x &lt;- [-1,2,-3,4], x &gt; 0, do: x) 
Whoops you're right of course, I should have read TFA more closely! Updated my original comment.
It is great! The viewpoint remind me of this artice: http://bionics.it/posts/flowbased-vs-erlang-message-passing although I prefer the ultimately language for processing to be Rust ("fast"): https://github.com/hansihe/Rustler "safe" https://github.com/utkarshkukreti/select.rs "servo" though Rust currently only includes synchronous IO in the standard library 
Great! thank you haqkm!
Not the OP, but I do a lot of graphics work. I don't see Elixir/OTP providing any benefit to graphics programming. What it does provide benefit to is possibly certain things associated with whatever the app may be. But graphics programming itself I don't see a benefit. The disciplines and what benefits them are quite different. Only reason I could see using Elixir/OTP for some graphical work is if whatever else you're doing benefits from it and you just need a graphical component. Although if using an API like OpenGL I'd prefer handling the rendering in a NIF rather than Elixir itself. Another concern is OpenGL doesn't have very good threading support (it's a painful experience). But something like Vulkan which is much better designed for multi-threading could be interesting use in Elixir (albeit there would be a lot of overhead unless you're using binary's). But I feel like the Erlang VMs scheduler might make it not as worthwhile (for instance if you're submitting some rendering commands but then your process reaches its run limit halfway before submitting the entire command queue). The main thing you want out of rendering APIs at this level is efficient GPU utilisation (where the GPU is running constantly and the CPU is running constantly with no bottlenecks between the two, at least when talking about realtime graphics). And I can just imagine it being harder to deduce how effectively you'll be utilising the GPU in an Elixir app. If performance isn't the utmost concern then it would be fine, but I don't see Elixir making the API or process better anyway, so personally would probably just defer the rendering to a NIF. With that said though, a notable Erlang application to that does use OpenGL is Wings3D. 
Great walkthrough! I'm planning on doing the same to slowly replace some rails servers, this is an excellent starting point.
I was partially inspired by the ElixirDose Job Board to open source ElixirStatus. The site is http://jobs.elixirdose.com/ - the GitHub repo is https://github.com/rizafahmi/elixirjobs The problem with this and ElixirStatus is that you have to maintain the source code to keep it up-to-date or else the examples it gives might become outdated. (I will have to overhaul ElixirStatus in the coming weeks for that very reason.)
While I did know this, it's a good reminder to go directly to the source if you want to know how something works. It's also a small bugbear of mine with the language that functions with optional arguments are still only referred to by the maximum arity and don't offer a hint that there is a default. 
How did you start elixir app?
just: iex -S mix
Also. If you want to benchmark stuff, always make it in a way, that someone can reproduce it on his machine. 
Maybe not a whole microbenchmarking framework with multiple runs etc. - it sure would be nice, but nothing I see as the concern of a language. A simple benchmarking module that measures the run time of a given function on the other hand, [like ruby's Benchmark](http://ruby-doc.org/stdlib-2.3.0/libdoc/benchmark/rdoc/Benchmark.html) I'd have suspected to be there. There probably is something in Erlang, though. At least I know of [:timer.tc](http://erlang.org/doc/man/timer.html#tc-1) which returns the run time of a given function in microseconds. I use it internally in benchee.
I think in the end it isn't about benchmarks. These three technologies do well or have a way to do well on multiple cpu machines. I think it comes down to what they help you solve. Even if Elixir was slower which I don't think it is it helps you solve many things that Node and Go need Redis, RabbitMQ, or some other product or framework to solve. OTP solves so many issues with distributed systems. Supervisors let you handle any issues. The win is bigger in Elixir as in the end it is less moving parts, and less code, to get the same great system. 
The benchmark that the book refers to is [this one](https://github.com/mroth/phoenix-showdown). The benched server is still too simplistic IMO, but it [does way more](https://github.com/mroth/phoenix-showdown/blob/master/plug/benchmarker/lib/benchmarker.ex) than what you're doing here, by rendering a dynamical template showing the list of members inside another template (layout). That's IMO much closer to something real than your server. Based on that benchmark, I wouldn't personally jump to conclusion that Phoenix is faster (or vice versa). Different platforms will excel at different things, and we really need to work hard to make implementations as similar as possible to properly compare the platforms. Even then, I wouldn't draw general conclusions. A properly constructed bench can only show that `foo` is faster than `bar` for this particular type of job. However, I'm less interested whether Phoenix (or Plug) is faster/slower than some other framework/platform. A point of a bench should be to prove that it's fast enough for the desired case. When I first evaluated Erlang, I tested a quickly hacked simplistic simulation of the system I was going to build. A 12 hours test convinced me that I can easily reach desired latency at 10x of the estimated target capacity. Those results were good enough for me to pursue Erlang :-) To summarize, my point is that it would be better to test a real-life like scenario, and that comparisons to other frameworks are IMO usually less revealing and can often be misleading. Perf should not be the primary reason for choosing a web framework. Fault-tolerance, first-class support for microservices through Erlang processes, fast and concurrent in memory k-v store with ETS, distributed computing, and great introspection capabilities - those are the properties where Elixir/Erlang excels compared to most i(if not all) alternatives.
That's true, I could just use :timer.tc for one-offs, and move up to Benchee for full-fledged benchmarking... And it was probably Ruby's Benchmark that got me thinking about a standard-lib benchmarking tool being missing to begin with! Need any help with benchee, btw? I have some time and want to contribute to something OS. [I've been dabbling in Elixir for a while](https://github.com/pmarreck/elixir-snippets/) but it's really time for me to try to contrib to something (if not invent my own thing).
When you're building a web app with Phoenix, it is advised to keep the models simple and only let them contain the schema and changeset functions, and to put Repo-calls in the controller IIRC. I understand that might have been a choice for the sake of simplicity, but I fear many people will learn the wrong lesson and put their Repo-calls in the model modules.
How does the VM handle variables with no references? 
Not sure I understand the question, but BEAM has two types of variables: registers (used to pass parameters and return values, can also be used as temporary variables) and stack variables (temporary). Registers (I presume there are 255 of them) are addressed directly by their index, stack variables -- by offset from stack top. **update:** whatever is in a register or stack variable can be a reference of some object in the heap.
Can you link to a place where that's advised? Curious to read more about that. Thanks!
it's certainly in the Phoenix book, properly the guides. Everything that does side effects should go into the controller if possible
Somewhat odd/unfortunate timing for an introduction to Ecto when Ecto 2.0 is right around the corner (rc6!) and has some [significant changes](https://github.com/elixir-ecto/ecto/blob/master/CHANGELOG.md).
I am aware that Ecto 2.0 is just around the corner. The article was partially written for forwards compatibility with it, since it skips over a number of the deprecated and removed incompatibilities. In fact, the only change that needs to be made for this article to be compatible with Ecto 2.0 is to update the config.ex file with `config :notex, ecto_repos: [Notex.Repo]` (as well include the DB connection info before compiling). In hindsight, I should probably have made a note of this though...
I love you
Hallo :) Also deine Videos gefallen mir ziemlich gut. Du **sprichst klar und deutlich** und machst deine **Videos nicht zu lang**. Die Idee für jedes Thema ein Video zu machen finde ich sehr gut weil man bestimmte Sachen schnell nachschauen kann. Es scheint manchmal so als ob du beim Aufnehmen des Videos zum ersten Mal die Aufgaben siehst. Das ist auch vollkommen in Ordnung, weil du die Antworten weißt aber vielleicht könntest du deine Videos informativer machen wenn du dich etwas vorbereitest oder das den Zuschauer nicht merken lässt. Alles in Allem finde ich die Videos **sehr gut** für Anfänger und werde mal versuchen ein paar Freunde dazu zu bringen sich deine Playlist anzuschauen. Edit: Ich wollte noch fragen ob du noch vorhast für die anderen koans Aufgaben Videos zu machen?
My life just changed.
Wow, das nenne ich mal ein Premium Feedback, vielen Dank dafür! Ja manchmal stürze ich mich sehr schnell in die Aufnahme ohne mich vorzubereiten, weil ich denke, dass mir das Thema vertraut ist. Manchmal erwischt mich das dann aber auf dem kalten Fuß. Ich werde in Zukunft einmal einen kleinen 'Dry-Run' machen, bevor ich dann mit der Aufnahme beginne. Zu deiner Frage, ja mein Ziel ist es alle Koans durchzuarbeiten und dann noch ein paar Videos zu funktionalen Konzepten, Mix, Hex, ExUnit usw zu machen. Ich denke bei den Koans schaffe ich aktuell so 1 Video pro Tag, aber das Tempo werde ich wegen meines Jobs und privater Verpflichtungen leider nicht lange halten können ;) Zu Phoenix und Webentwicklung allgemein habe ich auch noch Videos geplant! Ziel von mir ist es mit dem Channel und auch meiner Website die Sprache auch in Deutschland etwas bekannter zu machen und vielleicht eine kleine Community ans Laufen zu kriegen :)
What's the difference between doing that and &gt;iex -s mix ?
Ach genial! An Videos zu Mix, Hex und ExUnit bin ich auch sehr interessiert :D Und du musst dir ja keinen Stress machen. Bei sowas würde ich eher auf Qualität setzen als auf Quantität, so wie bis jetzt auch. Und du hast recht! Mehr Leute sollten diese Sprache schreiben :)
http://stackoverflow.com/questions/34340690/should-i-use-ecto-repo-in-controller-or-model-for-elixir-phoenix
Haha, wie klein die Welt doch ist ;) Ich glaube das erste mal habe ich bei HackerNews drüber gelesen... Dann hatte ich irgendwann mal Langeweile und wollte was neues lernen und joa... war eigentlich sofort begeistert :) Und du?
I've come to be suspicious of Enum.reduce whenever I see it. It's often a sign that the author was thinking imperatively and didn't refactor to a functional pipeline once the code was working.
Why did I never see this before ?
Nice. I just started watching the project on GitHub (user "pmarreck"). Not sure how much simple console graphics might help convey benchmark information but I wonder if [sparklines](https://github.com/pmarreck/elixir-snippets/blob/master/sparkline.exs) would help, haha. (that was one of the first things I wrote in Elixir, based on some Erlang code I found, and was quite eye-opening from a "wow, this is easy AND concise" perspective)
Duplicate [*], posted to the same subreddit, by the same person a day ago. [*] https://www.reddit.com/r/elixir/comments/4op0ha/use_witai_natural_language_processing_in_elixir/
Luckily I didn't write 2.0.0 :) 
Does this mean that Phoenix 1.2 release isn't far behind?
Yeah I'm currently running the RC and I think the only wait was for Calendar types to be released.
looks super concise indeed. :) Sounds like a good idea for a benchee plugin I'd wager. We can continue the conversation on github if you want to, I'm more active there than I'm here (only check reddit occasionally) :)
Whats the logic behind the deprecation of imperative assignments? (First section of the article)
I do imagine though that if you were using multiline if statements, you wouldn't assign the value of the if. I wonder if it'll print out a warning for those as well.
The way you are doing it is correct. Yes, like others have shown, you can metaprogram it. You could even try to be fancy and metaprogram it at compile time with a macro that expands to the code you wrote. However, this comes down to the added value vs the readability. Having to grok a reduce or some compile time macro versus the readability of the plain repetitive code you have is a high price to pay for the readability lost. Ruby teaches you that every repetitive bit of code must be obliterated. You see repeating code and think how to make it more DRY. A more sensible approach (one that Elixir favors) is to weigh the cost vs the benefit. Everything has tradeoffs.
If this was a single-model occurrence even for multiple fields, I'd agree. But I'm working on a set of app-wide defaults, to provide a default wrap for the "field too long" postgres error for all string types, as well as a sane default setting for business logic perspective. For our needs, 80 chars seems reasonable maximum unless I consciously decide on a longer limit for a particular field that needs it. I realize overabstraction and overuse of metaprogramming can be bad, but I feel convention-over-configuration is one area where it's still valuable.
In short, it's a refactoring hazard since there's implicit assignment in the `if` expression (and other similar expressions) that isn't seen outside the `if`. So you can't just lift the expression to another place without having to read the inside to make sure its not assigning anything first.
It would be nice if Ecto.validate_length/3 simply could take an array as its 2nd parameter. 
f you want to know what Phoenix 1.2 is all about, check my ElixirConfEU keynote which demos Phoenix Presence: https://www.youtube.com/watch?v=n338leKvqnA José Valim and I were also on the changelog podcast talking about what's new: https://changelog.com/208/
The podcast was great, thank you guys!
Thanks so much for all your work chris and jose! Phoenix and elixir are a beautiful thing to have :)
Agreed. Thank you guys. Coming from a python background but I wanted the reliability and feature set of the Erlang runtime. Great package there and elixir overall has a wonderful community. Thanks for putting it together. 
Generating swagger spec from Phoenix project https://github.com/everydayhero/phoenix_swagger - with some helpers for specing json-api apis. forked from https://github.com/xerions/phoenix_swagger Integrating swagger spec with Bureaucrat markdown output to produce docs that can be processed by the slate static site generator https://github.com/api-hogs/bureaucrat/pull/10 
Not sure if it qualifies as "working on", but I just started learning Elixir seriously from yesterday. So I would be solving Project Euler problems and start converting a function or two out of my existing works (eg Vehicle Tracking System)
Done thanks man
Yeah ... I had kind of ... a less than great impression of it as well. Thought I'd see if anyone had anything positive to say about it. That's alright though, my copy of programming phoenix should be here soon!
Well, this man is genuine. And I believe you should wait until he release videos of how to build an entire app. Till then go learn by reading book and videos that are already available on youtube. Note: I've already purchased the subscription. But there is nothing significant out there yet. I'll provide update in future.
Use exrm, build an OTP release, which gives you a binary with all dependencies inside of it. Then the deployment story is mostly the same as any other language really. Canonical way? There are many tools that might be canonical for your team depending on your needs. Each PaaS has their own "canonical" way, so that part of the question doesn't really make sense out of context.
I don't use elixir, but probably the most widely spread way to do it would be to package the app into a docker container and then deploy to Google AppEngine Flexible Runtime (custom runtime), Google Container Engine (GKE) or Amazon Container Service (ECS).
Not sure about canonical but my teammates and I have been deploying Elixir apps to Heroku and it's been great. http://www.phoenixframework.org/docs/heroku is a good setup guide. Minutes, not hours, to get going. Cheap to run, reliable in production, low maintenance overhead.
Is there any "canonical" / recommended way to deploy the binary to self-hosted server? I am thinking about something capistrano-like, which will replace older version and do any necessary reloads/restarts?
Agreed, was very easy to get going. The only problem I have is not being able to do zero downtime updates.
I use edeliver right now and it works well enough. https://github.com/boldpoker/edeliver
https://github.com/SenecaSystems/relisa looks like another interesting tool to simplify the build release -&gt; SCP to servers -&gt; hot reload server process.
The way ive found to do stuff like this is using escript. Whats nice about it is that you can still structure your app as otp but still just use it for a quick script. Example below: https://github.com/Civcraft/Patreon-Slurp
That should be fine. I don't think Erlang is a necessary prerequisite. I started learning with the online guide at elixir-lang.org and then the Phoenix framework guide.
I think it's best to use `Enum.each`, since it's specifically intended for side effects and makes it immediately clear that's what you're doing, whereas `for` comprehensions are intended to create some new data. If you're not going to use the data it returns, don't use the construct that returns data over the one that doesn't.
Neat stuff, I always wanted to put database lookup tables in memory. Is there a library to log or get notifications when non essential things go wrong in Elixir/Erlang?
Oh, also, this is totally unrelated to learnphoenix.tv... I started working on this before learnphoenix.tv was a thing (there was just learnelixir.tv) and I fear this will lead to some confusion :(
I think it is. I think elixir sips moved to daily drip? The elixir sips twitter retweets everything daily drip says and hasn't made any tweets of it's own in some time. The episode list appears to be identical as well. (Or at least starting from 001 until I got bored of checking.) It would be nice if I could find something that clearly said if it was a move or if the content was just taken and re-hosted, or what exactly happened, but I haven't found anything like that.
Started work on an Elixir client for the Slack web API yesterday, and today have full web API coverage done! Next tasks on my list are 1) Writing a GenServer for folks who can use a single token in their app 2) Writing some tests and 3) Adding support for the RTM API.
I don't think Erlang is a prerequisite. I started learning Elixir with *barely* any comprehension of Erlang whatsoever, but now feel quite familiar with it from reading lots of Erlang source code in my Elixir dependencies.
Great job documenting it. 
In a clustered deployment, how do the requests get routed to the process holding the session state? 
But the comprehension will always allocate memory for the unused result?
Not sure why you're being downvoted here. I've found exrm to be perfectly fine. I just cobbled together a script that would do the few steps to deploy to a Digital Ocean droplet. Also if anyone spots any grave errors let me know :) :application.ensure_all_started(:ssh) {:ok, conn} = :ssh.connect(to_char_list(System.get_env("APP_NAME_SERVER_IP")), 22, [ {:user, to_char_list(System.get_env("APP_NAME_SSH_USER"))}, {:silently_accept_hosts, true}, {:rsa_pass_phrase, to_char_list(System.get_env("APP_NAME_SSH_PASS"))}, {:user_dir, to_char_list(System.get_env("APP_NAME_SSH_DIR"))} ], 10_000) vsn = AppName.Mixfile.project[:version] server = System.get_env("APP_NAME_SERVER") ssh_dir = System.get_env("APP_NAME_SSH_DIR") IO.puts "deploying version #{inspect vsn}" System.cmd("rm", ["-f", "app_name_*.tar.gz"]) System.cmd("tar", ["--exclude", ".git", "--exclude", "_build", "--exclude", "deps", "--exclude", "deps", "--exclude", "\"*.log\"", "--exclude", "\"*.log\"", "--exclude", "root@*", "-zcvf", "app_name_#{vsn}.tar.gz", "."]) IO.puts "copying to server" System.cmd("scp", ["app_name_#{vsn}.tar.gz", "#{server}:"]) IO.puts "unzipping release" untar = SSHEx.cmd! conn, ~c(tar -zxvf /root/app_name_#{vsn}.tar.gz -C /release) IO.puts untar IO.puts "getting deps" deps = SSHEx.cmd! conn, 'cd /release &amp;&amp; mix do deps.get, deps.compile' IO.puts deps IO.puts "making release" rel = SSHEx.cmd! conn, 'cd /release &amp;&amp; MIX_ENV=prod mix release', exec_timeout: 30000 IO.puts rel SSHEx.cmd! conn, ~c(cp /release/rel/app_name/releases/#{vsn}/app_name.tar.gz /app) SSHEx.cmd! conn, ~c(cd /app &amp;&amp; tar xzfv app_name.tar.gz) run = SSHEx.cmd! conn, ~c(cd /app &amp;&amp; bin/app_name upgrade #{vsn}) IO.puts run 
edeliver is another good option, I forgot to mention that one.
You're probably looking for [edeliver](https://github.com/boldpoker/edeliver)
Very simple project: https://github.com/era/pay :)
pids are unique across the "cluster" so if you know the pid the vm knows what node its on 
I am working on an app to monitor other web apps (Something like Pingdom). It is a phoenix app and is open source https://github.com/rawcodehq/webmonitor
Brisbane Australia, Everyday Hero https://blackbaud-careers.employeereferrals.com/jobs/a1nNH4bay 
If you're interested in learning about Phoenix, swagger, macros and DSLs, we could use some help over at https://github.com/everydayhero/phoenix_swagger We're using it to generate swagger specs for internal APIs, and adding to it as needs arise.
This is just an abstract example but you could handle a lot with just a gen server holding a map of sales ids to pids, and then have it back its map up in another genserver which just holds the latest map to give to the salesid mapper if it ever restarts These two could be registered as named processes so instead of keeping track of their pids you can use a name and just assume they are up (or soon to be up)
Hiring Americans I wonder? I really, really enjoyed Brisbane. I would probably say that Brisbane/gold coast is one of the top three places in the world I've been to as far as livability.
I would prefer the solution by @mbilical - does not use the "if" construct - the 3 possible cases are easier to read/understand 
Interesting. What makes you say so? I've never been there, but I might have some time for traveling coming up.
nice!! I have been looking for resources to learn Elixir from a Python user POV. 
True enough, but in my 'real' code, the layout of is_valid, etc. is actually more complex than just returning false on a switch. I simplified things somewhat for display purposes. 
I jumped into Elixir with 0 knowledge of Erlang. Started reading the docs and bought the book - "Elixir in action". 
There's already one remote worker based in the US on our Elixir API team, and our parent company Blackbaud is US based. Brisbane definitely has a good mix of warm weather, close beaches, affordable accommodation, friendly culture. The tech scene in Brisbane tends to be more conservative than Sydney/Melbourne. Until recently almost no-one was doing any functional programming commercially. Lots of JEE and ASP.NET. So when I got a chance to do elixir for my day job I jumped on it :) 
This is awesome thank you!
This is an amazing talk. He laid out in simple terms on what actor model can do. I think I finally get it.
This is sort of a reply-all: All the three projects posted so far look interesting, but only one has any open issues and those issues don't have descriptions. So it's kind of hard to know how to contribute. I appreciate everyone is working on this stuff in their spare time but I feel like the actual elixir repo is an excellent example for encouraging contribution: https://github.com/elixir-lang/elixir/issues I'm sure all of you have plenty of ideas floating around in your head about where you can take your project, just put them in the `issues` :)
 iex&gt;list = [{"Server", "nginx"}, {"Date", "Thu, 30 Jun 2016 07:23:20 GMT"}, {"Content-Type", "application/xml;charset=UTF-8"}, {"Content-Length", "17"}, {"Connection", "keep-alive"}, {"Set-Cookie", "OTHERCOOKIE;Path=/youtrack;HttpOnly"}, {"Set-Cookie", "jetbrains.charisma.main.security.PRINCIPAL=COOKIE;Path=/youtrack;Expires=Fri, 30-Jun-2017 07:23:20 GMT"}, {"Expires", "Thu, 01 Jan 1970 00:00:00 GMT"}, {"Access-Control-Expose-Headers", "Location"}, {"Cache-Control", "no-cache, no-store, no-transform, must-revalidate"}, {"X-Content-Type-Options", "nosniff"}] iex&gt;elem(Enum.at(list, 6), 1) "jetbrains.charisma.main.security.PRINCIPAL=COOKIE;Path=/youtrack;Expires=Fri, 30-Jun-2017 07:23:20 GMT" Or: iex&gt;list = [{"Server", "nginx"}, {"Date", "Thu, 30 Jun 2016 07:23:20 GMT"}, {"Content-Type", "application/xml;charset=UTF-8"}, {"Content-Length", "17"}, {"Connection", "keep-alive"}, {"Set-Cookie", "OTHERCOOKIE;Path=/youtrack;HttpOnly"}, {"Set-Cookie", "jetbrains.charisma.main.security.PRINCIPAL=COOKIE;Path=/youtrack;Expires=Fri, 30-Jun-2017 07:23:20 GMT"}, {"Expires", "Thu, 01 Jan 1970 00:00:00 GMT"}, {"Access-Control-Expose-Headers", "Location"}, {"Cache-Control", "no-cache, no-store, no-transform, must-revalidate"}, {"X-Content-Type-Options", "nosniff"}] iex&gt;list |&gt; Enum.at(6) |&gt; elem(1) "jetbrains.charisma.main.security.PRINCIPAL=COOKIE;Path=/youtrack;Expires=Fri, 30-Jun-2017 07:23:20 GMT" 
I'd love to see this as well, as my work has been bitten by the gRPC bug. 
All of your comments have been really helpful. Thanks. What would be really good would be if I could do an Enum.find with regex. I've tried a bit with no success. This is my attempt Enum.find(list, &amp;match?( {"Set-Cookie", "jetbrains.~r/.*/"}, &amp;1) )
http://pocketworks.co.uk are looking for a a Rails/Elixir teammate. We're a small team in Leeds, UK working on a handful of long-term products for clients. You'll need to be able to work on-site. We're investigating moving from Rails to Elixir for some new upcoming projects.
Oh tell me about it. 
Sure thing! I'm glad you enjoyed it.
What about `with`? with :miss &lt;- get_cached(storage, variation, image, path), :miss &lt;- get_remote(storage, variation, image, path), :miss &lt;- make_thumbnail(storage, variation, image, path), do: raise "it all failed, abandon ship" This gives a bit more flexibility because they don't all need to receive exactly four arguments. Although I would likely keep the same return types for simplicity. 
I can't tell if you're using a figure of speech or not, but it's really not clear to me when to use what datatype and how to do simple stuff like split a string like "key:value" into what in Ruby would be a hash.
https://github.com/c-rack/quantum-elixir is what I'm using
Figure of speech. I am a noobie as well :)
I know this isn't exactly what you're asking but just wanted to throw it out there. Slack has a /remind feature that is pretty configurable. You could set it to send a reminder message to your slack bot every day at the right time and then your bot can respond to the message. Then you could easily reconfigure the schedule if you wanted with no code change. 
I would add edeliver is based in exrm is the Elixir wrapper for relx. These libraries are tried and true and package an entire erlang release, including the runtime system.
That worked well. Thank you.
I guess you're supposed to say what you primarily spent your time doing. I mean I could claim I have a Ruby background, but realistically I only spent 1 year of the last 10 in Ruby-land. The rest were in a combination of other OO languages.
One of the examples you gave.... 500lines is not Javascript. It's Python. I just thought that was interesting since I've done Python professionally for a long time, and (a) never knew it existed and (b) after looking at it, wouldn't recommend it for any true newbie---those examples are all far beyond the level of TodoMVC, even if they're short.
You cannot test the API itself. Tests should not call an external service, since that can change at any time, and is influenced by e.g. any network issues that might currently exist. Instead, you should test how your API responds to the data it receives. You should save some API-results as snippets, and use these in your tests. Often, the snippets that are used in the documentation of the original API are a good starting point for this. If your wrapper is able to parse these snippets properly, then it will probably be correct. The most important task to do is thus to let your API wrapper switch between calling the real external API endpoint, and calling the fake local API endpoint that returns the prerecorded data. This problem is relatively common, so there exist tools like [bypass](https://github.com/pspdfkit-labs/bypass) that try handle most common cases for you.
Check out this library: https://github.com/parroty/exvcr
Whatever is out there on learnelixir.tv or learnphoenix.tv is to the point but in the end you will find that it's not comprehensive. This is because its emphasis is on concepts and there actual coding is less. But if you see learnphoenix.tv roadmap, it has plans for app-building(e.g. OTP apps). But that series has not been released yet. That's why I told wait till it gets released. Moreover, your framework of choice might not be react as you maybe into ember or angular or elm. So, let's see if what comes out. But if money is not a concern for you then definitely go for it.
On the other hand, while I've spent vastly more of my time in C than in any other language it would feel weird to claim this in the current context because C was in no way how I arrived at Elixir. I feel like I landed with Elixir after using Ruby and (to a lesser extent) F#.
Thank you for your comments. Like I mentioned I'm brand new to Elixir so I'm still grappling with conventions. It made sense for all the methods to return `DateTime` because the most powerful feature of this to be able to pipe everything. However, the `date` method is meant to be a sort of gateway where you can give it all kinds of input to get the representative `DateTime` back. Of course, the user could try to input something that cannot be parsed and so an `:error` is appropriate. How do you suggest reconciling these competing ~~conventions~~ ideas? Edit: conventions -&gt; ideas
I might use this as the base for making a Rust client for Phoenix Channels, assuming there isn't already a project out there for it. 
Microsuggestion: you can pattern-match out some of the data %{body: %{"user"=&gt;%{"displayName"=&gt;user_name}}} = OAuth2.AccessToken.get!(token, "/1/user/-/profile.json")
Just to make clear why this might be preferable: you'll get a match error instead of a weird nil error with not as much context. It's clear that the left hand side describes what "shape" the data has to be in.
Sweet! I really like that. This is one of the ways I believe that Elixir is "semi-statically typed". 
I'm on board with the first way as it seems to be convention. However, what about add/3 and subtract/3? Should these return `{:ok, DateTime.t}` as well with add!/3 and subtract!/3? What's cool about this module is you can do `Momento.date! |&gt; Momento.add(4, :days) |&gt; Momento.subtract(6, :hours)` and so on. Are you suggesting add and subtract gain the bang too? If `Momento.date!` goes through successfully then so would add and subtract. Also, FWIW I see the discrepancy on my date functions and I'm cleaning them up now. Thank you for your feedback; it is much appreciated!
Awesome - thanks for all the good starting points. 
No they don't need to. It's only useful if the function can fail (either due to inputs not being formatted correctly or more importantly a side-effect like IO or some other external dependency fails). Although with input formatting if it's something that can either be expressed through pattern matching or guards then it may be preferable to just let it crash (there's people that prefer either approach here, letting it crash vs returning an error). My view is if it's something that isn't expressed by the spec, then it should not crash (so should return an error if it could fail). But it's really just a matter of personal taste here, there isn't a right or wrong approach (crash vs error). And luckily enough whatever approach someone takes, it's recoverable anyway. 
This is exactly my thought process, amazing talk, really fired me up. 
I'm just starting with Elixir. I just implemented a small library to convert the Gregorian calendar into the fictional Darian calendar of Mars. :P [github](https://github.com/THeK3nger/darian-calendar-elixir)
A little nudge in the right direction: a hash in Ruby is akin to a Map in Elixir; it's a basic expandable key:value store. 
Cool article. Poking around on the brightball.com blog -- lots of interesting stuff there, especially some Postgres love. Very cool.
Me too
I'm using Codeship and Heroku for my pet project. They are quite easy to configure.
We use Travis CI. It also serves as CD by updating our Kubernetes cluster. 
Travis CI for me. Works like a charm, and their support is great.
Jenkins, of course. :)
We're using Jenkins with it's new Pipeline system to build debian packages. I'll probably write up a blog post explaining how we do this soon!
We're using CircleCI for most things. We used to use Travis but the ability to ssh to the Circle container is really nice. 
We're using * edib (https://github.com/edib-tool/mix-edib) to package apps into containers for use with docker. This uses exrm (https://github.com/bitwalker/exrm) under the hood * conform (https://github.com/bitwalker/conform) to handle configuration See http://carlo-colombo.github.io/2016/05/04/The-3-E-Elixir-Exrm-and-Environment-Variables/ for how to use environment variables to override the defaults set in conform (for configuring your containers).
Circle CI is so fantastic. I've used Jenkins and a little Travis, but I love Circle
Gitlab CI for me. Configurable and easy and self hosted
I use Circle Ci for erlang and elm code. Its free for non open source. You just need to have your circle.yml file install erlang and elixir for you. Their default erlang is R14 
Final Results (140 votes): * 54% Ruby * 06% Erlang * 09% Other functional language * 31% Other OO language
I've recently started using Phoenix and Turbolinks in my own projects as well, and it's really nice to know I'm not the only one. I find myself being way more productive using Phoenix's command line generators and Turbolinks just works so effectively with no effort required. My only question is whether it's worth using a JS library for small interactive components rather than using RJS? Something like Mithril or Vue just for minor Javascript updates on various pages, rather than a full single page app.
Server rendered HTML snippets over a web-socket connection is an approach I haven't encountered yet. Is there a name for that pattern?
Turbolinks is framework agnostic, much like Pjax. 
I've heard it referred to as RJS, a term which I believe originated from Rails.
Who said anything about SPA?
Obviously they didn't say react and redux are dead, but the title is a little sensationalist. React is really popular because of the rise of SPAs, so it's a reasonable critique to make. I'm interested in this though, because I'm starting a Phoenix project and would love to try it out as a back end app.
Isn't that almost always the case, that, you know, you never needed javascript to render your page?
ugh, I need to hurry up and publish my library. It's doing exactly this just macro'ing out the legwork of writing the same boilerplate JS to move the html to the front end. to think, if I wasn't lazy I could've published a library that does this for you and posted it in the comments and raked in all that sweet karma....
I was just experiencing the same feeling with my first phoenix/react app. Does anyone have any recommendations of tools that work well with turbolinks/pjax for pages that have a lot of client-side interaction? 
Seems weird Phoenix puts so much emphasis on server side rendering, doesn't that take a toll on the server? I thought the more you could do on the client the better it was for performance. 
You should look into Elixir and Erlang, how it makes concerns like that not matter. 
If it's usually that coarse-grained how does this even pay off?
If I recall correctly, the reason Phoenix templates are so fast is because it stores the parts in a linked list. So when it adds variables it's using linked list operations to join the strings instead of string array concatenation. I remember Chris Mccord talking about it somewhere.
As this is a get request you pass in parameters in the query string. Example: http://localhost:4000?potato=tomato
[Templates are functions](http://i.imgur.com/aXvNzQj.png ) that used linked lists, not string concatenation!
OK, let me be more general then: How can I from a line such as `get "/", MyController, :index` pass information to `MyController.index` in addition to the `conn` object? Would a closure be useful here?
Ah! Thanks for finding the source. I knew I read it somewhere :)
To me, this is such a nice example how Elixir is full of win: * Templates get compiled to functions * Those functions work on a simple, efficient data structure * The BEAM vm executes the functions in an efficient manner
Here's an app that uses Sigma to build out it's dashboard. https://stackshare-status.herokuapp.com/ --- Source code here: https://github.com/Leanstack/stackshare-status
The title is not at all sensationalist. It is an accurate statement of what they did: almost entirely removed React from their client code, and replaced the rendering functionality with server-side rendering w/ Phoenix.
Just finished my first elixir project. A simple slack bot written in elixir that wakes up every 30 minutes and reminds you to exercise. https://github.com/dmccraw/hans_and_franz
To be fair. The 'fast' templating systems in languages do not use strict string concatenation. For example Mako templates in Python is one of the fastest out there for Python, and it uses an in-line mutable string builder. And like all of Python, the performance sensitive parts are all using functions written in C (e.g. markup escaping, is in C). 
I think you've got it a little mixed up. Erlang and Elixir handle strings differently (what they consider as the default string type, though they can both have strings using lists or string a using binaries). In Erlang strings are lists of numbers. This is where the slowness people are talking about comes from. In Elixir strings are binaries (you can test this by calling is_binary/1 with a string, or by appending a bitstring onto the string). This is why most common string handling you'll do is quick. Not necessarily faster than another language (although becomes comparable as many languages implement strings in similar contiguous ways), but faster in the Erlang VM world when compared to Erlang. Strings made up of lists can be nice sometimes, but I think Elixir going with using binaries as default makes a lot more sense. Edit: To better explain the different between these types. For the most part you can think of a binary and bitstring as the same, a binary is just when the size of the data is made up of whole bytes (&lt;&lt;1 :: size(16)&gt;&gt; is 16 bits or 2 bytes long, so it is a binary; &lt;&lt;1 :: size(17)&gt;&gt; is 17 bits or 2 bytes plus an additional bit, so it is a bitstring). Now a binary vs a charlist. A charlist is a list of characters (integers), while a binary is a chunk of contiguous data. Why is a charlist so slow for certain operations? Well you have to understand what a list is. A list in the Erlang VM is implemented as a linked-list. So every node in the list is a different character making up the string. The problem with this is unless you're doing operations that are very fast for the list. Such as prepending a character to the list. Then other operations are quite slow. Some slow operations are obtaining the length of the string (it has to iterate until it reaches the end of the list; now this might not seem slow because that's the same algorithmic complexity O(n) as would be needed by a binary string, but the problem lies in the non-contiguous layout of memory). Why is memory layout important? This falls down to a more lower level detail than Erlang or Elixir would care for you to concern yourself with, but when CPUs are operating on data to speed up repeated data access they will cache (unless instructed not to) the data (to improve the efficiency of caching they go about it by storing a chunk of contiguous data to the data that is accessed into cache, this known as a cache line; these lines are often 64 bytes wide; optimizing data layout for this is known as spatial locality). Now why do we generally want to operate on data that is in the cache? Because it's orders of magnitudes faster than accessing it in RAM. So one thing developers strive to avoid is a cache miss (this is when the data being nice accessed is not in cache and so it has to access it from main memory). They optimize for this by optimising their data layout for spatial locality (trying to make sure the important data will fit in as few cache lines as possible), and optimising their data access/algorithms for temporality (when they know that data will be in cache try to do all of the important operations on it there instead of operating on other data where this data may no longer be in the cache). Now one exception to this (though this isn't important in the context of charlists vs binary strings) is if your algorithm operates on the data where it would not require it to be stored into cache (an example of this is write once and not reading), in these cases CPU architectures often include instructions or hints that do not do a temporal cache store or load. Ok now depending on what languages you've had experience with in the past the above might not be completely obvious how it relates back to charlists vs binary strings. We can take a look at a pseudo memory layout of a charlist vs a binary string. Note: the value on the left is the memory address, and values after are the bytes at that location. Let's first define what a typical list layout may look like: * N = byte making up the pointer (or address) that indicates where the next node in the list can be found. Let's just say addresses are 16 bits. * C = byte making up a single character. For simplicity we'll say it's 8 bits. In practice binary strings can use UTF-8 encoding (space saving form), while lists are probably storing a pointer (albeit likely a tagged pointer in case of an integer; but can be thought of as storing at least a whole UTF-32). Charlist: 0x0101: C N N 0x0303: C N N 0x0505: C N N Or actual data 0x0101: 0x61 0x0505 0x0303: 0x63 0x0000 0x0505: 0x62 0x0303 So assuming the head of the list is at 0x0101. This means that to read each character of the string ("abc"), it will store one cache line for reading from 0x0101, then another for reading from 0x0505, and then another for reading from 0x0303. That's not optimal for spatial locality at all, every character is a cache miss when we're first iterating through the list. Now the Erlang VMs allocator may try align the nodes up together in memory for better spatial locality, but having to store extra data at each node (the address to the next node; is reducing the amount of characters that will fit into a wiggle cache line). Now compare that to a binary string: 0x0101: C C C Or actual data 0x0101: 0x61 0x62 0x63 Reading the first character will bring in the other two characters into cache as they got within the same cache line. This is optimal for spatial locality. Now that's a very basic overview of it. But hopefully should make it a bit clearer what's going on. There's a few additional things to note about the Erlang VMs implementation of lists. It doesn't cache the length. So calling length will re-iterate the entire list everytime. While I believe binaries keep their size (so would be constant time to lookup). Also hopefully there's no mistakes. Writing this on a phone. 
So does Phoenix's template speed come purely from Elixir's string speed? I notice that Phoenix's view rendering is much faster than Rails', but I was under the impression there was something more to it, than just string speed.
Yep. I believe there's some additional information associated with it. Though can't recall what exactly. 
Original Mail List http://erlang.org/pipermail/erlang-questions/2009-September/046384.html James Hague wrote: &gt; I'm puzzling over some details in section 4.1 of the Efficiency Guide, &gt; "How binaries are implemented." &gt; &gt; When a greater than 64 byte binary is created, both a RefC binary (on &gt; the global heap) and a ProcBin (on the process heap) come into &gt; existence. Also, any time a smaller binary is matched out of that &gt; binary, a sub binary is created. According to the docs: &gt; &gt; "All ProcBin objects in a process are part of a linked list, so that &gt; the garbage collector can keep track of them and decrement the &gt; reference counters in the binary when a ProcBin disappears. " &gt; I think the terminology is a bit confusing and Björn has to correct me if I'm wrong here. A ProcBin is the (Eterm) header that points to the reference counted offheap binary, the refc binary. &gt; Is a sub-binary essentially the same as a ProcBin, in that it gets &gt; tracked by being added to a linked list? No. Only ProcBin binaries headers are included in the list which points to the RefC binary. The list is there for performance. Instead of scanning the heap for unreferenced ProcBins after a gc, only the list has to be traversed. Also, the sub-binaries are process bound. If you send a binary, which is a sub-binary, to another process a new ProcBin is created for that process which points to the refc binary and reference counter is increased. &gt; &gt; I'd also be interested in more details about when the reference &gt; counter for RefC binaries gets incremented and decremented. &gt; The counter is in direct correlation to the number of ProcBins (Eterm headers) to that offheap binary. When the unreferenced ProcBins gets garbage collected the reference counters are decreased and when new ProcBins are created to the binary the reference counter is increased. Also, drivers may increase and decrease the reference counter for an offheap binary. Sub-binaries will point to the ProcBins and keep them alive until they themselves become dead and collected. &gt; I'm trying to get a handle on if extreme use of sub binaries is a bad &gt; case for the emulator. Imagine an XML parser that keeps the entire &gt; document in a big binary, then has tens of thousands of sub binaries, &gt; which are really just pointers and lengths into the master binary. It &gt; sounds like it would be cheap, but is the runtime designed for that? &gt; Or would it be exploiting binaries in ways that cause additional &gt; expense? Yes, it is cheap and the runtime system is designed for it. Additional expense? No, but some words of caution. A subbin will reference its master so its not only the subpart of the binary that is live, its the whole binary. If subbins or procbins survives to the old_heap (generational heap) the binary might be more longlived than intended and more expensive in terms of memory. Explicit calls erlang:garbage_collect/0|1, which will do a fullsweep of the process, will remedy this. This is a drawback which is being studied to improve current gc strategies. Regards, Björn-Egil Erlang/OTP
There are three important reasons why The Phoenix Framework is fast compared to Rails. 1. EEx, Elixir's templating engine, can compile an eex file into an elixir function with the power of macros. So instead of reading in a file and creating an internal data structure for the template the template becomes part of the Erlang binary and optimized by the Erlang VM. 2. Appending strings in Elixir is a O(1) operation. When you append a binary in Erlang(Elixir) there is no copying of either binary to create a new string. Since both binaries are immutable a Linked List reference can be created to construct the full string. 3. The third reason is the way that Erlang writes data to a file descriptor. Often times with large strings Erlang will use the writev sys call. Which can drastically improve throughput performance. The writev command also plays off number 2 because strings are immutable there is no need to copy the data as it can be directly read into the write buffer. I use strings and binaries interchangeably, but that is an over simplification for the purpose of explaining things.
Or one of the elixir images built on alpine. These are good ones: https://github.com/msaraiva/alpine-erlang/blob/master/README.md Roughly 25mb vs I think 800 for the official elixir image. 
I a big fan of Alpine for containers! For a real-world use case, it's definitely worth it to keep the sizes down, as bandwidth and disk aren't free (and at times, not even cheap!). For the sake of the tutorial, I figured that centos / ubuntu would be more familiar for most readers. I do wish I had pointed out that Alpine is a good thing to check out, though!
Is there a downside to placing the BEAM in a container?
I'm curious, any reason for not using flask again?
Not that I'm aware of, but I'll admit that my experience of running BEAM VMs in production is limited. That said, containers effectively piggy-back off the host kernel, so there shouldnt really be any appreciable difference. 
When you submit data through the form, this data is often nested. For example, when creating a user, instead of sending its name as "name", you would send it as "user[name]" so all the user parameters is inside the "user" key in your controller. The `:as` option, in this case, is "user". It tells the key you are nesting under. If you feel the docs could be improved please send a pull request.
`broadcast` sends an event to all clients that are in the channel that you're handling right now `broadcast_from` does the same as `broadcast` but does not send to the client that sent you the message you're handling right now `push` only sends the message to the client you pass it
Hope this explains better: https://github.com/phoenixframework/phoenix_html/pull/100
Yeah. Good that you also noted the Endpoint thing because that is necessary to send events that happen "outside" of the channel GenServer to specific topics.
I tried a guard the other day that wouldn't work that is similar. I'd love to know why too. when !is_number(x) never worked for me. 
Do you have a way to capture emails? I'd love to be notified once it's live. 
I do not but that's actually a great idea. I'll see about getting a simple front end built with Phoenix for posting updates and such. Thanks!
The VM restricts what can and cannot be in a guard; you can't call arbitrary functions. [Here](http://elixir-lang.org/getting-started/case-cond-and-if.html#expressions-in-guard-clauses) is a description of what is allowed (which does include `not`).
Elixir is great, but it's not a silver bullet. I would not recommend the actor model for something like games. It's amazing for durability and concurrency. It's not good for low latency needs.
Depends on what you mean by game engine. Elixir/Erlang absolutely used for game servers with huge success. For the client, not so much. Check out this awesome talk from a guy building MMORPG servers with Elixir (very informative on several levels): https://youtu.be/_i6n-eWiVn4?list=PLWbHc_FXPo2h0sJW6X2RZDtT1ndw6KKpQ
What would you recommend instead? 
As I understand there is no boolean type in elixir, but 'special' atoms. And `! &amp;&amp; ||` operators are not "strict" meaning they will convert any of it's arguments to true or false, while `not and or` are strict operators and works only with booleans(:true and :false atoms). Every guard function guarantees to return boolean. Maybe that's why its disallowed to use non-strict operators, because of type casting of any truthy / falsy value to true and false, potentially on every function call will reduce perfomance(of function dispatching or something else..). Just a guess though :)
If a sleeping robot who lives in a place called slack told me to exercise, I'd give him a piece of my mind! Not that it takes anything away from the coolness of your project.
There still are, they are for list comprehensions. Example from my current project: ``` &lt;%= for xp &lt;- @machine_xps do %&gt; ... &lt;% end %&gt; ```
You're right indeed - seems I've acquired a Python induced blind spot for the things
Not the OC but in the new crop of languages I think rust looks fairly decent for game engines, though the tooling around it for doing such isn't as mature as the stalwart that is C++ (which Cryengine and Unreal are predominantly coded in) .
Try: `when not is_number(x)`
Cheers, I'll be ordering those books in short order. I've done some research on the OTP and Erlang so I've a brief understanding of the foundation of Elixir.
They're also used in `with`. with {:ok, result} &lt;- Enum.fetch([1, 2, 3], 1) do result + 1 end
The best way to think about processes is to think about how things fail. If one user's interaction should cause an error and fail, what should it bring down with it? If it makes sense for all 10,000 user states to fail just because one does, then one process makes sense, but this seems like a generally bad idea. "Objects" (OOP) however, are usually a bit too granular for processes. It often makes sense for a single process to hold onto the state of several linked objects at once. A user's state may include many data structures which would be represented as separate objects in another language, but all of that data is so tightly linked that it should all fail at once. As for your example, 10,000 user processes for 10,000 users is exactly the sort of thing that you should be doing. In fact, 10,000 user connections might spin up 10,000 supervisors which each monitor several processes for each user connection, maybe creating 40,000 processes.
I know it's kind of a one liner with `System.cmd`, but I wanted a nice package with a readable API. :) Hex is so easy to use so I thought why not make an open source package for anyone who wants it. Usage: def deps do [{:curltime, "~&gt; 1.0"}] end Curltime.time_namelookup("http://reddit.com") Curltime.time_connect("http://reddit.com") Curltime.time_appconnect("http://reddit.com") Curltime.time_pretransfer("http://reddit.com") Curltime.time_redirect("http://reddit.com") Curltime.time_starttransfer("http://reddit.com") Curltime.time_total("http://reddit.com")
When would you use RoR over Phoenix? I too have used Rails since the 1.x days, and I can't see myself going back except for the rare cases where someone has done considerable work for me already. I can prototype something just as quickly in Phoenix, and Elixir is even more expressive than Ruby.
There is an application process which is at the root of the application's supervision tree and it spawns what are usually a few high-level processes to get things going. A 'users supervisor' would make sense at this level. That users supervisor might spawn a supervisor for each user which needs to be active in the system. Each supervisor shouldn't be doing anything interesting, it should just start its supervised processes with parameters which don't become stale (e.g. a user_id) and let them handle getting their persisted data themselves (e.g. in `init`). This way, restarted user processes will go grab the latest available persisted data.
Coming from an OOP background and getting familiar to Elixir, this is an interesting topic. Can you comment a bit more how things would work in detail? It would help me to see how things fit together on a high level after looking at stuff like processes in isolation. Specifically: * How would those processes be spun up/wound down? * What kind of messages would be exchanged? * Does this make sense in the context of a REST API implemented in Elixir, or are we talking about using Erlang's distributed process communication features directly thus doing away with any REST API? I'm a little confused how the actors would be wired up to the HTTP session.
This is a huge topic and the details vary depending on what you are doing. I would recommend "Elixir in Action" (first book in the sidebar) as having the best OTP introduction of any beginner book and walks you through doing all of this in a couple of different ways. If you are new and wanting to make a REST API, I would highly recommend using and eventually digging into the code of [Plug](https://github.com/elixir-lang/plug).
Ah, indeed. You made me realize I was engaging in premature optimization. I really appreciate the help, thanks!
I was recently hired as an Elixir/Phoenix developer, and so far the only one in the company who has experience with Phoenix. In August - September, we will migrate the entire product from Rails to Phoenix. It will be huge, and pretty exciting to say the least. I didn't have experience with it before I got hired, but I quickly learned on the job. I love it, and so does my team. 
In general, yes, but here it doesn't make a difference. You're right that it's "O(n) complexity", but the "n" here is the length of the lefthand operand: X ++ Y involves reversing X then cons'ing to Y, so is O(length(X)). And in OP's code, X is one element long -- so | will be a tiny bit faster than ++ because the latter has an extra step (pointlessly reversing a one-element list), but nothing noticeable. So if OP finds the ++ version easier to read, I say go for it, as long as s/he understands that ++ will only be efficient if the lefthand operand is small, as it is here.
*waving* if you plan to hire remote Erlang/Elixir devs :)
In Elixir, lists are immutable linked lists. The immutable bit means that you can't change the data in-memory, you have to make new data instead. Linked lists means that a list isn't necessarily a block of memory with everything sitting right there, instead it is a bunch of one-element data structures which each hold a pointer to the next element (or to the empty list). One of these data structures has three parts, a little bit of meta-data that says "this is a list element", a pointer to the data that this element holds, and a pointer to the next element of the list. The upside to this is 'adding' or 'removing' an element on the head of a list doesn't even touch the tail of the list. You can even have two lists which converge on the same data in the tail. Also, `[head | tail]` is essentially the memory representation of making a new list element. Concating two lists of this sort together (`++`) involves the steps of walking to the end of the left list and for each element in that list, making a new `[head | tail]` which points to the element and the (growing) right list. Ideomatic Elixir code is going to prefer `[head | tail]` so this is what you will usually see if you read the code of others. `++` is still useful and does come up, but it is the less preferred option when there is a choice.
You don't have to spin up 10,000 user actors because you have 10,000 users in your system, but be prepared to do so. So, when interacting with a user model, first resolve the appropriate actor: if it is not running, start a new instance of the actor, and initialise it with the loaded model. You can also set a timeout for messages such that after a period with no messages, it can be unloaded. I use this approach, and have a single supervisor monitoring templated agents.
You are motivating me to play more with elixir! Thanks.
wow, very cool... not sure if I'll use it, but hey! really cool!
Thank you!
Very good question, this approach feels dirty to me, but that doesn't mean it isn't the best way to do it./ Here's an article that appears to be solving the same problem http://blog.plataformatec.com.br/2015/10/mocks-and-explicit-contracts/
Thanks! Partly inspired by looking at pact after you were interviewed on full stack radio. 
Haha yes, definitely a dirty meta programming hack! But a fun exercise in learning just how flexible elixir is. The style used in the article is what I use in real apps. It handles the 99% use case of one configuration for each mix env.
X-Post referenced from /r/erlang by /u/elbrujohalcon [BeamBA 2016 - Erlang, Elixir, Efene &amp; LFE Meetup @ Buenos Aires - CFP is open!](https://www.reddit.com/r/erlang/comments/4tr0qp/beamba_2016_erlang_elixir_efene_lfe_meetup_buenos/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
When you say sync'd up, what does that mean? I thought those conferences would be in person, not digital.
The start and finish times are synced, so that if you want to switch tracks then you won't miss anything
your assumption is correct. there will be different talks in different rooms all lasting (roughly) the same amount of time. between talks you can change rooms to "switch tracks," so you can choose which talk you want to see most for each timeslot of the day.
Thank you that makes sense. I think I'll go this year. I love Elixir and want to invest in the language and general ecosystem.
Makes sense! Thank you!
As others have said, think about this design decision before making it. However, technically, this is fairly straightforward to do. You can make a function that will first try to find your process (either by name or using `Supervisor.which_children` for dynamic spawning). When it doesnt find a process it starts one and returns the pid. That way your controllers always call that function to get the pid and use it to get the data. Then as others have mentioned, use a timeout to check in on (itself) every 5 mins or so to decide when to exit.
Its just a tradeoff of simplicity vs maintainability. If you think you need it, do it, but be sure its necessary. If its still plenty fast enough to just go to the database each time you need data, that may be good enough for your mvp, then add this technique later once you've scaled. On the other hand, if you're trying to build an instance based type of game, it may make perfect sense to do so if you wanted to keep all the data out of the database all together. This is a typical pattern in Erlang, Joe Armstrong advocates not using databases at all.
Thanks. I'm actually just doing to learn more about design patterns in Erlang/Elixir. I find them difficult to learn about. Does Joe Armstrong talk about it in his book or somewhere else?
I would say you'd return a tuple, i.e. {:ok, data}, when there's a possibility that you might not get what you asked for. For example, I wouldn't return a tuple for a function called 'get_random_number()'. You wouldn't expect this function to raise or return an error. However, a function like 'save_data_to_db()' might realistically fail due to a number of possible reasons (invalid data, couldn't connect to db, etc), hence returning a tuple indicating whether the desired result was achieved.
The recoverable? function is super dense - I'd avoid nested pipes and try to break it up in to smaller pieces of functionality... something like this sloppy pseudocode: @spec recoverable?([op()]) :: boolean() def recoverable?(history) do # for all read operations... history |&gt; Enum.with_index |&gt; get_read_ops |&gt; Enum.all? &amp;(recoverable?(&amp;1, history)) end def recoverable?({{ta, :read, data}, idx} = stuff, history) do # check if this TA reads from another TA that has not aborted before... history |&gt; Enum.take(idx) |&gt; Enum.with_index |&gt; get_write_ops |&gt; Enum.filter_map(fn other_stuff -&gt; thingy?(stuff, other_stuff, history) end, &amp;(elem(elem(&amp;1, 0), 0))) |&gt; Enum.all?(fn ota -&gt; # ...and if so, wether the other TA commits before this TA does. i_dont_know_what_to_name_this(history, ota) &lt;= i_dont_know_what_to_name_this(history, ta) end) end def i_dont_know_what_to_name_this(history, ta) do commit_index(history, ta) || abort_index(history, ta) || Enum.count(history) end def thingy?({{ta, _, data}, idx}, {{tta, :write, tdata}, tidx}, history) do tta !== ta and tdata === data and not ta_aborts?(history, tta, tidx..idx) end 
This would definitely work; I was just trying to do it the Actor Way^TM. :-)
Yep, ets is a fine solution for that kind of problem if it is only a single app and you do not intend to do a multinode
ETS seems to be in-memory, not persistent.
 1&gt; :ets.new(foo, [named_table]). foo 2&gt; :ets.tab2file(foo, "foo.ets"). ok 3&gt;:ets.delete(foo). true 4&gt; :ets.file2tab("foo.ets"). {ok,foo} Write simple supervisor that will save your table as you go and you ready to go.
I'm looking for a SQLite equivalent KV store. PostgreSQL needs a db to be set up, deployment &amp; distribution is more complex. But it's a good option if you already have a PostgreSQL running.
Good suggestion, apparently mnesia using DETS might work. I'm now watching https://vimeo.com/132692834 to find out about the limitations.
From that presentation: DETS’ problems ● All accesses require I/O. Slow. ● A table is limited to 2GB. Larger tables must be split into sub-2GB fragments. Needs monitoring. ● Unclean shutdown causes slow “repair” phase during startup. Orderly shutdowns also slow. 
Would this work? https://github.com/vanillahsu/lmdb I'm impressed by what I read about LMDB's simplicity, speed and especially [reliability](https://blog.acolyer.org/2016/02/11/fs-not-equal/), but I can't say I've used it. https://symas.com/products/lightning-memory-mapped-database/
The best would be mnesia_ext + LMDB or mnesia_ext + Bitcask... if it existed.Sight. LevelDB has known reliability problems. 
Are you using Riak data types?
Have you made sure to update your package.json and rerun npm install and brunch build?
Yeah, but package.json pulls the stuff related to my issue from the file, not the web. "dependencies": { "phoenix": "file:deps/phoenix", "phoenix_html": "file:deps/phoenix_html" },
It sounds like you weren't looking for an AP database in the first place. Riak also does indeed clean up after itself. There's lots of log spew about it.
You currently defining send out of scope from where you calling it. Read this: http://elixir-lang.readthedocs.io/en/latest/technical/scoping.html Hope, it can help you. And try not to use send as function name, because send is a elixir special word. 
Ahhh thank you for sharing that! Taking a look now. And thanks for the advice about send. :] 
In your example you defined send out of any scope 
self's output works for me as is, but I'm quite possibly not understanding your question :-) send self(), "amazing" flush() #=&gt; "amazing" :ok 
Ah I see, very interesting. Thanks guys! :)
The reason is that close terminates the process you are running your code in. It terminates the process because you called start_link in connect so your process got linked to the started process. Linked processes get terminated "bubble up". Therefore result you see is a termination signal and not function result. 
&gt;Ooooh I thought that it terminated the process related to the pid I sent it. What's the point of the sent PID then? Yes is does (exit). However because you've started the process with start_link, as the name implies it was "started" and "linked" to the calling process (your process). The sent pid is a kind of ID of the process (if you'd want to send it further messages). &gt;Also, on a side note, do you have any advice as to how I might go about closing the connection to the database, if not that way? I was gonna say that we can discuss this code further when you're ready because this is not the way to go about what you're trying to do. 
If you mean a @foobar attribute, those should only exist at compile time and the compiler has inlined the values before the program even runs, I think. Being very new to elixir myself, ~~I unfortunately have no better idea for you, but afaik, module Attributs cannot be used to store data that only exists at runtime.~~ EDIT: Maybe registering those agents in the global namespace could help you, if those counters are truly global counters and should be available from everywhere: https://www.amberbit.com/blog/2016/5/13/process-name-registration-in-elixir/
It's always good to share ones learning process with others, so first: good job :) I though of a way to improve the API for this module though. By breaking up the process of performing the curl request and selecting a measurement, it would be possible for consumers of your module to receive multiple measurements from the **same** request. If you don't mind, I would suggest an API like this: ``` Curltime.request("http://reddit.com") |&gt; Curltime.time_namelookup() ``` This would require `request` to return a %Map containing all the measurements from curl's output. Using that result, one could either grab a value directly from that map (which would require assumptions about the internal working of the module, which is not desired) or use the provided helper functions that return one of the internal keys. What do you think, /u/randomelixirdevdude? Cheers!
If you have your counter's pid, you can call `Process.register/2` as described here: http://elixir-lang.org/docs/stable/elixir/Process.html#register/2
I imagine that, for starters, I should be spawning a new separate process for server requests that involve pulling data from the database
is there a specific reason why you are not using ECTO (https://github.com/elixir-ecto/ecto)?
When learning new technologies I generally attempt to start with less abstraction and then add abstraction when I feel ready. Does ECTO abstract away the problem I'm dealing with? 
I normally just turn to the tests when I get stuck on ecto - here are the ones for DISTINCT on MySql: https://github.com/elixir-ecto/ecto/blob/master/test/ecto/adapters/mysql_test.exs#L87
This, by the way, is the same thing IEX.Helpers does with its `pid/1` (available in the IEX shell, /u/ClemDev): https://github.com/elixir-lang/elixir/blob/v1.3.2/lib/iex/lib/iex/helpers.ex#L640
Local to a hosted cluster. It's what's in use elsewhere and an exemption for Postgres was not possible. I too prefer Postgres. 
Thanks. I'll look into those flags more. 
I am looking for Elixir work on an appealing-looking open source project (obviously I have a [github](https://github.com/pmarreck/)). So far I have only [tinkered](https://github.com/pmarreck/elixir-snippets/). I will work for free for the time being, just want to get my hands dirty. Willing to commit X hours a day/week (negotiable), pair-program (like over google hangouts), do code reviews, help with unit testing etc. etc. (although I prefer TDD). My other interests include music (mostly electronic, but many genres), gaming, any space where technology has high impact potential (thinking old and crusty institutions such as healthcare and governmental stuff), cryptography, Bitcoin (and other decentralized tech), and 3D printing, among other things.
Plug.Static.Index.Html Serves static `index.html` pages for requests to paths without a filename in Phoenix / Plug applications https://github.com/mbuhot/plug_static_index_html https://hexdocs.pm/plug_static_index_html/readme.html https://hex.pm/packages/plug_static_index_html
It isn't functional. You should really think hard about what you are trying to do here and why you are trying to carry so much mutable state around inside the code. If you really need to have counters like this I would recommend you take a closer look at ets tables.
There is a lot to know here. Perhaps you could start with on one the elixir books? I'm happy to help however there is a massive amount of background info you'd be missing. 
When you call `start_link`, you are linking the postgrex connection process to the process running your code. This means that, if the postgrex process terminates with any reason that is not `:normal`, it will cause the link to break and your process to crash too. In your particular example, you are using `Process.exit(pid, :exit)` to terminate the pid with reason `:exit`. Since `:exit` is not `:normal`, that leads your process to crash with reason "(EXIT from #PID&lt;0.190.0&gt;) :exit". How to fix this? Since a connection is a GenServer, you can use `GenServer.stop(pid, :normal)` to terminate it synchronously (`Process.exit` is asynchronous).
When I upgraded I personally had to dump the node_modules directory and run `npm cache clean` to make sure that when I did `npm install` again I was getting the newly fetched 1.2.0 phoenix JS file.
That's a less convoluted and to the point version of what I meant. Good stuff from the creator of Elixir.
Nope. Something needs to tell the compiler where to find httpoision, and since there's no 'global' package directory... you're bound to using mix for connivence sake.
Thanks, that makes sense.
This was my first thought on how to do this as well.
No, I'll try later
Yes, the whole conversation was excellent! I just wanted to add a a summarized view of what is happening. :D
OK, I'll report it.
thanks, i wound up doing it this way. appreciate it!
 A couple things actually. 1. A simple chat server [in Elixir](https://github.com/KaySackey/magpie-server), and chat client in [ES2015 javascript](https://github.com/KaySackey/magpie-client). Both at the beta level right now, but I'm going to start working on them again to bring it up to snuff. 2. An image server that resizes images on the fly. 3. An S3 interface that uses AWS v2 signing. The ones that exist now all use v4, which doesn't work with Ceph, RiakCS or any of the providers who only mime an AWS api. 4. A new router. It's agnostic to Phoenix, but works well within it. It lets you apply constraints to routes in the same way Django does with regex. 5. A templating framework in Elixir. Not that EEX needs any real competition, but ever since I started using React I've been intrigued with the idea of doing something similar in Elixir for server-side rendering. This project is the least far. ... and other stuff, but this is all my Elixir work.
I'm not sure about nerves, but I did get a few issues upgrading to 1.3 with non-nerves projects. Try running `mix clean --all` to get all build artifacts cleaned up.
No dice. Thanks tho.
While the example explains the approach, this is susceptible to a brute force attack where you just try different tokens until they work. To further improve the security, the author shold include the `userId` in the magic URL and make sure it matches with the `token`. Some sort of rate limiting could work as well.
I haven't written the post as a final solution for the authentication. The `userId` in the URL is a good solution to avoid brute force attack. Another useful complement is setting an expiration time for the token as I said at the end of the post.
sweeet, I just started exercism.io with elixir
Asking how `String.t` is different from `String` suggests a subtle misunderstanding of how Elixir works. **`String` is in no way a type––it's just a module, which is a bundle of functions that operate on data.** In fact, `String` is really just a shortcut special kind of atom that you can call functions on. Elixir and Erlang don't let you define custom types–you're bound to the primitives that the VM supports. The typing engine only recognizes these so called "basic types". However, you can define composite types, using what are called "type literals", modelling the types and shapes of your data structures with basic types and containers (lists, tuples, maps, bitstrings). For example, the data structure you think of as a keyword list could be summarized as `[atom: anything, ...]`. Remember that the key-colon-value syntax inside lists is just sugar for two-tuples, so more accurately that would be `[ {atom, anything} ... ]`. If we were to write our own typespec it'd look a lot like that: @type keyword_t :: [ { atom(), any() } ] Elixir pre-defines typespecs for common data structures like these. Those are called "built-in types", but they're really shortcuts that can be broken down into basic types arranged in containers. Elixir offers a lot of niceties over erlang by baking in a lot of powerful composite types to the language. However, it doesn't want to necessarily expose those internals in the type system. So, it lets you declare "opaque" types that others can use but not peek in to. We could define our keyword type as opaque like so: @opaque keyword_t :: [ { atom(), any() } ] Elixir already does this for keyword lists, and by convention stores them in the module most suited for their manipulation with the name `t`. So somewhere in the Elixir codebase, you can see: defmodule Keyword do #... @opaque t :: [ { atom(), any() } ] #... end (Edit: it's [here](https://github.com/elixir-lang/elixir/blob/master/lib/elixir/lib/keyword.ex#L33-L37), and not even opaque) As per scoping rules, inside the Keyword module you can just refer to that type as `t`, outside of it you have to preface it with `Keyword.t`. You are encouraged, when making your own composite types with `defstruct`, to define your own type `t` using special typespec syntax for structs. But remember that's all shorthand for a bunch of primitive types and shapes: structs are just maps that have a module name inside the key `:__struct__`, and module names are really just atoms. TLDR; modules aren't types, custom types don't exist but composite ones do, everything else is just the logical application of typespec definitions, scoping rules, and macros. The [typespec docs](http://elixir-lang.org/docs/stable/elixir/typespecs.html#types-and-their-syntax) are great but there's a little more going on behind the scenes.
Thanks for the explanation. 
I just first want to say if you can't find docs for whatever you're looking for under Elixir. Try looking them up for Erlang (as it's likely it'll either be in their docs, or mailing list, or stack overflow/somewhere else on the net). The official Erlang docs cover all this stuff regarding distributed Erlang, inet/networking, etc. Anyway, as far as how to connect there's a few ways you can go about it. The first is using distributed nodes. You basically just name a node, provide a cookie (nodes can only connect to other nodes if they share the same cookie; note it's not really for security but rather just to limit the connections between the network, so you don't have every node connected to each other but rather only the nodes that need to be connected together be connected). The second approach is to use SSH, for an actual secure connection that would be the better way to go for what you're intending to do. An example going about it using the named node approach. If the network is all on the same machine you can just do the following: First terminal: pc$ iex --sname server --cookie cookie iex(server@pc)1&gt; node :server@pc iex(server@pc)2&gt; Node.get_cookie :cookie Second terminal: pc$ iex --sname client --cookie cookie --remsh server@pc iex(server@pc)1&gt; defmodule Test do ...(server@pc)1&gt; def test, do: 1 ...(server@pc)1&gt; end That should successfully connect "client" to "server". You should be able to go back to the first terminal and call Test.test and it should return 1. To connect to a remote machine you will instead have to use the --name flag so you can specify the full name (it could use ip or domain). First Terminal (192.168.1.2): pc1$ iex --name remote@192.168.1.2 --cookie cookie Second terminal (192.168.1.3): pc2$ iex --name client@192.168.1.3 --cookie cookie --remsh remote@192.168.1.2 If it fails you can try pinging the remote node to see if you can actually access it. e.g. `:net_adm.ping(:"remote@192.168.1.2")`. If you get `:pang` as a result it's possible your machines aren't allowed to actually connect to each other (possibly a firewall or some other configuration problem). The other approach is to setup an SSH daemon (what I'd recommend for your actual intended usage). Basically setup your SSH keys like you typically would, then in your remote you'll call `:ssh.start` and `:ssh.daemon(port)` (note: if you're using a folder other than your system's default SSH directories, you can specify different ones by calling :ssh.daemon/2 and passing in the options to override that behaviour, you can override a lot of other stuff too). Then just simply ssh into it from your other machine and it should automatically connect you to that Erlang shell. To connect to an elixir shell I don't think IEx or IEx.CLI actually implements the behaviour of :ssh_channel there might be a library that's done that already, I haven't actually checked. If there isn't anything available for that, you may have to make your own implementation of the callback and then pass that as the :ssh_cli option. EDIT: Ok just tried playing around with SSH now and you can use an Elixir shell by customising the shell option when creating the daemon. pc$ iex iex(1)&gt; :ssh.start :ok iex(2)&gt; :ssh.daemon(12345, [{ :shell, { IEx, :start, [] } }]) {:ok, #PID&lt;0.94.0&gt;} Then connecting to the server will use IEx instead of the Erlang shell. 
Can you provide a link to Telegraph? I couldn't find a link to it?
Yep. I did say error monitoring. Just thought it would be helpful to share as New Relic does error monitoring as well and the OP mentioned it.
Telegraf, I misspelled it. https://influxdata.com/time-series-platform/telegraf/
Thanks! Just hooked this up for error monitoring, though the performance metrics they offer is in a package that is very expensive. :/
Thanks! I'll look into this. I may have to implement something along these lines, but I was looking for something a bit more packageable.
Yes it's a bit more of a hassle than something like New Relic. But it wasn't too bad to set up to be honest :) And very flexible.
Where did ETS fall short since you needed Redis for this?
Not yet. I'm probably 80% done, I just need time to solve some issues with forwarding to other routers and its okay. My problem is with how Phoenix rewrites the Plug's request context before forwarding, and how to emulate that in my system. ---- So far what I have is. Constraints set as URL parameters, that accept a function the render get "/hello/&lt;int:pk&gt;/", View.hello_world, name: :hello URL routes that accept a do block as a render In the 'do' block, the variable 'name' is automatically introduced into local scope. get "/path/&lt;alphanum:name&gt;", name: :test_path, do send_resp(conn, 200, "Hello #{name}") end Arbitrary constraints on a URL. The above can be checked at compile time to make sure there isn't any overlap, but the ones below cannot. # This route ensures that the parameter :test is always &gt; 0 before :hello_world is called get "/rule/:test", View.hello_world, rules: [pk: fn(x) -&gt; x &gt; 0 end] Constraints always convert their URL parameter into an Elixir data structure, which means that you can use your own data structures. # Set up a validator validate :album, using: ModelParser, via: {Ceres.Album,:slug} # View.hello_world is going to receive a parameter named :album that is a Ceres.Album get "/hello/&lt;album:album&gt;/", View.hello_world, name: :hello Additionally .... and this is a big break from Phoenix, you don't ask for paths in the same way that Rails does. Instead of `album_path('example')`, you write `path(album_example)` and every URL has to essentially be given a unique name. It's configuration over convention, ala Django which makes a bucket-load of sense to me since it means (a) you can override your routes more easily when sharing apps and (b) you *always* use the same function to get the path. The path helper, and route macro work like they do in Phoenix---in that compile time guarantees ensure that you are doing things correctly. Last thing.... the view functions arguments can be set to take either a map of parameters (like in Phoenix), or just have the correct number of parameters (did you know you can guarantee this at compile time? Yes you can!). So a view can either be: def some_view(conn, %{site_id: site_id, album_id: album_id} _params), do ...... end or def some_view(conn, site_id, album_id), do. ... end I can tell you which one is shorter. 
For one, I don't believe ETS publishes events when keys expire like Redis does with keyspace notifications. There's a non-trivial amount of overhead in getting ETS working across a distributed network of nodes. The benefits of removing a Redis dependency definitely do not outweigh the cost of having to switch to some sort of distributed Mnesia setup.
It's a single Redis instance, and on the BEAM side there's no node management in place. It seems unnecessary to replicate logic in Erlang/Elixir for a key-value store with expiring keys and notification events when what Redis provides is already sufficient. It smells a little bit like "not invented here" syndrome. This whole setup was only a couple hours of work. I'm using a very reliable third-party Redis provider, and therefore have no need to build my own management infrastructure for it at this point. It's already a part of our infrastructure, as well, because it facilitates communication with some other non-Erlang systems. 
Nice work! Sounds like there won't be much of Phoenix left once you replace the Router and templating engine :)
Ok, so I have a stupid follow-up question. What exactly is the difference between a struct and a type? Where should you use one vs the other? defmodule User do defstruct name: nil, age: nil @type t :: %User{name: String.t, age: Integer} end What is that ```@type``` line actually doing? What does it allow me to do in my code? 
That's what I thought too when I started, but as I begun digging into the internals of it all I've come to realise that Phoenix is doing *a lot*, and has a quite a lot of APIs that are only in the docs---and not any of the guides that are on the site (or any other site I've actually seen). The guy behind has a pretty good eye for specifications, however, a lot of the work is focused on the channels. This makes sense since it's what drives people to hear about Phoenix in the first place (I know it's what first attracted me), but it means a lot of polish has been left out for more traditional request/response work. That and... Plug is a pretty well qualified specification to build on, leaving little room to get 'creative' with your solution and do alot of NIH work.
Working on [exfacebook](https://github.com/oivoodoo/exfacebook) . it's graph api implementation but it supports batch requests and pagination. if you used in Ruby koala gem it's using the same approach `get_object`, `get_connections`, `put_connections` to interact with Facebook API. I've decided to make separate library because of using it as part of aggregator of musicfeed's in my project [musicfeed](https://itunes.apple.com/us/app/musicfeed-discover-new-music/id953895783) . 
It's documentation, but it also allows a tool like Dialyzer to point out bugs before they actually occur. See http://elixir-recipes.github.io/types/type-checking/
Excellent name.
Developing a REST Api with Maru
I'm doing a [battleship game engine](https://github.com/joaquimadraz/battleship_engine) to apply some learnings. It's work in progress but feedback would be nice. 
Do you build in docker images?
Yes. Here's an example of one of our projects: https://gitlab.com/stackshuttle/grow_journal/blob/master/.gitlab-ci.yml That's the configuration file you need to provide for the gitlab runners. More information here: https://gitlab.com/gitlab-org/gitlab-ci-multi-runner
What is the benefit of using docker? I never really get what the benefits are
I can maybe see its benefits in development, but for production it seems like taking all of the scheduling benefits of a BEAM application and throwing them out the window, since (as I understand it) docker containers get a single virtual processor to work with most of the time. I'm not a Docker expert by any means, so am I missing something there?
You also give up hot code reloading.
For anyone who comes across this late, it's https://github.com/influxdata/telegraf. :-)
I think that's just there to make it clear that the name can be anything you want. It may often be the module atom but it doesn't have to be.
I can't think of any reason to use @name instead of `__MODULE__` I am a fan of the solution that came up on this [Elixir Style Guide Issue](https://github.com/niftyn8/elixir_style_guide/issues/22) So in this case you'd do something like: defmodule KV.Bucket.Supervisor do use Supervisor alias KV.Bucket def start_link do Supervisor.start_link(Bucket.Supervisor, :ok, name: Bucket.Supervisor) end def start_bucket do Supervisor.start_child(Bucket.Supervisor, []) end end or defmodule KV.Bucket.Supervisor do use Supervisor alias __MODULE__, as: BucketSupervisor def start_link do Supervisor.start_link(BucketSupervisor, :ok, name: BucketSupervisor) end def start_bucket do Supervisor.start_child(BucketSupervisor, []) end end ... with my preference being the first because it feels more explicit.
Yeah. I don't have an issue with this scenario. You can quickly infer the name at the top of the file. I'm not saying I would follow this pattern all the time, but I can imagine a time and place for it. 
Erlport makes running python models in parallel bearable and really easy
Thanks for looking into it! I should have some time this evening to mess with it a bit. I'll update the post if I find a solution. 
Very cool! But why is the default partitioner random? Stream processing frameworks like Samza expect the partition to be based on the key.
You should ask in the Elixir Mailing list as well (https://groups.google.com/forum/#!forum/elixir-lang-talk). Usually the response times are way better and you will have access to the core team members.
Is kafka a magical thing that I should take the time to understand as a lone developer? Or does it just serve a good purpose in large organizations, like a lot of apache tools? 
I'm using it (although this is my first time with it so I'm still very much a beginner with it/learning as I go), even using the implementation linked in OP (Absinthe). There are a couple of other alternative GraphQL implementations out there, though I personally liked Absinthe for the simple DSL, it was also one of the most complete implementations (in terms of features it supports). And yes, GraphQL is essentially a replacement for REST APIs. Although you could decide to have some REST end points and GraphQL for other API. It has quite a lot of benefits over REST. Such as no longer needing to access multiple end points, only get back the data you need (you could do this in REST but it's more effort on your part), API introspection so you can build powerful tools around them, simpler/more traditional approach to versioning (instead of creating multiple API versions, the consensus is you deprecate the functionality you plan to remove over time), etc. The only caveat to using GraphQL is it's still quite new so there isn't always a standard answer to a problem you might be face. But if you are interested I recommend reading the [spec](https://facebook.github.io/graphql/). 
This might be of interest, written by José a year ago. http://blog.plataformatec.com.br/2015/06/elixir-in-times-of-microservices/
Hey guys, just wanted to share my first small plug for those who'd want to use turbolinks in their app. Any feedback on code will be be appreciated!
Its more of a blog post in PDF format. https://s3.amazonaws.com/versioned-apis-with-phoenix/Versioned+APIs+with+Phoenix+-+Elvio+Vicosa.pdf
&amp; should be the short form of the anonymous function, eg &amp;Map.get(&amp;1,key) ==&gt; fn x -&gt; Map.get(x,key) In this case the Agent line means Agent.get(bucket, fn x -&gt; Map.get(x, key)) I think nothing changes compared to explicit form, it's just a matter of tast/style/code shortness (explicit form can become quite tedious for pipelines/nested calls) Please correct me if i just wrote shit, I'm on elixir just since 2 days 
You got this one exactly right. Those 2 different expressions will actually compile to something identical, and in the manner you described. `&amp;` just takes an expression and turns it into a function with the arguments listed.
Thanks for the clarification!
Any progress on this?
As someone toying with the idea of creating a simple game server, this is fantastic.
How well does it perform? Erlang and elixir aren't really intended to be used for math-heavy applications. 
Heh, I had to draw the line somewhere. Maybe you reindent a big file or copypaste stuff, then you get massive amounts of XP and it loses all meaning. So I thought let's just give XP for actual typing. But, due to technical limitations (me not being familiar enough with the Atom API), the Atom plugin gives lots of XP sometimes for multiple character operations. Whereas the IntelliJ plugin does not detect them at all (only detects keystrokes). Would love help with these things from people that know the editor APIs better.
I'm making a revolutionary social game called phase4. Think of a short-form text communication platform like Twitter that's as addictive as Pokémon Go. The twist is that phase4 incentivizes thoughtful discourse and deep connections, not sensationalism and outrage. Unlike other apps, phase4 has a **business model**. I've been working on my own but it's too big for one man. PM me for more details.
The reasons I personally like to use `@name` are: 1. it avoids me from repeating `__MODULE__` throughout the module (`@name` is shorter and easier to read) 2. it documents itself as being the `@name` we are going to use for the process we are defining as the module 3. if later you want to change the name to use `{:global, ...}` or `{:via, ..., ...}`, you only have to change one place. Some proposed an `alias` but aliases only work with atoms and not tuples TL;DR - I prefer it for readability purposes.
I've decided to learn Elixir by building a Tor crawler, pretty similar to your project. I've been struggling with how to schedule and execute web scraping jobs regularly. This code is incredibly helpful to have as a reference. Thanks a lot, Sergio, for posting this online! Also, it's a really cool idea. Since Kickass Torrents was shutdown, I've been looking for a good alternative. A self-hosted aggregator is exactly what a lot of people have been wanting for a while.
This is over 2.5 years old, some of the code uses deprecated features
Thanks for this. I just finished the OTP chapter in the Programming Phoenix book, and my head is swimming. I need articles like this to help the basics sink in a bit more.
You're welcome! Let me know if there is anything you think should be added to it and I'll take a look! :D 
I can shred Snow by Red Hot Chilli Peppers on guitar and I can play bass a little. Maybe this is an elixir job for me
This is all possible without redis, its just that poolboy is a super low level building block. The author of Exq also took the time to write out a nice interface to using it. It will actually scale better across multiple machines without Redis. With GenStage/GenBroker thats José Valim is introducing, this will become significantly less complex to handle in pure Elixir.
Something else I just noticed is that the CPU load is much more evenly balanced across the 8 cores in Phoenix. I imagine this is due to the true multithreaded nature of Elixir/Erlang processes, versus whatever the hell MRI is doing. I have a web app in Rails 4 that relies heavily on WebSockets and messaging. I built my own WebSocket server, pub-sub system, all backed by redis, and it runs in the Rails environment. It's basically ActionCable, and guess what - it has trouble keeping up with messages when we're under heavy load. I could throw more servers at the problem, but I think it's time to explore Phoenix as a drop-in replacement for that part.
I would love for ruby 3 to have multi-core support. It would solve so many problems. In the mean time, elixir gets more attractive to start playing with.
The Ruby one seems to only allow for more even utilisation early on (fewer connections) but then when the connections increased it started looking crazier. My guess would be (greedy) locking. As now the threads have more to process and as Ruby is not designed well for concurrency you end up with threads holding a lock for a long time not allowing other threads to process whatever that unsafe data is. That's just a guess though, it would be interesting to look into. 
[literally. ](http://elviovicosa.com/blog/2016/07/27/phoenix-api-versioning-accept-header.html)
You should really use Mix for multi-file projects (can't look more than that atm)
How are you saving it? Is it an embedded field? I suspect you need something like this: https://robots.thoughtbot.com/embedding-elixir-structs-in-ecto-models
There definitely should be a speed up. I'm not in prod with elixir yet, but even on heroku, you'll spend much less time in app code for a comparable unit of functionality in a language like Ruby by virtue of the fact that elixir does so much more work at compile time that Ruby had to do at runtime. 
On one of Chris McCord's interview he mentioned a company using Phoenix that was able to reduce their Heroku nodes from ~10 to 1 by switching from Rails to Phoenix. An order of magnitude difference is non-trivial for smaller companies in the amount of savings you'll get. Those nodes can be expensive if you have a tight budget!
You will have a race condition with https://github.com/leohahn/chat_server/blob/master/apps/server/lib/server.ex#L53 See http://stackoverflow.com/a/11410279 
This question is a bit like asking if Usain Bolt as the driver would make my truck drive faster. Application speed is independent of host if all the hosts are just linux. So does phoenix bring better performance? Better than what? It sounds like you're asking if heroku has better performance than AWS when running phoenix applications and the answer is no - it's the same (more or less, some custom compiled kernel flags might be at play)
"Server internal error"
Nowhere does OP mention rails or any other tech stack for that matter. You all just assumed he meant that because it wouldn't make sense otherwise but for all you know you've all just confused OP and my comment is the only one that cleared up some misconceptions.
I've removed it from the sidebar for the time being, if it comes back up I'll add it again. Also, when addressing moderators please use the ["message the moderators"](https://www.reddit.com/message/compose?to=%2Fr%2Felixir) next to the list of moderators. It sends a message to the moderator mail which is easier to notice :)
Yes, I'm running an elixir app on heroku that drives an api for a legacy rails app I'm migrating to phoenix. Every concurrent user of the rails app opens a connection to api via channel, does a small but non-trivial amount of work periodically, in addition to the calls that have been migrated. I run 2 basic dynos on the api (and the only reason for that is redundancy) and up til a month ago ran a single dyno. It only uses about half the available memory on a dyno, and the legacy side runs between 10-20 dynos at peak usage. Response times on the api are about 5-10% of those on the Rails side, especially for large json payloads. Is it worth it for a small app that's already written in ruby? With negligible traffic, your time and money is probably better spent on writing new features. For a greenfield situation, or if you're starting to $cale, then I'd start looking real hard at a migration story. I'm not advocating for The Big Rewrite™ but you should look for ways to gracefully move functionality over to Phoenix. That might look like moving background jobs that are queued up with Sidekiq to being processed by Verk or similar. It might look like developing new features in Phoenix for awhile until you've got parity with the capabilities of the platform, then migrating old apis over. But I can confirm that at what I would consider to be medium amounts of traffic, its definitely worth looking at.
Why? 
Why use a fullstack framework as frontend?
thanks! didn't know about that (:
looks neat i will check it out
Cool! I know Elixir -&gt; Erlang interop is great, but what's the experience like going the other way? I imagine macros aren't available from the erlang side?
Thanks for the kind words, the idea is definitely to save some other people the time spent.
+1 to the why, totally terrible idea... The article goes through copying the already built assets into phoenix. Eww. Just because it may work does not mean its in any sense a good idea.
I'm a developer who jumped ship from Erlang to Elixir very early (Elixir 0.12 days...I remember the record to struct transition). I learned Erlang because I loved the semantics, and it was something that allowed me to build services that I felt safe operating in production. I was a web developer, and used to the request&lt;-&gt;response lifecycle, but was moving into more complicated areas, like doing streaming audio over http in 2008 or so (not particularly "early", but things have changed a lot there in the last 10 years). I needed to keep state between requests, but also have cheap resource utilization for mostly-idle requests. One of the things I had a hard time doing in Erlang was setting up projects exactly how I wanted to. I could always do what I wanted, but I'd have to write almost everything myself to wire up the http library to erlydtl (Django templates for Erlang, neat project). And I really loved writing basic services in Erlang, but I could not get the polish I really wanted on it. When I saw that Elixir had matured (when I first heard about it, it wasn't very good...but for some reason, looked again at it later). The thing that really grabbed me was mix. It seemed so much easier to deal with than rebar (rebar2 at the time). It also seemed to be a lot easier to set up the OTP-compliant app, and while you can change the way all the pieces hooked together, I liked that there were some stronger conventions created by the Elixir tooling. The other thing I flat out loved was the Enum module and protocols. Being able to support arbitrary collections was just amazing to me. And I could implement a protocol for some collection that didn't have support for it originally, if I wanted to. It seemed like I had the Erlang semantics, but it also felt like something that was easier to fight for to get other developers to learn.
https://github.com/patrobinson/nibbler A tool to help troubleshooting distributed systems by allowing you to run diagnostics on many systems at once. Sysdig cloud does this but it's $20/month/host
From my naive experience pheonix doesn't seem like the right framework
Yea, pretty much. After spending some time playing with phoenix channels, I moved to directly using :gen_tcp and :gen_udp, which is working much nicer. The hardest part has been figuring out how to properly dispatch each component using supervisors/tasks, but I think I have a handle on it now.
Just for information, master now supports macros as well. 
I too remember the record to struct transition! A co-worker of mine worked on a 0.12 to 0.15 upgrade branch. Days upon days of `sed` and `awk`. I honestly thought he was going to implode. I agree with you 100%; the two biggest improvements were ecosystem and tooling, followed closely by macros and protocols. They have been a huge boon to productivity. I still have a soft spot for tuple modules, even though they are 'evil.'
In addition to the other benefits listed here, one big improvement in Elixir is the ability to define default implementations for functions that make up a behaviour. That saves having to write a lot of code that is often boilerplate in Elixir.
See also Joe Armstrong's post "A week with Elixir" at http://joearms.github.io/2013/05/31/a-week-with-elixir.html
why -- because.
heh, CICO. Good luck.
The supervisor itself crashes. If it's your top level supervisor and it's set as permanent (which is default in production) it will perpetually restart your top level supervisor.
Working with barrel-db https://github.com/barrel-db/barrel-platform Created a new plugin for Erlang guys to use Elixir with rebar3. https://github.com/barrel-db/rebar3_elixir_compile
[If a permanent application terminates, all other applications and the entire Erlang node are also terminated.](http://erlang.org/doc/man/application.html)
Yuss! I was terrified I'd have to write this library myself. Thanks for the work you've put in!
Thanks mate :)
[removed]
https://github.com/igas/faker/blob/master/USAGE.md
&gt; Elixir has ruined node form me Actually, I'm pretty sure Node ruined Node for you.
Started working on a library for handling BLOBs in different data stores. In addition making the way from and to the consumer (Phoenix controller or any other process) as resource minimizing as possible. Erlang/Elixir can "send" big data in a message by keeping it in memory and sending a reference, but this seems to have a problem (I am still investigating) in distributed systems.
I've used it and had no success. Where do you run the file ".deliver/config" -- locally or on a server?
From README.md: "it allows writing markup with Elixir syntax, while reaching the performance of precompiled templates." I wonder what is meant by that, both in terms of what aspects of performance (execution time, memory usage, ...?) and how they have been measured. It does not seem surprising that using bunches of macros in functions could be similar in performance, but I also don't trust my intuition on such things ;)
Sorry, I don't understand. why did you say it's the same machine if you have a Mac but build it remotely on Ubuntu?
I'm definitely interested. I've never had to write a NIF, but it'll happen eventually. And having *someone* blog about it will save tons of time for every engineer that google's across it. So yes, please do it.
I'd love to see posts on stability and degradation points. (it's recommended nifs respond within milliseconds - what happens when they take longer?)
`.deliver/config` defines your build and target server(s). You run the edeliver commands locally (at least I do) - but it could be run from a CI process, I suppose. I have a tiny 'build server' at AWS with elixir installed and all of that which is different than my target machine, but you can use your target machine for builds if just trying to get it working - like it sounds you are. From my perspective, your server you are deploying to should have no erlang or elixir even installed on it. Thats the nice thing (again, for me) about a 'release'. Try something like this for your config file... #!/usr/bin/env bash APP="your_app_name_here" AUTO_VERSION=git-revision # localhost build server BUILD_HOST="target server ip or hostname" BUILD_USER="ssh user" BUILD_AT="/tmp/erlang/project_name/builds" PRODUCTION_HOSTS="target server ip or hostname" PRODUCTION_USER="ssh user that will run the app - maybe a `deploy` user? idk." DELIVER_TO="/home/deploy/apps - or some other path" #RELEASE_STORE=optional s3 path if you don't want your dev machine getting every release package Now try the following on your dev machine (but make sure you commit the latest to git, since edeliver uses git): *Only continue to the subsequent step if the one you are running succeeds* 1. `mix edeliver build release --verbose` 2. `mix edeliver deploy release to production --verbose` 3. `mix edeliver start production --verbose` The `build release` (step 1) will be important the first time. It will run remote ssh commands on your build server you defined and do the full build there and assemble the release, create a tar gzip file that will SCP back down to your dev machine. This sounds silly, but its because you must build the release on the same OS as your target server. Once that build and SCP completes, you can deploy - which will SCP the gzip file to the target server, unpack it, and execute. Again, the target server (in a perfect scenario) is not running erlang whatsoever. The release contains everything it needs to run - unlike a rails app for example. One caveat I ran into early on was that the erlang/beam release tended to not pick up certain child dependencies from my mix file (but i had a fairly complex project that was way outside the scope of a basic phoenix app). I figured this out by tailing the log files on the target server in the `DELIVER_TO` directory you defined above in your config. After a deployment, you should spot a log directory in there with files like `erlang.log.1`, `erlang.log.2`, and so on. I was able to figure a lot of issues out by watching them at first. I looked at this blog post (http://blog.plataformatec.com.br/2016/06/deploying-elixir-applications-with-edeliver/) to get me rolling as well as searching old edeliver issues on github. Once i got my old way of thinking about deploying Rails apps out of my head - it went real real smooth and I love this build/deploy approach now, especially with the S3 artifacts. I'm not suggesting you're in that boat, but just take one step at a time and isolate issues as you go. 
As far as the npm stuff goes with phoenix, just create a project that uses no JS/brunch and no ecto if you're focused on just deployment and seeing it run on a remote box. Figure those issues out at another time. Use the `--no-ecto` and `--no-brunch` args mentioned here - http://www.phoenixframework.org/docs/mix-tasks Honestly, when I read your original post - you come across as extremely impatient and irritated. Maybe I'm off here, but remember this isn't magic and there will likely not be a perfect end-all tutorial to hold your hand. It will take some effort to go from locally working to working on a real server at scale, especially if you have never worked with erlang before. 
That's an interesting topic, I will try to expand it!
that's what I'm doing again and having a failure. please look at my update 
maybe, but has been mentioned in any tutorial out there? how are people supposed to know?
Do you have a git repo? Have you properly set up the branch to release? Can you show us your .deliver/config? Do you have ssh permission to your server? Have you even tried to solve it yourself? You're coming off as a bit of an impatient cunt here to be honest, expecting everyone to solve things for you without you having to do anything.
Where did you see it runs mix on the server?
fuck off.
I suggest using plain exrm releases instead of edeliver until you get a grip over the whole Erlang release system. A release is a self contained tar ball which you can just extract and run without installing Erlang on the server. I generally recommend releases than installing Erlang on the server. There is a very nice deployment tutorial in official Phoenix guides. [0] One more thing you have to remember is you have to compile and build the release on the same os you are deploying to. So just use a Ubuntu VM if you are deploying to Ubuntu. This is because of the difference in glibc version on different distros. [0] http://www.phoenixframework.org/docs/advanced-deployment
can I just compile it locally with "mix release" and upload to a server? My local comp. is Arch and remote server is Ubuntu. Do I really need complexity brought by edeliver?
somewhere I read that a release should be built on a server because it 's dependent on its hardware or software. thus doesn't that involve mix involve? also when I was tried to build it, it got an error that mix wasn't installed. I've installed erlang and elixir on a server, should I remove them? 
Generally speaking the easiest way is to handle the SSL offloading at the nginx layer. You would setup a server block that exposed 443 with the key configuration you want and hand the request to Elixir via a proxy_pass directive. I use this configuration in most of my production environments as it saves the complexity of key management from being a concern in your codebase. 
yes, but that answers only one of my questions.
Hi, author of Marker here. What I meant with that statement is that Marker's render performance should be comparable with precompiled EEx templates (as used for example in Phoenix), both regarding memory consumption and execution time, as they both are expanded to roughly the same code during compile time. If you compile this simple EEx template: EEx.compile_string("&lt;div&gt;&lt;span&gt;&lt;%= 1 + 1 %&gt;&lt;/span&gt;&lt;/div&gt;") The result will be a quoted block of code which roughly looks like this in unquoted form: "&lt;div&gt;&lt;span&gt;" &lt;&gt; String.Chars.to_string(1 + 1) &lt;&gt; "&lt;/span&gt;&lt;/div&gt;" In Marker, the result after macro expansion is almost the same: div do span 1 + 1 end =&gt; "&lt;div&gt;&lt;span&gt;" &lt;&gt; Marker.Encoder.encode(1 + 1) &lt;&gt; "&lt;/span&gt;&lt;/div&gt;" So to render the template, the only thing that needs to be done during runtime is converting the result of the '1 + 1' expression to a string and concatenate it with the prerendered HTML. Hope this answers your question :) 
Would love to see more on this and possibly some real-world "C Node" examples with Elixir.
so how come I don't need elixir, erlang and mix on a remote server?
Regarding examples. Have you looked at https://github.com/davisp/nif-examples ? Most type of NIF usage has a basic example, including using dirty schedulers and spawning a background thread. Looking at other projects that implement a NIF might help too. I'm the author of https://github.com/zambal/elmdb a NIF driver for LMDB that uses an async thread for background work. I dont't consider myself a NIF expert, nor a C master, but if you have any questions or want to exchange ideas, I'm happy to help. As of having suggestions at this time, I would't use Elixir when writing a NIF, because for most NIF's the amount of Erlang/Elixir code is pretty minimal and writing the code that is needed in Erlang makes sure that the Erlang folks can use your NIF easily too. If your library does need a significant amount of Erlang/Elixir code, I would still write the NIF library itself in Erlang and write a Elixir library that uses the Erlang NIF library. I use exactly this strategy for the library I'm developing at https://github.com/zambal/exmdb This library uses the above mentioned elmdb nif library, but tries to provide a more convenient interface for Elixir programmers by, among other things, providing Enumerable and Collectable protocols. 
&gt; Regarding examples. Have you looked at https://github.com/davisp/nif-examples ? &gt; Yes I have used them to better understand some concepts &gt; Most type of NIF usage has a basic example, including using dirty schedulers and spawning a background thread. Looking at other projects that implement a NIF might help too. &gt; That's a good suggestion, I will search for some good-sized project that can help me exemplify the domain. &gt; I'm the author of https://github.com/zambal/elmdb &gt; &gt; a NIF driver for LMDB that uses an async thread for background work. I dont't consider myself a NIF expert, nor a C master, but if you have any questions or want to exchange ideas, I'm happy to help. Thank you very much! For sure I will ask, prepare your inbox! :P &gt; As of having suggestions at this time, I would't use Elixir when writing a NIF, because for most NIF's the amount of Erlang/Elixir code is pretty minimal and writing the code that is needed in Erlang makes sure that the Erlang folks can use your NIF easily too. If your library does need a significant amount of Erlang/Elixir code, I would still write the NIF library itself in Erlang and write a Elixir library that uses the Erlang NIF library. &gt; &gt; I use exactly this strategy for the library I'm developing at https://github.com/zambal/exmdb &gt; This library uses the above mentioned elmdb nif library, but tries to provide a more convenient interface for Elixir programmers by, among other things, providing Enumerable and Collectable protocols. Your suggestion is correct, but my aim is not build something to be released, and the Elixir part is only the last step for providing a context on the usage. I can also add a chapter on Erlang, but I'm not proficient in Erlang and I'm not sure if I can pull it off sincerly (time constraints mostly). Maybe here I could use your help!
could you expand please? I don't understand what you mean with "C Node". :)
I have only a local computer and a remote server on which I want to run my website, that's it. Do I need to install elixir and erlang on that server?
A couple of handy links from the Phoenix guides: http://www.phoenixframework.org/docs/configuration-for-ssl http://www.phoenixframework.org/docs/serving-your-application-behind-a-proxy
Let me see, tnx.
I super excited to announce my first production Elixir/Phoenix app: BookDepository.cheap! For all those bibliophiles, you might know that BookDepository is a pretty good site to grab books. However, the often advertised "Free shipping worldwide" is ... iffy. This app is built to find the cheapest prices across Singapore, UK and US. Give it a try and tell me what you think!
http://erlang.org/doc/tutorial/cnode.html
Thanks for information
Nice write up. One additional step is creating the dialyzer PLT and adding dialyxir to the project.
And I'd also add that it's also a good practice to return back the current usage of the rates, to inform the API consumers when it's best for them to retry. This may be done by customizing the response headers or by adding them along the error message.
Plus let be honest. If you really want speed, you can completely make the parser a NIFs. You can have Rust NIFs, so.... probably the best of both worlds :)
It's less important on where you store them, and more important that the location is secure. Technically you can store them where ever you want. If you want to put them in your application folder, have at it. Just make sure that things that shouldn't access the files can't.
Where in Germany is it? And can you give some indication of salary?
It depends on how you deploy. Some deploy systems could cause a loss of data if you place the data inside the application. Placing the data outside of the application 100% prevents that from happening. You have to make the call based on how you're deploying.
&gt; 1) Should I remove "http: [port: 4000]," compeletely from "prod.exs"? No, you still need Elixir bound to some port so nginx can forward requests to Elixir. &gt; 2) Should I instead uncomment "https: [port: 443,...." ? Or should I have them both? I don't want to website to be accessible at http or I'd let nginx take care of it by redirecting a user from http to https. Port can be anything &gt; 1024, I like 3000/4000/5000. Nginx will be bound to 80 and 443 and do the heavy lifting. &gt; 3) Or should I remove https and http and let nginx handle that? Yes &gt; 4) How about the key "url" and its "port"? Just set url to your production domain. Let me know if you have any questions about the nginx config. I'm incredibly familiar with nginx and SSL. 
Can you explain a bit more about the issues with ipv6 and how you would solve it? I agree that for people under NAT that's a bad solution.
I've been hacking on [Work With Elixir](https://workwithelixir.com/) – this week I'm going to be diving into Thoughtbot's [Bamboo](https://github.com/thoughtbot/bamboo) library for writing/testing/sending email through the app. (thanks to all who supported Work With Elixir's [post](https://www.reddit.com/r/elixir/comments/4z1si8/show_relixir_work_with_elixir_an_elixir_job_board/) here, and also on the sidebar!)
Pretty cool-- I've found credo to be useful, but as our project gets larger, the tool is really slowing down. Anyone else hitting similar speed blocks with &gt; 500 source file projects?
You would want to block at a higher level. Any API that needs to be rate limited shouldn't be anonymous.
Really great series! My one recommendation is that you link to parts one and two at the beginning of your post. Hope to see more posts from you in the future! 
... #1 and #3 are in contradiction. #2 isn't clear -- yes, I know, but how about my questions?. #4 isn't clear either -- setup how?
thanks.
70k minimum
euro or dollar and is this the net salary , after tax cuts?
Thanks for sharing. Btw: Macros is already plural (singular is macro). Macroses isn't a word. :)
New to elixir here. Every time I learn more elixir stuff I get incredibly happy. This article is magical.
It's also a good practice to use a different domain for your images (and static assets in general) like an assets.example.com. If you do that then the cookies attached to your application domain won't be sent along with every asset request. This also makes it a lot easier to setup a CDN like Cloudfront or Fastly at some point since the only setup involved would be telling it that your subdomain is the origin server. Probably the better question is, what's the advantage to putting them in your application directory?
I hope elixir doesn't only become a web dev tool like ruby is ....
Great article! I was looking for ways to gather vmstats data and this seems perfect.
Awesome to hear! :D
It shouldn't, see nerves and it's growing popularity. To me the Embedded story with nerves is way more compelling then the web dev side.
I see the error is being thrown from perform, but is that called directly from start in the crawler? If so, I think that's your problem. You need to defer your perform function a bit to ensure all the applications have started. To do that, have the crawler send itself a message during start, then handle_info that message and begin crawling in the handler. It will still begin almost immediately, but this ensures all the processes are loaded first.
&gt; Starting the children in a supervision tree is synchronous. I have wondered about this. I tried appending `supervisor(Exq, [])` to my list of children. Everything seems to be working fine now; no more errors. Thanks for the info!
I completely agree, I think the Phoenix story is compelling too, there is just more options and competition in that space. For me nerves is more compelling because it gives you something in the embedded space that is much harder to achieve without it. A high level, fault-tolerant, distribution capable language that's pretty accessible to hardware amateurs. And you can throw Phoenix in there and have a great hosted website.
It occurred to me the other day that RabbitMQ is built in Erlang, and now that I know some Elixir I totally understand why. This looks cool too.
This is interesting, but I feel like the syntax is too heavy. Compare what you have with RSpec (in Ruby) and you'll see what I mean.
I don't think it's intended to serve as a replacement for RSpec, but for Cucumber. I agree that the keyword list mapping to anonymous functions is a bit awkward, but that's exactly why he's posting a 0.1 version for feedback and help.
I think the given/when/then test structure becomes superfluous outside of the gherkin syntax. If I wanted to make my tests more expressive, I'd go for a less restrictive format like [ESpec](https://github.com/antonmi/espec), or if I wanted the test descriptions to be easily readable and separate from the test execution code, I would lean toward using something like [WhiteBread](https://github.com/meadsteve/white-bread). This seems like a less flexible compromise between the two. Sorry to be a nay-sayer, just wanted to throw out my two cents.
[I wish I could go](https://www.youtube.com/watch?v=ol-SAWurTxY) Does anyone know if they're coming back to Florida next year?
I doubt it, usually they like to move them every year. I expect ElixirDaze will be in Florida again though.
I am going, not presenting. Speaking from past experience, I imagine a couple of events will popup every night. I would join irc or the slack elixir channel to find out more about those.
I'll be there. Going to the SimAlchemy training, along with the main conference. I'm really looking forward to meeting a bunch of fellow Elixir users.
For those going, you should make sure to check out James Edward Gray II's training. He's a friend and coworker, as well as somewhat of ruby celebrity. Dude singlehandedly got me excited about elixir. 
Realtime applications using Phoenix &amp; Swift. Nice! I'll see you on Thursday, David Stump ;)
Thanks for the heads up about Confreaks, I was super bummed about not being able to go this year and didn't even think about Confreaks!
See the answer here: http://stackoverflow.com/questions/30037914/elixir-lists-interpreted-as-char-lists 
&gt; I really wish there was a "phoenix light" version, something like express with Plug middleware you could just bolt on. maybe something like Flask in Python. This might be a few months late, but I'd like to take this opportunity to shamelessly plug (pun intended) for [Sugar](http://sugar-framework.github.io/), an alternative web framework for Elixir. It's really more of a controller/router framework (the view helpers don't really do much beyond passing stuff to compiled EEx/Calliope templates, and the current philosophy for the model layer is "just use Ecto"), but I reckon it's pretty close to what you're looking for.
If Elixir encounters a list of numbers which look like ASCII codepoints, it'll treat that list as an Erlang-style string/charlist, since Erlang "strings" are basically just lists of numbers (Erlang also has "binary strings", which is what Elixir normally uses for "strings"). So, if you fire up `iex` and put in a list of codepoints, like so: [72,101,108,108,111,44,32,119,111,114,108,100,33] You'll get back an Erlang string, like so: 'Hello, world!'
Thank you both! That makes a lot of sense :)
It's a quirk of presentation. The actual underlying values will be as you expect them (for serializing or sending). 
How can we rewrite this to return a number like OP (probably) expected?
This is pretty counterintuitive behavior. How do we get the expected result, either by changing the function or IEx configuration?
I wrote the blog post, but I'm not the author of the library. :) [Here's a comment](http://nithinbekal.com/posts/elixir-laboratory/#comment-2868468603) from Emil, the creator of the laboratory package, about where to use it: &gt; To give an example, if you are doing an overhaul of your homepage, you could do one switch at the controller level instead of making multiple small switches in the view. In most webapps, the ideal place to make feature switches would be in the router or the controller, hence the conn param shouldn't be a problem. So if you're trying out a new version of your home page, you could do this in your controller: def index(conn, _params) do if Laboratory.enabled?(conn, :funky_new_homepage) do render "index.v2.html" else render "index.html" end end Ideally, you would isolate the feature flags in controllers and routers, and avoid sprinkling them all over your codebase. Keeping the dependency on `conn` ensures that you can't use them elsewhere.
Vim + https://github.com/elixir-lang/vim-elixir + https://github.com/slashmili/alchemist.vim
Seconded. As for why: I just like vim.
Spacemacs + Elixir layer, and sometimes Atom as well (there's an addon). Both work quite well.
Intelij - because there's an Elixir plugin and I personally don't like vim
Or: "I've been Vim brain damaged and there is no known cure." Source: have same brain damage
All things IntelliJ! For Erlang too (works on free Community edition). And CLion for C++ (paying sub for that one because it is good).
I use Intelij and it works great. I was hoping this was good. 
Completely agree, I have been Vim brain damaged. I can't physically use any other editor... it's a sickness :( Just tossing a +1 for the vim/vim-elixir/alchemist setup. It works splendidly. 
Visual studio Code + Elixir plugin love the CTRL+Click feature to navigate (when it works)!
have to say I am not a vim user, but I tried spacemacs+vim+alchemist and just vim+vim-elixir+alchemist I also tried IntelliJ , Atom and VSCode. VSCode seems to be my favorite so far. Doesn't feel that heavy as IntelliJ, and took me less time to configure to run tests/builds other tasks form IDE
I've been using Atom for the few very small projects I've done so far. Only thing I wish it had was a "format code" option (maybe there is one, but I haven't found it yet) In general, I like Emacs, but I haven't quite grok'ed how alchemist.el works yet. 
You can launch `emacs --daemon` once and then use `emacsclient -t` to attach to the daemon. That would be much, much quicker :)
I just launch emacs once and basically never shut it down. Unless it crashes for some reason, my emacs uptime is typically within five to ten minutes of my system uptime.
Spacemacs + alchemist - great combo!
emacs/spacemacs, with the Elixir layer.
Sublime
That's odd it works for me. Maybe try asking a question to the slack group
I find the intellij plug in to inferior to the one in atom or spacemacs. Until it has usable code completion I am using something else even though I love my rubymine.
How does the VSCode plugin compare to the Atom one? I have heard the Atom plugin is significantly more robust.
Thank you very much for all your comments Apologize for all those Elixir coding crimes I could be committing hahaha. I came from Java, Javascript, html, Python, Scala or even Go ... very different programming languages and I only got 1 month of experience, a real beginner hahaha To be honest I am trying to put Gherkin syntax close to the developer, but not writing long scenarios. I was more concerned about provide the developers with a syntax that would allow them to write structural tests ... I mean when we write a test we always find 3 steps: Given, When and Then.. For me that is not only for writing scenarios. If we want to test a Math operation: we define the variables, then we invoke the operation to test and finally we eval the result. I totally agree with comments about the heavy syntax: I put a very bad example for promoting a bdd framework hahah. I will try to do it better in second release. I really enjoy coding Elixir and I am developing this "framework" as a hobby so I really appreciate feedback (good and bad of course) Currently I am working on providing the framework with other features: Allow use of beforeEach and before i A global context for each test Implementing a Should evaluation that would like this: should :eq 1, 2 Provide reporting. Fix and correct al my coding mistakes. Very appreciate for your comments guys!
and of course, I am also working on providing a better way to trace the errors.
In my experience it actually was vise-a-versa. However I am just starting with elixir and phoenix, so I might be biased because of the way how intellisence works in VSCode. Since I am using it for js, ts, c#, f# and some other stuff too.
+1 code formatting. To think that it has some pretty damn good autocomplete but shitty code completion is mind boggling!
Why not write this with OTP? If you know you have more than 1 page... spawn as many processes as you have pages? Good work, but try to go beyond ruby programming.
How is Rails distributed? I don't really know anything about their websocket stuff yet.
Edit: Actually I'm guessing Scala Play would be most comparable to Clojure.
I think, for what I had read about ActionCable, that relying on an external redis instance, it could easily be distributed using redis as a pivot.
[alchemist.vim](https://github.com/slashmili/alchemist.vim) is a game changer! Thanks for the tip.
Yup, that's correct. Then the Rails instances themselves would be load-balanced at the websocket layer.
Why so much RAM?
The examples aren't wholly comparable. Phoenix Channels is more than just a websocket, as the other language examples are. Phoenix Channel adds a PubSub layer for each user. So each connection has two or three processes (I forget the exact number) spun up per.
I recently switched from ST. I like the file system better (creating, renaming, dragging, etc.) But boy is it slow compared to ST. The autocomplete popup is noticeably laggy. It's crashed on me about 5 times in the last month (ST did once in 3 years). Some times when I enter a new line two lines merge into one. Is this just me or is Atom buggy for everyone?
Is there a way to skip that? The memory usage seems outrageous.
spacemacs drives me insane. The learning curve is insane and the simplest tasks are overly complicated. Say I want to write a controller. In atom I right click on a directory and say create file and start typing. I still haven't figured out how to do that in spacemacs.
Probably need to consider whether they were running Phoenix in production mode too. There's a ton of overhead in development for debugging, hot code reloading etc. Production is generally much more performant and uses way less memory
You can disable a lot of Phoenix functionality if you wish. While it's opt-in by default, when creating a new Phoenix app you can pass in any of the flags `--no-ecto --no-brunch --no-html`, to disable ecto/models, asset pipeline, and views accordingly. But because Phoenix doesn't strictly lock you into a particular workflow you don't even have to stop there, you can then remove the routing and unnecessary plugs in the endpoint, etc. However the memory usage isn't solely because of Phoenix, the Erlang VM is quite notorious for higher memory usage in general. However this is because of design rather than an actual problem or bug. 
Cool to see how it compares. There is some questions about whether they set up their Elixir/Phoenix app to be the most performant (although that question probably applies to all the other languages/frameworks they used). I did want to point out that the C++ example in theory should be able to be even faster. They're using a web stack that relies on explicit locking using mutex locks, whereas there do exist (or they possibly could've even achieved it themselves?) lockless web stacks (using atomic operations instead). 
Atom editor + [atom-elixir](https://github.com/msaraiva/atom-elixir) package is working well for me. The package provides autocompletion, code navigation and split pane documentation of the module/function under the cursor.
There's an issue raised against the [atom-beautify](https://github.com/Glavin001/atom-beautify/issues/545) package to add support for Elixir. That might provide you with a solution once it gets picked up.
It's one process per user per channel + one process per socket. So 100 users on the same channel is 200 processes (100 user-channels + 100 sockets). It's true that you should not be hesitant to create processes. In fact, I frequently state at my talks that we're writing highly concurrent systems in Erlang, powered by thousands, or sometimes even millions of processes. That being said, processes are not free. The initial cost is about 2kb, so at large scale and in some special situations, it's sometimes worth making different decisions. For example, if the logic in my channels is very simple, and I deal with a bunch of users, then I might trade fault-tolerance for RAM savings. It's a special situation, but still one that might happen occasionally. So essentially, processes are great and we should use them where they make sense. Phoenix has a good default process structure, particularly suited for fault-tolerance (which is always very needed in software systems). However, nothing comes for free, and in this case, the cost is extra RAM usage (~ 2kb per connected user and ~2kb per user-channel).
Thanks for the heads-up. I'll keep an eye on that. 
With the BEAM VM you get integrated, distributed message passing for free. Should be all the RPC you need. I think you might still be stuck in the old OOP mindset. You might want to check out this video, it's not Elixir/Erlang specific, but the patterns still apply: https://www.youtube.com/watch?v=E8I19uA-wGY If you like Erlang, check out N2O: https://github.com/synrc/n2o If you prefer Elixir, Phoenix Channels would really be what you're looking for: http://www.phoenixframework.org/docs/channels
hey dude, we just found out it's going to be in Bellevue, Washington for 2017.
hey, any reason you're not using fragments in query? 
At the very least, this post needs to include the following points: - Phoenix Channels is a higher-level abstraction over raw WS. We spawn isolated, concurrent "channels" on the underlying WebSocket connection. We monitor these channels and clients get notified of errors. This contributes to overhead in both memory and throughput, which should be highlighted with how Phoenix faired in the runs - Phoenix channels runs on a *distributed pubsub* system. None of the other contestants had a distribution story, so their broadcasts are only node-local implementations, where ours is distributed out of the box Phoenix faired quite well in these runs, considering we are comparing a robust feature set vs raw ws/pubsub implementations.
When I wrote it I was not familiar enough with the Ecto API to realize that they would be a much better answer than the complicated arity-5 function I am using. I dervived the technique from reading source, not the docs. Woops. If you look at the pull requests, there is one open from @micmus that uses fragments, but is also from the Ecto-1.x API and I need to update it all to 2.0 using fragments etc.
I was the guy from PagerDuty with long hair, if you saw or met me. I'm also from Miami! Wish I'd seen this sooner, it would have been nice to carpool with someone instead of making a 3 hour drive alone! :P
cat &gt; filename.ex =)
Have fun :-)
You could - use a database to store a token/url and an expiry date - use an agent to store a token/url and an expiry date and on `join` call, check db/agent/whatever you choose to use, and `ok` or `error`. just remember to run a process to cleanup your db/agent periodically.
you should leave this as is, and make your own channels which can be identified by said token/url. your websocket connection should always connect to `/socket`, it's the channel that you need to make unique and temporary.
I saw him post in chat that he wasn't planning on releasing his slides until after the vids came out or something
Thanks for this great explanation Chris! I was thinking a bit about this too when I saw this come across my friends Twitter but had not had time to look through the other implementations yet.
I just released [Paasaa](https://github.com/minibikini/paasaa) - natural (human) language detection library. This is my first Elixir library, any feedback is very welcome :)
basically, IEX always try to print as char list, because otherwise every error message coming from erlang and the BEAM would end up being list of integers...
Hi, Rafert. Thank's for the feedback. I'll write a teardown article, explaining all the details, so everybody can know more deep about the project.
Thanks in advance! I hope the article will make a more tangible comparison (cost/effort and savings) as to guide others that are contemplating a similar decision.
This is cheaper to use than a crypto function, but why should we not just use a crypto function? 
This looks quite useful, we gonna give it a serious try in our current app! Thx! 
Our migration to 4 bare metal servers to 93 containers was our first try to see what it will run in this new scenario. Change from a big computation unit to small ones are not only a deploy. You'll always need to adapt your software to run in a small container, especially using technologies like Ruby which blocks the CPU for each HTTP request it receives. But as I mentioned before, a teardown article will explain the details.
Shameless plug, since I'm a cofounder, but check out https://www.honeybadger.io 😉
Thanks for sharing. I hope your pun was intended ;)
that's popular nowadays
Neat but is there any reason to go with the macro-based approach instead of just using functions? Today it looks like code translated from Ruby instead of something an Elixir developer would write.
Hi @ScrimpyCat, the sub-unit to unit value is now exposed via the property size_to_unit in %Currencies.MinorUnit{}. Cheers!
Mean spirited comment? I was trying to give him an advice to go beyond sequential programming. Use everything that Erlang/OTP provides. /u/randomelixirdevdude You can limit how many pages you parse per process and not get banned, but you probally know that already. 
Right, although I would assume such to still be very fast even when using anonymous functions.
I've been using a combination of PaperTrail for logging (with the [logger_papertrail_backend](https://hex.pm/packages/logger_papertrail_backend) library) and DataDog for monitoring/metrics (with the [dogstatsd](https://hex.pm/packages/dogstatsd) library). Both are what my team has been using with our Ruby systems for years, so it was just natural for me to utilize the same.
Getting a 403 (Forbidden) error, even though I'm logged in. Is it set to be public?
Nice looking so far. If you want extra inspiration, Python has a large set of libraries for NLP to draw from, e.g. http://www.nltk.org/
Also `code_change` for hot code upgrades with state migration: http://elixir-lang.org/docs/stable/elixir/GenServer.html#c:code_change/3 
Oh, I feel so dumb right now -_- Thanks.
Thanks!
I've only read Programming Phoenix that still uses Ecto 1.0 so I'm not uptodate on that information anyway. Do you know a good talk on Ecto 2? :o
How I would do it: 1. EC2 instances would run on a private vpc and should only be accessed from outside via a combination of an ELB (private) and a public DNS route. 2. Elixir nodes could use some discovery service that could potentially discover new nodes periodically. You can easily build your own discovery module by using some aws api (similar to how elasticsearch discovery works) to get the list of nodes (this could be based on a security group or EC2 tags) a just do Node.connect on them.
What's with the camelCase function names like `beforeEach`? Gross. They don't fit with Elixir's convention of snake_case function names.
I decided to use that project as my "I'll learn Elm here too" project, and erm, that was a bigger learning curve than I anticipated. So, no. Major sadface. But now that you mention it, god that seems like a fun project, doesn't it?
Thinking I'm just going to take a codebase like Diku and as a first attempt just do a function by function translation in elixir.
That'll definitely get you writing some Elixir :D I'd love to hear/follow the progress, let me know if you keep the code open-source somewhere.
My 2 cents 1. pick a cool name, e.g. Ecto, you have got XBitsy 2. define your domains and data types 3. group your data manipulation functions into a module named by your data type, e.g. Ecto.Changeset, in your case you could have something like XBitsy.AST or XBitsy.Token 4. group your domain specific functions into a module named by the domain, e.g. Ecto.Schema, Ecto.Query 5. I think in your case XBitsy.Parse and XBitsy.Tokenize would be better, but I have seen modules named in the `-er` way, e.g. Bamboo.Mailer, Swoosh.Mailer, they are two email libraries. They could have named it differently though, something like `Swoosh.Delivery` or `Swoosh.Mail.Deliver`
Appreciate the reply
Didn't realize he was the author -- good book, I've just finished it a couple of weeks ago.
What about the new docker swarm and the overlay network ? I ask because I'm new to Elixir and it looks to me like the perfect complementary solution to add node discovery and encrypt the traffic, but I have no practical experience with elixir so maybe I'm missing something. 
Is this a problem with consul?
I just renamed the package to `bodyguard` and published to Hex.
Indeed, I got the idea from the elasticsearch ec2 discovery plugin. 
Before reading anything, I just want to say: 1. Thank you for having an ASCII animation 2. Thank you for making it stoppable.
Did you solve the problem? I have the same one!
I ran into the issue on my FreeBSD host and worked with a guy on the slack channel to make some changes to the toolchains source. It fixed my problem. Are you on Mac or Linux? Are you getting the same error?
I dunno, maybe maybe it's a problem with the library.
Thank you for this. Im already using it on a project and it works wonders! A++