Maybe you should start by explaining why you think learning Python might be a good idea, because I completely fail to see it. It's like you've said "I want a hammer. I don't know how to use one and I've never seen a nail. Tell me why I want a hammer?" You may in fact be surrounded by screws. Usually I like to start with a problem I'd like to solve and then look for solutions. Your cart seems to be missing a horse. 
My idea is to create an all in one program with GUI for computer administration. Be able to get remote computer information, machine name change, join a computer to a domain, and continue to add to it. I've already created something like this in [PowerShell/VB.net](http://imgur.com/a/SEBYG).
It is a general purpose language. Use it for whatever you want. The first thing I ever made with it (other than tutorials) was a barcode scanner interface using a COM interface on Windows. Then I made a website with it. Then I wrote software to control a tape library and VTRs. Lots of other stuff too over the years. Now I think I am going to finally break down and make some GUI stuff with it. Maybe Qt or Kivy or something.
I wish I had more than one up vote to give you. Thank you for the real world examples. Would you mind sharing your website that you created, I didn't realize that Python could be used for website creating. 
Got to agree with OP, don't have much stamina for random self-study. I need an actual goal- but I do agree that once you have that goal and have hacked around a bit it's time to pick up a book and/or start looking at tutorials so you can learn it the 'right' way.
I've been in the same spot as you and usually recreated my existing PS/VBScript in Python (or whatever language I wanted to learn). Go ahead and learn the basics of Python and create separate projects for system development/IO, networking and GUI to get a sense of how they will work separately. Afterwards start with the GUI and try to add some of the features you created in your separate projects. Regarding real world usage, and depending on the market and place you're at, I would say web development might be most predominate. Also, Python is used in system development (primarily in *NIX-based systems), BitTorrent-clients, USENET-clients, AI-stuff, computation and science, games and fun, all kinds of stuff! Regarding executables of PS/VBScript, the latter can be run as-is without a EXE. For PS I would either create a batch file to kick it off or create a simple EXE-wrapper (simple console app) in .NET/Mono to launch the script. Hope I've answered your question :-)
This has been very helpful. Thank you!
Pfft, you aren't cool if you haven't rolled your own website without a framework. It's a nice project, obviously it's more hands on and more work. All you need for a Python web app is `flup` for a FastCGI server and if you want to have convenience, CherryPy's web server. Obviously, with just FastCGI, you can just use the same app with a real webserver in addition to CherryPy's.
This is probably going to be an unpopular response because it's unfashionable, but for that I'd recommend Tcl/Tk. Tcl does networking and interaction with external programs extremely well. Tk is a very good cross-platform GUI library which is very easily driven from Tcl with a minimum of code. You'll also be able to leverage Expect, which is the only real option for driving remote programs with complex behaviour - eg "scripting an ssh session that does _". SQLite grew out of a Tcl extension, and using it from Tcl is arguably the most natural and powerful API. You can also package up your application as a single `.exe` for Windows, Mac, Linux, BSD etc. I'd say Tcl/Tk is the language most closely aligned to your needs. You might get more mileage out of Python, Ruby or even Perl as a more "general purpose" language with lots of libraries available - others have touched on this. Python's big draws IMO are things like Scipy, Numpy, or specific frameworks like Django. These other languages also have a much larger community and more online resources. On the other hand, Tcl does very well in the "batteries included" sense (see ActiveTcl) and while the community is small I find them extremely helpful and the general quality of available resources is higher than a lot of other languages. Sorry this post isn't very dense with links, I'm on my way out. If you can't find enough information by searching or want me to expand on something particular, feel free to ask and I'll take more time on my response.
&gt; but I would definitely consider Python if I could be bothered to make them all install it. This is a downside for Python on Windows. With Linux it's almost always preinstalled or one `$pkgman install` away. However, you can make a single exe file with PyInstaller to distribute your app.
The best way (reason) to learn stuff is to learn it because you need it. Unfortunately, if you don't have a need to learn Python and you need to justify learning it, you shouldn't learn it. Of course, it's hard to identify a need if you don't know what the tool can be used for, but you should identify your needs and then figure out the best tool to solve that need. The truth is that Microsoft software is very vertically integrated, if you want to work in a Microsoft environment, you're most likely better off learning C# or some other Microsoft language. Python just doesn't integrate that effortlessly into Windows and its APIs. Also, since Python is not preinstalled on Windows machines in the *vast* majority of cases, you will also have to install it first or use a "kludgy" packaging tool like PyInstaller to get a single redistributable .exe file. However, Windows examples: - I have used Python on Windows to create a GUI based Excel file parser that generates PowerShell commands. (A C# program could probably generate and run PowerShell commands directly, you could "hack" that with Python) - I've also created various tools to parse text for various purposes (this is where Python really shines, IMO). - I've created a tool that lists out home directories that don't have a corresponding user anymore (however, this requires glue stuff to get Active Directory user listings and another one to get share/directory listings (doable in Python, of course)).
Wierd. I work with a guy who just built something like this (checks username)...wierd you are that guy! So, rbevans...when DOES the narwhal bacon? For serious though, Python is the glue that holds my linux system together. The update manager and taskbar are both Python powered.
Oh yeah, sorry. I was also going to add that a lot of applications will be C++ for some libraries/components with Python stuff on top.
The backbone of spotify is written in python :) 
impressive that both/all of your friends named Dan can be described as "dirty"
Ruby has a lot of gems, I know they have at some point surpassed CPAN but what about Python libraries (eggs?)
As many have said before, Python is a general-purpose language. I for one am a physicist and use it for data analysis. There are modules available (numpy/scipy/matplotlib, pylab) that give it a lot of the functionality found commonly in specialized niche languages. Python is currently encroaching on territory formerly held by Matlab and (*oh God why*) IDL here. It's also very easy to join Python with C and Fortran libraries, and for our heavy stuff we use a C++ library with a Python interface instead of binaries.
I used psychopy, it was psychophysics stuff so it was relatively simple. In retrospect I probably should have contributed some of the stimuli stuff we developed back to psychopy, although we had loads of lab-specific issues with the monitor, video card, etc. which made it difficult. Depending on what you work in specifically, you might also like to check out Blender with python and the Blender game engine. A diploma student I worked with used it to do some experiments based on rotating ball and stick models, and it generated them all in 3D based on constraints, etc.
I do software integration/validation testing on Windows machines. We built a pretty large framework that will, basically, fetch firmware, drivers, os, applications and all kinds of packages and install each one, without us ever pressing a key. The whole framework is about 95% Python code. In the same manner, you can automate ANY repetitive task you do in your administration. What language you use is just preference. I prefer Python because it can really do about anything. 
Do you write or maintain shell scripts? Python was born to be a better scripting language than bash.
well, if you are a sys admin it can be quite handy... think of it as a language that can help you do all those tasks that are a pain in the ass (or impossible) with simple bash scripts. You can connect to a db and do stuff there, you can download and parse files, use web services, build a website/app... it's one hell of a language and you'll find hundreds of uses for it.
Google uses Python for many of its pages. (Look at the URL of help pages, eg. http://support.google.com/drive/bin/answer.py?hl=en&amp;answer=2375078&amp;topic=2375002&amp;ctx=topic) And YouTube runs on (or used to run on) Python. (http://highscalability.com/youtube-architecture) Quora uses Python (http://www.quora.com/Quora-Infrastructure/Why-did-Quora-choose-Python-for-its-development). etc.
Automation, creating web applications, pulling system statistics and generating real-time graphs and charts, scraping data from the internet. Hell, Python has yet to fail me. 
&gt; Personally I would still write my actual administration scripts in Powershell and then wrap that in a Python application (to provide a GUI) This would be a bit pointless, as Powershell can already access the GUI elements of the .Net framework.
I'm actually shocked. I have a .NET gig to carry me through school, so I haven't looked very hard recently, but my impression of dice was that there's a lot of enterprisey stuff on there. I wouldn't have expected something as hip as a Python job to be on dice.
&gt;Fuck this community sometimes. You should reread your comment. You dismissively suggested he should learn a language first before evaluating it at all. It a) is bad advice and b) sidestepped OPs question which was whether he should learn the language at all. His (correct) assertion was that he should know whether he'd use it *before* learning it, not after.
Oh................................................................................................................................ then don't bother with Python.
No, I said he should read a chapter or two of a book, and that would be enough for him to see if he has a real need for Python. Point is: my comment added to the discussion, even if you disagree with it. numbakrunch's didn;t. I got downvoted, he got upvoted.
That's great to hear. Whether the job is enterprisey or not, Python usage suggests a certain level of hipness and enjoyment to me. Once I'm out of school and looking to make my next move it sounds like I'll have to give dice a good look.
I'm not much of a programmer, I'm really only starting to learn, but a friend of mine and I are using python, wxpython, and twisted to create an online chat application, so what you're saying sounds like it shouldn't be too difficult to do in python, with the added benefit of having the GUI be cross-platform compatible. 
Take a look at some of the examples here... http://technet.microsoft.com/en-us/library/hh848797.aspx
Use for anything you would normally use bash, awk, perl, C/C++ or Java for. Python is easier, more portable.
I learned python to do some research in cosmic ray physics. I used it to make an underground cosmic ray simulator supporting multi-coring and capable of processing gigabytes of input data without running out of system resources on a 32-bit Ubuntu machine.
Thanks for that, I hadn't thought of building the GUI directly in powershell before, though I knew it could use some elements.
As well as YouTube , Reddit and Instagram among many others.
I'm doing some package scanning UI's in Python and wxPython at work. I've also done some text file processing, and one app that archives and renames PDF's sent to server by a scanner. Those are programs that we use extensively daily, and it they wouldn't work we would be in trouble. It's flexible and fast to develop, overall fun. Python also works, it hasn't failed me and I trust it. Some people say Python is slow but it isn't, or you are doing something wrong. Obviously Python isn't the best choice for 3D-engines, but the are packages like nympy for number crunching and if that's not enough you can make modules in C. The biggest bottleneck is usually network and database I/O so you won't gain anything by writing blazing fast GUI in C++ when it ends up waiting for 1000 ms queries... I can't help you with Windows remote control but it's an interesting project.
It is used a lot in computational linguistics. THat's the stuff that makes google's autocomplete, that designs algorithms to detect spam, and e.g. speech recognition. It has a toolbox called the natural language toolkit that has a lot of functions predifined to do certain calculations on text.
Huh? XChat and Pidgin is written in C. Inscape is written in C++.
Maybe sourceforge's search thing sucks, I just filtered by language and looked for names I recognised.
I use it any time I want to whip up a small program and don't want to spend hours reading man pages and debugging trivial issues in a language like C. I just want it to work. And it's nice to be able to write some programs in a few dozen lines of code, sometimes only 1 or 2 lines, when it would be hundreds in C. A hobby of mine is breaking codes, and python is beautiful for that. If I need to brute force something strong obviously I'll need a faster language like C, but for simpler ciphers or decrypting RSA python lets me concentrate on the problem, not on the code, pointers, system calls, etc.
If you're using windows, C# is probably better for that. If you're using linux, fuck GUIs.
A small suggestion - a great source of not completely trivial assignments are 400-level cs courses. Look at a course that is relevant to your interests, then nab its project write-ups. For example, I'm currently taking a 400-level cs course and used the last project as an opportunity to learn python. Maybe around 800 total lines of code, but enough problem solving to actually force you to learn a fair amount of python. 
Based on what you have described, I think where Python would really benefit you the most is in the shift to a general purpose language. If it isn't clear, by general purpose it means that the use cases are not so specific. A personal computer is general purpose in that it can do word processing or act as a web server. A tablet or smartphone has more specialized uses, and though you can even make them behave as web servers, or other exotic roles, you rarely hear of anyone developing programs for a smartphone within the smartphone. This is a long way of saying that Python can be used for writing simple scripts and writing interactive scripts that implement a simple flowchart. However, it can also be used to create very complex programs. Reddit for example, and has been used extensively at Google for prototyping. Why Python is an excellent choice is because there is a enormous ecosystem around learning general programming that happens to use Python. Take a look at the two programming courses right now on EdX, or for practical projects using Python look at Udacity, and there are a number on Coursera, too. As others have said, C# might be better for what you want to accomplish, but Python is likely to be an especially good path for learning general programming skills first that will necessary to build anything that is nontrivial.
The fact that you're asking this tells me you're either: 1. Not a programmer 2. Know a little programming, but don't *really* understand it Python's a programming language (duh). So you can use it for basically anything you'd use any other programming language for. The benefit of using Python is: 1. it has arguably the simplest syntax of any language in existence 2. has a huge community of users publishing a huge variety of free open source packages for doing nearly anything you can think of Connecting to remote computers? Use [Fabric](fabfile.org/). Create a GUI? Use the built-in [Tk](http://wiki.python.org/moin/TkInter) library, or something a little fancier like [Wx](http://www.wxpython.org/). Package your program into a Windows exe? Use [py2exe](http://www.py2exe.org/). Based on what libraries and functionality you use, you might be able to seamlessly port it to Linux and OSX as well. I manage the IT infrastructure and website for a small company, and I use Python for nearly everything. Database access. Code deployments. File system maintenance and reporting.
If you have to ask you probably don't need it. One of the things I used it to, that is related to system administration is to write automatic backup that uses tower of Hanoi algorithm for differential backups (daily), incremental backups weekly and full backups monthly. All happens automatically. 
This appears to be in effect - http://docs.python.org/ redirects to http://docs.python.org/3/
So... you basically have no clue about anything yet still want to offer an opinion based on "what you've heard"? Ok then.
Having them read through convoluted urllib code is doing them no service either. All of these things are broken, but at least Requests tries to fix one of them.
Jokes aren't allowed here. SRSBSNZ all the time.
Is there a reason why you want to do that in Python instead of sticking with what you have? Python is a really cool language but you're going to have a tough time figuring out and setting up what packages you're going to need to accomplish the task you're setting out. Everything you want to do is built in to VB.net. I use Python at work to setup and test custom hardware. I like writing code in Python but I choose to distribute the source code and getting people to install python instead of figuring out this py2exe stuff and the interfaces were all commandline based.
How many times do they want you to keep saying this? ;)
I use it a lot for web scraping. Anything from data gathering to personal pet peeves. An example: there's a great sub shop (Jersey Mike's) that has all their nutritional info in a huge table on their website (http://www.jerseymikes.com/menu/nutrition.php). It's very difficult to compare nutritional info between sandwiches because of the layout. I scraped the page and reordered things so it was easier for my use. Another project I'm working on is a traffic estimator. Scraping the CA DOT current freeway speeds with python, and have written some analysis scripts using NumPy to estimate future traffic based on day/time.
Reddit runs on python, and there are loads of different web frameworks that use python as well. See Django, Pyramid, CherryPy, etc. It is a very popular language for web development.
Did you read the article? The author gave one example of the subtlety.
It is a far better scripting environment than anything Microsoft sells. I use it at work in that capacity. I have also used it for file mugging or processing. Honestly if you are a systems administrator and don't know why you would want Python installed you have an awfully easy job! Simply put it is a tool to make your life easier. 
Have you read about how the [Python mro](http://www.python.org/download/releases/2.3/mro/) is determined? It sounds as if you haven't, since you didn't mention anything about how common base classes are skipped, and how the order is guaranteed to be monotonic, in the sense that a base class never comes before any of its derived classes. In this case, an alternative to making the mixin come first would be to derive the mixin from TestCase instead of object.
More like traits FTW. Implementation-less interfaces open up a different can of worms...
Good move.
&gt; The most "direct" ancestor is listed last. You mean "first", surely?
Crap... no more fun print statements.
Which is exactly the point of the article. The problem is if a third-part module makes a single call to datetime.now(), then you go off the rails. And, no, "even badly-written 3rd party modules" will not just-work if you provide them with UTC because of the cannot-compare-neive-and-localtized values - unless you can somehow guarantee they themselves are never going to the system for time information.
Can we get a poll of those who have used multiple inheritance vs those who have had single inheritance not work right on the first try because of it? Seems to me like Python has had its priorities backward.
So... is now a good time to switch to python 3?
Oh really? I have read so much about interfaces lately, but you are the first person I have read that objects to them. 
How does single inheritance ever not work right because of multiple inheritance?
What would you say is needed before this switch?
The doc page for 3.3 has a different style. I like that. That should avoid missunderstandings.
depends what for, are you using Django? If so then not yet, wait for the final 1.5 release
I think you are not familiar with interfaces in Google Go. The flexibility interfaces can offer if done properly may be even more to your liking than either of those options.
In any other language, calling __init__ on the parent class would be trivial, but in Python 2.x, there are two or three possible ways to do it, and each format is a bit brittle. 
Here is another one for matplotlib. I don't think its perfect, but it is certainly workable. That said, a more beautiful alternative would be awesome!
I've built the visualizations for my grocery data analysis app in matplotlib. Donated after JD Hunter died as well. For other things I roll my own using SVG. Would love to use D3 more, but need support ofr old versions of IE so I end up using graphael for web projects.
Realistically, you should only ever do it one way in Python 2.x. You can get the nitty-gritty info at http://rhettinger.wordpress.com/2011/05/26/super-considered-super/ and working Python 2 versions of the examples at http://code.activestate.com/recipes/577721-how-to-use-super-effectively-python-27-version/
C3 gives the *semantics* for multiple inheritance in Python. You can't ignore it any more than you can ignore Python's name resolution rules, or the APIs of built-in types. However, you are right in that C3 turns local correctness into global correctness, as far as monotonicity is concerned. A user may choose to rely on the fact that the mro is monotonic over subclassing and not care about which algorithm provides this property. Such a user should have made `MyMixin` inherit from `TestCase`.
Ooooooh... pretty....
nice - thanks! (agreed about the readability) but always fun to know. :)
matplotlib + 1.
Holy shit, this is amazing!
R I think the plotting syntax is much more concise than matplotlib.
I heard about it on this subreddit about a month ago. I thought it was excellent as well.
I think this is a very good time for this. In the last few months a lot of crucial projects in the python world have finally made the jump to Python 3 and I expect the pace of adoption to speed up from now.
i used matplotlib until we started using D3. I'm loving D3 - lightweight sinatra app pulling json data from s3 and doing all the fun stuff client side is awesome (and disentangles the data generation from data visualization)
very nice! thank you!
i'm using gnuplot because it is the only plotting-software supporting cmyk colors in vektor mode (afaik) but your limited to ~8 colors. i've build a little python workflow around it using subprocess. for spezial visualization i've written eps files with python. just open("test.eps") and writing to the file. not as complicated as is might sound.
The company I work for started off using matplotlib, but changed to chaco because it was better for making interactive plots with customized functionality. Out of the two, I'd say matplotlib is better for built-it-quick visualizations. Chaco is better if you want to make your own custom plots and want to base it off of an existing framework.
super can already do it &gt;&gt;&gt; class a(object): ... x = property(lambda self: 1) ... &gt;&gt;&gt; class b(a): ... @property ... def x(self): ... return super(b, self).x ... &gt;&gt;&gt; b().x 1 
http://d3js.org/
Looks fairly useful then. This is one of the reasons why it's generally discouraged to use multiple inheritance though.
Can you not use html colors in vector mode? I've made some pretty intricate colormaps. Inspired by this guy: www.gnuplotting.org he is a god.
Dude, this subreddit isn't for your private job advertisements. If you included some more details at least there could be a discussion around the topic. But not like this.
Perhaps some matplotlib customization? http://messymind.net/2012/07/making-matplotlib-look-like-ggplot/ http://tonysyu.github.com/mpltools/auto_examples/style/plot_ggplot.html
I use D3.js
Tastypie allows field filtering as well.
This visualization is nice, reminds me of [this](http://www.edwardtufte.com/tufte/graphics/poster_OrigMinard.gif) graph that Edward Tufte uses in his talks on visualization. I wonder if the style of the visualization was Tufte inspired.
That is a great graph. We referred to it in a class on data visualization.
Somebody is really getting off on charts... [no seriosly](http://bl.ocks.org/1136236) ;)
Have a look at Chart Director. It's commercial but you can use it without a licence in which case it appends a small banner at the bottom of images (it's possible to trim it automatically if that bothers you). I use it to create some graphs related to the football fantasy league i'm in with my colleagues. See http://aelse.github.com/eplfl/
I also love ggplot2 for its flexibility, modularity and great-looking defaults. For the d3 / Javascript folks, there is a sort-of port of ggplot to Javascript creating d3 plots called [Polychart.js](https://polychart.com/js). Haven't tried it out myself yet, but it looks promising judging by the examples. Since I'm in academia, I don't mind the licensing, but be aware that you will have to buy a license to use it on for-profit websites. 
Yeah, Matlab is just rad. I mean, who does't like non-antialised graphs and fonts? At least in LaTeX mode the fonts look decent if misaligned. Seriously, Matlab has to step up the game a bit here. Not providing antialiasing in 2012 is really ridiculous.
I love how I was downvoted for stating what visualization software I use, directly answering the question that was asked. "Python and non-python answers are welcome"... yeah, sure buddy. Python community is turning into the Ruby community. Too bad, was a decent language.
That is pretty hilarious! I can imagine some "enterpricey" software using that and some managers etc. watching and wtfing about those charts
http://www.highcharts.com/ http://raphaeljs.com/ http://d3js.org/ 
Thanks for the tip, although you might expect such behavior when passing an object and not a value. 
the best part is the long-ass comments in the source.
Take Celery for example, we have a port using 2to3, but the generated changes are rather massive, and unoptimized. You may start playing with it now, but in no way would I call it ready for use in production. It would be ready to use in production the day Celery is written in Python 3 and automatically backported to 2.x and not the other way around.
Here's the command that I use to export SVG to PNG with inkscape: inkscape --without-gui --export-png=&lt;destination_file.png&gt; --export-dpi=72 --export-background-opacity=0 --export-width=&lt;width_in_px&gt; --export-height=&lt;height_in_px&gt; &lt;source_file.svg&gt; Worked like a charm for me so far.
For the record, I did not downvote you. I sure don't think that Matlab is a great tool for data visualization, but that does not make your comment a bad one. Mine however, probably was. I am sorry if I was offending you. It's Matlab I don't like, not you.
Yes, I said that poorly. You can pick the fields with Tastypie but you can have different fields in the list and detail view with Piston. Furthermore you narrow the fields you want returned in the url (e.g. http://api.example.com?fields=id,title,author)
Coincidentally, I just merged the fix for this issue. (https://github.com/nose-devs/nose/commit/be14c74d40aa2b36858a270d6513e8a0290d19ce)
What's the best way to report? The Google group or the Github issues page?
I don't have anything *productive* to add, but something I can't help but note. Sorry. You quoted OP incorrectly. What he said was: &gt;seen it's visualization awesomeness
Sure. Here's the free version: http://www.tableausoftware.com/public/ The regular version is at http://www.tableausoftware.com I'm not sure the complete difference of the two since I've only used the paid version, but check it out!
I wonder how many other people followed this link after thinking, "I wonder what gevent is?", only to find a page with ZERO information. Really guys, would it hurt to include a little "this is what it DOES" info on the release link?
Thank you.
Using the Release preview for 3 Months now. Never had Problems with IDLE or basically python.
What is "gevent"?
"gevent is a coroutine-based Python networking library that uses greenlet to provide a high-level synchronous API on top of the libevent event loop." http://www.gevent.org/ Note: As I had no idea...
People making fun of you for not running Win8, but nobody's mocked you for running IDLE yet?
&gt; A touch of a learning curve if you don't know Javascript or DOM. a touch of an understatement ;)
hahaha thats too funny!
Why so much complaints in this thread? gevent is great. Congratulations to the team. Keep up with the good work.
Sometimes people light-heartedly joke around with each other about everything under the sun. I agree it has no place in the workplace. End tomfoolery and hyjinx.
Try here /r/forhire 
&gt;People making fun of you for not running Win8, but nobody's mocked you for running IDLE yet? Was thinking the same thing. Use just about anything else. 
The point ====&gt; you
I don't know.
Those more knowledgeable should correct me where I am wrong, but many people are asking what gevent is so I am going to try to explain quickly. For those who don't know, gevent offers an exceptionally easy way to make multiple things happen 'at once' in Python, with less overhead than conventional threading. Essentially what it does, under the covers: I have a couple things I want to run at once, A and B. Whenever A says 'I am going to sleep for a second' or 'I am going to wait on this socket,' gevent can swap over to B to work for a while, until B yields. You can have many more of these running than you could have threads. 
&gt; razz'd at work &gt; for not upgrading to 8 Stop working at a child care.
&gt; If you are going to joke in the workplace, can you avoid groan inducing shit that everyone is fucking tired of, and attempt something with at least a modicum of originality. I told someone I work with we were thinking about moving to Utah. Cue 2 weeks straight of mormon jokes. It was annoying. I decided to roll with it and make the joke my own at that point because what was I going to do, sit down and have a super serious talk about how the constant jokes were annoying me? That seems ridiculously thin-skinned. Life is about rolling with it so I made the best of it and just laughed along with them and then it suddenly died off. I guess I could have decided they were babies and stalked back to my cubicle fuming.
It is Ok to use it, if you're just starting, but is also is never too early to switch to something more handy.
What else is good/better? (New to python, could use some suggestions)
Yeah, worked fine. Prefer sublime text.
ah shiit -1 point =( guess I'm the only one who thinks its funny 
If you're new, using IDLE is probably fine for a while - it gets a lot of stick, but the minimal features mean there are few distractions for a new user. When you're a bit more advanced, you'll likely want some more features. About every 2 weeks there's a thread asking for editor/IDE recommendations, so I won't start listing them here. ;-)
gevent is one of the secret treasures of Python, in my opinion. It's way easier to use and has a much cleaner API than Twisted, and it's more lightweight than regular threads. It's the best concurrent networking library I've found for just about any language. (I've heard Erlang has this kind of stuff built in but I've never used it, so no comment.)
yes.
http://www.skulpt.org/
http://repl.it/
Thank you, this is more what I was looking for. Now to see if it's worth rewriting my project :)
What's wrong with IDLE? /s 
Completely built in Python/Flask/Mongoengine. Hosted on heroku (they really are super awesome over there, donated some time/resources to help us out).
My friend says that, in terms of Reddit submission history, I look like a Microsoft shill. In actuality it's just that MS blog posts are less likely to have already been submitted
You can't. Unless 1. OCR screen as a picture and figure out how to control mouse and text input. 2. Capture http communication and emulate it.
An experimental dialect for the Firebird : fdb driver is added with this version please test it with Firebird 2.5.2 http://docs.sqlalchemy.org/en/latest/changelog/changelog_08.html#change-274d52dddd6cd76f57ce87648cc1473c
Sadly, persistently using UTC is not the end of the matter, if your dates and times cross the inter gravissimas, a leap second that has been smudged or even a daylight savings change that breaks the rules due to a local event such as the Olympic Games.
I can't believe you couldn't find one from a quick google search
Yup. Now released to master / PyPI / http://django-rest-framework.org
Python 3.3 was just about that point.
You may also like [GateOne](https://github.com/liftoff/GateOne). It uses python + websockets to provide an ssh client. Replace the references to the ssh code with some form of terminal access and you'll be set. 
As I understand it, there's an important technical difference: Skulpt translates Python code into Javascript, which is potentially faster, but makes odd corner cases more likely. repl.it has compiled the Python interpreter itself into Javascript.
Personally I actually like to go pretty lightweight. I use SublimeText2 with package control addons to add any functionality I see myself actually needing. Much preferred to starting out in a bloated environment for me. www.sublimetext.com
i hope you all have "sqlalchemy==0.7.9" in your requirements file since "pip sqlalchemy" installs the beta version
yeah .. i just was a bit surprised when I started a new project today that I got the 0.8 version ;)
[oooooohhhhhh enhanced postgres array type](http://docs.sqlalchemy.org/en/latest/changelog/migration_08.html#enhanced-postgresql-array-type) I wonder if there's support for the operators in query.filter(...)
:( sorry
Originally the plan was to geolocate exactly where people are (most likely by admins doing google queries, and getting lat/long results to put on entries) so that people could do geographical queries and get more relevant responses. When I'm doing anything with geospatial data, I cant avoid using MongoDB. It's way to easy to get geospatial queries running on it (My last app, market.hydr0.com, which did need geospatial querying greatly benefited by it). So that was the main spark behind using Mongo. The other side of it, is that I absolutely **love** MongoEngine.
Looks good in concept. I'm skeptical about how well the development chain will work for compiling cross-platform binaries. I have no doubt it works well on Ubuntu, but I bet there is a lot of pain to get it to push out iOS binaries that work.
This app encapsulates the messier parts of building the websockets between django and the browser. https://github.com/stephenmcd/django-socketio For the part with redis, you can use gevent.Greenlet to spawn off handlers for redis messages to send socketio events as well as to handle incoming socketio stuff and notify the rest of the world via redis. 
Fix your typo in the title
http://en.wikipedia.org/wiki/Server-sent_events You could do worse than use [this blog post](http://curella.org/blog/django-push-using-server-sent-events-and-websocket/) as a guide, it sounds almost exactly like what you are describing.
So python doesn't work for me on repl.it. I get some javascript exception that's a million lines long. It appears that both skulpt and empythoned are in their infancy - I won't be using python as a client-side immediately.
sweet can't wait to play with it. 
I like this one. Good job and thanks.
This is nice, but I'll need an example application to fully grasp the flexibility of this. The one thing that could make this better than say Django, would be, if it somehow simplified the DB modeling process and restarting of server process.
I think you are mis0understanding the post. This will be no help you you if you use django. It is a replacement for the built in class views in [Flask](http://flask.pocoo.org/)
Congratulations. Cool new features. :-)
Thanks for the post. How hard would it be to texture the bricks in this example? Is that easy in Maya?
The inspection system looks fantastic. I can't wait to rewrite my code to make use of that. This should make things much, much cleaner.
Both web sockets and SSE are going to use up a tcp connection per connected user. But unless you're planning on having more than 30,000 simultaneous users, you'll do fine. And if you do reach that point... that's a nice problem to have. 
Sounds like a fun course to take. I studied engineering in school. But I program for fun. Python is a great language to learn. There are so many libraries and fun projects. If you have a final project, I would love to see the results. Please post if you can.
Maybe I should issue a warning here - this project has nothing to do with PyPy (other than using it) and comes (as far as I know) with no tests.
This is great! Please post more!
Convoluted way of namespacing, presumably to take advantage of inheritance.
Many non native english speakers say pronounce it like "phyton". I personally find it ear-raping.
but.. but... but, in your domain you use "python"...
It would be cool if you used the streaming twitter API, filtered on all the misspelled "python" and listed those tweets.
I too find it ear-raping when I hear Americans pronounce python as in song. If you have a problem with it just correct them politely, this just seems extremely obnoxious and doesn't even help those people it's poking fun of. 
For the poor bastards who have to develop in Python on a Windows stack.
Maybe you're saying something other than what I'm reading, but why would views do any of those things?
Love me my flask
meanwhile, later on the page... [Getting gevent+redis+gunicorn working for a realtime djagno app](http://www.reddit.com/r/Python/comments/12e3nm/getting_geventredisgunicorn_working_for_a/)
I didn't get the notification. But I did read the post. Thanks for the very complete answer. I've used "POV" renderer in the past and the process seems similar. I may hit you up with some other questions later. I'm definitely going to play with Maya when I get a chance. Looks like fun.
Great start! As you learn more advanced techniques, I suggest you rewrite this while keeping the behavior of the program identical. Rewriting a program to do exactly the same thing in a more elegant way is an excellent way to learn programming and a valuable skill. Learning what "more elegant" means for you is also an essential part.
We use Python extensively in the games industry, but many (most?) of us are under heavy NDAs and simply can't post anything that we're up to here :(
I KNOW RITE, i dont even speak to people who doesnt spell/pronounce words correctly. 2nd class citizens for sure, who do they think they are. 
Even worse when it is preceded by Monty...
For your first program it is too early, but in the future you might want to check out the State pattern. The nested if-statements can make the program hard to maintain. Let's say for instance that you want to create some kind of dungeon with connected rooms (I think this will be your next step anyway ;) ). If you type in "n" you go to the room to the north, "w" to the west, etc. If you do it with the if-statements, it will get really messy really quickly. Also, you can travel to the same room multiple ways: you can go "w", "n"-"w"-"s", etc. With if-statements this can become a problem. The State pattern solves this in a more dynamic and flexible way. If you are interested, you should check out the book "Head First Design Patterns". It explains the pattern in a simple way using real world examples. The code samples are in Java but if you read over the lines it should be clear what the intentions are.
&gt; I KNOW, RIGHT? ftfy
neomodel uses cypher + the rest api under the hood where as bulbs uses gremlin. I believe gremlins being removed from the core neo4j distribution. It soon won't be available in their heroku addon. Personally I much prefer cypher as a query language. I am too familiar with bulbs, neomodel supports automatic indexing of your models. I am not sure if there is a way of doing this in bulbs.
For a days learning you really made use of the tools you learnt! great work. Im learning right now and this is valuable to me for simple implementation. Thanks for posting! 
This is a *great* answer. I hope OP takes note and you get more upvotes. To add onto it, once you get your "if, elif, elif, elif" on, it is often easier and more elegant to move to a dictionary of actions keyed on state. I usually call them handlers. So you would have: handlers = { 0: doThisThing, 1: doThatThing, 2: doThisOtherThing, } Where doThisThing, doThatThing, and doThisOtherThing all take in one param, x, the user input. Possibly the state as well. The biggest deal is that they all take the same parameters.
the very first things I'll do in order: a) get a logbook (yes that paper one!) b) setup a source code control 
Omigosh! This reminds me of one of my first independent Python projects! I'd recommend reading up on OOP, and thinking about what you could turn into a class. In my old game, I had rooms as objects. They looked something like this: Room ------ * Dictionary of connected rooms (room, direction) * Long description * Short description * Contents Definitely save this code and look at it a few months from now. You'll see how much you've changed :)
I played it just for: print "Sorry man, it's too late now, and just for being a pussy, \nYOU DIE!" Definitely a good start learning the basics of python. Use this program to refine your skills the more you learn.
Having maintained and written accounting system modules in my youth for $$, I can tell you that you do not want to write something from scratch. I'm actually surprised the client would be willing to do that. And I may be mis-interpreting what you have been asked to do.
Very good start! Here is an idea for your next two steps: 1) look at the list and dict data structures and think about how you could move all that text out of a print statement and into a data structure... Keep a basic loop (display message and options, get option, show next message/options) - would allow you to build a much bigger ”world in the game” 2) now....move that data structure into a file (csv, xml, json, whatever) or a sqlite database..... Two bonus tasks: A) write an editor routine that lets you add/edit/kill steps and save it to the file b) extend your interaction loop to allow user to do a few events outside the flow (put jacket on, take jacket off)...figure out how you will preserve state.... Each of these will teach you a little bit more about python....and are professionally useful modules to learn......
Looks pretty good. You should try making it load the adventures from files and use not only numbers, but also "yes", "no", "go west", etc.
Reminds me of "Colobot". It was a game where you would travel between planets in search of a new earth and by the way unlock new buildings and robots. It's really dated by today's standard, but it was really awesome for me back then. You could write programs for your robots to make them do all kinds of stuff - fly and transport materials, intercept enemies, follow you, provide cover fire, etc. I don't think I'll ever forget how awesome it was, especially the part about writing programs for friends. Maaaaan, I felt so cool that I don't think it's even possible to describe it.
Lol, I could do that, but man that would be so much more coding. I originally thought that if awk == "1" or "Go west": would work, but it turns out it needs to be if awk == "1" or awk == "Go west": and that would take forever to do, but you know what. I just might.
step 1: Requirements document, as detailed as possible. step 2: Gigantic whiteboard and a partner; design the program on whiteboard step 3: do more design step 4: see step 3 step 5: write some code step 6: realize you didnt design enough ad nauseum.. 
Reddit takes itself too seriously.
It will, though I'm personally not a fan of berating users ;)
Just pushed this up for you - an example of Server Sent Events using Flask &amp; Redis (pusub). https://github.com/bwghughes/flasksse
Requirement is kind of vague at present, but it would be better in a few more discussions. Design is what I will pay attention to.
*I'm actually surprised the client would be willing to do that.* By saying "do that" If you mean "writing from the scratch", then I would say this: My clients are not bothered about what software I use, as long as it is user friendly and robust and does what is required out of it. I have some time and I think writing from scratch would be a nice learning for me.
IANAL, but be careful about liability with accounting programs.
I'm somewhat amazed by how many components a team must manage. This also makes me wonder if one-man-shops are possible/viable anymore. 
The guy who wrote this occasionally turns up on reddit. I can point out this thread to him if anyone has questions or comments
I assume you have other, significant programming experience beyond Python?
Remember ... this is a business, not a sandbox. 
While that would work, think of a way to do it that wouldn't exit the program, but instead waits re-asks the user until a valid answer is provided.
I don't know about that. I mean, it's my favorite too, but it's slower than most other ones I've used.
If you don't use source control (like Git), you need to learn that promptly. If you're handling money, you need to be careful. At a minimum, you should have **tests** in your code that can validate that your models behave correctly. Are you going to use a database? Learn about, understand, and use transactions and locking.
I have actually wanted to post to see if anyone has done it nicely. ASP.NET works pretty much painlessly, but if you get away from that you are in for troubles.
Yes exactly. 
Keep in mind: you're stepping from generic programming to programming governed by industry standards and potentially legal constraints. There are very specific standards code that touches credit card and bank information needs to conform to. I've never had to deal with this stuff personally, but this should set you in the right direction: * [PCI - Payment Card Industry Data Security Standard ](http://en.wikipedia.org/wiki/PCI_DSS) - Wikipedia * [PCI Security Standards Council](https://www.pcisecuritystandards.org/) - Home for all primary information and documentation you will likely need The existence of this kind of standard (and potentially legislation depending on where you are) is a good argument for using an established framework if you can.
You should look into getting incorporated first. It may cost you next to nothing and protect you financially if you later get sued. Not know what OpenERP is and not having any real specs from you, who can say which you should use?
I see your printing a lot. You should do something like this to minimize the amount of "print" statements you have: print """This will output the text on multiple lines""" Output: This will output the text on multiple lines
As many have said, its really only good for beginners. As you move on to bigger and greater things in python, you will shortly realize that there are many things that you would like in your IDLE, that the standard IDLE doesn't have. But its very good to start out with to get used to the basics of the language.
python doesn't have switch/case.
If you have any specific pain points we'd love to hear them and hopefully we can help fix them - either from our side or by working w/ the IIS team. As part of our Azure integration we've been integrating into IIS, and we've developed a wfastcgi.py script (which you can see on our downloads page) which integrates between IIS's FastCGI module and Python's WSGI protocol. So this is definitely something we're interested in improving.
One thing I learned when I created a similar style of game: I found that by creating each room as a function, possibly taking input of text/choices, You can simply call the proper function based off of the states. Go learn functions! They will be great for this type of thing, and this type of coding will help you to learn them, it's a win win. I find by having your rooms as functions it is easier to change order/add more areas.
i agree with everyone else, source control, scrap book and notes, unit tests (nose and optionally virtualenv) FTW, test before or while you implement , use a framework, django seems like a great idea, but Flask, bottle, or web2py are worth a look, ... google app engine too. whatever you do, keep things simple and enjoy
Lol, that definitely seems like fun. I'm working now on using in in the stead of ==, so I can uses lists of words, making the game read words as well as numbers, and then after that inserting an else prompt. But Making a user_name variable would be fun as well. Thank you all for your support.
I just learned for looping, but I don't quite have it yet.
If OP doesn't know about everything you described already, they have no business writing this application.
Yeah, I wasn't talking about IronPython either :) The installer for wfastcgi will install the script into [Python27InstallDir]\Scripts\wfastcgi.py. From there you can set it up as a handler in IIS. To do this in IIS you go into FastCGI Settings and add a new handler pointing at python.exe and the arguments are the path to the wfastcgi.py script. This needs to go into the main applicationHost.config which can be accomplished via the UI or editing the file in %windir%\system32\inetsrv\applicationHost.config. The entry should look like: &lt;fastCgi&gt; &lt;application fullPath="C:\Python27\python.exe" arguments="&amp;quot;C:\Python27\Scripts\wfastcgi.py&amp;quot;" instanceMaxRequests="10000"&gt; &lt;/application&gt; &lt;/fastCgi&gt; Then you can add a handler mapping which points at the same file - again you can do this via the IIS config or by editing web.config in your site which would look like: &lt;system.webServer&gt; &lt;handlers&gt; &lt;add name="Python FastCGI" path="*" verb="*" modules="FastCgiModule" scriptProcessor="C:\Python27\python.exe|&amp;quot;C:\Python27\Scripts\wfastcgi.py&amp;quot;" resourceType="Unspecified" requireAccess="Script" /&gt; &lt;/handlers&gt; &lt;/system.webServer&gt; The important part here is that the scriptProcessor matches the entry for FastCGI added to applicationHost.config. This mapping will map all requests to the WSGI handler. This works great for Django and other frameworks which want to handle the URLs themselves. You can also disable the mapping for subdirs in additional web.config files for serving static content. You can also use URL re-writing to re-write to something like "handler.fcgi" and have the path limited to only dispatching to handler.fcgi. Lots of options here, and they basically line up with the similar sort of options for hosting Python in Apache. Finally you need to tell wfastcgi.py what your WSGI handler is. This can be done in the same web.config with: &lt;appSettings&gt; &lt;add key="WSGI_HANDLER" value="django.core.handlers.wsgi.WSGIHandler()"/&gt; &lt;/appSettings&gt; You can also set other env vars here such as PYTHONPATH and they'll get propagated into the process. This one's for Django but we've tested Flask and Pylons as well, so basically any WSGI compliant stack should work. Currently we use wfastcgi.py for our Azure hosting and our integrated Django support. So if you create a new Django project, and have the Windows Azure SDK for Python installed (available via https://www.windowsazure.com/en-us/develop/python/), you can right click on the project node and add a cloud tools project. Then hitting F5 will launch and run from IIS Express setting all of this up automatically. The main reason we have the MSI is because of the work we're doing to get Python up on the Azure "Web Sites" feature. We have a rough preview of this here: https://www.windowsazure.com/en-us/develop/python/tutorials/web-sites-with-django/ which includes similar directions to what I just described. We're working on simplifying that so that Python + wfastcgi is already installed on the shared hosting servers. So that's the reason why the installer doesn't seem to do much, it just an overly complicated way to copy the bits down :)
It's not weird, you're a beginner! Anything is good as long as it works in the beginning. Save all your projects and go back and edit/ optimize them later. Makes for good practice.
Dammit, now there's a problem with one of the branches and I can't figure it out. It just ends, even if I put the correct term in. 
I actually wrote a program using something just like that for my little sister a couple weeks ago. Instead of functions, they're tuples with the text to display as first element, and a tuple as second element, containing the nodes it will lead you to depending on which path you chose. I like your way better though, if your things are functions that return the new state, it's much more practical and permissive. Just in case anyone's interested, [here's my code](http://pastebin.com/FRh2uksG).
Nice! To give you a tiny nudge in the right direction, Ben347's suggestion would use a `while` loop. Honestly though, don't worry about that yet. You're on the right track, for loops are very important :D
It's really hard to beat a whiteboard for visualizing the early stages of a program. Something about physically drawing it out helps things click.
PaaS makes one man shops more manageable then before I would think. If you have the cash you can pay someone like Heroku to handle all the ops work for you, and then buy [addons](https://addons.heroku.com/) for any features you don't have time to build yourself.
Create a tree (that is, a "node" class that can have other nodes as "children"; the tree itself would then start at the "root" node) which represents all the branching possibilities. The player would then play the game by navigating the tree, while the program essentially reads the current node, outputs the choices to the user, then navigates to the child that the user specifies. You could even have interconnecting children if different events eventually lead up to the same outcome (though technically speaking, that wouldn't be a tree anymore). Edit: some people are suggesting using dictionaries as nodes. This has the advantage of being concise.
You're using Learn Python the Hard Way aren't you? I recognize the code. :) If so don't try refactoring yet, Zed will lead you through it. 
This is another great idea. Learning to use Dict in many ways has become one of the most powerful and efficient ways for me to move data around. 
Cool. I didn't think about using lists like that before
 Spend time defining what it is your are supposed to deliver, and exactly how it supposed to perform. Then when you have the requirements done, spend more time on that because this is where most problem stem from - differences in expectations.
Ah, I didn't see your comment above. Sorry about that!
Because you have a numpy compiled for 32-bit. Use Python 32-bit with numpy 32-bit. You can run python 32-bit with Windows 64-bit.
Consider blogging the experience, I think it may be an interesting read, and you may get some help from comments. 
Oh - good point. Thanks - I'll look up my version of Python and try that out! Edit: huh - it *does* appear to be a 64-bit install...
Get Python xy. It standardizes for which you have, plus it has everything in one place. It's a very nice package.
Great job! Can't wait for the final release!
Heh. It's a bit ambitious, yeah, but to be fair it would be good to apply it to some libraries out there, especially some django libs. I find it common courtesy to be honest; I discontinued several of my github repos when I found out other projects were doing the same thing but better. With that said, having multiple libraries to do one thing can be a good thing sometimes. I'm thinking specifically about the PIL situation, and the *few* promising alternatives (such as [pymaging](https://github.com/ojii/pymaging)).
Oh cool - I'll look into that! :) Does it work on 64-bit Windows 7 PCs? :)
Welcome!
Yup, same here. The problem is that with binary third-party-packages (e.g. cjson, pyodbc) you need to have a 64-bit toolchain set up. If I'm not mistaken, it is possible to get a 90-day trial version for visual studio 2012 (mingw can be a pain in the ass to get correctly configured, plus some projects default to a visual studio toolchain, as the other toolchains break too often or are impossible to get right). The default for the VS2012 install is 32 bit, and of course, you can make it produce 64-bit binaries/objectfiles, but you get an idea of the cumbersomeness of the whole process..
* Gunicorn for django apps * uwsgi for our custom servers for git and mercurial http remote operations * nginx as proxy for everyone 
Managing server side component is not time consuming when everything is correctly configured. With some organisation and time planning one guy can manage servers, code and customers.
Web.py is a good choice too for micro component. I'll give a try to Flask.
Well, I'm saying there's a very minute chance you'd want to do this. Say there are some functions in the standard library which you want to 'overload'. You could do it by writing your own module of the same name and using that in further projects. Have I done it? No. I sure as fuck *have not*. It's the only contrived example I could find for a reason to shadow built-in modules.
The import mechanism checks the directory containing the script being run (like `os.path.dirname(__file__)`). Unrelated to putting `.` to `PYTHONPATH`, which is based on the shell's working directory (I think?) http://docs.python.org/2/tutorial/modules.html#the-module-search-path For example, `thing1.py` imports `thing2`: $ cat blah/thing1.py import thing2 $ cat blah/thing2.py print "From thing2" $ PYTHONPATH=/nonsense/path python blah/thing1.py From thing2
Whoops. Looks like I linked to a comment instead of the actual question. [Here](http://stackoverflow.com/questions/13186067/efficiently-split-str-using-multiple-separators-and-retaining-each-separator/13186133) is the link to the question (or you could always just scroll up). Sorry!
&gt;A common example of related problems happens when people try out new libraries: I've never heard of anyone having this problem. I've always heard it's common practice *not* to name files with existing names, because it'll result in obvious conflicts. Try naming in `test_pygame.py` instead. No conflicts, and it's clear what the file's purpose is.
And then after that week is over then we'll implement Python in assembler for every OS/architecture combination that ever existed since 1982.
Really depends on the size of the program and how you happen to break it all up. But some people are amazing at just putting things together in their head on the fly, it's awesome to see
You should definitely give Selenium a try for this. Since then you're just automating browser actions, you *should* be able to do pretty much anything. Do let us know if you find any solution. It's an interesting question.
mmmm....no thanks. I prefer the current model as I think it lends itself to a broader set of ideas and implementation approaches. Also, I am sure it leads to less bikeshedding...nothing pisses me off more than bikeshedding esp from people who commit no code.
&gt;Managing server side component is not time consuming when everything is correctly configured. Yes, but the trick is to know the correct configuration. A successful one-man-webshop should know: * HTML and its standards and quirks on different browsers * CSS and its standards and quirks on different browsers. Maybe LESS on SCSS? * javascript (are there quirks?) * a server-side language and some framework in it (I guess node.js is a viable choice these days? So that saves a bit of cognitive load) * a templating language in that system * a database system (I guess most databases are abstracted behind ORM-s, but they still need configuring and optimization) * a server and its correct configuration * a caching mechanism * and probably something, I forgot The amount of moving parts baffles me. I mean, I could fire up a EC2 instance and code up a barebones web page with flask, style it with bootstrap, forget about caching and optimization and just serve it up via nginx. And it would take me an hour or three. But I wouldn't bet that a well-cordinated team, where everyone has in-depth knowledge about a particular subsystem in this chain, wouldn't achieve a nicer and more performant result in the same time or less. One-man-webdev is obviously not dead - and I guess Heroku and the ilk are making it somewhat simpler to get up and running in less time and aggravation. But my point is that I'm baffled.
Of course, I was just extending upon the OP's example of running a script named with the same name of a module s/he is trying to import, and my reply is specifically catered to that context. If pygame.py is nested somewhere within this same situation will naturally occur, as stated in your example. Edit: Actually, in your example having a filename same as system libraries do not cause this problem. I moved my broken os.py file into there and modified thing1 to also import from os. No errors.
I will, although I have updated it since the original post.
I think it's future-proofing. Say Python didn't allow shadowing. If you have a module named `foo`, and one day Python adds a standard library module named `foo`, suddenly all your code stops working, and the only hint you have is a bunch of undefined function names and such. This is extremely confusing. Sure, shadowing is confusing too, but I think in this case it's the lesser of two evils. Self-import is allowed because it makes the implementation easier, and because some Python programs do have a legitimate reason to import themselves. For example, `pickle` uses dynamic imports to access classes for objects it reads off the disk. If you pickle an object with a class definition in `pickle`, then `pickle` needs to be able to import itself to work.
&gt; resorting to an orgy of nested loops That's pure gold.
&gt; *We're all adults.* You take that back!
In this example, can you somehow refer to the "actual" pygame module from pygame.py? Would you do symlink_to_pygame_dir.pygame?
It's a 32-bit distribution, but 32-bit programs work on 64-bit machines. Please remove your c:\python..\ directory before installing it, though, or you might get a half-64-bit half-32-bit mess.
You are right, lots of technical stuff ! That's why it is a good idea to stick with few different technologies in action. Many startups run with very few people during the first years. Most of the time the technical parts can be seen as the interesting parts but do not forget others parts of a commercial project : marketing, accounting, administrative ...
Lest we forget the major reason why multiple libraries exist: some people like handling identical problems in different ways (Or they have to handle it differently for a specific technical reason). I'm sure they exist, but I've rarely come across multiple libraries that handle a specific situation in the same exact manner; and usually each library has it's own strengths and weaknesses that can't be fixed by simply merging them all. And those libraries that do end up functioning the same as others, tend to be merged or deprecated in time anyway. This is somebody looking for a solution to a problem that's not really a problem.
It's important to consider which implementation of Python you mean, as CPython, IronPython and PyPy will have different performance characteristics. From my experience of CPython: sometimes it will have a speed/memory impact, yes. Although I'd say that most of the time it's for the sake of clarity to other programmers, and not reinventing the wheel. There are a minority of situations where the uglier code gets the job done faster, and if it's a somewhat speed critical portion of your code then it's worth taking the aesthetic hit.
"pythonic" just means "I like it". It's not a particular category or set of rules.
A good counterexample is NumPy. It was born of Numeric on one hand and the separately developed numarray on the other. The fragmentation was killing the scientific ecosystem, so Travis Oliphant merged the functionality of the two. The world is a better place for it.
When you've done this for long enough you will realize that designing usable APIs is hard. Bikeshedding happens, sure, but dismissing the opinions of people who "commit no code" is dangerous and foolhardy. Not everyone has to commit code to be a useful contributor.
Please use a set or a tuple for this, though. :)
import this
The itertool [recipes](http://docs.python.org/3/library/itertools.html#itertools-recipes) can also be handy. I've thrown them in a module (`iterutil`) for easy access.
Most big websites (like Reddit) don't do much CPU-bound computation. CPython is a fairly slow language - a program written in CPython will be between 10 and 200 times slower than an equivalent program written in C or Java. Of course, the C or Java program will be 2-10 times as many SLOC and much more brittle when it comes time to fix bugs or add new features. Pypy is faster than CPython, but it's generally still much slower than compiled code. It's okay that Python is slow; CPU time is usually cheaper than programmer time, and most programs will run "fast enough" on modern hardware anyway. Python is an extremely expressive language that multiplies the effectiveness of the programmer using it, but it does so at the cost of execution speed.
I think a tuple is the most commonly used, but really, a list is just fine. It's a minuscule amount slower, but conceptually a list makes more sense in my opinion. Lists are generally for homogeneous data, and tuples for heterogeneous. In tuples, the specific position often matters, lists less so. Of course, there sets are the least, but I think a set is overkill and cluttering here. PS - you've responded to four of my python posts in the last day! Thanks.
Oh don't get me wrong.. there's certainly a time and place to merge; but to artificially force that across the board is stupid, especially when it'll happen organically if it makes sense as was the case with NumPy. There's reasons why there's various plotting libraries for python besides matplotlib and I don't really see that fragmenting anything. Some people like matplotlib and others like chaco or gnuplot for a variety of reasons. Does that mean we should merge all those and/or go to a "standard" library for plotting? But I guess this is getting into a vi vs emacs type debate.
http://www.python.org/dev/peps/pep-0008/#tabs-or-spaces &gt;Never mix tabs and spaces. The most popular way of indenting Python is with spaces only. &gt;Limit all lines to a maximum of 79 characters. &gt;Separate top-level function and class definitions with two blank lines. &gt;Method definitions inside a class are separated by a single blank line. Just a few rules.
&gt;(Partly because we both rely on vi commands) Why not just stick with vi, then?
&gt; But my point was, people who build really big programs in Python usually do so because CPU performance doesn't matter. The Python code behind Reddit will spend so much time waiting on the disk and the network that the amount of time it's actually doing anything on the CPU is negligible. my own daily experience disagrees with that quite a bit. When Reddit first moved to Python using Pylons, we had a lot of back and forth over how they might best use my template library, Mako templates, as they were having performance issues with some of the namespacing functions; we agreed that they were using a few features of Mako that hadn't been optimized for function overhead, and they stopped using them. This overhead is something you can see clearly in watching the response times as you performance-test a website. They also use my database abstraction library SQLAlchemy - they only use the Core, which is much, much faster than using the ORM. Over the years, we've optimized the Core, particularly on the result-set-fetching side through the use of C extensions. Again, the speed increase by reducing the work spent in CPU is significant here. As for the ORM, the performance impact of the ORM is something that users need to consider for any high performance application - the goal of an ORM is to speed development time by applying a significant layer of automation to the task of dealing with a relational database. This ORM has a lot of performance cost, and I regularly need to work with users of the ORM who are trying to optimize its speed for high-volume websites. While the IO overhead of waiting for queries is not insignificant, the majority of time spent for many ORM operations is within the Python code in the form of function call overhead, not IO. I talked about this a lot on my blog, including [a tale of three profiles](http://techspot.zzzeek.org/2010/12/12/a-tale-of-three-profiles/) where I discuss the link between function call complexity and time spent. Suffice to say the SQL queries in that test case, if invoked with the pysqlite driver directly, take less than a second. The kind of performance issues I deal with as a developer of "plumbing" libraries is completely different from the more classical performance issues a product like numpy has to deal with. I don't deal at all with "algorithmic" complexity and CPU impact, which is what you're emphasizing here. I deal lots with the overhead of pushing data around through lots of automating procedures, each of which adds to an overall set of time spent within the Python interpreter in the form of function call overhead that very quickly outweighs IO overhead.
Because Sublime Text 2 has the minimap, multiple cursors, graphical limits on find/replace all (which uses multiple cursors) and it's faster to hit cmd-b or ctrl-b than to type "wq;!py" or click on the file. Sublime text is also extremely consistant between windows/mac/linux, which is really important because I code on all three simultaneously because I need perfect cross-platform support. (I guess the only reason that vim isn't that consistent is because I tend to use it in terminal only) All of the above only really matter when I'm writing 2000+ line files of super repetitive GUI code, so I *do* use vim for everything else; if I'm going to type self.fooSizer.Add(foo, 1, wx.ALL | wx.EXPAND | wx.GROW, 10) on 15 lines, with only "foo" changing, it's slightly faster to do that using 15 cursors than using "yypppppppppppppp" TL;DR: Sublime is slightly faster for really large files of repetitive GUI code. 
&gt; Why doesn't Python do anything about this behaviour in the implementation of the module object? Is there some esoteric situation where you'd need a file to import itself, or is it just something nobody bothered to explicitly disallow? I wish I could find the original text, but I remember reading something about the philosophy of Python which basically said: * Make it easy to do the right thing * Don't prevent the user from doing something unconventional If you really try, you can do all sorts of crazy, "professional on a closed circuit" sorts of things in Python. Want to add a attributes willy-nilly to an existing object? You can do that. Want to reassign a method call on an existing instance (monkey-patching)? You can do that too. You can hide, rename, and reimplement every built-in method. You can fundamentally change and break every thing you take for granted. And maybe you have a good reason to do so sometimes. But you don't have to. Python makes it possible, but it doesn't necessarily make it easy, and it certainly doesn't require you to do these ridiculous things. So, quite plainly, they didn't specifically disallow it, because that's part of their philosophy. It's not that they didn't care, it's that they didn't want to force it on their users.
[The zen of Python](http://www.python.org/dev/peps/pep-0020/) is worth bringing up here. Based on that (and already existing features), I thought the Python philosophy is more like "do the right thing by default, but give an option to take a detour if the user knows what they're doing". The "freedom to fail" mentality sounds closer to C or C++.
lambda overkill ftw! hardly related obscure, yet useful python: grouped = lambda i, n=2: zip(*[iter(i)]*n) explanation: def grouped(i, n=2): """iterates over i in chunks of size n. grouped([1,2,3,4,5,6]) → (1,2), (3,4), (5,6) """ i = iter(i) return zip(* [i]*n) # zip(i, i, i, …)
I highly recommend PyMel for Maya. Created by [Luma Pictures](http://lumapictures.com/tools/). This, in my eyes is way more pythonic then the "python" you see implemented in native Maya (maya.cmds). And from my experiences is widely excepted by most studios. Also you might be interested in city/building generators(procedural). Much like your castle example, but you will be able to use the UI to edit the building in real time. Like [this](https://www.youtube.com/watch?v=vkzzaTcAZk4) and [this](https://www.youtube.com/watch?v=XfJdSJ-Flhg) &lt;that one is in Houdini not Maya... but you get the idea. After that spiel, welcome! If you enjoy to learn and explore you have the right tools in front of you. Good Luck! Oh one last thought, you seem to be interested with Mocap, there are tons of things you can do with bringing in your Mocap data to a character rig and applying the information. Now I am just rambling!
&gt;Still trying to think of a good reason to import the file you're in. Self-modifying scripts. NB: Python doesn't execute modifications to the live script, but it does on import. Compare; import sys import os if __name__ == "__main__": f_handle = open(sys.argv[0], "a") f_handle.write('print("This is a test of self-modifying code.")\n') f_handle.close() __import__(os.path.basename(sys.argv[0]).rstrip(".py")) and import sys if __name__ == "__main__": f_handle = open(sys.argv[0], "a") f_handle.write('print("This is a test of self-modifying code.")\n') f_handle.close() Now, you might be able to argue that there's no good reason to write a self-modifying script; but that is beside the point, as python is about letting *you* make that decision (and not getting in your way one way or another).
Why would you not switch? There aren't too many changes in the syntax and pretty soon everyone will be using it. 
Anyone here who's going to speak or is planning to participate?
nonlocal, aka lexical scope, did it for me. So useful for functional programming.
Express versions of VS don't support add-ins.
Pythonic Python code may happen to compile into more optimized code paths. Pythonic Python code fits idioms that will be understandable by other Python programmers. `timeit` will show actual performance on your system. * http://wiki.python.org/moin/TimeComplexity * http://docs.python.org/2/library/timeit.html * http://ipython.org/ipython-doc/stable/api/generated/IPython.core.magics.execution.html#IPython.core.magics.execution.ExecutionMagics.timeit * http://www.doughellmann.com/PyMOTW/profile/ * http://stackoverflow.com/questions/3378953/is-there-a-visual-profiler-for-python 
Kudos to itertools, but with the batteries included ethos, would it not have been simpler to work with [nltk](https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html) here?
Front End Engineer in Boston here. I have worked at two startups in the past 2 years my work mostly being in css,html, and js but both are using python as a backend. First startup I was still in college making $22/hr that went under and I got a new job at another startup as well as got my degree now getting 65K/year. Former colleague from first startup does his work in python but now works for a RoR shop makes 90K/yr 1.5yrs out of college but very experienced. My Brother is also a web developer uses mostly RoR makes 80K/year. So salaries vary a lot in Boston but id say they are on the medium to high end for developers especially if you are a cut above the rest there is a lot of demand here but also a good amount of competition obviously not quite the same as NYC or silicon valley.
I'm waiting for gevent[1] to complete my py3.3 django stack. Psycopg2 and gunicorn are already there. Also, judging by process list, my ubuntu 12.10 is indeed switched to py3.2. [1]: https://github.com/SiteSupport/gevent/issues/38
Ah okay - thanks so much for your information and offer! Yeah - the Windows environment variables were fucked up. I've actually simply tried installing everything as 32-bit Python 2.6.6 for now (while removing Python 2.7.6), so they finally all work for now.
Both. Also, can't find the link now but I do recall PyPy developers recommending avoiding code with heavy itertools (ab)use.
From the [performance](http://pypy.org/performance.html) page: &gt; **Abuse of itertools**: The itertools module is often “abused” in the sense that it is used for the wrong purposes. From our point of view, itertools is great if you have iterations over millions of items, but not for most other cases. It gives you 3 lines in functional style that replace 10 lines of Python loops (longer but arguably much easier to read). The pure Python version is generally not slower even on CPython, and on PyPy it allows the JIT to work much better – simple Python code is fast. The same argument also applies to filter(), reduce(), and to some extend map() (although the simple case is JITted), and to all usages of the operator module we can think of.
Kivy is really awesome! Thanks for your recommendation. I have read another article about kivy as well -- Python on Android? First impressions of Kivy http://archlinux.me/dusty/2012/10/16/python-on-android-first-impressions-of-kivy/ .
Inlining is an optimization supported by most modern compilers. Instead of calling the three functions, the compiler actually puts the code of function 3 into function 2, and the code of function 2 into function 1, where the function calls would be. Thus the function call has no overhead, because it's no longer a function call. If this happens a lot or the inlined functions are large, this can make the program significantly larger. For this reason most compilers use some sort of heuristic to determine which functions will be inlined.
Interesting. Thanks. I'm pretty ignorant regarding most/all of the popular python implementations.
Unfortunately (or maybe fortunately: less work) this library does rely (elsewhere) on python3-specific features.
This made me smile! I am also unemployed, so much like Idoesc* I fell ya. My last job used 2.5 x86 python on Windows, but used two types of Software Maya and Motionbuilder which used 2.6 x64... it was beyond frustrating. I am afraid of python 3... and until those two software's adopt python 3, I will be fine.
Like it or not, there are problems that are going to require performance. You're effectively arguing not to use Python in these cases, or worse yet, to suck it up and take hours for what could be done in minutes. Personally, worrying about speed is fine so long as you confine effort to algorithmic efficiency. The same probably goes for other languages since that's likely where the biggest gains could be made, but I think it's more so in python - the emphasis on readability disdains speed hacks but might make it easier to express algorithms beyond brute force methods. Then again, there are a few cases where the pythonic thing *is* the faster thing. A set is usually faster than a list, but a lot of times what you conceptually want really is a set. In line for loops are faster than wrapping a for loop around appends, but they only take one line of code. How about yield? Is there any reason yield exists besides efficiency? (I'm asking, I seriously don't know) What about xrange in 2.x?
For me it's about documentation. There is still boatloads more documentation for 2.x than 3.x.
Python 3.3 is already out, for fuck’s sake! And my answer is quite complicated. My Windows has a copy of Py2.7 for no reason at all, my Linux box has 3.3 and 2.7, my Linux VM #1 has 3.2 and 2.7 (because Requests fuck up on 3.3 and my project requires it), and I’m installing 3.2 on a second VM right now, for the exact same reason. (using Arch Linux, VM1 has 3.2 from not being upgraded since a long time and VM2 uses the Arch Rollback Machine).
Becuase so far, huge amounts of packages don't support it, and most people are still using 2.7.
Why not python 3.3?
Python 3 doesn't have any killer feature I need and doesn't support several libraries I use. Most people haven't switched. "soon everyone will be using it" is a weak argument and by no means necessarily true. 2.x is going to be around for a long time, and I need to balance the time it'll take to learn about 3.x against every other language and framework I'm thinking of learning.
So use 3 when you can and 2 for old stuff. They are perfectly happy side-by-side
2.7 till I die.
I am using 3.2 for work, but I will shortly be converting to 3.3. Of course, Python at my job is not central to our business.
This comes across as incredibly naive. I develop for organizations that run Red Hat Enterprise Linux. I just got to start moving from 2.4 to 2.6. I have tens of thousands of lines of legacy code that would have to be ported. Porting could introduce bugs, and offers relatively little benefit. In a couple of years I might start writing new apps for Python 3, but not until RHEL supports it. I have started writing my personal projects to work on 2.7 and 3.2, but the business world will take a while to move on for very good reasons. 
I suppose you're right. http://faassen.n--tree.net/blog/view/weblog/2005/08/06/0
I'm just now starting into the Python 3.X branch, but have so many holdover devices that have Python 2.4 and 2.5 on them that I cannot upgrade that I'm stuck managing two separate code bases.
What does pygame work with?
I should proudly say to that question "yes, if you use PyPy". PyPy optimizes pythonic code better than non-pythonic code.
I find the usage of named tuple awful and disgusting. I hate when people make such tradeoffs because of stupid runtime.
Google always sends me to the 2.7 docs, so then I have to switch to 3.X, search again and then find the right page. This makes life so much easier.
Use compatibility mode.
I'm planning to participate. Could be fun to speak as well, but I might not have time.
Yes but I'd argue that there are a lot of strategies to make code that blocks a lot less of an issue.
Pypy is essentially a much more optimized interpreter. Better garbage collection and a nice JIT compiler.
Like I said, I don't personally have any issues
2.4 - 3.2.
or Python is fighting my more-abstracted-than-usual approach :) 
"Pretty soon" is what people said in 2008 when Python 3 was first released, and they were probably saying it for 1 or 2 years before that.
&gt; ... we're getting very close to able to import the python part of the original numpy with only import modifications and running it's tests. Anyone know how tightly coupled the pure Python parts of NumPy are to the other (C, FORTRAN, etc.) parts? Is it possible for NumPy to basically separate them so future releases would work with PyPy (assuming they also provided pure Python implementations for any underlying dependencies)?
More referring to the packages that won't work in 3 that I would like to use in a 3 project. It only takes 1 package to stop me from using 3 and stay on 2 until all the packages have been ported. Edit: The hell do I get downvoted for? Did I say something wrong? If you're going to work on a new python project and a package that you depend on doesn't work are you going to stay on 3 or fall back to 2 and keep it forward compatible as much as possible?
Me too. A problem I have is that sometimes you just don't realise what new features you need. Recently, I was using the min function and called it with a key function. Turns out it wasn't supported in python 2.4 =(
Why would I use lambda and other confusing notations and ambiguous things like itertools/groupBy, when I can just use standard regex? 
re PyGTK+: Actually gobject introspection (pygobject) , which is intended, among other things, to replace PyGTK+, works on both 2.x and 3.x. And in many cases it's a drop-in replacement, just change your import statements `import gtk` -&gt; `from gi.repository import Gtk as gtk`. With the added benefit that you get access to loads of other installed libraries such as gobject, cairo, gdkpixbuf, pango, dbus, rsvg ... through the same mechanism, without the need to install specific 'wrapper modules' such as pycairo. 
As long as people are suggesting various Python distributions, I'll plug Anaconda from Continuum Analytics: http://continuum.io/anaconda The Community Edition (Anaconda CE) is free and is available in 32-bit or 64-bit flavors on Windows.
PyPyPyPyPyPyPyPyPyPyPyPyPyPyPyPyPy
"Reasonable people differ" is a far cry from "Just means 'I like it.'". 
there are two things. one thing is that when we're presented with a tradeoff, we choose the pythonic idiom to be the common one (for example we assume object.__dict__ will have few keys mostly contant while other dicts created using {} not necesarilly so). The other part is that pythonic code is often "simple" as far as I understand the term. Simple is good and JIT is making much more sense out of simple code than complicated code.
`import ast`
This is my biggest question, too. Numpy alone is great, but without Scipy and Matplotlib it is of little practical use to me.
I've diagnosed this exact problem dozens of times in the #python IRC channel. It happens quite often with a beginner trying out a module: "Hmm, I wonder how random works? [creates file named random.py to try it out...]"
Compiling regex is relatively costly. If your search criteria are going to change often, this can be a more efficient solution.
He's proud of having gotten a patch in, and is expressing that. Fine by me.
20 minutes to tokenize 40k spaces? I think the DOS potential here is bad enough to call it broken. I would agree if tokenizing that much untrusted python was something I was pretty sure nobody would ever do, but it's not.
I'm confident that there are many more such bugs in the code, and I'm even more confident that this is not the worst a malicious user could come up with. Limiting input size is never a sure-fire way to make your service resistant to such attacks. It is therefore absolutely necessary to also set timeouts on such web services, which makes your point moot.
I don't understand: you think this is an unnecessary patch? BTW: My title was meant to be self-mocking. I understand that this is a tiny patch that fixes an obscure edge case. That's why it's ridiculous to title it "I fixed Python," but I was proud to have a patch in the code.
Hey Ned. Knowing it was yours I probably should've realized the title wasn't entirely serious. I've been scarred by the linkbaitiness of headlines on reddit :) Do I think this is an unnecessary patch? Let's put it like this: I certainly wouldn't have lost any sleep had it not made it into trunk :) BUT! This is your patch. Your patch is in CPython. I can only congratulate you. Certainly, if I got a patch accepted I would be very happy as well.
It's a good idea, but it's not really going to happen. You really think you're going to get thousands of library developers/maintainers to go "yep, you're right, this other solution is better let me just close down shop and redirect my website to theirs".
I'm not shitting on anyone's thunder. I like Ned. I didn't agree with the title of this submission. That was before I knew it was a joke. &gt;How many patches have you committed? Solid ad hominem there, bro.
Has this bug not cropped up earlier simply because most input sanitizers will break extremely long lines or delete trailing whitespace?
I'm aware that some use it in production, but that is not the norm. Python3 may work, and some libraries may be ready already, but in general the ecosystem will need some time to mature, and I believe making Python 3 docs the default may lead to confusion. Really looking forward to 3to2 (the opposite of 2to3) being in a state where it can successfully backport to 2.6, so that codebases can be written in Python 3 , using new features like dict-comprehensions, etc.
Looking really good. Have you thought of making a pymaging plugin? https://github.com/ojii/pymaging
Absolutely. Lots of little patches make it better.
Well, it is very, *very* good practice to do that. It sounds like the whole purpose of this part of the edX software is to bring in quite big chunks of untrusted text (homework assignments) and do stuff (grading) to it. A timeout on that seems eminently sensible.
No, Ive switched away from Python altogether. When I realized I actually do need my programs to complete execution before we all die, and due the packaging problems meaning even if I managed to make my programs quite fast, there is still no means of delivering to the masses.
pymaging looks interesting. Basic plugin for reading 8-bit "merged" images from a psd file should be quite easy to implement (this is more or less what PIL currently offers for PSD). It seems there is no API for layered images in pymaging so I don't know how to make a full-featured plugin. There is also no support for ICC profiles, 16bit fixed-point and 32bit floating-point images so adding "layer.as_pymaging()" method to psd-tools may be problematic.
The author hangs out on Freenode as ojii. Pymaging needs some love, so it'd be great if you had time to work on that, and really useful =)
I just donated, if anyone from the project reads this stuff: thank you and keep up the great work!
...he wrote on a top Internet traffic site implemented in Python...
Components/Packages that I use as part of my job (sole maintainer): * Front-end: mako, javascript, less * Back-end: django, postgres, gunicorn, nginx, supervisord, apache, postfix * Misc: git, virtualenv, ffmpeg And that's not even all of it. So, yeah, it's possible.
There's also Collections.Counter if you just want to count the groups. 
thank you :) we're about to merge float16 support.
You are correct. I missed that in the documentation. The article has been updated. Thanks!
First thing that springs to mind is a PSD linter to automatically flag problems that designers cause for developers. Run it over PSDs and get warnings like this: * Layer "Foo" has no content. * Layer "Bar" uses a blend mode that makes it unsuitable for extracting for use as a standalone image. * Layer "Baz" uses a font that is not shipped as standard with Photoshop. * Layer "Baz copy 4" does not appear to have a meaningful name. * Layer "Layer 14" does not appear to have a meaningful name. * You have not used any groups. * You have not used any layer comps. Basically, an automated version of some of the rules from [Photoshop Etiquette](http://photoshopetiquette.com). 
We are using it to prepare game levels for iOS games straight from PSD files (psd file is converted to a set of images and a json file with meta information that game engine can use); we are also developing a private [kivy](http://kivy.org/)-based GUI app (very simple) for various tasks (e.g. mark a layer group as an "animation group" and export layers from this group to a texture atlas with animation frames).
How so?
Why is Python popular? - Very clean syntax, well-thought out design. - Comprehensive standard library, numerous third party libraries and bindings. - Extremely high "whipupitude" factor. I am constantly surprising myself at how few lines of code it takes to solve all sorts of problems in Python. As in, 5-10 lines of code to parse and screen-scrape information from a complicated table on a web page and store it in a database or CSV file. (With a proper parser, not regexp based fudging.) - Large user base, many resources. 
Also, the fact of the matter is that the majority of applications don't see enough volume to really be bottlenecked by Python/Ruby.
Thank you :) By "simple" web app, I think I meant light weight and most of the code would be on the client and Tornado would serve as a mechanism to get things into a store over websockets. I specifically avoided Django for that idea because I was scared it'd be too much of an abstraction and limit learning. In that light, Node would would be a decent example of another way to fire up a light weight http server.
Thank you! I'm definitely going to look up those libraries.
In that case yes, sounds like Django is explicitly not what you want, though I'm not sure if I would refer to it as a "web app" based on your description.
Absolutely, I've taken over the development of the Sulley Fuzzing Framework which is a massively complicated project. I have yet to ask a question to the old developer short of what he thinks about the direction it should go in. I'm able to figure out everything I need from the code (even if parts of it are un-commented, or wrongly commented). It's very hard to write code that no-one understands (even in a massive and scarily complex program like Sulley) I think another reason why it's so popular is the ridiculous speeds at which you can prototype your ideas. You worry a lot less on how you're going to solve the problem, and focus a lot more on solving the problem itself. Combine that with something awesome like iPython, and you have a tab-completing, very helpful platform to start building things in. Not having to compile anything, nor worry about memory management are two VERY helpful things. When I write programs in any other language it seems like I focus so much time on syntax and hedging against stupid caveats, with python I have almost none of that (except for the occasional silly thing). 
I thought C# was supposed to be almost comparable to C++ in practical use.
Flask looks interesting! Bookmarked. I don't really *need* websockets, but I like to include that sort of stuff in code samples :p
Not to mention the always handy [pdb](http://docs.python.org/2/library/pdb.html), which allows you to set traces anywhere in your code and examine things as they happen. Massively useful for code coverage to make sure things are working as expected without having to come up with a practical case in which it would (as some corner cases can be expected, but difficult to reproduce). 
Agreed, it's subjective, I'd probably call that a web service.
&gt; Would I be right in thinking that Python would be a good choice for elegantly working with data structures when performance isn't a first priority? I'd think so, yes. And if you're crunching large data sets, Numpy is really popular *and* performant - it has a huge user base in the scientific community. &gt; I'd assume they have their own things and I'd need to be more flexible than being familiar with something like Django. They do have their own thing (Google App Engine), but it's based on Django with a couple of layers swapped out. Django is comparable to MVC, but the layers are less tightly coupled. I think reading through the documents and maybe working up a small application will go a long way to helping grok the Python mindset when it comes to building a typical business application. Plus reading the Django source is handy for getting a feel for how a larger, more complex Python application ends up after several releases.
&gt; Python also makes it easy (via ctypes) to interoperate with C libraries - this is how most performance issues are addressed in practice; many bits of the standard library and some major libraries (eg, NumPy) are wholly or partially implemented in C to address performance hot spots. How simple/straightforward is this ? I'd like to know, I guess C# is faster than python, but I'd like to know how to use python when I need it to be fast...
I don't know a library that can do this. But there are open source editors that can save PSD files: GIMP, Paint.NET and (?)Krita. It seems the best PSD support comes with Paint.NET PSD plugin; GIMP and Krita are much worse (as an user I've used only GIMP, this was a conclusion after reading their source code: there are many PSD features that are not implemented or implemented incorrectly in GIMP and Krita while Paint.NET PSD plugin looks solid). If you don't mind C# then extracting some parts from http://psdplugin.codeplex.com/ may be the solution. Second option is to use GIMP's Python-fu and write a GIMP plugin. The third option is of course to contribute to psd-tools :) PSD writing support looks like a nice weekend project. I don't have a Photoshop license so unfortunately it may be hard for me to implement this feature (how would I know if a resulting PSD files are readable by Photoshop? - asking somebody to check this may be very time consuming because implementing write support may require a lot of such checks). By the way, it is not surprising that something doesn't work in Imagemagick for PSD files: the PSD implementation of ImageMagick is ...interesting: huge 800-line functions with zero tests and visible issues (e.g. layer group support is incorrect). If you're looking for C/C++ library for working with PSD, better check libpsd or psdparse.
For a C# developer, none of these points are particularly appealing. (Warning: a bit of Python bashing coming up, however I love Python). The standard library is at best described weird and crusty (just check out things like _Verbose base class in threading.py), broken/hacky/half-assed protocol implementations, a vast lack of uniformity throughout, etc. Compared to the BCL Python's stdlib is a bit of an ugly joke. As for syntactical lightness and ease of use, C# is at least comparable to Python, if in many places exceeding (LINQ and the C# style of lambda come to mind). C# also has significantly fewer warts compared to Python (especially 2.x), each feature added in later versions has composed astoundingly well (no doubt due to the direction of Anders Hejlsberg) Sorry. I started out with an idea about making a few suggestions of my own after commenting on what you said, but while writing this I've realized I can think of few if any good reasons why a C# guy would want to jump. These languages aren't particularly revolutionary, they basically all do the same thing. They just look and smell a little differently One thing that does come to mind is accessibility: Python is imminently hackable and debuggable, both in terms of its "small picture" standard library, and the interpreter itself. This is a capability you simply don't get with the CLR.
Well, I mean one could always do the came with Python and ctypes (I think that's the name).
&gt; How simple/straightforward is this ? About as hard as it would be in C#. I'll point you at the [ctypes documentation](http://docs.python.org/3/library/ctypes.html) as a good starting place for when you just want to wrap some calls to a DLL and the [extending and embedding guide](http://docs.python.org/3/extending/index.html#extending-index) for handling more complex scenarios. 
A majority of them don't, however some do and those some would be picking the wrong technology if they went Python/Ruby. Or, have the cash to throw a lot of hardware behind it.
Even then, as I understand it, throwing in some C for CPU intensive things, and intelligent use of caching can go a long way.
I've never used python in a bussiness application; however I use it for experimental testing and data processing because it can be quick to develop and powerful when you master it.
&gt; So you mean I can use raw dynamic libraries with python ? That is correct. &gt; I think I just came. I don't see how that's relevant or appropriate.
&gt; I can think of few if any good reasons why a C# guy would want to jump 1) Quick prototyping / REPL - no compiler. 2) Platform compatibility (Mac, Linux) The OP may only be interested in Windows, but having used Python (and IronPython) in conjunction with C# in the past, #1 still applies. 
 &gt; It's really about wanting to be the best engineer I can be Learning a new language will not help you achieve this. Learning many new languages will help make you a diverse and agile engineer. Learn how to solve problems in any language. Learn how to dissect a problem, how to design efficient algorithms and data structures. Learn how to create simple solutions to complex problems. 
There's also [Cython](http://cython.org/), which lets you write code in a statically-typed variant of Python that gets compiled to C code that then becomes a Python module.
If you're going down that route, I suggest pairing up with CherryPy, as it's WSGI server is better from what I've seen. Full disclosure: I'm a sucker for some CherryPy http://stackoverflow.com/questions/4884541/cherrypy-vs-flask-werkzeug
except the database access on YouTube goes through some Go middleware called [vitess](http://code.google.com/p/vitess/).
I tried to add pymaging support to psd-tools in recent commits; it turns out there are more missing parts necessary to make it happen. PSD image data is often compressed using PackBits (this may be the most used compression method); PackBits decompression is supported by PIL and psd-tools uses this PIL feature; pymaging doesn't provide PackBits decompression routines and it seems there is no package for PackBits decompression at pypi. 
I don't know your specific need, but needing something "faster than Python" is usually a premature optimization. Python is plenty fast as-is for most tasks.
I still wonder if it's still fast enough if I call code from a c++ 3d engine (ogre for example) from python. Python can be used as a scripting language in games, but if that's the case, you have to link python and use swig, which is painful. Since games are resource hog, I was thinking about writing many features which require to be fast in C++, like tight loops, make all those in a big library, and then use python to start and run the game, but only for the code which has few loops to save cycles, instead of using python as a scripting language. Instead I can just separate (de-couple ?) my game into one C++ part which has low-level call functions, and one other higher level python part, which has code which can be complex, but not executed at all loops. I'm still wondering how python does call C++ libraries, if they're ran at their 'best' speed etc. I don't know if I expressed myself right about I plan to design my game architecture...
That argument may not be valid after a while: http://blogs.msdn.com/b/csharpfaq/archive/2011/12/02/introduction-to-the-roslyn-scripting-api.aspx Combine that with Mono which is making relatively huge strides (it's up to date with .Net 4.5, I think) and C# really isn't that bad. I still prefer Python, though.
What you will have to do in the field depends on where you are going and what you will be doing. Python is a popular, conventional, &gt;20-year-old, open source, general-purpose, strongly and dynamically-typed, garbage-collected, bytecode-interpreted language designed for readable conciseness (call it DRY, low-boilerplate, whatever). Thus, one does not learn Python for any of the following reasons * to be hip or use an exotic, obscure or cutting-edge language * to fit in better with the Microsoft or Java ecosystems, or get away from *nix-like systems * to replace 3-line shell scripts which don't need extension * to write hard real-time software * for situations where you NEED 100% assembler or low-level C/C++ at all levels of your software (boot loaders, drivers, etc. etc.) Among reasons why people do use Python * They resonate with the design philosophy, e.g. enjoy writing less code and more explicit and readable code by default - and are not afraid of unit testing or profiling to find bottlenecks * It's a nice glue language, not bad for embedding either, easier to pick up and maintain programs in than some of the alternatives for this purpose * They would use something else (e.g. Lua) except that Python's library and documentation ecosystem is much larger, etc. * They need to work on projects/at companies/in communities which use Python, of which there are many (big in many parts of science/big data, a healthy segment of web work, communities like Blender's or ESRI's, etc.) * It's the only thing they've learned since it's a pretty good learning language which has become popular in higher education If you don't get it, you don't get it... no use having a language-advocacy trollbattle over it
It's widely used and battle-tested. 
&gt; rapidly churning code out. Why do you use the word "churning"? There's no reason you can't make it as robust as you want and, usually, as performant as you need. Everything can be substituted with other things. Take something you wrote in Java. Why not anything else? Java isn't at a global optimum even for things like performance. Of course it can be substituted. So does this mean Java is pointless? The idea that Python has to have an extremely narrow domain of applicability is rubbish. It's a general-purpose language. Many languages will do many jobs competently. Technical considerations rarely constrain you to exactly one language, usually most of the constraint is coming from circumstances and taste.
Just to be a picky bastard, Reddit was written **in** Common Lisp. Not in a couple of dialects. It was, however, developed in OpenMCL while it was running CMUCL. But this is implementations of a single language specification, **Common Lisp**, not dialects.
I haven't read any "language advocacy trollbattle"-esque posts so far. Thank you for your input though! You highlighted some points others hadn't.
Good point! Npm is growing but it seems like python has 20 years of headway 
I don't think you can forget pseudo code.
* PyPy: http://doc.pypy.org/ * Cython: http://docs.cython.org/ * Numpy: http://docs.scipy.org/doc/numpy/reference/routines.testing.html * Numpy: http://docs.scipy.org/doc/numpy/reference/routines.financial.html * http://quantlib.org/extensions.shtml * http://pandas.pydata.org/pandas-docs/stable/ * Documentation: http://sphinx-doc.org/ * http://pypi.python.org/pypi/ipdb * http://pypi.python.org/pypi/ipdbplugin * SQL: http://docs.sqlalchemy.org
I'm a net admin who uses python for scripting. I feel like a rare breed , as most I know use VB, powershell, perl, or sometimes nothing. In my brief stint in college to get an AAS degree, I never had a CS course, so I took an online course which was in python. I found it fascinating, so I took another, and another. All in python. Eventually I learned it well enough that I have a pretty good grasp of all the common modules that I'd use to do common administrative stuff, like os, re, shutil, telnetlib, paramiko, time etc, etc. It also compiles to executables easier than other scripting languages (not that it's meant to). I guess I have a hard time finding a reason to switch.
Python's syntax maps well to English. I find it easy to think in Python. I used to joke that if you know English and at least one other programming language, you already know Python.
can you give some examples of some scripts you are most proud of?
I'm refactoring this now, using functions instead of so many indentations. I was wondering if anyone wanted to see the finished product. (Not finished yet though)
As a systems engineer who works in cross platform environments, I try to ensure Python is on windows whenever possible. My alternatives are powershell and vbscript, none of which appeal to me. 
I'm not going to say one is more reasonable than the other because I'm not qualified to make that assertion, however, in the case of running a web server, it's my understanding that I could get more performance out of a node server than, say, Tornado, a similar web server written in Python. I also think JavaScript might be a cover a wider base of web developers. I also know that Node is a lot newer than a lot of the Python libraries and frameworks, so that's always an argument against it. To be clear, it was just an example of me falling back to what I know and not me necessarily understanding *why* businesses use Python :)
Ummm, reddit? That's a pretty nice use case for Python. Almost the entire Spotify backend is built in Python, that's another. How about this: Python is a language that is extremely powerful, allowing you to effortlessly express complex ideas and designs. At the same time, Python maintains a high level of readability that allows a team of programmers to cooperate as well as enabling a high degree of code reuse. As such, Python is a perfect fit when a small team of highly talented individuals want to accomplish (seemingly) impossible tasks. 
I think this is absolutely its greatest strength (coming from primarily developing in Perl). For me, a lot of this keys off of there being "one right way" to do things, so when you start writing method number 2 that does something similar to method number 1, it becomes blindingly obvious how to refactor the code because it's for all intents and purposes the same. There are going to be some differences that are a matter of taste (e.g. list comprehensions vs. for loops), but there are not so many differences that it's like reading an entirely different language (Perl example, much as I love it: Acme::Bleach, the source filter that turns your code into nothing but whitespace - not gonna get much more different than that).
Yeah go ahead and post it I'd be happy to take a look. What kind of experience are you coming from?
Who's in Palo Alto? A9?
I'm a reverse engineer and Python is our language of choice most of the time. If I need to make something faster then I can always write a module in C, or use IronPython/Jython for tasks requiring interaction with .NET or Java. IDA Pro and Immunity are both scriptable in Python too. I do like C# but the cross platform nature of our office makes Python a much better choice. We even use Python and ZeroMQ to run our processing system which runs on a small cluster to parse close to two million samples with the latest heuristics we're working on. The core processing modules are C but everything else from web scraping for samples, file handling, to logging is all in Python. There's even a few Java modules, but Python will tie it all together into a very robust system. If you write your code with it's strengths and weaknesses in mind, just like you should do with any language, it'll serve you very well.
I have genuinely no idea what you're asking about. If you are asking why I prefer objects, it's because they have methods (and inheritance and a bunch of other cool features).
I don't know what options there are to deploy .net applications in clouds like heroku or google apps. I do know how to do it in python. Most likely I'm simply misinformed but this could be an advantage. 
why - just why - xmlrpc
&gt; You don't have to shoe-horn all of your data into strings so that you can leverage a dictionary. Isn't this also true in C#? I've never used C#, but I assume that its dictionaries work like Java's Maps. I've never had to shoe horn data into strings when using Maps in Java...
Low barrier of entry. The choice of xmlrpc is, by design, a very superficial aspect of traad, and one that can be easily replaced or made parallel with some other technology. I'd love to use json-rpc or some other mechanism, even if just for the sake of trying something new. The fact is, though, that python provides a very usable xmlrpc server in its standard library, and xmlrpc support in emacs lisp (my initial client target) was pretty easy, too. Likewise, xmlrpc is probably supported in whatever other languages/environments *other* potential clients might use. It's kinda like C: it's got its problems, but it is everywhere. And let's be practical: if I hadn't told you that traad was using xmlrpc, would you have noticed? (Without poking around in the code, obviously.) In the end, it's a tool for talking between processes, and it should be (and is) practically invisible. 
well, literally my first interacction would be to try and make a vim client and trying to figure how to propperly get progress notifications xmlrpc plain sucks at various things one needs to communicate between a editor and rope the ones that come to mind are progress notifications and buffer changes
That is a good point. Progress feedback is completely missing from traad right now. For the codebases I work with, I guess it just hasn't been an issue; for things like django or twisted I suppose it would be necessary. Any suggestions or alternatives to xmlrpc would be great. The primary principle I'm concerned with is keeping the dependencies between the refactoring tool and the editor/IDE minimal. Somewhat lesser concerns are simplicity, wide availability of the technology, and build/deployment issues.
From http://scipy.org/Topical_Software : * Astronomy * Artificial intelligence &amp; machine learning * Bayesian Statistics * Biology (including Neuroscience) * Dynamical systems * Economics and Econometrics * Electromagnetics and Electrical Engineering * Geosciences * Molecular modeling * Signal processing * Symbolic math, number theory, etc.
Wow! Thanks for the post OP.
From http://www.python.org/about/apps/ : * Web and Internet Development * Database Access * Desktop GUIs * Scientific and Numeric * Education * Network Programming * Software Development * Game and 3D Graphics
So predictable.
In my opinion it's often times much easier to just wrap the C library with a C API extension. [Python C API documentation.](http://docs.python.org/2/c-api/)
My feeling is that you don't want any of this shown to your (current) boss ;)
It's a good idea to know what options you have for optimisation, but applying them before you find the bottleneck would be premature.
I really hope you were trolling when saying this. If you're unclear about why you're being downvoted to oblivion, ask.
Awesome, going to have to look into if this exploitable or not :)
windows support is still a pain i think
You could stay well within .NET and still be using Python. For example, we're using IronPython to develop for SharePoint - you just install our solutions and you can go hacking at a live SharePoint site through a web console with IronPython scripts, and use both SP object model API, all of .NET libraries, as well as standard libraries coming with IronPython. edit - link to an (outdated, sorry) version - https://github.com/kerray/NAVERTICA-SPTools/
True enough.
Well that's the fun thing about Python. You constantly learn something new and find better ways to do things. Thank you for the tip.
I was just illustrating the point that you can make any language look pretty hard if you try, there is no magic bullet. But pythoon does stand up to such contrortions pretty well (horrible to obfuscate), and of course in normal practice is very readable.
Downvoted for trolling. Is this a http://cwe.mitre.org/data/definitions/770.html ?
I know a guy in Palo Alto. Tend to forget about Mountain View for that reason haha
A9 has a team here in Seattle, actually
I saw a listing for the team that's building their Windows Phone YouTube app haha. Was like the only post that mentioned c# ;\ 
You can also connect a washing machine to a kettle, you know...
I guess it can be this, since the definition is incredibly broad. Anyway, how is this trolling?
You rock, thank you :)
I use Python on Windows at work (and at home). So does a co-worker. So do members of my family and extended family. So do many at the local Python group I attend. It may just be my region, but Python is more than rarely deployed onto Windows 'round here. There's lots of work to be done at businesses who rely on Windows. Why not use Python for as much as you can?
This will truncate up to n-1 items at the end due to zip(). Most of the time, I use this: from itertools import izip_longest def chunk(iterable, n=2): def test(x): return x is not None for chunk in izip_longest(fillvalue=None, *([iter(iterable)] * n)): yield filter(test, chunk) You'd get the same output (except not truncated) from your grouped by wrapping chunk in a list().
&gt; Not necessarily true. He's referring to using the generic collection that was introduced in .net 2.0. Of course one could just put in object for the value type. &gt; &gt;.Net 1 had collections similar to the hash map of java. It kind of sounds like you're working under the assumption that Java has no generic support, but it's had generics for about 8 years. I still have no idea what dcfix is talking about. If you feel the need to "shoe-horn all of your data into strings", and you're not actually trying to serialize for transmission/storage, then you're doing it wrong.
Yes, but not in four lines of code, without extra libraries.
Its ridiculously patronizing that women are segregated into their own Python class.
There are many reasons that PyLadies runs a class just for women: * Our mission is to bring more women into the the Python community. Having a class just for women is one way to do this. * As more men join an environment, the dynamic of a classroom changes in a way that hurts woman. For an excellent run-down, read this: http://www.columbia.edu/cu/tat/pdfs/gender.pdf * These women have not been told that the only class open to them is this class. They're free to take any Python class they wish. In fact, I regularly help women decide what class would suit their budget, time, and goals best. The women in this class are here because they want to be in *this* class.
Just don't drink the tea made with water from the whites :(
I caught your lightning talk, and I think even condensing it down from 45 to 5 minutes and spanish to english translation, it went over well. Thanks for taking the time to come and give a talk!! 
Yeah I absolutely agree, but I guess I was comparing it to something like Perl in my head ;)
&gt; All we did was instruct every person ever to do a fire-drill upgrade to every Python ever to a more secure version that isn't more secure. That doesn't remind me of PHP at all. Implies that anything that falls below perfection is unsuitable as a response to a security concern. Implies that security concerns can be addressed by never deploying any patch (because can perpetually gather more information before making an "informed" choice). Unless this issue was known earlier and there was a conspiracy to cover it up. I recommend we all move over to that project that always releases timely perfect security patches. The one with a lot of useful features. You know, that one.
Because Javascript was there in the browser early on, and cross platform, so is the only universal web language. Even if it were possible to wave a magic wand, though, you can't claim that Python, as much as I love it, is objectively better than JS in all respects. While I always miss Python's kwargs, comprehensions, and generators in Javascript, I might miss JS's anonymous functions and clear, predictable, and easy to use closures more when I'm writing Python. Furthermore, much of what you need to do in FE programming is XML DOM manipulation, which Python is famously bad at, and Javascript has tons of support for. The other 20% of FE work is handling events from the browser, which JS's anonymous functions and closures make much, much easier, but would require hugely cluttered Python code. Still, if you want to try, you can always compile Python down into JS for delivery to web clients. 
I plan on holding the class for all genders soon (I'm talking to the PyOhio crew at the moment). Also, that's why I wanted to release my materials: so if someone wants to run the class themselves, they can. And I'd like to reiterate: men are not forbidden. We have the policy of 'women and their friends' to keep the ratio of women to at least 50%, not to keep men out 100%.
Sorry about the confusion. I work with a lot of delimited files where I work. Importing, de-duping, merging, etc. For merging and de-dupe, I do a lot of my work with dictionaries. My shop is still on .net 3.5, so when you want to declare a dictionary, you do it like this: Dictionary&lt;string, string&gt; dictionary = new Dictionary&lt;string, string&gt;(); even better: Dictionary &lt;string, Dictionary&lt;string, string&gt;&gt; = new Dictionary &lt;string, Dictionary&lt;string, string&gt;&gt;(); That would allow you to create a dictionary of dictionaries, as long as everything is a string...
I was never happy with declaring a dictionary of objects. If I remember correctly, when I asked for the typeof() it always returned object, not the real type (say, an integer.) This meant that I had to track my types external to the dictionary, which was another pain in the but altogether.
Javascript can be minified, which is great for the web.
One of the industries that has embraced Python is CGI for the film industry. Companies like [Industrial Light And Magic](http://www.python.org/about/success/ilm/) use python for their rendering pipeline - C modules do the heavy processing, and Python is used to control everything. You can also do realtime video processing in pure python [using PyPy](https://www.youtube.com/watch?v=5DtlBC_Zbq4). **Edit**: [more details on PyPy video processing](http://morepypy.blogspot.co.uk/2011/07/realtime-image-processing-in-python.html) 
Assembly it is.
I'm speaking more of deployed into production environments like for web apps and network servers. For those types of projects, they're mostly deployed into Linux environments. Being familiar with that environment is helpful.
Sorry, I meant ES6. And you're right, it does look like SpiderMonkey has started implementing ES6 features faster than everyone else: http://kangax.github.com/es5-compat-table/es6/ However, widespread ES6 support is so far out, I wouldn't consider using crucial syntactical features like generators, comprehensions, and default function parameters on the web yet. I am looking forward to those though, plus getters and setters, and read only properties. There are cross compilers for ES6-&gt;ES{3,5}, but I haven't tried them, and I don't think the Closure Compiler supports any ES6 yet: http://stackoverflow.com/questions/6506519/ecmascriptharmony-es6-to-javascript-compiler http://code.google.com/p/closure-compiler/wiki/EcmaScript6
I agree with you, but I certainly disagree with your assessment of debugging. Whenever you scale large C# projects the debugger starts causing issues and things stop working and errors are more difficult to find. In Python, everything is pretty simple, and errors can be pinpointed because it is AN INTERPRETED LANGUAGE, which means the code can know where things went wrong. Meanwhile C# errors are convoluted and stacktraces are inaccurate.
Minified JS can still be read, I did it just this morning.
Which? The PyPy demo was definitely realtime - it showed it was running at ~30 fps.
I imagine the very fact that it's sending a "buffer overflow" error means it detected the overflow and has stopped operation. Which would imply it's not exploitable; it's probably just a bug in PySMB, not Window's SMB server. PySMB is just a client. See: http://support.microsoft.com/kb/193839
All of which apply to named tuples. They too are objects with methods. If you want the features of a named tuple then it is best to use a named tuple. If you want a superset (or subset) of the features of a named tuple it is best to investigate starting from a named tuple - the source of which can be generated and modified. In [this](http://rosettacode.org/wiki/Pig_the_dice_game/Player#Python) example the namedtuple Game has its __str__ method modified to change how it is printed, otherwise it is used as a named tuple.
Basically, there are actually only 256 (2**8) possible hashings for a given key with the current code. Despite the fact that there should be several orders of magnitude more possible hashings (~2\*\*32, which is 16.7 million times more than 2\*\*8). The fact that the randomization space is so small makes it quite computationally *cheap to generate hash collisions*... which can be effectively used to DoS anything Python which receives untrusted python strings over the network and uses them as dictionary keys. This is true because of the way hashtables work (each hash collision inherently slows the lookup of items with that key hash) Similar problems exist for other hash-based structures like sets. That said, weak hash randomization is still an improvement on -no- hash randomization. So 'ineffective' is a much more accurate word than 'broken'. Heh, that wasn't exactly "Like you're 5" :) Hope it clarified the issue :)
you can also use chrome's pretty printer for the code(not as customizable as jsbeautifier of course)
Sure, and if you do it that way, you can even load the symbol table generated by Closure and retrieve variable names. However, at this point, it's virtually identical to standard decompilation, complete with needing the (never offered alongside the code on the web) symbol table. Still no easy share/reuse/audit for external users. 
I see both genders there, so I don't think it's a single-gender dynamic.
Yep, you win.
I'd be really interested in seeing an example of the kind of code you're talking about. I do a lot of coding in both Python and I pretty much never attempt to have heterogenous types for the keys or values of a dict.
Yes, I was thinking it was a bug in PySMB, but that would make applications which use PySMB vulnerable to attack. Also most memory protections can be bypassed, just because it detects a buffer overflow doesn't mean it's protected (though it certainly doesn't mean it's exploitable either). Edit: Though after reading Microsoft docs, it does sound benign.
how is this actually an improvement? generating 256 requests instead of one and then guessing which of the requests took way longer (and keeping dosing with the guessed ending) is not an improvement at all. It does not increase the complexity of attach.
there is a serious problem in designing O(log n) algorithms when keys are not orderable (you have __eq__, but not __cmp__). There are other solutions (like explained on the bugtracker) though. It's up to discussion whether it's broken or not, however the Python movement was to consider it a security problem and a security fix, so it prodded new releases of every single possible Python. While you may argue this is not a problem, if it is a problem, this is certainly not a solution.
&gt; you have eq, but not cmp cmp by hash, oh wait.
exactly :)
An improvement on what?
Though fijal's other reply in this thread was inscrutable, I find myself agreeing with them here. In the bug report (and related reports), the timing data that was given seemed to show that for Python's actual semi-typical usage of hashing, SipHash was faster on some architectures -- x86_64 IIRC -- and Python's existing hash faster on others; ie. inconclusive. 
haha :) maybe I'm too serious ;-)
C# only runs well within an MS ecosystem. Which for people from a Hacker's background (e.g. reddit was first implemented in LISP) isn't too enticing. It's a cultural thing: outside the business world, noone really likes C#, so it's a rather unpopular choice. Furthermore, C# is jitted, not compiled. You can jit Python too, if you want. But speed isn't that much of an issue in web development, to begin with. 
That's a step in the right direction but I don't believe their goal was to replace js, only to enhance it with the ability to add C or C++. 
I tried. I really did. After 2 weeks of trying to get pyodbc to build properly, I finally just gave up and installed Apache.
That's not the collision rate, it's the number of possible 'hash seeds'. hash('foo') can return up to 256 different values on subsequent invocations of python, but it should return up to (2**32) values on different runs. tl;dr: we're talking about how predictable the hashing is, not it's direct collision rate. The collision rate remains reasonably close to optimal (optimal in this case being 1 in 2**63 items colliding.)
Run a mozilla nightly, they have Python embedded last time I checked.
Yes. You test membership by looking up by key. If you get a hit. In the absolute worst case, it's a linear operation, but ~100% of the time it's basically linear.
Yes, it was a compiler issue. Python, IIS, and pyodbc had to be built with the same compiler, or you you couldn't see pyodbc from python when it was running through IIS. It would work fine in Apache and via python on the CLI, but through IIS it refused to import.
So far, the change-gain isn't worth the change-pain. Until that threshold is reached, I'll stick with 2.7.
I think it's more useful to define common interfaces in the stdlib and let everyone else go hog-wild. Examples include DB API and WSGI. There is a proposal to do the same for asynchronous IO (a la Twisted, gevent..) in the form of [PEP 3153](http://www.python.org/dev/peps/pep-3153/).
anything that tells me how to use oauth is great. oauth is great when it works but ive seen libs that will work with one oauth ,but not another. &gt;:(
&gt; Fixing the hash() function will not fix the DDoS. Sure it will. The goal of a hash flooding attack is to make a hash table generate far more collisions than would be generated by random chance, thus causing hash lookups and insertions to be O( n^2 ) for a large n. Using a cryptographic MAC with a randomly-generated secret key protects us from this.
http://pytz.sourceforge.net/ explicitly warns against using datetime constructor with tzinfo parameter; tz.localize() method could be used instead. Arithmetics on localized dates might require tz.normalize() for timezones with DST; it is simpler to use utc everywhere and convert to a local timezone only for display. To convert a naive utc datetime to posix timestamp in Python 2: (dt - datetime(1970, 1, 1)).total_seconds() 
I don't understand why so many proponents of Py3k don't get this. It's my job to ship product. It would take me months to port my code over to Py3k and I'd see absolutely no benefit from it. It would be a huge waste of money. 
Put 4 spaces before each line of code, or use [Reddit Enhancement Suite](http://redditenhancementsuite.com)
This is actually a thing, [LOLPYTHON](http://dalkescientific.com/writings/diary/archive/2007/06/01/lolpython.html) I mean. I thought it was just some asshole spamming. Surprised me, good hustle OP.
Yes I mentioned in the post that there exists libraries to use, but sometimes it is more fun doing things yourself :) Especially if all you want to do is track some keywords through the streaming APi, then tweepy might seem a bit overkill.
Of course there are python twitter libraries, this guy obviously wanted to write his own as an exercise. The real question is "Why pycurl and not requests?"
lxml is written using it
In theory if cpyext, which tries to emulate the CPython extension API, came along far enough, then numpy could work in pypy without changes. However, cpyext is a complicated beast that is unlikely to ever be compatible enough, and will always be terribly slow as it has to do things like emulate the reference counting semantics of CPython. It's much easier and faster to rewrite it in rpython, and the resulting code will run significantly faster.
Yes, moved to 3.2 (involved porting an app from Flask to Bottle). 3.2 is the latest version available on Ubuntu. Reliable unicode handling was important. The biggest library pains I have are selenium and boto. I plan to do S3 work in 2.7 and expose it over HTTP. Not ideal but it's alright. I'm going to look at getting a python 3 branch of selenium working.
What if I just change the filename?
So there goes any semblance of a plan to do any work tonight! Thanks!
I recognized this from quite a long way away.
I'm pretty sure the metadata is stored in the filename &amp; the file? It's just stored in the filename, so package managers (think `pip`) can easily know which package to download. If I'm wrong, then that means that you could potentially download a python package, change its name to make it "installable" and then have it not work. I'm not sure that's so terrible.
ಠ_ಠ
ITT: people don't get the joke. awesome :) actually smiled at it.
For those who haven't understood it yet: http://www.youtube.com/watch?v=bch1_Ep5M1s
not talking about style. It's just not python. 
you must be fun at parties...
Could be useful down the line for sure 
hahaha! Awesome! Thanks for this.
I like the improvements to box-and-whisker plots and stream plots, but I'm not anywhere near to using 3.x yet.
Can I apply for financial support for this yet? 
Same. Python 3 seems like a big mistake. It's been out for years and hardly anybody is using it. I love the matplot package though -- good to see it is so well developed.
The amount of reddit downtime makes it a weak argument for python. Not that I know much about reddits downtime in general, but the frequent messages that the server is overloaded makes me sad.
This library totally saved my ass in a few computer science classes at my university. Many thanks to the authors!
Okay, but the concept of rewriting still feels like the wrong approach, it isn't scalable. PyPy can't go on reimplementing every popular library in rpython. Wouldn't another possibility be to migrate the official numpy code to C libraries and then hook into them with ctypes (or similar) from every Python implementation, or something along those lines? Avoid having two codebases for the same thing. 
Yup, see the link about financial aid here: https://us.pycon.org/2013/registration/
Like another Google engineer said, Python at Google is very much a scripting language and most big projects are done in c++/java. C# is used in a few places iirc (maybe some use on the chrome team but i am not sure). The notable exception is YouTube which uses python very heavily but if you're looking for a google interview language or for a google recruiter to think your skills are a good fit for Google, you would be far better off sticking to c++ or java. Also remember if you use python at a google interview, your interviewers will expect you to know the language in quite some depth. Python hides so much complexity that a lot of interviewers will drill down into the depth of the language to make sure you understand the concepts. E.g. asking you how you would implement a dictionary or what the time characteristics of the default dictionary implementation are etc.
i just choose it because of its batteries-included philosophy, unicode support, and consistency in syntax and macros/environments. note that i talk about ConTeXt mkIV (LuaTeX-based), as mkII (XeTeX-based) is obsolete. but since ConTeXt mkIV isn’t considered stable yet, breakage can occur.
I actually started working on fabric and then started learning python to leverage what i needed from it
You forgot Mono-D :) PS. It's GDC not GCD
For most libraries, I agree that the thought of rewriting isn't scalable and that moving to C libraries with ctypes calls to work across multiple pythons is a good solution. I think particularly in the case of numpy, there are a number of reasons to avoid going this route. A number of the optimizations that pypy is already able to perform, like the delay and jiting of ufuncs, wouldn't be feasible without the current approach. I think numpy is that special beast that deserves extra time and attention to get the best performance.
Depends on what you're doing, but they occupy alot of the same space, so in general answer is going to be no.
If you have the time and the inclination, why not? Python can do things that Perl can't. Ever seen a commercial game made in Perl? I haven't. Python has some very interesting libraries, particularly in scientific domains. Python is much easier to embed in other applications than Perl is. Python tends to be higher level (and have higher level libraries) than Perl. This is both a blessing and a curse, but it is a great reason that knowing both Perl and Python lets you pick the right tool for the job. Here are a few situations in which you might find Python to be the more effective tool: Most importantly, while Perl is legendary for its ability to write obfuscated code (I know most experienced Perl programmers try to keep their code clean and legible) Python takes the opposite approach. It is hard to write ugly Python code unless you're specifically trying to do it, and even then it takes particular effort to make it confusing. Enforcing proper whitespace helps everyone's code look decent, not just yours, and makes it easy to understand and follow. Like Perl, Python is great for simple shell scripts, command line utilities and daemons. But Python would leave Perl struggling to compete in areas like web development, cross-platform application development, and game development.
I know about vibe.d, in fact I've been wanting to do something with it just because I like D and fuck everything else but I haven't got enough time to spare. I've beel following D since 2007 I guess and I've been watching everything you describe lively. I think I'm gonna write a simple arcade-like game in D just to practice my skills. The thing that I really didn't know was D-ide, looks nice! I'm gonna check it out right now. HAve you seen the .net implementation of D?
Python is better for numerics and building larger things sustainably. I honestly don't think there's a *huge* productivity advantage though, nothing on the scale of moving to Python from something like Java. Do it because you want to become a better programmer and learn some other ways of thinking. Not because it'll "replace" Perl for you. Also the libraries in Python aren't more numerous, but they do tend to be higher quality and play nicer than stuff from CPAN.
It's easy. You'll have more job opportunities and you'll also become a better developer in the process (because you can actually read code after it has been written)
This is quite easy to do with mpl, have a look at: https://gist.github.com/4047834 
Addendum to #1: Compare: $foo =~ /.xml$/ foo.endswith('.xml') 
PIP is never going to be as close to CPAN. EVER. Sorry, but it just isn't! EDIT: Can those who downvote please go and install one package from CPAN and one package from PIP and see the difference.
&gt; operationalize and productize Maybe we could synergize our target assets and make a key play in our core business area?
Pragmatism, the enemy of upvotes, it seems.
Two years ago I asked myself the same thing. I haven't written a line of Perl in a year. Python is so much...cleaner. I can go back to Python code that I wrote a month ago and understand it at depth almost instantly, whereas it used to take me a few minutes to re-figure it out in Perl. A few minutes doesn't sound like much until you get used to a few seconds. TL/DR: Once you go black you never go back.
Python is similar to perl. Except that it doesn't cripple the mind.
Perl is awesome and powerful. Python is awesome and powerful too, yet in a totally different way. Learning Python will bring you another perspective on coding but that requires you build something substantial with it (to learn the up and downsides of the various idioms). As an appetite wetter read up on descriptors for instance.
PROS: As someone who made the switch from Perl to Python, I can tell you that my primary reason was modules. Perl has tons of modules available via CPAN, but the standard distribution at the time did not include many modules that i felt should be in there.... XML processing, CSV handling, simple databases. Most of these things that have become more common have been brought into the python standard distro. I can't tell you how frustrating it was to go "man, this would be awesome if I tied a little DB like SQLIte to it" then have to go install DBI, DBD::SQLite and the billion dependencies from CPAN to make it all work. Some in the community would say that this adds 'bloat' to the distribution, but it my mind for the # of *megabytes* that this adds to the distro, you're not really bloating anything out. CONS: Many changes have occurred from Python 2.4 to 2.7, and now with 3.3 out there are even more. There's some backwards compatiblity issues you need to be cautious of that in my experience was not something I ran into nearly as often with Perl as I did. Heck I think almost every box I touched had perl 5.4 or 5.5 on it. Today my environment includes python 2.4,5,6,7 and 3.2. 
I'm sure this is totally off-topic but could you compare D to Go for a C++ fanatic? I love Go and it solves a lot of use-cases where I'd hoped Python would. But I keep hearing about D and it sounds extremely interesting.
How's the view from that cherry picker you have there?
Because every Perl programmer I know that learned Python, always expresses to me "why the fuck didn't I do this sooner?!"
well it's probibly not a good idea to go to church and start telling the people there that they don't need jesus.
This was posted about a month ago. It doesn't provide an example like you said about goats or anything, but it's a Python natural language processing project. [Here's the link to the post.](http://www.reddit.com/r/Python/comments/1179if/looking_for_collaborators_on_a_pythondjango/) Hope this helps, sorry if it doesn't :(.
&gt;&gt; Python can do things that Perl can't &gt; nope Easily compilation into LLVM-IR like with numba? Even easy compilation into C modules like Cython. They key being "easy." 
I'd like to contribute. I was working on theory code with a similar idea. I'm a python person, django person. Web stuff is my day. I can't promise to add a lot of coding hours right now as I have a demanding job. but I'd like to help as I'm very keen. It's also fair to say I have _zilch_ (maybe less) real work knowledge of NLP. -- I hope I haven't sold myself out of a job here 
This is awesomes. It's very similar to the project I have in mind - I've signed myself up to help.
As a bonus the python string method is faster than the regular expression in this particular use case :)
You don't expect me to disagree with everything you wrote, do you?
That has also been my experience. I used Perl for a year or two before switching to Python 7 or 8 years ago. Haven't needed Perl since. I'm sure there are areas where you'd run into Perl more often though. 
Everyone needs enough Perl to get by, except when you don't. I'm a Python guy first, but anecdotally, I'm seeing a whole lot more Ruby and Python than Perl these days. Your Mileage May Vary, obviously. I don't think you really *need* Perl anymore, depending on what you're doing. To say everyone needs *some* Perl is flat out wrong now. Likewise, you don't necessarily *need* Python or Ruby if you're at a Perl shop. Overall point is: What you *need* depends on what you're doing. Perl isn't a no-brainer anymore, though.
Well, that is kind of what you're implying when your post is nothing but unsupported criticism of minor points. Adding even a few words of qualification like "I mostly agree but..." would completely change the tone of your post. Your criticism still doesn't include enough details to be worth responding to, but at least it wouldn't look so hostile.
pip is a CLI tool. CPAN is a web-based repository of software, with accopmanying CLI tools. These can't be compared. PyPi and CPAN can be compared, however. While PyPi does indeed suck, crate.io and a few other efforts are improving on it, and may eventually meet or exceed CPAN (which has stagnated).
I discovered Perl back in 97, and thought it was absolutely fantastic. As I learned more I got frustrated with it, and abandoned it in favour of Python. I wrote [a rant about this](http://www.garshol.priv.no/download/text/perl.html) in '98 that got all the Perl anger out of my system.
In general, even if you never use a tool or language, learning it introduces you to new ideas and approaches towards solving problems. There really isn't anything that you can do in Python that absolutely can't be done in Perl...but opening your mind to new paradigms is never a bad thing IMO. Also, of all the languages out there, Python is just about the easiest to get up and running with. You can learn enough to be useful over the course of a weekend. 
What might look like minor points to you strikes me as important. Turing complete languages have the same capabilities. There is no magic dust in language X that allows it to be used in super-duper commercial games. The notion of "high-level" is pretty much unambiguous when used in the context of programming languages. It makes no sense to say that Python is "higher-level" than Perl. They are both high-level and that's that. The explanation you are looking for is actually less technical - Python is easier to write, read and understand so it allows every Tom, Dick and Harry to rewrite the ID3 parsing library. No experience required. This is usually a good thing, but let's not be smug about scientists using Python just because they are amateurs when it comes to programming. Take the language as is, with the good and the bad, and use it when appropriate. Same for Perl, Lua (commercial games! :-) ), Common Lisp, etc.
I completely agree with this. I'm up over 10 languages and my experiences with each one have developed me further as a programmer. And each one is useful in its own context -- there is a context where each one is *more* useful than any of the others.
&gt;But Perl and Python are too similar for one to really expand your horizons if you already know the other. I wouldn't completely agree with that. Python uses quite different paradigms than Perl. I think going from Python to Perl would make little sense, but you could probably see some benefit going from Perl to Python.
Perl was the first scripting language I learned. I learned Python when I realize that large projects were harder to manage with Perl. Python has a really big strength with readability and code reuse.
Perl is great. However, I think if you're a Perl user, considering where that language is in its lifecycle, you should start looking at some of the newer languages that have picked up the lessons of Perl and are more active these days than Perl. CPAN is great, but CPAN has this reputation of being *the* big repository of modules, and that reputation is lagging behind reality. [Reality is](http://modulecounts.com/), Rubygems eclipsed CPAN in size a while ago and is closing in on double the module count, while Python's PyPI is closing in on being the next repository to surpass CPAN's size. And Node's npm is growing at a rate to where it will close the gap pretty soon, too. As you can see by the growth numbers, CPAN is pretty flat, while Rubygems and npm shoot up like rockets, with PyPI right behind them. That's where the activity is now. Now close your eyes and imagine 5 years from now. Perl's not going to stop being useful, but you should be seeing a very good reason to look at those other, much more highly-active languages. There's a lot going on in those communities. This does not mean Perl or CPAN are bad or outdated. Far from it. But if you're asking the question, "why should I learn Python?", well, that's one valid answer. You should look at Python and the other languages I mentioned because of how much activity there is and how much useful code is being shared in these incredibly fast-growing package repositories.
Please keep us updated on this. I'd love to see a library like this.
Here are my thoughts on the subject from 1999. http://www.netfunny.com/rhf/jokes/99/Nov/perl.html
you really should x-post to r/sanfrancisco. There are enough geeks in the city to appreciate this.
I am one of those guys. When i was in grad school 10 years ago a postdoc friend of mine constantly reminded me that i needed to learn python. Well, 10 years later and millions of lines of C/C++/Matlab code i finally picked it up and im glad i did.
yes, because Perl
I think the most important reason to learn a new language is to grow as a developer and be able to solve problems in a variety of different ways. Each language has its strengths and weaknesses so being able to solve problems using many languages is extremely beneficial to your general problem solving skill set.
I would not put D and Go in the same category. While both are general purpose languages, D is pretty much in the camp of C++ (systems language, focus on metaprogramming, multi-paradigm, can get close to the metal, etc.). At the same time, it is safer than C++ (immutability, optional garbage collection, etc.) Go takes a much more simplistic/minimalistic approach to things. It is mainly an imperative language with some concurrency constructs built into it. As such, it lacks certain constructs that exist in the C++/D camp: no templates/generics, very simplistic error handling (no exceptions, Option types, etc), no notion of immutability built into the language, no const variables/pointers (which is surprising given that the language focuses on concurrency), etc. On that note, you might be interested to look at Rust from Mozilla. It has aspects from both camps above, and adds on some more. It seems to be coming along very nicely. It is a systems language that has concurrency built into it (channels/ports/pipes), at the same time it has generics (not as complex/advanced as C++/D's though), clean syntax (type inference), some functional inclinations, non-nullable pointers/references (which I think is a very big plus), immutability by default, etc. There is an ongoing project by Mozilla to use it to write the core of a web browser: https://github.com/mozilla/servo 
There is so much glue code out there that the syntax basics and understanding simple data structures are worthy efforts. Look at Git. That said, these days I much prefer Python.
Because it's fun!
Because people like me would rather stick an ice pick in their ear than look at another line of Perl. But if you are the only one that will have to use it an maintain it or are sure only Perl enthusiasts will take part in those tasks, go for it.
I've never once gone back to Perl after I started playing with Python. It is so much easier to use well than Perl. I shudder to think about using Perl for anything now. 
Yes, because everything comes with a Python API these days and not so for Perl. It will seriously take you one day to learn Python. It's practically the same thing. And the regex syntax is 99% the same.
More people use it, it's easier, it has better libraries, and has better support.
Hahaha, I know, I know. :) It's a good gig so..meh... back to Perl it is. :)
I'm also one of those guys. Did perl for most of my quick one shot type scripts and small applications. Then I decided to learn python, mostly because I needed to embed it into a larger Java environment. Once I learned Python, I look back at all my Perl days and wonder how I ever managed to live.
Out of curiosity, what kind of Regex use cases would be faster than regular string operations?
That's my favorite part of Python, and is the way it should be. 
LOL!
I read a snippet of PHP the other day and thought it was Perl from my limited experience with Perl. Not saying the case proves the conjecture, just giving an anecdote.
Without seeing the rest of your code, that looks like you're using a dictionary when you really want a struct/record/object. Yes, in most dynamic languages it doesn't matter much which you use, but in statically typed languages things get difficult if you use a dictionary in the wrong place.
There's always [pyinstaller](http://www.pyinstaller.org/) to create portable executables from python scripts :-P. 
Looks very cool. I can see this being good in my next projects
Half of that is solved by text editors handling pasting indented code well. [Chocolat](https://chocolatapp.com) has never once failed to adjust the indentation of pasted Python code correctly for me. Chocolat is closed source, but at a guess, I think its algorithm is this: - If the first line of pasted code begins with whitespace: - strip that amount of whitespace from the beginning of every line. - If the first line does not begin with whitespace: - If the first line is a statement that does not expect an indented block: - strip the amount of leading whitespace on the second line from each line (except the first, obviously). - If the first line is a statement that expects an indented block (ie ends with a colon): - strip one indentation level less than the amount of leading whitespace on the second line. - Finally, add enough whitespace to every line to get it to the position of the cursor before the paste button was pressed. Whether Chocolat does it specifically that way or not, I can't think of a circumstance in which this method wouldn't work (although I haven't thought about it in great detail, so such a circumstance may exist). The only difference between an implementation of what I just described and a tool like perltidy is that it would need some contextual information at the point the paste happened, and so would need to be implemented as an editor plugin rather than a generic commandline tool that could then be set up as a hotkey in any editor or whatever. So the whole code-appearance/pasting-indentation tradeoff is still a tradeoff, just not as big as it seems depending on your editor. Other than that, refactoring tools (PyDev for Eclipse, and the Rope library) and style checkers (pep8) do exist for Python. &gt; It's a bit like comparing Linux to say iOS. "theres more than one way to do it, choose the one that fits you best" vs "this is the right way, do it like this or don't do it". Neither is inherently better, both have their pros and cons and everyone has their own preference. Agreed. In fact, I think the Zen of Python's "There should be one -- and preferably only one -- obvious way to do it. / Although that way may not be obvious at first unless you're Dutch." is a deliberate reference to Perl's "There's more than one way to do it". *edit:* pseudocode formatting *edit2:* basically undo the first edit since it made it harder to read
Readability counts.
It's on my list. Despite 20 years of jumping head first into difficult coding challenges and loving languages in general, Haskell scares me. One of the smartest people I've ever known - lead programmer at a large company - has been learning and using it for awhile now. I finally asked him to really explain monads to me, as I've tried many times to figure out what they are on my own. He said he doesn't really know what they are, just how to use them. He wasn't even certain we could really know what they are.
Forget about monads, the language has more to offer. Monads are but an excuse to use imperative code in a fully functional language.
 (?&lt;!foo)(ba{3,5}r)\s*[Bb]az(?!\1)\b The state machine of a regex will do this quite fast, but your manual functions will be hard to read and miss a case, or be obviously right but slow.
Static languages like C and Fortran have the same problem. Worse even with the implicit assumptions in Fortran. This has little to do with dynamic typing.
Can you explain why this is bad? And I don't particularly see the connection here between function import semantics and typing systems.
I'm not talking about compile time. I'm talking about before that, when you *as a human being* read the code.
1. When your extension changes from .xml to .xslt, you have to change in two places. 2. "4" is an avoidable bit of hardcoding.
Indeed! This was the first perl programming I've done in 11+ years. I completely forgot about the 'eq' requirement. That also means that I've never seen the language constructs you've used here.
Then under your definition C (pre-C99) and Fortran are not static languages. They don't know about the function call locations until link time. % cat x.c int main(void) { openurl(); } % cat y.c int openurl() { return printf("Hi!\n"); } % cc -c x.c x.c:2:3: warning: implicit declaration of function 'openurl' is invalid in C99 [-Wimplicit-function-declaration] openurl(); ^ 1 warning generated. % cc -c y.c y.c:2:10: warning: implicitly declaring C library function 'printf' with type 'int (const char *, ...)' return printf("Hi!\n"); ^ y.c:2:10: note: please include the header &lt;stdio.h&gt; or explicitly provide a declaration for 'printf' 1 warning generated. % cc x.o y.o % ./a.out Hi! This holds even if you have declarations. How does the compiler know at compile-time that "openurl()" will call the correspondingly named function in libX or libY, given that the library order may affect function resolution?
All I wanted to show was that Perl supports non-regex approaches, too. It's true though that a function/method for that specific purpose is a more convenient way to check suffixes. Therefore, I wrote a tiny Perl module: % cat endswith.pm package endswith; use Exporter 'import'; @EXPORT = qw(endswith); sub endswith { my ($str, $suffix) = @_; substr($str, -length $suffix) eq $suffix; } 1; 
&gt;Python is easier to write, read and understand so it allows every Tom, Dick and Harry to rewrite the ID3 parsing library Ouch, this one hit me too close to home.
[ipython notebooks](http://ipython.org/ipython-doc/rel-0.13.1/whatsnew/version0.13.html) are really cool too
No. You're conflating two entirely separate issues (import semantics, the issue I brought up, and compile-time type checking, your pet issue), and using it as an excuse to bring up something tangential, which is completely irrelevant to the conversation, which is the value of learning python to a person who has experience in perl. Unless you can explain how explicitly named imports are a wart of dynamic languages, and why they'd be impossible to implement in static languages, and why this would be a good thing.
Right. Which is why you were off-topic. 
Just to give the complete picture, there are also several programmers who, having learnt Python, still prefer Perl when given a choice. I'm one of those, and so are a few of my ex-colleagues. 
Yes! Build a webapp, it's fun and you'll learn a lot, I just finished building my homepage site/app, it aggregates all my social apis into one place, all written using Python http://jmoz.co.uk
example of such a black magic? :)
I used Perl for years and years. One of my projects in Perl eventually became a website that did billions of page views a month. I mostly use python now. Reasons: - I make dumb datastructure mistakes far less. - development seems to go faster, even though I am not yet as good in Python as I am in Perl - there is just as much capacity form wacky language bending as Perl - There is more numerical, scientific, and statistical support (numpy etc) The downside I see is that the web deployment picture is less clear. Perl -&gt; mod_perl, python -&gt; a huge list of possibilities, each with their own fragilities.
Yeah, actually pyinstaller is freaking amazing. You can compile any of your python programs (even PyQT stuff, etc), and it works cross-platform! One caveat is it doesn't play nice with OSX's python-coca bindings. I [patched both to work with it though](https://github.com/Fitblip/PyObjC-PyInstaller). 
How about CPAN in your experience?
I have (well, not Haskell, but Clean, which is very similar to Haskell, and not Forth, but Déjà Vu, which was inspired by Forth). Perl and Python still seem quite different to me.
That last sentence made me laugh. Thank you. I should be said more often.
Nothing has been as formative as Lisp to me though. Well, SICP, really. It really changed my game, even though I probably will never use Scheme professionally. Maybe that is reason to be sad.
can you expand? alternately, since there's a similar discussion in r/perl apparently, can you link to a comment you feel representative?
Probably not the library, but still: http://www.clips.ua.ac.be/pages/pattern
Very awesome, thank you for sharing. Out of curiosity did you try out Hyde? What made you use Pelican in the end? 
Maybe OP prefers python to ruby. I wish there were a way to get non-technical clients about to use these static CMSs.
Honestly that just sounds like a gap in your learning habits: Not making yourself do these small but useful projects that get you to the point of having a serious as opposed to academic interest (or choosing to do projects that are not that useful, and hence don't hold your interest as much) That said, I never try to pick up new languages for the sake of it. I believe that doesn't work well; you need to have some specific application in mind that means something to you. For example: * I learnt Python because I found myself writing lots of simple utilities, could see all these other Python based utilities around, and C remained an eldritch abomination. * I learnt shell script because I was doing a lot of simple batch processing, and bash is easier for that than Python (when quoting errors are not invoking homicidal rage in me, that is.) Of course, other people are different from me; but I'll stand by the assertion that *for anyone, learning projects need to be meaningful for the learning to stick*.
I would say, if you knew both Python and Ruby, you should definitely check out [Octopress](http://octopress.org/).
You don't really need to understand what a monad is to use Haskell. You just have to understand what a monadic function is. It's effectively the same thing that most languages call a function. A function in most languages performs an action that returns a value. A monadic function is very similar, except that formally, it returns an action instead of actually performing it. But, the runtime executes the actions so it ends being basically the same thing. You can think of `f &gt;&gt; g` as perform f then perform g just as easily as you can think of it as an expression resulting in an action that performs the action returned by f, then the action returned by g.
Cool. Consider committing your code to the examples section of the pyMCU documentation. 
The more small explanations like this I get, the more I feel I'm developing a sense of what monads are. I'm probably way off base :) Thanks.
I just used octopress , which uses ruby/jekyll whatever, it was quite possibly the most painful experience ever. What exactly does what? It uses 8000 different technologies so you can't figure out what to debug if something goes wrong. Liquid templates are pretty bad compared to stuff ive used in other languages (like Twig) , and of course ruby was a pain inm the ass (i wanted to install a gem, but it wouldn't ever load it unless it was in the Gemfile, why the fuck is this one file dictating what i can and cannot require in a ruby script? )
Ah, sorry, reading comprehension fail. However, it is a large part of the rhetoric hardcore Go users say.
I am using PostgreSQL. I am building an application using Pyramid because I prefer using SQLAlchemy over the Django ORM. Also, I can use SQLAlchemy in non-web projects so learning SQLAlchemy is helping me strengthen other parts of my Python skills, same can not really be said about Django's ORM. I am using Mako (the same templating engine used by Reddit). Never liked Jinja's syntax, and Pyramid has built-in support for Mako (Chameleon as well, but not a big fan of that one either). In my case I am deploying on top of a dedicated server. I like Heroku but as soon as you have even a little bit of a database need your costs go up tremendously. Yes they offer a lot of value-add, but in the past when I have used them the sites weren't making any money, which makes it much more difficult to be happy with their pricing.
At least for my needs there are multiple times that stuff has to be recompiled, and unfortunately blogofile has no way to leave certain pages alone if they don't need to be touched (such as yearly archives, only the latest really has to change, unless something in the side bar has also changed, in which case a full rebuild is required, or a template has changed). My main blog (http://funcptr.net/) has only 17 articles at the moment with about 25 or so in draft status that I still need to clean up and on my MacBook Pro 2.4 Ghz it takes about 5 - 6 seconds to build, that is a cold build (as in, I haven't run blogofile nor touched any files on disk besides the new blog post I created, so everything has to be loaded from disk). That isn't too terrible, especially considering it is a slower hard drive and all of the extra processing I have built into it, and the URL structure I require. On a hot build it takes about 2 - 3 seconds. It creates 242 individual files. On another blog that I am helping maintain for a friend (can't link to it, sorry) he has 300+ posts, when he compiles it using blogofile it takes a good minute or so. He has tried using Hyde and various other static generators and they have the same issue, and some don't allow the customisability he requires/wants. When you start getting into complex layouts, or want to have the ability to change content on certain pages (such as latest articles in X category) you have to rebuild the entire site from the ground-up each time you change content. Even parts of the site that may only get 1 or 2 hits in a month need to be re-generated. With a dynamic site you take the hit when the visitor hits the site, but most of the time the data stays in the DB and doesn't need to be cached. Don't get me wrong, I think static site builders can be incredibly powerful, and in most cases they are more than adequate. The one use case why I decided to go back to building my own (other than the fact that it is an excellent way to learn a new framework!) is because I want to be able to blog from anymore from almost any device. I want to be able to create drafts on the fly and flesh out the details later. I almost always have my laptop with me, but for when I am at work and want to jot down some ideas, or a piece of code that I want to remember from the future having a web interface works really well. I could use a gist, but I have noticed that since they are not at the forefront of my mind that I forget about it, and no longer have a central store for all my ideas and thoughts.
One way to get around this is to make an Amazon micro instance that runs your DB, Heroku runs your app, and all for near free?
Because certain pages changes based upon new input. I may save time for only half, or less of the pages because of the extras I have in my pages. Yes, there are systems that allow incremental builds. The one thing I really wanted to add, but still haven't, is the ability to have a list of "See other articles" at the bottom of articles. This requires that all previous articles need to be regenerated due to this one new article being added, or at least all the articles that would be a good fit, on top of content in the side-bars such as 5 newest articles, stuff like that, eventually incremental no longer functions correctly.
That's not true. You can build incremental compilation that does not re-generate HTML from already compiled posts. Yes, you have to re-render the template but not the content. I can rebuilt my site from scratch (using cache) within one or two seconds. Rendering Markdown and reStructuredText is expensive, but filling templates is not (at least for ≤ 1000 posts, since the filesystem is a lot slower than a dedicated sqlite db).
This release includes: * New website * Migration of the test suite to nose * Documentation is now written in sphinx * 2 new layouts * Many new widgets * Lots of bug fixes [Please see the release notes for more details.](http://docs.qtile.org/en/latest/releases/0.5.html) In the 27 months since the last release there have been contributions from approximately 49 hackers in 12 countries. A giant thank you to all of them, as well as anyone else who contributed to the project in other ways (issues, irc, mailing list, etc)!
And in my opinion better than a "cloud" service: I do scientific research in a no-profit organization and I would not use a "cloud" service to analyze my data. My big question with services like this is: what's the data policy? What happens if the service is discontinued? And so on.
That would indeed be a good idea.
Because it makes the signal to noise ration in your docstrings plummet, it is not a complete suite for testing IIRC, and the Zen of Python says, "beautiful is better than ugly" and "readability counts." I would think putting all of your unit tests in your docstrings would go against both of those quotes. To clarify, I'm not saying that putting examples in your docstrings is bad (assuming they are helpful), only using them as your unit tests.
I have five Pythons on my OS X and have no problems. 
&gt; To clarify, I'm not saying that putting examples in your docstrings is bad I think the initial idea for doctest was to check that such examples are correct, since incorrect examples will be worse than no examples. For such a use case, it's fine, but anything beyond that... just don't do it. 
That is *an* opinion. For those who have a barrier to testing, docstrings can reduce it phenomenally. For those pushing a testing methodology docstrings might not fit in. If you don't 'do' testing, you do, but you probably don't leave the evidence hanging around (unless what you deliver is habitually grossly wrong). Doctests are a way to make some tests stick around that doesn't cost the earth. For those that say its doesn't fit some methodology or some such rule, then try doctest anyway - it may be the only way to get experience of what they are saying. You can't loose compared to no tests and you might learn more and become a better programmer starting with doctest and progressing through its features rather than being dumped in some huge testing framework copied from some other language without Python idioms and that is harder to learn.. 
Actually, with respect to your last statement, I agree far more with Tetha's reply. Doctest does a great job of making sure the examples in your docstrings are not providing wrong examples. nose, pytest, or anything else that was specifically designed for testing would be better than a separate file with tons of docstrings that exist for testing.
Yep, I'm sad because I won't be able to find it when and search it in a few days.
You should call it Dubya.
I don't. Try emailing the lead developer.
This looks really nice. I work with Pyramid and though I've been happy with the existing debugger I'd love something better.
little late to the party on this, OP. there were topics up here about this happening last week, not to mention specific discussion of the PEP on python.org.
but there's already better solutions for that - cython on one hand and pypy on the other.
Nice! I used ast in the past (can't remember all the details) but ended up tapping into the 2to3 library that has a someway better understanding of the python language. 
&gt; Defeats your editor, syntax highlighting etc. The effort required to tell vim that a doctest is "&gt;&gt;&gt; " followed by python syntax is minimal. I have the same power available both within straight python and doctest files. &gt; Makes it harder to get in their with a pdb The effort required is even less &gt; tests things by repr, rather than by something more precise You have the entire precision available to any python programmer. And using doctests for while makes you (well - made me) that little bit fussier about what a repr() actually shows, which has made my debugging, and my REPL usage a lot easier
Read this in the voice of George Dubya Bush.
I can't seem to install this through pip under ubuntu 12.10. Anybody else getting complaints that ft2build.h doesn't exist?
His point is that, being a dynamic language, just statically compiling it can't achieve huge speed improvements. PyPy's RPython works because it's a subset of Python with the dynamic features taken out. Cython works because you can add type annotations to make key bits of the code static, which lets the compiler make them fast.
you might want to try out http://errormator.com - its mainly for production use though.
cool, disclaimer: I the owner, so feel free to PM me in case you need assistance.
&gt; No werkzeug dependency. Nope, just a w-debugger dependency now. :P
I've had a look and I won't be going for it for a few reasons. Firstly the system I'm using is behind several very strict sets of firewalls and can't access the internet except via a proxy (and even then new domains need to be approved etc etc). Secondly I've already got much of the same logging stuff in place (Exceptions are logged in depth, all page views have some basic info stored about them). And thirdly the project has minimal budget so I'm not using anything that costs money beyond what I really have to. It does look like a very nice product and the site is well laid out too.
Its already there mostly ;-)
I think he meant Canadians would be going
for, while, if/elif/else, try/except, def, function calls, assignments (one variable), obj[index], obj.attribute. It only translates the syntax not the semantics. It only supports pyhton syntax that has an equivalent in the target (for example try/except works in JS but not in C99). If the decorated python code calls a function f(), it is assumed the function is defined in the target language. I suppose one could create a set of functions that have same names in all supported languages and perform equivalent tasks but that is not the goal of this library. The goal is not to able to translate any python code but to be able to write using python syntax any code that can be written in the target language syntax.
Pfft... whatever. I know all of those technical details and a whole lot more. Deep knowledge of Python, JavaScript, CSS, HTML5, and whatnot for the actual app (Gate One). Database, security, and PHP skills plus decent enough design skills to put together a nice looking web presence for the company (Liftoff Software, Drupal BTW). Skills with Amazon, Rackspace, load balancers, DNS, and enough automation to handle the simultaneous load of Slashdot's front page plus Reddit? Piece of cake. Any Tuesday, whatever. Knowing how to market and sell? Knowing how to price software? Those are vastly more difficult (for me, anyway). If only I could clone a Github repo to solve those issues I'd be all set!
It's not a need, but you lost at least one potential user for going that way. There's so much **actually** free software out there that I don't even bother looking at GPL tools anymore. Sounds like a neat idea though.
wtf-db
I use a script to get data for mutual fund and etfs; I run it at the end of the year. See, I keep an excel chart to track portfolio %s -- particularly some data at the end of the year, and so this script gets this data so I can put it into my spreadsheet and make better investment decisions. Cuts down the effort a whole lot.
Very interesting, thanks for the insight! May I ask as well, what would be a proper way of live-reloading python modules (or pieces of code) with a program that's running (without restarting it entirely)? It's a pretty relevant subject to me, so any resource you could point me to would be amazing, thanks!
I hope soon. I use Python 3 exclusively now -- granted I'm not prolific, but I'm interested in learning more and I think it would be awesome for Python 3 to be more accepted.
New innovation vs old garbage code :P
I really wish the Matplotlib team made more regular releases -- usually it's about once per year.
idle? You are aware that vim is available for every OS and language you can think of (and most of the ones you can't) right?
I'm attending/speaking. Hope to see you folks there! (Also: agenda to be posted hopefully this afternoon (CET), but we'll see)
Ok folks, it's now officially *wdb* with a similar API to pdb (set_trace). I fixed a lot of things since yesterday if you want to try it again.
Types.
Thanks a lot, I really appreciate it!
That is very vague. Are you saying perl has no classes? What does interactivity even mean here? What are examples for things that are cryptic in perl and terse in python? Which exact features or lack of features gives perl less maintainability or readability? In what way is python more extensible? The only clear point i understand immediately is Perl's lack of embedability.
I'm not sure if this is exactly what Isvara is talking about, but things like `'5' + 3` (yielding 8) are possible. That can make tracking down certain types of bugs rather difficult if you dont add things like python's `isinstance` everywhere in your code. 
I can see how one could be worried about that, but in 8 years of writing Perl i've never had a bug occur because of that.
I'll try to answer. Classes, object, inheritance, etc in Python feel significantly more natural and powerful, to me and to many other devs it would seem. Yes, yes, I know I should try Moose. The fact that a third party library is the answer often given when I ask about writing object oriented code is *not* a point in Perl's favor. Interactivity probably means the Python interactive interpreter. When you type "Python" you get this neat little shell that can actually be pretty useful for debugging code, testing out snippets, etc. Perl doesn't seem to have that, at least not built in. The point isn't that Perl is cryptic and Python is terse, the point seems to be that cryptic code is almost something Perl mongers are *proud* of. Perl golf is a thing, and there's really no excuse for it. The point is that readability is one of the main points of Python. Over and over, when I've gotten stuck on some Python problem, I'll check the ref and be surprised at how natural and readable the solution is. When I do the same with Perl I find myself shaking my head and wondering why it's like that. Maintainability and readability go hand in hand with that. Surely you've also heard Perl referred to as a "write-only language"? Another thing I find telling - a lot of programmers push for better testing. But Perl seems to be the home of the really hardcore guys, the TDD guys, the ones who say you have to have a test written for a function before you write the function. In Python, you have a far higher chance of understanding what a function does and if a change will break it, because you can actually read and comprehend the code. Perl you HAVE to use that level of testing if you're doing real development, because there's no other way to do it. I have less experience with the embeddability and extensibility. Here's my biggest beef with Perl. $_. sub foo { shift a; shift b; /* lots of stuff */ bar(a); /* lots more stuff */ } sub bar { shift w; shift x; shift y; shift z; /* just oodles and boodles of stuff* } The fact that that's perfectly OK makes my head hurt. A basic thing I need from a programming language is structure. I need it to let me compartmentalize my data and logic so I can think about them. I can't actually think of another language that doesn't have some sort of function definition. "Here is what this function takes, here is what this function does, here is what it produces" should be as easy as possible to deduce. Yes, it would be great if the previous coder added comments to make that blatantly obvious, but we're talking about the real world here. Perl just doesn't seem to have a way to do that. Debugging the above code snippet is made much harder, to me at least, by the fact that you need to know what calls "foo" before you know what gets passed to "bar", and keep all of that in your head as you debug bar. I mean... while(&lt;FILE&gt;) { chomp; /* do stuff */; } for line in file: line = line.strip() # do stuff These two snippets are equivalent and are (to my knowledge) the idiomatic way to do this operation in both languages. To me, programming effectively is all about having to remember as little as possible in order to know what each line, function, class, module, library, etc does. The Python snippet just seems unequivocally easier to read and parse. I am doing something to each line in a file, I don't have to remember what the &lt;...&gt; operator does or that it sets the $_ variable. "line" is a string object, and I'm stripping whitespace from it and assigning it back to itself. I don't have to remember that chomp is getting an invisible parameter, and I don't have to remember where that invisible parameter is being set and so-on. No, it's not a lot more to remember, but it's incrementally more than Python needs me to. And that adds up after I've written a thousand lines. I think that may be some of the issue. A lot of people that like Perl seem to be sysadmins who use it for advanced shell scripting. That's fine, but that's not real programming. Yes, I've read Modern Perl. It didn't really impress me. I will not lie, I liked Perl after I read it. Then I spent a few months debugging and trying to improve a very kludgy legacy Perl system. With like threads and stuff. As far as I can tell thread support is still pretty crummy in Perl. Last time I checked the general answer seemed to be "you're a psychopath if you're using threaded Perl in production" and that was from people that *liked* Perl. Sure, the answer to many of my issues might be in CPAN somewhere. But that begs another question. An argument I've heard *for* Perl a few times now is that it may be installed on systems where Python is not. But if I can't install Python, it seems doubtful I will be able to install CPAN extensions... and if I can install those why not install Python?
Perl only has a small number of real types -- scalar, array, hash and (I'm not a Perl expert, so I'm not sure if this is a real type) file handles, and it's very weakly typed. You don't get anything like a metaobject protocol, and "classes" are a poor hack on top of hashes. Yes, modules like Moose improve on this, but it's not core to the language, and that shows. Python is more strongly typed, and will give errors if you attempt to combine values in ways that are not meaningful. Perl is also the only language I've ever seen that has what I choose to call 'return type polymorphism'. Very odd, and quite antithetical to robustness. 
For me it was iterating and manipulating hashrefs in perl. I was on a project where we stored data in a minimal form of attribute-less XML. In perl, we read in the data and then had to deal with this kind of stuff: ... while ( $time &lt; $lasttime ) { foreach my $name ( sort { $a cmp $b } keys %{$allnames} ) { $bytime-&gt;{$lasttime+1}-&gt;{$name} = 0; } $lasttime++; } ... After I got used to python syntax, that kind of perl style punctuation noise bugged me more and more. Once I figured out how to do python one-liners and use itertools, my eyes got tired just looking at all the curly braces and special variable modifiers in perl. When I have to go back to some bit of code I wrote a year ago, it takes me significantly longer to get my head around perl. Python just reads easier to me now. And it don't think it is just because I am more familiar with python now. I noted this back when I was first learning python. Right now I'm probably equally strong in both languages, but I'll use python everywhere I can now.
I'm not quite your target audience because I still tend to use both, but one point I might make: Perl makes a lot of things handy at the expense of having to know more language details. Examples of the sort of details I mean include: * how aliasing in subs (`@_`) works * also how aliasing of list items works while looping * interpolation rules (ex. `@this` interpolates but `%that` doesn't) * regex rules (ex. "what does a match do in list context with a `/g`?") * copy semantics (`my @this = @that` makes a shallow copy) * the way the sigil changes when dereffing nested objects * rules about context * details about how package globals, `local`, and `our` work * remembering when `$_` is the default and when it isn't * knowledge of modern alternatives vs. older-style modules and/or older ways of doing things So, I think one feeling the new Perl-&gt;Python convert might have is that of relief of not having to keep in-mind so many language details. Python is a pretty simple language. 
People use dict() instead of {} to avoid having to quote the keys. 
After spending years writing "OO" Perl, I can honestly say it was a huge relief to move to an interpreted language where everything was an object and they were "real" objects. In Perl it always felt like a hack, especially with multiple-inheritance. And don't get me started on the even-length arrays (aka associative arrays or hashes). It can be perfectly readable, but tends not to be in the wild. We had a running joke at work where we said we'd hire a Perl programmer immediately if we asked them to write code during the interview and they held down shift and mashed their hand along the top row of the keyboard.
And things like *'5' + 3* (yielding '53') are possible in Python which can be annoying if you hadn't checked your input first. It's really all *Swings &amp; roundabouts* - http://news.ycombinator.com/item?id=4617021
'5' + 3 raises a `TypeError`. 
Not an interesting post to start with and way too long.
I would never have guessed that there was any difference. Glad that I always use the curly braces, which turns out to be a happy accident.
TL;DR: Calling a function is more expensive than not calling a function, and functions have a big overhead in CPython, particularly when using keywords.
It seems python is moving a bit away from "there should be one, and prefrrably only one obvious way to do it". Maybe i need to switch to python 3..
* remembering when $_ is the default and when it isn't I've stopped remembering, and just assign to a named variable every time. 
I didn't mean is be unable to install it technology wise, I meant policy wise.
&gt; And yet, PyCharm suggests using dict() instead of {} syntax. No, it does not, it has two *intentions* for converting back and forth between literal and callable forms. They are not inspections, and PyCharm doesn't suggest one way or the other, it just provides a lightweight refactoring for switching between them when possible (which also means you can trivially have pycharm rewrite a callable form into a literal one) I'd know, because I'm the one who asked for these intentions in the first place. It *does* have a dict-rewriting inspection, which suggests rewriting this: d = {} d['foo'] = 3 d['bar'] = 4 d['qux'] = 5 to this: d = {'foo': 3, 'bar': 4, 'qux': 5} &gt; It's not more readable this very much depends on the surrounding code, dict literals can be noisier due to the number of quotes. &gt; now it's revealed that the performance is markedly worse. Which isn't exactly surprising, and might be an issue if all of your applications are dict-creating benchmarks. If *that* is your yardstick, I'd strongly suggest *never* using dict.update because it's way slower than straight setitems.
Since you're not installing it system-wide, policy would seem to be irrelevant. I mean, heck, with App::FatPacker you don't even install it, you just bundle it into your script; and with Object::Remote it never even touches the system's hard drive.
It actually is, especially App::Fatpacker, since you're tying specific versions of libraries into your app when you prepare it for deployment.
Oh, thank you—I am not familiar with the new set syntax as I develop almost entirely in Python 2.6 at this time. I stand corrected.
Essentially this comes down to "function calls in Python are slow". It's fairly common wisdom whenever people talk about optimisation, but I guess people forget that dict() is just not different syntax to {}, it's actually a different operation that just happens to come out with the same results. Presumably Python can't optimise that away just in case you had another `dict` function in the namespace.
I don't know how much of it you could turn into improvements in Perl. To my eyes, a lot of the difference is the simplicity. As others have mentioned, Perl has a lot of features like `$_`, that might save time, but make it harder to think about the program. I think this acts on two levels: people unfamiliar with the language don't know what this mysterious punctuation means, and the use of special variables makes it harder to trace the program logic, even when you can identify them. Removing key features from the language is a pretty obvious no-go. You could [try to persuade people](http://tinypig2.blogspot.co.uk/2008/07/why-dollar-underscore-should-be-avoided.html) not to use particular features, but good luck getting any kind of agreement on that. And if you decide that such features aren't a good idea, why stick with a language where they're idiomatic?
Python3 still has both dict() and {}.
I don't think the performance difference has a huge real-world impact. Since Python 3 makes the syntax for creating empty builtin types inconsistent (there's nothing comparable to ```{}``` for an empty set), I see a case for using ```dict()```, ```set()```, etc. 
The "my ($size, $color, $num) = @_;" thing, while being a bit more compact, doesn't seem to change anything. If a function wants 3 parameters and you pass it 10, you should get an error. That should be natural. Perl that's an add-on. Yes, I realize I could bend Perl into a form that it would be what I want... but why do that when there are well-designed, well-supported languages like Python that already do that stuff?
&gt; If a function wants 3 parameters and you pass it 10, you should get an error. Easy: sub foo($$$) { my ($size, $color, $num) = @_;
&gt;readability before microoptimization Exactly. The situations where a tiny tiny optimization like that will make any tangible difference are fairly rare. If the code in question is being called less than a few hundred times *a second* then just forget about it. The time that you've already spent thinking about it is more than you'll ever save by doing this.
&gt; Only in the most insanely performance critical code is it worth doing this kind of optimisation. If you have to resort to this kind of """optimizations""", you're not using the right language.
That makes sense, thanks.
See, this is a good example. I can imagine the code mentioned first appearing in a legacy app. An old app. I say old because nowadays any Perl dev i know, would write said code more like this: (assuming the less-than sign there was actually a greater-than) my %sorted_names = map { $_ =&gt; 0 } sort keys %{$allnames}; # the sort is actually pointless $bytime-&gt;{$_} = { %{ $bytime-&gt;{$_} }, %sorted_names } for $lasttime + 1 .. $time;
For those who are unfortunate enough to live in Python &lt; 2.7 world, dict() allows you to perform dictionary comprehension.
Yikes. for ts in range(lasttime + 1, time + 1): for name in sorted(allnames): bytime[ts][name] = 0 To be fair: for my $ts ($lasttime + 1 .. $time) { for my $name (sort keys %{$allnames}) { $bytime-&gt;{$ts}{$name} = 0; } } I guess these both perform the sort multiple times, but it's useless anyway, so.
I just stumbled upon [xdocreport](http://code.google.com/p/xdocreport/wiki/RESTSOAPServices). At a first glance it seems to be a good tool. Not Python, but if it gets the job done and has a REST API, it's good for me.
Well, if CPython had dead code elimination, clearly he'd have written the benchmarks differently. But yes, it *is* cool.
 my @args = 1 .. 3; foo(@args); Not enough arguments for main::foo at...
&gt; I posted this a few hours ago but Reddit seemed to have eaten it... along with another comment of mine elsewhere I received your comment in my inbox, but I couldn't find it in the thread. The same thing happened to comment of mine in another subreddit.
OK, busy with other stuff lately but I am a web developer so I agree, REST is easy enough to live with (and if you ever feel really generous/bored, I guess you could implement their REST API in python and send it along to them as a Pythonic implementation of their API). Bookmarked for later. Thanks silviot!
Interesting. I prefer `{}` over `dict`. `"key": val` seems clear than `key=val` to me.
Nitpicking, because it's interesting: Perl effectively has three kinds of types. * Structural: scalar, array, hash, file/dir handle, sub, format, regexp, typeglob. * Data: string, integer, float...? * Contextual: list, scalar, void. (Though with `Want.pm` you can discern others.) Classes aren't built on hashes; they're built on any one of the _structural_ types, which describe the types of variables and references. The `URI` module, for example, uses a reference to a single string (the URI) as its state. The class system in Perl isn't really a hack and is actually quite novel, interesting, and powerful; the real problem is that it's not actually a class system, but a system for _building class systems_, and thus core Perl has never shipped with a reasonable default class system. Perl is weakly-typed, yes, but it manages to avoid the worst pitfalls of weak typing by having separate operators for numerical and stringical operations, so discerning between the _data_ types is easy enough. It's also a lot happier to throw type errors than, say, JavaScript or PHP. "Return-type polymorphism" is something of a misnomer; the _contextual_ types don't describe variables or values, but _expectations_. There is no single item in a Perl program that I can point to and say "this here is a list context"; I can only point to surrounding code and see what it expects to get out of a function. It's weird, but it's also convenient and natural if done really, really well. And it's a disaster if done poorly. :)
Python3.3 on Windows: C:\py -m timeit -n 1000000 -r 5 -v 'dict()' raw times: 0.0176 0.0176 0.0177 0.0176 0.0175 1000000 loops, best of 5: 0.0175 usec per loop C:\py -m timeit -n 1000000 -r 5 -v '{}' raw times: 0.0176 0.0177 0.0176 0.0176 0.0176 1000000 loops, best of 5: 0.0176 usec per loop
I've never seen anyone using dict() in production code unless they need to build a dictionary out of an iterable.
And not all keys can be passed as kwargs
Neither of your examples are simple to read or computationally efficient. The example i gave is two distinct steps: 1. create a data hash from $allnames 2. copy the data hash onto all the hashes stored in the relevant time keys of $bytime (overwriting any existing values matching the data hash contents)
I wonder if that's faster or slower than `dict.copy()`
It is, however, a relatively fixed cost. The performance impact goes down drastically with the size of the input.
Yes, that is correct. Using prototypes disables flattening.
&gt; And don't get me started on the even-length arrays (aka associative arrays or hashes). I'm sorry, but I kind of have to, because it's the first I hear of it. What *is* the problem with associative arrays, and why should they be conflated with even-length arrays?
The reason is quite simply that they are inherently different operations. Concatenate is NOT the same thing as adding two numbers.
I need a block instead of a top-level global declaration. There's some syntactic sugar modules that do this, somewhere, but (a) that's a lot of mucking around and (b) that would've been hard to sell to a big existing codebase. I gave you the three major things that I can't believe I lived _without_, and three other such minor things that occurred to me. The last one is clearly an afterthought and more of a meta-feature, but it instills the same feeling in me—there are those who believe PyPy is the natural future of Python, but Perl doesn't even have such a direction as an option.
But I didn't; I passed `n`. Prototypes are not a magical silver bullet answer to this problem, that's all.
&gt; I need a block instead of a top-level global declaration. How about this? package Cow 0.5 { use Moo }; my $a = Cow-&gt;new;
I must remind you that you're defending the language where `++` works on strings. ;) The problem with overloaded `+` was never that it means two different things; it was that _which_ of those things actually happened was difficult to understand with different types. Perl avoids this problem by having one operator for each operation. Python avoids this problem by not allowing `+` with different types. I used to gripe about addition and concatenation being different, but after sufficient FP brain-damage where numbers can be represented as sequences of nested functions, adding and multiplying other sequences as though they were numbers feels perfectly reasonable.
Also worth mentioning: the [python-dateutil](http://labix.org/python-dateutil) is a great alternative to pytz.
5.14 to the rescue again, I see. Better! Though now I wonder why it's even called a package any more. :) There's still some other awkwardness, like needing fully-qualified names everywhere and packages not being first-class. (I expect there will be a lot of confusion when newcomers try to nest `package` blocks and assume the namespaces automatically chain, for example.)
&gt; I must remind you that you're defending the language where ++ works on strings. ;) Well fuck, that should generate a warning, since it's nonsensical. I'm gonna pester p5p.
Nonsensical? It's been overloaded for... well, since the dawn of bloody time, as far as I know. I think I even used it. Once. In some obfperl. `..` is also overloaded: it works on numbers, strings, and _regexps_. You could say `-&gt;` is overloaded, as it works on both references and strings. Eh. `+` for concatenation in a strongly-typed language is certainly less weird than these.
&gt; Nonsensical? The correct word being inconsistent, and p5p is aware it is, but thinks its useful and not dangerous enough.
&gt; you just moved the for off to the end of the line where it's easier for someone to miss. Only easy for someone who doesn't notice the otherwise nonsensical $_ right at the start.
Your edit is correct, and i'll happily agree to disagree. :)
&gt; Though now I wonder why it's even called a package any more. :) Because you still get a package when you don't use Moo. ;) &gt; packages not being first-class I think p5mop will clear that up when it gets in core.
Also, dict isn't a function in the first place. It's the dictionary *type*, as in ``type({}) is dict``. It makes no sense to get rid of it.
I can confirm for Python 2.7.2, and I even added a zero to the number of loops (because 0.017 looks awfully close to the low precision timer resolution, so, just in case). And it's 0.0123 microseconds per loop for me even, I wonder what's the OP's and [kingkilr's](http://www.reddit.com/r/Python/comments/134gu8/the_performance_impact_of_using_dict_instead_of/c70qcgs) HW specs are (I7 2600k here, not overclocked). That's weird, how is that possible?
The syntax sugar isn't working quite right yet, but the functionality exists: https://github.com/stevan/p5-mop/blob/master/t/010-core/102-clone.t 22:58:21 (doy) eventually, "my $Foo = class { has $bar; method baz { } }" will work
All are using c:\python27
And that's one reason I dislike dict() used that way. It feels like a bit of magic that creates strings where you provided identifiers. If I want strings as keys, I'd rather pass in strings. Explicit is better than implicit, and all that.
I think i should add this for later readers: When i asked the question, i was hoping for exactly this kind of answer. Sadly the rest of the people posting here disappointed, mainly by not actually having a deep understanding like eevee's and complaining about things that are misunderstandings or mostly skin-deep and easily fixed by a little learning. However i do not wish to dwell on that and instead explain something: Perl 5 as a community is entirely aware of all these issues and agrees with them as issues and as something needing to be fixed. And in fact, quite a few of these are already being adressed. In order: * **Data structures** As Eevee mentioned, these are actually much easier and nicer now that commands like push actually understand array references. * **Cheap classes** This is being worked on. It's already much better thanks to new package keyword syntax, and Moo/Moose. However Moose and friends are not the end game. They are an experimentation ground to work out how a modern object system has to look like. Right now the perl community is working on prototyping a meta object protocol which will, when complete and satisfactory, be implemented in Perl core. Details on that are available as [slides](https://speakerdeck.com/stevan_little/perl-5-mop) or as a [talk video](http://act.yapc.eu/ye2012/recordings/Wednesday_1st_Track.html) (the second entry). * **Exception handling** This is a hard one. It sucks. We know. It will not be solved in the near future, however it is [being targeted](http://www.modernperlbooks.com/mt/2012/09/structured-exceptions-for-perl-5.html). * **first-class keyword arguments** Named arguments, this would require function/method signatures. Something Perl does not currently have, but wich is [being worked on by Peter Martini](http://blogs.perl.org/users/perl_5_porters_summaries/2012/11/perl-5-porters-weekly-october-22-october-28-2012.html) in Perl core. Right now the plan does not include those, but it is in early stages and might contain them at some point. * **Bound methods** I think [curry.pm](https://metacpan.org/module/curry) fits the bill. Maybe it should be cored sometime. * **replicatable** This is an issue. Not a very burning one right now, but it is acknowledged and with [Perlito](https://github.com/fglock/Perlito) there are actually first inroads and some parts of p5p do consider it to be crucial for perl's future.
Your post title says python-qt4, but your text says PySide. I'll guess you actually meant PySide. The last time I installed PySide, it was not possible to easy_install it, so you had to use a dedicated installer. It worked for me without a hitch on multiple Windows 7 machines in a variety of configurations. I see that the dedicated installer is [still available](http://qt-project.org/wiki/PySide_Binaries_Windows). Have you tried that?
&gt; Alternatively I can just have one obvious way to do OO out of the box that's clear from the outset. As i [laid out to Eevee](http://www.reddit.com/r/Python/comments/1349hh/if_you_switched_from_perl_to_python_and_carry_the/c70u4xp#c70xawe), that is in the process of happening. :) As for the rest of your post, i can answer a lot of it, but i'm not sure if you're interested. Are you? :)
I'm a bit confused by "the illusion of having a container". I never thought packages were meant to be a container. Similarly, "prototype for an instance" doesn't map to anything in my mental model of Perl. I suspect we think differently about how Perl "really works". As for syntactic sugar, IMHO there's a big difference between saying "`grep { EXPR } LIST` is syntactic sugar for `map { (EXPR) ? $_ : () } LIST`" and "`package Foo { ... }` is syntactic sugar for `{ package Foo; ... }`. The latter really is a minor difference. (Also, I'm not sure the map/grep thing counts as *syntactic* sugar.)
I think you missed the point. Its not about modern vs legacy style coding. Its about what the community and core language implementation provides and helps to enforce. 
We speak of what's "in" a package. Packages can be examined as hashrefs. The very word "package" is a type of container. Surely they're intended to _act_ like containers, so being reminded that the keyword is actually just some semiglobal state for the compiler is icky to me and probably jarring to a beginner. Er, prototype was a terrible choice of word, sorry. A package may be _either_ the behavior for blessed references, _or_ a set of static behaviors with no state associated, _or_ behavior for "itself" (`Foo-&gt;bar`), _or_ some combination thereof. Which is super flexible, and that's cool, but it severely blurs the semantics of "package" and means there's no single concept in Perl that clearly means "class". I don't think the `map`/`grep` transformation is really that major; it may end up executing slightly differently, but it does the same thing, and there's a trivial and reliable mapping from one to the other. `grep` is far less common than `map` and the extra typing is minimal, but it conveys very different intentions to human beings. Just like `package`-as-block.
As I said somewhere else, the real problem is that Perl doesn't have a class system, but rather a system for building class systems. And while that's really cool and all, I rarely sit down at my keyboard and say "boy, I think I'll build a class system today!". I just want to write a program, and even with Moose it feels like I'm making a burnt offering—albeit a much smaller one. Simplicity and elegance under the hood is fantastic. I love being able to fiddle with all the moving parts of a language feature and twiddle them however I want. (Except for the syntax, you could reimplement all the semantics of Python classes with Python code; there's not really that much to them.) The problem here is that _Perl's hood is transparent_, and thus I do not feel so much like I own a car. this is a car analogy &gt; It's not perfect by any means, but saying P5 classes are expensive is like saying Lisp has complicated syntax. It occurs to me that while Lisp has trivial syntax, Lisp _programs_ have much more complicated syntactical _constructions_ to compensate. Which is a perfect analogy to the state of Perl 5 classes.
&gt; mainly by not actually having a deep understanding like eevee's and complaining about things that are misunderstandings or mostly skin-deep and easily fixed by a little learning. I would like to point out that I *used to* know perl very well -- though it was apparently a different beast then. My complaint, however, is precisely that all these things need to be fixed by "a little learning". Accumulate enough "little learning"s together and you end up with a lot of learning. And that's a legitimate issue: that the language really requires you to know and remember a lot of little things, and keep them in mind while coding. That's what Python gave me that Perl didn't: the ability to not have to stay constantly immersed in its intracacies\* to be able to read and write code in it. \* Not everyone has scripting languages as primary language; being able to call on it when needed and forget it the rest of the time is a real benefit sometimes.
Bingo. If all I cared about was performance, I'd use C. We use Python because it has other advantages.
I'm very much with you on the part about integrating greenlets into the language. The `yield` keyword should still exist, it can just be greenlet-friendly. Greenlets (namely wrapper libraries like gevent) provide all of the advantages of asynchronous programming without the disdvantages of callback-style code. I'm not quite sure why the concept is not more universal.
I believe that's what he said: &gt;unless they need to build a dictionary out of an iterable. This is the only reason I use it. A lot of my systems only have 2.6, so I'm forced to rewriting all my dict comprehensions with the dict() syntax.
Is the distinction between "function" and "constructor" useful in this instance? I was ignoring it because it seemed unnecessary to bring it up. Or was your point that there has to be *some* kind of constructor for the `dict` type?
couldn't you do that with a comprehension though?
I think that is a bit unfair since the way that you become fast *is* by avoiding unnecessary work.
&gt; we have Jython and IronPerl if that's your bag Did you mean Iron*Python*?
Mhm. It's still crazy impressive, but it's also not fair to *really* be impressed by this benchmark in this context, since it doesn't actually have anything to do with dictionary initialization.
&gt; Most of the time a dictionary with fixed keys looks better as dict(), and requires less typing to boot. Not in my opinion. Why obscure the fact that your keys are meant to be strings? The dict is a fundamental data structure and everyone who uses python should be intimately familiar with it's syntax. &gt;&gt;&gt; c = dict(a=a, b=b) &gt;&gt;&gt; d = {'a': a, 'b': b} The end result is obviously the same but the later explicitly declares that the keys are strings. Plus you can use any other immutable you want for keys using this syntax. Plus it's faster. Plus the constructor matches the repr of a dict. The only reason to call the dict function (IMO) is when you need parameter expansion or converting other existing data structures (iterables of tuples, etc) to dicts. 
Using single quotes instead of doubles is sound advice in any Perl-like language. It's less a question of performance than of readability though. The reader doesn't need to pause to make sure that there's no magic going on within single quotes.
So, how often do you need to create 1000000 dicts in a tight loop?
I use the lpod library (http://lpod-project.org). I can generate and manipulate OpenOffice Documents (ODT, ODS, ODP) with this library and Python. The last version was realeased in September
How accurate is that timing information when we're talking about sub-usecs?
Please point out in which ways the Perl community embraces writing esoteric code, as opposed to simply acknowledging it as a hobby, never to be used in production.
WELL. That makes for a very different sentence. Yes, I did.
When writing blog posts explaining the speed difference, duh.
A bit of a silly reason here, but sometimes I use dict() because {} looks too much like it's the empty set. It takes me slightly longer for my brain to parse it as a dict, and always trips me up when I'm actually trying to initialise an empty set.
 &gt;&gt;&gt; print {}.__class__ == dict True And then `dict` implements all dict methods (including `__new__` and `__init__`), see `dir(dict)` and compare to `dir(lambda:{})`. Creating an instance is a small part of what a class object does.
Yes, but in this context, by using that optimization, you are comparing "creating nothing in PyPy" with "creating nothing in PyPy". It's not a real world scenario.
&gt; The end result is obviously the same but the later explicitly declares that the keys are strings. So's the former, since callable argument names *can only be strings* (even more restricted, only names). And if you're writing Python code and you don't know that, there's little hope for you.
&gt; Only in the most insanely performance critical code is it worth doing this kind of optimisation. And even there, if the code is so performance-critical you should probably use bare tuples anyway: &gt; python3 -m timeit -n 1000000 -r 5 -v '()' raw times: 0.039 0.0403 0.0383 0.0376 0.0377 1000000 loops, best of 5: 0.0376 usec per loop &gt; python3 -m timeit -n 1000000 -r 5 -v '[]' raw times: 0.0686 0.0775 0.0742 0.0749 0.0722 1000000 loops, best of 5: 0.0686 usec per loop &gt; python3 -m timeit -n 1000000 -r 5 -v '{}' raw times: 0.129 0.131 0.13 0.128 0.131 1000000 loops, best of 5: 0.128 usec per loop 
Amen to that. I first met coroutines in (eeek) 1980, for some applications they cannot be beaten.
And yet people are amazed and walk off feeling like NOW their python code will be soooo fast. :)
Right. This was designed for python 2 and I am now porting it to 3. The conversion works but the compilation not yet. Once I figure it out, what you suggest will be an option.
I agree.
That's why I was ignoring the distinction, personally. I'm not as versed in the details as PJE, though, so I thought maybe I was missing something. Like I said, my best guess as to his point was "types have to have *some* sort of constructor, and `dict` is a type, so you have to have `dict()`".
The funny part is there *is* no literal for an empty set.
Thanks gthank, for sharing this article. I have never seen Python's dis module before, and this opened my eyes.
Yes indeed, it is explicit that function argument names can only be strings. That's explicitly in the grammar of the bloody language.
You're right. It seems much healthier on that link than on the [pypi page](http://pypi.python.org/pypi/appy.pod/). I wonder why nobody updates the pypi release.
For me reading this page was like a flash-back to Prolog from the late 80's. If I was going to revive anything from the 80's it would be Borland's Pascal.
Terms is a thing. viscence is a person. confuses is exists, subj a thing, who a person. confuses Terms, who viscence.
Has anyone really been far even as decided to use even go want to do look more like?
Think virtualenv for python. Its actually a very good way to run production because you can update the system with much lower risk to the application. In turn, applications can be updated without worry of breaking the system or other applications. 
Dictionary comprehensions were introduced in 2.7 which wasn't released until the middle of 2010. It took a while after that before the branch was considered stable enough to use in serious projects.
pep-8 won't help with `new Flower()`. That's just some other language.
I'm with you, semarj. They could have at least written something that would compile. Or at least come close. 
When parsing giant text files line by line. Python is a hot thing for data analysis nowadays.
I don't know that Cobol needs to be revived, its still alive.
Any time! It's one of those parts of the stdlib I keep meaning to explore (thanks to previous articles by Doug), but haven't gotten around to, yet.
And it will burn ever so fast...
I love this topic. Reminds me of [google graph](http://www.google.ca/insidesearch/features/search/knowledge.html).
Whichever the implementation details are: We're on our way towards them by way of modernizing signatures.
My point was, keyword arguments aren't inherently part of having signatures. It really seemed like you didn't understand what he meant by 'keyword arguments', and decided to say something about named arguments instead. Lots of languages have signatures and named arguments but no keyword arguments or even no optional arguments, and the post you linked makes no indication that the new signatures will support either. (Some of the emails linked from there do talk about optional arguments.) For example, say your function has a dozen arguments, all with default values as mentioned [here](http://www.nntp.perl.org/group/perl.perl5.porters/2012/10/msg194726.html), and someone wants to only specify the last one. Or someone wants to specify a name that _isn't_ part of your signature. Python can do both.
That's exactly what i mean with named arguments. And while the current signatures prototype does not include them yet, they're inevitable, since we're already emulating them with hashes, especially in object constructors. To give an example: Web::Dispatch-&gt;new( app =&gt; $app, node_class =&gt; 'Web::Simple::DispatchNode', node_args =&gt; $node_args ); There are three arguments passed there, with their names. Only app is required though, the other two [are optional and have defaults](https://metacpan.org/source/MSTROUT/Web-Simple-0.020/lib/Web/Dispatch.pm#L14). As well, there are two further arguments not used in this call that have defaults or are built at runtime. And lastly, i could also pass more arguments, and depending on how the object is set up, it could complain, ignore them, or store them for passing on to other objects.
Try replacing one or more of the word "string" in that declaration (hint: the one on the left) with the name of a different type. Object, if you like.
The disassembly shows that it's loading up a function called `dict` and calling it. Once it gets down into the bowels of the interpreter it presumably has a special case for handling that in terms of `dict.__new__(x)` or whatever, but as far as the syntax, the compiler, the byte code, and indeed the performance is concerned, it's a function called `dict`.
If you want to see a really good implementation of Zope, take a look at Zenoss (http://zenoss.org). It's an amazingly complex setup that just seems to work well with Zope.
It's important to note that "Zope" != ZCA. ZCA (`zope.component`, `zope.interface` and `zope.configuration`) are small, lightweight libraries extracted from the Zope codebase. Also, it can be noted that Pyramid, the popular and lightweight web framework utilizes ZCA quite heavily.
Plone 2.5 is like five years old. I suggest migrating it to latest Plone and enjoy. Is it a site with heavy customization?
Really difficult to tell what they do from that website. Is this some something like Zabbix?
Zope itself does not offer anything that you are probably very interested in. [ZCA](http://www.muthukadan.net/docs/zca.html) on the other hand (which is what the article is about) might.
No i do not.
Restructured text with jinja2 templating. Using rst2odt.
Zope is a web application framework written mostly in Python. It's a batteries included framework in that it includes everything you need including an awesome object database, ZODB. Zope2 is/was rather monolithic hence the batteries included phrase, lots of magic, and boilerplate. So there was Zope3 which was a complete rewrite to compontentise the former. Not really successful as it broke a lot of code so much has been back ported to Zope2 and that's where the ZCA really shows its stuff. Want to learn more? than mosey over to Plone.org. 
&gt; parsing giant text files line by line It's not a tight loop -- it's I/O bound. So, the speedup would be like 0.00001%.
Each time I read something about Zope I remember [zope.interfaces](http://wiki.zope.org/zope3/WhatAreInterfaces) and how horribly unpythonic it is.
I'll use the Zen of Python to explain my self by emphasising the relevant areas. &gt; The Zen of Python, by Tim Peters &gt; &gt; &gt; Beautiful is better than ugly. &gt; **Explicit is better than implicit.** &gt; Simple is better than complex. &gt; Complex is better than complicated. &gt; Flat is better than nested. &gt; Sparse is better than dense. &gt; **Readability counts.** &gt; Special cases aren't special enough to break the rules. &gt; Although practicality beats purity. &gt; Errors should never pass silently. &gt; Unless explicitly silenced. &gt; In the face of ambiguity, refuse the temptation to guess. &gt; There should be one-- and preferably only one --obvious way to do it. &gt; Although that way may not be obvious at first unless you're Dutch. &gt; Now is better than never. &gt; Although never is often better than *right* now. &gt; If the implementation is hard to explain, it's a bad idea. &gt; If the implementation is easy to explain, it may be a good idea. &gt; Namespaces are one honking great idea -- let's do more of those!
Sorry, but Zope left a very bad taste in my mouth when I tried to install Plone. So much effort, for such so little result -- felt like I had installed Microsoft Sharepoint.
I'd really like to disagree with you, but I can't quite do so. I'd actually hacked up something similar to ZCA (but much less fleshed out) for one my larger projects. It's working great, and the decoupling the article advertises is just plain beautiful. But this method of decoupling is *so* thorough, you loose all sense of connectedness about what class is doing what - the only way to find out is to open up a console and see what class ``adapt(foo,IBar)`` spits out. Looking at it from the right angle, I think it's very much like Aspect-Oriented Programming... there's the same useful decoupling of cross-cutting concerns, but at least here you don't have monkeypatching and method injection. I think your invocation of "explicit is better than implicit" probably sums up this approach's greatness weakness. Unless it's used very sparingly, you can get lost in adapter hell. 
Exactly ! My first as a Python programmer was with Zope and everyone was lost in adapter hell. 
I agree, the matplotlib docs are a mess. I have to rely on google and find-in-page.
The do have links to the respective functions in the html file, [for example](http://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.plot). I just wish I was familiar enough with all the doc html etc to suggest a patch to the doc generation.
Ok, find the function to set the background color on an axes. It's easy to find stuff for often used items, which is also the time when you least need the API.
[Ask and you shall receive](http://matplotlib.org/sampledoc/). The sort of thing you specify could be accomplished by a plugin for [Sphinx](http://sphinx.pocoo.org/).
Once you learn ZCA, it's hard not to see usage in any other python application of mid to high complexity.
the pyplot api is not the complete matplotlib api - it only has a subset of functions: namely those that are generally used in matlab. As such, you generally need to give things like the background color up front. In your case: by adding a parameter to the axes() call - see the axes() docs.
This comes up in discussions of Pyramid also. ZCA has *horrible* branding. Nobody thinks of "small and lightweight" when they see Zope in the name.
Except in math, where {} is the empty set.
This seems pretty great, will definitely try it out.
The problem ZCA has is that it hasn't been adopted by no one meaningful in the Python community except the Zope community. (Pyramid is probably the most relevant one, even though it has ties to the Zope community.) Django, Flask, SQLAlchemy, OpenERP, Tornado, gevent, web2py, etc. And the list goes on. No one uses ZCA. (I haven't checked to be 100% sure of this statement, but I guess you get my point.) Off the top of my head, I just remember it being stated as a requirement by the Twisted framework. The problem here is that the big players in the Python community aren't using ZCA for anything. I agree with rspeer saying that the branding is horrible. Any Zope reference should be erradicated for it to have any chance, not because Zope is good or bad, but simply because Zope currently is regarded by almost everyone as evil. Just look at the comments. Regarding the issue of interfaces: You would assume SQLAlchemy needs to implement all sorts of interfaces and should benefit from ZCA. But no ZCA in SQLAlchemy. So either someone respected in the community uses them and openly includes them as a requirement or it won't have a chance. I have nothing against ZCA and would be happy if someone proves me wrong on the statements I made.
It seems ZCA has similar purpose to [abc](http://docs.python.org/2/library/abc.html), but to a person who has used abc only a couple of times and has never used ZCA (me) it looks like abc is simpler (and it is in standard library). By the way, abc's "a couple of times" turned out to be an overkill in my case.
At least one incarnation of Guido disagreed at some point: http://www.artima.com/weblogs/viewpost.jsp?thread=92662
What zzleeper said. Also, you generally don't need to remove objects (=None) by hand when performance isn't critical, the garbage collector does that for you. Some more hints for you: os.path.join() joins the two paths together, which is what you want. And, you can rewrite the D:\test\ path as a raw string. Like this: os.path.exists(os.path.join("""D:\test\""", book.Sheets[0].Cells(1, x))) or this: os.path.exists(os.path.join(r"D:\test\", book.Sheets[0].Cells(1, x))) 
This is a good analysis. FWIW, the reason Zope is regarded as evil is because the Reddit echo chamber keeps repeating that it's evil in posts like this one. No one seems to actually be able to describe why it's evil or even know what they mean by the word "Zope" when they render judgment. What they always actually mean is "I have no use for it", which is completely reasonable, because a) they don't know what it is, so they can't know why they'd use it and b) even if they did, it's not something that is required for a run of the mill bespoke app. That gets shortened to "Zope is evil", for better or worse (worse). I actually think posts like the one that this reddit topic points at are wildly offbase when they tell run-of-the-mill application developers to use ZCA. It's not really all that useful unless you're trying to create a system that allows for pluggability; and even then, you might not need it. But if you need to create an extensible system, you're always going to end up using some of its ideas (e.g. a nonglobal application-specific centralized mutable registry that allows complicated lookups quickly) because those ideas are pretty much the only way to create a system that is maximally extensible.
I already mention Twisted as one of the exceptions. And indeed it is a meaningful project.
Zope is a project many people decided to move on from. It's a huge investment in time and effort and I think everybody should give it it's share of attention, simply because it was there long time before most of us where even interested in programming. Seen under this prism, this article is exactly this. Someone who cares enough about Zope, that tries to give it back a bit of it's momentum. (Even if it's only a small part of it.) I would want to see a more meaningful and/or interesting example, like maybe using it together with something like Flask to do something interesting. Maybe this way it would get better responses.
Well, we should only be glad that you do not design a new language as this is list of bad ideas mixed with not so bad ideas. 
zope is evil . arcane crawling on ceiling head spinning evil . I could itemise the ways it has hurt me personally and wasted my time, but I would rather not relive the assault
That's the stuff.
Normal linux-y procedure for installing python modules is generally to decompress the downloaded archive, navigate to the resulting directory (containing a setup.py file) using the command line (is it still `start&gt;run&gt;cmd` on windows?) and typing `python setup.py install` Now it looks like box2d needs swig, so I had to install that first, but I can't help you so much there because I don't have windows. 
Since you're new to coding, you should head over to www.stackoverflow.com for any future programming help needs. Not to say that you can't consult reddit as well (although the sidebar says that questions should be directed at /r/learnpython, but don't feel bad), but stackoverflow has an amazing community for helping out everyone from the newest of noobs to the most experienced of gurus. Not only that, there are other forums in the stackexchange network for math, physics, religion, and everything in between.
So `dict` is a type, and it is also callable?
Yes, but we're discussing Python source code, not a math paper, so I thought there was enough context surrounding my statement.
Right. And that's why `zope.interface` has been such a useful tool to me. I'm curious what problems you've encountered.
The main problem with `abc` is that it adds a new meaning to `isinstance()`. It confuses checking for interface conformance with examining the implementation of an object.
only because it’s easy for you in your editor of choice doesn’t mean that 1. it’s easy for other vim-users 2. it’s easy for users of other (good) editors once vim comes with a python highlighting definition that has doctest highlighting enabled, and Kate, gedit, and Sublime Text 2 do, too, i’ll reconsider your counter-argument. until then it’s much easier to create a new file called `test.py`, putting a `test_x` function in it, and running `nosetests`. besides, it sucks to define functions inside doctests to not repeat yourself (this makes docstrings horribly bloated, if they aren’t already by the tests in them) also i don’t like to type `...` if some test needs more than one line, as this messes with all editors’ indentation methods and copy/pasting code into/from there is always accompanied with stripping/adding the `&gt;&gt;&gt;/...`
Is this bad because this is not pythonic or because there are some practical consequences? For example, import abc class IDrawable(metaclass=abc.ABCMeta): @abc.abstractmethod def draw(self): pass then somewhere in code there is a check if isinstance(foo, IDrawable): # ... Regardless of `foo.__class__` implementation if the check above is true foo conforms to IDrawable, because it is not possible to instantiate an object of abstract class. class Foo1(FooParent, IDrawable): def draw(self): # ... class Foo2(FooParent, IDrawable): pass foo = Foo2() # this fails if FooParent doesn't have non-abstract 'draw' method I'm not defending abc, I just want to understand what you've said better :)
Maybe someone could give it better branding. Come up with some hipster name (like _plugger_ or _winston_), get rid of the cammelCase, create some twitter-bootstrap docs website for it. Then it would fly. It would fly _hard_.
That `create_user()` function is riddled with shell injection vulnerabilities and is a textbook example of why you shouldn't use `os.system()`. 
Look into virtualenv and pip. These tools, together, let you set up a custom python environment for any task. 
So I think vpython might be what you're looking for. I did a really basic ball inelastically bouncing under gravity essientially using euler's method for ode's for a class project. I can send you the code if you pm me.
It's bad because `isinstance` is, otherwise, a code smell indicating bad design. `abc` smushes an arguably good use case into exactly the same appearance as a bad one. For further reading: [isinstance() considered harmful](http://www.canonical.org/~kragen/isinstance/) [Explaining Why Interfaces Are Great](http://glyph.twistedmatrix.com/2009/02/explaining-why-interfaces-are-great.html)
Actually I like the idea of statically compiling Python code the way Nuitka does it. It's simple and compatible. Once we have a complete replacement for CPython, then we can look at what other opportunities there are for extending the compilation phase. I like to see that we could hoist some of the decorator patterns and metaclass hackery into metaprogramming, so that these things would not introduce even the smallest runtime penalty. This would of course require that Nuitka gains the feature to permit us to explicitly remove complete compatibility, in exchange for performance. Together with this, we could then provide optional jitting, something like Numba-style decorator patterns. http://numba.pydata.org/ . Numba JIT-compiles specific functions while understanding numpy data structures, thus allowing you to write tight loops in python, that are at the same speed of C loops. Summary: Use something Nuitka-like for removing the bytecode interpreter. Use something Numba-like for JIT compiling explicitly annotated functions. The benefit of this compared to PyPy would probably be embeddability and memory usage. PyPy is certainly known for it's bigger memory footprint compared to CPython, which in some cases kills it for being an alternative. Sadly Nuitka have gone the way of Apache licenses. Which is not bad, but certainly not the norm in python circles. Which greatly limits how much other developers wish to combine efforts. At least it isn't LGPL/GPL (dunno which) as it was when it got released.
I'm a big believer in the virtualenv+zc.buildout method, personally, so I guess the big question is, "Are you making it easier, or harder, for someone else to work with your software?" I'd suggest that virtualenv is only half of a solution; it provides a specific python version of your choosing and excludes site packages, but doesn't provide a way of reproducing your environment for others; you might have PIL 1.1.7 by default while a colleague has PIL 1.1.3 that came by default in their OS, and blows up when they try to run your software. While VirtualEnv is a good way of excluding site packages, buildout is the best tool I've found that helps recreate a specific execution environment for your program; think of it like a personal shopper to whom you give a shopping list; they go out and get all the packages you need and sets them up for you. Buildout also has a *lot* of companion software going for it, including stuff like checking out and installing eggs directly from SCM tools and installing those "difficult-to-install" packages you were asking about (have a look at http://pypi.python.org/pypi/z3c.recipe.staticlxml, http://pypi.python.org/pypi/Pillow/ and http://pypi.python.org/pypi/zest.recipe.mysql/1.0.4). I use the virtualenv+buildout one-two punch on all software I write now, and appreciate it immensely when I develop software from others who do the same—Much more than I would following some complex instructions from a developer about how to exactly reproduce their development environment, or worse, having to report defects that arise because you didn't provide enough detail and your copy works but mine doesn't.
I wasn't speaking to the particular details. Only to the fact that such isolation is in fact, frequently beneficial to production settings. Python has other facilities to achieve what you describe. Informative nonetheless. Thanks. 
Ah, that makes sense then. And yeah, i was aware python had them. :)
`pip` doesn't support binary packages, so it must compile from source. For things like `lxml` that have a C library dependency, it means you need to have the foresight to have already installed all of the required development packages for each of the dependencies, as well as a functional toolchain. Both of those things are outside of the domain of `pip`, and it cannot manage them for you. It requires that you knew to run `apt-get install libxml2-dev` and `apt-get install build-essential`, or whatever your distro calls them. Moreover, you'd have to wait while everything compiles, which can be a pain if you're automating this as part of setting up a testsuite or as part of an automated deployment. And, there's often a policy of not allowing toolchains on production systems. `apt-get install python-lxml` on the other hand suffers from none of those drawbacks. It will automatically install the right dependencies, and it will install precompiled binaries so there's no waiting. 
Docs for axes() are listed in the link I gave.
Ok sorry, bad example. I was using Figure.add_axes, and the only place I could see that mentions it is in an example under typical usage "fig.add_axes(rect, frameon=False, axisbg='g')". I've never come across API documentation before where stuff is strewn out all over the place like this. In the list of kwargs for that function it's not mention at all.
I've used [Relatorio](http://relatorio.openhex.org/) once to create invoices. The documentation is a little sparse, but it got the job done.
Hey takluyver, no love on "/hacked", but I'm keeping an eye on the logs for attempts to mess with stuff. Someone even submitted a "rm -rf /" :) boy-oh-boy, time to git commit! Love you guys! Please PM me with your cron DOS idea. You mention that you wouldn't rely on PHP sanitation. Is there a different way to do this?
Just browsed the site. Homepage doesn't explain what they do. Checked out their "About" page, which doesn't explain what they do. My best guess is that it's some sort of monitoring application but it's unclear how it's any better than the millions of other monitoring applications. If a project's site fails to convey the basic purpose of the project in the first couple paragraphs on the homepage, that's a really bad sign. 
Ideally it'll symlink in any libraries as needed; for example with gtk2 one has to do stuff like this: http://stackoverflow.com/questions/3580520/python-virtualenv-gtk-2-0 
What you mean by 'as needed' ? Does " it'll symlink" refer to virtualenv ? From what I can see you have to do it manually. But yes, symlink maybe better that direct copying. Thank's for reminding me to that !
well also I've avoided anything zope* as much as anyone else. If someone wants to show me a patch of heavy zope.component integration into SQLAlchemy, illustrating how it makes the code more succinct, stable, testable, and performant (and they're willing to do all the work too), I'd certainly take a long look at that. To date we've gotten away with having no dependencies in SQLAlchemy other than the DBAPIs needed.
Most of those projects don't have a use for the ZCA. ZCA is useful for making pluggable/extenisble applications/frameworks. The number of projects out there for which it's suitable for use is reasonably limited. SQLAlchemy works beautifully as a library. You can associate classes with tables without any need to modify those classes and make them implement any particular interfaces or plug into any system. Adding the ZCA to SQLAlchemy would be bad. However, zope.interface (which ZCA relies upon but are a seperate library from ZCA) can mesh very well with SQLAlchemy ... or at least zope.interface as extended by the zope.schema package. SQLAlchemy used in conjunction wtih zope.schema can allow you to define your data schema within your Python interfaces. Model classes can then declare they implement those interfaces and you can auto-generate database tables based on those classes in a fashion similar to the way Django does it. However, things are cleanly decoupled, so you could then later declare that a web Form also implements a particular data schema without having Form base class code intermingling with Model base class code. For web apps like Flask or web2py, these benefit from being lightweight or easy-to-use. They'd need to be a notch more ambitious to justify any use of the ZCA. Django is a potential use case, the Django SETTINGS registry could be replaced with the ZCA and more complex Django apps could benefit from the increased flexibility offered. 
Someone please do the same with list() and [] now. Hehe.
If you ever use **kwargs, the keys are strings. It's not really a novel concept.
Thanks everybody for the help, already looking into it!
If I remember right, this is easier than box2d to set up. And this is where the docs are: http://www.pymunk.org/en/latest/examples.html
Certainly, for any of those frameworks, if you are yourself writing something that needs to be componentized on top of it, then ZCA will work great. 
I have never used zope but, based on my django install experience, Django is or will be the new zope but without zca. Fast forward five years and this discussion will be happening but the names will be changed assuming django can sustain that long. Im sure some zca concept will emerge. The question is then how can we prevent this thread from reoccuring? How can solutions to complexity be passed from one generation of developers to the next? How can we reduce learning curves? I think we can all agree the people that wrote zope, including the extracted libraries, and plone are not idiots and were not idiots.
Great ! Didn't know about this. But this won't work for packages installed to system site-packages like lxml, PIL or mysql-python since that will include the whole site-packages dir.
I also think your example is harder to read than lexyeevee's. I can grasp that there's a list comprehension there.
&gt; I'm not saying that "dict is a function" but that "when you type dict(), you call a function named dict" That's gibberish. How can you call a foobar named baz if baz is not a foobar? ``dict`` is not a function, so when you call it, you're not calling a function named dict. In Python, a function is an object of type FunctionType or BuiltinFunctionType. "Function" is not a name for "any callable object", as you seem to be thinking it means.
That guy bellow tricked me to respond. no C: implement the interpreter in RPython: - this is a little silly. What is C's fault?! Java is implemented in C and has the fastest JIT on Earth. no drags-you-down batteries: - This is plain stupid no yield: use greenlets - Since when yield is a problem?! no underlying blocking on IO: base it all on event loop, yet provide synchronous programming model through greenlets - No thanks. When I need IO I like to have sunchronious interface and know what am I doing. no c-level API nor ctypes: use cffi to interface with c-libraries - Yes C level API, because when I need something done I want to be able to do it the most direct way. no global state: - In Alice of Wonderland maybe. In reality we need global state, because the world around as have global state. no GIL: - OK no setup.py: have a thought-through story and tools ... - OK no import, no sys.modules - Not sure what is the problem, but sound stupid. no testing as an afterthought: - Stupid. The language actually have already build in testing mechanism. And how many more testing do you need?! no extensibility as an afterthought: support plugins and loose coupling through builtin 1:N calling mechanism (event notification on steroids) - Again with the stupid events?! What is wrong with you? no unsafe code: support IO/CPU/RAM sandboxing as a core feature - Excuse me I still want my program to run fast. no NIH syndrome: provide a bridge to a virtualenv’ed Python interpreter allowing to leverage existing good crap - Have no idea what this means. 
That isn't a blogroll. A blogroll is just a list of links to blogs. (Yes, it's kind of dumb that someone felt the need to give that a special name.) What this post is talking about is more like feed splicing (far more interesting/useful than blogrolling, IMHO), though it's also rendering the spliced result to HTML rather than to RSS or Atom. Source: I helped create Google Reader.
It's not gibberish at all. The problem is that you are focusing on a certain term being precisely one thing, when in fact a term could mean different things in different contexts. In this context, the language syntax provides a callable with the identifier 'dict', and the bytecode runs the CALL_FUNCTION instruction on that identifier. It is useful for the user to know that a dict is a type, but it is equally useful for the user to consider 'dict()' to be a function that returns a dict instance. A function is a construct that takes some input and returns an output - it doesn't have to conform to any one language's specific type system.
I meant any potential script (ie the syspkg.py listed above) - python syspkg.py pygtk path/to/venv would be able to find libraries used by say pygtk - and then create symlinks inside the virtualenv. The manual process I listed above has to do - ln -s /usr/lib/python2.7/dist-packages/glib/ lib/python2.7/dist-packages/ ln -s /usr/lib/python2.7/dist-packages/gobject/ lib/python2.7/dist-packages/ ln -s /usr/lib/python2.7/dist-packages/gtk-2.0* lib/python2.7/dist-packages/ ln -s /usr/lib/python2.7/dist-packages/pygtk.pth lib/python2.7/dist-packages/ ln -s /usr/lib/python2.7/dist-packages/cairo lib/python2.7/dist-packages/ I'm not sure how to find which libraries need to be symlinked - though looking inside pth files might be a start.
OK, looking at this more closely; cairo and some others are seperate dependencies so dont need to worry about them. Looks kinda simple in this case: pygtk.pth - points to the gtk-2.0 directory - which has most of the things needed in it (I guess, the whole dir gets linked in). A couple of other suggestions: If it could read a requirements file and link anything with a -g prefix (for global) - it would be really useful. Make the path to virtualenv optional if already in a one ($VIRTUAL_ENV) is set.
I'm not so sure about that. At least I would probably write worse assembly than the optimizer of the C compiler would produce.
You must add `DEBUG_PROPAGATE_EXCEPTIONS = True` to your settings, otherwise django catches exceptions and keep them for him.
Nice. How do I run this with the development server?
Unless you know otherwise, its less likely to be python related. 
&gt; Be ready to drop support for older syntax. We have a deprecation policy and will make use of it as appropiate. &gt; a better language like Python It would be nice if you could refrain from stating your opinion as fact. :)
I honestly don't get what package nesting has to do with modifying stuff in other namespaces.
Take Fabric *(a library used for easily executing SSH commands in parallel on remote machines)* and add in pieces of Puppet and Chef *(Ruby apps used to provision and manage the configuration states of lots of servers at once)*. However, Puppet (and I think Chef too) works on a "pull model" where the servers update themselves periodically using cron jobs. Salt uses ZeroMQ to implement a very fast "push" provisioning process, which is handy when you need to update an app across multiple servers at once and getting to a consistent application state ASAP is vital.
Older versions of puppet had a "puppet kick" which one could use to force an immediate puppet run so you could do the same, update an app across multiple servers at once. Nowadays puppet users use mcollective which also use messaging to force an immediate update of the nodes[servers] that you want to update immediately. I don't think anyone actually implements puppet runs as a cronjob anymore. We schedule and distribute runs using mcollective which may run on the puppet server itself or on a dedicate mcollective server or from your workstation. 
It was partially a joke *but* you are talking about improving Perl, and asking for insights in a Python subreddit, so I suspect you are thinking about making Perl more pythonic. You know what language is more pythonic than Perl? Python! Also: Ruby! Despite all the bickering between our communities Python and Ruby look and function a lot a like, except Ruby loves the idea of Perl of having a huge language with a myriad ways of achieving the same thing for the sake of playing Ruby Golf. &gt; We have a deprecation policy and will make use of it as appropriate. I'm not sure your current deprecation policy is good enough. For instance, `$_`. Many a Perl guru warns that `$_` is never safe to write to and not always safe to read from. Why is that usage not deprecated then? To me Perl seems like a mine field of potentially disastrous gotchas where there are many ways to do the same thing and most of them are wrong. The proper, safe way of programming happens to be optional, and is often more verbose that the unsafe way, which happens to be the more convenient one and the default. This is a recipe for disaster. You need to make bad usage deprecated and not the default. And you need to make good usage the default. But of course. that's only IMHO.
well this was timely. I'm currently in the middle of refactoring a fairly complex python system and noticed that some patterns throughout the code were implicit but would be made better if they were explicit. duck-typing is awesome and flexible but as complexity increases, you need things to help manage it and zca should work splendidly where used appropriately.
Which is really extraordinarily generic and doesn't tell you anything about the primary use-case: managing configuration of lots of servers at once.
I'm not the OP, but thanks. I saw that MS-Word's ODT files are gobbledigook when you open them and took a guess that they were merely compressed. I changed the file extension to .Zip, double-clicked the file, and the archive opened up to reveal XML files and a manifest. 
I'm not a Matplotlib user but those docs look like Sphinx docs. There's nothing stopping you downloading Matplotlib and generating the Sphinx documentation yourself AFAICT.
Totally agreed, "true" explicitely passed around locals are always the best. Some people argue that it's hard to pass around a lot of state just in case some function might need it, webframeworks in particular. Today i wrote a bit about the [execution local idea](https://tetamap.wordpress.com/2012/11/16/execution-locals-better-than-thread-localsglobals/) which supersedes and improves on "thread locals" which really are thread globals. 
Yeah, the chef server pushes out. You could set up a notification system that would kick the client.
feed? 
If you have say anywhere above a few hundred servers then you will want to have better control of when your puppet clients will execute. Putting it in cron takes that control away from you. Although there are better solutions nowadays, I had to implement an "intelligent system" on the mcollective server which for example prioritizes what clients should run asap and let others wait so as not to swamp the puppet server. 
&gt; ... that "6 times slower" number is rather disconcerting. It isn't. If you're worried about the impact of calling dict() in your code try profiling. I'm positive that building dictionaries is not what makes your code slow.
Branding is key. At risk of offending any Zope fans, any "good" ideas they want to share outside the Zope community they need to drop "Zope" from the name. It's a non-starter for a lot of people as it implies: 1. It's hugely complicated. 2. Dependent on the rest of Zope, requiring I install the rest of Zope. 
&gt; Why obscure the fact that your keys are meant to be strings? It's not obscure. It's a pretty well know idiom. And keyword arguments and dictionaries are associated even by beginners (those who have gone through the tutorial, at least). Going by the same logic, why hide the fact that members are actually dictionary key and values during type creation? class Foo(object): spam = 14 "spam" actually is a string acting as a key to a dictionary. And we're not talking obscure implementation details, ```__dict__``` is clearly documented.
That's the confusing point. ```(1, 2, 3)``` is a tuple, ```()``` is a tuple. ```[1, 2, 3]``` is a list, ```[]``` is a list. ```{1, 2, 3}``` is a set, but ```{}``` is a dict.
`{'a': 1, 'b': 2, 'c': 3}` is a `dict`, and `dict` predates `set` (and especially `set`-literal syntax) by quite a bit.
Went to six documentation pages and found no example code and no clear explanation of why I would want to use this. Went home. 
I configured my whole company server infrastructure with it. All I read was the documentation :) that was 3 weeks ago.
What is a QR code?
It's pretty much the same as a bar code but it includes a few features that let the reader know the orientation and angle it is reading the code, so it is a lot easier to read with a shitty webcam and a little image processing.
i see. I googled it and it looks interesting. I need to do some research on how to use it 
Thanks for linking this post, I like the xlocal approach quite a bit more.
I still don't get the importance.
Could use a raspberry pi hooked up to a handheld USB barcode reader that acts like a keyboard
Python can't be used because there is no reliable and fully functional, sandboxed version. If one were to embed python as is, websites would have full access, to the extent of the browser's user, to your computer. 
What was the plan to address the endless security implications?
The importance of what?
It was a restricted python, we would modify the interpreter itself (something "simple", like [tinypy](http://www.tinypy.org/)) to simply avoid anything we would find to be unsecure.
I'd give Salt a +1 here for its native use of ZeroMQ, which arguably makes it even easier (and certainly faster in message passing) than setting up SSH tunnels.
You want to set the 'aspect' to 'auto'. This will leave the axis positions fixed. It's all there in the docs: http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.imshow 
Some screenshots? *Edit:* Nevermind. http://people.gnome.org/~dscorgie/screenshots.html
Thanks. Here's a couple of more recent ones: http://screenshots.ubuntu.com/package/labyrinth
How is virtualenv for packaging? I thought its to create an isolated environment...
&gt; its to create an isolated environment... ...the entire purpose of which is to install packages in a sandbox.
I don't feel so bad for the fact that I still use distutils/setuptools now.
The question is not _how_ but rather _why_ ???
Because eval() is insecure, and because it's a lesson. Using this exact same method you could parse Python or C.
hmm, isn’t wheel just for binary distribution and lives besides/on top of distutils2? and crate.io is wheel’s cheeseshop? and wtf is distlib and, most importantly, *why*?
Yes but since the author uses a library (which itself uses PLY) the number of lines, which is usually useless, becomes even more meaningless… Calling the post, "parsing with PlyPlus, the compulsory calculator example" would have made much more sense to me.
Misleading title. It's like saying "I wrote this website framework in 200 lines. It uses Pyramid and SQLAlchemy"
It's actually like saying "I wrote this non-trivial website in 100 lines, it uses pyramid and sqlalchemy" (which are both great btw)
* wheel is a binary format: http://www.python.org/dev/peps/pep-0427/ * crate.io : a pypi mirror + website. so it's not related to wheel * distlib: subset of distutils2, containing the PEPs implementation * why: why what ?
If I understand correctly he's only using it to get around the blocking of subprocess.call so it shouldn't be much of a problem 
Yes it will only use one core because of the GIL. The use case of threads in python is best when you have IO in which case you won't spend much time in the CPU hence the GIL is not an issue. If your code spends a lot of time in the CPU and hence you want to spread the workload over multiple CPU then you are best served by the multiprocessing module which has a similar api to the threading module. 
It was never my intention to mislead. The upvotes suggest I'm in the wrong here, so I'll be more careful in future posts.
The Popen() function doesn't wait for the process to complete, so you can ping multiple addresses in parallel without needing any special threading code: pings=[subprocess.Popen('ping '+ip, shell=True) for ip in targets] completed=sum([p.wait() for p in pings]) 
Take a look at sympy.
This is too useful to me, so I went ahead and did it; https://github.com/stuaxo/add-dist-package.git It takes into account $VIRTUAL_ENV so works in the current virtual environment. Examples: Gtk2: add-dist-package glib gobject gtk-2.0 pygtk.pth cairo Gtk3 (Gobject introspection): add-dist-package gi 
Went ahead and implement it, as doing gtk2/3 with virtualenv is just too painful without something like this. https://github.com/stuaxo/add-dist-package.git 
It can work both ways, blocking and non-blocking. http://docs.python.org/2/library/subprocess.html#replacing-the-os-spawn-family
The GIL might be dropped when you make certain calls, particularly those that run for awhile and have no opportunity to corrupt the interpreter (things in subprocess would be good candidates for this, but I do not know whether or not the GIL is released after invocation of the subprocess). Also, this is a bit pedantic, but python will run on many cores, because threading typically creates actual OS threads. It just prevents them from actually running in parallel via the GIL. Basically: the threads will be run serially--which is not the same thing as "they will only run on one core."
Ok, thanks. I will update the text again, just to not spread wrong information about this. How would you write it?
off topic until the line: heh, you seem like a nice dude with good knowledge and intentions who enjoys his role as the misunderstood community worker ATM. use that status to get a few rants out there, but stop before you become known as the bitter guy ;) --- on topic: i meant that we need that until the distutils2 successor (sigh) is ready, in order to be forwards-compatible. and i fully support moving on and beyond setuptools to static metadata. and why was the path shot down? you said because some missed legacy compat… but why is that needed? deprecation processes are all about slowly rooting out obsolete legacy stuff. lastly, i agree with the haters that decisions like “how to replace `setup.py develop`” are important and something where that isn’t decided on smells unripened.
I have seen bugs that cause Popen to hang when created using multiple threads. Google "popen threads deadlock" to see what I mean. I redesigned an entire app because of their unrealability. The right thing to do is what Zouden suggests in this thread: run the ping calls using Popen and collect their results as they finish. 
I think you're wrong. My mind maps are for note taking and brainstorming, not for sharing with anyone else. Furthermore, DropBox just works for my syncing use cases. Most of the time, my mind maps will not make sense to anyone else, especially the ones I create for notetaking. Some of my mind maps would be too embarrassing to show, let alone explain. Sometimes I have to brainstorm for things under NDA (and therefore a browser would be *completely* inappropriate). So I don't see much validity to your arguments. Of course I'm also arguing from personal anecdote, but the fact that I can counter your premises means you may want to re-evaluate your position to determine if you have objective data that supports your premises across a broad-enough user population.
&gt; I think you're wrong. I'm not wrong, I just have another opinion. And I clearly stated that. It's completely reasonable not wanting to share your mind maps and I included this possibility in my comment. So you did not really counter it. Besides that, downvotes are for punishing comments that have nothing to do with the topic or are pure trolling. I was just expressing my opinion and provided reasons. If one does not like my opinion, they can just do nothing and upvote comments they like better.
Well, I've yet to see a web app for this that I really like, and Labyrinth is one of only two desktop tools that work the way I want (the other was a big, closed-source Java app). Most mind-map tools seem to work with a strict hierarchy, and they often place the nodes for you automatically. I like to place the thoughts myself, and link them together in arbitrary ways. So I'm working on the tool that most closely fits my needs. I accept that it won't fit everyone's use case.
Any sort of vector graphics export would be good. I guess SVG might be easier than ODG, but I don't know much about either.
Let me rephrase that. Concluding that something "is a waste of time" on the basis of anecdotal premises (and leading with that conclusion) does seem to obscure the part of your criticism that could be considered constructive. I was attempting to demonstrate why a web application might not be the optimal solution as you suggest, and therefore might want to reconsider whether this is actually a waste of time. I am not suggesting that you are necessarily wrong, however without stronger arguments you are unlikely to cause many to reconsider their own positions. I am unsure how your comment re downvotes is relevant to the discussion.
If you think IPython is cool, wait until you try DreamPie: http://www.dreampie.org/ It is the best interpreter for interactive/scientific type work in my opinion. It works great with matplotlib (unlike IDLE) and doesn't crash even when your code does.
Install EPD: http://www.enthought.com/products/edudownload.php
&gt; I am unsure how your comment re downvotes is relevant to the discussion. You are right, it is not. But I wanted to mention it. I didn't mean to imply that you may have downvoted me.
Does anyone have this code for Python 3 so I can play around with it?
Iterating over a dict itself produces its keys.
I wouldn't say that eval is insecure (it is but there is a much better reason). When you are developing your own language (albeit this "language" just consists of a few simple symbols) if you rely on eval you really aren't writing your own anything - you are just hinging on whatever language you are using. There is actually a book on how to develop your own language and the author uses evals all over the place - I want to strangle him.
Not relevant to OP, so downvoted. Post it as a top-level /r/Python submission instead. Personally I'd be particularly interested in a point by point comparison of DreamPie and IPython-Notebook from someone who obviously uses it, as the description on the site gives the impression there are very few effective differences between the two.
This is the same explanation as the one in the article, not a simpler explanation.
I guess you forgot to import matplotlib library. I wanted to try your code out but `plt.figsize(17,25)` didn't make much sense to me since I've never used matplotlib before.
Can you explain your second point please?
By simpler I meant short summary. Fewer details.
there is also the [Free version](http://www.enthought.com/products/epd_free.php), it has all the necessary stuff, and provides a very clean install on Macs!
I saw him on a web show about flags once.
The IPython notebook has the matplotlib and numpy libraries already imported.