When did Django become "bloated" and "enterprisey"? That thread has an amazing amount of Django hate in it.
That's an understatement. I found asyncore/asynchat so buggy that it was unusable. I remember it would just freeze and stop responding to requests at random, among other things.
10 year old stale code with some bug fixes slapped on it. I expect more from Python.
There's a course for Python 3 on [lynda.com](http://www.lynda.com/home/DisplayCourse.aspx?lpk2=62226), might be useful.
Sure, Peter, but not just because you're critical. For example, I have added improvements to Python logging not only because of criticism, but also because of *helpful suggestions* from a variety of people. The specific changes relating to formatting were discussed on python-dev about a year ago, where I made some of the suggestions which I have now implemented, but at that time it was not clear what the best way forward was. However, the perfect being the enemy of the good, the discussion petered out and nothing got done. When I revisited this particular area (and not because of any specific criticism I came across, I might add) I decided that I might as well implement some of those ideas, as nothing better had surfaced in the meantime. Note that while this change to logging may be helpful, people could have had {}-format support in logging at any time by implementing their own Formatter class, not a big deal if they wanted to use {}-formatting in logging across all their projects. Actually when people are critical, they don't necessary suggest any helpful way forward; for example, your comments in your twiggy-introducing post about how "Python logging is excessively like Java" didn't help at all, not just because I didn't find them convincing but also, even if I had, they wouldn't have helped make things better. Most of the improvements to logging have been because of specific suggestions for changes or additions (aimed at making logging better, and sometimes including patches) rather than open-ended criticisms which are not motivated by a wish to improve stdlib logging. Obviously bug reports are criticisms which are usually specific enough to suggest the way to make it better, so those are always welcome :-) 
I used [Dive into Python](http://diveintopython.org), there's an [updated version](http://diveintopython3.org) for Python 3 too.
Thank you so much everybody for the great links! Upvotes to everyone.
Really, who uses asyncore anyway?
This wasn't asyncore. Asyncore is a thin wrapper around select or poll. If you were experiencing lockups, then your code on top of that wrapper was responsible. Either that, or the [underlying implementation](http://blog.ccpgames.com/kristjan/2010/09/27/selectmodule-on-ps3/) of select or poll can't be ruled out either. I've maintained code based on asyncore for several years, and I've never found a bug in asyncore. I've never had any of the bugs reported in it turn out to be problems in asyncore either. However, I would in retrospect just base my code on select or poll, and remove a layer of unnecessary abstraction.
Who doesn't use it? :-)
It really is. Example: The entire way the settings system works with lazily-bound globals.
I use smtpd which I believe is a thin wrapper over asyncore. I left it on for about 3 months and it crashed saying that asyncore failed to catch a certain exception. Normally asyncore is supposed to catch all exceptions that occur and continue the event loop.
The problem with Django is it is very hard to extend if you need something it doesn't do sometimes. At work we need to be able to do datetime interval expressions via the ORM, and there is simply no good way to do that other than maintaining a custom, patched Django source tree.
speaking of learning of example. I would like to recommend Python Cookbook: [Online Version](http://code.activestate.com/recipes/langs/python/) and [Print Version](http://oreilly.com/catalog/9780596007973/)
miss the days when Alex Martelli chasing SO, learned a ton of python tips.
Could you not get your patch applied to trunk of Django?
I use eric and I love it. Started using it when it was eric3. I use VIM for large edits and the built in editor for small changes while debugging. Source control I use is git so I just run the git shell or gui when I need. 
I use eric4 and gvim. Open the file up in both then when you switch between the to editors you will be prompted with file changed update file yes/no. Easy solution.
Its a hard thing to do right, the basic patch is actually from an old ticket. I've spoken with Alex Gaynor about it, it is on his list to look at but there are only so many hours in the day. Writing clean extensions to the ORM is very hard, but ugly hacks get the job done for now.
Maybe after it's lingered in design decision needed for a couple years you can.
Yeah it seems like in addition to worrying about scaling to Facebook's size before you build your site, the new thing is to presume your site will be so custom and hardcore that Django will be like *so limiting man.*
&gt; Hmm. I wouldn't really have expected the ORM to manage schema migrations - is that the ORM's job in other frameworks? Many don't Rails does. South is an add-on for Django. &gt; I have no idea what the generated SQL looks like (isn't that the whole point of having an ORM?) You *HAVE* to understand what your ORM is doing eventually, if you want performance. For Django, an easy way to see what's going on is to user the django debug toolbar. I find I don't usually have a particular problem with the SQL generated but rather with the number of queries that can be generated by seeming innocuous Python statements. &gt; but if it doesn't do good query optimization then I'd agree for sure that that's a flaw. An ORM really shouldn't do query optimization per-se. It should generate reasonable SQL. But SQL optimization is something left to the guys who have stats tables on the data (i.e. the RDBMS itself).
Thanks, that's a good response. I know about South, and of course it's vital - but it makes sense to me as an addon, not as part of the ORM. Thanks for clarifying about SQL perf - I think I know what you mean.
THE HARD WAY??? http://learnpythonthehardway.org/index :p btw how to think like a computer scientist is pretty good imo, its short concise and by the time you read a bit of the book you can begin writing meaningful programs(but all good introductory books are like that) http://openbookproject.net/thinkcs/python/english2e/
I can't recommend this enough. I went through all the class last week and i found the method at teaching is amazingly good. The very first thing I wrote after finishing the lecture was a web crawler that connects to NFL.com and finds every player on every team and downloads their photograph. I had no idea how to do this just 7 days ago.
Are you serious? Interviewers aren't allowed randomized question sets? Where is this exactly?
who doesn't know what phoenix is.
2.7 will die before 3.2
Really great !
It was asynchat, actually.
The US. If an interviewee knows he was asked a different set of questions compared to another person, bam lawsuit.
I wouldn't hire me either if I provided that code in an interview.
Good point. I should have said "previously" instead.
Commenting for future reference. Thanks :)
Would you say keeping python backwards compatible into the future from 3.2 be possible? I'd say backwards compatibility could be the greatest thing that happens for python. Look at java, anyone can use the latest greatest release and still include old libraries/packages.
You're confused about what's "special" or "magic" about the underscore methods. The underscore methods are magically invoked by special syntax which does not resemble a method or function call. So, as you alluded to, `foo[i]` invokes `foo.__getitem__(i)`, and `import bar` invokes `bar = __import__('bar')`. The `__len__` method is magically invoked when an object is evaluated in Boolean context and no `__nonzero__` method has been defined. That's because Python's built-in collections are considered False when empty and True when non-empty. Try this code (adjust as needed for Py3): class StupidList(list): def __len__(self): return True if list(range(10)): print 'list 0-9 is True' if list(): print 'empty list is True' if StupidList(range(10)): print 'StupidList 0-9 is True' if StupidList(): print 'empty StupidList is True' (edited for clarity) 
Yahoo! Pipes has always seemed like a fantastic idea to me, like we finally found an easy way to bring the full potential of the internet to a casual user. However, every person I've ever demoed it to has been like "meh", even when I solve a difficult problem they have with pointing-and-clicking.
Man... I didn't even *try* to go all the way through, the section between the two snakes was too thin for my blood...
If she's your fiancee, you knew she's a keeper for a while now, haven't you? :)
There was still time to back out if I failed at the PumpkinPy.
There *was*? It's not in the bag yet, is it? Just askin' :P
Yes, but /r/Python didn't know yet. :-)
I love the seeds for eyes, good work to her!
We are happily and geekily affianced. :)
`chaco.shell` provides interactive plotting commands.
I wussed out of a companion SciPy pumpkin.
Nooooooo!
so its a swastika 
cool ! now send it to Guido
 import pumpkin
No, the whole point of Py3k was to break backwards compatibility in order to revert some design mistakes which were done a long time ago. While this might be desirable for some people who want a clean language, it is very impractical for others.
I didn't mean keeping compatibility from major version 2.x to 3.x, but between minor versions. Packages written for 2.6 aren't compatible with 2.7, same goes for 2.5 not being compatible with 2.6. I've never seen such a fragmented language before. This requires one to find necessary packages and then get the intersection of their support, which hopefully results in something like 2.6 or 2.7 and not the empty set. Lets make 3.2+ compatible with 3.2 for posterity, now that design mistakes made a long time ago have been fixed. Imagine one day having PyOpenGL for Python 3.2 and upgrading from Python 3.4 to 3.5 and just using it.
Logtailing to generate rules based on events... I.E. a big spike in failed logins from a specific address or network could result in some sandbagging, blacklisting, or other behavior. 
How do you know a redditor has a pumpkin? he'll tell you
Previously on Python reddit: [pycparser v1.0 is out! - a complete ANSI C parser written in Python](http://www.reddit.com/r/Python/comments/7dke2/pycparser_v10_is_out_a_complete_ansi_c_parser/).
TYL: The Python logo has Pythons. The FedEx logo has an arrow in it, too. Good luck unseeing!
I am glad that PyPy is making progress. I just wish it was making it faster. I am hearing rumors that the unladen swallow development is slowing down. Python will a huge boost when stop being perceived as a slow prototyping language only.
Dynamische macht frei
it takes time you know. besides, there is a paypal button on the blog if you really care about faster progress :)
TIL There was a verb for "fiancee".
What are the eyes made of? I see they are being held by some wooden sticks, maybe foodpicks, but I can't figure out the eyes. Is it painted foam?
You can do those checks much faster if you first try to resolve the domain name via dns. If this fails try whois.
&gt; you see my friend, there are two kinds of people in this world. those who have the gun, and those who dig. you dig.
It doesn't even have an entry on [PythonSecurity](http://www.pythonsecurity.org/)
To be fair, Flask also uses lazily-bound globals.
Python written by an incompetent programmer is just nicely indented crap. Chances are they will never read PEP8, never write tests, always use "from x import *", use bad variable names, write badly designed classes/functions. Given a choice between maintaining code written by a competent PHP developer and code written by an incompetent Python developer, I'd choose the former. All that aside, Python is a far, far better designed language. But it's the developers (and management) that make or break a project, not the language.
I don't know what I like more. The pumpkin or that pun.
Pumpkin seeds.
According to Guido himself, the whitespace sensitive syntax makes it impossible to have proper, multi-line lambdas. If that's true, that's a very good reason to think block-by-indentation was a shit idea.
This is exactly what I'm doing in my app before I call the code I just released :)
&gt; first try to resolve the domain name via dns Could you please give a concrete example of how to do this from your python script? 
Maybe she's friends with [this guy](http://www.reddit.com/r/Python/comments/dykhk/just_finished_carving_my_pumpkin_its_more_dynamic/).
Newest types in Python: pumpkin
Magic is good.
I wont be using your code, but I can give you a comment on the code. if json_obj[domain] == "available": # do stuff if json_obj[domain] == "unavailable": # do stuff etc can be if/elif blocks. 
I will throw it out there that some sites that check this for you sell the search data to companies that then might buy the domains holding them hostage. Be careful how you search for domains. 
s/pumpkin/girlfriend/g
kewl
good.
Hmm, shouldn't this kind of thing be on stack overflow? There have been a few questions lately, from newbies looking for help; those should all go on stack overflow.
Thank you! I was just looking for a way to do this using python and PIL. 
cool, I've explained this idea without the saturation component for use in a couple projects, but I'm really wondering about that google logo choice. the dark green in the l vs the blue of the G and the g. anyone explain that one?
Is this used with Ubuntu's Unity? I was wondering about how they were figuring out the dominant color on the icons. This might explain that.
&gt; Any mass and/or automated inquiries made of our Whois database, as well as any failed or successful attempt to bypass the technical security measures in place, and any use, extraction, transfer, storage, reproduction or redistribution, for any other use or intent other than the viewing of this information, the registration or modification of a domain name, is strictly forbidden and may be grounds for criminal or civil prosecution. From Gandi.net's Terms of Use.
Indeed. As stated in the docs, most of the math module's methods return floats, which are obviously inappropriate here.
It's a fair cop.
Have you run the code on the Google logo? I thought that result was interesting, too. I'm heading out the door literally as soon as I leave this comment, or I would try it myself, but my approach would be to run the code, tweak a parameter or two, and run it again, comparing the results.
No. Care to share? *My math sucks
I'm suddenly having flashbacks to the 90s :) I think I learned to program Perl by writing an IRC bot, way back in the day.
But has she upgraded to Python 3 yet? This is could make or break the relationship..
I've found that even though I get less 'for free' and have to write a little more glue code with Flask, the individual components are of better quality. I think Jinja2 is far superior to Django templates and SQLAlchemy is a more powerful and configureable ORM.
i like to think that python provides nice convenient builtins to access some of the most common values you'd need on an object. using len is super easy, dude plus, i can rewrite len and automatically get extra functionality for all my collections
Yes, visit http://www.cj.com and sign up. Every time someone uses a coupon, advertisers pay a referral fee. Also relevant: http://www.abestweb.com/
Thanks!
Ha, that was the idea.... 
Ugh, I wish they stayed with \_\_cmp\_\_ for python 3. 
Sort of what I was thinking a few days ago. The more I use Python 3, the less I like.
repoze.bfg is supposedly one of the backends for Pylons2. This looks pretty snazzy to me, light enough for someone to really understand your entire application from end to end, yet pythonic enough to not be tedious about it.
 import socket try: output = socket.gethostbyaddr('google.com') except: print "Not Found!" gethostbyaddr throws a exception if the domiain isnt found 
I spent twenty minutes this evening nslookup'ing hostnames for a new blog while pondering whether if would be easier to script up the process in python. Thanks!
Define "dominant".
true true.. As much as I like the unification of strings, dealing with email (which are rarely unicode) and some web which mixes UTF-8 code and UTF-8 web template files and responses, etc.. the fact that I know it's converting to UCS 2 internally and then back internally drives me a nutty. 
unity is written in vala, so no, they are unrelated.
There's also a nice nose-ipdb package (nosetests --ipdb &amp;&amp; nosetests --ipdb-failures); btw, this is the reason I use nose :) Also, if you have some unchecked exception stopping your script and you can use pdb (or ipdb, as of october 20th) in postmortem mode by running python with the -i script: $ python -i (my script and args) ... (traceback) &gt;&gt;&gt; from ipdb import pm; pm() And you're in an ipdb session. Sadly, `-m ipdb` does not work.
&gt; Keep in mind that pycparser doesn't do the pre-processing, so to support preprocessor C99 features (such as variadic macros and C++ style // comments) use a C99-ready preprocessor. That's not so good. A C-parser without a C-preprocessor has little value because one can't parse anything in practice.
I use i all the time for filtering the news. It's the greatest !
Related: I remember a [recipe](http://code.activestate.com/recipes/576685-total-ordering-class-decorator/) to automate the process of implementing missing comparison functions, though it could certainly be extended with __cmp__.
use os.system or whatever. It's the most consistent answer to WHOIS in Python I have seen. Plus that way you get to use the system WHOIS info (i.e. /etc/jwhois.conf). 
coolness.. Sorry for asking a dumb question but I can take that info as save it as a dump within python or will I have to use the system pipe function ? And that is , or so I gather, just for linux/unix ? So I guess windows users are out of luck ? 
A quick Google search reveals a few python WHOIS clients out there you could use. Edit: Though I am an idiot who missed the critical "3" part. No worries though. Here is a skeleton client here that should work cross platform: http://pastebin.com/6nvDHVXU 
&gt; I want to learn python to get rich and make sexy chit chat with girls.. I appear to be doing it wrong :-|
Pumpkin macht pie.
http://dreampie.sourceforge.net/
http://dreampie.sourceforge.net/
I had a couple problems getting VirtualEnv, VirtualEnvWrapper, Django, and Postgres working together properly - here's what I had to do to get it to work in Ubuntu 10.04. - Hope this can help someone: sudo apt-get install python-setuptools libpq-dev sudo easy_install pip sudo pip install virtualenv sudo pip install virtualenvwrapper wget http://python-distribute.org/distribute_setup.py python distribute_setup.py vim ~/.bashrc append the following: export WORKON_HOME=$HOME/.virtualenvs export PIP_VIRTUALENV_BASE=$WORKON_HOME source /usr/local/bin/virtualenvwrapper.sh mkdir $HOME/.virtualenvs source .bashrc mkvirtualenv --distribute --no-site-packages &lt;example.com&gt; easy_install -i http://downloads.egenix.com/python/index/ucs4/ egenix-mx-base workon &lt;example.com&gt; wget http://initd.org/psycopg/tarballs/psycopg2-2.2.2.tar.gz tar zxf psycopg2-2.2.2.tar.gz cd psycopg2-2.2.2 python setup.py install
I'm surprised you needed to get `distribute_setup.py`, that should come with virtualenv (as should setuptools). Personally I've stopped using `--no-site-packages` and just keep my global `site-packages` clean, while installing things like mx-base or database drivers from packages.
It's Godwin's law
I find it perfectly fine. That char == 'O' or char == 'Q' is clearly low level C-style stuff.
Careful now. Asking different questions is not inherently illegal, it is merely possible *evidence* of "discriminatory selection."
Turning sage or parts of sage into a python module is an exciting idea. It would make it much easier to create sage-powered webapps using your web framework of choice, be it Django, web2py, Flask or something else.
ah yes, I should mention that the steps noted above are just how I managed to get them all working in an existing (messy) development environment, which had a ton of old site-packages to begin with, so realize that my packages may have been out-of-date (causing me conflicts), or some steps could just be completely unnecessary and redundant, so I appreciate the heads up for future reference. Ideally in a clean environment, omitting --no-site-packages, might be an idea worth considering, thanks. I suppose it really depends on how often psycopg2 and mx-base update, and how multiple Django instances, spanning different versions would be affected when updating your site-packages. I tend to doubt it would be a problem, but I'm in no position to argue either way.
That is not how inheritance is supposed to work, so if you want to break the programming model, then yes, you will have to do it manually. If you explain what you are trying to get done, we can probably suggest a more idiomatic, fluent alternative.
&gt; That is not how inheritance is supposed to work Not sure about that, it's trivial to do it with methods, so why not with attributes? &gt; If you explain what you are trying to get done What the example shows: (optionally) define a given class attribute at each level, and be able to collect sum total of all defined attributes from any point in the hierarchy. In other words, I want this: &gt;&gt;&gt; class Foo(object): ... a = classmethod(lambda cls: ['foo']) &gt;&gt;&gt; class Bar(Foo): ... a = classmethod(lambda cls: super(Bar, cls).a() + ['bar']) &gt;&gt;&gt; class Baz(Foo): ... a = classmethod(lambda cls: super(Baz, cls).a() + ['baz']) &gt;&gt;&gt; Baz.a() ['foo', 'bar', 'baz'] using attributes rather than methods.
IMO it's always been that way.
Yes. From what I hear on the interwebs, you have to learn Ruby or JavaScript for that.
You could try implementing your "attributes" as properties. Then you'll have a defined class when you try to call super. Also, reiterating your planned implementation is not what I meant when I asked what you are trying to do. Presumably there's a real world use case for these gyrations.
&gt; You could try implementing your "attributes" as properties. I'm not sure how that would change anything. &gt; Also, reiterating your planned implementation is not what I meant when I asked what you are trying to do. Presumably there's a real world use case for these gyrations. Filters. Each class in the stack can get 0..n filters, a given class applies the filters of all the mro class. This is specifically used to filter in and out extensions (think open classes) in a pluggable system. (also gyrations? seriously?)
`"-".join(["{0}{1}".format(k,v) for k,v in myDict.iteritems()])` will give you `"a1-b2-c3"`. Knowing that, you can do something like filename = "-".join(["{0}{1}".format(k,v) for k,v in myDict.iteritems()]) filename += ".txt"
 filename = '-'.join(['%s%d' % item for item in myDict.items()]) + '.txt'
Ah -- very cool. Thanks!
 filename = "-".join([str(k)+str(v) for k,v in myDict.iteritems()]) filename += '.txt' Generators are you friend. Learn to love them. They handle problems like this very cleanly. Also the join method on strings is perfect for handling exactly this type of problem. Keep in mind this code is NOT deterministic...for the same input dict iteritems is allowed to return the items in any order. 
Thanks!
No prob.
Strip the last character before adding the .txt?
Why not just cut the end off of the string? filename = filename[0:-1] + ".txt"
 filename = '-'.join([k+str(v) for k,v in myDict.iteritems()]) + '.txt'
BTW, if you ever need a count variable, you're better off using enumerate() to wrap your iterator: for index, (k, v) in enumerate(myDict.iteritems()): print "%d: %s=%d" % (index, k, v)
filename = "-".join([str(k)+str(myDict[k]) for k in myDict.keys()])+".txt"
I was wondering how I could use the enumerate() function in this way. That clears it up a great deal. Thanks.
I was worried about this. Is there a way to order them, say alphabetically? *edit*: terribly worded
If you need the keys sorted: filename = "-".join(["%s%s" % (k,x[k]) for k in sorted(x.keys())]) filename += ".txt"
Change `myDict.iteritems()` to `sorted(myDict.iteritems())`
Awesome. Thanks.
Try using ... sorted(mydict.iteritems()) ...
Exactly. Using `join()` is definitely the right way to go about this. Also should note that when using a dict the elements are not ordered. So if you do in fact expect them to be in some consistent order between different dictionaries you may want to use `sorted(myDict.items())` instead. You can provide a custom comparison function to sorted if you want a sorting other than straight on alphabetic.
That's how I would have done it, but I guess '-'.join() is more Pythonic. Honestly, though, I just can't get my head wrapped around `'-'.join(txtList)`. Why isn't it `txtList.join('-')`? I'm taking a list of stuff, and joining them with '-'. It just seems to read better. edit: On consideration, maybe because I'd have to create the txtList first, before I could join it. The current behaviour allows me to join on the fly, so to speak. Still...
In much the same vein as the other solutions posted so far: filename = "-".join(sorted("%s%s" % i for i in myDict.items()))+".txt"
I think the [] inside the join are unnecessary. Plus you will use a generator expression instead of a list comprehension, which is a lot more memory efficient.
The lack of proper encapsulation.
I know this is 5 months old now but something I tripped over in the 0.9.7 book when attempting to learn 1.0 is that everything falls apart when you try to construct the simplesite example.
bad idea. just get a good ops guy to make you some images of different environments and automate your build system to boot and run them on cloud servers automagically and on demand. problem solved.
the best way to remember how to use enumerate() is as follow: items = [10, 20, 30, 40, 50] for count, item in enumerate(items): print count, item 
Problem solved if you can afford a good ops guy, you mean. And cloud servers. And whatever licensing you need to run those images. What if you don't have any of those things?
Exactly, it'd be a pain for Django (even with DSF funds) to put together all the possible combos of servers, it'd be impossible for me to put it together for my random apps.
This is an excellent question to which I learned the answer at this year's [PyTexas](http://pytexas.org/PyTexas). It's a method on str because it needs to work with pretty much any iterable. That is, you don't always want to join a list. Sometimes you want to join a tuple, or a string or what-have-you. Of course, I guess they could have just made it a part of the iterator protocol that you have to implement, for example, a \_\_join\_\_ method for every new iterable, but then it's perhaps not clear what you should get back. This way it's very clear that you're getting a string back. Moreover, this method preserved backwards compatibility for iterable objects. Note: I haven't read any of the relevant PEPs on this. It's just the impression I got from the discussion, and it makes sense to me.
This is the right answer. For the record, though, if you want to capture the index while you're iterating, the enumerate() method is what you want: &gt;&gt;&gt; d = {'a': 1, 'b': 2} &gt;&gt;&gt; for i, t in enumerate(d.iteritems()): ... print i, t[0], t[1] ... 0 a 1 1 b 2
filename = "%s.txt" % "-".join("%s%i" % (k, myDict[k]) for k in sorted(myDict.keys())) I find that easier to visualize reading, but I don't think you gain any real performance improvement by using string substitutions. the %i may end up fragile, but you can use %s or %r if one will properly format whatever type you're using as values If you've got predictable keys (e.g., it's always 'a', 'b', and 'c') and you're going to do this several times (like inside a loop) you should pre-sort them and move that outside the loop.
Small note to say the "[ ... ]" list comprehension is optional since Python 2.4, since: '-'.join(k+str(v) for (k, v) in myDict.iteritems()) ..forms a [generator expression](http://www.python.org/dev/peps/pep-0289/).
Amazon AWS with the API so I can automate booting and using spot instances when AWS is low on traffic and pay dirt cheap rates, I pay ~$16.00 a day and I spend 2 hours of time to run my tests. that gets me: * 2 Windows boxes (2 hours - Amazon pays windows license fees) * 4 linux boxes (8 different images rotating each hour) * 1 MySQL server (2 hour on a mid size box) * 1 Postgres 9.x server (1 hour on a medium box) * 1 Postgres 8.3 server (1 hour on a small box) Each linux image runs all the different versions of code to test for test for regressions on updates. Some servers run the db locally and others connect to a central with mock data. What is awesome is that it's all automated so with one button on a webpage, I can launch a server, run a test, and shut it down when it's done and pay ~$0.10. I used to do this on my continuous integration process but I gave up and just left it to the nightly. 
these aren't exact hours. machines shut down when they are done. app server boxes trigger database servers to boot and they have to do a keep alive ping in the code that resets a timer on the db servers to keep them up or they will shutdown. I may open source all this automation some day when I remove all the hardcoded bits. 
That's actually really impressive. Thanks for sharing your process.
&gt; I may open source all this automation some day when I remove all the hardcoded bits. Please do!
Thanks. As you say, with '-'.join(), .join() only gets implemented once, and is therefore more Pythonic. But I would never have figured that out on my own. Which is why I'm not designing languages, I guess.
With all the similarities there are in syntax, what makes Jinja2 better for you than Django templates?
Have a tested django in freebsd is a good motivation for me to donate machine resources. I don't care about security. It's easy give you a jailed instance or virtualized one. But you must be careful about security. I should fake the tests. It's common implement restrictive politics for avoid fakes. I don't want manage strange ini files. You can get this information running a script (Os family and version, python version, ...). If you trust me I can share some freebsd resources to django project. It's good for me too. I can spend some hours installing software base and writing automation scripts.
Which C++? No two programmers know the same features of the language, so it's sort of like Grace Hopper's quote on standards — "the lovely thing is that there are so many to choose from."
"No one sane" — well, that excludes the PHBs who back techies into that corner. And WTF is going on with Reddit that you now have to wait *ten frigging minutes* to post a second comment in a thread? If that's the best we can do in the fight against bots, the terrists have already won. (Though if the BBC can hire even a tiny 23K-node botnet for a demo on Click, that's pretty much a foregone conclusion, no?)
I agree, except for the last paragraph: &gt; Python’s distribution utilities system is well designed and thoroughly implemented.
That makes more sense. =)
I use python almost everyday, as a 2nd year physics undergrad at the UofToronto. 
Very nice. I'm using python for semantic analysis. 
speculative: metaclass+closure+specification pattern class Meta(type): def __new__(meta, name, bases, attrs): def get_filters(cls): filters = set(cls.filters) for base in bases: if isinstance(base, meta): filters.update(base.get_filters()) return filters attrs['get_filters'] = classmethod(get_filters) return type.__new__(meta, name, bases, attrs) class A(object): __metaclass__ = Meta filters = [1, 3, 5] class B(A): __metaclass__ = Meta filters = [2, 4, 6] class C(B): __metaclass__ = Meta filters = [0+1j, 1-1j] print(A.get_filters()) print print(B.get_filters()) print print(C.get_filters()) B.filters.append('xxxxxx') print(A.get_filters()) print print(B.get_filters()) print print(C.get_filters()) C.filters.append('yyyyyy') print(A.get_filters()) print print(B.get_filters()) print print(C.get_filters()) B.filters[:] = ['banana', 'apple'] print(A.get_filters()) print print(B.get_filters()) print print(C.get_filters()) 
Yeah, that had me let out a long pfffffffffffffffffffffffffffft.
R + Python is most of what I do (okay, there is a smidgen of Bash in there). Python doesn't have the analysis tools that R has, but I like coding in Python MUCH more than R. Ooo... but, R has nicer graphics systems... but it sucks at parsing files... [and this goes on]
&gt; speculative: metaclass+closure+specification pattern Yeah, didn't want to introduce a metaclass (I tend to use them far too often already), but it looks like I don't have a choice.
I haven't really found any place where matplotlib didn't do what I needed. I'm curious to hear if you have.
A different albeit partial solution : reduce(lambda x,y : str(x) + '-' + str(y), sorted(m.iteritems())) Since iteritems return tuples, the output is not exactly what you want : "('a', 1)-('b', 2)-('c', 3)-('d', 4)" I'm guessing it could be tweaked, but I don't know how. I will be thinking about it though. 
How sure are you that a) you actually need this problem solved and b) this is the best way to solve it? Honestly, I'm not sure I fully understand what the purpose of it all would be. Why should a user want to click on what is essentially a version of what's already on the screen? 
[Pony Swarm. Hehe.](http://fc07.deviantart.net/fs70/f/2010/055/f/f/ff4524ff7762b1b19dfda0cad199cf91.jpg)
history repeats with new tech ;) 
After reading this article I definitely want to try out python's plotting capabilities. The only thing I dislike about python is its whitespace dependence, it kept me from using it for several years (which is only annoying because of it I missed out on several years of experience with it through my own shortsightedness).
I guess I've never seen the whitespace thing as that big a deal. It forces a modicum of readability, which I think is a good thing, but I've never noticed it getting in my way.
 filename = "-".join(k + str(v) for k, v in myDict.iteritems()) + ".txt"
Ditto. I've done most of the data processing for my thesis with Python, and have now mostly switched to R for stats and analysis. R is fantastic for stuff like regression modeling, where it gives you basically exactly what you need. And there's a package for everything...except its probably not a very well-thought-out package, and you may have to actually use three packages to accomplish a single task (each one will perform one quarter of the task, and 15% of the function names will overlap). And don't get me started on data structures in R...did this function just return a list? Data frame? Object? Matrix? Have you used the SciPy stats module at all? I haven't, and would be curious to know what it's like.
I don't mind it now, it was mostly just because it was a departure from how I had previously programmed. The _only_ issue I still have with it is I like to have a second window open with the python shell to test code snippets. But in order to paste them in I have to go through a second step of removing the indentation. And to be honest if this is the only issue I have with a language, that is a pretty good sign for me.
 @classmethod def a_list(cls): for t in cls.mro(): try: yield t.__dict__["a"] except KeyError: pass
%cpaste from the [ipython](http://ipython.scipy.org/) ~~might~~ *will* do the trick for that bit. (just tested)
Then, I want to know, which Linux distribution works better with Python related scientific tools? I still use MS Windows mostly, with some Ubuntu experience. I want to build the lastest version of Python(2.7 at the moment), NumPy, Matplotlib, SciPy, IPython...etc I didn't compile matplotlib's source code (on ubuntu) yet. Update: At first, sorry for my broken English. Thanks for all of your replies. these few days I try ubuntu and archlinux. now I am using Arch Linux. First of all, I select ubuntu, my old friend which I started to use with version 5.10. Ubuntu is easy to install with a USB Stick and user-friendly. apt-get already has the mainstream python scientific computing software: numpy, ipython, matplotlib, scipy, etc, although not the lastest version. I can comiple python itself(2.7 or 3.2) with source code, and some python scientific packages, and lastest ipython, numpy. so far so good. After reading the news that [Arch Linux - Python is now Python 3 (archlinux.org)](http://www.reddit.com/r/Python/comments/dtkpz/arch_linux_python_is_now_python_3/), I think maybe I should give it a try. It's not as easy to install and use as ubuntu (at least to me). a lot of times I have to reading manual or searching the web, for X configure, input method (for my native language), choose, install, and configure Desktop Environment. on the other side, the version of software is up to date, and document said ArchLinux is a rolling release distribution, it means there is noneed to reinstall or perform elaborate system rebuilds to upgrade to the newest version, Simply issuing pacman -Syu periodically keeps the system up-to-date and on the bleeding edge. I think mathbuntu maybe follow the same software version and upgrade policy with the whole ubuntu family. I did not know about the concept of this "rolling release" and now I am satisfied.
Very cool. Now I feel like I owe you something in return as I know this will increase my efficiency :)
In all fairness, it was a significant improvement over how things were done previously, and certainly better than how to do it in e.g. matlab. But well designed it is not 
Funny you should show up, I was pretty much thinking of your toils when I wrote that. :)
All Linux and all major BSD distributions should work equally well.
You might like to try http://www.mathbuntu.org/ not sure of the version of python though. On windows I'm still using 2.6 with all those packages and it seems fine. Also, I think if you install scilab it will install a working complement of those packages too (might be 2.6 or 2.7 not sure)
*cough* *cough* 3D plotting *cough*
Python rocks, because it is Python. 
Book spam.
In Python: import math def fib(n): return math.floor((phi**n/math.sqrt(5.0))+1.0/2.0)
Ditto for imagemaps/contourmaps. 
Our [C++ guidelines](http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml) are rather comprehensive.
It's like the parentheses in the Lisp languages, you get used to in not too much time. It's what you'd do in anyway in C/Perl/Java etc, to make the blocks readable. Some of my first coding experiences were with punch cards with BASIC, Fortran, and [RPG](http://en.wikipedia.org/wiki/IBM_RPG_II#Sample_Code) which, because of the medium were whitespace dependent. Python's is nothing like them!
Or bust out Emacs, load in a \*.py file, `M-x run-python` to get you a python shell going, select a region of code, and `C-c C-r` to execute the region in the python shell. Lather, rinse, repeat. Edit python-mode.el (or python.el, or something) so that it runs ipython instead.
For Windows, try [Python(x,y)](http://www.pythonxy.com/). It's "batteries included" squared, with a [metric arseload](http://code.google.com/p/pythonxy/wiki/StandardPlugins) of additional libraries and IDEs, weighing in at a mere 500 Mb (or so). Get the .torrent, and have fun.
imagemap and countourmaps are working fine. imshow, pcolor, contour and contourf are your friend. the 3d plotting sucks, agreed. and i would love a pgf backend.
agree, just some distribution are newbie-friendliness. some distribution focusing more on performance 
thanks, after these years with Python learning and coding, I feel Linux maybe a more right choice than windows, isn't it?
thanks, looks interesting, l'll check it (with ubuntu 10.10 and archlinux)
It depends on what you need to do. I got Python(x,y) because it includes a lot of stuff I get myself, then have to compile and install; I've long wanted a really complete Python distribution, and the Python(x,y) version for Linux is just not at complete as the Windows. So I use it on Windows, and on my Ubuntu Linux side I use the default Python with any other libraries I can load from the repositories with Synaptic/apt-get/etc. But even with that, the windows Python(x,y) is more complete and easier to install. I still do most most of my Python hacking on Linux, however.
Is python as popular in the outside world as it is on reddit?
I would like to know too. I didn't see it during my studies, only Matlab.
Isn't there a "transparent" R python binding?
While pycparser doesn't implement a C-preprocessor, it allows you to use one of your own. This is trivial on Linux and other systems with a handy gcc, because there 'cpp' just runs the C-preprocessor for you. For Windows, pycparer carries around a pre-compiled executable 'cpp' for the user's convenience.
a) I do need this problem solved. I am making a mock up with a projector that will have portions of the screen covered (and in accessible), but I still need to hit those buttons. b) I'm not sure what the best way is, that's why I posted here. Imagine you had half a screen (left side), but still needed to hit the buttons on the right side. The only way I can think of achieving this is by mapping those buttons on the right side to the bottom of the screen on the left side.
&gt;The only thing I dislike about python is its whitespace dependence After taking a whole *day* to get used to it, I've hated semicolons ever since. The most pointless character in programming language history. 
http://rpy.sourceforge.net/
For all your 3D - plotting needs, there is Mayavi. http://code.enthought.com/projects/mayavi/
would a shared screen be workable? I mean use a vnc or ms terminal services to access the other half? Sorry if I don't have a lot pythonic suggestions, but you could take a look at an opensource vnc for hints as to how they do it. Im kinda doubting there's gonna be a lot of libraries to make this easy in python.
rpy &amp; rpy2... they can be helpful but there still are some gotchas so, unless I have some really strange data structure, I just export to TSV or sqlite or whatever and then import into R.
I remember the first time I tried Fortran 77 &gt; Why won't this compile? Oh, duh, I'm in the wrong column. I remember feeling free when I saw Fortran 9x :D
depends on the field. I've seen quite a few Physicists who like it for munging data, but I've seen an equal number who prefer something else (such as perl, awk, fortran (yes, it's scary too), C, &amp;c.). Amongst programmers, it's obviously dictated by what your job is, but many people write Python at home. In the enterprise, I've seen Jython quite a bit, because of it's integration with quite a few enterprise-oriented products (like Weblogic and the like). 
Yeah, and numpy, I also use Scientific Python for its netCDF support. The other is [Sage](http://www.sagemath.org/), which is the newer cool-kid on the block, but also VERY bloated and for stats they just say "[just do it in R]". Actually, I've found R's packages to be more credible and legitimate, for the most part, the packages are part of peer-reviewed journal article or a book that an academic has published. I've backed away from using Python because the [3rd party] modules are sketchy and/or aren't 'all on the same page', if you know what I mean. Scipy's stats package is a real disappointment. After having used R, I have trouble taking it seriously. It's not that it's poorly written, someone has spent a lot of time on it, but it's just too sparse for high-powered stats. That package might be handy if I was to go and code something by hand, but chances are that R already has a pretty-good or better implementation of what I need.
Or the underlying library, VTK, which also has Python bindings and none of that "traits" mess that Enthought's VTK wrapper seems to be rife with. Sometimes, I'd just like to be able to do a 3D surface plot in less than three commands akin to how it's done in Matlab.
matplotlib has become much better in recent years, and I see it getting better in the future. I think that R still gives you quite a bit more control over your graphics and it's considerably more developed. I'm trying to think of a more specific example aside from multi-dimensional data... I've recently started to work with more R's '[ggplot2](http://had.co.nz/ggplot2/)' which has some very sexy design to it. This kind of polished product would take *considerably* longer in Python. If I didn't already know R, I could probably survive on matplotlib, with what it is today. But, I wouldn't have started using matplotlib for publication until recently. I've also started to work with [Processing](http://processing.org/about/), which is a bit lower (Java-based), with a strong emphasis on real-time graphics and interactivity, which neither python nor R do particularly well (there are domain-specific exceptions to this). I would love for Python to be able to do this as elegantly as Processing.
Yeah, they work fine. Not. Getting an arbitrary axis around them doesn't work 'fine'. Feeding your contourmap a matrix (from a csv-datafile) and using the axes to zoom: no-go. logaritmic axes and ditto contourmap: no-go. 
hear hear!
I use Ubuntu.
"Pythonic" is beautiful, explicit, simple, complex (but not complicated), flat, sparse, readable, and [many other things](http://www.python.org/dev/peps/pep-0020/).
Pythoresque.
&gt; Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, **only 19 of which have been written down.** But, why?
the last one is ineffable
Interesting, although I couldn't find anything decent describing how to use VTK directly from Python. But how is using VTK directly easier (or more compact) than using Mayavi? Many basic plots (surfaces, points, 3D lines, meshes) are single functions when you use the Mayavi mlab api.
If you must think about, it's not pythonic.
Yeah, I started out using the mlab api with Mayavi scripted from Python, but it's just so cumbersome to code that way; it's centered around a GUI. Conversely, the VTK Python bindings are meant for doing advanced plotting non- or semi- interactively, which is what you need in a HPC setting. Plus, bare-bones VTK has support for parallel rendering, which Mayavi doesn't do at all.
Check the stackoverflow comments, the "stop saying pythonic" guy goes on to endorse the lambda version as being cleaner and faster.
So he has something to tell the young guys that climb all the way to the top of his mountain asking for advice.
if you *must* use python and you can settle for scripting clicks to pre-defined coordinates try the [win32gui](http://python.net/crew/skippy/win32/Downloads.html) module or the [winGuiAuto](http://www.brunningonline.net/simon/blog/archives/winGuiAuto.py.html)(beware bitrot - "2003"?) module which uses it. if you're *nix or *bsd you might have some luck firing your target app up in a window-based x server such as [Xephyr](http://www.freedesktop.org/wiki/Software/Xephyr). perhaps that would allow you to move your target app into your visible screen real estate.
Yes. It's widely used at Google and has an enormous number of third party packages across many fields. There are python bindings for basically every standalone tool that supports an external API.
10 minutes?
That's crazy.. this was 10 seconds.
Thanks :-/
Try playing with [Clojure](http://tryclj.licenser.net/) or some other lisp that doesn't even use commas to separate function arguments. You'll learn to hate those, too.
I knew a guy who used awk for AI/machine learning programming, because it was the only tool he had. It was slow, but it worked, and he could hack the crap out of it.
&gt; Python‘s syntax is very well thought out. I really hate having to pointlessly type "self" in every method declaration :-\
Oh man the awk programs I've seen. C &amp; awk compilers, complex data processors, &amp;c. &amp;c. Sheer Craziness. 
Probably only Perl has been as [mis|ab]used in so many different ways!
&gt; That guy on stackoverflow did have the best code though. Why was that lambda bullshit getting the most votes? Erm... that guy also used "lambda bullshit", except he sorted twice rather than once to avoid the tuple "trick". &gt; If it's convoluted enough to require extra explanation, it isn't very pythonic. It's not convoluted at all actually, it's in fact extremely simple and elegant provided you know the ordering of booleans and tuples. edit: on the other hand, your suspicion that execution speed is lowered is entirely true, the top-voted solution is 74% slower than the fastest solutions of the second response (the initial (most basic) one and the one using a pair of list comprehensions) on a short list (100 words) and 140% slower on a long list, according to timeit.Timer.repeat(10). Only on a very short list (~5 elements) does it become competitive with the others (the most basic solution still stands on top, the listcomps are ~4% slower, the ternary and lambdas are about 5% slower and the lambda + double sort is 17% slower)
idiomatic is by far a better world and portable to other languages too.
&gt; Check the stackoverflow comments, the "stop saying pythonic" guy goes on to endorse the lambda version as being cleaner and faster. Even though as far as my tests go it's not actually faster. And things worsen as the list to group/sort grows.
Now I'm curious, but also lazy. Care to share your test code?
This idea is kind of frightening.
&gt; Actually, I've found R's packages to be more credible and legitimate, for the most part, the packages are part of peer-reviewed journal article or a book that an academic has published. Actually, this doesn't mean much per se (and I've been using R for years). Some packages are buggy (like everything out in the world) but you can't properly report bugs because some projects (Bioconductor, I'm looking at you!) don't even have a bug tracker.
i do all those things? maybe your version is too old? but theres a bug with the log-contour map, but it is fixed in the trunk.
With interpreters, I think it is a little fuzzy, but there's definitely a possibility that your work would be infected with the GPL as well. The LGPL makes it clear that you have nothing to worry about.
if this really is for use with a projector, then all the main OS' allow you to setup screen-spanning where you could just drag your window half on one monitor and half on the projector. If you still need to do this programatically, then I would find a way to trigger the actions you need via keystrokes. Maybe the window you are trying to communicate has shortcuts or file-menu items that could be scripted. Any solution along this route is going to need more info about the OS/problem, but I can't imagine cloning a bit of the screen buffer and duplicating events is going to be a good path.
Just the thing for marking up my notes and homework! DIA is such a pain for doing these things, Thanks!
This will probably help: http://stackoverflow.com/questions/1181464/controlling-mouse-with-python Here is some code (from the site above) that will help with mouse clicking if you're using Windows: import ctypes # see http://msdn.microsoft.com/en-us/library/ms646260(VS.85).aspx for details ctypes.windll.user32.SetCursorPos(100, 20) ctypes.windll.user32.mouse_event(2, 0, 0, 0,0) # left down ctypes.windll.user32.mouse_event(3, 0, 0, 0,0) # left up
I've played around with Sage, but all the symbolic math stuff is more than I need most of the time. Yeah, after poking around the stats package it appears to leave a fair amount to be desired. Oh well. Though I do feel kinda motivated to go add some stuff to it now...as you say, there's a decent foundation of distributions, etc. 
My biggest problem with "Pythonic" is that it is thrown about as a term to deride any library (or interface to a native library/framework) which one doesn't like. For example I do not know of a single GUI framework that has not been described by more than a couple people as "non-Pythonic". The reasons given vary, or are frequently not given at all. I've used several of them and found many to be pretty reasonable to implement GUIs in Python. If I had my choice to make a GUI interface for anything, I'd probably use something like Python+wxPython with ctypes to interface to the heavy lifting code. I think it makes programmatic GUI construction easy and readable.
on "Module system" what is so special about his example ? 
what's "R" ?
I just took a look at it as well and found the exact same thing. In this case, the cleanest, clearest solution is by far the fastest. prefix_words = [] no_prefix_words = [] for word in words: if word.startswith('x'): prefix_words.append(word) else: no_prefix_words.append(word) sorted(prefix_words) + sorted(no_prefix_words) That runs in 0.13 seconds for 100,000 words on an i5 ThinkPad running Ubuntu. Switching to sorting in-place makes no real difference. Stack Overflow's lambda version took 0.33 seconds. None of the other methods I tried were better than the clean version. From what I can tell, the lambda version takes two hits: Python function calls and using tuples. Python function calls are relatively expensive, so requiring n function calls doesn't help performance. From what I can tell, the extra comparison calls for comparing tuples slows things down quite a bit. The difference between these is in the microbenchmark range, so just use the most readable solution.
anyone have a demo of this working?
Yes, but then I'll also learn to hate parentheses. 
http://www.cs.wustl.edu/~loui/sigplan
Nice! I like visualizing FA as well. Here's an example of what I do when I generate FAs with DOT: digraph { x0 [label=&lt;0&gt; shape=circle] xSTART [color=white label=""] xSTART -&gt; x0 x0 -&gt; x2 [label=&lt;&lt;FONT face="Courier"&gt; a &lt;/FONT&gt;&gt;] x0 -&gt; x1 [label=&lt;&lt;FONT face="Courier"&gt; b &lt;/FONT&gt;&gt;] x1 [label=&lt;1&gt; shape=doublecircle] x2 [label=&lt;2&gt; shape=doublecircle] } What's useful is the xSTART as it clearly indicates the start state by what appears to be an arrow without a start going to x0. Here is the picture of the above DFA: http://graph.gafol.net/cHIXDzMAU. Also, I like putting spaces around the transition labels as it tends to look a bit nicer. 
&gt;My biggest problem with "Pythonic" is that it is thrown about as a term to deride any library (or interface to a native library/framework) which one doesn't like. Your comment is not very Pythonic.
I implemented the "MyHill-Nerode DFA Minimization Algorithm" in python as extra credit for my CS-Theory Course a while ago. https://github.com/bgianfo/cs-theory/blob/master/dfa-minimizer/dfa-minify.py Reading your code reminded me of using UTF-8 symbols in my code. It's a nice feature :) 
Give Pyper a look. I haven't gotten a chance to play with it, but it's supposedly faster.
If you grok python your pythonic code will allow you to have a good coding experience.
My thoughts, with code examples -- http://www.pixelmonkey.org/2010/11/03/pythonic-means-idiomatic-and-tasteful. "Pythonic isn’t just idiomatic Python — it’s tasteful Python..."
I decided this was not a bad idea and [cooked up a demo](http://jmoiron.net/misc/argot-demo/). It's doing a few things that might screw it up; it's using `lxml.html.clean` to clean the output (to kill scripts and other nasties), and there's almost no error handling. (disclaimer: I'm the author of argot) 
I like the first comment. To paraphrase, "I just ask myself What Would Guido Do?"
Ergot - a small set of fungus on the barley bread language designed primarily for cutting off peripheral circulation.
You may also want to [look into this](http://web2py.com/markmin).
Ineffable. Good word. Ta!
It's nothing impressive, just a bunch of Timer calls: #!/usr/bin/env python import os import timeit LIST_LENGTH = 5 REPEAT_NUMBER = 100000 strings = [os.urandom(20) for i in range(LIST_LENGTH)] def lambda_tuple(words): 'lambda + tuple' return sorted(words, key=lambda x: (x[0] != 'x', x)) def basic(words): 'basic' list1 = [] list2 = [] for word in words: if word[0] == 'x': list1.append(word) else: list2.append(word) return sorted(list1) + sorted(list2) def ternary(words): 'ternary' list1 = [] list2 = [] for word in words: target = list1 if word[0] == 'x' else list2 target.append(word) return sorted(list1) + sorted(list2) def listcomps(words): 'listcomps' list1 = [word for word in words if word[0] == 'x'] list2 = [word for word in words if word[0] != 'x'] return sorted(list1) + sorted(list2) def lambda_twosorts(words): 'lambda twosorts' return sorted(sorted(words), key=lambda word: word[0] != 'x') timer = timeit.Timer('sort(strings)', 'from __main__ import sort, strings') for sort in (basic, ternary, listcomps, lambda_twosorts, lambda_tuple): print sort.__doc__ print min(timer.repeat(10, REPEAT_NUMBER)) Fiddle around with the two constants at the top to test varying list length without the execution being too fast or too slow (generally remove an order of magnitude to REPEAT_NUMBER for each order of magnitude you add to LIST_LENGTH)
The winning example is a useful, extensible design pattern that should become more widely accepted. If you have to sort first on X then on Y for items of equal X then on Z for items of equal Y ... Then create a sort key function that returns the tuple (X(item), Y(item), Z(item), ...) The second voted answer by the guy who doesn't like the word 'pythonic' would be more clumsy when extended. Just as we all learnt DSU (Decorate, sort, un-decorate), at one stage - the pattern above can be named and learnt in a similar way. 
Might not be what you are looking for but [matplotlib](http://matplotlib.sourceforge.net/) is a scientific computing visualisation library that tries to emulate a lot of functionality from Matlab.
Try building PyCairo through ports on osx? Matplotlib/pylab is quite powerful - I used it to generate this: http://gromgull.net/2009/09/heat/heat.html (although with some graphviz layout in the mix) Another option is to output SVG + javascript for interaction, shows directly in modern browsers: http://gromgull.net/2010/01/swball/swball.svg But this is of course not very pythonic.
Matplotlib. /thread
Also: [Chaco](http://code.enthought.com/projects/chaco/) if you want something more programmatic/interactive. You also might want to check out what's listed [here](http://wiki.python.org/moin/NumericAndScientific/Plotting) as well.
I'd really try building pycairo from source. I see some others have recommended matplotlib, but I think a general vector rasterization library is much more appropriate for your usecase, and cairo is the finest of the bunch. If you don't have any luck with cairo, you could try AGG instead (check out 'aggdraw').
Matplotlib is awesome as others say, but if you are doing connected graph stuff you should check out graphviz.
Check out NetworkX. I used it for a project in the past for graph visualization and it ended up looking pretty nice and was simple to use.
If you want to do it all by yourself but have loads of flex, try pygame :)
Would not [PyGame](http://www.pygame.org/news.html) be good for this? (This coming from someone who has never used PyGame and barely knows MatPlotLib.)
Nice price tag. Just a sample of the fine writing you get for your money (page 93 of freely available v1.0) &gt; Tips For Debugging &gt; 1. Do not use a “debugger”. A debugger is like doing a full-body scan on a sick person. You do not get any specific useful information, and you find a whole lot of information that doesn’t help and is just confusing. &gt; 2. The best way to debug a program is to use print to print out the values of variables at points in the program to see where they go wrong. &gt; 3. Make sure parts of your programs work as you work on them. Do not write massive files of code before you try to run them. Code a little, run a little, fix a little. So yeah. Two bullshit points and one mildly correct. Worth every penny. 
spunky looking viz on Mac Os using Python ... spells NodeBox to me : http://nodebox.net/code/index.php/Home
Holy god damn shits, man! That is fucking impressive! :o
If you want to do 3D visualization, [Mayavi](http://code.enthought.com/projects/mayavi/) is an excellent tool for it, built around VTK. 
Depending on what you want to do, VTK could also be a very good choice for you.
There are plenty of tools and code around Graphviz's DOT format. You're pretty much safe working with it. http://www.graphviz.org/Resources.php Tools to generate, import, export and even interact and edit. Also, not just tools but source and libraries so you can roll your own.
you should really check out [ubigraph](http://ubietylab.net/ubigraph/). I've used it on a couple of projects and it's awesome. 
That is beautiful :o
Wow, dunno why you were down voted. This is atrocious. He wasn't wrong when he said learning it the HARD way!
Hating on Zed is a crime.
Most people do not know but [visit](https://wci.llnl.gov/codes/visit/) has a python API. More than that it has code generation capabilities. You put it in record mode, do something with the GUI, and it generate the Python code to do with code what you did via the GUI.
I've been an avid user of Nodebox for a while and it's awesome. I recently came across [NodeboxGL](http://www.cityinabottle.org/nodebox/) . I haven't used it myself, yet. And I am not sure how actively it's being developed. But it's worth looking into, especially if you want to build something in 3D or just need more speed I guess.
The first bullet, "Freedom," is the justification I use for using Python for my research work. I'm surrounded by MATLAB fans, but I always explain that anything we write in MATLAB and/or Simulink is useless to everyone who doesn't have access to the US$3K packages in the first place. Our research software is freely distributed, but I try to explain that it is completely inaccessible to students or hobbyists. I usually hear back, "Well, the MATLAB student edition is only US$100," but as a student I know I didn't have US$100 just lying around. Python sometimes isn't the easiest language to use for research (although a lot of the time, it certainly is), but the fact that it is free, in my opinion, makes it superior in many cases over MATLAB or other proprietary packages.
After reading your comment, I was about to suggest it was well worth of a blog post in it self. It's excellent. Thanks for sharing it.
He isnt the first known programmer to denounce using a debugger.
For a strong implementation of Markdown that includes some well-thought-out and readable minor extensions to the Markdown syntax, [Pandoc](http://johnmacfarlane.net/pandoc/) is very good. 
"Pythonic" has become a rather irksome term to me because I most often hear it said/written by folks who also happen to be in the middle of telling someone that the way they are doing something is *wrong* or *incorrect* regardless of whether their code is providing the right answer in the end. Having "only one right way to do it" is a blessing and a curse. It's a curse because it brings out the worst in people's bossy side. Python users who have a chip on their shoulder against Perl would be better served by just letting it go. Yes, Python tries to offer only one obvious way to do it, it's a good system and works for Python, we all get it, but if I decide to do something a little differently I don't need to keep hearing that it's wrong and "un-pythonic". Maybe instead just amicably show me a way that you think is more idiomatic and tasteful (nod to pixelmonkey -- those are excellent terms to describe it). 
Except this is not "it's like, my opinion, dude", it's a programming intro, so he probably should remember that not everybody is Zed Shaw and maybe that `pdb` beats `print` any time. Especially since you can do `import pdb; pdb.pm()` and actually look at what has happened. While with `print`-approach you need to put some `print`s in and rerun the program, hoping that you are looking at the right stuff and you will land in the same branch again. I mean it is some form of `logging`, but setting it to be used instead of a debugger is moronic. EUR11 for the ebook.
I've been trying to learn python for close to a month now. I've tried a few of the tutorials. I have no previous programming experience whatsoever. Would this be good for someone who understands the concepts on a beginner level, or do you need to have experience in previous programming languages? A lot of things I've tried to work my way through ends up getting to a certain point where I just get lost. I don't have any real life peers who program, and no one to really ask or talk my way through the initial roadblocks that experienced programmers consider menial and elementary. I will give this a try this evening after work.
&gt;it's a programming intro Yea, so why is using `print` bad then? Its an intro, not a comprehensive guide. It is obvious, especially after working through the book, that `print` would be the perfect tool for debugging. It is easy to explain and easy to understand what is going on. How much overhead would he had to have included were he to discuss debuggers and how to actually use pdb and gain something out of it. &gt; EUR11 for the ebook Free on his site ------------- Worrying about the "right way" to do something is the number one cause of analysis paralysis and typical symptoms include scouring reddit/hn instead of hacking on whatever your idea is. a beginner should not have to worry about "the right way", yet.
I've been looking for a good python visualization library that supports saving the image to a file and never displaying it on screen (the code runs on a remote system), and the most promising thing I've found yet is [reportlab](http://www.reportlab.com/software/opensource/).
Check out VPython if you want to do 3d stuff.
Ubigraph is awesome. I wish they'd open source it or release updates though :/
The [QtSvg](http://doc.qt.nokia.com/4.1/qtsvg.html) module with [PyQt4](http://www.riverbankcomputing.co.uk/software/pyqt/download) works great for displaying vector graphics. [Edit: You may also want to look at [Sage](http://www.sagemath.org/tour-graphics.html).]
The fact that he took time to make something of such value that does more good than harm is greatly commendable. I'd gladly give money for that. But then, I already do a fair amount encouraging the use of python.
My experiences with mayavi2 were... mixed. For a quick project, if you're lucky enough that what you want to do matches up with a use case they've already thought of and wrapped up in their nice, neat mlab API, you're sorted. But (it seemed to me) as soon as you want to edge outside of that, it feels like a rabbit-hole where the further you go down, the deeper it gets, as you realise that that neatness is as a result of abstraction layer upon abstraction layer upon abstraction layer on top of a ridiculously powerful and complex C++ visualisation library, with it's own set of concepts that would take far more time than you have to fully understand and get to grips with.
http://vis.stanford.edu/protovis/ is a neat tool for doing network analysis. http://gephi.org/ is also interesting, but i never had that much of a chance to work with it. networkx is definitely good for manipulating graphs, matplotlib to actually get them to display.
I believe Torvalds has a famous rant on the subject as well. Regardless, I think debuggers and repl in general are fantastic devices for learning how to program. There isn't a faster way to poke around in a running process just to see what all is going on and try out "what-if" scenarios right while the program is running.
I, like many others I would assume, was first exposed to using a REPL with Python. It is one of those things that once you use it, you cant really ever go back to not using it. Speaking of which... Is anyone aware of an extension to firebug or the Rhino shell that would make it more rich for developing (help() from python for example) ??
I've basically been doing it this way on my development box except I install python as root and do make altinstall in /usr/local/bin and then install easy_install, pip, and virtualenv. That way any user can create a virtualenv without having to compile python first. So far it has been the easiest way to setup python projects. Also, I never activate my virtualenv, I find it much easier to use an alias or bootstrap using the full path.
Heya, I imagine it would be very easy for a non programmer to pick up, as the guy who hosts the lecture makes watching it very interesting. He also gives you 'homework' at the end of the lesson, and the homework consists of everything you he taught you in the lesson. I found these to be really helpful. Obviously, someone like me who already has programming knowledge, found python really easy to pick up. From all the programming languages I know, python is by far the easiest to learn ( and in fact its now my new favourite language ) The prerequisite is that you need to know some kind of language or have an understanding of programming, but if you have been reading through various tutorials, then Im sure you will have enough knowledge already. I think it would be best to just go and watch through them, and follow along - and pause the video to test things out - Its worth the couple of hours investment. Once you start to understand the logic and terminology, it will just 'click' and you will find learning it a lot more enjoyable. Let me know how you get along!
[This guy's](http://www.thenewboston.com/?cat=40&amp;pOpen=tutorial) stuff is basic, but pretty good if you are a first timer. 
Good link. NodeBox is Mac only, while NodeBox for OpenGL is cross-platform. (I'm running it on Ubuntu.) I don't think there are any performance gains, since I believe NodeBox uses an OS X drawing library that in turn use OpenGL.
Ouch! I'm also working full time as a developer and after seeing our Architect drawing out states in Visio I quickly introduced him to graphviz and showed him how he can write SQL that outputs DOT graph descriptions.
There is also markdown2: http://code.google.com/p/python-markdown2/wiki/Extras 
Thanks for the example! I could not get your link to work so I generated a graph from you DOT description: http://i.imgur.com/I1obM.jpg Yes yours does look more pleasing visually! Im also playing around with the idea to render IDEF0 diagrams using Graphviz so I will need your xSTART trickery to do branching of edges.
Project Euler has some fun problems that can be readily solved by programming. That won't directly help you with what you're trying to do, though. What kinds of task are you trying to automate? People around here could probably point you to appropriate libraries.
I don't know if it is good or scary but if your editor support it you can actually use δ as a variable/attribute name: &gt; In [1]: δ = 'lowercase delta' &gt; In [2]: print δ &gt; ------&gt; print(d) &gt; lowercase delta But strange how IPython output the delta as a d?
Yeah, I am not sure if NodeboxGL is faster than plain Nodebox in practice, though the website does claim (in the section titled 'Purpose') that it is "quite a bit faster than the classic NodeBox".
What kind of tasks are you trying to automate? Please be a little more specific.
One example would be checking communications with devices over a network, basically call up a table, check a field, if field is true, ping a device's IP and if there's a response update the table else return a separate table or list of those that failed to respond. 
Ah, cool. So there is some incentive to use it on OS X, as well.
Pandoc is great, and I wish these Markdown syntax extensions which (mainly) add code highlighting would standardise on their syntax. Or at least, standardise on _one_ code highlighting syntax. Sigh.
Yeah, easy over right, gotcha. FTR: That's what I thought when I started Python years back. It turns out that you need to know about four `pdb` commands to top `print`out method (invocation, `l`, `p` and \^C). I felt stupid when I finally got to it and learned the enough basics in something like 5 minutes. So he could have that instead of the bullshit list. But yeah, just assume that I'm some kind of a purity wanker, not a practitioner with actual learning/teaching experience.
Argot's code block syntax comes directly from trac/moin. I'd love to have a standard, but unfortunately core markdown has poor code block syntax (as far as web form input is concerned), and there are so many other markup systems out there (reST is widely used for python docs, etc) people will want to bring their favorite syntax. I suppose if a large site that uses markdown with python ([ahem](http://reddit.com)) adopts one, it will become the de-facto markdown variant in python, which would be good for everyone!
Yes, this is true.
[Here](http://www.r-project.org/) is their web page. If you'd like to give it a stab, there are quite a few decent intros online.
Hi, You can also try shoebot (disclaimer - I'm one of the developers). It's a cross platform implementation of shoebot that renders with cairo (I realise you couldn't seem to get it working on osx, but it is possibly). Cairo means we can also output to svg and bitmaps easily. Quite a few of the nodebox libraries are working with shoebot now, clipping is probably the biggest thing not implemented in the current versions. Heres my experimental branch http://bytebucket.org/stuaxo/shoebot-dq/ With nodebox-gl you have to look out as the syntax is slightly different, so a lot of the nodebox1 libraries won't work. Nodebox2 beta looks interesting, it's based on jython and has a different api again though (so a lot of the old libraries won't work).
http://docs.python.org/tutorial/ is an excellent short guide for learning python, so I'd start with that. it sounds like you want to do things like ssh'ing to remote hosts - check out paramiko or twisted.conch for that.
It's one of the aims for shoebot, I'm not sure how well that works yet (or even what the api to do that should look like)
Your question is a little vague. Certainly Python would be a reasonable language to code what you generally describe. You might get more useful suggestions is you gave more details. That said, if you want to use the extern ping executable, look at the "subprocess" module. There may also be a Python module that gives you access to the same. If you want to do it in low-level Python code look at "socket". Assuming your reference to "call up a table" means a database connection there are many interfaces to many databases and modern Python installs include native sqlite database support. python.org has all the documentation you should need. Once you learn the basics and if your application needs it you can go on to more sophisticated programming with things like [Twisted Python](http://twistedmatrix.com/).
Neat, if you don't type float literals much. :) (Or include the 0)
Cool stuff. It's always nice to see python showing up in more places.
I consider ".5" float literals harmful.
Why? The '0' is as redundant as typing 0x0000001. (Just playing devil's advocate here - this is a really great little script!) 
haha, purity wanker
I am learning Python as well and found myself watching tutorials better than just reading. The latest videos that I found starting from scratch are.... http://www.newthinktank.com/ its updated every day with a new content and I think its very good. There are other excellent tutorials on that page. The second is http://www.youtube.com/user/ryanmshea Hope that helps. 
agreed. i offered to help them on their camera implementation, its insanely difficult to use on any sort of larger dataset. hell, i'd settle for complete documentation at this point.
Additionally, isn't ".5" closer to the pronunciation than "0.5"? That's at least how I say it: "point five", not "zero point five".
Python for non-programmer in Chinese: (http://code.google.com/p/hashao/wiki/ChinesePythonTutor) 
I would guess that it's because "." is smaller and easier to miss than "0.".
Gandi has proven to be quite trustworthy in the past.
They have an official API for this sort of stuff, but it's broken. Hopefully they will fix it so I can use that instead.
np :)
I'd be really interested in this for web automation. It could take the place of Mechanize+BeautifulSoup quite handily and also possibly allow access to the javascript as its running; which would be *awesome*.
This is a common dilemma when using a full-fledged framework like Qt with Python - as there's quite a bit of duplication between what Qt/Pyside give and what the stdlib gives [the more common choice is between thread classes]. Personally, I've always been following the pure-python path, for the following reasons: 1. If I ever decide to change the GUI framework or run the program not from a GUI (say, command-line or on a web-server), I won't have porting to do (or carry Pyside around), since the stdlib is always with Python. 2. This somewhat encourages nice decoupling of the GUI from non-GUI elements of your program. 
"Readability counts" (Zen of Python)
Well I know at this point that I'm going to either rewrite nntplib or modify it at run time. It blocks on LIST commands, which is a no go, even if it is running in a separate thread. Many servers have several thousand groups. On a moderate connection, it can take several minutes to get all of them. The user needs feedback. So I need for at least that function to handle a callback or signal.
I'm confused, wouldn't zipping the code and unzipping at the destination be more efficient than keeping workable code on both ends of the QR code?
Agreed. I've never understood this obsession with minification in the Javascript/web world. I always set up any performance sensitive web servers to use mod_deflate and send any Javascript compressed. Minification is almost completely pointless if you're also using compression, and it makes your site significantly more difficult for others to play around with, unlike compression which works transparently. Using minification, you lose the ability to just open up somebody elses code and figure out what makes it tick, which I consider to be the biggest advantage of languages without a compilation phase, like Python, Javascript and Ruby. Glad to see this disease won't be spreading to my desktop any time soon.
&gt; Using minification, you lose the ability to just open up somebody elses code and figure out what makes it tick I think that is the point. Some kids just don't like to share. 
I know some people use minification as an excuse for obfuscation. But lots of open source JS libs are minified too.
minification is used to save bandwith and to speed up page load time JQuery uncompressed is 150KB, Minified: 70KB, a saving of 80KB if a webstite gets 10,000 unique visits a day minifiying just jquery would save almost 800MB of data transfer. 
You're missing the point I was trying to make. I get that minification can reduce bandwidth, I even said so in my post. But as I said in my post, compression is more important. Let's look at the numbers. * The latest unminified jQuery is 177 kB. * The latest minified jQuery is 76 kB, a 57 % size reduction. * The latest unminified jQuery, compressed is 43 kB, a 75 % size reduction. * The latest minified jQuery, compressed is 25 kB, a 85 % size reduction. Look at the second and third item in the list. Compressing is a _much_ bigger saving than minifying - the size reduction is almost twice as good with compression than with minification. Meanwhile, the difference between a minified and an unminified jQuery is just 18 kB. Also note that the difference that minification makes is much smaller if you're also using compression, because most of the redundancy that minification removes is also picked up by the compression. If you need to squeeze out every tiny little bit of performance, you might sometimes want to use minification _in addition_ to compression to squeeze out another 10 % in bandwidth, but frankly, I feel it's rarely worth the effort - those last few kB per visitor isn't what's killing your bandwidth. It's not that minification is useless, it's that compression is about twice as useful _and_ lacks the drawbacks of minification but minification seems to be way more common than compression, and I really don't get why. It seems to me like minification is the -funroll-loops of Javascript.
Try [UltiSnips](http://www.vim.org/scripts/script.php?script_id=2715). Expanding . to self. is just a tiny example of this module. For this: if __name__ == '__main__': import nose nose.runmodule(argv=[__file__, '-vvs', '--with-doctest']) I just type *ifmain[tab]nose[tab]*.
You could probably just subclass nntplib.NNTP and override getline() or getresp() to get your feedback hook.
Maybe it's because of the CPU overhead associated with zipping/unzipping? If I understand this correctly minified code can be run/interpreted directly without a de-minification phase, but the zipping/unzipping phase would require additional computation.
I think your looking at it wrong. **JQuery** gziped unminified 43KB gziped minified 25KB, 40 % size reduction also most websites I work with don't just include JQuery they have their own libs and other libraries like JQuery-UI **JQuery UI** gziped unminified 84 KB gziped minified 50 KB, 33 % size reduction even if you only save 20KB on each file it is still a massive reduction in bandwidth. 
True. Good point.
That's cool. A few years ago I actually got a verisign ftp account and pulled their entire domain file and just grepped it ;)
No, calling a function in Python is slow. You want to avoid calling more functions than necessary. `reduce` is going to call `max` over and over again, whereas `max(len(line) for line in open("filepath"))` only calls `max` once.
Web automation ? Do you mean on client-side ? I was describing server-side apps in the blog post.
CPU is faster than IO.
you know the httplib module? You'd probably want to start there. Or better, hunt around for someone who's coded something similar and revise their work. I'd be interested to know where you get to...I can maybe help more EDIT: I think Beautiful Soup is an html parser that might come in handy
This might be a good place for me to [start](http://scrapy.org/), I think
I would suggest Python mechanize http://wwwsearch.sourceforge.net/mechanize/ Its a lot easier to program if youre new to http 
You say Mozilla, I think Firefox. -- I know the uses of Python on the server side of things but what does Mozilla do (or have software for) regarding the server side of things?
I've had good results with pyquery - butI've already done a fair bit of javascript so I already knew the jquery api (which it's a port of).
If you can do it on a computer; you can do it in Python. So your answer is "yes".
There are ways of acquiring a list of all the registered .com names? HOW!?
http://www.verisign.com/domain-name-services/domain-information-center/tld-zone-access/index.html
You can also watch Ryan Shea's videos at http://python.sourcequench.org , and you can find a torrent file for all of them if you look in the history of /r/Python.
IMO, Pandoc's block syntax is prettier than moin's: Pandoc: ~~~~{.python} def hello(): print "Hello World!" ~~~~ Moin: {{{#!highlight python def hello(): print "Hello World!" }}} Regardless, Pandoc is solid, has readers and writers for multiple formats, and is the best tool I've found (after lots of experimentation over the years). I'm not crazy about one or two of the pandoc-markdown syntax choices (ex. definition lists), but they're pretty standard across other markdowns and I can live with them because overall result is well worth it. If the author of Argot were looking for advice (*edit: whoops, that's you. :)*), mine would be to have Argot implement the same markdown extensions as Pandoc. There's basically only 3 *major* plain-text markup formats out there: Moin (which isn't great and which you're stuck with for many wikis), ReST (which is not the prettiest, nor very widely-used outside of Python circles, though many Python folks do use it), and Pandoc-Markdown (which is arguably the best looking and easiest to read &amp; write). So rather than make a frankenstein (no insult intended) out of Markdown + Moin (and then have to convince users to use your new markup instead of one of the big three), you might consider instead simply throwing in some more weight behind Pandoc-Markdown. 
Yeah, they are very nice. I'm at lesson 15, and very satisfied.
Not only for hobbyists. Sometimes smaller research institutions or those on a budget can't affort software licenses like MATLAB. 
Second this. Go with mechanize and lxml http://codespeak.net/lxml/
I've been doing something similar with yahoo finance; it's completely doable (although yahoo finance has a separate API for doing this stuff). Look into whether the site has any sort API to return the specific data you are looking for. Might make it easier.
Httplib seems a bit excessive. I think urllib2 provides all the needed functionality with a somewhat simpler interface.
:o THANK YOU!
&gt; EDIT: I think Beautiful Soup is an html parser that might come in handy urllib2 and BeautifulSoup should have everything he/she needs and should work easily.
Upvote for beautifulsoup I always make mine a family guy reference From BeautifulSoup import BeautifulSoup as chowda #who wants chowda?
I'm going to recommend [snipmate](https://github.com/msanders/snipmate.vim), which has this functionality and a whole lot more.
You should definitely give it a try, and if you get stuck, you can come to #scrapy on irc.freenode.org.
If you're looking for a parser written entirely in Python, yes. Otherwise, lxml is the recommended HTML parser. 
"pythonic" just means "I like it".
This question on StackOverflow is similar, and the accepted answer has code samples with links to the places in the Python documentation that you need: http://stackoverflow.com/q/86206/1694
Yeah, BeautifulSoup is intended to work with markup even if not properly formed, etc. Good stuff, I've used it for page scraping many times.
I'm not sure if you can do it in straight Python. It looks like the table is generated by JavaScript, which would require you to use a browser. I don't see where the source for the data in that table is... Some of it is in the JavaScript, and the rest may be calculated by the JS on that page, or it may come from elsewhere. I'd try to hunt down the source of that data. If it's easily reachable, then you should be able to grab it with a Python script. I'd check into their API, first. If that won't work, then, if you really need to go through the web site, then you may need to use a browser automation framework (like [Selenium](http://seleniumhq.org/)) and some Python to accomplish your goals. Can you get the data from somewhere else, like Yahoo Financials? 
I want to do video analysis. How would I look up how to load the video for access by my python program?
Sorry, don't mean to hijack the thread but what about visualization libraries for Windows? Any favorites? 
Have not used lxml yet but i've heard of it. I will have to see if I can do the same thing with it that i've done with BS for parsing MSN Chat logs when you don't have the MessageLog.xsl(t) file.
http://pymedia.org/ + numpy
Yeah, I did the same thing not too long ago, only for Java. I generated about 30,000 lines of very repetitive Java using Python + Cheetah templates. One could fairly argue that I should have found a more elegant solution to the problem instead, but I didn't have time for anything but the brute force approach. Automating it at least ensured I didn't have to kill myself to get it done.
If you can share your script! Some people on this reddit likes to read other peoples code and they might even give you pointers etc. Also the static content was it other HTML files, images, word documents? and then why 90 .Net pages if it sound like you actually just needed static html files? But yea good for you for learning something new!
Matplotlib is cool, it has tons of plotting capabilities "out of the box" and even more if you are willing to study the insides of the system. One con is that it has rather poor performance. I've been using it in GUI applications with PyGTK and sometimes figure drawing is just painful. Perhaps there are some performance tricks I don't know of...
I'll dig up the code and post it! The code came as static HTML files. We sent the content in MS Word format to a marketing firm and they crafted HTML files with images and CSS, with page titles, sub navigation, SEO-friendly file names, and then a content area in between HTML comments. The script broke out those chunks and created .NET pages (with codebehind files, so a total of 180 new files actually) using simple templates and {field} areas. It was such a hack job, but it did precisely what I needed it to, and once the script was written, it only took about 2 seconds to run. It was very exciting to see the resultant files! Everything went better than expected. EDIT: I should also mention, there were 3 or 4 runs of global search/replace for things like changing .html to .aspx, and some links. But, the script worked as designed. EDIT 2: We are using .NET master pages in our project, so I had to create them as .NET to utilize the functionality from that. Tracking code, authentication, etc.
What you said: "Yesterday, at the recommendation of a fellow employee, I wrote my first-ever Python script to take static content files from our marketing firm and turn them into 90 new .NET web pages, a total of 16,000 lines of new HTML code. Everyone should learn Python." What I heard: "Blah blah blah blah... Everyone should learn Python." Needless to say I totally agree.
Hah! Well if that's what you got out of it, that's fine with me. It's really the entire premise of what I wrote. :)
Yes, agree! ;)
I've also found Python to be great. It's so flexible and easy to use. Even GUI design is easy with it. As long as you're not looking for a speedy language, I totally recommend it too :)
OP, where did you learn python and what version did you use, just out of curiosity if you don't mind?
Here is the code. Keep in mind this was hacked together including all of my testing prints, etc. I could have used regex, but I'm not that strong with it on a whim, and the goal was to get this working as FAST as possible. The code loops through a directory of provided files, takes the chunks of code from the .html files, and creates a result .aspx file, and a paired .aspx.cs file of the same name in the same directory structure. Also, I had to change a few path names and such, just for privacy sake. :) http://pastebin.com/S2DvgHmE
Ignore all the advice below that suggest parsing the HTML. They have not even looked at the page. All the data is generated by JS, that means you can download the page (python urllib), use regular expressions to extract the JS variable that contains the table with the option data (python re), convert it to a python list (python simplejson). Once the data is in a python list, what you ask is trivial.
Honestly, it's very easy to learn it seems, but a lot of what I generated was gleaned from reading sample scripts and searching for blocks of code that did similar to what I needed. It was a one-time script, so I threw it together to make it work. For example, I needed code to recursively loop through a directory. So I just googled for "python recursive directory", found some code and adapted for what I needed. I used Python 2.6 for this particular script.
Makes me want to automate EVERYTHING now.
I will post suggestions as I see them but here is the first one: endswith() can take a tuple of extensions so instead of your set of "or" statements you could just do: if filename.endswith(("html","htm","aspx")) note also that you don't need the . before each extension 
That is not terrible code, either. Congrats! The only things I'd suggest: Use **os.path.join(...)** Follow PEP8 for variable names. os.walk(os.curdir) No parens around single expressions in **if**.
second, the follow code is pretty verbose: fin = open(fullfilename, 'r') file = fin.read() fin.close() you can just do file = open(fullfilename,"r").read() and the file descriptor will be closed for you by python when the next loop iteration comes around and you re-assign file
when you slice with -4 doesn't that kill the ending '.' in htm file names ? did you have any htm files in you test bed?
finally, unless there is something subtle I am missing: return html[titleStartIndex:titleEndIndex].lstrip().rstrip() calling .strip() is equalivent to your .lstrip().rstrip() see: http://docs.python.org/library/stdtypes.html#str.strip
Very nice, thank you! I do love the shortcuts that Python affords. Still much to learn!
Awesome, thank you! :)
Oooh, good to know. Much appreciated.
Nope, all .html. I checked all the files with a search first In a less controlled environment I would be more careful, but I knew the parameters and it was a rush job. :)
Hah, good to know. :) I did a google search for "trim string python" and found lstrip() and rstrip(), so I just combined them.
I don't see what's so special about that code such that the same couldn't be done in Perl or Ruby in approximately the same number of lines. Where in this is the "everyone should learn Python" argument?
Because this is the Python subreddit and I just happened to choose Python to do this. :) I actually have done some Ruby in the past, and despite liking the syntax more, I have more use for Python since it's already used in my work environment. I should have rephrased to say that everyone should used an interpreted language for automating tasks.
Indeed, that's what I meant actually. Too high to think straight
you can also use the same shortcuts when you write out the files at the end when you use writelines
Nice. :) I like the shorthand versions of things. Python is definitely something I plan to continue honing my skills with. A pleasure to work with.
Now only if you could convince them to change their system to just use python in the first place.. srsly tho gj op
That's not safe to assume in all interpreters. Better is to use the with keyword like this: with open(fullfilename) as fp: file = fp.read() When using open(), as well, 'r' is implied and unnecessary.
Pretty much everything in IT is finding ways to automate your life and still make money.
You're getting paid by the hour, dumbass.
Relatedly: http://nedbatchelder.com/code/cog/
PEP 8!
Python's pretty great for one off scripts...I've discovered the hard way it's not that great when scaling up
&gt; note also that you don't need the . before each extension You do if you don't want to match foo.barhtml
I was looking at the scrapy website- it looked really neat. 
In that case, put some slowdown loops in the code ;-)
Python *is* great at scaling up. Nearly our entire molecular modeling toolchain is written in Python, with parts accelerated in C++ where necessary. We process a large amount of data on a good sized compute cluster. The key is building a good software architecture and finding and optimizing out your bottlenecks.
I am?
Ahh, the elusive barhtml!
be careful ... python owns you (and me)
Great quote.
Unlikely, but possible. I like to be correct when it doesn't take too much effort. It would also match filenames without an extension, just ending with html.
I wrote my first python script yesterday too! I had to parse through a large (23 mil. rows, 2GB+) text file and within 1 hour I had a 5 line python script that read line by line (from disk) and parsed each line before writing back out to a new file. It took a few minutes to run, and afterwards I concluded I should have started learning Python years ago!
&gt;No parens around single expressions in if. Why?
I thought you made money by automating.
"0" in "0.5" is redundant in the same way that "self" is redundant in opinion of C# coders. Seriously, ".5" is harmful because it can be quite easily confused with "5" (it happened to me in the past). Also, I believe that "there should be one--and preferably only one--obvious way to do it". &gt;&gt;&gt; str(.5) '0.5' 
It depends on language, e.g. in Polish one always says "zero point five".
I use snippets extensively but it's not the case as this plugin is all about something else. It let's you type "self." with a *single* keystroke and what makes it so cool is that the key you have to press is the dot key. Of course you could use ".&lt;Tab&gt;" snippet or a simple mapping like "imap &lt;F12&gt; self." but it wouldn't be as convenient.
If you work so well that there's suddenly no more work for you to do, then you get fired. Careful!
It has not. See [previous comment](http://www.reddit.com/r/Python/comments/e1d98/selfdotvim_vim_plugin_that_lets_you_type_self_by/c14k3nk).
So that's why you do it the fast way, spend the next 2 months of work on reddit, then tell everyone you just finished doing it the manual way. 
I dunno. Lies like that cause me too much stress. I couldn't do it personally.
I took a compilers class a couple of years ago, and had to write a Pascal compiler using the language of our choice (I chose Python). For one part of the project we had to calculate a parse table, then convert that into code for a syntax analyzer. About ten minutes in I realized that converting the parse table to python code was incredibly mechanical. Instead of writing the rest of the syntax analyzer, wrote about 16 lines of Python that read in the parse table and output about 600 lines of valid python code. I'm not sure I really saved much time (getting the parse table into a form Python could make use of was time consuming) but it was a lot more interesting than doing it by hand.
They're unnecessary. As would be ((two)) or (((three))). It's noise you can avoid.
Do you post your code anywhere? I'd love to take a look at it if you do.
I don't really understand what you did, but python is fantastic. Everyone should learn it.
I find it helps readability coming from a c/java standpoint. Just something that makes the code stand out. 
MySQLdb. The latest version is maintained by a fellow redditor; I've tried the successor to MySQLdb - OurSQL, but ended up reverting back to MySQLdb
Cool thanks ill take a look at it.
Yes! I love the agility of *getting things done* with Python. Good stuff! Welcome!
This project is actually currently closed source, if you give me somewhere to drop it I can drop a small module that parses stock tickers and performs lookups.
Excited to see what this brings.
*"BARHTML!"* - Homerhtml
it is not safe to assume that the file will be closed when its reference count becomes 0? how is that possible?
When you're not using CPython, it is not safe to assume, hence *in all interpreters*. It's an implementation-specific detail.
&gt; makes the code stand out That's usually the opposite of what you want to do in Python.
Before you walk away with this advice, be sure to read: http://www.reddit.com/r/Python/comments/e1ts8/yesterday_at_the_recommendation_of_a_fellow/c14m2fq transt unwittingly gave you a (debatable) bad habit.
I really wish its package was simply "mysql" or at least "mysqldb". Every time I try to easy install it I go insane trying to remember it's "python-mysql" or some nonsense.
You work at a baaaad place.
with statements are pro
blah blah blah, it is still faster for most people to use python
I'm self-employed :)
web2py uses psycopg2. Never had a problem with it.
For a bit more context: http://docs.pylonshq.com/#faq
And if he doesn't get it done sufficiently fast, he's not getting paid at all.
I'm not sure that's true. I think what he's suggesting is that the "if (expr):" pattern tends to be more readable to him, and I say good then. Readable trumps in python.
Ruby syntax might look prettier at first, but take it from an old pro, Python has the correct balance of structure and expressiveness.
As long as he's the only one who needs to read the code. If other programmers need to read it, then write like they do.
psycopg2
There are two solutions: 1) Run your program in another thread. 2) Break your loop up so that processing can happen in chunks and give the VM a chance to do other things in between chunks. One way to do this is to write your loop as an iterator/generator. Use the Tkinter idle handler to process a chunk when it has nothing else to do. 1 is quicker to set up: e.g. from time import sleep class waiter(object): def __init__(self): self.i = 0 def wait(self): print "started..." for i in xrange(50000): self.i = i sleep(0.001) print "end." if __name__=="__main__": w = wait() from threading import Thread thd = Thread(target=w.wait) thd.start() I just tested this in IDLE and it seems to do what you want: i.e. the command prompt remains responsive and you can probe the 'w' object to check it's state. For 2 you need to know how to set up the "idle handler" for tkinter. I don't use tkinter (prefer wxpython) so I didn't write an example of this.
Fusion of two others...so it's actually a good thing
How about pastebin? I'm just looking for something simple to see how its done and work from there. The idea I posted above gives good situations for writing puts when return/chance of breakeven (or delta) is &gt;1. 
I think it's difficult to say without knowing more about what you're trying to do or seeing some code. Is this for a GUI program? Why is it important that the "temporary objects" are different from the real objects? Is it for performance? What is the difference between the two? My initial thought from reading is that you are making it more complicated than it needs to be, just use a real object, let the user do what they need with it, then grab `obj.__dict__` and serialize it / save it so you can use it to reinstantiate later.
pg8000 - pure-Python DB-API 2.0 module for connecting with PostgreSQL. Available for both 2.x and 3.x branches. [The Great Linkage](http://pybrary.net/pg8000/)
I believe the OP doesnt want to _kill_ the current loop but still want to know what's actually going on. I solution .. 1. Find the PID %&gt; ps aux | grep &lt;name_of_script.py&gt; 2. strace it %&gt; strace -p &lt;pid&gt;
And snakes!
If I could automate more and be more efficient in my job, the result would be a promotion. :) But I see you're self employed, so I understand where you're coming from. The key is to automate tasks and raise your rates simultaneously!
Awesome, so you wrote python code that generated python code? Yo dawg.
Thanks! I actually started learning Ruby a few weeks ago, but after some consideration, it seems Python is a better choice for me. I really want to look into Google AppEngine sometime, and likewise Python is a good fit for our work environment. I think I'll stick with it!
Hmm, well my code is a bit sloppy, but I would be happy to explain any of the steps. Basically the code does the following: - I have a directory of HTML files given to me by a marketing firm. I need to convert those into .NET pages (which are .aspx files, each having a .aspx.cs partner file) - I initially create some functions to handle some dirty work, those are the "def" functions at the top - The code starts below by first defining an OUTPUT directory. This will be where my .aspx/.aspx.cs files will be created - I loop through the HTML files recursively, and for each file I extract its title, filename, and content. The content is in between "start" and "end" comments, so the marketing firm knew where to put their content - Once I have the title, filenames, and content, I then open up a template file (which I created, it's basically just a blank file with variables in it, and I will replace these variables with the title, classname, and content) - I create a file in my OUTPUT directory with the appropriate .aspx filename and partner .aspx.cs file. Then I just do a replace for the variable names, like {title} is replaced with the title I extracted from the HTML file. {content} is replaced with the content from the HTML file, etc. - I loop through each HTML file this way, and output a .aspx file (and aspx.cs file) which matches each one. So, I started with 90 HTML files, and in the end I have 90 ASPX files which match the format I need. Hopefully that makes sense. I'd be happy to elaborate more if you'd like. :)
i think sqlalchemy can interface with it
The documentation looks top-notch. I'm not surprised, considering it's part of the Pylons project.
I am very confused... It is not clear to me if you use the word "View" in the MTV sense (like Django) or the MVC sense (everybody else). Looking at your examples you seem to have Controllers but you say you do not. In MVC, M="data representation", V="data presentation", C="application logic". So you say you don't have an "Application logic"? Clearly I misunderstood something.
I agree. It's exactly this aspect of Pyramid which is drawing me away from Turbogears 2.0 which has highly fragmented and incomplete documentation, which assumes far too much on the part of the reader, by comparison.
I've heard this argument countless times, and have believed in it for years. But in a few projects that I've seen actually scaled up in Python (Some of them mine, some aren't), my personal discoveries were: * Performance-wise: There was no hotspot to optimize. A whole lot of code was slowing things down, and optimizing it only moved the bottleneck to the next location with only a minor improvement. Overall, the effort of writing and optimizing was larger than using a more optimal language in the first place. * Complexity-wise: Even very well-written Python code can be pretty hard to "get into". Newcomers really make use of static types to make sense of code, and they're really lacking. Documentation is not easy to keep up-to-date and is not as trustworthy. The ability to just "jump to declaration" and see *for sure* what attributes some object has, or what arguments a function takes, is a *huge* benefit to newcomers. * Reliability-wise: Even with unit-tests, we've had a *lot* of errors slip by when refactoring code, which would have been caught by an advanced type system. The extra coverage needed by a dynamic language in terms of testing does not, in my experience, justify the effort. Dynamism is supposed to reduce effort, but if you want reliability, I think you spend more time overall. I realize I'm not exactly preaching to the choir here...
I think the argument goes more like "Most web apps call the view layer the templating library, but it's not a true layer and more just the result of what most frameworks call the controller returning it's view data parsed into the templating layer. So the actual callable run during the url dispatch is the view layer, and it talks to the data layer in order to get information to display/output to the user.
I think what they mean is their view includes the controller. The misunderstanding is in terminology. The idea is that the view takes a request as input like a traditional controller and then returns rendered output like a traditional view. This is different than MVC where your controller would take a request from input and outputs processed data, and your view would take the processed data and render output. So it is only MV, django just calls it MTV because the view usually delegates the rendering to other methods(template).
I think the fact that your company uses .NET and has a marketing firm is enough lead people to believe you work at a large company, and therefore must be paid by the hour, because after years of experience, large companies still don't know how to manage programmers. 
Great. This isn't C or Java, though, and you should adapt if you're going to use Python for long.
&gt; from \_\_future\_\_ import braces
Does http://docs.pylonshq.com/pyramid/dev/designdefense.html#pyramid-gets-its-terminology-wrong-mvc help?
Ben's announcement to the Pylons list about Pyramid: http://groups.google.com/group/pylons-discuss/browse_thread/thread/97faa18a3429a28e
well crap, I've spent the last two weeks learning Pylons... now this. Grrr.
the most powerful example for me is print "hello world" what you can do in one line with python takes 5 lines in Java... 4 of which are boilerplate. This example, however simple, embodies why python is so great vs. Java... now if it were only as fast :) I follow it up with what it takes to read a file: f = open(somefile).read() God knows how much fiddling with input stream this and byte array that you have to do in Java to make this simple thing work. I have to look it up every single time I use it... not so for python.
Thank you, sir.
Thank you, sir.
Well shit... I have some reading to do before I can decide if this is good or bad.
its ok, a lot of stuff like sqlalchemy or mako etc. will be present in pylons 2.x so dont worry about that
As described in the entry: websites and web services. I work for instance on the server-side part of Firefox Sync, a product that allows you to synchronize bookmarks, tabs, etc. among all you devices/computers.
I mean, what interest is it to the general public? Are you just blogging about your use of Python @ your job @ Mozilla? I thought maybe Mozilla had some consumer server tool/offering that had something to do with Python. I'm all for Python news and stuff but a blog about how it's being used in standard day-to-day operations doesn't seem all that intriguing.
Obviously you're not from Estonia.
That Design Defense impresses me. I have long thought that MTV and MVC were rather loose descriptions. These folks have really thought things through. I particularly appreciate their understanding of decorators as module-level code (where side-effects like "registration" may be unwise), as well as the ambiguity of a module that is normally both executed and imported. Very lucid.
also the thing i miss most in java are literal lists and maps: a_list = [1, 2, 3] a_map = { "a" : 1, "b" : 2, "c" : 3 }
Granted MVC originally applied to event-loop driven GUIs. But there's still a "three layer" approach at work - the initial impression of seeing just "MV" is that code is going to be shoved into one or the other layer which does not belong. Maybe if you used a term like "MMV" for "model / mediator / view", though you run the risk of conflation with the mediator pattern, perhaps "MIV" for "model / interaction / view", or some other kind of "middle" term, I bet you'd have less need for as much of a defense.
It helps *me* understand why I've always thought most web frameworks (other than Pyramid) have such an odd concept of controller and view. (I have a background in desktop application development). It probably goes some way to explaining why I just felt more *comfortable* reading the Pyramid docs - which are high quality in other ways too.
I think I disagree. The view *might* use a template to render itself, but if it does, that is an implementation detail of the view, not a separate system. And often views do not use templates at all. So I'm not sure how there really are three things instead of two.
MVR ? model-view-render ?
Pg8000 is good, but it lacks supporting `xml` type that is one thing I need. That’s why I don’t use pg8000 yet but psycopg2.
No it does not. It confirms they use the V(iew) in the Django sense while still referring to MVC in various places in the documentation. To somebody coming from outside MV sounds like they are doing PHP in Python (no separation between logic and presentation), which is not what they do. This is not a critique but a suggestion for improvement. Do not come up with your own terminology. Use a terminology that other people understand and build explain what makes your system better than the others.
You know ... there is one web framework that has been backward compatible since 2007 and makes backward compatibility a long term goal. ;-)
I'll bite... which is it? I don't actually know much about frameworks. I decided a couple of weeks ago to write a web app in python and then looked for a framework. Pylons seemed to fit the bill nicely so I dove in.
sqlalchemy is not a connector. It provides access to postgresql via psycopg2: http://www.sqlalchemy.org/docs/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.psycopg2
And they know how long you worked because? When I freelance I have my choice whether to bill hourly or for a particular job, and it is based on my estimate rather than the actual work generally...
When it makes sense we'll use existing terminology. When that terminology is wrong, we won't.
No worries, the Pylons framework isn't going anywhere. I posted another message to the mail list to try and expand on that. http://groups.google.com/group/pylons-discuss/browse_thread/thread/92d8ef2d8678fdca
[we](http://web2py.com/book/default/chapter/00) make of backward compatibility a goal. Unfortunately lots of people do not seem to understand the importance of backward compatibility.
When two framework development communities merge, it's very difficult (and often even undesirable) to have complete backwards compatibility for both older systems in the same package. Of course, no two relatively popular Python web frameworks have ever merged before, so this is untrodden territory. But we feel that reducing the senseless number of credible web frameworks in Python-land is of more long-term benefit than endless backwards compatibility for both. Of course Pyramid *is* largely backwards compatible with one of the two (BFG): http://docs.pylonshq.com/pyramid/dev/tutorials/bfg/index.html .
I would caution anyone against using this framework, as it was built by aliens who worship an ancient crocodile god.
Why would anyone want to keep doing menial work if there is a more efficient way, even if they are paid by the hour? I'd rather finish this project faster, and move on to the next one. You still get paid the same (maybe now by someone else) and have something new and interesting to do, and feel happier because you aren't wasting your life on unnecessary toil.
Obviously you are not from Estonia.
Wow, I wonder how slow this is. Python is one of the slower VMs, but writing a Scheme VM inside it... just... why? I guess since this is a hobby/education project I shouldn't complain. Maybe you could extend it emit real assembly code. Maybe you could write an interpreter for the bytecode in C, which should be quite easy, and compare it to the Python implementation.
How is it any different in Estonia? Are you referring to some bizarre work ethic, lack of work to do, types of contracting clients or something else there?
&gt; I guess since this is a hobby/education project I shouldn't complain. But you are.
Speaking as a Python developer, I've been told about Scheme a few times and it's been recommended that I try it. This merges a community I've quite involved in with one I'm interested in, and now I'm more likely to look into it. Is Scheme worth it to add to the toolbelt?
I suppose so. I can understand writing a compiler in Python (since you don't rely on Python for program execution), but an interpreter seems to be taking it too far in my humble opinion.
Bizarre management ethic.
This misinformation must stop: http://docs.pylonshq.com/denials/pyramid.html
&gt; Is Scheme worth it to add to the toolbelt? No, most likely not. I use it (I code in Scheme &amp; C mostly, with a bit of Python for clients), and it wouldn't really be worth adding to your tool belt. It takes a lot of time to get a Scheme system to be "production ready", so tool belts are not really where it belongs (at least, not right away). If you're looking to learn something because things like continuations, delimited continuations &amp; `syntax-rules` will teach you new things you may never have seen before (or have seen, but in an abstraction), then I would recommend Scheme. I've been using Scheme for quite some time now, and have quite a bit of Scheme in production; it is *extremely* useful to me, but it takes time to get everything together (and I cheated &amp; created my own Scheme interpreter, native code compiler &amp; vm) for production runs, unless you stick to specific ecosystems (such as PLT, which would be the healthiest for just starting scheme &amp; attempting something useful). Even if you stick to one, it can sometimes be frustrating to do something outside of what other people regularly use; before I had my own system, I used Scheme48 for servers, Gauche for unix scripting, Gambit-C to make fast-ish code and STKlos for GUIs. They each had their own strengths, and I just focused on that. 
http://pastebin.ca/1983990
I agree that the template is an implementation detail. However, it could be argued that there *really* are three things: in addition to the model (data) and view (presentation thereof, via template or otherwise), there are also application rules/application logic. Typically, the data is the least volatile; application logic and presentation tend to be more volatile (by which I mean more susceptible to change over time). Unless you are combining application and presentation logic (which has generally been regarded as an anti-pattern) then where would the application logic go? Some might say, with the model - put it with the domain classes, which are typically the model. This might work in some applications but tends not to work so well in more complex applications. Pyramid's "Design Defense" document contains some good stuff, but I would like to see some update to that document to discuss where application logic goes. In case it's not clear what I mean by application logic: I mean the sort of things that involve manipulation of model data before it's presented, or before it's written to database, and which are not purely related to how the data is presented - for example, how Amazon determines what to recommend to you based on your buying history and browsing history. Those histories are part of the model, and the way the recommendations appear on the page are presentation (view), but *what* the recommendations are is determined by application logic.
You'll need to dump the text in and parse it, good news is this isn't to hard and there are some already written Python programs for this.
Beautiful, well-maintained applications that do their job very well without becoming difficult to understand/build/maintain. For example, [these 490 lines](https://github.com/facebook/tornado/blob/master/tornado/httpserver.py) (mostly comments!) are the heart of one of the fastest asynchronous and high-connection-count webservers, [Tornado](http://www.tornadoweb.org/). You don't need to speak a word of Python to understand it completely.
Thanks for sharing- much appreciated. I'm just trying to teach myself Python in order to work on some projects and I'm looking at as much code as I can to see if I can follow the logical flow. Hope you're doing well in the market!
actually, I'm just getting into it and don't really understand what to do with the information. Go figure I learned how to write the code to mine the data before I learned how to use it.
If you really want to learn about the market, [read this](http://www.amazon.com/Little-Beats-Market-Books-Profits/dp/0471733067) Its written by a quantitative hedge fund manager and is probably the best book I've read about the market. Its written with the beginner in mind but does a great job explaining some quantitative ideas. He's got a great [stock screener](https://www.magicformulainvesting.com/stock_screener.html) too that works alongside the book. Its free, but you need to register for the screener. The ideas he mentions are perfect for someone who wants to datamine and research stocks. Even if you don't ever buy a stock, it gives you real insight into what makes a good investment.
[You're not the only one, you know](http://docs.djangoproject.com/en/1.2/misc/api-stability/).
Much appreciated!
You can also 'save' things, too. I only recently started using that feature-- pretty useful. :)
psycopg2.. but I only ever use it with sqlalchemy anymore. There is next to no reason to use psycopg directly when you can do: from sqlalchemy import create_engine db = create_engine("postgresql://...") for row in db.execute('select ip,name from device'): print row.ip, row.name the sqlalchemy engine/sql layer is what dbapi should have been, but wasn't.
I think maybe part of the confusion here is that, while you believe we took the words "model" and "view" as the M and V out of "MVC", and just declined to include "controller", that's not how it happened. Long before anyone tried to apply the "MVC" term to a web framework (2001), the Zope 3 project started and it chose the term "view" to represent the unit of code which gets dispatched to when a URL is matched. This is the same concept of "view" as used by Pyramid. Django appears to use a similar definition. It's the callable dispatched to when a URL is "matched". When BFG was created (Pyramid's direct predecessor), we needed a term for the kinds of objects that get traversed during "view lookup". Independently of the choice to use "view" as the name for the callable which maps to a URL, we chose "model" for this. This was probably a mistake (as explained in http://docs.pylonshq.com/pyramid/dev/designdefense.html#pyramid-uses-model-to-represent-a-node-in-the-graph-of-objects-traversed). But c'est la vie. So the View and Model terminology (as defined by BFG) were developed almost entirely outside the context of "MVC": the BFG terms just don't map one for one (or even directly one for N). If this is a sin, it's the same sin made by the people who needed three terms to represent three parts of their web framework that they believed were distinct, and whom chose to reuse (incorrectly) "MVC" (which has an *entirely* different definition when applied to event-loop-driven GUI programs). I'm afraid that, in reality, the term "MVC" just has absolutely no business being part of any web application that isn't event-driven. It's just stealing words from another concept (event-driven GUI applications) entirely. We would have been better off if Fred was the name for what you'd like to call "model", Bob was what you'd like to call a "controller", and "Alice" was what you'd like to call a "view"; it at least would have the benefit of being called "FBA", which would still mean nothing, but at least wouldn't conflict with an existing concept. In our applications, the *view* is the workhorse both for interrogating and shuffling through model data and making it suitable for display: it is the application logic. The view might call methods on the model to produce that data. It might also use a template, which would be your concept of a view (but also might not). But your favorite web framework almost certainly works exactly the same way, although it might call itself "MVC". It just chooses to make some distinction between the controller and its concept of view (a distinction that doesn't matter to me). Pyramid works exactly the same way as supposedly-MVC frameworks, we just decline to perpetuate the misuse of the term.
Every project should have a page like that in the docs.
Who are you talking to?
thanks for the clarification.
Why should a page that says "hello world" require that much boilerplate code? They're using that to make a point about *simplicity*?
&amp;#3232;\_&amp;#3232; um... is this sarcasm? that is pretty damn simple FOR A FRAMEWORK (not a simple CGI like thing that just does a simple "print Hello World"). given how much does, it's amazing it can scale DOWN that far to something that simple. edit: spelling. 
&gt; Pyramid is 5,000 lines of runtime code. Pylons 1.0 has about 3,000 lines of runtime code. Django has about 60,000 lines of runtime code. You’d practically need to bend the laws of space and time for Django to be simpler than Pyramid. I dunno. Django and CherryPy both have simpler Hello World code than Pyramid. Pyramid's Hello World even *feels* less intuitive. And the rest of the code samples are even worse. If your design needs a huge, giant webpage to defend itself, then it's probably not a good design.
It's not sarcasm at all. It's clearly a complaint other people have (because it's on that page further down). In their response, they diagram what every line does -- and they note that two of them are "bogus, but required". You can tell directly from their "hello world" that they have designed themselves into a corner and that their configuration system is unwieldy. Now that I've read through that page, I've seen every one of the most unpopular aspects of Pyramid's design in one place, plus really defensive-sounding justifications for them that amount to "you'd do this too if you thought like us". I can't imagine how they benefit from going to all the effort to write that page -- it wins them some mailing list arguments while losing users. By the way, since you had to be snippy: you can't spell "sarcasm", CGI is a straw man, and accompanying your reply with a look_of_disapproval and a downvote was rather unnecessary.
Speed is, of course, absolutely of no importance in the design of Bob. I'll quote from its "getting started" guide: ---- The single guiding design philosophy of Bob is *clarity and simplicity*. There's a huge amount of optimizations I can envision applying to Bob to make it more compact and faster. I avoided these optimizations on purpose. A sad inherent conflict in software is that optimziation and clarity are almost always at odds - you can gain one, but usually sacrifice the other in the process. The mechanisms Bob implements are complex enough even without optimizations. Therefore, I aimed to create a design that's as simple as possible, and implement it with code that's as simple and clean as possible. 
As a learning aid to expand your horizons as a programmer and expose you to new design techniques and ways of thinking about software - YES, Scheme is worth the effort. As a production tool to get the job done, I honestly think that having Python on your toolbelt already, you will not gain much from Scheme.
Well, let's look at the code, shall we? from paste.httpserver import serve What the fuck is "paste"? What does it have to do with Pyramid? Is it some clipboard access library? from pyramid.configuration import Configurator Because all the simple names like "Config" were taken. from pyramid.response import Response def hello_world(request): return Response('Hello world!') if __name__ == '__main__': config = Configurator() config.begin() Why!???? Why the fuck can't Configurator call self.begin() inside its \_\_init\_\_() ? Unless you're doing low-level stuff like OpenGL, there's no justification to having any sort of begin/end in your API in ye modern times. config.add_view(hello_world) Why is this not a dictionary? Why can't I change the view's mount point? config.end() Why!? Is make_wsgi_app() unable to call self.end() when it's called? app = config.make_wsgi_app() serve(app) If it's a WSGI app, then I suppose you need to deploy it with mod_wsgi? So in addition to writing this "simple" code, I'm gonna also have to set up Apache on my laptop to run it? Bollocks!
Most projects *don't need* a page like that in their docs.
Any performance comparisons? I don't see any between Pg8000 and psycopg2. Also, I routinely process a lot of information... and am glad that psycopg2 has an async mode.
I said there is one framework, I did not say there is ONLY one framework. Technically Django 1.0 was released in 2008, so I suppose, it is possible there are some apps written with 0.96 in 2007 that broke with Django 1.0. Anyway, I do not know for sure so I did not feel about making statements about it. It is your job to do that. Anyway, the page you linked says "In a nutshell, this means that code you develop against Django 1.0 will continue to work against 1.1 unchanged, and you should need to make only minor changes for any 1.X release." What about 2.x? The web2py definition of backward compatibility is VERY different: If you program using the documented API of any version (including those available in Oct 2007), you will NEVER need any change to work with the latest version, not even minor changes (you will need changes to take advantages of new features). This is a goal for the project.
you are arguing more than the length now. &gt; What the fuck is "paste"? What does it have to do with Pyramid? Is it some clipboard access library? If you have ever used pylons directly and or the one of frameworks based on it (like TurboGears), you know paste. Pylons is combination of a number of different smaller projects. See: http://en.wikipedia.org/wiki/Pylons_(web_framework) &gt; from pyramid.configuration import Configurator &gt; &gt;Because all the simple names like "Config" were taken. &amp;#3232;\_&amp;#3232; Fine. Picky. &gt; Why the fuck can't Configurator call self.begin() inside its __init__() ? Because an instance of Configurator can be used across threads and over and over. If you read the docs, the begin method stores the current request and registry into a thread local. Not the clearest design but it makes complete sense. &gt; config.add_view(hello_world) &gt; &gt;Why is this not a dictionary? Why can't I change the view's mount point? This makes absolutely no sense at all. have you read the signature on this method? Do you know you can add other things to this? You know its a layered stack and not a dictionary? http://docs.pylonshq.com/pyramid/dev/api/configuration.html#pyramid.configuration.Configurator.add_view &gt; config.end() &gt; &gt;Why!? Is make_wsgi_app() unable to call self.end() when it's called? you would rather have a stupid lazy shortcut that causes unexpected behavior more than explicit and well defined behavior each and every time? &gt; If it's a WSGI app, then I suppose you need to deploy it with mod_wsgi? So in addition to writing this "simple" code, I'm gonna also have to set up Apache on my laptop to run it? Bollocks! no. that's what paster's serve function does. it launches a development server for you. If you remove that bit and just return the app, you can host it in apache. if you want a WSGI app, just return wsgi app variable it for what ever server you want. if you are using an actual config file and not setting up configuration like this (like who you would in a non hello world app), then you can just use the "paster serve" command on the shell. 
Question: why not just put # -*- coding: utf-8 -*- in every .py file? I've been doing this - helps me solve the problem with clashing encoding. Is this bad?
That tells the Python interpreter how to decode Unicode literals in your module, but it doesn't affect any of the issues described in the blog post.
Psycopg2 (trolling)
The problem with this document, and this example code in particular, is that they're trying to be everything to everyone. They know they're a big framework. Their users know they're a big framework. The piles of existing Zope code they're compatible with know they're a big framework. And then they try to show off their single-file usage pattern as if they were a microframework. A large framework showing off their single-file usage pattern is like the chess grandmaster Garry Kasparov showing off his naked body. Probably nobody wants to see that, and it isn't even representative of what he does.
the funny part is that it's really not that big on it's own. compared to the 60,000 lines of code in django, this is pretty low. I concede that its not really that fair since it's only piece of the stack that includes paste, pylons, webobj, etc. 
I can't find a hello world with django that is that short. Everything involves building up a settings.py for config, urls.py for routes, and some kind of views py file to put your view code. You can combine these but it's still longer than this example. What drives me nuts most about django is that the devs have confused the concept of a controller with a "view" with and a view with a "template" (the models with their ORM make sense though). It's basically not an MVC but a MTV. Their FAQ has a note on it and says "Where does the “controller” fit in, then? In Django’s case, it’s probably the framework itself". Seriously. 
Seconded. The docs are good and I haven't had any problems with it.
I like how it's a 5000 word essay to explain how this technology is 'simple'
I hadn't heard of Pyramid until reading this, but I'm afraid this document pretty much convinced me not to look any further in to it. UPDATE: OK, I've read more now and can see that Pyramids is more interesting than the document lets on. I don't think it's a very effective advocacy tool though!
Do you think? I always find it interesting why people make certain design decisions.
&gt; the view [...] is the application logic Thanks for the clarification; IIUC in your terms, view = application logic + presentation logic. But the issue is not just about terminology, is it? ISTM it's about how many independent concerns there are. I agree that Smalltalk-style MVC does not map neatly onto web applications, but in the most general case, wouldn't you agree that the following are separate, orthogonal concerns? * The data that is persistent and typically models the problem domain * The specifics of presentation for different media (e.g. for desktop browsers vs. smartphone displays) - i.e. *how* things are to be displayed * The application logic (determining *what* is to be displayed as well as manipulations of persistent data) (In Django for example (loosely speaking) M = data, V = controller, T = view.) Perhaps you are right that MVC is a bit of a misnomer in the web application space. I'm not hung up on terminology, but I have found it useful on various occasions to separate out the *three* concerns I mentioned.
I used PyQt for a decent sized project recently. It is almost as good as using Qt in C++. Inter-thread communication is a breeze. It also packages well into a py2exe installation. The only big issue I have with it is that the community isn't that large yet, so you'll probably get stuck when you're searching for solutions to problems online. In some rare cases you might be the first person to have a question.
I find it much easier to see beyond the limitations of a framework if I understand why they are there. I applaud this page!
&gt;I have with it is that the community isn't that large yet, so you'll probably get stuck when you're searching for solutions to problems online. In some rare cases you might be the first person to have a question. Rings true for me. I like what PyQt provides, it just sometimes feels you are the only one using it. Solving, what seems like, a trivial problem can be hard until you go through c++ examples. Now i'm kinda thinking of there are very few advantages of PyQt over Qt(c++).
I don't disagree that having more than two concepts might be useful. It's just not the framework's job to try to define all of them in terms of its internal components. This goes back to the grandparent post of this one: "I think I disagree. The view might use a template to render itself, but if it does, that is an implementation detail of the view, not a separate system. And often views do not use templates at all. So I'm not sure how there really are three things instead of two." While the programmer will want to carry around some distinction between templates and views in his head, the framework (even in supposedly MVC systems) doesn't make one for him, at least not in a way that it *always* makes sense to use both a template as a view from a controller. In such a system, the programmer can always choose to not use a template and just return a response object directly from the controller, which, MVC-terminology-wise, means that he has discarded the "view" layer. In "real" MVC, it is not possible to discard the view layer; it's what provides the UI. I don't disagree that there may be divisions beyond "model" and "view" from the perspective of an application programmer, but the exact divisions between "M", "V", and C" as defined historically are not present in the framework itself, and we only found two "truly meaningful" concepts that matter at an architectural level *to the framework*.
The irony.
So basically you are not planning to learn from the mistakes you are making? That is certainly an interesting approach.
That'll make u'föö' be handled correctly in source, but won't affect byte strings at all.
There are two types of mistakes: bugs an wrong design decisions. We do fix bug. We also have some design decisions that turned out to be mistakes. We have always been able to deal with them by API compatible internal rewrite or by creating new API to replace the old ones but without need to break the old ones. For example we have a complete rewrite of the template system, we have a new experimental rewrite of the Database Abstration Layer. More customizable forms. New htmp5 scaffolding app and new admin app with app development wizard, etc. I am just saying we think twice about making something stable so that we do not have to regret it, and we work extra hours because our priority are the needs of the users not the ambitions of the developers. In general I can say users are happy with the overall design as of Oct 2007.
How does it stand against flask ?
&gt; Because an instance of Configurator can be used across threads and over and over. If you read the docs, the begin method stores the current request and registry into a thread local. Not the clearest design but it makes complete sense. But each non-boilerplate design still manages those things under the hood and responds to "messages" like `make_wsgi_app()` to close the `Configurator`. Another safe option was to use context managers. Explict use of `.close()` is so Python 2.4. ( Just a nitpicking remark. I don't really use Python web frameworks and I have no idea why there are hundreds of them ).
&gt;If it's a WSGI app, then I suppose you need to deploy it with mod_wsgi? &gt;So in addition to writing this "simple" code, I'm gonna also have to set &gt; up Apache on my laptop to run it? Bollocks! I can't stop laughing at this. 
Very nice! Would you add a web2py example too? I just made it: Controller: import urllib from gluon.contrib.simplejson import loads def index(): endpoint='http://graph.facebook.com/search?q="so%20starving&amp;type=post' def get_data(): return loads(urllib.urlopen(endpoint).read())['data'] data = cache.ram(endpoint,get_data,30*60) return dict(data=data) template: {{extend 'layout.html'}} &lt;ul&gt; {{ for post in data: }} &lt;li&gt; &lt;div class="picture"&gt; &lt;img src="https://graph.facebook.com/{{=post['from']['id']}}/picture"/&gt; &lt;/div&gt; &lt;div class="message"&gt; {{=post['message']}}: &lt;span class="author"&gt;{{=post['from']['name']}}&lt;/span&gt; &lt;/div&gt; &lt;/li&gt; {{ pass }} &lt;/ul&gt; (assuming it is ok using the default layout.html) It is [running here](http://web2py.com/fmld) and the source code is here [as a zip](http://web2py.com/fmld/static/fmld.zip) or [as a w2p](http://web2py.com/fmld/static/web2py.app.fmld.w2p). EDIT: works on app-engine by redefining cache.ram as gae memcache. 
So think about that for a minute. You introduce new API's, and leave the old one's there.... and here I am introducing Pyramid, a completely new set of API's, while leaving the existing pylons package there and not breaking any of the old ones. People can then move later to using the new API's, or not.
That is good. Thanks for clarifying. That was not obvious from the docs.
Sure. Are you on github by any chance. I would prefer to do a pull if you fork. If not, Ill just copy.
Do you mean that you try to type a backslash and it doesn't appear? You may want to submit a bug report to [bugs.python.org](bugs.python.org).
Sorry. I do not have an account. You should be able to just unzip and copy it.
 Shift+Alt+7 Also checkout "Keyborad viewer" built in System preferences &gt; Keyboard
actually there are more files zipped then you need. You only need: - controller/default.py - views/default/index.html - views/layout.html (or make your own) - views/web2py_ajax.html (unused but included in default layout) - static/* (used by default layout) Thanks for adding web2py.
Done.
:-)
I agree with you in general, however the problem is that example here isn't the default pattern that these classes are used. Just the condensed way for showing the smallest hello world example. Normally you load your configuration. You are also are reusing these instances over and over in the lifetime of the app. Also calling `make_wsgi_app()` can technically happen before you `end()` to setup of the request for that thread on the `Configurator`.
appengine looks pretty terse, but I haven't really checked the others too intensively.
haha. you guys made my day.
so django is a micro framework now?
Django is a micro web framework?
One thing I found interesting.... most frameworks (not web2py) use the following cache logic: data = cache.get(...) if not data: compute data cache.set(....) While this works file, this allows a possible concurrency issue where two or more requests concurrently recompute the data. If the computation of the data is expensive (in this case it is because it involves an outbound connection) this causes a denial of service vulnerability: an attacker can guess when when the cache expired and submit a lot of requests that cause data to be recomputed concurrently, stalling the server. In the web2py cache design there is not get/set. There is a single function that takes the computing function and uses mutex lock to prevent the computing function from being called twice within the same process. I am posting this as a suggestion for improvement, not as a critique.
&gt; It's just not the framework's job to try to define all of them in terms of its internal components. I don't disagree as far as the framework code goes, but if these concepts will come up in app development there's no reason why they can't be addressed in the documentation; as the question of "where does the application logic go?" occurred to me, it will no doubt occur to others. &gt; This goes back to the grandparent post of this one Well, the template vs. view thing is a red herring, as I agree that templates are just implementation detail for views. 
Since when django is a micro framework? (60000 lines of runtime code) Or is it just for the sake of comparison with a well-known large one?
Not if your editor saves files as ISO-8859-1 or another one that's not utf-8.
Recalculating data is cheaper than a mutex across your network. If all your synchronization just happens for one process and you depend on locked cached updates you are screwed when you need more than one server, so why support it in the first place? What however is important are atomic operations such as adding and incrementing next to a set that just overrides. And as far as I know, all cache systems support that in some sense.
You should be using `cache.add` (in memcached language), which will raise an error if the key already exists.
&gt; more than one server Or more than one process, as many deployments use.
Do you have any opinions or conclusions about the frameworks after doing this? What about documentation quality?
Umm, wanted some reference to compare to. I know Django best, so that is what I used.
I will be comparing with another bigger app again, before I come to a conclusion.
I am not suggesting this is done across the network. I talked about mutex locking at the level of each process. If you have multiple servers and there are two requests hitting the same process (different thread) you lock, if they hit different servers you do not. This is sufficient prevents the denial of service vulnerability. 
This is called the dog pile effect. There are quite a few ways to avoid it. Relevant blog post : http://highscalability.com/strategy-break-memcache-dog-pile
Seems like you could make something like: def join_files(filenames) src = [] for filename in filenames: src.append(open(filename).read()) return "".join(src) print join_files(["file_a.txt", "file_b.txt", "file_c.txt"]) And that might just blow a Java programmer's mind.
Indeed, its meant more to answer critiques about a particular aspect. I'm working on some blog posts to highlight some of the features that I think really make Pyramid a compelling choice, which would definitely be more of an advocacy type thing.
&gt; I am not suggesting this is done across the network Then I don't see the point. Why introduce something in your cache layer when that functionality goes away when you scale big? &gt; This is sufficient prevents the denial of service vulnerability. If the code that generates that cache entry is *so* slow you fear it could become DDOS'ed you better start using a message queue right away and move that away from user triggered code.
That'd be more funny except for the fact that the single-file Pyramid app results in less function calls executed for a simple 'hello world' than most if not all of the "microframeworks". If your criteria of 'microframework' is that it has a small execution stack, maybe you should benchmark some of them and Pyramid. If your criteria is that there's very little code in the 'package', that seems a bit weak cause then I could just create a new package that configures some existing larger packages of code (like Flask does, and to some extent Bottle if you use the Beaker sessions). But really, arguing over semantics like 'micro' or 'big', or however people want to try and throw the words around to influence PR is a waste of time. Let's be programmers, and talk about efficiency, function calls per request, and code loaded for an application. Then debate the results of such a comparison, and try and decide which words to apply where.
It's called "[addition assignment](http://www.google.com/search?q=Addition+assignment)."
I'm not 100% sure what you're uncertain about but "+=" works like this: &gt;&gt;&gt; x = 3 &gt;&gt;&gt; x += 2 &gt;&gt;&gt; x 5 That is, you can use it instead of writing &gt;&gt;&gt; x = x + 2
They're all doing it wrong. They all should be doing HTTP/1.1 caching first and rely on ad hoc solutions when that fails you. All these frameworks suggest you do it ass backwards. 
It's probably not my job to try to slot general application development concepts into arbitrary categories beyond the framework's own internal concepts via the Pyramid documentation (at least this set of docs). If folks want to treat their webapp as fifteen levels, good on em. If they want to do it as two, fine. I'm not going to tell them how to think about it, I'm just going to tell them what the framework does, and how it fits in to making their app. I don't understand how the template thing is a red herring. Your previous post pointed at templates as "view logic". 
Correct. Have a look [here](http://www.tutorialspoint.com/python/python_basic_operators.htm) at the section "Python Assignment Operators" for other operators that support this syntax.
Thanks for the answer. You have saved me enough time so my use my daylight savings time bonus hour to make pancakes.
99% of the sites run on a single server and this is an easy way to avoid a DDOS. Moreover locking cache scales well since you still lock at the process level. If the DDOS attack originates from a single client and you use a smart load balancer that sends all requests from the same client to the server, the problem is solved. Yes you can use a queue but in that would be an example of over-engineering when the computation takes a few seconds. If it takes minutes, I would go with a queue.
Actually that is a good idea. Yet it would not change the API necessarily.
To Java folks who don't know anything about python I usually tell them: Everything is an object, although you don't have to have a single class declaration for a working program. Modules, functions, even classes are objects. And you don't have funky primitive types like int, char etc. You don't declare types, although objects do have types. You don't declare any return values to methods, you don't declare that a method throws an exception. You can inspect objects at runtime. Also you can morph any object at runtime. For web applications development is orders of magnitude faster even after you spend a month learning the language. Performance is usually better than equivalent Java web applications. To all that they usually respond: "I can't imagine dumb people writing serious software with a language like that". People who know only Java, and who will refuse to learn anything else in this day and age, like the language because of it's bureaucracy, or so they think. To them the choice they've made to stick with Java no matter what is of outmost importance. Even when they struggle for months to build the simplest webapp with 10 db tables, they will shrug when you tell them they can do that with wordpress or drupal in 50 minutes or python in one day. You can't expect a person like that to reason pragmatically. Believe me, I've tried suggesting Groovy to people like this as a language that is backward compatible with Java, still they'd rather spend injecting FactoryFactoryFactories in their implementations of AbstractSuperSpecializedController for months going nowhere instead of giving a few hours a week to learn something that would make them orders of magnitude more productive. On the other hand, there are Java people who just HAVE to use Java sometimes because of no other choice and are fully aware of the productivity gains from using other languages. They most often know about Python, even though Groovy, Ruby or Clojure may be their cup of tea.
Even if you initialize the mixer with with only one channel, make_sound still wants to make a stereo sound. Try something like this: pygame.sndarray.make_sound(numpy.array(zip(snd_ary, snd_ary)).astype(numpy.uint8) + 128) Also, using -16 as the second argument to pygame.mixer.init means it will output using 16 bit signed numbers, so you might want to use numpy.int16. EDIT: Next time, you should put the error you get. It really makes figuring out what you are doing wrong much easier.
Just adding to this, once past my employers CDN and internal caching layer, we have to use some sort of locking mechanism on all cache set operations to avoid stampede scenarios ( 100's or worse 1000's of concurrent distributed processes are all trying to set the same cache item using expensive system resources). Until we surpassed 100 million requests a day, it was not fiscally viable to use a queue.
Funnily enough, most of these frameworks' caching tools will set the appropriate outgoing headers for you. Which I'm pretty sure you know. I'm pretty you also know that "just set the HTTP cache headers" covers only a tiny fraction of the actual problem.
&gt; Pyramid is 5,000 lines of runtime code. Pylons 1.0 has about 3,000 lines of runtime code. Django has about 60,000 lines of runtime code. You’d practically need to bend the laws of space and time for Django to be simpler than Pyramid. Well at least you got the 5000 word/line part correct.
Sorry to answer your question with a question: But how does Django, Turbogear or pylons stand against flask? I believe it don't because you are comparing micro web frameworks with web frameworks. It is like comparing lemons to oranges. What I would rather ask is how does flask compare to [Bottle](http://bottle.paws.de/docs/dev/index.html)?
Sorry, I was wrong, I underestimated, wc reports 11565 words.
the view code may be equivalent between the 2, but i find it hard to believe that settings.py + urls.py + views.py is shorter than that example. where's an example of the django app that didn't start with django-admin.py startproject 
It did work. a = pygame.sndarray.make_sound(snd_ary.astype(numpy.uint8) + 128) a.play() -&gt; click from speakers a.get_length() -&gt; 0.0022675737272948027 Input array is only 100 elements, in 22050Hz resolution this does not make any sensible noise ;)
I guess you meant "not fiscally viable to use multiple machines" because I am quite sure last time I looked a lot of the queue packages came free.
I meant as far as resources for training, implementation, testing, and maintaining an additional tier of complexity. The life cycle of a web product usually starts as 1 machine that has all its dependencies ( http server, db server, application logic, static content) and should only expand as needed. Usually the product splits into 2 machines ( application server &amp; db server ), then expands as needed ( n+ application servers, n+ content servers, and n+ levels of redundancy for the database). Its good to plan for expansion, but within reason. At some point it becomes increasingly desirable to implement a MQ into the infrastructure but all corporations usually have a charter rule saying that the executive officers must make decisions that maximize the profits of its investors. Until you can justify to a company's officers that any additional infrastructure additions are necessary, the addition is not fiscally viable. 
Speaking of top-notch docs, web2py also has top-notch documentation. It even includes introduction to Python and jQuery for complete beginners, which is a nice touch.
&gt; I'm not going to tell them how to think about it Fair enough. &gt; I don't understand how the template thing is a red herring. In the sense that I don't think I disagree with you on the point about templates being view implementation detail, but perhaps I'm confusing "view = presentation logic" (how I think about the word "view", except when I'm discussing Django views with somebody) with "view = application logic" (how you think about the word "view").
well, in that case, your editor is broken, since it's not respecting the `coding` line.
In your model, if the "controller" is the application logic, and the "view" is the presentation logic, what pieces of software represent these things to you, if the "view" isn't necessarily a template? Does that division of software responsibility exist in your framework, or does it exist only in your application?
Hahaha, so you DO admit aliens exist, right? "These aliens are not telepathic. They do not look like human babies." What do they look like? :)
aaahah. No kidding. Thanks =]
Apparently on the Internet, there's lots of people eager to throw digital feces at each other anytime someone uses code, or writes code in a manner that somehow offends someone else. Maybe because the "Wrong" library was used, or the "Wrong" terminology, or uses some other library that the person has a hate-fetish with. It's kind of a bummer there's so many of those people online, and loud ones. I wonder how many great contributors to open-source we're missing because of the feces-throwers. Not all programmers want to put their neck out on the chopping block. Anyways, that page is meant to try and be a little more logical when answering some of these attacks thrown at it (thus the title mentioning a defense). It does mean though, that all the responses to all the critiques are in one place, so at the very least one can peruse the "worst-case" set of critiques of the framework. That takes a lot of honesty to do such a thing, and yes, seeing every critique in one place is probably not great PR. We're open to edits and suggestions on how to improve them to clearly communicate why a particular decision was made in a "less harsh" manner. We're certainly not PR folks, just coders trying to communicate design decisions to other coders. Every web framework has plenty of critiques, many are scattered around the Internet. Sometimes answered, sometimes not. I think they'd be well off to do something similar so that people do understand where the framework developers are coming from though, because there is no end to those attacking.
&gt; Does that division of software responsibility exist in your framework, or does it exist only in your application? Could be either. For example, I may use a template to generate HTML, or a specialized view class to generate PDF. That's often part of the application, but there's no reason there couldn't be some support for it in a framework. For example, many frameworks result in a call to a view (using your terminology) returning some kind of response object which gets directly converted to an HTTP response. But you could have a view perform the application logic and return something representing the model + something representing the presentation, which get merged together in a subsequent rendering step. The existence of a separate rendering step allows the possibility to change the rendering: for example, render to HTML, render to JSON, render to PDF. This step you could call "presentation logic". As to whether that belongs in an application or a framework - the idea of a framework is surely to incorporate the things that need doing repeatedly. Not everyone might need to use separate renderings, for example, but conceptually it seems to be useful to distinguish that step from the application logic, because there would be scenarios where it would be valuable to do so. (I accept that most scenarios just involve generating HTML using a template. But I've also found instances where the same application logic is used to feed into web pages, using HTML templates, an API interface to the site, using JSON, and RSS or Atom feeds using an XML renderer.) 
psycopg2
Sorry James, I'm misunderstanding you. It may just be my impression, but most full-stack frameworks tend to favor ad-hoc caching by talking directly to a cache backend instead of utilizing HTTP/1.1's built-in caching mechanisms. Most frameworks, if they provide caching tools for caching internal data, for instance, Django's cache backends or Beaker's APIs which Pylons provide APIs for caching by hand. I'm whipping together a quick benchmark using the code linked to here and an implementation utilizing webob, the HTTP expires header and varnish. I should have the results within the hour. Preliminary results shows webob+varnish kicking the ass of any of the frameworks keeping it's own internal cache. It's pretty obvious to see why webob+varnish kicks their asses because once varnish gets the initial version of the content, it never has to talk to the WSGI app for another 30 minutes. A added benefit to passing the Expires header to the browsers is they don't even have to talk to Varnish for 30 minutes. So in essence, when using HTTP/1.1 cache headers, you create a caching layer that has a pool size of audience+1 varnish server 
I'll have to add that the test I'm doing is not a comparison of framework speed but the failure of the way we all do caching at the moment.
I think what you want to do here is *look at how this stuff is actually implemented*. For example, Django's cache middleware and its caching decorator for individual views (which are actually the same code) set the outgoing `Expires` header (and `ETag` if you're using those, and `Last-Modified` when needed). This gets you best of both worlds: things which respect HTTP caching will respect those headers. And things which don't will still get responses served out of server-side cache. Which is necessary: if you're under the impression that we live in a happy world where every client respects HTTP caching, brother, I've got some bad news for you...
I'm well aware that clients don't respect HTTP caching. Hell, developers don't even respect it. I think you may be missing the layer between the browser and the server, which is Varnish or some other HTTP caching server. If developers actually follow the stateless convention that HTTP suggests and keep client side state in the client, a majority of your caching problems can be taken out of the application layer and into a layer in front of the application. If you were ok with setting a 30 second expiration in memcache, there's no reason you couldn't tell Varnish to hold the content for 30 seconds and bypass Django all together.
Man. This isn't even wrong.
Have a look at [this video](http://video.google.com/videoplay?docid=6297126166376226181) done a few years ago by a guy at NASA comparing various web frameworks including Java and Ruby ones. It is a nice presentation style and you may find some of the things he compares useful to look at too (restarts, errors, auth etc).
he is referring to web2py
Could you elaborate?
which web2py API's have been depracated? In my memory nothing significant.
Juno? Last I saw that framework was entirely dead. I tried maintaining a fork for a while, but there is simply no reason at all to use it over Flask. The two are actually close enough to make the port almost painless.
true
Celery + ghettoq DB transport is all you need. No new infrastructure and a smooth upgrade path (usually -&gt; Redis -&gt; RabbitMQ). There is no excuse to not be using some kind of task queue anymore.
Here's the result of my tests. I didn't test all the implementations but a did a number of them. https://github.com/ericmoritz/so-starving/blob/master/test/README.rst
The problem with the Django cache middleware is that you're still hitting the Django app layer for no reason. Stick Varnish in front of Django and let that handle caching much faster than Django would.
[Flask](http://flask.pocoo.org/) is nice (though I'm partial to web.py)
Note that I never said "don't use Varnish" (or some other caching proxy). I said Django already supports HTTP caching. Clients which don't respect that or which don't already have a response they know they can cache are going to hit you anyway, and Varnish is a help with that, yes, but your initial comments implied that we just stick stuff in memcached and ignore HTTP, which so far from the truth it's not even funny. tl;dr if you want to say stupid stuff that's wrong and use it to imply other people don't know what they're doing, you're gonna get called on it.
I apologize if I gave that impression. I intended to mean that it is my impression that frameworks encourage the use of talking to memcached directly instead of encouraging the use of HTTP caching. I believe that it would be better if web developers started with HTTP caching (and a caching proxy) and only relied on memcached directly when absolutely needed.
I believe there may have been some confusion on my part, when I read, "most of these frameworks' caching tools" I was thinking of the django.core.cache module which provides tools for talking to data caches. You were talking about Django's HTTP caching tools (which I'll say are top notch btw)
Tornado, originally developed by Friendfeed and now by Facebook, has a similar API to web.py and is purportedly very fast. I used it for a project which was intended to be scalable and never had a problem with it (admittedly, it never reached scale) :).
&gt; I believe that it would be better if web developers started with HTTP caching (and a caching proxy) and only relied on memcached directly when absolutely needed. Except that doesn't work for any site which has authentication and personalization (i.e., the vast majority of interesting sites people build). Vary on cookie just doesn't cut it for those cases, since you still end up hitting your DB once for each user, and that quickly grows to an unacceptable load. Far better to start by figuring out your actual needs, and mixing and matching caching techniques to suit; some things will be as simple as "let the HTTP headers do it", but many things will not. (and that's without getting into invalidation, which is famously one of the hard problems...)
I'm currently facing the same problem. I can't tell if it is any good but at the moment Node.js + CouchDB + Express.js have caught my attention... 
I think you'll find that many of the authentication and personalization problems will go away if you put the client's state in their browser and don't keep track of it on the server. If you'd look at the type of sites we build, news sites, most of the content is common among all users. The personalization stuff can be either stored in the browser using localStorage, Dojo.storage or cookies and inserted using javascript. That'll cut out the need to personalize the page on the server. If the personalization is not done on the server, then the content of the page becomes common among all users. Slap a two second expiration on it and Varnish in front of it and tada, Django doesn't have to mess deal with it for a few hundred requests. Other things, such as personalized lists of stories can be fetched on-demand using AJAX (if it needs to be included on a HTTP cached page). If it's personalized, google will never see it so there's no real reason to not pull it in using AJAX. Since the personalized list of stories is now a separate resource from a common list of stories. We can use the vary header to cache that page based on the auth tokens. If you're targeting a platform that can't keep local state, like a dumb phone, then you'd want to create a state-full layer on top of your stateless site/web service. You'd piece together data while fetching it through the proxy layer. All the common data will be cached and fetched really fast and all the personal stuff will be be as slow as it would be if fetched via AJAX. Invalidation is a hard problem. Often, the solution is to keep the expiration times short enough so that stale content doesn't stick around. The same is often true with memcached. I don't find myself having to delete keys from memcached because the with the data I'm caching, eventually consistent is good enough.
I'm glad that someone is thinking about this, but I don't like the new usage. It's longer than the normal idiom, and the "correctness" fixups alone don't really justify its existence. Since I always make a `main` function anyway, I'd prefer something like this: def main(args): return 0 import demain ... where importing demain would conditionally execute `sys.exit(main(sys.argv[1:]))`. Implementing `demain` in such a way that it always executes when imported might be tricky, but I think it would be worthwhile.
Most of modern web frameworks written in Python are web server agnostic. Of course that’s because of WSGI. You can choose any web framework relatively freely from scalability problems. Instead, you should do good choice what WSGI server is suitable for scaling your web application. There are good surveys that deal with scalability and performance of WSGI servers (for example, [this one][1]). Try find them. Plus, I recommend you Werkzeug rather than any other full-stack web frameworks like Django. It is designed to be unrelated to model layers or templating engines, so you can glue good third-party ORMs _e.g. SQLAlchemy_ (or NoSQL adapters) and templating engines _e.g. Jinja2, Mako_ to it. First of all, it is a metaframework (that said, it is a library which makes own your web framework) that helps you to scale freely your web application and the web framework (you made with Werkzeug) your webapp depends. Additionally you can use Werkzeug with your favorite web framework because its primitive is just WSGI application function. [1]: http://nichol.as/benchmark-of-python-web-servers
There is no situation in a properly organized Python project where code that could run as `__main__` could be imported as a module. This is an attempt to add features and complication to what is only an expedient shortcut, not suitable for use in anything other than quick hacks.
Isn't repoze i.e. pyramid itself a micro web framework? Here is the quote from http://bfg.repoze.org/ &gt; Minimalism: BFG provides only the very basics: URL to code mapping, templating, and security. There is not much more to the framework than these pieces: you are expected to provide the rest. 
&gt; A terminology overlap confuses people who write applications that always use ORM packages such as SQLAlchemy, which has a different notion of the definition of a “model”. Here's a grep for the word "model" in our documentation's ReST files: classic$ grep "model" `find . -name "*.rst"` ./core/pooling.rst:to provide compatibility with SQLite's restricted threading model, as well ./core/tutorial.rst:constructs are modeled to resemble those of the underlying database as closely ./core/tutorial.rst:user-defined `domain model ./core/tutorial.rst:&lt;http://en.wikipedia.org/wiki/Domain_model&gt;`_ which is transparently ./core/tutorial.rst:persisted and refreshed from its underlying storage model. The other ./orm/extensions/associationproxy.rst:Above are three simple tables, modeling users, keywords and a many-to-many ./orm/extensions/associationproxy.rst:Above are three tables, modeling stocks, their brokers and the number of ./orm/relationships.rst:"nested sets" model, sometimes called "modified preorder". Despite what many ./orm/relationships.rst:online articles say about modified preorder, the adjacency list model is ./orm/relationships.rst:always assumes a "parent/child" model of row population during flush, so ./orm/session.rst:model to some degree since the :class:`~sqlalchemy.orm.session.Session` ./orm/tutorial.rst:user-defined `domain model ./orm/tutorial.rst:&lt;http://en.wikipedia.org/wiki/Domain_model&gt;`_ which is transparently ./orm/tutorial.rst:persisted and refreshed from its underlying storage model. The other So not really true. Right in the tutorial, we remark that we're building *domain models*, and that is not exactly an arbitrary choice of term, note the wikipedia link. If I talk about this stuff I'll usually say "domain model". The *Pylons* docs OTOH, use the term "model" to reference what I call the "domain model" like crazy, and I'm always whinged a teeny bit when I have long email threads with "model-centric" types who base everything off a class `Model` and are model this, model that, particularly in phrases like "where do I put my models"...are these your little airplanes? But that's fine. That said I find Pyramid's notion of "model" completely confusing, but it seems to be specific to that whole Zope "traversal" thing which, as a relational guy, hes right ! has for well over a decade seemed to me to be an entirely unholy creation and I'll be sticking to the URL dispatch option.
Wow, I didn't know that! I just assumed it was all over the docs, given the colloquial way that folks talk about "model" objects in #pylons (almost always in reference to SQLAlchemy models). Thanks for letting me know. And yeah, as a purely relational guy, traversal will be a divide by zero error for you.
Names are hard. There's no hard and fast definition of "microframework". I don't consider Pyramid a microframework, but on the other hand, it does share a common feature with lots of microframeworks: you can make single-file applications.
Around 1997 I wrote a big in house do-everything framework for my job, and while it used relational storage, my * big idea * was to base the *entire thing* around a *big giant hierarchical tree!*. So content, urls, security, all in this gigantic LDAP like system (since maybe you remember what a *big fing deal* LDAP was back then). It also had a completely restricted template model where no code whatsoever was allowed...it later inspired the project that became [FreeMarker](http://freemarker.org/whoWeAre.html) (look there's me!). The depth and expanse of the failure of that in house system are how today, I'm generally -1 on monolithic hierarchies and -10 on restricted templating models.
Not a horrible name. I wouldn't complain much if people referred to it that way.
What makes you think you will have a million users? Do you already have a million users somewhere else? If you do, what do you use to scale to that level already and why not just use that? How can anyone know what it will take to scale to a million users if we don't know what those million users will be doing? Is your app read heavy, write heavy? Does it require a lot of computation or is it mostly shuffling shit in and out of a data store? What about 'coupling' will make your scaling harder? The admin being tied to the ORM or render\_to\_response to the template system is gonna slow down your scaling how? You want a model system that will allow you at half a million or a million users to just unhook from its backend and replace that with a non-relational system? what does it even mean to not incur a strong dependency between your models and backend? you think your big problem in switching your site over to nosql from a rdbms will be rewriting your models? really? really?
We use the snot out of tree traversal for content management systems for three main reasons: - Our storage of choice is ZODB, and we usually store a big tree in it (filled with content). - "Nice" urls - The way BFG (and before it, Zope) maps URLs to views takes into account the type of the last object traversed (the "context"). We use the type of the context (along with some other things) to find a view after we do traversal over the graph. This pattern is baked into my brain at this point, so I'm probably not going to be able to do it explanation justice, but it's a handy way to do view dispatch, especially when the graph is not of a fixed depth.
Yah my thing was for a CMS, and I'd still go with hierarchies for CMS. Though I still feel like I'd want the "take urls and map to hierchical objects" thing to be locked in some very limited-scope set of functions. URL routing, like those generated by Routes, produces very "nice" urls as well, I'd argue "nicer" since you can tailor them independently of the organization of your "models".
go python. 
Most definitely: https://github.com/kennethreitz/tablib
Thats great, but if you look at the TIOBE index there is still some way to go. I guess too many still think that the correct way to go is to start with Java/C or some other static language, which leaves the dynamic languages to pick up the crumbs, where what should be happening in a lot of cases is the reverse: Script a solution quickly and accurately then check if it is fast enough and re-code only where it is not.
Now, this is a much nicer indication of Python's popularity, unlike the useless TIOBE index.
So maybe more tools will come out in Python now? I don't know why, but it grates me to put something like Fedora or Gentoo on an EC2 AMI so I can do Python stuff only to find out I have to install something like Ruby or Perl to do some obscure system management thing which should have been written in Python in the first place.
Just thought I'd drop by to say thanks for web2py!
Great news. Now only if more companies would use it. 
That only happened because it's true!
Mozilla builds open source applications to provide various online services, and application like Sync can be run on your own server. So the interest to the general public --well to the Python community since that's who we talk about-- is like any other blog/ML where people share about their Python projects how they build it. The MoPy mailing list is about sharing ideas on how to build Python software for the Mozilla project as a whole - You could see it as a cross community between Python and Mozilla projects. So that's not just me blogging about my day-to-day job that would be boring ;). You said you where interested in Mechanize+Beautifulsoup: If I remember correctly we have some test automation projects in the Mozilla project so that would be definitely a topic of interest in MoPy. Last, I am not sure what do you mean by a consumer offering that has something to do with Python. 
What is weird is that they compared GIMP vs Inkscape vs Blender. Also Best Bookmark Syncing Tool -&gt; XMarks witch is dead....
I like Python. But how can it be called the "Best Programming Language"? Can't the quality of a language only be jugded in the context of its use?
An even simpler version. I constantly amazes me how little bash beople know that would maker their lives so much easier. cat file_a.txt file_b.txt file_c.txt &gt; joined.txt
Yes, I wasn't trying to suggest Python had the easiest way of concatenating files, I simply meant to design something that was weirdly hard in Java.
Another attempt read_crontab = subprocess.Popen(['-l'], executable='crontab',stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE,shell=True) print read_crontab.communicate() ('', "crontab: invalid option -- 'c'\ncrontab: usage error: unrecognized option\nusage:\tcrontab [-u user] file\n\tcrontab [ -u user ] [ -i ] { -e | -l | -r }\n\t\t(default operation is replace, per 1003.2)\n\t-e\t(edit user's crontab)\n\t-l\t(list user's crontab)\n\t-r\t(delete user's crontab)\n\t-i\t(prompt before deleting user's crontab)\n") 
Or use packages and have your main file outside of the package and not importable.
doubt their judgment not their number.
&gt; What makes you think you will have a million users? I don't, but the startup in question has funding in the tens of millions of dollars from people who are very interested in making this work. The use cases for this app are well-known, to the point where anyone who's anyone is using NoSQL or SQL with a caching layer, and it's exactly because we don't know at this point how it's going to play out that I need something with little coupling. Besides, it is likely that different parts of the data will reside in different levels, and Django's conventions for dependence on a relational backend don't help. Most people who scale up Django end up rewriting a lot of components anyway. 
[http://svn.python.org/view?view=rev&amp;revision=77136](http://svn.python.org/view?view=rev&amp;revision=77136) - added context manager support to zipfile.ZipFile Although it was a trivial change compared to everything else we'll see in this thread, it was one of the first patches I submitted to Python, and it was the first one that got committed. It felt good to have made an impact, no matter how easy it was to do, and it was the platform that pushed me to keep contributing. Starting off with an easy issue like that allowed me to go through the contributing process pain-free, and it started me off 1-for-1. I didn't have to defend that patch or go through any rigorous reviews -- it was a simple starting point and it worked. I kept going, submitted more bug fixes and features, then got commit access a few months later.
Not sure what your issue is here. Are you saying Python shouldn't be called a "programming" language or that no programming language should be called "best" because what's best depends on the situation?
XMarks is no longer dead. http://blog.xmarks.com/?p=2007 
But how is damned blood-sucking GoDaddy on this list?
Mercurial rated *below* CVS and SVN ... seriously?? I can't really speak about git having not used it much at all (other than for checking out project code on github), though I've heard great things. Really surprised not to see Hg in the honorable mention spot. Something I'm missing?
Portage is written in Python.
Nice try potential employer...
It's a popularity contest. That is all. Popular != better
True; guess I'm just surprised to see how popular CVS is still. **Wake up, people.**
"Hey, remember how Java was so much better than C++?" -- "Ya" "Python is like that, only a bazillion times more" 
For many it may be the only one they've ever used. It's hard to believe but many (most?) devs in the world don't use *any* source control.
Mercurial is fifth!? After baazar?! hmm....
&gt;Can't the quality of a language only be jugded in the context of its use? It's possible for a language to be the best in most use cases yet be weak in certain areas. Read "Best Programming Language" as "Best All-Around Programming Language" or "Best General Purpose Programming Language" and you will be OK. :)
Multiple database support for Django. Huge patch (20k line diff), large existing codebase to work with (including backwards compatibility concerns), highly requested feature.
Distributed version control systems can be extremely confusing, *especially* for a programmer that has used a client/server system like CVS/RCS/Perforce for many years. Combine that with git's name recognition and push, and I think it's likely that the programmers that are passionate about using a good DVCS standardized around it, and everyone else who doesn't care just stuck with CVS/SVN.
NO VERSION CONTROL? Edit: spelling Edit: removed trailing spaces Edit: switched to caps lock Edit: Fixed formatting.
Although I'd agree that Mercurial is better, I can't think of many projects forcing use of Mercurial, while a very large number are forcing Baazar, which may lead to this trend. Ubuntu projects are all on Baazar.
Thank you for using it!
Yep, and people often vote on topics that they really don't know much about. Database is a perfect example where mysql absolutely has more installs than postgresql. But better? nobody who's a database guy would ever think so.
&gt;IWriteBadCode More like nice try _current_ employer.
I had a quick look at the bottle.py file that is on my hard drive it is just over 1,250 lines where pyramid is 5,000 and pylons 3,000. So what makes bottle.py and flask a Micro Web Framework and Pylons not a Micro framework? Once again I answer your question with a question(sorry I also don't know the answers) but like mcdonc pointed out the problem is there is no definition for "microframework". Edit: Sorry I'm retarded I brought in the microframework vs framework angle and sorry after some investigation not even flask or pyramid is a micro framework in my books due to its size and complexity compared to Bottle. So I still have not answered your question and my have only misguided you. Sorry.
I'll allow Python to be considered a "programming" language because you really are deploying a living, running "program" when you build a pyQt app or deploy a Django app. I'll allow Python to be considered a scripting language for obvious reasons. Technically we could run java as a scripting language but that would be pretty sick.
Great!!! In the back of my mind I'd planned on writing my own similar solution if this ended up actually dying...
Nice!
Well, most probably you are rightm but hell, it was a voting. Oh well, I don't want to start another flamewar (blah...), I am just surprised.
haha! This reply scared me for a moment. Not because I was afraid my current employer knows about my account. (I recently deleted my 2+ year old account and created this. I sent it out.) I was surprised that certain coworkers had the *balls* to actually reply to one of my comments and reveal theirs. I don't believe this is the case here.
Well, SVN is HUGELY popular (and if you count corporate installations, it's likely much more popular than git)... And "betterness" is somewhat subjective. But, the fact that CVS is even mentioned, makes everything else suspect ;-(. I was under strong impression that cvs is mostly dead both in open source and corporate worlds..
Thank you so much for this! When I heard multi-database support for Django launched, it made the project I was working on so much simpler and more enjoyable.
imo you don't want system management tools written in python these will just be used as an excuse to keep legacy versions of python around
flame fail
It must have come out wrong because I don't understand how I've attempted a flame war. I've merely stated that Python can be considered both a programming language and I stated reasons. Troll fail?
A specific parser. I had no idea what I was doing and ended up writing an LL(1) parser without knowing it. Made me proud later on when I found out what it was what I wrote :)
I'm kind of -0 on tree traversal for CMS's, because I'd rather treat a path as an opaque string... there's historical paths, redirects, transcluded content, "content" that really points to external pages, etc. Jamming those cases into traversal feels like more trouble than it's worth.
gimp vs inkscape I don't have an issue with because they do work very well together. I have not used inkscape much at all, mostly speaking from a photoshop perspective. There are times in gimp where I wish it had better vector and/or text design tools and say to myself I should learn inkscape (which I haven't done yet). Blender is a stretch, but given that if they put it in a separate category it'd have no competition I think it's fine.
Ok this I can accept ;)
[Sahana eden](http://eden.sahanafoundation.org/) uses GIS, [example](http://pakistan.sahanafoundation.org/eden/gis/index). It is a relatively large Python (web2py) project.
I wrote this because I plan on using the World Bank API in a project I'm working on (recreationally) I've never written code for others to use before. I tried to follow PEP-8 and document. Should something like this be packaged and/or put on pypi? I'm mainly looking for comments about how I could improve the code and the style in which its being distributed. I realize the code itself is quite trivial, but I want to learn "the right way" to do it.
Your script doesn't look like python. QQ Having to run that external system call looks very messy. Is there a better way to do it? Can you edit the user crontab file directly? You don't need to use any kind of class. This is an appropriate method if that is the best way of accessing the crontab. From the manpages: '''Additionally, cron checks each minute to see if its spool directory’s modtime (or the modtime on /etc/crontab) has changed, and if it has, cron will then examine the modtime on all crontabs and reload those which have changed. Thus cron need not be restarted whenever a crontab file is modified. Note that the crontab(1) command updates the modtime of the spool directory whenever it changes a crontab.''' So, I suggest editing it directly.
Everyblock.com is a Django site also. 
If you don't want to couple Form generation to Models, then there are few options. [Grok](http://grok.zope.org) uses the zope.schema and zope.formlib to do this. This is a "full-stack" framework though, so you might find other aspects on the heavy side. [Pyrmad](http://docs.pylonshq.com) (aka Pylons 2, aka BFG) uses the schemaish and formish libraries to do the same thing. All of those libraries are loosely coupled to the frameworks that they're used with that there isn't any reason why you couldn't just use them in Django or whatever framework you happened to be using. 
Useful. The README says from wordbank import Worldbank wb=Worldbank() but it should be from wordbank import WorldBank wb=WorldBank() Why does this return data: wb.get_country(code='br', indicator='NY.GDP.MKTP.CD') But this does not? wb.get_country(code='br', indicator='AG.PRD.NFOOD.XD') Incomplete data, parsing error, or is it me not understanding? 
I doubted them when pidgin was voted best IRC client.
&gt; and corporate worlds :( I wish... Oh, how I wish... I honestly don't care that this application has been under CVS for the last 12 years... Pick a point in time and just take the hit. Git, SVN, Hg, I don't f###ing care - just let CVS die in a fire already!
Reddit should have a versioning feature so that you can look at the history of a comment that has been edited. 
I wrote a parser once. Its pretty stable and very usuable. Like you, I didnt (still dont) know what I did. I should probably read up these stuff to figure out what I actually did.
I use Python all day long in GIS applications because I develop the Python APIs for ArcGIS. [gis.stackexchange](http://gis.stackexchange.com/) is a good place to ask GIS related Python questions, both for ESRI products and open-source stuff.
Built-in support for mappings, bar none. But, and this is a fair critique, several of the Java people I know will not use Python until the tool support in Eclipse is as robust as it is for Java, language features aside.
I use Python bindings to [GDAL](http://gdal.org) to apply coordinate transformations when preprocessing model input.
If this isn't an argument for RESTful websites then I don't know what is.
nose, particularly the plugin system.
I wrote a simple little module that interfaces with Google Voice to help send mass text messages and phone calls (It also can grab your Google Contacts!). There are others out there, but I didn't like them much so I made my own - [Link to my blog + code](http://everydayscripting.blogspot.com/2009/10/python-custom-google-voice-api.html) Also, I wrote a class that allows you do to do page scraping with cookie support on Google App Engine - [Link to my blog + code](http://everydayscripting.blogspot.com/2009/08/google-app-engine-cookie-handling-with.html). I have written a few GAE apps that use this, so it definitely comes in handy! Not the best code, I will admit, but I was proud of the fact that I figured out how to make it work. 
Like reddit's logo.
Variables referenced in a function but not assigned to are assumed global. Variables assigned to in a function are assumed local.
So you can use a variable in a function as long as you don't change it. Got it. Now, what's the best way to change x globally within the function? It seems kind of strange to me since you can manipulate global lists in a function but not variables. E: Ah, I think I figured it. x = 10 def y(): global x print x x = x + 1 y() Yeah?
I doubted them when GoDaddy came out at the top of best Linux-friendly hosts. Most probably the voters didn't know what to vote for and just voted for the most popular.
I don't know what you're talking about here's an upvote!
Here you go, http://stackoverflow.com/questions/423379/global-variables-in-python 
&gt;what's the best way to change x globally within the function? Return a value: x=10 def y(): print x return (x+1) x = y() Or better yet, def y(x): etc. Better to avoid globals when you can, unless it's something like pi that should never change.
It's a pointless benchmark.
Why are people comparing HTTP caching with stuff like memcache? These things operate on completely different levels and depending on the kind of application you're building you will need either the one thing or the other, or both.
global x (in the function) You can manipulate items in a global list. You cannot assign to a list in a function e.g. a=[1,2,3] 
Yes, that's the way. But now knowing how to do it should not bring you to use it in larger projects. Alternatively you can hand-over x in the signatur, modify it and return it. That would be a more modular and therefore in large apps better approach.
Thank you so much! 
I do aerial photogrammetry and part of our post processing tools involves doing image classification and analysis. Numpy + PIL/GDAL is fantastic for this. I do some ArcGIS python stuff as well, but i much prefer using GDAL binding if possible just for openness and cross platform compatability as we are close to being a 100% linux shop.
Oh yeah, I forgot all about return values. That's perfect. Sorry for my noobiness. =) Here's what I've got now: x = 10 def y(z): return (x + z) x = y(1) print x Does that seem about as clean as it could be? E: Okay, I guess that still doesn't solve my problem. That works when y() is called outside a function, but not when it's called inside one. This is the only way I can get that to work, and it seems way too Matryoshka Doll to be right. x = 10 def y(z): return (x + z) def a(b): b = y(1) return b x = a(x) print x 
I'm doing some realestate stuff, using GeoDjango, but nothing terribly extensive, just some "distance from point" type queries at the moment.
 x = 10 def y(x): print x x += 1 y(x)
That's still pretty awesome though. I just like to know real life applications of things, you know? However, it's not like I'm learning any Python in this program. I would love too, and am seriously looking into taking a programming course after this. 
Oh wow, you just said 3 of my favourite things in one sentence! That sounds like some fascinating things that you're doing. Does the Linux aspect really change a lot for you? Sorry if that sounds like a silly question, I just want to know how different everything is on that os. 
I like how [varnish resolve this issue](http://www.varnish-cache.org/trac/wiki/Performance#VCLSetting)
def a(b): b = y(1) return b You assigned to the variable b which was also an input parameter. I think you meant: def a(b): c = y(b) return c x = a(x) print x You can certainly call functions from other functions, the namespace rules are the same as for any variable. Are you just trying to write an adder? def add(a,b): return (a+b) x=10 x = add(x,1) In your functions you're still treating x like a global. When you're writing a function it's best to pretend there are no variables except those defined in the function whenever possible. Keeps code modular and easier to maintain. I assume also these are just for education, the pythonic way of adding would be x += 1
I guess what I'm trying to ask is whether there's any way to directly manipulate x within a() without using the global statement, which is apparently bad practice though I don't understand why. Here's an example: say I'm making a text adventure game (since that's exactly what I'm doing at the moment, working my way through LPTHW). I have a variable called "gold", defined at the very beginning (gold = 0) to store how much gold the player has, and I have one function called *add_gold()* that looks like this: def add_gold(amt): return(gold + amt) Then, whenever I want to award the player 10 gold, I use gold = add_gold(10) Now, say I have a function for some particular event. def slapped_a_troll(): print "You slapped the shit out of a troll. He gives you ten gold." gold = add_gold(10) This won't work, since "gold" hasn't been defined for *slapped_a_troll()*. What's the best way to handle that? e: saw your edit and it looks like I'm approaching it all wrong, so now I'm confused as ever about the proper way to handle this. 
Presumably to come up with provocative results that go against conventional wisdom. Despite the fact that this comparison is flawed to begin with (although I suppose you could argue that Varnish+ESI could take the place of *some* of what memcached might be used for on the app layer), the data is also highly suspect: I'm not sure why a cache-using version of a website would *lose* to a control that is not using cache, unless your website is doing something faster and less complex than a cache lookup to begin with. What's the use of such a limited site as a test for caches, and what could any benchmark on such a site possibly teach anyone?
What you want to do is skip to the chapter on classes and objects. class Player: def __init__(self): self.gold = 0 def add_gold(self,amt): self.gold += amt def slapped_a_troll(self): print "blah blah" self.add_gold(10) In your main script: playa = Player() playa.add_gold(42) playa.slapped_a_troll() Debatable whether this is the best design if you have only one player, but you never know what the future will bring. The "globals are bad practice" isn't limited to the global statement, it's generally accepted programming practice to limit the scope of your variables as much as possible. There are technical reasons, such as memory footprint, but the main idea is to have your code be as modular as possible. That way when something breaks it's easy to know what, or when you want to upgrade it's easy to do that too. Say you implement the Player class above. You say to yourself "yourfriendlane, this is stupid. I only have one player, I'm sitting here typing playa.this and playa.that when I don't have to". But you do it anyway. Then you come out with the multiplayer version, or add some AI players, and instead of redesigning your program from the ground up, you just add a map for players. That may not happen here, but it happens more often than you think. Note: Apparently reddit renders double underscores as making text bold. Consult textbook on the special method init for objects, it has to be named properly.
This just shows a series of benchmarks for full-page caching. All of the things benchmarked are doing full page caching. The in app caching will always be slower because even if the operation is a no-op the Python interpreter has to be loaded. Comparing Varnish to Nginx/Memcache is completely fair since they are both doing full-page caching and Python is not being loaded. Comparing Varnish+ESI to in-app fragment caching backed by memcache would also be fair.
Thanks! I didn't follow all that much of what you said, but it's comforting to know that I'm not just missing something that should be forehead-slappingly obvious. I guess I'll just forget this sort of thing for right now and come back to it when I get a little further.
It would also be cool if the author benched Varnish+ESI vs Django's template fragment caching. And just for completeness Postgres' buffer cache or MySQL's query cache vs [django-cache-machine](https://github.com/jbalogh/django-cache-machine). I suspect in-app caching will either be slower or have hardly any advantage to make it worth it to add all sorts of complexity to your app.
I say go ahead and use the global keyword. def add_gold(amt): global gold gold = gold + 10 Global variables are a pain, because they can be changed from anywhere at any time, so it can be hard to keep track of them. Generally you want to avoid them. In this case, though, I say go for it. For one thing, you're just learning, and making things too complicated at the start sucks.
To edit the crontab directly it looks like the script needs to be setuid root. This is what crontab -e does - then it drops privileges. If someone knows how to do that then it'd be great. Cron's source isn't *too* hard to read, so I might have another go myself. 
This looks **incredibly** useful for me...I'm forever taking in data in one format, doing something, outputting it in another format(s). So, **thank you** for putting tablib together. However, I just ran through the readme @ https://github.com/kennethreitz/tablib#readme and the tutorial @ http://tablib.org/tutorial.html and I get an error when trying to add a column: &gt; data.append(col=('age', 90, 67, 83)) or &gt; data.append(col=['Age', 22, 20]) depending upon the example used. In either case, I get a traceback ending on &gt; --&gt; 220 raise InvalidDimensions Hopefully I'm just doing something stupid, but if not, let me know if I can provide any feedback to make the 'immediate walk through' experience a bit better. Thanks again! 
I think you're looking at it wrong. You seem to be making the #1 web developer mistake, namely thinking that you are Google. Cost of 1 GB of data bandwidth: $0.05. Number of unique daily visitors to a successful web site: 10 000. Amount of money saved each year on said successful site due to minifying compressed jQuery + jQueryUI: (34 + 18)/1024/1024*365*0.05*10 000 = $9 Unless you're Google, those bandwisth savings are completely irrelevant.
Explain, please.
Fairly extensively used here for remote sensing and ecosystem modeling. It's nice in providing an excellent interface to data (through GDAL/OGR), numerical libraries (linear algebra, fft, the lot), nice plotting through matplotlib (or mayavi if doing 3D lidar/SAR/tomography), testing and documentation thrugh sphinx. What little work to do with vector data I end up doing is also well covered (OGR has the GEOS engine for geoprocessing). Additionally, we can re-use loads of fortran and C code directly, so that we can use loads of code developed by others, or to write the processing bottlenecks in these languages and link to python. Some people link it to R, but it breaks my heart seeing them call R from python X) Some notes which may come useful: http://sites.google.com/site/spatialpython/
The cache using version has some latency to in when compared to using hard coded data. For instance the locmem backend uses locks to prevent concurrency issues. Memcache vs the control adds network latency (however minuscule). 
&gt; Some people link it to R, but it breaks my heart seeing them call R from python X) I use R pretty much exclusively for my 'programming', mostly because I'm comfortable with it, and, generally I'm not doing much more than fancy-dancy data processing. But could you explain this comment? I do use R for spatial analysis (along with grass &amp; gdal), is there a better way of doing it? Or are you saying there's no reason to use R when you're using python?
We are (un?)fortunate enough to have nearly all of our software written inhouse. The move to linux was pretty natural for us because the majority of stuff we do is command line based anyways, infact we started out on redhat 5 iirc but moved to win2000 for various reasons, but otherwise we have been careful to make sure code is portable, or easily portable if needed. Our inhouse built linescanners have recently moved to linux using bitflow capture cards so that we could use a realtime filesystem (xfs) for streaming to disk, NTFS couldn't keep up reliably. Since we are primarily focused on the capture and delivery of raw mosaics we dont have do deal with alot of different customer file formats as we usually will receive a simple shapefile for a area to be mapped, from there we have full control from start to finish which i think is nice, others may disagree and would preferre a more prepackaged system like a vexcel or leica. The only issue we have had regarding linux is the fact that universities/tech schools seem pretty bent on teaching ArcGIS the *application* is seems, at least in my area. We have had a few people with masters degrees in GIS come on and not grasp any of it at all, i wish they would teach more fundamentals of how the gis algorithms work instead of how to use model builder! I fail to see how anyone could really do GIS effectivly without at least some programming ability with something like python.
For those wondering about ESI vs alternatives. I am planning on benchmarking those solutions next. This benchmark is one experiment in a series of experiments testing the assumptions made by REST proponents. I have a list of requirements that a modern website needs and I am testing RESTful solutions against existing solutions. I am presenting the raw data to the community quickly to serve as a peer review of my methodologies. I encourage your critiques.
django-cache-machine serves a different purpose of what this experiment is testing. django-cache-machine or johnny cache would be a sub experiments of in app testing itself. Given that the simple in app test cases for both locmem and memcache are slower than an external HTTP cache proxy, I think we can safely assume that these solutions would be as slow or slower than the direct cache access test cases. 
I agree, full-page caching &gt; fragment caching &gt; db caching, in terms of speed. I don't mean to compare any of those to each other only to investigate different options along the caching spectrum.
Yes? Coordinate transformations... geometry comparisons.
Calculate standard deviation. If you do science, please do it correctly. Just showing average might mean nothing if the times strongly variate.
Yes I will be exploring these types of caching in future experiments. Probably after my data collation experiment (comparing ESI/Ajax/Templatetags, etc) 
Try to iterate over a copy of the original list: for i in lst[:]: if i == 2: lst.remove(i) You can also do that without a for loop, with list comprehension: new_lst = [i for i in lst if i != 2] Note that solution may be faster, and is considered cleaner.
I wrote a script that asks for your mother's name, stores it as a variable, and then prints out, "That's funny, because I heard", yourmum, "was a massive ho". I don't mean to brag, but this shit is fucking groundbreaking.
WTF? GoDaddy wins for best "Linux-friendly" web host? LINODE, BITCHES!
If you're interested in spatial analysis, check out [PySAL](http://geodacenter.asu.edu/pysal), a cross-platform library of spatial analysis functions written in Python. The project is spear-headed by Sergio Rey and Luc Anselin, so you know it's high quality stuff.
 if not kwargs.get('per_page', False): kwargs['per_page'] = self.per_page could be if 'per_page' not in kwargs: kwargs['per_page'] = self.per_page or even kwargs.setdefault('per_page', self.per_page) 
I hear that algorithms thing loud and clear. We're not touching model builders yet, and all this theory is driving me insane. However, I know it's essential so I grin and at least try to enjoy it. I understand it, it's just that I want to know shortcuts already! There hasn't been a single mention of programming at all in this course, and I doubt there will be. Thankfully, one of my profs is a super tech genius, and he's kind of showing me what kind of stuff I should be looking into. Plus, knowing how to write code can do nothing but help me, so I really do want to learn it! 
Don't change the container while iterating over it. In fact, if you do this with a Dict, you will get an error about "dict changed size during iteration".
I know GeoScience Australia uses it for parsing/import/export between various GIS formats.
Thank you! Also, I now have a desire to go to ASU. I understand Canadians do well in Arizona? I'd like to test that out. 
Yes. GeoDjango for spatially-enabled web apps. GDAL python bindings for scripting geoprocessing. Mapnik and Mapserver python bindings for web mapping services. Shapely, RTree, etc for 2D geometry manipulations (http://trac.gispython.org/lab). arcgis scripting module if I'm on ESRI. Python as a generic scripting language for GRASS - a very powerful open source GIS. Numpy for matrix algebra, specifically raster math. LibLAS for lidar data. There's probably more that I'm missing and lots of related/supporting libraries. Suffice it to say - Python IS the glue that holds the GIS world together. 
Liar! Your post does not have a star, you couldn't have edited. Edit: Damn you and your ninja editing claus9 .
Yes - we use it via GeoDjango. We developed http://blipboard.com with it. This [map](http://blipboard.com/redditors) uses GeoDjango.
There isn't a single correct way to solve this problem. If the list is small, this might work: stuff = [1, 2, 3] stuff = [item for item in stuff if ...] Fill in the `...` with whatever filter makes sense.
Only if you add or remove elements. If you only update elements which are already in the dictionary, it won't change during iteration.
The script does everything I need right now, but it's fairly messy. It could be the perfect starting point for any aspiring Python hacker, though. Some obvious things to do: * Clean up code, putting things like print margins in proper constants. * Command line options to specify paper size, margin around barcodes, amount of barcodes, etc. * Calculate the placement of all the barcodes automagically instead of the current hack. Feel free to fork the project on bitbucket and send me a pull request later. As for the backstory... I needed the barcodes for inventory management as one of my businesses is importing clothing that doesn't always have barcodes attached.
Yes! I use python for everything I do that's GIS related. GDAL, OGR, OWSLib, etc etc etc...
Also, the first iteration of the Everyblock code base was released under the GPL, according to the terms of the Knight Foundation grant that funded the development. This code has been forked and is being actively developed (again, funded by the Knight Foundation) as [OpenBlock](http://openblockproject.org/).
He did edit all those times, but he only checked the comment in once.
well said. Little known fact - List comprehensions are faster than lambda/filter because the latter involves a function call and all the expensive ops (stack push, stack pop etc) that comes with it.
FYI: Code sharing is much easier with version control. Try Github, BitBucket, or any of the many others and your readers will thank you. Also allows people to fork your projects and contribute back much easier. Edit: grammar
Very cool project! I'll definitely check it out further next time I have to write Yet Another File Processor ... 
I do a bit of GIS for a number of things dealing with earth sciences. Most of what I do is in R and Python. I've worked very hard to move away from ArcGIS and do all of my work on Linux.
YES! I've pushed myself away from GIS programs and classes because they're disappointingly "this is how to use ArcGIS for X". Even in advanced courses at larger universities. It's a let-down. 
If you need to destroy the entire list; while myList: someoperation(myList.pop(0)) # destroy from head OR while myList: someoperation(myList.pop(-1)) # destroy from tail. You can also use slices or index references in a while loop for a similar effect (warning: you have to `pop()` from the list itself, not a slice thereof, or your while loop never terminates!). # Edit: I realise this isn't exactly the problem you were talking about, but other posters have pretty much covered it (list comprehension, copying the list). I guess if you were really so inclined, another technique is explicitly referencing the index (like so), in a similar fashion to a C-style "for" loop (Python's "for" loop is more like a C "foreach" loop); idx = 0 try: while True: if myList[idx] == 2: # IndexError if out of range.. myList.pop(idx) else: idx += 1 except IndexError: pass
:O it looks like [python.org](http://docs.python.org/py3k/tutorial/index.html) just put up a python tutorial for 3.1 p.s. I'm posting this as its own thread as well
You just blew my mind. As a recent geography/GIS and python convert my only hope is that your 50 years old so I can hope to someday learn all of what you know.
We use it for a viewer for work (Western Australia state gov) (unfortunately still a bit specific, but working on splitting it up and making it more reusable), for users to create their own maps and visualise several GIS layers from all kinds of sources in one place. Code is here, mainly Django backend, jquery frontend =), http://bitbucket.org/fms/firesource - we also use it very heavily for backend map production with gdal to tile and inkscape to create high res output pdf's for printing/sharing. Our backends for resource tracking data collected from vehicles via satellite ([beam](http://www.beamcommunications.com/) iridium, globalstar) is also pure python - reads emails and decodes various formats then saves to postgis for a long term record of everywhere our vehicles have been for the past 2 years now =) Projects used by us (all opensource): [gdal](http://www.gdal.org/), inkscape](http://inkscape.org/), [mapserver](http://mapserver.org/), [geoserver](http://geoserver.org/display/GEOS/Welcome), [postgres/gis](http://postgis.refractions.net/), [jquery](http://jquery.com/) + plugins, [django](http://www.djangoproject.com/) + plugins, [Celery](http://celeryproject.org/) (background tasks), [ubuntu](http://www.ubuntu.com/server) (server), [apache_mod_wsgi](http://code.google.com/p/modwsgi/), [nginx](http://nginx.org/). In future I hope to move from apache to [gunicorn](http://gunicorn.org/) for better management by Django. All stuck together with python (Django) XD
Definitely — I can't point at a public URL, sadly, because the project in question hasn't been launched yet, but we're using GeoDjango + PostGIS + PIL + Cairo + Google Maps to do some (IMO) rather cool custom overlay stuff to model agricultural data, and also linking in with R for some of the analysis work. The beauty of this sort of set up is that although Python proper may be slow, all the hard work's being done by libraries (GEOS, GDAL) and packages that are really, really fast. As others have said, at that point, Python's just the glue.
For those interested, part two of my tests (which I've officially named, "REST vs The World") can be found [here](http://www.reddit.com/r/Python/comments/e38tu/rest_vs_the_world_part_2_data_collation/) This test pertains to data collation. Which is the need for a framework to collect data from various sources and combine them into a single response.
My experiment testing ESI among other things can be found here, http://www.reddit.com/r/Python/comments/e38tu/rest_vs_the_world_part_2_data_collation/ I opt'd to remove Django from the picture in this case to level the playing field. Since I'm not testing Django itself, there was no reason to require it for the test.
I was just at a guest lecture this evening in regards to Python programming for ArcGIS. I'm a GIS major and CS minor hoping to learn more about this subject and it's possibilities.
Looking at the diff, 2 lines changed...
So one of those was the version number line? :)
If you're aching to remove things from an array, stack or queue while iterating through it, go from back to front with respect to the way the construct packs. for i in range(len(lst)-1, -1, -1): if i == 2: del lst[i] edit: edited to remove stupidity. The initial premise holds true. 
The size of the list has nothing to do with it and to the extent that it does, list comprehensions should be preferred for larger lists as they're normally more efficient. The question normally comes down to if the conditional logic in the loop can be readably written as an expression or not and if a single expression can do any processing necessary. 
Thank you! Someday hopefully soon I hope to start taking a look through Django tickets myself and seeing what I can help with. I don't feel like I'm qualified yet - I keep realizing how ignorant I am while developing my own Django apps.
PCLinuxOS shouldn't be runner up for best distro either. 
 x = 10 def y(x): global x print x x = x + 1 y()
GDAL &amp; OGR, Shapely (is nicely). Used for an avionics mapping system. Couldn't get along without it.
&gt; (such as PLT, which would be the healthiest for just starting scheme &amp; attempting something useful) [Chicken](http://call-cc.org/) seems to be pretty useful -- they've got a fairly large number of [eggs](http://wiki.call-cc.org/chicken-projects/egg-index-4.html) ready for use. 
Chicken **is** pretty useful, but for someone *starting* scheme, I think PLT has a greater amount of support for what you might get confused upon (such as the syntax &amp; macro steppers). 
I'm a python novice. What code does this generate? Code128?
Iterating and adding or removing items from a sequence turns into a quick pita. Hopefully you don't just copy and paste what I am about to put up but hopefully you learn something. It's a 2 step process just to keep things simple. The first thing you do is identify everything that needs to be deleted. Then you basically remove everything you've identified. This is easily seen in the mutate_remove function below. I chose to use a callback for the last parameter as it extends the function nicely. You can see the primitive examples below for an idea. One last word of advice. Learn or die. My code isn't gospel and isn't meant to save the world. It comes with no warranty and hasn't been thoroughly tested. You've been warned. def mutate_remove(seq, callback): """ The sequence passed in is expected to be mutable and have a remove method. The callback is expected to accept 1 argument (the current item in the sequence). If the callback return evaluates to True, it will be removed. Since the sequence passed in is expected to be mutable and will be modified in place, it is not returned. """ _null = lambda: 0 # step 1: identify items for removal with _null for i, item in enumerate(seq): if callback(item): seq[i] = _null # step 2: remove all _nulls from the sequence while _null in seq: seq.remove(_null) def my_int_callback(item): return item % 2 def my_str_callback(item): return 'a' in item # example usage 1 lst = [1,1,1,1,1,1,1,1,1,2,3,4,5,6,7,8,8,8,8] mutate_remove(lst, my_int_callback) print lst # [2, 4, 6, 8, 8, 8, 8] # example usage 2 lst = ['abc', 'alligator', 'red', 'blue', 'yellow', 'rat', 'cat', 'pillow'] mutate_remove(lst, my_str_callback) print lst # ['red', 'blue', 'yellow', 'pillow'] 
They could give PyCharm a try.
yeah maybe like 2043...sigh
Are you just trying to pare down your list? If so, jleguen's [comment](http://www.reddit.com/r/Python/comments/e363a/question_removing_objects_from_a_list_while/c14x9ok) about list comprehensions is right, but really this is only legible if the condition is simple enough. You can also do this: &gt;&gt;&gt; lst = [1, 2, 3, 4, 5] &gt;&gt;&gt; l = filter(lambda x: x != 2, lst) &gt;&gt;&gt; l [1, 3, 4, 5] Again, this is only legible if your condition is pretty simple. But filter will take any callable, so instead you can do this: &gt;&gt;&gt; def myfunc(x): ... return x != 2 ... &gt;&gt;&gt; l = filter(myfunc, lst) &gt;&gt;&gt; l [1, 3, 4, 5] This could allow you to check for multiple conditions. Just write a function that'll return True if you want x in the final result. At any rate, filter is a pretty Pythonic way to mutate a list. Also, I'm pretty surprised at all the suggestions to iterate over range(len(lst)) or a copy of the original list. list.remove is an O(N) operation, so if you do this on large lists you're begging for performance problems. It's typically worth it to try to find a faster solution.
 &gt;&gt;&gt; x = 1 &gt;&gt;&gt; x += 1 &gt;&gt;&gt; x 2 &gt;&gt;&gt; 
Yeah, I built and open sourced a GeoDjango App I built about a week ago for the Stewart/Colbert rallies in DC - it was meant to help Redditors identify one another at bars and stuff, very much like a shallow Foursquare clone. **Site**: [http://drinkkit.com/](http://drinkkit.com/) **Source**: [https://github.com/ryanmcgrath/drinkkitcom](https://github.com/ryanmcgrath/drinkkitcom)
Ha, crazy, didn't know you were a Redditor. Figures I'd find you in the Python section. ;) Tablib is cool, I hadn't seen it yet. Could potentially use it in the future...
Probably Twython. It's not a significantly advanced piece of code (it's a Twitter API wrapper), but the act of managing the community and interfacing with the people using it is one of my favorite things, and is a huge part of why I still actively maintain it to this day. ;) [https://github.com/ryanmcgrath/twython](https://github.com/ryanmcgrath/twython)
Note that there are also shortcuts like this for other operators, like `*=`, `-=`, and `/=`. [Here](http://en.wikibooks.org/wiki/Python_Programming/Operators#Augmented_Assignment) are some more examples.
Do you even know Python? list.remove removes an element, the first it finds equal to the argument. You want `del lst[i]`. Your range is off by not just one, but several. When you want the reverse indices of a list, use `xrange(len(lst)-1, -1, -1)`. You shouldn't be checking if `i == 2`, you should be checking if `lst[i] == 2`. You posted three lines, and every single one of them is wrong. If you can't be arsed to actually test your code, don't fucking post.
I can't begin to thank the NumPy people enough for making it possible to make fast, interchangeable, memory-efficient, C-compatible data analysis across systems. Python-as-glue FTW.
This is relevant to my interests, because I'm implementing a similiar system myself. Finally some useful reportlab example... Good job and thanks!
&gt;this shit is fucking groundbreaking. So is your mom.
For the record, this syntax is common in most languages.
x+=1 is shortage for x = x + 1 (and it works with */, -=, **= ... the same way)
I agree. Git, or any DVCS, is good for projects that are highly likely to fork, where many programmers could be working on the same problem each with his/her own idea for a solution, and they're away from the main repository that they can't afford making commits all the time, or they can't afford or don't want to pollute the repository by having each programmer have his/her own branch in the repository. But in a small organization where all the programmers are in a single office, a specific person is assigned to tackle the problem, and the solution is discussed well enough, SVN is just perfect. No need for an additional step for a push/pull request to/from the main repository.
yeah, those system management tools should be make altinstalled and in their own virtualenvs...
It started with C.
def \_\_int\_\_(self): Use backslashes.
Actually, Ken Thompson got it from TMG, which was the compiler definition language that he wrote the B compiler in. McClure (who wrote TMG) got it in turn from ALGOL's `+:=` (ALGOL also had `x +=: y` which does `y = x + y`!^1 ). In B, it was originally =+, and was changed in 1976 during C's development. ---- .^1: For systems which couldn't handle the symbols, the original ALGOL-68 had `+:=` as `PLUS`, and `+=:` as `PRUS` (R for right...). These were later renamed `PLUSAB` and `PLUSTO`.
Just curious - why the "never xml"?
http://www.thefreedictionary.com/shortage
One thing I miss in python is the ++ notation. It's not really needed regarding pythons automated looping/iterator design, though it comes in handy.
From the comments in the code - looks like EAN-13.
Not implemented for reasons of clarity.
Our primary development language at CloudMade is Python. We use Mapnik and PIL for tile and staticmaps rendering, GeoAlchemy and inhouse wrappers for PostGIS data access and processing in vectormaps and geocoding. Some in-house scripts also actively use Shapely and Proj.4 bindings (pyproj that is). Most of these services are tied together using Twisted.
Take a look at http://www.freshbooks.com - they have a pretty simple API.
In newer versions of Python, in place manipulation is sometimes different than x = x + 1. Case in point: an in place add with a list is the same as list.extend(iterable). def foo(l): l += (1, 2, 3) l = [] foo(l) print l # [1, 2, 3] The associated hooks simply add an "i" before the name of the regular hook name: \_\_iadd\_\_ for +=, \_\_iand\_\_ for |=, etc.
Dude, obviously text editors and IDEs can contain a limited number of x=x+1 expressions, and when you start running low, you switch to x+=1.
the notation we're talking about here is called http://en.wikipedia.org/wiki/Augmented_assignment and is in fact very common in many languages, not just Python
Thanks. Since I posted I've also found Zoho Invoice (http://www.zoho.com/invoice/index.html), which seems similar to freshbooks. I think both these might be useful, but I think [Tryton](http://en.wikipedia.org/wiki/Tryton) might be closer to what I was originally thinking. I want to be able to maintain ledgers locally. I'm okay with pushing out a bunch data for a 3rd party to handle invoicing every month but the ledger can't be off-site. Anyone have experience with Tryton? 
As vsajip already pointed out, it's EAN-13. Prefix the code with a 0 and you've got UPC.
If you find it useful and develop it further, consider sending me a patch or a pull request :)
Here's a stupid trick: &gt;&gt;&gt; from itertools import count &gt;&gt;&gt; i = count().next &gt;&gt;&gt; i() 0 &gt;&gt;&gt; i() 1 &gt;&gt;&gt; i() 2
No idea about pexpect broken in 2.7, sorry, but a quick note to say i used to use pexpect for ssh automation until i discovered python-fabric. Depending on what you're trying to do it's probably a better option for ssh controlling. (and a *lot* more robust than pexpect).
Also because it wouldn't really fit with python's model of assignment as a statement, rather than an expression. The only value `++` really adds is where its used inline in an expression, which isn't really an option in python.
haha, nice this is why I like python
 while _null in seq: seq.remove(_null) `lst = [x for x in lst if not callback(x)]` would be faster than this shit. On each iteration of your loop it linearly scans the list in search of the element, then `remove` scans the list to find the element once again, then moves all items above the element down one step. In other words, performs in O(n^2 ) in worst case, and slower than straight copy with filter even if only one element has to be removed. Not to mention that in most cases you can rid of `callback` in list comprehension, speeding everything up significantly yet again. Seeing this kind of utterly retarded 'optimized methods' makes my blood boil. You should abandon programming forever and go into business administration or something like that. btw, `_null = object()` is the preferred method for creating unique sentinels.
Disclaimer: I have never used pexpect, so I could be way off here. Just making a guess, "from pexpect import pexpect" instead of "import pexpect".
... which accounts for one of the best Python WTFs: &gt;&gt;&gt; t = ([],) &gt;&gt;&gt; t[0] += [2, 3] Traceback (most recent call last): File "&lt;input&gt;", line 1, in ? TypeError: object doesn't support item assignment &gt;&gt;&gt; t ([2, 3],)
Yeah, this is definitely the way to go. One thing to note though it that it won't mutate the original list, but rather makes a (filtered) copy and assigns it to the same variable. This is only an issue if you've other references to the list that need to be modified, which chances are is not the case here. If it is, an easy solution is: &gt;&gt;&gt; l[:] = [x for x in l if someCondition(x)] (The [:] is slice assignment to the whole slice, effectively meaning "replace the contents of l with the list on the right)
I can't think of any that I'm particularly proud of right now, but I do like yours. Very nice.
Yeah, except that the former is slower than the latter, so you might not want to use it in performance-sensitive code.
I love the for x in xrange(integer): loop. I don't know if any other languages use it, but it's super handy. I just wish if you're looping through an array for i in somearray: that you could call i as well as like iter(i) to return the number of times you've looped (without adding additional itterators)
Makes me wanna find any code that I've done it the hard way and fix it.
that's why we have 64-bit operating systems
DO IT!
Is it really? 
Also, os.walk will give you a lot of code to rewrite.
With your idiom python has to build a tuple. Besides that, with the or chain, you can take advantage of [short circuit evaluation](http://en.wikipedia.org/wiki/Short-circuit_evaluation#Avoiding_the_execution_of_second_expression.27s_side_effects) and order the three conditions from the most likely to the most unlikely. These concerns make sense only in performance-sensitive code, of course. I really like your idiom and I'll use it: short and clear!
Sorry, my comments were purely cynical and only display my confusion with programming in R :-) I find working with data is very simple using python, as well as most numerics. I also find plotting easier with matplotlib, but that's probably due to its closeness with matlab. For some particular problems, being able to pipe data in and out of R easily is indeed very nice (i.e., to use some specific statistical method already implemented in R). Horse sfor courses can of thing, if you want.
Use of the any() and all() functions are idioms I enjoy. Your idiom would be better as: if not all((foo, bar, baz)): ... Other examples: if all(condition(item) for item in list): ... if any(condition(item) for item in list): ... etc 
Very interesting. Good to see that I'm not the only one finding that the "out-of-the-box" solutions fall short.
You've got to be kidding me. He's using *python*! If it's performance-sensitive, he's already made the wrong choice. The biggest reason to use Python is for clear code, so it makes perfect sense to sacrifice negligible overhead for readability. 
 not all((foo, bar, baz)) is not the same as None in (foo, bar, baz) as they handle empty objects (strings, lists, tuples) differently: None in ('','bar','baz') == False not all(('', 'bar', 'baz')) == True EDIT: forgot to put a 'not' in the last example
For all practical purposes: no. More precisely: if you're running such performance sensitive code that the ~0.4 *micro*seconds it takes to build a measly 3-item tuple matters, then you want to be doing your optimisation by rigorous profiling and benchmarking to see which parts of the program are actually bottlenecking you (and maybe rewriting those parts in something like C). Not by randomly unrolling small tuples. Remember [Knuth](http://en.wikipedia.org/wiki/Program_optimization#When_to_optimize).
Can anyone suggest other clever ways to make my code more inefficient and harder to read?
Nice find.
Fantastic! I'm glad to hear that it would be useful to you :) Sorry about the syntax error, I need to update the Readme. The new syntax is: `data.append(col=(90, 67, 83), header='age')` The documentation over at http://tablib.org is up to date.
&gt; If it's performance-sensitive, he's already made the wrong choice. Downvoted. Most of the code in my performance sensitive work is still Python, as with other people I know who are doing the same stuff. I could put almost all of my projects into C or C++, but it's not necessary. In most places, a language's execution speed is not the bottleneck. Algorithm choice, design, and implementation is typically more important than language choice. Poorly written C code usually isn't faster than smartly written Python code.
You are right, but your example is not. None in ('','bar','baz') == False all(('', 'bar', 'baz')) == False
Haha, I'm on reddit occasionally. :) Mostly a consumer over here though. 
I really want to get to know Linux a lot better than I do. A lot of people seem to say the enjoy it. 
Thank you thank you! I'm planning on getting back into government (Canada) when I'm done school, so this was really interesting!
And, updated.
For the compare, I went of his code, he's not checking lst[i], so the code I posted didn't either. Beyond that, yeah, I should vet my code a bit better before I post.
Why is this harder to read? In some applications performance doesn't matter so much, why use Python in the first place if performance is so important?
Note that your code is actually slightly different from what you're replacing, because "`in`" tests by equality, rather than by identity as "`is`" does. so in the presence of certain objects, you can actually get different results: &gt;&gt;&gt; class AlwaysEqual(object): ... def __eq__(self, other): return True ... &gt;&gt;&gt; foo, bar, baz = 1, 2, AlwaysEqual() &gt;&gt;&gt; None in (foo, bar, baz) True &gt;&gt;&gt; foo is None or bar is None or baz is None False 
Check if the destination host got the "msvcr90.dll". Since py2.6 ... this mvc runtime is needed since python is compiled with recent MSVS. I've got same kind of trouble with py2exe in the past ... but not sure 
&gt;With your idiom python has to build a tuple. Not neccessarily. If it only contains constant values, python will create the tuple at function creation time, and will always use a reference to it. Eg: &gt;&gt;&gt; def foo(x): ... return x in (1,2,3,4) &gt;&gt;&gt; dis.dis(foo) 2 0 LOAD_FAST 0 (x) 3 LOAD_CONST 5 ((1, 2, 3, 4)) 6 COMPARE_OP 6 (in) 9 RETURN_VALUE Note the LOAD_CONST for an existing tuple, rather than building one from scratch. The tuple approach is probably going to be faster in this case, as the loop is done in pure C, rather than checking each value as a seperate python opcode. Though in this case, it sounds like the tuple is dynamic, so this probably doesn't apply.
when do we start saving on performance by not addressing these sorts of non-issues every step of the way
When you say "the destination host got the dll", what do you mean? I've downloaded the correct DLL, but the tutorial for py2exe mentions alternative ways, with manifest files and whatnot. 
if the machine where you try to run your "exe" ... But yes, you must provide the manifest file too (like explained in py2exe wiki), not the dll only
Alright, I'll look into that. Thanks for being patient with an idiot. :P
may I refer you to pypy?
True. Sometimes you do want a boolean, other times it's much safer to check for None explicitly. I shouldn't have suggested the semantics were the same, although there's a substantial overlap between the cases where you need one and the other.
Hijacking a bit just to say that although this answer is correct, we really need to discourage an O(N^2 ) solution when faster ones are available. See [my comment](http://www.reddit.com/r/Python/comments/e363a/question_removing_objects_from_a_list_while/c14ylnz) below.
Good point, but you actually don't need to slice: &gt;&gt;&gt; lst = [1, 2, 3, 4, 5] &gt;&gt;&gt; lst = filter(lambda x: x != 2, lst) &gt;&gt;&gt; lst [1, 3, 4, 5] &gt;&gt;&gt; lst = [1, 2, 3, 4, 5] &gt;&gt;&gt; lst = [x for x in lst if x != 2] &gt;&gt;&gt; lst [1, 3, 4, 5]
The above suggestion was to micro-optimize a conditional that involved checking if 3 things were None. Let's say this involves a 2x speed up for this particular line. Do you think this would trump the speed up of moving to c? That's all I'm saying. &gt;In most places, a language's execution speed is not the bottleneck. We probably operate in different universes; in machine learning this isn't really true, though it depends. Fortunately most of the matrix algebra/optimization stuff is outsourced to c, anyway. &gt;Poorly written C code usually isn't faster than smartly written Python code. This is a straw man, of course. 
&gt; Why is this harder to read? It's not as clear what its intent is as the second version. I assume the OP included the second version for this reason. &gt; why use Python in the first place if performance is so important? Sure, if this idiom had something to offer other than mild obfuscation and smartypantsness I'd happily ignore the performance consideration.
Agreed, iterating over a list to remove elements from it is not the clean way. But I assumed OP wanted to *also* do something (print, compute something) while in the loop, so instead of just saying "don't do that", I provided a hackish way to solve his problem. Now I agree this is not good python, and he'd better create a new list with filter or list comprehension (or even generation if it's really big) and only *then* do something on the remaining items. Maybe should have said that in my original reply.
There are some really great ones over at [Idiomatic Python](http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html)
I would use BeautifulSoup and urllib2 for something like this. BS is for scraping and dealing with broken HTML. If your data is already in tables it shouldn't be difficult to scrape. urllib2 supports basic HTTP authentication. * BeautifulSoup - http://www.crummy.com/software/BeautifulSoup/ * urllib2 - http://docs.python.org/library/urllib2.html
Once I started using [generators](http://docs.python.org/tutorial/classes.html#generators), coding became a lot more fun for me. Decorators can also come in handy.
I needed to generate a long (about 150,000 items) Javascript array using data stored in a python dictionary of dictionaries. I had a for loop to job it but after seeing the posts a week or two ago about using join I came up with this: data = 'var data = [\n' data += ',\n'.join(['[new Date("%s"), %s]' % (self.get_time_string_from_timestamp(k), ', '.join([server[1] for server in sorted(v)])) for k,v in sorted(self.combined.iteritems())]) data += "];\n"
I scraped roundabout 400 pages off a website with BeautifulSoup. A nice little helper is http://www.selectorgadget.com when it comes to CSS selectors. There is a video tutorial at http://railscasts.com/episodes/190-screen-scraping-with-nokogiri for a ruby gem called nokogiri. If you understand the concept it is not that hard to transfer the idea over to BS in Python.
The point is that that doesn't update in-place - it creates a new list and rebinds `lst` to point to it. So if you intend to mutate, it won't work. Eg. &gt;&gt;&gt; lst = [1, 2, 3, 4, 5] &gt;&gt;&gt; lst2 = lst &gt;&gt;&gt; lst = filter(lambda x: x != 2, lst) &gt;&gt;&gt; lst [1, 3, 4, 5] &gt;&gt;&gt; lst2 [1, 2, 3, 4, 5] &gt;&gt;&gt; lst == lst2, lst is lst2 False, False Whereas if you do: &gt;&gt;&gt; lst[:] = filter(lambda x: x != 2, lst) &gt;&gt;&gt; lst [1, 3, 4, 5] &gt;&gt;&gt; lst2 [1, 3, 4, 5] &gt;&gt;&gt; lst == lst2, lst is lst2 True, True ie. it actually mutates the original list, so the change is seen by all names accessing it.
`min` and `max` with the `key=` parameter (along with a few more, but I just remember these): AGES = {'Martha': 40, 'Mary': 30, 'Jean': 20} youngest = min(AGES, key=AGES.get) # Returns Jean oldest = max(AGES, key=AGES.get) # Returns Martha The above roughly translates as "iterate `AGES` (which will yield the keys from `AGES`), using `AGES.get` as the key function, which will be passed the value being iterated and is expected to return another value, which will be used to compare instead of the value yielded by the iterator" (i.e. sort keys by the order of their associated values) &gt;&gt;&gt; sorted(AGES, key=AGES.get) ['Jean', 'Mary', 'Martha'] &gt;&gt;&gt; max(AGES, key=len) 'Martha' # The longest key Also all the crazy stuff that can be done with itertools. The second form of `iter()`, that accepts any callable and a sentinel value to create an iterator: # Roughly equivalent to iter(sys.stdin): line_it = iter(sys.stdin.readline, '') Good for adapting older code to work with iterators.
Fair enough, but even then there are typically better solutions. For example, adding to the end of a list is pretty fast. So you can do your processing (e.g., print i) then tack i onto the end of a result list. Then the solution is O(N). Anyway, good stuff, just wanted to make sure the efficiency conversation happened.
If I utilized these resources (are they considered libraries?), would I be able to have a list of my url's and have it cake through? Python seems extreemly diverse. Also, for a mediocre javascript/ as3 programmer, do you think I'd be able to get this done in a month of after work developing? I'm very interested in learning python. This isn't really in my job title (designer), but it'd save the company time and further my skill-sets.
Ah, excellent point! Have an upvote.
I would recommend taking a look at the [mechanize](http://wwwsearch.sourceforge.net/mechanize/) library. It should make logging-in relatively straightforward, as well as scraping. BeautifulSoup will work nicely for the parsing itself, as others have said. The main recommendation I would make for actual development is to download several pages and just load them from disk when you are working on the parser. It will make iteration much faster. Then you can get the scraper working separately. It's definitely doable in a month of after-work, and if you aren't afraid to ask for help when you get stuck, here or on StackOverflow.
 x, = y instead of x = y[0] when surely `len(y) == 1`.
Anyone here use [Mapfish](http://mapfish.org/)? It sits on top of Pylons. We're going to use it in the coming months, looking forward to it.
Thanks for the encouragement and resources. And here comes a real dumb question: when working in python, do you use a standalone compiler? I've visited python.com and noticed the downloads, is that just for a library? I apologize for the dumbness, like I said my programming experience is limited. **edit:** just read more on python.com and realize I need the interpreter. 
nice!
Another quick guess: did you name your python script "pexpect.py"? Then you would be importing your script instead of the real pexpect module.
My first idea was exactly yours except I had lst[:] =. I did not optimize for speed or performance or post that solution because my main focus was staying away from copying or creating a list at all. How would you modify a list in place without copying or creating a new list and making sure all references to that list are updated? 
Writing all your code on one line does not make it run any faster.
You could try http://xpenser.com/ for an easy way to keep track of your expenses and income (as well as time and mileage). The backend is mostly python, and you can use the xml/json APIs to upload/download data.
Any more details? Libraries used?
They're modules, fyi
I would suggest [scrapy](http://scrapy.org) for it, specially if you are just starting with python. It has a very good [documentation](http://doc.scrapy.org/) that will make your life much easier. Also, this might be useful for your case [simulate user logins](http://doc.scrapy.org/topics/request-response.html#topics-request-response-ref-request-userlogin). Edit: small text fix.
&gt; How would you modify a list in place without copying or creating a new list and making sure all references to that list are updated? If you want to have all references updated, then yes, '`lst[:] =`' is correct. Though fairly good chances are that you are doing it wrong if you have this requirement, but OK, sometimes you might need just that. Why would you want to avoid copying if you are not concerned about performance? btw, I have a terrible suspicion that '`lst[:] = [...]`' is the fastest approach achievable in pure Python, by a wide margin, despite copying.
This is hardly clean and probably discouraged, but I think it's pretty neat that it's possible: ('some_method() returns False', 'some_method() returns True')[some_method()]
I agree with omab, Scrapy is an excellent framework for screen scraping. Also, Scrapy has a very nice community behind, the scrapy user list is very active, and is being used by a lot companies &amp; projects around the world (http://dev.scrapy.org/wiki/CompaniesUsingScrapy)
Heh, no worries. Yes, you need the interpreter. It should come with the standard library -- however, this does not include libraries like BeautifulSoup or Mechanize. You will need to download those separately.
I used openFrameworks (via a SWIG-based python wrapper) to do the graphics. It has nice quicktime and image support. pyserial to talk to the hardware, numpy to do some matrix math... Those are the main ones that come to mind. 
Cool. What sort of image processing do you need to detect the "puck" positions?
wow impressive! i would love to play with that table in python 
Actually, list comprehensions are faster than for loops.
But running join instead of adding at the end of one string in a loop does.
What's wrong with: 'some_method() returns True' if some_method() else 'some_method() returns False' 
If you really want to put it into an Excel file, you can use the xlwt module: http://pypi.python.org/pypi/xlwt http://www.python-excel.org/
how long did it take you?
For building lists, perhaps. But if you really need infinitesimel performance improvements like this more than code clarity, you should consider using another language (or e.g. Pyrex).
Good job!
Uber kool :)
Why the hell would anyone want to down vote this? At least leave a comment if you do! 
My example wasn't the best. There's a difference if, for instance, the two possible outcomes involve method calls. In that case, (foo(), bar())[do_we_want_bar()] calls both `foo` and `bar` but only returns one of them. OTOH, bar() if do_we_want_bar() else foo() only executes the applicable one. I've had a need for the former version a few times, though I can't remember why exactly. Again I'd hardly advise people to use it, but I thought it was interesting that the fact that `1==True` (and `0==False`) allows for something like this. Oh, also I believe the `foo if bar else baz` construct is Python 2.5 and above, but I could be in the wrong here.
That is awesome, I both envy and hate you.
I can understand the first version perfectly well and crucially I can read and understand it faster. Obviously your opinion may differ from mine but the downvotes suggest that broadly speaking people find it readable.
I wasn't aiming for performance in terms of speed or least amount of lines. I got stuck on the idea of modifying a list in place. For some reason beyond me now that was my motive. In the end I discovered several ways to do exactly this but thought my code above was the most pythonic and met my goals. If I had to modify a list in place without creating or copying the contents the clearest solution I could come up with is identifying what needs to be deleted in the first pass then deleting them in the second. No tricks or shortcuts just readable and imho pretty pythonic if not a speed demon. To address your question about copying Vs performance and why I pretty much chose my solution over a list comprehension to a slice assignment was based on memory. I know the ops list was pretty simple and just a few bytes but I just kept thinking what if the list was really frigging huge? So at the sacrifice of being the fastest solution, my aim was to be clear and readable, functional and without penalty to memory.
Very pretty, I like it a lot... Know anyone who does particle simulation? This would make a fantastic front-end!
What do you win by doing this? Just 2 characters less typing?
So aweseome!! I was actually there just 2 weeks ago and played with it wondering how it was made. I work on a open source python project to create these kind of installations for multitouch and other input like fiducial. checkout http://pymt.eu. How did you get the hook up for doing an installation at the museum of science and industry; do you work on things liek this professionally? So awesome!!
I was thinking of a similar sort of interactive table at Heineken which you control with your glass of beer. It had a camera directly over the table. If you lifted the glass high enough it would not detect it, so it must have been looking for circles within a certain size range. It was less responsive and accurate than this one though.
Is the project open source? Can we look at the code and what-not to see how you did it?
thanks, I switched to if 'per_page' not in kwargs: kwargs['per_page'] = self.per_page I was a little bit confused on exactly what I wanted when I wrote that line I think haha
Writing the actual app was a few months. The hardware was more tricky, because there is a Tesla coil near the exhibit, and I needed to tweak the hardware to make sure the Tesla coil wouldn't mess up the sensors in the table. 
Thanks, I fixed the capitalization issue. I don't think AG.PRD.NFOOD.XD is a valid indicator code for the World Bank API. http://data.worldbank.org/indicator/NY.GDP.MKTP.CD is a valid page, but http://data.worldbank.org/indicator/AG.PRD.NFOOD.XD is not. Did you just make up that code yourself?
&gt; but I just kept thinking what if the list was really frigging huge? Running your algorithm on a list of 1 million integers (~20 megabytes (or only 4MB, if all of those are below 256)) can involve up to 10^12 operations. That's a couple of hours if you'd written it in C++, or a couple of weeks using Python as it is. 100x more integers (that's when you begin hitting the available memory limit for 32bit apps) would slow stuff 10,000 times yet again, so we are talking about **CENTURIES** here. I repeat: this kind of "optimization" performed without having a slightest idea about what's going on makes my blood boil, and for a good reason. It is stupid. Don't do it. The most Pythonic way is the one-liner with a list comprehension. You could try to write a straight C-like loop, but I'm afraid it would turn out a bit slower than list comprehension/replacement, because the latter is efficiently implemented in C. Your solution is just plain wrong. It doesn't have a single redeeming quality. Stop writing code like that.
I've had great luck just using pyGTK and matplotlib. The end result will look much more polished than LabVIEW but you'll need to be a pit more programming savvy to get things working.
So I'm a senior undergraduate Computer Science major with an interest in stuff like this. To get your job or a similar one, do I go to graduate school or do I start working? Where? Sorry to hijack the thread, but this is sooo cool and I admire the OP.
pexpect works fine on py2.7, so i expect nilsph hit the nail on the head. a similar issue bit me in a large project i work with. we had a module named abc which causes no issues whatsoever on python 2.6, but with the inclusion of the abstract base classes in 2.7 imports started to fail.
Basic examples but list comprehension is awesome. &gt;&gt;&gt; print map(lambda x: x*2, [1,2,3]) [2, 4, 6] &gt;&gt;&gt; print map(lambda x: x.upper(), ["one","two","three"]) ['ONE', 'TWO', 'THREE'] &gt;&gt;&gt; print [x.upper() for x in ["one","two","three"]] ['ONE', 'TWO', 'THREE'] &gt;&gt;&gt; 
THanks for the kind words! Your project looks great! I'll give that a try. I do this professionally out of a studio in Brooklyn. [Here's my other stuff](http://www.pattenstudio.com/projects). I've been doing interactive table stuff for awhile (since 1999), and had a crude idea for a chemistry exhibit online somewhere. They saw that, and the [Audiopad](http://www.pattenstudio.com/projects/audiopad) (2002) and approached me about doing an exhibit.
It's not open source yet, but I hope to be able to change that and get this used in more places. In the meantime, I'm happy to answer any specific questions.