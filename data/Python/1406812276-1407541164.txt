tuples can also be made with mutable objects: &gt;&gt;&gt; t = (1, [2], 3) &gt;&gt;&gt; t[1].append(2) &gt;&gt;&gt; t (1, [2, 2], 3) The real reason is linked by /u/ondra, above.
anyway, you can always tuple(f(x) for f in y), but shouldn't unless you really need a tuple :P
Non-python spam.
I'll just using [Flask-SuperAdmin](https://github.com/SyrusAkbary/Flask-SuperAdmin)
non-python.
&gt; lookup complexity seems to be same as in set best 1, worst n This is not correct. Set's lookup has ammortized O(1) worst case complexity, while this has O(n). This also has O(n) average case, while set has O(1).
What's been said is good. These may help as well: What's the difference between a Python module and a Python package? question on StackOverflow http://stackoverflow.com/questions/7948494/whats-the-difference-between-a-python-module-and-a-python-package "Different languages have different definitions of package and module." http://stackoverflow.com/questions/3680883/whats-the-difference-between-package-and-module
Learn Python The Hard Way: http://learnpythonthehardway.org/book/
Is there a way to avoid "x for x in" in (x for x in iterable if predicate(x)) ?
That is for Python 2.7.
My bad, @pydanny is right. I read the old version and for some reason I thought this one covered Python 3.
Django for web development might be a start.
The `any` function is a more general way of doing this if you need more than just the equality checking you can do with `in`. `any(x == 10 for x in s)` is the same (when `s` is a generator expression) as `10 in s` , but longer and less convenient. However, `any` is great if you instead wanted `any(x &gt;= 10 for x in s)` If you wanted to get the next value that is at least 10 instead of seeing if there is one, you would instead do `next(x for x in s if x &gt;= 10)`
I'd say the requests package. (And arrow for time handling). They replace standard library packages' functionality with cleaner abstractions and more functionality.
&gt; Both approaches seem to have strong arguments. In more detail, what are the benefits of enforcing/not enforcing a source encoding? There is only one approach. By default, the source is decoded as ASCII. This is not Java, where some platform default is used if the source encoding is not specified. 
I understand the needs to make awesome list for flask, or another framework. But isn't /u/pydanny already build [this](https://www.djangopackages.com/) for django ?
Just to be explicit, what would that approach be?
This should be moved off google documents.
UTF-8. If it's ascii, it'll work as-is. If there are non-ascii characters, UTF-8 is the best choice.
* Always enforce the encoding you are actually using. Explicit is better than implicit. * Use the same encoding for all files * Use an encoding that supports all the characters you need. Readability counts. * Don't expect people to deal with some obscure encoding but use a well known one, like UTF-8 * Any decent Python editor/IDE should be able to use the coding line to get the encoding for saving the file, or at least to be told what encoding to use for all Python files.
You have to specify the encoding if it isn't ASCII. In Python 2, that is. The platform doesn't enter the equation.
Do it like [Python 3 does it](https://docs.python.org/3/tutorial/interpreter.html#source-code-encoding): &gt; By default, Python source files are treated as encoded in UTF-8. Enforcing this with the coding comment (incase you expect the file might be run with python 2) is fine. If you never expect to run the code under a Python 2 interperter, then the coding comment is unneeded unless you wish to specify it for an editor like vim or emacs. &gt; However, I recently was told that forcing the source encoding to UTF-8 can be bad for cross-platform compatibility, since Windows doesn't default to UTF-8. Fuck any code editor that can't handle UTF-8, and calls itself a code editor (yes, this means Notepad is **not** a code editor).
&gt; However, I recently was told that forcing the source encoding to UTF-8 can be bad for cross-platform compatibility, since Windows doesn't default to UTF-8. First of all, as far as source files go, Windows doesn't default to anything, programs do. Notepad in Windows-7 defaults to UTF-8, if your code editor is worse than Notepad then you probably should get another code editor. Maybe the person who told you that was thinking about encoding of stuff like file names in syscalls? I'm not sure what's supposed to be the alternative approach then. If you use ASCII encoding for source files and manually encode Unicode characters with backslash escapes in bytestrings you still has to decide _how_ to encode them, in UTF-8 or UCS-2 that Windows expects. And the correct way is, of course, to just use unicode literals instead. Also, you can't "not enforce" the source encoding, if you don't specify UTF-8 then it's 7-bit ASCII. Which is good, because I can't imagine how an _unspecified_ source encoding can be good for cross-platform compatibility -- if someone runs your script on Windows and Python there somehow decided to use UTF-16, they get garbage printed out?
The first thing I point people to (who have some programming experience) is: http://www.diveintopython3.net/ Not a TON of exercises, and it starts with the basics, but its a great foundation.
After many weeks of hard work from multiple contributors the Boto project now supports Python 3! Please try it out and let us know what you think. PyPI Link: https://pypi.python.org/pypi/boto Documentation: http://boto.readthedocs.org/en/latest/ Report issues: https://github.com/boto/boto/issues List of contributions: https://github.com/boto/boto/issues?q=label%3APython3
Thanks for that! I updated the post and linked to this comment.
I created a video guide at http://youtube.com/Deusdies2 but I'm using pyinstaller. Sorry for not giving the direct link, I'm on my phone. 
Docker runs on top of a single Linux install, your default OS if you run a Linux box, or through a VM on Windows and OSX. Each Docker instance is effectively another VM, but with the benefit of not running another full instance. All the basic OS tasks are shared with the Linux install and only the difference is being run on top of that. Example: Launch a barebones linux install. Then run two docker instances. One that runs a web server, and one that runs a db server. To the outside world it would appear that their are three full Linux instances running. While in reality there is only one with the two servers both sandboxed from each other allowing for more granularity and security. Another way to think of it would be as virtualenv's but with no site packages. Whatever you install within your docker instance is all that is available, no conflicts to worry about and you still only have the overhead of a single OS instance.
It is. [What constitutes spam?](http://www.reddit.com/wiki/faq#wiki_what_constitutes_spam.3F).
This one? https://www.youtube.com/watch?v=11Q2QADsAEE
This was probably the most popular project left without Python 3 support, so thanks for getting it ported!
Yep, that's the one! Along with the silly wallpaper. Thanks!
This is probably the first guide/tutorial I've made, I published it through google docs because it was easy. I know it doesn't look that good, if anyone has any suggestions or ideas for a better place to put the guide up I'd be happy to look into it.
Which packages do you need that are not available in 3.x.x? Just curious. [A good rant explaining why Python 3 is the future](http://sealedabstract.com/rants/python-3-is-fine/)
I'm kind of surprised nobody mentioned the PythonAnywhere integration at http://python.org - its obviously very limited (its just an iPython shell), but its literally zero-install. 
How about wikibooks?
[Real Python](https://realpython.com/), not free, but dozens of exercises (in Python 2 and 3), and covers pretty much everything, from basic Python to scientific computing, web scraping. And web development. I bought it and think it is eally worth the money.
What points do you think I broke it?
Looking at the scripts in the bin/ directory, I see that they have invalid Python 3 syntax. Or does only the boto library support Python 3?
I just borrowed it from a friend; THIS is what I wanted. thanks you made my day
Currently it is only the library itself. The bin scripts are a little more difficult to port over as they typically contain no tests, but I hope to have those ported over in the future as well. Community contributions are always welcome :-) You may also want to check out the AWS CLI: http://aws.amazon.com/cli/ It can replace many of the bin scripts and supports Python 2/3 out of the box as well as supporting all of the latest AWS service versions.
wordpress.com is pretty easy, and you have lots of control over the blog. But the defaults are sane enough for just using it if you don't want to customize anything. 
All the major packages are ported to py3. This is no longer a good reason to start learning with 2.7.
Did you read it? Look through your submission history and check the second, third and fifth bullets.
thanks guys very supportive
Actually, I think worst case complexity is indeed O(n). **Average case** is O(1), but there's no better lower bound (even amortized) on how bad a set lookup can be, since I think python is just using a hashtable - it can degenerate into a linear lookup on certain sequences of elements. (This can actually have relevance under security analysis, since a tailored sequence of operations can constitute a DOS by causing an average case O(1) operation to become O(n).) (Though I think saying set here was just a mistake anyway, and he meant sequence)
Looking forward for Google Cloud storage utilities package to be ported also, as they were waiting for boto to support python 3. gsutils.
I'd say Tornado since you can do a lot more than just Web development with it. I also recommend: * Apache Libcloud (skip boto and nova since Libcloud handles everything) * dnspython * paramiko (ssh/sftp) * sh * pexpect * Django (or SQLAlchemy) * PIL/Pillow * Sphinx (for documentation) * requests * xpyb (if you want to fool around with X11) 
But the difference is that tuples of hashable objects are hashable, whereas tuples containing non-hashable objects are not. Slices are always unhashable, even if they contain hashable objects. 
https://python3wos.appspot.com/ The top 19 packages (by downloads) now support Python 3, and 31 out of the top 32, and 165 of the top 200. =)
How about this: original = [1, 7, 12, 5, 2, ...] decorated = [(o, i) for i, o in enumerate(original)] decorated.sort() mapped = [(i, some_function(o)) for o, i in decorated] mapped.sort() permuted = [o for _, o in mapped] The idea is known as "decorate-sort-undecorate" (or, it was 10+ years ago, in a Python cookbook I read). You probably need your function to work on the sequence, so it would look more like this: original = [1, 7, 12, 5, 2, ...] decorated = [(o, i) for i, o in enumerate(original)] decorated.sort() # This implementation of this step may be slow. values, indices = zip(*decorated) mapped = some_function(values) decorated = zip(indices, mapped) decorated.sort() permuted = [o for _, o in decorated] The extra transformations add more overhead, which might make it less worthwhile. More generally, the second sort here is an O(n log n) solution, when O(n) solutions are available. But, it might be worth it, to avoid Python overhead. I pretty much go straight for Cython when the data is numeric and I want speed --- you can just do something algorithmically sensible. If you're trying to write fast Python, you have to think about many more contingent details. Btw, if I've understood you correctly, and you're going to be sorting the data in between "order" and "permute", that's a very important detail! 
Not really. At least not in the same way. In both cases your code gets indented to increase readability. However a curly brace is sort of like a period at the end of a sentence. Instead of indicating to your mind that the sentence has ended it deli minutes a block of code. If you spend time on forums you will run into idiot posts from people that use no punctuation at all. Im not talking improper or poor punctuation here but none at all. Such text is jarring to read often cause two or three trips through the text. Python can be like that sometimes. The other big gotcha is editors screwing up cut and pasts. This can catch you off guard if you don't recognize it right away. 
 &gt;&gt;&gt; a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] &gt;&gt;&gt; import itertools &gt;&gt;&gt; s = itertools.chain(a) &gt;&gt;&gt; 2 in s True &gt;&gt;&gt; 2 in s False &gt;&gt;&gt; 5 in s True
I host on AWS, so that's a big checkbox for moving to Python 3. Thanks, Boto contributors!
This seems weird and somewhat pointless. What are the use cases for using Highcharts (Javascript) in Python?
Possibly because this question is about code editors and what encoding they use to save files with.
That's like saying writing code in coffeescript, gorillascript, typescript, ruby or whatever language that cross compiles/transpiled or running in the browser is pointless. Freedom to choose whatever language to express yourself in and having that option is the point. Do you think writing HTML markup with jade/slim is pointless? Is it pointless when you can say the same thing with less code while being easier on the eyes at the same time? I'd prefer saving as many keystrokes as possible and use them on solving more problems.
Paste is not maintained by anyone anymore, so I suspect someone will either have to come up with a replacement or someone will need to become its maintainer.
Probably had issues or just wasn't that good.
So what was the hardest part in the conversion? What did you guys find to be the most painful in maintaining python2 and python3 compatibility?
&gt; I'd say Tornado since you can do a lot more than just Web development with it. &gt; Like what?
Because if you're editing python source files, you need something to edit them with?
Line 6 onwards in your example cannot happen. &gt;&gt;&gt; s = (val for val in range(10)) &gt;&gt;&gt; 1 in s # stops when it hits 1, the first element True &gt;&gt;&gt; 1 in s # iterates through the rest of the generator False &gt;&gt;&gt; next(s) # nothing is left StopIteration 
fair enough. see the point. *takes a bow!*
Paste is unmaintained, and was last released in 2010.
Been using the beta for awhile now and it's been quite solid, well done on the release!
Which is totally fine, because utf-8 is a superset of ASCII.
When python 2 support is dropped, and most modules support it - sure. Otherwise there is no real benefit in switching to python 3 for 99% of the projects, which is the primary reason why python 3 adoption is so slow - people are told to switch to python 3 "just because".
*when you're an english-speaking person
No. ASCII is ASCII. 7 bit. The 8 bit extensions aren't ASCII anymore.
That's not the impression I got from the boto devs, whom I was in contact with. I got the impression python 3 was just something nobody cared about. I couldn't even find test users.
ASCII is a 7 bit encoding. So UTF-8 is *always* a superset of ASCII.
Pelican's site is http://blog.getpelican.com/
How could you derive that from “totally fine”? All I meant is that Python 3 is backwards-compatible to Python 2 here: any validly encoded Python 2 source file is always also decodable by the Python 3 interpreter, as any ASCII document is also an UTF-8 document.
Yes? I didn't just poop a 12000 line diff and get huffy when it wasn't merged right away. I was in email and irc correspondence with Mitch and Daniel. We discussed what versions of python they wanted to support, and what the best way to perform the change was (2to3? six? Separate branch?) but in the end it was always "really botocore will solve everyone's problems". Coupled with the fact that I could literally find nobody who wanted to use python3 and so who could test it, there wasn't much point in maintaining it. It was only a few months ago in one of the boto bugs I was on that people actually started clamoring for python3 support that wasn't botocore, and Daniel finally agreed to support it. I'm not complaining that it isn't my patch, but pointing out that python 3 is still probably way less desired than maybe python enthusiasts think. Personally I wouldn't port another project to 3.
The flair was a dead giveaway...
This is a back-asswards way of going about programming. Start with the problem, then ask yourself which library will help you solve it. If you go about it in the opposite manner, you run into the proverbial [golden hammer](http://en.wikipedia.org/wiki/Law_of_the_instrument) pretty quickly.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Law of the instrument**](https://en.wikipedia.org/wiki/Law%20of%20the%20instrument): [](#sfw) --- &gt; &gt;The concept known as __the law of the instrument__, __Maslow's hammer__, __[Gavel](https://en.wikipedia.org/wiki/Gavel)__ or a __golden hammer__ is an over-reliance on a familiar tool; as [Abraham Maslow](https://en.wikipedia.org/wiki/Abraham_Maslow) said in 1966, "I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail." &gt; --- ^Interesting: [^Statutory ^Instruments ^of ^the ^United ^Kingdom, ^planning ^law](https://en.wikipedia.org/wiki/Statutory_Instruments_of_the_United_Kingdom,_planning_law) ^| [^Copyright ^law ^of ^Pakistan](https://en.wikipedia.org/wiki/Copyright_law_of_Pakistan) ^| [^Copyright ^law ^of ^Georgia](https://en.wikipedia.org/wiki/Copyright_law_of_Georgia) ^| [^Copyright ^law ^of ^Bangladesh](https://en.wikipedia.org/wiki/Copyright_law_of_Bangladesh) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjdxgrc) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjdxgrc)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
The question was not about code editors but about explicit UTF source encoding in files.
The question was not about code editors but about explicit UTF source encoding in files.
The question was not about code editors but about explicit UTF source encoding in files (I was sitting next to the poster (audreyrg) when the original question was asked).
This doesn't tell me very much. What is happening, and what do you expect to happen? I don't think a comment is just a string, it's probably an object that contains other data (parent, replies, karma etc). You probably need to get the actual comment string out of it. Why are you using a set for listOfComments? You could probably use: for comment in com_gen: io.write(comment) Unless you're worried about duplicate data.
source files do not magically come into existence. The programs which access them are either a Python interpreter (which I addressed intially), or a code editor, used to create and edit them. Both of these types of programs need to be able to interpret the encoding of the file, so *both* are relevant to its encoding. A coding comment can also enforce the encoding interpretation used in some editors like Vim and Emacs, as well the Python interpreter. OP did not only ask about one or the other.
I did this for Django (ported to single code base with all tests passing). IMO this definitely helped everyone to see that the thing was achievable, even though my patch wasn't merged (and I wasn't expecting it to be). With a big code base to port, it can be a bit like eating an elephant, so people are reluctant to get started. There are always bugs to fix, new features to add ... those things are generally higher on the priority list than porting.
So this is the future? Cool. Feels kinda the same as the last though. ;)
You need to get comment.body to get the text. You also need to make sure it is a Comment object and not a MoreComments object. If you want all comments from a submission without caring about order of replies, use praw.helpers.flatten_tree (submission.comments). Otherwise you'll have to recurse and get comment.replies. Check https://github.com/NSchrading/redditDataExtractor/blob/master/RedditDataExtractor/redditDataExtractor.py specifically the functions _getAllComments() and _getCommentImageURLs() to see how I did it.
I've been heavily involved with boto development for about two years now. Python 3 has been on the map for every bit of that. Link your PR or commit and I can probably tell you what was wrong with it.
Hi. You can pass in a dict, then unpack it with setattr: class example: def __init__(self, somedict): for key, value in somedict.items(): setattr(self, key, value) somedict = {"a":1, "b":2} e = example(somedict) Then you have e.a and e.b
I have played around with ipython parallel and found it pretty easy, but still feel like we don't have anything quite as easy as Matlab. Is there something comparable to the parfor that it has. The represents the most simple case where you have a bunch of independent things you want to loop over in a batch. A paradigm we use at work is to write a function to work on a dataset. We have many independent but commonly formatted datasets. We then have a function that calls our function we created on each dataset and rolls the results into one table-like structure with an additional column which details the source of the associated rows. However, this is in serial and could obviously be performed in parallel. I was able to make the function that calls our functions across many runs work in parallel by just using parfor. So, you end up with a batching function that worked the same as before. But, this requires the parallel toolbox that no one else has, which describes the pain of Matlab. I suppose what you have is very similar, but for the scientific community adding par in front of a for loop is a bit more approachable.
No problem!
Some of the groundwork for that is already under way: https://github.com/GoogleCloudPlatform/gsutil/issues/214
Usually so many parameters mean that your class is doing too much or that the parameters can be grouped somehow, e.g. instead of class Entity(object): def __init__(self, name, x, y, z): self.name = name self.x = x self.y = y self.z = z You should probably have class Point(object): def __init__(self, x, y, z): self.x = x self.y = y self.z = z class Entity(object): def __init__(self, name, position): self.name = name self.position = position # position is a Point If you have many options of some kind, consider using arbitrary keyword arguments and store them as a dict: class Example(object): def __init__(self, a, b, **options): self.a = a self.b = b self.options = options x = Example(1, 2, option1=True, option2=False)
https://github.com/kurin/boto/tree/py3kport was my branch https://github.com/boto/boto/tree/py3kport is my branch in the boto git repo 
It's important to note that there are two Daniels involved with Boto. I'm @danielgtaylor on Github, the other was @toastdriven. I wasn't working on Boto 18 months ago. I can definitely understand your frustration. Unfortunately I can't say much on the subject. I'm sorry that your experience wasn't a good one. Over the course of those 18 months the codebase diverged quite a bit and I decided to take a piecemeal approach where I would implement Python 3 support for the core and ask the community to contribute Python 3 support for individual service modules. That approach seems to have gone really well. We now support almost all of the services with Python 3, have had many testers and have worked with downstream projects to make sure this release goes smoothly. Python 3 support has become a bigger and bigger pain point for Boto users over time and I for one am happy we now support it. Python 3 may or may not be as desirable as some think, but now our users have the choice to decide for themselves.
&gt; [If you have a procedure with ten parameters, you probably missed some.](http://www.cs.yale.edu/homes/perlis-alan/quotes.html) -- Alan Perlis
The lack of good test coverage and handling str vs. bytes in Python 3 were probably the most difficult to deal with (if I ever complain that a pull request needs tests, think of this!). For example, we have a method in S3 called `get_contents_as_string` that really returns `bytes` - but in Python 2 that's the same as a string. In Python 3 we needed to continue returning `bytes` but now the name makes much less sense. There are now checks and `encode`/`decode` calls peppered throughout the codebase to handle different possible inputs from either Python 2 or 3. The best decision we made, in my opinion, was to do this piecemeal. Start with the core, get it working for a couple of services and then ask the community to contribute Python 3 ports for each individual remaining service module. That made reviewing, testing, and merging a much simpler process for me. The community support for this project has been amazing. I couldn't be happier with our users and contributors. They really make a project like Boto shine. 
Where is/was your pull request?
Explicit is better than implicit.
I don't think I ever made a pull request on github. I was mostly talking to garnaat and toastdriven on IRC.
This is super magic'y in a bad way. Explicit attribute assignment is a big part of self documenting code, good luck figuring out what that dict holds in 6 months. The real problem is bad class design and this just helps hide it. There is no hard rule on the correct number of arguments for a function, but a rule of thumb is to give things a second look if that number goes above 5. If any set of arguments are related to one another chances are they the should be a class.
Most Linux distros package Python 2 and 3 as completely separate stacks, so it's easy to have both installed, without having to switch between environments. On Debian based distros, Python 3 is packaged as `python3`, and packges for it have that as a prefix (e.g. `python3-matplotlib`). In almost all cases, `python` starts Python 2, and `python3`, Python 3 (except Arch, where it's `python2` and `python`). Command line tools where the Python version matters usually get installed with a suffix as well, like `pip3`, `nosetests3` and `ipython3`. If you're stuck on an older distro and you want to use newer versions of Python or the scientific libraries, then you could download [Anaconda for Python 3.4](http://continuum.io/downloads#py34).
The first part of this is good, but when you go into the packing and unpacking syntax I start having a problem. first, using the kwarg packing syntax **, many easy to catch errors will be ignored on instantiation. Second, this would just help ignore bad class design. Third, this is not self documenting in any way. In 3 months this code will be unreadable. The __only__ time I see packing/unpacking syntax as useful in class design is when using the `super` function to access base class methods. The implicit assumption being that the base class is both the location of documentation, and the location of argument validation.
How about some crazy? assert set(somedict) == set(["a", "b", "c"]) self.__dict__.update(somedict) 
To expand on what I think is the most level-headed advice in this thread. using the dict packing operator ** for anything except class / function wrapping is a surefire way to introduce insanely hard to find bugs. A big part of the init function is validation. Perameters should either be required, or they should have a default value. ** denies you the ability to do either. This this is the sort of solution where we get the phrase, "throwing the baby out with the bath water".
It absolutely must be documented, but I'm not unpacking anything though, i'm just taking a `dict` argument with nicer syntax.
[I'll give you 10.](http://www.reddit.com/r/Python/comments/26y26u/10_awesome_features_of_python_that_you_cant_use/)
Number of those I actually needed to use: 0.
Your loss. Let me know if you ever try out csvtomd. I'd like to know what you think. 
Good point. Having a tuple with valid keys is a way to keep control (and it is somewhat self documenting): class example: fields = ("a", "b", "d") def __init__(self, somedict): for key, value in somedict.items(): if key in self.fields: setattr(self, key, value) else: print("Not valid: " + key) somedict = {"a":1, "b":2, "c":3, "d":4} e = example(somedict) print(e.__dict__)
There might be a great reason not to do this, but namedtuple can auto-implement pretty much all of this for you, as well as a variety of other goodies, assuming they are kept constant. from collections import namedtuple examplebase = namedtuple('examplebase', ['parameter1', 'parameter2', 'parameter3']) class example(examplebase): pass 
Yeah I hate that about Python and Java. For Java Eclipse can generate the constructor for you, not sure about Python. As someone mentioned 10 parameters is too many, so consider introducing a [parameter object](http://c2.com/cgi/wiki?ParameterObject) ([introduce pattern object](http://sourcemaking.com/refactoring/introduce-parameter-object)). As an alternative you could try out the [value objects library](http://stevewedig.com/2014/07/31/value-objects-in-java-and-python/#python) I shared yesterday. It does the assignment part for you, and you can use EntityMixin instead of VluaeMixin don't want value behavior. (To give credit, I got the idea from this other [value object library](https://github.com/halst/value)). 
Maybe because its right way to do it. Unlike all those garbage hacks posted here. All they do is make core less readable and less maintainable. Besides its not that terrible when one is dealing to 4-5 args. If there are really too many args maybe one should consider alternative designs.
Are you running it via cron? If so, you can use a different rule for Sunday (day 0) vs. the other days (1-6). If you want to do it from within the program, you can use [datetime.now()](https://docs.python.org/2/library/datetime.html#datetime.datetime.now) to get the current day of week and hour. &gt;&gt;&gt; from datetime import datetime &gt;&gt;&gt; now = datetime.now() &gt;&gt;&gt; now.weekday() 4 &gt;&gt;&gt; now.hour 11 &gt;&gt;&gt; now.minute 47 &gt;&gt;&gt; #today is Friday at 11:47am (Friday = 4, Saturday = 5, Sunday = 6, etc.) Use those values to build an if-statement that makes the script exit early during Sunday from 6 to 10.
I write a lot of code for my research, where actual correctness of certain core routines is paramount (F=ma within some tolerance, regardless of whether the m is a real mass or a virtual mass or who knows what else). Unit tests have *saved my ass* on numerous occasions when I've had to make changes under immense time pressure. This is slightly different from the business model of software development (which is what I did before going back to grad school), but not really all that different when you consider shipping things like hotfixes. &gt; Keep regression tests around for up to a year — but most of those will be system-level tests rather than unit tests. This is insane. So I should stop testing the most tedious, convoluted customer applications after a year, regardless of whether that customer is still paying for upgrades the software? I should just get rid of the regression test and pray hard that when they decide to take the next release, it doesn't break anything? I understand what he's trying to say, but he could have shortened the entire article to: &gt; Testing can’t replace good development
Hi. use the datetime library. Some hints: https://docs.python.org/2/library/datetime.html#datetime.datetime.weekday https://docs.python.org/2/library/datetime.html#datetime.datetime.now Good luck :)
I do this.. def __init__(self, *args, **kwargs): self._inc = 0 self.clone_db = kwargs.get('clone_db', None) self.db_files = kwargs.get('use', []) self.db_directory = kwargs.get('use_dir', os.path.join(os.getcwd(), 'db')) self.db_extension = kwargs.get('extension', 'sqlite') self.name = (generate the name) Properties go first, Then use `get` on `kwarg`s to set defaults if they're not supplied Then assign inner class variables that aren't passed as parameters.
I find the habit of unit testing very beneficial. 1) Especially for larger project, after I drafted the outline, I write unit tests first so that I define an expectation of what the function - that I will then implement - should actually do and not do. 2) Also, unit testing is an invaluable tool for optimizing code and check if everything still behaves as before 3) New unit tests can and should be added when bugs and exceptions for unusual scenarios occur: it is a good way to keep track of every exceptional case by writing a new test case 4) it is a joy to see all your tests pass with an "ok"
You are right, I also think the whole idea of everybody responding to this is a little crazy; the assumption being that `__init__` functions are too tedious to write out fully. I aspire to write clean short code, but I would never trade stability for brevity. Truthfully, I spend far less time writing code than I do debugging, tweaking, and re-factoring. I think most of us who are actually writing code day-to-day can type between 1 and 10 thousand times faster than we can produce working code. I have a friend who works at Google and he says the average Googler commits about 300 SLOC per week. I'm sure the standard deviation is massive but the average is still instructive.
 self.is_bound = data is not None or files is not None self.data = data or {} Fuck, why did I never think of that. This will make my classes that assign deafults to None so much cleaner.
Your initial code was the best: it's clear for everybody what it does, it gives the expected error messages if something is wrong with the number of arguments and you're unlikely to have a bug there. So it's slightly ugly when there are over ten parameters, how often does that happen? And it's still not as ugly as all the other hacks in this thread.
You can't do potentially useful things like passing in an empty ordered dict then, though.
This will be controversial but I sort of like this idea, though I've never used it myself: https://github.com/ionelmc/python-fields &gt;&gt;&gt; from fields import Fields &gt;&gt;&gt; class Pair(Fields.a.b): ... pass ... &gt;&gt;&gt; p = Pair(1, 2) &gt;&gt;&gt; p.a 1 &gt;&gt;&gt; p.b 2
This is a partial "solution" using a decorator. You probably don't ever want to do this though, except for fun of course. (edit: [gist of an updated version](https://gist.github.com/anonymous/5632d8db566dec14dfe5) #!/usr/bin/env python2 from __future__ import print_function from functools import wraps import inspect def magicinit(func): @wraps(func) def newfunc(*args): argnames = inspect.getargspec(func).args assert(len(args) == len(argnames)) self = args[0] for ind, argname in enumerate(inspect.getargspec(func).args): if argname == "self": continue print("setting " + argname + " to " + str(args[ind])) setattr(self, argname, args[ind]) return newfunc class Foo(object): @magicinit def __init__(self, a, b, c): pass if __name__ == "__main__": x = Foo(3, 1, 4) print(x.a, x.b, x.c) 
Good point, this slight modification from requests seems a better way to do it data = [] if data is None else data files = [] if files is None else files IMO more readable and explicit while also explicitly only dealing with the case that no argument is supplied (or None is given)
Heh, I guess my initial post should have included what you have mentioned. My experience with **setattr** comes from a binary GIS-structure where each spatial point has 23 attributes. I have a large tuple where each attribute name, default value, type, byte-size etc etc is defined. I use this tuple in a Point class to control **setattr**. I'm the only one using the code (at least to day), and I think it works alright. I hate to write out self.a = a, self.b = b etc when there's a lot of attributes, so I like the **setattr** approach better. But I do have learned a lot from this thread though!
I've always done it like class example: def __init__(self, **kwargs): for k, v in kwargs.iteritems(): setattr(self, k, v) anybody have any feedback on how (non)pythonic this is? edit: nevermind, /u/olavrel posted the same comment
You should use "raw" strings for Windows paths and regexes so you don't need to double all the backslashes r"c:\Users\k\Desktop\file2.txt" instead of "c:\\Users\\k\\Desktop\\file2.txt"
&gt; A smarter approach would reduce the test code mass through formal test design: that is, to do formal boundary-condition checking, more white-box testing, and so forth. That requires that the unit under test be designed for testability. Like a few other articles popping up lately, this is the crux. Unit tests are tools, they are neither good nor bad on their own. However they should be used to tell you an honest story about your design. In other words, don't stop writing unit tests but make sure to listen to what they tell you. Otherwise, I agree, they are mostly wasteful. Don't blame unit tests themselves, blame people who can't see their design is poor or wrong. 
Ok, so here's the real deal: have some very large set of tuples T. For every t in T I have to compute V(t) which is very expensive and yields a tuple, and I have some other function P(t) = U(V(t)) which I'm trying to approximate by iteratively computing V(t). Now T is flabbergastingly large, combinatorially large, to be precise, so it is extremely expensive for me to calculate V(T). It turns out that T is symmetrical, so that for every permutation s of |t| elements s(t) = t', for some t'. V also has the property that V(s(t)) = s(V(t)), therefore s(V(t)) = V(t'). My plan is to partition T into subsets under permutations of |t| elements S, each such subset can be generated from a canonical t, this particular subset is T'. Now I can just calculate S(V(T')) to get what I need.
 class Point: defaults = { 'units': 'ft' } def __init__(self, **kwargs): self.kwargs = Point.defaults.copy().update(kwargs) 
The only problem with that is that your function signature isn't self-documenting, when it could be.
This is even more incidious. It will run of course, but what if I do the following? p1 = Point(1, 2, 3, unit="m") Now I have a hidden logical error which will put my results off by a factor of roughly 3. If this is burried deeply, I may never see it, I'll just know my number is wrong for some reason, that's if I'm consious enough to validate my output. Anyways, this is a terrible terrible plan. If the wrong argument is passed, the class should throw an error when instantiated. For this reason, the pass-argument-by-dict method should never be used in real code unless it is being used to wrap a function or method, or if the data structure on which the method operates is innately a dict. Just because something _can_ be done doesn't mean it should be done. See my [thread with olavrel](http://www.reddit.com/r/Python/comments/2cc9ut/is_there_more_sensible_ways_to_arrange_class_init/cje877s?context=3) if you want more details on why this is a bad idea.
Right, but I was specifically asking about IPython =) As far as I can tell, IPython uses either 2.x or 3.x ... or am I missing something obvious?
However on the up side, it did find the one that I was most expecting it to not find. Obscure track from a video game mod (Neotokyo).
Have you looked at MapProxy? I got started working on that and gdal2tiles.py Edit: I'm in GIS obviously
Another Python-powered static site generator is [Nikola](http://getnikola.com/).
The best way to get flexible hours is to get a remote gig. The best way to get a remote gig is to have experience. Business work is going to be easier to get than academic. /r/cscareerquestions might be a better place to post
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Crobots**](https://en.wikipedia.org/wiki/Crobots): [](#sfw) --- &gt; &gt;__Crobots__ is a [programming game](https://en.wikipedia.org/wiki/Programming_game) released for the first time by Tom Poindexter in December, 1985. The [robots](https://en.wikipedia.org/wiki/Robot) are controlled by a program written in a stripped-down version of [C](https://en.wikipedia.org/wiki/C_(programming_language\)). The robot's mission is to seek out and destroy other robots, each running different programs. The robots can be controlled in order to move around the battlefield, scan the environment to find enemies and fire at enemies using a cannon. &gt;In October, 2013, Poindexter released the source code of Crobots under the &gt;[GPL](https://en.wikipedia.org/wiki/GPL). &gt; --- ^Interesting: [^Crobot](https://en.wikipedia.org/wiki/Crobot) ^| [^Robocode](https://en.wikipedia.org/wiki/Robocode) ^| [^Wind-up ^Records](https://en.wikipedia.org/wiki/Wind-up_Records) ^| [^SnoCore ^Tour](https://en.wikipedia.org/wiki/SnoCore_Tour) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjeh5xk) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjeh5xk)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
In the recent Delphi XE6 ($1000+)there was a function inlining improvement for functions returning a floating point value. After release it was discovered by users that while this did markedly increase speed for those type of functions, *every other function typed was slowed down*, in some cases 50%-70%! It had also been stated that the focus for XE6 was going to be "Quality, Performance, Stability". Ouch! Either there are no regression tests or no one bothered to run any before release.
You can use: class example(object): def __init__(self, parameter1 = blah, parameter2 = something, parameter3 = 'cat'): self.__dict__.update(locals())
I really don't like throwing everything in kwargs for sake of a few lines. It makes the code less inspectable, makes epydoc/sphinx/etc. docs worse, and in cases where you don't allow arbitrary kwargs, means you won't catch silly typos until runtime.
OpenStack, it's almost all python based Apis and Swift is mostly python. 
It also means you don't catch silly typo errors until runtime. If you have a method that doesn't accept arbitrary kwargs, pyflakes will catch bad invocations of it.
With all due respect, I find that this is the kind of thing that breaks down over time, especially in a situation where multiple people are maintaining the code. For example, someone may insert some new thing in between kwarg parsing (eg, calculating a new thing based on a kwarg), or they may add a new kwarg and forget (or be too lazy) to update the docstring. In my experience, self documenting code is much more maintainable over longer periods of time.
So you use setattr on locals() and pass parameters as normal.
Some will say this isn't explicit, but, y'know, we have programming so we don't have to repeat ourselves.
With all due respect, it makes me a little sad to see so many people in this thread suggesting that *args and **kwargs is the best way to deal with this. /u/chickenphobia has spelled out some of the downside of this in a few places, but I'd like to get them (and my own) in a top level comment, too: * The code is not self documenting anymore. * Sphinx and other doc generators will not produce useful overviews of the interface. You can write doc strings to help this, but it's easier for those to get out of date over time than the actual method signature. * Silly errors (such as mistyping a keyword arg name) cannot be caught by the interpreter, and are more difficult to debug. It's a fun exercise to try and figure out to make code like that more compact, but I would not trade any of the solutions I've seen for the original code given. edit: formatting
Thanks, MapProxy is blocked at the coffee shop I'm at but I'll look into it later. You're suggesting this as a project to contribute to that would give me experience?
&gt; Perameters should either be required, or they should have a default value. ** denies you the ability to do either. This is not true. With kwargs you can assign default values like: self.parameter = kwargs.get('parameter', None)
Sorry, What are you suggesting I do with this?
Oh I thought you were looking for python jobs that were flexible and non web based. 
Laziness is not an excuse, and there are no conflicts following the guideline I gave as generated values follow kwargs assignments. Doc strings can also be generated automatically on merge based on kwarg assignments and naive return type interpretation. Many very popular modules use this approach, it's not new and it's not bad. See, http://docs.python-requests.org/en/latest/api/#main-interface 
MySQLdb is blocking, not Tornado friendly. One can modify Alex Gaynor's MySQLdb-ctypes driver to be Tornado friendly by using send_query and read_query in IOLoop. For your question, use connection pooling or dbutils.
Typos are a side-effect of an interpreted language, infact it's arguably better to use kwargs to combat typos because the variables are set from defaults and the position of those variables don't matter; because there are no positional arguments. Positional arguments can then be used as seed data because there are no required parameters in front of them. You tout maintainability...I'd like to see one of your projects that's big enough that this has been a problem.
Laziness is not an excuse, but it happens. And it's not just laziness - sometimes it's an extremely time sensitive fix, and then forgetting to go back and update the docs. Or all the people that were strict about it left the project/company. Maybe I'm not looking in the right places, but I've not seen a project or company that's able to maintain this sort of thing in the long term.
It's a lot easier to upgrade packages and bounce around to a solution if you have 100 tests that test all those little edge cases you don't think about. Ever tried to upgrade a Python package that's 10 years old? None of the packages you need to switch to even existed back then.
Committed code will never be merged if it didn't pass the coding standards, unit tests, code review and documentation. The developer can be as lazy as he wants, but his feature won't be implemented until it passes a strict green light. 
Typos are more a fact of life in a language like Python, certainly, but that doesn't mean you shouldn't guard against them where you can. I don't agree that it's better to use kwargs to guard against them, because rather than hitting a error, you end up silently "succeeding", but with the wrong value. Eg: def foo(**kwargs): a = kwargs.get("blah", True) if a: print "yay!" foo(bla=False) In the above code foo runs without any errors, but it does the opposite of what you intended.
I'm really happy for you that you work somewhere where that's the case. I'm jealous, in fact. Where I work, time pressure (internal and external) and other factors sometimes force us to cut corners, and make it more difficult to come back and repair them.
defaulting to `True` is the real tragedy in this example... def foo(**kwargs): a = kwargs.get('blah', None) if a is None: raise ValueError ...now `a` can be anything, except nothing.
Yeah, but why would you default a value to True if you're specifically checking for that value?
That's fine for some situations, but None is not a good default in all cases. Here's a more real world example: def setup_logging(**kwargs): level = kwargs.get('level', logging.WARNING) logging.basicConfig(level=level) setup_logging(levels=logging.DEBUG) Now I get WARNING level logs instead of DEBUG...
Poor example, I suppose. There's certainly valid cases for defaulting an arg to True though. Here's some real code: https://github.com/mozilla/build-tools/blob/master/lib/python/signing/utils.py#L22 In this case, if all of those keyword args were in **kwargs, and did: signfile(..., faked=True) You'd end up actually doing stuff, rather than faking it.
Good idea. Thanks.
[`logging.basicConfig([**kwargs])`](https://docs.python.org/2/library/logging.html#logging.basicConfig)
Completely insane.
OpenStack is a huge project that is being actively worked on by dozens of companies. I believe /u/sasimpson was suggesting you start digging into the project and then apply for a job with one of the companies working on whatever section interested you.
mmm OpenStack
Or Atom. This whole project is awesome.
It's addressed in the Readme. Though I should mention I've used namedtuple many, many times, and I've never used Fields. So I don't disagree.
I'm pretty sure that hash randomization was added to Python3 for precisely the reason of preventing that type of exploit. This script demonstrates it (IIRC you need 3.3 or newer.) words = 'the quick brown fox jumped over the lazy dog\'s back'.split(' ') for w in sorted(words): print ("%x\t%s" % (hash(w), w)) On consecutive runs of the script, the hash value of the same word is completely different.
You're writing more code than it would take to just make `blah` a mandatory keyword-only argument. And you have to write that for *every single one*. Why do this?
...What's a "LINE client"? This is an empty Python class. I understand it says it's a work in progress, but I don't understand why you'd put it on Reddit and on PyPI before it does anything, or even what it's supposed to do once there's some code in it. Edit: I'm wondering if this is a git accident. There's literally more code in the screenshot than in the repository.
&gt; ASCII is a 7 bit encoding. So UTF-8 is always a superset of ASCII. That's interesting. The struct module deals with bytes and it's 1 byte in there. Ints/floats are 4 bytes (1 word) and doubles are 8 bytes (2 words).
Have you had any luck with [blucat](http://blucat.sourceforge.net/blucat/)? It's a tool like netcat, but for bluetooth. It was presented last defcon. It's not python, but it's probably going to be the easiest way to mess around.
I've been using https://github.com/IanHarvey/bluepy successfully 
I &lt;3 u
I agree, the other solutions make a very basic thing too complex. Also, it's a design smell if your class needs so many attributes: if you have more than four to six, I would suggest breaking up your class into two smaller classes. It's very rare to have a class with so many attributes that doesn't violate the single responsibiliy rule.
Nice! Reminds me of the AI challenges Google did a few years back (http://en.wikipedia.org/wiki/Google_AI_Challenge)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Google AI Challenge**](https://en.wikipedia.org/wiki/Google%20AI%20Challenge): [](#sfw) --- &gt; &gt;The __AI Challenge__ was an international [artificial intelligence programming contest](https://en.wikipedia.org/wiki/Competitions_and_prizes_in_artificial_intelligence) started by the University of Waterloo Computer Science Club. &gt;Initially the contest was for University of Waterloo students only. In 2010, the contest gained sponsorship from Google and allowed it to extend to international students and the general public. &gt; --- ^Interesting: [^AI ^Challenge](https://en.wikipedia.org/wiki/AI_Challenge) ^| [^Competitions ^and ^prizes ^in ^artificial ^intelligence](https://en.wikipedia.org/wiki/Competitions_and_prizes_in_artificial_intelligence) ^| [^Competitive ^programming](https://en.wikipedia.org/wiki/Competitive_programming) ^| [^Galcon](https://en.wikipedia.org/wiki/Galcon) ^| [^Artificial ^intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjepl3w) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjepl3w)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Amen! 
This is the best way to do it : - it's self documented, provided your arguments have proper names - it explicitly shows what you do with your arguments. \_\_init\_\_ can get way more complex, and it's easy to see what's going on - it's fast to read. Reading is much import important than writting - it's not code you write very often. How many objets do you create in your entire year ? At worst it's 5 minutes / year of your life. - there is actually much more going on there than you think there is : for example, you sayfexplicitly the order of parameters, and which one are mandatory. Then you choose which one you are exposing, and the name of public attributes. These may seems obvious, but they are important logic steps and should be a conscious choice. What's more, if you have 10 parameters for an object, you need to pause and reflect. You are probably in a case where composition or aggregation should be used. Plus, you are certainly not using the Python stdlib types to their full potential. It's ok to write no so perfect code in the end. You'll improve on the way, don't stop at every line to wonder if you can make it more perfect. Code. Improvement will follow : you'll read other's people code, you'll use libs, you'll get other dev's comments, and best practices will come to you naturally. It's good that you want to improve, but don't sink in the details. Code. 
Naver Line is an amazingly popular messaging app in various Asian countries. I am currently living in Thailand and -&gt; EVERYONE &lt;- uses line. And i mean EVERYONE! If you're out on public transportation you will see Line being used on smartphones everywhere. Line also has various games that you will also see being played on smartphones EVERYWHERE you go. It's just crazy. 
Dude when it comes to design there's often multiple right answer. This isn't one of those cases. Typos should always generate an error and never cause silent failure or default fallback Not only is this less maintainable in the future this is barely useful in the present. If you want to shoot yourself in the foot that's your prerogative but don't encourage others to do so. See my comments elseware for a detailed view on why this approach is less maintainable, less concise, and more error prone than the canonical instantiation method. Finally, if this was ever a good idea wouldn't you expect to see it at least occasionally in professional and OS software? The fact that you don't is the most telling evidence of all.
It is very short sighted. The author could use some exposure to other industries where failure happens over years and decades. Software is not somehow immune to regressions after N years.
Glad it worked for something!
Actually, I have a bunch of code which works pretty well to send a LINE message through python. But, I'm beautifying my code for readability, and also worrying about that LINE will take down my code DMCA claim...
If you've got a C/C++ library to interface with these, making a python wrapper is very little work. If you've a C library, just call it with ctypes (or maybe CFFI). If it's C++, you need a compiler (thats C++ for you) in which case, Cython would be my prefered route but there are more automated tools like SIP/swig. 
Look into this - [PyUserInput lib](https://github.com/SavinaRoja/PyUserInput)
If you ever need a reminder just "import this"
Well, in the Python world you have matplotlib, chaco,... Excellent libs to make static files (you also could add some interactivity but it is a little bit weird IMHO). In the js world you have some first class libs (d3, raphael, highcharts,...) to display very rich graphics with interactivity that allow you to tell stories from data in a more dynamic way. The IPython notebook is used as a tool to tell this kind of stories and it works with a front end that is a browser. Why not merge some excellent tools from both worlds using Python to glue them?
Plenty of other fun ones, too! Here's a few... `from __future__ import braces` `import __hello__` `import antigravity # xkcd reference` And my personal favorite: import this love = this this is love # ==&gt; True love is True # ==&gt; False love is False # ==&gt; False love is not True or False # ==&gt; True love is not True or False; love is love # ==&gt; True &amp; True
Only new about the antigravity one, thanks!
Or ``python -m this`` in a terminal.
My doppelgänger! Checkout ZeroMQ and gevent. Have fun. 
&gt; Writing function wrappers when a class would be more succinct. I could go on. There's a good reason to use function wrappers rather than classes when taking the role of a decorator. Using a class of your own, unless you specify that behavior, will not have properties of functions like methodization. Replicating that behavior using a class of your own is generally more work and gotchas then just returning a function. Maybe you're talking about another use case though?
https://seanmckaybeck.com/2014/07/26/improving-readability/
&gt; As long as I've been a member of this subreddit (longer than I've used this username), there's been a tenancy to get "fancy" with python. One might say it's fairly recent design change is indicative of this. Ugly, complex, unreadable, specially cased, impractical.. &gt; Although never is often better than right now.
the issue was actually client-side.. long story short I had the wrong URL
With bluetooth low energy, or with bluetooth classic? I've had success with the latter, but it was my understanding that it didn't support BLE
I'll have to check it out. Thanks for the direction!
I don't have a C/C++ library to interface, though finding one seems like a more promising option. My background is in mechatronics, so programming isn't my strong suit. I'd like to make sure I'm understanding you correctly - you're suggesting to find/make a C/C++ library, then effectively wrap it in python using cypthon or other tools that do a similar function. Is that correct? 
I like how smtplib has elaborate docstring examples. Can you copy your examples into docstrings? It isn't clear how to package documentation for a library.
&gt;&gt; Flat is better than nested New programmers, live by this. 
From my experience, I started learning Python3 and have rolled back to Python2 for various side projects - it's not a huge transition for me so far, but I suppose it depends on the work that you're doing.
&gt; ...into 20 aphorisms, only 19 of which have been written down. So what's the twentieth one?
A few years ago I wrote the anti-zen of Python. $ pip install that $ python &gt;&gt;&gt; import that Pull requests welcome at https://github.com/pydanny/that Disclaimer: The entire package is explicitly a joke (and says so). Don't use the advice related unless you want to be hated. Please don't hate me because I made a bad joke out of a Python package.
I don't know if there's an official answer to this. I once heard that the 20th one is a formatting character because source files should end with a newline.
True. 
 love is not True or False; love is love # ==&gt; True &amp; True should be love is not True or False; love is love # ==&gt; True or False
What pygame does might also be a interesting thing to look at: [src link](https://bitbucket.org/pygame/pygame/src/). What they do is have the examples be a sub-package. Although pygame has to support python versions that don't have `from .. import blah`, so you can use that (or a simple sys.path hack) to import your package code in your examples. Of course if your examples are small enough try and fit them in docstrings or such. Most examples also work along the lines of tests so maybe read up on how to structure tests?
Thank you! Will check out
Having made something like this before, I have several comments. First, &gt;&gt;&gt; bd = BiDict() &gt;&gt;&gt; bd[1] = 2 Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "bidict.py", line 117, in __setitem__ self._setTypes(key, value) File "bidict.py", line 39, in _setTypes raise TypeError("Keys are of the same type as values") TypeError: Keys are of the same type as values This seems like a significant flaw to me. It's common to want bidirectional mappings between things of the same type. For example, between two different integer IDs. &gt;&gt;&gt; bd[1] = "a" &gt;&gt;&gt; bd["a"] 1 So far so understandable. &gt;&gt;&gt; bd &lt;__main__.BiDict object at 0x7f1f38b8fdd0&gt; Eww. &gt;&gt;&gt; str(bd) "{1 &lt;=&gt; 'a'}" Better; you should add a `repr` too. &gt;&gt;&gt; bd[2] = "b" &gt;&gt;&gt; str(bd) "{1 &lt;=&gt; 'a', 2 &lt;=&gt; 'b'}" This works. &gt;&gt;&gt; bd[1] = "b" &gt;&gt;&gt; str(bd) "{1 &lt;=&gt; 'b', 2 &lt;=&gt; 'b'}" That's not meant to happen! &gt;&gt;&gt; bd["b"] 1 &gt;&gt;&gt; del bd["b"] &gt;&gt;&gt; del bd["b"] Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "bidict.py", line 133, in __delitem__ del self._dict_a[self._dict_b[key]] KeyError: 'b' &gt;&gt;&gt; bd &lt;__main__.BiDict object at 0x7f1f38b8fdd0&gt; &gt;&gt;&gt; str(bd) "{2 &lt;=&gt; 'b'}" Uh oh. Well, carry on. How does it deal with types? &gt;&gt;&gt; class X(int): pass ... &gt;&gt;&gt; class Y(int): pass ... &gt;&gt;&gt; bd = BiDict({X(1): Y(1)}) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "bidict.py", line 27, in __init__ self._dict_a = copy(initial) NameError: global name 'copy' is not defined Oh, hold on... &gt;&gt;&gt; from copy import copy &gt;&gt;&gt; bd = BiDict({X(1): Y(1)}) &gt;&gt;&gt; bd[1] Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "bidict.py", line 50, in __getitem__ raise TypeError("Key is of invalid type") TypeError: Key is of invalid type &gt;&gt;&gt; bd[X(1)] 1 That sort of makes sense, as 1 is not a subclass of X. &gt;&gt;&gt; bd = BiDict({1: Y(1)}) &gt;&gt;&gt; bd[1] 1 &gt;&gt;&gt; type(bd[1]) &lt;class '__main__.Y'&gt; &gt;&gt;&gt; bd = BiDict({X(1): 1}) &gt;&gt;&gt; type(bd[1]) &lt;class '__main__.X'&gt; That makes sense, but it implies: &gt;&gt;&gt; bd[Y(1)] Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "bidict.py", line 50, in __getitem__ raise TypeError("Key is of invalid type") TypeError: Key is of invalid type Yeah, you're probably using `type(x) is cached_type`. Not ideal, but sort-of justifiable. *checks* Oh, no. You're using `==` (which is probably too greedy). &gt;&gt;&gt; list(BiDict({"x": 1})) [('x', 1)] Makes some kind of sense, but it breaks convention. --- Well, it's OK I guess, but it misses the fundamental point of what you're trying to solve. So typically one starts with a mapping of X → Y and realises later that it needs to be reversed; Y → X. This is common, and there are a lot of solutions. One solution, that I happen to dislike, is just to use one mapping when the types are different. This has a few problems with obviousness, so one decides to move to a dedicated type. There are a few ways you can do two-way dispatch. Different methods works. So does something like [the current famous bidict's technique](https://pypi.python.org/pypi/bidict/0.1.1) of `[:x]` and `[y:]`. You chose type-based dispatch. Unfortunately that has a lot of problems. For one, Python is hideously bad a type-based dispatch, as it's a duck-typed language. If you want to add in a new type, even benignly: &gt;&gt;&gt; type(1) == type(10000000000000000000) False Well, everything breaks. There's the fact that sub- and super-classes are hard to support as types need to be distinguishable. There's the more important fact that mapping x ∈ X → y ∈ X and back again is fundementally impossible. --- My approach was much simpler. You generate X → Y and Y → X as different objects, but keep them synchronized: """ A small 1-to-1 dictionary library. Shares inspiration from bidict: https://pypi.python.org/pypi/bidict/0.1.1 Implementation similarities are mostly coincidental. The ordered variant was an idea stolen from https://bitbucket.org/jab/bidict/issue/1/ordereddict-integration I'm not sure what you'd want one for, but it's there if you need it. The advantage of this package is the simplicity of implementation and usage. There are no hacks. You just get a forward view and a backwards view. Both act just like injections, and the isomorphism is transparent. This is also correct with initialisation and updating, although that may come at a speed loss. Because the implementation is simplified, a lot of internal and external magic is now gone. This is also doesn't compare equal to non-Injections by default, much like `[] != ()` by default. Getting the inverse of a pre-existing Injection is purposefully not supported. If you want such a thing, build a relevant namedtuple of the two directions and pass that around instead. If you really don't want to do that (you're insane, perhaps), fork the code and add an inverse property yourself. The OrderedInjection prevents use of pair methods, because it complicates equality. The expected behaviour for equality is unclear as it's not obvious when order is relevant. With single-direction injections, only forward order exists. Further, __setitem__ needs additional checking for bidirectionally ordered variants, to prevent deletions from removing order. Despite the copied ideas, the code is my own. Thought credit goes to everyone on the mailing list from https://mail.python.org/pipermail/python-list/2009-November/558834.html For reasons of sanity, all code is public domain. Seriously, do anything you want with it. """ from collections import OrderedDict from collections.abc import KeysView, MutableMapping class Injection(MutableMapping): """ Create an injection from a mapping or iterable, and keyword arguments. An injection is like a dictionary except that values must be unique. To use the injection in either direction, use Injection.pair. Like with dictionaries, setting an item can remove an old pair. Unlike with dictionaries, this happens in both directions: print(inj) #&gt;&gt;&gt; {0 → 0, 1 → 1} # Shares start with 0 → 0 and end with 1 → 1, # so both paths are collapsed into 0 → 1. inj[0] = 1 print(inj) #&gt;&gt;&gt; {0 → 1} """ def __init__(self, mapping_or_iterable={}, **kwargs): self._forward = {} self._backward = {} # The trick in bidict wrecks the order, so don't use it. self.update(mapping_or_iterable, **kwargs) @classmethod def pair(cls, *args, **kwargs): """ Create and return a (forward, backward) tuple of Injections, with backward being the inverse of forward. Mutations to either object affect both Injections. """ forward = cls(*args, **kwargs) backward = cls() backward._forward = forward._backward backward._backward = forward._forward return forward, backward def __setitem__(self, item, complement): # Make sure they're hashable before # destroying things {item, complement} # If we point to something, there's # something poining back to us. Remove it. if item in self._forward: del self._backward[self._forward[item]] # If our target is pointing to something, # we are pointing to it. Remove that. if complement in self._backward: del self._forward[self._backward[complement]] self._forward[item] = complement self._backward[complement] = item def __delitem__(self, item): del self._backward[self._forward.pop(item)] # Shalow wrappers def __getitem__(self, item): return self._forward[item] def __iter__(self): return iter(self._forward) def __len__(self): return len(self._forward) # KeysView &gt; ValuesView, so override Injection.values # to return a KeysView from the inverted dictionary. # # Also override `keys` to return a view directly on the # underlying dictionary, for prettiness and speed. def keys(self): return KeysView(self._forward) def values(self): return KeysView(self._backward) # Printing routines def __str__(self): if not self: return "{→}" pairstrings = ("{!r} → {!r}".format(*pair) for pair in self.items()) return "{{{}}}".format(", ".join(pairstrings)) def __repr__(self): return "{}({})".format(type(self).__name__, self._forward) # More type-strict than default def __eq__(self, other): if not isinstance(other, Injection): return NotImplemented return self._forward == other._forward # Faster than default def clear(self): self._forward.clear() self._backward.clear() clear.__doc__ = MutableMapping.clear.__doc__ # Not in ABC, but dict has it def copy(self): new = type(self)() new._forward = self._forward.copy() new._backward = self._backward.copy() return new class OrderedInjection(Injection): def __init__(self, mapping_or_iterable={}, **kwargs): self._forward = OrderedDict() self._backward = {} # The trick in bidict *still* wrecks the order self.update(mapping_or_iterable, **kwargs) @classmethod def pair(cls, *args, **kwargs): raise AttributeError("{} does not support 'pair' method".format(cls.__name__)) --- Now, one way you can tell that my variant is fundamentally simpler is that the longest method is 7 lines of code (plus six of comments). This well approximates the current `dict` type, largely by using the `MutableMapping` ABC (Abstract Base Class). A fundamentally simple design is easier to maintain and use. 
Did you intend to only include a single .py file (client.py). And client.py is 37 lines and doesn't do anything on its own. The screenshot you have in the readme shows way more code as pointed out above. Unless you have a legal issue with the content of your code, I'd put it on Github, the community may even help you beautify it. I'm not a lawyer, so this isn't legal advice, but if anything it seems the name of the project (Line) might have some sort of trademark issue, not the content of the module. 
What does this mean? "Although that way may not be obvious at first unless you're Dutch."
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Guido van Rossum**](https://en.wikipedia.org/wiki/Guido%20van%20Rossum): [](#sfw) --- &gt;__Guido van Rossum__ (born 31 January 1956) is a [Dutch](https://en.wikipedia.org/wiki/Netherlands) [computer programmer](https://en.wikipedia.org/wiki/Computer_programmer) who is best known as the author of the [Python programming language](https://en.wikipedia.org/wiki/Python_(programming_language\)). In the Python community, Van Rossum is known as a "[Benevolent Dictator For Life](https://en.wikipedia.org/wiki/Benevolent_Dictator_For_Life)" (BDFL), meaning that he continues to oversee the Python development process, making decisions where necessary. He was employed by [Google](https://en.wikipedia.org/wiki/Google) from 2005 until 7 December 2012, where he spent half his time developing the Python language. In January 2013, Van Rossum started working for [Dropbox](https://en.wikipedia.org/wiki/Dropbox_(service\)). &gt;==== &gt;[**Image**](https://i.imgur.com/K929ycB.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Guido_van_Rossum_OSCON_2006.jpg) --- ^Interesting: [^Python ^\(programming ^language)](https://en.wikipedia.org/wiki/Python_\(programming_language\)) ^| [^ABC ^\(programming ^language)](https://en.wikipedia.org/wiki/ABC_\(programming_language\)) ^| [^Van ^Rossum](https://en.wikipedia.org/wiki/Van_Rossum) ^| [^Centrum ^Wiskunde ^&amp; ^Informatica](https://en.wikipedia.org/wiki/Centrum_Wiskunde_%26_Informatica) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjf2evs) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjf2evs)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I don't personally think the differences are big enough to matter. Either one will work, though, you may be better off using the python 2 book and then learning the differences later.
Sometimes you need them though. I'd rather deal with nested for loops than nested comprehensions any day. I will admit I wrote a nested list comprehension that just did a bunch of function calls that didn't output anything because it I was working in IPython at the time.
not False
But nested looks so much cooler...
Thank you very much!! you answered my question!
Nested for loops are one of those things you use because you have to, not because you want to. They feel so gross, but they do their job. And I think I'd rather have a nested comprehension. At least it's just one line. 
It was something I was testing at the time. I needed to pull values from two lists and run them into a function. Since I was already in IPython I just did: [func(x,y) for x in x_list for y in y_list] As for not needing nested loops? There's generally a better way to handle the situation, but I fail to see how using generators would alleviate the potential issue. Consider [Euler #9](https://projecteuler.net/problem=9), this was my solution: outer = 1000 for a in range(1, (outer//2)+1): for b in range(a, (outer//2)+1): c = (a**2+b**2)**0.5 if a+b+c == outer print(a,b,c, (a*b*c)) break How would you have handled that by replacing the loops with a generator? Edit: [Euler 11](https://projecteuler.net/problem=11): n = [ "08 02 22 97 38 15 00 40 00 75 04 05 07 78 52 12 50 77 91 08", "49 49 99 40 17 81 18 57 60 87 17 40 98 43 69 48 04 56 62 00", "81 49 31 73 55 79 14 29 93 71 40 67 53 88 30 03 49 13 36 65", "52 70 95 23 04 60 11 42 69 24 68 56 01 32 56 71 37 02 36 91", "22 31 16 71 51 67 63 89 41 92 36 54 22 40 40 28 66 33 13 80", "24 47 32 60 99 03 45 02 44 75 33 53 78 36 84 20 35 17 12 50", "32 98 81 28 64 23 67 10 26 38 40 67 59 54 70 66 18 38 64 70", "67 26 20 68 02 62 12 20 95 63 94 39 63 08 40 91 66 49 94 21", "24 55 58 05 66 73 99 26 97 17 78 78 96 83 14 88 34 89 63 72", "21 36 23 09 75 00 76 44 20 45 35 14 00 61 33 97 34 31 33 95", "78 17 53 28 22 75 31 67 15 94 03 80 04 62 16 14 09 53 56 92", "16 39 05 42 96 35 31 47 55 58 88 24 00 17 54 24 36 29 85 57", "86 56 00 48 35 71 89 07 05 44 44 37 44 60 21 58 51 54 17 58", "19 80 81 68 05 94 47 69 28 73 92 13 86 52 17 77 04 89 55 40", "04 52 08 83 97 35 99 16 07 97 57 32 16 26 26 79 33 27 98 66", "88 36 68 87 57 62 20 72 03 46 33 67 46 55 12 32 63 93 53 69", "04 42 16 73 38 25 39 11 24 94 72 18 08 46 29 32 40 62 76 36", "20 69 36 41 72 30 23 88 34 62 99 69 82 67 59 85 74 04 36 16", "20 73 35 29 78 31 90 01 74 31 49 71 48 86 81 16 23 57 05 54", "01 70 54 71 83 51 54 69 16 92 33 48 61 43 52 01 89 19 67 48" ] nums = [[int(x) for x in l.split()] for l in n]
That site looks awesome.
`not not not False`
You're right. I'll figure this out. Thanks.
Not 100% sure I understand you. Generators don't change time complexity, they just simplify syntax (in some cases) and save a bit of memory. I'd rather read 4 nested loops than track down 4 nested generators; this only extends the complexity of nesting rather than reducing it. I think the real lesson from this aphorism is that if we have four nested loops, perhaps we should consider a different design or data structure.
TIL: there's a static blog "movement"
after reading the blog, I am inspired to blog about other movements, like my bowels. it has effected me that profoundly
I would not use simplehttpserver. For a task like this , I would use flask [or django even django is a requirement], and then use celery for actually executing the other scripts. So In summary: 1) Have a look at Flask 2) Celery 3) The sh module for executing other non python scripts or system binaries. I find sh makes it trivial instead of using subprocess module to spawn other non python scripts/binaries. Here's a link for flask/celery http://flask.pocoo.org/docs/patterns/celery/ but there are gazillion of resources on the net just by googling. 
 def inner(a): yield from calc_stuff(a) [inner(a) for a in things] That's just disguising the nesting. I mean I get where you're coming from. But sometimes it's easier to read a nested loop than have to refer to a different function definition. If there's *a lot* of logic going on in the loop, then yes, break it down to functions. Other wise, *I* think that: for a in range(1, (outer//2)+1): for b in range(a, (outer//2)+1): is more readable than breaking it into two separate functions. Especially because the two loops are tightly bound. I'll admit that the second example could be written as: nums = [map(int, x.split()) for x in n] As for `[func(x,y) for x,y in zip(xlist,ylist)]` I wanted each value of both lists to be matched, i.e. permutations but it was before I grew to love itertools so much.
I did the same switch 18 months ago. If you haven't tried the pythonxy distribution, give it a go. They setup the spyder IDE very similar to matlab. Happy hunting.
&gt; pythonxy Looks like it's unfortunately Windows only - we opted for generic Anaconda, since it will make Spyder available on Windows and OS X (why do academics love OS X so damn much...?) 
Thank you!!
It's alright - I use windows day to day (mostly because installing Linux on laptops is more trouble than it's worth, and there's always that *one stupid machine* that only ships with windows software. I'm looking at you, spectrophotometer). I do most of my actual work either in a Mint virtual machine or on the beefy headless CentOS workstation sans IDE. My concern is mainly the students - over half of them come in with Macs. EDIT: Thanks for reminding me that I get flair! I'm stealing yours.
Sure, always happy to share. I've got lots more where this came from. Used to only do YouTube but decided to put all of the code online in text as well. Lots to port over from the channel!
 Yep that's me! Slowly but surely porting all of the code from the videos over. 
not not True
So when you restyle your blog in the next few months, won't you need to change a few dozen files, one for each post? I get that there's some simplicity to this, but I don't really see the point.
No, you use a tool like Pelican or Jekyll to do that for you.
He mentioned that he wrote the posts with Pelican, but I must have missed the part where he was able to edit them. The lack of a db makes it hard for me to see how it'd work unless they do HTML parsing.
You have editable files for each blog post that contain only the content of that post. Then you have a template that is used for every blog post. You run the tool and it regenerates a bunch of output HTML files that stick each blog post's content into the global template. Then you upload this HTML to your web host. Change the template? Just re-run the generator command and re-upload. Every page will be updated.
I disagree that this way of working for a company full time is improbable. There are small agencies (especially in the Python/Django space) that are decentralised and completely in a culture of remote work. There is no massive effort needed from any party with tools like Sqwiggle, Slack, Hipchat etc. I think you can work effectively with a team regardless of location as long as you can communicate well. I know a company with this attitude that it hiring right now. PM me if you want details.
Oh I see, thanks for the explanation.
This is great.
I PMed you additional info, btw.
I will definitely take a look at this when I get home.
G'day Dirk, There's nothing wrong with what you're doing ... sleep isn't using any CPU time. The only problem is that if ``foo()`` takes any significant length of time to run then your schedule will run slightly slow, so if running exactly every 60 seconds matters you probably should do something trickier. I'd also tend to write the 'try' outside the 'while' for clarity: try: while True: foo() time.sleep(60) except KeyboardInterrupt: print('Manual break by user') except OtherThingWhichCanGoWrong: print('Other thing went wrong ... stopping!') just because I think that's clearer.
I just put the `try` outside. Duration of `foo()` is not a real issue here because it’s only important that it runs again 60 seconds after it’ done. Thanks for your answer!
No, this time it isn't; just stumbled across it and found it pretty interesting :)
I think I got it. Just to be certain... Rackspace employs you to write open source python to make OpenStack more robust and more attractive/active? And other companies have devs working with you on open source projects because their business uses OpenStack? And if I were qualified (enough experience), I might be able to work for a company that uses OpenStack, and be writing open source software that makes our company's OpenStack services better?
I'm kind of partial to this route. It would be a good way to start with R/pandas and would open many doors I imagine. Thanks for the info.
Yup. I actually restyled the blog recently and that's exactly how it went down. New theme, re-run the generator and push the new files to my GitHub page repository.
...Why? To obfuscate?
Is there a memory leak ? It could be hitting swap and just swapping, try profiling memory, or just watching how much it uses over time, you might find some library you use has a subtle leak.
Why did not use `collections.deque`? It supports left and right appends and pops as well as rotations. Things like `deck2.insert(0, deck.pop(0))` become: `deck2.appendleft(deck.popleft())` and `deck.append(deck.pop(0))` becomes `deck.rotate(-1)`. Deque is also pronounced deck. I know it was for the sake of explaining the algorithm, but it seems like a serious missed opportunity. Edit: Released OP wasn't the author. My point still stands.
I thought Decorators Day was Monday? Isn't Saturday for the Metaclass Stack Overflow post?
Agreed. Maybe he doesn't know about it? I knew deques from C++, but didn't know for a long time that they are in the `collections` library. Btw., if you are interested, here I did some benchmarks comparing it to Python lists for removing and adding items on the left side: http://nbviewer.ipython.org/github/rasbt/One-Python-benchmark-per-day/blob/master/ipython_nbs/day11_deque_container.ipynb
While I'm not surprised that popleft/appendleft are much much faster than pop(0)/insert(0,x), I'm surprised *how* much faster they are. Reverse is a little surprising, but I don't find myself reversing lists very often. Edit: :/ I recently revisited a couple of project Euler solutions I posted to my Github a long time ago, seeing how I could make them better. Now, I'm just confused. # Project 8 # The one with that huge list of numbers. from functools import reduce from operator import mul from collections import deque nums = [int(x) for x in ...] def naive_prods(nums): limit = 13 run = [] largest = 0 while nums: run.insert(0,nums.pop()) if len(run) &gt; limit: run.pop() x = reduce(lambda x,y:x*y, run, 1) if x &gt; largest: largest = x def smarter_prods(nums): run = deque(maxlen=13) largest = 0 for n in nums: run.append(n) x = reduce(mul, run, 1) if x &gt; largest: largest = x %timeit naive_prods(nums) 1000000 loops, best of 3: 229 ns per loop %timeit smarter_prods(nums): 1000000 loops, best of 3: 978 ns per loop So I revisited the even Fibonacci numbers under 4,000,000: from itertools import takewhile def naive_even_fibs(): a,b = 1,0 total = 0 while True: x = a+b if x &gt; 4000000: break if n%2==0: total += x a,b = a+b,a def fibs(); a,b = 1,0 while True: yield a+b a,b = a+b,a def smarter_even_fibs(): nums = takewhile(lambda x: x&lt;4000000, fibs()) total = sum(n for n in nums if not n%2) %timeit naive_even_fibs() 100000 loops, best of 3: 9.11 µs per loop %timeit smarter_even_fibs() 100000 loops, best of 3: 16.8 µs per loop Truly baffling to me. I really thought my year old, naive, just make it do it for now code would be not only uglier (it is) but also much slower. At least I was half right.
I discovered this module on PyPi purely out of curiosity. I just tried `pip install that` and bam, it worked. I thought it was pretty funny. I use it as one of my signatures `import this; import that; del that`.
I use try/except and a while loop.. However, you might want to take a look at https://docs.python.org/2/library/sched.html
good stuff man. I use matplotlib in my job, but because it is so infrequent I often just learn only enough to accomplish the task. This looks like a good set of tutorials to learn the fundamentals on. Keep on posting! edit: wording
Glad you liked `that`!
The faster "pop(0)/insert(0,x)" was basically expected as Bringschuld of a "deque". However, I still was interested in doing this quick benchmark - as you said - to see "how much faster" it really is. Wow, I find you benchmarks really surprising ... nice and good to know
You can also try [schedule](https://github.com/dbader/schedule) ([reddit comments](http://www.reddit.com/r/Python/comments/1f6s7s/schedule_python_job_scheduling_for_humans/)) or APScheduler (I have a blog post on it [here](https://pythonadventures.wordpress.com/2013/08/06/apscheduler-examples/)). `schedule` is lightweight, here is an example: import schedule from time import sleep def job(): print("I'm working...") schedule.every(10).minutes.do(job) schedule.every().hour.do(job) schedule.every().day.at("10:30").do(job) while True: schedule.run_pending() sleep(60) But your solution is also good.
I assume that it's the function call "fibs()" that causes some overhead here. 
Hi @kurashu89, I am the author. Do I know about collections.deque: Yes Why didn't I use it? * I was exploring an algorithm. That bit didn't 'click' * When reviewing the code for style, i missed it. * It was fast enough for my purposes. If the code was slow, then I might have used that optimization. 
My blog. Google blogger does what it does, and I don't want to split/migrate the several years of history it contains. Yet! My apologies.
First of all thanks for being so thorough with the code. The recutsion in __setitem__ was placed there because I didn't feel comfortable with having conditionals that would only be useful in a very particular circumstances yet executed on every setitem, instead I was able to only test whether or not _type_a exists if an AttributeError exception was raised. Just raising the error there without performing the check would lead to some very confusing error messages about _type_a and _type_b that someone not familiar with the code probably wouldn't understand. Returning a generator on __iter__ was the intent yes. I don't see the problem with the way __init__ takes it's arguments, it was made so that a user could just pass pretty much whatever made sense, and it would work, also to behave like dict(). The get() function is supposed to emulate the built-in dict.get() that's why it slurps *args instead of taking a keyword argument. I know the difference between *==* and *is* but could you elaborate on why you prefer one over the other? I'm guessing any increase in performance is probably something the python interpreter could optimize when compiling into bytecode.
I have been writing Ruby and Haskell for work lately and I absolutely hate the way both languages pollute the global namespace every time you import something. Modules being namespaced by default greatly aids readability. I often look at code and wonder where the hell this function or class is coming from.
FWIW, with either approach you can speed things up by explicitly generating only the even Fibonacci numbers (using a little number theory). This is over twice as fast: def fast_even_fibs(): a, b = 2, 1 total = 0 while a &lt;= 4000000: total += a a, b = 3 * a + 2 * b, 2 * a + b return total And we can apply the same principle to the generator (also, we can do slightly better than the `lambda` performance-wise, with the appropriate imports): def even_fibs_gen(): a, b = 2, 1 while True: yield a a, b = 3 * a + 2 * b, 2 * a + b def brainy_even_fibs(): return sum(takewhile(partial(operator.ge, 4000000), even_fibs_gen())) I get (my machine is clearly slower than yours - also I tweaked your versions to actually `return` the result): &gt;&gt;&gt; timeit.timeit(naive_even_fibs) 16.730089578424074 &gt;&gt;&gt; timeit.timeit(fast_even_fibs) 7.720849158203066 &gt;&gt;&gt; timeit.timeit(smarter_even_fibs) 26.07617406681578 &gt;&gt;&gt; timeit.timeit(brainy_even_fibs) 11.386644042748344 But yeah, generators tend to create this kind of overhead, the price you pay for elegance. At least with short data sets, anyway (there aren't a lot of even Fibonacci numbers less than 4,000,000). Builtins like `sum` are frequently faster when passed a list comprehension instead of a generator expression, similarly. ---- Edit: It turns out that the tuple unpacking is a bottleneck as well, at least for CPython. If we go to ridiculous lengths to avoid that: def absurd_even_fibs(): a, b = 2, 1 total = 0 while True: total += a b += a a += b b += a if b &gt; 4000000: break total += b a += b b += a a += b if a &gt; 4000000: break return total We get a noticeable improvement: &gt;&gt;&gt; timeit.timeit(absurd_even_fibs) 4.929859032363083 But at this point, why are you still using native Python? ;) And yeah, I could have used a temporary variable, but that's no fun. ;)
Doctests are an excellent solution for this.
Yeah you can use slicing, which is a really nice approach. But my implementation uses the getitem syntax, because I wanted there to be no difference in how things are looked up, which then required that the internal dictionaries would have different types for keys and values. I originally used isinstance to check the types, but I ended up wanting it to be stricter than that. I think I've gotten all those breaking bugs that you pointed out fixed now.
there are plugins to run the script or you can utilize a smaller side window (with tmux) pointing at the terminal
Personally I think it's easier to handle logging through python, incase something goes wrong
Is the same blog code of code examples in site(webpy.org).
I use notepad++. Vim is for nerds.
cron job doesn't prevent you from logging things in python at all
I agree that using Disqus can create some issues, if people choose to go this way. Interestingly, there are other options for commenting, at least in Pelican. You can look at them here: https://github.com/getpelican/pelican-plugins
Finally! Thank you for this!
then post an exact link, and a link to the entire output.
https://code.google.com/p/pythonxy-linux/ , FWIW 
I want to see tulipcore production ready so I don't have to choose between good looking code and Guido blessed async
Note that typing __foo__ gives "__foo__", but typing \_\_foo\_\_ gives \_\_foo\_\_. --- &gt; The recutsion in \_\_setitem\_\_ was placed there because I didn't feel comfortable with having conditionals that would only be useful in a very particular circumstances yet executed on every setitem, instead I was able to only test whether or not _type_a exists if an AttributeError exception was raised. I definitely see the reason here, but you've ended up adding a try-except. Just adding `if not self._dict_a` at the top, has only 6 nanoseconds more overhead on my computer than the `try` variant in the good case of no exception thrown, so in practice is not something to worry about. You have *far* bigger costs than that; type-checking with `==` alone costs me 20 times as much. &gt; Just raising the error there without performing the check would lead to some very confusing error messages about _type_a and _type_b that someone not familiar with the code probably wouldn't understand. I meant replace `raise e` with `raise`, not to replace the whole lot :). &gt; Returning a generator on \_\_iter\_\_ was the intent yes. I mentioned because not using `yield` or similar generator-centric tools made it less obvious, which is bad for readers of the code. &gt; I don't see the problem with the way \_\_init\_\_ takes it's arguments, it was made so that a user could just pass pretty much whatever made sense, and it would work, also to behave like dict(). You accept a variable number of positional arguments but then ignore all but the first. I suppose you were mislead by the nonstandard implementation of `dict`, but the proper way to write it would be def __init__(self, initial={}, **kwargs): and to use *both* `initial` and `kwargs` in the constructor, like `dict`: dict({"a": "b", "c": "d"}, a="d") #&gt;&gt;&gt; {'c': 'd', 'a': 'd'} Note that when giving multiple positional arguments, `dict` does dict({}, {}) #&gt;&gt;&gt; Traceback (most recent call last): #&gt;&gt;&gt; File "", line 1, in &lt;module&gt; #&gt;&gt;&gt; TypeError: dict expected at most 1 arguments, got 2 so in practice its nonstandard constructor isn't a problem. &gt; The get() function is supposed to emulate the built-in dict.get() that's why it slurps *args instead of taking a keyword argument. `dict` is only how it is for silly reasons about being implemented in C and other boring things. In reality, though, `get` emulates a default argument pretty well: {}.get(1, 2, 3) #&gt;&gt;&gt; Traceback (most recent call last): #&gt;&gt;&gt; File "", line 1, in &lt;module&gt; #&gt;&gt;&gt; TypeError: get expected at most 2 arguments, got 3 Note that going to a keyword argument should use a sentinel instead of `None`, in order to accept `None`: no_argument = object() def get(item, default=no_argument): ... if default is not no_argument: return default &gt; I know the difference between == and is but could you elaborate on why you prefer one over the other? Although `is` *is* a lot faster, my reason is correctness. If you want to check the exact type of things and ignore arguments like inheritance, `is` is better. `is` checks by identity, which isn't overloadable. `==` checks by value. For most cases, `==` should redirect to `is` for classes, but being explicit in these cases has benefits when you want to work on all given data. &gt; I'm guessing any increase in performance is probably something the python interpreter could optimize when compiling into bytecode. Seeing as they do different things, this is not the case. In fact, CPython is particularly bad at optimising this kind of stuff and PyPy , being a JIT, will have at *least* a warmup penalty, and using `is` would probably be extremely useful to a type-specialising JIT.
The crawler is about as simple as they get. The biggest limitation will be speed, since it crawls one page at a time and if any page times out, the whole thing will just stop. Still, it's a good article.
Call me when gevent is ported to Python 3.3+
What country are you in? I'm in the UK and I've only really heard of craigslist through the blogosphere. Freecycle is originally from USA but is reasonably popular here.
This is why I like this sub. :) I knew there was a way to generate only even Fibonacci numbers. Just like there's an algorithm for generating the square of a sum of numbers and one for generating the sum of the squares of numbers. Straight math will always be faster than loops. :p def naive_sum_sq_diff(limit): sq_total = sum(range(limit+1))**2 total_sq = sum(x**2 for x in range(limit+1)) return sq_total - total_sq def smarter_sum_sq(n): return (n*(n+1)/2)^2 - (n*(n+1)*(2*n+1)/6) %timeit naive_sum_sq(100) 10000 loops, best of 3: 54.4 µs per loop %timeit smarter_sum_sq(100) 1000000 loops, best of 3: 1.06 µs per loop But benchmarking those two previous functions showed me quite a few bottle necks. def smarter_prods(nums): run = deque(maxlen=13) largest = 0 for n in nums: run.append(n) x = reduce(mul, run, 1) if x &gt; largest: largest = x Originally, I coded the last bit as: largest = x if x &gt; largest else largest Turns out that made that run time 5ms. Which was a 5000 times *slower*. Once I got that straightened out, the speeds were comparable. I also noticed the very minute speed up of feeding sum a list comprehension rather than a generator comprehension (I think it was about 3ns). Using `reduce(lambda x,y: x*y, run, 1)` also improved speed by about 2ns. These two were small enough to not even be perceivable on *these* data sets. However, I imagine if the data sets grew much, much larger (saying finding the product of 10,000 numbers), you'd be hunting for any speed ups you could find.
I get that. I was just nitpickicking. I enjoyed the article, didn't mean to come off as lambasting you.
The two options that first come to mind are -time how long foo takes to run, subtract that from the sleep duration of 60 seconds -use a "Timer/scheduler" of some sort, setting the next run interval before running the foo() function
Link to code: http://webpy.org/src/blog/0.3 Photo of error: http://i.imgur.com/PeAd68P.png
Yeah :) I need to do some sort of “resetting” the run time every now and then (I already have a function for that \*g\*) and not only at starting the program to wait for a full minute for first run. As of 12 days it’s off by ~36 seconds. Maybe I’m going to add a scheduler running every 2 days or so for adjusting.
Hmmm thanks. Need to find a Mac to test it on.
Liked it better when it was fuckitdb
If I get just one more line cookie-run message tonight, there are gonna be murders! PS. Hi fellow Thailanderer!
How about receiving line messages? Sadly being in Thailand and having everybody use LINE, I would love nothing more than being able to do my line messaging in a tmux session! Looking forward to this!
This is the best explanation I've heard about decorators. I finally understand them now! Thanks! 
I used gevent at work for years. Here are some problems with it: * When you combine it with anything else interesting you can do with Python, things break in mysterious ways. For example, combine gevent with ipython and you get an interactive prompt that lies to you and doesn't do what you tell it. * It's really not that stable. Sometimes the "monkey-patched" functions it sticks in the standard library aren't actually compatible with the functions they replace. * It's stuck in the Python 2 world, and nobody seems to know how to finish the job of porting it. * It's against one point of the Zen of Python -- "explicit is better than implicit". Do you want to have code that does something different when you add a logging statement to debug it? asyncio's strategy where you explicitly yield is much more robust. The realization that so many of our bugs were caused by gevent led to us hunting down "monkey.patch_all()" statements and removing as many of them as possible, and then hunting down "import gevent" the same way, as we found other alternatives.
I too joined this "movement" at the beginning of 2014. You can read about my setup with Pelican [here](https://www.zufallsheld.de/2014/05/11/serving-static-content-and-comments-with-pelican/).
Just a tip with handling non-ASCII text: Remember that the built-in functions .lower() will lower all text, but .casefold() will actually flatten text, so the German "ß" will be turned into "ss" Otherwise I haven't come across many Unicode encoding/decoding problems.
The locale thing is the biggest issue, and it's probably at the root of the other problems you're encountering. (Your filesystem doesn't actually keep its filenames in a mix of different encodings, right? There is presumably one correct encoding that would prevent surrogate escapes.) What Python should do when your locale is wrong is an ongoing debate, but one where the language developers aren't budging so far. A language that's more lax about Unicode, like Ruby, believes that when you ask for "ASCII", you actually want "ASCII plus random non-ASCII bytes that are left alone as much as possible". They call this "ASCII-8BIT", a name that sounded like a baffling contradiction until I understood that this behavior has been a Unix tradition for a while. Whereas Python takes a hard-line stance. If you ask for ASCII, it doesn't give you "ASCII plus whatever Unix does", it gives you ASCII. If your OS asks for ASCII and then passes in data with the high bit set, Python believes your OS is buggy. You know exactly what you're doing when you're encoding and decoding, so when you're in a cron script, just set the locale to the right one. The command will look something like LC_ALL=C.UTF-8 python scriptname.py or maybe LC_ALL=de_DE.UTF-8 python scriptname.py
You could use a thread timer: from threading import Timer foo() while True: t = Timer(60, foo) t.start() t.join() 
Just took a quick glance at it. It appears to me that this could be easily tied into a basic threading script.
I agree, scrappy is much better for the job. I found that I could use it even before writing the article. But I also like exploring simple problems
It could be threaded but you'd have to spawn as many worker as you found URL on every page. It's actually a very interesting problem that I want to resolve later, aka how to share the discovered URLs across the crawlers, and common issue, prevent n worker from starting to work on a same URL simultaneously.
PYTHONIOENCODING is an option, but it won't fix the filesystem encoding; for that you need LC_CTYPE (or LANG, LC_ALL, etc.) set correctly when Python starts. And if you set those correctly, you don't need PYTHONIOENCODING at all. To handle your program to be run from an environment that is missing locale settings, your best bet is to check sys.getdefaultencoding and sys.getfilesystemencoding as early as possible; if they're ascii, then either fail fast with an informative error message or re-exec your program with LC_CTYPE=&lt;something&gt;.UTF-8.
just curious. mind sharing one or two examples of that ?
It is indeed extremely simple. The reason why I built it was because I'm writing an article about plagiarism. As any experiment, I needed a subject to test my theories on. I thought that if I'm able to blindly detect that articles from different sources are talking about the same subject, I'd be on the right track.
If you want to handle broken filenames, most of the os functions accept bytes (e.g. os.listdir(b".")) and will return bytes in that mode. Similarly, you can always open files in binary and perform decoding/encoding yourself.
here's a discussion of python 3 surrogate-escapes: http://lucumr.pocoo.org/2013/7/2/the-updated-guide-to-unicode/#different-types-of-unicode-strings
~~PYTHONIOENCODING is probably a better and more specific environment variable to set than LC_ALL, sure.~~ (nope, I'm wrong, read scatters's comment instead) Setting the locale correctly has been where the story ends for me, so I don't have any expertise on taking more drastic measures to cope with bad locales. If you need to deal with arbitrary bytes in filenames, you could use a workaround like `os.listdir(b'.')` which gives you a list of bytes objects as your filenames. I suspect you can't fix the locale once the Python process is started. Armin Ronacher's approach, in his `click` command-line library, has been to detect when the locale is set to ASCII and show a warning. Unfortunately, the recommendation in his warning is "switch to Python 2". I believe he's using this as leverage because he's the strongest voice on the other side of the bad-locale debate.
I feel that, as a developer, I often overlook existing algorithms (sorting, connection pools, routing...). We got taught not to reinvent the wheel, but it should not mean don't play with that wheel. For example, this article made me wonder about greenlets and sync/communication across processes (which I'm planing to explore soon).
I'll be experimenting with greenlets within the next week or two. I think you're right, this would be the essence of it. I remember watching a video of Feynman explaining that he was studying rules behind the wobbling of a plate. As he was playing with the equations he let himself carried on the subject. Later on he won the nobel Nobel prize as the subject got him exploring quantum electrodynamics. This is pretty much the spirit, keep having fun and explore simple algorithms... I know what you're going to wonder now, and the answer is yes, I'll probably get a Nobel Prize for my work web crawlers :p.
This is the correct answer. Unfortunately, it is not portable to Windows, which stores everything internally as UTF-16. If you ask for bytes on Windows, you'll probably get a mess of ANSI nonsense.
&gt;I guess I must have been doing Unicode right if I've never encountered those before. These only occur if your system is misconfigured, e.g. the locale is UTF-8 but the file names are in Latin-1 for some reason.
&gt;That's an utterly horrible hack. It's not intended to be correct. It's intended to be round-trippable.
Yes, your Python 3.2 code should run on Python 3.3.
I second /u/jnvilo's suggestion of [Flask](http://flask.pocoo.org/). Django is probably overkill for what you have described.
This is the greatest tutorial I've seen in some time. Thanks
This is an awesome tutorial! Were the visualizations also done in Python?
and this is why I have a hard time believing Python 3 is better at unicode than Python 2. It's just differently bad.
well you could then explicitly handle the cross platform issue by checking the os. though that's a pain in the butt. 
can't remember the exact example now (my german classes are from a while ago), but there are definitely words that have a different meaing if you spell them with ß vs 'ss'. As a french speaker this is also a big deal with accents. e.g. "cure" = cure (like a medical cure) vs "curé = a preist
The tutorial was filmed but I moved around a lot and we didn't use slides so I suspect the video wasn't successful. Also, 3 hours of tutorial is kind of boring. What we did was take ~60 people and build a decentralized community with a dozen or so projects, driven by RFCs and interoperability... it was the first time I'd done this on such a scale. Great participants. Most of what I explained you'd get from the ZeroMQ Guide, Chapter 6.
Thanks! I've got quite a few other topics to port over, and quite a few more to cover. Been trying to be 100% Python 3 lately. It's hard with some topics, but so far the transition seems worth it.
As always, happy to share!
tulipcore not asyncio
Haha, sure -- I really don't like this "ambigous" term that means everything or nothing ;). About ggplot: It is already in the list! :)
Thanks for the response. I am just learning how to program and have not learned this yet as I am sure the author wants to first build a strong foundation before going to the more complex matters. I will keep this in mind for the future though.
&gt; It's really not that stable. Sometimes the "monkey-patched" functions it sticks in the standard library aren't actually compatible with the functions they replace. Can you give an example for a function where this is the case? I keep hearing about this when gevent comes up, and I'm curious how this looks in practice, but have never seen an actual example.
Is there any disadvantage in using python's version of ggplot?? 
It's worked great for me in my uses. Monkey patch all is perfectly safe if you know what it's doing
It's not the monkey patching per se, but what happens if a function you patch out is not compatible with what it replaces, which is a problem.
Yes, there is : http://stackoverflow.com/questions/5048329/python-decorator-for-automatic-binding-init-arguments http://stackoverflow.com/questions/3652851/what-is-the-best-way-to-do-automatic-attribute-assignment-in-python-and-is-it-a This way keeps the function signature, while removing all those needless assignments.
It looks like someone came through and downvoted everybody in this thread. Come on, do you need to be *that* petty just because people are talking about Python 2 and 3? People will disagree sometimes. This is okay.
Mostly just read the code and patch as you need
Have you found and fixed those functions?
Many of these would be helpful in the /r/pystats sidebar
Hey, I wrote blucat. I just posted the source on github so it's easier to use: https://github.com/ieee8023/blucat People have asked me about how it works with BLE but I've never had any to play with. I believe it should work as long as they still use the RFCOMM or L2CAP protocols. To go a bit more low level you can also try the Linux tool hcitool which gets closer to the kernel module that blucat does. Blucat was designed to be cross platform. I did add an RSSI feature if that's what you mean by "calibrating." Just connect to a device on an l2cap chennel or rfcomm pcm and pass the cli argument -rssi to have it printed every second. 
It is also (usually) only done if you ask for it. The default behavior is to throw a `UnicodeError` on encountering such sequences, and you can register other possible error handlers (in addition to the numerous built-in handlers) via [`codecs.register_error()`](https://docs.python.org/3/library/codecs.html#codecs.register_error).
I wrote a similar maze-solving routine several years ago, but I had it search from both ends simultaneously, and stop when the frontiers met. I had the impression that it considerably decreased the number of points that had to be searched. It might be much better if it used the Greedy-First algorithm toward the nearest point of the opposite frontier.
It would be great if you could do an ELi5 on this toolkit. (Any way to show visually with graphs or non mathematical terms?)
Not sure what you mean. I was asking for an example for such a function.
&gt; I had it search from both ends simultaneously, and stop when the frontiers met. On several of the algorithms at /u/Xylon-'s link you can tick a "bi-directional" box that does exactly this.
True, things which cannot be easily recovered from default to `'surrogateescape'`, but you can easily convert back to bytes and re-convert to Unicode using your preferred error handler. It's round-trippable, so you will not lose data by doing this.
Thanks for sharing, OP. Been wanting to learn about this and now that it's in my face in Python, I might as well.
cool, thanks!
You mean compared to "R" or compared to Python's matplotlib? I think that ggplot is a nice alternative "approach", but somehow I never got warm with the syntax (it's just personal taste and I like matplotlib better)
Edit: I posted the code to pastebin. http://pastebin.com/7kGHrj0R
Is this a class assignment?
Everytime I buy an programming book, either the cover gets destroyed or I lose it. There's nothing you can learn in books that's not already online
Depending on how large the html files are, it might be worth it to try and parse it yourself. You'll have to loop over the contents of each file and pull out all of the tags. If you are working with 1000s of different elements in one page this technique may not work. But if you have a pretty basic bootstrap template html page (or something similar) with some navbar stuff, a couple container/row/col sequences and then a footer, chances are you can parse out the tags quite readily yourself!
What about this? #! /usr/bin/env python3.4 import sched, time def foo(): print("Foo!") def main(): s = sched.scheduler(time.time) def run_task(): try: foo() finally: s.enter(60, 1, run_task) run_task() try: s.run() except KeyboardInterrupt: print('Manual break by user') return 10 return 0 if __name__ == "__main__": exit(main()) Although I still believe that the code you proposed is actually better (you probably should move try-except outside of the loop, but if the interval is one minute it probably doesn't matter). The scheduler that I showed here is probably best used when you call more than just one function, and even better when the functions are inserted based on events. I also have different approach: #! /usr/bin/env python3.4 import threading import signal import sched def foo(): print("Foo!") class PeriodicEvent(object): def __init__(self, interval, func): self.interval = interval self.func = func self.terminate = threading.Event() def _signals_install(self, func): for sig in [signal.SIGINT, signal.SIGTERM]: signal.signal(sig, func) def _signal_handler(self, signum, frame): self.terminate.set() def run(self): self._signals_install(self._signal_handler) while not self.terminate.is_set(): self.func() self.terminate.wait(self.interval) self._signals_install(signal.SIG_DFL) def main(): task = PeriodicEvent(60, foo) task.run() return 0 if __name__ == "__main__": exit(main()) This one is a bit more explicit and handles SIGINT (Ctrl+C) and SIGTERM (kill &lt;pid&gt;). It's especially nice when you use threading and want to implement a clean shutdown. I use Event's wait() without timeout on async based code. For example halting the main thread when using Kazoo module. So as you can see there are many different solutions, some are better for specific cases. For your simple case of executing a single task per minute, while True and sleep() are probably the best options. Edit: from one of your responses I see that timing might be important to you. In that case you could try to use first example and replace enterabs() method instead of enter() and/or alternatively move the scheduling call from the bottom to the top of the function.
The dumb answer is that you could always attempt to decode it and check for an error.
The basic idea in my understanding is that everything that can generate surrogate escapes does so because only returning `bytes` would confuse new users. When possible, just use the `bytes` apis. When not, decode to bytes. Use the bytes everywhere (for you're assuming don't know its encoding). All of a sudden it's sane behaviour.
r/cscareerquestions
&gt; r/cscareerquestions Thanks.
Thanks, deleted and moved.
In your situation, I would investigate [FastCGI](https://docs.python.org/2/howto/webservers.html#fastcgi-and-scgi). With this option, each Pyhon program will be an instance, and that one instance can be reused for each request. If you want one instance total... well, you could try making one FastCGI program and have it act as a gateway to the other programs, depending on what your requirements are.
mainly because I am not that skilled of a programmer. I am a network/systems engineer with relatively little real programming experience. I was hoping that there was a simple solution to this as I am sure there are much smarter people with similar use cases that would have figured this out by now.
Someone else I am chatting with just mentioned FastCGI. I am looking into it now.
You did not specified which application server you are using for serving cgi script in nginx (it does not manage them without external help). uWSGI is one of the most complete cgi server for nginx, and has the advantage that it can cache the interpreter for specific extensions: http://uwsgi-docs.readthedocs.org/en/latest/CGI.html#example-10-optimizing-cgis-advanced 
Depends on the data and the program, but you could always write the data to file as you are processing it instead of assigning to a list and then looping through the list. 
FastCGI is only a communication protocol, unless you develop a server speaking the protocol and implementing your specific need, what you will find (like the flup project) works exactly like uWSGI and all of the others WSGI servers: loads the app persistently in memory and serves it at every request.
Hi. I'm working on a personal project at the moment that mighy meet your needs but it's not ready for release yet. If you could give an estimate of approximately how many data rows are in a typical csv file and the typical complexity of the `secret_science` function, I could run some tests and if it works out I may be able to send you a pre-release version of my software. OTOH, would a spread-sheet be a possibility?
Coming from a PHP world where you are used to having everything in separate scripts, you are likely coming at this the completely wrong way when it comes to Python. Python web applications are simply not done that way and certainly with some WSGI servers you will be fighting them all the way if you try and do it that way without good knowledge of how those WSGI servers work and how to best set them up. What I would strongly suggest is you step back and stop overly worrying about the hosting mechanism and first learn what would be regarded as being best practice for Python when writing web applications and not try and impose your PHP thinking. To that end, I would strongly recommend you start out by learning how to use the Flask micro web framework as it provides a simple to understand system which is a very good example of the Python way of doing things. If your reaction is that you don't want to use a Python web framework, then you certainly will not be the first person coming to Python from PHP who felt they knew better and so who had that view. If you do care to ignore the suggestion, then all I can say is that you will definitely make your life harder. Reality is though that the type of model for development which you give the impression of wanting is nearly never actually used in Python. This doesn't mean it isn't possible and someone with good knowledge of Apache/mod_wsgi and how to make use of Apache's own URL routing mechanisms for mapping to distinct WSGI script files could do it, but the fact is that it simply isn't a good way of doing it even if it is possible. It is always going to be better to adopt the Python way of doing things and use a web framework. BTW, the naysayers who claim that Apache is overly bloated simply don't know how to set up Apache and a WSGI server module such as mod_wsgi properly. Many people have the wrong impression because mod_python did have some issues, even when Apache was setup properly, but mod_wsgi doesn't have the same issues. 
thanks for sharing!
Yes it was [meditation](http://en.wikipedia.org/wiki/Guru_meditation). 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Guru meditation**](https://en.wikipedia.org/wiki/Guru%20meditation): [](#sfw) --- &gt; &gt;__Guru Meditation__ is an error notice displayed by early versions of the Commodore [Amiga](https://en.wikipedia.org/wiki/Amiga) computer when they [crashed](https://en.wikipedia.org/wiki/Crash_(computing\)). It is analogous to the "[Blue Screen Of Death](https://en.wikipedia.org/wiki/Blue_Screen_Of_Death)" in [Microsoft Windows](https://en.wikipedia.org/wiki/Microsoft_Windows) operating systems. It has later been used as a message for unrecoverable errors elsewhere, such as [Varnish](https://en.wikipedia.org/wiki/Varnish_(software\)), a [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy), and HTTP accelerator, and [VirtualBox](https://en.wikipedia.org/wiki/VirtualBox), a [Hypervisor](https://en.wikipedia.org/wiki/Hypervisor) environment. &gt; --- ^Interesting: [^Guru ^Meditation](https://en.wikipedia.org/wiki/Guru_Meditation) ^| [^Crash ^\(computing)](https://en.wikipedia.org/wiki/Crash_\(computing\)) ^| [^Maharishi ^Mahesh ^Yogi](https://en.wikipedia.org/wiki/Maharishi_Mahesh_Yogi) ^| [^Joyboard](https://en.wikipedia.org/wiki/Joyboard) ^| [^Kickstart ^\(Amiga)](https://en.wikipedia.org/wiki/Kickstart_\(Amiga\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjgdlx6) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjgdlx6)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It would lose optimality though. 
uWSGI doesn't require you to do that. Just create an emperor, point it to your instances and let them die and recover as traffic surges and dies down. It's a question of RTM.
&gt; The problem is, Python 3 insists on decoding text it can't necessarily decode cleanly (i.e. it doesn't know the correct encoding), and as a result it silently hands your code invalid Unicode objects. Any *valid* utf8 byte sequence (this includes ascii of course) will decode to a *valid* unicode string. Any *invalid* utf8 byte sequence (i.e. containing chars &gt;127 that are not utf8) will decode to an *invalid* unicode string. Consistent, isn't it? Furthermore, the invalid unicode string is not randomly broken - it can be round-tripped to the original byte sequence. The surrogateescape idea is to ensures that if your ecosystem uses utf8 consistently it will just work automagically. If you don't, you will either preserve the legacy encoding through roundtripping or generate an error. Fine-tuning which of these two responses is generated in each case is your responsibility, though. Encode 'strict'ly when you want to ensure you are generating valid data for the next processing stage. Encode 'surrogateescape'ly when you want to preserve legacy ancoding data and not accidentally destroy on-disk data by processing it incorrectly.
collections.Counter
Clean implementation of [Timsort](http://en.wikipedia.org/wiki/Timsort) in Python: sorted(arr) :-)
An extremely silly implementation of quicksort using generators: from functools import wraps def receiver_sender(f): @wraps(f) def _f(*args, **kwargs): it = f(*args, **kwargs) x = None while True: x = yield it.send(x) if x == receiver_sender.full: yield ... def new(*args, **kwargs): it = _f(*args, **kwargs) next(it) return it _f.new = new _f.full = receiver_sender.full return _f receiver_sender.full = object() @receiver_sender def gsort(pivot): left = None right = None while True: value = (yield) if value == gsort.full: break elif value &lt;= pivot: if left is None: left = gsort.new(value) else: left.send(value) else: if right is None: right = gsort.new(value) else: right.send(value) if left is not None: left.send(gsort.full) yield from left yield pivot if right is not None: right.send(gsort.full) yield from right def gsort_wrapper(seq): sorter = gsort.new(seq[0]) for v in seq[1:]: sorter.send(v) sorter.send(gsort.full) return list(sorter)
Not really no, that is the reason why PHP is still kind of popular and big. You cant with Python just drop a .py file somewhere and have apache or nginx deal wit it, like they can with php. Oh no, you have to set up routes and an entry point for your application, see application not html-generating script, and then you need to worry about process prefork or threading model to use and how to configure that and so on. But once youve done all the setup required, then you can claim your language and framework is better than PHP. Oh right, the reason its like this, is because Python is not made to output HTML, there is no explicit http request and response dictionaries in every script available as global variables as in PHP. You cant just call a .py script and expect textual output, it may in fact actually perform something useful instead of generating a personal homepage. With PHP for 90% of its existence, if you run a php script even from command line it would generate text and nothing else. Apache/nginx modules just know to hand it request/responsse and other stuff in global vars, because fuckit. With python, we have wsgi to clearly define what and where the apache/nginx/modules can call, so your script anyway needs to implement that interface, or your web application framework.
1. `WORDS` makes more sense as a list than as a tuple. 2. Instead of excessive string manipulation, I'd write the code to generate `jumble` as follows: jumble = list(word) random.shuffle(jumble) jumble = ''.join(jumble) 3. You don't need the parentheses in `while (guess != correct) and (guess != ""):` 4. Instead of: guess = raw_input("Your guess: ") guess = guess.lower() else: guess = raw_input("Your guess: ") guess = guess.lower() use: guess = raw_input("Your guess: ") guess = guess.lower() 5. As you have noticed, using nesting for repetition doesn't work. You use a loop for the game from the second time around, but why not start with the loop? # set up things that only need set up once (WORDS = ...) again = "y" while again == "y": # set up game (word = ..., correct = ..., jumble = ...) # first guess while guess != correct and guess != "": # next guess if guess == correct: # ask again 6. Also, you end a game when pressing enter, but start a new one without asking! You can really only quit after winning. Solution: if guess == correct: # ask again else: again = "n" 7. Also, for hints you use yes/no, but for again you use y/n. It's good to be consistent.
I agree. I love that sub. (Or any Python-related sub.)
Quicksort is usually implemented in place, requiring hardly any extra space, or implemented with one secondary array to allow stable sorting, requiring O(n) extra space. This version makes a new copy on every recursive call and therefore takes O(n log(n)) extra space in the best case, O(n^2 ) in the worst case. 
It's creatively called [Understanding Computation](http://computationbook.com), picked up the ebook version the other day for half price from orielly and so far it's been pretty good, but I'm only a few chapters in. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Tf–idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf): [](#sfw) --- &gt;__tf–idf__, short for __term frequency–inverse document frequency__, is a numerical statistic that is intended to reflect how important a word is to a [document](https://en.wikipedia.org/wiki/Document) in a collection or [corpus](https://en.wikipedia.org/wiki/Text_corpus). :8 It is often used as a weighting factor in [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval) and [text mining](https://en.wikipedia.org/wiki/Text_mining). The tf-idf value increases [proportionally](https://en.wikipedia.org/wiki/Proportionality_(mathematics\)) to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to control for the fact that some words are generally more common than others. &gt; --- ^Interesting: [^Vector ^space ^model](https://en.wikipedia.org/wiki/Vector_space_model) ^| [^Information ^retrieval](https://en.wikipedia.org/wiki/Information_retrieval) ^| [^Okapi ^BM25](https://en.wikipedia.org/wiki/Okapi_BM25) ^| [^Gensim](https://en.wikipedia.org/wiki/Gensim) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjggwa1) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjggwa1)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
yes plz
http://bit.ly/badqsort - I guess the title should have included "Inefficient". This is just a use case of list comprehensions.
I think the point is to help prevent people from posting beginner programming questions in this sub.
[/r/learnpython](http://reddit.com/r/learnpython) is such a wonderful sub. People are friendly and willing to help at all times. It's one of the best subs on Reddit.
Don't let Reddit know that. They might make it a default. We all know what happens to defaults.
What do you consider beginner programming questions? I ask specifically because /r/math was all upset over people posting "beginner questions" a few months back but no one ever defined what constituted a beginner question. Everyone, and I mean everyone, assumed that anything below their level was a beginner question (and many of them have PhDs in math) which really isn't fair to the entire community
A little info on it here... https://www.kickstarter.com/projects/tomchristie/django-rest-framework-3
&gt; ue instead of ü is like having to use vv because your software doesn't understand w. ha ha, now, i get the humorous picture of the painful situation ... 
That's a good point. I guess by "beginner" I mean people who: * don't understand why their code "doesn't do what I mean" * have syntax questions (usually beginning with "how do I...") * haven't googled their easily-googled problem I think the first two are pretty easy to get people to self-filter themselves with a sticky (something like "Don't understand why your code doesn't do what you expected? Have questions about how Python works? Post on /r/learnpython!"). The last point would be harder to pull off without sounding unwelcoming, though it is an important skill.
Got it, asyncio's original name was tulip, thus my confusion. I hadn't heard of tulipcore before and it appears to be built on top of asyncio: https://github.com/decentfox/tulipcore
How about a CSS rule for prefilled text in new post submit box: "Would this question be better posted to /r/learnpython?"
 ('?,'*len(fileId))[:-1] Then pass fileId in as the subtitution tuple, not as the 0th element of the tuple.
honestly, I don't know why i chose Tornado. It's hardly asynch and on GAE i have to give up that feature anyway. In your opinion, what's a good, scalable, asynch framework I could use instead of tornado?
There is no direct parameter subsitution for lists. You need to build a list of ?'s and then pass that along to the SQLite. http://stackoverflow.com/questions/4788724/sqlite-bind-list-of-values-to-where-col-in-prm explains this... Something like sqlstr="SELECT ... IN " + ",".join("?"*len(files)) Then pass your tuple in at the end. Something like the following might work. def filterOnFileId(fileId): retVal = [] db = sqlite3.connect(dbPath) with db: cursor = db.cursor() cursor.execute('SELECT DISTINCT tags.tagId, tagName FROM (files INNER JOIN fileTags ON fileTags.fileId=files.fileId) INNER JOIN tags ON tags.tagId=fileTags.tagId WHERE files.fileId IN (%s)' % (",".join("?"*len(fileId))), fileId) for row in cursor: retVal.append((row[0], row[1])) return retVal
This *as well as* a gigantic sticky post.
that is because i was not using an application server for cgi. I was just going old school and starting scripts with #!/usr/bin/python or #!/usr/bin/env python and running from there.
Bunch o' a-hole fucktards is what they are!
True, I've been working with 2.5 for too long!
I see this, or similar responses a lot. That I, or others asking the same question are looking at it the wrong way. But it is not my php background that is clouding my expectations. It is my background as an old school unix/linux admin and my use case. What I need is the ability to throw together tiny scripts to do simple tasks with low overhead, high reliability, low latency, and variable granularity. This isn't a web application. I have complex monitoring tools that can scrape web pages, parse output, graph data, and send me reports and notifications. But what I don't have is simple web interfaces for it to scrape. In this case I am using the gpio pins on a pi to interface with an airflow sensor, an oxygen sensor, and a couple of other devices. I have been provided with python libraries for some of it already and the rest I will just be using the already solid gpio libraries. In this use case most of the scripts will not be called frequently. But If I or someone else wants to view data in real time we might set our monitor to re-pull every 1-30 seconds for some unknown period of time. Writing the scripts, and setting up cron jobs to update static pages would be lowish overhead but not give me the variable granularity I need. Old school cgi scripts calling the interpreter every time would work, but when we put our monitor into real-time mode the overhead could cause latency issues that might interfere with result accuracy. And using something like uWSGI would be overkill 95+%. If I pair uWSGI with a couple dozen scripts/pages then that is a lot of memory overhead for mostly idle workers. Your response was well written and polite. But it, like others, seems to push a "This is the python model" ideal that does not fit all use cases.
...as far as you know.
If you're on RES, go uncheck "use subreddit style"
Am I hearing a little bitterness in your voice there? I get it, I did a little work with PHP in the past; and for the most part it just did what I needed it to do when I needed it to do it. But I am not hear to badmouth python. It has its advantages and its drawbacks. I only used PHP as an example because I needed something to illustrate what I was hoping for. Due to my need to use specific libraries, that have been provided to me, I need to use python and I came here to get assistance with my specific use case. 
Hmm. So I was once playing with a fitbit that I couldn't get to show up on scans. It seems like it needs a GATT service which I don't know anything about. Here is the android source for their BluetoothAdaptor which connects to BLE devices. It might help to see how they do it. https://android.googlesource.com/platform/frameworks/base/+/refs/heads/master/core/java/android/bluetooth/BluetoothAdapter.java This post talks about using hcitool and the lescan method: http://joost.damad.be/2013/08/experiments-with-bluetooth-low-energy.html One you have the MAC I think you should be able to talk to it like a regular Bluetooth device. I'm not sure though. 
Playing around with hcitool and gatt stuff now. I'll let you know what I find. At the onset, there seem to be different scans for classic vs ble - "scan" and "lescan". 
Thank you so much! http://www.reddit.com/r/Python/comments/21163n/explain_it_to_me_like_im_five_asyncio/#cg8yrex : &gt; http://www.reddit.com/r/Python/comments/1ozd0a/how_does_tulipasyncio_relate_to_go_and_clojures/ccxlmbk : &gt; &gt;&gt; https://en.wikipedia.org/wiki/Concurrent_programming &gt;&gt; &gt;&gt; * https://en.wikipedia.org/wiki/Asynchrony &gt;&gt; * https://en.wikipedia.org/wiki/Asynchronous_I/O &gt;&gt; * https://en.wikipedia.org/wiki/Non-blocking_algorithm &gt;&gt; * https://en.wikipedia.org/wiki/Futures_and_promises ... HTTP/2.0 for Python: http://www.reddit.com/r/Python/comments/1xn24p/http20_for_python/
This article would be more convincing if it used a real-life example instead of Foo's and Bar's, because I currently don't see the point in *re*-raising an exception.
Alright so the emperor would be helpfull, but it looks like it still loads every script in memory regardless of frequency of use. Yes it eliminates the need to update and restart my web server every time I deploy a new script. Replacing that process with the need to build a config file for the "app". I put app in quotations as a script really isn't an app until you start bolting all this other stuff onto the outside of it. 
What about it is unreadable? I think it's really clean and usable. 
Imagine you are writing a big api. The api needs to define many sorts of exceptions to handle common failure modes of your target application. Obviously some failures are in the default namespace, but some are highly specific to you project. At the back end these failures manifest as a variety of errors that are also either global or module specific (not in the global namespace). Your choice is to either alert the user to import those exceptions or you can wrap an exception in your custom exception by re raising. 
You have a valid point here. I can understand their motivation, for example on Windows file system names are not string of bytes but truly Unicode names. Also, to complicate things ZFS for example has UTF-8 support. Granted that by default has standard POSIX behavior you can enable it to enforce the encoding. Stuff like this makes things more complicated, I suppose they could have an option that would allow to disable decoding and return file system objects as bytes, but then how such script should behave in non POSIX system that supports encoding. This is IMO a huge mess with not obvious solution. Although I would say it is actually POSIX fault here.
according to this i can't use tornado's asynch features on GAE https://groups.google.com/forum/#!topic/python-tornado/E0NesRVi3qY
Commenting here so I can edit this post when I'm not at work. I think a simple Flask / Bottle server would work for your needs, be happy to talk to you about it later today. I'd set up the Bottle / Flask server to serve 2 directories, /static/ and a /python/ and then use some simple templating and boom, i think it will deliver what you want. Not sure that Nginx or Apache is really required for the low amount of traffic you're stating in your use cases. they will handle your 1sec polls just fine as there are only a few people doing that at once.
Hmm I appreciate your help. Most of that went over my head but I will look into it and see what I can learn. Thanks again.
Let me try to explain. Event driven frameworks like Tornado need something to tell them when they have more work to do. By monitoring a connection from each of many clients for work, a single Tornado process can do a little bit of work for each client. If we need to do a long running operation on each request, we can still do work for other clients. At a bare minimum, we need to be able to access each connection sent by clients to our server. We also need to be able detect when any client wants to send/receive more. Such connections are represented in the kernel, the core program of most operating systems, as something called a "file descriptor". Think of it as an object that allows the Tornado server to read client data from and write a response to. A file descriptor can signal that it is "ready" for work. Google App Engine doesn't grant you access to those file descriptors, or anything else for that manner. It simply hands your Tornado service a pre-digested bundle of information and says "This is all you have". Notice how we don't have a way to ask GAE "does your client have more work for me?" So you can't balance work any more by doing a little on each. There are many ways to ask a number of file descriptors (clients) if they're ready with different levels of performance. One of the oldest methods is the UNIX select() function. It doesn't scale super well. One the other end, one of the newest functions to use are epoll and kqueue. Both perform better than select() on very large numbers of file descriptors. Hope this helps enough to interest you in learning more about sockets, evented servers and how everything plays together! 
Wow, thank you for that response. Very helpful.. Regarding this line: &gt;Notice how we don't have a way to ask GAE "does your client have more work for me?" If I'm understanding this correctly, it sounds like it's impossible to do asynch on GAE? I know google does auto-scaling but I'm not sure if that makes up for the lack of asynch
If there are no `bytes` apis, just decode with `surrogateescape`. It round-trips for a reason. Unfortunately there's no way to get `bytes` from the start, but that's largely because of OS concerns methinks. On the platforms you need to handle pure `bytes` with, handle `bytes` there. On the ones need to handle actual `unicode`, use actual `unicode`. It's not great, but there's some sort of sense to it.
Pretty much, I would say so. GAE appears to abstract/hide away access to what we want. According to https://groups.google.com/forum/m/#!topic/google-appengine/P-1Gpwpry7w , one still can't use the oldest method for "checking for more work" (the select() function), even though they offer a little more in terms of access to our client connections. 
I will look into flask/bottle. I have normally avoided frameworks as an old school shell/utility scripter. But it looks more and more like there isn't a way to do this simply.
I use it when I have 2 or more different toolkits being used, e.g. in a facade pattern. LibraryFoo raises a FooException when encounters some error, and LibraryBar raises a BarException when it raises a semantically similar error. So in my facade I'll raise a SophaclesWrapper exception for both, and let the higher level logic do what is needed to rollback or recover or whatnot.
Adding context info is key to about 90% of the time I use `raise from`, mostly to help clarify where an error might be occurring. For example one of my network-aware programs instead of raising some obscure socket error I catch that and provide a bit of context around it "failed to ping to server, is it online and accepting TCP on port 1234?"
For anyone interested in doing image processing in Python I suggest giving [Scikit-Image](http://scikit-image.org/) a look too. I find it to be nice and simple to use.
Hooray, you are ready to start learning information retrieval. There are a bunch of metrics one might use here; perhaps you could start with jaccard similarity?
I always wondered what kind of clique has the Python foundation with Caktus Group. Why nobody else can do the website and get the free (kind of) publicity.
That's exactly what I was looking for. Thanks!
Or *you* can import those exceptions and expose them as part of your API.
Thanks for the helpful answer.
Would this help? https://uwsgi-docs.readthedocs.org/en/latest/Broodlord.html
- how about `elk.Has` instead of the wordy `elk.ElkAttribute`? - werent builders lazy by default in Moose? I would prefer them to be lazy so that dependency chains could be specified without the hassle of declaring each attribute as lazy 
I think bottle would be best for you, its very simple, I'd be happy to help you write the app your looking for if you are having any trouble. Send me a PM with any questions
Anything would be better than ElkAttribute at that point :)
cron
This broodlord/zerg modes look more like they are intended for scalability. Unless I am reading this wrong it still spins up at least one worker resident in memory at all times and you still need to specify a zerg instance for each app. So If I have a couple dozen scripts, but normally only call one or two a day I am wasting a lot of memory. Though when I have a monitoring system start aggressively polling it looks like this could protect me from instances of locked or hung workers. 
One other advantage is that of making it easier to debug *accidental* error coverup, due to bugs in your exception handling logic. Eg. maybe there's a subtle bug in your error logging that only triggers on a certain exception type or scenario. Previously, you'd just get the error logging exception, but actually seeing exactly where the error that triggered that bug in your logging code can save you a bit of time in understanding what is triggering the error.
Why not? It's not one that's mentioned much after all. "Most underrated" doesn't mean best, just the one which he thinks has the lowest praise to usefulness ratio. Most of the time, that's going to be a fairly boring one by the very nature of that metric.
`os.fsencode(string_you_got_from_elsewhere).decode(sys.getfilesystemencoding(), errors='whatever')`
lets google that thingy &gt; jaccard similarity 
You can restart workers after X requests. Although locked workers are the result of bad coding.
Very little detail to go by here. However it might be that the task somehow brings the worker into a state where it can't respond to signals. By default `SIGTERM` is used according to the [docs](http://docs.celeryproject.org/en/latest/userguide/workers.html#commands). You could try the following (in increasing order of desperation) `SIGINT`, `SIGQUIT`, `SIGKILL`. That being said, I would seriously review the design of the task if having to cancel it midway during execution is a routine occurrence.
VVhat's so funny? VVhere's the joke?
How do you use Annotations? Are they just a form of documentation or do you use the values somehow?
my coding experience is not extensive. locked workers would be a real possibility.
These are great, but I find that most work is in the data collection, transformation, and analysis space. Here's a couple of tools I use every day for that: * https://github.com/kenfar/DataGristle * https://csvkit.readthedocs.org/en/0.8.0/ In particular gristle_determinator &amp; csvstat are useful when you're always getting new csv files to analyze.
Yeah I was a bit lazy. Here are few examples: * Libraries that abstract the same task over different implementations. Example: a protocol library abstracting different transports. The transports don't raise the same exceptions (and not in the same way) - so it's a good idea to unify everything. * Dealing with broken libraries. At some point you may have to use library has bugs or it's badly designed: it will raise the wrong errors. 
Isn't docx just XML or similar? That's easy to parse with BeautifulSoup or something. 
My eyes started to cross trying to figure out what was going on here.
DocX files are zipped bundles of XML and contained objects (images). Create a temporary directory, copy a .docx in there, rename it to .zip and unzip it, then start having a look at what's in it.
celery is excellent
As I said, you can still use the model you would prefer with certain WSGi servers, but you do have to understand well how those WSGI servers works and be a little careful in setting up your separate scripts so that they don't use conflicting global data. In other words, you do not have to use a setup which would see each WSGi script running in a separate process or even sub interpreter instance with a resulting bloat in memory usage. For Apache/mod_wsgi at least it is possible to have the distinct WSGI script files all execute in the same process and interpreter context. That is, they all run in the same persistent Python interpreter instance and share the same memory. Apache/mod_wsgi will worry about loading the scripts and dispatching different URLs to the appropriate one. This will cut down on memory as not duplicating everything, but because they run in a shared memory space, you just have to be mindful of cross impacts between scripts and possibly multithreading considerations if you use a configuration which uses multiple threads for concurrency instead of processes. Although, from what you say you may well get away with a single process with a single thread if requests are infrequent and aren't long running. Given the multitude of options available for uWSGI, then this sort of configuration may be possible with it as well, but I am not 100% sure. There are no other WSGI servers for Python that I know of which you would be able to do it with without having to at least use some Python code to implement a script loader and dispatcher. It isn't particularly hard to do that, but for most people at that point they would be better off fitting what they do in a web framework where that is done for you. So I am more than happy to explain how you can do what you want with Apache/mod_wsgi, setting it up so it doesn't consume huge amounts of memory, but on Reddit isn't the place for that and you are better off jumping onto the mod_wsgi mailing list which is listed on the wiki page for the mod_wsgi site.
The point is that those exceptions are internal, not to be handled directly by the end user and potentially even implementation specific, so we explicitly don't want to expose them as part of the API.
&gt; https://csvkit.readthedocs.org/en/0.8.0/ Thanks! How could I forget `csvkit` ;). I didn't add DataGristle, yet. It looks cool but it seems that it is not as mature as the other projects, yet. But I will definitely keep it on my radar! I added it under a "Data formatting and management" section because I spontaneously associate "data transformation" more with linear algebra (e.g., principal component analysis) than with formatting a data file.
cron is built in to the os, no extra dependencies to install. 
I believe /u/alenajoykrieger was specifically talking about "How do I..." questions that are answered with a syntactic trick (e.g. slicing, `break`, `return`, etc.) rather than an explanation of some library's API.
I know but since it was vague I'd rather toss in the 2 cents of a beginnimediate Python user.
cron is a completely different thing, and doesn't scale beyond one machine. Also, what's wrong with extra dependencies? There's a huge ecosystem of awesome Python modules out there. Why re-invent the wheel (more shittily) just to avoid dependencies?
That would create a tighter coupling between the client and your module's dependencies. If those exceptions change, not only do you have to modify the module, but it would also force the clients to do the changes as well. 
Endless conspiracy rants about Edward Snowden and things that are "just like the Nazis"?
Wouldn't non-beginner questions be better directed to Stack Overflow though? And I guess beginner questions to /r/learnpython, and... um... no questions here? Hmmm..... Lots of language popularity metrics use number of Stack Overflow questions as one of their indicators. None I know of use Reddit indicators (other than me personally). I guess technically the fact that we have a place... or places.. to talk about Python hurts the perception of Python's popularity, paradoxically enough. 
Thanks all, i've got more than enough to look into here to simply the code. /u/donnieod - Typical number of rows in a csv file is small to medium size, ranging from 100 - 100,000. The functions themselves will likely grow in size as I continue working, but for now they are pretty basic equations such as soil moisture balances or energy balances at the surface of the earth for various locations. 
You wouldn't need to change any client code as long as the exceptions have the same name, which is easily arranged.
only docx
I dont know, I'm the kind to alias main modules as a single letter sometimes. `e.Has .... ` is cute enough. elk.HasAttr .. \#notInspired
If the underlying exception gives more information, I don't catch it. Let it bubble back up. Less code for me, and more valuable information for the user.
This is largely what I was thinking when saying roll your own. Using something like flask would work well, or any of the serving frameworks void of nginx amd apache. I tend to go this route because I am only serving python and nothing else, flask is one example of many that may help. Twisted is another that is very much worth looking at. I am a devops engineer, I get your hesitation in writing the code. its a worthy investment pf knowledge though. I personally would enjoy getting away from sysadmin work and into full app development. 
very heavy. 
Thanks for the input. I was trying to avoid Apache due to the overhead of it's worker model, I see there is an event model that is supposed to be stable in the most recent release and be more resource friendly. But I will look into mod_wsgi and if that meets my needs I will reevaluate Apache. 
IDE's can enforce them, but I've never actually really seen a usage that seemed elegant.
You need one more option to `celery`: `--concurrency=1`. By default it uses one process per CPU. This limits it to one. You may also want to specify `--autoscale=1,1` to prevent it from automaticaly scaling the number of processes. I think it defaults to NOT autoscale, but you never know - it could change one day. I don't think you can rely on FIFO. I get around it by manually executing tasks one at a time, waiting for the response each time, but my use-case is atypical. I mostly use celery as a way to distribute tasks across the network. EDIT: As for where to run the celery worker command, usually you want to run it as a daemon. There are a few ways to do this: - Set up init scripts manually via [the docs' example](http://celery.readthedocs.org/en/latest/tutorials/daemonizing.html). - Some Linux distros (Ubuntu and Debian for sure), have a celery package that sets up init scripts for you. Advanced setups (running many projects from the same box) may want to use supervisord. - If you're using virtualenv, install celery into the virtualenv for your project and run it with the full path to the binary: `cd /var/myproj/` and `/var/venv/bin/celery worker -A myproj`. It's common in celery init scripts to have a variable like [`CELERY_BIN`](http://celery.readthedocs.org/en/latest/tutorials/daemonizing.html#example-configuration) that points to this path.
Or with python's builtins for XML: https://docs.python.org/2/library/xml.html?highlight=xml#xml
thanks for the info. 1 is the first prime by the way. the majority of sane people will tell you the opposite. thats why i'm insane.
There is nothing wrong with Apache worker MPM if setup properly to match the requirements your system has. So not sure what specific concerns you have. Concerns over memory bloat is generally completely overblown and just shows a lack of understanding of how to setup Apache as well as how to use it effectively in conjunction with other tools including front end proxies.
I'd second cron, as I use it all the time on linux/mac boxes for just such regular python tasks (also for scheduling git pulls) Q: Is there a comparable, low level (no dependancies needed) task for the windows/.net world? (never programmed for pc)
It's once instance of celery, with the configured number of workers. I think that's the concurrency. I believe that by default, the number of worker is configured automatically according to the computer's capacity (cores). You also might see some workers idle if the active ones are processing really big jobs. Check out [flower](https://github.com/mher/flower) for monitoring and some control. It's a bit rough around the edges, but is usually easier than using the command-line monitoring tools. In practice, jobs are pulled from the queue FIFO, but a worker might reserve several long-running jobs, while the other workers burn through all the small jobs and are left sitting around. You can prevent this to some extent using named queues. We use AWS and run celery on dedicated boxes. Our APIs run on other boxes. RabbitMQ makes it as simple as possible to route all these jobs and results around. In our development and CI environments, we run everything on one box. We control celery via supervisorctl, but you can run it as a daemon from the command line.
Please use descriptive variable names and comments. It may not make a difference in small programs, but for bigger ones it helps a lot. It's a good habit to get into. Anyways I would create a prime test function that tests individual numbers. This way you could use it in other programs, and use it for more things. It can be used to test individual numbers for primality, get a range of prime numbers or generate a stream of all prime numbers. This is what I came up with: import itertools import math def prime_test(p): if p &lt; 2: return False if p == 2: return True if p%2 == 0: return False sqrt_p = math.sqrt(p) sqrt_p = math.ceil(sqrt_p) factors = (num for num in range(3, sqrt_p+1, 2)) for f in factors: if p%f == 0: return False return True #All positive numbers nums = itertools.count(1, 1) primes = (num for num in nums if prime_test(num)) for prime in primes: print(prime) input() So we just have a simple prime test that covers all corner cases, like negative numbers, 0, 1 and 2. We then test if the number is even, as there is no reason test even numbers for primality. Next, if the number is none of the above we perform trial division on all odd numbers less than the square root of p. The itertools thing just gives us a stream of all positive numbers to use in our generator comprehension. The generator "primes" then gives us a stream of all prime numbers. If you have any questions ask away! 
Cron + SSH if you need to centralize scheduling. Bonus: contabs are simple text files. Additional bonus: combined with logger, you can output everything to syslog instead if spamming an inbox (or like in my case, there's regulatory incentive to centralize logging).
So now if someone is using your library, you either: a) have to document all the exceptions you use from other packages b) force your users to look at all the packages you've used in your library in order to be able to properly catch all the exceptions your library might raise or pass along. In either case, that's a library I wouldn't want to use due to poor conception and execution, let alone organization.
Err, no. You import them. Then your users just use them as YourModule.ExceptionName. Doesn't matter where they come from as long as they're in your module's namespace.
At the risk of leading you down a rabbit hole, celery emits all sorts of [signals about job status](http://celery.readthedocs.org/en/latest/userguide/signals.html). In your case, you probably want the task_failure handler, something like this: from celery import signals @signals.task_failure.connect def task_failure_handler(sender=None, **kwargs): """ capture task failures """ pass
I suppose that's a route I could take, was hoping they had an email error handler built-in that I hadn't found. Thanks though.
Here's two examples of where you'd re-raise exceptions, that are much closer to things you might do in reality: ``` import contextlib class FooException(Exception): pass class FooCollection(object): def __init__(self): self.id = None @contextlib.contextmanager def foo(self, id): _id = self.id self.id = id try: yield self # if there were a case where you needed to do different cleanup for certain exceptions # to restore state, you'd do that here, and then re-raise the exception. As library code, # it's not our job to fix everything for potential users, because we probably can't predict # what they might have done wrong to cause the exception to be raised to begin with # it is our job, though, to be consistent and sane with the exceptions we send their way finally: # always do the basic cleanup regardless of whether the exception were handled or not # if you don't do this, if an exception happens while in the context managed state, # cleanup won't happen self.id = _id def fooed_bar(self): try: fooinate_bar(self.id) except BarException as exc: # in this case, we want to capture BarException from fooinate_bar so that we can re-raise it as our own exception # this leaves client code less tightly coupled to our own dependencies, and allows us to maybe change # to a different dependency that fooinates bars for us but might raise a different exception down the road # without requiring software using our library raise FooException from exc ``` In this example, we have a class with a method that can be used for context management. We want to capture exceptions there so that we can do cleanup afterward so that our internal state doesn't get screwed up, but nothing more. We also have a method that calls a function in an upstream library, that can sometimes raise exceptions. We don't want to redocument their exceptions, and we don't want to force our own users to have to know what exceptions our dependencies raise, so we capture the exception and re-raise it as an exception of our own. You might also do this if you have a function that an exception can be raised within, that needs some sort of cleanup after the exception is raised, but you want or need to do the actual error handling further up the stack. I find that is not often the case, though.
This sounds super hacky. You've got to deal with your central scheduler going down, and it sounds like you're also going to start managing specialized SSH keys at that point. What if your SSH attempt fails? Do you retry? How do you retry? What if the target machine goes down, but the scheduler remains up? Do we choose another machine to run the task on? What if we want to run the task on multiple machines? What if SSH fails to just one of said machines? I'm always amazed at the shitty solutions people suggest, when you could just use a more appropriate tool for the job (assuming we are looking to expand beyond a single machine for the sake of reliability). Included with celery: * Tasks are written in very simple decorated Python functions. * Workers can come and go as they please. The pool can be expanded or contracted as desired. * Configurable auto-retry on task failure. * Tasks can execute on any worker listening to the queue. You don't need to worry about the machine that runs a certain cron task going down and not executing on time. Run workers on more than one machine and you're pretty resilient to failure with minimal effort. * Tasks more or less automatically load balance across multiple workers (machines). * The tools for monitoring job status, history, and results are excellent with celery. You can also pipe to syslog easily, if that's your thing. * Tasks can be scheduled in a format that closely resembles crontab. Or if you don't feel like being too granular, you can just say "daily", "hourly", "weekly", etc. * celery is well-tested, actively maintained, and is probably going to grow with your codebase more gracefully than a set of shoddy shell scripts. While celery can do a whole lot of crazy advanced things with celery, if you distill it down to its simplest form, you're just writing functions that are decorated and running a worker process. The documentation is pretty good, and the community is very active if support is needed. I don't know if OP needs to scale to multiple machines, but if that is indeed the case, use a proper task system like celery, gearman, or RQ. Or at the very least poop out your own crappy task system with a more appropriate approach than cron + SSH.
Hi, I saw some projects already taken down like https://github.com/github/dmca/blob/master/2014-05-26-LINE-Corp.md ... Anyway I updated some codes which might works pretty well (but need to write more function). If you can do something for me to be not taken down, it would be glad to hear a good news from Github!
I think I finished sending line messages but not receiving yet. So I'm working on Long polling part right now! Please keep in touch with my project!
 for i in range(34): read(hex(i))
I tried doing this but for some reason it just read the data from 0x20 33 times. I'm not sure if there is a difference in the hex(1) vs a string '\x01'.
Shouldn't be a difference. There might be some other problem. Copy &amp;amp; paste my code to check - maybe you made a typo. EDIT: I've never worked over serial before, but maybe there's a maximum speed it can work at. Try adding a few milliseconds of sleep after each `read`. EDIT 2: Love that someone is downvoting my posts but doesn't have the balls to post the right answer.
Hey I really appreciate it. Thanks for the background info. I'm basically trying to prevent Mac OS X's fairly aggressive file cache from getting involved when working with large files that I wanted accurately checksummed. If I have to checksum a file twice I don't want the second pass to read it from memory. I'd like to pull it off the disk no matter what. This is a Mac OS X specific fcntl op. I'm using this in a py-objc app bundle context. It's possible the correct version of the header or lib wasn't copied into the virtualenv I'm building from. 
How is this better than [Scrapy](http://scrapy.org/)?
This is exactly why I'm excited. A very influential programmer asked me why don't I start learning some python, he'll include me in some of his projects if I do. For weeks now I'm learning by the books and some interactive and non-interactive online tutorials. This would add so much 'fun' into the learning process. 
"Python, you magnificent bastard, I read your book!" /Gen. Montgomery
If you pass this surrogate-escaped string as an argument to open() or mkdir(), for example, it will roundtrip back to the same sequence of bytes to the underlying OS, just like in python2. There is no one "right" way to handle an unknown encoding. This is true regardless of which python version or any other language you may be using. You need to choose a strategy and all strategies will sometimes have situations where they fail. The ways to implement the strategy in py2 and py3 are different. Python3 made some choices differently that make certain strategies easier to implement. Python2 made some choices (e.g. default encoding is ascii and is awkward to override) that make many strategies harder to implement. Please give a specific scenario, what strategy you choose to handle unknown encodings and how you would implement it in python2.
But pools were removed in Sims 4. Along with toddlers. [Link](http://www.thesims.com/news/whats-out-and-whats-in).
Can we still buy 5 hifis running different channels surrounding a baby cot, and then brick them into a sound dungeon, so that they can never sleep and the ai can't rescue them? Also I'm just learning python and it's great!
&gt; tl;dr how do I create a for loop in which I can call a function: foo('\x01') to foo('\x21') ? Just in case you don't see my answer further below: for n in range(0x22): c = chr(n) foo(c) 
If you look back at *very* old mathematics papers, occasionally mathematicians did consider 1 to be prime. But that was a long time ago, these days primes are defined to exclude 1. The reason for this is so that all numbers &gt; 1 have a *unique* prime decomposition. E.g. `15 = 3*5`. But if 1 was prime, it wouldn't be unique: `15 = 3*5 = 3*5*1 = 3*5*1*1 = 3*5*1*1*1 = ...` 
Huh. Not that I'm complaining, but I'm a bit surprised they didn't go with Lua.
Without unittests? May be [Pomp](http://pomp.readthedocs.org/en/latest/) is amazing scraping library ))) 
Blog spam. Poor mans scrapy. Edit: a little bit of time has passed, and I realized I shouldn't post such negative comments. Seeing this crawler you created, I can't help but wonder why you didn't just update and extend scrapy redis instead of creating a whole new crawling framework? You're trying to make it seem like this redis queue functionality in your crawler is some huge improvement over scrapy, but scrapy redis already exists. Just like someone else posted, what can this do that scrapy can't?
Lines 10 -- 14 are quite expensive f = 3 while f * f &lt; p: if p % f == 0: return False f += 2 return True may be better
Anyone know if it python 3 or 2? edit : I cannot spell
It's a trade off - Lua is a bit easier to embed but Python is more well known. If they do it right, the modding system and the content design system will be the same. This makes it really easy for them to hire content developers and crank out content. DLC and add-on/expansion content is where the money is made on The Sims.
It seems like you are just testing all numbers &lt; sqrt(p) right? That should be easy enough to add.
If your answer to a simple question of "how to run a function every X seconds" is "install a distributed message queue", you may have a bit of a problem with overengineering.
&gt; assuming we are looking to expand beyond a single machine for the sake of reliability and there's your strawman. give me a fuckin break. the question i was answering was strictly periodic tasks. not enterprise level wankery. if you want distributed robust systems, may as well go with erlang, if we're just going to redefine problems in order to have an argument. 
there is a task scheduler in windows but it's GUI driven or at least it was in XP, I stopped using windows around the time Vista came out. 
Why would they go with Lua?
 raise Exception('Joke killed')
IDEs (e.g. PyCharm) and [jedi](http://jedi.jedidjah.ch/en/latest/) use them. And then there is [MyPy](http://www.mypy-lang.org). They strive to implement a statically type checked variant of python that remains 100% source compatible.
It's a very common choice for game scripting.
Lots of great resources there, thanks for sharing. Hey, I see even I'm there!
Lua is groooooossss. (It's unfortunately got a better sandbox, but it still makes me nauseous.)
It's easy to forget what it was like to be a beginner. Things you might take as obvious are not. Especially in a public setting, people are far less likely to actually ask questions if they are slightly confused. That's their loss, but you can try to make it as friendly and open for questions as possible. 10 hours is a substantial amount of time. What do you plan to cover?
I hope Python 3 so I'll have some reason to finally learn the changes from Python 2 :/
Seems they are going to [pull a SimCity](http://www.neogaf.com/forum/showthread.php?t=845072) with this.
That link does a good job of not explaining why it isn't possible to both overhaul the build/create-a-sim systems AND have toddlers/pools. How is that a trade-off?
It isn't.
There's a [CELERY_SEND_TASK_ERROR_EMAILS](http://celery.readthedocs.org/en/latest/configuration.html#celery-send-task-error-emails) setting that can be configured to send emails when something fails
[0x8047175a](http://xkcd.com/138/) I suppose it's not actually that helpful in python though... I would suggest coding examples in real time over using pre-prepared slides, that always helped me pick up new things. It also becomes easier to take a tangent if studends are particularly interested in something that isn't in your lesson plan.
[Image](http://imgs.xkcd.com/comics/pointers.png) **Title:** Pointers **Title-text:** Every computer, at the unreachable memory address 0x-1, stores a secret. I found it, and it is that all humans ar-- SEGMENTATION FAULT. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=138#Explanation) **Stats:** This comic has been referenced 22 times, representing 0.0758% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjhj8yi)
Print("message") You're welcome. 
 atCursor.createRoom(center,4,4) for i in sims: sims[i].location = [cursor.x,cursor.y,cursor.z] atCursor.createFire() Sit back and watch the show.
Are the people you will be teaching new to programming in general or just new to Python? If the former, basic concepts like loops, conditionals and types are good. Go through basic stuff like FizzBuzz or pick some of the easier challenges from Project Euler. If the latter, teach them how awesome Python is! List and dict comprehensions, generators and generator expressions, decorators. Tidbits like the fact that functions are first class in Python and Python's MRO are good to know as well. Explore some of the awesome things in the stdlib and introduce them to packages like `Request` that makes life easy. 
Well, except for the fact that everything seems to be a generator now :D
Cross-platform distribution turns the OS bundling cron to an external dependency.
This sounds like an awesome source of motivation! How is he influential though? Writing books, open source project, etc.? Good luck on your journey! :) 
Their budget is $X. $X buys them Y hours of developer time. They chose to spend those y hours working on other features instead of pools.
Similarly, waterboarding is a very common choice for torture.
Hey, me too!
I think negative comments are warranted for code with no unit tests and no explanation of the ways it's better than comparable libraries.
I wouldn't use `_` to name things...You're going to have a bad time when it creates a weird bug from referencing the last value instead of some function or variable.
Clearly that's the implication, just seems like a cop out is all. The programmer in me understands the constraints... the cynic in me expects to see a near day one DLC with these features, especially considering EA's (and the Sims' specifically) past actions.
By a 'game of chance' we partnered to do an Android application. My experience is about 18 months with only Android development (I am young), and he has more than 10 years experience in Web Development, most of that with python. He pushed me more than people in company I work at, made code review of my work in his spare time, commented on software architecture.. He basically lectured me during the development. After we were done, he offered me to work with him in my spare time, which I gladly accepted. Now I'm learning python, slowly. I like it a lot so far :-) 
Thanks, if you were under the impression that I was looking to set constraint - I'm not. Just want to know how many Workers get created by "celery worker...." command 
Anytime I throw a file (docx) at it, it fails and says I suck at files. I'm not at my python machine so I don't have the err in front of me, but it was something to the effect that it didn't know what docx were. (Windows) textract.exceptions.ShellError: Command failed with exit code 1
why use this over sql alchemy or the myriad of other wrappers ?
Man, did I miss a paren?
It is using the same name as the project which is shipped with Python as the ``sqlite3`` module. Seems like a bad idea to me. https://github.com/ghaering/pysqlite
Human sacrifice! Dogs and cats, living together! Mass hysteria! cross platform only matters if you are developing a library or product that you want to be cross platform. we're talking about the ability to run periodic tasks here. In practice most of the time. The platform it is intended to run on is known and it's often an unnecessary expense going for cool points like "run's on window(the last time I tried it)" 
Most system management in Windows *can* be done through CLI, but it tends to be clunky (IMO, at least.) Generally it's easier to use the GUI if you're just setting it up on a single system. Task Scheduler stuff can be controlled through [at](http://technet.microsoft.com/en-us/library/cc755618%28v=ws.10%29.aspx) and [schtasks](http://technet.microsoft.com/en-us/library/cc772785%28v=ws.10%29.aspx) commands. I've sometimes used timer loops in Python to schedule one-off tasks for weekends or overnights though.
I thought I had configured that, I'll have to check again. Thanks.
I never said don't get the book, just not the physical copy. And what do you know, it's on piratebay right now.
Sanity or insanity does not enter into it, it is a definition. A stoner can say black is white but everybody understands the situation. Also, this is very old stuff, for those genuinely interested you might be interested in active state recipes: # =========================================================================== # Note: n can be as large as 1,000,000 (or larger, depending on your memory) # {{{ http://code.activestate.com/recipes/366178/ (r5) # =========================================================================== def primes(n): if n==2: return [2] elif n&lt;2: return [] s=range(3,n+1,2) mroot = n ** 0.5 half=(n+1)/2-1 i=0 m=3 while m &lt;= mroot: if s[i]: j=(m*m-3)/2 s[j]=0 while j&lt;half: s[j]=0 j+=m i=i+1 m=2*i+3 return [2]+[x for x in s if x] Good luck, compute on! 
If you're just going to make up your own math, why ask?
 def is_prime (n): """ Naive-but-good-enough primality check. Will be slow for very large primes or composits with very large prime factors. """ # Don't need to check any higher than the square root of n maxf = math.ceil(math.sqrt(n)) # Primes are natural numbers greater than 1 if n &lt; 2: return False # First prime is 2. Checking here because following loop # would mark it as composite if n == 2: return True # Check the natural numbers up to sqrt(n) # Return False immediately if a factor is found. Don't care # What factor is, or how many --- just that it has one. # Be sure to check maxf + 1 or you'll miss perfect squares like 9 for f in xrange(2, maxf + 1): if n % f == 0: return False # Return True if no factors were found return True # Function can then be used in generator of arbitrary size primes = (x for x in xrange(2, 4000000) if is_prime(x)) 
There are frameworks that you can use with python to do web development (django, flask etc) so you're covered there if you want to stick with python instead of php. And php is a hard language to learn apparently (at least compared with python) so that *may* be the better route. And mention which libraries you're looking for (that are python2 only) and I (or someone else probably) will try to find some nice alternatives for you. Honestly, at this point, it's either pretty easy to find an alternative or to just code up a quick method to do the same thing (unless you're looking for something very complicated)
Ohhhh... I am not a Windows man by any stretch of the imagination. I will say that that's a useless error. My guess is that a dependency is not being fulfilled, so when you call it, the subcommand fails. Processing `docx` files relies on the `python-docx` module, so make sure that you have that installed. It just as well may be one of the other dependencies (*python-dev*, *libxml2-dev*, *libxslt1-dev*, *antiword*, and *poppler-utils*), and I'm not quite sure how to fulfill those on a Windows box. May be easier to work with [Tika](http://tika.apache.org/) on Windows. There are python bindings, but none that I could find on pypi. Godspeed, programming Python in Windows can be soul sapping.
This appears to be a thin, non-standard wrapper around the real pysqlite (aka sqlite3), but GPL. Strike 1. Looking at the actual code, it constructs a lot of SQL by string interpolation and I don't see any kind of sanitization. Strike 2. I'm don't really see any compelling or interesting feature inspiring me to really look any farther to find a strike 3, which is sorta in itself a strike 3 I guess. No thanks &gt;.&gt;
Like /u/wmcscrooge said, Python is not a bad language for web development at all. Dynamic web development is mainly PHP, Ruby (Rails) and Python (Flask, Django, web.py, ...)(and js, but come on...). What follows is my personal opinion: PHP can be pretty fast, and is easy to setup, but hard to maintain once your project grows beyond a few pages. It is also ugly and can do pretty much nothing aside from web development. Ruby and Python fill the same basic role, they are modern, interpreted languages. They both have mature web frameworks, live-shells, package/module managers and can also be used for regular application development. While Ruby is more commonly used in the web sector, it is used pretty much nowhere else. Python is commonly used for desktop development, pretty much every API in existence has one or more Python binding, and imho the syntax is much nicer to read. **Tl,dr;** Python is more versatile while still being popular enough in the web sector to be used. Also, this is /r/python, and I just love Python for everyday use (read: when I can't be bothered to use C).
Looks interesting, and Python's time/datetime could use a cleaner and more useful API (changing timezones in particular is nice). That said, "humanize" is a weird way to describe that particular operation. :) I can't think of a better one, granted.
Not a great example, if you're looking to torture someone, waterboarding is a pretty good way to do it. Cheap and effective.
Could you come up with a better example? I'm having a hard time understanding why this would be better than the alternatives. Why is def each_d(lst): def decorator(func): for item in lst: func(item) return func return decorator @d_each([1,2,3,4,5]) def _(i): print(i) better than: def each_d(func, lst): # Could swap these parameters if you like for item in lst: func(item) each_d(print, [1,2,3,4,5]) ? 
This is the ugliest blog I have seen in a long time... Why use this over lxml, BS4 or Scrapy? No reason to...
&gt; Godspeed, programming Python in Windows can be soul sapping. You don't know the meaning of that until you build a Django app on your desktop and then you try and push it to production. Never got it to work.
Awesome! Thanks for the appearance! 2 questions: 1. Why "Arrow" 2. How would *you* use Arrow to determine if a particular time is within a specified time range? Just by quickly looking at the API, something like if arrow.now() in arrow.now().replace(minutes-=30).span("hour") However, I know that's not the right syntax, would be nice tho =P
&gt; the question i was answering was strictly periodic tasks. not enterprise level wankery. &gt; &gt; if you want distributed robust systems, may as well go with erlang, if we're just going to redefine problems in order to have an argument. What an absolutely ridiculous reply. Celery is used by projects of all sizes. It's not difficult to use, it's purpose-built for periodic Python tasks. I'm not even going to address the erlang or "enterprise wankery" parts, as they have nothing to do with OP's original question.
I think it's both. The actual production team struggles to fit everything they want to add in their allotted temporal and financial budget, while the EA executives and shareholders sit in their chairs twirling their moustaches, fantasising about DLC.
If I name my Sim Buzz, would that make me a buzzkill?
&gt; Celery is used by projects of all sizes. It's not difficult to use, it's purpose-built for periodic Python tasks. another ridiculous reply, cron is used by even more than celery. you missed the whole point about how you took the simple requirement of how to run tasks periodically and blew it into this enterprisey circle jerk. you must be a consultant with a decent hourly rate. 
Unless `sims` is a dictionary, then it all works. The API is a bit questionable, though, with both `cursor` and the [god object](https://en.wikipedia.org/wiki/God_object) `atCursor`.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**God object**](https://en.wikipedia.org/wiki/God%20object): [](#sfw) --- &gt; &gt;In [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming), a __god object__ is an [object](https://en.wikipedia.org/wiki/Object_(computer_science\)) that *knows too much* or *does too much*. The god object is an example of an [anti-pattern](https://en.wikipedia.org/wiki/Anti-pattern). &gt;The basic idea behind [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) is that a big problem is [separated](https://en.wikipedia.org/wiki/Separation_of_concerns) into several smaller problems (a [divide and conquer strategy](https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm)) and solutions are created for each of them. Once the small problems have been solved, the big problem as a whole has been solved. Therefore there is only one object about which an object needs to know everything: *itself*. Likewise, there is only one set of problems an object needs to solve: its *own*. &gt;A program that employs a god object does not follow this approach. Most of such a program's overall functionality is coded into a single "all-knowing" object, which maintains most of the information about the entire program and provides most of the [methods](https://en.wikipedia.org/wiki/Subroutine) for manipulating this data. Because this object holds so much data and requires so many methods, its role in the program becomes god-like (all-encompassing). Instead of program objects communicating amongst themselves directly, the other objects within the program rely on the god object for most of their information and interaction. Since the god object is tightly [coupled](https://en.wikipedia.org/wiki/Coupling_(computer_programming\)) to (referenced by) so much of the other code, maintenance becomes more difficult than it would in a more evenly divided programming design. &gt; --- ^Interesting: [^God](https://en.wikipedia.org/wiki/God) ^| [^Anti-pattern](https://en.wikipedia.org/wiki/Anti-pattern) ^| [^False ^god](https://en.wikipedia.org/wiki/False_god) ^| [^God's ^eye](https://en.wikipedia.org/wiki/God%27s_eye) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjhtkeq) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjhtkeq)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
php also _looks_ awful. I realize that's probably trivial when you're picking the best language for your application but it becomes a mess to try to read and reason about, whereas I would argue it's easier (in my experience) to have an organized and concise python application. 
this belongs in /r/learnpython but I'll go ahead and waste time answering anyway. When presented with the question "Should I learn..." the answer should always be yes. Assuming there are not other pressing matters in your life that take precedence like feeding yourself. Also, there's language bias built in to this sub just like other sub for &lt;favorite programming language&gt;. Ask the same question on the ruby sub and you'll get different answers for example. So really, only you can answer this for yourself. 
Truly, our hour has come!
1. I was thinking of 'Time's Arrow', either the book or the Star Trek: TNG episode 2. Interesting use case, I'd probably grab the timestamp for each and check that the absolute value of the difference between the two was &lt; a given # of seconds. 
I'm not even sure I like the design that much..
&gt; another ridiculous reply, cron is used by even more than celery. Wat? It appears that you are having a hard time putting together coherent thoughts. So here's one for you: http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics Oh so difficult! Functions, decorators, oh dear! Bless your little heart, being scared by big ol' bad celery. OP wanted Python, I give him simple Python. Not a shitty cron hackjob.
If you use i as piped value for keys from a dictionary, you are *literally* the antichrist.
[Image](http://imgs.xkcd.com/comics/python.png) **Title:** Python **Title-text:** I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I'm leaving you. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=353#Explanation) **Stats:** This comic has been referenced 70 times, representing 0.2408% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjhv20q)
Definitely using this :D Looks awesome!
Thanks. Do you have any non-obvious examples that I might overlook?
The lesson plans will be finalized early next week and I will post them here but the class is supposed to be an introductory 101 level type of course and we plan on a 102 course afterwards. Since this particular class has a lot of experience we plan on going into more advanced topics such as object oriented programming and design patterns. 
&gt; Oh so difficult! Functions, decorators, oh dear! check you meds guy, no one said it was difficult. I will say it's an unnecessary hammer in some scenarios though. and your "rebuttal" doesn't even prove that celery is used more than cron. i love it when trolls start to flounder. Soon you'll run out of things to say and just post whitty memed gifs from imgur. #kidz 
I don't see that word in text very often, so the first time I saw it I imagined surfing and was confused.
&gt; I will say it's an unnecessary hammer Hammer seems to infer difficulty or inconvenience. You haven't really addressed the central criticism of your cron approach, though: It's brittle. By the point you build in the monitoring and fail-safes to reliably execute your tasks, you might as well use an actual Python background task system, for running Python tasks. dumbOps
If you're looking for something really interesting, check out [apsw](https://github.com/rogerbinns/apsw) -- it's a low-level SQLite driver that exposes much for of SQLite's functionality than the standard library sqlite driver. Also, `pysqlite` is the name of the standard library's SQLite driver, so definitely consider changing the name.
As someone who has embedded Python and written a complete game with it. I am very surprised by this. It's a fucking nightmare. That said it's still the best language ever designed so yay, Python! 
I plan on releasing one really soon. I'll make sure to ping you when I do.
&gt; Hammer seems to infer difficulty or inconvenience. Wrong, http://en.wikipedia.org/wiki/Law_of_the_instrument &gt; You haven't really addressed the central criticism of your cron approach, though dont need to. I never asserted it was better than celery, b/c a blanket statement like that is rarely ever true. only someone being as faggey as you ( [in the annoying way not the gay way](https://www.youtube.com/watch?v=U-otAJrtY-w)) as you would try to argue such things. 
It's an order of magnitude easier to embed than python and gives you much more control over the interpreter for things like GC. That said, I haven't embedded either for over 5 years.
We've been using it at work after I discovered it on reddit sometime last year. It's the best thing since sliced arrays! Thanks for taking the *time* to write and share it.
Pretty excited about the TNG reference. That was my favorite episode (wasn't it 2?) when I was younger, and my first instinct re: why you named it. 
The one that I keep having to remind myself of is that objects in a mutable sequence (lists, dicts, sets) defined globally can be modified globally from within a function without the `global` keyword. That's a shit description. Something like this: my_dict = {'a': 1, 'b': 2, 'c': 3} my_list = ['spam', 'spam', 'and', 'spam'] def my_func(): my_dict['b'] = 5 my_list.append('!') print('Inside:') print(my_dict) print(my_list) print('Outside:') print(my_dict) print(my_list) my_func() print('Outside:') print(my_dict) print(my_list) Result: Outside: {'c': 3, 'b': 2, 'a': 1} ['spam', 'spam', 'and', 'spam'] Inside: {'c': 3, 'b': 5, 'a': 1} ['spam', 'spam', 'and', 'spam', '!'] Outside: {'c': 3, 'b': 5, 'a': 1} ['spam', 'spam', 'and', 'spam', '!']
https://en.wikipedia.org/wiki/Impact_factor https://en.wikipedia.org/wiki/Plotly https://en.wikipedia.org/wiki/IPython Thanks for the parsing code! A caching REST API would be great! http://www.reddit.com/r/semanticweb/comments/2ck0hq/whats_the_difference_between_rdf_linked_data_and/cjgk42b : &gt; http://5stardata.info/ &gt; &gt;http://www.w3.org/TR/ld-glossary/#x5-star-linked-open-data : &gt; &gt;&gt; * ☆ Publish data on the Web in any format (e.g., PDF, JPEG) accompanied by an explicit Open License (expression of rights). &gt;&gt; * ☆☆ Publish structured data on the Web in a machine-readable format (e.g., XML). &gt;&gt; * ☆☆☆ Publish structured data on the Web in a documented, non-proprietary data format (e.g., CSV, KML). &gt;&gt; * ☆☆☆☆ Publish structured data on the Web as RDF (eg Turtle, RDFa, JSON-LD, SPARQL) &gt;&gt; * ☆☆☆☆☆ In your RDF, have the identifiers be links (URLs) to useful data sources.
&gt; the advantage of this is you don't have to think up or read an unnecessary non-sucky name for the function But I _do_ want to read a non-sucky name for that function...
Did they copy paste the SimCity blog post about it when that was an upcoming game? It sure read like it.
&gt; only someone being as faggey as you I can link to Wikipedia, too: http://en.wikipedia.org/wiki/Ad_hominem You really don't have a leg to stand on, so keep on flinging the insults. To everyone else: don't take this guy's advice if you need any guarantee of execution. You won't even come close to getting it with cron. It may seem like a simple approach upfront, but it's like having sex with a hooker without a condom. Might be fun for a while, but you're going to get something painful eventually.
That works in 2.7 as well.
I don't see it as getting everyone to learn to code so much as merely exposing them to it. There may be some people who never thought about coding or thought they'd never be capable of learning to code, that will suddenly find something they really like doing. 
&gt; Time flies like an arrow. Fruit flies like a banana.
Well to be honest, I created this for my own personal needs. I used to use SQL for web development (mainly for MySQL databases with PHP/PDO) and I always had trouble with small syntax issues (MySQL seemed to impose more restrictions on syntax than SQLite). So, I created this to avoid issues in small syntax so I could focus on my work rather than issues in SQL syntax. I thought it might be useful for others to use if they needed it, so I put it on GitHub. I didn't know about the name conflict (I heard about pysqlite when I was researching about Python and SQLite), but I thought it was replaced almost everywhere with sqlite3. I will see if I can change the name without any major conflicts (I have already used it in a few of my own projects).
I thought the whole point of CPython was to make it easy to embed? And I also remember reading the mailing list exchange in which it was first proposed to make Julia and Python interoperate. One of the Julia people decides to check out what's needed to call Python from Julia, pops back an hour later with a post that reads "Well, that was easy" and a copy/paste of Julia calling Python from a shell. 
 &gt; I thought the whole point of CPython was to make it easy to embed? It's easy to get the thing interpreting code but achieving interoperability is not fun at all. I even ended up using boost.python, which makes it a lot less messier but has it's own issues. To be fair I wanted to do pretty mad things, but I was still fighting it too much. I've gone from writing things in pure-python, to embedding python, to just using straight-up C++. I think with my current project when I come to add a scripting engine I'll go with angelscript.
You might want to check out some of Dave Beazley's talks. Dude just has a great way of talking to the room. It helps that he was a professor for several years, but I'm sure even mortals such as ourselves could pick up a few tips from his presentation techniques. Here's a few of my favorites: - [Discovering Python (2014)](https://www.youtube.com/watch?v=RZ4Sn-Y7AP8) - [Generators: The final Frontier](https://www.youtube.com/watch?v=D1twn9kLmYg) - [PyCon Keynote (2012)](https://www.youtube.com/watch?v=l_HBRhcgeuQ) - [Python 3 Metaprogramming](https://www.youtube.com/watch?v=sPiWg5jSoZI) And you gotta watch him [use Python 3 to literally remove the GIL with a CNC machine and sharp whirling knives](https://www.youtube.com/watch?v=SNBKWuM-Lu8). Aside from having a great sense of humor and an amazing brain, one of his notable skills is live-coding with iPython, and knowing enough of the "%"-commands to be able to load/edit files, that sort of thing. Having watched my fair share of python talks/tutorials/presentations, I'd say that being skilled at live-coding is a big plus. Good luck with it!
Da ganeshka, ohnimi. Mani mazo i kanama. Ashamenadia.
I'm a very old school shell/utility scripter but I picked up bottle in no time for my raspberry pi based home alarm system. Very simple, works fantastic. I use it with bjoern web server, which only required one argument in bottle startup to use instead of default server. Performance is phenomenal.
I'm going to go out on a limb and say this is the real reason, especially given EAs shenanigans with expansions in general (The fact that there are over 20 expansions to the sims 3 is ridiculous). Also, I just have to say that the Sims 4 looks almost identical to the sims 3.... I have a hard time seeing this as being a huge "overhaul" (do comparisons of screenshots) Rather, I think this is just EA gutting Sims 3, adding a few new wizbang features, and then repackaging their gutted features in expansions later on. I really hate EA as a company.
Lua is pretty well known in the game modding world primarily because it is embedded in so many games. It also has the benefit of being a pretty nippy language, making it possible to push more logic into the scripting language vs having dedicated code in the engine. Not that I'm complaining, Python is a fine language.
If you're looking for professional developers, especially contractors, so you can crank out commercial content, Python's the only way to go from a business perspective.
There is a power in popularity. Heck, I'm pretty sure some the reasoning for going with python instead of something else was likely "Python is a really popular language". There is a reason Java and C++ still flourish.
What's so gross about lua?
What about Java? C#? Groovy? Ruby? Javascript? All of these languages are as popular if not more popular than python with hoards of professional developers.
I know a fair bit about Python, actually! In the first year of my undergraduate degree I learned C, so the term array tends to stick for me more than list. My comment was largely made in jest. I was pretty sure calling someone the antichrist would have made that clear, but I added the italicized 'literally' just to be absolutely certain. Even so, there is actually some truth in what I said. I had a fascinating math professor in my first year who explained the origins of common loop variables. A lot of people mistakenly believe that the use of i originated in Fortran, but it actually originated well before Fortran in mathematical summations. In either case, though, it was an integer. Its history aside, generally most people would assume that i is short for index. While the argument can be made that a key in an associative array *is* an index, I think most people would (reasonably) assume that an index should be an integer. That is, of course, not necessarily the case for associative arrays. I suppose this doesn't broach the broader issue of whether you should simply make it clear that it's a dictionary by piping both the key and value (as either k, v or key, value) into your for loop. I can't make as well-reasoned an argument in favor of that, so I won't even attempt to! 
While I disagree with FionaSarah and her attitude, it doesn't justify your response. Your response is crass and distasteful. It also makes no sense given that Ruby uses the term array, rather than list.
^ ------- the voice of reason and sound logic right here. 
It doesn't really matter what type it is, as per duck typing if something behaves like a dictionary then you can treat it like a dictionary. You can't (and it's pythonic to not) do much hinting on what it really is without bad applications of hungarian notation. I'm not convinced that if someone sees 'i' they immediately think integer. i, j, k is soooo common as short loop indexes to think otherwise is a little mad. And I would *absolutely* say that a dictionary's key is an index into the dictionary.
I'm sorry I came across bad to you, you said something that made absolutely no sense to me while using terms that catagorically are not used in reference to Python's collection types - I just assumed you were one of those people who think they know something that they don't! Those are the worst people!
An index implies ordering. Associative arrays are not ordered, hence the preference for the term key over index.
You're right I'd never personally refer to a dictionary's key as an index myself. But I don't think there would be much misunderstanding if one did. I guess the only place we're butting heads on really is if using the letter 'i' when looping through dictionaries is inappropriate. I say it's common enough outside of any integer connotations. You disagree.. Horses for courses? :P
Deal!
Wow, this confusing. Isn't there already a pysqlite package already? The only reason why I even know about pysqlite (sqlalchemy user here) is because I was evaluating peewee and it can use sqlcipher via pysqlite.
I'm a bit shocked there was ever a sims 2. To be quite honest I never found any of the fascination with the game after you buy things and help your sim go pee and sleep.
This worked thanks!
With this addition.
I was under the impression that the original pysqlite was renamed to sqlite3. I am definitely going to rename my package once I come up with a suitable name.
Just wanted to say my thanks. Your package has saved my so much time between `floor` &amp; `ceil` and grabbing ranges. Your docs are always pinned in Chrome when I'm working!
I like it.
CONTINUE to learn* He was asking what the best language was BETWEEN PYTHON AND PHP for web use. Ruby was not included. Not to mention that i saw ruby mentioned her at least once, and it was pointed out is much more used in the web sector than python. Doesnt sound very biased to me. i read some other stuff you posted, you're full of shit.
if anyone's interested, there's also simple-date - https://github.com/andrewcooke/simple-date (disclaimer: i'm the author). but it's 3.2+ only (sorry).
Life is so unfair. http://www.reddit.com/r/Python/comments/28r85w/what_modules_of_the_python_stdlib_do_you_use_more/cieoje7 (edit: for posterity; I was -1 for explaining the same thing in a past reddit thread on Arrow)
Thanks
You're good. I just looked up in the current peewee docs, it uses pysqlcipher to use sqlcipher. It wouldn't hurt to come up with another name so its not so confusing.
sanity does have thing to do with it. if i'm insane, that means i could think this is the matrix. due to "i think therefore i am", if i think im in the matrix, im in the matrix. same with whether or not 1 is prime.
how do you know it's useless to test anything more than the square root of the number you're trying to figure out whether or not is prime? i know it's useless to test with with anything more than half of the number you're trying to figure out whether or not is prime, but why square root
&gt; input sanization should generally be handled by the program That hasn't been the practice for years now.
bad day?
if a number N has two factors A and B, that is N = A * B then if A &lt; sqrt(N), B has to be &gt; sqrt(N); excepting for perfect squares of course. In other words, if a number has no factor below the square root, it cannot have any! because two factors both greater than square root are impossible. 
This is super useful.
A few years ago I wrote some [notes precisely on this topic](http://fperez.org/py4science/decorators.html) that you might find handy. They have a few other examples as well.
I thought it was something like [this](https://s3.amazonaws.com/assets.svpply.com/large/1657588.jpg?1400489688).
This worked great, F_GLOBAL_NOCACHE is defined as 55. I'm now seeing zero memory caching and can checksum files in the 100s of gig range while writing the to tape. Thanks again for the help. And yes my first shot was writing a c lib and locking the file externally globally. This is much cleaner.
I really will like a scraper that can process JavaScript as a web browser does and allow me to scroll down the page so new content loads and get scraped... I'm not sure if Scrapy or Pomp or any other can do that, I have to rely on Selenium and PhantomJS which would be perfect if it weren't by the cmd window it needs open when it gets started on Windows...
Look at [ghost.py](http://jeanphix.me/Ghost.py/) You can implement in Pomp framework custom request/response/downloader/... with ghost.py. But it is not so easy but real.
Thanks! It's working perfectly now. My solution was to loop through each element of the tuple and put the results in a temporary table, then pull everything out of that. Knew there had to be a nicer way of doing it.
This flies pretty directly against everything I've learned of correct database development for a long time -- never construct SQL from input without making sure it can't go wrong. This module is at best decribed from my reading as a simple wrapper that /removes/ the need for the caller to understand the nuances of SQL -- so you expect the caller to pass SQL-safe strings? And I know its not supposed to be "a" standard wrapper, I'm saying it not being DB-API compliant is a negative (additionally, a GPL wrapper around a permissive resource another). You don't feel that way, that's fine and dandy. DB-API doesn't have to be the end-all of database interfaces, but I believe in it being the starting point. pyodbc adds several things on top of DB-API, but you can ignore it too. To me that's bad.
According to http://stackoverflow.com/a/5893946/259130 python has 3 main uses for _. &gt; 1. To hold the result of the last executed statement in an interactive interpreter session. This precedent was set by the standard CPython interpreter, and other interpreters have followed suit &gt; 2. For translation lookup in il8n (imported from the corresponding C conventions, I believe) &gt; 3. As a general purpose "throwaway" variable name to indicate that part of a function result is being deliberately ignored I am using the this for the 3rd purpose, the name is meaningless and wont be used again. I suppose you could use a variation of this technique to return stuff or a function returning stuff by returning it from the decorator but in that case you wouldn't name it _. As for the interpreter you put in the decorator and function as one input unit so you don't have any such problem(I just tried it with both the python and ipython repls, try it if you don't believe me).
Thanks that looks useful, I considered returning a value bound to the functions name as a way to do return value(which you have in the middle) but it does look a bit weird/hacky.
Err, why don't you use `rdiff-backup`? Installable via `apt-get`, written in Python, battle-tested.
no you don't. Or to be more precise in languages which allow and frequently use lambdas including multi-statement lambdas, such as ruby and c#, you can usually also pass in a named function but frequently you don't. Part of this is due to the order things are to be read and written, if you wanted you wanted to name the function you would have to write it in the ugly regular python order(creating the function before calling it instead of creating it inline), but using the decorator-execute pattern you can name this and keep the inline(or at the end) function order. Still the other part of it is that the inline function should usually be small and easy to understand without a name and when you pass the inline function to a higher order function it should be understandable without a name and giving it one just adds to backround noise.
Looks nice to me, might try it out some time.
What is new about it? I already use it for quite some time. Very nice package.
"readable"
Historically, python didn't need a (object) specification. Things changed when this style of classes was too restrictive for many different needs. Additionally, there was a different method resolution order when you had a hierarchy of classes and multiple inheritance in particular. To solve this problem without breaking compatibility, the developers introduced the "new style classes", which are created by inheriting from object. Most of the features involving classes (properties, super etc...) require the use of new style classes. Old style classes still work, but you will get errors like the one you saw when you try to use advanced concepts on them. Clearly, it's generally a bad idea to mix old and new style classes in an inheritance hierarchy. In python 3 there's only new-style classes, so the need for direct inheriting from object goes away.
[repost][1] [1]:http://www.reddit.com/r/Python/comments/2cpeuj/arrow_a_new_date_time_package_for_python/
note the '_d' suffix stands for decorator. The standard way to do this in python is for item in lst: func(item) @each_d(print) def _(i): print i isn't as nice looking but at least it has the right order. Inline/at end functions are much nicer then declaring the functions before passing them to a higher order function. You rarely find people in ruby/c# creating local functions before a function call and then passing them to that function call(although to be fair ruby has that weird blocks aren't passed as args thing). /u/fprezeg_org article(http://fperez.org/py4science/decorators.html) has some uses such as tracing and timing pieces of code. http://stackoverflow.com/questions/24689800/async-like-pattern-in-pyqt-or-cleaner-background-call-pattern/24901800#24901800 shows a non-trivial example I have used(scheduling a unit of code to run on a different Qt QThread(you could probably do something similar w/ regular threads)) I once abused context managers to add pieces of context a bit like rspec's describe to a failing test, this could do the same thing and is sort-of cleaner.
http://crsmithdev.com/arrow/#ranges-spans
Or [Duplicity](https://launchpad.net/duplicity) if you want to securely backup. It is also written in Python and battle-tested.
In short: If you use Python 2, always inherit from `object`, i.e. always use new-style classes. In Python 3 new-style classes is the default, thus you don't need to inherit from `object` explicitly.
Which I always shortened to: &gt; Time flies like banana. It doesn't make much more sense.
The `return True` in your check_config is not needed, because the return value is never checked. I would also recommend to use configparser for your config files, json is very portable but it is better to use something built in. And using those `sys.exit(-1)` statements to control your program flow isn't really pythonic, throwing an exception and capturing it would be much nicer.
Sorry, somehow missed the repost message. Will be taking down. Thanks for pointing out.
You could do a pull request ;) 
In short, always inherit from object as your base, regardless of version.
You could use inotfy or watchdog too see if any file changes. Why backup a file again if it's the same? You are wasting CPU and HDD access.
Yes! I'm definitely trusting Rekvijem's python script, for which there are absolutely no tests, with my precious data, instead of just using `rsync`. 
Don't use global variables. If a object needs access to another object, just give it to the object. This is the core of dependency injection: if an object A depends on object B, make this dependence explicit. If you move object B to the global namespace, it's no longer clear that object A needs object B. A good explanation for why global variables are bad is [here.](https://www.youtube.com/watch?v=-FRm3VPhseI). So in your example, Person could look like this: class Person(object): def __init__(self, sun): sun.bind_on_sunrise(self.work) def work(self): #your code here And Sun could look like this: class Sun(object): def __init__(self): self.sunrise_listeners = [] def bind_on_sunrise(self, method): self.sunrise_listeners.append(method) def sunrise(self): for method in self.sunrise_listeners: method() So when Sun.sunrise() is called in some way, all methods that are bound to it are also called. Person then simply binds one of its methods to the sunrise event and thus can react to it however it wants. This is called the Observer patter, by the way. As you can see, if you want to initiate a Person, you can only do this by also giving it a Sun instance, making the dependence of Person upon Sun explicit. If Sun was global, you could f.e. forget to initiate Sun before creating a Person instance, leading to all kinds of bugs. *Edit* Person.sun_is_up is not needed, removed.
We use arrow at work. It's pretty solid. I like how it follows the style and ethos of the requests library in terms of having a simple, yet powerful, API.
&gt;Should I have a Sun class then have the person ask Sun if the sun is up Good idea &gt;or should I use a global variable Bad idea &gt;or maybe pass sun to the call to work()? Good idea You got it figured out. What are we doing here?
Good stuff, as the author of an active-record style ORM I enjoyed learning more about the session / unit-of-work pattern. For another interesting discussion of SQA internals, also check out the *Architecture of Open Source Applications* chapter on SQLAlchemy: http://aosabook.org/en/sqlalchemy.html Mike Bayer spoke a bit about the downsides of active record -- I am curious what (if any) potential downsides exist when using the Unit-of-work / session pattern?
you mentioned RDP so I'm assuming you're on windows. You can use cygwin, install ssh, and use [Fabric](http://www.fabfile.org/en/latest/). Seems like a lot of work though. You could use a web microframework like [CherryPy](http://www.cherrypy.org/) on the remote host and run your program via a URL entrypoint. You could also use a weird feature of RDP called [remote app](http://technet.microsoft.com/en-us/library/cc755055.aspx) to run the application remotely from an icon over RDP. This is probably your best bet if your version of windows supports it. edit: I reread your question. Sorry, none of my answers will directly support your use case. If you can mount your local drive on the remote host, all of them can magically work again.
I'm taking AP Comp Sci next school year (in a month) and yes, the tests are done in Java.
1 and 2 seem like they would do the same thing, might as well take the shorter one
Thanks, but i wanted to access the camera on the host computer(hosting website) not the client computer(viewing website)....
Wow, that website you linking to. Thanks!
I tend to do #3 in this situation because it's reproducible for other users.
Stream the camera live via Twitch/YT and embed their player on your website? 
Thanks for your answer. However I don't understand the advantages of DI vs simply passing it in as a parameter: class Person(object): def work(self, sun): ... It seems like the Observer pattern tightly couples Sun and Person. Does it make sense that on a sunrise, an instance of sun initiates a bunch of workers? To me, it seems like this sort of logic should be done outside of both Sun and Person (although I can definitely see use cases for this pattern). I've always viewed methods as functions reserved for modifying internal object state.
CherryPy will do the trick
&gt; However I don't understand the advantages of DI vs simply passing it in as a parameter DI is the new fancy name for passing things in as a parameter. (NB. It obviously also extends to other situations and usages, but ultimately DI merely means "Tell the object which other object to use" instead of expecting it to find or create it for itself. And this almost always comes down to passing the other object in as a parameter.)
mjpeg is extremely easy! You can use ffmpeg, vlc, or you can write your own streaming service with python and opencv. You will just need to port map the host with the webcam if it's behind a NAT. There are hundreds of scripts that do opencv to mjpeg. Google around and I'm sure you'll find something. I have written an opensource client-server webcam proxy that doesn't even need a nat, but you need to run the client on the webcam host. Here's the twisted based implementation https://github.com/dekomote/webcamforward_server of the server. Check my github for the client and the Go implementation of the server. 
They look similar but notice that they installed two different versions.
Now you have to have a third class (SunManager!) somewhere that coordinates the Person and the Sun and calls work(sun). The OP wanted the Person to be told when the sun was up.. one way to do that is for the Person to be told about the Sun on construction, where it registers itself as a listener like -Knul- suggests. Therefore there is no outside coordination, beyond the dependency injection (passing in Sun when making Person). Now this is a combination of two patterns - you could likewise have the Person just remember self.sun and have def work(self): if self.sun.is_up: # dostuff .. but now of course you still need some kind of coordination to regularly ask all Person's to work(). Dependency injection means that for testing you can easily inject other mock objects instead. If you are later changing implementation of Sun (say to a REST-ful client that checks weather.com sunrise time), then you can inject that in from the same point based on a configuration. However if that logic of finding the Sun instance was done inside some ShouldWeWorkManager, then now you would have to modify that code and all other places. So the choice of if you are going to inject through a method or the constructor kind of depends on how likely the caller is to have that object or want to have control over which instance is passed in - it would be silly for 5 classes to have to pass along the Sun if they only need it for an inner-most call to person.work(sun).
How does it compare to arrow?
mjpg-streamer works great for me.
Good library, but I'm a bit disappointed that `arrow.now().timestamp` doesn't give a float directly, like `time.time()` (I know there's a `.float_timestamp`).
i haven't looked in detail, but the "big thing" that simple-date does that arrow probably doesn't is that it will search for timezones. so you can parse dates with abbreviations like EST etc (without search, you have to say something like "US/Pacific"). on the other hand, my initial impression of arrow was that it had more functionality added to what exists in python. simple-date is pretty much *only* a wrapper round existing code (plus the timezone search), while arrow seems to extend things with more functionality for intervals (and perhaps other things).
In Kivy and Wxpython, you bind methods or functions to certain events. I assumed in my example that Sun.sunrise() was some kind of event. Sun.sunrise() could be bound to a mouseclick, a GUI button, a schedule event or any other GUI event.
Sun and Person are not tightly coupled: Person depends on Sun, but Sun does not depend on Person. The bind_on_sunrise method does not know or care what kind of objects it services: all it knows that some callable is given and that it needs to call it with no parameters at sunrise. You could have a Vampire and a Rooster class and bind Vampire.disintegrate() and Rooster.cock_a_doodle_doo() to the sunrise event and the Sun class would work as well. If Sun was tightly coupled to Person, that wouldn't work. You are right that initializing Sun and Person instances should be done outside both classes, in a Factory function or class. However, Sun does *not* initialize Person in my example: it just calls methods that have been bound to an event that Sun generates. As the event 'sunrise' is a responsibility of Sun, alerting listeners of this event is also a responsibility of Sun. Otherwise you would have to make another class that would just be polling Sun all the time, which in most cases isn't an elegant solution. 
In Python, all-caps names are considered by convention to be constants and thus can be used without problem in the global namespace. 
Yes, I read that disclaimer. But in that disclaimer and your followup post you *assume* that global variables shouldn't be used, but then deign someone aware of this self-evident fact can break from best-practices if they want. I disagree with that view -- there are perfectly legitimate best-practice reasons to define global variables in python; particularly for settings used throughout an application defined in one location. To quote code complete: &gt;Data purists sometimes argue that programmers should never use global data, but most programs use "global data" when the term is broadly construed. Data in a database is global data, as is data in configuration files such as the Windows registry. Named constants are global data, just not global variables. Used with discipline, global variables are useful in several situations: &gt;Preservation of global values. Sometimes you have data that applies conceptually to your whole program. This might be a variable that reflects the state of a program—for example, interactive vs. command-line mode, or normal vs. error-recovery mode. Or it might be information that's needed throughout a program—for example, a data table that every routine in the program uses. &gt;Emulation of named constants. Although C++, Java, Visual Basic, and most modern languages support named constants, some languages such as **Python**, Perl, Awk, and UNIX shell script still don't. You can use global variables as substitutes for named constants when your language doesn't support them. For example, you can replace the literal values 1 and 0 with the global variables TRUE and FALSE set to 1 and 0, or you can replace 66 as the number of lines per page with LINES_PER_PAGE = 66. It's easier to change code later when this approach is used, and the code tends to be easier to read. This disciplined use of global data is a prime example of the distinction between programming in vs. programming into a language, which is discussed more in Program into Your Language, Not in It. [...] (goes on to list a few more reasons that are less relevant in python).
You will want `rtext.encode("utf-8")` probably.
Thank you. I just made a few changes, including having the function return the text of the request, not just the request. I'll update the source code there.
Agree on sigh. I was trying to point out legitimate uses of global variables -- merely adding relevant content -- but get sworn at for defending myself against *tongue-in-cheek* posts that imply I can't read.
&gt; pip install https://github.com/django-extensions/django-extensions/zipball/master This installs 1.4.0 for me, too. Are you using pip's cache? Then watch for "Using download cache from ..." - if I remember correctly it use the existing download always (and not verify if it has changed remotely). Otherwise a (misconfigured) http proxy might be interfering.
Global variables are useful (math.pi is defined as a global variable). If you have a module that uses a random number generator and you need to set the seed during a test, you need to modify global variables. Global variables and classes/functions are generally a bad idea. If you're starting out, never use them. If you know what you're doing, they're OK in limited quantities, but in that case you probably wouldn't be asking the question. They exist for a reason, but it's easy to be burned by them.
kinda like math.pi The all-caps constants is not a convention I've see in Python. It's a convention I've seen in pretty much every other language.
I was going to use configparser but wasn't sure if it supported lists
Because I had a problem and I decided to program a solution...
Well you could look through the damn script to see that your "precious data" isn't going anywhere except where you want it to
[PEP 8](http://legacy.python.org/dev/peps/pep-0008/#constants), the official style guide of Python (which is recommended by many to follow) states that constants are all-caps. The standard library does indeed deviate sometimes, but then again, the standard library isn't perfect :)
The prevalence of SQL injection attacks that happen, even today, shows you how good we are at input validation at the application level.
Every time someone says global variable I shiver. I just remember the PHP's REGISTER_GLOBALS and how wonderfully it worked on shared servers.
You are right. I had cache activated for pip and it caused the problem... After removing the `~/.pip` folder, Options 1, 2 and 2b give the same result. Thanks for the tip!
Yeah, personally I'd much rather see yaml config files with something like validictory to validate them, and integration with argument parsing &amp; env variables for easy overriding. Ini files via configparser is an obsolete approach.
If you think that rsync is a backup solution you're already failing.
I predict Germany will win. Where's my food?
You can achieve this using git: * setup git repos locally and remotely * add a post commit hook to the remote repo to run your app with the latest changes This way, when you're ready for your code to run, you just push to the remote repo. I do this with some static websites: code it locally, commit locally, push to remote git, and have post commit hook copy latest changes over to public_html directory. 
You have at least, since you insist on commenting everywhere.
sweet burn brah
Thank you for your work. Have a cookie! /u/changetip
The Bitcoin tip for a cookie (2.571 mBTC/$1.51) has been collected by *sentdex*. [ChangeTip info](https://www.changetip.com/tip-online/reddit) | [ChangeTip video](https://www.youtube.com/watch?v=_AnfKpypMNw) | /r/Bitcoin
&gt;However I don't understand the advantages of DI vs simply passing it in as a &gt;parameter: It's Java-itis starting to infect Python. When terms such as "dependency injection" and "singletons" start popping up, it's time for people to watch Joe Grigorio's [The (Lack Of) Design Patterns In Python](http://pyvideo.org/video/146/pycon-2009--the--lack-of--design-patterns-in-pyth) again. Here's a Stack Overflow discussion on whether this is really necessary in Python: http://stackoverflow.com/questions/6880884/is-there-a-point-to-dependency-injection-container-in-python
&gt; I am curious what (if any) potential downsides exist when using the Unit-of-work / session pattern? The obvious one is that you need to structure your code to work with transactions. Primarily this means that if transactions are expensive on your database the pattern might be a performance problem. With databases supporting MVCC this generally should not be a huge deal.
As i agree with Dependancy Injection is a good thing, it is really a side-effect of fully object-orientated programming. Its basically required in Java to write any kind of nice code. A more pragmatic view of why dependancy injection and passing it in as a parameter is that it makes any code 'reentrant' meaning its Thread Safe and it also depends solely on itself and the caller to setup everything correctly. You could also argue that Dependancy Injection is just an abstraction over global state because in each class you will for instance in objc for iOS use objection to ask for the singletons you want. in C you don't really think of things like this you just talk about making your code-reentrant and passing in the state for everything into each function. You can't get away from the need for globals in any programming language what dependancy injection does is hide this more and make it much cleaner and more standardised and just better code. For python i tend not to think in fully object orientated way as much as i can. Duck Typing and just having common sense is all thats really needed. you don't hear of any major dependancy injection libraries in python for a reason. In Java or objc etc its all that developers talk about.
[EDIT] If you can rely on users having pip configured to pull from an index server (default: pypi.python.org) which has a package named what you have in your `requirements.txt`, something like ``django-extensions==1.3.10`` in ``requirements.txt`` should work fine. Sometimes, I'll add a `requirements-dev.txt` with a `-e &lt;vc&gt;+&lt;uri&gt;` editable line for everything, in manually-topologically-sorted order. There's been discussion of supporting `requirements.lock.txt` files (to separate version specifiers from just package names in `requirements.txt`). To install a specific version of a Python package (with a `setup.py`) from GitHub with pip: **Option 2c** Install from a git tag and/or a GitHub release: https://github.com/django-extensions/django-extensions/releases $ pip install https://github.com/django-extensions/django-extensions/archive/1.3.10.tar.gz # - or - $ pip install https://github.com/django-extensions/django-extensions/archive/1.3.10.zip **Option 3b** Install as editable (git clone, cd, python setup.py develop). Add a version specifier to the editable URI: $ pip install -e git+https://github.com/django-extensions/django-extensions.git@1.3.10#egg=django-extensions
There can be other methods (apart from Init) where you pass parameters in that will act as dependencies. eg. Setting them as properties. The only point I was making is that people tend to get hung up on the Dependency Injection term as if it's a new way of life which requires a mental paradigm shift, when in fact it was always possible and practical to just pass things in to constructors, saying "use this please".
You don't put things in a global namespace in Python anyway. You put them in module scope.
/r/beermoney /r/Jobs4Bitcoins
sweet, thanks for all the good answers. I see the difference now
What does "editable" mean here? It's not clear to me.
I made a simple web app. that generates these pip lines for you; you don't need to memorize them. [It's here.](https://jabba-sandbox.herokuapp.com/github/)
If you want people to use the exact same version, note that you can also specify the exact commit (as option 3 shows you from `pip freeze`). Though I would recommend to find a commit that lines up with a version release tag and specify by version instead.
Well module scope is global scope in python. In the context of python, a global variable is everything that's returned by `globals()`, including functions, imported modules, classes, and variables/constants defined outside of a function or class. It's still a good idea to avoid having variables that may be modified in lots of parts of the program be defined at module level and used from global scope in functions.
I'm still a newb here, but I get a syntax error trying to run it. if self.x&lt;30: print "lost" global points I think you need () around that print, so it looks like print("lost") 
I'd say the majority of the time. It's pretty rare that I'd find myself throwing a ValueError or KeyError. Most of the time, I'll be generating exceptions more specific to the application, and so will have my own exception subclasses.
I usually write a base exception class for my API, then for API specific errors I inherit from that class to make an exception specific for that error. This does not mean I don't reuse those specific errors when possible, I try to keep the class count low. Then users of my library are able to catch multiple specific errors to handle those in particular before catching the library error for handling "something bad happened here". I usually reserve KeyError and ValueError for things that are more like bugs than actual errors. If a KeyError is raised, then I've probably written some bad code somewhere. 
Pyongame?
It's great you actually finished something! The first thing that I completely finished was a pong clone in Python/Pygame too; it's a fantastic learning tool.
You're using Python 3, he is using 2.
Pypong
As a rule of thumb, define your own exception when you want to handle it independently of other exceptions. try: expression1 # custom exception except MyError: expression2 # standard exception except ValueError: expression3 # all other exceptions except Exception: expression4
 def world_cup_2014_predictor(): return 'Germany'
Python's nomenclature is a bit wonky in that regard. The symbols returned by `globals()` aren't really global in the usual sense of the word, and in fact it will return a different set of symbols depending on where you call it! `__builtins__` has Python's real global variables. Even Martijn Pieters says it's misleading. 
PyCharm has remote execution.
Yeah it was the most rewarding part. I'm glad you liked it! 
...
Yeah, but if you're the one throwing the error, there's no reason not to subclass `ValueError`, so people can catch just your specific exception if they want to.
&gt; Using exceptions in the global namespace ValueError, KeyError, etcetera can get one fairly far (...) Certainly -- and _that's the problem_. By throwing only those generic errors, you're contributing to the problem that Python standard library is plagued by, which I [ranted about at length](http://xion.org.pl/2014/01/03/two-exceptions-to-rule-them-all/). Since most of the exceptions it throws is either TypeError or ValueError, recognizing what exactly went wrong requires unreliable message parsing, or is straight up impossible. Don't repeat the same mistakes in your own code. Sure, you can inherit your custom exception classes from ValueError et al. (this gives your users some leeway in how precisely they handle errors coming from your code), but please do *define* those custom exceptions. It's literally two lines of code: class InvalidFlorbError(ValueError): """Raises when given florb is invalid.""" With custom exceptions, not only your clients have fine-grained control over their error handling. Your test cases can also stop walking the line between underspecified ("this should raise ValueError") and brittle ("this should raise ValueError with message matching this regex"), because you can now assert the specific failure mode you're expecting and not just your best guess at it.
[Kivy](http://kivy.org/) may be an option. I have never had time to properly get into it though.
Pygame *is* available for Python 3. Here are the downloads from the official repository: https://bitbucket.org/pygame/pygame/downloads
I use pyglet 1.2alpha1 for scientific experiments that are basically simple games. It's stable enough. I highly recommend it, it's much more Pythonic than Pygame. Clone the `hg` repository at http://code.google.com/p/pyglet and give it a shot. Personally I think they were tentative calling it an alpha. It was released in 2012, they called it a "major" release, and it's been stable for us on a variety of different machines. The project has recently had a revival and it's very active if you check the commit logs. I expect an "official" 1.2 release soon. I also made a library for geometric shapes in pyglet, since there's no easy way to do rectangles and such: http://github.com/hsharrison/pyglet2d However if you're making a game you may prefer sprites, which are included in pyglet. We want to keep our displays simple so had to go a little futher.
But you don't really want to get in the habit of doing a lot of things "just in case". If it turns out it needs to be caught separately, it's easy enough to change. Until then, why bother? It's also still possible to catch ab exception in a specific place with just the stdlib exceptions. For example, there are lots of places in the standard library where you can catch a ValueError and be pretty certain that there's just one specific thing you can catch. They didn't have to make a different subclass of ValueError for every time it comes up, and yet it can still be caught with precision. Just my $.02.
Since I seem to be in the minority here, thinking you should rarely define new exceptions, I'll just post this: https://www.youtube.com/watch?v=o9pEzgHorH0 He talks specifically about exceptions.
Panda3D now has experimental python 3 support in the development versions. Key word is experimental. Compiling only works on Linux at the moment.
Insanity is a medical condition. Thinking the world is not real is a philosophy call Subjectivism. It, simply, is wrong, as are you.
Good talk but I'm not convinced that custom exceptions are bad. The speaker even mentions they can be good for specific problems. The gist of the talk is that "simple is better than complicated." Here's the most relevant quote from around 12 minutes in: [The python standardlibrary] only defines 165 exceptions in 200,000 lines of code. So anytime you think you need to write an exception, you probably don't, because, again, the python standard library gets along just fine with just 165. [Edit] fixed auto-correct mistakes.
Am I reading this right that he used the results of games from 80 years ago to predict this year's games? 
Thanks for sharing this. 
I come from there, Ghost.py hardly works with Facebook and YouTube, and after a time of scraping it breaks the application :(. If there where an alternative for getting the stdout without opening a terminal window, it would be perfect...
Thanks for putting the effort in, I didn't want to rewatch it just to find a quote or summarize. Yes, I agree, they're not "bad", you just shouldn't use them often. I'm mostly putting this up as a foil to /u/Brian, who says you should use custom exceptions the majority of the time, and /u/ksion, who says that the standard library makes a big mistake in not putting enough variety of exceptions. I don't really understand that point of view. If you're having trouble catching a specific occurrence of an exception, because you're catching others of the same type, you're probably making your `try` blocks *way* too big. I can't think of any other reason to make that critique of the standard library. I used to always write custom exceptions, thinking "Hey wouldn't it be nice to be able to catch this?" But at some point I realized that I *never* took advantage of that. Or at least, I never did it in such a way that would not have been possible with the standard library types.
I would absolutely recommend Pygame if you have no experience making games. Pygame's ease of use will teach you all you need about 2D game creation quickly and well.
I recommend Pygame, to the point where I wrote a (free) book on it: http://inventwithpython.com/pygame
but it works amazing well, at least on my machine :)
So, a part from the "Singletons are BAAAD" train-of-thought, anybody on why not a singleton on this? Since outside double-star-orbiting planets the Sun is as single as a singleton can be. 
For a python 101, I believe covering topics like: *1. why python? *2. python basics (integers, sets, binary, hex, strings etc) *3. python lists, tuples, dictionaries *4. python conditional statements *5. python while and for loops should be good enough i guess. I must've missed out on a couple of things but I hope you've got what I'm trying to convey here. Hope this helps! Oh, and good luck!
Thanks for this feedback! I find it very confusing... PyGame website always mention PySDL2 updates in its news, never anything about Pygame for Python 3. In addition, I found a ["Pygame Reloaded"](https://code.google.com/p/pgreloaded/) project that was turned into what is today known as PySDL2... Finally, it looks like there is no Pygame for Python 3 package in Ubuntu 14.04, only for Python 2.x: Package: python-pygame Priority: optional Section: universe/python Installed-Size: 6021 Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt; Original-Maintainer: Debian Python Modules Team &lt;python-modules-team@lists.alioth.debian.org&gt; Architecture: amd64 Source: pygame Version: 1.9.1release+dfsg-9ubuntu1 Depends: fonts-freefont-ttf, python-numpy (&gt;= 1:1.8.0), python-numpy-abi9, python (&gt;= 2.7), python (&lt;&lt; 2.8), python:any (&gt;= 2.7.1-0ubuntu2), libc6 (&gt;= 2.14), libjpeg8 (&gt;= 8c), libpng12-0 (&gt;= 1.2.13-4), libportmidi0, libsdl-image1.2 (&gt;= 1.2.10), libsdl-mixer1.2, libsdl-ttf2.0-0, libsdl1.2debian (&gt;= 1.2.11), libsmpeg0, libx11-6 Anyway, thanks for the info, I'll try to figure out how to get it on Ubuntu!
A function raises an exception when it meets a situation it cannot handle. The caller, however, may be able to handle such a situation or maybe interpret it differently, but it needs to be able to recognize different situations from the exception type. So you need a new exception when you need to handle it in the calling code: switch to a different branch, reinterpret it, etc. A conservative approach would be to use as few exceptions as possible and only add new ones when you clearly need them. This may get harder if you write a library though. For example, Zip archives can be appended to files. Somewhere deep down in a Zip library must be a function that tries to open a file and check if it's a Zip archive; if it's not, it will raise an exception. Also, functions that unzip individual files from the archive may also need to raise an exception if something is wrong. Would a single exception, e.g. `BadZipError`, be enough for these two situations? I don't think so because I can imagine a valid scenario when I need to tell a difference between a damaged Zip file (which I don't want to touch) and a non-Zip file (which I can use to append my archive to).
Gevent is really neat, thanks for sharing!
Part of the problem seems to be that Pygame's website is quite outdated... then again you could probably guess that from the early 2000s web design :P There's also Kivy which while not fully game-oriented can and has been used for games. I like it because it also pays attention to the packaging and distribution story including on mobile platforms. It does support python3 though that's not super obvious from glancing over the site.
An alarm clock tells me when to wake up - once I've set it. I don't consciously check the clock and roll back over if the alarm hasn't gone off yet.
In the last several years of writing Python, I have not once used a "global variable" the way the term is ordinarily meant. However, every Python programmer, including myself, technically uses global variables all the time. This is because top-level functions and classes *are global variables*. A quick demonstration: def gibson(): print("teh gibson") def hax(): global gibson gibson = lambda: print("oh noes!") gibson() hax() gibson() (Try it with an actual nested function definition instead of a `lambda`, too.)
Kivy certainly is a good option for some kinds of games (where 'some' is a broad category, not too limiting). That's especially true with the [kivent game engine](http://www.kivent.org/) by kovak (one of the core kivy devs), which is an early release but provides some really nice simple but performant game-oriented apis to kivy graphics and integrates some things like physics (via chipmunk).
Kivy is nice. I love how it separates UI and code. Sometimes, though, you may have to...reinvent the wheel for some things.
I'm currently using Kivy for a strategy game and so far I quite like it. Decent documentation, a rather nice API, can be ported to both desktop and tablet. I doubt it is the best option for platformers or any other game that doesn't require much GUI, though. As an aside, Panda3D is a rather nice option if you want to go 3D. I've tried it, but my main problem with it is the difficulty of implementing a GUI of any complexity in it.
Had no idea gevent was so easy to use!
 import datetime print ((datetime.datetime.now().hour % 12) + 1) * "WORD " 
had no idea what gevent was! :D
You might want to check out [asyncio](https://docs.python.org/3/library/asyncio.html), as it's a native Python way to do the same
 ((datetime.datetime.now().hour % 12) + 1) * "WORD ".rstrip() To remove any extra spaces at the end.
Many thanks - worked spot on. The script passes this as sys.stdout, I just need to work out how to use this as part of a command line arg for another python script now! Cheers again
Good post for someone who's starting get python addiction. Thanks for Pycallgraph.
Why not just call the other python script from Cron and import this function?
just google for "install pygame python3 ubuntu" and you should find some blog posts helping yout with the install procedure. pygame works fine with python3 once it is installed correctly.
Thanks, a delicious cookie! 
Since there are no transaction fees (usually at least 30 cents traditionally) with bitcoin, it makes for a good tipping method online. ChangeTip is tied into Reddit for this purpose.
Used Print instead of sys.stdout, Thank you.
It doesn't seem to work for python 3.4 on Mac.
Great article, thanks for sharing.
I recommend PySDL2. It's a new effort, and actively being developed right now. The pysdl2.ext package provides a lot of convenient libraries for game development. It also suports PyPy. If you care.
It pleases me to hear that active development has started back up with pyglet. The lack of active development is one of the reasons I stopped using it.
What i strugled with was all my points moved in the same axis, ive been takling that thing for a while. yeah i cannot understand everything in your code, but i will put effort into learning what they all mean, Thx for your help, Whats your btc address?
u should fuck him up for talking shit about your people like that, m8. stop being so nice to racists. 
When I am writing an application that other people are going to use, and I find a particular error may crop up, but the generic Python exception confusing in context, I usually create a custom exception. The goal is to get users as quickly as possible to the source of the error, and sometimes generic exceptions can throw you off the trail.
ipython notebooks look awesome. Wish I knew about them in college...reports would have been so easy. Beyond that, I'm unsure of how I'll use them at work.
[Virtual environment support is built-in in Python 3.3](https://docs.python.org/3.3/library/venv.html), so you don't even need to install an external virtualenv package. Can someone contact the author? I think it is a nice idea to add this info to the post.
To me, IPython notebooks are my productivity tool of the last few years: I use them in work to write my code, produce plots inline, and take notes all the way. Before IPython notebooks, it was all very tedious: separate script files, separate image files, separate documentation files, and a document that bundled everything together: and you always to have to make a lot of changes everywhere every time you wanted to modify or update something. &gt;"Wish I knew about them in college...reports would have been so easy." Don't blame yourself, IPython notebooks are fairly new and probably didn't exist at that time ;)
I use them all the time, especially when I have a dataset I know I need to manipulate, but I'm not *entirely* sure how I need to go about it. As an example, recently, I started writing a test script for Nagios in Python, with the goal of being able to provide memory usage stats for linux machines. I knew I could read the data I needed from `/proc/meminfo`, but figured I could be more efficient in Python than the 150-200 line bash scripts a coworker was tossing my way. I loaded the contents of a meminfo file up as a string in an iPython notebook, and was able to sit there and tinker with my code until I got to my goal, which was to convert the file to a dict in as few lines as possible. Here's the final code I put together: def convert_unit(size): mb_convert = {'b': 1024*1024, 'kb': 1024, 'mb': 1, 'gb': 1/1024} size = size.split() return size if len(size) == 1 else round(int(size[0]) / mb_convert[size[1].lower()], 4) with open('/proc/meminfo', 'r') as meminfo: mem = dict((x.strip(), convert_unit(y.strip())) for x, y in [z.split(':') for z in meminfo.readlines()]) The entire script turned out to be less than 30 lines long.
Isn't the convention to put requirements in the setup.py file now?
Wow! Thanks for sharing this!
I don't have an answer for you from experience, but here are two links about stuff built into python for distributing your own modules: [Distutils Python3.4](https://docs.python.org/3.4/distutils/) [Distutils Python2.7](https://docs.python.org/2.7/distutils/) There is also [py2exe](http://www.py2exe.org/) for packaging and distributing in a Windows environment
Thanks! Assuming the exe can hold external libraries like PyGame, this is exactly what I was looking for.
Wow...small world. I recently did the same type of project.
They make good data analysis notebooks and engineering notebooks. Helps you can inline R and Matlab as well.
I wouldn't have a setup.py if I don't intend to distribute the project I'm working on. This is typical for me developing web applications that are deployed onto my own servers, not distributed to a third-party.
As someone who has been programming in Python for some time, I wish I had known about buildout sooner than later.
I think it is always a good practice to package your (web) application as a python package. There are several tools which allow you to do that. 
Python 3.
I'll add a note for that, thank you. Edit: post is updated. Thanks again.
i don't really see what you're going through. How do you KNOW most of the time the smallest prime factor will be the square root? I understand that it is just a fact that the mojority of the time it is true, but i want proof. also "When we test square numbers however, the smallest prime factor will be the square root" is not true. Take 36, square root of 36 is 6, which is not prime, ergo it is not the smallest prime factor.
I try to resort to violence only when everything else fails :-P Cheers!
Good stuff. Makes me think about reorganizing our 4+-year-old Django project at work. (I once worked on a related project to the one he mentioned. Enterprise calendaring can, indeed, get pretty scary.)
It was an interesting watch/listen. I wish he gave concrete examples of how he'd abstract components and give a little more meat to what makes a good 'internal API'.
 ((datetime.datetime.now().hour % 12 + 1) * "WORD ").rstrip() Attribute access binds more closely than `*` so you need parentheses there.
I've heard of that as a practice, and I find it interesting. Do you have much experience with this approach, or know of any good documentation on it? How would this work with using virtualenv's for isolation of dependencies? I've seen fpm referenced as an easy-to-use packaging tool (https://github.com/jordansissel/fpm), which looks cool. Any others that you know of?
Uh sorry I'm bad at explaining this type of thing. The way I understand it(I may be wrong) is like this: 1. There's a prime factor less than the square root. In this case we know our number isn't prime. 2. There is no prime factor less than the square root, which means there can't be a prime factor greater than the square root. This doesn't mean it's prime in all cases though, because of square numbers. 3. Square numbers: It's the only time where it's possible for there to not be a prime factor less than the square root and the number not be prime, because the prime factors can be the square root. This is the "worst possible scenario", meaning that if we don't test up to the square root we will miss the numbers that fall into this category. I don't really know why it works this way, I just know it works. I'm not really too far into math yet so that's the best I can do.
I know how you feel, but if he provided concrete example, the talk would have probably been 3+ hours rather than 30 minutes. However, my take away is that the same principles apply as with everything software dev related. [High Cohesion](http://en.wikipedia.org/wiki/Cohesion_(computer_science\)#High_cohesion)+ [Loose Coupling](http://en.wikipedia.org/wiki/Loose_coupling) leads to more maintainable software. The abstractions provided by &lt;your favorite framework&gt; only get you partially the way there. So we devs need to continuously look at things and refactor b/c useful abstractions reveal themselves during implementation that are probably not obvious when you are thinking about how to design it. edit: broken link
You should use setuptools these days. Use this cookie cutter https://github.com/borntyping/cookiecutter-pypackage-minimal
From a style perspective, I would suggest using a tool like pylint or pyflakes. It will point out some sloppy whitespace issues, for instance. Python has an integer division operator, // , so int(x/i) would probably be better as x // i, especially since you know that i divides x. From an algorithmic perspective, you're actually wasting a lot of time by incrementing i down from x-1, rather up from 2. There are three reasons for this: 1. The number of factors of x between 2 and sqrt(x) is equal to the number of factors between sqrt(x) and x. (You use this fact in your prime-checking code). However x - sqrt(x) is much larger than sqrt(x) - 2, so you're checking way more numbers to find the same factors. As an illustration, every number between x/2 + 1 and x-1 is guaranteed to not be a hit, and you're checking all of those numbers *first*. 2. The smallest factor of x (besides 1) is guaranteed to be prime. (A non-prime factor 'm' would have prime factors, all of which are themselves factors of x and smaller than 'm') The largest factor of x (besides x) is very unlikely to be prime. (The only way that it will be prime is if x = p * q, both prime). 3. Once you find a prime p, you can dividing by that same prime until x is no longer divisible by it. Then you never have to check it again, and you only have to check up through sqrt(x / p^k ), rather than sqrt(x). Besides that, since you're dealing with such small 'n', you might as well compute all the primes once and look them up, rather than running the actual 'prime' function so much. You'd have to benchmark to check, but even for n in the hundreds you're probably better off with an LUT.
You would probably want to make an extension, i.e. Chrome Extension in JS, for whatever browser you are using. I don't think python is your best option here. That being said, I don't know if Chrome's capabilities would even allow you to do this.
Just as the author advises, the downside of this post is that it is very Python 2 centric and most issues are irrelevant if you use a modern day Python version. If you want to start working with Python today, you'd have to be masochistic to use anything but Python 3. Much of the tools mentioned (requirements.txt -&gt; setup.py, virtualenv -&gt; pyvenv, nose -&gt; tox, gevent -&gt; asyncio is built-in) have better or more advanced counterparts if you work with an up-to-date version of Python. And those still hanging on to Python 2 with all their power should know everything mentioned in the post, anyways...
This was pro bono (free). Let me briefly explain how it works. The main outline is to convert the screen 600x600 into math axis x,y (line 93 is the same as lines 70-73). Note that in math the y-axis is positive going upward, in the screen it is backwards, y goes 0 to 600 moving down. To calculate the quadrant we need x and y relative to the center math(0,0) = screen(300,300) , XY2Quad(x,y) is complicated but a simple division into cases. For rotations, the polar = (radius, angle) system is crucial, so we have XY2RA(x,y). The actual rotation is simple then, just increment the angle for counterclockwise movement (decrement for clockwise). Then just unwind all the transformations to go polar to mathxy (RA2XY(r,a)) to screenxy (line 104).
Drive of eratostenes....Google it I'm on mobile.
Also the subprocess module int he standard library has a async map / threadpool implementation, so if that is all you need check it out.
No need to - check out tabmans tab manager for chrome
I did. However it's different to my idea. In the Tab Manager App, you can only switch through the tabs by using the mousewheel, when the mouse is on the docked app. While I'm thinking you can switch through the tabs with the mousewheel, simply by pushing the mouse cursor up to the bar where the tabs are. 
It works this way in Chrome and Chromium on every single Linux-based operating system I've ever tried. Maybe it's time to run from the sinking cruise ship that is Windows to the under-construction-but-floating dingy that is Linux? You can even try Linux for free without installing it to your hard drive (eg: you reboot and it's completely gone). Start here: http://www.ubuntu.com
i see what you mean, thanks!
Disclaimer: I love Python, much more than R. But to be honest, I suggest you investigate R. With ggplot and shiny+RMarkdown you can streamline a process to generate those reports as nicely formatted Word files for your boss. Generating great looking plots in R is way simpler than in any other environment I am aware of.
Your comment makes me realise how abstract and high-level this talk was. As someone who has done Django projects at a range of scales over the past 8 years this talk chimed with the lessons I myself have learned about how to organise a project. I tried to write this up a long time ago in a [blog post](http://mauveweb.co.uk/posts/2011/05/mvc-controllers.html) but re-reading this now I realise it's pretty badly written. So to turn my thinking into practical recommendations: * keep ORM queries in models.py, learn how to define custom managers, queryset subclasses etc as appropriate and encapsulate the object queries you need using these features * keep presentational code in templates. Write template tags and filters to add power to the template syntax. * build on the forms system to do business validation in forms.py, so that errors can be faithfully tied back to specific inputs. I typically go beyond "is this value in the right format" and validate whether it refers to correct data and whether it constitutes a valid business transaction. * having done all this, what business logic remains should go in a new python file of your own invention, not one that Django provides * what is left in a view function should be just glue code to link requests to forms, forms to business logic, outputs to templates 
That's impressive! About switching to Linux : you need to realize I'm not that extremely tech-savvy person...I'm very very new to programming(although past 'hello world') so at the moment Windows provides enough functionality for me. Later on...maybe I switch to Linux. Also...how is Linux better? I'm not challenging your comment, just wondering what specifically does Linux offer that Windows doesn't?
I think in the end I'll do it with Autohotkey...will be much much easier. I'd have to start learning JS(which I will at some point). 
 lcm(a,b) = a*b // gcd(a,b). Use Euclid's algorithm for gcd; the answer = reduce(lcm, range(2,21)). If you want a list of millions of primes though, a really fast sieve is http://code.activestate.com/recipes/366178/
Excel is shit. Period. It's expensive, slow, difficult to program for, and has all kinds of quirky stuff that does things like cause investment banks to make billion dollar blunders. It persists due to institutional inertia and nothing else.
Python over Excel any day. Python gives you far more logic, customization, and automation than Excel ever could. Matplotlib is capable of having nice looking, simple graphs, you just need to customize. The easiest thing to do there is to use styles, and you can even use some pre-made ones so you don't have to bother learning much at all: http://pythonprogramming.net/matplotlib-styles-tutorial/ I'd personally suggest you look into Python, Pandas, and Matplotlib to do the majority of your work. From there, look into one of the many py-PDF modules out there to see if one of those suits your needs. As for pandas: might I suggest the link I posted in here earlier: http://pythonprogramming.net/python-pandas-data-analysis/ Best wishes! If you have any questions, I'd be happy to further elaborate. 
Don't underestimate ugliness of R. Sure, R is a great tool for statistics, but design of language is very poor (my opinion). It wasn't designed by CS guys/software engineers. I use R only when I absolutely need to. I'm not experienced with rpy2 (I've only used R with Mathematica via RLink) but coziness of Python/Mathematica isn't something I'm willing to sacrifice ...
I appreciate the response fnl! I've thought a lot about using R as I know ggplot is basically the premier plotting tool. Perhaps I'm simply ignorant and R can do all of these tasks, but I was worried that R does not excel in these areas: * Not just automatically pulling data, but sending out that data daily. * Implementing conditional logic such as: if 3 metrics have a 10% increase day over day, send an email to foo@bar.com Even if R is able to do these, it would need to be a good amount better than Python as I'm already decently familiar with Python. If it is indeed much better at achieving my goals and doesn't struggle in the above areas, I'm definitely open to using R for this. 
I really appreciate the response sentdex! Exactly what I was looking for, going to read over all of this shortly.
Appreciate the response jmmcd! It seems like a lot of these focus on interactiveness and advanced features where I need to focus on something that makes it easy to create visually appealing relatively simple graphs. I appreciate the links though!
Some great resources here if you want some depth on the approach: https://hynek.me/talks/python-deployments/
I prefer 3.4, but knowing 2.7 isn't a downside. Most of the LTS systems I touch on a daily basis have 2.6/2.7 on them, so when I want to write a quick script I know will run on all the machines in my environment (without resorting to bash scripting), the older version tends to be the way to go. Same with ruby (1.8.7), which sucks horribly compared to 2.1.x. But ya, starting with 3.4 is definitely the way to go, I would certainly want to know 'how' it differs from 2.7 though for when I go to use it for a quick script. 
* [pandas](http://pandas.pydata.org/) for data handling. It's incredibly useful! * matplotlib to plot. You may have to code more than in some newer plot libaries, but on the other hand you can customize everything. Also it seems to be the most matured plotting library for python. * python-reportlab to generate PDFs
Also [`seaborn`](http://web.stanford.edu/~mwaskom/software/seaborn/)
[`seaborn`](http://web.stanford.edu/~mwaskom/software/seaborn/) gets you ggplot-like faceting and other powerful plots. Not to mention [`ggplot` for python](http://blog.yhathq.com/posts/ggplot-for-python.html).
If you know ggplot2 I highly recommend Seaborn, see my reply to /u/fnl. There's also a ggplot clone for Python as well, but Seaborn is more Pythonic. Also even if you just stick with matplotlib or the plots pandas spits out, even only importing Seaborn will make your plots look better.
One idea is that you have dependencies required during development but not in production like your test framework of choice. You can keep the production required dependencies in `setup.py` while having the development-only dependencies in a requirements.txt file.
When you mentioned Excel and Python, my mind went straight to [Resolver One](http://en.wikipedia.org/wiki/Resolver_One). Unfortunately, Resolver Systems seem to have shut down. Back to your problem, i think the main difference between Excel and Python is the knowledge gap: Python is still a niche, whereas VBA or C# people are a dime a dozen. The ecosystem for that sort of requirements is much more developed on the Microsoft side, so chances are that Visual Studio or an add-on can already do 90% of what you need; whereas with Python you'll likely have to write everything from scratch -- more fun for you, but more of a headache for the company to maintain if you step under a bus, so to speak. If you go for Python, you'll also have to decide how to store your data (csv files? xml? rdbms?) -- again, more work and complexity. If you know of some specific python lib or tool that would help you (I remember reading of a python lib that could do OLAP cubes, and I know BoA developed quite a bunch of python-based systems of debatable quality) or you are 100% sure that python will speed up your development times because of feature X or Y, by all means consider it, but to be honest, for something like business reporting, the ecosystem is so much more developed on the Microsoft side, I would struggle to justify my beloved snake. EDIT: ...or you could use both, via IronPython :)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Resolver One**](https://en.wikipedia.org/wiki/Resolver%20One): [](#sfw) --- &gt; &gt;__Resolver One__ is a [Spreadsheet](https://en.wikipedia.org/wiki/Spreadsheet) program created by Resolver Systems with the [IronPython](https://en.wikipedia.org/wiki/IronPython) programming language. It is a desktop application, with a web server version, and intended as a platform for the rapid development of business applications. It runs on [Microsoft Windows](https://en.wikipedia.org/wiki/Microsoft_Windows), and is free for use in Open Source projects. &gt;Resolver One combines the functionality of a spreadsheet and an [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment). Spreadsheets are turned into Python code and executed in real-time. Code can be mixed, using both [Python](https://en.wikipedia.org/wiki/Python_(programming_language\)) and [.NET](https://en.wikipedia.org/wiki/.NET_Framework) libraries. Spreadsheets can also be exported as stand alone programs. &gt;On 3 October 2012, Resolver Systems announced that the product had reached its end of life, and would no longer be developed, due to insufficient sales. &gt; --- ^Interesting: [^Resolver ^\(Shinhwa ^album)](https://en.wikipedia.org/wiki/Resolver_\(Shinhwa_album\)) ^| [^Systema ^Naturae](https://en.wikipedia.org/wiki/Systema_Naturae) ^| [^Comparison ^of ^spreadsheet ^software](https://en.wikipedia.org/wiki/Comparison_of_spreadsheet_software) ^| [^Domain ^Name ^System](https://en.wikipedia.org/wiki/Domain_Name_System) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjk30d2) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjk30d2)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
seaborn and bokeh are dope. highly recommend them.
The original data is stored in a redshift database. Thanks for the info, will look into it.
Chrome extensions are written in JavaScript so python would not help here. 
Since you're saying excel I'm going to assume windows environment. I'm wondering what db you're using. If you're using Sql server as your db, I'd recommend using SSRS since it comes with SQL Server for freezies. It's really good for showing off charts or tables of data that your user can filter. Plus it's web based which makes it accessible to all. When it comes to alerts, I'd also do that on the db side, much easier to do that on trigger than to have some script that monitors (IMO). Plus depending on your db, you can really fine tune your alerts (email, sms, voice, etc.).
R is FANTASTIC for one-off graphs and exploratory analysis (fitting a quick model, visualizing data interactively, etc.) but horrible for anything you need to be scriptable and reliable. 
&gt; how is Linux better? Simple answer is that it's smaller, lighter, infinitely more flexible/customizable and you are allowed to see what's going on under the hood should anything break. You should be aware of it's drawbacks however. Namely if you rely on proprietary software it may not work for you, or it may almost be more trouble than its worth. For example Netflix doesn't work natively, unless you want to use a chrome book or Chromecast. That isn't true for everything though; there are Windows applications which are actually faster on Linux and Wine than Windows. &gt; what specifically does Linux offer that Windows doesn't? Complete control, timely updates, better security model, a central curated software repository for the distribution being used, a modular approach so you can swap out desk top environments or file systems or window compositors or kernel schedulers relatively easily without breaking your system, ownership - generally speaking if something broke, with a few exceptions, then it was not only likely your fault but you should be able to fix it. Edit. Linux is, in my opinion, more democratic than anything else outside of a text book. It's developed by the people for the people and is completely transparent. If there is something you don't like then you are welcome to change it. Many people have disliked how something was done and decided to roll their own version or even distribution and people can switch between versions or distributions with virtually no cost other than the time it takes to install the new system (provided you kept your home on a separate partition). Of course the side effect is the user has a ton of choices which can be overwhelming to new users and what I consider to be a huge strength is often portrayed as a weakness by some. While some vendors have found great success by limiting their costumers choices and locking them into their proprietary systems I prefer the choice and community offered by Linux. Also the vast majority of things run either Linux or Unix, from Amazon and Google to toasters and cars along with everything in between - like robots and breweries, so there is that.
Asking `python` vs `excel` in the /r/python is like asking about the benefits of having children in /r/childfree. For some balance, I'll talk about excel. Excel automation is very, very, easy to do. You can even use an odbc connector for excel to automatically pull in data on a daily basis. 40 graphs/day, especially if you already have the excel knowledge, may actually be the easier option. Beyond that, think about the technical knowledge at your company-- are the people around you more familiar with python, or excel? The alerts you mention are very straightforward to do in excel. The reports are just as easy, excel natively supports saving to pdf (you'd do this in vba). That said, you need to also mention other specifics about your company. Do you use sharepoint as your 'server' architecture? If so, consider excel. Do you have linux servers sitting around? Consider python. The cost of learning python is actually a little steeper than learning vba, but python has greater flexibility. Do you have very limited time resources? Consider excel. Are you using this as an excuse to learn a programming language? Strongly recommend python. Lastly, the people in here saying excel is shit don't realize the power of excel. Excel is a very powerful application and can be used to do a great many things, all of which are possible in python. However, the visual environment that excel provides cannot be underestimated-- it may actually be faster to develop a full *application* in excel than even python. Also, if you're doing any truly serious business analytics, you shouldn't be using either of these; you should be using Tableau, chartio, pentaho, or another piece of software for business intelligence. 
I still think module scope is a global scope where the general arguments against global scope apply. E.g., generally bad to define variables that get mutated (in many locations) at the top level in a module. Yes, you can use `__builtins__` to make an ugly hack that has globals that transcend modules, but you really shouldn't do this when globals at a module level (plus `from blah_module import SOME_GLOBAL`) is more maintainable (e.g., every file that uses SOME_GLOBAL has a reference where it came from; and even the bad practice of `from blah_module import *` lets you which modules to check). The standard arguments against mutable global variables (spaghetti code; threading issues; tough to follow logic and maintain) are present, plus additional issues arising from how python's internal rules on how it deals with globals/locals: x = 0 def this_works_fine(): print x def this_sort_of_works_fine_but_doesnt_alter_the_global_x(): x = 1 def this_changes_x(): global x x = 1 def this_raises_unbound_local_variable_exception(): print x x = 1 
 def primes(n): nroot = int(n**0.5) sieve = range(n+1) sieve[1] = 0 for i in xrange(2, nroot+1): if sieve[i] != 0: m = n/i - i sieve[i*i : n+1 : i] = [0] * (m+1) return [x for x in sieve if x] 
Linux or Unix then use screen and or SSH, Windows 7 and beyond have power shell with a commandlet called invoke-command which more or less just runs remotely whatever you tell it to. For power shell to work like this will take a little configuration.
As much as I hate to be the one to say it, I'd recommend going with Python to generate csvs, and Excel to display the data in graphs. WAIT HEAR ME OUT Of course matplotlib is going to be better. But If this guys office is anything like mine, the number of people who "get" that will be close to zero, and will insist on receiving their data in Excel anyway. Especially if you're going to be showing this data to anyone in sales or accounts. Excel might as well be the operating system for half of corporate America, and if they figure out a way to receive email inside of Excel, it may very well happen in the future ("Excel OS" ha ha CRINGE). VBA is gutter trash, yes, but it's easy to jimmy something together and everybody will think you're a motherfucking wizard for knowing how to use it. Give Excel a second thought. It's the second technical choice, but it may very well be the first logistical choice in your situation. 
I think in Python's case it can be argued in different ways. It's global in the sense that it's a variable from outside of the local scope that can be referenced without qualification, but the same is true of locals from enclosing functions. On the other hand, it's also quite clearly namespaced such that it won't interfere between modules. That's probably getting needlessly pedantic, though. To avoid confusion, I tend not to use the term 'global' when talking about Python programs, favouring instead 'module scope'.
Perhaps you should ask the person/company offering the job. I'm just as clueless as you.
Does your company have JMP? I started with Excel, got frustrated and then moved to Pandas/Python, now I'm using JMP. 
I still would recommend python over R. R is just not very accessible and the code that ends up being written is inevitably shit and unreadable. Python at least has enough constraints that the code is usually readable. If you have a programmer then yeah go for R. But they probably wouldn't need to debate about it, they would just do it.
I'm not set on best practices, but ruffyens suggestion is what I spent the summer doing. Developed a really nice financial application with a lot of charts/customization with python to json to highcharts/javascript.
Sure. So you write whatever's appropriate to the situation. The Listener pattern is a common idea.
Most all programmatic aspects (e.g. conditional logic) are more straight forward in Python, but to R's credit it is a very flexible language and can handle the two issue you listed with not too much more effort. Here is how I use R to generate a weekly report: 1. Chronjob an rscript which handles the data loading and conditional logic (for setting recipients etc), 2. use rmarkdown/knitr to generate a pdf report, 3. and sendmailR to send reports. Would I do redo it in Python? If I had the time to learn the packages, yes. I'm really used to R's eco-system including data.tables, dplyr, ggplot2 (ggvis) and knitr. Those come at the cost of a clunkier syntax unfortunately. 
If You are about games or desktop/cli applications, then executable will fit You. But if You are about web application, its common practice to keep all dependencies in requirements.txt file. After that, another person will just set up environment like that pip install -r requirements.txt
I recommend pyInstaller. It works with Python 2/3 and is multiplatform. http://www.pyinstaller.org/
What particular factor distinguishes you from people who are sane/not crazy?
You should only calculate the md5 if there are 2 or more files of the same size, it reduces the number of file reads considerably (100+ times faster if you are running on a large number of files).
Oh and my recommendation would obviously be to use Python. Excel is great for summing up some numbers or viewing a table but anything else can be done better in another tool. Add to this the fact that Excels scripting language, VBA, is a nightmare to use when compared to Python. A lot of people have suggested using Python to create PDFs, Excel Spreadsheets or whatever. However, I suggest stepping out of the box and not going that route but instead using Python to create a website that will display your reports and visualizations exactly as you want them. This will also allow for interactivity. Creating static reports in PDF form or in Excel format, which is just as static, is a complete waste of time. The future of reporting and analytics is creating dashboards and reports that are repeatable, tell a clear story that is backed up by data, and are accessible on any device. Python provides a method of doing this. Excel/VBA/PDF does not. 
I see. The whole issue that I'm not using Linux, would be limited programs. Though I find myself doing most of the things in my browser, from time to time, I use programs as well, and I don't want to use all that capability. 
Well, for one it offers you the ability to switch between tabs in Chrome/Chromium by simply flicking your cursor to the top of the screen and scrolling up/down to switch tabs to the left/right. That right there is extremely useful. I find it much, much easier to do software development on Linux. Microsoft products try to make things "easy" for you, but they don't teach you what the computer, OS, and all the other parts actually do. Linux basically forces you to learn gcc, gdb (if you're so inclined), Bash, Make, etc. It sounds like it sucks but being able to understand what the computer is actually doing is useful when you're learning how to code. Basically, if you want your hand held, use Windows. If you want to actually learn stuff, learn Linux. If you can afford it and don't hate them, get a Mac. Furthermore: Linux has had an "app store" where 99% of "apps" are free since before iTunes. It's much more mature and a breeze to use. Finally: Linux, and 99% of the packages you can install from the repositories are open source. So you know the NSA/GCHQ isn't spying on you through your webcam via a vendor-created backdoor specifically tailored just for them. I realize that most people don't care about this (yet), but I think it's important to take a principled stand and make it as difficult for them to do this as possible.
The point about having my hand held is a good one. I'll definitely think about it. Thanks mate! 
Have a look at [Cocos2d](http://cocos2d.org/) it has everything you need. Here's a simple game I made with it: https://github.com/aikikode/pyvolley
Gotta agree here. I'm only a python very-beginner, but it's much faster with a bit of python code to (just as an example) download stock data from google finance, and instantly plot my own graphs looking for specifics I want, using matplotlib. Excel is very useful, but it's actually more work once you know some python.
Which programs are you using? Often times there are alternatives, usually free, which are perfectly adequate for getting work done. The big ones I've heard of there being issues with are AutoCAD, Photoshop, and some video editing software. Another thing about Linux is that there are some great tools which are seamlessly integrated into the system like vim, Emacs, GCC, etc... One other thing I've come to love it for is the shell, or terminal; it's incredibly powerful and extensible - you can automate even the most complicated tasks if you know what you are doing.
That's one of the reason i like to share my post, so that my errors don't hide in the dark, I have updated it nharding, many thanks for the advice. note: my pass is not working for marvatu and i haven't set email :(
On top of this EVERYTHING is going to the Web. If you do it right, people can access your charts from any Web device. [Example ](http://m.imgur.com/7MKm9eG). This is a screen shot of live data that updates when new data is available every month. From my phone. Also the chart is interactive you can click the dots for more info. This one specifically is done with dx.chartjs.js from devex but you get the idea. 
Can't you always make your exception match one of the built-in, even in a somewhat more generic way?
I hate microsoft as much as the next guy, but, this post is the best so far. Personally, I'd go for a python solution because when the project grows, or they ask to add something unusual, or publish to the company intranet as html which is running apache, etc. I THINK it will be easier and more robust to do that in python. that said, if i needed to bang this out and move on, but wanted to know that it would work reliably, i'd look at excel. if i had excel on my desktop. which i don't. so. ;-)
That's basically covered in every game making tutorial.
i love python, but let's put it this way. if this isn't your business, go with what you have. if excel is what everyone uses, use excel. you don't want to force everyone to learn a new technology just because you like it. an example is Daimler, these guys make super cars. business analytics? my friend spent 6 months over there writing VBA scripts. Comp Sci grad. Comp Sci at its core is effective problem solving. solve your problem in a way that would cost your company the least amount of money while delivering a product they can effectively use. 
I would build these as web applications. So you could use Python on your back end and JavaScript, HTML, CSS for your front end. If your mentioning excel though it sounds like Visual Studios and C# would be a better option. Or you can use an out of the box solution like Qlik, Tableau, or Spotfire. 
As a way to learn how creating and publishing packages in Python works, I put together this little project. We're using it in our team along a few other test frameworks. Hopefully it's useful to someone else.
[Example code](http://toga.readthedocs.org/en/latest/introduction/tutorial-0.html)
I watched the talk, wondering when he'd actually say something interesting, and ended up feeling the same as you. The talk should probably have been called 'I don't like this crappy bit of Django code' So far as I can see, the example he used doesn't actually do MVC in anything but name. Everything is jumbled up into the one function. There are conflated problems. What Django calls a 'Model' (and to be fair so does SQLAlchemy) isn't a model. They are both ORMs or ActiveRecords. My understanding of 'Model' in the MVC sense is 'business logic', ie., an interface that manipulates business data. How the data is stored (RDBMS, document store, whatever) doesn't matter (at least in principal; it may have big effects on how well the application runs). What Django calls a 'View' I'd think of as a 'Controller', and a 'View' is a template plus maybe some separate logic than handles presentation of the business data. The Controller should mostly be limited to moving data to and fro between Model and View (with maybe some logic related to the workflow). 
&gt; there aren’t many good options for Python programming on mobile platforms, and cross-platform mobile coding is still elusive. Toga aims to correct this. It's interesting to see this, though I'm not clear what the roadmap is for making it possible with toga's native widgets approach.
Oh I don't use that many programs to be honest. At the moment no programs at all...all my time spent on a computer is on the browser. However you know...though you only need 5 minutes to complete something, you'd feel so much better of they told you 'you have unlimited time' rather than 10 minutes to complete it. Just knowing that the increased program capability is there, is comforting, though I might not use it. However and on second thought this might be a bigger reason : I'm not planning to go all out on programming. I have other things going and was thinking that maybe taking on small projects or learning a bit about programming would be cool. Not really thinking about taking a whole computer apart or invent the new Facebook or something. However as I said, this might be temporary. My ambition might soar in the future. 
the fact that i am insane/crazy
Why not use opensource alternatives to Excel if you want to go towards open source b.s
Lol
I agree with others about pandas, matplotlib, ipython for chart-hacking, seaborn for making graphs look nice. But - this is a plan for making static graphs. sure, you can run the script(s) multiple times a day, but you're making a static report. You should consider a tool like Looker, if you have the budget. Looker will connect to your db back end and, with some modeling that will be familiar if you're in the sql world, make charts that update with the click of a button. Also check out Chartio. PM me if you'd like to discuss. 
Why not both? found this on /r/Python a while ago. Haven't tried it, and don't know what exactly it can do (if it can help you with the graphs), but go ahead and read about it, and it might help. http://xlwings.org/
Are you serious? Sounds like a homework request.... If that's the case, why don't you follow any tutorial and actually learn something rather than cheating? If your name is really Kendall Mclennan, AND you're posting here for a homework request, this is even dumber than I thought. Same Kendall Mclennan on various essay selling websites as well I presume? Boo you.
It's generally worth doing cheaper tests (such as file size etc) to eliminate things that are definitely non-duplicates before bothering to do the expensive "read the entire file and build md5". There are other checks that can be cheaper too, especially on huge files, such as just sampling parts of the file and generating an md5 from that (ie. take a block from the start, middle and end - generaly sufficient to eliminate differing files before falling back to a full scan to confirm that) [Here's](http://www.reddit.com/r/Python/comments/27gumv/share_your_helper_scripts/ci1nb7t) a script I wrote for the same job, that uses those strategies. It's a bit longer since it does a few more things (args, prompt for delete etc), but the core of it is to put potential duplicates into a sequence of buckets (where items in different buckets are guaranteed not to be duplicates), and then putting each bucket through tests that divide them up in increasingly expensive methods until they've only got one item and so can be ignored, or everything in a bucket has passed all attempts to split them up (in which case they're presumed duplicates). 
I would guess that you'd need to demonstrate proficiency in * list comprehensions * decorators * operator overloading * named parameters I recently had an interview and they asked about new vs old style classes, something that I wasn't able to answer.
Sure; check out the shiny docs, e.g., the observers. And you can send HTML-formatted email with a few tricks from an R script, too. (Oh, and regarding some of the other comments here: R does support OOP and modularity, and you can write some very beautiful code in R, designing elegant libraries and applications -- if you know how to work with R, just as with any other framework/tool/language...) However, be aware that, in the end, all roads lead to Rome. Your tool of choice (Excel, Python, R, whatever) should be the tool you know best - unless you are getting payed for investigating a new tool and the months it will cost you to use it properly. So, for you, that probably means going with either Python or Excel. I don't know enough Excel/VB stuff to give you an answer on that question, however.
Got any factual reasons for that or is that just your opinion? How do you explain that mosty banks or insurance companies nowadays have scores of R programmers in addition to their "traditional" SAS teams [1] (http://www.nytimes.com/2009/01/07/technology/business-computing/07program.html?_r=0)?
can we not depricate win32 already?
And again - any factual evidence for those arguments? R supports modular, object oriented code and I think you can write your code beautifully or unreadable, just as much as in any other programming language. Grasping the "vectorization approach" of R might be tough at first, but then, writing clean Python with its "generator approach" is tough and unique, too.
Sure, but even all upcoming LTS releases will now have 3.x support, see e.g. [Ubuntu] (https://wiki.ubuntu.com/Python/3). However, as I said, my point is that it is important to make people aware of the fact that by learning 2.7 "first" they are committing an outdated dialect of Python that will die sooner or (hopefully: than) latter...
use the right tool for the job
This. The question is really: what is the least common multiple of the first n natural numbers? #!/usr/bin/python def gcd (a, b): """ Find the greatest common divisor of two numbers using Eulclid's algorithm: gcd(a, 0) = a gcd(a, b) = gcd(b, a mod b) """ if b == 0: return a else: return gcd(b, a % b) def lcm (a, b): """ Find the least common multiple of two numbers using reduction by greatest common divisor. """ return (a * b) / gcd(a, b) # Sample usage to find the combined LCM for all natural numbers up to # and including maxn least_mult = 1 maxn = 20 for n in xrange(1, maxn+1): least_mult = lcm(least_mult, n) print least_mult 
Excel is a good choice if: - you won't need to scale - only one process will use this data If you can tell up front that these conditions are guaranteed to hold long enough to have made the development worthwhile, then go for it.
(Unfortunately) unpacking in function parameters is removed in python3. http://legacy.python.org/dev/peps/pep-3113/
This is cool. Would love to see a blog post or something going into more detail about the OS native part.
Wow, they actually removed something useful in Python 3? I guess that explains why I've never heard of this. I'm going to give that a read to try to find out why, because I really really like this syntax.
Only in function parameters, everywhere else they actually expanded its usefulness.
Which is?
your webcamforward_server app gives error when i try to run it regardless of what i name the app ERROR: conflicts with the name of an existing Python module and cannot be used as an app name. Please try another name. 
The more advanced statistics or uncommon statistics packages you need, the more likely you'll need to use R since it goes without saying it has a vast repository of statistical packages. Otherwise, I think Python is good enough without knowing your specific use cases. Some R people have stayed with R due to its ggplot2 plotting package, but there is also a Python version of [ggplot](http://blog.yhathq.com/posts/ggplot-for-python.html) by yhat and also check out [seaborn](http://web.stanford.edu/~mwaskom/software/seaborn/) plotting package which also does trellis-style plotting too. If you want do more besides doing data analysis or statistics, then Python will most likely have libraries available for you to expand into other areas. When the Julia programming language reaches critical mass in terms of acceptance in the data science or computational science or high speed computing community, it will be easier for a Python programmer to transition to Julia compared to an R programmer due to similarities between Python and Julia. At least it was easy for me. Hope this helps.
The layout stuff is incredibly readable. It looks like container.constrain(button.TOP == container.TOP + 50) container.constrain(button.LEADING == container.LEADING + 50) container.constrain(button.TRAILING + 50 == container.TRAILING) container.constrain(button.BOTTOM + 50 &lt; container.BOTTOM) Apparently it's a similar engine to Apple's AutoLayout. 
You'd be surprised. Tkinter looks outdated and is hard to work with on large projects. I'd use wxPython.
I like what this person has to say about that: https://twitter.com/seriouspony/status/488036445337030656
After running 'pip install toga' on 32 bit Python 3.4.1 in Windows 7: Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:38:22) [MSC v.1600 32 bit (Intel)] on win32 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; import toga Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\Python34\lib\site-packages\toga\__init__.py", line 92, in &lt;module&gt; from .platform.win32.app import * File "C:\Python34\lib\site-packages\toga\platform\win32\app.py", line 5, in &lt;module&gt; from .libs import * File "C:\Python34\lib\site-packages\toga\platform\win32\libs\__init__.py", line 38, in &lt;module&gt; from .constants import * File "C:\Python34\lib\site-packages\toga\platform\win32\libs\constants.py", line 533 MB_TOPMOST = 262144L ^ SyntaxError: invalid syntax &gt;&gt;&gt;
Nice. I can certainly see uses for a clean wrapper for PyPI that just deals with getting information, not downloading and installing packages.
It depends on the ones asking but it may include knowledge of generators, descriptors, metaclasses, and context managers.
Thanks. I actually wrote it for https://pypip.in/ and decided to write it properly and cleanly and open source the library itself. =)
As usual with these types of threads, what's the benefit of using this over pyqt4 or similar? Laying stuff out in qt4 can be done with the creator , or its surprisingly intuitive to just do it by hand
Still though... Toga only has implementation in Windows for win32 NOT win64... At some point a call needs to be made and development on net-new needs to cease and desist for win32. sigh. I love windows and hate it at the same time.
I'm running windows 8.0, a Toshiba AMD, and IDLE won't open, won't even show up in the task manager processes, gives no error message - just shows the swirling circle by the mouse arrow briefly. I've used other text editors, but was hoping to use IDLE. This is with Python 2.7.8 (64-bit) installed. 
Python already runs on Android, and there are ways to actually bundle Python apps into APKs. iOS on the other hand...
*not python* in this case.
Why?
I'm new too in GUI use, I'm starting with Tkinter because: - It is into the standard library. - I'ts simple, easy to learn. - It has a good documentation. I've chosen it for starting in GUI dev, when I master it then I'll change to other more powerful library.
To be pedantic, GTK on linux is a choice, not a native UI. (Personally I hate both GTK and QT). But then I don't know what native is on linux. X's own toolkit maybe. Hopefully Wayland will provide a toolkit that might settle the issue.
There are projects to bundle Python into iOS apps too, although less developed than Kivy's android build chain, we do have packagers for iOS as well. Not many make use of it yet as python mobile development is definitely Android biased, but to highlight a few: [Process Craft](https://itunes.apple.com/us/app/processcraft/id526377075) [Kivy 2048](https://itunes.apple.com/us/app/2048-with-kivy/id841404915) [Quizzn](https://itunes.apple.com/us/app/quizzn/id848759338) All built with [kivy](http://kivy.org/#home) Over the next year we hope to expand most of our build tools to cover more frameworks than just kivy.
The code looks so awesome, i will look at it tommorow once i get home. I am on phone now(and drunk). Kudos Brian
I made a (crap) [game for Android](https://play.google.com/store/apps/details?id=com.fredspipa.lolol) using [pgs4a](http://pygame.renpy.org), and it was surprisingly easy. Anyone who knows basic python should be able to make simple mobile games now, and it's really fun!
Tkinter is not the standard. The standards are PyQt/PySide and wxPython. I like PyQt a lot better (PySide is the free version of of PyQt) &gt; wxPython is the best and most mature cross-platform GUI toolkit, given a number of constraints. The only reason wxPython isn't the standard Python GUI toolkit is that Tkinter was there first. &gt; -- Guido van Rossum http://www.wxpython.org/quotes.php That quote is probably 10 years old, so grain of salt.
Qt (either PyQt or PySide). The best in my opinion. I've mentioned this before in this sub, but I teach a course at http://udemy.com/python-gui-programming . It's a paid course, but if you enter `REDDITFREE6`, it will magically become free! 
You could start by running Linux in a virtual environment (VirtualBox is free and pretty good), there are many guides on how to get started out there. You get to keep your windows, and you get to play around on Linux with little to no risk of fucking things up. Alternatively install it side-by-side with your Windows -- this requires some hard drive repartitioning. That way, when you boot your computer, you can choose if you want to start in Linux or Windows.
How much power does toga provide? Is it easy to do custom text handling and layout?
A lot. The answer is a lot.
I installed the demo with pip exactly like the homepage says—for both 2.7 and 3.4—and then ran toga-demo. Both times, I got this error: Traceback (most recent call last): File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/libs/objc.py", line 841, in has_property return self.properties[name] KeyError: b'initWithContentsOfFile_' During handling of the above exception, another exception occurred: Traceback (most recent call last): File "/Library/Frameworks/Python.framework/Versions/3.4/bin/toga-demo", line 7, in &lt;module&gt; from toga_demo.__main__ import main File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga_demo/__main__.py", line 8, in &lt;module&gt; import toga File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/__init__.py", line 80, in &lt;module&gt; from .platform.cocoa.app import * File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/app.py", line 7, in &lt;module&gt; from .widgets import Icon, TIBERIUS_ICON File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/widgets/__init__.py", line 5, in &lt;module&gt; from .icon import * File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/widgets/icon.py", line 32, in &lt;module&gt; TIBERIUS_ICON = Icon('tiberius.icns', system=True) File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/widgets/icon.py", line 18, in __init__ self._impl = NSImage.alloc().initWithContentsOfFile_(get_NSString(filename)) File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/libs/objc.py", line 936, in __getattr__ if self.objc_class.has_property(name): File "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/toga/platform/cocoa/libs/objc.py", line 843, in has_property selector = get_selector('set' + name[0].upper() + name[1:] + ':') AttributeError: 'int' object has no attribute 'upper'
Do you know if pgs4a still maintained? I've used it in the past, but the current versions don't seem compatible with the Android SDK and fail when running python android.py installsdk. I tried pgs4a versions 0.9.6 and 0.9.5 and both have the same problem. After download the Android SDK automatically (android-sdk_r20-linux.tgz), the script errors out because he can't find the following file: Traceback (most recent call last): File "android.py", line 66, in &lt;module&gt; main() File "android.py", line 40, in main install_sdk.install_sdk(iface) File "buildlib/install_sdk.py", line 225, in install_sdk get_packages(interface) File "buildlib/install_sdk.py", line 160, in get_packages with open("android-sdk/extras/google/play_apk_expansion/downloader_library/project.properties", "r") as f: IOError: [Errno 2] No such file or directory: 'android-sdk/extras/google/play_apk_expansion/downloader_library/project.properties' Looks like the pathing in the SDK changed a while back and pgs4a never updated. Did you have this problem? I would contact their support directly, but the maintainer contact info isn't listed and their support page just points to the forum which is "down due to massive spam". I have no idea how long it's been down or how long the pathing has been broken, but the last release was 2013 and aside from your post I can't even tell if the project is dead or not. 
Excellent point about creating a web site to serve the result, a way better solution, way better done with Python.
I've got a decent amount of experience with writing scripts that pull, manipulate, and report on data using Python. I have zero experience creating online dashboards with Python. Do you have any tutorials or resources you'd recommend?
Thanks Jetien.
This is a great suggestion. I'm likely going to give the pure python route a try as a proof of concept and if it doesn't seem efficient, I will fall back on this. Thanks. 
Qt?
Thanks toyg, great response. Unfortunately our company uses Mac which I believe renders a lot of the premade solutions out there useless. Very frustrating. Tons of good info in your post though, thanks.
Thanks. Talking about the NewStyle vs OldStyle class, on the top of my head, I can only answer that NewStyle classes inherits from *object* but I don't know why. Looks like I am still an intermediate pythonista. But I wonder why this is important since Python 3.x is using NewStyle all the way ?
Thank you for contributing some ideas. 
Thanks for the reply LessonStudio. Regarding using Python to generate an Excel spreadsheet: I've thought about this but it seems pretty difficult to create Excel graphs in Python. Therefore, I might end up going the route of just spitting out raw csvs with Python and then having a Excel template with all the graphs and such. As far as answering your questions: * All data is stored in Redshift, which is a AWS data warehouse based on a heavily modified fork of postgres. * Probably will run on a ec2 instance but I'm open to other options. * Mac * Redshift, so updated by the engineers in our company. * I originally was thinking a PDF due to simplicity. A web app seems like it would be superior but I'm not really sure where to start in that regard. If you had any solid resources, that would be great. 
I strongly disagree but thanks for the reply. I probably have 50-100 tasks a week in my job that are accomplished faster with Excel than any other tool. 
In Windows?
I *think* it is important because the default changed between 2 and 3, so if you were using old style and multiple inheritance in 2, you might run into problems running it in Python 3.
I like PyGObject better.
There is also kiwi [kiwi](https://github.com/nucleic/kiwi), which is a faster reimplementation.
... has Pythonista and Editorial (there's probably more, but that's what I have installed).
Wx. I've tried out GTK, but didn't get it at the time. Wx also taught me OOP better than anything else so far. I also use PyGame, but that's kinda different.
Please don't hesitate to contact me if I can improve something or if you would like to suggest a topic for my next posts!
No problem man. I've been faced with this same question a couple of times, and no matter how I decide to do it, there's always that one guy (or department) who, regardless of the format you prepare, insists that you send it to him embedded in an Excel workbook anyway. So I realized it's faster and cuts through more bullshit to just render it in Excel.
Otherwise known as "too many".