Python is available on Unix systems. If it has bash, very likely that it has Python too.
It's not an exam, just a homework among several others.
I'm a fan of [Nullsoft installer](http://nsis.sourceforge.net/Main_Page) since it's easy to use even though it seems to be inactive now. Others might like [Inno Setup](http://www.jrsoftware.org/isinfo.php). Check them out.
It doesn't matter how you install them, you can still run into version conflicts: your OS doesn't know 1.5.3 of package X requires an old version of package Y. 
That's an improper assumption. If a client told me “Write a Java program to parse a CSV file” and I wrote a Java wrapper around some Perl script that would not constitute “solving” the problem. That would be called “being a dumbass.” I appreciate out of the box thinking, but this isn't it. This is in-the-box laziness. 
So then, wouldn't you suffer less with bash if you took the time to learn it? I somewhat take issue with the idea that Python would be installed on everything system that runs bash. For example, consider a router or other piece of networking equipment that may have busy box, which provides bash and other GNU utils, but not Python. Lastly, consider if you had a different programming assignment that involved writing C++ code to manage memory, but then submitted java code (which handles that automatically). Would you still expect to receive credit?
I'm interested too. Does anyone know if it lets you use platform dependent APIs, or are you stuck with whatever functionality is built into Kivy? I assume the full Python stdlib is supported?
I've tinkered with pip and virtualenv before, but I was discouraged by the additional complexity of the update-and-test cycle. Rather than "sudo python setup.py install", (switch terminal) "foo -i bar", and observe output, instead one does the following: "increment version", "python setup.py sdist upload", (switch terminal) "pip install foo", "foo -i bar", and observe output. This might not seem too bad, but I might be doing this cycle more than a dozen times per hour and it builds up. I don't like bumping the versions so often and spamming PyPI either.
fair enough, but it becomes challenging when you have a heavily user personalized website and cannot cache in a separate process because it is unique to a single user.
i didn’t say i assumed that, i just said that i do that because i haven’t run into any problems yet and it’s the easiest way. i don’t like stuff in my `/usr` that my OS package manager doesn’t know about.
A similar but slightly different approach that I have taken a liking to: http://justcramer.com/2011/01/13/settings-in-django/ It's really slick being able to define all your installed apps/middleware/etc in one place, and then pop them off for development/production as needed. (I tend to use the above settings module, but have a development.py file, a production.py file, and for each server, a local_settings.py file that has sensitive info like AWS keys, etc - that way I can keep most of my dev/prod settings in source control, and only the sensitive settings get .hgignored out)
MongoDB haters exist in response to MongoDB fanboys making unrealistic claims. Mongo's good for some things &amp; bad at others. If 90% of your DB use cases involve using it as a dumb object store (as many web applications tend to do), it's a great tool.
Scalability: Jacob Kaplan-Moss Everything he has ever published - watch it like a hawk. Keep an eye on his github. Seriously just stalk his online presence. He's got really amazing stuff, and is certainly regarded as the core dev with the largest focus on deployment and scaling. As you learn, check out the DjangoCon and PyCon videos. Great stuff out there. The Django docs are wonderful. Check out Instagram's technical blog history - they give a number of really nice rundowns of their infrastructure. Also stalk David Cramer's stuff - not only is Sentry one of the most valuable pieces of software for a django dev, but he's got some amazing insight on his blog, in his presentations, etc.
Because he also uses redis as a cache, whereas rabbitMQ is only used as a messaging queue system.
Yeah, the author really lost me on three points: * nginx over apache. huh? * mongodb on django. what? * a rails app as part of a django build process. come on! And much of the remainder of his advice just kind of sounded rather obvious to me. Though I still can't quite figure out why he's including a local_settings.py from his settings.py instead of just using environment variables.
re multiprocessing: good suggestion! now that I reread that chunk of code, it's probably wrong. Will fix it. It was meant to 0 the timers, rather than validate the values. Validation was just to make sure that only parameters the snippet supported were passed. Eg, desired behavior is that an HOURLY task triggers at 08:00:00, 09:00:00, or if I specify 'minutes': 20, should be 08:20:00. Without some zeroing, it would trigger at the minutes and seconds the task was requested.
tried installing it, doesn't work (the same thing that happens to the windows CD happens when I try to go linux) Edit: But I really plan on switching to linux, what version can you recommend? 
You're avoiding learning something new by re-applying something you already know? You are cheating yourself.
Here's an example of a uwsgi option that should rub your reliability desire the right way: harakiri (the process timeout setting) has a python decorator you can use to indicate longer timeouts for certain methods than your default settings. If you dig deep in the documentation for uWSGI, and learn its more prominent features, you'll never go back to gunicorn. But yes, learning those features is a bit of a nightmare, as there are easily 100. uwsgitop is a really useful tool as well, giving you insight into the running workers nginx "speaks" uwsgi protocol, so you can use uwsgi upstreams that include unix sockets and don't replace HTTP headers like proxy_pass with gunicorn. Here are the uwsgi config options I typically end up using: * harakiri (timeout) * cpu-affinity (makes sure workers distribute themselves across CPU cores in an even manner -- play around with setting to find the sweet spot -- [more info](http://uwsgi-docs.readthedocs.org/en/latest/Options.html#cpu-affinity)) * max-requests (recycle the process after this many requests are handled) * reload-on-as (recycle the process if its virtual memory goes over this amount) * reload-on-rss (recycle the process if its real memory usage goes over this amount) * touch-reload (gracefully reload the server whenever this file is updated) * stats (sets the settings for accessing the stats info using uwsgitop) Then for Sentry, since I co-host with my application on one of the servers, I switched it over to uwsgi from gunicorn, and add the "cheap" option (which spins up a new worker as a request comes in, rather than keeping them in memory) - which is fine and dandy as most of my logging is via UDP anyways. A lot of the fancier features (broodlord mode and zerging, clusters, etc) aren't really necessary for 90% of deployments, but it's nice to have the options. If you're actually hosting a bunch of clients on a single server, then you should definitely take a closer look then - this is exactly what a lot of the extra features were designed for.
Django is still a fine framework even if you only use 20% of its features. At least you know the other 80% is there if you need it and it won't suck when you do (ymmv). Or you can learn just urls and views and get a decent python web server with plenty of room to grow (batteries included: auth, form handling, ORMs, templates and others) without the need to reinvent the wheel, refactor when you aren't smarter than everyone else, juggle plugins or rely on a completely separate software stack for one feature. I guess I haven't been looking, because I haven't really needed many 3rd party django apps. I really only use south for traditional models. He mentions MongoEngine which implements most of Django's ORM interface. You're right that a lot of 3rd party apps don't try to write for MongoEngine (and it's not exactly drop-in compatible). It's a boon because MongoDB could actually use a system for schema declarations, type checking, validation, business logic and being able to apply filter/skip/limit iteratively. Using MongoDB extensively often results in designing a model class anyway. The Q object filtering system works for both standard ORM and MongoEngine. That's been good for me because I've been able to build a "find expression" DSL for building Q objects that lets us author complex queries in a text box without exposing the dangerous guts of pymongo's find() interface (namely the threat of being able to use JS and catastrophic regexes).
what packages do you use to do data analysis
The standard set : numpy, ipython, scipy, matplotlib, statsmodels, scikit. I have Anaconda installed as my system-wide Python distribution and was using Sublime Text as my editor, but I just started using Spyder for writing and testing individual scripts or classes. It's great, and compact. https://code.google.com/p/spyderlib/downloads/list
Yea what's wrong with it? What makes you think it's not normal?
I know these are both popular systems but they are both filled with pitfalls and are particularly bad at doing things like upgrades. imo if you are making installers for windows software, it's worth doing it right and using [wix](http://wixtoolset.org/) to make a .msi
I know that with [cx_Freeze](http://cx-freeze.sourceforge.net/) you can create a [`setup.py`](http://cx_freeze.readthedocs.org/en/latest/distutils.html#distutils-commands) and use it to create a Windows MSI installation, and apparently also works for other platforms (Linux and Mac). You write a valid `setup.py` and run `python setup.py bdist_msi` to create a MSI package -- it's not the best but may be enough for your needs. [Here are the samples from cx_Freeze](https://bitbucket.org/anthony_tuininga/cx_freeze/src/d9461c3d4ff5/samples?at=default). Whatever you decide to use, read the documentation and check the samples/examples first!
You've obviously not been around Unix systems long. Some old IRIX beast may have bash, but good luck with python.
Co-worker of mine built a wxpython application and packaged it up with py2exe and Inno Setup. The silly part is the company later bought another tool to do what his program did, but was more limited. I've used py2exe for small work projects and then I gave the exe bundle to co-workers who needed it. 
Personally I use Arch, but I wouldn't be adverse to a minimal Ubuntu based distro. However, for beginners unless you want to do a bunch of reading, go with either Linux Mint or Ubuntu. 
Just use pip and virtualenv / virtualenvwrapper. Put it all in ~/Env. Then use piptools to check individual projects, and pip freeze to lock the requirements.txt file down. 
Could your scheduler run in a django cluster? (e.g. multiple nodes adds cron, one node executes it.)
I don't disagree it's henius, but it worked quite nicely -and is pretty clean. it also made it easy to 2&amp;&gt;1 /to/logfile when my python wasn't playing nice. 
You may know something about pip that I don't. I will do some more research soon.
It's looks like you need the Oauth module installed on your system. If you have pip or easy_install setup on your system you can simply type either: pip install oauth or easy_install oauth If you don't have pip or easy_install on your system, you can get it by doing the following: * Install Distribute from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#distribute * Then install pip from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip They are both Windows executables. Also, either way you'll want to add C:\Python27\Scripts\ to your path (replace "python27" with your version). Edit: Oauth, not Oauth2.
&gt;Traceback (most recent call last): File "recipe__oauth_login.py", line 7, in &lt;module&gt; from twitter.oauth import write_token_file, read_token_file ImportError: No module named oauth There's the error. That's telling you that line 7 contains an import call that produces an error (because it tries to import an oauth module it cannot find). Line 7 taken directly from the file recipe__oauth_login.py attempts to import some methods from twitter.oauth. Presumably that should be available when you *import twitter*. Clearly, it isn't. In this case, I think your error is only informative for you. For example, I do not know how you set up your environment. Do you globally install packages? Do you have different python versions cohabiting? I do not know. It is also possible that there was an error when you install the twitter module. Oauth is a bit of a hassle to work with. There are great libraries that work to address the burden. I am not sure how python-twitter handles it. I have no issues working with OAuth and Twitter but I do it mostly manually. I use requests and requests-oauthlib. It is pretty straight forward and the Twitter Developer docs tell you most everything you need to know. All of these recipes are built from studying the API more than using the specific twitter module. Additionally, there are a few recipes I skimmed through that incorporate non-trivial modules. Some of these recipes utilize a persistent data store for graphing aspects of groups. Others use memcaching provided by Redis (I think because it is emulating Twitter a bit). Redis is brilliant but I do not think it was necessary for the recipe. Certainly not for someone struggling with ImportError (no offense). You've got your work cut out for you trying to wield these heavily specialized modules. I *strongly encourage* you to step back to a more powerful module like requests and use the recipes for reference. Code your own requests based alternative recipe. You'll learn a lot more this way and probably get more out of it. If Twitter changes their API again (don't be so sure it wouldn't happen) it could break these recipes. Working with Twitter is semi-hostile. Best to understand how to respect the API rate limits. Graphing friends or 'cliques' is seriously not possible with a single API bucket. You'll need to add some logic to watch your rate limit status. API rates reset every 15 minutes. You don't want to attempt to graph Justin Beiber's followers with a single API bucket and no certain understanding how your code handles requests. You'll probably get listed temporarily for abuse. I'm rambling because whisky but tl;dr get an Oauth module explicitly and write your own twitter client with httplib (I recommend *requests*).
Thanks for your awesome response. I will mention here since I didn't at the top that I am running Linux Mint 14. &gt;Do you globally install packages? Do you have different python versions cohabiting? I do not know. It is also possible that there was an error when you install the twitter module. Honestly, I am still learning the lingo -- does globally install mean install from the terminal? I have tried both ways -- install through the terminal and install directly from Github. I am honestly not sure where the different repositories are supposed to go. I may have different python versions cohabiting. I think I have python 2.7 and a version of python 3. The reason I am mainly using python-twitter is that the books I'm following ("Recipes for Mining Twitter" and "Mining the Social Web" -- Matthew Russel for O'Reilly) depend on it pretty heavily. That said, I'm not even sure that these books are ideal for what I'm doing. Supposedly Matthew Russel is keeping his recipes updated and I would assume he'd move away from python-twitter if it was no longer functioning. Anyway, thanks for the help. I do know about *requests* but thought I'd keep trying a bit to use the O'Reilly books. I think my problem could either be caused by having duplicate repositories or because I don't have the repositories in the right places, but I don't know.
/r/sysor 
Just curious if any here have experience with sched module. I stick with Celery but I've used gevent's spawn_later with supervisor for a light scheduler on small projects. I've never really tried sched, though.
R is pretty handy for data as well. Cran libraries are far more numerous than Python.
If that doesn't work, use the coupon code "freedom"
From your shell, run python by typing "python". From there, type: import oauth Do you get an error?
I always found it kind of annoying that `sed` requires escaping of parentheses to use them as match groups. Always messes with me when I switch between Python `re` (or most other languages') and `sed`, because it's completely opposite behavior.
&gt;Honestly, I am still learning the lingo -- does globally install mean install from the terminal? I have tried both ways -- install through the terminal and install directly from Github. I am honestly not sure where the different repositories are supposed to go. This is a challenge for experience programmers, too. When you attempt to install a package you might use *easy_install* or *pip*. I use pip. When you install a package with pip it attempts to download, unpack and setup/install the package to a a site-package folder Python refers to. This folder might have restrictions that would require permission to proceed. Most people *sudo !!* the command and crack on. That's a global install. A global install is installed for all users to access. If you type *sudo* to install a package, it is global. That's not necessarily a problem. Not now. But one day you'll have a pretty kick ass chunk of code that uses a module you feel good about. Then that module will change. You'll maybe update that module to enjoy the fresh newness. Now your old kick ass code is broken because of some obscure API change in the module. You learn the hard way that it is best to use *virtualenv* and compartmentalize your modules to a set environment. This way you never need to *sudo* an install. That is a *local install*. Hit me up if you aren't sure how this works. I'll break it down. Pretty straightforward once you activate the virtualenv. Lots of tutorials out there. &gt;I may have different python versions cohabiting. I think I have python 2.7 and a version of python 3. user:~/# which python That will tell you the default your system is pointing to. user:~/# python -V That will tell you which point version you have (e.g. 2.7.3) You can encounter problems with package managers and versions. If you aren't seeing any other issues than this ImportError, perhaps best to note that it is something you *might* encounter one day. &gt;Supposedly Matthew Russel is keeping his recipes updated and I would assume he'd move away from python-twitter if it was no longer functioning. He states requirements for python-twitter. It is not bundled for you. You'll need SimpleJSON, [OAuth2](https://github.com/simplegeo/python-oauth2), and HTTPlib2. Looks like a simple oversight, maybe? You probably use setuptools as he does. Pip has a flag for a requirements.txt argument to handle requirements for you. pip install -r requirements.txt EDIT: for when the author is nice enough to provide a requirements.txt file, which Matthew Russel happens to be :)
Yes. Here's what I get: &gt;&gt;&gt; import oauth Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "oauth.py", line 58, in &lt;module&gt; from .auth import Auth ValueError: Attempted relative import in non-package
Good timing, just started teaching myself Django. Works fine for me using Facebook login. 
Distros tend to maintain a cohesive set of packages. It's my impression that with Python the process is a lot less centralized, if so that would account for the difference.
I heard some good stuff on wix as well but I haven't had the pleasure to try it. I might some day.
I don't think the prof. will test it on an old IRIX. edit: That's why I wrote "very likely". I know it's not always the case.
Not as creative as piping the heredoc directly into Python; temporary files can suck it.
What's wrong with `cron`? It's everywhere and damn reliable. Edit: There are other python-cron systems, too; see, for instance, https://github.com/ahawker/crython
A common django testing pattern involves creating objects (via fixtures or setUp), saving those to the DB, then using the test client to call a url, which renders a response, then we check for expected output. Involving both the database and the template rendering in a test case will invariably slow it down. But if you write proper unit tests, testing only small parts of code in isolation, then you will have faster tests. A decent explanation of testing views in a faster manner [here](http://chase-seibert.github.io/blog/2012/07/27/faster-django-view-unit-tests-with-mocks.html).
Good advice
Interesting. Thanks for pointing me in the right direction. So you would recommend it then?
What specifically?
I would argue that a better solution is to not try to circumvent the scheme that was introduced for good reason. Why do you need to hold on to an exception after the `except` clause, anyway? Wasn't the point of writing that clause to, you know, *handle the exception*? If you can't, then just let it propagate. If you really need to do something locally and then let more code handle it, then re-raise it. Yeah, maybe you'll need to wrap the `try` block in a new function. IMX, when languages force function decomposition like that in order to make things work structurally, I never regret the refactoring. (Same thing with using `return` to break multiple loops, instead of falling back on a less-restricted control flow mechanism like `goto` or a language-specific labelled `break`.)
Signed up. Looks interesting
thanks. please share thoughts!
If I were the teacher I'd call you to my office and ask, no, seriously, why are you taking this course?
Looks good so far. I was about to try Django (with the "help" of Heroku), so that was awesome timing.
Yeah, I'm not new to Python and common tools, but web-wise all I have done is try out Flask. So I will probably be substituting however you run Django with Foreman, but not much else.
Thank you, I'll give it test too.
I could never get it to install properly, My imports would work one day, fail the next. It might just be my machine, but I ended up going a different route. 
I think that's a good way but you could make it even better for the sake of easier unit/integration testing. Use only essential params in request method so it can be easily replaced with different transport implementation (full url is not needed here every time right?). def get_provider_thingy1(self): return self.request("get", 'getproviderthingy1") Later on you can mock request method or replace returning content you expect from the provider and test only *get_provider_thingy1* logic. 
You might want to check out /r/computervision. There's all sorts of algorithms for identifying faces, objects, and logos. Also, check out OpenCV or SimpleCV for Python. I do a lot of image processing and computer vision work in my spare time, and its insanely fun.
Oh thanks a lot
Thank you. Going to start learning Django from next week and this is amazing. Do you have any recommendations on Books to read for the course. 
had to search myself .. but i hope this is it https://speakerdeck.com/pyconslides/transforming-code-into-beautiful-idiomatic-python-by-raymond-hettinger-1 and even more ;) https://speakerdeck.com/search?q=raymond+hettinger
The question is: can it do migrations?
Use hdf5. Module [h5py](http://www.h5py.org/) handles hdf5 very well.
Why must you save the initial conditions and intermediate results in the same file? It might also be faster to use NumPy's native storage format, at least for the intermediate results (assuming they're only being kept around for checkpointing). The functions scipy.save and scipy.savez may be what you are looking for. scipy.save saves a single array as an .npy file, while scipy.savez saves several arrays as a multitude of .npy files in a zip container.
Not free, "coupon code ***freedom*** is sold out" ***EDIT*** Fast-forward, 13 hours later, got in with redditwins, thanks op 
Seems I'm out of luck on this one
Well, it should - cause it's not free for commercial projects ...
I don't find it slow to start on my laptop or my phone app (the initial run after first install on the phone *is* slow due to unpacking, but subsequent starts are definitely not slow).
if the exception is KeyError, then e = KeyError instance, if it's ValueError it's a ValueError instance, otherwise it's None. The assignment only happens if the exception matches the left-hand side of e (and then no more except blocks are executed)
I read all the information on the course and was excited for it, as I have wanted to learn that stuff for a while now and then it's like; Nope, sold out. Sort of a shame.
no more coupons? :(
Can you confirm you entered lower 'v' rather than upper 'V'? Looks that way. Either way, you are running 2.7.x Feel free to ask if you have a question 
Same. Oh well.
As others mentioned, this code is all used up. Nice looking class though.
He's not trying to print the exception, he's trying to print the `e` variable that was defined *before* the exception was caught and given the label `e`.
Hi, we plan to add migrations in the nearest future. The migrations will be generated automatically when modifying scheme in the visual diagram editor
Disclaimer: my point of view is of course totally biased. I'd say that Python has got me addicted to it. I would often try to achieve a programming goal using a random programming language, struggle for a while (because I don't know it that well, so obviously I'm not trying the simplest way) and at some point I'd just say "fuck it" and do it in Python in a couple of minutes. This is of course cool because it saves time and keeps me happy as programmer, but at the same time I wonder if me being so used to Python stops me from mastering other programming languages. Just because Python is so suited to be used by human beings, despite its technical problems (dreated GIL, incompatibility between v2 and v3 - you name it).
Yeah, it's pretty much assignment, but not really "`KeyError = e`", more like "`e = &lt;the exception&gt;`". e gets assigned to whatever exception was generated (ie. the *instance* of KeyError or ValueError that got raised. It's entirely optional if you don't actually need the exception (as in your example), but often you'll want to do something based on the error, such as log the message text or look at the error code. The older syntax for this was seperating the exception type and the variable with a comma: try: ... except KeyError, e: # (Again the ", e" is optional if you don't need the exception object.) But this was a bit confusing and ambigious, because you could also specify multiple exceptions as tuples (ie. catch KeyErrors **or** ValueErrors) with: try: ... except (KeyError, ValueError): print("I got either a KeyError or a ValueError") Which was inconsistent in that if you leave off those brackets, you get completely different behaviour, unlike using tuples everywhere else. "as" was introduced to remove this ambiguity and make this more consistent. It's also used for similar purposes elsewhere. Eg in import statements: import module as different_name from os import open as os_open or context manager blocks: with open(filename) as f: # do stuff with f 
As I'm sure others would, I'd love a new coupon code. Looks like a good class
I am fairly certain that "locally developed module" is an accurate description... "remote" doesn't really describe how I develop or execute it. I am working on a software package that is still under heavy development, so I frequently make changes to the source, re-install, and run tests in a separate directory with input.
Sold out :( Maybe OP can deliver with a new Coupon Code.
I would love to take this class, hoping OP gives out another coupon code for my broke ass.
Depending on the system you're using and your comfort with file FIFOs you could do something like we do for a project at my work. We have large data sets with multiple fields that we analyze in realtime. The data is received and analyzed by python code and then further analyzed by matlab. Each data field (the arrays) is stored in a separate binary file which can initially be done using "numpy.memmap". This memory maps the file for python which allows python to flush changes to disk and then matlab can view them by reading the binary files starting from a specific file offset. You could do this with regular binary writes, but we just happened to use memory mapping. We identify each iteration by a record number. When python is done with a record it writes the record number to a fifo (http://linux.about.com/library/cmd/blcmdl4_fifo.htm) and matlab can read this as a normal file. The nice thing about this is that you can start up matlab and have it block on reading this fifo, which works out very well for realtime processing. To save space on disk (if you don't need all the intermediate data afterward) we consider the binary files circular-buffers. So if we define the files to be 40 records in size, and python just finished working with record 81 then the output is written to the binary files as record 1's location (81 % 40 = 1). This is pretty involved and if you aren't doing realtime or something that needs results right away then I probably wouldn't do this. If you are ok with running the python, then running matlab you could just do as others have said and have the intermediate format be hdf5 or a binary file (numpy.ndarray.tofile).
From my understanding badly_named_variable = 1 try: 1/0 except ZeroDivisionError as badly_named_variable: pass # This line throws an UnboundLocalError because badly_named_variable was deleted. x = badly_named_variable See http://bugs.python.org/issue17792
I would say it's actually the reverse problem now. Finding competent python programmers now is actually rather difficult. We're forced to hire PHP/Java programmers and teach them.
damn cool syntax + evil implementation
That syntax is really awesome. Something like that on top of SQLAlchemy would be awesome ** 2.
Use sed's `-r` flag. This gives you extended regexes, Python's are PCRE, but the grouping syntax is the same.
Ubuntu will have it (pulled in by ubuntu-minimal).
sorry about that. please use 'learnmore'
sorry about that... please use 'learnmore'
sorry about that... please use 'learnmore'
sorry about that... please use 'learnmore'
sorry about that... please use 'learnmore'
sorry about that... please use 'learnmore'
this Foreman? https://github.com/ddollar/foreman
Books are not needed as of now... I say just dive in and ask questions when you get lost. To be honest, it's a lot of pause, rewind, replay and you'll get it. If you get totally lost.. ask a question on Udemy and I'll point you in the right direction!
sorry about that ... try 'learnmore'
Thank-you. You just made my night :)
Nice and refreshing approach, but I notice a lack of JOIN, LEFT JOIN examples in the documentation. It's challenging to capture all of SQL syntax using Python. How do you do the following typical scenario? Two models Post and Tag. Each post can have zero or many tags. How do you define these models in ponyORM? How do you select all posts and its corresponding tags? 
sorry about that ... try 'learnmore'
sorry about that... please use 'learnmore'
sorry about that ... try 'learnmore'
Thank you so much!
Thank you so much for this. I've been looking for a good project to help me get my teeth into Django.
It looks great, sad not be not fully open-source. I can understand some reasons, I know it can be complicate and expensive to maintain a library/framework, but ... anyway, I'll try it this weekend.
Ah thank you! Didn't know that.
Thanks for adding more, I enrolled lets get building! 
Excited to get some guided learning on Django. I have played around with it a bit, but once I get beyond the basic examples with the DB and helping users log in it all falls apart
Think of it kind of like PyQt, is awesome and free for GPL but charge for commercial projects.
Yesh
It's great to see more examples of loose coupling in languages like python, although [DHH would tend to disagree](http://david.heinemeierhansson.com/2012/dependency-injection-is-not-a-virtue.html). Also, this post nicely answers a question [I asked on Stackoverflow](http://stackoverflow.com/q/13877777/173957) about DI + python back when I was first learning the language. 
The `compile` part certainly looks [at least a glance] interesting, though the rest of it sounds a little janky given `django-compressor`, `django-webassets` *et al* exist and fill the "do one thing well" criteria better. Remarkably, I really had never heard of it. If you assume GitHub stars are in any way a reflection of an app's popularity, it definitely needs to be on more people's radar, at least.
1) Simple model definition can looks like this: &gt;&gt;&gt; from pony.orm import * &gt;&gt;&gt; db = Database('sqlite', ':memory:') &gt;&gt;&gt; class Post(db.Entity): ... name = Required(unicode) ... tags = Set("Tag") # one side of many-to-many relation ... &gt;&gt;&gt; class Tag(db.Entity): ... name = Required(unicode) ... posts = Set("Post") # other side of many-to-many relation ... &gt;&gt;&gt; db.generate_mapping(create_tables=True) &gt;&gt;&gt; t1 = Tag(name="t1") &gt;&gt;&gt; t2 = Tag(name="t2") &gt;&gt;&gt; p1 = Post(name="First", tags=[t1, t2]) &gt;&gt;&gt; p2 = Post(name="Second") &gt;&gt;&gt; commit() 2) Pony has left_join() function which works as select() function, but uses LEFT JOIN instead of INNER JOIN &gt;&gt;&gt; left_join((p, t) for p in Post for t in p.tags).show() This will generate the query: SELECT DISTINCT "p"."id", "t-1"."tag" FROM "Post" "p" LEFT JOIN "Post_Tag" "t-1" ON "p"."id" = "t-1"."post" But with Pony, you don't do such queries very often, because what you typically want is to do two nested "for" loops, one for posts and one for tags. You can write this with Pony as this: &gt;&gt;&gt; for p in Post.select(): ... print p.name ... for t in p.tags: ... print t.name In most ORMs, this lead to N+1 problem. But Pony can undestand this situation and automatically minimize the number of queries. Pony does this the next way: First query selects all posts, which satisfy the criteria (in this example, just all posts). Many ORMs try to fetch database result lazily, but Pony retrieves all posts beforehand. This makes sense because typical use of Pony is for web application, where number of rows selected by a query should be relatively small. Moreover, AFAIK PostgreSQL driver just emulates the lazy row fetching, when in reality it retrieves all rows at once and store them in an internal buffer. When a row with object data loaded from database, Pony constructs not only an object for this row, but also all objects which are mentioned in the foreign key fields, and stores all those objects inside the identity map. At this moment, Pony knows only ids for such objects, and other attributes are unknown. Such object called "seed" in Pony, it is the object which exists in database but not actually loaded yet. When you try to access other attributes of such a seed, lazy loading will be triggered. Then the interesting part: Pony loads seed object not one-by-one, but in batches. That is, instead of the following query: SELECT id, name FROM Tags WHERE id = ? Pony executes this query: SELECT id, name FROM Tags WHERE id in (?, ?, ?, ...) to load many seeds at once and hence to minimise number of roundtrips to the database. It may be even more scalable then using a query with left join, because in this case Pony relieves the database from the necessity of doing joins and in fact transfers the join operation to the Python side, which can be scaled more easily. In practice it works really well. With this kind of optimization, a programmer can have performant application without manual preloading at all. If you want to optimize even further, you can preload objects to transaction cache with the simple query: all_posts = Post.select() all_tags = Tag.select() # this is for preloading only for p in all_posts: print p.name for t in p.tags: print t.name In this case, only query to the intermediate table of many-to-many relation will be needed during for cycle (actually, two query, because Pony must be sure that more then one Post.tags collection is required).
looks useful but other than progress bar and nicely formatted output , how is this better or worse then pdsh ?
Thanks Brian. 
sorry about that... please use 'learnmore'
definitely take the class and let me know when you get stuck on each lesson. use code 'learnmore' if you haven't already signed up.
THANK YOU! You get an internet hug from me. name = 'jmitchel3' if get_Code: hug(name)
wow. Reddit. you are really loving this class. When 'learnmore' ends please use 'redditwins' Thank you!!!
Didn't everybody switch to PySide because of this? Phillip.
One question: why license per proccess, instead of a more simple licensing such personal and business only? And, suppose I have a django app with 20 balanced processes behind gunicorn, do I really need the commercial license?
Thanks! Just started it. FYI, on Lesson 2 lecture 3, it might help users to know the the "django-admin.py" is located at \build\scripts-2.7&gt;django-admin.py Looks like a great project! 
thanks a lot
Thank you.
Yes, I entered the lower case v originally. I corrected this: hal9000000@hal9000000 ~ $ python -V Python 2.7.3 So yes, I have Python 2.7.3. 
Should mention that I'd love feedback on this project. My ultimate goal would be to make something that could be installed using a single command and that didn't require Django installed somewhere in the path already. Unfortunately, I haven't yet come up with a good way of doing the templating in the sls files without taking advantage of Django.
This is awesome! thanks OP
Any way to download the course for offline viewing?
Thanks!
It just doesn't get a lot of spotlight. And for many Python programmers/web developers, including myself, the standard is typically "use Django for big projects where you want a lot of scaffolding, use Flask/Bottle for small/medium-sized projects where you want to roll more of your own stuff," and Pyramid doesn't really get much consideration.
Nice project to get in with python. Just don't miss [ansible](http://ansible.cc/docs/examples.html) when doing the market research.
Command args and visual basic. I needed to ship quickly, and forms was the easiest route. 
learnmore is sold out, am I out of luck? Are you affiliated with udemy? Very generous of them/you to do this for us. Edit: don't forget to sort by **new** for additional coupon codes.
Happens all the time. 
learnmore is sold out :(
learnmore sold out :(
I just got it using 'redditwins'
Kind of sucks how I seem to keep being too late with redeeming the code :/
seems this is used up now as well.
Thank you so much for this!
Worked for me too, thanks.
How does it compare with [Fabric](http://docs.fabfile.org/en/1.6/)?
worked for me too, thanks
Regarding your flair, "research scientist": if you like python, you should definitely check out [ipython](http://ipython.org/).
At this point I think further discussing the point DOES have merit, because there seems to be a lot of confusion about how the loader works. It does NOT cache template output. It caches the compiled template in memory. You could have a template where the output is random each load, and you still get all the benefit. I feel like the word "cached" is throwing people off. It's basically like if python, instead of creating .pyc files, stored the compiled .pyc data in process memory. You still get the speed benefits of the compiling, but that compilation is cached. So when I say jinja and mako are doing the same technique as Django, I mean it really is the same technique. It's just Django stores the compiled template in memory. [Link to Django doc explaining loader]( https://docs.djangoproject.com/en/dev/ref/templates/api/#django.template.loaders.cached.Loader) Again: as best I can make out, **the cached loader has nothing to do with template output or dynamic content, and everything to do with caching compiled template objects**
All these worn-out coupon codes make me wonder if an eCommerce site is a good pick for your first Django project.
Another similar project. http://code.google.com/p/parallel-ssh/
Are you sure? IIRC the apps you mention are for external JS/CSS only. This app has options for **inline** JS/CSS. Which is also why it has such strict requirements for where you put your HTML tags. (Which is why I don't use it)
You don't have to install anything with Fabric either, other than a ssh server. sudo also works. I'd recommend using Fabric extensively before putting too much time into developing this.. Fabric works great and is widely used. You'll need to figure out some weaknesses and instabilities of it before really finding a niche for another tool like it.
Redditwins is still working as of this post. 
It's obligatory. That's the only reason I took it. If I could avoid this shell scripting course, I'd gladly do that, believe me.
Use `redditwins`.
Hi, I wrote peewee. I saw pony orm in my python weekly newsletter the other day and am really impressed with what you've made. I can't speak to the goals of pony orm, but peewee aims for simplicity of implementation and a consistent, expressive querying API. Also, that's a typo in my docs, it should be: staff_users = User.select().where(User.is_staff == True)
No, I checked their source and it definitely looks like they're calling the sudo executable. Their pydocs mention it at operations.py:1021, but you're going to have to jump around a bit to find where they wrap the call. The link you posted to their docs says they actually have problems doing su -c, not that they only do that. You might want to look into them more just to see what they do well, just saying. I'm not trying to dissuade you from working on this, but I'm trying to get you to learn more about what they did so you can try to do it better.
+bitcointip $5 verify
Looks similar to SSHPT (which I wrote ages ago): https://code.google.com/p/sshpt/ I haven't maintained it though. If I were to do it over again I'd probably make something like Fabric.
Thanks. I've heard of it, but haven't taken the time to read up on it.
If you use non-commercial or AGPL licenese you can launch as many processes as you need. Our commercial license and the number of processes limitation intended for closed-source projects. The "number of processes" in this case is the number of processes which have PonyORM code inside. We are not trying to squeeze each penny form a typical freelance developer. We limit the number of processes in order to avoid a situation when a big company buys a license for 1-2 developers and then uses Pony heavily. In this case if Pony helps to save money on developers/development time for a big company we'd like to get more money than just for a developer license. In the same time we are still considering the right license poilcy. Do you have an idea on how many processes are required in order to comfortably develop and run a single middle-sized commercial project?
Thank you, good job with peewee! We build Pony because we believe that it brings simplicity, agile approach to development and allows a developer to concentrate on a business logic of application.
Do you do that on Windows? Because if you do, you should should stop worrying and just ignore virtualenv for the moment, since somewhat ironically using Python on Windows is the most newbie-friendly way of using Python. You just install Python, then install everything you need (most serious packages provide Windows installers), and by "install" I mean download the installer, double-click on it, click "next" three or so times. If you fucked something up, reinstall Python into a new directory (or delete the old one and install anew), install everything you need, do what you want to do, that is, continue learning the Python language and writing useful stuff with it. All that virtualenv stuff is necessary if your OS has its own version of Python installed, necessary for its functioning, and other versions of Python and/or packages you want to install also get installed globally. On Windows that's just not a concern, the OS doesn't use Python and you can have as many (necessarily local) Python installations you want.
You are awesome thank you so much :)
PySide seems kinda dead recently. Correct me if I'm wrong.
This currently works.
Maybe try updating it? I think the command is `apt-get update oauth` And remember to put all your code and errors in the code format
Looks good! There are several other similar tools out there, but Im always willing to try out a new one. Nice work!
+1
Thanks for the info. Honestly I've went the easy was to start and installed Python(x,y), which packages a lot of modules in one, including Spyder, which is the IDE I've been using. I haven't run into any issues so far. Then recently I've heard about Kivy for making mobile apps, and I wanted to know if I could use that with Spyder, but had no clue on how to go about it. So I'll probably just install Kivy separately and see if other stuff gets messed up. 
Sounds like you've re-invented Fabric. It does all of those things.
This looks fantastic. Nice work! I'd love to see PostGIS and GeoDjango included by default (or by some settings toggle) as well. That's a pretty common use case. 
`py3kwarn` is a small wrapper around `lib2to3` that reports Python 3 incompatibilities. While existing projects like [`lib2to3`](http://docs.python.org/2.6/library/2to3.html) and [`python-modernize`](https://github.com/myint/python-modernize) are good for refactoring existing code bases, I've found it hard to break my Python 2 habits when writing new code. That's what gave me the idea for this project. I'd really like for this to become a tool that helps developers write forward-compatible code. It works in vim (via [syntastic](https://github.com/scrooloose/syntastic)) and through the command-line so far. SublimeText and Textmate support would be great, but I'm unfamiliar with the way those editors handle extensions. This project is in it's infancy right now, and there are a lot of ways it could be improved, so **pull requests are very welcome!** I will be adding anyone who submits a valid pull request as a collaborator. [@dustinmm80](https://github.com/dustinmm80) and [@myint](https://github.com/myint) deserve a big shout out for their significant contributions!
'Seems like many approaches have been taken to this problem: http://code.google.com/p/parallel-ssh/ http://www.tundraware.com/Software/tsshbatch http://sourceforge.net/projects/clusterssh/ http://www.theether.org/pssh/
&gt; I lost count of the times when a method in C++ has something called in its body, and I don't know if it's calling a static function, a class method, a parent class method, a global function.... I don't follow. Granted, I have no more than a few hundred hours of C/C++ time, but in C++ you usually see the `this` keyword where you would see `self`in Python. The difference is that you don't explicitly define `this` as a method parameter.
Awesome! Keep up the good work!
Thanks! I just updated with a few bug fixes that seemed to have cropped up between Vagrant 1.0 and 1.2. Everything should be good to go now though. I was tempted to look into things like PostGIS and GeoDjango but I wanted to keep the defaults to a minimum. If someone has an idea on how I can make these optionally installed in some way, preferably in a straightforward way. I haven't touched PostGIS and GeoDjango in ages though, if it's something that could be added just by being added to the requirements file, you could always add lines to requirements/vagrant.txt to have it installed by default.
I withdraw my argument. You are right. I think that Django's terminology here is a bit confusing. Thanks for explaining this! :)
redditwins, still working! Thank you Justin! This was my third attempt at joining the class, time to upvote all jmitchel3 comments in this thread.
Anytime! I'm glad we ended up sorting that out!! (And yes, considering how often I hear about other templates being faster, I really wish the docs called out this setting better - maybe even generating in settings on project creation but commenting out)
Supports Fusion. Think workstation is next. I thought I saw mention of an aws-provisioner
haha funny. Honestly, the codes are a measurement of person success and analytics. I realize it causes some inconvenience but, at the end of the day, I've been doing my best to ensure the community gets them!
Happened to me twice, but third time's a go with ***redditwins***, thanks Justin!
250 people away from 1k sign ups within 24/hours. So excited. Do you think I can hit 1k sign ups by 8:00pm PST? If you haven't joined, please do and use 'redditwins' for free access. By all means, please share with your friends!
There is a [vagrant-kvm](https://github.com/adrahon/vagrant-kvm) provider, but it requires QEMU 1.2+ which is relatively new.
AWS and Rackspace plugins are available for Vagrant 1.1+ https://github.com/mitchellh/vagrant-aws https://github.com/mitchellh/vagrant-rackspace Disclaimer: I haven't played with either of them yet (but I should). 
Awesome. Does it have graphs of tasks and services? * https://fabric.readthedocs.org/en/latest/usage/parallel.html * http://libcloud.apache.org/docs/using-libcloud-in-multi-threaded-and-async-environments.html * https://fedorahosted.org/func/ * http://docs.saltstack.com/ref/cli/index.html * https://gist.github.com/rubiojr/3914299#file-puppet-node-run-rb (pssh) * http://docs.opscode.com/knife_ssh.html * https://juju.ubuntu.com/docs/service-config.html * http://pythonhosted.org/Bladerunner/
A shame really. I dislike Django for imitating Rails and wish Flask had better extensibility. Pyramid fills both niches for me, since I like resource trees and Graph dbs
Thank you so much.
Is the code actually hosted somewhere, like Github? Clearly you can download from PyPI, but it would be nice to see the author encouraging contributions from others (especially with a scientific package).
60 people in 50 minutes? Can we make this happen???? I think so reddit! Click link, use 'redditwins'
thanks so much 
The entire Python stdlib is supported. Kivy is a framework on top of Python, so all normal Python features exist, just with extra Kivy functionality added. All other external library imports are also supported (e.g. numpy, scipy).
Can ctypes be used to call native APIs (e.g. in iOS)? 
I think what he's trying to say is it's always good to do your homework before running off and making a library. That being said, have at it!
* http://www.w3.org/TR/ldp/ * https://restful-api-design.readthedocs.org/en/latest/
http://cornice.readthedocs.org/en/latest/sphinx.html
* https://tastytools.readthedocs.org/en/latest/tutorial.html#generating-documentation * https://github.com/juanique/django-tastydocs
Ansible is a wonderful product. I use it all the time with my web cluster. 
* http://jakevdp.github.io/blog/2013/03/23/matplotlib-and-the-future-of-visualization-in-python/ * https://github.com/danvk/dygraphs
But without a good knowledge of what the incumbent is, you can't say that your product is better.
I normally use a simple bash script for this but will definitely check it out.b thanks for contributing!
Joke, troll, or general insanity. Taking all bets!
Im going for troll or faking stupid
thank you so much for your books. Your first book is the book I started to learn to program with a few months ago. The fact you released a book about cryptography is amazing since it's a subject I'm very interested in. I already took a lot at most of this book and expanded some of your code - like making a transition cipher program for any key length that also emails your coded message to a recipent with smtplib :)
Alternatively-"Bloocoin, you don't need to know if the crypto is any good, we'll look after that, no worries mate! "
The diagram will be great for a novice such as myself. I really like the dal from web2py. Any important scenarios where pony orm would be a distinct advantage? Pleasure of coding is if course no small advantage :)
I guess the interesting part is the code of the central server. Mining on a centralized server is meaningless, though.
Why are you using old-style classes?
https://github.com/kivy/pyobjus
Disadvantages: * Deployment (install, uninstall, upgrade) in noncontrolled environment is problematic. * Official documentation expects a mindset, average programmer does not have. No offcial guideline to document a project. * Maintaining outside library dependencies can be a peace of hell. * Cooperative programming is hard with bigger (over 4 persons) staff. * GIL and other parallel computing issues. These all can be a non-issue for a particular project, or can be handled with lots of work.
yes, looks easier http://pythonhosted.org/Bladerunner/#use-of-bladerunner-from-within-python
I think it could've been awesome if there was an editor (ahem PyCharm?) to create GUI stuff otherwise it's quite painful.
upvoted + bookmarked, i can actually see this being a handy reference
Nice piece of work. A helper that I'll keep around.
You are very welcome.
jmitchel3 try to use screen for running Django server ;)
Very welcome. Good luck with the class, let me know if you have questions.
Interesting, but unfortunately I can't really follow a single thing that's going on.
Just a guess, but if syntax highlighting doesn't matter too much (or if you're willing to do a little extra work to use syntax highlighting (pretty much just using something like SyntaxHighlighter which I think is a few lines of JavaScript at the beginning of your file)) you could possibly just design your own page? HTML and CSS are simple enough for first-timers, it's probably doable. You would retain the same URL for each edit.
Your avatar on GitHub is from XKCD apparently. The work there has a license: http://creativecommons.org/licenses/by-nc/2.5/. Please respect it.
You sir are incredibly awesome :D
This might well be something I use, I've been looking for an Excel library for Py3 for a while now and had been making do with CSV files. Thank you :)
In fact, the reason why we used a template-based approach was because all the other available libraries rely on XML parser-based approaches, which are inevitably slooooooow. While we admittedly lose the benefit of strict XML schema validation we are still able to quickly create a file that can be opened by Excel, even if strict compliance to the Open XML spec is not explicitly guaranteed.
Cool Can it do formatting stuff? Or formulas?
Ah. Thanks. So how do you suppose I "respect" it? Should I just put a little thing on my profile that says "my avatar is from the webcomic xkcd?" Or should I just change it completely. I love xkcd, so I would hate to do something that gave the author a hard time.
I guess a small watermark saying "xkcd.com" will do.
I'll get on it right away! edit: changed. Thanks for the tip, man.
There is support for formulas - you can just set the value of a cell to be a valid Excel expression and things should just work. Formatting is not supported ATM, but it's definitely on the list of things to do.
Gists is the only one that I'm aware of. Perhaps if you signed up for a free Pastebin account you can edit your pastes, though I haven't verified this.
Why do this at the shell level instead of the fuse level (and then you get cd, ls, cat, pwd, find, grep, etc. for free?) I really like this idea btw, I will try it out and perhaps have more feedback later
Restructuring code for dependency injection has always made it more brittle than my tests using patching ever were. And now the *code* is more brittle, more highly coupled and more difficult to refactor. All that just to avoid a small amount of brittleness in tests? And what about things you want to patch that aren't attributes of the class instance but something else like relative imports or other non-class based approaches? The only answer from DI advocates is: put it in your class! Great, now I have all these optional parameters everywhere and dependencies getting threaded through all sorts of levels AND I have these insane Java patterns like FactoryFactoryFactories just to encapsulate simple non-class patterns into classes. In a language like Python this seems crazy to me. It makes sense in other languages where the tradeoffs are different but this seems like a religious argument to me and not a pragmatic choice made about the best choice for the language you are working in. You may prefer DI and that's fine, it is a valid approach and excellent when it makes sense (both tests and live code will use it), but presenting it as the "one true way" to do things, especially in the context of Python, is completely wrong.
late to the party, reditwins also out. Any more?? Need this. EDIT: nm im in. thanks alot for this.
Umm, download from where? The place indicated in the comments doesn't have any installers or other downloads in it.
Either way it doesn't meet my qualifications, because I would need the URL to remain static.
It does not do calculation on its own - Excel evaluates them when the file is opened.
I like what you're describing here. It's sort of a cascading "reverse lookup" for views. Have you proposed this on the Pylons mailing list? Will you submit this to the Pyramid github page? (https://github.com/Pylons/pyramid)
It does stay static. Original: http://pastebin.com/ahi8FEf0 edited http://pastebin.com/ahi8FEf0 Try it yourself.
Ah, thank you, you're absolutely right. I had never tried signing up as a member until now.
Congrats on writing/publishing something. I see that you can define hosts, can you define roles? enviroments? Look forward to seeing the docs mature
Legally, I'm not sure the license is relevant: this would almost certainly fall under fair use, just like nearly every other avatar online. It's an insubstantial copy, it doesn't harm the rightsholder's ability to profit, and it's so easily recognized that OP obviously isn't trying to pass it off as their own work. By my (non-lawyer) reading of the law, no infringement here. I'd also argue that there's no respect issue. It's not like we're rehosting a full strip and detracting from their traffic; it's just an avatar and clearly intended as an homage. If I were the artist, I'd feel honored, not disrespected. Then again, I'm not an artist, so I'm not entirely familiar with the code. 
If you're looking to implement it in Python, why not just use something like this? import urllib; exec(urllib.urlopen('http://pastebin.com/raw.php?i=gistid').read())
Just because everybody on the internet violates copyright doesn't mean that's all covered by fair use. It's better to be safe than to be grey.
Funny how you made this thread because I was just going to ask the same question about my own little mediafire wrapper https://github.com/superstepa/PyMediafire
Nice idea.
I didn't get you there. How can the fuse level give you anything for free ? I had to code all those commands to simulate a shell-like command line interpreter. In reality, it's a code browser so those commands don't actually work on files but on python classes, functions and methods as if they were files.
That's fair. But I wonder... you know how there's news articles abounding about random excel fuckups? And memory overflow in gigantic excel spreadsheets? It seems like having an alternative for calculating excel sheets would be valuable. It seems like it might just be a matter of translating excel functions to python functions and going from there. I dunno, just an idea.
Here's a one-line tree ([defaultdict docs](http://docs.python.org/dev/library/collections.html#collections.defaultdict)): tree = lambda: defaultdict(tree) [Discussion here](https://gist.github.com/hrldcpr/2012250).
add ajax.
https://launchpad.net/mergou
I'd surely not pay for a whole class when only a function is concerned: import os def exit_with_result(function, exit = os._exit): result = function() if result: exit(0) else: exit(1) This is only a difference in grade though not one of principle. DI is still used. 
the music sounds loudly
Look ma! no semicolons! exec(__import__('urllib').urlopen('http://pastebin.com/raw.php?i=gistid').read())
The state of Python itself is terrible, you have a lot of new people who are new to both programming AND Python. Coupled with the fact that the community puts heavy focus on libraries and frameworks (looking at you requests). What you end up with is a programming language where people try to do as little programming as possible. Which is great if you've *actually done some programming before*, but not when you're learning. In the end you have programmers which are doing little more than assembling frameworks to get what they want, instead of understanding things from a broader perspective. It's sad really, Python's powerful abstractions and lovely ability to make beautiful library and framework code will, I believe, ultimately water down the landscape so much that it will be it's undoing.
Looks very interesting. Ive been developing some similar stuff with PyQt/SqlAlchemy for animated films production. But how about providing some basic installation - at least some links to where to start reading. Im sure Im not the only potential user with no idea about Django and not enough time to start learning it just to test this thing out
I feel like we could do a better job of funneling the basic questions and bad scripts over to /r/learnpython. 
Perhaps making the r/learnpython subreddit more prominent would catch more of these questions?
The news articles are about how people use Excel incorrectly, not "random excel fuckups." It might be due to UI issues, but it's not random.
Where do you draw the line? Do I need to know about sockets in order to play with some simple REST API? Does it matter if I know what REST is and if the API complies with it? If you want a garden with less newbies making noise it might be better to fork /r/python and from there put some rules up to keep newbies at bay. Take for example: /r/VideoEditing and /r/Editors
What do you think of mplementing man, apropos and whatis ? (something that should work like pydoc -k more or less).
It's a hard line to draw, that's for sure. Python blurs it so it's practically non-existent, however.
I said a small one :D Make it 4 times smaller, half transparent and put it in the corner. I was just saying all this because I want to spread XKCD. I didn't mean to sound accusatory (is that right? sorry, I'm not a native speaker).
What is so bad with new people joining python and programming in general? Wouldn't this be hailed as a good thing and something programmers have been trying to do for a while now, break the stereotype of the programmer and making it a more knowledge general, with that comes little "actual" programming. What I'm trying to say is growth is good, it brings with it lower skills and we need confident, great programmers, now more than ever to teach them the right way. Getting all hierarchy on the lower skilled doesn't help. If you are really annoyed by "nooby" questions just add a filter with [RES](http://redditenhancementsuite.com) ^Sorry ^For ^any ^bad ^England ^or ^misinterpretation ^it ^2am 
"...where people try to do as little programming as possible." So, like Java. 
The language has became mainstream, a lot of people in this world are interested to learn Python and most people simply didn't know that /r/learnpython exists.
The more I look into PonyORM, the more I like it. I'm not knowledgeable to talk about its limitations, so I hope others will look into it. But I like the fact that you guys started from a user interface that is sensible, and try all possible tricks to realize that.
I look into this subreddit every day since several month, maybe a year. I don't think I've ever seen a post like this.
Both of these "problems" can be explained by new members participating. This is healthy. If a script is too "basic" provide some tips on making them better.
Real programmers pay it forward. Real masters learn from teaching. 
As someone that just subbed to both /r/python and /r/learnpython, you're kinda making me feel unwelcome :(
Let me ask instead. What do you want to see *more* of in this subreddit? Personally, I'd love to see more links to excellent talks like [Transforming Code into Beautiful, Idiomatic Python](http://www.youtube.com/watch?feature=player_embedded&amp;v=OSGv2VnC0go) and code reviews of intermediate+ programs.
While true, every post has its place, and /r/learnpython is nearly 10,000 members strong with a large contingent of teachers ready to help.
and thats a good thing. something that is very hard to achieve from a language design point of view. writing code is horrible, and the less you write the less bugs you've got. oh shit i forgot to null terminate this post%*6%::357;.:$°€√÷Π€€|•
And what happens when these 'programmers' need to do something which Python doesn't handle very well?
Merging of scopes does a disservice to both subreddits. No need to feel unwelcome, but it does raise some eyebrows when one sees less programming news and projects/libraries and more "what is self?"-style questions.
but but but... we'll pay you in meaningless internet points! 
Syntax higlighting for your cat equivalent?
Here's a simple heuristic: every bad/stupid/newbie post ends in a question mark. Why not write a script to funnel those out? I'm not sure how this would work, maybe userscripts? EDIT: Or maybe you could PM the mod and have them modify their spam filters.
They'll be forced to do research and expand their skill, the same way we all learned when we were noobs, and will hopefully continue to do forever.
Which is entirely my point, this doesn't seem to happen.
Rather than funnel them out, why not just politely suggest to people; "Looking for coding help? why not try /r/learnpython?" Then again the agrument is stupid in general. Why bother to have a subreddit where people cant ask questions.
/r/advancedpython ?
I think with java it's actually reversed. They try to do as _much_ programming as possible, even when it's totally not required.
I think you're looking for [PyPI](https://pypi.python.org/pypi).
For community maintenance, it's more important to be nice than to be correct. Everyone thinks other people's code is bad, so that tends to be very subjective. But I agree that a better job could be done funneling basic questions to learnpython. 
Hmmm... something like that would probably be possible by interfacing with git (either as a subprocess or with pygit2) to check the diff since the last commit. I wonder how many people would find this useful though.
[youtube-dl](http://rg3.github.io/youtube-dl/) could be interesting for comparisson
Eternal September?
&gt; ### FAQ &gt; &gt; * … &gt; * Why not use JSON? Answer: because JSON is quite limited in the number of datatypes it supports, and you can't use comments in a JSON file. &gt; * … that being said, YAML solves all the problems mentioned on the page, can do even more, is supported in more languages and is even more readable.
I've seen that before. I made one in Perl a few months ago. [https://gist.github.com/013/2874931 ](https://gist.github.com/013/2874931) But then Youtube broke it with some updates, so it no longer works.
You can't run python natively on a chromebook, otherwise I would be all over vim.
Thank you. I don't know how I have missed this, but I will definately need it.
https://github.com/theiviaxx/Frog/wiki/Server I would agree though about getting a server up and running with minimal effort, its still way complicated just to see django running. I think it's a problem with django in general. There's no LAMP type thing. There needs to be something like the plone installer.
which should still be None? the code strongly suggests it should have been changed by one of the except clauses, not that he wanted to print None. i came across that behavioral change before and my wording in [this pull request](https://github.com/halst/schema/pull/6) (where i fixed it) tells you much about how clear it is to rebind a name with “`as`”. &gt; ## scopes are less leaky in python3… &gt; …so i made the assignment of x explicit. &gt; to be honest: i didn’t understand that part at all, so exploiting the leak was probably never a good idea in the first place. i always understood “`as`” as scoped assignment, because that’s what it is! sure, the error message could be more clear, but i immediately understood the code i didn’t understand before when i found out what the error was (which tells you how good it is that it’s an error)
Ok, im doing the criticism part then! For json - have you looked at using http://tastypieapi.org instead of the manual json serializing? I just took a quick look at the code and the first thing i see is division / overloaded operator on strings, please consider the first rule about operator overloading &gt; **Only overload operators if it's the natural, expected thing to do and doesn't have any side effects.** to have string / string mean concatenate is not the natural, expected thing at all, and it does have the side effect of changing the first parameter ;) Also, take a look at http://www.python.org/dev/peps/pep-0008/ which is the style guide to write python code, and if you don't want to follow it at least use a concise style across the board, don't name half the files file_name.py and my_func() and the other half fileName.py with myFunc(). Its a great project, keep up the good work! 
I had a problem using non-ascii chars (äÄüß...). Encoding the stream in the Writer.py in line 47 to utf-8 seems to have worked.
Simpler: disallow self posts. 
How about "mv" to help with refactoring ? (should take care of renaming in definitions and in function calls or attributes access)
I doubt this means anything.^[[REFERENCE]](https://pypi.python.org/pypi/awesomeness)
nice! an way to add the.possibility to download an entire directory? like: all the songs from a user
Serpent also provides Java and .Net implementations, so you can read and write serialized data there as well. Serpent is even smaller than cerealizer, clocking in under 400 lines of code including comments. The serialized data is just a Python literal expression so you could directly pass it to ast.literal_eval() if you want. Serpent's deserializer is only a very thin wrapper around that.
Mmmm. That's nice, I think I'll give it a try on my next project instead of using Yaml just to read dicts.
Nice. So does this mean I can write functions in Python and then just feed them to Mongo via pql for example for map reduce? 
How is that still up? It seems like spam.
Personally I don't find YAML more or less readable than a pretty-printed Python literal expression (which is what Serpent produces), but I guess it is a matter of taste. I do know that I'm used to Python syntax. I agree with your other statements. However: serpent is tiny (&lt;400 lines) and doesn't require additional libraries; it also simpy uses the standard library's ast.literal_eval. It scratches a different itch than YAML does.
yep stuff like that and links to interesting libraries that are useful not imgur scraping scripts that use requests 
This seems very promising! Thank you!
Not really spam, just a for-fun module.
Thanks for the crits, im not proud of the styling, i was lazy when i wrote it. I jeed to do a full pass and clean this up. About the string div stuff, path.py may not conform to gvr, but its one of the most handy modules ive come across. https://github.com/jaraco/path.py Ill look into tastypie, but it may too much work to port for any gains. Thanks for the link though, i wish i had it when i started.
I would funnel the questions to sites specifically for questions. Stackoverflow comes to mind.
I'm wondering about how this is implemented. By the looks of it, it dredges up your Python code from the Python AST (perhaps re-parsing your code) and then turns that into SQL. That in turn means that the allowed syntax is just a subset of Python--what if you call a Python function in your select-equivalent? Does this translate to a terribly-performing table walk in Python? It sounds like there's too much (ultimately brittle) magic going on...
Ignore the downvotes. Programming in bash is suffering for most people who are comfortable with python.
&gt;if you had a different programming assignment that involved writing C++ code to manage memory, but then submitted java code (which handles that automatically). Would you still expect to receive credit? Bad analogy. Most stuff in bash is done by calling external programs. The OP IS calling an external program
Nifty approach to use `ast.literal_eval()`. API comment: It'd be more consistent (and more of a drop-in replacement) for other Python serialization APIs like JSON, pickle, marshal, if you made these changes: * Renamed `serialize()` to `dumps()` * Renamed `deserialize()` to `loads()` * Added `dump()` and `load()` 
This is actually quite cool and definitely something I'll be using next time working with excel spreadsheets in python.
Thanks for the bug report!
That syntax is actually pretty useful, creative and intuitive - rather than Django's .filter(Q() | Q()).filter(x=5)
How I code for fish?
As it stands, I find that comprehension incomprehensible. For such a complicated operation, I think it would be best to define a quick helper function and do it in multiple lines. I think the best practice is to always maintain clarity, but go for conciseness when you do not sacrifice clarity (which is why comprehensions are really useful, they can help with both for simple operations). The sort method is nice. I believe using key functions can be slow though.
I do like the syntax; it's far better than Mongo's default, at least. I might just use this...
If we assume that these joke modules are added at steady pace over time, shouldn't CPAN have more joke modules, since it's been around longer? 
What makes it too big? Are you doing lots of writes? Could zodb be an alternative? (It's like shelve - but it's not py3k yet).
Haven't used mongo since 2007 or so. Been considering using it for a project at work, and maybe this is my ticket!
According to Ray Hettinger, key functions are way faster than the cmp option (which was removed in py3k). This is due to key functions only being called once per element where cmp is called nlogn times during the sort. Source: https://www.youtube.com/watch?v=OSGv2VnC0go&amp;t=10m6s
Thanks for posting this, I've been building a tool like this myself. 
Awesome timing. I was just about to add some visualization and analytical capabilities to my django app and this looks great. 
Definitely true! I was probably just misread, I thought he was saying key functions in general are slow. Lambda's are completely slower than the operator functions. :)
Also from the same author is https://github.com/alonho/mongoq which lets you use python objects instead of parsing a string to generate your mongo query dict
Sorry, I don't see it. Here's /r/python - lots of perfectly interesting articles at all levels. 
Wonderful! I like it!
~~Interesting. What could I use as an operator function instead?~~ EDIT: I mean, what is operator.itemgetter(0) doing? Double EDIT: After reading up, this makes a lot more sense. Thanks for the tip!
Thanks :)
Everything deadwisdom was discussing was things that he would like to have in Django. What feature are you recommending for Pyramid exactly?
Wow, fantastic work! 
What people seem to be missing here is that `request_method` is only one predicate in pyramid influencing lookup. The system is infinitely extensible, including several other default predicates. Want to invoke another view if the browser accepts json? No problem, `accept="application/json"`. Want another view if the `user` in `/users/{user}` matches `admin`? Sure thing, `match_param="user=admin"`. Yes, `request_method` is cool, and every framework under the sun supports it in some way. But they don't usually go much further than that before you start having to add if-statements to your view and dispatching things on your own.
Is there anything to help against "PQL injections"? http://xkcd.com/327/
It still consumes a lot of space (more than a RDMS and more than other databases that don't store the field names for each document like Cassandra). There are two tickets complaining about this but nothing done yet: [SERVER-164](https://jira.mongodb.org/browse/SERVER-164) [SERVER-863](https://jira.mongodb.org/browse/SERVER-863). Sadly it affects me too now that my databases are far larger than RAM.
When I first got here, it was a subreddit about snakes. Those users have every right to ask "what has this subreddit turned into?"
I'm using Pyramid 1.4. I want to be able to let a different view takeover based on certain conditions. For example, I'd like to be able to detect whether the user is on a mobile browser, and then have different view takeover based on that.
Seems less magical than pony ORM
I agree that Mongo's query language is superior for coding, as it composes better and is safer. but I use PQL as a query language intended for humans used in a RESTful API
too bad it's only 128 kbps
I have been using the [sqlite3](http://docs.python.org/2/library/sqlite3.html) module for a small project and am very satisfied. I don't have any arguments for the use of sqlite3 in terms of performance or portability, but i know some SQL basics, so it seemed the logical decision to me. Correct me if i am wrong, but i think the module comes built-in. EDIT: sqlite3 seems to be aimed at smaller DBs and a DB can be set up in a few lines of code, so it might be preferrable over "full grown" DB APIs. Here's an [overview](http://wiki.python.org/moin/DatabaseInterfaces)
Why not just downvote the chaff and upvote the wheat? Ironically, this meta post is the worst (most non-Pythonic) on my python page.
This is now you do a do while in python while True: &lt;some code&gt; if &lt;some condition&gt;: break check out /r/learnpython Python does not have tail call optimisation.
See: JSON &lt;= XML &lt;= Flat Files
Weirdly enough, I learned to use yield in C# before I came to Python. In fact, I was really hoping Python had the feature - was not disappoint.
Python doesn't support tail recursion http://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion
Oookay! Now I gotcha =) Out of curiosity, you mentioned earlier that enforcing access modifiers was "mostly useless". Is this because they're somehow easy to get around?
Thanks! Yes, sqlalchemy seems rather heavy, albeit more pythonic than the sqlite module.
My problem is that I have several pickled files that are about 3 gigs each. They're grouped by eights in lists (each list corresponds to a subject, each file to a session) and those lists are shelved with an `str` key. Unless I've completely misunderstood how shelve works (entirely possible) it seems as though I can't do something like `myshelf['key'][0]` and load file `0` *only*. Instead, I get the impression that shelve loads the entire list and then selects the first item. Try loading 24 gigs of data on a laptop and you're going to have a bad time...
For django (aka templatetags for nvd3) have a look @: https://github.com/areski/django-nvd3 Should be trival to mix with a fork of vincent. 
No. the compiler enforces them and you cannot work around them simply because you can't compile if you violate access modifiers. The point is that if you declare some method or member variable private, then you can't invoke or keep an eye on the content (respectively) from the unittests, meaning that you can only test the public interface. This is most of the time not sufficient, because the public methods act as facades, doing a lot of underlying low level operations done by the private ones. The resulting granularity is too coarse and tracking down a bug becomes more complex. So you end up making public methods that are supposed to be private (not part of the client-accessible interface), but they must be public because the compiler gets in the way of good programming practice (testing). Unless, of course, you start littering your code with friend declarations, and making all your code class marked friend MyClassCPPUnitTestClass is really, really bad. 
I see. I'm going to keep picking your brain until you get tired of responding ;-) Do people still use access modifiers, then? What's the official dogma in the C++ world? I took my first programming class about 5 years ago (C++) and they insisted that access modifiers were The Right Way^tm of doing things. Has this changed? I ask because I've actually been itching to dive back into C/++, although I can't really find a suitable project at this point (for 99.9% of tasks just makes more sense to use Python), but I get the feeling that my understanding of the language is largely academic as opposed to practical.
In your case one personal license should be enough. It allows to launch up to 50 processes with Pony ORM code inside. We believe this amount is enough for a mid-size web app. If 50 processes is not enough one can purchase several personal licenses.
youtube-dl supports more than just youtube. youtube-dl --list-extractors
SQLAlchemy has already been mentioned, and is great, but is a behemoth of a project which can be overwhelming to newcomers. [Pony](http://ponyorm.com/) and [Peewee](https://github.com/coleifer/peewee) are two options for a more lightweight ORM.
You shouldn't fear sql. It's much easier to learn than Python. The trick is you have to think in sets of data rather than iterations of data. Even with a heavy hitter like sqlalchemy you will be miles ahead if you understand how sql 'thinks'. Joe Celko's Thinking in Sets is a great text for this. 
Thanks for the reading suggestion! I tend to like reading about theory before diving into practice, so this seems like a good way to get started. My (vague) understanding of sql is that it's conceptually similar to Pandas DataFrames in that we're essentially dealing with labelled arrays. Is this correct?
Cool! It reminds me a little of web.py in its simplicity. Anyone using it?
Yes, that is exactly correct. I find that SQL is easier to work with than Pandas DataFrames.
I would be interested in the benchmark source code.
Why is that? You've pretty much convinced me, but I find Pandas DataFrames to be pretty easy. What makes sql comparatively easier?
This looks fantastic! Though I'm a bit unsure about the benchmarks. Are they using pure python wsgi servers for flask (not uwsgi and ngnix)? But that might be comparing apples to oranges. I also would have thought cherrypy would have done better. 
debian is one reason. debian always makes sure everything is completely stable. the next release, coming out next month, will finally include python 2.7... and of course the software developers want their software to run on all major platforms, so everything is written for 2.6 now. furthermore, there are changes and certain calls which are deprecated over the years.
For me SQL seemed more natural. I find it easy to move around in. Granted, I am more at home in SQL than Pandas.
When you build a codebase, you trust the codebase. Upgrading any part of that codebase requires tons of regression testing. Sometimes it's hard to get your business to make time in the plans for work (such as regression testing) that doesn't produce tangible progress on business-related objectives.
Yeah, this is actually what I'm trying to do! Pandas + sql seems like a hell of a stack =)
Good suggestion, will certainly consider adding/renaming the functions
This makes sense. However, why do they delay so much? If they have a large codebase, I am sure they would certainly benefit from some of the newer features that would have been added in the newer releases - both for code that has already been written and for code that would be written in near future.
Thanks :)
1) Sometimes you don't control the environment and the Python stuff is a mile down the priority list on that particular box. 2) Working code is *always* more valuable than cool new features 3) There is more to "code compatibility" than just your old Python source code in a mixed environment. For example, third-party libraries are not always backward-compatible. Even you're own code is not backward-compatible if you're running from a .pyc file --- it will have to be re-compiled using the updated interpreter if the .py files live in a different directory. I could list about another dozen ways a "simple" interpreter upgrade can hose a system in surprising ways. The fact is that a lot of work environments are not ideal, and there are a lot of variables that go into whether upgrading the environment is worth the time, effort and risk. Sometimes it's *much* more efficient to just crank out some 2.5-compatible code and move on. If Python development were a main part of what we do, I would be more insistent. That said: on boxes I control, I maintain the latest stable release of 3. 
&gt; everything is written for 2.6 now. Not sure how much of this statement is correct. I believe must of the code is written for 2.7 now (with a bit of push towards 3.x). Regarding deprecation, I am a beginner, so correct me if I am wrong, how does it matter between 2.4/2.5/2.6/2.7 - they all still support those calls - although deprecated.
I just signed up, thanks for doing this!
This is something I never truly understood until I entered the working world and saw a project or two fail. 
Unless you're personally writing "most of the code," you are not in a position to say it's mostly 2.7 and 3. You're probably correct to say that a lot of it is --- especially for companies where Python is a major part of what they do. However, there are experienced people here telling you there is a lot of 2.6 out there, and they aren't just making stuff up. If you know your target users are likely to have 2.6, and the code needs to run locally on their machines, then you better write for 2.6.
Py 2.6 is the default in debian
...Or broke an entire system in a surprising way because of some hidden, hard-coded dependency.
This is an entirely valid point, but too many use this logic as justification to just sit on their ass and never change anything. If you control your entire software stack and don't need to interact with outside tools or safely deal with the internet, you can be as out of date as you want to. If your software depends on a third party platform or deals with "unsafe" data (read: anything that wasn't directly input by someone trusted, and potentially some things that may have been) there's only so far this goes before you're still the moron for not keeping up to date. How far this goes is entirely situational, but the important point is that if the time isn't made to update regularly all you're doing is putting it off until you eventually have to make a massive change when one of those new things suddenly becomes critical. Regularly scheduled update intervals, but not as often as the upstream packages (as long as it's not a security issue): good "This is the supported version end of story": Bad edit: Basically the short version is think about the shitstorm that awaits all those lazy companies still loafing along on Windows XP to support some ancient DOS application they refuse to update or replace when 2014 comes along and there are no more security updates. One good worm and they're in a world of pain.
We also use RHEL 5 and we installed 2.6 because we needed it. You can install 2.6 from the EPEL repo from Fedora. The issue is that you can't upgrade the system python from 2.4 because lots of RHEL tools depend on it. Also, if you have RHEL you get support from Red Hat for the RHEL maintained packages so frequently the IT department doesn't want you installing packages outside Red Hat support. The reality is that code must be maintained and new features added. The hours you spend fixing bugs and adding new features can easily be seen by managers as having been worth it. Upgrading code to work on a newer version of python and possibly introducing new bugs isn't really worth it unless you're addressing a particular issue that is visible to the customer/management. Upgrading for new syntax/language features is low priority for managers. It really just comes down to time and risk management.
From the top of my head: specific types that cannot be serialized without loss of type information in Json: tuple (will turn into a list), non-string keys in dicts (will turn into strings) Types that can't be serialized at all: decimal, set, complex number, any custom class. A struct will just be a byte sequence when you pack it, so that's not a type by itself.
Systems guy here. Typically you'll see a need to write for older versions of interpreters because production servers rarely change. For example when I started working at my current company 3 years ago I standardized our server builds on CentOS5. It's extremely important to have a set OS standard when you're working with hundreds of servers. Now even though CentOS6 has been out for awhile the company has been slow to migrate. This is mostly a "if it ain't broke, don't fix it" mentality. The #1 cause of customer outages from a Sys Admin perspective is caused by Sys Admins doing things to production systems. So in generate if your shit works, you don't fuck with it. Now you can put Python 2.7 on these older systems(and I've done it), but for a really large company with a really large install of systems they're going to be slow on adopting and pushing out new revisions of software. Also if there's a lot of old code out there running on older interpreters, you really need to test that code on the newer software version even though it's supposed to be compatible. That can be a lot of work. So typically unless there's a really good reason to move to the newer version of software the inertia will be to remain on the older version because 1&gt; It works and 2&gt; You tend to want to keep your production systems all the same. Virtualization helps to get around this. All my prod servers are CentOS5 running Xen, but the Xen VMs can be anything. So I can keep production hardware on CentOS5, but dev can send me Xen VMs running anything they want. But a lot of companies haven't really adopted virtualization fully yet.
From the [README](https://github.com/alonho/pql/blob/master/README.md) &gt; PQL is resilient to code injections as it doesn't evaluate the code.
Python 2.7 and OpenSSL 1.0.1 are far and away the two things I'm most excited about in Wheezy...
I have run the benchmark with web.py and wheezy.web too. This is the result I got. I have tested with 10, 100, 1000, 10000 iterations. Order was same in all iterations (req/sec numbers were different obviously). 1. Wheezy.........51,070 req/sec or 20 μs/req (45x) 2. Falcon.........29,838 req/sec or 34 μs/req (26x) 3. Bottle.........12,204 req/sec or 82 μs/req (11x) 4. webpy...........6,771 req/sec or 148 μs/req (6x) 5. Werkzeug........6,599 req/sec or 152 μs/req (6x) 6. Flask...........3,313 req/sec or 302 μs/req (3x) 7. Pecan...........2,849 req/sec or 351 μs/req (2x) 8. CherryPy........1,146 req/sec or 873 μs/req (1x)
Sorry if it sounded like I was jumping down your throat. Didn't mean to. Asking questions is the Right Thing. Just pointing out that if a lot of people are saying 2.6 is common, then it probably is. To a beginner, it probably sounds like fatalism to give up progress for stability. In some cases it may be. But I've found that experienced developers value stability with a longer view that keeps them from chasing every new thing. I'm not saying progress is always frivolous. Sometimes it's critical even if it is messy. I'm just explaining why change can be slow. In many cases that's a feature, not a bug. 
build for rpm based distros: http://download.opensuse.org/repositories/home:/cavallo71:/opt-python-interpreters You can use just sourcing the provided env.sh file (eg. $&gt; . /opt/opt-python-hgXXXXX/env.sh) and no impact on any other installed package. I hope this helps 
I work in Solaris development at Oracle. Specifically, I work on the installers for Solaris. Our installers are completely written in Python and use the Solaris packaging system (also written in Python). We have a GTK-based installer using pygtk, a curses-based installer and an automated network installer. All 3 installers use libraries outside of the standard Python distribution which *all* must move in lockstep to whatever version we want. It's an incredibly difficult task and one that has to be tested extensively. If customers can't install, they can't use Solaris. In short, moving from 2.6 to 2.7 is really hard for us. This is why we won't be moving to 2.7 at all. We're drawing up plans now for a mass migration to 3.3.1 later this year.
I still don't see any reason why this framework might be better than something like tornado.web. Is this using WSGI + Apache2+mod_wsgi, Nginx+uWSGI, or an internal server? 
One thing to keep in mind is that when you have a codebase that must be read by many people, you don't always want new features. In some cases, you don't even want to use all the features a language offers. For instance, take a look at the Google Python style guide... http://google-styleguide.googlecode.com/svn/trunk/pyguide.html Note that they specifically disallow "power features" such as reflection and even limit how features such as list comprehensions can be used (in the past, IIRC, list comprehensions were a no-no, so this has changed). So if you aren't going to make full use of the features a language offers due to complexity, you might care a great deal less about having the latest and greatest features.
I definitely agree ... IF you can get approval from all the concerned parties, including whoever schedules your time. The thing that broke in this case was written by a guy that doesn't even work here any more, so the owner didn't even know what it was doing. Not presenting this as ideal --- certainly not how I maintain my systems --- but it's a reality we face, and it's not always practical to make a system-wide fix, and it's not always our call. And even then, the switch isn't necessarily seen as cost effective when reverting to the older interpreter is an instant fix. Especially when Python is a tiny portion of the codebase on that machine. I'm not saying it's the decision I would make, but we don't always get to make the decisions. The nature of the OP is about the difference between theory and practice and why smart developers use legacy systems (i.e. Python 2.6 and earlier). The environment is a variable that must be considered --- even if that environment is stupid. 
In my case, I use a number of programs that have Python built in as their scripting language. The two main pieces of software (Maya and Nuke, both CG/VFX tools) both use 2.6 (2.6.4 for Maya 2013, and 2.6.5 for Nuke 7) Python 2.6.5 is also the default version of Python on the OS I'm on at work (Ubuntu 10.04) and while it's 2.7.1 at home (OSX Mountain Lion), I make sure I never write anything that wouldn't work in 2.6. I'd love to see 2.7 picked up, especially as I'm trying to write things in more of a Python 3 these days, and I'd love to be able to do "{}" in format strings rather than "{0}", etc, but until the tools I use are on 2.7, I'll be sticking with writing for 2.6
&gt; but is a behemoth of a project which can be overwhelming to newcomers People here who are so quick to recommend it often neglect that fact. And it's documentation is nowhere on par with that of Django, for example.
&gt;Don't they want to exploit newer features available in later releases? Doesn't features excite them? That's your key misunderstanding. The answer to both questions is emphatically "NO". *Getting the job done* is what excites the businesses, and mostly the programmers as well. Python 2.5, or whatever is still a very mature and capable piece of software and it works as well as it ever did. Unless one of the new features addresses something specific that is painful to the business, there's no motivation to upgrade. That lack of incentive balances against the possible risk of screwing up the production software. (For the record, I'm using 2.7 in my work but there's no compelling reason to make the jump to 3.)
I finally got my company to upgrade to 2.7 a few months ago. Unless there's a compelling reason to change, change is not going to happen. I used the 2.6 is currently not maintained argument. It's a fear based argument. You want to be behind the times because it's safe, but not so far that you're unsupported.
 AggregationParser().parse('sum(a)') pql.ParseError: Unsupported function (sum) print AggregationParser().parse('group(a)') pql.ParseError: Unsupported function (group) Hi, I just started to use the [the aggregation framework support of pymongo](http://api.mongodb.org/python/current/examples/aggregation.html#aggregation-framework). Am I missing something or the PQL does not support [the aggregation framework functions \(I mean the $group, $sort, etc.\)](http://docs.mongodb.org/manual/reference/sql-aggregation-comparison/) and also $sum? 
Unless you KNOW how to code in 2.4-2.7, you're going to code in whatever version you know. They're not the same (e.g. python 2.7 can't float "1.234d+07", but 2.4-2.6 can). Major code breaking functionality was added in 2.5, and 2.6.
What (many) people making decisions (often) fail to realize is that general maintenance is a compelling reason. Sure, it may seem efficient to upgrade large portions of software all at once and deal with all the problems then, but it makes for an ordeal and puts you in a situation where you **can't** upgrade quickly if you need to. Staying up to date may be more risky on a daily basis, but the transition from point A to B is less turbulent along changes. It also puts you in a position to change your mind rapidly and pursue different goals with your software. Keep it up to date!
I understand your point. BTW, I am not arguing about 2 vs 3 right now. The whole point is only about 2.x :) 
&gt; All the code written for Python 2.4 must have been compatible with Python 2.6 as well. Assign a value to a variable named "as" in Python 2.4, then try the same thing in Python 2.6. This is one of the reasons why you can't just upgrade software arbitrarily. 
Python code injections, ok. How about MongoDB query injections? For example: untrusted_user_input = '" or "a" = "a' pql.SchemaFreeParser().parse('name =="%s"' % untrusted_user_input)
Some of us still have to use IE6...
Need to add Pyramid to those benchmarks. 
Sorry I wasn't clear. &gt; Major code breaking functionality was added in 2.5, and 2.6. As in I can do new things that aren't compatible with say 2.4. The float 1.234d+07 thing didn't bother me (cause it's ugly Fortran syntax), but it doesn't follow the rules. When I upgraded that big piece of software, I think it too me 2 hours. I always assume upgrading will come with a few bugs. I had to upgrade some very ugly 2.3 code to 2.7 a month ago so we could package it up for a new customer (the code has been dead for years). Other than replacing numeric with numpy and sticking a wx thing in a list so list comprehension could be used, it wasn't a big deal. That took 2 days.
https://github.com/racker/falcon/blob/master/bench/bench.py
As you can see from existing responses it is thanks to RHEL. I know the pain because we also use it. Can you believe that the latest RHEL released not too long ago still comes with 2.6? It is interesting that they are so conservative with upgrading, yet RHEL still had tons of bugs. For example recent leap second issue (which knocked quite few websites offline last year, thanks to them), apparently they did not even have patch for the kernel when it happened, while Linux kernel had it already fixed for few months. Or never ending issues with net-snmp. Yet 2.7 apparently is not stable enough for them. 
There are 1.8 million XO-1 laptops around the world right now. They all use Python 2.5. Can you pop over to Peru and upgrade their laptop's Python distribution? :)
Do you have the soundcloud link, which you used? Linux or Windows? [This image](http://i.imgur.com/oUs0eq7.png) shows an MP3 downloaded on 2 different machines, one with ID3 module installed and one not. Both have the full song so that can't be causing the problem. Also, you can download the ID3 packages in Debian with apt-get install python-id3, probably the same in Ubuntu, if you're on windows, you might need to google it to find a installer.
That's really not a valid comparison, as it's using the count of perl *distributions*, not perl *modules*. CPAN currently has 120,604 modules, 4x the number of modules on PyPI. 
No no, you have to pay for everything. They don't have money :P
I would just use a hardcoded value for mtime,atime,ctime or any other part of the interface that's not applicable, it seems to be a common practice in fuse-land. Or maybe they should inherit from the parent file's *time attributes if you feel like it must have something meaningful. I don't understand what you mean by "but it wont give me any of the above for "free", I still have to implement each function myself :)". You just have to implement the fuse interface and command line tools like cd/ls/cat/find/grep can't even tell these are not real files and work as you would expect. Or did you mean something else by "implement each function myself"?
The trouble is that in most development situations, the tasks that get done are usually determined by teams other than the development team. For example, in many agile environments, it's the product owners who determine which cases get prioritized, and the developers will squeak and whine and make noise about the need to upgrade, but when you put an estimate on that work, the product managers are faced with a choice: kill an iteration getting no tangible deliverables, or deliver a non-zero number of things that your stakeholders desire. Dev usually loses that standoff until the cost of not upgrading becomes greater than the cost of doing it. Sad, but true.
Take a look at the Telldus devices, they're big here in Sweden for 433.92MHz home automation devices. They've got an open source C API with several python wrappers developed so far. 
looks promising, i've been waiting for something like this for a while now. some more information on how relationships and indexing are handled would be useful though. 
Ha, spot on! Thanks a bunch!! Man that was a dumb mistake.
The problem with using newer versions of python on RHEL isn't so much the technical part. If you are using RHEL you are most likely working for a company that wants the support from Red Hat (why else would they pay for it), and if you are installing and using software that Red Hat doesn't support then people at your company aren't going to like that. Speaking from experience, when things break the unsupported applications often get blamed (not by RHEL, but by company folks). Sometimes it really is our python app, but sometimes not.
&gt; if it’s only tiny in its native language implementation while advertising that it runs in three environments I'm not sure I follow. The .net assembly is 28k and the Java jar is 36k. both are smaller than most corporate email signatures I get. &gt; none of which being a universally usable C binding True, I can see that this would be useful. Maybe I'll add one in the future. Also Json is a good choice as you argue, if you can live with the limitations. Serpent is not meant to replace that. It grew out of an experiment to accompany ast.literal_eval with a serializer that writes literal expressions, and that is accessible from Java and .net. I've already written a [pickle implementation](http://pythonhosted.org/Pyro4/pyrolite.html) for Java and .net (to connect with Pyro) but we all know the drawbacks of pickle. So I put some time in another approach. I posted it here to get some feedback :) 
Would be nice if it could be used as a plugin for [flake8](https://pypi.python.org/pypi/flake8).
Can someone be kind and post a guide on how to install all the necessary stuff for windows? Thanks. EDIT: For anyone that needs help. How to install PIP- https://github.com/simpleservices/app_report-python/wiki/How-to-install-pip-on-Windows django- http://mirkobonadei.wordpress.com/2010/01/25/install-python-and-django-on-windows/ 
There's a cost to maintaining and supporting our own version of Python. Often, that cost is higher than the benefits from newer features (especially if you want to upgrade. regularly)
and i gave you feedback ;) as i said: i personally would rather use YAML, mostly, and don’t see the niche serpent fits into, but if you and others do, that’s fine.
No you were right, I didn't realize that the interface was indeed minimal and that the commands would automatically work. Some of them may have weired behaviour though (I think of the "file" command for example). Also, right now I'm thinking of one option that I don't know how to implement in fuse : create new file types (you'd have files, directories, but also new kinds of files that would be "methods", "functions" etc. so that commands like find . -name "*.foo" -type "method" would look for all methods that would contain the "foo" string. This would also allow me to write things like find . -type "functions" | grep "pattern". It would be very exciting to be able to have this kind of options. Could this be possible ? **EDIT** Or maybe use conventions like character file =&gt; method, block file =&gt; function, socket =&gt; class etc. ? I don't how far I can go with this. 
You could try that : https://github.com/languages/Python
The Python inventor is Dutch, by the way ;)
That's because you're looking at some random person's fork of the repository and not the actual [upstream repository](https://bitbucket.org/pypa/pylauncher).
I love looking through GitHub for projects to work on but I must confess it's challenging. Although I can find "popular" projects I can't really filter to projects which are actively looking for help. Does anyone know if there's a solution where projects more actively request assistance?
I know that this isn't the type of feedback you were trying to elicit, but I started my career basically as an RA for a bunch of scientific computing projects. For the sake of yourself and your future RAs, I'd highly encourage you to pick less ambiguous variable names. I know it's totally clear to you right now what things like K1 means, but then you'll put down this project and come back to it in 6 months after you get back ref comments from the journal (or maybe in 5 years you'll want to revisit this with a new dataset). At that point I've seen in a single project "That variable name comes from the variable in the paper I'm publishing, that's what X is, read my paper.", "That variable name comes from the variable in the study that I'm replicating, that's what K means, read the other papers.", and even "I had to dig this equation out of this esoteric math book, and they called in Z in that book, so that's why it's called Z, read this esoteric math book." All of which can be avoided by calling it something like growth_rate_of_calcium or whatever it's supposed to stand for. The way you've coded this is 100% consistent with everything I saw in my past, because after all, you're not gunning for enterprise code that need to be highly reusable and extendable with changing requirements, you're just trying to support your research assertions. But a simple clear variable and function naming scheme can really save you a world of pain later in life.
I work in a python 2.5 shop. One of the first things I did when I started was port our app to 2.7. The reason was actually somewhat sane: we had some old no longer maintained dependencies (as in, no longer maintained since like 2006) that our out of date web framework used that would have required a ton of refactoring to move to whatever library replaced it. I decided that rather than do all that work, just start maintaining that library ourself and make it py2.7 compatible (it was actually a huge pain in the ass because it uses some weird bytecode stuff that changed from 2.5 to 2.7, and that genre of stuff is, well, very archaic.. but I did get it working). Anyways that was like a month ago and we're just now getting the green light to merge that into our develop line, scheduled for release in a month. Why did it take us so long? Because when the CEO says "what are you working on today?" and you say "porting our app to python 2.7," the response is "yeah, well our customers called and said they have a lot of money that doesn't give a fuck if it's written in fortran, they want Widget X, so if you want to keep getting paychecks, make Widget X"
www.code.google.com www.sourceforge.net http://code.nasa.gov/project/
I agree completely. I actually just did what you suggested after having this mistake pointed out. Also what's an RA? This kind of work is actually outside of my usual field, but it seems really cool. 
RA = Research assistant. It's a job typically given to certain graduate students where they are given a part-time wage and a tuition scholarship in exchange for working on some project/s. They tend to support the research of their sponsoring Professor, and even do some of their own research as well, however are not usually required to teach as in the case of a "Teaching Assistant" or TA.
Research Assistant. Also, I recommend using [camelCase](http://en.wikipedia.org/wiki/CamelCase) variable naming scheme. It's a lot faster than putting underscores between words, but you can still have effectively descriptive variable names. 
That's definitely a thing that I think most tech teams would love to do, but you have to have the budget for it, and... well...
There aren't any actual WSGI application servers involved in the tests. If you examine the create.py file in the bench folder you'll see a simple app of each flavor (Flask, Falcon, CherryPy, etc) is instantiated and a few "requests" are thrown at each. More akin to a mock request or traditional test suite. It's an interesting benchmark. It accounts for time spent in each framework before &amp; after the actual work of your application. Keep in mind, however, these frameworks aren't equal in terms of features, convenience and paradigm. So while Flask performs poorly in comparison to Falcon, Falcon is purpose-built for constructing API's. Flask can do that kind of work, but it's more general-purpose.
nice work. there is one thing that I couldn't understand the motivation behind it. In order to configure my redis connection, I *have to* monkey-patch your get_connection method. Isn't it weird?
Falcon was created at Rackspace. I assume they use it to build internal APIs to some degree. One working example would be Marconi - an incubator-stage message bus for OpenStack. https://github.com/stackforge/marconi 
It's not a random person's fork - I'm the original developer. I very recently moved my repository to pypa ownership (a number of packaging-related projects are moving there) and forked back to my own clone; I'll add the launchers to my repo, so the problem shouldn't occur in future.
Thats when you point out that the last binary release of Python 2.5 was 2.5.4. There were 2 more version of Python 2.5 that were source only releases, 2.5.5 and 2.5.6. Python 2.5.6 is the final release of the Python 2.5 branch, and Python 2.5 will not receive any more security fixes. Then you tell your boss, that you won't have any customer after they get hacked because your application requires an older insecure version of Python. But, then again I'm sure your boss isn't the only boss that doesn't give 2 fucks about security, so you guys probably aren't the only ones shipping insecure software.
 It's amazing how many people have no clue what value RHEL provides. If you're on RHEL 6.X, you're guaranteed that you'll have the same upstream versions of all core supported packages throughout the entire RHEL 6 series: e.g. from 6.0 thru 6.9 (or whatever they end up with). This means python-2.6.6, glibc-2.12, gcc-4.4.7, perl-5.10.1, kernel-2.6.32, etc This means that you can safely move software from RHEL 6.1 to 6.5 (for example) and all your APIs and ABIs will still work. *BUT* what you're getting in those minor RHEL releases are backported security and bug fixes (and sometimes even more than that -- see NPTL below) in all those packages. So the upstream Python/glibc/gcc devs find and fix bugs and make them available in new releases (which may not have the same API/ABI as the version where the bug originated). So RHEL goes through all the hard work of cherry-picking those specific bug fixes from upstream and figures out how to backport them into the versions that RHEL adopted back in RHEL 6.0 w/o breaking the API/ABI. This can sometimes be incredibly difficult since the upstream fix may have been made in a newer version that doesn't even resemble the version running on RHEL. The thing that confuses people is they see a version number (e.g. Python version 2.6.6) in RHEL 6.0, 6,1, 6.2, etc and believe they're all the exact same Python -- assuming nothing has changed. RHEL cannot modify the upstream version number (otherwise a bunch of scripts/etc would break), so the only way you can tell what's been done is by looking at the RPM "release" number (or running "rpm -q python --changelog"). And there's often quite a bit going on hiding behind those identical version numbers. Back in RHEL3, they backported NPTL (native posix thread library support) into their 2.4 kernel -- although it wasn't introduced into the upstream kernel in 2.6. I think they had the only 2.4 kernel in the world with NPTL -- all because there was customer demand for it. 
I only became aware of the `startswith` tuple-argument feature recently. It's been extremely helpful. More blog posts should definitely talk about it. One thing though: &gt;I'm not sure it's a documented API, but you can call set 's methods as static methods and give a list of sets as arguments: &gt; `set.union({1}, {2})` This isn't actually a special "feature," but rather is how Python handles bound methods and OOP in general. `obj.method(arg1)` is syntactic sugar for `obj.__class__.method(obj, arg1)`. So for example, with dicts this time: {"a": 1}.get("a") is the same thing as: dict.get({"a": 1}, "a") That's why there is the explicit `self`; it's where the object instance is passed in when you call a bound method.
In the Live installer we wanted to cut out as much unnecessary stuff as we could so things like the name of the root pool and advanced zpool management options are all unselectable. The point of the Live Installer is to give people a glimpse into what's going on with Solaris and to get a single system installed. For complex installs, we're trying to steer customers to using the automated installer. The number of options you can select for both the disk layout and zpool layout is far more extensive with AI. Glad to hear you gave it a try. I'd almost bet your pdb trace went straight through my code. ;)
I understand the liability factor in choosing a software stack but as soon you use third party app you'll have the same problem again: the problem has just shifted. Beside maintaining the same python stack across their server line is not that difficult: I do it in my spare time and I'm betting they can do the same (actually they have to do it anyway). 
Thanks. The apt-get got me to http://id3-py.sourceforge.net/ Downloaded and installed but now have trouble getting the correct url. Thanks anyway. Sorry to have bothered you.
While I use 2.7 as my primary Python install I keep a 2.6 install around in order to use certain analysis libraries for which I only have 2.6 compatible binaries. Getting updates isn't likely at this point, so I take what I can get. Running multiple python installs on the same system is relatively straightforward.
Another great one is list comprehensions: alist = ['1', '2', '3', '4', '5'] ints = [int(x) for x in alist] [x for x in ints] [1, 2, 3, 4, 5]
I *must* be cute because of that typo. Isn't it :)
Have you given pudb a try? It's an awesome full screen debugger.
How do I shot web 2.0?
To get the correct url I found out clicking on the share button, not the download button, then select the short url box. This will give a shortened url without blanks. Works now. I may have to do a slight modification to sanitize the filename for Windows because some characters such as ':' are not allowed so 'Radiolab:' will not download yet. http://snd.sc/14KgHoh def add_id3_tags(filename, title, artist): Getting Information... Downloading File 'You Keep Me Moving (David Stone's Swedish Meatball Mafioso Remix) - davidstone.mp3' Downloading (6.09MB/6.08MB): 100.14% / 100% Download Complete Attempting to add ID3 tags ID3 tags added 
Thanks to great distributions such as SLC and other "enterprise-y" ones.
I'm sorry to hear that. If it offers any hope, I've seen it at all of my last 3 workplaces.
While looking around for myself I found this one: http://www.lookingforpullrequests.com/ 
This is absurd. The argument boils down to "high-level languages are bad for beginners because the fact that they get stuff done in fewer LOC means that beginners don't get as much experience actually writing out the code." That is trivially addressed by *getting more stuff done*. Which is actually much better for the beginner because there's more motivation, more positive feedback. "libraries and frameworks" actually work in the opposite direction at the beginning, because first off you have to understand the entire concept, and then you have to learn how to use the library or framework in question. For many tasks, especially the ones beginners should be looking at, it's overkill and/or useless. However, this is also exactly why it doesn't matter WRT the relative strength of Python as a teaching language. Or the relative strength of any other programming language for teaching, for that matter. The Java community puts tons of emphasis on "enterprisey" toys like Spring, but that has zero impact on how Java is taught to beginners. There's a secondary argument that "assembling" things at a high level comes at an opportunity cost of "understanding things from a broader perspective". I do not believe this to be true, either - not from my personal experience as either a student or a teacher. Python is a multi-paradigm language; and elegant, masterful Python code exploits that multi-paradigmatic nature; and writing such code requires an ability to understand those paradigms, in a way that forces "broader perspective" on you.
In my current workplace, we recently reached the point where we were able to dedicate one full-time resource to technical development unrelated to business requests. But in my experience, this is not typically the case. The difference, in my opinion, always comes back to the tech team's ability or inability to speak to the business in terms that translate into figures on a finance sheet. 
Good real world example that's funny, https://bugs.php.net/bug.php?id=50696 Basically they used a feature of a specific PHP stdlib function that changed with a generation update and it appears to have been used extensively enough to make it difficult to fix correctly.
 val = d.get('key_name') if val is None: raise Exception() Why would you do this? Just use the `d['key_name']` and it raises the keyvalue exception anyway. Also, `d.get('key_name', 0)` is useful, if say, you want to return 0 as a default value. I use this a lot to assign default values to config settings. for v in vals: if v in invalid_vals: raise Exception('Invalid val') if not set(invalid_vals).isdisjoint(set(vals)): raise Exception('Invalid val') I think the first version is easier to read, and its quite clear what it does. 
yes, things like find . -type "functions" | grep "pattern" is what makes abstraction at the fuse level so exciting. You do sometimes need to tweak your caching strategy to make that reasonably fast but it's still *way* easier to make that work than doing it yourself. And yeah, you can get weird behaviour from stuff like the file command or other ones that do low level stuff that don't map to concepts you are dealing with. You're right about file creation not obviously mapping to something ... I would just make the directories always read-only so that you don't have to deal with it. Unless you come up with some kind of great idea. Hell, you could do some weird weird shit with metaprogramming if you made file creation add stubs in the appropriate places ... but that's getting off track and out of scope :) Mapping different system file types to methods/blocks/classes is a really interesting idea ... you'd have to get lucky for that to map cleanly and in a way that wasn't totally confusing ... I'm not sure about that one. It might be more trouble than it's worth, a simpler way of communicating that extra info is maybe to map to files and directories and then messing with case? You've got all lower, camelCase and UPPERCASE to work with maybe? Or the ugly version is to add some kind of prefix or suffix but that might not be as clean as you want it to be, forcing people to grep for "function_foobar" for example. I think this is a very cool idea, with a smart enough system running fuse you could get all sorts of cool IDE features in your text editor like "open this virtual file that has all the subclasses of Class X in it and let me edit them and have the changes saved to the right place" (haha, more scope creep!)
Nitpick: while `dict.get` is very handy, the example given is exactly one where you shouldn't use it, and just let the `KeyError` bubble up. If you need to provide extra information, check with `in` before raising an alternative exception with additional information (`raise TokenError("{}: No such token defined".format(key_name))`). When would you want to use `dict.get`? When the key might not be in the dictionary, and _it is not a problem_, you have a specific default value instead. The second argument to `dict.get` is very useful here. You usually want the default value to be something that could just as well be found among your actual values. I did not know about the first one, looks to be very useful.
You should try to mock up a zodb database and see if it can suit your needs.
I do something like this with fabric. 
I think you are able to pick up the basics by going over the examples in the python doc. I myself am not a master of the SQL arts, for me its only CREATE TABLE foo ( bar TEXT, baz REAL ) and INSERT INTO foo VALUES ( 'foo', 3.14159 ) And then read it out again.. Point is: i get along\^^ Best luck for your project.
Especially cool when you use generator expressions on long chains, because no intermediate lists are created: trees = (item for item in bin if is_tree(item)) aspens = (tree for tree in trees if is_aspen(tree)) quaking_aspens = (aspen for aspen in aspens if quakes(aspen)) # ... etc for item in my_final_iterator: print item 
&gt; But, then again I'm sure your boss isn't the only boss that doesn't give 2 fucks about security, so you guys probably aren't the only ones shipping insecure software. Like any good boss, he cares more about money than security. And if you're actually concerned about security and not just using it as an excuse to spend your time on architecture instead of billable work, then I seriously doubt that the greatest security risk in your application is that it runs on an old version of python. Begs the question: how much risk are you taking by running an insecure web app? In many cases, it really is an acceptable risk, when put up against the increased development cost of fixing every security loophole - especially when that time could have been spent building things that directly make you money instead. Hell, even having all your passwords out there because someone decided to be an asshole probably isn't the end of the world for you, financially.. just an embarrassment that _may_ cause you to lose a customer or two. Obviously this is highly dependent on the industry, but I'm not in the security industry and my users would rather pay less for software that is "pretty secure" than more for software that is very secure. ....but I hear ya, people need to get on the bandwagon and leave py2.5 behind. For so many reasons.
One situation where it's especially useful is `','.join(str(s) for s in lst)` where lst contains non-string values.
One I learned about just the other day was the two-argument form of [iter](http://docs.python.org/2/library/functions.html#iter), iter(func, sentinal) -- the iteration will call func() and return its values till it returns the sentinal value. I'm not sure exactly when you'd use it, but it's kinda neat (the example in the docs is sort of worthless -- why wouldn't you just iterate `for line in file: ...` directly?).
I kind of agree with this. Unicode can be pretty annoying if you're working with big data that might contain some binary or whatever. In some scenarios you need to deserialise, serialise and deserialise - as annoying as that is to have to do. You can run into problems when you want to treat everything agnostically as a stream. There are also performance implications for this.
IE6? some of our internal websites still show a "best viewed under IE3" line... and of course are completly unuseable under chrome/firefox
Do you always have to pass in `self`? It seems redundant compared to other OOP languages. `self` is often a keyword within a method, but it always means the parent object, and doesn't need to be passed in. If just seems to fail DRY in Python, or am I missing something?
This. Developers (and teams of developers) have to be able to explain to the stakeholders/business owners/insert-buzzword-ers the value of the upgrade in terms they can understand. Usually that term is money, in my experience.
My company just upgraded to IE8, only a couple days before [Google dropped support for it](http://www.computerworld.com/s/article/9231316/Google_to_drop_support_for_IE8_on_Nov._15).
with Python (and other interpreted languages) the situation is even more dire, because your code can be wrong without knowing about it since there is no pre-compilation or type checking. For instance, the difference between '/' and '//' in python2 and python3.
I thought I had more a extensive examples, but I can't seem to find them. I've placed some simple setup.py scripts that maybe of use. http://pastebin.com/QrLs78X3 http://pastebin.com/HUavhfrf 
* http://www.python.org/dev/peps/pep-0008/#naming-conventions EDIT: * http://scikit-learn.org/dev/developers/index.html#coding-guidelines * https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt
Self doesn't have to be passed in but you always have to include a self parameter in the method definition. The name "self" is just a convention though. This was a very deliberate design decision. I think it helps a lot and makes it obvious when you're dealing with a member variable or method, rather than a local or global variable. Looking at C++ code for example it's not so obvious where a variable comes from. &gt; Explicit is better than implicit.
http://scipy-lectures.github.io/advanced/debugging/index.html
I still like `map` for that stuff, though I know `map` is sort of depcrated now. (as in `','.join(map(str, lst)`)
Yes, it it always necessary (for instance methods anyway, `@staticmethod`es don't get one, but they're weird) You are missing something, and you aren't. Tons of blog posts have been written about why explicit self is a good idea, but it's true that it does kind of violate DRY. Short answer: explicit better than implicit, the Zen of Python. Essentially the `this`, `$this` and `me` of other languages is too magic for Python. Python is generally fond of "less magic" not "more magic" even the magic stuff (like generators, metaclasses, and decorators) are distinctly unmagical now that I know how they work.
Benchmarks? preferably ones that include PyPy please? I'm guessing the hard parts use the builtin zlib module though.
In my humble experience, it boils down to whether the senior staff / founders are technically grounded or from the business side, where immediate results are demanded.
Wisdom that you will gain from learning Java: 1. Verbosity is painful (Java is very verbose). You'll learn to appreciate Python more because of its simplicity. 2. Static typing / generic which can be good or bad depending on the situation. 3. checked exception, unchecked exception 4. The pain of null error (NullPointerException) 5. ~~The pain of redeploy~~ 6. The pain of restarting the server 7. The pain of compiling 8. The pain of 10,000 configuration files. Especially if they are written in XML :( 9. Concept of build script (eg. ant) 10. Learning how to use IDE. Because Java is too verbose that without IDE your life will be damned. 11. Understanding that Python is slow in performance term compared to Java and that's why a lot of people put a high hope in [PyPy](http://pypy.org/). 12. The concept that Java makes it easy to collaborate with a lot of beginner programmers because the language itself contains a lot of features to avoid shooting your own foot. You will also understand that Python might appear simple to learn, but in practice, it requires more expertise to code properly in it. A low quality Java code is more bearable, fixable, and maintainable compared to low quality Python code. Wise people say with power comes responsibility, hehe.. 13. The most important one: **Handling legacy code**. It is quite probable that if you ever work as a Java developer you'll be handling *enterprisy-gigantic-legacy-code* without unit test/documentation. It's painful but worth the experience. Sorry for the rant. Peace and love! :)
I have found the same issue, most popular is a lot of frameworks like Django. One thing I find fun is to just occasionally browse by most recently updated. Not only will it show you projects that maybe no one else discovered but it is obviously something that is being worked on right now. I have found a couple of cool ones that way. 
&gt;So in generate if your shit works autocorrect?
I learned Java way before touching python, so this is my perspective. When comparing my python code/scripts to my coworkers code, I notice not only a stylistic difference, but I tend to lean towards OOP versus a scripting/procedural style. I'll create classes and make most everything I write modular/reusable (even unnecessarily so sometimes...) just because that's how I think. Java will beat OOP into your skull. If you create enterprise-size applications, you better have your design flushed out, or you're in for a world of hurt. Java will teach you about inheritance, encapsulation, polymorphism, strongly-typed objects, among other stuff on a scale you can't really achieve in python. If you get into Web apps, you'll learn about the many frameworks that people use. You'll get into MVC and related design patterns, dependency injection, annotation-based programming, and so on. Basically, this is stuff people use on big projects to make life easier. If you write smaller web apps in php, python, or ruby you wouldn't normally come across this stuff. In my opinion though, unless you have a real reason to use Java, it's probably going to be a frustrating road ahead. Using Java to build a website that doesn't really need it is like killing a fly with a sledge hammer. It'll take a lot of work and you'll probably end up making a huge mess. Maybe a better route would be to pick up some books on OOP, design patterns, concurrency, and MVC. You could use Java to practice these methodologies, but you can do most of it in python too. I'm not trying to discourage you from learning Java, just trying to save you some time and possibly some frustration. I love Java. I don't use it as much as I used to because of the projects I've been on recently, but I still find myself missing it. It would be cool if you tried it out and let us know you're perspective as a Python dev learning Java. Sorry for the rant, and good luck!
&gt;Java will beat OOP into your skull. &gt;inheritance, encapsulation, polymorphism, strongly-typed objects That is emphatically not what defines OOP, no matter how common those notions are.
You will learn to hate semicolons... So there's that...
Why? The existing object mappers for Redis offered features and functionality that I thought to be unnecessary and/or poorly designed. Since I had the expertise on the Python side of things and on the Redis side of things, and had the need for these features, I built it. As for connection configuration, I thought I'd try something different than the typical set_config() call. Which I can still add/change. This is only the *first* release :)
You may want to check out the other Python/Redis object mappers if you find that rom doesn't match your aesthetics. That's part of the reason why I built it in the first place :)
To be fair, a lot of what you describe aren't faults with the Java language or platform, per se, but rather in how too many people unfortunately choose to write Java code.
I'll add some documentation. Thank you for the feedback :) Long story short: * The numeric indexes will just work the way you expect, as long as you only expect to be able to search by (possibly open) numeric ranges and order by those ranges. * String indexes basically allow you to index/search for words. You can think of it like a *really* simple full-text search on a column in Postgres/MySQL/etc. As an example, say that you had a model (and keygen, because Text columns don't have a default keygen) like... def text_keygen(content): return content.encode('utf-8').split() class Post(Model): title = Text(index=True, keygen=text_keygen) body = Text() tags = String(index=True) author = ManyToOne('User') post_time = Float(index=True) created_at = Float(default=time.time) And you wanted to find posts with the tag "python" ordered by most-recent first. You could perform the following query: Post.query.filter(tags=['python']).order_by('-post_time').execute() As-is, there isn't a way to offer "or" queries to match 'python' in tags or 'python' in the title in this example, but that's an API issue more than anything else (I couldn't think of a simple way of offering that as an API without using SQLAlchemy style and_(...) and or_(...) style bits (which work, but which I see as not exploiting available syntax). updated: fixed formatting
I wrote a [blog post](http://www.phi-node.com/2013/01/a-journey-through-parakeet.html) about this a few months ago.
... And it will not teach you OOP concepts any better or more efficiently than you would by practicing them natively in Python. If anything, it will detract from that effort, because the class-oriented design of the language forces you to write the sorts of stupid classes highlighted in that "stop writing classes" talk. Sure, it will force encapsulation upon you, in the form of data hiding. But it can't teach you to build better interfaces (if you were abusing properties before, then now you just have even uglier-at-the-call-site get/set pairs); in fact, the existing ecosystem strongly encourages people to build terrible interfaces (automatic code generation of get/set pairs, and the whole "beans" idea). As for inheritance, that's really half of two concepts spliced together: code reuse and subtyping. On the code reuse side, the experience of the c2 wiki OOP folks has generally shown that it works better to err on the side of interpreting class relationships in terms of composition (with delegation if needed) rather than inheritance. And on the subtyping side, well, there's this thing called "duck typing" and it works really well in practice, especially considering that formalizing your subtype relationships with inheritance *doesn't really matter because there are no compile-time checks anyway*, and can even shoot you in the foot by imposing needless restrictions at runtime (e.g. expecting a subclass of `list` when any iterable will do). Polymorphism: see subtyping, more or less.
On a jailbroken iPad 1, I was only able to find Python 2.5 in the default repositories as of a few months ago. So weird architectures and packaging situations could be a part of slow adoption rates in some cases.
IE 3? Sweet luxury. I have to use Spyglass Mosaic. 
Python "packages" as uploaded to PyPI are analogous to Perl "distributions", not Perl "modules".
&gt; The pain of null error I hear Optional&lt;T&gt; are finally going to be integrated to the core language in Java 8! The time of that damned NullPointerException is coming to an end! I am so excited.
Valid points, which is why I said it the last paragraph of my original comment, there are easier ways to learn these concepts. That doesn't take away from the fact that if OP were to devote his/her time to learning Java, he/she would end up having to learn about these topics. Sure, you could do it in Python, but like you said, Java forces it upon you. Of course Java has shortcomings and OOP isn't the end-all solution to everything, but it does work when it's well thought out and applied correctly. If someone takes the time to learn a new programming language, they're going to experience some things they didn't think about before, a new perspective on coding. They can take that new knowledge and apply it anywhere. In this case Java will teach you some new tricks you can use in Python, which is what OP asked about. When you implement the ideas from Java in Python you can do it however you like, and hopefully make it better. 
Thanks for the link. Since I am just a beginner at Programming, it helps to know the kind of wierd things waiting before me :)
What is SLC? Google couldn't help me much with only an acronym :)
&gt;If someone takes the time to learn a new programming language, they're going to experience some things they didn't think about before, a new perspective on coding. Agreed, almost any new perspective is valuable. Python is multi-paradigm, so in a sense you can't really pick a radically different perspective. But you can use a language that forces you into one of those paradigms, to train with it and become able to evaluate its strengths and weaknesses. (Java being statically typed also means you expand your horizons; it can be much more frustrating to deal with, but it might also instill the developing Pythonista with a new appreciation for TDD or similar techniques.) Though if we have free choice here, I'd rather recommend something from the FP realm, such as Haskell. Rationale: most non-super-advanced-level educational material for Python seems to put fairly heavy emphasis on the OO and imperative features of the language, while functional techniques are seen as some kind of magic. (Like, if I were writing a Python curriculum from the ground up, I would plan on introducing list comprehensions before for-loops; and honestly that sort of thing is exactly why I picked the flair I did.) I find this unfortunate, because when they are used properly they produce very elegant code. Personally I feel that obsession with purity is the enemy of this elegance, though - so perhaps better to experience "purity" first-hand and thus understand it :)
Re: OOP, I would suggest learning Objective-C/Cocoa or C++/Qt instead. Both will teach you object orientation, too, but in a much nicer environment.
A RHEL fork. RHEL =&gt; [SL](http://www.scientificlinux.org/) =&gt; [SLC](http://linux.web.cern.ch/linux/scientific6/) And guess what - it comes with Python 2.6.
Except that the a large numbers of apis return nulls. Unless you want to deprecate 1/4th of the stlib(and 3rd party libraries).
If I understand it right, that will only help if types become non-nullable by default?
That is *great*, because it makes the young and bright minded move to Python... the last thing we want are the old and dull-minded ;-)
As a former Java programmer and Python advocate, Python is not perfect: - The fact that it is joy to refactor code with powerful IDE and static analysis: just click a method name, get all places where it is called and then rename it and get updated everywhere in the codebase - More working date / time system - Able to write GUI that runs in decend speed - Setting breakpoints from within IDE and it works 
Explicit interfaces.
To be fair, your answer is just words from an angry man? I mean come on, pain of null error!? I don't see how python is any different from Java on that regard.
I'm interested as well, but I'm having difficulty imagining an application where encode/decode speed is really the bottleneck. Normally an image is used (e.g. blitted) much more often than it's loaded.
You can always learn from a different language. I used to hate Java but it actually has a lot of very sexy features. But let's rather talk about what I learnt from Java that I can use in python as it was your question. Being verbose is actually true. Every starting pythonistas has this codegolfing habit. The shorter the better. That's just plain crazy when people start refusing using long variable names. Java will definitely crush that feeling. Scale... Java seems to have been designed for scale. A lot of things you're going to hate at first will totally make sense once you get to work on a huge codebase. I'm thinking about one class per file for instance. There are a couple of features Java got righter than pretty much any other mainstream language. Because of that, the Java community uses them more than any other and you will learn about their benefits. For instance, I'm thinking about - exceptions handling (you gotta love the concept of Checked/Unchecked exception). - OOP : interface + abstract classes - multiple inheritance - Enum
Semi-automated type based refactoring. Enums
Huh. I actually had a bit in there about asking forgiveness and how you could catch the `KeyError` and throw another inside the handler, but it seems I thought it didn't contribute enough to the comment and deleted it. You're completely right about that, of course.
5) ... and the ease of initial deploy. Trying to deploy cross-platform applications is a breeze in Java, and an absolute pain in Python. With Java you'll learn just how badly fragmented Python is. 15) Parallel programming. You'll learn with Java just how easy parallel programming is without a GIL.
Actually, most Java programmers I know that are fed up with it are moving to Scala instead. They really like their static type checking.
The "if it ain't broke" argument is reasonably pragmatic in the short term, but the long term problem with it is that the longer you leave something, the harder it is to upgrade it when you have no choice (e.g. your hundreds of servers go EOL with the manufacturer and you can't buy new ones anymore and your old OS doesn't support the new hardware you have to buy). If you can automatically deploy your software/configuration onto a server and then validate the deployment with some kind of acceptance test suite, you ought to be able to increase your "release" velocity pretty significantly and can be keeping much closer to the leading edge.
you know about it when the testsuite fails.
5) I'm handling a legacy java app in my company, deployment is terrible, took like 2 hours, jar dep hell and many other things. 15) http://docs.python.org/3.3/library/multiprocessing.html http://docs.python.org/dev/library/concurrent.futures.html under development: http://pypy.org/tmdonate.html
define "broke" :)
The benefit of tooling such as ... - super fast code navigation (reduced time to knowledge): - Go to declaration (what is happening in this method really) - Open Type (let me jump to that class real quick) - Find references (who calls this method?) - Fast API explorability (what other components are there, that are not covered well in the docs ?) - Integrated visual debugging without having to add "import pdb; pdb.set_trace()" to your source code - Guaranteed code refactoring to restructure your application without introducing unrecognized errors .. which reduce some of the perceived pain of developing in Java.
Damn, I was hoping you'd have a Python solution for the first one. I'm considering moving to Python, but deployment is just fucking terrible. I know about py2app,exe,freeze etc, but it just doesn't work for anything but simple applications with almost no external library dependancies. Multiprocessing, ok, I wanted to say multithreading for parallel programming, but I said what I said, you got me there. It seems like most of your woes are about some legacy Java software you had to maintain. Have you tried maintaing legacy Python software of similar size from the same era? (Considering that the languages were both released about the same time). If not, perhaps, 16) You'll learn that Java apps scale better and last longer.
It just happened to occur to me. Python 2.6 would no longer be supported after 1st October 2013, however RHEL 6 comes with Py2.6 which they plan to support till 2020; they are going to provide bug-fixes and backports for so many years. I can't imagine it!!
Yes unfortunately it is just a tool (it is actually part of the Stream API of "Project Lambda") and null will still exist, but the mere fact that it will be integrated to the core language (as well as the lambdas themselves) gives me hope for a brighter future.
http://www.reddit.com/r/Python/comments/1ak2tt/open_ended_question_about_ipython_and_its_usage/c8ybl4m
Most of my Python job is to implement the backend app server in Linux environment. With GCC setup + virtualenv, all is breeze. Even if there's no GCC, we normally use staging server then move it to the production server and all is still breeze. I don't really have experience in multi-platform desktop app both in Java and Python. So I think I should revise my answer. Please forgive my ignorance. However, based on my limited knowledge, Dropbox desktop client is written in Python and running well on all of the 3 major OS. Maybe we should pick their brain regarding the deployment and everything: http://en.wikipedia.org/wiki/Dropbox_%28service%29#Technology Lastly, 16) Java apps scale better and last longer -&gt; It's totally depends on the expertise of the team :). The same thing can be said that Cobol apps scale better and last longer. 
1. errr.... PyCharm can do that.... or maybe I miss something here 2. I always thought that Java's datetime is sucks... That's why there's Joda time. 3. Kivy, PySide/PyQT, WxPython, GTK+ also have quite decent performance 4. PyCharm can do that...
I've peek at Scala before, the complexity of it reminds me of C++. Last time I check, build time also longer than Java. Scala seems like a good research project but not for a mere programmer like me. Kotlin is ok, but maybe it will take around 5 - 10 years from now to get paid coding in it. 
PyCharm is far from ubiquitous amongst Python devs though, compared to Eclipse and Netbeans in Java. If PyCharm were more freely available, I think it would attain that ubiquity very quickly.
Isn't it free for open source developer? If you're developing commercial app, I think it's wise to spent some of your income for a good tools. 
It is, but I'm thinking about the availability as well as the cost. Eclipse and Netbeans are free and can be downloaded immediately, no cost and no application to prove your status as an open source developer or student. As such they (but especially Eclipse) have nearly complete adoption amongst Java devs. Python would, I think, benefit from a similar, free and freely available, high quality IDE.
Anyway, how about Eclipse plugin called Pydev, isn't it suppose to be able to do basic IDE refactoring?? (Our team didn't have experience with it, mostly just use either IDEA with Python plugin or PyCharm)
Eh? Pardon me... but last time I checked Java didn't have multiple inheritance. As of Python 3, python also have abc.
Hmm maybe this will help: http://hynek.me/talks/python-deployments/
I haven't tried the eclipse plugin, I use PyCharm! And I agree with you, I don't mind paying either, but it's the effect on adoption that's the problem. People, especially people first trying Python, tend to look for a free solution first and many stick with what they know.
Have you given [PyInstaller][] a try? [PyInstaller]: http://www.pyinstaller.org/
Certainly completely false for JBoss.
:)
Another one: http://stackoverflow.com/questions/2678180/how-does-dropbox-use-python-on-windows-and-os-x http://stackoverflow.com/questions/2741507/a-simple-python-deployment-problem-a-whole-world-of-pain http://highscalability.com/blog/2011/3/14/6-lessons-from-dropbox-one-million-files-saved-every-15-minu.html
My biggest pro: The java language helps your IDE a lot. Because everything is strictly typed the IDE can do some more advanced reasoning about your program. This can help you to a lot in some situations. My biggest con: In the land of strict OOP there are a lot of "patterns" needed to do things that are easy in Python. In a lot of situations where you want your code to behave differently in some situation you would create a new type or a new hierarchy. My teacher even went so far to say that for every switch statement he saw it was a (possible but very likely) sign that you were not building real OOP software. In Python I have noticed that a lot of those patterns can be avoided because almost everything can be assigned to a variable. The simplest example is if you have 5 different classes and you want to build them based on a string. In python what I would do is build a dict where I have those five classes and reference them by key. In Java there is no way that I know of to cleanly build a those 5 classes based on a string. Except by building a big switch statement. case: employeeType = "manager"; return new Manager(); That would mean you'd have to add an extra case statement for each new type. But in python I could just add them to the dict.
Every day I don't have to use Eclipse is a happy day
Perhaps a better choice is [Scala](http://www.scala-lang.org/) whch can use all Java libraries but has a dynamic feel and a more "pythonic" syntax.
OO and design pattern
Well, as the Python developer OP, I use Vim :) Do you consider Java on any kind of larger scale at all possible without a full-blown IDE?
In terms of deployment (I'm thinking in terms of package management), Python [knows it has issues](http://pyvideo.org/video/1731/panel-directions-for-packaging) (summary: PyCon US 2013 panel with all the guys behind Python's various packaging systems; all say, essentially "everything sucks right now")
No, except if you're masochist. Last time I had a bet with my boss that I can finish one Java module coded only with VIM on time (include the testing and everything). I was wrong and the pain still haunt me until now ... :x
Playing devil's advocate here... I wonder how much of the greatness of being able to easily jump around in Java to methods and classes is perceived as great simply because Java systems tend to be so enormous and labyrinthine. I'm the clueless OP, but that's how I've felt scanning through Java libraries, [this](https://github.com/Mikkeren/FizzBuzzEnterpriseEdition) being a hilarious parody thereof. I use Python in Vim, and with ctags and Vim's simple abilities, I can jump to definitions, follow call chains backwards and forwards, etc. The libraries are so small, I can even use grep/vimgrep to find instances of words and hop back and forth through them, finding what I want in a few seconds usually. It's not high-level call detection, but things *seem* to stay simple enough that it doesn't matter (I could simply be unaware of enormous Python based systems where these abilities would fall down).
Dicts are amazing. The number of thing I've solved easily and elegantly with them feels at times a bit staggering.
The example is a little questionable as it reads the whole file. I think the intent was to read the file up to the first blank line (i.e. the first paragraph), in which case you'd need to write `iter(fp.readline, '\n')` instead of the empty string. You might also use this if you have delimited items of some kind: &gt;&gt;&gt; items = ['foo', 0, 42, 0, 123.45, 'bar', 0, 12+16j, 'baz', 0] &gt;&gt;&gt; it = iter(items).__next__ &gt;&gt;&gt; while True: ... group = tuple(iter(it, 0)) ... if not group: ... break ... print('got a group: {}'.format(group)) ... got a group: ('foo',) got a group: (42,) got a group: (123.45, 'bar') got a group: ((12+16j), 'baz') This is rather clunky because creating the tuple swallows the `StopIteration` exception and returns an empty tuple, so you have to manually exit the loop when that happens. I'm not sure this is any better than using `itertools.groupby`: &gt;&gt;&gt; from itertools import groupby &gt;&gt;&gt; for key, group in groupby(items, lambda x: x != 0): ... if key: ... print('got group: {}'.format(tuple(group))) ... got group: ('foo',) got group: (42,) got group: (123.45, 'bar') got group: ((12+16j), 'baz') 
Yes, I could easily be called a dict-slut. :P
Ah, sorry, I actually did mean AI env, not "live boot". As in, I had a custom manifest and was using the AI server stuff. It was fun tracing through the installer with pdb though :&gt; The root pool I was trying to create was wacky. Massive big raidz thing with 16 disks. What's kinda' funny is that had I not known Python, I probably would have spent more time reading the docs and then eventually realized that you can only mirror root pools, not raidz them. But I did know Python, so my first reaction was, hey, I can pdb this! ;-)
**PLEASE** format your code! See here: http://www.reddit.com/wiki/commenting Then post in /r/learnpython.
Ever wondered what the abc module is there for?
Correct URL for google code is : https://code.google.com
I've recently discovered 'else' can be used with 'if' in list comps: [x if x%2 else None for x in range(1,10)] [1, None, 3, None, 5, None, 7, None, 9] 
There are multiple things you can learn from Java, like a decent IDE, however the main thing I'm personally concerned about is that in Java abstractions don't hurt performance. They do hurt in CPython, they don't on PyPy, so people would often write unreadable code to work faster on CPython, so as a PyPy dev I would like people to relearn that
You are already experiencing the described benefits, because you setup your tool support (VIM + ctags) to enable it. My list did not mean that the concept "fast code navigation" is only possible in Java. When I started Python, I used a simple text editor like TextMate and I guess a lot of starters go this route. In contrast to that, nearly all Java development is done in an IDE. Most starters will see the described benefits faster in java, because the IDE is nearly a prerequisite, whereas in Python, Vim + ctags is just a more comfortable way for advanced users. Because of that you are more likely to experience this benefit when starting with Java compared to starting with Python.
I cannot tell you how much I hate these naming practices and how happy I was when the playframework started to open people's eyes on how retarded they are.
I hate to be [that guy](http://lmgtfy.com/?q=python+get+raw+mouse+input+linux), but you obviously haven't tried much or researched the problem.
I do most of my python development in Eclipse/pydev. pydev does refactoring, but I haven't used it much, and I avoid having to tuch java when ever possible, so I can't compare it to refactoring java in Eclipse. One thing I do like about using Eclipse is with the right plugins it supports multiple languages, which means when I'm doing optimization work in python, it very easily to switch to C/C++ and wrap it with cython/swig and not have to change gears.
In two weeks it won't be. Yay.
doesnt work on osx ❯❯ python g.py /bin/sh: ip: command not found False
* Take up meditation or a religion 
Debian actually had a problem that has stalled progress on Python packaging (a boneheaded maintainer, and no way to bypass him). [LWN timeline](https://lwn.net/Articles/521027/) and [followup](https://lwn.net/Articles/496335/).
What would be the advantage of using ```str.translate(None, 'XY')``` over ```str.replace('XY', '')```? The second seems much more readable to me. edit1: The second method also works with unicode strings. edit2: Nevermind, I answered my own question. The ```translate``` method will replace any character given in the ```table``` attribute (e.g. ```'XY'``` replaces any occurrence of ```'X'``` or ```'Y'```) whereas the ```replace``` method only matches the entire string. If unicode chars are required, the best solution I can think of off the top of my head would be the following. my_str = u'aaXbbYbbX' for char in u'XY': my_str = my_str.replace(char, '') print(my_str) # &gt;&gt;&gt; aabbbb
With Python, I'm King Kong. With Java, I'm facing the 3 tyrannosaurids myself, with my hands tied behind my back.
Thanks for testing. Could you run in ipython (or something): import os os.uname() , and pass me the output? (or just tell me what the first part of the tuple is. EDIT: I have updated the gist, and think it should work. It's can really be a pain developing for multiplatform when you do not have the hardware, so I really appreciate your help!
your initializer in class Block is missing the double underscores and you're not calling the initializer of the parent class either. Should be: class Block(pygame.sprite.Sprite): def __init__(self, color, width, height): pygame.sprite.Sprite.__init__(self) # yada yada yada There's no method called `randomrange` in the random module. I think you mean `randrange`. If you set done to true or false write the loop condition as: while not done: #yada yada yada instead of: while done == False: #yada yada yada No spaces around parantheses, spaces around operators. Although it's correct don't write things like: while done == False: for event in pygame.event.get(): if event.type == pygame.QUIT: done = True additional lines don't cost a thing and this is more readable: while not done: for event in pygame.event.get() if event.type == pygame.QUIT: done = True This will not run: score +=1 print( score ) If you must have multiple statements on one line you need a semicolon between them score += 1; print(score) Score = 0 .... score += 1; print(score) variable names are case sensitive and usually lowercase. Uppercase is reserved for classes. And for the love of everything that is holy, format your code when you post it. 
I think the most important thing you can get from Java if you're a Python coder is concurrency. Java has a great concurrency architecture. Clear and basic concepts (mutex, locks, etc.) that are good things to know when you start. There's a project called Akka that adds better concurrency features based on Actors. Said that. I don't recommend anybody to go from Python to Java. If you want to get something from the JVM i'd recommend you Scala.
I have a large ten-year-old Python codebase that my team developed to manage data warehouses. We were new to the language when we wrote it, so there's little use of sophisticated features. And a lot of short-cuts that are a little embarrassing today. For the most part it's very easy to manage the code.
That `methodName(String argumentName)` or `methodName(int argumentName)` tells you a lot more than `function_name(argument_name)`. Also, that Scala exists. And it is quite nice...
 for num, time in times.iteritems(): command = 'relog ' + blg_name + ' -f csv -o ' + blg_name[:-4] + '_' + num + '.csv -b ' + date + ' ' + time[0] + ' -e ' + date + ' ' + time[1] subprocess.call(command.split(), shell=True) FAIL. Completely misuses the subprocess module. for num, (stime,etime) in times.iteritems(): output = "%s_%s.csv" % (blg_name[:-4], num) begin = "%s %s" % (date, stime) end = "%s %s" % (date, etime) command = ['relog', blg_name, '-f', 'csv', '-o', output, '-b', begin, '-e', end] subprocess.call(command)
I agreed wholeheartedly, I simply meant that if a pythonist is curious about java (or needs to use it for some reason) Scala may be a better option than pure old java.
/r/learnpython is that way ---&gt;
the `abc` module is pretty bad, because it reuses subclassing (which already has a meaning) for the purpose of interface declaration (which is a different thing). `zope.interface` does a much better job here.
And that is why windows can not be called a good OS for engineers: you need to learn a python module, just because M$ still did not manage to add some posix-based shell.
Ah, well, I could have done this is Powershell, but I chose Python to handle the logic because I'm a fan of it, and for the sake of learning the module. Having said that, I still don't rate Powershell as highly as either the shell in Linux, or using Python for handling the logic as I have done here.
To be fair, there are a lot of situations where static type checking is nice. Why trace down silly mistakes by hand when the compiler does the work for you? Other times however, you can end up in crazy generic or "cast it all to object" hell... It just depends on the situation. Sometimes I wish python would do static type checking, and sometimes I wish Java/c#/etc would just move aside.
Perhaps I'm misinterpreting what you're saying, but as far as I can see, you're just forming the command string in a slightly different way?
Or Clojure. Learning Clojure made me a better Python programmer because thinking about things in a functional/immutable way is often a good alternative to mutating objects in place.
I think it's appropriate to make the distinction between the language and it's ecosystem, although arguably OP's question is vague enough that it could mean one or both. As a language, Java will emphasize learning to code with: * strong static typing * object oriented design/development emphasizing formal Design Patterns * global package management * multiple abstraction layers * enforced insulation from the underlying environment 
&gt; one class per file for instance That's a huge improvement over working with C++ code where you can't infer where anything is defined. And it's actually one public class per file. You can have other classes in the same file but they can only be referenced by either that file or that package, I forget which.
Yes, the proper way.
http://youtu.be/lKfupO4ZzPs
agree with your approach to formatting the cmd string disagree with your approach to criticizing someone's work can't stand subprocess anyhow, would use envoy instead
Here's a snippet of how to do PowerShell counters to CSV (every 5 seconds). $counters = @() $counters += @("\Memory\Available MBytes") $counters += @("\Memory\Pages/sec") $counters += @("\Memory\% Committed Bytes In Use") $counters += @("\Network Interface(*)\Bytes Total/sec") $counters += @("\Network Interface(*)\Output Queue Length") $counters += @("\Network Interface(*)\Packets/sec")` Get-Counter -Counter $counters -Continuous -SampleInterval 5 | Export-Counter -Force -Path c:\temp.csv -FileFormat csv
will these be available to people who can not attend?
So either you work for the same company as me, or we had the exact same problem with the exact same solution. Our app is on a no longer maintained version of a no longer maintained web framework, and has some bytecode dependency issue. We are finally testing the Python 2.7 version, were we just decided to maintain the old code in house until we can get rid of our dependency on it.
Discussions like this make me want to stay far away from Java. I'm sure it's not all bad, but I hear more horror stories about Java than any other language...
I've been using PyPNG for image processing, but I recently started rewriting my module in C, with wrappers for libpng and libjpeg. PyPNG is actually fairly fast for being pure python, and will get you off the ground pretty quickly, but it just doesn't suit my needs anymore (I'm doing image processing on directories filled with hundreds of images).
This might be a good question for StackOverflow if you haven't asked there already.
As someone who just recently started trying to learn Python, thank you. My first initial reaction is to go to the first subreddit that might come to mind. I figure there's a bigger chance that /r/python exists over /r/learnpython. But, thank you. 
Hahahahaha. No. Not for weeks. And they will be grainy and hard to follow. Pyvideo.org will have them though. Maybe. 
It becomes easier if you need to call from it a lot. It moves code into data, which is always nice. Now everywhere you need to access things, it's just `myDict[key]`, instead of yet another full switch/case block.
I've used App Engine with Python and was impressed. **Pros:** 1. The webapp framework is easy to work with. Not just webapp, mail &amp; xmpp services, google accounts integration etc. are really useful and easy to use. 2. If you would rather use django, it is available with App Engine. 3. Good number of Python modules (viz numpy, pycrypto...) available. 4. Free tier is good enough for prototyping. Pricing is according to resource usage, not uptime. **Cons:** 1. Your application is kindof restricted to App Engine. Developing using django might be a solution. Deployment in an EC2 instance, Azure etc will offer more flexibility in terms of technologies to use and in terms of portability. 2. I am not a fan of GQL (the query language). NDB (a newer replacement) seems to be better. This may be a pro or a con depending on you: No need of sys admin work with GAE. Google does it for you.
They will be recorded, not sure when they will be uploaded to the web. Our video team last year didn't work out super well, but we have a new team this year. I encourage coming to meet other folks and interact in real time. 
I would love to attend that conference. However being a student in Michigan does not allow for many cross country trips... lol
Use ipython. http://stackoverflow.com/questions/5740835/how-to-use-pipe-in-ipython
Fixed!
&gt;I encourage coming to meet other folks and interact in real time. Would that I could! I look forward to the videos though -- will you post the videos in this subreddit when they're out?
Can you put constructors in them?
yes django sucks on appengine. that was my point about frameworks in my comment. third party modules aren't a problem though if they are pure python. &gt; zero transparency into downtime you mean this? https://code.google.com/status/appengine &gt; extremely high cost that's because django is inefficient on appengine. 
As others have mentioned, I got scared away from AppEngine when google radically changed their pricing model probably 2 years ago, and I realized that I had no alternative other than either live with it or change platforms. I'll never endorse a platform that I can't install on my own hardware ever again. At the time AppEngine also had probably the worst track record for downtime among the "big" cloud players -- I stopped following it so I don't really know if that's a problem anymore.
sub classing feels more natural to me and the abc module is really simple to use and to explain. 
Support for grouping functions is coming soon! meanwhile you can only parse expressions used in '$project' and the value of the group functions. 
Using the Python included with OS X 10.8 (CPython 2.7.2): ('Darwin', 'thor.home', '12.3.0', 'Darwin Kernel Version 12.3.0: Sun Jan 6 22:37:10 PST 2013; root:xnu-2050.22.13~1/RELEASE_X86_64', 'x86_64') Looking at your script, you probably want to use the full path to netstat (`/usr/sbin/netstat`) to remove any errors that may come from a munged PATH. Also, checking through the docs, you probably want to check against [`platform.system()`](http://docs.python.org/2.7/library/platform.html#platform.system) (which is `'Darwin'` on OS X).
nothing about programming is natural. and it's not simple to explain, because it makes `isinstance` mean two unrelated, often opposing, things.
WingIde, Eclipse/PyDev and several other IDEs, free or expensive, can set breakpoints, find code usages etc PyQt/PySide is more than fast enough for most uses as long as you optimize your code
No. Why?
with experimental py.test support SWEET!!!!!
Yo, AT(errel) You taking bitcoin this year??
*an ancient frankenstiened version of Django.
Points 1, 2, 3 are especially painfully in a commercial environment. It looks that "predating" from java could in order... a bit. Point 5 is not a showstopper but I agree it's a huge pain, sometime. Point 4, 6 and 7 My experience is quite the contrary. Once the total time (development+execution) is taken into account: they are fairly comparable. Beside python can scale, just not in the same way java scales. Point 8 yes I agree but that's true for quite a lot of opensource done on volunteer basis: I'm grateful it exists. Nevertheless there are industrial strength commercial application written in and for python: they are just not as much as visible and they aren't by any measure crappy. &gt; I'd much rather write a large, enterprise app in Java than in Python Nope I can't disagree more on that! Large enterprise python apps are already written in real world as today, just not very much advertised ;) PS.Enums in python... it looks like they might happen :D 
Not the author, but I'm sure there will be confusion about the name. Yes, it is titled "Unicorn" (like the Ruby server or the GUnicorn library"). Yes, it is using the same image Github uses. They said they received permission for this. I do not agree with the name, and hopefully they will change it. So far, I'm not sure what benefits this has over the Requests module.
use substrings. example: print mystring[:100] should print the first 100 chars. try with [-100:] for the latest 100 chars.
Ok. Though if your willing to mull bitcoin over and are looking for a payments processor [bitpay](https://bitpay.com/) converts bitcoins to usd instantly for merchant accounts. 
no ipython, just a standard install...like poster below, it looks like it cant find netstat to set default_gateway after the update.
GAE is a total load of shit...you might as well go back to geocities. Try heroku.
I know that, but after [:100], for the next one, do I have to do [100:200]? That's kind of annoying. Some ugly hack could be for i in range(100): print(data[100 * i : 100 * (i+1)]) rawinput() But I wonder if there is a prettier way to do that. 
 import pydoc pydoc.pager('\n'.join('Hello World %d!' % x for x in xrange(200))) :)
Thanks! 
[Kivy](http://kivy.org/#home) may assist if you want to consider a framework 
Have you checked out [PyGame](http://www.pygame.org/)? They have a few tutorials on their website.
Their main page should answer the obvious question: why should I use you over requests? (and wtf is up with the unicorn theme? is this some kind of an april fools joke?)
I gave a tutorial at PyCon this year: http://pyvideo.org/video/1718/introduction-to-pygame The source etc. for the tutorial is in bitbucket so you can play along: https://bitbucket.org/r1chardj0n3s/pygame-tutorial
i have integration tests that take approx. 20 minutes or more. 
This is probably the shallowest possible criticism of java that you could possibly come up with. 
I tried Heroku this week end. I was *really* impressed with it. And as far as I tried, no custom development was required at all. 
Thanks, that's good to know! This is where my ignorance comes in since I've never done full scale web stuff with python.
The fact that everything is shoe horned into a class no matter what should stand out way more the semicolons at the end of every line.
I guess. It just means an extra line of code. I never bothered with multiple classes. I mean, I know how to make them, and call them within other classes, but I never found the need for them.
I know performance isn't the main driver here, but I'm curious as to how it compares to cPickle. Is it in the same ballpark? I suppose I should just download it and see...
I think this is a very good point. I saw a discussion today comparing JSON and XML as data transport formats. Someone said that JSON was lightweight, schemaless and hard to verify but easy to work with; XML comes with a great variety of schema and query tools but is rather heavyset. Reading this discussion, I see a parallel. JSON is lightweight and working with it is fast and easy; It's compact or dense, and it translates exceedingly well into very simple data structures. XML can really only ever translate into a nebulous "object" whose meaning has to be defined externally in namespaces and schemas.
Weird homework assignment, but would be good for a bash/shell class I suppose.
We have started looking into that possibility, but have not come to a conclusion yet. Will post here if we do.
this was exactly what I was looking for. 
Is PyQt still actively developed? I used PySide but I think I will switch to PyQt.
 python yourfile.py | less
As /u/jeebal suggests, you can use the built-in csv library for python to parse the data, but for exporting to a Word document you'll want to look at connecting to it as a COM object using PyWin32. You'll have to learn some VBA, but it isn't that bad.
jeebal has the correct approach to reading csv files in. It might also be useful to review simple file io in python: [file input/output](http://docs.python.org/2/tutorial/inputoutput.html) It's not impossible, but also not terribly easy to get data into Word docs with python. [guide/tutorial to pywin32](http://www.blog.pythonlibrary.org/2010/07/16/python-and-microsoft-office-using-pywin32/) Word piece is about halfway down. searching google for python docx also turned up this stack overflow thread which might be helpful. [thread](http://stackoverflow.com/questions/188444/reading-writing-ms-word-files-in-python) I would probably not output to word at all, but either html (perhaps using templates) or something like latex. Html would be quite easy, you could even embed css/styling with the html to get various font sizes and styling. You could also go straight to pdf if printing is the desired end result. I think reportlab is the generally accepted starting place for pdfs. Reportlab/Platypus is the easiest way to get going: [tutorial](http://www.reportlab.com/apis/reportlab/dev/platypus.html)
To throw out a slightly different method. The pandas library has a .to_latex method associated with DataFrames which can output the DataFrame as a latex table. I'm not sure on how to get this into word however though. Example usage: import pandas as pd df = pd.read_csv(csv_fname) df.to_latex() # Whatever the latex options are you want to use
Use http://cocos2d.org/ It's better than pygame (really, it is) and has a tonne of high level features. You almost certainly do not ever want to use GTK. For pretty much anything. The only thing going for it is that it's cross platform, but it's also: Slow, hard, ugly. Don't use it.
Interesting stuff. Can you provide some more links with your work?
You might also be interested in the (free) Computational Investing course at Coursera.org. Lots of people interested in the same kind of stuff! https://www.coursera.org/course/compinvesting1 
&gt; Most stuff in bash is done by calling external programs. That is true for any command-line shell. Also, the assignment given to OP is solvable entirely in bash! ^^P.S. ^^I ^^thought ^^it ^^was ^^a ^^pretty ^^good ^^analogy...
Parse the CSV and do whatever you want with it, then just write it out to a text file (or html/latex) and put that into Word if you really need it in Word.
You could do that with a map. I do see what you are saying, and that is something I missed when I first started java. The switch statement isn't the only alternative.
Yes, you will have to wrap it in a class. Although in java 8 you can probably get closer to what you are asking.
Or just output a docx file... it's XML, and there are python libraries to handle it, though like duallain said there are simpler formats to use (HTML, PDF, RTF, etc.)
I have played with ystockquote too and written some of my own stuff. But what have you done? Or what exactly will you do? Make a new website?
Ha! Agreed! But maybe the OP isn't so ... ummm ... discriminating(!) as you and I. :-) 
Another vote for pygame here. The documentations a little lacking, but if you download some of the prototypes on pygame.org and check their code and do a little bit of googling you will be up and running in no time. On advice from a friend I got my own basic 2d tile engine up and running with no prior pygame knowledge in four or five hours, and hes got some excellent isometric tiling and turn based stuff done himself, and he only started learning python about two months ago Im at work at the moment, give me a few hours and Ill edit this post with a link to some of my own code and some other things to help. Some of the tutorials are much better than others.
Also, the course is a bit too dependent on their software for my taste. I like the homeworks though, they are quite interesting. Will you take part 2? Im really interested in the machine learning part of it. By the way, yesterday a course in machine learning started, in case somebody is interested.
I'd also recommend using python-requests instead of urllib2. Download from: python-requests.org
+1 to cocos2d (with the same pyglet for beginners caveat above) - it adds a bunch of stuff that's very useful for writing games. I really must finish off and commit the changes I'm making to the TMX loading to handle object layers...
&gt;Can you skip an integration test, though? yes I can &gt;Integration tests are one of the things I use Jenkins for. and so do I and my team. and yet it would still be useful to me to get a heads up on what of those tests I just broke before pushing rather than later when I've switched context to something else. even knowing whether or not the rebase or merge with remote before pushing back breaks anything would be beneficial.
&gt;despite being one of the largest apps running on GAE at the time. do tell. 
In the video, you discuss the issues associated with differential financial sources being delayed and representing different data. You did not describe how you propose to ameliorate or remedy the solution using python, can you do that now in one sentence?
Very good idea !
I used [python-docx](https://pypi.python.org/pypi/docx) and [python-pptx](https://pypi.python.org/pypi/python-pptx/0.2.2) to good effect with pyodbc a few weeks ago. The built in csv reader should be fine.
I love how she keeps trying to get the mic back from you. (Also, awesome presentation and thank you for sharing)
Depending on what you want to do with the data, loading it into a [pandas](http://pandas.pydata.org/) DataFrame may be more convenient.
Sure it is http://www.riverbankcomputing.com/news
For correction, pandas has nothing to do with creating charts, but the rest of your statements I agree with!
Thanks, I'll look at it.
Somewhat tricky on anything but Windows.
I do not know the exact setting in which these presentations are held, but I highly doubt that counts as a poster ;) To the left I see something a bit more in the right direction, but even in stunning 480p it can be easily seen that that particular poster has a way too small font. But on a serious note, making good posters can be hard but can be both beneficial to you and your audience. I seriously recommend looking into it.
I assumed that he was using Windows. And while docx works in LibreOffice, just yesterday I was having trouble with it not preserving formatting when it exported. If these reports he mentioned have complicated formatting he might have difficulty preserving it, which can be a big deal to some companies. And I wouldn't suggest dumping to an XML because unless there's a library out there to do it he would have to learn all about the XML for making word documents, including formatting options and more. That kind of solution is hard to learn, hard to maintain, more code, and less intuitive. If you can use named methods without having to roll your own library it makes the code easier for someone else to understand.
Check out https://github.com/mikemaccana/python-docx
To do complicated document formatting I use an ODT template, with a headless OpenOffice renderer running on a server, and [APPY POD](http://appyframework.org/pod.html) which let's you use Python variables and an abridged syntax to render complicated documents. This pairs with the renderer, which can be told what to output (.doc, .pdf, .html, etc)
That, and pyglet reeks of a dead project...
That's what I mean, there are a number of docx python packages out there that are made to format data into the xml docx format. You don't have to know anything about docx. 
I've written this type of pipeline with operator overloading a few times for DSLs, and people do it pretty often in Lisp with macros. It can be useful, but not necessarily clearer, and brittle as soon as you get away form standard types. For example, can you create a function with your pipeline where the first input isn't initially given, and provide it later? What happens if you use a generator for input, is the pipeline a generator? A good portion of the examples can be accomplished on the fly with something like def pipeline(*args): return reduce(lambda a,b: b(a), args) 
Yeah we create reports in Word with pre-defined fonts. I want to transfer data based off of column headers to a certain font, header size, etc in our template.
And that is just why I mentioned it as a con. In Java the answer to almost every problem is make new classes. Whereas in python I could simply pass around an object (functions are objects)
I have yet to hear your explaination why django is inefficient on GAE. And you then make another low effort comment, demanding /u/suprshreddr explain more. Put up or shut up first.
Indeed it's a case of "No thanks we already standarized on Requests"
&gt;python 2.6, 2.5 or 2.4 (2.4 will need ctypes and elementtree) maybe im missing something ehre but to me it seems that neither 2.7 nor 3.3 are supported. makes you wonder if there is any future to this project &gt;It's better than pygame (really, it is) as someone who has used pygame for a few projects i would be interested to know what i am missing out on.
Great, thanks! Then I will move to PyQt. Fortunately there is no big difference between PyQt and PySide.
 [ ["Hello,", "world"] ] -&gt; string.join -&gt; print Why are there two levels of parentheses here? 
Thanks for the feedback. It's a general purpose programming language because it's not made or designed for a specific task or domain. Also, I'm not using operator overloading.
By default, Pythonect takes iter-able objects such as: lists, tuples, and etc. and iterate them, mapping each element in them to a different thread. So: [1,2,3] -&gt; print Will print 1, 2, 3 (and not [1,2,3]) To override this behavior, (to escape the automatic-map-reduce) for a given dataset - I have introduced the double parentheses. So: [[1,2,3]] -&gt; print Will print [1,2,3]
Not sure I follow. Can you give a small Python example?
Im taking the intro to python programming class on coursera. Idk if you are talking about the same guy but the music cracks me up everytome
i think it would be easiest to import from csv, do your magic in python, and then export to an excel file. There are plenty of libraries that let you do that (pandas, openpyxl, xlwt), and they even offer styling options. Then from there convert the excel file in a word file, which seems much easier than writing and styling a word file directly 
also, for your tone, you can suck a fat one. 
In non-pipeline syntax: from itertools import * list(islice(imap(lambda x: x ** 2, count()), 10, 20)) 
this really needs the slides to be useful. 
Way ahead of you ;-) % ls ~/src/sol11/usr/lib/python2.6/vendor-packages/ IPython g11nsvc osol_install solaris bootadm-helper.py libbe.py pkg solaris_install bootmgmt nss rad terminalui dtd orca sol.tar zfs Grabbed a copy of everything to peruse locally once I saw how useful it was ;-)
For complex programs, wouldn't the parentheses become quite confusing? Could there be a prefix implemented that would result in the same behavior? Something like: ni[1,2,3] -&gt; print Or maybe a different operator: [1,2,3] =&gt; print
Yes you can, but currently there's no Pythonectic way of doing it. It's more functional programming style: See: sum(`range(1,10) -&gt; _**2`) -&gt; print is: print sum([n**2 for n in range(1,10)]) 
(somewhat vague answers as I'm not familiar with cpython specifics) &gt; Are any checks performed before running the code Yes, there's a formal Python grammar that is used to parse the code. If it fails to parse with this grammar things blow up (the SyntaxError exception). This catches stuff like mixing up the single = assignment with the == equality, indentation errors, unclosed parenthesis and other stuff. &gt; Are these checks performed before or after the conversion to byte code The grammar is used to create an abstract syntax tree. You can peek at this with the AST module in the standard library. The AST is used for certain optimizations. For example, the expression x = 2 + 2 contains a + operator with two children, each with the constant 2. Python can traverse this tree, see that both operands are constant and optimize the 2 + 2 part into the result of 4. &gt; Is calling this "compiling" to byte code incorrect Later, the AST is compiled into bytecode for the Python interpreter and stashed in pyc files if it's a module. See the dis module. Calling this step compilation is most correct but calling the entire thing compilation isn't wrong.
Maybe with other languages like Java Unicorn can be good, but now I'm pretty confortable with Requests too. The name tricked me: I thought Unicorn, the server, was expanded to use with other languages like uWSGI do.
The most common one is CPython, not Cython.
Doesn't support Python 3.
* http://sphinx-doc.org/rest.html#directives * http://docutils.sourceforge.net/docs/ref/rst/directives.html#csv-table * http://docutils.sourceforge.net/docs/user/links.html#openoffice
How do you handle errors in a pipeline here? Is it possible to branch out and handle them, or jump in the pipeline or somesuch? http://en.wikipedia.org/wiki/Hartmann_pipeline
It's a nice demonstration, but utterly useless since all of these features already exist.
You can avoid repeating the class name with metaclasses, though it might be easier with macros. (NOTE: python3) from collections import namedtuple class NamedTupleMeta(type): def __new__(self, class_name, parents, attrs): if parents: #don't do this for intermediate tuple assert len([attr for attr in attrs if not attr.startswith('_')]) == 1 return namedtuple(class_name, attrs['fields']) else: return super().__new__(self, class_name, parents, attrs) class NamedTuple(metaclass=NamedTupleMeta): pass # - create named tuple - # class MyNamedTuple(NamedTuple): fields = ('field1', 'field2') 
No they don't
A bit too verbose to my taste compared to the ideal: MyNamedTuple = namedtuple(field1, field2)
We've thought slightly about making something more general: algebraic data types, like in haskell/scala/F#. Haven't decided what they'll look like yet, but they'll basically be namedtuple with a few extra goodies
shelve is terrible. don't use it, we have sqlite now.
Thanks for catching it. Updated &amp; have an upboat.
Always nice to see these kinds of experiments. Are you familiar with the idea of quasiliterals, as seen in [E](http://www.erights.org/elang/grammar/quasi-overview.html) or [ECMAScript 6](http://wiki.ecmascript.org/doku.php?id=harmony:quasis)? I feel like they'd be a valuable addition here, especially since there's no other good use for ` in python right now.
I hate getting segmentation faults with python&amp;opencv. I'm just fine with numpy/scipy and their jolly stacktraces thank you very much.
This would really be nice, if you can do it before the next Ludum Dare. Thanks.
1) Actions. Actions are higher level constructs that can easily be chained in sequence or run in parallel to perform object attribute changes. For example, scaling, rotating, moving, etc... and cocos2d will automatically interpolate the changes based on the framerate. In pygame this requires a lot more code. 2) Performance. As pyglet is base on OpenGL, the performance is much better. You can have thousands of objects and particle systems (several are built-into Cocos) running on screen and animating without the hiccups I usually see with Pygame. I've used both, Cocos2d is always smoother.
PEAK-Rules actually uses backquotes for a similar purpose in its [syntactic pattern matching](http://peak.telecommunity.com/DevCenter/PEAK-Rules/Syntax-Matching) features. Unfortunately, backquotes are no longer valid syntax in Python 3.
It was pony orm. Looks interesting.
There are .pyc files and IDE garbage commited in the repository.
sounds good, might give it a shot if i start a new project that benefits from those features. (if cocos supports 3.3 by then)
and python itself
I have to agree. Macro generation is one of those giant, complex, complicated, unreadable things that don't really mesh well with python. That said, for anyone who wants to use them, knock yourself out. 
old versions of django is 80% of the appengine sdk 
could you give an example of what you are trying to do or a subset of the data?
Why do people have a lot of high hope for PyPy?
Not sure how that matters.
Lexical scoping would go a long way too, shadowing a macro when a variable existed in a nearer scope. This looks brilliant; excellent work.
answering your own question ?
I found this after I posted my question and I figured others who stumble upon this might like to see it :D
I use python to create web applications, and command line tools.
Just clearing up what seems to be a common misconception: Macros are in no way substitutable for runtime reflection, nor is runtime reflection substitutable for macros. There is a lot of overlap in what they can do, but there are some things you can do with macros that is [basically impossible](https://github.com/lihaoyi/macropy#quick-lambdas) to do with runtime reflection. I don't care how many metaclasses or decorators or descriptors or lambdas you use, or how much stack introspection you perform! You're not going to be able to achieve the same functionality without using macros, certainly not in [26 lines of code](https://github.com/lihaoyi/macropy/blob/master/macropy/macros/quicklambda.py)! EDIT: and with zero runtime overhead too!
Nope, it's not the same one. This one is about using computational tools for investing, but it is heavily based on python.
Semi-related. I was trying to do some Twitter parsing a few weeks back, and found two libraries (both with the same generic "twitter" namespace too). They both seemed roughly as popular too. Does anyone have any suggestion on which one is better? [python-twitter](https://github.com/bear/python-twitter) and [twitter](https://github.com/sixohsix/twitter). I ended up using the first one since it showed up higher in search results, but then on Github, the 2nd one has a lot more stars and forks. Furthermore, the one I used didn't really follow pep8 naming, but the API looked a bit more straight forward. Does anyone else have any opinion or criticism about either?
It's not dead, but could use additional contributors with specific platform knowledge.
I can't guarantee that, I'm afraid.
if you do not require authentication, you do not need any lib to get/parse twitter. see a console twitter feed in bare python3: https://bitbucket.org/rndblnch/twitter-feed/src/tip/feed.py
Computer sends the code to Guido's email, he responds really fast and you get the output. Python is generally an email client and a print command. 
Some say Guido has developed a program to automatically process the emails and create the output, but that's just crazy talk.
 Since I'm am advocate of the new string formatting; the following is more beautiful than dict unpacking: world = ... result = "Hello {world}".format_dict(locals()) #or vars()
App Engine -- don't. I've used Google App Engine for two clients, per their request. App Engine is like that hot girlfriend, that you don't realize is crazy until it's too late. Easy to get into, but hard to deal with, and impossible to ditch. The Python environment isn't too difficult to work with, especially if you don't want to be bothered to do sysadmin work. However, in dealing with the datastore, you will soon realize you've made a huge mistake. I never felt like my data was safe. It was damn near impossible to transform, move, or export. You don't have the luxury of using the infinite amount of linux tools out there. You have to sift through multiple versions of backwards incompatible documentation to figure out the current API version's way to write a mapreduce, so you can do a simple schema change. [The Unofficial Guide to Migrating Off of App Engine](http://www-cs-students.stanford.edu/~silver/gae.html) doubles as a good article against using app engine. Despite being 2 years old, it's still very relevant today. My advice -- stick with tried and true databases that aren't proprietary. Any flavor of SQL or maybe MongoDB/CouchDB if you want to get frisky.
latest will alias to the newest version that is explicitly listed, so 1.4.
Mmm... I think you'll find the 1.2 version does.
How stable is the AST data structure / API across Python versions and implementations? Is it hard to run the same code on PyPy and CPython 2.x and 3.x?
Oh great, I've always wanted a copy of the NumPy Beginner\&amp;amp;#8217;s Guide.
You might checkout EuroSciPy in Brussels in August.
well? let's hear your rebuttal smart guy :)
Right now an Exception will stop all the flows. I'm not sure there's a place for Exception in flow-programming, perhaps it should be treated as False, i.e.: [1, 2, 'a'] -&gt; int Will return: [1,2] Or: [1,2, False] 
how is this related?
After seeing your childish comments, I have no interest in dealing with you. Admittedly, you finally came up with a reason why Django doesn't do well, but they're a variant of the "Google's DB backend isn't good" statement. You elaborate on a facet through claims that the ORM is not properly adapted, so I grant you that. Your attitude stinks though and never will win you any respect, only grudging acceptance. But your kind needs only the latter to get by, so no worries there. Cheerio Old Bean.
PyPy can run any idiomatic 2.5+ code. A python 3 branch is in progress.
No, no. It's resting.
&gt; Python parses your script and converts it to bytecode (although this is just an internal implementation detail of CPython, not part of the language) That detail has various important consequences though, such as the classic: x = 10 def f(): x += 1 print x Also, the way closures work. I mean, if you know that Python gets compiled to bytecode (and something about how it gets compiled), then stuff like this is logical.
Newcomers that need to do anything at all complex often find simple ORMs frustrating and time-consuming to work with - since mildly complicated models or queries can require nasty work-arounds. SQLAlchemy exposes multiple layers - so you can work with legacy models, you can write SQL by hand if necessary, and or you can work with an extremely full-featured ORM if you prefer. I think that's a better approach. 
I haven't used the package in your first link, so I can't comment on its usefulness. Sorry! But I have used [Tweepy](http://michaelmartinez.in/playing-with-tweepy.html) which is pretty popular as well. The second link in your post is for SixOhSix's `twitter` package. This is the package/lib I explore in the post and subsequent iPy Notebook. I guess the 'real' name is `twitter` as that is how you install it via pip. Python Twitter Tools is the 'long version name', I guess??? It is confusing... I was in a similar position, in that it wasn't clear if one was better implemented or whatever. So I just started installing them to have a play... 
I don't understand the significance of the example, would you mind elaborating? 
Awesome... The only problem I see is that [you](https://bitbucket.org/rndblnch/twitter-feed/src/tip/feed.py#cl-7) are using [API ver. 1](https://dev.twitter.com/docs/api/1/get/search) which has a big-fat depreciation warning. 
Only the coolest thing since sliced bread my friend. See here: http://ipython.org/notebook.html
Cheeky reply on your cake day? Have an upvote! 
And the tanks!
As a newcomer it's definitely not a better approach. For a pro, I'm sure it's better than Django's ORM though.
How about a subprocess helper, something akin to what the [Julia](http://julialang.org/blog/2012/03/shelling-out-sucks/) language has?
There seems to be an issue with the captcha for me.
Psst, you don't need cat for that -&gt; `less 500.txt`
Such an informative article and very useful discussion. Thanks everyone!
It depends: * are you a newcomer to ORMs but experienced with relational databases and SQL? * are you a newcomer that needs to support a legacy data model? * are you a newcomer that needs to support complex queries or data models? * are you a newcomer building an app that is expected to grow in complexity and performance challenges over time? I'd say if you answer 'yes' to any of the above, then SQLAlchemy may be the better solution for you.
http://pythonhosted.org/sphinxcontrib-exceltable/
That would be possible, but I *think* you do not need macros to do this? I suspect a decorator, e.g. @forked def my_func(arg1, arg2): ...dostuff... return result could be made pretty easily, which would wrap any call to `my_func(arg1, arg2)` in a forked subprocess which exec's the `code` attribute of the function, shipping the `code` along with the pickled arguments to the subprocess and returning the pickled return value from the subprocess. 
While I don't exactly understand why you're manipulating strings like that, I'm sure there's a reason. Python already has the bit-manipulation operators so I'm assuming you're working on some abstracted form. Why use an 'out' argument instead of returning the result? Also, `out` is never initialized, it appends its result to whatever you pass it. This may or may not be desired behaviour. For `range(0, X)` you can omit the `0` and just be `range(X)` However, many of these are...not right... For example: if a[item] or b[item] == "1": the `==` binds tighter than the `or`, so it means (`a[item]` is boolean true), *or* (`b[item]=='1'`) **not** that (a[item] or b[item]) is equal to '1' I see similar mistakes thru the rest of the methods. And remember, the string `"0"` is boolean `True`. So, for example, in your AND method: if a[item] and b[item] == "1" If `a[item]` was `"0"` and `b[item]` was `"1"` then it would give you incorrect answer. "0" is True. The expression in XNOR: `not a[item] != b[item]` I would probably do as `a[item] == b[item]` to avoid the double negative Homework assignment? 
Yeah, definitely very confusing. I had the first one installed on my PC, and wanted to get my script working on my laptop, so I went pip install twitter, and it ended up downloading the other one, and since they both use the same namespace, it messed up stuff. Tweepy's documentation seems to be really lacking. I can't find anything about not authenticating, which is what I was doing. I just wanted to quickly pull someone's timeline.
Fantastic!
That has nothing whatsoever to do with the use of bytecode as an internal representation. The [execution model](http://docs.python.org/3/reference/executionmodel.html#naming-and-binding) states that any assignment to a name not named as `global` binds that name to a local variable, and that applies to all uses of the name in the function, not just those that occur after the assignment. Since `x += 1` is notionally equivalent to `x = x + 1` it's an assignment, causing all uses of the name `x` in the function to be bound to a local variable, not the global variable of the same name. But to evaluate `x = x + 1` requires first to read the contents of `x`, which as not yet been assigned a value, which is an error. At no point did bytecode ever enter into the explanation, as it's irrelevant. In fact you get the same `UnboundLocalError` exception raised if you run this in, say, PyPy, where it's JIT compiled rather that converted to bytecode. 
Hey. I've been doing some Twitter data gathering myself using the sixohsix Twitter package but I'm having some trouble. I'm using Matthew Russell's "21 Recipes for Mining Twitter," which uses the sixohsix version of the "twitter" module. Here's an example of a script where I'm getting an error that's related to the api.py script within "twitter." Or is it something I've done wrong? My-MacBook-Pro-2:Recipes-for-Mining-Twitter homefolder$ python2.7 recipe__analyze_users_in_search_results.py Traceback (most recent call last): File "recipe__analyze_users_in_search_results.py", line 61, in &lt;module&gt; sn2info, sn2location, sn2tweet_ids = analyze_users_in_search_results(t, Q) File "recipe__analyze_users_in_search_results.py", line 16, in analyze_users_in_search_results search_api.search(q=q, rpp=results_per_page, page=page)['results'] File "build/bdist.macosx-10.8-intel/egg/twitter/api.py", line 173, in __call__ File "build/bdist.macosx-10.8-intel/egg/twitter/api.py", line 198, in _handle_response twitter.api.TwitterHTTPError: Twitter sent status 403 for URL: search.json using parameters: (q=&amp;rpp=100&amp;page=1) details: ?VJ-*?/R?R??/U?--.QH?+I-RHT(,M-??S?????kw# Thoughts?
Ha! Thanks for posting this. I'd run into the same problem, and even diagnosed it to the point of finding that the heap walking functions were what were causing the delays, but then I dismissed it as something that must be particular to my circumstances.
HAL... your problem may be that you are lacking proper oauth to access the API. A status of 403 means access forbidden. The first thing you need to do is read the recipe_analyze_users_&lt;etc&gt; to see what it imports; It imports the oauth recipe among others. In the oauth recipe there is an area to specify your access keys.... Hope that helps...
Yeah, I had problems with Tweepy in that I was unsure about what it returned after any given request. Sometimes it was a dict and others it was a tuple or named tuple. Sometimes it just returned raw json... I haven't had any surprises with the PTT lib, however. That is nice when you just want to get stuff done. 
Thanks, understood now.
My domain name generator is at http://www.nomipede.com It does quite a lot of computation but seems to cope fine on GAE Go for it.
Well, I fixed the oauth recipe, including my app name and consumer key and consumer secret -- and still getting the same error message back... Edit: I think I fixed it.
I was a kickstarter backer for this as well and really got a lot out of the book. I'm going through the beta of real python for the web as well and even in beta it's great
Interesting but what's the link with python
By the way, another different question I'm wondering if you could answer. I have a script (also from the "21 Recipes for Mining Twitter" (here: https://github.com/ptwobrussell/Recipes-for-Mining-Twitter) that I'm wanting to use for the following: I have several Twitter users I want to put in what I'll call a "Multiple ego network/matrix." So Let's say theres 500 screen names that I want to put into a matrix including a follower graph of their followers. And I have a script to (this one: https://github.com/ptwobrussell/Recipes-for-Mining-Twitter/blob/master/recipe__get_friends_followers.py) that I can use to collect all of the followers of a single user. OK, so two questions then: Can I make a single, customized script to collect the followers of all 500 screen names in one request? I realize there are rate-limits, but I have a way of pausing the call to deal with rate-limits, so I realize that a call like this (if possible) could take days or more. Secondly, and here's the money question for me right now: HOW do I collect the data into an actual matrix that I can analyze? Where does the data go when I do a request like this? I want to get it all into a matrix. I've read up on NetworkX and some on matplotlib, so I know that it's possible to get all of this stuff into a graph/matrix. But I haven't figured out how to get it all into a matrix so I can run formulas like centrality metrics. Any thoughts? I realize this is potentially a couple of big questions.
http://en.wikipedia.org/wiki/List_of_tools_for_static_code_analysis
Seriously though, ROOT in "C++" is awful. It's not great even in python (with the standard PyROOT bindings). In general it's a horrible system. This at least made it possible to live with touching it every so often.
Huh, very interesting. Out of interest, how much heap/RAM is your Python process using?
Tangential, some of us use VS and would prefer to use Git without a separate application for managing it. 
Amen. I'm getting pretty tired of these links to "Wow, look what I found!!!" pages that then require a search-and-rescue mission to find a link to the link to the link (...) that lets you download it. 
Dicts are just the beginning! Throw JSON into the mix and suddenly every problem becomes a "simple matter of serialization" ;)
if you're starting a git repo just add .ds_store and *.pyc to the gitignore??
split() will give you a list, for which you will want to do indexation on. Say sample[0] is your date.
Different problem. For example, with Django, let's say you have a template tag library called `myapp/templatetags/foobar.py`. It's in the wrong module, it really should be in myapp2. So you move it. The old pyc file stays behind and is ignored by git -- so if you `{% load foobar %}` you could end up getting the stale version from `myapp/templatetags/foobar.pyc` instead of the new version in `myapp2/templatetags/foobar.py`. There are a pile of other times where it can bite you, that's just one that bit me recently.
Yes and the schema knows that event[0] is a date and it applies the query.
Yeah, but "post checkout" implies "the problem is someone is really careless with `git add` and committed the files in the first place."
This comes up regularly in all the language subreddits I read. The *reason* subreddits end up with mostly beginner topics is that there are, by definition, more beginners than experts. People learn the most from seeing the work of those with more experience/skill than they. The non-beginners get less and less benefit from beginner heavy subreddits and spend less time there. Not that teaching others isn't useful, but it provides massively diminishing returns. And people who aren't good at teaching do more damage than benefit. Since we're already going meta: the responses whenever this question comes up always seem to be: "Fuck you, you elitist! Lowest common denominator subject matter is democracy/freedom!" "It's mean to say that you don't like beginner questions. Non-beginners have no right to a place where *they* can go to learn things" "Just learn from teaching!" (No one with any education in the field of Education ever says this, because it's stupid) "Wanting to advance from intermediate to expert in anything is selfish" Basically, Reddit is not a platform that works well for things like this unless you have a subreddit with very strict moderation which is often very unpopular.
What do I get with PyROOT that I don't get with just matplotlib and numpy and other libraries?
How are you ensuring they check at the same time? Or are you just relying on you sending it to each queue at the same time and "same time" means within a reasonable margin.
Eh? It's just a clean-up script to clear out `.pyc` files after checkout. Not to remove them from the repository -- they were never checked in, but DO exist in your local working copy. If you're on a branch where `xyz.py` (and thus `xyz.pyc`) exist, then switch to another branch where `xyz.py` does not exist, Git already removes the tracked `.py` file. But it leaves the ignored `.pyc` file in your working copy. Thus `import xyz` will still work on your local checked out branch copy -- it will import the stale `xyz.pyc` file. This can lead to bizarre/hard to trace bugs since something can work on your copy, but not on someone else's (who doesn't have an `xyz.pyc` in their working copy).
I agree. And most modern programming editors do the indentation automatically. So really not a headache. Plus the code is so much more readable with indentation.
I don't know... I prefer to eat my celery.
You responded to the linked comment in the original thread with: &gt;One of the dumbest things I've read in a while. Yet you say you're not trying to start something? The Python community doesn't need people like you.
Some people like to make judgements without fully understanding all the pros and cons of a design decision (or any decision made by someone else). You see it all the time. It's a pretty superficial complaint which tells me this person has not completely weighed the advantages against the disadvantages, and has just made a knee-jerk reaction, formed an opinion, and probably has no intention of changing it. In short, it's a troll, whether intentional or not. Ignore.
He doesn't say he's not trying to start something. He says he hates to do it.
[Raymond Hettinger succinctly explained why significant white space is a winning feature in his PyCon 2013 keynote](http://www.youtube.com/watch?v=NfngrdLv9ZQ&amp;t=595)
Zeromq is a black box? Is that so? I thought zeromq was more like networking legos you get to use as you wish? Are those legos black boxes and quite substantial in their amount of responsibilities?
Good point, but still, he's clearly enjoying himself.
I accept that you accept me winning the argument. So I will ignore your shitty attempts to twist my words to make you feel better about your poor choices in tech. Hope your employer doesn't see this and realize how much $$ you are costing them. 
&gt; That would require the community agrees on what is "chaff" and what is "wheat". Clearly that is the subject being brought up I don't think we need to agree on it, I think it's an emergent behavior. If most of the community downvotes a topic, than by defnition it's average "value" is very low. What we have here is 5 or 10 people in a topic who (apparently) disagree with 47 thousandish subscribers. I'd agree that the experts are probably a lower percentage of the community, but I suppose the disconnect for me is where the small percentage of "experts" get to dictate content to the presumably much larger percentage of newbies. Beyond that, having a high percentage of purely-expert focused posts isn't necessarily a good thing. I'm quite a proficient developer, but I'd hate to see arcane posts about category theory (for example) dominate the sub. I think the voting system is perfect, for Reddit. I also think that some people in this thread should probably be frequenting Hacker News, where the average proficiency is much higher. Horses for courses.
I'm going to sacrifice some karma for this, but please politely fuck off with this useless post. Regards.
It matters that backquotes aren't valid in Python 3, because that means a tool like this one can't make use of them as spare syntax.
Can someone please explain to me what the point of Celery is? Why do we need a large complicated framework for something as simple as persistent message queues... especially when it doesn't actually implement the data structures? If you've got Redis installed then I've got just two words for you: BLPOP and RPUSH. There, now you don't need Celery. Or you could just whip up a deque with condition mutexes. Sheesh
celery chords and celerybeat are kind of useful, but i kind of agree with you. I just started using rq, its nice to not have to register tasks and worry about other annoying details.
works quite well with Python for Visual Studio (an oss msft product -- http://pytools.codeplex.com)
Is this still an issue with the way Python 3 stores python cache in a separate folder?
Lots of terrible, terrible things. Stay away. Particle physicists have to use it because over the last 20 years it has become the standard for storing/accessing/analyzing the massive amounts of data. If you work on ATLAS or CMS, you can't simply decide to ditch ROOT because the jiggabytes of data are only available in ROOT format. I know there are other people who use it, but I have no idea why.
Anyone who isn't forced to shouldn't use ROOT. There are much better tools out there for work that doesn't involve working with the Analysis Objects coming out of the experiments. Python, Numpy, SciPy, Matplotlib, Pandas, IPython is a great data analysis / manipulation combination. But yeah, the PP world is stuck with ROOT and it's clunky and archaic ways of doing things unfortunately.
Yes. 
As an European, with no way of attending the conference, that would be awesome.
save your karma, is Friday.
We keep an eye on number of tasks being executed on our destination queues. But yes if location queues are busy doing previous checks, that could mean that current need to wait a bit (with reasonable margin) to be executed.
 PYTHONDONTWRITEBYTECODE=1 in your environment is also a useful way around this problem. You occasionally have to disable this setting during installation of some virtualenv packages, but the error messages in those cases are clear. Otherwise, it's set and forget.
This seems like a bad idea. It scans through the directory structure counting *.pyc files, and if it finds more than 0... it scans through the directory structure looking for the *.pyc files again. It appears the double scan is all just to get a count of the # of files you deleted. **News flash**: you can actually execute multiple actions with a single find command. This can be done twice as efficiently and more succinctly: (( total_files += `find . -name "*.pyc" -delete -print | wc -l` )) if [ $total_files -gt 0 ] then printf "\e[00;31mDeleted $NUM_PYC_FILES .pyc files\e[00m\n" fi Of course, you really only need consider deleting *orphaned* .pyc files, but this script delete's *all* of them. If you are going to brute force it like that, just save yourself the trouble and do: export PYTHONDONTWRITEBYTECODE=1 I think there is an opportunity here though to do "the right thing" (delete only those .pyc's without a .py peer) and in a more pythonic fashion. from os.path import join from os import walk. unlink import re import sys PYTHON_RE = re.compile(R'^(.*\.py)(c)?$') directories = sys.argv[1:] if len(sys.argv) &gt; 1 else ['.'] for directory in directories: for (dirpath, dirname, filenames) in walk(directory): source_set = set() pyc_set = set() for filename in filenames: match = PYTHON_RE.match(filename) if match is not None: (source_set if match.group(2) is None else pyc_set).add(match.group(1)) to_remove = (pyc_set - source_set) for extra in to_remove: unlink(join(dirname, extra + 'c')) count += 1 if count &gt; 0: print 'Removed \033[91m%d\033[0m file%s' % (count, 's' if count &gt; 1 else '')
Maybe blackbox wasn't the most fortunate term to describe it. But let say is more low level than using Celery and RabbitMQ. Especially when you are trying to accomplish things fast. ZeroMQ is good tool if you have good reason to use it, you probably should.
I know that, I’m asking about the AST described here: http://docs.python.org/2/library/ast.html
* [FWRD](https://github.com/wewriteapps/FWRD), a "micro" framework that tries to do 90% of the bare-minimum needed for webapps, eg. gets rid of the View and Model from MVC, you just route directly to a callable, return plain python data types, and the result is converted to something usable (HTML via XML+XSL, JSON, YAML, etc). Still a work in progress, and I've got lots planned for it. * [python-yaml-logger](https://github.com/wewriteapps/python-yaml-logger): a formatter for the standard Python logging module for logging data as YAML. Handy if you need to parse your log files later on and do something with them (build stats, filter, etc) as YAML files can be appended to, where JSON/XML cannot (without some trickery). * [PySO8601](https://github.com/wewriteapps/PySO8601): parse ISO8601-formatted dates, inc. ordinal and week dates, built because the "main" ISO8601 lib on PyPI is absolute sh*t and can't handle simple "YYYY-MM-DD" formats. * [exemelopy](https://github.com/wewriteapps/exemelopy): convert simple python datatypes to XML. A bit like SimpleJSON and PyYAML, but for XML. * [RunnerBean](https://github.com/wewriteapps/RunnerBean): a tool for creating long-running Python workers listening for Beanstalk jobs. Plus a number of others I've yet to complete/release.
So this is specifically a python 3 problem?
The examples are in Python, so it's related.
you probably want to use your touchpad in absolute mode then. check out this post: http://askubuntu.com/questions/239823/use-laptop-trackpad-as-graphics-tablet There's a xorg config that allows for the synaptics driver to be set to absolute mode.
Its interpreter is very well built, overall its very fast
You should look into [PySide](https://pypi.python.org/pypi/PySide) instead if you're concerned about licenses; PySide is LGPL.
Ooh ooh I know the answer! When instead of using readable words or plain English like "or" or "mod" or "xor" you use gibberish like | % ^
The development of PySide slowed down recently and the future of the project is not very clear. Note that it is "Qt" and "PyQt" correctly.
Oh, I missed that macropy does not include a parser. That does indeed limit its possibilities.
I have look a bit on it and as jabbalaci says, the future of PySide seems unclear. Though it certainly seems like the LGPL could be better here, and only because of OpenSLL. :/ 
I think it's fine to see it either way as a matter of preference. It's helpful in that syntactically significant whitespace actively enforces some degree of readability. It detracts, in that it's a relatively easy (potentially invisible) thing to get hung up on where you wouldn't have created a syntax error in another language. To each their own, though... I certainly wouldn't consider it an issue of extraordinary "ignorance." tldr; It's a programming language, not a constitutional rights issue.
Pointers
Ha! That's kind of experimental. Even the Wikipedia page is kinda unsure on some aspects. I should add "USE WITH CARE"! :D On the other hand the complexity stuff is absolutely correct! (Currently hitting 100% coverage in the tests).
Every programmer alive needs to read [Clean Code](http://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) and study it like a Bible. I keep my copy next to my bed and read it for divine inspiration every morning.
Have you considered GnuTLS? it provides SSL capabilities in a GPL'd library. It might be worth considering, and seems the popular solution for avoiding the OpenSSL/GPL conflict.
How about "plus", "minus", "times" "divided_by"?
&gt; ... is like killing a fly with a sledge hammer. True story: I have killed a fly with a machete. Would that be considered more 'pythonic'?
Luckily, the code you'd write for PyQt and PySide are almost identical, changing from PySide to PyQt in the future is trivial. 
Bad programmers. 
&gt; I don't think we need to agree on it, I think it's an emergent behavior. It is certainly emergent behaviour, and therefore suffers from the lowest common denominator problem. You are basically arguing that /r/python should be mostly beginner content, /r/learnpython is kinda pointless and that if any non-beginners wants to learn new skills on reddit they need a more moderated /r/advancedpython subreddit or similar. Fair enough, maybe that is the answer. But that's not what the sidebar describes. This idea that hands off moderation and complete populism creates good communities is idealistic but runs contrary to what everyone who has tried to build good communities has seen in practice. See: the frontpage of reddit.
Yes, i actually looked at GnuTLS first, but I found that I for one couldn't compile some of the dependencies for GnuTLS. I get a strange error, suggesting that commands given to windres was wrong. And the makefiles are rather confusing to me. But the Python-bindings for GnuTLS seems to be outdated, and only supports Python 2. I also couldn't find any bugtracker or source code repository for the python-bindings, just a one year old source-package. I also looked at NSS, but it seems like the bindings are in the same state there.
No, because we have a symbol called "the plus sign," etc. Humans read "+" as "plus," "-" as "minus," etc. Using "percent" to mean "mod" or "remainder" makes no sense. Using "caret" to mean "xor" makes no sense. Using "pipe thing" to mean "or" makes no sense. It just makes code harder to read while saving all of 1 or 2 bytes of hard disk space. I'll give them the ampersand, though.
When you combine your code and one or more libraries, you are creating an aggregate work. That work must be licensed in a way that simultaneously satisfies all of the licenses of the individual components. Try an experiment, consider you had chosen vanilla GPL2 as the license for the aggregate: package|component license|aggregate license|legal :-|:-:|:-:|-: Your Code|(anything compatible with GPL2)|GPL2|yes PyQt|GPL2, GPL3, or GPL2+exceptions|GPL2|yes Qt|LGPL2.1|GPL2|yes OpenSSL|4-BSD|GPL2|no Three out of four are satisfied, but the 4-clause BSD (old BSD) license cannot be converted to GPL2, so the combination fails. You can try other various combinations, like 4-BSD for everything, but that fails because you can't convert PyQt's license to that. But 4-BSD can be converted to GPL2+exceptions, which is the whole point of the exception clauses, so that one will work, and it's the only one that does -- assuming that the license of your code is something that is compatible with GPL2+exceptions. 3-BSD (new BSD) would be fine, but perhaps a bit confusing, since the result will be GPL2+exceptions and presumably your code isn't very useful by itself without PyQt, so you might as well just make your code GPL2+exceptions to avoid that confusion. Or, perhaps if you expect there to be a source-compatible replacement for PyQt in the future, you might want to stick with 3-BSD to allow that potential flexibility. 
If you think you can just JUMP into programming without learning how to structure code, you are gonna have a bad time.
Thank you for that explanation, much appreciated! The reason that I didn't specify my code as GPL2+exception was that GPL2+exception wasn't on the list of licenses that the exception allowed me to use. :)
ouch, that sucks. I don't know of a good solution then. Since python typically isn't linked in the traditional sense (unless you're shipping binaries via freeze, py2exe, etc;), would the licence issue even come up if you just plan to post it to github or something? I wonder how the licencing applies in this case....
I tend to see stuff like this rather frequently and it feels like I'm missing something. def crawler(urls): urls = list(urls) Do these allow future flexibility if you want to change that function or does it serve some sort of other purpose? Should I be creating more functions in my code even if they don't do much?
Yes, I think the license things behave in the same way be it Python or C. And if you distribute your source on GitHub I would think the licenses would need to be in order. But Rhomboid gave an [easy to understand explanation here](http://www.reddit.com/r/Python/comments/1d5jcv/licensing_with_pyqt_and_opensll/c9n9ta8), so it doesn't look like there's any problem! Just confusion form my side. That is if his answer is correct, which I hope it is. :D
Thanks for introducing me to this! Also, have to love the given example is how to write a torrent scraper.
If you don't like percent for mod then you should be equally annoyed by the use of asterisk for multiplication. Using slash instead of the actual division symbol is also a bit of a stretch.
You can blame that on Ken Thompson. Also: a few bytes of drive space really *was* an issue back then.
Learning and getting used to whatever symbols your language uses isnt the hard part. What you need to be able to do is create an accurate mental model of what the code is doing compared to what the code needs to do. Try making the same program in Java, then Lisp, then Forth.
Fucking magic methods. When shit happens that's not spelled-out in the code, it can lead to hunting for hours for unexpected behaviors.
That doesn't look like it is doing anything at all. A general rule of thumb is when you feel like your method is getting too long you should split it up. On the other side, if you are writing a ton of tiny methods you aren't really helping the readability of your code. Another rule is to write your code for your current specs, not your future specs. Most of the time the future doesn't come and you can always refactor in the future if you need to. 
Slash is the conventional symbol for fractions already, though, which is the same thing as division.
I assume the asterisk comes from using a dot instead of "x" in algebra/calculus. And what punning said about /. I'm sure you've written 1/4 by hand at some point. I also prefer how pascal/delphi/ada uses / and div, instead of / and //.
&gt; Try making the same program in Java, then Lisp, then Forth. You wash your mouth out.
I know, but the punch cards have gotten a lot bigger nowadays, it's time to upgrade.
wow this looks sweet. Thanks edit: It can read xlsx also ?
No. Write only.
I did that, just straight up learned python and started using it for scientific computing. But I know my code is shit.
Elegant code is hard to write. Fuck I still think my code is shit. However, my code is getting shorter which also means what I can do in one line used to take 3 or 4. Think of it as a work in progress. It's not something that just happens over night. It'll come.
I usually go for readability, my simple scripts don't usually pass 1K lines and it involves a whole lot of debugging. And I really need to be sure it is doing what I think it is doing, scientific publishing is a bitch.
sure, but what is a pipe key and why is that on a keyboard?
To be fair, Pandas has some simple api tie-ins with matplotlib that make basic charting easier. I agree it's not a major focus of the project, but it's not inaccurate to say it has "graphing capabilities".
It really just takes practice and experience. Anyone can learn to program after a few lessons and reading a book or two, but to write elegant, nice looking, readable, idiomatic code, you just have to keep writing and reading code.
I'd say its a combination of poor variable names, inconsistent coding standards (weird indenting, etc), no or misleading comments, and hacked together code. Basically, bad programmers and time constraints.
&gt;You're not going to be able to achieve the same functionality This is incorrect. Programs are turning complete. There is no computational problem they can not solve. In the worst case they can always build a string of their own source code, modify into any other form, and then execute it.(Although this is really not necessary). Run speed of course is a different story. And there are good arguments why macros are bad choices in many cases. They are very hard to read and abstract. 
&gt; what is a pipe key and why is that on a keyboard? That is actually a really good question, that sent me on an hour of reading. :) Wikipedia proved insufficient, and even the very geeky [Annotated history of character codes](http://www.wps.com/projects/codes/) did not shine its light on this very specific question. What I take from that page is that: * It was not in the first version of ASCII (1963) and previous codes (FIELDATA, ITA2, etc.). * It *was* in the second version of ASCII (1967) As [explained in the article](http://www.wps.com/projects/codes/#GRPH), there was a lot of debate at that time about which symbols to include in such character sets. Scientists were used to having a lot symbols at their disposal, and this variety was accommodated in printed books, but there was only so much codes available in 7 bits (which was already a stretch from the standard 6 bits of the time). As an example, the Wikipedia article about ALGOL notes that the [backslash character was introduced in ASCII to support both its /\ and \\/ operators](http://en.wikipedia.org/wiki/ALGOL#Timeline_of_ALGOL_special_characters).
You misspelled "psychotic", and yes :)
It is very true that the "C++" root forces people to use is plain awful. It's based on an old C interpreter that was later hacked to support C++. The interpreter itself isn't even supported by the original developer anymore. HOWEVER, there are very exciting things happening with root currently. The new clang/llvm project has made the task of writing a true C++ (supporting C++11) interpreter downright trivial. They've developed a working interpreter that only requires a fraction as the previous interpreter, and works wonderfully. https://www.youtube.com/watch?v=BrjV1ZgYbbA https://www.youtube.com/watch?v=eoIuqLNvzFs Supports Qt/OpenGL/anything else right out of the box, not possible previously. This also has the potential to make python (PyPy) fully able to interact with all of C++'s features as well through shared libraries, http://morepypy.blogspot.com/2013/02/cppyy-status-update.html http://indico.cern.ch/getFile.py/access?contribId=10&amp;resId=0&amp;materialId=slides&amp;confId=217511 I've used ROOT, matplotlib, numpy, pandas, etc. I frequently use them together. What I think many people don't realize is that ROOT: * When it was introduced was the only solution to allow scientists to analyze data in a fully interactive manner. * Is still faster than pure python based solutions (this is mandatory, even a 5% increase is worth it when people run distributed analysis that have thousands of cpu hours). * By itself, is not a bad data system / graphing interface. They recently supported a Quartz backend for macs that looks pretty good. TL;DR: I agree that ROOT as it stands today is a bit dated, and the pseudo-C++ aspect is awful. But, when it comes to speed Python simply is not mature enough to be a solution. I fully expect that when cling is fully integrated with ROOT it will continue to be a great way to save and analyze data.
Why is this using lisp naming conventions? There are much more user friendly to call these things. 
&gt;I assume the asterisk comes from using a dot instead of "x" in algebra/calculus. How does that make it any more appropriate than using | for "or"?
I've been using this for a couple months and it's been really solid. OP is adding new stuff all the time (porting code over from another language, so it's growing rapidly).
This is damn cool! Any modules you suggest for reading excel files?
very cool indeed
Well who the fuck knows what you meant by 'vagrant support'?
You're doing it right.
You might want to read up on the Python datetime object. There is a timedelta object that makes comparing dates painless.
Actually Celery as a framework doesn't implement much of the message or queue handling, that's done in Kombu (which itself is a queue framework which can use Redis, Rabbit, Mongo, Couch, MySQL etc. etc. as backends). As others have said you get a lot of other advantages from Celery, some not mentioned are inbuilt exception handling, retrying tasks, throttling, different serialisers, etc. Think about it this way, you could just as easily rephrase your question as "Can someone please to me what the point of Django is? We can just write everything as a WSGI app and it works!"
I'll probably use this. Thanks! Planning on adding PivotTable support?
The [xlrd](https://pypi.python.org/pypi/xlrd) module can read XLS and XLSX files. The [OpenPyXL](https://pypi.python.org/pypi/openpyxl) module can read, write and re-write XLSX files. There are a [few other options on GitHub](https://github.com/search?l=Python&amp;q=xlsx&amp;ref=cmdform&amp;type=Repositories) as well.
Pivot Tables are probably a feature too far. Mainly due to the potential complexity of the interface. Also, it would probably be necessary to collapse rows/columns programmatically since the file format doesn't do that for you. However, it may be possible to add basic Pivot Table functionality. I'll definitely see what can be done. 
I use [xlrd](https://pypi.python.org/pypi/xlrd) [1], works well in my experience (extracting data from spreadsheets used to record experiments). [1] https://pypi.python.org/pypi/xlrd 
Why not use csv? Are there some hidden advantages I'm not seeing here?
3.3 support?!?! You rule! I have been using csv as an output for my project, but I know that my users would prefer Excel. This looks like exactly what I am looking for.
Actually, articles written by Fred W. Smith (one of the authors of ASCII) have been scanned and referenced from the annotated history. On [page 4](http://www.wps.com/J/codes/Revised-ASCII/page4.JPG) of his introduction to the second (final) version of ASCII, he describes the newly introduced characters and writes: &gt; The broken vertical line in position 7/12 will probably be widely used as either the "logical or" symbol, or to indicate "the absolute value of." The overline in position 7/14 will probably be used in many systems to indicate logical negation or complementation; in some devices, it will very probably be printed as a straight overline.
CSV doesn't support: * Multiple worksheets * Formatting * Charts * Autofilters * Page setup for printing * Conditional formatting * Images * Data validation * Grouping XlsxWriter does support these features and more. Excel reads csv files but csv isn't an Excel file.
Because one is very similar to a common symbol, the other has nothing to do with the word "or" whatsoever.
Cool [These guys](http://www.reddit.com/r/Python/comments/1crit1/pyexcelerate_accelerated_excel_xlsx_writing/) have a xlsx writer called pyexcelerate. It doesn't do formatting or charts or anything, but they claim it's faster than other libraries. I think I like this better - I don't think speed is an issue for most people, but formatting definitely is.
Such as what, Cons and Nil? That is the common nomenclature for the data structure he's describing. 
So far this is just a proof-of-concept sort of endeavor. Anyone who is going to be interested in it is probably going to be familiar with the concept of cons cells. Besides, the whole point is that you can create arbitrary recursive data types, so if you prefer class LinkedList(ADT): pass class Node(LinkedList): payload = Anything() next_node = Require(LinkedList) class EmptyList(LinkedList): pass EDIT: BTW, I like your username. At first I thought it was 'ace of fears'. Then I realized there weren't enough f's so it must be 'ace of ears'. But it could just as easily be 'a CEO fears' or "ace o' fears". On the topic of naming conventions, I guess.
http://www.reddit.com/r/Python/comments/1bx3vj/z/c9b5tea
&gt;Writing parsers has often been a task difficult for programmers to take on. Many of the tools available for parsing require writing grammar rules and code that work very different from everything else in Python. Parsley is a library that mixes the benefits of concise rule-based grammars with a parsing model that works the way you expect Python code to work. Allen Short 
I see where you're saying its sort of unintuitive, but if you think about them a little bit, it's the best they've got. The pipe is OR because its a dividing bar, so either the left side or the right side of the pipe is the value. Modulo is two characters with a split through the middle, where the remainder is the bottom 0: % I mean, it's stretching, but if you try to remember that, it makes pretty good sense.
I meant `car` and `cdr`. 
The comparison was between Clojure and Scala. Still pretty different but a Java programmer would learn a lot of the same functional ideas.
I downvoted this submission, not because I am not interested in document classification or python, but because it isn't news. It is just a project's github link. There is no context. There is no explanation of what this is unless you view the primary sources. If this story had any content at all, I would almost certainly have upvoted it.
That's fair, though I think your definition of what's relevant to this subreddit is too narrow. One of the reasons I find python and its community so appealing is the brevity of getting stuff done. Appealing, applicable demonstrations of getting stuff done with python--even if using several-year-old packages--I think are good content for this subreddit and you're free to express your disagreement.
This looks really cool - not sure if you wrote, but if so much appreciated!
And the output: +cpp +herpderpbuttsbuttsbutts herpderpbuttsbuttsbutts.cbp herpderpbuttsbuttsbutts.layout main.cpp +bin +Debug herpderpbuttsbuttsbutts.exe +obj +Debug main.o
That's because you're reading about Java in the /r/python subreddit, where people had mental breakdowns when they learned they had to use parenthesis with the print function in Python 3. A lot of the arguments here are exaggerated, come from a place of ignorance, or are petty ("I had to use semicolons at the end of my statements in Java!"). Python (or any dynamically typed language) people like to paint Java (or any statically typed language) as a terrible language but it isn't, it's a decent language with a great ecosystem, so overall a good package.
&gt;That's because you're reading about Java in the /r/python subreddit Not exclusively, no. In fact far from exclusively. I don't doubt that it's a useful language. I'm just saying that it's got a particularly bad rap on the internet.
That's pretty awful. You're bending over backwards to compute the indent level because you're letting `os.walk()` do the recursion for you. If you do the path walking yourself, you get that information for free. This uses the new `yield from` in Python 3.3: from os import listdir from os.path import isdir, join def walk(path, level=0): indent = ' ' * level for entry in listdir(path): fullpath = join(path, entry) if isdir(fullpath): yield indent + '+' + entry yield from walk(fullpath, level + 1) else: yield indent + ' ' + entry This can be used as e.g.: with open('c:/path/to/output.txt', mode='w', encoding='utf-8') as outfile: outfile.write('\n'.join(walk('c:/some/path'))) (It's preferred not to use `file()` to open files, but `open()`.) 
In the interests of being fair and unbiased you may want to read the responses to these questions in /r/java: [Can some explain to me why there is so much Java hate?](http://www.reddit.com/r/java/comments/w7pmf/can_some_explain_to_me_why_there_is_so_much_java/) [Why do people hate Java?](http://www.reddit.com/r/java/comments/1aw1rh/why_do_people_hate_java/) [A bit of a vague question, but why does Java, as a language, draw so much hate and scorn?](http://www.reddit.com/r/java/comments/18qg9u/a_bit_of_a_vague_question_but_why_does_java_as_a/)
very nice indeed! congrats. if you happen to use Visual Studio + Python, check out PyVot which can be complementary: http://www.youtube.com/watch?v=Oi3QKuFugWk&amp;hd=1
read its abstract
Summary? Defying how?
They rewrote the code from scratch and were successful in doing so.
it tells you on the 4th slide.
The first half hour doesn't have much in the way of code. The presentation was delivered to a mixed crowd, most people wern't programmers. I found their methods or dealing with the C++ API very interesting. Useful notes: Trinity is Eve's graphics engine.
"Frameworks are a trap!" Loved that bit. Feels very true for a lot of software development these days. I also really like the emphasis on monitoring and profiling. As some one who works with very high performance systems doing anything to improve that performance without data and monitoring is basically a non starter so good on them for taking the time to be scientific in their optimizations. 
$ tail -f $(ls -lt /path/to/logs | head -n 1) Is quite a bit like $ tail -f /path/to/logs/* in which case you get something like ==&gt; logs/x &lt;== ==&gt; logs/y &lt;== ==&gt; logs/x &lt;== A B ==&gt; logs/y &lt;== C
So if you admit its "disgusting hacky code" then I'm confused why you posted it here. If you want help on making it better try /r/learnpython I know that sounds mean but this just isn't good content for this subreddit. ^^sorry
Is there supposed to be sound to this?
There is also https://pypi.python.org/pypi/ScriptTest/1.2
I think this presentation is misleading. You're trading development time for performance. Not everyone needs or wants to make that tradeoff. Frameworks, used properly, save you a lot of work. If you don't need extreme performance, you can save a lot of time and money using frameworks and other prepackaged tools.
I need to look into Jinja2. Seems like a huge win performance wise. 
I'm doing the next version with BeautifulSoup and I used datetime in that; although I didn't use timedelta. I'll check that out. Thanks.
very impressive.
I was under the impression I shouldn't reinvent the wheel 'cause people much smarter than me have done it better first. Seemed to me like the os module might be written in C or something.
I feel like this would be a fantastic talk, if only we could hear it. This is kindof like only looking at the pictures in a magazine, and not getting the words.
They wrote Django?
They rewrote their application (to use Django rather than LAMP).
So Joel Spolsky says "Never rewrite an application"? Reddit seems like the quintessential counter-example.
Anyone have link to the famous stack overflow post by Andrew Clover?
How does this compare to [PLY](http://www.dabeaz.com/ply/)?
I'll never understand people who put slideshows online without a transcript or audio. Isn't the first rule of presentations to not include more than a highlight on your slides, so people actually listen to you talk and don't just sit there reading?
Agreed. You still can do raw sql if it's needed, and if you know what you're doing you'll know when to do that vs using the ORM. For the other 80% of the time though, the ORM not only allows you to be more productive, it helps improve the quality of the code. 
Isn't that ActiveRecord? I once took over a RoR project that had the same sort of problem. The implementation of RoR and ActiveRecord were fine for banging out a quick weekend project. But once it got up to a few million records it had major problems. Don't even get me started on data integrity. We had to write so many hacks around activerecord that we wound up basically coding our own ORM layer. It was fast but if I had to do it over again I'd have just re-done the site in Django, my current nerd-crush.
PLY is based on lex/yacc, i.e. the old bad way of doing stuff. :) I wrote Parsley because using PLY resulted in code that was too hard to understand or debug.
Not reinventing the wheel is generally a very good philosophy to live by. However, if the existing wheel makes you bend over backwards to use in the way you want, and a replacement wheel can be written in ~10 lines, well... To be clear, `os.walk()` has a number of advanced features (such as the ability to specify a DFS or BFS, or the ability to prune directories from the list) that make it a lot harder to replace in reality; but if you weren't using those features to begin with, they don't need replacing. 
First one just continually tails the newest log file in the directory. Second looks like it would continually output additions to all the files in a directory.
Here's the actual post he made about it. http://www.joelonsoftware.com/articles/fog0000000069.html Obviously it's not a hard and fast rule, like in the case of reddit. The common thread of these "successful rewrites" seem to be when you have a clear reason why your current stack is insufficient + need to switch, whereas Joel's argument is more about taking code written in a system + tossing it just to rewrite it for the same system "only this time done right." 
Saving this for a later date, but this is about the programming of the engine right? Also I assume they talk about Carbon (the server framework)?
Their presentation presents their conclusions as being universally true instead of describing the use cases where it's reasonable to roll your own custom solution. I also think it's dangerous to declare that you can't do anything original within a framework. I've done plenty of reasonably original projects that include frameworks in some or all of their components. 
The first one will only follow the current log as opposed to rotated ones, though I'd use -F
No one here ever programmed in COBOL? MULTIPLY PRICE BY UNITS GIVING COST 
And while we're at it, one of the handful of things Guido's ever been wrong about. = is NOT an assignment statement. That's "equals". a = 7, "a equals 7", is something that should evaluate to true or false. a := 7, "a is assigned the value of 7", should give a the value 7. a == 7, this should be a typo. :-) Even Guido shouldn't be allowed to redefine the "equals" symbol.
PERL programmers seem to be voting all these posts down, so here's an upvote. 
&gt;&gt; The broken vertical line in position 7/12 will probably be widely used as either the "logical or" symbol, or to indicate "the absolute value of." Absolute value I get, but why did he think (correctly, it turns out) it'd be used for logical or? &gt;&gt;The overline in position 7/14 will probably be used in many systems to indicate logical negation or complementation; in some devices, it will very probably be printed as a straight overline. Didn't this end up becoming tilde? (Interesting that tilde actually did end up being used for complementation in C.)
Wow, the indexing integrations seems to be really nice and also customizable, i like it! The thing i still don't get is, how relationships are handled - do you use some of the intersection functionality for that?
how many talks do you attend where that's the case? i sure don't see many. :(
You mean advice given on the internet isn't right in 100% of cases? Shocking...
Oh yes, at first I have tried to use it but ScriptTest runs the the script first then gives you output. You have no control during the lifetime of the process, i.e you can't kill it because you don't know it's process id. Then I have tried [pexpect](http://pexpect.sourceforge.net/pexpect.html). It seemed to do what I want but I had a problem with signals. I was testing a process manager process by the way. Tried to understrand pexpect source code and fix the problem but I could not. It was to complicated. "What" is just a wrapper around subprocess.Popen that gives async access to it's output stream.
Did you see that slideshow just a few days ago, from the LibreOffice guy? That was brutal....
His example confuses me. Shouldn't this be removed by the DB Query optimizer? SomeModel.objects.filter(other = None) SELECT "myapp_somemodel"."id", "myapp_somemodel"."arbitrary", "myapp_somemodel"."other_id" FROM "myapp_somemodel" LEFT OUTER JOIN "myapp_othermodel" ON ( "myapp_somemodel"."other_id" = "myapp_othermodel"."id" ) WHERE "myapp_othermodel"."id" IS NULL vs SomeModel.objects.filter(other_id = None) SELECT "myapp_somemodel"."id", "myapp_somemodel"."arbitrary", "myapp_somemodel"."other_id" FROM "myapp_somemodel" WHERE "myapp_somemodel"."other_id" IS NULL
Thanks. Yes, I wrote it. :-)
I agree, I would have loved to know what this actually does. I looked through the sources and it seems to parse a bunch of files and "lemmatize" the contained words. For a non-NLP-expert this simple example would have been fantastic to get introduced to these things.
can we get a tl:dw?
Could you hook me up with some decent profiling tools for Django?