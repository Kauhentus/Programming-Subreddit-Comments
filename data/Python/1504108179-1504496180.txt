So forget the %s?
If you're just polling the database I would suggest trying SQLite module. Lots of info on the web about it too.
You could stick what I gave you into that statement but it's very messy. Why don't you want to use the function? if check_words(comment.body) and comment.id not in comments_replied_to and comment.author != r.user.me(): 
care to share said solution?
I know it's been a few months since you posted, but just to answer quickly: I've been using IQFeed API to do financial programming (mostly in Python but also in C#). So far, I'm very happy with it. For the Python IQFeed code, I used the following: https://www.quantstart.com/articles/Downloading-Historical-Intraday-US-Equities-From-DTN-IQFeed-with-Python If you have more questions, hit me up...
Recursion is one of those things that people think is better because they were taught it later in school, but in fact, it's worse. Yes, some problems are more easily described as recursive processes, specifically tree searching. But there are actually very few problems like that, and most of the time a loop or a map is a better fit for solving the issue at hand. The problems where it makes sense to recur are the problems where you want the intermediate stack (again, tree searching), so saying "TCO fixes recursion" is overly simplistic.
 Python 2.7.10 (default, Oct 23 2015, 19:19:21) [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; sell = {15:300, 14:100, 13:200, 12:400, 11:100} &gt;&gt;&gt; buy = {15:100, 14:300, 13:200, 12:100, 11:500} &gt;&gt;&gt; shares_bought = lambda buy_price : sum([shares for price, shares in buy.iteritems() if price &lt;= buy_price]) &gt;&gt;&gt; shares_bought(12) 600 &gt;&gt;&gt; shares_bought(13) 800 &gt;&gt;&gt; shared_sold = lambda sell_price : sum([shares for price, shares in sell.iteritems() if price &gt;= sell_price]) &gt;&gt;&gt; shared_sold(14) 400 &gt;&gt;&gt; shared_sold(12) 1000 
docopt.
This one is named after the god of betrayals, so compared to the others, it's better documented that it's going to stab you in the back.
Damn. I assumed it was a port of a particularly badly named Java tool.
I see weird apostrophes there, that's most likely the reason. "Cannot compile" is also not helpful when you're asking for help, providing the actual error is much more useful information. Also, /r/learnpython
**Here's a sneak peek of /r/learnpython using the [top posts](https://np.reddit.com/r/learnpython/top/?sort=top&amp;t=year) of the year!** \#1: [Python 201 Book is Free for 48 hours](https://np.reddit.com/r/learnpython/comments/5814lw/python_201_book_is_free_for_48_hours/) \#2: [Python 101 Book FREE for 48 hours!](https://np.reddit.com/r/learnpython/comments/5bmaz0/python_101_book_free_for_48_hours/) \#3: [90% Python in 90 minutes](https://np.reddit.com/r/learnpython/comments/661o5a/90_python_in_90_minutes/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
[Home Assistant](https://github.com/home-assistant/home-assistant) is the biggest one I'm aware of. Uses aiohttp as it's base
Here's what I came up with. You can play around with it online [here](https://repl.it/K7yu/1). It calculates everything in linear time (assuming the lists are sorted) and caches it as dictionaries so all future calls to find_num_shares_moved is O(1). class Solution(object): def __init__(self, buy_price, sell_price): # assumes buy_price and sell_price are sorted in increasing order; # we can change code to sort them if they're not. self.buy_price = self.parse_list(buy_price) self.sell_price = self.parse_list(sell_price)[::-1] self.buy_cache = {} self.sell_cache = {} b = self.buy_price s = self.sell_price # iteratively add previous value to accumulate totals # save to dictionary for O(1) accesses for i in range(1,len(b)): b[i][1] += b[i-1][1] self.buy_cache[b[i][0]] = b[i][1] for i in range(1,len(s)): s[i][1] += s[i-1][1] self.sell_cache[s[i][0]] = s[i][1] def parse_list(self, l): for i in range(len(l)): price, shares = map(int, l[i].split(":")) l[i] = [price, shares] return l def find_num_shares_moved(self, b, s): return self.buy_cache[b], self.sell_cache[s] A very preliminary test: def test(): buy = ["15:100","14:300","13:200","12:100","11:500"][::-1] sell = ["15:300","14:100","13:200","12:400","11:100"][::-1] S = Solution(buy, sell) print(S.find_num_shares_moved(13,13)) print(S.find_num_shares_moved(12,14)) print(S.find_num_shares_moved(15,11)) with the following results: (800, 600) (600, 400) (1200, 1100)
But research has shown it to be true* *citation needed.
Had to vote this down because of several things: - the low quality of the text and obvious errors - missing client code: where is the part that *sends* the message - being yet another blog/tutorial conveniently forgetting the fact that ``send()`` and ``recv()`` may not actually send or receive all the data that you want.
I spent over 10 years doing deeply advanced, highly performant C# development. It is a wonderful language and is a joy to use. After that, Java is like getting a lobotomy. Generics don't actually exist, primitives types are too primitive (no value types / "structs" ? wtf), and Enums are a performance nightmare (every enum "value" is actually an object allocation). And if you're on Android, which is where I'm doing development, every "good" language feature -- like lambdas -- is too expensive to use enough of to get any real productivity boost at scale. (every single lambda is a completely new anonymous class, which has I/O cost for first-time loading, and then some memory usage forever thereafter, and this really kills you on lower end devices) You don't get iterators (I mean "yield return" stuff, not Iterator&lt;T&gt;), there's no await async syntax, and the lack of extension methods means you end up with this constant mix of calling instance/interface methods and trying and hoping to find all those static helper methods scattered everywhere. "Streams", their version of LINQ, really suffers because of this. Pointers aren't possible unless you use undocumented, unsupported, deprecated APIs that will probably be removed -- and even then, the syntax is ridiculous (why can't I just use pointers?!). Every single thing you do requires an allocation, which just puts more pressure on the GC and brings performance down even further. In C#/.NET you can use structs for a lot of simple data handling, and these just get passed around on the stack (or are embedded into existing object allocations), which is very performant. And if you want an array of packed data in Java, you can't just have an array of something like "class Point { int x; int y; }", you actually end up with an array of pointers to these Point objects -- which is HORRIBLE for performance in so many ways. In C# it would all be packed inside of 1 big allocation if you just replace "class" with "struct" (you know, just like how C/C++ would do it?). So in Java you end up creating big arrays of int[] or byte[] for everything and then having helper methods to manually pack and unpack things. I thought this was supposed to be a high level language?! Java is high-level enough to afford you great organization of application architecture into classes and packages. But when performance matters, and it does, it's not low-level enough and doesn't have enough escape hatches to afford you the ability to stay within its sphere without having to constantly punch holes into native/JNI land. And it's not high-level enough to let you afford you better abstractions within classes and methods. It's stuck in this middle area where C# surpassed it by miles and it never evolved to catch up, and it really pisses me off that Google chose Java for Android and is ruining the minds of programmers across the world with it. (Also, who the fuck invented Optional? That is the stupidest abstraction ever. We already have "null", ffs!)
Not Invented Here Syndrome perhaps?
async await is actually based on the generator pattern, and there is disagreement over whether or not those really constitute "coroutines." [Under that definition](http://wiki.c2.com/?GeneratorsAreNotCoroutines) proper coroutines are not supported in python. I'm inclined to say that python generators (and thus async) is not really a coroutine system because I cannot control where I yield to, I always yield back to my caller, which means that you have to introduce things like cooperative (event) loops to implement things that could in theory be done by jumping directly to the paired co-routine. That said with the addition of the main loop you can in fact do all those things, and it is probably easier to understand the concept of an event loop than code that jumps directly into another specified coroutine (plus the need for python to introduce some kind of `goto` type functionality) means this is perhaps the better way to handle things.
Thanks for this answer. Primitive values are too primitive - funny and true. You can't make a matrix in Java without a hassle. Java apps always did seem a bit clunky and slow. Well I do like Python a lot, but seems like I'll have to learn a bit more of Java just because the market asks for it. Again, thanks for the answer. 
Right. I was just relaying what the code does.
No problem. My best advice for being productive and enjoying Java? Stay away from C#! It will just spoil you rotten and make you hate Java.
[My response](https://www.reddit.com/r/Python/comments/6wrd8t/nice_lil_easter_egg_i_suppose/dmb8wnv/) was actually more along the lines of "Ugh, **I** knew that but forgot to include it" and not "get off my back". Give a little credit...
I did a bit of C and C++, and even python allows for some speedups. 
I have access to that article. Holler at me if anyone want to give it a read.
I find `valgrind` tends to be the least painful way to see what's going bad in a C-extension. It will detect invalid reads and writes, but you need a suppression file otherwise it will find thousands of problems with the Python interpreter. I have an example script and suppression file here: https://github.com/pydata/numexpr/blob/numexpr-3.0/val.bash https://github.com/pydata/numexpr/blob/numexpr-3.0/valgrind-python.supp The suppression file there is for Python 3.5. The memory model changed in Python 3.6, so you'll get all sorts of false positives in that case.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [pydata/numexpr/.../**valgrind-python.supp** (numexpr-3.0 → 98b488f)](https://github.com/pydata/numexpr/blob/98b488f783e2646d2fc3b9368cb3498fd7637301/valgrind-python.supp) * [pydata/numexpr/.../**val.bash** (numexpr-3.0 → 98b488f)](https://github.com/pydata/numexpr/blob/98b488f783e2646d2fc3b9368cb3498fd7637301/val.bash) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmc564u.)^.
In general, yes.
Spyder (comes with Anaconda distribution -- which I highly recommend) is pretty close. IPython (which is used in Spyder) is similar to the command prompt. Jupyter Notebooks are a different type of experience but I also think are great and you should try them. I will now give some unsolicited advice: Don't try to use python exactly as you use matlab. It can be done...but shouldn't be! Everything in matlab exists in a global name space. That is AWEFUL. In python, there is strong namespace support. So while it is faster to type `sin` instead of `np.sin`, the latter tells you and others much more about your intent. The best way, in my opinion, to use Python as a matlab user is to work through [SciPy Lectures](http://www.scipy-lectures.org/). That will teach you the skills without the bad stuff. Matlab to python cheatsheets are good for specific things (like Matlab's `eps` is NumPy `np.spacing`) but should be used sparingly vs learning it the pythonic way the first time.
Thank you, your thorough answer has made me less dumb!
Sweet! Thanks I’ll check this out. 
&gt; If I was stuck on an island and had to choose one thing to have with me, it'd be python Good choice. With python, you can knock up an assembler without too much effort. Then you can write a primitive C compiler. Then you'll be able to write real programs :-)
When you propose something new, whether it is software, hardware, food, methods, etc that have wide competition, it is imperative that the first thing you do it sell it! The very first thing you should see in the readme is "why this is better (or at least different) than a,b,c and d". It may very well be that it isn't better but it is exactly what I wanted (as is the case with most of my stuff. Better exists but I like my way) I will say though, the fact that there are so many ways to skin this cat is indicative of a need in the python community for simpler, better, argument parsing. I would be lying if I didn't think of writing my own as well! Of course, there is a [relevant XKCD](https://xkcd.com/927/) to the ensuing chaos. (all of this is to say, I agree with you)
Is there *ever* not a [relevant XKCD](https://xkcd.com/244/)?
[Image](https://imgs.xkcd.com/comics/tabletop_roleplaying.png) [Mobile](https://m.xkcd.com/244/) **Title:** Tabletop Roleplaying **Title-text:** I may have also tossed one of a pair of teleportation rings into the ocean, with interesting results\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/244#Explanation) **Stats:** This comic has been referenced 182 times, representing 0.1090% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_dmc627v)
Google not working for you? :-) https://duckduckgo.com/html/?q=define%20homoiconicity Broadly speaking, it means it treats code as data.
You can do it with a decorator. https://github.com/baruchel/tco I agree with you though (minus the rudeness, Guido is awesome), it should be in the language by default and we could find some middle ground for the stack traces (such as keeping the first ten or something)
I'm trying to automate trade in HitBtc. This week i tryed to develop a GUI ( with PyQt5) to automate refresh pair value and show to user. https://github.com/jmarcolan/pyQtGuiHitBitc
 In [12]: hash? Signature: hash(obj, /) Docstring: Return the hash value for the given object. Two objects that compare equal must also have the same hash value, but the reverse is not necessarily true. Type: builtin_function_or_method &gt; **the reverse is not necessarily true** Hash functions need to be equal for two objects that aren't equal, but hash collisions are allowed :)
TIL about pressing "y" on github. Definitely a good bot!
&gt; However, what is the point of python if you have to go though all this trouble? You could just learn C++ instead. Maybe you already know Python and don't want to spend [10-50 thousand hours](http://www.newyorker.com/news/sporting-scene/complexity-and-the-ten-thousand-hour-rule) of practice to become a C++ expert. 
Are you sure about that? Because I am 100.0% sure that 34895348573857348759 is not a bot. --- ^(I am a Neural Network being trained to detect spammers | Does something look wrong? Send me a PM |) ^\/r\/AutoBotDetection
Soooo, you are saying there is a chance? More seriously, although my skilllset is limited, how would I go abpout organising something like that from an Open Source perspective? 
Or just `10` in base π
Does it still go back to the loop if you have something like retval = yield from another_coro() 
`yield from` is really just sugar around ([the differences are mostly in cleanup after an exception](https://www.python.org/dev/peps/pep-0380/)): for x in generator_expression(): yield x Its just nesting that generator within my body. I'm still yielding to my caller. ------------ And explicit use of "yield" like this DOESN'T go back to the main loop, because there is no main loop in standard generator usage. `await asyncio.sleep(0)` yields back to the event loop, and the async/await system uses generators internally (which is why you CAN'T "yield" within an async function, its already a generator with a particular structure, you can't add an unstructured generator on top of that).
I'm interested if it is worth reading. Planning to start exploring numba when I get time..
When someone offers this for free I will sign up. 
You don't need a class for this. You also don't need to manually accumulate values for this. In fact, you can solve these kinds of things in one line if you don't care about readability: buy_sell = lambda price, buys, sells: (sum(v for k, v in buys.items() if k &lt;= price), sum(v for k, v in sells.items() if k &gt;= price)) Or make it readable: def buy_sell(price, buys, sells): return ( sum(v for k, v in buys.items() if k &lt;= price), sum(v for k, v in sells.items() if k &gt;= price) ) And if you want to cache results, just wrap it in [functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache).
I needed a tool to integrate a CLI with my Sanic app. Really?! there is no asyncio framework for CLI tools? Okay ... let's make one: https://github.com/ahopkins/asynccli
If you can put code only in the spot of `# CODE HERE` and nowhere else, I don't think you can do much better than: def find_num_shares_moved(buy_price, sell_price): buy = { 15:100, 14:300, 13:200, 12:100, 11:500 } sell = { 15:300, 14:100, 13:200, 12:400, 11:100 } buyers = sum([shares for price, shares in buy.items() if price &lt;= buy_price]) sellers = sum([shares for price, shares in sell.items() if price &gt;= sell_price]) num_shares = min(buyers, sellers) return num_shares If you can be a bit more flexible with your code, then pre-calculating the share amounts at each level is the way to go: def build(data, func): rv = {} for price in data.keys(): rv[price] = sum([shares for k, shares in data.items() if func(price, k)]) return rv buy = { 15:100, # 100 14:300, # 400 13:200, # 600 12:100, # 700 11:500, # 1200 } sell = { 15:300, # 1100 14:100, # 800 13:200, # 700 12:400, # 500 11:100, # 100 } buy_cached = build(buy, lambda key, child: key &gt;= child) sell_cached = build(sell, lambda key, child: key &lt;= child) def find_num_shares_moved(buy_price, sell_price): num_shares = min(buy_cached[buy_price], sell_cached[sell_price]) return num_shares Then, of course, if you REALLY wanna get crazy (complete with locking!): from threading import RLock class TradeBook(object): ACTION_REPLACE = 1 ACTION_ADD = 2 ACTION_REMOVE = 3 @staticmethod def _sell_caching(key, child): return key &gt;= child @staticmethod def _buy_caching(key, child): return key &lt;= child def __init__(self, buy_book, sell_book): self.buy_book = buy_book self.buy_book_cached = {} self.sell_book = sell_book self.sell_book_cached = {} self.lock = RLock() self._cache_update(self.buy_book, self.buy_book_cached, TradeBook._buy_caching) self._cache_update(self.sell_book, self.sell_book_cached, TradeBook._sell_caching) def update_buy(self, price, shares, action): return self._inner_update(self.buy_book, self.buy_book_cached, price, shares, action, TradeBook._buy_caching) def update_sell(self, price, shares, action): return self._inner_update(self.sell_book, self.sell_book_cached, price, shares, action, TradeBook._sell_caching) def get_buy_shares(self, price): return self._get_shares(self.buy_book_cached, price) def get_sell_shares(self, price): return self._get_shares(self.sell_book_cached, price) def _get_shares(self, book, price): with self.lock: return book[price] def _inner_update(self, book, cache, price, shares, action, func): with self.lock: if action == ACTION_REPLACE: book[price] = shares elif action == ACTION_ADD: book[price] = book[price] + shares elif action == ACTION_REMOVE: book[price] = book[price] - shares self._cache_update(book, cache, func) def _cache_update(self, book, cache, func): with self.lock: for price in book.keys(): cache[price] = sum([shares for k, shares in book.items() if func(price, k)]) buy = { 15:100, # 100 14:300, # 400 13:200, # 600 12:100, # 700 11:500, # 1200 } sell = { 15:300, # 1100 14:100, # 800 13:200, # 700 12:400, # 500 11:100, # 100 } book = TradeBook(buy, sell) def find_num_shares_moved(buy_price, sell_price): num_shares = min(book.get_buy_shares(buy_price), book.get_sell_shares(sell_price)) return num_shares
He offered it for free when it first came out. I was one of the lucky recipients. It's ten dollars most of the time. It's definitely worth it.
Could you help me understand how there are 800 buyers at a buy price of $13? Maybe I'm just having a brainfart, but I get 600 buyers at a buy price of $13. People would be willing to buy at a price of $13 if their buy price is $13 or greater. So that includes $15, $14, and $13. The buyer numbers for those prices are: 100 + 300 + 200 = 600.
...
Would it be possible to get around the web assembly requirements by creating a javascript module that creates hooks for arcade? 
Ok, that error is telling you that you need to assign the key in g_val if it doesn't already exist in the dict. You can't reference a key that doesn't exist yet, apart from assigning to it. You could either use collections.defaultdict, with the default set to a dict (when you define grid - you'll have to look up the collections module). Alternatively, you can put a guard if statement in: if g_val not in grid: grid[ g_val ] = {} ...which creates the key with an empty list. If you put that before your assignment in the error, that bit should start working. I'm not guaranteeing that's your only problem, but it'll move you forward ;-) 
No, you're right, I shouldn't have presumed your response. I apologize for that.
Would using .encrypt() be acceptable if you were to make a real messenger (even something simple like IRC)? E: is this way of making connections secure enough to be used for real messengers?
I haven't read it - just skimmed. It looks like it has a good deal of information as well a number of references ton other papers about JITs in general as well as python JITs. Direct Message me if you'd like me to send along a copy.
/r/python because it is love (and pays my bills) /r/rust because it is the future :) /r/lua because it is fun 
Not really, unless you want to run the game logic on the server and the presentation on the client.
It's people willing to *buy* at $13 **or lower**. And people willing to *sell* at $13 **or higher**.
Note that NIST has released an updated version of their dataset (that was used to create MNIST), called EMNIST: https://www.nist.gov/itl/iad/image-group/emnist-dataset It also contains samples of written letter characters.
[No, they're pretty bad at these kinds of problems.](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/)
You can do this with a dictionary of tuples where the first element of the tuple corresponds with the shares bought according the the price (key) and where the second element of the tuple corresponds with the shares sold. def find_num_shares_moved(buy_price, sell_price): d = {11:(500,100), 12:(100,400), 13:(200,200), 14:(300,100), 15:(100,300)} shares_bought = sum([shares[0] for price, shares in d.items() if price &lt;= buy_price]) shares_sold = sum([shares[1] for price, shares in d.items() if price &gt;= sell_price]) return shares_bought + shares_sold Here's another way to write the list comprehensions, though a little less clear: def find_num_shares_moved(buy_price, sell_price): d = {11:(500,100), 12:(100,400), 13:(200,200), 14:(300,100), 15:(100,300)} shares_bought = sum([x[1][0] for x in d.items() if x[0] &lt;= buy_price]) shares_sold = sum([x[1][1] for x in d.items() if x[0] &gt;= sell_price]) return shares_bought + shares_sold edit: typo
Right of course! Dunno why I was thinking about that backwards. Thanks
Aptana a eclipse bundled IDE supports py; breakpoint debugging, code completion, the normal ide stuff. http://www.aptana.com/
Good point. Hmmm, the dream of developing webgames easily with python do seem far off...
I've never taken a udemy course before, can I go back and revisit the course materials indefinitely or is there a start/stop time for it? What was your python background before starting this course? 
Bingo! I've done quite a bit of "recursion" with a stack/queue and a while loop. While loop is not empty, pop the next item off the stack, do a thing, maybe add more stuff to the stack, and carry on. Eventually you get to where there is nothing more to add to the stack, and it slowly empties, one item at a time. Include some accumulation variables to collect up the various results, and you're golden.
I'd love to hear your thoughts on Julialang, if you've evaluated it at all. Or any of the Python-speed-up libraries, like Numba or w/e. I definitely understand needing to use Fortran, though.
If I'm not mistaken one purchased its yours to use and reuse, some courses are even updated by the creator to keep things current and to add new content which costs you nothing once purchased. 
I would fail this as I cannot think quickly in interview situations. This would render my engineering degree useless. It would render my being a former member of the British Computer Society useless. It would render my being a former Chartered Engineer useless. So what is the point of this type of question? How would you answer it if you were autistic? I am no longer a member of the British Computer Society or a Chartered Engineer as my health is poor, so I live on UK state benefits, so I cannot afford the membership fees.
You can revisit udemy course material anytime. It does not expire. As the other poster mentioned, instructors often add more to their courses over time as well. Also it has a really impressive UI imo.
&gt; for i in range(1,len(b)): This is always a code smell in Python. Before you take this path always look at the [itertools module](https://docs.python.org/3/library/itertools.html) and then [more-itertools](https://pypi.python.org/pypi/more-itertools/3.2.0) to give yourself an insight into Pythonic code. Enjoy :-)
I got it for free and I highly recommend it. 
Some people use languages because they have to. I've used these [CORAL66/250](https://en.wikipedia.org/wiki/Coral_66) and [Intraduction to Monk Language using in Seebeyond eGate](http://seebeyondinfo.blogspot.co.uk/2010/06/intraduction-to-monk-language-using-in.html). Now go and research [Plessey System 250](https://en.wikipedia.org/wiki/Plessey_System_250). It worked during the First Gulf War, unlike the system that the US government bought from the French.
Thanks for the tips, especially lru_cache!
Thanks for the feedback! :)
Maybe Dart?
This solution doesn't work as two numbers need to be specified, the buy price, and the sell price respectively, and the answer should be the intersection not the sum. For example, using the original table in the post, for a buy price of 13 and sell price of 12, the answer would simply be 300 shares moved because we have people willing to buy at 13 or below (800) and people willing to sell at 12 and above (1000), yet the intersection of keys represents the shares moved, in this case 300 because 600 shares are being sold at 13, 12 yet only 300 are willing to be bought at 13, 12
I'm actually taking this course now and so far so good. I finished 15% and one lesson away from finishing the NumPy section. I've also taken his pure Python course, and it was also really good. His explanation is clear; he's not too fast, not too slow; his voice is not mechanised and not distracting. The course referenced in the article is the newer iteration of the course, he had an earlier one (starts with "Learning ...") so be careful when you choose.
Here: def buy_sell(buy_price, sell_price, buys, sells): return ( sum(v for k, v in buys.items() if k &lt;= buy_price), sum(v for k, v in sells.items() if k &gt;= sell_price) ) That gets you how many buy orders there are at `buy_price` and how many sell orders there are at `sell_price`. You can now figure out how to calculate the intersection.
There’s always a coupon out their to make any class on udemy only $10. Worth it IMO Edit: there’s also a chrome extension called honey that will try any coupon in circulation 
This still returns a tuple, and doesn't take into account the intersection of keys, which is necessary to produce the answer. We want one number which is the number of shares moved. I'll draft up a mock solution quickly
I know it doesn't fully solve the problem because *I'm not going to do all of your homework for you*. I'm showing you a simple and clean way to calculate the kinds of numbers you'll need to solve it.
I'm simply stating the solution doesn't produce anything that would help you answer the final question, for the final question, you want to store the subset of keys and values that satisfy the price limits and then be able to check if the keys are equal to each other before adding to the total. This answer only has the sum of the values for the given limits which isn't enough information to produce an answer. Sidebar, this isn't homework, it was an interview question I did poorly on that I was curious about.
The techniques I've shown in the examples I've given -- performing filtering folds over simple data structures, and how to efficiently cache results of specific calculations -- are in fact going to help you with this. The fact that they are not the exact precise implementation you happen to want is, again, because I'm not going to do your homework for you :)
Hey all, I'm the author of the post. Thanks for your great discussion. I just want to clear up a few things however. Regarding my discussion on what's actually happening in the interpreter I thought that I had adequately explained that I was no python internals expert &gt; I attempted to look at the Python interpreter source code but to no avail, if anyone has an answer stemming from this I would very much appreciate you contacting me and explaining it however, it is my fault if this was not conveyed well enough. Furthermore, this meant that my analysis was primarily me pondering why things "might" be happening based on my current understand. Following from this last point, I'd like to thank you all for taking the time to provide examples of decorators, better memory analysis, and other discussion. So not to cause flame wars, this post was not meant to serve as an argument for recursion or against iteration and I'm sorry if it was received that way. With these corrections in mind, I will attempt to communicate better blog posts in the future as I do this as a form of education for myself as well.
I'm not trying to challenge the authority of which I am a member of?
I'm using join() in my code: print "\nStarting OSCServer. Cancel with ctrl-c" #where serve forever is a function within the OSCServer class in OSC.py st = threading.Thread(target = s.serve_forever) st.start() try: while 1: time.sleep(10) except KeyboardInterrupt: print "\nClosing" s.close() print "Server thread finishing" st.join() print "Done" 
If the GIL was so bad then why has no one been able to improve over it with Guido's constraint of not decreasing performance? From my perspective it is really good as it is a very simple thing that is very efficient.
I've also found that just googling the name of the course in an incognito window will give you the discount as well. I guess they just automatically discount google referrals. 
It is unfortunately a bit contrived getting the right context in the handler and so the way `drain_and_shutdown` accesses `s` and `st` is a bit dodgy, but this should work. #!/usr/bin/python import time, signal, threading print "\nStarting OSCServer. Cancel with ctrl-c" class Serve: def __init__(self): self.running = True def close(self): self.running = False def serve_forever(self): while self.running: time.sleep(0.1) s = Serve() st = threading.Thread(target = s.serve_forever) st.start() def drain_and_shutdown(): print "\nClosing" s.close() print "Server thread finishing" st.join() print "Done" signal.signal(signal.SIGTERM, drain_and_shutdown) signal.signal(signal.SIGHUP, drain_and_shutdown) try: while 1: time.sleep(10) except KeyboardInterrupt: drain_and_shutdown() 
NFL
I hadn't heard of IDP until now. Here's the mess of files I've been working on. There may be a few mixed in tutorials I've gone through as well. I'm mostly interested in aggregating my leagues data in order to post a 'stat of the week'. https://github.com/pdbartsch/ff2017
Oh my bad, I didn't add the context to "s". S is defined as an OSC server: s = OSC.OSCServer(receive_address). However I'm still getting the following exception in Thread 1. Exception in thread Thread-1: Traceback (most recent call last): File "/path/python2.7/threading.py", line 801, in __bootstrap_inner self.run() File "/pathb/python2.7/threading.py", line 754, in run self.__target(*self.__args, **self.__kwargs) File "/path/python2.7/site-packages/OSC.py", line 1816, in serve_forever self.handle_request() # this times-out when no data arrives. File "/path/lib/python2.7/SocketServer.py", line 271, in handle_request fd_sets = _eintr_retry(select.select, [self], [], [], timeout) File "/path/python2.7/SocketServer.py", line 150, in _eintr_retry return func(*args) error: (9, 'Bad file descriptor') 
Can anyone tell me what kind of data they look at? And do they discuss how and why they use the analysis that they do? I really want to get into open bio data but could do with a good starting point. 
Real time embedded or safety-critical applications: Ada. Low-level drivers: C. Otherwise, Python.
A few questions from someone new to this subreddit: 1. Is sharing the norm here? 2. Are people overly concerned with pep, tests, documentation, and efficiency on low-stakes personal projects? 
Yes, most folks make their code available via github, blog, or even pastebin. &gt; are people overly concerned... This sub isn't immune to *rabid pythonistas*. You could just offer to share one-off for anyone really interested with the preface that it's not meant for general consumption.
Well to be honest nothing can be really as secure, because someone will always find a way through it
Anyone who doesn't want to PM this person, just search sci hub on your favorite search engine :)
Thanks for this!
A mix of these two is an unbeatable combination.
Just use Pandas, with your timestamp as the index.
&gt; s.close() I wonder if that is the right way to stop the server; is `shutdown` what you want? https://docs.python.org/2/library/socketserver.html#SocketServer.BaseServer.shutdown 
I just started reading this! The section on Ipython is hella informative by itself. I've just started into the Numpy section and it's really filling in the gaps for me. Definitely recommend looking at it if this falls under your interests.
Small world. I just watched a really fascinating talk by the author of this book yesterday. https://youtu.be/W9dwGZ6yY0k and https://www.youtube.com/watch?v=9by46AAqz70
I'll try. I ran the exact same program on windows and it didn't show any exceptions. It may be a OS X thing 
Thanks for the notice, I fixed the problem!
zxcvbn
I'm about halfway through his class. It's a good intro, but I haven't seen anything with regards to hyper tuning parameters with the scikit-learn models. I prefer thus course so far: https://www.udemy.com/machinelearning/ Although I'm not planning to learn R any time soon, you get a double whammy with this course.
A quick look reveals that I think this is exactly what I am looking for. Thank you for your response. 
Recursion is a great way to perform side effect free iteration. With TCO it's just as fast as a while loop
I was never under the impression that you have to 'think quickly' in interviews, but rather just demonstrate your thought process. &gt; ok my data is like this so I would use this structure to start because there's not a lot of it should be good enough. &gt; Next I need to make sure I understand the problem and solution so given input a the result should be A1 and given input b the result should be B1 etc. &gt; Next I want to do step_a followed by step_b so I'll stub those functions out and call one after another in my solution. &gt; Now I have the basic skeleton so I will fill out my stubs and run through the test examples I wrote out earlier to see if they pass. &gt; Now that my code completes the assigned task I'll give it a once-over to improve readability by making certain things more pythonic, using more descriptive variable names, and adding some docstrings. (To the interviewer) &gt; Does this solution meet the desired criteria? What type of feedback would it be likely to receive in a code review?
Only the people between the bid &amp; ask count - you can't sum-up the others (as most people have done here). def find_shares_moved(ask, bid): buyers = {15: 100, 14: 300, 13: 200, 12: 100, 11: 500} # bidding sellers = {15:300, 14: 100, 13: 200, 12: 400, 11: 100} # asking bidders = {price: num for price, num in buyers.items() if ask &lt;= price} askers = {price: num for price, num in sellers.items() if bid &gt;= price} # The crucial part: common_ground = bidders.keys() &amp; askers.keys() bought, sold = 0, 0 for price in common_ground: bought += buyers[price] sold += sellers[price] moved = min([bought, sold]) print('Ask: %s, Bid: %s, Moved: %s' % (ask, bid, moved)) # return moved Output: Ask: 13, Bid: 12, Moved: 0 # lowest seller @ 13, highest buyer @ 12: no transactions ! Ask: 12, Bid: 13, Moved: 300 Ask: 13, Bid: 13, Moved: 200 Ask: 14, Bid: 14, Moved: 100 Ask: 12, Bid: 12, Moved: 100 Ask: 15, Bid: 15, Moved: 100 Ask: 15, Bid: 14, Moved: 0 Ask: 15, Bid: 12, Moved: 0 
Penises
Not an expert at all just genuinely asking, what's the reason for using python and the other tools (I'm not familiar with those) instead of something like Tableau for a dashboard? 
I do not think that for physics or mathematical codebases, `np.sin` is at all more communicative of intent than `sin`. You are writing an equation, namespacing math functions obfuscates the underlying equation.
Yes, this is the only working solution. Curious, is there any more efficient way to achieve the 'common_ground' without using &amp;, if I were in Python 2 than a nested for loop?
In the context of python... variable type is pretty obvious when declared. Mutating a variable is mostly perverse and should be avoided. Saving static type defs is good for productivity. For functions/methods, type hints mostly give me what I need for static analysis and self documenting/user understanding.
Trying to get up to speed for a data science program I'm starting in mid September!! Starting to feel a bit overwhelmed to be honest...but I Love the feeling of writing something and having the expected outcome appear. So gonna keep chasing that. 
`set(bidders.keys()) &amp; set(askers.keys())` Just one extra step - still very fast.
&gt; 600 shares are being sold at 13, 12 yet only 300 are willing to be bought at 13, 12 This is the part of the problem I don't understand. Can you explain how you got the 600 shares being sold at 13, 12 for the table above? Is it because under 'sell' column the values for 13 and 12 combined are 600? and then for the 'buy' column under '13' and '12' the values add up to 300? This example problem would be much clearer if you provided example inputs and outputs to the `find_num_shares_moved` function.
Yeah this isn't a shady request at all.
 from itertools import chain [key for key in chain(bidders, askers) if key in bidders and key in askers] Certainly not more efficient but no nested loops.
Yes exactly. In plain english, if you were willing to buy an item for 5$, and it was being sold for 4$, would you not by virtue of your upper limit, be willing to buy the same item at the lower 4$. The inverse applies to the seller. If you were willing to sell an item for 13$ and there was a market for it, would you also be willing to sell the item for 14$ if there was a market for it. This is the logic for the transaction in general. Basically if you were wiling to sell at X, you would sell at X, but you would also sell at any higher value than X. If you were wiling to buy at Y, you would also be willing to buy at any lower value than Y
Also the only caveat to this, that is rather than using separate variables for bought and sold, you would rather just add to moved for those sold, and subtract from moved for those bought, and then return moved if it was &gt; 0, otherwise just 0. Then we have a 100% working solution. Very much appreciate your help/input
Yes, nice observation. Also, I was thinking - if you absolutely wanted to avoid the `&amp;`, you could use `in` to test for membership in both sets of keys. Slightly more verbose, but it would work. Aaah -- I see that u/Starcast has beaten me to it ... nice job.
Working on a crawler. Trying to get a db form the entire WEB without googling. 
Just a guess, but I bet it's an explicit type check, because a tuple on the right side of % means positional formatting instead of name based. It's far more likely that passing a tuple along with a format string with named fields is a mistake, than that it's a custom class that happens to both be a tuple and have a \_\_getitem\_\_, I suppose.
Yep, that's it. That's profoundly annoying. If I create and monkey-patch the same object as in the example, but I subclass `tuple` instead of `object`, the working example starts to fail :(
Try this [link](https://www.freetutorials.us/python-for-data-science-and-machine-learning-bootcamp-2/)
to be more specific, in byteobject.c and unicodeobject.c, PyTuple_Check(args) is called before the format string is even evaluated for a dict.
+1, reusing variable names within the same scope is a cardinal sin in all programming languages. Carefully documenting the type(s) and purposes of arguments and the return values/types of all your functions and methods will help a ton as well, so that maintainers can perform type inference without having to look up where a given variable is initialized and what type of object the initialization returns. (Without this you could end up diving down a rabbit hole of function calls and class construction. The presence of function and class factories can make this task nightmarish.) This is even more important if you're using a version of Python that doesn't support the optional type hinting syntax.
I've always found anything programming oriented involving electronics like you said go into how to do stuff step by step and not as much why it works or how you can understand it outside of just mimicking what the tutorial says to do. 
This isn't/wasnt part of that humble bundle. Sorry folks, got the book mixed up with another. 
I thought the same after having seen the the Humble Bundle earlier today and checked https://www.humblebundle.com/books/data-science-books, but it's actually not part of it.
Looks like a simple rotation cypher. So, no, not even remotely secure, but useful as a learning tool.
I don't know much about python, how long will it take to be able to get a job in data science and how can I do it? edit: wow, talking about a very supporting sub right here huh!
I completely use it. I change None's to values all the time. If the code is bad, then I hate it. If the code uses it well, it's fine. And why would reusing variable names in the same scope remotely bother me? I'm overwriting it. Kill the old variable. Forget what you learned in CS. It's just math. It should do what you tell it to. What's wrong with: def some_function(y): if y &gt; 4: is_passed = False return is_passed, None x = np.array([1, 2, 3]) is_passed = True return is_passed, x 
Honestly I didn't even know what type of cypher I was making, I just used my available tools and that's the best I came up with.
almost every line is wrong actually and that isn't hyperbole. it sucks as an api, it shows a lack of rudimentary python knowledge, and it is 5 lines longer than necessary. def somefunc(y): return None if y &gt; 4 else [1,2,3] ret = somefunc (y) if ret: do stuff also this isnt what the op is talking about. reuse that's bad is more like ... x = 5 . . . x = 'hello' . . . return x*5
Mostly automation. This platform has a 'billing run' where our customers pay for their usage. There is currently no portal for customers. But by using admin command line, usage can easily be determined. So with python this is easily automated and with all the javascript charts that are available. It looks pretty too. The snmp monitoring and giving a sence of the health of the platform is just a nice to have.
dynamic typing absolutely can be useful. duck typing is the first thing to come to mind but consider the chore of mapping types to a SQL return. you're code will end up many times larger in a statically typed language. what about parsing something like json or yaml that returns an arbitrary native type? that sucks to deal with at compile time.
&gt; reuse that's bad... Why is it bad? Attempt #2 def some_function(y): result = None if y &gt; 4: is_passed = False return is_passed, result result = np.array([1, 2, 3]) is_passed = True return is_passed, result 
see my above edit
If you'd like to play with something more secure which is also not trivially braekable, look into RC4.
You should really take a look at PEP8! Your names are just *unpythonic*. Do you know another programming language or do you have a learning resource that suggest pascalCase for functions? Nevertheless just learn about PEP8 and try to refactor your code. If you use a learning resource that violates it in many aspects, consider this resource as **bad** and strive for a better alternative. And you should stop using abbreviations for variables! ``let`` for letter is really a mess. Often it is considered that you should only use those in a well defined domain. For example in Germany you could use ``mwst`` in a tax related context as it us a well defined abbreviation for our tax on goods and services. Another examples are ``i`` for an index value in a mathematical context or ``x``, ``y`` or ``z`` for coordinates and so on. But overall prefer explicit names for things.
This makes for some interesting read,does you have an in depth analysis that goes as far as country level?
I use Powershell to do a lot of Windows administrative automation tasks. It can control things that are not directly possible to do with Python, specifically creating Exchange and Office 365 accounts. It may be possible to do this with REST interfaces that are provided for some O365 services, but I haven't looked too far into it since it's easy to do in Powershell and I haven't needed to. Powershell is an extremely powerful and capable language in its own right, although it's definitely got a unique syntax and approach to things. But very much worth learning if you are in the Windows world.
Haha don't get discouraged man, keep at it.
His name shows up any time theres Python and AI/ML lol. He wastes no time getting the latest scoop. Granted, it's literally his job to. I made a post about something he made in the ML subreddit and he commented on my submission. 
There is probably a wrapper for the C library or something, but why bother.
Do you have the code for the Redis cluster publicly available? Would like to have a look disclaimer: I'm the maintainer of aiocache, like to see if people use the library as I wanted/thought :)
Thanks for the review, I'll definitely look into this now
Yeah probs, but only ever gonna pass a couple of arguments and just a little tool to make a repetitive job automatic. So yeah, why bother.
&gt; almost every line is wrong actually and that isn't hyperbole. it sucks as an api, it shows a lack of rudimentary python knowledge, and it is 5 lines longer than necessary. You are correct that THAT function is horrible from a design perspective, but it should be clear it's faking the result for the failed case. The point was not that function (or the lack of a docstring or useful variable names). The point was to make an if block that made the result split and do something generic that if that function was actually conceptually dense would be hard to see. You're complaining about something that is not the discussion at hand. Your example could be shortened as well two two lines because there's a junk line, but obviously that's missing the point. Were I to set: x = 5 x = 4 return x This is also bad. x = 5 del x x = 4 return x Why is this less bad? string = Still, whatever...it's not hurting anything. If the function is documented and designed sanely (and say uses None as a placeholder), then who cares? I've been coding in Python for 11 years. I run an open source library. I know how to make an API "Pythonic". I also know how to make an example that illustrates a simple point without getting caught up if my code is 100% "Pythonic". Don't be so argumentative for an obviously fake function.
thanks for sharing this, been looking for logging module, and this won.
I think your code can be simplified. buyer_count = sum(num for price, num in buyers.items() if ask &lt;=price &lt;= bid) seller_count = sum(num for price, num in sellers.items() if ask &lt;= price &lt;= bid) return min(buyer_count, seller_count)
woah dude that is seriously awesome! Thanks stranger! It was a risky click but was definitely worth it
Same for me, but no interviews. I even participated in lowrezjam.
Responding to technical concerns of highly experienced contributors with name-calling is not respectful or constructive. Please do not do this.
The answer as to whether homebrew crypto is secure is with overwhelming probability bordering certainty "no". As long as you know that and see it as a toy to play around, there's nothing wrong with that. Just don't even think about using it for anything remotely serious.
I tend to use Java a lot. Through Java you can learn a lot about programming and how developing works. You can learn almost everything from the internet. Furthermore, there many things that are easier with Java than Python. The only thing is that java is a lot more complex than Python which makes it a lot more vulnerable to errors. Those can be dealt with programs as checkmarx but it's better to try and avoid them. Good luck.
I know that in theory everything could be cracked, but how hard is it to find a way through this?
sadly no, but it was pretty much "oh you a single redis? here's a sentinel cluster instead" 
Using the APIs of Reddit and Spotify to automatically populate a [Spotify playlist](https://open.spotify.com/user/savvasstephanides/playlist/5ma1IZtNj6bZ2EGu7KO0GV) with the most upvoted Spotify track and album links posted on my favourite music subreddit. The playlist is updated weekly. https://github.com/SavvStudio/RHardstyleSpotify ^(I am aware that I said Spotify 3 times in the same sentence.)
Wow. Why does hash(-1) return -2 instead of -1?
Mostly building out our CI process and testing frameworks, django-nose and coverage. 
Hy, because it has great interop with Python and powerful Clojure-like macros. Cython, because it is rare and valuable for a compiled Python-like language to be useful inside Jupyter notebooks. Nim and Julia, because they both drew some inspiration from Python but did some things in a more elegant way. Also, I still (sometimes) need some C++ to understand PySide and some Tcl to understand Tkinter and (rather often) some Javascript to use Flask.
Your code formatting looks dodgy mate
Are you talking about errors like "character U+12345 is above the range allowed by Tcl" or about something more obscure? The problem exists, but using Chinese input with Tkinter is definitely possible.
https://github.com/Rapptz/discord.py is a completely async Discord API wrapper.
/u/tetroxid /u/cephalopod1 &amp;nbsp; You can also do the same thing in pure python with a bit of setup: https://www.youtube.com/watch?v=js_0wjzuMfc&amp;t=28m09s
That's not a python...
Thanks, will take a look into that.
Scraping the shared directory at work and establishing symlinks to automate test distribution to related CSVs.
Meh
Programming paradigms.
I'm trying to make a script that analyze a password to see how long it would take to break certain with certain methods haha. I swear I'll use it for good not evil.
Looks like just what I needed thanks!!
Isn't that conflicting what you said earlier? &gt; Why can't there be healthy competition? Shouldn't you, especially as a member, try to contribute **directly** to the project you're a member of?
As in, not style. The formatting seems broken. Line breaks not coming through properly.
Yes I think that encrypt() should hold up some people from looking at the data that is being passed through the socket. Can't really assure that that is the best solution though.
This is a small GIS software that I've been working on. In roughly 100 lines of python code (in tkinter or pyQt5), you can import shapefiles to display a map using any projection supported by PROJ4. At the end of the readme, you'll find a detailed explanation of the algorithm used in the software, as well as the algorithm itself (it only takes 15 lines of python to open the shapefile, extract the shapes and draw them). FYI, this is actually a "spin-off" of my main project, pyNMS, an open-source Network Management System (as yet unfinished: https://github.com/afourmy/pyNMS). It occured to me that the GIS / map visualization part alone might be of some interest for GIS people and I made it a stand-alone project. 
Why does it need to be specific to Zybooks? You're learning python, python specific resources should be all you need. If you have any specific questions, you could ask her or on r/learnpython
Bad code is bad, and changing the contextual type (hereby meaning the protocol/interface your function expects that variable to maintain), of a variable binding over its lifetime, is generally a bad idea. I'd probably assert that maybe 75% of my code would be fairly straightforward to add full type annotations to, if I so chose, and the much rest would be bothersome for the same reason writing types is tiresome in statically typed languages, not so much because its misusing the types.
Eh, I dabbled in LUA and c++ but I'm completely self taught so I never learned want kind of formatting other than what felt right
Yep! I basically went into this going "I could make l33tspeak to English translator"
Whoops, you're right. Comment fixed. 
U better start learning Python first and when you are confident about ur basics start learning data science. Work on 2-3 mini projects then move on to bigger projects. Can't say how long it will take for u to get a job as a "data scientist" because it totally depends on how fast you are able to learn , understand and implement.
I'd rather not because they usually become mandatory, which causes a lot of good posts to get removed (see /r/oculus )
You might be interested in the cryptopals challenges--there's a set of 8 challenges that take you one step at a time. cryptopals.com 
Python is a glue language for the web because of the tools we have, but there's probably a tool out there that allows Python to do JavaScripts job and it'd do quite well. I've written games in Python, and fully fledged desktop programs. It's certainly not a replacement to bash (although it can be used like bash). JavaScript is great for websites, but on the desktop it's rather slow. Python is just as maintainable as Java if not more, and yeah. It kinda can do anything it seems. Everything I come across seems to have a Python binding, it's one of the things putting me off learning C++! So I'd probably have to say learn both since they can each be used for different ways of pentesting. 
Welcome to my dream team. To clarify for those of you who dabble in sports dream teams, my CS version is the exact opposite, namely people I definitely would not like to work with.
&gt; I've been coding in Python for 11 years. You know I've been watching one of the test engineers in our company struggle to do any actual testing or automation for the past year, and apparently he's been working for our company doing God knows what for the past four years. Doing something for a long time doesn't mean you're any good at it. &gt; I run an open source library. You mean you host the repository for it and somebody else writes the actual library or what?
Currently setting up a MySQL database to go with a database program I've made. …it isn't going too well…
Soz bro, I know there's always someone who pops up as 'waah this isn't PEP8 standard eeeeh'. Yeah unfortunately blogger is a bit of gash and screws it up when I copy paste straight in from pycharm. Might have to try something else. But for now meh. Cheers though 
Generally speaking returning tuples is a bad thing and leads to messy, unreadable code down the line. It relies on the calling function to unpack the result and automatically know which variables are what based on position alone, which leads to lots of implicit structures with unnamed members. If you're going to have a "valid" flag you should put it in a class or namedtuple or something.
Learning what the heg python is, how to use it and how to get nice visuallizations of my (at leas to me) complex data with it. It probably is very simple but I have very little experience with stuff outside GUIs. Wish me luck.
Learning what the heg python is, how to use it and how to get nice visuallizations of my (at leas to me) complex data with it. It probably is very simple but I have very little experience with stuff outside GUIs. Wish me luck.
Thank you! I know something about Javascript and few of Python,I can program with some programming languages,so I'm not new on it. Should I continue to study Python or Javascript? Thank you!
I work in the industry, learn both. There are a lot of vulnerabilities in JS code, but you will probably wanna use python to create your potentially malicious web requests in an automated fashion. It is certainly fast enough for pentesting. Learn JavaScript so that you know the best practices and can spot bad code, write tests etc. Look in to Asyncio, threading, multiprocessing. If your program needs to run more than a single function and wait until it is done. They serve different purposes and i can't tell which one you need.
This looks amazing, thanks for sharing both projects.
Other subreddits have AutoMod assign a default flair if the poster doesn't choose one instead of removing it.
[removed]
The humblebundle has a datascience ebook collection this week for 15 bucks and pretty sure this book.it in it along with 8 others.
Thank you so much :)
https://benchmarksgame.alioth.debian.org/u64q/compare.php?lang=node&amp;lang2=python3
Thanks for sharing! It's always nice to see what resources other people are using. I use a site called The Huddle, here's my script that just takes info for a year and saves it in a pickle for later. https://pastebin.com/VYu5nURL *Btw, IDP is pretty fun. It makes things a bit more complicated but I end up enjoying football more because I pay attention to defensive players more than before.
 It certainly is a good replacement for bash script. I have all my co-workers write their scripts, that they used to write in bash, in python. It was slow going at first but we've built up a library and now it's actually faster and easier to write everything in python. I've done tests and found that I can do a lot of things faster in python, like searching files for strings (grepping) and sometimes even one liners run faster when written as a python script. 
Both. If you want to make desktop apps then Python, if you want to make web apps then JavaScript. If you want to do API injection then Python, if you want to do website injection then JavaScript (and SQL).
No. I write the library and started it. It 200k lines and gets 400 downloads a month. It''s 6 year old and still going.
You asked whats wrong with this code...and i answered you. I also gave you an example of bad reuse in the sense that OP was asking and you keep ignoring it. You are literally talking about something else entirely. I also gave you an example of bad reuse in the sense that OP was asking and you keep ignoring it. ** Edit - perhaps i'm not communicating this well so i'm going to try to be more explicit The key to understanding this is that there are more criteria to consider than the domain / range of a function. First, related to your code example ... You are saying you created this bad example to illustrate it's ok to reuse. But you actually are making the opposite argument. Because of the resuse, your code grew 500% - and yes this is a criteria for the usefulness of a dynamic language. Also you are doing obviously unrelated, but equally bad things like forcing me to use your numpy array instead of just returning a list but I digress... Code size and complexity matter and in non-trivial examples where you are maintaining a return state ( a common but bad idea ), you can get quite far into the control flow and be unsure what is actually going to return. This sucks if I'm your coworker. This is very important in dynamic languages b/c I have to infer the type from usage or from your good naming conventions, etc. You can have crappier names in c++ b/c i can see that some variable named 'f' is a float and always will be a float. More on this later ... Ok but reuse in general ... There are two kinds of reuse that are being conflated here: * Changing the value of a variable; normal practice, bad for mathematical analysis * Changing the type of a variable; terrible practice in all cases You are changing the type of a variable in a normal way. You're assigning default values that may change, you're changing state variables, you could be accumulating a value etc. This is normal and fine most of the time. it does make it hard to work out a mathematical correctness of a function though. BUT when changing the type as in the above example, you find different types respond to the same operators in different ways. I gave something like: x = 5 if condition: x = 'hello' return x*5 There is almost never a case where sonething like this is intended behavior. A more common example from python 2 is: def divide_x(x): denominator = 3.0 if expression: # let's assume this is true denominator = 2 return x / denominator divide_x(5) &gt;&gt; 2 divide_x(5.0) &gt;&gt; 2.5 # OOPS ... This case has two different types responding to __div__() but with __div__ doing integer division in one example but not in the other. If the type didn't change, 2 would be coerced to a float in the if clause and you'd get the expected result. Changing type mid-flow is bad in the function but it's also bad if I'm receiving your output b/c now I have to type check your result. In a dynamic language predicated on duck-typing I now have to type check your result. Let that sink in. I now have to type check your result. It bears repeating: I now have to type check your result. This is not ok - and if you try to contrive an example then you are just out of your fucking mind.
Huh?
I wouldn't consider myself a data-anything, but I use IPython all the time actually. It's really great on it's own to explore third party modules and do on the fly testing compared to the regular python shell.
A numpy array is not a tuple. It's one data object with contiguous memory. It uses fewer bytes of memory for the same length and precision list/tuple. It's also 500x faster you use it right and 2x slower if you don't. You can unpack it though. Are you referring to `(x,result)`? The example is a pure function. Why would you make a class for that? It's just more code and more ways for things to be confusing. People know standard data types and standard library error messages. Unless, you really need to organize something because it's complex, it's better to ueuse tgings people already know like tuples. I don't see the point of making a class. Still, the point of this thread is dynamic typing, not how to shoten/improve code that demonstrates why someone might use a dynamically typed function.
I never heard about API Injection! Thank you :)
This is directly contributing. 
who's this "everyone" that says Python is "only a glue language"? There's a ton of software written using Python as the primary implementation language. If you're interested in web development than try doing a little development with a Python web framework like Django. 
This guy (the author) knows his shit
10 years because you need a phd in say math or cs.
If you want to make cross platform games, just use Unity engine. Python on mobile is not good. A lot of python is used in security. Computation speed doesn't matter for network because the cpu is much faster than the network traffic, so even the slowest program will end up waiting for network traffic, resulting in no speed difference for language selection. Most programmers know multiple languages, so don't worry too much about language selection in the long term. Javascript (node) and Python both have the same concurrency problems. Are you really going to write something that runs into scalability issues? Many huge websites like youtube and instagram use python backends. Scalablity is something they have to worry about, but do you have a billion users?
Thanks :)
Thank you :) 
Thanks :)
The main reason to use JavaScript is the fact that it runs natively in web browsers. Don't discount that; it's a HUGE advantage and when random people walk up to me and ask what language they should learn first I usually say JavaScript just because they already have it installed (in their web browser) which illustrates just how ubiquitous it has become. Python is better for basically everything else. It's more powerful, flexible, expressive, cleaner, better more mature ecosystem, I could go on. Don't worry about performance until you've written something and proven that it's too slow. Python and JS are both fast enough for most stuff and both have ways to make them even faster if necessary. Programmer time is far more expensive than computer time.
&gt;I want to be a Pentester (and It is my focus) and I like making the Front end side of websites and videogames. With JavaScript I can be a Frontend Developer and making games even for mobile thanks to React Native,Cordova and Ionic and inject Javascript on websites and many other things. &gt;Actually I am interested on knowing more on Python and use It as a main language, but everyone say Python is only a "glue" language,It isn't easily scalable and maintainable than languages like Java and it isn't very suitable for concurrency and other things because it is slow. Well "everyone" is full of crap. Python is a very usable language in a number of realms. It isn't Java (that is a good thing) and lily will never be "THE" language for concurrency but the need for concurrency in a language varies with how it is applied. No language is perfect but Python has a lot of things right. &gt;On the other hand, I heard people very enthusiastic about Python and this language can do everything,but it is true? Well yay there is a subset of programmer out there that are enthusiastic about Python. That however means very little as there are always enthusiastic flakes out there, Even APL had its champions. As for do everything the is also a bit of a joke. Python simply isn't an optimal solution for many tasks just like Assembly language isn't optimal for many tasks. Python however is a lot more useful in the modern world than assembly language. &gt;Is Python more than a replacement of Bash? It isn't a replacement for BASH anymore than RUST, SWIFT, C++ or any other language. Python has many many advantages over Bash, one being readable code. Sometimes it is better to Use Python over BASH to get something done, but not always. These days writing simple scripts is perhaps the least demanding use of Python. Python is used heavily in the sciences and other domains outside of the world of the web. I suspect what happened here is that your opinion was colored by a few initial comments that frankly are uninformed. Consider instead where Python is being used these days. It is seeing very heavy adoption by professionals that aren't programmers for example. &gt;Thank you!
Python: selenium (if you need to create web browser based testing). Scapy if you need to craft custom packets. Scapy even has an easy way of defining new packets (basically just a field name and the byte length).
This is total bullshit. A primary use of dynamic typing IS To SHORTEN CODE. What he's telling you about the tuple (while i'm not sure i agree 100%) is that it relies on meta knowledge about the data you are returning that goes beyond it's semantics. Numpy arrays while not what he's talking about at all, are bad here b/c now i'm forced into your data structure and your code is instantly less useful than if you used a more generic type. If you didn't 'forget everything your knew in cs class herp derp' you might understand these very fundamental concepts. Are you oblivious to the fact that you keep using the jargon wrong and keep getting confused about what people are saying? This is a huge clue that you are not as conversant in the subject as you think you are.
I notice you haven't linked it ...
You definitely want me on your team. I use exactly one variable to hold the string 'l337 h4x0r' and I format my code into ascii art related to the thing I'm coding so you know what the code is doing. I don't waste time on useless things like functions and more than 1 file so you never have to search for where I defined things. 
he talks about it every other day. it's some glue library for ancient simulation shit. his background is matlab/fortran bullshit so no surprise he doesn't feel much need to be pythonic.
OOOH now I know why he sucks LOL. I wouldn't even be offended at his underwhelming coding ability if he hadn't started by trying to shit on CS people while simultaneously being wrong with every single thing he says. Many scientists have pet libraries written in shit no one wants to deal with. They are often the only game in town for other non-programming scientists and so it gets some uptake regardless of quality. I don't find this to be an indication of success or quality.
I'm going to be honest with you here, not just pissy... You have no idea WTF you are taking about. Making a function return the expected value is only the beginning and is nothing special. If you are maintaining a wide spread library and aren't spending more time asking the USERS about usage semantics and designing your API around that, then you are fucking up. Once your terrible api gets uptake, it can almost never get fixed. I'm guessing you are a scientist or engineer of some sort who thinks that b/c he opened up a python interpreter 11 years ago that you are a programmer. You are not and it shows. You need to admit you can learn something. You are trying to respond to someone about a subject you have no fucking clue about and the problem is you don't know you have no fucking clue. You need a little more self awareness and the ability to admit there is more for you to learn.
Not surprised to see discussions regarding Python as a glue language when "Python glue language" is the query. Try filtering the results to the past year and see how the conversation changes.
Nice! Thanks for responding. 
Python, C, ML While training to PenTest remember to only practice on machines/networks you own! VMs will help a lot, simple IoT can be accomplished with a handful of SBCs and a cheap "Preowned" server from a flea market
If you're doing time series analysis you may want to look into time series shapelet as an option here is a brief page about it https://www.uea.ac.uk/computing/machine-learning/shapelets. This is becoming an increasingly popular high powered option for analysing time series data. Here is an example of someone who has written a package for it https://github.com/mohaseeb/shaplets-python but there are more out there. 
I would say that out-of-box the main drawback is that it isn't built for speed--though you have libraries built in C/Fortran/Rust with Python bindings if you really need that kind of power. I believe that you may have some misconceptions about the language's domain. Python, in addition to being really awesome for scripting on servers (like Bash), can build just about anything given its wealth of libraries and expansive package ecosystem. This is why it is so popular for not only prototyping ideas, but building the scalable backends of production systems. For instance: I work in a data engineering role where Python runs our entire pipeline in which we process terabytes of data daily. This isn't just simple server scripts but robust programs built on top of Apache's data processing libraries like Hadoop, Spark, and Kafka to facilitate large scale concurrent processes. Testing, deployment, and everything else is done in Python.
A downvote for advertising PEP8? Come on... why are you reading here in this subreddit? 😎
This book is not in it. 
I must say that, while I clearly knew not to modify a dict while iterating over it, the fact that it did the loop 8 times still confuses me even after I rationally understand what's going on (it preallocated 8 buckets, and fills them with dummies). I just don't see how it makes back into python. 
No. That is incorrect. Function calls are always more expensive than loops. 
Forked scikit-multilearn (for multi-label classification) and tried improving the documentation. Next step would probably improve the unit tests and fix dependencies in graph-tools and MEKA (mostly C++ and Java libraries) when doing integration tests
Thanks for recommending, I'll check it out. 
&gt; "Python" destroyed the existence of "JavaScript"? I like this guy.
Why not provide a pip package instead of npm?
Congrats, but this would be better suited to /r/LearnPython as this is pretty much Python for beginners
"... bite you off!" Ouch!
Very true, anything that involves string manipulation and the 'while read x' type bash stuff is probably up to 100 times faster in python, even something that uses the OS command line from python like renaming files. 
Oh, thanks for letting me know
&gt; \#TODO: Add pypi package for reading via command line
Aka "gotchas" 
Not with proper tail calls. The interpreter turns it into a while loop
At last, an appropriate use of the WTFPL.
Backslashes not being allowed at the end of "raw" strings has always seemed bizarre to me. Very odd indeed. Raw strings are supposed to be simpler than escapable strings. Seems more of a bug than a feature to me.
&gt; It isn't easily scalable and maintainable than languages like Java and it isn't very suitable for concurrency and other things because it is slow. As someone highly fluent in both, I'd say that in practice Python is both more maintainable than Java and more performant; you *write* quite a bit less code in Python than in Java to do the same thing, and while "straight" Python can be pretty slow, you can get back up to C speeds (well, C-*ish*) with the right libraries and code layout. As far as maintainability goes, our Python projects don't have a tenth of the production bugs that our Java ones do, because they're about 100 times simpler.
Sorry, I don't get this at all. Why would you *print* a class definition instead of generating a type object? Like, are you supposed to ```exec``` the string, or what? Why not return the actual class you could instantiate?
Curious to see others' responses, but if you have any sort of Android device [AutoCast](https://joaoapps.com/autocast/) might be a quicker/more convenient solution. If nothing else, given Tasker's extensibility you might be able to use part of it in your solution.
&gt; Forget what you learned in CS. It's just math. It should do what you tell it to. If I may - and this isn't meant to belittle - the difference between *writing code* and *being a programmer* is contained (or, not contained) in this statement. Yes, it's all just algebra and the lambda calculus, and yes, the computer does exactly what you tell it to do. But *what* did you tell it to do? Managing what you're telling it to do, and knowing that you should tell it to do some things and not others, is what makes a programmer; and the way the programmer makes that decision is on the basis of the code as a text artifact that talks to *humans*, not merely to computers. Sometimes that's wrapped up with the phrase "readable code", but a lot of people take that to mean "as long as the code is technically correct, you can make it readable with lots of comments." This view is short-sighted, in my opinion. Readable code *itself* expresses the programmer's intent clearly. Good readable code has almost *no* comments; the code is the comment. And one of the ways in which you achieve that clarity of intent is to not trick or mislead the human reader with whip-crack shifts in variable type. It's *not* just math. It should be like poetry.
Yeah I was quite surprised that it was faster. They're both essentially using compiled c code 
Shell tools such as grep/awk/sed are actually very optimized, possibly more so than Python. The problem is that any time you want to actually use/parse/process those results, you have to do them within shell by spawning these tools repeatedly, and that's extremely slow even by Python standards. The bottleneck of shell programming is in the forking/spawning new processes, which ranges between a few hundred μs to a few ms depending on your disk, RAM, operating system, and the size of the program itself. The only exceptions are a few commands that are native to the shell itself (“shell built-ins” as they are called), such as `echo` or `[ … ]`, which are comparable to Python. This is why running `rm *.tmp` is faster than `for f in *.tmp; do rm "$f"; done`, since the latter requires spawning the `rm` program as many times as there are files, whereas the former only does it once.
Pedantic note: Rust and Swift are not acronyms and don't need to be all-capitalized, by the way. "BASH" is also kind of unusual.
Started good then &gt; arrow/pendulum: Drop-in replacement of the standard datetime module Nope 
This was such a good read, nice work!
Sorry for a (very) late reply. I discussed it with my QA team and we agreed that this would be the best approach. Thank you very much for that idea.
Great idea will definitely read it. I do something similar with things i dont quite understand... Write examples and explanations for myself.
IMO python isn't as slow as developers just need to move crappy code out of the way to let the hardware churn, you know what i mean?
Some pretty good ones here. I may have to swipe a few for my [pywat repo](https://github.com/cosmologicon/pywat). I especially like "Return return everywhere!"
I don't waste my time with different databases and different apps to do things, just the_whole_word database and the_whole_world.py. The only downside of the latter is that I have to edit the code on a massive server or I run out of memory. I'm sorry that I can't tell you how many LOC it is as it takes too long to count them all.
https://github.com/reddit
Yeah that's what I was thinking :D
I am not exactly sure what you wanted to do. But i am guessing you would like to do the following: - Click on the 3 Dot menu on the top right corner - Click on Cast - perform your test. For the above you can do the following: - Install PyUserInput : *pip install pyuserinput* - Also install dependencies for your OS: *[Github of PyUserInput](https://github.com/SavinaRoja/PyUserInput)* Then in your code: from pykeyboard import PyKeyboard from time import sleep keyboard = PyKeyboard() keyboard.press_key(keyboard.alt_l_key) keyboard.tap_key('e') keyboard.release_key(keyboard.alt_l_key) for i in range(0, 12): keyboard.tap_key(keyboard.down_key) sleep(0.5) keyboard.tap_key(keyboard.enter_key) Hope this helps you in your script. Cheers.
At the same time though: &gt; You can also read these examples at the command line. First install the npm package wtfpython
What's unclear to me is why subclass? Why not simply create a class that has the dataframe as an attribute... EDIT: ok, mulled it over, read it again, followed some links, read those a few times, slept on it and I think I start to see the point of subclassing pandas objects But maybe the pandas docs on this aspect could be a little more clear on use-case and benefits of subclassing Also, still thinking that 'getting rid of *stringy* API' is not really worth it, but well, maybe after some more sleep And yeah, that `print(source)`... mystifying... 
OpenStack is written in Python, a gigantic, full-blown suite of cloud services that you can mix and match for VMs, object stores, image and volume management, bare-metal control, scheduling, auto-scaling.
I contrived this gem when monkeying around with Numpy empty array allocation, the most fragile way possible to send a number. import numpy as np def aether_send(x): np.array([float(x)]) def aether_receive(): return np.empty((), dtype=np.float).tolist() aether_send(123.456) aether_receive() # 123.456...usually.
TBH I feel like he might have done it on purpose...
Very nice! I didn't look too deep into dependencies, but how much work do you think it would take to make this cross-platform? Specifically, do you think it could be bundled in pyinstaller/briefcase and run on windows?
&gt; So if the is operator returns True then the equality is definitely True I don't think so: &gt;&gt;&gt; a = float('nan') &gt;&gt;&gt; a is a True &gt;&gt;&gt; a == a False
What.
Yes, it can be done in tkinter. It's not the default look, so there would be a lot specifications and custom widgets to make. From those screenshots it looks like you really want a web app. There's a lot of python web frameworks that will let you use a browser as a GUI, giving you all the power of HTML and CSS to work with. --- If you have more questions like this it's better to post them on /r/learnpython. Be sure to [format your code for reddit](https://www.reddit.com/r/learnpython/wiki/faq#wiki_how_do_i_format_code.3F) or use a site like pastebin. Also, include which version of python and what OS you are using. 
You can try using selenium for browser based automation tasks. Or beautifulsoup to parse the source. Either way, try asking /r/learnpython for more help. 
What were the deviations like??
The first one, where unicode characters are abused to make a different variable, was a little bit contrived I feel but other than that I could see some of the others showing up in the wild and being a bit confusing if encountered.
You need a way to escape a quote that matches the opening quote. Without sitting at a console, I can't recall if `r'''xyz'''` is valid.
You're basically grabbing portions of memory, so it depends on whatever was formerly used. If I print out 1000 bytes, I see a bunch of strings maybe from when I was printing other numbers. 0123456789ABCDEF-0123456789ABCDEF-0123456789ABCDEF-0123456789ABCDEF 0000 ................ Y.0e+000, 0.00 000000e+000, 0 .00000000e+000,. 0040 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0080 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 00C0 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0100 2.18360 265e-314, 3.60 198095e-157, 0 .00000000e+000,. 0140 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0180 2.17942 248e-314, -5.78 881765e-081, 0 .00000000e+000,. 01C0 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0200 ..............$. ................ @............... ................ 0240 0.00000 000e+000, 3.07 587334e+238, 0 .00000000e+000,. 0280 -3.61417 474e-194, 2.18 359403e-314, -4 .02489271e+303,. 02C0 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0300 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0340 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 0380 0.00000 000e+000, 0.00 000000e+000, 0 .00000000e+000,. 03C0 0.00000 000e+000, 0.00 000000e+ 
The numpy array created in the send function is not returned, so that memory space is free to reallocate. numpy.empty() gives you back the next free memory slot **without** zeroing it out (as opposed to numpy.zeros() which will zero out the memory). This memory spot just happens to be the same one that was just freed. Usually. Obviously not guaranteed behavior.
It's hilarious because I have a Matlab background too.
I know a bunion named mb 
&gt; A numpy array is not a tuple. Look at the function you "fixed" again, this is what it returns; def some_function(y): ... return is_passed, result ... return is_passed, result If I call your function I have to accept a 2-tuple as the return value, and then I will either have to 1) pepper ``some_output[0]`` and ``some_output[1]`` throughout my code if I want to use the flag/actual result respectively or 2) unpack the 2-tuple into a pair of local variables. In the former case I will have to remember each variable's members by index. I _will_ be overwhelmed by implicit data structures, it's only a matter of time, and then code maintenance will become nigh impossible. The latter case is fine as long as the output of the function I'm calling is well documented, but if it outputs a 4-tuple, 5-tuple, etc. I will quickly be overwhelmed by a sea of local variables. tldr; you completely misunderstood my critique and your code is still full of antipatterns Edit because I didn't see this on my first read through; &gt; Are you referring to (x,result)? The example is a pure function. Why would you make a class for that? I wouldn't make the function a class method, I would make the _output of the function_ an instance of a class with members for ``is_passed`` and ``result``, _if I needed one_. (Which I don't here if I can use ``None``) A ``namedtuple`` would work beautifully here, because then I could use ``some_output.is_passed``, which is much more readable and understandable. (In case you haven't realized it ``namedtuple``s are essentially class factories that combine the functionality of tuples with dictionaries using class attributes)
&gt; Numpy arrays while not what he's talking about at all, are bad here b/c now i'm forced into your data structure and your code is instantly less useful than if you used a more generic type. This is fine for open source libraries, so long as they don't depend on libraries that are obscure and not well maintained, neither of which describe Numpy. (Numpy has the drawback of being enormous though, but that's neither here nor there) If you're writing an API for internal use in a company then yeah, third party libraries become a headache.
&gt;&gt; Forget what you learned in CS. It's just math. It should do what you tell it to. &gt; &gt;It's *not* just math. It should be like poetry. Math *is* poetry 
Great, thanks! Those are just imagen that i quickly find googling, just to clarify which kind of design i was looking for, but yeah, for some reason (maybe because it's easier) that kind of ui are common in websites. I wasn't aware of /r/learnpython, my bad, should i post this there and remove it from here?
**Here's a sneak peek of /r/learnpython using the [top posts](https://np.reddit.com/r/learnpython/top/?sort=top&amp;t=year) of the year!** \#1: [Python 201 Book is Free for 48 hours](https://np.reddit.com/r/learnpython/comments/5814lw/python_201_book_is_free_for_48_hours/) \#2: [Python 101 Book FREE for 48 hours!](https://np.reddit.com/r/learnpython/comments/5bmaz0/python_101_book_free_for_48_hours/) \#3: [90% Python in 90 minutes](https://np.reddit.com/r/learnpython/comments/661o5a/90_python_in_90_minutes/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
Lots of good stuff written in Python. My goal in the post though was to indicate the wide adoption outside of the world of the web.
Don't mistake the moon for the finger that points to it?
Tutorials on how to train a neural network in tensorflow to recognize MNIST data (handwritten digits). 
It might have been the tone of the advertising, who knows? Whatever... upvoted you, you didn't deserve downvote in my book...
&gt; Pedantic note: Rust and Swift are not acronyms and don't need to be all-capitalized, by the way. Some times you capitalize for emphasis. More importantly I don't really care for the note. &gt;"BASH" is also kind of unusual. Actually BASH should be capitalized. At least the first three letters. 
The reason people say it is a glue language is that it IS. Just like every other language is. Much of a programmers life is spent trying to figure out how to leverage previous programmers' work by "gluing" it to other previous programmers' work. Not just in Python, although Python can make it remarkably easy. 
Stating "I'm not sure if my method is even remotely secure" is a clear indicator that your method is not even remotely secure.
https://github.com/rocketOC/papercount.git
I dont know... a google search for R turns up a lot of results.
i am new to coding and i tried selenium but i could not figure out how to automatically view source of every page
That and the profiling got me hooked. Finding bottlenecks in my stuff without having to do anything other than `import things`? Sign me right up. Definitely not a data-anything myself either. I get the sense that this is a really solid starting point though.
What on earth does this comment mean
Data scientists aren't switching to Python. More developers who already know and prefer Python are becoming involved in data science.
Oh man I'm interested in fintech and I'm planning to make an app for searching stocks on various markets with filters and details of certain stocks with access to financial statements and I can't decide between python or R. 
version = - log10(1 - 0.9999...)
`driver.page_source`
It's a pre-release, but also kind of not. I guess 1.0bX would be the current working version because they haven't released an actual 1.0 yet, but because pip avoids pre-release versions by default, they decided to change the version format so that it would install instead of making an actual release. So they just kept adding 9s to the version to show that it wasn't 1.0 yet. EDIT: according to the [issue tracker](https://github.com/html5lib/html5lib-python/milestones?state=open) we're mostly just waiting on documentation.
The best tool is one that doesn't sit on the shelf. Choose one and run. 
&gt; A primary use of dynamic typing IS To SHORTEN CODE. Exactly, so don't be afraid to use it. &gt; **Numpy arrays** while not what he's talking about at all, **are bad here b/c now i'm forced into your data structure and your code is instantly less useful than if you used a more generic type**. Maybe I took that out of context, but I try... It's maybe not 100% ideal, but what's an alternative library that is 500x faster than lists/tuples for processing numerical data (e.g., computing centroids of 200k triangles and quads) or taking the L2-norm of some xyz data? You can use something else, but again, you're locking yourself into some other library that has it's own API that's also got it's own learning curve. Numpy is one of the foundational libraries of Python that scipy, matplotlib, pandas, ipython, scikit-learn, numba, numexpr, etc. are built on. Yeah, it's weird to someone when they start out, but so are pointers and so are a lot of libraries. If you're doing work with large numerical data and you're not using numpy, your is probably slow or you did everything in Cython/C++, which no thanks. If you're making a library and you don't support the standard interfaces in that field, your code isn't going to be used. Presumably you've seen this. It's a great talk. https://www.youtube.com/watch?v=o9pEzgHorH0
If you use numpy as a fundamental part of what you're doing sure. I am mostly saying that in this case, he's returning a numpy array that can be a native type here. If you can return a native type instead, you skip the dependency. 
I was a c/c++ coder who would transform prototyped matlab into production code. matplotlib is how I made my transition to python.
I love R for data analysis and Python for apps. If you are primarily making an app, I'd suggest Python - there is fantastic tooling for creating webapps (e.g. django, flask).
Thank you.
Ok, that is my case too. But today's libraries universe are pushing new data scientists, without former coding experience, into python too.
book BLACKHAT PYTHON https://www.amazon.com/Black-Hat-Python-Programming-Pentesters/dp/1593275900/ref=sr_1_1/143-0084473-3328332?ie=UTF8&amp;qid=1504234147&amp;sr=8-1&amp;keywords=blackhat+python 
OK, this looks great. I've added it to my cart. Thanks a lot! Can you suggest any materials that would teach me the basics of Python? 
But the Humble Bundle still is a great deal - I totally got it :)
True true, and for that matter numpy.array supports math operations with built-in lists so you lose nothing by using them.
saw a gotcha here but can't remember exactly what it was something like both `a &lt; b` and `b &lt; c` is true but `a &lt; b &lt; c` is false. Anyone can recall?
Hi guys, I've had this project in my backlog for a little while now and seeing [this other post here today](https://www.reddit.com/r/Python/comments/6x6upn/wtfpython_a_collection_of_interesting_and_tricky/) finally got me to work on it again! To be honest, I thought it was going to end up being bigger, so I'm posting it here right now mainly to get feedback and maybe let you know about a cool trick or two you still haven't heard about! Please let me know if you think I should add anything else to the collection since I definitely thought it was going to get bigger than it ended up becoming...
I am making a Flappy bird like casual game with iPhone/Pythonista :) https://github.com/yukinarit/flappy_alien
Hi, I am the author of the post. The work-flow I had in mind while suggesting generation of classes is as follows: Say, you are building a new feature using a new file that will have to be used as the data source. Typically, one would load the data in a data frame ( or some class that wraps that data frame) and then pass that data around for further processing. In the workflow I had in mind, the step of class creation is like creating a parallel schema for the data file as a Python class(that will be the subclass of the Data Frame). That class can either be written out by hand or generated by the script. This script is only used to one time generation (similar to an ORM tool generating parallel Python classes for a related database table). The script becomes useful when we are trying to define a file that as say 20/30 or even 100 columns. In the domain I work in we are always dealing with files which even have 600 columns. To dynamically create a class, as you suggest, an exec could be used. I would guess a meta class approach is enough to generate such a dynamic class(that is no need to even create a string-typed source). But, since that class is not available during static analysis, the dynamic class approach may not be useful in some scenarios, but could be handy in some. Hope that clarifies your question. 
I responded to crashfrog. Your question on ```print(source)``` hopefully is clarified in the reply. Getting rid of `stringy` API may not be worth when you are dealing with data frames created locally and passed around between module level functions. But, once they get exposed to different layers in the code, since mostly the same data structure gets passed around, less 'stringy` API in those cases could be useful. 
If you're co-worker is not typing on an English keyboard, there's a fair chance it could happen. I've been tripped up with "smart quotes" when copy/pasting code. 
People sometimes build "wtf" examples out of chained operators that exploit the general lack of knowledge of how they really work. One that went around a while back was `False == False in [False]`. The way chained operators work is that `a op1 b op2 c` is equivalent to `(a op1 b) and (b op2 c)` with a guarantee that `b` is only evaluated once. So in the example I posted, it's `(False == False) and (False in [False])`, which of course evaluates true since both halves of it are true. People often seem to expect it to involve some kind of associativity rule (so that, for example, it would evaluate `False in [False]` first, then ask if `False` was equal to the result of that), but that's not how Python defines chained operators to work.
One way to think about it is this: Python *always* parses the string in a way which recognizes the backslash sequence as a potential escape/replacement unit, and then after parsing, something else decides whether any such units need to have escape or replacement behavior applied to them, based on the type of string. And this is what causes the end-of-string behavior, because when Python parses, say, `r'foo\'`, it ends up with a situation where it can't find the expected ending unit of the string (a plain `'`), because it still parsed `\'` as a single unit that might need escaping/replacement to happen to it.
You could try pyautogui to click that coordinate of the screen when you want to activate casting.
 requeriments?
Thanks! Hm... I reread my post and I cannot find anything unpolite or even offensive in it... Hope the OP haven't ignored my advices due to the temporary 0 😉
Don't know the answer to your question, but FYI in case you care about anonymity, your username is in that error path...
Funny enough, this is something I tried. Problem is, the VM that I am casting from, is hosted on an ESXi server. So the screen resolution is dynamic and it starts clicking on random locations.
That's just a raw triple quoted string... I use these all the time when I'm messing around in a Jupiter notebook and I need to paste a bunch of text into a string. In this case single and double quotes are no problem, but escape characters would be needed to allow you to have 3 consecutive single quotes inside a triple quoted string, I think.
Isn't this the same behavior as None? That there is only a single global instance of None so all variables that have been assigned as None refer to the same object, thus appear equal using "is" but by it's nature, nothing can equal None, even itself, so two things that are both None cannot be equal because neither is equal to anything because if you equal something you aren't nothing... 
And people say BSD messed up with their license compared to Linux... Python's liberal license is a win over R being ~~GNU~~ GPL.
Wait. I don't write that query and saying "oh it's a glue language,so it's a bad language and now let's write on Reddit". I wrote that After your reply,because I don't remember the exact articles I saw,and your question was "Who is that everyone Who Say that" and I wrote you the query. I Hope this isn't misleading.
 &gt;&gt;&gt; None == None True tho
Two hours doesn't come anywhere near endless googling... This isn't a particularly well documented repo, that's for sure, but it does for the most part state in the README.md exactly what env vars **must** be set: IOTA_SLACK_WEBHOOK_URL MODUM_SLACK_WEBHOOK_URL Those are, no doubt, the Slack webhook URL(s) you want them to post to; I'll assume you can figure out how to set up that integration, so you just need to set these env vars to either two different webhooks (i.e. if you wanted the notifications distinguished by source) or the same one. You also need to sign up with the Twitter API and provide these by env var too: TWITTER_API_KEY TWITTER_API_SECRET TWITTER_ACCESS_TOKEN TWITTER_ACCESS_TOKEN_SECRET And then there's these, which use stupid defaults, and should be changed just in case anyone ever actually does gain any sort of communications with your Postgres process: POSTGRES_USER POSTGRES_PASSWORD Ultimately you should be able to set all of these in the `.env` file and **docker-compose** should pick those up and incorporate them (obviously you should **never** commit that file anywhere, and it should be readable only by you). Alternatively you could use a bash script and **setenv** calls to prepare the environment, then run **docker-compose**, or I believe you can also pass them to **docker-compose** as command line arguments. Of course, not sure I'd rely on anything that ignores it's own advice, as it does in `crypto_news/settings.py`: # Quick-start development settings - unsuitable for production # See https://docs.djangoproject.com/en/1.11/howto/deployment/checklist/ # SECURITY WARNING: keep the secret key used in production secret! SECRET_KEY = '79%pozs8r1v)08t5g7obc#d8!z#=kmotzocyg8ssy(820j7ilv' # SECURITY WARNING: don't run with debug turned on in production! DEBUG = True But it sounds like you're probably just using Docker to run this on your local host, so unless you're machine is already compromised it probably doesn't mean much, but if you *ever* run this on a server somewhere I'd at least read, understand, and make the changes [Django recommends](https://docs.djangoproject.com/en/1.11/howto/deployment/checklist/).
Python. It's better for string mangling, scraping the web, and serving data, which seems to be 90% of what you're looking for.
You already posted this in [r/learnpython](https://www.reddit.com/r/learnpython), and that's the appropriate place, so I'd suggest deleting this.
Um, this statement: &gt; The way this works is that a == b == c is equivalent to (a and b) and (b and c). Is so wrong. a = b = c = 0 print(a == b == c) # True print((a and b) and (b and c)) # 0 a, b, c = range(1,4) print(a == b == c) # False print((a and b) and (b and c)) # 3 Notice how they don't even give you back the same **type** of answer?
That's true. If done some 'prototypes' in both. I barely know R but I find it interesting. I know python a lot more I'm just bad at programming so it's going to take a while. Worth trying though and perhaps learning something. 
If you have been through Java, you will find Python easy to pick up... there are many similarities. Having said that... Learning Python by Mark Lutz is an excellent starter book 
Thank you, I'll give it a shot. 
The Generator Expressions section as a typo that will give a NameError: words=Set(word for line in page for word in line.split()) That should be **set** without title-casing.
For **in-place value swapping** you might want to point out how bad `True, False = False, True` used to be, though they appear to have fixed that in Python 3. I hadn't tried it in years. **Edited** because yes they fixed it. Still interesting as a historical point. Many *very* mean tricks were played on those who walked away from open REPL sessions for a lunch break. Hashtag programmer shenanigans.
Maybe ask [these guys](http://www.pentesteracademy.com/) who do a Python course for pentesters: 
R - 8,640,000,000 results Python - 907,000,000 results Don't hate the player.
Doing web scrapping on a spanish university site, where they upload free courses they had. [The repository](https://github.com/Mangu93/UCM-Scrapping) I have to improve it, ofc.
I did not know this, and was actually intrigued enough to go read the [specification](https://docs.python.org/3/reference/expressions.html#comparisons) for it. Thanks for the insight!
It was probably installed as super user and permissions were messed up. Uninstall it, and install it using `pip install conda --user`. If you are using a virtualenv, use that to install conda. 
One of main R's claim to fame is its fantastic data visualization. Python already has the binding to use same data visualization package along with some others. With numpy, scipy, pandas, tensorflow + python simplicity, its hard to beat python.
The feature works beautifully in cases like `if 0 &lt; a &lt; 10` but they made it so generic that it can be extremely surprising sometimes.
Thanks for the link. Will add it to the list.
Oh well, happy it was useful then
Working on making one of my simple scripts more usable by putting in a simple Tkinter GUI. It probably is far less impressive than what most people are working on but it's something 
I really hope it does edge out R over time - of course I'm biased because I love python and i've sunk a lot of time, but in general I think it would be a huge pain in the long term to have two roughly-equal platforms without a clear winner, meaning you never quite know what to use. Also, having dipped my toes in R I have to say i really don't like it as a language - visually messy and it's approach doesn't encourage good code organisation and presentation. Scientists in general, data scientists included, really need to get better at producing clean, tidy, and reusable code and I think Python's language and culture encourages that much more.
I love seeing just how much discussion and how many competing, but valid and valuable, points of view can be brought to bare on something seemingly so trivial. The number of times I've seen all discussion trumped by "but it's NN times faster!" and then watched a rain of unintended consequences fall. The number of times it's been me pre-empting that discussion. Fascinating.
Or maybe Python is just not a painful language.
BeautifulSoup would work great in this situation. There's lots of resources to get started. Selenium would also work but it's far more tricky to get to work in my experience. If I were doing this I would go with BeautifulSoup.
https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values
Conda has to be installed via Anaconda or Miniconda. Don't use pip to install conda, it's not supported (see the installation warning on https://pypi.org/project/conda/) and can lead to strange behavior like OP's.
I don't know, a potential 40x speedup on something that is used throughout the standard library and has been identified as a major bottleneck doesn't seem trivial to me.
And there's a way of doing that for CPython, it just doesn't transfer to the other versions. So why not use the fast method for CPython and (for the time being) use the slow method for the others?
That's probably what they'll do. 
You've probably done some conda operation with `sudo` at some point, creating files owned by `root`, which you can't modify as your normal user. To fix it, change the owner of all the files to yourself: chown -R username:username /path/to/anaconda 
I didn't imply that it was trivial, I'm just glad to see that it gets hashed out so fully by so many smart people with differing points of view, and that a speedup only, thus far, applicable to one (even if the dominant) implementation doesn't just get accepted on that merit alone.
Working on a encrypted backup solution for linux machines. Python2 for now, but soon will be ported to 3. (Note: Pull requests appreciated!) https://github.com/mtverlee/pyBackup
&gt; for idx, item in enumerate(list_1): del item &gt; del removes a specific index (That's why first list_1 was unaffected), raises IndexError if an invalid index is specified. This code just deletes the `item` local variable, it doesn't call `list_1.__delitem__`. That requires `del list_1[idx]`, which behaves like `remove`.
I think this comment explains it better: https://www.reddit.com/r/Python/comments/6x6upn/wtfpython_a_collection_of_interesting_and_tricky/dmevny6/
Yeah, that statement would not be always True. Fixed it in the project :+1:
But how many of those are stack overflow posts going 'help, I can't do x'?
Finishing to implement a search engine for tagged images.
working on a command line SMS/MMS messenger that uses the twilio module. I have really only used python for automating tedious or time consuming tasks before so i wanted to try something object oriented. its goinggggg.... not great
An event-driven turtle game.
&gt; Know why a list comprehension is faster than a for loop (which really is to say understand how bytecode is generated, at high level) I never suspected they were such a difference ? Do you mean a for loop with an append inside ? &gt; A decent knowledge of some non-standard library modules in the domain. This would highly depend on the field, but scipy stack, django/flask/sqla/jinja2, etc. This is overselecting in my opinion, one could miss a fantastic python engineer who did not work on the domain or just not enough. I would say that knowledge of some non-standard library should be enough; 
&gt;I THINK it's ascii, but I can't decode it. I've tried most examples and suggestions for solutions I can find, but they all generate rubbish. It's not ascii, ascii byte-strings are displayed as strings not as raw bytes e.g. `b'foo'`. In fact you can see such an example with the "b" and "K" letters at indexes 5 and 6. Bytes are printed using hex escapes when they're not printable ascii, and `\xff` or `\xe1` definitely aren't ascii since it's a 7-bit encoding. Trying a few ISO-8859 encodings only yields garbage as well. &gt; Any suggestions? :) Maybe try feeding it to "chardet" (the actual entire thing) and see what it says, but a better idea would probably be to find out what it actually is from either documentation or people having reverse-engineered it. It could be a packed binary stream mixing text and non-text data.
Maintainer is being cheeky in an annoying way. The library was described as "approaching v1" a while ago
Statisticians will continue to use R Developers will continue to use Python Knowing one and being aware of the other is all you need to do.
I think \xff (at least from reading various sources) denote the endianess of the byte string. This is a real problem. When I do this in C++ I get the proper string, no weirdness. In Python I get bullshit. Is there any reason for this? The documentation doesn't specify. I've done my research, this is my last resort.
&gt; I think \xff (at least from reading various sources) denote the endianess of the byte string. No, 0xFF alone can not do that (the point of the BOM is to show the ordering of bytes, so you need multiple bytes). The BOM is U+FEFF and will be encoded as 0xFE 0xFF or 0xFF 0xFE in UTF16, and as 0xEF 0xBB 0xBF in UTF-8 (where it makes basically no sense anyway). Your example thing has multiple instances of 0xFF (not just one at the start) and none of them look like a BOM. &gt; This is a real problem. When I do this in C++ I get the proper string, no weirdness. In Python I get bullshit. Is there any reason for this? That C++ assumes things about the data (such as an encoding) which happen to work? You need to find out what these assumptions are then translate them to Python, not randomly fling poo at the wall hoping something sticks.
There are courses on Python networking on Udemy that are really useful. I've done one or two when they were on sale at about $10 A free intro you can download or read online is 'Think Python'
Can I ask roughly how long it took? I have just bought the course, and am going to do it in my spare time. Roughly how many days/weeks did it take to go through, and roughly how many hours were you spending on it per day?
I use both and I actually disagree. I think R is superior to Python for actual data analysis. I don't think Python is cleaner, and in fact certain Python solutions, like Pandas for data manipulation, is much inferior and uglier than the options available on R. The same goes for data visualization, and most statistical analysis. Even in the machine learning realm, R is almost completely caught up with Python. It now has Keras implementations and just as many options for machine learning algorithms these days. That said, Python will always have a huge advantage in that it is an actual production capable programming language. If you are just doing some kind of analysis, R is hands down a better choice, but if you are somehow incorporating the analysis directly into another program, algorithm, or app, R really can't compete with Python. I think that has some part in Python overtaking R. I think the biggest part is that more and more people are coming into Data Science from programming backgrounds, and for those people, Python is a more comfortable transition.
If it's going to be an app, Python is probably better.
Fun. My first experience with graphics on a computer was a dos program called FRACTINT that allowed you to explore fractals like these. It was freaking glorious and I knew then that these computer things were going to be a big deal. 
That's simply not true. I came into my DS job knowing python and having to pick up R. Started with 0 R experience on day one. 9 months later I'd classify myself as a R expert (writing code using Rcpp, NSE, datatables, etc)
Agreed 100%. After using dplyr or data.table in R, Pandas just seems clunky and non-performant. That being said, when we need to shift things into production, it's python all the way.
&gt; If you are just doing some kind of analysis This is roughly where I see the main problem with R - not necessarily in terms of pure syntax, but in terms of 'language culture'. Comparing projects in Python vs R, on the Python side I think you really see the positive influence of people coming from the software engineering side in terms of test coverage, CI, use of Git, etc. Documentation is strongly encouraged and well integrated (while on the R side - PDF documentation? Really?*). Python teaching and tools emphasise things like legibility and modularity. These are topics which become ever more important as more science is done with code - frankly I forsee serious mistakes being made due to people doing supposedly rigorous science with half-assed ad-hoc code. In that context I'll take every bit of assistance from the language and ecosystem I can get. *NOTE: clearly not all R packages deliver only PDF docs. But many do, and it's pretty unwieldy.
It amazes me when people describe pandas api as ugly when compared to data.table, when they look very [similar](http://datascience-enthusiast.com/R/pandas_datatable.html).
`[x for x in L if f1(x) and f2(x)]`?
This works, but how can you generalize this for a list of functions, say [f1,f2,...,fn]?
Well I absolutely agree there. I think that there is a ton of ground to be made up for in that respect, though I think we have started to see some of that (I think the tidyverse has done wonders to the uniformity, legibility, and modularity of R code. Lots of people haven't really adopted it yet though).
Dude..socket programming is alot of fun. Im going through Violent Python right now. I also have Black Hat python.
There is no need for it. &gt;&gt;&gt; filters=[lambda x: x%2 == 0, lambda x: x&gt;6] &gt;&gt;&gt; list(filter(lambda x: all([f(x) for f in filters]), [1,2,3, 6, 12, 24, 25])) [12, 24] next time, go to /r/learnpython
Depending on his background, id say start with Violent Python and then go to Black Hat Python. As awesome of a book as it is, I think Black Hat Jumps in too fast as with Violent Python goes over the basics more in depth..which..its not really that deep at all, but it covers alot of basics more than Black Hat. I started with Automate the Boring Stuff with Python, got the book and took the authors Udemy Class. Then Went to Violent Py, and will do Black Hat Py next.
Are you talking the Cisco based course where you have to set up your routers etc? Also, thanks for the info about Think Python. Where can i download it from?
Also consider that more work is often spent preparing the data for analysis prior to analyzing it. Whether it's full-blown ETL or just ad hoc transformation and cleansing of a csv file you were given. And in these cases python also wins by a pretty huge amount.
https://skycocker.github.io/chromebrew/ $ crew install python3
Thanks for this
`[x for x in L if all(f(x) for f in fs)]`
When you get about half way down, you start to see how verbose pandas becomes when you want to describe non-trivial data manipulation tasks. For things like head(), rows 1:3, and max(), yes of course they're very similar. Performance is really where the difference becomes clear.
..and in Python 3.6.1 (due to dict's re-implementation) it loops only 5 times.
&gt; In the workflow I had in mind, the step of class creation is like creating a parallel schema for the data file as a Python class(that will be the subclass of the Data Frame). Yeah, I get it - you're introspecting the table to create a subclass of DataFrame where the columns are available as instance fields instead of via dictionary syntax. Like a kind of named tuple around a DataFrame. No problem with that, I love accessing attributes instead of using string indexes. But the fact that ```namedtuple()``` returns a *type* and not merely a printed-out definition of the class should be instructive to you; moreover, due to the similarity in problem scope it's how your users might naively assume they're supposed to use *your* tool. &gt; This script is only used to one time generation (similar to an ORM tool generating parallel Python classes for a related database table). Sure, but the thing about your metaprogramming approach is that if anyone actually uses it the way you intend, they'll be violating DRY ("don't repeat yourself.") They'll have two definitions of the same data model - the one in the file, and the class def they've cut and paste into their code - that have to agree, but aren't guaranteed to agree. When they get out of sync, they'll have to run the script again and then update the script that contains the class definition. &gt; To dynamically create a class, as you suggest, an exec could be used. It could, but you shouldn't. ```exec``` is a risk almost every time it's used, and if there's something wrong with how your metaprogramming works (remember, on your end you're just munging strings, you have no guarantee that the string output you produce is valid Python) you've deferred the errors to the person least able to troubleshoot them - the end user. &gt; I would guess a meta class approach is enough to generate such a dynamic class(that is no need to even create a string-typed source). I think it's pretty obvious that this is the more Pythonic approach. Python allows - encourages, even - runtime definition of types, and that's a powerful tool for building API's that elegantly encapsulate behavior.
I disagree. You are right how a huge portion of work is spent on those tasks, but I think data cleansing and transformation is much better in R. I included data manipulation and ETL in my original assessment that R is superior for analysis. It's only when you enter a production environment that I think Python becomes preferable.
&gt; What's unclear to me is why subclass? Why not simply create a class that has the dataframe as an attribute... The "IS-A/HAS-A" distinction is always a toughie. Here, I would suggest that if your intent is to make something that acts like a DataFrame but does *additional* stuff - like enables column access via attributes instead of string indexes, like a namedtuple vs a dict - then subclassing is the right call; as a wrapper *around* a DataFrame, you're stuck re-implementing all the rest of the DataFrame interface if you want to support that stuff, even if all the implementations are just boilerplate calls to the same method of the instance's DataFrame attribute. This is Python, not Java. Check that boilerplate at the door! &gt; But maybe the pandas docs on this aspect could be a little more clear on use-case and benefits of subclassing My sense from Pandas is that they're not going to expect that you're subclassing DataFrame, and some stuff will likely break. Maybe not. Overall I'd say that Pandas (and NumPy and a lot of other stats/numerics libraries) assume the users will be statisticians and not programmers. But I'm prejudiced against statistics packages.
When you've been programming for 15 years and you've been exposed to everything from declarative to imperative, functional to procedural, and everything in between, picking up a new language is pretty darn easy. 
To yourself, yes. Even look at the title of your post here: "[...] (by Kenneth Reitz)". Is that supposed to be a seal of quality? A direct contribution would be a PR to the project [mentioned earlier](https://www.reddit.com/r/Python/comments/6wkk9p/a_humans_guide_to_setuppy_by_kenneth_reitz/dm8pi2l/) or [this one](https://github.com/pypa/python-packaging-user-guide) for example.
We switched from python to scala
Or, as others have pointed out, someone knows Python from a different domain and really doesn't want to go through learning another language from the ground up. That's certainly the case for me. I actually think doing this kind of work in Python has a lot of rough edges (poor documentation, stuff spread across multiple libraries, Jupyter not being quite 'finished') but the fact I have *never* touched R keeps me from touching it at all.
Detailed deliberate all-the-way shit like this is why Python is not a insane festering swamp like PHP.
Why not install Linux? Its very straightforward. I would install crouton with the xiwi extension. This allows you to run Linux apps in a window without running the whole desktop. It also means you don't need to a browser from inside your crouton chroot, just use the Chromebook version. For example to run gimp in a window crosh&gt; shell chronos@localhost / $ sudo enter-chroot Entering /mnt/stateful_partition/crouton/chroots/xenial... (xenial)user@localhost:~$ xiwi gimp&amp; It also supports better copy pasting between the environments. From the [Crouton github](https://github.com/dnschneid/crouton) &gt;**Wasteful redundancies are wasteful: one clipboard, one browser, one window** &gt; &gt;* Install the crouton extension into Chromium OS. &gt;* Add the extension or xiwi version to your chroot. &gt;* Try some copy-pasta, or uninstall all your web browsers from the chroot. &gt;Installing the extension and its target gives you synchronized clipboards, the option of using Chromium OS to handle URLs, and allows chroots to create graphical sessions as Chromium OS windows. 
That is almost as evil as replacing a semicolon with a greek question mark (;) to someone's source code in Java, C++, C#, etc xD
+1 for BeautifulSoup and I would also recommend [Requests](http://docs.python-requests.org/en/master/)
Thanks, I've run most of the code in the page to test it for myself but since this snippet came directly from a PEP it was one of the very few I didn't bother with! EDIT fixed!
Yay! I finally procrastinated long enough and now I don't have to learn R anymore!
This is straight from the Python manual: &gt; Comparisons can be chained. For example, a &lt; b == c tests whether a is less than b and moreover b equals c.https://docs.python.org/3/tutorial/datastructures.html#more-on-conditions I never claimed chained expressions are identical to the long form presented, merely equivalent. If you want to suggest a better way of explaining it I'll be happy to edit the page, though. Just to be clear, it's not *wrong*. If you really think it is, please take your complaint to whoever is responsible for saying exactly that in the Python manual.
As a data scientist who switched from R to Python, please speak for yourself.
Well if you do ever touch it, you will feel like you burned your fingers. R has some nice builtin features related to data science for sure, but the language as such is just not very nice at all to work with. 
So basically. First you should use pip for python modules. Second that kind of problems doesn't have to do much with pycharm. The most time it is the wrong working directory or wrong imports. If you can't run your program from console you have a problem.
I don't mean to be an ass but his explanation is literally the same I give in the Notable Python list: &gt; "a op1 b op2 c" is equivalent to "(a op1 b) and (b op2 c)" Same words too, pretty much, except I use "and" and he uses "op1" and "op2". Anyways, I've added a link to the official manual now so anyone with further complains can take them up directly to them since this is also how they explain it on the official Python documentation.
TIL namedtuple are everywhere in stdlib. I 'd use them more often if I could do something like a = ntuple((1, 2), ('foo', 'bar')) a.foo &gt;&gt; 1 a.bar &gt;&gt; 2 a[0] &gt;&gt; 1 
You shut your whore mouth Mr. I am currently learning R and want it to remain most used language for data science.
[removed]
Well, after you get it from a response you have to convert it to a Python data type. You can use the *loads* function from the *json* module. After that you can get the first element as you would with any array. 
The proper way is to make it a service/webapp. "Protecting the source", just like any security thing, only makes sense with a defined threat model: who are you trying to thwart from doing what?
Your code can be improved a lot. Setting colors per pixel is very inefficient and not pythonic. You may see [this example](https://github.com/neozhaoliang/pywonderland) here. The example image there was generated in one second.
Did you set up the serial port the same in the Python rewrite? Baud rate, start/stop bits, parity, etc? The `0xff` bytes look nothing like the binary protocol from the [Technical Specifications document](http://static.garmin.com/pumac/GPS_18x_Tech_Specs.pdf) too...
FYI you could do `x = namedtuple('name','x y z')(1,2,3)`
Would you mind providing a short example? I am already using json.loads but my Python knowledge isn't the greatest. Thanks, George
Yay. An ad infested web tool. Nothing in the policies about users retaining copyright. https://pyfiddle.io/privacy/
There are three methods for getting a more standard *nix setup on ChromeOS. 1. Put device in developer mode and install crew as /u/kirbyfan64sos highlighted 2. Put device in developer mode and install crouton as /u/Misio highlighted 3. If you have a [supported device](https://www.chromium.org/chromium-os/chrome-os-systems-supporting-android-apps) that can access the Play Store, you can install [Termux](https://play.google.com/store/apps/details?id=com.termux). This has the advantage of not putting your device in developer mode if you are worried about security or aren't allowed to by your employer.
What is it about R that is so painful? Can you give me an example that is hard in R, but easy in Python that is related to data analysis or data? I know R sucks for creating games or websites, but as far as data goes, everything is no harder than pandas.
This is a python shell script that acts as a wrapper for the MongoDB `mongodump` and `mongorestore` commands. I'm looking for feedback, thank you!
It's probably easier to get started in R than Python. The reason that I learned R was because I was able to connect to Postgres from R and I couldn't from Python. You download and install R and RStudio. 
I agree. In general, SaaS is the proper way. But it is not always the case. For example, sometimes you get paid for code, and distributing binaries to a client could be an acceptable way of proving the work done without revealing the source... In my case it was a custom cryptocurrency trading bot, made for a client. I wanted to show a demo to him, without leaking the actual intellectual property. Obviously, the more profitable thing would be coding some kind of a bot app hosting service! Maybe I'll do it next time, sounds easy ;)
I worked on it for like an hour a day for a few weeks (maybe 3/3.5 weeks). Because the lectures are separated by topic the course is very easy to do in small bits. I think doing it in your spare time will work well, but depending on how much spare time you have it could take a while. 
No, I did. The claim was made that &gt; Data scientists aren't switching to Python. I am, in fact, a data scientist that switched to Python (from R).
If you want a flat UI in Python `kivy` is the most likely option. 
I disagree with the above statement. There is a bigger push in software to hire "data scientists" who can also write some level of automation. There is also an enormous growth in applied deep learning where most APIs have a python counterpart before R Personally, I learned R first through school and used it for a couple years in industry and "switched" to doing most analysis and modeling in python over the course of about 6 months.
I've found pandas to be pretty damn fast, tempted to say faster than dplyr but I don't actually have any benchmarks. You just have to be careful of column types and try to avoid `.apply()` when there is a vectorised/numpy solution available.
username\\: is preferred to username:username as the former will use your primary group even if it's called something different, which might be the case on multiuser systems
The appropriate place for this would be [/r/learnpython](https://www.reddit.com/r/learnpython).
And engineers will use Matlab. And embedded coders C. Tools are tools. I laugh at the 'language wars' that people seem to get into. Could you imagine a general contractor and a machinist getting into an argument over which type of hammer is better? &gt; No. CLAW HAMMERS are clearly superior. &gt; You idiot. Ball Peen hammers are clearly better.
When performance in dplyr is significantly better than pandas it means you are using the pandas api wrong and probably not respecting the boundary between numpys ctypes and python objects. I have never seen operations that can't be more performant with pandas. That said, there are a fair number of pitfalls in the pandas api that make it easy to do transformations in a extremely less than optimal way
The python ecosystem for data augmentation and transformation that isn't even specific to pandas is astounding. I haven't worked much in R recently, but this was far from the case 4 years ago. If there is a public api, there is usually a python wrapper for it. In terms of data manipulation and ETL, what do you think makes R superior? For small datasets that you can fit in memory, feature parity exists across languages. For tools for larger datasets most apis are more fully featured and are developed first for python. Take pyspark and sparkr. Not to mention, some of the best open source ETL tools today are in python ( airflow, luigi)
I thought we were table about data.tables performance? https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping
Totally agree. Should have been more clear: pandas looks much more verbose than dplyr. Pandas looks more verbose and is less performant than data.table https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping 
Well I only use a cross pein pin hammer because once you learn how to use it you never need any other hammer. Those other hammers are 0.005s slower at hammer lists
Thanks srikavig for sharing my article, I hope you liked it!
I was obsessed with Fractint back in the day. I would have it running for days rendering a single image.
My intent was to show a code easy to understand, even for someone that isn't a Python programmer. I don't have the time right now but I will carefully read your code. I will probably add a link to your github for people looking for other implementations.
What's the error message?
it's on the script I'm running, the script just won't recognize PyWin
Could you post a copy of it &amp; and the script you're running? 
Had the exact same error you did. One day it worked, the next it didn't. Still doesn't work. I went looking for the issue on Github but couldn't find it. Was this resolved already?
This is simply security by obscurity. There actually exist human beings using human being created tools that can understand assembly produced by an optimizing compiler.
Tools are tools. But some tools are better suited for specific tasks. You don't (and you shouldn't!(?)) see anybody doing Data Science in Assembly. While it is technically possible.
This is the link to the script makers blog, at the top of the FAQ it has the Py version it needs, the PyCrypto and the PyWin (https://tetrachroma.wordpress.com/2010/06/19/short-faq/) This is the link to the actual script download (http://ainept.freewebspace.com/) Its the PDF Decrypter 8.4.51, I also found the code for it if you want to see that rather than download off a site. ------------------------------------------------------------- The script should help me unlock some PDFs I have that are locked with FileOpen and I cant get them to open with the FileOpen plugin.
They didn't say the same thing as you though, not even close. You said that: a == b == c Is the same as: ((a and b) and (b and c)) But that's fundamentally untrue because it switches the **==** operator for the **and** operator, while this: ((a == b) and (b == c)) Is correct. The manual example is also correct, because: a &lt; b == c Is the same thing as: ((a &lt; b) and (b == c)) Now what your statement is actually correct for is: a and b and c Which is indeed the same as: ((a and b) and (b and c)) And I'm pretty sure the people who wrote the manual would agree with me. 
I have no clue why you're getting downvoted. The distinction you made is very important as Python is inherently object oriented. 
But **and** *is* an operator!!! The other post is using the generic term *op* to represent any operator in the chain, your use of **and** is incorrect because it replaces entirely the semantic meaning of their use of *op* as a short representation for any given Python operator.
For data Python is best if you want to programatically deal with it. R is best if you want to statistically deal with it and Matlab/Simulink is best if you want to ID &amp; control it.
You need to use string formatting to make strings like that. print('The total amount of interest you paid is ${}'.format(total_interest_paid)) --- If you have more questions like this it's better to post them on /r/learnpython. Be sure to [format your code for reddit](https://www.reddit.com/r/learnpython/wiki/faq#wiki_how_do_i_format_code.3F) or use a site like pastebin. Also, include which version of python and what OS you are using. 
Ah now I get it, it was a typo then, I meant to use the same operators, obviously! Probably got past me in the copying-pasting as I was working on the document :) Sorry for taking so long to understand what you meant. All your code examples kept making me think it was a semantic issue, not a typo (writing "and" when I wanted to use "==" instead). Totally my fault though, not yours! My sincere apologies again and thanks a lot for bringing that up! I have fixed it now - "a == b == c is equivalent to (a == b) and (b == c)". Hopefully I got it right this time :D
thanks a ton man, didn't even know that sub existed
Which comes back to using pandas to its potential. Those benchmarks don't. Try those benchmarks yourself. Then try them with a couple non obvious changes. If you want string handling in pandas similar to data.table, you need to use pandas category types instead of python string objects. To be fair these didn't exist when this benchmark was created, and to get similar performance you had to use string folding. Secondly, historically, R has only had first class support for 32bit numerics aka data table defaults. On the other hand, numpy will default to 64 bit floats/ints. With those changes, I would wager performance is almost identical, if not generally better in pandas 
The supplied solutions are fine for small datasets and small/fast filtering functions. If that's not the case you might try something like: f1 = lambda x: x%2 == 0 f2 = lambda x: x &gt; 6 l = [1,2,3,4,5,6,7,8,9,10] fs = [f1, f2] def multi_filter(fs, l): if not fs: return l return multi_filter(fs[1:], (x for x in l if fs[0](x))) print(list(multi_filter(fs, l))) It's super overkill for simple cases but if you need some extra speed then it can be worth it. 
To be honest, nor do I. I do appreciate your support! :-)
No worries, got there eventually.
Shiny
What was the reasoning behind the switch, and what has your experience with it been? 
Thank you so much dude. I figured it out after all, kinda stump now on different problem, but hey...that's how it's supposed to be, right xD I don't want to bother you anymore, and thank you once again for all the help you provided :) Cheers!
Did you get it going? Glad if I helped you get there.
&gt; Python already has the binding to use same data visualization package Which package is that? So far all the visualization packages I've worked with in python have been a major pain in the long run.
Did you know that pep8 recommends never assigning a lambda? I didn't either until last week. Instead they recommend a def statement. &gt; Always use a def statement instead of an assignment statement that binds a lambda expression directly to an identifier. &gt; Yes: def f(x): return 2*x &gt; No: f = lambda x: 2*x &gt; The first form means that the name of the resulting function object is specifically 'f' instead of the generic '&lt;lambda&gt;'. This is more useful for tracebacks and string representations in general. The use of the assignment statement eliminates the sole benefit a lambda expression can offer over an explicit def statement (i.e. that it can be embedded inside a larger expression)
Thanks, sounds pretty do-able then.
This seems pretty useful. How does it handle serializing nested objects?
Scala runs in the JVM so as it serializes data to the EMR cluster the transfer is more efficient. With python the interpreter has to serialize and transfer between py and spark, which is less efficient. Scala is no big deal. I was resistant at first but it has grown on me.
Backwards incompatible change!!
you might want to add something about keyword-only args
Here's an example of how to nest serializers - https://serpy.readthedocs.io/en/latest/#nested-example
How and why is this better than marshmallow? 
Man, more people got to read this.
I like it. It's got inheritance and all the stuff you want in a serializer. 
Than*
Speed. It's fast. Really, really fast. It has less than half the features of marshmallow, so if you compare the two in terms of features, marshmallow is obviously the better choice. But if you want a tiny library that only does simple transformations, serpy is a very good library.
Which interpreter? A [sufficiently smart compiler](http://wiki.c2.com/?SufficientlySmartCompiler) can do a lot of things, but TCO itself doesn't eliminate the cost of calling a function for each iteration. You need inlining for that, which may or may not be possible for the compiler to safely do. At any rate, why? What is the purpose? What is recursion's performance cost buying you? For dealing with trees, it buys you nice code. For everything else, not so much. If the data is flat, just use a loop. If the data is nested, think about whether you can safely assume you won't blow your stack, then maybe choose recursion if it makes sense.
Python for Network Engineers- Netmiko, NAPALM, pyntc, Telnet
Have you seen Lyft's fork [`toasted-marshmallow`](https://github.com/lyft/toasted-marshmallow) that gives a 9-21x speedup as a drop in replacement?
I have, but I discovered this project only in the last few days (I think it was posted on /r/python like a week or two ago). Looks pretty cool though - I'll try to play around with it this weekend.
Is this still relevant, more than 6 years after it was written?
actually, `__reduce__` is not the only way to execute code via pickle, pickle mini language is actually a turing complete machine able to call any function of the language including `__import__`, no need to limit to one function call. (But such pickle can't be generated by pickling a valid object
Don't hate me, but: Lack of braces It's not statically typed It's not compiled. Java with Python's simplicity would be amazing...
Works fine for me on the version I just installed fresh (0.12.7). What version of bokeh are you running? Try upgrading and run again.
Something like this should also work: filtered = filter(f1, filter(f2, l)) To get the list: list(filtered)
pickles are serializers. They're __meant_ to do this.
Unfortunately yes
Yeah but the reason why you shouldn't just unpickle stuff anyone could be sending over an open connection to your server is the same reason you don't open .exe files in email attachments or open a .zip file on a random USB stick you found lying around. Really, the lesson here boils down to using common sense, since the issue is people not paying enough attention to where malicious code might be hiding, not the functionality of pickle.
Recursion allows you to perform side effect free iteration. Loops have a side effect. Specifically I mean interpreters that implement TCO for proper tail calls. TCO is a description/specification of what a "sufficiently smart compiler" should do when it sees a proper tail call. TCO implementations must remove all of the overhead of calling a function **by definition** With TCO no need to store registers and push a stack frame, no need to worry about inlining or looking up the function (a TCO function is already inline with itself because it is itself)
You mention that bools are a subclass of int but you leave out gems like this: text = 'Some example data' vowels = 'aeiou' vowel_count = sum(char in vowels for char in text) which is much cleaner than vowel_count = sum(1 if char in vowels else 0 for char in text)
That is a very nice trick! Took me a little hands-on hacking to even figure out how it works! I'll definitely add it to the list, if you don't mind! EDIT added it with a concise explanation https://github.com/tukkek/notablepython/blob/master/README.md#boolean-arithmetic
I worked on a project adding hardware crypto acceleration t OpenSSL. So I subscribed to the developer's maillist. It was terrifying to read how many ways it could be subverted. I realized I could never do network or computer security work - it takes a seriously devious mind to be great at it. At least I know to be careful with pickle and eval.
I can absolutely imagine contractors getting into arguments over which hammer is right for a particular job.
Monoliths are the past, the future is microhammers.
A Ph.D has 10 years of experience because they have to learn how to design studies and interpret the results, not learn some statistics and a programming language. You don't need a college degree to do ETL shit and process the data using well-understood techniques and get actionable output. That's what most of us that are messing with Pandas and Scikit are trying to do. 
You don't need to be an expert to make it work in prod.
Many of us in the software industry steer clear of GPL on purpose.
That's been a standard way of doing things for years, something like e.g.:- try: import cPickle except: import pickle 
But it's *not* common sense! Serializing and deserializing is something you do to embed structured data objects in a serial (byte/string) format. Yes, `json` is a better choice some of the time, but that's not obvious to the novice developer.
You can use Selenium to do this
How do I use pip?
Think that falls under the golden role of "never trust user input"
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community is not a fan of "how do I..." questions, so you will not get the best responses over here. On /r/LearnPython the community is actively expecting questions and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. No matter what level of question you have, if you are looking for help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community is not a fan of "how do I..." questions, so you will not get the best responses over here. On /r/LearnPython the community is actively expecting questions and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. No matter what level of question you have, if you are looking for help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
My company did all it's work in R and then had to port it all to python in a month when they wanted to release a product.
This is a handy script that goes a bit further and parses the spf tree, resolves includes recursively, and prints out in a nice pretty format with some data about how many lookups there are, sum of ip4, ip6, and includes (handy for validating RFC compliance on large domains). https://github.com/b14z3/random/blob/master/spf_tree.py 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [b14z3/random/.../**spf_tree.py** (master → c1c4c62)](https://github.com/b14z3/random/blob/c1c4c62f1a9f702cb90de81b84b8c44e33e04abe/spf_tree.py) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmg94gy.)^.
But security by obscurity works, to some degree. Like with a motorcycle helmet, it doesn't give you absolute protection, but sometimes it's worth using! :)
I like Shiny but it's single-threaded and doesn't scale very well. How well it performs depends a LOT on how you host the app.
worst of both worlds, now it's both long to type and not reusable
Python 2.7 b/c today is 2017? I hope at least in 2030 new tutorials would use Python 3 by default.
Amazing job man 🙂
ggplot
Tried to ctrl-delete to delete the previous 'word', all my input disappeared. ctrl-z didn't bring it back. Well, that's me done.
Ah that is actually my bad! I use 3 as my primary, but OSX has 2.7 installed by default and I wasn't paying attention. You are completely right though, I should update that later to the info to 3 instead of 2. **Edit:** I actually did have a note in the original (when talking about virtualenvwrapper) that you should use --python=python3 and not use the default python (2.7)
Neat. Now I just wait for gitlab to copy this feature.
Originally read this as deep learning with scratch, was very impressed. Now I'm just a normal amount of impressed 
this is a common issue with any serializer. there's a reason i stick to json.
I do know but this was not really relevant to the example. Furthermore the motivation in pep8 is bullshit as they could have fixed the repr so it doesn't suck instead of saying "it sucks so don't use lambdas". 
 def ntuple(names, values): return namedtuple('anon_ntuple',' '.join(names))(*values)
Several deserialization vulnerabilities have been disclosed in major products in the past year. Most have been in languages other than Python, but using libraries with the same weakness as pickle. And it's very rarely necessary to need the object serialization these libraries offer. In most cases, json or msgpack would serve just as well, but I guess many developers find it too tedious to break their objects up into values of basic types.
Can you explain this to me? I thought serializers take data and condense it for transportation over a network, storage, whatever. How are they exploiting pickle? Is it just because they are running whatever they unpickle?
Uhh how many are just web pages containing the letter r?
why not `return namedtuple('anon_ntuple', names)(*values)` 
That'd work too.
or even better: from collections import namedtuple def ntuple(**kwargs): return namedtuple('anon_n_tuple', kwargs.keys())(**kwargs) &gt;&gt;&gt;ntuple(a=1) anon_n_tuple(a=1) &gt;&gt;&gt;ntuple(a=1, b=2) anon_n_tuple(a=1, b=2) &gt;&gt;&gt;ntuple(a="Test", b=[1,2,3]) anon_n_tuple(a='Test', b=[1, 2, 3]) This is actually the API i've always wanted out of the default named tuple. I don't want to go through a factory function
LearnPython does not support Videos-posts
What do you mean by "secure"? Password protected?
I think part of the problem is developers often don't think of headers *they themselves have set* in a previous request as "user input", even though it obviously becomes completely untrusted the moment it's coming from a client machine and may not be a header the developer set at all. In the client-server Internet, the saying "Never trust the client" is a bit more of a clear way of conveying the same idea.
That only works in 3.6+, it will give you unspecified orders in older versions of python, so probably shouldn't exist as a namedtuple alias.
Pickle runs whatever it unpickles. It does it on its own, it's part of the library. One of its methods of serialization is that it will take *an arbitrary function from the data* and run it to process that object's own data. This is not a security hole this is an intended feature. Pickle is designed for **local** use, where the pickle is being stored in a database only you can access, or on in a file only you can make changes to. As soon as other users gain access to modify your pickle, it's totally untrustworthy and can execute arbitrary code. Pickle can be used in network scenarios, but you have to be extremely careful to ensure the serialization data you anticipate has not been tampered with. Cryptographic signing is basically required to be assured of that point. If you want to use pickle in a networked situation, you're much better off with json instead. It cannot serialize arbitrary python objects as thoroughly as pickle can, but that also makes it much safer. It's pickle's object-storage features that get you into trouble.
The error message you've quoted is pretty clear: you need to install Microsoft Visual Studio to build the module you're trying to install. It even gives you a link to the page to download it from. Why not download and install the compiler and try reinstalling the module?
I've already done the same. Yet getting the very same error message. 
Yes. Sorry if that wasn't clear.
Look into the [zipfile](https://docs.python.org/3.6/library/zipfile.html) module in the standard library, specifically the `setpassword` method on `ZipFile` objects.
The setpassword method is used to set a password for decrypting the zip archive. It unfortunately does not secure one. 
I use protobuf because it's efficient and support different languages.
What's your end goal here?
Makes sense. It also makes sense that, in such a dynamic language as Python, deserialization can use the full expressivity of the language. But you could totally have an object deserializer that accepts a whitelist of acceptable constructors. Or one that, by default, only populates `__dict__`. Or one that only serializes/deserializes specially decorated fields. Having to turn your object model into basic types sounds like a lousy compromise.
I want to encrypt a file or files into a single archive and pass them on to prevent snooping. If I zip the data manually and pass it, the decryption part works perfectly. Encryption is a problem.
Does it have to be a ZIP file? If your end goal is security, ZIP isn't exactly the most secure file format out there. Who are you 'passing it on' to?
Being a zip is not essential. Steganography will be used as well. It's just to increase security. Essentially want a way to put multiple files together with a password which will work cross platform.
How are you going to 'pass it on'? Over a network connection? Do you want the password to be transferred in-band or through a different method? Why steganography at all?
Network connection. Password transferred externally. Steganography to hide the data.
Sounds like you want TLS then. TLS, even with a self-signed certificate, will be more secure in transit than anything you can come up with yourself. You can also have the receiving end send the sending end the password when initially estabilishing the connection, and then just send your files unencrypted over TLS (which will encrypt the data in transit).
Yes, how dare I write documentation.
https://gist.github.com/soulslicer/4459fb456cc37454eb7b5791011f4e6f Little script i was messing with to learn about ANNs based on this dudes work: http://www.deepideas.net/about-me/ ``` How to Run: python3.4 deep_learning.py # Right click to make Red points # Left click to make Blue points # Middle click to start optimization # You can modify the network on line 631 # Modify dimensions with DIM Flag ``` Example: ``` DIM = 2 # Build a hidden layer W_hidden = Variable(np.random.randn(2, DIM)) b_hidden = Variable(np.random.randn(DIM)) p_hidden = sigmoid( add(matmul(X, W_hidden), b_hidden) ) # Build the output layer W_output = Variable(np.random.randn(DIM, 2)) b_output = Variable(np.random.randn(2)) p_output = softmax( add(matmul(p_hidden, W_output), b_output) ) ``` http://imgur.com/a/ReYhm As you can see, circular data cannot be optimized because we would need to press the model into the 3rd dimension to do this So we change DIM = 3 and http://imgur.com/a/TlvZd And now it works!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/IVUIdQ2.png https://i.imgur.com/udtFxNW.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dmgjzh0) 
Sure Can you post a link to the code?
Why do you want to do this with Python, specifically? I ask because there are plenty of utilities out there that create password protected zips. The MacOS Archive Utility for one, 7Zip, etc. All of these will be considerably more efficient in use than you're likely to get out of a Python implementation. BTW, a password-protected zip isn't especially strong, and if you're using that idiot Steganography program from sourceforge (or any similar one that allows you to simply open the archive "hidden" in the usually jpeg directly with a zip archiver or by simply changing the extension) then you're not really doing much of value.
RoboBrowser?
Install pip. And use it for python packages. pip install &lt;package name&gt;. For a newbie the simplest way: install anaconda that should have everything you need
This is what frustrates me with django(as a php developer). In django i have to set up multiple files in order to get a project to run. In PHP, i only need literally one php file to run a script.. 
Try [hyperopt](http://hyperopt.github.io/hyperopt/). Its a project dedicated to hyper parameters optimization. This is agnostic to what library you use. If I'm not mistaken the search is smarter than vanilla grid search so it should be faster. You could also try scikit-learn's grid search.
Two different people contacted me last week after trying to pickle crypto keys between untrusted parties using pickle! So I'd say a reminder is still valid! https://blog.n1analytics.com/pickle-is-not-for-crypto/
I know about two possibilities you have: 1. Using some kind of test framework like mentioned in other comments Selenium or the other one. From experience it's faster to implement solution, but it lacks flexibility. You do only what you need to do it. Code is hard to maintain and expand. 2. Using reddit api https://www.reddit.com/dev/api/ with some http library (requests for python recommended) to make requests and get responses. It takes longer time to understand api docs, but in a long run it's more elegant and easier to expand solution if you want to build up from it. 
Nice! I'm not on reddit much lately, so my replies aren't exactly timely :P My problem with forced subclassing is: a) Multiple inheritance. If the concrete storage class has to inherit from something else as well, does it play nice with BaseStorage? b) Required dependency. If a library that provides a storage mechanism wants to be compatible with your library, it has to add it as a dependency, even if it could otherwise function very well without it c) Something else. I don't expect to know all use cases, and I'd prefer to not place unnecessary limitations on my users. But these are all solved by `register`, even b), because you could just do: try: from gecaso.storage import BaseStorage except ImportError: pass else: BaseStorage.register(MyStorage) ... which is way less problematic than: try: from gecaso.storage import BaseStorage except ImportError: BaseStorage = object
Thanks, I will look into hyperopt. I am using scikit-learn algorithms and its grid search. Does scikit learn grid search splits tasks into multiple threads/tasks? I am looking at which implementation or library will allow me to create cluster like structure to split work while keeping scikit-learn functionality? Or it won't improve performance?
In many cases hiring such an engineer that could reverse a program would cost more than writing similar program from scratch.
I haven't tried scikit-learn's grid-search. But, I did try random search and for me it yielded better results (read [this](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) if interested, basically, vanilla grid-search doesn't cover a lot of areas worth exploring). Also, a quick search suggests that scikit-learn allows you to parallelize. read [here](http://scikit-learn.org/stable/modules/grid_search.html#parallelism). And if this doesn't work, I would start from breaking down the search space into chunks/intervals, the running concurrently on each space (duh).
It all depends on the ultimate value of the secret you are protecting. If knowing the secret is not worth that much, what really is the value of your product/service/business? Conversely, if knowing the secret may actually be worth something then all it takes is one determined "russian hacker".
One of the best talks on concurrency I've seen to date.
I have tested your code and it takes 1.2s on my computer vs 10.7s for my implementation. However, you are using numba to compile the code, if I remove it, it takes 12.9s. It's impressive the gain obtained using numba!
Best and most secure way is to write your own DSL for your specific architecture. Also remove Turing-completeness unless you are ready to waste time and implement checks and bounds. 99 out of 100 use-cases do not require Turing-completeness. I'll also stretch it out and say that 99% of internet pages do not require Turing-complete languages (however they still use them and that's why we have security issues). Remove pickle and eval from your syntax unless you know how to put them in a real sandbox above dimension of the process you are running it. Though ways to do it are available it's easier not to use them at all. Besides eval is not that comfortable in that case as well as pickle, because you can run full on interpreter in a sandbox which will be more functional than any of those options..
Nice tutorial, thank you! If you don't need a database, there are also static site generators. You can use them to generate your blog as a folder containing html and assets and then deploy it in an S3 bucket and use CloudFront as a CDN. That way you can build awesomely fast, super-scaling websites which cost you nearly nothing. You can do a lot of crazy things with static websites. I built one having seo-friendly infinite scroll. You just have to let it generate json files.
yes, using numba is much faster than plain python, and numba has a very easy-to-use API. For comparing the performance, have you set your maxiter to 200 instead of 80 and image size to 800x640?
!RemindMe 2 days
I will be messaging you on [**2017-09-04 10:31:57 UTC**](http://www.wolframalpha.com/input/?i=2017-09-04 10:31:57 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Python/comments/6xhbib/hey_guys_i_built_a_quick_terminal_based_note/dmgopal) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Python/comments/6xhbib/hey_guys_i_built_a_quick_terminal_based_note/dmgopal]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dmgopuu) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
So I have been doing python for 3 years and a bit javascript so my view is biased. But python is not slow. As backend i guess it is as good as node or even better since much more clearer language and better error handling in my view. Also if you do something with math numerics python is the way to go. On the frontend side of course you have to use javascript in web. So basically if you are doing web development it maybe a good choise to stick with javascript on both sides. BUT i think for more complicated stuff I dont think javascript is the way to go on the backend.
Yes, the only difference is how the color is computed. Pil uses an array internaly, it's not as bad as it seems.
Update setuptools in the venv to the latest version using pip. I think that was what fixed it for me when I was getting a similar error related to problems finding the Microsoft Build Tools compiler. If it still doesn't work after updating setuptools then also make sure that Build Tools were added to PATHS properly. 
Pickle is relatively obvious. PyYAML's yaml.load is the one people tend to forget about much more often.
Well done Stripe. Hypothesis is a really nice testing tool. It's good to see company investing in it.
I have a wordcloud bot running on most days. :) /u/__word_clouds__
You're gonna need to give us a bit more information than that. How do you access the bot? Where do you run it? What kind of libraries are you using? Stuff like that.
right now i access it by literally "python bot.py" it uses psutil and pyautogui and the standard library. But apparently it looks difficult to bucle down and keep python programs secure from research ive been doing.
Is it possible to add python modules like sympy? 
You want people to not see the source code of your app, you mean? In that case, forget it. It's not worth it.
I wrote something like this . Pretty raw, put works for basic tests https://github.com/mmagnus/doctest-cmds
Thank you! Ill try to replicate this in another language then. Ill keep tinkering with it though. Its interesting to say the least.
pickle was designed to serialize objects and methods. In A basic sense, a pickle is unpacked the same way as a script is executed. My meaning is the pickle was meant to do what its doing. The article is using a dunder method to tell the unpickle to define a part of an object using standard library subprocess. Since subprocess can get shell access, its used as a pickle defined method unpacking. The remaining discussion about tornado is how to exploit the shell access. 
You can use a single file for an application, it's just a bad idea. The django-admin script just sets up the convention that is recommended.
Oh no, I meant that it's not worth the effort, in any language. No-one wants to steal your code. And even if they would, what would you lose?
Yeah but that is expected behavior though, isn't it?
I was expecting more interest in it. What's wrong with this?
Raymond is awesome!
No I think this behavior is completely arbitrary. Imagine if any other process happens to allocate memory in between your function calls. The behavior is probably different depending on what platform you are using or which compiler was used for numpy as well.
I wonder why use sqlite? Maybe it is just me, but my notes need to be safe, future-proof, and easily backed up. Keeping notes in markdown (for example) lets me use git as a backend for backup/sync/version. They are just text files so searching with `grep`. 
Zip is good for archiving files, gpg is good at encrypting stored data, and TLS is good for encrypting data in transit. Use the right tool for each situation.
I was there. He's definitely an amazing speaker.
Someone has been stuffing the body of a post in the title...
Does anyone else have trouble with accessing notes from the link in the description?
Have you looked at the gridsearch in sklearn? If not write somthing using multiprocessing? Keep us updated when you figure somthing out. This stuff is always helpful to know. :) 
You will want to store things in your own db to avoid repeatedly pulling the same data. This is to help with rate limiting the apis will have
I recently wrote an app that uses the Yahoo! fantasy api to set up lines for hockey (albeit some what poorly). Hopefully this helps. https://github.com/stussified/nhllines
https://github.com/bwjgmail/YFF-Transactions This is my first Python script. I've tried to add comments. I'm sure the code could be cleaner, but it works. I want to make this run on a batch of leagues + add error checking. For the batch, my idea is to keep list of leagues in leagues.csv (one league per line). leagues.csv has 4 fields: * email * yahoo_league * user_key * api_token Open leagues.csv, read each line, run the main function on each league, then repeat.
Bot seems to be quite friendly with python...unlike you the angry young man.
This should get you started. # import the libraries you need to access the API and load the json import requests import json url = "https://zkillboard.com/api/losses/groupID/513/" # load the response from the API as json r = requests.get(url) json = json.loads(r.content) # print the first thing with the index of 0 from the json object print json[0] 
Pretty sweet stuff
&gt; def ntuple(**kwargs): &gt; return namedtuple('anon_n_tuple', kwargs.keys()(**kwargs) You're missing a closing parentheses after `keys()`
I mean the benefit of using a database is that you can just link the scrybe.db file to your restore repo if you want to have your nots backed up and future proof, and the notes are stored in plaintext, so using markdown shouldn't be a problem. `grep` is a fantastic tool, but as far as I know (not that familiar with it), it doesn't include support for "metadata" like tags and titles, which is the benefit any note taking application brings. You're also able to apply weighting to the search results when you're writing a search function versus just using grep But hey, whatever workflow works for you man!
Guilty as charged, but I guess I was going for a "I built something, feel free to use it" feel, where linking to the thing was the best way to show people it
Thanks man!
Unfortunately slim on the async part. 90% is recycled from a past talk, and the interesting part would have been to see the counter and url examples migrated to async, for a start. I love Hettinger's talks, but I have to give him a thumbs down for this one.
If you're doing this on your own machine for yourself, you can be lazy as hell and just write: from subprocess import call call(['7z', 'a', '-tzip', file_name, destination]) Assuming you have 7zip installed, this will just launch 7zip as a new process.
I can't access the slides either. It's not just you.
OK, I got the batch process running. It opens a leagues.csv file and loops through. If you can help out with error checking that'd be great. If it encounters an error I want it to keep running as a looping batch process to process the other leagues. The main errors I'm worried about: * "Broken" yahoo league - e.g. wrong league number or league isn't publicly viewable * Any issues scraping the transactions table * Did the pushover API call actually work? Error code? 
There's a typo about half-way through the article. &gt; Then you just need to add a few quick lines to your rc file (usually **~/.basrc**) and start a new virtual environment: Should be `~/.bashrc`. Not a big typo, but it might be confusing to someone new to linux-like shells. --- On another note, very nice tutorial. There's just enough information to help someone get started without overloading them with too many details. Also, I find it useful to see how other people structure their projects.
Is there a way to make PyQt use the asyncio event loop instead of the Qt event loop?
Pythonista is what started it all for me. Me and my fam went on holiday and I had it on my iPad. Python was easy to learn. Holiday well spent imo
I mean that's how it works in every version of assembly, which ultimately is what all languages are running. 
oh thanks for your kind remark :)
No, afaik. The Qt event loop is integral to Qt, nothing works without it.
Thank you for pointing that out! I'll make that change later tonight. And thank you, that if exactly what I'm striving for. A lot of tutorials just blaze through a laundry list "do this, install this, run this command, put this in your rc file." They never explain why or what you're doing. Then you end up having a bunch of stuff you don't need or don't even know what it does. I'm hoping to try and avoid that and give the minimal setup, then add on as needed. 
That can be a good thing though. PHP has no way to globally version PEAR packages. Every developer setup can be different so you don't know what version of PHP or plugins their using. You don't know how they are structuring their system, and it may not be obvious if they're hiding it behind something in nginx or apache. Trust me I started with PHP, it's easier to get started in, but that simplicity comes at a huge cost in the long term. 
Yeah, but theoretically, someone could write an asyncio eventloop that makes the calls that the Qt event loop would make?
I strongly recommend you stick with python for this project, and for the limitations of the API's I think you can use ur imagination to counter that.
Just released. In case anyone is wondering why: it can work without gdb (I don't like gdb injection, not very reliable).
`GridSearchCV` will run across all available cores on your computer if you specify `n_jobs=-1`. Have a look at the sklearn docs, they're really good.
can someone post a link to the slides? I checked his twitter post and got "file not found" from dropbox
I'll check that out, i'm reading about ttk and may be easier to just learn kivy 
Nah. The Qt stuff is managed at c++ level. You'd need at the very least some c++ hack and then exposing that to python- and I expect Qt would likely blow up all over the place anyway.
Hello great tutorial I'm just stuck trying to get the blog Information into the collection when I run app.py I get the error typeerror: missing 1 required positional argument: 'self'
Knock yerself out...
More like "Not understanding Python's logging module" /s Well actually this article is overall well written and a pretty good introduction to the logging module, except for one specific piece of advice: "you'll want ot leave the root logger alone"[sic]. One of the great advantages of having a logging module in the standard library is that all other modules can make use of it, because it will always be there. This is the only and most obvious way for libraries to communicate non-critical information back to the programmer. Since libraries can not know the correct logging configuration in advance they should(and are) leave their logger unconfigured, apart from the name. If the logging messages should go anywhere useful, these logger instances need to be configured. This is the root loggers purpose. If the root logger remains unconfigured, it cannot do its job, and you lose one of the greatest advantages of the logging module. If you configure the root logger and then you see that there is a library spamming your log, you can easily disable that logger, as all you need for that is its name.
Yup... Didn't work for me until I disabled Firefox's Tracking Protection.
Attempting my first project that's not from a educational book. Trying to write a program to hash a password. It's going very slow...... 
Maybe "libraries shouldn't touch the root logger, and applications totally should" would be succinct. 
I aaah, I absolutely aah-agree.
I am still looking for help clanign up and improving [cvechk](https://github.com/evitalis/cvechk) if you want to contribute to it.
&gt; But security by obscurity works, to some degree. Really? It took two days to crack the latest DENOVU protected game and this is heavy shit, reverse engineering a compiled app of this scale shouldn't be that hard. If your project is worth it they will crack it in no time.
\*holds up hand\* I hate the GIL. Games aren't the only desktop applications that want to profit from parallelism.
Yes, you could totally sum up my rant like that. :) At least in my opinion, this would be the best use. Libraries just need to create a logger, leave it unconfigured, throw all their log messages at it(using an appropriate level of course) and then the application or even its user can decide, which log levels of which loggers should go where. An application should always configure it's root logger. A library should not configure logging at all, instead just use it.
No namedtuples??? Instead of this: class Person: def __init__(self,name,email,age): self.name=name self.email=email self.age=age def __repr__(self): return "User "+self.name You can use a simple namedtuple: In [1]: from collections import namedtuple In [2]: User = namedtuple('User', 'name email age') In [3]: a = User('Alice', 'alice@example.com', 23) In [4]: a Out[4]: User(name='Alice', email='alice@example.com', age=23) In [5]: a.name Out[5]: 'Alice' In [6]: a.age Out[6]: 23 Or if you really need to mutate the data, in most cases you didn't you use SimpleNamespace instead of a stub class: In [8]: from types import SimpleNamespace In [9]: alice = SimpleNamespace(name='Alice', email='alice@example.com', age=23) In [10]: alice Out[10]: namespace(age=23, email='alice@example.com', name='Alice') In [11]: alice.name Out[11]: 'Alice' In [12]: alice.age Out[12]: 23 Or if you don't care about the dot notation a simple dictionary is enough: In [13]: alice = dict(name='Alice', email='alice@example.com', age=23) In [14]: alice Out[14]: {'age': 23, 'email': 'alice@example.com', 'name': 'Alice'} In [15]: alice['name'] Out[15]: 'Alice' In [16]: alice['age'] Out[16]: 23 I prefer a namedtuple for structured records.
A simple virtual assistant that does virtual assistant stuff lol. https://github.com/PyPiPie/Scarlet-1.0-Virtual-Assistant-Made-With-Python
I'll just leave this here in case you haven't seen it https://m.youtube.com/watch?v=MCs5OvhV9S4
Thanks for the feedback. I have used static files as blog. They work well and the serverless blog has some niceties. 
I gave Golang a really solid shot earlier this year but it is a HUGE paradigm shift. I really feel like my prior programming experience held me back because it's just so different. What really attracted me was the speed of execution but having to reset my mindset for even the simplest problems was too frustrating so I left it behind. Also the community is tiny compared to other languages like JS/Java/Python/C#. 
Yes. Someone sent me a workaround -- and I just went to the github for exchangelib ... and I don't see the post in issues. Bizarre. Let me see if I can find the original text. 
Working on a reddit comment scraping pipeline: 1. that stores to a db 2. classifies (terrorism related/not terror related) 3. visualises results on a live dashboard Its been a lot of fun!
See Culex96 below. [https://www.reddit.com/r/Python/comments/6wbrsh/my_exchangelib_scripts_all_stopped_working/dm9ujgv/](https://www.reddit.com/r/Python/comments/6wbrsh/my_exchangelib_scripts_all_stopped_working/dm9ujgv/)
Would love to know what openings in data science are available at yelp!
Is there a message here somewhere? Is this why twitter is so useless, which is why I avoid it like the plague?
"For starters, this course will help you develop Python programs for Microsoft Windows, Mac OS X, and Linux" so I will not bother with their hype as by definition Python is cross platform.
I've never worked with .so files. You mention these are what you want to distribute. But how do users run them? How many .so files are generated? Do you put them in an installer? Also, do you know if this process will work with Python applications that use third party modules? 
Wow, interesting project you got there! May I ask why you decided to name it Scarlet? :)
Ultimately all languages do nothing but fire off tiny electrical impulses in a physical computer, so therefore 5/0 should electrocute the programmer. /s
My intuition says most of these are real-time. Gaming is probably the most popular example of real-time and includes realtime control systems plus live audio and video processing. IMO the only significant difference is that missing a frame/update results in a virtual death not a real one, which puts a higher demand for reliability/consistency.
What. All programming languages are some translation into assembly. Having the default "divide" operation in a Python be the exact result as the computer hardwares implementation of divide makes a lot of sense to me. /s
one of the best talks on concurrency I've seen to date. 
Those are great options and I'll definitely add them to the document! I have omitted the SimpleNamespace example, though: all this class does is wrap a dictionary into a class-like syntax, which I don't really see a real advantage for and is also very error prone since it will not throw errors in case you're assigning data to a mistyped field, etc. I can't see any real advantage for it when compared to using dictionaries except for the dot-notation (and even that is a minor preference). If seems very unpythonic and redundant, opposing Python's rule of "there should only be one obvious way of doing things". If you fell it should be included, let me know why, please. Here's the link the new section - thanks again for the cool suggestion! https://github.com/tukkek/notablepython/blob/master/README.md#structured-data
It already exists. https://github.com/harvimt/quamash
Your error is almost certainly due to having installed Anaconda as root. So, you should change it back to being owned by yourself: `sudo chown -R teddykoomen:teddykoomen /Users/teddykoomen/anaconda3` 
this is a really weird subreddit to ask this question in. `ffmpeg` is written in C. I wouldn't be surprised if mods remove it. Regardless... we're probably not the only pythonistas that've had to deal with `ffmpeg`. I've switched to using matroshkas and `mkvmerge` for all my personal stuff in part because it handles this sort of thing better. I'm guessing this is an unresolveable problem unless you re-encode the audio so it's all in the same format.
same reasons for being interested and exact reasons for not pursuing further. i still want to but.... probably will stick with larger communities 
Unfortunately you are correct, 90 % of this talk is about threads and processes and asyncio is a footnote. Asynchronous programming is by no means the be all and end all of multicore programming, but it's not really addressed here significantly. 
Sorry, I'm new to programming. Okay I'll try that out then, subprocess call should do the trick it seems. Thanks for the help.
Sounds like an assistant name lol and also it's an acronym for "Sorta Crappy Assistant Robot Lazily Engineered Today"
fantastic talk, just thought I'd add another in case no one has seen it on asyncio https://www.youtube.com/watch?v=M-UcUs7IMIM
maybe try /r/jobbit/
Excuse my ignorance, but this looks like the other way around: forcing asyncio to use the Qt loop. Am I wrong ? This approach actually makes sense, basically providing another pythonic wrapper on top of Qt. Qt will still use its own event loop and all will be well.
Stack exchange has job boards [too] (https://www.stackoverflowbusiness.com/talent#) 
 def goinacircle(alex): from random import randint n = 0 while n &lt; 10000: v = randint(-1,4) d = randint(0,6) alex.fd(d) alex.left(v) n+=1 This isn't interesting at all...
&gt; I can't see any real advantage for it when compared to using dictionaries except for the dot-notation (and even that is a minor preference). The dot notation makes auto-completion in IDEs/editors/repl available because the keys are now attributes. It's a nicer interface IMHO instead of using keys or the `dict.get` method. I have never seen that a REPL auto-completes dictionary keys. Besides this you can inherit from SimpleNamespace and still get the automatic `__repr__` method and the dynamic attributes: In [1]: from types import SimpleNamespace In [2]: class Person(SimpleNamespace): ...: def send_mail(self): ...: try: ...: print('Send mail to {}'.format(self.email)) ...: except AttributeError: ...: print('No mail address provided.') ...: In [3]: people = [Person(name='Alice', age=23, email='alice@example.com'), ...: Person(name='Bob', age=43)] ...: In [4]: for person in people: ...: print(person) ...: person.send_mail() ...: Person(age=23, email='alice@example.com', name='Alice') Send mail to alice@example.com Person(age=43, name='Bob') No mail address provided. It is not as useless as you maybe thought it is in the first place. There is also `typing.NamedTuple` which offers a class based interface to collections.namedtuple, but I encountered problem while playing with it. 
It draws circles that are similar to my hand drawn ones. I thought it was stupid enough to post ironically.
 Upwork.com Are you qualified to interview and review the code?
I saw this post from last year on hacker news today. https://www.r-bloggers.com/the-real-prerequisite-for-machine-learning-isnt-math-its-data-analysis/ It might be timely for you, but he recommends what you should learn if you are working in R or Python, which I am guessing you just learned with your degree. 
hunter2?
Can you post the name of this? All I see is *******. 
The .items property was added to Legend in the last year. The code you are running is too new for whatever version of Bokeh you have installed. 
A fascinating read, thanks! Yes, I took a course in data mining recently which incorporates the data cleaning/munging, EDA, etc. What s/he mentions is that it's better to learn sklearn or caret than study mathematical theory. I feel that if I knew programing fundamentals, I'd be able to execute caret/sklearn better. 
Great tool. I once started to write something similar and called it "xtrace" by analogy to the -x argument of bash. Yours is much more complete. Some features I had that you might want to consider: * Option to exclude tracing into system libraries. * Indent according to call stack. Use a character other than space (e.g. '|') * Abbreviate file name to last two components (containing directory+base name) And some other ideas I never got to implementing: * Assign colors to call nesting lines (like git log --graph) * Annotate loop iteration count (line number goes backwards on same function) * Annotate time taken (with threshold) * Annotate first time a line is ever reached * Annotate changes: if branch taken/not taken that is different from the last time this code was reached. * Collapse iterations/function calls where no differences in trace structure were encountered to keep output compact. 
Is this your blog Sir S?
Cracking a game is one thing, whereas I'm talking about getting full sources. Most companies don't distribute their apps in source form, because they want their intellectual property to be protected. This is what I'm talking about.
Users don't run .so files, they're not executable. They're just importable modules. Third party modules shouldn't be a problem. Sorry, I haven't got much experience with installers.
I... Don't know what this does... Should I?
Go to a populated area and execute one of the following: * Loudly declare your love of ruby if you want a python framework user. * Loudly declare your love of R if you need a python mathematician. 
Ruby. Eww...!
[Playlist of all the PyBay 2017 talks](https://www.youtube.com/playlist?list=PL85KuAjbN_gtuA4pNPftJWaui-8ARervQ)
I'm really not sure what you're asking. Code an interface? That's so vague. Coinigy might already be what you're looking for. If you're trying to exploit a freelancer to automate your trading using an exchange API with more than a coin toss success rate for $1,000. Maybe ask yourself why a developer would do that instead of trading for themselves... This is exactly why I stay away from these projects because most likely you don't want to pay a fair price and the expectations are absurd. 
&gt; If you configure the root logger and then you see that there is a library spamming your log, you can easily disable that logger, as all you need for that is its name. Author of the article here. This is basically what I meant with "leave the root logger alone". There are some libraries out there that log directly against the root logger (`logging.debug`), instead of defining their own child logger. Those libraries screw up the usefulness of the root logger because it prevents you from disabling logging output for those libraries. I'll clarify the article, because the same point you made came up last time this was posted. **edit**: I updated the article and basically replaced "Don't use the root logger" with "Don't log directly against the root logger". Does this clarify things enough?
You really start to appreciate Python once you dive into JavaScript. I wish JS would just go away but I think we can blame FB because of React...
a topic about [redesigning Python's named tuples](https://lwn.net/Articles/731423/)
There are code that have to use stdlib's namedtuple, and code that can use whatever library for its purpose. For code that are free to choose, there's one obvious benefit of using a third-party library: No need to wait for an upgrade of Python for improvements
FYI, I've removed the dependency on werkzeug.
What does it do?
Yes. Thanks for accepting criticism.
[a C-based implementation of named tuples](https://github.com/llllllllll/cnamedtuple)
Next time you post a link please use the `-q` for quiet mode in the title :-)
Pn me. Have done it before
The readme could use an example 
yes. plain chinese works, but chinese names sometimes require additional annotations to pain the correct variant of a glyph to be displayed correctly. (looking for links to source this claim, i found that it seems to be more japanese texts than chinese that encounter that problem, but alas, emojii and math characters are the more widespread astral characters: &lt;https://stackoverflow.com/questions/5567249/what-are-the-most-common-non-bmp-unicode-characters-in-actual-use&gt;)
&gt; Maybe ask yourself why a developer would do that instead of trading for themselves... Because a lot people just aren't interested in trading, or don't want spend their time doing it.
An impressive amount of sleuthing!
Doesn't build on Windows :-(
Doesn't work for Python 2.7 :-(
tl;dr any1?
Nope. He's in one of my feeds
I did upgrade to the latest version after running pip install bokeh
You made that up just now, right? 
Working with pandas and xlwings to automate most of my office work 
Hi GeneralEbisu, Probably your first read should be PEP8, Style Guide for Python Code @ https://www.python.org/dev/peps/pep-0008/ . Afterwards, there are interesting links to follow to complete the first read: 1) https://pythonschool.net/category/oop.html 2) https://stackoverflow.com/questions/12391621/best-way-to-design-a-class-in-python 3) http://anandology.com/python-practice-book/object_oriented_programming.html 4) https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/ Regards.
This is sweet! I've done a lot of data extraction from Gen II and III, but I haven't really looked at Gen IV and beyond. Nice to see some of the inner workings of those games.
[removed]
&gt; Option to exclude tracing into system libraries. It has it (`stdlib=False`). To exclude stuff in site-packages you could use `~Q(filename_has='site-packages')` or similar (dist-packages?). &gt; Indent according to call stack. Use a character other than space (e.g. '|') I guess I could use pipe instead of spaces. But there would be lots of lines then. Is this really so useful? &gt; Abbreviate file name to last two components (containing directory+base name) I decided that I'd use all the available space. First version of hunter actually did that but I always needed more than 2 components. &gt; other ideas The color one is a great idea. Not sure about the annotations - what would they be useful for?
It's like strace but for python code instead of syscalls.
&gt; give me the ability to pick up any language I may need in the future Doesn't seem to be realistic (unless you have like 10k++ hours to spare). As this would includes math "languages" such as number and set theory, linear algebra, topology, statistics, ... and programming languages of all kinds (assembly, C, D, Kotlin, Js, Haskell, Matlab, SAS, ...) I would be more specific in your aim. Identify the (theoretical) parts that you are lacking, and study them. Also, don't spend to much time on learning specific software, rather focus on methodologies (query or code profiling, testing, workflow, ...) that can be applied to diverse problems.
What name? I don't get it.
Wait is ruby even still around? I haven't heard anything about it for months.
&gt; If you're trying to exploit a freelancer... That's a little harsh. I'd normally put this down to lack of experience of how complicated tasks can be as well as how expensive people are to hire (compared to most other costs that people pay for). There is a flip side that developers can be inclined to do things that you don't want them to do because they don't internalize your needs. For this sort of thing, I might be more inclined to network and try to get someone from whom you have buy-in and profit share. 
A couple of other suggestions: * University job boards (for smart yet inexperienced people who have some time on their hands, e.g. PhD students who need distractions from their thesis) * Cryptocurrency meet up groups * Hackathons
http://knowyourmeme.com/memes/hunter2 
I have a leaky tap in the bathroom which I'm going to tackle.
The lines + color are useful for matching entry-exit of specific function calls. While tracing of values is sometimes useful, I find that the most important and useful information produced by such a trace is the execution path. Annotations help anomalies stand out in a long dump where large nested execution paths repeat exactly - except for that one time where it is only \*almost\* exact.
Pretty sure you need to follow the instructions for concatenating inputs with different codecs at https://trac.ffmpeg.org/wiki/Concatenate#differentcodec However, the filter complex is going to be extremely complicated for anything over a couple of input videos so it might be easier to break this process down into 2 steps: 1. Convert every input file individually to an intermediate output file with a standardised encoding across all the files. 2. Concatenate the intermediate files using the concatenation techniques for inputs with the same codec (see the first section of the page I linked above). The third option (and most pythonic as this is r/python after all) would be to see if [MoviePy] (http://zulko.github.io/moviepy/) is suitable for your needs. 
Why are you opening the file in append mode? That's not going to do what you want at all. Regardless, I'd like to take a guess for a second. Are you running on Windows?
Since it's a binary file, you should write into the file as such, ie: ``` with open("test.pdf",'wb') as f: ``` also `close()` is a method, thus `r.close()`
Aha, success! 
Isn't 'a' append mode?
Ah yeah, you're quite right, `w+` is open for writing and reading. An odd mode for this usage, but not as odd as append mode. 
However, you didn't answer my question: are you on Windows?
I am not sure that you need classes for that; python has modules. FP has lot of resemblance in pure Maths. Regarding OOP, use composition than inheritance; 9/10 times composition is better. 
/r/forhire 
Requests does that by default.
Attaching via GDB can be done with less chances of crashing the program if instead of calling PyRun_SimpleString directly you do it via Py_AddPendingCall. This function is designed to be called from signal handlers.
Hmmm yes, I might do the colored lines, unless you wanna help out on that ;) About annotations, still not clear how it would look. Some examples if you will? 
[Raymond Hettinger: Python's Class Development Toolkit](https://www.youtube.com/watch?v=HTLu2DFOdTg)
Implementing internet protocols. Started with making some base classes like a class to handle tcp connections, and sending/receiving data. Then, using a list of protocols on wiki, I started implementing them. Currently got echo protocol and discard protocol. Planning to flesh it out to make ftp server and such over time.
Python is a language that makes code easy to understand, write, and reuse by removing visual noise from code ({;), leveraging white space as a programming structural element, and providing inherent and obvious standards for common class operations (e.g. `__enter__`, `@property`, etc.) so code is elegant and consistent between programmers. Too technical? Python is a language that translates ideas between a human and computer. Other languages work like a computer thinks, then makes the human comply. Python emulates the way a human thinks, so that writing code is easier for us. 
I like the sound of this. I'll have to give it a go in the future
Thanks, this is very useful.
I'm not exactly sure what your question is, but I'd explain the terms module and class as being like tools. A module is a toolbox and a class is a tool intended for a relatively specific purpose, like a hammer and screw drivers in a toolbox. A class also describes a type. You could have a Screwdriver parent class and Phillips and Flathead as children of that class. Just in Python and programming in general, we have a lot of toolboxes that can do all sorts of different things. 
Don't worry about using classes. Instead follow the principle of reflection. Try to make your code reflect the mathematical solution of your problem. So if your problem involves finding the gcd of two numbers then you don't need a class design. However, if you are doing this many times then it might speed up your code if you had a list of prime numbers. In order to keep track of the primes you might want to construct a class. 
Do you mean their Session object? I did look into that but it seems there's some uncertainty with regards to its thread safety (or lack thereof) https://stackoverflow.com/questions/18188044/is-the-session-object-from-pythons-requests-library-thread-safe 
Why do you need thread safety here? Are you running all of your tasks in the same process?
Ryzen has AVX2, just not AVX-512. Machine learning stuff is better handled by GPUs. Realistically, you probably won't load either CPU to 100% anyway.
&gt;I'm really not sure what you're asking. Code an interface? That's so vague. Was intentionally short because I didn't know if it was OK too post here... &gt;Coinigy might already be what you're looking for. Seems the opposite of what I'm looking for - Fancy GUI and manual intervention. &gt;If you're trying to exploit a freelancer to automate your trading using an exchange API with more than a coin toss success rate for $1,000. Maybe ask yourself why a developer would do that instead of trading for themselves... What I want is prob best described as a wrapper to a trading strategy. I'll code the strategy, but want to just plug it into an interface, or wrapper. A dev would take on this project because it's not a profitable trader and/or has no interest in trading. I would not reveal the strategy. I think it's a clean project. &gt;This is exactly why I stay away from these projects because most likely you don't want to pay a fair price and the expectations are absurd. I actually want to pay a fair price... But one thing is sure, in the era of global business and cross border collaborations, I will shop around for the best price. 
I just took up learning code and I wrote a my first "program" I guess it's called. All it does is simple addition but I've remembered how to do it so I'm happy. Not sure where to go from here but yea
Lol Sounds like one of those ethernal rivalries? Not fully in the know, just guessing...
From some of the benchmarks, it looks like the new Ryzens have *really* good floating point performance.
Python has its fans. 
Thanks for replying. It make sense . Correct me if I am wrong ..... So library is collection of modules ? 
Can I ask what you use Hypothesis for? And what's your favourite feature and biggest gripe? I mostly end up pointing it at small side projects, so the introspective bits and test inference are always fun. The stateful testing also looks really powerful, but it's also so verbose and class-based that I've never really gotten into it :/
The tasks are Celery tasks using the prefork method, so they're running in their own threads (iiuic). Although eventually I plan on switching to the eventlet method, if it's compatible. 
With Eventlet, you have to use async I/O (i.e. not requests) to run more than one task concurrently. If you want to move to Eventlet in the future, just do it now before you write sync code.
Some people would say that typesetting math with Unicode characters is a perversion anyway. On the other hand, I definitely don't think people should go through all the trouble with (for example) matplotlib and FigureCanvasTkAgg for something simple like ℝ \ ℚ. You can use prerendered images for emojis and ImageDraw from PIL for characters you didn't anticipate, but it is rather unpleasant for both the coder and the user. However, it has nothing to do with being legacy. Neither Kivy nor PySide are able to "just work" with 💩𝕊ℝℚ like they do with abcd. They don't even throw an exception like Tkinter does, they just silently display nonsense. Kivy can't even "just work" with non-European digits like ٣३༣৩៣೩ that don't cause any problems in Tkinter or PySide. I don't know much about gtk3 and wxwidgets. Are they more satisfying in this regard?
There's no point to this, it's about reading about the process. There's maybe a reveal but the article is about how they did it, at least so far. edit: nope finished it, it's just about the process. Super cool article, I hope they've got more.
You're probably right, but a better way to correct people on this might involve wording it a little more softly. No one actually cares if it's a they or a she or whatever. It's still good to know, but just keep in mind that you're fighting an uphill battle.
It's in the Readme
I'll word things "a little more softly" when people stop assuming that "technical == male", because that assumption does way more harm than someone not enjoying being corrected on it.
I know that l'll need some GPUs for ml/dl, but I want anyway the best "base platform" to work with for my current needs.
[stop writing classes](https://youtu.be/o9pEzgHorH0)
Maybe it does, but as long as most people in tech are males, I'm going to assume that the generalization is true. This isn't the place to get into politics so I'm going to end with saying I appreciate what you're trying to do, just don't be *as* aggressive about it. You're not being aggressive but if you're trying to convince people to change their ways you need to appeal to them in ways they understand. Making people feel like they did wrong by assuming, after seeing 9 red blocks in a row, that the next block will probably be red too won't help your case.
There's a good article recently posted that summarized this. Look at /r/Python in the last week or so and you'll see.
Thanks for the suggestion! More qualified to review the code than to interview... I'll code the actual trading strategy myself. The second point is one of my concerns. Have any tips to share?
&gt; You're not being aggressive but if you're trying to convince people to change their ways you need to appeal to them in ways they understand. There isn't one. It literally would not have mattered what words I used to make that comment; *someone* like you would have come along to object to them and shift the problem from "this person didn't even bother to look up who they were talking about" to "well, *you* should have been nicer in pointing it out". Want to help? Stop trying to shift the focus away from the real issue.
I'm only interested in making sure you are able to help champion your cause as best as you can man. You can say you did nothing wrong, and maybe you're right but the point I'm trying to make is that you're going to face a lot of opposition in fighting for a cause that's fairly political right now. Good luck!
 That's a good video. It's not saying don't write classes but to write good classes for things that need it. 
Yea yea, I did it! You did helped me a lot...with logic and some of the code helped me a lot. I just need to output that and I am done :)
A lot of programming languages have the concept of modules and classes. As others have already pointed out, not sure what exactly you are asking. Python is dynamically, but strongly-type programming language with lots of "batteries included" libraries to do a lot of things easily so when I think of Python I think of [this](https://pbs.twimg.com/media/B38J1JTIUAAW1NS.jpg).
There is always a point. I am not asking for complete know-how essence in 1 sentence, but just for short abstract about: * What is this post about? - "datamining Pokémon" tells nothing; OP, you've put there a shitty title. * What brilliant ideas / standing out techniques are covered by this post? * Is is worth reading (a good tl;dr has this part implicit)? A good tl;dr should cover the main ideas, bust most importantly, it should encourage you to reading the article.
I think the default here is that things you post are posted with a permissive license. You'd really need a good disclaimer for you to suggest paid/proprietary code and get away with it here :)
I'd agree; python is a programming language with an emphasis on the UX needs of the human. Takes into account our innate psychological tendencies . Zen of Python sums this up very well.
I started learning R, and then saw this post. I jumped over to Python and started learning NumPy, Pandas, etc.. Why would you not do it in Python? What are the benefits of R over Python? I have yet to see any. Edit: Thanks for the responses and supporting your arguments. As a software engineer with little to no background in statistics, should I start with R or Python, or does it really not matter?
I personally find the way R works more intuitive in terms of statistics. Of course it makes 0 sense as a programming language, though, and I also prefer Python over R. Just playing the devil's advocate here.
Profit share would be awesome. I've just seen the described scenario on Upwork far too many times since the crypto bull run.
~~Also, by virtue of using `with` the `close()` call is redundant; `close()` is called when the `with` block is exited.~~ disregard i'm a potato
As a programmer with little statistics background, this comment makes sense.
As far as I know, Python has good community than R according to stackOverflow and R has an edge in statistics and visualization, whereas Python has an advantage in machine learning and building tools.But the trend is changing now,As you said It has great modules like Numpy, Pandas, Matplotlib etc
My experience as well is the statistical support in R is more robust. Stats models is good, but R has a huge number of stats specific libraries. I also prefer python however, but respect R for what it is good at.
&gt; Identify the (theoretical) parts that you are lacking, and study them. Also, don't spend to much time on learning specific software, rather focus on methodologies (query or code profiling, testing, workflow, ...) that can be applied to diverse problems. ^ This. This is exactly what my question was intended to determine. I just don't know anything about fundamentals, so I don't know the methodologies that I would need to know.
tl;dr has always been the point. ie you'd read a post by forum moderators' new rules and the tl;dr would be 'dont be dicks'. You've got a formalize definition of tl;dr but I don't know where you got it. tl;dr isn't an literary analysis it's a sentence or two that summarizes the point of the post. Agreed that there's missing a few words to the title. It's worth reading. 
This is fantastic - wish I'd known about it before. [The IPA on your site is kinda off. The 'c' in Cristian should be transcribed with a 'k' and 'ˌ' denotes secondary stress before a syllable, so unless 'tj' is its own syllable, it shouldn't be there. Also, I don't think your first and last name contain a different 'e' sound. With that out of the way...]
Back to school in systems programming. Learning to implement my own string library in C without arrays, memory allocation, or anything that requires #include except for 'write'. This class is fixing to either be great or terrible, but it'll be fun.
If it's not working after updating to a recent version, then your update didn't succeed. Or possibly you have multiple versions installed in different python locations and are using a different (older) one than you intend. 
R is better on the statistical side of things, beginners with no programming experience find it a bit easier to learn with syntax such as `t.test(measurement ~ group, data = my_df)`, will work out of the box without importing any packages. There are also some more exotic statistical methods in R that don't really have a python equivalent. For everything else I would 100% use python.
Once you're comfortable writing classes yourself, look at the [attrs library](www.attrs.org). It's a great way to help simplify your classes. [The One Python Library Everyone Needs](https://glyph.twistedmatrix.com/2016/08/attrs.html) 
I think a developer would have to be somewhat interested in trading for a project like this to be successful. Sure someone might need the work but OP probably has some TA strategy they want automated, so familiarity with trading is probably a prerequisite. 
It's more or less English so very much readable. The concept of loops, classes, functions and so forth isn't that complicated in my opinion, it makes perfect sense once you used it in an example (or two) I really like Python exactly because of that
Thanks, the Uni job boards are an obvious choice, but totally didn't think of it!
"What I want is prob best described as a wrapper to a trading strategy. I'll code the strategy, but want to just plug it into an interface, or wrapper. A dev would take on this project because it's not a profitable trader and/or has no interest in trading. I would not reveal the strategy." If you could code the strategy then you could figure out the rest. "I actually want to pay a fair price... But one thing is sure, in the era of global business and cross border collaborations, I will shop around for the best price." Sounds to me like you want to underpay. The quality of the work is going to directly reflect what you pay. 
I won't claim to be a master at R, but I have been working in it for many years now and consider myself at least proficient at the language. I'm a beginner at Python, but like any other language what I expect to be there is there. I recommend picking one, become proficient, and then making yourself aware of what is going on in the other. Ultimately, Python is a more versatile language. This can bog down a beginner in statistics who just wants to get some work done. For example, searching for how to do something like draw a random number from a normal distribution (pretty common task for a programmer trying to solve a statistics problem through simulation) and you are now looking at packages available to Python (I would recommend numpy) as oppose to just reading about the rnorm() function built into R. Similar to how you store and work with tabular data. In Python you are going to be utilizing a package that has an advanced matrix data type -- it might only work in that package or it may be compatible with other packages (depending). For the most part if you keep to Numpy a lot of stuff will just work the same way it just works in R (that is my limited experience). However, it is a difference worth noting that R's data.frame data type is built in so any package is going to be sensitive to the R data.frame and the R list. I think that Python will continue to overtake R as it has proven to be more versatile. If you have already made the switch, just continue your course. I recommend keeping an open mind about R, and if a package comes up that looks enticing for you try it out. R has a lot of stock functions that are really great for professional looking graphics and a nice library of packages that can do some fantastic data visuals. Edit: For me, personally, I need to get more into the database (SQL) side of the whole process and I'm not liking the tools available in R for this. Python just seems to have a better arsenal available because it has been used in those applications extensively. Seem like the focus in R is just pulling data from a database and working on it -- not actual database management tasks.
You are prejudiced, seems like nothing I'm going to say will change your mind. &gt;If you could code the strategy then you could figure out the rest. Yes, I could. But for the sake of having this done asap, I'd rather pay someone. I'm also learning python now, not a professional coder, more like a hobbyist. Not even a professional trader, at that. I actually have a day job that has me work long hours, so when I get home I'm not the sharpest for coding. Leaves me the weekends, which means this algo trading won't be up and running anytime too soon. I'm looking to expedite this. &gt;Sounds to me like you want to underpay. The quality of the work is going to directly reflect what you pay. Want to pay fair, but I'm not going to pay $200/hr for a pro with 20+ yrs experience if a PhD candidate could do the same for much less. I want to try my hand at automated trading, not run a hedge fund. 
R is very much a modern/relevant language
They're different tools for different jobs. I posted this in the last thread: &gt; For data Python is best if you want to programatically deal with it. R is best if you want to statistically deal with it and Matlab/Simulink is best if you want to ID &amp; control it. Most people in programming are used to doing things programmatically. So analyzing data in that way makes sense. Statisticians went their own way when designing a language specifically for them. When I was doing graduate level statistics classes (Design of Experiments) we used [SAS](https://en.wikipedia.org/wiki/SAS_(software\)).
**SAS (software)** SAS (previously "Statistical Analysis System") is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics. SAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Python/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
R is one of those languages and tools that you almost need a STEM BS to start understanding what it does and why. It's a specific tool for a specific job (statistics). And for that it's great. I'm not going to "automate the boring things" in R.
R is a trolling language. It was never meant to be anything other than something to throw novice developers off the path. It's like a master carpenter is training an apprentice, but doesn't want him around, so he tells him to go bang nails into scrap wood. The master says that banging nails into scrap wood is a modern and relevant contribution to the field. Teaching young people R is like hijacking a highschool in America and teaching everybody to speak in Urdu. That's not to say learning Urdo isn't helpful, but the things you could do that are displaced by Urdo learning is a real loss to those people you harmed. In the same way, R is damage to the minds of people who are fooled into it. It's a trolling language, it's how you tell someone to "go harm themselves" by doing so in a way where the target actually does. We put criminals into concrete boxes when they get too effective at tricking others into harming themselves, setting traps for others to fall in, to eliminate them as competition. The fact that python is overtaking R is people slowing coming to this realization, which I believe I can prove beyond reasonable doubt, enough so a neutral person would acknowledge the hidden and defecting agendas behind the obvious facades. R is not the future of data science, charting, machine learning, computer vision, or any of that. At least languages like Chef "whitespace" and "Piet" make their trolling intentions obvious, R does the same while maintaining the ruse. For all the people downvoting me, what you need is some R programming. Write some programs in R and use up your brain space with it, Go ahead, see what happens. Lets get back together in 5, 10 and 15 years. Give me an opportunity to laugh at you after I've been proved right, and kick you while you're down so you can call me an insensitive asshole.
Does anyone have a guide to DSL? I know what they stand for ('domain specific language') but Google is utterly failing me. For now I'm using jsonpickle because this way, I can validate the data before (un)pickling.
What the fuck lmao
Ryzen looks promising but there are still questions if it will deliver. The price looks really attractive. AVX is pretty mature but expensive for what you get. Our data center is intel, but we are ordering some AMD gear to see how it performs. 
`tj` is in fact a single syllable (from the 3 total)
Lets see how many downvotes I can get from the R community lol. I'm a programmer who had R forced on them for a class, it was the worst class of the entire program, everyone agreed, R truly has no redeeming quality. I can really rustle the jimmies of the R community because when it comes down to point-by-point debate and argument by argument, I can indeed prove this point as more true than it is false. And I can do it by many avenues, character witnesses, the language itself, the libraries around it, the concensus of people who were forced into it, the novices who learn it and realize they have been taught wrong, the people in the people who use it not being programmers, having trouble wrapping their minds around "Variables" because R does away with the concept of "data type". Line of execution isn't stable. It's absolutly barking mad and insane. And the people who can't see it have no knowledge of compiler design and world class level enterprise engineering. That's just one tangent of hundreds of others I can use to demolish the R dummies. Walk into a Dev shop and celebrate your experience in 'R' on your resume. The experienced people there are going to stifle laughter, and you will not be taken seriously. And if you are, it's because they want to harm their employer by bringing dummies who have negative ROI on the payroll. You can take that to the bank. Look for this out there, this is what you will find.
Your talents are wasted here, go start a blog about Trump and SJWs
SQLAlchemy did this ages ago: http://docs.sqlalchemy.org/en/latest/changelog/migration_10.html#new-keyedtuple-implementation-dramatically-faster
Hm. You need a tokenizer and/or parser. In some cases an AST tokenizer would be useful. I wrote my own simple parser for a DSL for programming a simple DFA which parsed a script for a mail message and output whether a message is spam or not. It had several commands designating part to be applied to (ie headers or body, which particular header) and what type of search to perform (simple, regex or wildcard) and the spam weight to apply with a string needle written later. Unfortunately it was written 10 years ago and the server which used it has long perished with all the data. Now we have such realization inside exim which has better speed compared to python milter we used to make the dsl run (it was pretty fast as all it needed is find particular tokens and read till the next opening token but python is slow when you have 2M+ messages per day on Celeron).
azure is a cloud computing platform right ? Python machine learning projects mostly host on aws (Amazon web services) cloud 
Damn, comments started well, with balanced reasoning, and now this. I've been trained on both languages, although far more proficient in Python. I think anyone formally trained in stats likes R because it is strongly mapped to the math reasoning. Python being a more general language have less of this quality. This is both a weakness and a strength. I prefer Python as it is more generalist, and thus any non strictly statistic duty (working with files, manipulating strings, parsing html files, etc...) Is a lot more convenient. I think the whole "taking over" should be broken down by type of activity anyway : remove ML and TensorFlow, I am not sure Python leads anymore. 
Repost of [this](http://www.datasciencecentral.com/profiles/blogs/python-overtakes-r-for-data-science-and-machine-learning).
Using R's data.table the syntax for data manipulation is much less verbose and vectorized operations are much more performant. R is a language developed for statistics. So things that use stats are just very, very intuitive and easy to do.
I still use R for a lot of my plotting and statistical analysis. It's very easy to test ideas when all the functions I need are already built in. For pretty much everything else though, I use Python. Jupyter notebooks let me pass data from one language to another almost seamlessly, so you can easily exploit the strengths of both. Edit: words 
Further to other answers you may find [Python - Executable Pseudocode](http://archive.is/2013.09.02-020216/http://www.melbpc.org.au/pcupdate/2108/2108article9.htm#selection-41.1-41.31) of interest.
I'm not sure people are downvoting you because they're fans of R. It's more that you seem very emotionally-invested in trashing R, like you have a deep, raw hatred for it that no one on this sub can really relate with. Couple that with weird analogies and a relatively hostile tone, and yeah, it's pretty strange. But I'm not saying I disagree with your points. Never programmed in R, and maybe it is as bad as you say. Even then, there are languages that I dislike and even despise, but I never post multi-paragraph rants to trash them. I just ignore them and move on. I don't think it's healthy to be so obsessed with hating a programming language. Just my 2 cents.
Agreed. Numpy usage tends to be functional, and languages like R, Matlab, Mathematica all tend to emphasize functional programing. 
&gt;The fact that python is overtaking R is people slowing coming to this &gt;realization, which I believe I can prove beyond reasonable doubt, enough &gt;so a neutral person would acknowledge the hidden and defecting &gt;agendas behind the obvious facades. Then by all means go ahead and do it. Unlike most people here I'm all for language wars and smack talk, but it's got to be backed up with facts. If it's not, it's as empty as the Pascal folks' claims, including one employee of Embarcadero who claims Delphi is "five times as productive as Python". When pressed, he can only back it up with "That's what our customers tell us." 
[Code Github here](https://github.com/joaoperfig/mikezart)
Advantages over WebDriver's python client, which can also do chrome headless?
If you make it work with requests, you still could try Grab framework(https://github.com/lorien/grab). In my exprerience, it is faster and more feature rich than requests. 
Seems like that `close()` belongs to the requests connection, not the file.
Can you please describe what kind of database management tasks you use Python for? 
Huh? Azure is a cloud computing platform and Python is a programming language. Why are you comparing them? You can use both. https://azure.microsoft.com/en-us/develop/python/
Yep you're right, i misread that.
Sorry, I think I'm missing something here. Sqlite is in the public domain, and scrybe is under an MIT license. What's proprietary here?
I'm still learning out of frustration with lack of documentation support in R for any database related tasks beyond SELECT and basic data.frame table saves. I need to pull together a database of a wide variety of test data that I have accumulated over the years and just exploring what is out there. What seems most appealing to me in the Python language are Object-relational mappers (ORMs): https://www.fullstackpython.com/object-relational-mappers-orms.html
tidyr and ggplot2 are killer libraries in R. pandas and plotnine's ggplot are close substitutes in python, but I do find the original packages more elegant. **Edit:** I meant tidyverse, not tidyr.
I find R really, really intuitive if you want to customize your graphs. And the "data.frame" object with proper use of factors is an extremely convenient way to manipulate your data. I'm sure there are Python libraries that could offer the same convenience but in R it's all well documented and quick to get started with.
Do you think that with int32/fp32/fp64 data type, AVX512 on few cores can make the difference if compared to AVX2 on moar Threadripper cores? I can't find any benchmark about that.
I am no fan of R, I am downvoting because you are an annoying troll.
I'm building scripts in python to create self-service tools for users of a phone system. The system exposes some administrative functions via a web api. My process started with reading documentation on everything from What is a web api to python libraries, using postman to figure out the right things to send, then understanding what to do with the responses. It gave me experience with requests, json, re, getpass and a few xml libraries.
Numpy also offers vectorised operation. do you have any sources on that superior performance ?
X-Post referenced from [/r/django](http://np.reddit.com/r/django) by /u/half0wl [Server-rendered charts in Django](http://np.reddit.com/r/django/comments/6xuo7f/serverrendered_charts_in_django/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
My point was that you didn't have to point out that it is "free", as "free" is the default when you post here. You'd have to come up with an excuse/reason why you would point to something proprietary/paid when posting on the python reddit (as least that's my experience).
The more appropriate comparison to R data.table is pandas due to the type of operations supported, heterogenous data types, etc. Benchmarks [here](https://github.com/szilard/benchm-databases) and [here](https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping). Data.table is roughly 3-10x faster for merge operations (depending on whether keys are used) and 50% to 500% faster for aggregation methods.
haha [link for the original post](http://www.pybloggers.com/2017/09/psutil-5-3-0-with-full-unicode-support-is-out/)
Wow, I have never been to data mining Pokemon.Really It helps me a lot 
Heads up - in a few of the dispatch functions, you don't actually invoke engine.runAndWait, you just access it
Forking creates new processes, not new threads. You're not using multiple threads in that scenario, in the sense that each interpreter is its own process and there is no shared state, and hence no need for locking; everything is inherently thread safe when nothing is shared. 
Have you tried the odbc and DBI packages on R? 
R is easier for absolute beginners in the same way Visual Basic is easier than C# to pick up. And in the same way, it has weird little nuances and teaches bad habits and lacks consistency across packages because "reasons".
Thank fuck
Statistics and graphics. R is still the king of these. You programmers don't seem to get that scientists tend to value a tool that lets you get complex jobs done with the smallest handful of code. Nothing in existence can beat R's ggplot2 for example. R is a scalpel. It's unbeatable at a small number of tasks. Python (and Perl) are Swiss Army Knives. They let you get anything done. But some things *shouldn't* be done with them. Just ask your local surgeon.
Both can do headless. Pyppeteer does not require webdriver, just requires python and chrome (chrome is automatically downloaded at first run). I used pyppeteer to test my project. Then I felt that puppeteer/pyppeteer's `Page.evaluate` and `ElementHandle.evaluate` methods, which can execute JavaScript on browser and get return value, are very flexible and powerful. [Puppeteer's FAQ](https://github.com/GoogleChrome/puppeteer#q-what-is-the-difference-between-puppeteer-selenium--webdriver-and-phantomjs) says: &gt; Puppeteer works only with Chrome. However, many teams only run unit tests with a single browser (e.g. PhantomJS). In non-testing use cases, Puppeteer provides a powerful but simple API because it's only targeting one browser that enables you to rapidly develop automation scripts.
Channeling the spirit of Larry Wall!
/r/learnpython
Learn R. 
Ahhh, got you. Fair enough 
The downside is, you need to use a programming language anyway to access Azure's services. You won't even be able to do a hello world with azure. With python, you can do that.
The slides link is still not working. Which is a shame, as the most interesting parts are only covered briefly. Maybe this will be corrected after the weekend... If not, the most interesting non-ansyncio idea was that you should not use locks in threads, but a queue based approach. You can briefly see the keywords by pausing the video, then a google search wil give you enough ideas. It got me to re-read the chapter on concurrency in O'Reilly's Python Cookbook, which covers this idea well.
R is a domain specific language, Python is a general purpose language. If all you need to do are domain (in this case, statistics) things, the domain specific language is expected to be somewhat intuitive to domain experts and perform better. If you need to do many different things, use the general purpose language. This does beg the question of whether Python can call R.
https://www.instagram.com/p/BYHIaSqBlw-/
It's like a burrito.
Looks like the ResNet one is completely lifted from the PyTorch examples.
def poop(): print('lmfao') I can't believe u named a method poop XD
&gt; Walk into a Dev shop and celebrate your experience in 'R' on your resume Here's your problem. R isn't for 'dev shops.' There are other jobs in the world apart from software engineering. And there are other goals apart from building software. There are literally thousands of R packages that can run pretty complex statistical analysis as one-liners that would take a lot of effort to re-implement in Python (I know from experience; I've done this before). But for that to matter, you have to actually understand the math. Most analysts and data scientists in dev shops don't. So having access to those tools don't really do them much good. I use Python and I prefer it to R, but you're being wildly asinine. 
I'd raise **re** above **argparse** personally, but I've built an entire career out of **re**.
We haven't tested this formally, but I'd be curios to know. 
Fair point. What is your career in **re**? Sounds cool.
This is probably only because R does not have a good deep learning library yet, but some are being developed. 
Have you tried: sudo pip install faucet
yea, that's common nomenclature for any temporary or bad function of mine! X|
I rank `psutil` up there with `requests`, `toolz` and `gevent` for modules that are invaluable to any of my serious projects. Keep up the great work!
I've gone back and forth on this a bit and decided more cores is probably more suitable for my current workloads. My next workstation will be Threadripper. 
This is a neat project!
Check out the `pandas` data manipulation library. There's a `read_excel` function that you can use to easily transfer your data into Python.
&gt; I can really rustle the jimmies of the R community Sounds like you're the one with their jimmies in a rustle, my man.
&gt; don't know anything about fundamentals You seem to have identified a part you are lacking, and are still studying. Great :) &gt; highlighted my lack of programming (and math!) background or experience Reflect on this experience and choose additional education accordingly. You now have some hands on experience what makes dull math and algorithm classes interesting and relevant. I'm certain you can find a couple of classes (or online courses if you prefer speed). 
I'm not sure if there's anything R's dataframes can do that [pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/comparison_with_r.html) can't. I'm not familiar enough to say if pandas is more powerful for data manipulation, but if I had to bet on one I'd put my money on pandas
These are some really nice references. I'm definitely going to check these out. Thanks
Could you elaborate it please?
I dunno, seaborn is pretty damn easy to use. 
I work in VFX (The Hobbit, Superman V Batman, Thor, etc), and a stupidly large amount of that work ultimately comes down to being able to parse file formats that are sort of black-box DSLs (Domain-Specific Languages) for which no publicly known syntax exists. I'm the guy who swoops in, Tarzan-like, and says "everyone stand back, I know Regular Expressions" ... but seriously, truly grok the idea and it's amazing how many hard problems come down to pattern matching in text.
Sorry, yeah. I guess I meant people with little programming experience.
A library is computer code, that is written for the purposes of helping to write more code. It isn't a complete application that you ship in the appstore. A python module is a unit of computer code that has been packed into a ~~module~~ self contained unit, which allows it to be easily incorporated into other programs. A library may contain modules. A class is a template to make objects. An object represents a complete entity of some kind, containing information on its current state/condition, and computer code that represents its abilities. For example a Dog class creates dog objects that contain the dog's name, allowing it to exist in the computer memory, and also the code to make it bark. The bestvway for him to get an understanding of this is to learn to code. There are links in the sidebar of this subreddit.
If only it were that simple :-)
data.table is in fact amazing and at least as good as pandas
R is objectively superior in the amount of statistical packages available and it really isn't close. And though not objective, there are lots of people, myself included, for which the data visualization and data manipulation is greatly superior in R. I think tools like dplyr are much better than Pandas (which I think is clunky and frankly a little overrated in the data science community). And the visualization options available in R are unmatched in Python. Python historically has been better for machine learning support, though that really isn't the case anymore as some of the major packages used in Python like keras have R implementations as well, and both R and Python have APIs for big data tools like Spark. Frankly, the only area of data science which I consider Python superior for is integrating analytics into apps or other programs (which is a huge advantage). Python as a general programming language will never be beaten by R in this area, and it's a big part of a lot of data science products today. But if the project you are working on is simply some kind of analysis that doesn't need to be integrated as part of a larger product, R is unquestionably superior to Python, imo.
It's not that one can do what the other can't. At that doing it in R with packages like dplyr is much easier and more intuitive than with Pandas. Pandas feels incredibly hackish once you get some experience with R's options.
So that we can help you please tell us what Python for Rhino is, what OS and Python version you are using, what you have attempted so far and what error messages you have got.
All of Wickham's libraries in R are really solid. In addition to the two you mentioned, dplyr is really amazing, sort of adding SQL querying ability to R. 
It has keras/tensorflow implementations, which is probably what most people use on Python anyways.
How ironic that this is based on an online poll.
Thanks for sharing. What is, if at all, the distinction between this and other popular color libs? 
As a programmer with a statistics background, it still makes sense.
(tidyr is a superset of dplyr) **Edit:** I meant tidyverse, not tidyr.
I use Seaborn and Python 9 times out of 10 because they're more familiar to me, but I also consider ggplot2 superior to anything in Python, largely because it's based on The Grammar of Graphics (hence the name).
Beyond `re` I'd say "learn how to write parsers", it's hardly hard and it makes a world of difference. You'll still use regex for tokenising, but hackiness factor just goes way down when you can whip out a proper parser in 5mn, it feels like a god damn superpower.
That is the best asyncio talk I have seen to date. And I have seen many (including Beazley's)
So Rhino is a 3d modeling program. You can use python in it to influence the 3d model space, or you can set up parametric equations that feed into python. It uses Python 2.7 on Windows.
Which is okay. I just needed it for my understanding and it just has the basic parts. And I've put their link in the references below
Reminds me of [this xkcd comic](https://xkcd.com/208/). \^\^ But asking as a "beginner", are regular expressions really used THAT much?
Try For and while loops and if else statements
In my opinion ggplot2 makes the prettiest graphics overall, but I think seaborn has the best looking 1-line plots. Although once you want to modify the native plots it can become a headache.
Interesting article! While we're on the topic, how important would you rate focusing heavily on optimizing your code as opposed to writing code that works? I also find it interesting to see you tell people who are learning Python to avoid the packages that are most often recommended to beginners \^\^
They are used in countless applications. They aren't Python-specific. I highly recommend learning them in general, regardless of your interest in Python.
Nothing overly ground breaking or exciting. I had wrote a lot of ruby and Java for years. Now it's go and Python. I've been trying to get more practice with Python so I'm working on a log parser (specifically this is for learning) and forwarder for metrics to graphite. More specifically for video game combat logs. Also worked a bunch on creating a command line utility for doing some functional testing at work. Basically an execution framework for shell commands, or http requests with the ability to push metrics through our monitoring pipeline. Biggest hurdle for me going from go to Python for this stuff is data structures. Json serialization is more magical with Python and probably easier. I just miss structs for objects.
I think it is best to have a few projects going at a time. Optimizations and re-writes will be most significant if you do them monthly as opposed to daily. You'll find yourself transferring knowledge from new projects into old ones. To your second point: It is funny how that works. Sometimes teachers are more concerned with showing how cool Python is than creating a good foundation for learning.
So you're saying you need to get going with projects, and not necessarily focus on optimizing, but rather have the knowledge how to re-write something come with time?
For sure. Best of luck in your learning!
I use R daily. I use databases daily. Why would you want to use on ORM when you have SQL available?
It's pretty hard to summarize. The post could have easily been 5 times longer with gross detail in each section. I think the highlights were how discovering patterns was about the only way to figure out the file system, encryption, and compression. That and how a lot of the design persisted through the generations of games. It's also extremely impressive that the early games were flat assembly code. I didn't know what else to expect, but knowing that fact now is boggling to even consider. We are very lucky to have all the hardware and tools we do now to make our jobs and hobbies much more approachable.
Learn both. They are different enough that they can expand your programming worldview.
Yes, feel free to install SQLAlchemy to improve the startup time of your script. And make sure `lightweight_named_tuple` works exactly as you expected.
For `re`, I recommand learn string methods first. regex can often be avoided and lack of readibility. But I agree that it should be learned at some point. And used when needed. If you do not have complex case and do not need high performance...
What's package specific syntax?
A great example is NumPy. Where a pure python array is a list of lists and would be accessed as: my_list[i][j] a numpy array works like a N-dimensional matrix and is accessed as: my_nparray[i, j] Coming from R, this confused me initially.
Well intuited, keep guard from the parasites out there, many who are in this thread, who would like nothing less than to stuff an opponent's head full of fart button machinery, and then laugh at them, to keep the competition down.
Sure, because automating the boring things in R would still end up being boring... ;) 
Any examples of good tutorials or books to get my head around this stuff? I'm interested in Data analysis and working with obscure file formats, so this interests me.
One of my first problems I fixed with Python at work used regex, and that saved me days of work, (cutting down 40k rows from a database to 400 using a range of regex passes, as had to fix them on 8-9 different criteria). Took me around 4 hours to write, at a pretty beginner level, and it was ugly, but in ran in 3 seconds, output a perfect file that could be then uploaded into the new DB. Felt awesome.
Thank you for this article. I'm just starting with Python and I'm focusing on learning Python, not libraries. I have some programming experience so unfortunately I've had to slog through some of the tutorials (as they explain that lists are 0-indexed, for example). I appreciate the project suggestions at the end. I do have a question about regular expressions: are they suggested because they're faster than the string methods? If so, is the difference so great that they should be used more often than the string methods?
Pydal. 
I learned both in parallel coming from MATLAB. I work in Python mostly for my day job but legacy code is in R. I generally find that R has more comprehensive statistical packages and is more useful for tackling most data science problems while python has more comprehensive programming packages and integrates into a production environment better (which is why my work uses it now). Python is coming out ahead in machine learning libraries, as they are basically programs that build statistical models so a lot of software engineers are active there but boy is it frustrating how often a useful loss function/distribution is missing in Python but present in R. There are ways to get Python to call R functions I want to explore. I think ideally you should do programming training with Python, stats training in R and work is Python calling R where necessary to fill in the gaps. Once enough statisticians make the jump the libraries for Python will explode and there will be as good or even better coverage of distributions and methods in Python but we aren't there yet and R could make a comeback too.
I used Tkinter and PyGObject. Tkinter can be tweaked to look fine on Windows and Mac. Just use the ttk widgets. PyGObject doesn't seem to work on Windows?
PyQT5 license isn't really restrictive for commercial development - you just have a buy a developer license. Then you can do all the commercial dev you want. :-)
That's just normal python syntax: indexing an object with a tuple. It's not numpy specific syntax.
You can go a long time without ever needing them, or depending on what you do you might never need re. But once you do -- there is no substitute. I'm working on a side project now where I want to scrape some data from the web, and I needed to use re. I wouldn't contrive some fake reason to learn it though, personally. Just wait until you need it.
It depends on what you are looking to do. In general, if you are going to be concentrating heavily on statistics, and **particularly** if you have somewhat more esoteric statistical procedures to do, then R is the clear choice. If you have more general programming to do, a subset of which is non-esoteric (or sufficiently esoteric so as to cause you to have to program the estimators yourself), then Python is better. R is great for statistics and frankly not very good for anything else.
Gotcha. But won't the tasks need to share the connection pool so I could achieve to limit the number of connections to the server?
The [BeeWare Project](https://pybee.org/contributing/) and Russell Keith Magee have been outspoken about being willing to help guide any contributor. They have several interesting subprojects that you may want to look into. They always try to leave some minor issues open for the newcomers to solve/implement. You should check them out.
As a stats grad without a programming background, R is the only thing that makes sense
So let's say you make your compiled file on Windows called "banana.dll". How do you then import it into your Python file? I see you have the line: from logic import main But is logic actually a logic.so or logic.dll? I'm confused.
Don't you need a separate license for qt?
"Working arrow keys ffs" 😆
Sweet
Thanks
1. Yes. It was for a contracted job. 2. Yes. Dropbox does (did?) use wxPython for the desktop client for their service. 3. I found wxPython's cross-platform nature to be not quite as thorough as I had hoped, having tested only on Window and Linux, in that occasionally one does have to put in special code that checks the platform and makes a small adjustment. I also found it challenging at times generally, but I was also learning as I went and wanted it to do a number of somewhat custom things, so this probably wasn't its fault. (Overall, though, pleased, and wonderful community).
Vim, more specifically Neovim. Nvim-R, ipython REPL in split terminal and some solid Django plugin. 
Your website is kind of painful to read with that light font colour on a white background. Please consider changing the font colour to black. Contrast is good. 
Most of it is LGPL: http://doc.qt.io/qt-5/licensing.html
Numpy, Pandas, Matplotlib, plotly api, bokeh, sklearn, statsmodels have all been around for quite some time... Really as a lowly data analyst I can't think of any reason to use R over python, except that shiny dashboards are legit.
I love Pycharm, and recommend it. If your main code is Python and you're just using R because it's great, I'd recommend looking at [SageMath](https://en.wikipedia.org/wiki/SageMath). It allows you to use R right in your Python code.
**SageMath** SageMath (previously Sage or SAGE, "System for Algebra and Geometry Experimentation") is mathematical software with features covering many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. The first version of SageMath was released on 24 February 2005 as free and open source software under the terms of the GNU General Public License version 2, with the initial goals of creating an "open source alternative to Magma, Maple, Mathematica, and MATLAB". The originator and leader of the SageMath project, William Stein, is a mathematician at the University of Washington. SageMath "uses a Python-like syntax," supporting procedural, functional and object-oriented constructs. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Python/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
wxPython does look like a really good option and is the one I'm most excited to use. My conclusion so far is: - wxPython - for applications that you want to look native - PySide / PyQt - for applications where you really want to customize the UI. What I'm most concerned with now is how reliable a tool like PyInstaller or cx_freeze will be. Is there anything important I should know about using such tools?
Why are you making a game with Python? Not being rude just curious
Agree. Pythonista is the best app I've ever purchased :)
In addition to pandas, you can transfer your data into a sqlite db
That's for the rec. I'll tell you what I love R for and it's the summary ~ feature for large data frames. That's literally all I use R for. 
Yes it's painful to read :)
For analysis, I've really found ORMs to be more encumbering than helpful. ORMs have a tendency to write poorly optimized SQL on top of which, it can be much more complex to model a set of complex relationships in an ORM vs SQL. So you always end of dropping from your ORM to basic SQL to write analytic queries. So why use it all. The only benefits I've seen is that writing back to a database can simpler if that is your use case, since an ORM like SQL alchemy can generate table definitions and complicated upsert logic. But in general I'd only use an ORM if I was building an application or service as object oriented db access can be much simpler with an ORM. 
This is an interesting article. I'm a newbie and currently working my ass off on one of the packages in the avoiding list. Maybe, for once, vanilla is better than something else. I know practice makes perfect and that is the direction I'm planning to go. But do you have any tips for a newbie in learning this language?
Could you edit the README and have it show off some information? It's hard to take away too much from this, but I'm still curious.
Agree with all the points. Minor nitpick - I think you meant to say "core tenets" at the end. Not tenants. 
I know this is obvious, but it is still a good reference: https://docs.python.org/3/tutorial/classes.html 
Brian Lunduke had a guy from Pogo Linux on and they talked about EPYC compared to Intel's new Xeon SP lineup. The TL;DR of it is intel beat AMD in everything integer based. AMD won all of the floating point tests by a reasonable margin. The integer tests were varying in results as to the Intel lead. [Full interview here](https://youtu.be/SdWWAvspoy4) There are some statements that are incorrect (like Ryzen not working for Linux distros) and people pointing out the use of an inaccurate Anandtech benchmark. But overall you can get a sense of the offerings. If anything you can compare the EPYC's to currently available Broadwell/Haswell Xeon's.
&gt;~~Channeling~~ Exorcising the spirit of Larry Wall! 
I'm a QA guy at an R shop. I have automated some boring things in R. I would not recommend others follow suit. The way I see it, the proper way to QA code written in R is to port it to Python.
You can get creative within wxPython and customize the UI a fair bit, probably far beyond it's intended use if you really want to. One concern that did occur to me just now about wxPython: the support for Rich Text is not where it might be. It's there and usable, but it is based on a widget that never got finished, and has some drawbacks. For many applications, this wouldn't matter. &gt; What I'm most concerned with now is how reliable a tool like PyInstaller or cx_freeze will be. Is there anything important I should know about using such tools? I used py2exe (this was some years back) and it worked fine for my purposes. The resulting .exe file will be probably at least 5-10MB. Other than that, not sure what to tell you. I found GUI2Exe to be a helpful GUI-based tool for py2exe, though I don't think it's been updated for Python 3.x.
Dropbox has dropped the links: https://twitter.com/raymondh/status/903140753685098496
Interesting, thanks!
In my experience, 99% of people who say they hate the GIL have no clue what they're talking about. Its just one of those things, like hating on significant indentation. Are you one of the 1% who actually do have a reasonable reason for disliking the GIL, and if so, why aren't you using the GIL-less Jython or IronPython instead?
Definitely one of my top 10 talks. Live coding and a jokester.
You can do whatever you want with Qt (including selling software with it). The only catch is that if you modify the actual Qt framework files (99% of people won't need to do this anyway) you have to share your changes with the framework to the community. 
$500 for a 1-developer license I believe
Take my money!
since you already installed pymysql, see http://pymysql.readthedocs.io/en/latest/user/examples.html
Thanks for this! very interesting video!
can you start it straight from conda? Like use conda as mysql command prompt
Aaaaand fuck SPSS. 
It's a good start, keep at it! Read this effbot article [Hello, Again](http://effbot.org/tkinterbook/tkinter-hello-again.htm) and give a thought towards putting everything in a class. It really helps when your app starts growing. 
What wrong with learning tkinter? I'm learning Python after coming from php and lua.
Something that helped me was to find how-tos on the task I wanted then copy everything character by character. If you copy-paste the entire code then you may get the gist of what the program is supposed to do but you may not learn how to recreate it.
Of course it isn't numpy specific (I don't think OP meant to imply it's *only* used by numpy), but it's not syntax I've ever seen used by the standard python libraries. You can certainly create your own classes using this syntax, but again, it's not very typical python and is confusing for a beginner.
The article explained it pretty well. The problem isn't with learning tkinter. It's with learning tkinter when first starting out with python. The proper way to learn python is to learn the fundamentals of the language without being holed up with complex modules that themselves take as much time to learn as the language itself. 