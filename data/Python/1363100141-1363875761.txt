&gt;as it would be hard for him to establish any kind of authority required to actually lead anything. ?? sr. just means you get paid more than a jr. there's no implied authority or leadership requirements in that title. it's just a silly title
companies lie all the time, it's just business. 
Actually it does, admins and mods can choose when to use red/green and when not to. By default the colors are off, and you have to click the "distinguish" button to get the color.
"messaging queueing without broker" is not exactly a good description of zeromq. It's likely those who think so will be disappointed
Whoa, man, feels like just yesterday
He's not an admin, he's a former admin (quit two years ago) and his optional badge reflects so.
Well spotted. q.gi_frame means I don't need globals() to get access to the globals/builtins, in order to have persistent storage. I don't see any obvious sandbox breakers though.
Right, that's why it says emeritus when you hover over it. But I assume the same principle applies.
What makes those phones closed ecosystems? How can they be closed if everything on them is open sourced?
If you don't want any app functionality, what do you want python for? Sounds like you want HTML. Combine it with bootstrap, and you'll barely have to do any work at all. 
ah, interesting. i don’t exactly get how such a “trivial” getter would look like. i can do type('X', (object,), {'some': 'attr'}) and now? i can do the same with class X: some = 'attr' as soon as it’s exec and not eval. can i use type to define functions via bytecode strings or sth.?
Will there be support in the future for contacts, GPS, accelerometer for IOS and Android? Or is Kivy GUI-only and those things are outside scope? If they are outside scope, it would be nice if phonegap/apache cordova could somehow be used within Kivy. I know it's javascript in a webview so integration would not be easy. If Kivy has a webview, maybe run phonegap within it and "scrape" the results somehow. 
check out [Nikola](http://nikola.ralsina.com.ar/) too
Why would you assume a former admin to speak in the name of the current staff?
Float is fine, actually, because you're forced to make a decision. And as upofadown points out, we can still access good old fasioned integer division. One of these days I'll make the switch to python 3. One of these days...
my favourite intro by FAR was **"Rapid GUI Programming with Python and Qt"** by Mark Summerfield, with a ZetCode appetizer: http://zetcode.com/tutorials/pyqt4/. Windows users can also skip a lot of pain by straight-up installing Python(x,y) and then, after they know a thing or two, go for slimmer installations upon a reformat. I tried "Python the Hard Way" previously and found it a little crappy. The final project is impractical, there's potty humor sprinkled throughout, I couldn't care less about the little game at the end... idk it wasn't my cup of tea. The PyQt book really makes you hit the ground running- instead of some drab commandline "hello world" you immediately get applications that look native and DO stuff. Within a week you can be showing off little useful applications to acquaintances, it does wonders for self-motivation. I also find GUIs a fantastic way to jump into OOP. While scientific applications and webcrawlers are often best served with plenty of functions, nothing screams "inheritance!" like a custom QPushButton, or "composition!" like a QMainWidget.
Actually ghci&gt; (-5) `div` 2 -3 I think that rounding toward negative infinity is better, it seems more complicated at first (more complicated than "divide absolute values, calculate the sign as multiplication does"), but when you actually need to do integer division on negative values discontinuity at zero _always_ bites you in the ass. What sort of sucks is that then int(-5.0 / 2) != -5.0 // 2. Also, `fmod` uses C rules too.
No idea, but that's what the colors are "suppose" to be used for. If you go through jed's comment history, you'll see that he has commented without the admin colors before, it looks like he has the option to comment either way. What I'm assuming is that his colors work the same way as moderator colors: when you post in a reddit that you're a mod in, your comments are just normal, except you have a button next to "edit" that says "distinguish". If you press this button _only then_ do you get mod colors (green). I'm assuming because I don't have access to his account, but I'd wager that this is the way it works. You're only suppose to use these colors when speaking officially: on behalf of a subreddit, etc. 
StrangeCase, hands down!
I finally figured out the reason for this after reading [this](http://python-history.blogspot.com/2010/06/from-list-comprehensions-to-generator.html) blog that Guido van Rossum wrote. In python 2.x there are issues with variables in list comprehension. For example: &gt;&gt;&gt; x = -1 &gt;&gt;&gt; [x for x in range(3)] [0, 1, 2] &gt;&gt;&gt; x 2 By converting constructing a `list` from a generator expression you avoid this issue. &gt;&gt;&gt; x = -1 &gt;&gt;&gt; list(x for x in range(3)) [0, 1, 2] &gt;&gt;&gt; x -1 Zed doesn't use the variable `x` in that scope, but it seems he does this as a general defensive mechanism to avoid this type of issue. 
Try out CherryMusic instead. It already has the same features as shiva plus half of the wishlist of shiva. Additionally it's super simple to setup in comparison to shiva. http://www.github.com/devsnd/cherrymusic
Depends on the size. For example: Automating the creation of the menu structure is almost never overkill; Nothing is more tedious than doing this by hand.
Of course now the version number says 0.0.2 and the release notes have the much more moderate "you are 80% likely to hit a bug or rough edge if you use this right now", so you should of course feel free to go wild :-)
I used Python to fabricate the entire set of data I used for GCSE statistics. I submitted the code as part of the coursework as well. A in GCSE statistics, yeah!
Really? Could you provide an example? A simple homepage should not need a complicated menu, and I really can't see how automating it in Python would save you any time. I think HTML/CSS/JS should work fine here.
Interesting. I like removing senseless drivel from documentation and I like the possibility of static analysis tools catching more stupid mistakes. But it just doesn't look that well supported at the moment. Consider the following modified function from your example. &gt;&gt; def greet(name: str or int, age: int) -&gt; str: .. print('Hello {0}, you are {1} years old'.format(name, age)) &gt;&gt; greet("Peter", 5) Hello peter, you are 5 years old &gt;&gt; greet(5, 5) Hello 5, you are 5 years old &gt;&gt;&gt; help(greet) Help on function greet in module tmp: greet(name: str, age: int) -&gt; str Notice how the signature only lists `str` as a potential type for `name`. Despite `greet` supporting both `str` and `int`. This is outright wrong. I'd rather have users read a few extra lines of correct documentation than get incorrect information while using the builtin introspection function. 
The expression after the colon is evaluated as a standard Python expression when the function is defined. `str or int` evaluates to `str` because the `or` operator is short-circuiting. If you wanted something richer than a single type, you could use a string, i.e. `name: 'str or int'`. Your tools would then have to be able to parse a string and recognize the `or` operator and so on. 
Ahhh.... thanks for pointing out my brainfart.
Not sure I'd say it's doing it right. Or at least, it's irritatingly more complicated than you are suggesting. Namely, these two statements are not the same: ( -(5/2) ) != ( (-5)/2 ) Relying on implied parenthesis is a bitch. According to [kalgynere](http://www.reddit.com/r/Python/comments/1a4bwr/til_python_does_integer_division_right_unlike_any/c8tz7xh)'s link, the other way works correctly: 5 / (-2) but the implied parenthesis are far more obvious then.
What the shit, I never knew about this
I thought I'd help you confused the poor lad/lass a little more with operator and itertools! import itertools import operator l = ['eD', 'fC', 'hC', 'iC', 'jD', 'bD', 'fH', 'mS', 'aS', 'mD'] l.sort(key=operator.itemgetter(1)) groups = sorted((sorted(ocurrances) for letter, ocurrances in itertools.groupby(l, key=operator.itemgetter(1))), key=lambda x: len(x)) print list(itertools.chain.from_iterable(groups))
They're closed because they have a set store for apps, as well as they're laid out in a way so that you can't run a recording program while running netflix etc. If you wish to unlock them so that you can make actual administrative changes to the OS, you generaly have to root the devices.
I've had some good luck with [Flask](http://flask.pocoo.org/) and [Frozen Flask](http://pythonhosted.org/Frozen-Flask/).
That doesn't make any sense. They do not have a "set store" for apps. They have an optional store, and you never have to use it if you don't want to. You can install apps using adb or simply by tapping on them in a file manager. In order to make administrative changes to the OS on Linux, you need root access too. It's no different. 
I don't mind taking a look at it, is the source code available anywhere?
&gt; we ought to find better way to sponsor software that benefit everyone than "offering consulting", which is sadly the best you can do right now. I'm not sure about that. Of course, this is only for "rock star" developers, but there are several people out there that make good money for pursuing their interests and talent in F/OSS development, and giving away most or all of their work product.
holy shit, I just wrote a class today and felt really dirty about doing: `if type(name) == str` &gt; Python-Version: 3.0 damnit!
Wow. I'm already pretty perturbed by Haskell's penchant for using too many operators, and now I learn that `-` doesn't even bind tightly. Thanks for pointing this out!
Semantic nitpick: shouldn't that be "underused" rather than "unused"? I've used it a few times. Granted, not in "real code", only in one-shot experiments, but still.
After writing my comment, I played around with function annotations a little and wrote [this little script to demonstrate how a function-based annotation might work](https://gist.github.com/bovee/5147580). I think this is more Pythonic than `typelanguage` because it's not defining its own string-based language for type-checking. As an example: @validate_args def test(num: isintrange(2,5)): print(num) test(3) &gt;&gt;&gt; 3 test(6) &gt;&gt;&gt; AssertionError: Argument 6 does not match signature isintrange Function annotations are really cool. I wish I didn't still have to support Python 2 with my code or I'd plug this in for doing some unit tests. EDIT: And I see ceronman's library [typeannotations](https://github.com/ceronman/typeannotations) does exactly this. Nice.
use PyContracts
 if isinstance(name, str): is typically a bit cleaner. And that way, if you're passed a subclass of `str`, your code will still work.
&gt; now if only this was supported in 2.x :( Apparently not... /s
Annotations are also powerful for even closer integration with Cython... To what extent would vanilla python code bridge cython with type annotations?
I'm still angry about a few things I find completely abritrary. Why can I do: a = 1 print a but not print "answer: " + a or f.write(a) I have been using python for a few years now, but those language choices irritate me. (I come from a background of c, c++, java, and perl). I understand the reasoning, but I don't agree with it. Also, I wasted a couple of hours debugging a program due to the silly list copy notation. The problem was basically: &gt;&gt;&gt; a = [1,2,3,4] &gt;&gt;&gt; b = a &gt;&gt;&gt; b[0] = 5 &gt;&gt;&gt; b [5, 2, 3, 4] &gt;&gt;&gt; a [5, 2, 3, 4] In case others aren't familiar with this, when copying lists, you should use list() or the slice notation. Otherwise, you will just create a reference to the list. This seemed arbitrary to me since the same is not true of strings/ints.
Variables aren't things themselves in Python; they're just names for values. The assignment behavior is exactly the same for strings and ints; those are just immutable so you don't really notice it. You can also do an explicit call to copy() to be more explicit if you like.
I was a little hasty to say I'm out of things to complain about (lambdas are still kind of crippled, for one). I'm pretty surprised that f.write(a) doesn't work; I guess I've never written anything except strings to files. As for printing integers and strings, what's wrong with using print "answer: ", a I think your point about the list copying is debatable. If a and b were objects, and you did &gt;&gt;&gt;a = Obj() &gt;&gt;&gt;a.c = 3 &gt;&gt;&gt;b = a &gt;&gt;&gt;b.c = 1 you'd expect &gt;&gt;&gt;a.c 3 Admittedly, lists are types, not classes, so I guess it's reasonable to expect them to act like ints and strings instead of objects.
I even wrote a tutorial about how to create a site that is not a blog :-) http://nikola.ralsina.com.ar/creating-a-site.html
Note in 2.x you should use isinstance(name, basestring) to check for either string or unicode. http://stackoverflow.com/questions/1979004/what-is-the-difference-between-isinstanceaaa-basestring-and-isinstanceaaa
`six.string_types` * http://pythonhosted.org/six/#constants * http://docs.python.org/3/howto/pyporting.html# * http://www.canonical.org/~kragen/isinstance/;
He's saying that you're still not officially out of things to complain about (since you would want it to support python2) 
http://www.reddit.com/r/Python/comments/18w8si/z/c8jby1t
I actually started using this in a few of my projects. The only thing that remains confusing is how to handle when the return could be one of several different types.
A lot of things on Android phones are **not** open source. Manufacturer customizations to the OS are extensive and they're all closed source.
Believe it or not, using numeric rather than symbolic names for indices wins 5k lookups/sec on CPython. The other issue is memory allocation: by reusing the pointer list for the remainder of the node structure, around 72 bytes per key are saved (for a total of around 108 bytes per key on CPython).
While I agree on principle that it's not a straight right/wrong answer to this, when doing *integer* division, I've always wanted rounding towards negative infinity.
Integer or float, order of operations is important
Dude learnt the mapping protocol in 2003, presently his dudeness is less concerned by minutiae than efficient code 8)
This is out of this world kinda cool. Seriously, the flexibility of Python just amazes me to no end, looking at [shfo23's comment,](http://www.reddit.com/r/Python/comments/1a5tc6/a_powerful_unused_feature_of_python_function/c8uhlg1) I can't believe I can now write: @validate_args def assassinate_penguins(user: isadmin()): pass Instead of: @is_user_admin def assassinate_penguins(user): pass *And the latter blew my mind the first time too!* It's especially neat because I'd find myself writing decorators like 60 lines long just to accommodate for the ways in which my functions might be receiving parameters, and suddenly here's this super Pythonic way of working with parameters. I also love the idea of going all duck typing on this sort of stuff, something like: @validate_args def only_conspirators(user: has_method('conspire')): pass My only complaint being that you can't have this sort of stuff with \*args and **kwargs, though (at least it doesn't look like it). I'd love to be able to say: @typechecked def lots_of_kwargs(*args: (Model), **kwargs: {'paint': str, 'amt': int}): pass
proper capitalization is either IPython or ipython - it's not a "Designed in Cupertino, made in China" project ;)
`__getitem__` and the like have been around since Python 1.0, in 1994 (possibly longer, I don't have immediate access to older sources). The API exposed to the user isn't "minutiae"; it's the single most important aspect of your library.
I don't think this was meant as a library -- it's not up on github, there's no bug tracker, it's not pip-installable, etc. This is a proof-of-concept implementation to show how to implement this particular data structure such that it runs fast. As such, I think it's useful to make the resulting code as tight and free of extraneous fluff as possible; getitem would, indeed, be useful if anyone were actually going to use this, but that kind of thing obscures what he's actually trying to demonstrate, which is data structure optimization strategies.
Actually, a generic implementation of this would not use the mapping protocol. The use of a 'key' is superfluous, the structure only relies on the relative ordering of single objects; a mapping is just one convenient realization of this structure. You can in fact consider the 'search()' function entirely superfluous, since it can be implemented through iteration. Additionally, the *main event* here is the iteration function. There is no standard protocol for an iteration function (the mapping protocol does not cover `iteritems`), and even if there was, it wouldn't cover the ordered, arbitrarily positioned iteration provided by this structure. Ergo, the mapping protocol is not the single most important aspect of this code. In a robust implementation, it wouldn't even fit the code. What it does fit, is small minds that want to treat the code like a black box, which is evil, broken and wrong, and I'm more than happy to start a thread to discuss that if you like.
&gt; If you use this code for something, please click back to the blog’s home page, then “Ask Me Anything”, then drop a comment telling me what you used it for. He clearly intends for it to be used.
&gt; What it does fit, is small minds that want to treat the code like a black box, which is evil, broken and wrong You're full of shit. You have three methods that perform *exactly* what `__getitem__`, `__setitem__`, and `__delitem__` are for, and you're trying to make excuses for the fact that you didn't know about them. It's *fine* not to know everything, but don't fucking pretend that you do and everyone who disagrees is literally evil.
`__getitem__` throws KeyError for missing keys. Given the performance-raping implications of throwing exceptions, search() here does not. `__delitem__` throws KeyError for missing keys. Given the performance-raping implications of throwing exceptions, delete() here does not. `__setitem__` makes no restrictions on the type of objects being set, whereas `insert()` here relies on the use of `None` as an internal sentinel. None of these functions match the mapping protocol, and one of them wouldn't exist at all in a robust implementation. As I said, I'm more than aware the mapping protocol existed. However, it provides *no facilities for manipulating ordered maps*, therefore its use here is ultimately superfluous. It might be useful as an adapter to integrate with existing code, but a `dict.update()` workalike would be much moreso, which again is not covered by the mapping protocol. Finally, blindly using this structure as a generic dictionary is asking for trouble. It *burns* memory, and unless you actually need exactly what it provides, it should not be treated as a generic container. Not unless you have a few gb of RAM to spare. Regardless of the implementation, what you should take away from the post is the structure itself, not some nasty code put together in an afternoon.
I really dislike the annotations feature because it could be trivially implemented in decorators without tacking on extra (ugly) syntax without sacrificing an ounce of readability to boot. Annotations seemed rushed and poorly thought out and contrary to the language design sensibilities that make python great (specifically not to add syntax without good reason, and the lack of use is clear evidence there was no good reason). Someone will say "you can't use decorators because your tool would need to execute the python code to evaluate the decorator" but that ignores that you already have to evaluate the annotation, and can be little more sure that `int` evaluates to what you think it does. In practice an established annotation decorator could be used by tooling 99.999% as reliably as this annotation syntax.
Ha, my apologies. It's funny how they've created such a strong brand that i before anything seems like it should be iAnything.
There's nothing arbitrary here. Strings and integers (and tuples for that matter) are not mutable objects. Practically everything else is. Variables also contain references to objects, but are not the objects themselves. Assigning something to a variable actually assigns a reference to that object to the variable. Java actually behaves the very same way. Also, you're seeing a consequences of Python being strongly typed with the likes of `f.write(a)`: file.write() expects a string, not an integer. If you were to pass an integer to it, how would you expect it to deal with that? Coerce it to a string? Write it out as a 32-bit word? Should that word be big-endian or little-endian? The language can't know, and because function/method overloading isn't supported (as a consequence of the language being dynamically typed) without `isinstance()` hackery, there's no way to overload file.write() like Java can. There's nothing arbitrary about the `+` operator either. It's a *string* concatenation operator when used with strings. The assumption here is that if you're concatenating something to a string, unless it's also a string, it's likely you're doing something you didn't intend to do, so given being explicit is generally better and implicit, you have to cast the thing to a string first.
Actually, lists are classes too, and have been since types and classes were unified way back. The difference is that primitive types, strings, and tuples are all immutable objects: anything that appears to mutate them actually returns a completely different object from either object that the operation was performed upon.
As I said in my original comment, I understand the reasoning. As a user of the language, however, I would prefer it work the way I want. The word 'arbitrary' was a bad choice. I also realize my complaints are frivolous. What I should have said is "these are things I wish worked the way I think they should." &gt; (as a consequence of the language being dynamically typed) without isinstance() hackery, there's no way to overload file.write() like Java can. It works perfectly fine with print(). I wouldn't consider type checks hackery. There was an obvious tradeoff between performance and convenience for the write function and it was decided that only accepting strings isn't unreasonable (use a string formatter or cast to strings). I can (and have in the past) just write my own wrapper around the write() function that deals with this. I didn't mean my comments to come off as an attack on the language, just a list of language decisions that annoy me.
&gt; Google Closure uses a similar docstring approach for type annotations in Javascript. Except in that case, the second and third drawbacks don't apply. The syntax for how to write your docs (`@param`, `@return`, etc.) is specified exactly, and is checked by the Closure Compiler so it won't get out of sync with your code.
As best I can tell, the original paper provides the key separately only because it's cheap to do so in the node structure. The algorithm itself works just as well on individual elements, and would be more naturally implemented this way, and mappings could easily still be built on top. The main problem with a C implementation is that I wanted this particular project to also work with PyPy, and 'crossing the divide' repeatedly in PyPy (such as to call a rich comparison function) is known to hurt even with `cffi`. It would still be tempting though, since it'd save tonnes of memory FWIW right now PyPy uses about 187mb of memory to represent 943k elements, which could definitely be improved upon
great tip! by the way, PyCharm reads docstrings to find Sphinx annotations
`print` is actually the anomaly here, not the rest of the language. In Python &lt;3.0, it was a language construct and the implicit cast to string of all its arguments made sense, and still does. But `file.write()` isn't the same: there are too many ways to interpret how something ought to be written out to whatever the file object represents. Personally, when I'm using `file.write()` to write a literal stringified integer, I just cast it with `str()`. The trade off here isn't performance or convenience, but parsimony and consistency. I didn't interpret what you wrote as an attack on the language, but was simply trying to correct some misconceptions about how objects and values are handled in the language, which is exactly the same way as Java does (especially since Java introduced autoboxing).
It seems wrong to me. Two does not go into 5, negative or not, three times, but two. The most logical result is truncation, not rounding. In good old Pascal, -5 Div 2 would perform integer division and return -2, while -5 Mod 2 would yield the remainder, -1. What's the point of rounding anything? I thought "explicit is better than implicit"; there shouldn't be rounding without specifying it. 
pycharm doesn't seem to support this noooo :( And this annotation seems to be just for functions?.. can't do this in variables.
Sorry, not persistent as in on-disk. Persistent as in [persistent data structure](http://en.wikipedia.org/wiki/Persistent_data_structure). (I wish we had better terminology for this.) For unordered mappings, Bagwell's [hash array mapped trie](http://en.wikipedia.org/wiki/Hash_array_mapped_trie) works well. I've started thinking about ordered mappings though.
Wow that looks like exactly what I'm looking for. I'll try it out.
I would just use name = str(name) That way name can be anything that supports str. 
Aah! Hmm, I'm not so sure about tricks for that.. You *could* implement chaining quite easily, and implement iteration using something like `heapq.merge()`, although that would be the wrong algorithm to use here. A batch of insertions/deletions would produce a new list with an additional parent attribute pointing to the previous list, deletions would be represented using some kind of marker, and iteration would take the union of (this list, parent), overriding and omitting as necessary. Doing this recursively might get you what you want. But I suspect there are probably much better ways to achieve this with more appropriate structures
From experience, using str in this manner causes more pain than the short term benefits.
Correct me if I'm wrong but the annotations act as documentation and don't really do anything else. You still need to decorate it for runtime checking. Decorators also add an extra function overhead whereas annotations are just syntactic sugar.
The idea of making Python functions strongly typed through decorators and annotations is *fundamentally flawed* and in the typeannotations library linked at the bottom, implemented with wanton disregard for performance. One of the central purposes of having a typed language is to allow for compile-time optimization and error-checking, but this code performs extremely expensive *runtime* type checking. Putting this into any production code on any function that gets called often can have catastrophic effects to performance. Take a look at this example that shows a **200x slowdown**: &gt;&gt;&gt; def sqr(x): return x*x &gt;&gt;&gt; @typechecked ... def sqr2(x:union(int, float)): return x*x &gt;&gt;&gt; from timeit import timeit &gt;&gt;&gt; timeit(lambda:sqr(5), number=100000) 0.02607336299843155 &gt;&gt;&gt; timeit(lambda:sqr2(5), number=100000) 4.091551588004222 In Python, function calls have considerable overhead and the decorator in this code uses a wrapper function, which means that it doubles the function call overhead. On top of that, the wrapper function calls two other functions, which each call additional functions, multiplying the problem, *even if the annotation is empty*. Now, all that being said, it is sometimes reasonable to check the types of your arguments, but if you want to do so, you should put it explicitly in the code, so you're not hiding the fact that you're performing a series of potentially costly checks. Annotations are meant to annotate code, not enforce rules. **tl;dr** - The code linked at the bottom of this article incurs massive overhead and should not be used in production
You should be aware that this sort of typechecking with annotations and decorators will have a [huge negative impact on performance](http://www.reddit.com/r/Python/comments/1a5tc6/a_powerful_unused_feature_of_python_function/c8uqky9).
Just because it's new and fancy doesn't mean you should use it. I think it's a lot better and certainly performs substantially better to write def only_conspirators(user): if not hasattr(user, 'conspire'): raise TypeError("User is not a conspirator") pass Rather than @validate_args def only_conspirators(user: has_method('conspire')): pass The second example is putting some of the core logic of only_conspirators() into an obfuscated combination of several functions, annotations, and decorators. In order to properly understand the code, you have to understand the validate_args decorator (does it raise an exception? If so, what kind? How does it handle keywords? What's the syntax for the annotations it accepts? etc.) and the has_method function (or constructor? Does it use hasattr? Does it check the type of the attribute? Does it use dir()? etc.). For anyone (including future you) reading your code, you've just made it considerably harder to understand and debug than if you had just explicitly written out the checks (not as exciting, but better practice).
&gt; [...] small minds [...] Calling people this way is... unpythonic.
Yup, shouldn't have said that :( Was feeling a little over-defensive at the nit-picking.
Well if I'm going to say something silly, the least I can do is be honest about it
I did much the same for my A Level Physics. My only mistake was writing the report in LaTeX, which apparently made the head of department very suspicious of me.
I know the initial announcement was posted in here only a few days ago, but I figured given that people seemed to actually like the library they might like to know that it was now actually useful. :-)
Sounds like you need a HTTP server. Setting one up with Twisted is easy as pie http://twistedmatrix.com
I have no clue what this could mean for me. I love the idea of AI, but I don't know if this lib has a particular use for me. Could you explain?
It's true. That's why I said that run-time checking is not really useful in real life. I added the typechecked decorator like a proof of concept. What is really interesting to me is static checking, not run time to improve tooling support. I want to add something like that in the future. On the other hand, there are interesting uses to run-time checking. Like using logic predicates. And yes, the code is not ready for production. It's still an experiment. EDIT: Also, something I had in mind when I thought about the typechecked decorator, is that it could take a flag with DEBUG | PRODUCTION options. Type checks wouldn't be used in production, but they might be useful for running unit tests or just debugging some parts. Or you can just enable them and remove them when speed is important, remember premature optimization is not a good idea.
Good to know about mypy. But I prefer annotations as suffix, not C style. Take for example Dart vs Typescript. Typescript is more elegant IMO.
I concur with brucifer, doing it during runtime is not an ideal way to do it (in all cases). If you want annotation, it can just as easily be done by modifying the function object (now there's an advantage of "everything is an object"), e.g. just something like this: @decorator def typechecked(f): # inspect annotations of f's parameters ... f._typecheck = dict() for param, annotation in ...: f._typecheck[param] = annotation return f →Only incurs overhead when the decorator is run, i.e. it's negligible. Then you could use a static tool that checks invocations of the callable and points out potential problems (e.g. type of a variable unknown) or outright errors (called with a wrong type). Instrumenting/wrapping only if debugging is enabled sounds a reasonable compromise though, if you want to cover the potential problem cases better.
Looks cool, but the indexer breaks. It's complaining about the image url being None, and trying to encode it. Edit: The problem wasn't in shiva but in eyed3. I put an "elif self.image_url" around the problematic line, and it trucks along with only a few warnings now. 
I'm also not sure why python would be easier than HTML/CSS/JavaScript. Could someone give an example of where using python would be better?
Using websockets is still the best thing to do if you want real-time app. What about socket.io? It has alternative transports to use if websockets are not supported (flash, htmlpush, etc) and the integration is pain free. I would also recommend to look at [gevent-socketio](https://gevent-socketio.readthedocs.org/en/latest/) and its [examples](https://github.com/abourget/gevent-socketio/tree/master/examples). 
I hadn't come across gevent-socketio ... I am definitely going to look into this, thanks for the heads up
I recently tried to write a real time chatting server app. After reviewing lots of projects on github I concluded that the python real time options (twisted, tornado) are far more complex than a comparable node app. As an experienced python programmer even I would have a hard time recommending a python solution over node for this type of app. Node is kind of ideally suited for these types of server apps
yes, i built a real time remote controlling app for android with tornado, websocket and MQTT. 
I agree with you there... to get python to do what node can do, in this instance alone, seems to require multiple layers of software and understanding each one in order to get things to work. I did, at one point, have a python powered MUD engine running on my home server. I still do... but I'm not going to lie and say I understand how everything works together beyond the actual python MUD app.
That seems to be the general feeling I am getting as I'm delving into more of the documentation and write ups for the technologies I outlined above. It seems like there are a lot of interdependencies and frameworks that are required to be able to mesh it all into a real time application and as much as I would love to use Python for the project, it's looking like I may have to look into alternatives like node with socket.io for legacy browser support
Strictly not always necessary with good IDE introspection like that found in KDevelop's Python plugin. However, that too will use function annotations in the upcoming Python 3 version to help even more.
I guess because LaTeX isn't commonly used in secondary schools, and thus she though I had done a google for the topic and handed in the first paper I came across.
`@validate_args` is a decorator that creates a wrapper function for the decorated function at definition time. To remove the performance penalty in production, you just need to have `validate_args()` return the original function instead of creating a wrapper.
I actually just got some exposure to flask last night at the pysprings meetup, so I might try that route :)
I'm actually writing a book on this at the moment, based on my experiences of building a real time front end for a hedge fund. Here's how I do it: The app is two pyramid processes running behind nginx. One is standard pyramid running on paster's threaded webserver for database access and other stuff that might block. The other one is running on an eventlet web server (although gevent would probably be my choice if I was starting over), which streams events over a few web socket connections to all connected clients. I use an open source library called stargate [1] to add some real time capabilities to pyramid's traversal system. The traversal system in pyramid lets me map event sources (which are really zeromq sockets with protobuf messages) to urls that you can subscribe to with websockets from the client. Give me a shout either here or via a message if I can help further. [1] https://github.com/boothead/stargate
well... that seemed kinda conclusive. anyone want to dispute it or something?
CAG will be open sourced, but I have no ETA.
I hope somebody does, I like Django in general and I hope its not really that inefficient.
The big disclaimer on something like this is: Django templates are slow, but does it really matter for your particular usage case? CPython is much slower than C/C++, but if you use it for appropriate things and engineer correctly, it's "fast enough". The same argument applies here. You're likely to spend a lot more time waiting for DB, disk, and other things than your templates. I feel this is premature optimization for many people. However, if you like Jinja's set of tags and general approach, those would be much better cases for switching, IMO.
&gt; Tornado I'd suggest using gevent instead. Tornado will cause you to become bogged down in difficult to understand code, and if you're fine with that you might as well use Node.
This article is unclear; Django TEMPLATES are bad, but you can use Jinja in Django and get all of these benefits.
 [**@GeoBasesDev**](http://twitter.com/GeoBasesDev): &gt;[2013-03-13 16:28](https://twitter.com/GeoBasesDev/status/311876342721429506) &gt;GeoBases 5.0 is out! Now available on PyPI. Check [*opentraveldata.github.com*](http://opentraveldata.github.com/geobases/), release notes at [*github.com*](https://github.com/opentraveldata/geobases/wiki/News). Hoping to get feedback! ---- [[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1a810n%0A%0APlease leave above link unaltered.) [[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [[Translate]](http://translate.google.com/#auto/en/GeoBases%205.0%20is%20out%21%20Now%20available%20on%20PyPI.%20Check%20http%3A//opentraveldata.github.com/geobases/%2C%20release%20notes%20at%20https%3A//github.com/opentraveldata/geobases/wiki/News.%20Hoping%20to%20get%20feedback%21) [[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [[Statistics]](https://www.stathat.com/decks/PJSe8OF5J44Y) 
I actually much prefer Jinja2's set of tags and general syntax to Djangos. Calling functions with arguments in Django templates is a major pain point for me. I maintain a pretty large Django application with pretty complex templates. If it takes nearly 2.4k function calls to render one simple basic template then I am guessing one of my large ones, with multiple parent templates and lots of blocks will take a lot more.
Could you do a similar comparison using Mako templates?
Both Jinja and Django templates can be rendered independently of a framework. As this stands it's more of a Flask+Jinja vs Django benchmark. I'd rather see a completely barebones WSGI application compared where the only difference is the templating engine used.
Wouldn't it be a better test to use Jinja2 w/ Django vs. vanilla Django templates?
how'd they come up with 2400 function calls? and even if that's the case, 100 templates in under .3 seconds? does it really matter? At how many users do you need to start worrying about that?
&gt; The big disclaimer on something like this is: Django templates are slow, but does it really matter for your particular usage case? Sorry, but it's way past that point. Django templates are too slow for the usage case of generating web pages as they are requested. When you're comparing performance differences of 5ms to 50ms, sure, whatever, speed isn't an issue, but we're talking 150~300ms spent on rendering the page alone. You also have to consider the amount of time spent doing things other than rendering templates, things like accessing the database and so on and surprise-surprise, Django is slow at that too. I've got a site that uses werkzeug + gevent + jinja2 and it routinely gets under 50ms per request (less than 20ms on rendering) for some pretty complicated pages. Django is unbearably slow to the point of obsolescence.
But the rest of Django is bad too...
https://gist.github.com/orf/5153108 493 to isinstance().
You can use the csrf_exempt to not use csrf tokens. 
Which would then mean you are susceptible to CSRF attacks.
Django *does* have a setting to precompile templates: look up CachedTemplateLoader. Since the article didn't use this, the comparison is meaningless.
But hey! Fast templates! That's what matters, right? Right!?
Well there you go folks, time to shut up shop with Django. Django is just *bad*. Thanks /u/mcilrain you really saved us there.
I'm curious what your results would be if you tried rendering Jinja2 templates *from* Django. I can't imagine it being terribly difficult to do, but it would help confirm where the inefficiency is coming from. 
https://docs.djangoproject.com/en/dev/ref/templates/api/#django.template.loaders.cached.Loader 
As someone who worked on a django site doing about 250 million pageviews a month (well, when I left, surely more now), removing django's template engine in favour of Jinja2 was the single biggest performance improvement we made (thankfully we never used its "ORM"). Never looked back
I can take a look for you if you'd like. I'm fairly good at python and am always interested in new stuff.
Thanks, was on my phone so couldn't find it myself.
I've updated the post and graphs to reflect this discovery. Its still not faster than Jinja2 though :(
Regarding the debug option, Python actually has built-in support for debug options. There's a [built-in constant called \_\_debug\_\_](http://docs.python.org/2/library/constants.html) that's True unless Python was run with the -O flag. It's also hacked in a very interesting way. If you write: &gt;&gt;&gt; b = True &gt;&gt;&gt; b True &gt;&gt;&gt; def slow(): ... for i in range(100): ... if b: ... pass ... &gt;&gt;&gt; __debug__ True &gt;&gt;&gt; def fast(): ... for i in range(100): ... if __debug__: ... pass ... You can see that the fast() function is about twice as fast: &gt;&gt;&gt; timeit(slow, number=100000) 0.5064652940054657 &gt;&gt;&gt; timeit(fast, number=100000) 0.23574563700094586 Why is this? If the only thing in an "if" statement is a built-in constant, the "if" statement is actually optimized out when the function is constructed, so the fast() function is actually equivalent to &gt;&gt;&gt; def fast(): ... for i in range(100): ... pass ... The same principle works for "if True:" and "if False:". A caveat, though is that many people aren't aware of/don't use Python's -O flag, so if one of those people uses code that has a significant performance difference depending on \_\_debug\_\_, they will always be using the (probably slower) debug version.
* In the sidebar of /r/changemyview, we have a "top ten viewchangers" leaderboard that is currently updated manually based on the deltas that deltabot gives out. We'd like this to be updated automatically. * Make the bot ignore deltas that appear &gt; in quotes like this. * Make it so that OP cannot be awarded a delta in their own thread. * The idea of limiting the amount of deltas that can be given by one user to another in a single comment tree, explained more clearly [here](http://www.reddit.com/r/changemyview/comments/19cp20/mod_post_just_a_possible_change_to_the_delta/). :)
The speed gain is not insignificant, especially not with large complex templates.
You mean you did not use Django's ORM? What reason to use django then?
I would be interested as to how you did this. I love everything about Django bar the slow and clunky templates and the not-as-good-as-sqlalchemy ORM.
Websocket is the right technology to use for real time apps, but it's still problematic behind proxies or firewalls. SockJs [1] offers a nice websocket-like api (so it will be easy to replace with pure websockets when everyone is ready) and provides different transports (streaming, polling, ...) for fallback There are lots of server-side implementations for sockjs [1], and tornado-sockjs [2] is very mature. Here [3] is a small chat example with tornado-socks. [1] https://github.com/sockjs/sockjs-client#readme [2] https://github.com/mrjoes/sockjs-tornado [3] https://github.com/mrjoes/sockjs-tornado/blob/master/examples/chat/chat.py 
Ok, I will bite. I'm going to take a complex template I have and translate it to Jinja2 and see how it compares to Django. Watch this space.
Make sure you include the complete code for the view and template, please.
There was a lot of python around in that shop so django was an obvious choice, and when the choice was made there wasn't much alternative - django was a safe (ish) choice with a big name and easy enough to recruit for.
Python functions already have exactly what you just described built in. &gt;&gt;&gt; def foo(x:int, y:int=0) -&gt; int: x + y ... &gt;&gt;&gt; foo.__annotations__ {'return': &lt;class 'int'&gt;, 'x': &lt;class 'int'&gt;, 'y': &lt;class 'int'&gt;} As for static analysis, because of Python's duck typing, it's extremely difficult to catch any but the most obvious errors, and impossible to catch some errors (although obvious error catching can still be helpful). The main problem with annotations, though is that there's no universally agreed upon standard, so for a function that returns None, one person might write "f() -&gt; None", another might write "f() -&gt; NoneType" another might write "f() -&gt; inspect.Signature.empty", another might write "f() -&gt; 'None'", or leave it blank. So, static analysis is pretty much impossible unless a standard is enforced.
Affiliate link
&gt;`key=lambda x: len(x)` `key=len` works here.
Egh, everyone is wrong in the chat log. It's very easy to blow the stack doing things recursively. The comment about log n only applies if you're recursing in a "balanced" fashion, which doesn't always happen.
Well that does it. I'm not using django any more. Ever. 
Have you tried gevent-socketio? Many people consider Twisted and Tornado a pain in the ass, and they are, but gevent is much smoother in my opinion.
Im not sure what exactly you mean with resources but if you want to learn python i suggest a byte of python, dive into python, thinking in python and of course the official documentation from python.org
google will help you find many case studies, and a significant amount of those are in video form.
sarcasm?.. I hope :)
Please don't, Django is awesome. Speed of execution is not everything.
* http://www.reddit.com/r/learnpython/wiki/index * http://www.google.com/search?q=python+sensors
I've built both Tornado and gevent apps that have been used in production for semi large scale (1000+ active users) marketing type contests (sorry, they were built for ad agencies so I can't provide source). I actually find Tornado far easier to read, write and understand once you start using the tornado.gen interface to turn your functions into generators. Then you simply yield whenever you want async code to execute: http://www.tornadoweb.org/en/branch2.4/gen.html The biggest plus for Tornado for me is that it supports both Websockets and standard long-polling requests so it becomes trivial to write code used by both and then call the most optimal interface via the client side javascript.
http://appscale.cs.ucsb.edu
Yes, I am aware of how Tornado works, I have written a relatively large app in it before abandoning it due to how destructive having to write generators instead of functions was to the readability and usability of the code. Again, if readability and usability of code isn't a concern then I would choose Node over Python+tornado.
I was watching the pypy talk the other day and they were running Django through pypy. Can you put this in your benchmarks as well if it isn't too much effort? I also noticed someone commented about this.
I'm not finding any projects linked to your name which require the performance differences shown in the OP. Can you provide some credentials?
&gt; you almost always need a CSRF token somewhere in your template (anywhere with a POST form), What. What? Stop being a complete tool. Go look on any highly dynamic website and count the number of POST forms. (highly dynamic because... well, if you're using django for static sites then you may as well just pack it in.)
And the uselessness of benchmarks rears it's head. A lot of people are going to come away from this thread thinking they learned that Django is *bad*, that Django is *slow* and that it should be avoided. The claims that it's template engine is slow are completely founded (Hell, it's actually *fast* compared to a couple of releases ago!) but to say that it matters for the 99% of people who's shitty database wrapper webapps is just plain lies. Lies of the highest degree.
Mate, i didn't make the decision (i joined after it was made) and said plenty of times that it was a bit superfluous to requirements - but like I said there wasn't *much* else at that time (there was Pylons and whatever else but not much in the way of "micro"), and a bunch of people there already knew django. sometimes the best decision is to just go with what your team knows. it isnt like web stuff is hard! Yes, all that was being used was routing and a few other bits. Anyway, for what it was used for it worked well so it didn't really matter in the end as the stuff that django provided, in the grand scheme of things, was fairly insignificant
Just write a template loader and swap it in. I'll even provide one for you, if you like. Django isn't all that great, you already say you prefer SQLAlchemy and if you're interested in jinja2, you're more than half way to moving away from django. WTForms is nicer to use day-to-day than django forms too ;)
Nebulous would be a better word. There's a lot of different types of overhead in a program. Object creation, function call overheads, I/O and CPU bottlenecks, memory usage, and a lot more that I can't think of off the top of my head. Because different frameworks are, well, different, they each have different types of overhead. Depending on the way you write your service and the use patterns you actually encounter, different frameworks would incur different amounts of overhead in different areas. In short, it's complicated and worrying about scaling is probably premature optimization.
Not sure. MyPy allows typed variable declarations, which seem (to me) to be more natural C-style. I guess its one of those religious issues :)
None if you know how to cache!
Another implementation of websocket is [ws4py](https://github.com/Lawouach/WebSocket-for-Python) which supports gevent and CherryPy servers. Mind you, bare websockets are not as feature rich as socketio.
Twisted is pretty simple to use for this. The problem with twisted and all of the Python frameworks in this area is that they are pretty undocumented.
I'm curious why you think generators are destructive and mess up the readability/usability of the code? Do you mind elaborating? Is it just in the context of Tornado or do you feel that about generators in general?
why would you think that?
I've had great experience with Pubnub: http://www.pubnub.com/ They keep stuff very well in synch and have a few different distribution models depending on what you're specifically looking for. It uses hanging get requests (The client makes a request to the server, and the server just doesn't answer until it has new information.) Since it's not a websocket (just a normal http post iirc, with https option available for double budget). Ping is down in the 250ms range, being initiated by my server, sent through pubnub, to the client's javascript. That's pretty solid.
Benchmarks are useful, but you need to be careful about how you use them. If, say, you want to use Django (prior experience, availability of developers, whatever), then go ahead and use it, **unless** there is some specific reason not to. One possible reason is performance because the template engine is slow (though how you figure it's is too slow without having the application and knowing the loading is a different kettle of fish). Also, there are two separate aspects to speed. First, there is server load: you might want to minimise the size or number of servers you'll need, but then again hardware is pretty cheap and it might not be worth the development time. Second, there is server response time: your server may never be more than lightly loaded, but if the user is unhappy with the response time, that is a problem. For example, we use a custom MVC library with its own template engine (partly historical reasons, partly application needs). Most of the pages are data entry forms, lots of user fields, but still a fairly small number of substitutions, so the template time is relatively unimportant. **But**, there are also a number of business reports that can be many pages long and have a very large number of substitutions. I spent a lot of time improving the template engine, it started off slower than Django's (!) and ended up about the same speed as Mako and Cheetah; the perceived improvement in response time was well worth it. TL;DR; Horses for courses.
I strongly dislike using generators as a way of providing asynchronicity. Having yield statements littered all over the place looks like crap and makes using return impossible. Python's biggest strength is code readability, if you're going to throw that out the window then you might as well use a different language.
You need to rewrite your question, or rewrite your tests. This isn't a comparison of templating engines, it's a comparison of frameworks.
&gt; One possible reason is performance because the template engine is slow (though how you figure it's is too slow without having the application and knowing the loading is a different kettle of fish). You nearly admitted how this is impossible. Just admit it is. &gt; Horses for courses. Indeed, and that's the essence of what I said.
Really appreciate all the feedback - I have some things to ponder here for sure.
Bah, I got annotations/decorators the other way round in the first two sentences. Edited my post. My point was that decorators don't replace annotations, decorators create more overhead whereas annotations don't.
http://i.imgur.com/D0WPb.jpg
What did you use for the bridge? cython, swig, or something else?
Twisted can definitely be fairly complex, but with the appropriate hackery/finesse it's not so bad - https://github.com/devdave/txWeb/blob/master/README.md
I've started to do the "thinking in python" class. But I really can't get into too much because I'm in an A+ class and 2 networking classes right not. and the networking classes are kicking my ass. So, I am looking for something that has the code made already and I could jsut download it.
* https://github.com/pydata/vbench * https://github.com/amcfague/linesman * https://github.com/bdarnell/plop * http://firelogger.binaryage.com#python * http://scikit-learn.org/dev/developers/performance.html * http://nichol.as/benchmark-of-python-web-servers * https://bitbucket.org/akorn/helloworld/src
Well that's the point of a blog isn't it? To brag to the world about how much you know about stuff.
exactly... caching is so simple with django as well. I havent even delved into the realm of caching myself, but i know memcached is simple to set up and can be done per view. django templates may be "slow", but they're fast enough for me.
https://github.com/search?q=raspi&amp;ref=commandbar and for when you get lost: http://docs.python.org/2.7/
&gt; Calling functions with arguments in Django templates is a major pain point for me. This is by design. It shouldn't be done in templates.
&gt; werkzeug + gevent + jinja2 And I'm guessing you have to write every damn feature from scratch, even if you don't posses the knowledge to do it properly.
Third party templates.
People who don't know how to use the ORM say this all the time.
It's about the whole package. Try googling for a specific issue you are having when combining WTForms and ElasticSearch or something like that. Yeah, no answers.
reading from sensors makes me wonder if you need to communicate with them via pyserial or something. just a guess. 
View caching is easy to use, and works well, but short of pages that are 100% consistent between users its pretty useless. 
It's the reason I don't have a blog! ...
Not a recommendation as I've not used it, but Tinkerer may be interesting to you as it's based on Sphinx: http://www.tinkerer.me/
That's set literal notation actually.
It isn't clear what is considered thread sensitive beyond the low level stuff (eg reference counting). I'm the author of a CPython extension in C. The extension is thread friendly in that it acquires and releases the GIL. However during sections of code where the GIL is held it knows that things can't be changed under its feet. For example if it is passed a list then the number and members of the list can't be changed unless the GIL is released and the code knows that. This essentially means that virtually everything is thread sensitive. More accurately anything that makes changes. If C extensions had to cope with things changing underneath them then it requires considerably more code, is more fragile and would greatly impact performance.
Here's hoping that setuptools (and pip) get included with Python so we finally have a sensible way to install packages out of the box.
That's a huge topic that I'm hoping to tackle ASAP. The basic premise is that parallel 'Context' objects (well, structs) are allocated for each parallel thread callback. The context persists for the lifetime of the "parallel work". The "lifetime of the parallel work" depends on what you're doing. For a simple ``async.submit_work(foo)``, the context is considered complete once ``foo()`` has been called (presuming no exceptions were raised). For an async client/server, the context will persist for the entirety of the connection. The context is responsible for encapsulating all resources related to the parallel thread. So, it has its own heap, and all memory allocations are taken from that heap. For any given parallel thread, only one context can be executing at a time, and this can be accessed via the ``__declspec(thread) Context *ctx`` global (which is primed by some glue code as soon as the parallel thread starts executing a callback). No reference counting or garbage collection is done during parallel thread execution. Instead, once the context is finished, it is scheduled to be released, which means it'll be "processed" by the main thread as part of its housekeeping work (during ``async.run()`` (technically, ``async.run_once()``). The main thread simply destroys the entire heap in one fell swoop, releasing all memory that was associated with that context. There are a few side effects to this. First, the heap allocator (basically, the thing that answers ``malloc()`` calls) is incredibly simple. It allocates LARGE_PAGE_SIZE chunks of memory at a time (2MB on x64), and simply returns pointers to that chunk for each memory request (adjusting h-&gt;next and allocation stats as it goes along, obviously). Once the 2MB has been exhausted, another 2MB is allocated. That approach is fine for the ``submit_(work|timer|wait)`` callbacks, which basically provide a way to run a presumably-finite-length function in a parallel thread (and invoking callbacks/errbacks as required). However, it breaks down when dealing with client/server stuff. Each invocation of a callback (say, ``data_received(...)``) may only consume, say, 500 bytes, but it might be called a million times before the connection is terminated. You can't have cumulative memory usage with possibly-infinite-length client/server-callbacks like you can with the once-off ``submit_(work|wait|timer)`` stuff. So, enter heap snapshots. The logic that handles all client/server connections is instrumented such that it takes a snapshot of the heap (and all associated stats) prior to invoking a Python method (via ``PyObject_Call()``, for example, i.e. the invocation of ``data_received``). When the method completes, we can simply roll back the snapshot. The heap's stats and next pointers et al all get reset back to what they were before the callback was invoked. That's how the chargen server is able to pump out endless streams of data for every client whilst keeping memory usage static. (Well, every new client currently consumes at least a minimum of 2MB (but down the track that can be tweaked back down to SMALL_PAGE_SIZE, 4096, for servers that need to handle hundreds of thousands of clients simultaneously). The only issue with this approach is detecting when the callback has done the unthinkable (from a shared-nothing perspective) and persisted some random object it created outside of the parallel context it was created in. That's actually a huge separate technical issue to tackle -- and it applies just as much to the normal ``submit_(wait|work|timer)`` callbacks as well. I've got a somewhat-temporary solution in place for that currently: d = async.dict() def foo(): # async.rdtsc() is a helper method # that basically wraps the result of # the assembly RDTSC (read time- # stamp counter) instruction into a # PyLong object. So, it's handy when # I need to test the very functionality # being demonstrated here (creating # an object within a parallel context # and assigning it to a non-parallel # object). d['foo'] = async.rdtsc() def bar(): d['bar'] = async.rdtsc() async.submit_work(foo) async.submit_work(bar) That'll result in two contexts being created, one for each callback invocation. ``async.dict()`` is a "parallel safe" wrapper around a normal PyDict. However, it's considered to be "protected". I could have also written the following, which has the same effect: d = async.protect(dict()) 
What ``protect()`` does is instrument the object such that we intercept ``__getitem__``, ``__setitem__``, ``__getattr__`` and ``__setattr__``. We replace these methods with counterparts that serve two purposes: 1. The read-only methods are wrapped in a read-lock, the write methods are wrapped in a write lock (using underlying system slim read/write locks, which are uber fast). (Basically, you can have unlimited readers holding the read lock, but only one writer can hold the write lock (excluding all the readers and other writers).) 2. Detecting when parallel objects (objects created from within a parallel thread, and thus, backed by the parallel context's heap) have been assigned outside the context (in this case, to a "protected" dict object that was created from the main thread). The first point is important as it ensures concurrent access doesn't corrupt the data structure. The second point is important because it allows us to prevent the context where the parallel object was allocated from automatically transitioning into the complete-&gt;release-&gt;heapdestroy lifecycle. This is known as "persistence", as in, a context has been persisted. All sorts of things happen to the object when we detect that it's been persisted. The biggest thing is that reference counting is enabled again for the object -- however, once the refcount hits zero, instead of free()ing the memory like we'd normally do in the main thread (or garbage collecting it), we decref the reference count of the owning context. Once the owning context's refcount goes to zero, we know that no more references exist to objects created from that parallel thread's execution, and we're free to release the context (and thus, destroy the heap -&gt; free the memory). That's currently implemented and works very well. There are a few drawbacks: one, the user must only assign to an "async protected" object. Use a normal dict and you're going to segfault or corrupt things (or worse) pretty quickly. Second, we're persisting the entire context potentially for a single object. The context may be huge; think of some data processing callback that ran for ages, racked up a 100MB footprint, but only generated a PyLong with the value 42 at the end, which consumes, like, 50 bytes (or thereabouts). It's crazy keeping a 100MB context around indefinitely until that PyLong object goes away, so, we need another option. The idea I have for that is "promotion". Rather than persist the context, the object is "promoted"; basically, the parallel thread palms it off to the main thread, which proceeds to deep-copy the object, and take over ownership. This removes the need for the context to be persisted. Now, I probably shouldn't have said "deep-copy" there. Promotion is a terrible option for anything other than simple objects (scalars). If you've got a huge list that consumes 98% of your 100MB heap footprint, well, persistence is perfect. If it's a 50 byte scalar, promotion is perfect. I haven't implemented promotion yet (persistence works well enough for now). And none of this is integrated into the heap snapshot/rollback logic -- i.e. we don't detect if a client/server callback assigned an object created in the parallel context to a main-thread object -- we just roll back blindly as soon as the callback completes. Before this ever has a chance of being eligible for adoption into CPython, those problems will need to be addressed. As much as I'd like to ignore those corner cases that violate the shared nothing approach -- it's inevitable someone, somewhere, will be assigning parallel objects outside of the context, maybe for good reason, maybe by accident, maybe because they don't know any better. Whatever the reason, the result shouldn't be corruption. So, the remaining challenge is preventing the use case alluded to earlier where someone tries to modify an object that hasn't been "async protected". That's a bit harder. The idea I've got in mind is to instrument the main CPython ceval loop, such that we do these checks as part of opcode processing. That allows us to keep all the logic in the one spot and not have to go hacking the internals of every single object's C backend to ensure correctness. Now, that'll probably work to an extent. I mean, after all, there are opcodes for all the things we'd be interested in instrumenting, LOAD_GLOBAL, STORE_GLOBAL, SETITEM etc. What becomes challenging is detecting arbitrary mutations via object calls, i.e. how do we know, during the ceval loop, that foo.append(x) needs to be treated specially if foo is a main-thread object and x is a parallel thread object? There may be no way to handle that *other* than hacking the internals of each object, unfortunately. So, the viability of this whole approach may rest on whether or that's deemed as an acceptable tradeoff (a necessary evil, even) to the Python developer community. If it's not, then it's unlikely this approach will ever see the light of day in CPython. If that turns out to be the case, then I see this project taking the path that Stackless took (forking off and becoming a separate interpreter). There's nothing wrong with that; I am really excited about the possibilities afforded by this approach, and I'm sure it will pique the interest of commercial entities out there (much like Stackless and Eve Online), so I don't see this work as a potential waste of time. It'd be great if it were adopted into CPython, but that'll be a long way down the track (at least 4.x I'd say), and all these issues will need to be addressed before it's even eligible for *discussion* about inclusion. 
Oh wow, you went way above and beyond the call of duty, thanks a lot! Gonna read through this... and hopefully grok it too...
So, I talk about some of those issues in those two walls of text ~~above~~ elsewhere on this page. The thing I don't like about your example is that you're referring to a "main thread" list object that is then mutated within parallel threads. That's not something you should be doing. The thing I **really** don't like about that example is despite how much I don't like it and you shouldn't do it, I need to handle it. It could happen by accident, intentionally, or because the programmer doesn't know better. Handling list mutation is especially annoying because there's not an easy way to intercept foo.append() like there is foo['bar'] = x or foo.bar = x. I talk about that more above. So the short answer is: don't do that! I'm targeting problems that can be easily solved via shared-nothing architectures -- if you've got all sorts of thread cross-talking going on, this isn't going to be for you (at least not in the current state). The long answer is: yeah, we will need to address that if this is ever to be considered for inclusion in CPython. Even if your code shouldn't be doing that, the fact that it can be done means it will be done, and we shouldn't crash or corrupt data if that happens (I'm aiming for raising an exception telling you your code is wrong, that's better than silent corruption).
That wasn't my intention :P Was honestly planning on ending the reply after the first sentence ("huge topic, wait for later"). Got a bit carried away.
Could you somehow elaborate on this? What do you mean by "balanced fashion"—just that your call stack grows to log(N) in the worst case too?
It's HTTP, not rocket science.
What a python senior developper should know: * write pretty algorythm * python stuff like mutable, immutabe types, reference, list comprehention * python style code and phylosophy: Pep 8, 20, ... * standard lib * packaging: setuptools, pypi, virtualenv * write a correct setup.py with setuptools * what means "setup.py develop" :) * popular libs: dateutils, requests, IPy, sqlalchemy For web usage: * what is WSGI * a web framework or two: django, pyramid, flask And maybe: * celery * maybe what is Twisted, gevent. Difference between them and what usage. Good luck. Switching to python is something great!
No thanks. You can't feasibly do virtualenvs if you don't have a mechanism separate from the package manager for grabbing libraries.
Then check out C and enjoy some speed. ;) PD: yes this is sarcasm
I used to think this way, as a sysadmin you want everything packaged up nicely so your package manager can deal with it, so you can track installation of everything installed on a system. However, after years of Python development I've come to appreciate the need for a good package management/installation tool which works **alongside** the regular package manager for a system. Something which installs to a specific directory outside of the regular system packages (usually /usr/local) instead of mangling the directories used for system packages. Then there's the need for virtual environments. I've got a bunch of apps which rely on a specific version of a library, and I don't want to spend hours upgrading all of them just to use a newer version of it in another application. Python packaging/deployment has been ugly in the past, but fortunately things have been improving over the years.
BTW It isn't my C code doing it, but rather the Python caller that could be doing it. In GIL protected sections of my C code I know for a fact that a determined caller can't perform mutations or anything else on me which makes for greatly simplified C code. 
lol.
I disagree. A web app might have all kinds of things you're not an expert in. Authentication schemes, geo-spatial look ups, session cookie, cross domain sessions, etc.
Lol at what? "Almost always" is a ridiculous statement to make about CSRF tokens.
Or profiling... or caching... or server diagnostics...
[NewsBlur](http://newsblur.com) is open-source: you can pay $1/month (I think) for a subscription, or [get the source](https://github.com/samuelclay/NewsBlur/) and host it yourself.
setuptools won't be in the stdlib. distlib will be, though, to replace ``pkg_resources`` and ``setuptools.package_index``. pip *might* be, and in any case my hope is to (mostly) replace easy_install with pip, once pip has support for PEP 427 (.wheel format) and other binaries. The gap that remains is a replacement for setuptools' build-time functionality, sdist creation, upload, etc. A distutils replacement. Once all three of these things are mature (distlib, pip w/binaries, and a distutils replacement), setuptools itself can die a natural death.
Nod, point taken. I'll refer back to my long answer: yeah, we'll need to address that ;-)
When I posted the link, I had forgotten to 'publish' the slides, so they were 404ing for a while until someone notified me ;-)
Nice to learn about that, particularly the way code gets optimized out when testing a constant.
I don't deal with them specially. They "Just Work" out of the box, which is nice. However, if you're wondering how reads are kept consist in the face off possible concurrent modifications, that's a harder problem, which I discuss in those two walls of text elsewhere on this page.
Agreed! Just saw that today. Just one of those things that I wish more people knew about, seeing as it's not as useful when nobody knows to turn off debug mode in production.
right, i forgot that you can fine grain things even further like this.
I can grep a selection of my own projects and come up with a different number. My point was that saying CSRF tokens are *required* is a bit ridiculous.
But you wouldn't put an empty set in this situation, so I can't see that being a problem.
Well, by default in Django they are required, and its good practice use them with every POST request, so sorry I don't really get your point. Are you saying that using a proven prevention technique in every vulnerable location is ridiculous? Edit: You are also the first person to use the word "required".
For the lazy: Uses Django, Celery, RabbitMQ, MongoDB, Pymongo, Mongoengine, PostgreSQL, jQuery, Underscore.js, and miscellaneous jQuery plugins. (OP: I looked for the same thing last night :))
Good job deleting your posts. Need and required are very similar in meaning. 
I deleted it by accident and re-posted it shortly after, you can find it below if you cared to look. Do you actually have a point to this or are you arguing for the sake of arguing, even though you are wrong? I've read a few of your comments here to both me and others and you seem to be annoyed at this blog post. Please don't take the fact that Django templates are slow to heart, its only software after all.
It is indeed just software but you've made a benchmark for a tool used by a community of learning programmers. Doing this kind of thing is a bad idea usually because the level of ignorance surrounding issues such as performance is so unbelievably high it's astounding. This is why I'm defending "just software". The vast, *vast*, VAST majority of people would have never even realised the performance "bottleneck" of the templating system in Django since their shitty webapps will never even be used for more than 5 minutes.
Man... I never thought I would live to see this happening...
You also seem a bit full of your self and *really* want me to know you're involved with security. We can go back and forth trading insults or we can just leave it at that.
Mirroring the [Netflix thread](http://www.reddit.com/r/Python/comments/1a35h8/python_at_netflix/), here are some questions that I think are interesting and relevant here as well. &gt; One of the problems with Python at large orgs is packaging and distribution. Do you guys run your own private PyPi or do you just checkout from source? Source: [ekarulf](http://www.reddit.com/r/Python/comments/1a35h8/python_at_netflix/c8toc3z) &gt; [...] teams seem to have free reign in developing in any language they choose. Does this ever cause problems? Why doesn't the company standardize? Source: [Cha0ticGood](http://www.reddit.com/r/Python/comments/1a35h8/python_at_netflix/c8tx9xd) &gt; I do have one question. As a driver of Python, how do you get the ball rolling at first? Let say someone is hypothetically in a shop with tons of Java and would like to move things to Python. What would the best ways to push it? Source: [redalastor](http://www.reddit.com/r/Python/comments/1a35h8/python_at_netflix/c8tzfie)
I'm drawing parallels, you are obviously some form of high-grade performance expert who gets pissed off when his favorite software is shown to be slow and I get pissed off when people don't understand CSRF. And yeah, I know about security, unlike you and your "highly dynamic sites" that don't need POST requests to function.
I'm aware of security, I try. I'm not afraid to admit that I need to learn more. And I'm pissed at the flat acceptance of benchmarks when they get released like this. The situation is deeper than what you showed. I never said sites don't need POST requests to function, I said that the endpoints aren't *always* going to be POST endpoints. Reddit has very few from what I can see, for example.
You actually have to indicate sarcasm on the net? Still? That's sad. 
Most native package systems do not have great support for non-root users installing packages. So your suggestion would either require giving every user on your system root access and encouraging them to mess with systems packages, or requiring them to fill out request forms and wait for a sysadmin to install their packages for them.
it's great to see how the 'big boys' are using python but this article would have been way more cooler had their been some examples project code. Even stripped down, i think everyone could have benefited. This article reads to me as 'look at our toys, no you cant have any' 
I'll see you there!
Didn't people used to write RSS readers in a couple hundred lines of Perl and their filesystem? This has more dependencies than I would ever care to understand.
Heh. I first heard about Reddit when #lisp was all betrayed and hurt that you rewrote it.
Because we all know that using the ORM is always the right answer...
Python 3.3 is working, I'm using it right now. Works great for my small app. No real-life data, but there's no reason for it to be worst than 2.7
If you're running the Cedar stack^1 it does - see [heroku-buildpack-python][1]. Heroku uses [buildpacks][2] to provide support for multiple languages. Straight from the documentation: &gt; Heroku’s Cedar stack has no native language or framework support; Ruby, Python, Java, Clojure, Node.js and Scala are all implemented as buildpacks. That means that if you can make a buildpack, you can use the language. [And people have done just that.][3] But, back to Python - here are the [specific instructions][4] for configuring the buildpack to run Python 3. The repository's README should be self-explanatory, so I won't bore you by going over it here. --- ^1 If you don't know what stack you're running, you're probably running the cedar stack. [See how to find your app's current stack.][5] [1]: https://github.com/heroku/heroku-buildpack-python [2]: https://devcenter.heroku.com/articles/buildpacks [3]: https://github.com/search?q=heroku-buildpack&amp;type=Repositories&amp;ref=searchresults [4]: https://github.com/heroku/heroku-buildpack-python#specify-a-runtime [5]: https://devcenter.heroku.com/articles/stack
Cross-referenced the wall of text with relevant code links in a reply to python-dev: http://mail.python.org/pipermail/python-dev/2013-March/124692.html
Good to hear, thanks!
Actually I hadn't used Heroku before, so I'm still reading up on it. Thanks for the links, it looks promising. =) 
I'm lost. I think the next step was distutils2... 
Then, LET THERE BE STICKERS!
I'll be there! Sitting in the Portland airport waiting for my connecting flight to San Jose. I'm going by my lonesome and would be happy to meet some people to chill / learn / drink with. 
yeah, it should take what, 15 minutes or so to code solutions to all those security vulnerabilities that Django handles out of the box.
&gt; However, if you're wondering how reads are kept consist in the face off possible concurrent modifications, that's a harder problem, which I discuss in those two walls of text elsewhere on this page. Wait, I'm commenting under those two wall of text. Which I read but I might have missed something. You have explained how you deal with objects _produced_ by a parallel thread, and how you're planning to copy them out from the private heap to the CPython's heap maybe. But how do you deal with _reading_ from the CPython's heap? Like, your parallel thread wants to do `len(some_list)`, and at the same time some CPython thread adds some function to `__builtin__`, and your dictionary access to retrieve `len` from the `__builtin__` dictionary ends up reading discarded memory, or worse. Am I missing something, like you taking the GIL for the duration of your threads running in parallel? Or it "just works" for you because usually your main thread doesn't modify any of the CPython's structures, and is, basically, waiting on them to terminate? I mean, previous efforts to work around the GIL are unsatisfactory because they tried to do too much, to make sure that you can add a method to a CustomList class that was passed to the workers and they would see it appearing, and they would see it appearing when they use a CustomCustomList class derived from the above, and no memory corruption will happen. And that shit happens all the time, every time you retrieve an object's attribute calls go up the entire class hierarchy of that object and all relevant classes like `dict`, and its own dict, and you just can't lock that shit individually and remain performant. ------ The thing that seems __extremely__ promising about your approach is that it could be made to disregard, no, _disable_ that stuff by design. Because nobody wants that stuff actually, that's a sort of red herring really. So you just don't allow the host CPython to run at all while you do stuff in parallel, but still synchronize and release the GIL now and then to let it process signals and stuff. Then you have your thread which actually is 100 threads working in parallel, and whenever it has the GIL it can access the host's stuff as it wants because the host is frozen, and it's all awesome.
I, sadly, can't be there. Can I get stickers anyway?
Keep your eyes open, or PM me when you have time to hang!
NOOOO!
Speaking of Pycon, does anyone know of any way to get registered at this point? I know it's sold out, but some people have to have cancelled. The lady at the registration booth was not helpful in this regard. In fact, she wasn't helpful at all. She just sort of gave me a blank stare when I walked up, then started shaking her head no as soon as I started talking. 
YEEESSSSS!!!
Lol lisp. 
I've been using Django since 0.96 and have always found an efficient way to always use the orm.
Pssht, it's not *webscale* bro. If they managed to get requests in there it'd win the jackpot on Python dependency bingo.
And here you guys were at sprint last year: http://www.reddit.com/r/redditdev/comments/qx651/i_am_across_the_table_from_the_guys_working_on/
I will stop by!
 #http://pythonhosted.org/feedparser/ pip install feedparser import feedparser for feed in rss_url_list: d = feedparser.parse(feed) #do_stuff
Ah, right, sorry, I misunderstood. So, I wrote a reply to python-dev today that cross-referenced various points with corresponding chunks of code: http://mail.python.org/pipermail/python-dev/2013-March/124692.html If you scroll all the way down to the bottom, I talk about how I'd forgotten that I'd already started adding support for "protecting" lists much the same way as I can currently protect dicts and generic objects (by intercepting all the PyMappingMethods and using slim read/write locks where necessary). As for the GIL, basically, er, I'm holding it the entire time. The main thread, though, is intended to just call async.run(), which will just call async.run_once() in a loop until there is no more parallel stuff to do, then return. So, yeah, that, in itself, provides protection from main thread objects being mutated whilst being accessed from parallel threads (mostly). Basically yeah the gist of what you're alluding to is correct. The aim is to get it to a point where it'll raise an exception if you do something you're not supposed to (like try mutate a non-protected object, or whatever). ....as opposed to now, where if you do that, it either segfault or silently corrupt, which isn't so great ;-)
&gt; Come down and build that thing you've always wanted reddit to have. Is this where we get reddit to add an RSS reader to replace Google Reader?
I apologize for my country's onerous bureaucracy. 
There's not much we can troubleshoot without an example of your code. See the sidebar for code posting formatting. 
 import random def roulette(): bets = {'black': '2, 4, 6, 8, 10, 11, 13, 15, 17, 20, 22, 24, 26, 28, 29, 31, 33, 35', 'red' : '1, 3, 5, 7, 9, 12, 14, 16, 18, 19, 21, 23, 25, 27, 30, 32, 34, 36', 'dzn1': '1,2,3,4,5,6,7,8,9,10,11,12', 'dzn2': '13,14,15,16,17,18,19,20,21,22,23,24', 'dzn3': '25,26,27,28,29,30,31,32,33,34,35,36', 'evens': '2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36', 'odds': '1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35'} print('Possible bets:', '\n', 'black:', bets['black'], '\n', 'red:', bets['red'], '\n', 'odds,', 'evens,', 'first dozen (dzn1),', 'second dozen (dzn2),', 'third dozen(dzn3)') odds = bets['odds'] evens = bets['evens'] black = bets['black'] red = bets['red'] dzn1 = bets['dzn1'] dzn2 = bets['dzn2'] dzn3 = bets['dzn3'] while True: guess = input('Name your bet:') mon = eval(input('How much money are you betting?')) spin = random.randrange(36) + 1 print('The wheel is...', spin) if guess in odds: if odds in bets['odds']: print('You win!') print('Payout is:', '$', mon * 2) elif guess in evens: if evens in bets['evens']: print('You win!') print('Payout is:', '$', mon * 2) elif guess in black: if black in bets['black']: print('You win!') print('Payout is:', '$', mon * 2) elif guess in red: if red in bets['red']: print('You win!') print('Payout is:', '$', mon * 2) elif guess in dzn1: if dzn1 in bets['dzn1']: print('You win!') print('Payout is:', '$', mon * 3) elif guess in dzn2: if dzn2 in bets['dzn2']: print('You win!') print('Payout is:', '$', mon * 3) elif guess in dzn3: if dzn3 in bets['dzn3']: print('You win!') print('Payout is:', '$', mon * 3) elif spin in guess: print('You win!') print('Payout is:', '$', mon * 36) else: print('Sorry, you lose.') ask = input('Again?') if ask == 'no': print('Have a nice day') break
I'm here, where is everyone meeting? People going to the reception?
I'm not quite sure what you want to do here. You want to be able to switch what response the server gives without restarting it? You could use a file/db/whatever that the server consults each time before deciding what response to send. You could also have another path on the server that can be used to set the responses and then store it in some variable referenced the next time the server decides on a response. 
Ideally what I'm trying to do is server returns 503 for 30 seconds...after 30 seconds, returns 200. Basically I'm trying to capture that 503, and act on it...and when that 200 returns, I need to act on it accordingly (after the 503). 
That's the point -- to not do virtualenvs. System package managers have solved the issues of installing, un-installing, install transaction, pre-install, post-install scripts, recursive dependency management, conflict resolution etc. If you are deploying on a particular system and can use basic versions of packages from your OS do that. 
We'll be there starting tomorrow. I'm going to try and make it for the keynote, hang out all day, then beer it up in the hotel lobby in the evening.
I'm going to have to look into that. I would *love* to head down to Australia.
&gt; instead of mangling the directories used for system packages. That what OS package managers solve -- not having garbage pile up in your system, mangling and overwriting files, by running a ton of 'make ; make installs' on top of each other 'setup.py installs' but also throwing binaries and man pages in there for fun, or other binary data + some over-writing of /etc/init.d startup-scripts. After years of developing and shipping software I've also come to appreciate a mature and capable package management system -- so far I'll take either .deb or .rpm based systems. &gt; Then there's the need for virtual environments. Sometimes there is just no other way. Then virtual envs work but even then I package them in an RPM 
Cool! I'm here, hope to see you guys around
I'm incredibly jealous of those who will be in attendance! For those who are going, have loads of fun!
So you just want a quick and dirty test server? That's pretty easy with the standard library: from time import time from http.server import HTTPServer, BaseHTTPRequestHandler def elapsed_time(_since=time()): return time() - _since class SillyHandler(BaseHTTPRequestHandler): def do_GET(self): if elapsed_time() &lt; 30: self.send_error(503) else: self.send_response(200) self.end_headers() self.wfile.write(b'&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt;&lt;h2&gt;Foo bar&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt;') httpd = HTTPServer(('localhost', 8000), SillyHandler) print('Serving http://{addr[0]}:{addr[1]}/'.format(addr=httpd.socket.getsockname())) try: httpd.serve_forever() except KeyboardInterrupt: print('\nKeyboard interrupt received, exiting.') httpd.server_close() This serves up a 503 to any GET request for the first 30 seconds, and then a 200 dummy page thereafter. This is written for Python 3, but you should be able to adapt it to 2.x without too much hassle, e.g. change `http.server` to `BaseHTTPServer`.
hmm I'm not sure if I could emulate crowd noises using the default sound files provided for Python....would I be able to use any sound file found on the internet? 
They're definitely checking for people wearing badges. At least, they were at the welcome party thing tonight. Gotta register for that shit early.
What is the status of this initiative?
I have a badge, it just looks a little different because it's for the tutorials. And to be clear, I'm not trying to sneak in. I'm trying to find out if anyone knows somebody that can't make it so I can get a legit pass.
A couple things. Like branmuffin said, pull out that function definition. Whitespace is kind of hard to read on here, but it certainly look like that's where the dict lives right now. A couple other things that might be helpful: * Think about using a list of numbers rather than a long string of numbers. range(2, 32, 2) rather than the current evens string * The nested "if" statements appear to be unnecessary. If you set odds to bets['odds'], then "if odds in bets['odds']" will always eval to True. There is more refactoring that could go on here to make it more concise, but that's something thats fun to figure out as you work through your code. 
I recommend it (although Hobart in July can be cold).
Virtualenvs aren't for the deployment system - they're for the development system, where you likely have several projects using different versions of any given library and need to keep separate.
Great find, thank you. I'm going to email them, even though it's a longshot. At least I'll have consulted an official source.
Private PyPi: yes, we run a private one, for projects that are typical dependencies for other projects. But cloning the project's git repo is more common. Standardizing: we believe that giving development teams choice in how they build software (but mandating minimum requirements or standards that they have to meet in terms of quality, integration with other services, monitoring, documentation, etc) yields the highest productivity. When possible, people certainly will choose a framework that already has a lot of users -- this way they benefit from existing common libraries and experience. But they are free to pick something else if it makes a lot of sense for their project and they can justify it to themselves and their peers. For getting the Python ball rolling at first... it's hard for me to answer this for _any_ organization. At Hulu, Python got going when someone early on wanted to learn it -- and used it for a brand new project. In progressive-enough organizations this is probably the best way to get started -- do a proof of concept on a new project and show how well it works.
Polyphonic guitar tuner. 
**Free PyCon ticket** - if you are in the BA &amp; would like to attend PyCon, drop me a line &amp; I will give you the extra ticket we have (courtesy of msft &amp; http://pytools.codeplex.com). Cheers!
* http://docs.python.org/2/library/unittest.html * http://docs.python-guide.org/en/latest/writing/tests/ * http://wiki.python.org/moin/PythonTestingToolsTaxonomy * http://docs.python-guide.org/en/latest/scenarios/ci/ * http://en.wikipedia.org/wiki/Continuous_delivery 
Where are you from?
awesome! I'll be here for the first two days of sprints. hope to see you guys around... :) 
I think there's a big fear that releasing internal code like this to the public could potentially cause sensitive information to be leaked, perhaps from comments or strings buried in the code that contain important hostnames or keys or otherwise. I agree that after a careful audit, code like this should eventually be open sourced. Outsiders can suggest improvements and fixes, and people in general can benefit from it.
You can download the code (without graphics!) here: https://www.dropbox.com/s/66tlp39w58mlo2t/fumiko.tar.gz
actually I just didn't think of it. I'll check github out. :) thanks.
I created a github repository: https://github.com/skoam/fumiko-pyg.git It's my first one so I hope everything's done right. :)
Why did you need to deal with the US consulate? Isn't Turkey a member of EU? Norway is a member of [EEA](http://en.wikipedia.org/wiki/European_Economic_Area) so I only filled out a [ESTA](https://esta.cbp.dhs.gov/esta/) form, paid $15 and got my visa by email 30 seconds later. The whole process took maybe 5-10 minutes.
It would get very hard to read with high nesting - imagine a class that starts at col 40 ... And, if you ever need to use it, pickle doesn't work with nested classes
Very interesting! I wonder if you can switch to using processes + mmapping. So create a separate process, mmap the entire address space of the original CPython process as read-only and make your threads use their individual heaps for any new allocations. Then you automatically get page faults if they try to modify global stuff, in all possible cases, including when they try to cache stuff for performance in read methods, taking care of the gritty details of the metaobject protocol, and probably covering most extension modules as well. _EDIT:_ Or maybe use processes instead of threads, you're doing this for parallelism, not concurrency, so you want a single thread per core anyway, so you could use 4-8-16 processes instead. Though using a single extra process might be faster and easier. The key difference from just using multiprocessing is that you share the CPython memory with all loaded modules and stuff between all worker threads. Also, I don't think using a free-less heap is a good idea. It works for your benchmarks, but I would guess that for real-life programs it wouldn't work so well at all. And even if it's not completely horrible, good luck convincing people of that. Why don't you use a usual reference-counted private heap, but patch INC/DECREF macros to do like a single address comparison to never touch the original CPython heap objects (because you always allocate the private heap at a higher address)? I feel like working on that custom heap stuff diverts too much of your attention from the real important stuff. Also, I think you should accept that you will have to run some stuff in the original CPython context. Do you know how ctrl-c handling works on Windows, for example? And you absolutely want something like that for any real application anyway, because those can't just unconditionally wait until everything is done in the parallel section. You want to be able to terminate stuff, update the GUI, report progress and stuff. So better release the GIL sometimes and allow other threads to run in exclusive mode. Then maybe think on how to prevent accidental deallocation of objects that parallel threads could be interested in during that time.
&gt; Isn't Turkey a member of EU? Hahaha, good one!
Not a member of the EU
What about a text-to-morse code generator? Then a morse-to-text generator to go with it.
dude, please have a look at [pep8](http://www.python.org/dev/peps/pep-0008/) for naming your variables, methods, classes and objects. the way you declare your class member-variables, you share them between all instances of the class. see this example: class X(object): var = [] a, b = X(),X() a.var.append(1) print a.var, b.var # vs class X(object): def __init__(self): self.var = [] a, b = X(),X() a.var.append(1) print a.var, b.var the second one gives the expected result. the "GLOBAL_VARIABLES" Module isnt very nice either. 
Thank you for your prompt response.
[Wikipedia: Accession of Turkey to the European Union](http://en.wikipedia.org/wiki/Accession_of_Turkey_to_the_European_Union)
No I haven't, thank you I'll look it up and ad it to the ini file. Or maybe it's time to upgrade Spyder...
requests is a lot easier, and urllib is one of the worse stdlib packages. Anyways no matter how good your completion as long as you should write 130 lines of code instead of one. 
Looks good. Can you talk about how its built? did you use a framework? Any special libraries?
You can't, you can only see the HTML that it generates.
Can you think of a website that is open-source where I could see the (back end?) code? I'm learning python on my own and have never really been able to make the connection. 
The [flask tutorial](http://flask.pocoo.org/docs/tutorial/) might be interesting?
Sure, I think the easiest starting point is to read up on [bottle](http://bottlepy.org), then step through their [to-do list example](http://bottlepy.org/docs/dev/tutorial_app.html). Bottle isn't the only framework, but it's one of the simplest and will introduce you to the concepts a lot of the more complicated ones (like Django) use.
Why not just use SQLite, it's in the standard library already? BTW the items could use some timestamps, along the lines of "37 mins. ago" and so on.
Any chance this is online ?
well, pep8 isn't going to cut it, even though it would be a good start. Not releasing the graphics is the part I don't understand; I could never contriute to a code base like this, but if the graphics are hand-crafted, then you could surely find someone who would be interested in this style. To make this work, you have to rethink completely your goals and how to achieve them. You need simple physics? Try making a base class of objects that can interact physically and inherit. You need rendered object? Try making a base class... you get the point.
https://pypi.python.org/pypi/RPi.GPIO This should be all you need. It's very easy to get your "LED to blink" with this lib. 
I will look when I get to an actual computer. I think it was the first Atlanta year, so 2010? I know they put the slides up here on reddit but I think all sessions from that year made it to YouTube. 
Can I give a little bit of constructive criticism from the design standpoint? IMHO your font choice is terrible. This narrow, sans-serif, all-caps typeface just hurts my eyes. It looks good for headings and the toolbar up top. I think you could improve the readability of your site 100% if you removed the "all-caps" setting and allowed the links to be capitalized normally.
Sounds interesting. How do you know when it's working well? Do you have labeled data you use for tweaking the algorithm?
Sorry if this seems like a stupid question. But what is that site other than a compiled list of links to news stories, divided into categories? I have found no other functionality at all. The IndieGoGo campaign mentions something about fancy algorithms, but nothing really substantial. And you set the goal for 40K. I think I am missing something. 
I think another font would be better - look how horrible it looks without antialiasing: http://i.imgur.com/xSOKKpM.jpg
/r/learnpython is a much better place for your question.
Python has a built in sound generator?
This is a fantastic concept. Do you have a project page that provides more information and a donation mechanism?
I personally like the font
Nesting of views high enough to make code too wide seems pretty unlikely in the vast majority of web apps, and I can't think why you'd ever need to pickle views?
I believe this is it: http://blip.tv/pycon-us-videos-2009-2010-2011/pycon-2010-scaling-your-python-application-on-ec2-191-3276734
http://blip.tv/pycon-us-videos-2009-2010-2011/pycon-2010-scaling-your-python-application-on-ec2-191-3276734
The problem with that is: I'm trying to run 5 sensors, a camera, turn on and off a light and log all the sensor data for a weather station. then send it all to COSM to be accessible to my daughter's grade school. thank you for this page... Lots of good stuff there. 
Why are you nesting classes instead of encapsulating instances?
Citation needed. (My solution uses multiple threads, doesn't remove the GIL, and runs Python code in parallel thread callbacks faster than main thread code can run.)
So, heh, as for separate processes and a different memory allocator, sure, there are different ways to do things. I'm sticking with the one I've picked with so that I can keep making forward progress. I've got lots of other problems to tackle before heap/thread stuff needs to be reviewed. (And separate processes are a terrible idea on Windows.) And I've got lots of support for running stuff within the main thread. In fact, I've got explicit code in place to handle things like ctrl-C when parallel threads are active.
Some people actually turn off anti-aliasing on their computers so as to make their computers a little faster. It's not a browser issue. But furthermore, ALL-CAPS in typography is generally looked down upon as bad readability. Even for titles, nothing really stands out because everything is bold and kinda melds into each other. I can see that you want to go with a squishy/bold kinda look and I think you might like a bold Futura. Then play around with the typography, First word CAPS, first Letter Caps, etc., etc.
There is *one* talk I am interested and the stream breaks right before the speaker introduces himself. Streaming in the year 2013 makes me a sad panda... When can we expect the videos?
Hi, design-wise, I'm agreeing with maciakl; when you have such great information density as headlines, readability is paramount. The use case you want to think of here is would your grandpa with weak eyes find your site readable? If he can, it gets a pass. Don't use country codes as your headlines, it may seem cool, but it creates user friction; I have to think "Hmm... NZ is New Zealand..." to understand what each item in your toolbar represents. What I want the toolbar to do is to allow me to tell what the options are at a glance. I shouldn't have to decipher anything at all to understand what your site has to offer me. Hacker News is an example of design that doesn't look fancy, but puts its content first. It is not the best design, but it is good and very, very useable. What kind of audience are you thinking of? If you expect the layperson, a site without any pictures ain't gonna help. Also, you might want greater spacing between your items so that it doesn't look like a wall of text. I may seem pedantic, but all these small things add up to create a web site that dissuades potential users from devoting what little patience they have to considering your site as another they might want to visit sometime in the future. You don't want that. You want your UX to lure first-time visitors into exploring and interacting with your site so they can discover what you have to offer. Right now all I see is a homogeneous wall of gray text. For the educated surfer, he is thinking; do I trust this site to give me reliable and interesting news? Social media solves that by overtly telling the user that what they're seeing is probably important. Reddit scores with upvotes and downvotes, Facebook gives content endorsements from your friends, Quora has both. Hacker News has comments. How do I find what's interesting to me from your site? A headline doesn't offer enough information for me to click the links. Sorry how that turned out to be a rant, but I think these are useful questions to keep in mind when designing your site; but for a "first draft", this is really cool.
Ah thank you, did not know that subreddit existed.
There's jank when the page gets re-populated the disappearing and reappearing scroll bar causes the page to shift around a bit, looks terrible.
Good luck, that sounds like a lot of worthwhile goals, and I'll follow where it goes. Doing a (good) video is probably worthwhile - it's partly about the emotional thing of hearing someone's voice, it's partly about being shown the idea rather than having to explore it for yourself.
Just started learning python. Your site &amp; idea is really neat. Your design definitely need to be more easier to read, and maybe include a few more details/options, but don't lose the minimalist theme you have going.
I checked this out on my phone, was not disappointed.
I suggest the Cabin font. It's available on Google Web Fonts.
can it run on pypy ? any benchmarks against other awk implementations ?
Are you importing any libraries 'cause if you are it's not a 1000 lines of code, just a 1000 that you've written yourself.
"I purposefully made fonts on my computer look bad. Look at how bad it makes your fonts look!" ಠ_ಠ
I'm a huge fan of Python and shell scripting, but honestly, if you've got Python, why not rely on the built-ins in re that are standard and always going to be there? Conversely, if you're shell-scripting, the OS has its own standard built-ins that are always going to be there. (sed, awk, perl, python) I'm trying to figure out what use-case pawk was designed for. If it's to interoperate between Python and awk, why not just write a wrapper around awk in python instead? It's fast, standard, and people have put years of work into it. Why re-invent the wheel? I'm just hesitant to spend time learning a tool that is nonstandard with slightly different flags, and integrating it into my workflow when awk already serves this use case. I'm wondering if you're coming to Python from C++, or Java, because I'm noticing a couple antipythonic patterns here. One of the ones that popped out at me- this "if x is y" or "if x in y" construct that you rely upon. An exceptional Python sin is the use of "if x is None," for instance, on line 48 of pawk.py. We can just replace constructs like that with "if x." For more information about how booleans work in Python, look into the "bool()" builtin. You have a lot of nested if statements in general here. These really become a nightmare to debug. There are just too many different behaviors for each method/function. I guessed earlier that you may be coming from C++ and/or Java because I used to see this pattern a lot in those languages. Your code may run, but honestly, it's going to come back and bite you in the ass later on when you're calling functions that can take 10 different kinds of input and produce 20 different kinds of output. Python is somewhat unique amongst OO languages because it also allows the functional style. It's OK that you rely on OO, but you need to practice coding better functions. Try limiting your functions to taking one kind of input, doing one thing with it, and then returning one kind of output. Use just one return statement per function. if you have a function that could return multiple different outputs, assign them to one variable and then return that variable. Don't use multiple returns with multiple different possible output variables. Again, you'll thank me later when you have to run your code through a debugger. If you MUST write class methods that include all of these forking descision trees, then that's called a "handler" pattern. Write a function that exists solely to call other functions if given certain conditions. Separate the logic of your "handler" functions from the functions that actually "do stuff." Again, this goes back to what I was saying before: you need to practice writing code in the functional style, functions that do one thing only and do it well, even if that one thing is just deciding what other function to call next. If you learn to master this functional coding style, and how to interweave it with OO, you can probably cut the length of programs like this down by half. Seriously. A good place to start would be reading up on functools, especially functools.partial. Also, lambda, map, reduce, filter, and bool. Functions can provide a level of abstraction that is not quite as high as a class, but enough so that you can eliminate a lot of the visual clutter in your programs. By writing denser code like this, it will be faster and more algorithmic, but also, even though it may be harder to understand, it will have fewer room for bugs and errors, and it will be easier to debug because it is terse. I'd also like to see more thorough docstrings on all of your functions, methods, and classes. More decriptive function and variable names would be nice, too. One letter variable names are just kind of annoying for me, trying to grok your code. Lastly, you're using optparse, but there are more modern alternatives available now. Most people use argparse now. For reference: http://stackoverflow.com/questions/3217673/why-use-argparse-rather-than-optparse I know it's a pain in the ass, but again, I think you'll thank me later. I'm just giving you a hard time because otherwise you will carry over your programming habits that may make sense in other languages but are considered to be detrimental in Python. You will learn that Pythonistas obsess about code correctness. We consider it to be more important than speed in most cases. The exception being if you're going to do one thing over and over again, a million times. In which case you come up with an optimized algorithm, (such as awk, perhaps) code it in C, and then wrap Python around it. I think you can see where I'm going with this. ;p All in all, though, I like the Unix spirit of your program. It is a great tribute to our patron Khernigan, and shows real enthusiasm. Keep up the good work!
A good majority of top websites do not open source any of their code, unfortunately. Thankfully, reddit does.
"Console" tab in Chrome's developer console (F12), same should work in firebug if you have and you use Firefox.
http://www.dabeaz.com/python/UnderstandingGIL.pdf 
Can you tell me your browser? The site should make an AJAX request about 3-5 times every minute.
Please report breakages! Someone tripped over the A/V power in one room :(
Doesn't work.
Can you please tell me what your version is? It should be using HTML5 with acceleration if your computer supports it...
Chromium 25.0.1364.172 (187217) 
Version 25.0.1364.160 Ubuntu 12.10 (25.0.1364.160-0ubuntu0.12.10.1)
Here's short screencap: http://dl.dropbox.com/u/7704936/test.mpg
Fix pushed!
Seems fixed, thanks! 
Fixed for me as well! Awesomeness restored!
I'm at pycon! I'll see you guys around.
Testing is core. How would one know that the user always wins?
* http://www.reddit.com/r/learnpython/wiki/index * https://gist.github.com
Most of the time, it is better to limit the importable modules to only the set necessary for an app's particular function. A shorter `sys.path` is faster and less likely to cause problems. This requires extra work for sysadmins attempting to keep their packages updated.
Come on don't be that guy ... the fonts that the OS comes with look great without anti-aliasing. You have to agree that this looks very good http://i.imgur.com/4RaKnWV.jpg
Thanks :)
You can simplify this considerably and get rid of all of those boolean flags: from urllib.parse import urljoin from bs4 import BeautifulSoup import requests def spider_album(url): session = requests.session() while url: soup = BeautifulSoup(session.get(url).content) links = {a.text.strip(): a['href'] for a in soup('a', 'buttonblue')} if 'save image' in links: yield links['save image'] url = urljoin(url, links['next image']) if 'next image' in links else None print('\n'.join(spider_album('http://www.imagebam.com/image/c6b0cf70777810/')))
Must be a trend.. they did this at puppetconf 2012. Just a table with a black tablecloth and guys with hats and black shirts. 
As I recall from other conferences apple is incredibly protective of its brand and logoage. They rarely allow their logo or name to be used in conjunction with promoting events. That is to say — I've seen them do this at other events before. They sponsor on the condition that their sponsorship isn't advertised. These are often recruiting events for them. 
Um. Did you even look at the slides I linked? I'm pretty familiar with the GIL. And that presentation. My approach is completely different. 
"platformer"
pandas is an amazing library. I don't do my data management and analyses in anything else. I would like to also point out that with the .groupby function, you can specify the column *or columns* with the `by` parameter. e.g., # group by one column df.groupby(by="A") # group by two columns: A then B df.groupby(by=["A", "B"]) Very powerful functionality, and crazy fast.
And zero swag except a "free song" iTunes gift card. Bold. 
That looks like crap.
Very cool! May I suggest you add a README.txt file to the repo explaining what the script is and what it does? 
Seems like they will be up here soon! Amazing site overall too, for any python related video. http://pyvideo.org/
What about an app that you can do a tracert of a website, and it will output a tone corresponding to each hop. In a way you are manifesting the speed of the internet in sound?
Pandas are awesome. def some_complicated_function(variable): do_stuff_to_variable return x mydf[new_column] = mydf[values].map(lambda x: some_complicated_function(x)) #Makes a new column based on a function of another column
They did this at SurgeCon too. They were looking to hire.
Great, it would be good to change your repo's name to something like python-wolfram.
I think it looks great. I'm going to remember League Gothic because of your site. You definitely created a distinctive design. I do have to agree that it becomes a bit tiring to scan. I think the problem with all block caps is that every word is graphically too similar, so it becomes hard to scan. The impression is of extreme editorial confidence that every story on this site is very important. It's actually a pretty exciting effect. The paper of record will use this bold, all-caps style to announce a very important piece of news, for [example](http://graphics8.nytimes.com/images/2010/08/31/learning/19450814nytfront/19450814nytfront-articleInline.jpg). The effect is: pay attention! The tradeoff is you can't easily distinguish the words. I did notice that there's some shifting left and right as I switch between categories. It lowers the compromises the quality of an otherwise beautiful looking site.
Why the lambda ? just do: mydf[new_column] = mydf[values].map(some_complicated_function) (unless your line was extracted from some loop with x varying ...)
I have not tried Udacity, but having program experience before I found Code Academy good. Give it is free &amp; a small investment of time, I suggest giving it a try. Different people and experience will give varying answers, easier for you to try and decide. 
A big problem is that xlrd does not work with xlsx files. I really like openpyxl much more. It is very intuitive since you can specify column letters.
[LearnStreet](http://www.learnstreet.com/) has a somewhat new, but interesting Python course too.
I had limited programming experience before trying both of these. Personally, I found that the Python track on Codecademy was fairly easy to follow but it also felt a little shallow. Once you've completed it, you might feel like you've dipped your toe but I'm not sure it really gives you the confidence to try programming yourself. I found Udacity goes at a much faster pace and, while still very user-friendly, I started getting quite lost and confused near the end of the course. The up-side to this is that you'll probably get a lot more out of it and if you manage to make it all the way through then you'll probably have a lot greater knowledge of CS and Python. I would say that the main difference between the two courses are that Udacity will actually teach you some theory behind CS while the Codecademy will teach more of the syntax of the language (e.g. how to use loops, etc.) I'm currently working my way through the edX course which I find a lot more complex than these two. I would recommend it after you've completed Codecademy and Udacity. So in summary...do both and good luck!
Pandas works on top of numpy, so if you want, you can still use numpy's arrays directly. Apart from that, pandas simplifies a *lot* of the day-to-day data munging tasks. I'd recommend giving it a try.
It does, grab the [0.9.0 release](https://pypi.python.org/pypi/xlrd) and it works just fine. 
As runiq says it sits on top of numpy. I have just started using this over the last week, our reporting system isn't working properly so i am getting large dumps of data and loading into pandas in 2 lines of code and then basically treating it like a SQL data set, you can slice and dice how you want. 2 links on pandas you may find useful: Basic intro to pandas: http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/ pandas presentation at pycon, i think its about 20mins in where he starts with showing examples, the video is 3 hours long and wil walk through examples. http://www.youtube.com/watch?v=w26x-z-BdWQ If anyone has any other Pandas links apart from the official documentation can you share please, i'm still trying to learn how to effectively use it. 
You don't say what is your current level of ability? Those are both beginner sites. Is that right for you? If you can already code, the way I did it was the google python class, then I read 'python in a nutshell' by Alex Martelli, which gives you a very detailed explanation of the language, its features and the style in which it works. Python has very good docs on its own website, but they are hard to google, and you sort of need to know what you are looking for .
You can also try/start with learn python the hard way which free online http://learnpythonthehardway.org/book/ As others have said it depends on your skill level &amp; programming experience. You could also look at edx.org's MIT computer science course(https://www.edx.org/courses/MITx/6.00x/2013_Spring/about) offering which exclusively uses python for all assignments/labs etc 
Ah welcome aboard then. You might also want to look at r/learnpython.
Honestly, the official Python tutorial is really good IMHO for people who've never programmed before: http://docs.python.org/3/tutorial/index.html It introduces you gently to the concepts, giving context and reason for the way things are. Plus there's lots of Monty Python jokes.
I think the consensus is that it looks really nice but it's just too hard to read. I love the concept of the site btw.
Udacity will teach you some CS basics. It is not just programming. When you already have some pre-experience in programming do cs101 and then try peter norvigs follow up course.
I've tried both Udacity and Codecademy, and Udacity is hands down the better option, especially if you're new to programming. Like it's been mentioned before, Udacity teaches you more than just the syntax, which is definitely a big plus. Though having seen people having a lot of confusion over certain topics, I think having something to supplement Udacity would be a good idea. Learn Python the Hard Way is recommended a lot, but Codecademy as a supplement wouldn't be bad either. And it is certainly a good resource after you've completed CS101 on Udacity - Codecademys API track is pretty decent. The bitly API course by Hillary Mason is good. Oh and The Online Python tutor is something you should use. It will visualize the execution of your code, so you can see what happens to the different elements as the code works its way through the interpreter - http://pythontutor.com/visualize.html
Thanks for sharing. A question for you as i can't work it out. I want to create a new column and i need to pass some_complicated_function() more than 1 variable. How can i do that? The variables will be taken from the current dataframe row and used to do various calculations so i can get it to calculate the amount in USD. any help is greatly appreciated 
I will look in to a better way but for now here is a simple loop. #make a new column with all nulls Df[newcolumn]=None For i in Df.index: Df[newcol].ix[i]=mycomplicatedfunction(df[col1].ix[i],df[col2].ix[i])
I wouldn't call `*` and `**` "esoteric"...
 def row_function(row, other_arg1, other_arg2): do_stuff(row[0], row[3], other_arg1) ... return x df[new_column] = df.apply(row_function, axis=1, args=(arg1, arg2) There's yet more functionality in apply, but this covers most of what you should need -- your function gets a pandas Series which you can index (or you can apply ask for a raw numpy array if you like), plus other args if you like.
https://github.com/tooxie/shiva-server/commit/326a2c1934a5b81bf3d256d55b65dde76a8dd653 I'm planning to [replace eyeD3](https://github.com/tooxie/shiva-server/blob/master/shiva/utils.py#L50) as soon as possible. Thanks for the bug report!
I would agree, the only evidence that the author gives for calling this esoteric is that he hasn't seen it before. That almost makes it seem like he is dismissing it as some insane thing: &gt; OK, well and good, but WTF?! Where did that come from? 
yeah, they're probably one of my most used features of python. edit: I just want to mention - I use a lot of python features pretty evenly. I suppose "most used" is a misnomer, I use them a lot but I use most everything else a lot too.
yeah, this is basic python...
Why is that so esoteric? Python does the same thing with tuples, which is very, very common. It's especially common in class inheritance and function decorators, as it enables an arbitrary number of arguments. Lists and tuples have very similar behavior (except for mutability) so the situation OP encountered makes perfect sense. &gt;&gt;&gt; def print_everything(*args): ... for i, thing in enumerate(args): ... print '{0}. {1}'.format(i, thing) ... &gt;&gt;&gt; q = [1, 2, 3, 4] &gt;&gt;&gt; r = (1, 2, 3, 4) &gt;&gt;&gt; print_everything(*q) 0. 1 1. 2 2. 3 3. 4 &gt;&gt;&gt; print_everything(*r) 0. 1 1. 2 2. 3 3. 4
I'm teaching myself python at the moment, and ran into these last week with a similar reaction. To the other commenters: ok maybe esoteric is the wrong word, and yes this can be considered basic python. But for somebody new to the language and likely coming from another language, where this functionality is missing, this can be a bit surprising. The functionality is not obvious, like if it were an "unwrapped(...)" function. They're the same as existing operators (* for multiplication of course and ** for exponentials). I haven't seen any kind of agreement of what they're even called (Ruby refers to * as a 'splat' I think but I've looked for what Python calls these and haven't found anything). And they're also next to impossible to find in the online documentation. In fact, I still can't find where they are described in the official documentation. Used, yes, but not described. So yeah, I think this is a pretty cool piece of knowledge to spread around. Thanks OP!
If the number of optional named arguments in a function goes above 2 or 3 ** is coming out, that's a fact.
That's mostly the fault of the documentation for being so poorly organized. You can find the argument/keyword unpacking documentation here: http://docs.python.org/2/tutorial/controlflow.html#arbitrary-argument-lists But yes, even if it's not "esoteric", it is still fun to use in your code if you can find a useful situation :).
I can think of a lot of code in python that gets a whole lot uglier without them.
The only thing that makes these operators hard is that they're virtually impossible to google unless you see them in the form *args or **kwargs.
Starred! I do wish there was a little more on usage tho other than code examples :) But I'd understand if you don't have the time.
Virtually impossible? Not at all. I just went to google and typed `python **` and while the results were not useful, the first thing listed under Related Searches is `Python double asterisk` which returns tons of relevant information. The same applies if you search for `python *` -- you get two suggested related search queries that are describing exactly what you were looking for.
Thank you Oncocerca &amp; Imcinnes for your answers to my question. I will be trying both of these solutions!
Thanks! I'm glad you found it useful. I'm actually giving a talk today at 1:55pm (Room AB) if you're interested. Otherwise I'll be around all weekend, often at the Netflix booth.
https://github.com/samuelclay/NewsBlur/blob/master/fabfile.py#L495 Bingo! What do I win? My understanding is that much of the complexity is there because NewsBlur is designed to be multi-user and intelligently predict which posts you're going to be interested in.
Oh yes, also, I meant to add, but forgot... your comment up at the top of the program is in triple quotes " " " which actually indicates a docstring in python. They're slightly different. If you absolutely must have comments that span multiple lines, you should replace the docstrings with multiple single line comments using "#" instead. Refer to pep8 for more information about how comments and code are supposed to be structured: http://www.python.org/dev/peps/pep-0008/ The difference, in short, is that docstrings are only supposed to be paired with a function, method, or class so that you can pull them up with help(). Comments, on the other hand, can go anywhere. You might also want to check out the command line utility, pep8, that scans a python script and will notify you about any departures from the pep8 standard.
Sadly, I am not at PyCon this year. Instead I a am watching my brother's kids to enable him to go with his wife. 
You should post the corrected address: https://github.com/aseylys/Python-Wolfram. This is probably why you're being downvoted (which is a bit harsh!)
I work in recruiting for Apple, and I'm currently redditing at the booth. We're looking for skilled Python engineers for various teams, and who isn't? For PR reasons which I do not have more information on, we are not allowed to show our name on the sponsors list, but we are a "platinum sponsor" for PyCon 2013. Apple keeps it simple. Sorry we don't have more goodies.
Ah crap! Haha I changed the repo forgot to change it here. Thanks!
I don't see that listed anywhere. Where are you guys having it?
I have not seen a single person with any reddit gear on. I don't think you guys are as easy to find as you think you are!
Having some of those projects opensourced would be cool. Other than that, I don't know what code they could have shown in this article.
I noticed when I click the sci tab the whole header shifts 
[Tastypie?](http://tastypieapi.org/)
They were busy fighting off an incursion by the forces of desktop Linux in the borderlands. 
Def use requests. Since you're writing a client, tornado won't help you. Use gevent or eventlet to make async calls to the API (though you'll prob not need parallel API calls). It sounds like you want call chaining (service.object().association().other_attributes(). You can do that by easily returning new instances ala Django's query set or similar constructs in SQLAlchemy. There's probably some frameworks that could help you build classes that chain method calls (though you're doing little more than returning self).
It's fairly easy to write a pythonic client wrapper that hides all the actual http ops of your rest library. I just wrote on of these last week; basically you use abstract base classes to create a dict like object that holds your rest objects and another list like object to hold collections. All of your normal dict methods map directly to the CRUD methods of a restful API, so it really is just two fundamental class types to represent everything. Any additional non CRUD methods against those objects may require a bit of polymorphism to ensure that your class objects present the correct set of additional methods. The important thing to remember is that a RESTFUL api is at it's core objects and collections of objects. 
I think OP is asking about a client wrapper, not a server. Not sure how tornado would help for that.
There are a lot of people at the conference! I believe I'll be spending most of tomorrow checking out the posters (after Guido's keynote, of course!)
No, you're totally right. I've spent my whole weekend nose deep in textbooks and am sorta fried. PRAW doesn't use SQLalchemy at all, the reddit application does.
...is for creating server APIs. The OP needs to write an API client library.
&gt; Why does no one seem to talk about boto ( or any devops tools ). Meant to bring this up but the two impressions I've gotten is that DevOp's tools aren't sexy outside of DevOp's circles and the other is that it feels like some companies still feel that their deployment strategy puts them at a competitive advantage to their competitors. On that second theme, DevOp's is kind of abstract but also specialized field. Most of the company's I've worked with see their DevOp's as not being directly involved in making money but instead as a business expense. Whatever the DevOp's people do come up with is hard earned, so there's a feeling of wanting to keep it classified secret. Unlike say application developers, the DevOp's people know 100% everything about what the company is doing at any given time. Another time and place I worked for an advertising broker that tried to lock down that information so that no one but a very close knit core was informed. That's just one experience so I could be biased but it wasn't the only one like that. One last thought, I've been actively looking for a full-time DevOp's position that meets some specific criteria. During that process I've run into a lot of companies that don't even realize they need a dedicated DevOp's/cloud engineer and instead the task has been passed around to the most under booked engineer in their team. When I've pointed this out, the next question is what exactly a "DevOp's" role looks like and for each company its sometimes been dramatically different. In summary why people don't seem to talk about their usage of Boto or similar is either because of protectionism, people are just using the tools enough to make it work for the moment, and then its only when a company reach's sub-netflix/reddit scale does a company think about getting a dedicated devops person. Each one of those points probably cuts down the crowd from thousands to maybe a few hundred. Every year I've attended PyCon and gone to a DevOp's themed BOF it is generally the same people ( a few Google/Youtube engineers, Netflix, and similar ) and its almost always the same size ( not growing or shrinking ). 
Check out Donald stufft's slumber project
Author of the article here. Didn't know this. Will update the post based on your comment. :-)
a poet and didn't even know it
Whoa. Have an upvote.
Hi, Yup, @dfndoe is correct - we already have the server side. We want a client wrapper. A couple of other people here have been pointing to using gevent to handle the async side. Interesting - Jaime Gil de Sagredo's Finch project uses Tornado on the client side: https://github.com/jaimegildesagredo/finch http://jaimegildesagredo.github.com/2012/12/26/a-python-restful-api-consumer.html Not sure what the reasoning of that is, versus just using Gevent straight - what do you guys think? Also - Kenneth Reitz, who wrote Requests apparently has a "requests+gevent" project: https://github.com/kennethreitz/grequests/ I don't know much else beyond what's on that page - curious is anybody has tried this. Cheers, Victor 
Hi, issackelly - I mentioned Slumber in the OP =). However, is isn't async - and seems to hit each request serially. Hence, I thought an async approach might be better. Also - the REST service we're using is quite "chatty" - I know for a fact that in it's current iteration, there's a fair amount of back-and-forth to get basic data. Cheers, Victor
Oy. Now its obvious that I skimmed too much. Sorry. Requests + gevent would be my next choice probably. Maybe twisted.
Currently it requires you to be on *nix/OSX (requires /usr/share/dict/words), and will sort based on a possible word's frequency in the english language.
You know what this needs? Regex matching. Then it could solve crosswords, too.
Feel free to branch, if you add an optional third parameter, I'll roll it in :)
Just kidding. Implemented it real quick. Tested it with '.' wildcards. https://github.com/jamesmunns/CrosswordSolver Try it with '..tt.r' to find 'letter'. My Regexp is rusty, let me know if you have any improvements/bugs.
You could just do if re.match("\^{0}$".format(regexp)) or something like that.
The easiest way is the splinter framework. [Here](https://github.com/metaperl/compound-hrs/blob/master/go.py) is some code I wrote using Splinter.
Try this in google for projects already using pandas: site:github.com + "import pandas" -github.com/pydata/pandas (The "-github.com/pydata/pandas" drops the main repo from returning in the results. ) This google for ipython notebook with example pandas code: site:github.com + "import pandas" + ipynb 
http://wiki.python.org/moin/RssLibraries
Cool, thanks for sharing I will def. check that out!
What do you consider to be a fine example of a more "current" design?
Looks great. I wish that the example would (automatically) scroll to more examples.
This might be nitpicking but the example on that page use the letter `l` for the variable name. It's hard to tell the `l` apart from other characters, I think some other character like `L` would be better suited.
The font-family in Firefox 20.0 on Ubuntu 12.10 for code examples is for some reason overriden by the default stylesheet so it's no longer a monospaced font but a default serif! font. But it may be my profile because if I create a new one it works correctly. Otherwise, the preview looks great! 
? Mobile compatibility was around in the early 2000s?
I know that it's not easy to structure and design that much content but the way they did it might be confusing and too complicated. I'd also like to see the new corporate design. It's not consistent and clean enough imo. Overall it's a very old fashioned approach.
I like the look of it. It's very clear. And responsive too!
yeah, the page specifies “Courier”. While at least not “Courier New”, it’s still a pretty old-fashioned monospace that doesn’t fit into a modern site design like that. good thing i don’t have that installed and see a lovely “Source Code Pro” in its place, complete with perfectly distinguishable lowercase `l`. (but only because i care and mapped my system font alias “monospace” to that font: something few will have done.
Just came here to say the same thing. I was also hoping that I could type into the box to experiment with the language (I don't know how easy it is to sandbox Python, but presumably you could run one of those python&gt;javascript engines to give people a taste). edit: also show news item dates in the form 9/Dec/2012 because [American m/d/y dates](http://i.imgur.com/cwZVr0a.jpg) aren't easily readable to international viewers.
I like the progressive enhancement and not relying on font icons as the sole way of conveying information. Not sure how I feel about the "this is why Python is cool" example being more prominent than the blurb that actually describes what Python is. 
I just viewed it on my phone, and it's not bad. I'd like the option to view the desktop version. 
That was exactly my first thought too!
Or, perhaps not naming variables as single characters at all, which is one of the first things most programmers learn when writing maintainable code.
Curious, what would you consider a modern approach?
Use of single letter variable names need to go away. People lean on it too much after initially learning Python. I have a hard enough time convincing the brilliant mathematicians and scientists I work with to use real names. When they post this to github (said during the keynote this morning), I'm definitely going to submit a pull request with something slightly more verbose.
You'd likely use whatever async core fits your environment. If you're already using tornado or twisted or whatever, you should use their reactor core instead of gevent. 
Tornado has a generic event loop and an async http client, why is @Xavdidtheshadow downvoted?
- hoverintent on the second menu - awful font in the white area - overall too much color/gradients
Digging the redesign so far. Definitely more inviting and navigation has been improved massively over the deeper nested previous site. Much better/easier to use.
That would be even better, but anything is better than mm.dd.yy...
http://www.codestyle.org/css/font-family/sampler-Monospace.shtml
since the css is “Courier, monospace, …”, that means that most people who don’t own a mac and on the new python page will just see their standard monospace font (most have Courier New, but not Courier). so i’m not exactly sure what you want to tell me here.
The recommended download should be like a beacon on that page. A hero download button which guesses the right version based on user agent.
&gt; tornado won't help you. that finch library uses it, so if the OP decides to use it, then it would seem that tornado is actually helpful. also finch uses requests. i hope I get massive upvotes for having mentioned tornado and requests. 
Responsive design, like it
Wrong. &gt; The cached template loader is a class-based loader that you configure with a list of other loaders that it should wrap. The wrapped loaders are used to locate unknown templates when they are first encountered. **The cached loader then stores the compiled Template in memory.** The cached Template instance is returned for subsequent requests to load the same template.
nice overview of cython and ctypes regardless of the skeptical benchmarks. 
Anybody knows what framework is used?
&gt;never, never, never, ever judge performance of ANYTHING using fibonacci. Just stop I understand that you feel strongly about it, but it's effectively useless advice if you don't provide an explanation or a link to support the claim.
I think it was at RubyCon, upstairs.
I think in this context, dd/mm/yy would be better as it puts the most relevant parts of the date first.
Absolutely appreciated.
Overall there's great focus on the big reasons people will be visiting the page, but I'm not sure about hiding downloads and documentation inside menus. There's also some seeming confusion about whether these menu items are the main way to navigate to these features, or whether it's the four Get Started, Download, Docs, Jobs widgets below. Those widgets contain fairly small links. I would have expected those to be fat targets, styled like buttons. I immediately tried to hover the headings, thinking they'd be linked. Hiding social network sharing mess behind a menu is nice.
The day is the least relevant part of a date.
Yeah, [Python's style guide](http://www.python.org/dev/peps/pep-0008/#names-to-avoid) specifically recommends against using l for a variable name.
Browsing the source: looks like Django! I just started django about a month ago and I &lt;3 it so much i could just die.
All you nitpickers can eat it - this is a million times better and more usable than the current site. Well done and bravo. Of course it won't be *perfect* immediately, but this is a very well designed new look. Edit: Why is everyone's immediate response to find all the flaws?
Yes and no. Yes it's more than just a file cache. It loads the node tree in memory, but doesn't generate an AST or similar representation that would speed up the rendering process. That's why you see other comments of 10k method calls per request that could be drastically inlined. Everyone should enable the cached loader in production (it does speed up your response time dramatically), but Django should really shift towards Jinja's internal architecture.
Not only is Turkey not part of the EU, the idea terrifies most Europeans because it would mean having a muslim-majority country as part of the union.
I'm the author, so any suggestions for making this better are welcome. Also, *many* thanks to redditor BicubicSquared, who authored the original messymind.net code that forms the basis for this. 
tornado is overkill but you mentioned gevent? they seem either equally useful or equally overkill in my opinion. 
Looks like django indeed ~~(if we're to believe the HTML comments)~~ yea it's got the django admin http://preview.python.org/admin/ : &lt;span class="pre"&gt;Copyright &amp;copy;1990-2013&lt;!-- Insert Django date tag to keep year current? --&gt;.&lt;/span&gt; I started django about 3 or 4 years ago and I don't &lt;3 it that much, but I admit it serves its purpose well.
Hi! Finch was developed to remove the boilerplate related to consuming HTTP APIs and provide an abstraction to build REST API clients. In my opinion, one of the most important Finch features is that you define the API resources in a declarative way before consume it. Basically you model your resources and how they should be parsed and encoded, and leave Finch to do the repetitive work. As mentioned, Finch uses the Tornado Async HTTPclient to consume the HTTP API. This is so because I started developing Finch to consume APIs from a Tornado web server, so I wanted to fetch them asynchronously. Anyway I plan to support more http clients like requests, and should not be hard to do it.
when i say: let's meet on the 3rd you know exactly on what day we'll meet. but when i tell you to setup a meeting in april, we'll propably miss each other. so the day is kind of the most relevant part of the date. you can just infer the month or year.
Yep, Finch uses Tornado on the client side and on the server side, that depends on where your code will run. For example, if I consume a certain REST API to provide another API then all the code will run on the server side.
Great basic style, will definitely use it!
That's fair.
It's interesting, but as a heavy user of ggplot, what makes it interesting and useful is the syntax for specifying plots by adding together their parts, not the color scheme.
YYYY-MM-DD is an ISO standard, all "parts" of a date are relevant.
Yeah, async sounded a bit unnecessary given the situation described. Gevent is easier to integrate into an existing project since it can monkey-patch existing libraries while Tornado or Twisted tend to require you implement your I/O into their event loop/reactor model. Unless you're using tornado or twisted already, I'd advocate going with gevent for async/parallel processing. After the monkeypatch, you can usually use the now gevent-enabled services with little to no extra code. That's what really makes gevent so useful as a general async solution since you can continue write your code in a serial fashion for the most part (Disclosure: I've never tried to use gevent+requests).
I really like this project although I think it could benefit from some sort of time stamping without cluttering the experience.
The only problem I see is that the hover menu might be annoying. Moving your mouse from one place to the other, if you want to select something from below after clicking, if you wanted to right click somewhere, copy something, you'll always find that unwanted menu in the middle and spent an extra effort to get out of the drop down menu from hovering. 
Yeah, but if they say Python 3.x.y was released on the 3rd, I don't care about that. If they say it was released in 2013, that does matter to me, because I don't want a version of Python from 2012.
Hi, Curious - do you think you could run through the reasons why you went for Tornado on the client side, as opposed to just say straight Gevent? Also, in the Github Finch example you're using Tornado both for the sample code and within Finch itself? Cheers, Victor
This is pretty cool. Mind if I try to re-implement this in Ruby?
We've created an online IDE with python support. You get semantic analysis and autocomplete, at least as well as we can do given python's dynamic nature. We also support the Python version of Google App Engine. Check it out, see if you like it, and send us some feedback. http://devtable.com/
It also sorts nicely. That's a nice bonus as a prefix for filenames (think logs) and such where a default alpha-num order nets you a chronological one.
Thanks a lot! My first crude effort worked! The problem is that it's a bit slow with Firefox (default). The splinter documentation says that there's support for something called "zopetestbrowser". I'm not exactly sure of what that is but looks like a python embedded browser with no GUI. That could make it fast because it can cut the time of opening the GUI.
Out of curiosity, which web framework do you `less than three` the most? (Coming from someone who also isn't a huge fan of Django.)
Or when using a debugger
Because EVERYONE is a user interface/user experience/design expert
Hmm... what's the first package in that grid?
OH SHI! Forget that I asked...
It's bad. Especially the nav bar. What a mess... Harsh gradients, heavy shadows, ugly fonts, misplaced whitespace, margins and paddings on almost every element are completely different, several vertical guides in places there should only be one (look how the top nav, "Socialize" button, primary nav, and masthead are all on a different vertical guide), etc. etc. etc. The idea is there: the color scheme is on track; the content seems laid out in a decent hierarchy; and, hell, it's even kinda responsive (this is broke too). But the entire implementation is completely amateurish at best. I know there was a big fuss about who got to do the redesign, but this is just sad. Python is a great language, probably the best language, and *this* is the most professional thing the agency you selected could come up with? I'd offer a free redesign/development in Django, but no one ever takes things like that seriously. Meh... enjoy the ugly redesign everyone. You'll grow to hate it after a couple months.
Wierd - my windows install freezes every time I try to open this page in Chrome. Looks amazing under Linux, great work!
The list example uses 'l' a lower-case 'ell' as a variable name which can be mistaken for other characters and is only one character long. Since it is part of an example that is the main focus of the page it might be good to change the name to something like 'a_list' or 'my_list' and secretly avoid clashes with PEP-8. 
If you find django's orm restrictive, have a look at Flask with sqlalchemy. 
Faster to parse maybe, but not searchable. Maybe in a few years, Google will understand "two years ago", but not yet.
Where is the in-browser interpreter the proposals put forth?
Maybe you should take a look at [HAProxy](http://haproxy.1wt.eu/), don't be afraid by the doc, it's very easy to configure.
Im sorry if I offend but Im sad to see python get a half-assed job for a redesign. The overall design is a bit boring and looks like a bootstrap template. A website without its own design is a website with no design at all, nothing sets this apart. Python for me is minimalistic, non-bloat and beautiful, this should be reflected in the design. * Rounded corners and gradients are a bit outdated designs for a site being made today. * Dropdown menu style feels mal placed and doesn't fit in with the rest of the site. * Hover effect on menu are to narrow and mouse over the menu items should change the content-box, it *looks* like one kind of menu but behaves like another. * the zero margin between the menu and the box is annoying. * The menu is duplicated, twice, at the bottom, thats not pythonic. * And this needs to be stressed, *dont use both 1 and l in the front page tutorial, cant you see how similar they are?* 
True, your solution is much better.
hoverIntent works awful with touch screens and should be avoided at all costs, hover-effects aren't acceptable at all today, to the degree that js-authors are starting to remove the hover-event. Design for the least common denominator, dont *assume* i have a touchscreen, mouse, camera, kinect or similar.
so... why not have both? Reddit comments use the nifty [&lt;time&gt;](http://html5doctor.com/the-time-element/) element (hover over the "x hours ago" for post date), this solves the problem well I should think
Yep, that's a good solution. One of the many things reddit does right.
I have no idea how this works. I guess I better start writing some buggy code and testing it this way.
&gt; I'd offer a free redesign/development in Django, but no one ever takes things like that seriously. The proposal seems to actually have been done in Django, so fire away. I must say that I'm agreeing with a lot of your points.
yyyy/mm/dd is best for archival purposes but for something that's going to be updated every few days at least it's more helpful to know which day it was posted than which year.
I made changes to matplotlibrc file to get the desired effects (mainly inspired by Huy Nguyen tutorial on sane color scheme for matplotlib). But, I usually have to plot 12 lines in a chart and it gets very difficult to distinguish then from one another. Here is an [example](http://imgur.com/Xs1JvCW) I will give HUSL a try.
Not at all, thanks!
It's not yyyy/mm/dd but yyyy-mm-dd. Reliably following a common convention is much more important that coming up with new "best solutions" all the time.
That's your browser's responsibility, as it's neither easy nor necessary to provide a switch if you have a site with conditional stylesheets. my browser can request desktop mode by faking a bigger client width and that's how it's done.
Because nobody is an island
I'd do this if PSF would confirm they'd actually take it seriously.
Many mobile browsers do not support this option yet, so it remains the developer's responsibility. "It's not easy" doesn't seem like a compelling excuse here. 
Hi Victor! Well, the reason why I used Tornado is that I started using Finch in a Tornado web application, so make async request was straightforward. In the other hand, Finch really does not use Tornado at all. In the Github example I use Tornado because the AsyncHTTPClient needs the IOLoop to be started in order to perform requests. But if you use another http client, gevent based for example, you won't need Tornado. Implement another client should be easy. You only need to implement the same interface than the [finch.Session](https://github.com/jaimegildesagredo/finch/blob/master/finch/session.py). Anyway I'm working on more clients and synchronous support.
Raise the bug report with the maintainer of MVPA! I'm sure they'd appreciate the report.
The linked page lists font coverage stats by operating system; enabling design of more widely appropriate CSS monospace font style rules. The use of Courier indicates a Mac-centric design. Is there a reason for not just specifying `monospace` to respect local preferences? EDIT: http://www.w3.org/TR/CSS2/fonts.html#generic-font-families : &gt; Generic font families are a fallback mechanism, a means of preserving some of the style sheet author's intent in the worst case when none of the specified fonts can be selected. For optimum typographic control, particular named fonts should be used in style sheets. &gt; &gt; All five generic font families are defined to exist in all CSS implementations (they need not necessarily map to five distinct actual fonts). User agents should provide reasonable default choices for the generic font families, which express the characteristics of each family as well as possible within the limits allowed by the underlying technology. &gt; &gt; User agents are encouraged to allow users to select alternative choices for the generic fonts.
&gt; why don't you try piwik This looks cool, posting here to look later!
you can do this in your ini file too. http://pythonpaste.org/deploy/#composite-applications
This is just being fussy, you'd have to count every line of kernel code too if you were using this theory to calculate LOC
Your function names violate [PEP 8 naming standards](http://www.python.org/dev/peps/pep-0008/#function-names) 
In [changeVowel.py](https://github.com/garytse89/Python-Exercises/blob/master/autoCorrect/changeVowel.py) the call to `word.lower()` is suboptimal? because each call is recomputing the same thing? for line in dictionary: if ( line[0:-1].lower() ) == word.lower() : # convert user input to all lower Also no need to indent by 8 spaces - 4 is common. 
Awesome, thanks!
Perhaps it's just because I'm old, but I hate when things move on their own. It's distracting if I'm not looking at it, and if I am looking at it I probably wasn't ready for it to move yet.
Well I still use Django for many things, however when I have the choice, I use Pyramid. I find it very lightweight and you get to choose the components you like.
Nice article. I'd suggest two things to consider: 1) Just because Python has a module to deal with Foo, doesn't mean the command line foo program is now off limits. You would get better performance from this pipeline: bzcat logfile.bz | python your_script 2) Regexps looks deceptively simple, but a lot can go underneath. if you you have a simple file format that you know, you could be better off with just .split(). If I compare this: bzcat example.log.bz2 | python -c "import sys; items = [int(x.split(',')[-1]) for x in sys.stdin if ',cpu_usage,' in x]; print float(sum(items))/len(items)" to you parse_log.py, it runs in 5.25s (wall clock time) vs 11.48. This is on a 15972512 bzip2 log file generated by your scripts. "cython -a" output looks nice -- I am still on Pyrex, where the optimization procedure is to let Pyrex generates its C code then hand-read it. Without that you risked complex Python code running when you thought C code would run. I think Cython has improved a lot on that. However my Pyrex code has been working for quite a few years, and if it ain't broken.. 
I tried learning it, but dropped eventually. I know it's supposed to be better, but I just never really got into it. Can't say I miss it too much, programming is not about typing speed anyway. Also, if you _do_ think programming is about typing speed, go die in a fire.
I had a friend/co-worker that changed his keyboard layout to Dvorak. He said once that felt like he was typing faster and with less mistakes. But, on the other hand, I said the same thing when I changed my keyboard with a Das Keyboard. So, YMMV?
1) Great point...I completely missed that! This is an excellent addition to make to my article and the power of the Unix "small, simple programmes talking over pipes using text" philosophy. 2) I agree that a simple string operation completely annihilates regular expressions, however (I think) people would eventually use "more complex" parsing that needs regular expressions, or even a full lexer/parser. I went to all the trouble of re-writing the C example using PCRE because I wanted to show people how to use external C libraries as well. 3) The "cython -a" output is even better than it looks; if you double click a line it shows you the C code it generated, side-by-side with the corresponding Python code. It's quite enlightening!
It's because you're old.
It wouldn't be the first such thing I did like that the kids today are into. What with all the time they spend walking on my lawn and all. . .
The benefits are mostly from relearning how to type properly and giving up bad habits.
wow, I missed that. I always end up using l for my temporary lists. I can't just call them list and and calling them L seems weird since they're not constants.
Run that sucker through [flake8](https://pypi.python.org/pypi/flake8) and [pylint](https://pypi.python.org/pypi/pylint) too. Not only do the names violate pep8, the style deviates from it a *lot*. If you want to check if a sequence is empty, use len(), do't compare it to an empty list. also, when checking if something is None, us the 'is' operator, not '=='. Lines 89-91 of autoCorrect.py do nothing that the for-loop follow them can't handle, and there are no circumstances in which 'revisedMatches' will have the value 'None'. You don't need a temporary variable when performing a swap in Python, so rather than this: temp = repeats_c[r] repeats_c[r] = repeats_c[r+1] repeats_c[r+1] = temp Do this: repeats_c[r], repeats_c[r + 1] = repeats_c[r + 1], repeats_c[r] Taking lines 200-201 as an example, you don't need to do this: for r in range(0, len(repeats_c)): if( r+1 &lt; len(repeats_c) ): Do this instead: for r in range(0, len(repeats_c) - 1): That way you eliminate the repeated check, and the code has the same effect. You have code all over the place for checking if something is a vowel. That stuff should be in its own function, even if it is just an expression. The 'consonantPriority()' and 'vowelPriority()' functions are near duplicates of one another. You should refactor out the common code. Rather than doing the likes of this: i = 0 for elem in seq: # ...do something with elem and i... i +=1 Use the enumerate() function: for i, elem in enumerate(seq): # ...do something with elem and i... You're using a lot of paired lists with ('repeats' and 'indexOfRepeats' being examples) where you might be better off using lists of tuples. For instance, a more straightforward implementation of your repeatedLetters() function would be: def repeated_letters(word): repeats = [] prev = "" for i, letter in enumerate(word): if prev == letter: repeats.append((letter, i)) prev = letter return repeats Doing things like this means the code becomes *much* simpler. For instance, your repeatedLetters(), consonantPriority() and vowelPriority() function become this: def repeated_letters(word): repeats = [] prev = "" for index, letter in enumerate(word): if prev == letter: repeats.append((letter, index)) prev = letter return repeats def group_repeats(repeats): vowels = [] consonants = [] for ch, i in repeats: if ch in 'aeiou': vowels.append((ch, i)) else: consonants.append((ch, i)) return vowels, consonants def prioritise_vowels(repeats): vowels, consonants = group_repeats(repeats) return consonants + vowels def prioritise_vowels(repeats): vowels, consonants = group_repeats(repeats) return vowels + consonants The data structures you use can have a big impact on the complexity of your code. I'm going to leave it at that, but it should give you some ideas on how to improve your code.
Maybe to go along with that... Not everything needs to be speedy. The critical portions for performance are often centered around user experience and number crunchy. And the user experience can often be adjusted so that your performance problems are hidden while the user is making a decision. I think this always goes back to the "don't optimize early" rule. You can always polish it later after you have it functionally working.
A couple of other things: you should comment your functions using [docstrings](http://www.python.org/dev/peps/pep-0257/). If you include example code in your docstrings (and you should), you should make sure that it can be check using the [doctest](http://docs.python.org/2/library/doctest.html) module. You shouldn't use doctests for unit tests though. For unit testing (which should should be doing), consider using [nose](https://nose.readthedocs.org/). You should be doing unit testing as the tests allow you to (a) document your expectations of how the code should work and (b) catch any regressions if you accidentally break something. nose makes unit testing very straightforward.
also try PyVot: http://www.youtube.com/watch?v=Oi3QKuFugWk
That's a really good answer.
It just says "Going faster with Python" and, I assume, your name. (Actually, in IE it's just blank.) Clicked everywhere and nothing happened... Edit: NM, figured it out. Arrow keys. I guess it just never occurred to me that a Web page would only be navigable using the keyboard...
I'm getting this too... odd. Any plugins? Adblock? Edit: AAaaaaannd I'm an idiot. Use the keyboard keys! Edit 2: Looks like simultaneous realization.
Turns out, you have to use the arrow keys.
It usually takes a bit of time for the volunteers to transfer, edit, and upload the videos... I see one video up on PyVideo so far, give it maybe a couple weeks?
You should just get a smaller window inside of intellij that will show the prompt once you run the script. Then you would type in the input you're testing with, and whatever actions you have that follow should occur. I just double checked in my session before responding, so it should work the same for you.
Clicking on the right hand side advanced to the next slide for me (on Firefox).
I'm intrigued by the prospect of using kcachegrind with python profiler output. Is there a version of lsprof2calltree that works under Python 3.3? If not, I might try to convert the script myself.
The most appropriate way to display is always the language's or the user's locale, because that's what is expected by the user. But, since this is a programmer / technical site, I would expect users to be familiar and supportive of global standards when communicating information.
I think last year they were up within about a week of the con ending. The sprints are occurring the first few days of this week so the volunteers might be busy with that.
You're missing one little, but very important thing, if that's the exact code you have at the moment. You've defined your function, but you aren't doing anything with it, so when you run it in intellij idea, there's nothing else for it to do. Hope that points you in the right direction. 
Pros: * more comfortable typing * probably fewer errors * somewhat faster typing Cons: * Once I taught my fingers dvorak, they completely forgot qwerty, so now when I sit down at someone else's keyboard, I have to hunt-and-peck (unless I can easily switch it to dvorak, then switch it back when I'm done). Note though that dvorak is the keyboard *layout*. (And, there are [other layouts as well](http://mkweb.bcgsc.ca/carpalx/).) If you *also* want to switch to a keyboard that's far easier on your hands, check out the [Kinesis](http://www.kinesis-ergo.com/) Contoured. 
C go. Ekrpat xgy C-m bry yday irre ay cy f.yv
I'm really excited about this one. I think async/await is one of the few really neat things that has been coming out of MS recently. This sounds like it is pretty much the same for Python.
The punctuation is in weird places. Not that good for programming.
Their website just took me back to 1997. ..but I'll admit that keyboard is sexy.
They are going after the nodejs async model. I think it was about time, since a lot of users are migrating their apps / choose node for exactly this reason.
it's an alternative to even doing any parallelization. it allows you to *think* of things in parallel and then run them sequentially.
So still only one core gets used? Sorry I'm not getting why that'd be advantageous (I'm mostly interesting in doing numerical or GUI stuff in parallel, not networking BTW).
&gt; If you want to check if a sequence is empty, use len(), do't compare it to an empty list. alternatively, empty sequences are False so you can just go: if sequence: which I believe is sort of a python idiom. On the other hand, explicit is better than implicit...
Asynchronous programming is really nice as an alternative to creating background threads that wait for I/O to complete. For doing computation on multiple cores you will still need multiple threads or processes.
You can see in this blog post the advantages of this approach. http://codescience.wordpress.com/2011/09/05/non-blocking-io-node-js-and-pythons-eventlet/
I've used it for about 7 years. I don't really type much faster than I did with qwerty, but my hands hurt way less after a day of coding. The only annoying thing is that it takes a while to learn IDE shortcuts.
So this is primarily of interest to people doing networking/server stuff?
Yes, or a maybe a graphical user interface that should remain responsive during file operations.
The keyboard itself hasn't changed much in forever either... I think adding USB was the most recent change. It's amazing how backward this company is. But they're pretty much the only game in town. Maltron is the only similar alternative, and they're in the UK, even more expensive, and uglier.
Just a quick note: I work for Continuum Analytics, here are a couple of more links. I put them on the article too. Python along with NumPy and SciPy have been used for numerical coding applications for years. NumbaPro allows users to make NumPy code even faster. Here are some links for further information: http://en.wikipedia.org/wiki/NumPy http://continuum.io/blog/simple-wave-simulation-wi... http://continuum.io/blog/the-python-and-the-compli...
&gt; Particularly if they come before this Wednesday ;). Typo, double "is", slide 41. Twice.
Holy...thanks! That's embarrassing.
And, in the specific, log processing is one of these things that really isn't too performance sensitive. You're either doing it real time -- in which case you shouldn't be processing too many entries at any one time -- or you're batching it up, in which case you may well get away with a blinking cursor once a day/week or so. There's nothing wrong with being quick, but it's worth doing a cost/benefit analysis before you start working.
You're both right, these are excellent points and thanks for making them. I was **really** hoping to finally make a Python optimisation article / presentation that didn't do Fibonacci numbers, factorials, Mandelbrot sets, or something delightfully abstract in that vein. Rather than focusing on log parsing, or the deliberately poor code and idioms present, I'm hoping this Wednesday to help my co-workers take a step back and apply the **principles** I'm talking about to their code. In my article I devote several pages to when optimisation is appropriate. In many circumstances it simply isn't, and never will be. Computers are cheap and people are expensive. However, even Donald Knuth, in his deliberately misconstrued quote, *emphasised* that there are critical times when we must optimise. I'm hoping to help people rise to that challenge.
Upvote, that's the official response to GIL criticism from Guido: Python threading is designed to do what threads were originally supposed to do, I/O - http://www.youtube.com/watch?v=EBRMq2Ioxsc
It doesn't matter who came first, in this instance. It's a matter of who filed first. The PSF didn't have a filing in Europe and PO Box Hosting Unlimited did. Not sure what the details were, but I'm glad that they decided to work together.
The big drop down menu takes up a lot of screen real estate and the mouse-over dropdown is just annoying - at least wait until I hover or click to open the menu. 
Holy white, my lab is going to love this! We've been looking at buying a few Titans to expand our neurocomputational research but we're big on using Python since a lot of the post docs we get come from a biology background and don't know any C. This is freaking perfect. Any idea when this is launching, or what architectures it'll support? 
Guess they did it for some free publicity...
Yeah, that's a great approach. I've been trying to think of a non-abstract, inherently performance sensitive task and I'm drawing a blank. I really think it's anything that emerges once you've written a program and run it through a profiler and that's exactly what your slides show.
Just to be clear this implementation has a add_callback_threadsafe so threads can talk to the event loop, (you can spin off a task in a thread, then have it callback to the event loop when it's done), though that was only mentioned briefly. (the task thread could then run it's own event loop if it needs to do async io, but that could get a little iffy) Personally I find that a nice way to write code.
Website does not appear to work? All I get is "going faster with python" in grey on light grey, then nothing. What do I have to install, and why are they requiring me to do so? The URL I am getting is http://www.asimihsan.com/presentations/going_faster_with_python/index.html#1
since there is stuff like typekit and the google font api, there is no reason to use generic names *as fallback* anymore, as long as it’s a bit important that a specific font is used. you can always use one of the many freely available web fonts which are guaranteed to be displayed for almost all users. mostly if it’s not important that the users see your font of choice (e.g. for small body text, or monospace on non-programming-centric pages), designers opt to use a default font stack without webfont like in the olden days. for your headline font and big code sample on the frontpage of a programming language, the few kilobytes for two webfonts should be spent. PS: with “no reason to use generic names *as fallback*”, i meant that web designers can always choose to use generic names in order to let their users decide which monospace, serif, and sans-serif to use. font specs like “1em, sans serif” allow the user to control 100% how the body text of a page should look like. but for code, i’d never specify “monospace”, as that’d mean “DejaVu Mono” and “Courier New” for non-mac users, which are both ugly. PPS: my monospace fonts of choice are [Source Code Pro](http://blogs.adobe.com/typblography/2012/09/source-code-pro.html)^(Ctrl+F “Availability” for download), [Envy Code R](http://damieng.com/blog/2008/05/26/envy-code-r-preview-7-coding-font-released), and [Ubuntu Mono](http://font.ubuntu.com#charset-mono-regular). Monaco and Consolas are nice, too, but afaik proprietary and not freely available.
More that the same AV team is working PyData, so it will be a bit slower this year.
Well the end result was (I believe) not unexpected but no details of the settlement were given. Did PSF pay or was it a case of: if you're after tech start-ups for customers, making Python change its name is going to be bad for you (btw my dad's name sounds a lot like "Oogle")?
Are you using Microsoft Internet Explorer, if so which version? What happens if you try Google Chrome or Mozilla Firefox? Or if you're in a work environment and prevented from installation other applications you can try [Google Chrome Frame](http://www.google.com/chromeframe?prefersystemlevel=true).
IMO it's best to provide this kind of approach directly in the language, not via 3rd party libs (gevent, eventlet, etc). In this way, it can also be very well optimized. Also, I think the model described in the slides is stronger than the one node has (yield from &lt;future&gt; vs endless callbacks). Hope this will arrive very soon!
BTW I believe the USA has just switched to "first to file" from "first to invent"? (I know this is an EU case but I guesses you were old school from USA since euros probably know how the euro patent system works)
CFFI vs Cython is not totally fair, because you can interface with the same C code using Cython - I guess C+Cython would be faster than C+CFFI in CPython (and much slower in PyPy) in this case.
&gt; Are you using Microsoft Internet Explorer WTF? LOL, that's a really rude question!!! :-) If I connect to this webpage: &lt;script type="text/javascript"&gt; document.write (navigator.userAgent); &lt;/script&gt; I get this: &gt; Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:19.0) Gecko/20100101 Firefox/19.0 Do I need cookies or flash or something?
Thanks. ~greatest common factor vs browser preferences.
Sorry, I'm not trying to be rude! I've only tested this on Google Chrome 25 and Mozilla 19 on Mac OS X and Windows 7. So I think this should just work for you! There's no need for cookies, and I think the presentation just uses HTML5 and CSS3, nothing Flash-y. Hmm, I guess you're using a lot of extensions, is NoScript one of them? Would you mind trying Google Chrome on Ubuntu and seeing what happens? 
SO MANY METHODS. seriously, wasn’t there a more elegant way to do some things as slapping a new method onto the thing? why `loop.add_signal_handler`? fuck, i don’t want to write C or java! there are so many more pythonic ways… * current state of PyQt/PySide: `btn.onclick.connect(callable)`. * or how about `loop.handlers[signal] += callable`? /edit: ahah, `add_signal_handler` even is misguiding! it’s a setter, no adder. besides the fact that setters are per se unpythonic, this one is especially silly, as it’s called “add” and thus implies addition instead of replacement: i’d recommend `loop.handlers[signal] = callable` (for calling without args) and `loop.handlers[signal] = (callable, *args)` and `del loop.handlers[signal]`. /edit: [here is the discussion for everyone interested](https://groups.google.com/forum/?fromgroups=#!topic/python-tulip/d7f2SOzPp7Y). at first i missed the tone, but guido is a nice guy and allowed me to prove i'm no asshole :)
Yes, you're right that Cython can interface with the same C code. And in fact Cython can use inline C code, just like Weave. More here: [http://docs.cython.org/src/reference/compilation.html#compiling-with-cython-inline](http://docs.cython.org/src/reference/compilation.html#compiling-with-cython-inline). But why would C+Cython be faster than C+CFFI? I'm confused on that point. I stuck with CFFI because I was secretly hoping the grande finale of my presentation would be PyPy+CFFI kicks everyone's ass, but I was foiled by `bzip2` performance. In hindsight I probably should have piped in the output from `bzcat` or written a `libbz2` binding in CFFI.
OK I understand partly why it doesn't work now. I'm on a device with a 1024x600 screen - a netbook to be specific - and the webpage doesn't know to resize itself ... So, I realised I had to fiddle with zoom and after I do this the whole page appears - I see the title, then Asim Ihsan below it. No amount of clicking or refreshing does anything though.
No, it's me! It's my fault if I put something up and people can't use it. I'm going to add instructions.
Fixed now, see my other posts!
&gt;Was his product named Python before Python existed? - &gt;There is a company in the UK that is trying to trademark the use of the term "Python" [...] Specifically, it is the company that got a hold on the python.co.uk domain 13 years ago. At that time we weren't looking a lot at trademark issues, and so we didn't get that domain. ^^[1] And according to wikipedia Python (programming language) was conceived in the late 1980s, its implementation was started in December 1989 and it appeared in 1991. ^^[2] ____ ^^[1]: ^^http://pyfound.blogspot.com/2013/02/python-trademark-at-risk-in-europe-we.html ^^[2]: ^^http://en.wikipedia.org/wiki/Python_(programming_language)#History 
Thanks for the heads up - we've been using mixedCase by default in my OOP class at university when I took it.
Sorry, changeVowel isn't used in my code anymore, I switched to using regular expressions when I found out about it and it blew my mind.
Yeah, but if you changed to a Das Keyboard *with* Dvorak you'd type even faster! I switched to a mechanical keyboard pretty recently too, and I've been loving it.
Thanks for mentioning nose! I definitely needed that for development, instead of always getting screwed by random test cases. I actually panic a little bit when I test my code because those buggers keep coming up, then I have to trace through all that spaghetti code to find out what I did wrong. Hell, often it takes 1-2 hours each time I type up a bunch of new code. 
Django 1.5 on Python 3.3
&gt; If you want to check if a sequence is empty, use len(), do't compare it to an empty list. You're totally right. Don't know how I missed that. &gt; Do this: repeats_c[r], repeats_c[r + 1] = repeats_c[r + 1], repeats_c[r] So I'm guessing the first pair of variables, repeats_c[r], gets assigned first? Also, in many cases I needed some sort of counting variable to increment += 1 with a 'for' loop, is there a shorter way to code that in? I'll look into the other suggestions as well. Thanks for the tips.
Very exciting! Thanks so much for pointing `numba` out.
Among other things this approach makes it much easier to use multiple processes. ... Which is not very useful for working on shared in-mem data, though.
I have used openpyxl in the past, very easy to work with.
&gt; directly in the language It's more of a standard than an implementation, and even then it's not implemented "directly in the language" any more than Tornado is (and with C extensions and stuff the 3rd party libs can be very well optimized too, and sometimes even more optimized than builtins - see `cdecimal` for example).
if you want, I can turn on my web cam and ramble about the talks I went to. :)
Translation of your comment: Any application using blocking I/O.
This is what I would like to know as well 
&gt; So I'm guessing the first pair of variables, repeats_c[r], gets assigned first? Actually, what happens is that a tuple is constructed of the values on the right side, and it's assigned to the tuple on the left side. When you use a tuple like this, the technical term is that it's a 'destructuring assignment' as it breaks the tuple on the right side up so its contituents can be assigned to the variables in the tuple on the left side. It effectively happens all at once, which is what allows destructuring assignment to be used to swap values. &gt; Also, in many cases I needed some sort of counting variable to increment += 1 with a 'for' loop, is there a shorter way to code that in? If you're iterating over something at the same time, the best route to go is to use enumerate(), which will yield a sequence of tuples paired with elements of the sequence being looped over. For instance: &gt;&gt;&gt; for i, ch in enumerate('abc'): ... print 'i=%d, ch=%s' % (i, ch) i=0, ch=a i=1, ch=b i=2, ch=c It's not so much a matter of being shorter, but avoiding possible errors. Writing the code to maintain the counter yourself (assuming very short variable names) will be shorter than using enumerate(), but it's also more error prone: the line of code you don't write is a line that doesn't have a bug. The itertools module covers a lot of common iteration patterns. It's very rarely that I end up having to use range() (and xrange() on Python 2) directly.
When I'm coding, I use [watchdog](https://pypi.python.org/pypi/watchdog)'s watchmedo command to watch for when I save files, and when I do it has nose run the test suite for me. It saves me a *lot* of time and catches a lot of stupid errors I might make. That way, I get practically instant feedback so I know I'm not screwing anything up. I also save regularly and check code in regularly. That way, I've got a nice safety net that allows me to build code incrementally.
sqlalchemy is next on my list, it's been highly recommended and from the little I've read the getting starting docs, it looks amazing.
You mentioned some of the the functions in the git repo aren't used anymore. You should delete them so we know what's worth looking at and what's deprecated.
You can run scripts in iPython using the %run magic function. So something like [1]: %run your_script.py You can also use the debugger by: [2]: pdb It'll toggle the debugger on. My work flow with iPython is usually something like: * Try an idea in iPython * get it to work * transfer it to a script or module * make sure the script/module works 
Idle does have command history, it just has rather bizarre key bindings. Alt-P will cycle back through the commands, and Alt-N will go forward.
You're right, I am a moron.
I ran into Ian at the social event on friday and knew nothing of his work before hand. The man is a crazy genius and I look forward to learning more about his work. Plus he was very polite to my intoxicated friend :)
Tab completion.
genius plugin. Any tool designed to make you more like Gary is worth a nod in my book. Great TiP BoF this year.
If you like IDLE, check out [IdleX](http://idlex.sourceforge.net/). It fixes a lot of the weird choices in IDLE, such as the command history.
I wouldn't say pil is or was dead... Us plone folk have depended/needed PIL for zope to function properly forever it seems. I always found it annoying (or didn't understand) why that secret labs weren't making releases what more than a year apart at most? Give it up to Alex Clark et al to get PIL easy to use (eggified etc) and almost Py3 compatibility. Perhaps this will encourage other python frameworks to use it as well?
i did. he reconsiders the add→set change, but: &gt;Because a big point of the standard is to provide an interface that &gt;can easily be adapted from e.g. a Twisted Reactor or the Tornado I/O &gt;loop. The more we pretend there are lists/dicts that you can access in &gt;arbitrary ways, the more semantics we introduce implicitly that may be &gt;hard to emulate for those adapters. i proposed that we add convenience proxy objects which use those methods to expose a better api (e.g. `self.signal_handlers = ProxyObject(); self.signal_handlers.__setitem__ = self.set_signal_handler`), but he doesn’t like that idea (probably due to more than one obvious way to do it)
Sure! You might want to start by checking out RabbitMQ.
&gt; The keyboard itself hasn't changed much in forever either... That's a good thing. :) (Wait, I mean, if you're saying that the *Contoured* hasn't changed much in forever (that is, since they designed it), then that's a good thing, IMO.) &gt; I think adding USB was the most recent change. Yes. And, I think, the choice of black in addition to the beige. &gt; It's amazing how backward this company is. Don't know what you mean. I've had only good experiences with the company. 
I have no idea why someone would downvote you for that comment. It's the feature I most enjoy from ipython and a vim python setup.
with &gt; [1]: %run your_script.py how do I make the ipython shell (I get to it from the dos-type command line in windows) know where my scripts are stored?
Also if anyone needs to show you something or borrow your computer you always forget to switch it to querty for them first which causes a lot of confusion. 
You can change to the directory with the script using the shell command cd, then run the script, so cd /script/directory %run script.py Or you should just be able to do %run /path/to/script/script.py 
Just wanted to say thank you in advance. This is going to be very useful.
I didn't know that my files deleted from local repo would not delete from remote repo, still new to this haha. Used git commit -a -m to do the trick.
Any progress on this? 
IPython's notebook finally let me break free of the evil grasp of MATLAB. I much prefer the Mathematica-style notebook paradigm, so to me the IPython notebook is like MATLAB in the much nicer form of a notebook.
Guido is a bit wrong about gevent. &gt;So, about gevent... &gt;* Scary implementation details – x86 CPython specific stack-copying code &gt;* Monkey-patching – "patch-and-pray" &gt;* Don't know when it task switches – could be not enough – could be unexpected The first two bullets are pretty much right, but task switching is completely deterministic in gevent, in contrast to typical threading. It will switch in the same order every time given the same inputs (this means identical network conditions...which of course won't happen in practice, but this will be an issue no matter the async framework you use), and you can control switching more directly by accessing the `hub` directly or by using `gevent.sleep()`.
I won't lie. My number one reason for using IPython is because I can type "quit" instead of "quit()". Just using IPython as a substitute for the regular Python interpreter will make your life appreciably better. My second favorite feature is the IPython cluster. It has some *really* powerful cluster computing features that make large-scale computations much more convenient.
I used to think it was overrated, because mostly it just provided tab completion and a handy help system, which other interfaces (like [bpython](http://bpython-interpreter.org/)) also do. It has more features (like distributed nd parallel computing support) which I don't think most people use. IPython Notebook is awesome though, especially since there are now websites that let you run them in the cloud so you don't even need a local Python installation (e.g. [Wakari](https://www.wakari.io/), [PythonAnywhere](https://www.pythonanywhere.com/), etc.) Be sure to check out [IPython Notebook Viewer ](http://nbviewer.ipython.org/) BTW, it justifies the entire project IMHO.
Not always true. I've been processing a million records every ten minutes for years in Python. For a fairly heavy process with a number of lookups, validations and a lot of error-checking - it takes about 500 seconds on a slightly older server. So, I split it into 4 fragments, process in parallel and have it down to about 120 seconds total duration. Soon, I'll have to look up 2 million ip addresses against ISP blocks - which I know will easily double the total amount of time consumed by this process. I have been planning to explore a faster server, greater parallelism, profiling, Cython, etc. 
I used PyOpenGL back like 5 years ago for an art installation. Today I may use PyGame or maybe Jython or Clojure and myriad Java Libraries. What kinds of things might you get into?
It's pretty, but it looks completely unusable. Why do you need an iPhone friendly website for something that's only going to be used by people on a desktop?
I have used this. They have recently made some huge improvements, so now would be a good time to get involved if interested. The docs are very user friendly, http://www.vpython.org/contents/docs/index.html. What exactly do you hope to achieve? This way I can tailor feedback regarding vpython. But if all you want to do is use the build-in shape then I can recommend it. If you need more advanced features like constructive solid geometry then look somewhere else.
The conference ended *yesterday*. While the A/V team is awesome, they do need to sleep on occasion, and uploading everything does take time.
Very nice detective work. Mysteries like this are always very interesting to read about.
&gt;I’d recommend loop.handlers[signal] = callable (for calling without args) and loop.handlers[signal] = (callable, *args) and del loop.handlers[signal]. Why? In my eyes its not any more readable.
I get pretty much all of this code, the only part I don't get is the following: if not all(type(p) is str for p in para): raise ValueError('parameter unpacking is not allowed in __init__') When would you get parameter arguments whose naming doesn't come out as a string? Also, would this fail with unicode argument names? Maybe it'd make more sense to say `issubclass(type(p), basestring)`?
Looks cool, I will definitely try it out
The only problem with a clever optimization like this is that it confounds good developers trying to get a handle on performance. I'm glad you included that one had better not take advantage of such an optimization because it's not part of the official API. If at some point they need to switch Python interpreters, they would be S.O.L.
This looks great! thanks
Thanks for elaborating on your views. I visited the site again. Unfortunately, I don't think this is a good redesign, mainly because the mobile experience is poor. It's a step backwards. First, I'm briefly greeted with the "Python Network" menu, which lingers for a few seconds. Just as I'm taking that in, it jumps to another section of the page. I'm not sure what I'm looking for or what I'll find, so I begin scrolling. And scrolling. And scrolling. The page is just a long vertical list without any form or structure. There's no way to know what's on the page without an excessive amount of scrolling through the entire thing. This makes for an unpleasant experience, in my opinion. Then I loaded the python.org in my mobile browser. The text is too small, but I can take in the entire page at a glance, get my bearings, and zoom in to just the content I want. Maybe legible text and page structure are mutually exclusive on small screens.
I just wrote a thank you email to Veber Ltd. and apologised for the [hate mail](http://www.theregister.co.uk/2013/02/19/python_versus_veber_trademark/) they received from some in the community. It seemed the right thing to do.
Where in your code have you assigned a value to the variable 'letter'?
I actually realize I haven't done that, but am confused at to what value I should actually define to 'letter' for it to be logical.
 def determine_grade(test1,test2,test3,test4,test5): if letter &gt;= GRADE_A: return 'A' should probably be def determine_grade(letter): if letter &gt;= GRADE_A: return 'A' 
Two separate things: tools and process. IPython is a great tool. It sounds like your current process is a loop: * code * manually test * repeat Test driven development is all about automating testing (writing 'tests' first). Tests make assertions about inputs and outputs. A TDD feedback loop looks more like: * define domain and range (input and output) [problem statement / use case / user story] * code test * run test (fail) * code unit * run test (pass OR repeat) A unit test tests an individual unit of source code: * http://getpython3.com/diveintopython3/unit-testing.html * http://docs.python.org/2/library/unittest.html https://github.com/flavioamieiro/nose-ipdb can assist you with http://en.wikipedia.org/wiki/Test-driven_development . nose-ipdb is an extension for https://nose.readthedocs.org/en/latest/ which launches IPython as a debugger ( https://pypi.python.org/pypi/ipdb ) From IPython: %run nosetests --ipdb !nosetests --ipdb 
I tried that earlier and got an error message. Tried it again and got this error message: Traceback (most recent call last): File "/Users/cameronforeman/Desktop/test_average_foreman.py", line 52, in &lt;module&gt; main() File "/Users/cameronforeman/Desktop/test_average_foreman.py", line 26, in main determine_grade(test1,test2,test3,test4,test5) TypeError: determine_grade() takes exactly 1 positional argument (5 given)
That would depend where and how he decides to define 'letter' (it would be unnecessary to pass it as a parameter if he made it a global variable), but probably, yes.
I'm guessing you want the average of the 5 tests to be given a letter grade? In that case def determine_grade() should be def determine_grade(letter) and pass in average you get from calc_average() *typo*
In IPython: %logstart -o log_input_and_output_to_here.py %edit? # (`q` to close) %ed? %edit -p %ed -p ? `%edit -p` will launch $EDITOR and execute the entered code on editor exit. Re-running `%edit -p` will re-open the same block of code.
That's a definite possibility and I initially thought that, but the question is kind of ambiguous. It reads "This function should accept a test score as an argument and return a letter grade for the score..." So, maybe you're right and it just wants a letter grade for the average, but I was under the impression that it wanted a letter grade corresponding with each of the individual test scores.
Ah, that's definitely a possibility. Then you should just call determine_grade() 5 different times, not just once with 5 arguments. Having changed the function to def determine_grade(letter):
Sorry, your sentence was ambiguous. I thought you meant that there was, indeed, a Python equivalent linked to on that page that they should have used. Anyway, much of the information that GA does can only be done with Javascript, so there really can't be a Python equivalent. 
alright, I'm going to try that out really quick. THANK YOU FOR YOUR HELP!
Yep, no problem. Good luck
Everything worked out, so I appreciate your help!
we gonna fix that eventually, don't worry
"exit" works too
It feels like adapters dictated their way to an interface indeed. If I want Tornado, I can just use it really. I was amazed as well at the massive amount of methods, I got lost reading the presentation and the PEP to be fair. We'll see but for now, I can't say I'm buying this.
If the problem can't be more narrowly defined than "numerical stuff", I'm not sure the choice between libraries really matters...
This covers a little known Python 2 feature called "parameter unpacking" (see PEP 3113). &gt;&gt;&gt; from inspect import getargspec &gt;&gt;&gt; getargspec(lambda a, b, (c, (d, e)), f: None) ArgSpec(args=['a', 'b', ['c', ['d', 'e']], 'f'], varargs=None, keywords=None, defaults=None)
I have used Numba recently. The JIT compiler can only compile a small subset of python/numpy code, but the performance is very impressing. IMO the best way to use it is to keep the jit function small and only has the time-consuming part. Keep other fancy part in pure python. 
This value object is mutable. If you modify it, its hash will change. This means it can't be reliably used as a dict key, member in a set, etc. The result of __hash__ should be stable. Python built-in value types are immutable (hash = hash of value) Python built-in mutable type (e.g. containers) are unhashable (raise TypeError) User-defined classes are identity-based (hash(obj) == id(obj)). In all cases has is either stable or raises an exception. Value types should be immutable.
One thing to notice, these methods are all low-level intended for use by event-loop/adapter creators. To use a conforming event-loop one would only need to remember two entities: @coroutine/@task and "yield from". Well, maybe one or two methods to delay execution also.
Quite true. 
This is great for Django people :)
I saw that coming. [Value objects *can* be mutable](http://c2.com/cgi/wiki?ValueObjectsCanBeMutable). Philosophy behind this implementation is that if you *want* it to be hashable, don't mutate it; if you *want* it to be mutable, don't use it in sets or dict keys. This is how most of my value objects end up being. But I can see that it might too ad hoc, and there should be separate `Value` and `MutableValue`. 
You should check-out [lambdax](https://github.com/erezsh/LambdaX) for a more complete solution.
Apply your programming skills to build fun things
Where is the video?
&gt; A for-loop in Cython will do a fast (C-speed) iteration over a python list (which is just a C-array of pointers internally). Do you think the usage of `map(process_line, f_in)` similarly avoids the round-trip in the CFFI case, because `map` is a built-in C module? This is what the `dis` module is telling me. This is great detail for the presentation tomorrow; it seems I've been quite unfair to Cython and need to do more research.
There's also non-numpy-specific Cython which can transparently compile some of the python code/modules for higher performance (but less debugging/introspection ability); or allows writing more optimized code (including numpy-using one). And there are some similar projects. I don't think either of all those is "the most promising" in general.
There's more to it than readability. I chose that because dict access is a common metaphor for binding stuff to names/symbols and releasing those bindings. Also it's more obvious in that only one binding per signal can exist at a time. Finally it is only one thing to remember: a dict named `signal_handlers`, as opposed to two methods with more or less arbitrary names: was it add? bind? set? And the other one: Was it remove? delete?
That's right. I did that, by implementing a Tulip compatible event loop which uses pyuv instead of the stuff in the select module: https://github.com/saghul/rose and I liked the fact that everything worked after I replaced the guts, so the abstraction level seems appropriate. The Tornado author also did an experiment using Tulip as the guts for the Tornado IOLoop: https://gist.github.com/bdarnell/4582282 Of course it'll take time for this to take off due to the use of "yield from", we'll see, but I'm liking it so far.
Is there any site that you would recommend to learn Dvorak? I have found one with a simple google search, but I didn't know if there was a preferred method. 
object?? gives you the whole source code for object too!
Most of the time you should focus on using vectorized functions. setdiff1d, isin, etc... don't go reaching for another tool if the loop if avoidable. Then focus on the algorithm. Is it correct? Do you have unit tests and oracle-like tests verifying the functionality? Theano is more of a framework. It's hard to install. It's amazingly fast and does some very advanced stuff, such as automatic(not numeric or symbolic) differentiation and numerical stability tests. It's quite different from the rest of the ecosystem. I haven't seen it used outside of neural networks. I bet you could make an awesome gibbs sampler in it... maybe with an interface like JAGS? I haven't used NumPyPy recently... Numba is the simplest to use. Still though it's a heavy dependency. I haven't seen a deployed package with it. I like it a lot though. It's missing a lot of functionality, but it looks incredibly promising. Other options: Bitey. Use the llvm to compile c functions and call them directly. Definitely the easiest link from C to Python. Works in IPython as well. Cython. It's kind of a pita. You need a good grasp of C to optimize all of the python overhead away. But it's very forgiving. Just adding types to your program can give you massive speedups. Throw a few unsafe function decorators and you get pretty far. It compiles to a C extension, so it's as if you wrote the code interfacing python's c api, meaning package builds don't require cython only a c compiler. This is the only one I've seen deployed in codebases.
And for those who want to break free of the evil grasp of MATLAB, but still enjoy the fancy MATLAB interface, Spyder is a very good IDE for scientific python.
Mother of god...
`%paste` is also on my short-list of favorites!
Pandas is using numpy arrays and therefore will not speed up your calculations one bit. It is an awesome library nonetheless, because it makes working with large data sets just so much more pleasant.
I admit I'm having trouble seeing the difference between this an Twisted. What problems does Guido's approach purport to solve?
What's wrong with plain old lambdas?
No, but that is something that I am thinking about implementing. I am just not sure which syntax would look best.
Kind of. You put your ipynb files in a pastebin somewhere else, give it the URL, and it renders a static HTML view of the notebook. People share nbviewer links instead of direct pastebin links, so you can see their notebook without having to load it into your own server.
I really think that Numba is the most promising for fast numerical processing in Python, not only because they're are now well founded, but also because when they use LLVM, they also get for free the LLVM optimizations and all LLVM flexibility (LLVM has a well-known and documented IR lang), which are by far better than the optimizations done in PyPy.
Got it. That *is* pretty cool!
This was a very fun read. I wish to achieve this level of mastery of my entire stack at some point.
What you say is very interesting, as I've been considering Numba myself. Do you maybe happen to have a simple code snippet you might what to share to illustrate how numba works?
The slides quite appropriately mention xkcd/927... twice. The Guido's approach is meant, among other things, to make the mess more pythonic and less mindfucky. Can't say it does very much in that direction, though (as compared to what has already been done and mentioned in this thread).
Thanks!
cython interfaces pretty nicely with numpy. The only caveat is you need to be somewhat fluent in thinking in c like terms to get the most out of cython.
The interesting thing is that loops in Numba can outperform some vectorized NumPy function (e.g. [3x faster summation](http://nbviewer.ipython.org/3914904)). 
while pandas is using numpy underneath, Wes has done a ton of work to make sure the common data frame operations are as optimized as possible using a bunch of cython. So if you're problem falls into the domain of data frame operations pandas can speed things up over rolling your own using numpy.
Does anyone know how easy it is to add functionality to Numba for other libraries? For example could it be possible to give numba PySide functionality? Building python QT apps that compiled into C++ and also use NumPy would be incredibly useful.
Very impressed with the pictures you guys showed during the lightning talks and how far you got with the kids. Nice work!
Numba is an alpha product with a lot of potential. You can't integrate numba with non-numba code in the same class, at least as of a month ago. This means you really have to separate the code that does numeric operations from anything that operates on dicts/lists/strings. That makes it hard to structure your classes sensibly. It should eventually be a nicer alternative to fully typed cython, but expect bugs and breaking changes. Use cython if you really need something to work this week. Theano is great because it does (Nvidia) GPU optimization and automatic differentiation, but there's a steep learning curve. You'll run into things you can do in numpy but not theano, like advanced indexing. The error messages are cryptic. Performance can be unexpectedly bad if you write something that has to copy data on and off the GPU, and you have to learn a fair amount to debug that kind of problem. Once you get used to it it's great, but it's like learning a new language. 
[Coming soon](https://twitter.com/gvanrossum/status/313713600399278080)
If you had slides for this, I would really like to see them. I run a side class for homeschooling highschool students.
IDLE desperately needs an improvement and I'm really surprised at how neglected it's been. Even things as simple as pressing up/down to access previous commands is missing. I mean, how hard is that to implement? Not to mention some basic autocompletion. Python is known for being an easy language to get into and the default editor should be the same.
&gt; This means you really have to separate the code that does numeric operations That's already pretty common when implementing some sort of numeric algorithm, so I don't consider it a deal breaker. Theano seems to be a lot more specialized, and may not be worth investing too much time in (at least for me).
Dude. Dude wat. TL;DR I need new pants.
Thanks for your work! BTW will Pillow remain a separate project, or is there a plan to re-merge with PIL?
I guess to try and simplify what I mean. Perhaps I have a python module I've written. It has a few classes and has a couple functions written in C. How do I go about using this with numba so that I can get the advantages of JIT and having the end result be compilable LLVM bytecode? Basically, how do we as a community work to extend Numba to support more than Numpy? ...perhaps I'm missing something about how Numba works.
I think at this point they're mainly focused on compiling code which uses loops and NumPy arrays. You can still [interface with external code](http://numba.pydata.org/numba-doc/dev/doc/interface_c.html) apparently, but it'd take some additional work. After playing with Numba I'd love to see a full featured Python -&gt; LLVM compiler.
What I mean is you have to separate the numeric code from its configuration because you can't take string args the way scipy does. Theano is most useful for machine learning.
&gt; Theano is more of a framework. It's hard to install. It's amazingly fast and does some very advanced stuff, such as automatic(not numeric or symbolic) differentiation and numerical stability tests. It's quite different from the rest of the ecosystem. I haven't seen it used outside of neural networks. I bet you could make an awesome gibbs sampler in it... maybe with an interface like JAGS? The [next version of PyMC](https://github.com/pymc-devs/pymc/tree/pymc3) is being built with Theano. So… yes, you could!
True, but that's usually good practice even without Numba.
I am totally going to join the people who want to fix IDLE. I got several cards at PyCon. Yay for finally having something to contribute to core!
I'm going to do another post that's more about how to run a class, which will include our slides and such. I'll make sure to post it to /r/python!
Thank you!
At the end of the class, we gave the kids an hour to do what they wanted. They all opened the books we gave them and started typing examples. Had this been adults, we totally would have been on Twitter :\
Numeric datatypes, large arrays, nested loops, complicated indexing, vectorized function calls, etc. Basically the sort of things you use to implement algorithms. And both Numba and PyPy can make a huge difference with these vs. normal Python.
An even more complete version of what nodejs has has existed in Python for a long time: Twisted. Tulip is an attempt at standardisation.
It makes it incredibly easy to share research work, including live code. There's also a guy writing an [entire book](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) on Bayesian methods using just IPython Notebook, which is pretty damn cool.
my team is using mayavi2. it's pretty easy to use.
To deploy Numba, use the Anaconda Python distribution https://store.continuum.io/ . note:I work for continuum. 
Wow. I bet this completely annihilates the main loop. That's awesome.
The automatic derivatives let you do full full Hamiltonian MC a la [Stan](http://mc-stan.org/), so you can go well beyond Gibbs sampling. We live in fun times.
Indeed! Check this out: http://www.dabeaz.com/coroutines/
I don't know of any websites to refer you to, but I can tell you what worked for me. A lot of the articles I've read about learning Dvorak talk about pulling up your keys and remapping them. I've never done that. My desktop has a natural keyboard so can't swap those keys and I paid too much for my laptop to risk damaging it. I printed out an image of the Dvorak keyboard layout (something like [this one](http://ruscoe.org/blog/2009/02/trying-dvorak-as-a-programmer/)*). I taped it to the wall just above my monitor so that I could refer to it as I typed. For the first week, I spent a few minutes each day typing pangrams (sentences that contain every letter). There are dozens of lists of them on the web, [here's one](http://www.fun-with-words.com/pang_visitor.html) to get you started. Another thing that helped me quite a bit was instant messaging. I normally don't do this very much, but found that it was really helpful for learning Dvorak because not only do you have to type, you usually have to type quickly. I think that time pressure was good training for me. So while I was learning Dvorak I'd also chat with my wife or coworkers more frequently than usual. I think (*hope*) that if you give it a week or so you'll like it. You'll start to notice a difference pretty quickly. I can honestly say at this point that I'm never going back to Qwerty. * Its a coincidence that the best image I found is in a blog article from a programmer trying Dvorak.
&gt; The code uses Numpy for efficient computation on arrays, but it is still not good enough. For merely 64 particles, the average framerate is just ~2 fps on my Core 2 Duo. How is that possible? That's, like, _15 million_ cycles _per particle_, a pure Python implementation should be a thousand or so times faster than that...
Wooaah. Automatic differentiation is incredible. Theano is the first time I've seen it outside of C++ and Haskell. The only real beautiful example I've seen is Ceres, but it only had forward differentiation. Oddly enough it computes the derivatives at runtime, while Theano computes them at "compile" time, if you can call it that.
Accessing previous commands isn't actually missing, it's just bound to a really odd keyboard shortcut by default. I'd love it to be fixed, though. If you want to help make it better, the [Python developer guide](http://docs.python.org/devguide/) shows how to get started, and I believe most of the code for IDLE is in [Lib/idlelib/](http://hg.python.org/cpython/file/default/Lib/idlelib) in the source tree.
Numba has a way to compile a Python file to a shared library, [called pycc](http://numba.pydata.org/numba-doc/0.7/doc/pycc.html). I don't know how complete it is.
those numbers are meaningless
&gt; Accessing previous commands isn't actually missing, it's just bound to a really odd keyboard shortcut by default. I'd love it to be fixed, though. Well TIL. It's Alt-N and Alt-P (vim-like?). &gt; If you want to help make it better, the Python developer guide shows how to get started, and I believe most of the code for IDLE is in Lib/idlelib/ in the source tree. Thanks for the links. Seems like a two line change [here](http://hg.python.org/cpython/file/4c6463b96a2c/Lib/idlelib/config-keys.def#l22). Issue opened from before [here](http://bugs.python.org/issue2704).
Kudos, Katie and Barbara!
yeah, I was thinking a for loop would work, but I couldn't really figure out in what way to do it. Hence, me being new to programming. Thanks though.
That's true. The problem is that I can't tell the difference between: map( X.get_this_attribute, some_list) And X.call_this_function(with_some_arg) It looks exactly the same: \_\_getattr\_\_() and then \_\_call\_\_() So I chose to interpret it as the first one.
Then what if you want to send an array into your lamderp function?
I honestly can't be bothered, and I don't really care about the result. Jinja2 will be faster, I know it. I might do it tonight if I get really bored and wonder how much Jinja2 beats Django by, otherwise don't count on it.
If you're doing it on a domain-specific level, it might be easier to edit your hosts file and point said domains at localhost.
Ah, yep. Is there any way to clean up the syntax then? The ._() is kind of ugly. Maybe Xf.call_this_function() and Xa.get_this_attribute? Then "on/of X" is unambiguous, and set apart from "using X" which would still be just X..
Sure, it's possible, but it doesn't feel prettier. If you're so inclined, you can declare your own class and override call: class Xf(X): def __call__(...) ... It should work. Edit: Just to explain why I think it's a bad idea, consider what happens when you want to call a function, and then access an attribute of the return value. Or call the return value. You can't escape the choice, only defer it to a smaller likelihood.
So, now I'm feeling incredibly stupid... I can't figure out how to upload an ipynb file. What am I missing? Does my ipynb file need to already be online (e.g. github or dropbox)?
what sort of feeding instructions does a raspberry PI come with? can I keep it in the cage with my python? or will the python mistake the Pi for food? Does the Pi? have a solid exterior that could withstand the full force of constriction from an adult python?
I learned off [this tutorial](http://gigliwood.com/abcd/lessons/). 
I tried this, but the problem is that I am hosting on a specific port on local host, and the hosts file will not accept an entry that has a port number included: 127.0.0.1:1234 www.google.com
I don't have a choice what my work puts on my computer and allows me to install
&gt; The entire discussion regarding keyboard layouts assumes that everyone learns to type by walking into a classroom and learning things like the correct finger to use for each key, which seems wrongheaded no, its not wrong, since different layouts only matter for the ones who touch type. you can give a randomly generated keyboard layout and to a hunt-and-pecker and he will be as happy as he was with qwerty after he learned where eacy key is in the new layout.
NumPy performance depends on which BLAS library is being used. I think the default binaries use ATLAS, which is not that fast compared to, e.g., Intel's MKL and GotoBLAS (BSD licensed, now forked as OpenBLAS). I hope someday OpenBLAS will be used in the default installation.
Surely that doesn't matter for the case of summations and other simple vectorized functions?
Thought that was what you're trying to do? You can of course run IPython Notebook locally, and even share your server with other people. You can also use one of the cloud apps that support it (e.g. [Wakari](https://www.wakari.io/)).
BTW, on stage you mentioned some younger kids were having trouble with the typing. What do you think is an optimal age to get kids started on this? The Python for Kids book says 10+ but there are reviews out there from people with younger ages. I have an 8-yo who is interested in building things and has shown interest in computers. I don't want to start him too soon and have him get frustrated.
Not much to add, but I was really impressed with the Numba presentation at Pycon and it got me interested in the LLVM and learning how python code translates into the LLVM IR in a way that gets you the huge optimizations. So I've been trying to learn the IR so I can better understand Numba's contribution and better understand compilers generally. So far I haven't found a good tutorial for the IR though.
Oh, no no! I run ipython locally and do all the number crunching on my own machine. I was hoping to use the IPython Notebook Viewer as a pastebin of sorts, i.e., a place where I could upload my ipynb files and share the link with my immediate collaborators. The problem is that if I have to host my ipynb file as a gist, then it's automatically public -- bad news for unpublished data.
LLVM optimizations are low-level, PyPy does high level optimizations that are impossible to write without knowledge of the programming language. The LLVM optimizer is also quite slow.
Looks like it's Python 3. Might want to specify that somewhere since not everyone has moved to 3 just yet.
Good point, if upper limit is not a problem then _1, _2, _3... could work
Agreed. This is kind of a neat hack, but because plain old lambdas are built in, Python programmers will be able to understand them at a once. Also, in the case of the examples given on the GitHub page, it's more Pythonic (and significantly faster) to use list comprehensions: even = [n for n in range(50) if n % 2 == 0] 
Numba can JIT calls to ctypes or CFFI functions. So integration with C++ really available yet (it is limited by the FFIs we have for Python), but it's planned in the future (perhaps in a way similar to Seamless). The goal is also to intergrate seamlessly with other projects that expose their underlying data through a well defined interface, based on https://github.com/numfocus/sep/blob/master/sep200.rst . This would also likely include all Cython code (through modifications to the Cython compiler). This means that type inference and fast calling of entirely external, unknown code, becomes possible.
Damn, I was just wondering yesterday why his website was down. RIP Malcolm.
It was good to meet you :-) Thanks for the link here (and pycon is always about the beer!). I have an updated analysis of #pydata with a poor-mans Noun Phrase Detector which pulls some signal out of the current PyData tweets, to be posted in a couple of days.
Great idiom. Thanks for sharing!
I remember speaking to Malcolm over the phone about a commercial project I was considering for a company I worked for at the time. He was kind, patient and offered plenty of great advice and feedback with regards to the viability of using Django for it. He will be missed. RIP Malcolm.
Right, but what are some of the reasons why people dislike the syntax?
What's with all these python devs kicking the bucket? The matplotlib guy, one of reddit's devs, and now this guy. If this continues, I'm switching to Ruby! Jokes aside, this really sucks =/
Context: [MetaFilter: FAQ: *What does a single period in a comment by itself mean?*](http://faq.metafilter.com/#4): &gt; It's MeFi shorthand for a moment of silence and is usually used in obituary threads. 
The article delivers what it promises: having read it, it is clear [how to get oneself banned from PyCon](http://en.wikipedia.org/wiki/Cannabis_%28drug%29). 
I met Malcolm in person when Django was very young. I wanted someone to prototype an app for me and met up with him in Hornsby, Sydney maybe 7 years ago or more?. How he didn't laugh in my face, I'm not sure but as you said he was kind and patient and definitely full-on into Django (and previously a Gnome Developer)
Looks interesting. To make it easier to call and simpler, I'd recommend renaming it to something short in the style of other Unix utilities. "grist" or "grst" or "slice" or whatever.
I posted this to another r/Python post and to kcunning's blog entry, but: [IdleX](http://idlex.sourceforge.net/). Fixes a lot of quirks that IDLE has had for a long time. Works in 2.6, 2.7, and 3.x.
I think numerical computation will move in the future totally to GPU, and therefore, it makes more sense to see python ability to leverage such new medium. Compyte seems the most promising project on that front.
Relatedly, my wife (and I) homeschool our children. Any tips for teaching programming to homeschooled kids?
Ah, well what I do in this case (I use a bunch of ports to host different web apps I'm developing, plus fake domains to make them easy to remember) is use nginx to proxy_pass the connections based on the hostname.
A public apology would have been appropriate. So not cool on so many levels.
Thank you. After reading this my first thought was being disappointed it's only for two years. 
It doesn't seem they worked together so much as Veber capitulated in utter and total defeat. 
You just need to change your console into whatever directory has the files. In your example, your console was in C:\Python27 so you would put the file there. If you want, you can just create the file from your console, to make sure you know where it's created: C:\Python27&gt;echo print 'hello world' &gt; test.py If you want to check what's in the file at console, you can use: C:\Python27&gt;type test.py print 'hello world' And then to run it: C:\Python27&gt;python test.py hello world 
hi, http://pyvideo.org/category/33/pycon-us-2013 they have it already now
I was a big fan of the GCD/blocks approach, but after playing with async/await for a while, I think it feels a lot more natural without the extra syntactic sugar. Glad to see Guido going down that path.
If you're doing a GUI app, doing lengthy operations will often slow things down and make the app appear sluggish. One of the benefits will be to allow Python to pump the native GUI runloop and have things move along WHILE the heavy operations are going on. Heavy operations could include lengthy calculations, networking, database, etc. this lets the system give the UI a chance to periodically update itself. This is a trivial example, but in practice most users will think an app is frozen and will kill the process. This lets UI actions interleave nicely with other events. As Python gets more into the front-end (including gaming or updating webviews) it will become more and more important to have this.
Being able to interleave GUI operations with heavy tasks will make UIs defined in Python appear much more responsive. It directly impacts what I'm working on, so I'm extra appreciative of the timing. This presentation laid out the whole behind-the-scenes thing. Most end-users will only have to deal with a very small subset of the API, which is as it should be.
Add is there because C# has proven the += model of adding handlers allows you to have multicast event handlers. = assignment is more like a javascript single callback handler or Obj-C delegate. One callback at a time. Add is much more future-proof and way more functional in the long run.
Then what would you call the 8 other scripts in the set? And in any case; alias slice=gristle_slicer 
I'm new to python as well. I open my python files by right clicking them and selecting "edit with IDLE" and then I press F5 to run the program.
And we're all sponsoring his next illegal narcotic intake through blogspam. Awesome. 
This is great. It's way nicer to use than grep + cut. Thanks!
You could also be overhead making a joke to a friend by a psychopathic forking dongle with a camera and a twitter account. Of course that'd cost your job too.
Thanks for that trick, I didnt know f5 ran the program. But my cmd quits as soon as the program begins running.. granted, it's just one line of code that says print "hello" but it wont stay open. any ideas?
Try `raw_input()`. For example: def test(): print 'hello' raw_input() That should do.
That language is totally inappropriate for this subreddit. Please try to be respectful of other people. EDIT: The parent comment was edited, it originally said something far more vile.
The first place it looks is your current working directory. You put your files in C:\Python27, but are trying to run them while you're in C:\Users\Cameron. It's looking in C:\Users\Cameron, and they aren't there. This is probably overkill, but just so you understand when you go to tackle this problem again with imports... Python searches for modules in a certain order, defined by the "Python path". You can see what's in the python path by checking sys.path: &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.path ['', '/usr/lib64/python26.zip', '/usr/lib64/python2.6', '/usr/lib64/python2.6/site-packages'] (the empty string '' means it looks first in the current directory)
I usually prefer [`Logger.exception()`][1] for debugging. [1]: http://docs.python.org/3/library/logging.html#logging.Logger.exception
If you really wanted to do this right you'd use subclassing, but I don't understand it well enough to explain and I wouldn't want to spread false information. You have a generic Pokemon class that let's you create instances for individual pokemon. We can use this class as a simple template: class Pokemon(object): def __init__(self, name, type): self.name = name self.type = type From here you need to initialize an instance of that class. players_squirtle = Pokemon("Squirtle", "Water") enemys_charmander = Pokemon("Charmander", "Fire") For simplicity, let's assume there are only Fire and Water types, and you want to check if your enemy's Pokemon is a Fire type (making your moves super effective). You can now access your object's `type` variable by using `objectname.variable`, so you could find Squirtle's `type` by using `players_squirtle.type`. First let's make a dictionary to easily find what types are weak against each other. weakness_dict = {"Fire": "Water"} Now you can check to see if your enemy is weak against your attacks. Obviously in a real program your dictionary would be more complex, and this would error out if you tried to find a "Water" key in the dictionary. if weakness_dict[enemys_chamander.type] == players_squirtle.type: print "It's super effective!" You most likely won't know the names of the Pokemon battling, so you'll have to write a function that passes those names as parameters. def check_weakness(weakness_dict, player_pokemon, enemy_pokemon): if weakness_dict[enemy_pokemon.type] == player_pokemon.type: print "It's super effective!" # etc
True, the map function can fast loop over the list but the process_line function is still called by the python VM using python semantics (arguments are python objects which must be un-boxed and the return value must be converted into a python object). Cython can get a further speed up by calling a C-function using pure C semantics. This would give the compiler a chance to do a more agressive optimisation of the loop. I should add, the benefit of this might be negligible if the process_line function is really CPU intensive (i.e. if the python calling overhead is small, reducing the overhead might not be worth the effort). Finally, FWIW (and while I'm havin a Cython-fan-boi style rant) ... a further really cool thing Cython can do is let you create Extension classes (python builtins) as easily as creating a regular python class. These can have C-speed methods but can memory management to the python VM.
In most of the NextDayVideos the sound sucks. They never check the sound equipment before a talk. example: http://www.youtube.com/watch?v=T0_yaDz8Kd4
I wonder how this ever worked, not only in a concurrent application. Admittedly, i don't know what the code around this function does (copying the result to an other buffer or so), but the C function alone wouldn't even work in a normal case. char* result_a = pybc_bcrypt(key, salt); char* result_b = pybc_bcrypt(key2, salt2); result_a and result_b point to the same buffer and therefore result_a changed magically in the second line. That's one good reason to never return an allocated/static buffer from inside a function. If a buffer is needed, pass it to the function. I think the whole c library is done this way.
Yes. Look for the module xlrd to import it into Python.
I posted a similar comment in /r/netsec in reply to a comment asking if a pure Python implementation would be susceptible to the same vulnerability. Pure Python would have other issues, mainly being far too slow to actually use with a decent work factor. Feel free to look at my pure python implementation, https://github.com/mwielgoszewski/python-bcrypt, but I'm warning you now and in the Readme, please do not use this in a production setting.
y. done in matplotlib for work: http://imgur.com/YebjKpc
Ah, I see. He actually does 300 iterations per frame, and uses numpy for storage only, without vectorizing anything, so his computations are, I would guess, _slower_ than pure Python would be due to constant boxing/unboxing. Nice to see than my intuition is working properly!
This is extraordinary on many, many levels. - I've taken several machine learning courses and they've all waved their hands and said "Monte Carlo! Magic magic magic!". This is because they've all assumed a deep mathematical background is required in order to progress with Monte Carlo Markov Chain (MCMC) methods. I thought PyMC was the answer, but the tutorial was just, just insufficient. This book is a godsend, and a direct refutation to that "hmph! you don't know maths, piss off!" school of thought. - The publishing model is so unusual. Not only is it open source but it relies on pull requests from anyone in order to progress the book. This is ingenious and heartening. Thank you so much for posting this.
Yeap kudos from me too roger! Exactly what I was looking for, amazing work.
Very nice utility! Just one thing, and it's a terrible pet peeve of mine, a CSV file is comma delineated. CSV stands for Comma Separated Value(s). If a file is pipe delineated it should not, in my opinion, have a .csv extension.
Exactly what I was going to suggest, thank you sir.
We'll probably be coming at this from a different angle: shorter variable names! We were trying to be clear, but that only made it more difficult for the kids. So, this: cats = 5 Rather than this: number_of_cats = 5 
:( So sad to hear this. He was such a fun, smart and heplful man. 
* http://scipy-lectures.github.com/intro/matplotlib/matplotlib.html * https://github.com/mikedewar/d3py * https://github.com/enthought/chaco * https://github.com/enthought/mayavi * http://www.sagemath.org/doc/numerical_sage/visualization.html
* http://pandas.pydata.org/pandas-docs/stable/io.html#excel-files * https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#l1807 * https://github.com/pydata/pandas/blob/master/pandas/io/data.py
Ha! I told them not to feed (or eat around) their RPis, since they are naked. We then went into how to make a case, or where to buy them.
I'd love to chat about that! Track me down on twitter: @kcunning For now, I'd recommend watching very carefully how they're taking in information. Have them type along with you and give them simple problems to solve. Also, realrhema is right: Let them play games. Minecraft is great because there's a Python API they can hook up to in order so they can programmatically build structures.
Can I steal this? Because this is god-damn awesome.
Both are great and have their uses. I'd recommend learning both (as you're currently doing).
A man to whom part of my livelihood is owed: RIP.
You might also want to look into R. [CodeSchool has a free course.](http://www.codeschool.com/courses/try-r) But Python can do it too!
Go nuts, I got the idea from someone else anyways. I'd love to see the results of it in a class.
I am delighted that they've adopted transports and protocols - hands down the key aspect of twisted. I wonder if twisted's vast library of protocols could be split out/reused on top of the default loop implementation? 
nope, Guido said he didn't intend for it to support more than one handler ever.
Thanks for the feedback. Unfortunately., there are no solid standards for csv files. While commas and tabs are probably the most popular delimiters, there's quite a bit of variety in delimiters and especially quoting behavior. I'm accustomed to seeing them all generically referred to as .csv files. What kind of extensions do you typically prefer for colon, pipe, or tab-delimited files?
Thats pretty cool. I wish I had this sort of opportunity when I was in school (00-04). All we had was two levels of Web Design courses. But after the first course I already had demonstrated more knowledge than the teacher. This meant I could sit around and do as I pleased. 
Technically not narcotics, but that isn't even really the point (as far as I'm concerned). The point is that he acted like a childish asshole, and rather than admitting that, he's now trying to be cute about it. OP needs to grow up. We've all acted like assholes, but most of us had acted gracefully after the fact.
I've made pretty extensive use of matplotlib and mayavi for scientific visualization.
Hmm, I must have missed that part. Well, then somebody better have a talk with Guido. Multiple += on handlers is one of the better features of C#.
Sheeesh, what's the world coming to! You'd think people wouldn't be so uptight about breaking the law in a public place, and you'd think they'd be even more forgiving when the person involved was acting like a raging asshole.
As someone who has never needed to do this out of curiosity is it particularly good? 
[Chaco](http://code.enthought.com/chaco/) is nice for interactive graphics. Though the Traits overhead can be a pain in the ass. **Edit:** Damn you autocorrect.
His last tweet: &gt;Woke up late, afternoon nap and I still want to go to bed early. So tired; wasted the day.
I think everybody agrees. The question was about when and where we could expect them -- totally reasonable, imo.
~~On a related note: where can I find videos from past pycons?~~ Looks at pyvideo url... feels stupid... I'll see myself out.
NO. I only make this available for academic purposes. I strongly suggest you do not use this library. Python is not C, and I haven't been able to find a way to make it work with an acceptable work factor by today's standards of 14. If you can modify my code to compute a hash in &lt;1.5sec with a work factor of 14, I'd gladly submit it to PyPi.
[RFC4180](http://www.ietf.org/rfc/rfc4180.txt).
They are going up now by the way, 11 are up at the moment. Apparently they are waiting for the OK from the speakers.
if you are already adept at Python, or want to use this as a Python learning exercise this may be an efficient approach for you. If you want to build the visualizations in to software, likewise. Otherwise, staying with free tools, I think you would find it much quicker and easier to do simple visualizations and work with them interactively in R, especially if you use R Commander. This is what R is really made to do, and there really isn't a close equivalent in Python. Python is made for writing software, although it certainly has access to impressive visualization capabilities (though via high-level tools, still probably not as nice as R).
[do it](https://groups.google.com/forum/?fromgroups=#!topic/python-tulip/d7f2SOzPp7Y).
Yeah, that and the one about buying margarine several times that week because he forgot he bought it already could mean he had a severe case of sleep apnea. Remember guys, snoring isn't just funny or annoying, it's a killer. I kid you not, I have known people who literally died in their sleep from apnea, and weren't even 50 years old. Particularly if you're overweight, get yourself checked out if you're waking up tired or people you live with complain about your snoring. You'll sleep much better and feel much better as a result. (Yes, I know not all snoring is apnea, save it.)
Is there any sort of prompt or theme for the games? What's to stop someone from spending two years on a game and then entering it into the challenge claiming it only took a week?
This being the python subreddit, you can expect that any "is python good at X?" questions will yield responses ranging from "yes" to "very yes".
I'd previously implemented a pure-python backend for [passlib](http://pythonhosted.org/passlib/lib/passlib.hash.bcrypt.html), and hit a similar slowness wall, so had to take a look at your code. I was hoping we'd had different problems, but it looks like you singled out the exact same S-box lookup in encipher() loop that I ran up against. Sigh. I've bookmarked your project, and I'll let you know if I make any order-of-magnitude improvements. 
I agree, but you're not going to modify your functions at runtime every time right ? So, the optimization transformations are supposed to run just once.
And if there's also a "beware, this subreddit is biased" warning, it's usually a good sign ;) Jokes aside, you should always read critically. I think the dude around here who wrote about R probably makes a good point. I personally use Python (matplotlib and numpy specifically) for my vis needs, but have sometimes thought about trying out R. Never got around to it, though.
As long as matplotlib is there
In my opinion, no it isn't. It can certainly get the job done (using any of the tools that are mentioned in this thread), but it isn't the best. I find R is much easier to make look nice (especially using ggplot2). For interactive graphics that you're willing to spend a little more time on, d3.js is excellent.
tau, why not τ?
No, actually, I don't.
I agree. Python is perfect for consistently generating reports from a consistent set of data, but Excel is better if you're just playing around with disparate examples of data. 
there were about 100 signals I had to graph with basic string descriptions already. I didn't even bother capitalizing or adding units. 
Check out [vpython](http://www.vpython.org/) for visualization as well.
Create text output, run through gnuplot.
If you are dealing with geographical data, you should definitely take a look at this. http://opentraveldata.github.com/geobases/ It will work with any csv-formatted file.
Pandas has some methods for data formatting, but for xls file types you might need to write your own output function.
yes... but i think i'd use chart.js if it's going to be on the web
 pandas.DataFrame.to_excel()
I'm not going to speak for the entire SubReddit, but I personally would rather not have all this drama in here. It has nothing to do with Python as a programming language, it just happened to have occurred at a Python conference.
Also scipy for calculating
A _long_ time ago I used VPython for visualization, and I must say it was wonderful. I was examining massively multidimensional data and the regressions lines through it, and was able to project data easily into about 4.7 dimensions to view (X, Y, Z, hue, size, shape). The ability to get a full 3-D view with zoom interactively was key to being able to direct our optimizations. 
stroke = blockage in your artery aneurysm = major blood vessel widens like a balloon being blown up, and eventually ruptures 
The multi-user notebook is something I need desperately here at work. Having to get my Matlab-based coworkers into the correct Python environment is painful right now. I'm wondering if Wakari is going to get there first- my hope for that system is a multi-user interface where I can handle the environment building, and the rest of my group can just use the tools. 
Can't multiple people already connect to a single Notebook server and use their own files?
Now if we can only do something about python.com pointing to a porn site.
Looks good. I use matplotlib regularly, how does this compare? 
Glad you found it useful! I've been meaning to learn PyMC for a long time, and this has really been helpful for me.
Hm, maybe this is just iPython notebook ignorance on my part- I've only used it locally. Now I'm thinking about how to get a server deployed through IT, which could be painful... Thanks!
I love gnuplot, but this is a terrible idea for anything other than small datasets. 
 pip install Pillow 
I do not believe this to be *drama* in the least bit, but the community has spoken. I will be removing this post. But, two people lost a job because they made, albeit childish, jokes about forking and 'big dongles' to one another and it offended someone else. 
I read that, I was wondering more about the actual performance of it. I deal with large 3d datasets that can be crippling when I use matplotlib. Also can you use LaTeX on the axes? 
It should be in the terminal, it probably didn't work 'cause you don't have pip installed (which is not the default Python package manager for Macs). easy_install, on the other hand, always come with Mac python. So you can try sudo easy_install pillow and it should go (I just tested here, worked fine). One thing to keep in mind is that Pillow (and PIL) uses compiled libraries and, to compile those, you'll need the Command Line Tools (which will install Clang and LLVM on your computer). There are two ways of getting the Command Line Tools: 1. From the Mac App Store, install Xcode. Once it's installed, in Preferences, you'll find a "Downloads" tab. The Command Line Tools should be there. 2. Directly from [Apple Developer Downloads](https://developer.apple.com/downloads/index.action) page. You'll need to create an Apple Developer account, which can be done freely IIRC.
Terminal. Make sure you have installed pip which is the new Python package manager that replaces easy_install If not, run easy_install pip Weird, I know, but pip works wonders and is the encouraged method. Use sudo if needed.
(I helped at the two day "Young Coders" event at Pycon. Slightly older kids.) Beware the usability bugs in IDLE! Arrow keys don't bring up history (ALT-P), clicking somewhere on the screen moves focus and therefore supresses typing. enter will paste a random line if you moved focus. I could go on... But also I don't have a much better suggestion right now - unless you're willing to get radical. Would you consider IPython notebook? I'm really interested in using it as a teaching tool. After pycon I'm working on an educational game using turtle so I like that idea. I'm on a Python usability rant right now - see my poster linked from http://simeonfranklin.com/blog/2013/mar/17/my-pycon-2013-poster/ I wish there was one simple straightforward answer to the kind of thing you're trying to do... but right now there isn't. 
Perl?
Did you install the Command Line Tools as mentioned? If you did, try to simply close the terminal and open it again. It probably installed more stuff and change your PATH to find this new stuff; closing the terminal and opening it again will force reloading all config files and update the PATH.
This subreddit is the place where I found out about this. I have a very strong extreme personal opinion about such situations, which I won't voice here, just to avoid adding to the already giant shitstorm. No problems with the OP posting here, after all I come to reddit to learn something new and it's up to me to filter it instead of expecting to see only what I would like.
Cool, I appreciate your opinion. As I stated I wasn't speaking for the community, just voicing my thoughts. In the end that's what the upvote/downvote arrows are for. In this case though I thought it prudent to explain my reasoning as it would technically be a legitimate story to post here, but at the same time, it's not the type of story I come here for.
&gt;I will be removing this post I see no reasons why you should, as long as we don't create another drama by starting a holy war in the comments. IMHO it's not that much of a big deal, in a few days it will get autoscrolled down anyway and will become invisible. It's just a link to another resource, not an attempt to gain supporters for a side that got involved.
Thanks for the tips! I'm not very familiar with IPython Notebook, but I'll check it out. Love the poster, btw.
That always surprises me. . . I accidentally type python.com and whoa. . . that's not what I was looking for.
*bossception*
How many lectures/classes are allotted? It's easy to get carried away and trying to get them the kids to program right from the get-go making games and stuff. But if you have time, I feel like the first lecture shouldn't even have any code at all. For most, if not all of these kids, this would be their first experience with programming in general. So why not start with that? I would argue to make it a computer science class. Not a python class. Python is simply a tool you use to practice the concepts you teach in class. What would serve a 10 year old better in the long run? Telling them to copy a couple while loops and if-statements, or getting them to realize *why* programming is important? Just off the top of my head, the first thing I would do as a first lecture to 10 year olds on programming, I would have each of them try to race each other to get as many numbers of the fibonacci sequence as possible. After 2 minutes, stop the clock. Find out which student has the most numbers and take their number down. The kid will be proud and show off to his friends. Then I would load up python on the projector, enter the script for calculating fibonacci sequences and execute it for 2 minutes. The kids will be interested. Again, I have no idea if this approach is effective at teaching young kids programming or why it's important, but it's what I would have wanted if I ever had that opportunity at that age.
But the time it takes to run your optimizer can negate the speed gain of the optimizations for short living programs. You can also recompile a function to take into account information you get at runtime. LLVM also don't remove allocations which are a big bottleneck in dynamic languages.
&gt; there's like 9 levels between me and the CEO. and I bet *every* single guy thru the levels took the trouble to check out the site
Hah, thanks! I was going to wait for the Pycon keynote video to come out just to make sure I didn't forget anything before diving into the group. I'll definitely be posting there.
Sorry for ruining your joke :D
[Where's my porn!](http://i.imgur.com/8CkuqP9.png)
Yes! At least to the question in the title. I don't know about excel spreadsheets, but csv data (which can be exported from excel) can easily be used. A great tool I recently discoverd for quickly hacking together a plot is [spyder](http://code.google.com/p/spyderlib/), which essentially mimicks the matlab gui and uses matplotlib in the background which has already been suggested.
Nice catch. Thanks
* http://docs.python.org/2/library/functions.html#open * http://hg.python.org/cpython/file/v2.7.4rc1/Python/bltinmodule.c#l1484 * http://hg.python.org/cpython/file/v2.7.4rc1/Objects/fileobject.c#l319 * http://docs.python.org/3/library/zipfile.html#zipfile-objects * http://hg.python.org/cpython/file/v2.7.4rc1/Lib/zipfile.py#l699 * https://github.com/python-excel/xlrd/blob/master/xlrd/__init__.py#l400 * https://github.com/python-excel/xlrd/blob/master/xlrd/book.py#l582 * https://github.com/python-excel/xlrd/blob/master/xlrd/xlsx.py#l713 * https://bitbucket.org/ericgazoni/openpyxl/src/tip/openpyxl/reader/excel.py?at=default#cl-109 * http://wiki.openoffice.org/wiki/Python
I cannot say I have ever heard of Malcom before this article, but I have lost far too many programmer and developer friends alike over my experience in the IT world. Looking at the responses in this thread, he is someone I would have liked to meet and speak with. RIP Malcom
that's a non-good language feature there. I guess Python 3 included some of the functionality (but in a much saner way) with allowing keyword arguments after the splat operator (or whatever it's called), as in: def myfun(a,b,*args,q=None): b,c = args[0], args[1]
Based on the experience from last week there's now an enthusiastic set of volunteers - including the developer of the awesome IDLE fork at http://idlex.sf.net/ - looking to fix a lot of the problems in IDLE. I have high hopes. IDLEX is a good IDE to use right now if you have the option to install something beyond the bare Python installer.
I have had some positive feedback from parents and teachers that used rur-ple (http://code.google.com/p/rur-ple). It's basically karel the robot but using Python and comes with a number of lessons. I have not used it myself to teach kids; I just wrote it.
Sounds like you're asking someone else to do your research for you.... http://windows.podnova.com/trends/python_process_engineering.html
Me too, but I learned it by looking at your reaction. whitehouse.com is still safe, right?
How about you use dyndns api and a CNAME? That way you don't have to change the IP yourself everytime it changes ie thelair.me -&gt; my-pi-box.dyndns.org using a CNAME then, whenever your script detects a change in your IP url = "http://{0}:{1}@members.dyndns.org/nic/update?hostname={2}&amp;myip={3}" url = url.format(username, password, "my-pi-box.dyndns.org", ip) output = check_output( ["curl", url] ) You can probably use urllib or urllib2 instead of curl but you get the idea Edit: clarity
Highly recommend the iPython Notebook. It can be used for so many great things and teaching is one. You can display graphics right in the browser, conduct analytical analysis and do all kinds if things a student would find interesting. For example, you might want to teach the concepts of a dictionary... So have them get everyone's age with the name being the key. Then show them how to plot that on a graph. Just 1 of thousands of ideas. That example might be to much for them, it might not. Adapt it to their needs.
probably. the problem is that I've been doing a lot of energy and mass balances in excel for my company, and to be honest, it is becoming harder and harder. I looked at pypi but I didnt find anything that I could use. I didnt know that website. Thanks for the help.
One time during middle school, some people in my class were in the library with our teacher researching stuff, and a girl went to that site thinking it was the White House website. Her reaction was priceless. She didn't get in trouble, fortunately. I don't think she would have deserved that. But she did learn a lesson.
The "fix IDLE team" is growing - another potential contributor popped up today at the PyCon sprints and she's already got a patch ready for contribution :-)
The free PDF "Raspberry Pi Education Manual" is a great place to start (and I kept forgetting to tell everyone about it, silly me) http://downloads.raspberrypi.org/Raspberry_Pi_Education_Manual.pdf
It seems like not many programming languages have a .com address. In fact, I can't immediately think of a single one. Presumably this is because .com domains are intended for commercial websites.
I'm going to take the stackoverflow approach and ask what have you tried so far? A quick google search shows this module: https://pypi.python.org/pypi/pyad, but I've never used it. Sorry couldn't be of more help.
`http://java.com` Other than that, I can't think of anything either.
I've used [this](http://www.python-ldap.org/download.shtml) to work with Active Directory before. The only thing I've really done is query for a user's email based on their actual name, and frankly I know very little about Active Directory, but if you need some help getting started with that module I should be able to provide a bit? Don't have any examples with me, but I can try and post one when I get to work tomorrow.
[his own license plate](http://www.python.org/~guido/images/license.jpg)
Very interested in this module, I actually added that one and tried to use it. I got stuck trying to figure out how to search a computer object...and return the dn. I was able to initialize the connection, bind it, and that's pretty much where I got stuck
GGplot is great.
Better than the IT guy finding it and reporting it.
Python? Eh, this uppity little programming language is never going to catch on! Seriously, how the fuck did a tech company not know about Python? It's heavily used by both Google *and* Microsoft...
"Sir, I caught a seat-warmer in sector 7B viewing a porn site for exactly 3.7 seconds. What should I do with him?" "BRING ME HIS HEAD!!!"
Still, it's their job to report anything like that. Damage could be done before the boss brings it up to him and he can explain himself. He probably wouldn't get fired, but it's still better to just deal with it before it's discovered. 
Sorry dude, they're no longer excepting. Probably moved back to error checking.
I'd avoid actual computers, at least until you drill some foundations in. The best computer science for kids is to have them try to "program" the instructor to do something. I recommend making PB&amp;J sandwiches. Really play up the mistakes caused by omitting steps. Variables can store how many sandwiches to make. Introduce functions by making a spread() function and calling it for PB and then for J. loops can say how many times to call spread(). Then you move on to more complicated sandwiches and finally finish with a program to make all sorts of different food. Lists and dicts store ingredient details and introduce data structures. Voila, abstraction taught in under 1/2 hr. Then break them into groups and have them sort each other by height or birthday. Maybe have them program their way around the room.
or it explains the state of perl in /r/python 
Giving them a real, achievable project that can be built in steps seems like a good way to teach them important concepts while capturing their interest. shakedown is right; just having them copy some boilerplate isn't going to impart any great wisdom. If you want to do anything beyond adding numbers together you're going to have to teach concepts and guide them to the final code. Having them paint by numbers that last few lines won't do them any favours. Perhaps an excuse generator that starts as little more then random string substitution on 'the {0} ate it'. Next make it clever enough to do verb, noun and adjective substitution from different lists. Maybe have them do some mad lib creation at this point as well since that's the exact same thing. Then to get really advanced add some markov chains (possible example http://www.rayhe.com/random/). That might be a little too advanced though. I did find a cool little pig latin creator: http://pythonicprose.blogspot.ca/2009/09/python-pig-latin-generator.html That could be pretty fun. Once they understand that and expand it to translate whole documents you could have them make their own secret languages and translate text into it. From there encrypting/decrypting messages to their friends would be really cool. Start with some simple substitution and go wild from there. It might even be fun to write some functions to try and "break" simple substitution encryption but trying substitutions until enough valid clean text is seen.
I love the PB&amp;J concept. We'll definitely be doing that on day 1.
For graphics, I'd do something simpler than pygame. PyProcessing is good ( http://code.google.com/p/pyprocessing/ -- be sure to get the latest version of the Pyglet dependency in source code, not 1.1.4--that's broken I think) Turtle graphics would be a good alternative too and it's built in. ( http://docs.python.org/2/library/turtle.html )
Ahh... So what you are looking for is a simple program and not a generalised framework? Numpy (et al) are simply tools for expressing data in a more intelligent format that makes calculations faster (This isn't how they're actually done). Similar concept to working with vectors and matrices. In python it depends on what equations you are attempting to solve. If the equations are linear then you need to express them in a matrix format, however if those equations are non-linear it becomes more complex (Check out scipy.optimize which has non-linear solvers) If you are looking at numerically integrating ODE's then pyDS has methods for that I believe. The difficulty in helping you is typically the difference between a specific and a generic framework. A lot of open source tools try to provide at least some level of generality so that they are useful for a wide audience. A specific (for your use) Boiler house model, though of use to you, is not of as much use to the wider community unless it is developed in a generalised framework and integrates with the wider community. Hope I've been able to be of help.
Take the time to do the official Python.org tutorial. It explains a lot of little stuff like that, which you'd otherwise never learn. 
What, you can't say forking or dongle? Is that a sick joke?
I knowHere's a similar project, with a little more breadth - https://github.com/Eugeny/reconfigure
As long as he realizes he needs to cut down on the alcohol, or somehow control himself enough while he's inebriated, fine by me if he comes back.
Thanks for coming to PyCon! Certainly looked like y'all were having fun with the blimp. Didn't get a chance to talk to one of the engineers, but it's certainly great to see how many of the things I use every day that use python... And this breakdown really shows how far into the stack it goes! 
Having been down the path you are trying to go I can only say stop and learn Powershell. It is very powerful and will serve you much better than Python in this regard, it goes way beyond AD, even Exchange can be fully managed in Powershell. I am a Python fan but, use the right tool for the job. Python is awesome at many other things don't stop learning it.
Its more about the inefficiency of Django rather than the speed of Jinja. Most level headed individuals would agree that 2300 function calls to render a basic Django template (one used in the tutorial) is excessive. But yes, you are right, speed alone is not the reason to switch. 
Well, have of a lot of great albums have up and poofed away from Spotify in the past few days.... Any idea where they went?
`http://cplusplus.com/` ?
Microsoft uses Python? I didn't know that...
This is great :) 
They sponsored development of IronPython.
I tested matplotlib, chaco, guiqwt and pyqtgraph for my data acquisition software, where i do need some speed. **matplotlib** is only fast if you can use blitting (that means no dynamic ticks etc...). It produces very nice graphics and has many features, also its event system is very easy. It is quite good documented. Building it is hard (windows user here). multiple backends. **guiqwt** the fastest (it is frontend for qwt) in my tests. for a beginner, making plots is easy but using events is a little hard in the beginning, it uses a tool system (like chaco). the toolsystem is powerful, but i prefer the simple event system from mpl. plot quality is medium, turning on anti-aliasing is a big speed penalty and even than, agg is much nicer. it has good documentation but fails sometimes to explain the big picture. building is hard (but much easier than mpl). **chaco** fast enough for most of my applications and uses agg, so it also has nice graphs. but to use it, one have to use traits, which can be overwhelming in the beginning. also it is missing some convenience functions if you don't want to write some boilerplate again and again. the api is a little to direct with all its mappers and ranges. especially because the documentation is quite sparse or simple not there. uses the same tool system like guiqwt. Here again, i am little overwhelmed with where to register the tool etc.. i did't try to build it. **pyqtgraph** doesn't has to be build which is a big plus. i like its api, if you know qts event system, you pyqtgraphs. it also much faster than mpl. but i wish it had also some kind of light-color scheme. the direct structure of the package (which uses qt-graphicsscene to do the heavy lifting) is really nice. documentation is ok, especially due the fact that the package structure is quite simple. overall result: every package has some strong point, so it depends on the usage. For my case, updating speed was king, so i used quiqwt, but i still miss the agg-based render of chaco. mpl has a beginner friendly event system and a lot features. pyqtgraph is very portable and a thin layer above qt (this is a good think) with a nice api. 
p.s. Please package makers, always include the following example: User clicks on the graph, and somethings happens depended on the data coordinate of the click. Thank you! (Maybe i was too dense, but i wrote a simple ClickTool for chaco and quiqwt.)
I excepted you, Mr Bond…