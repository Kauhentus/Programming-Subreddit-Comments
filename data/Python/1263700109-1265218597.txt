Heh... Nah, he wants to know things like client encoding and such. SQLa in 06 loads this stuff per dialect so pooling doesn't matter (edit: except for the things the DBAPI itself still wants to know). But like we've been through before... What possible issue is there with having a set of connections persistently available? If the "anti connection pool" crowd weren't so fringe the argument would be tedious.
[optparse](http://docs.python.org/library/optparse.html) is generally much easier than getopt.
Whats with the music?!
Seems like encoding and such are things you could know in advance, since presumably you're the one who set up the stack. In that case why shouldn't it be configurable? It'd certainly work better with connection-pooling utilities (and since there are a lot of people who develop and use external pooling utilities, it's hard to make the argument that they're "fringe"...).
and [argparse](http://pypi.python.org/pypi/argparse) is generally much easier than optparse.
If that's all you want, why not just use autoload mapper in SQLAlchemy? You have to list all the table with class TableName(object) : pass and may be add a little code to add foreign key where SA have trouble inspecting it. But still the code to inspect DB and add extra customization, if any, to SA mapper will still be a lot less than the code to inspect DB and create your own ORM layer.
Somewhat similar library that uses | syntax: http://pypi.python.org/pypi/iterpipes
No because it's a system on its own. A Python projects repository server. The only link PyPI has is with Distutils : the latter provides the "register" and "upload" commands to push content at PyPI.
I think the biggest problem with site-packages is related to a lack of a common installation standard/format. We are working on that in PEP 376. This PEP will provide among other things, a uninstall feature and a way to query installed distributions, no matter which tool installed them (pip, distutils, distribute, setuptools)
easier if you don't consider the need to install.
easy_install uses a protocol to browse PyPI to look for dependencies, It's described here : http://packages.python.org/distribute/easy_install.html#package-index-api But on server-side, PyPI doesn't use Distribute or anything else to implement this structure: it's roughly a directory layout. (and you can implement your own PyPI server if you wish, as long as it implements this layout) 
I'm following this submission with interest, because each time I've tried to use SQLAlchemy the same conclusion has struck me in the face. I'm sure it's a combination of my own dull-headedness and some kind of deficiency in the SA docs, but the end result is always that I have to ditch SA because I don't have the time to digest the voluminous documentation enough to understand the errors I encounter. This is a shame, because I really think SA has a lot to offer me, and possibly vice-versa. I've posted about some of my experience in reddit threads and been soundly flamed. The popularity of the toolkit suggests I'm missing something vital, but damned if I know what it is -- hoping this thread can give me some answers. There are two culture clashes between me and SA that make this more difficult. First, I know SQL pretty well. I learned it circa 10 years ago when abstraction layers simply didn't exist in many environments. I only learned enough to do what I needed in a fashion I found elegant enough, but it turns out DBAs find more complicated ways to do the same things, so I must have some kind of edge. Second (and a large part of the reason I'm drawn to Python), I have a huge esteem for simplicity and transparency in code. This puts me at loggerheads with what appears to be an impenetrable confusion of layers in SA, meaning that when I encounter issues my usual practice of source-diving just gets me in more trouble. I actually think my ~20 years development experience is working *against* me here, because SA simply isn't built towards any of the patterns I'm used to. My habit of not accepting that a block of code works until I feel comfortable that I know *why* is more harmful than helpful, which makes me very uncomfortable. Several comments in this thread and the below linked [stackoverflow post](http://stackoverflow.com/questions/860313/sqlalchemy-is-convoluted) point towards one shortcoming that's not on my side. To quote an SO post: &gt; the tutorial you mention doesn't "build" a complete example, the different snippets of code are not meant to be concatenated into one source file. Rather, they describe the different ways the library can be used. From my experience, this means that when I'm trying to solve X, just searching for it in the documentation is a mistake: I find answers, but without a deep understanding of their context, applying them just makes a bigger mess because I'm mixing components that aren't supposed to be mixed. The docs are fabulously detailed, but I always feel I'm missing the big picture. The "tutorial" chapters don't resemble what I expect from a tutorial, and every other section seems to presume knowledge of all the others to the point that I can't find a solid foothold upon which to build an understanding of the rest of the library. I'm hoping the [pycon videos](http://pycon.blip.tv/search?q=sqlalchemy) linked below will help me make sense of it, though it's a sad state of affairs when one needs to watch a video rather than read a manual to understand something. Any other suggestions of what someone in my position should read, or where I should start would be immensely appreciated, and hopefully useful to others. You can assume I'm a competent developer, with a lot of experience and completely comfortable with DB-API, its operating modes and its shortcomings. I'm looking at SQLAlchemy to take out a lot of the hack work that a thin layer like DB-API requires, and to produce more elegant code that better reflects my domain and supports DRY. I might need to be frequently reassured that the sort of problems poor toolkits have led me to expect are actually avoided when I see a similar pattern. I'd like to know how the layers fit together, so that when I use a function I have a sense of how they are interacting and what will happen on the DB layer, and when. My database probably has multiple users which are not SA, so invasive alterations to the schema are out of the question. Probably a fairly complex database already exists, as seems to be the case with other people ITT having similar struggles. Finally, assume I know nothing about the library, because I have to assume at this point that everything I know is wrong. I should probably be instructed what documentation or practices to *avoid* as well. Elixir might be a good example .. or is this recommended now? In any case, I don't think adding layers is the solution to my problem.
I agree! That's why I hacked together a prototype [Activerecord in Python](http://bitbucket.org/jonromero/pyactiverecord/src/) . Any feedback?
great, so here is our code, using pg8000, without pooling, and optimizing the (edit) 1700-row SQL statement it executes per connection so that they only happen once: if options_which_pg8000_needs_to_run is None: # connect for the first time conn = pg8000.connect() options_which_pg8000_needs_to_run = \ conn.get_options_pg8000_requries() else: # connect subsequently conn = pg8000.connect(dont_autoload_data_info=True) conn.set_options_pg8000_requires( options_which_pg8000_needs_to_run) conn.close() Needless to say no such options exist on pg8000 so the above is not possible. Here it is with pooling: conn = pool.connect() conn.close() your suggestion is essentially to pool *only the metadata pg8000 requires*, but not the connections themselves. Again what is the point ? edit: pg8000 loads the full list of available datatypes from pg. its a 1700-row statement call so is very expensive per connect: SELECT pg_type.oid, attname FROM pg_type INNER JOIN pg_attribute ON (attrelid = pg_type.typrelid) WHERE typreceive::text = 'record_recv' ORDER BY pg_type.oid, attnum
Did you go through the current SQL expression tutorial? Its all just connecting, make an expression object, and execute. Is that what you can't figure out?I wonder what DAL tools you *would* prefer if so..
&gt; can we deduce from this that C++ programmers cannot program properly? One has to create lots of boilerplate code in C++ to express heterogeneous containers like `[0, "i", [1, None]]` and those structures are usually hard to infer unless one relies on powerful type systems.
Having used both, I find argparse is handy when you need a feature that optparse doesn't have (subcommands, positional argument parsing), but equivalent otherwise, and optparse has better documentation (not great, but better). Plus there's the installation. So for a beginner optparse is really the best choice.
I've played around with a few game engines, and Panda3D is by far my favourite. I'd definitely recommend if you're just starting out with game development. It's very complete and very easy to use. Some aspects of it aren't documented well, but the support is good should you ever get stuck. In terms of performance, it runs faster for me than equivalent code in Ogre. It's also much easier to structure code in Panda because it has more helper features. Post-processing effects are literally switches that you can enable and disable. If you have any more specific questions I'd be happy to answer them.
Apparently he didn't read the update posted on [one of the articles](http://muharem.wordpress.com/2007/07/31/erlang-vs-stackless-python-a-first-benchmark/) he linked to. &gt;Erlang ... is a language and a high-quality runtime system optimized for &gt; concurrent and distributed execution. The examples below do not &gt; really exercise any of these strengths. These bench marks, yet again, do not test concurrency, distribution, parallelization, etc. The only mention of it is the author's complaint that Erlang didn't support SMP until 2006 (which is four years ago and before much of this hype).
I agree with the author in the sense that everyone says python is slow those days. Maybe it's too slow for facebook chat or a super custom app but for everyone else it's good enough, and has a huge set of cool feature, very easy to read/write has tons of libraries out there. And before trying to scale to 60.000 request per seconds people should take care of having more that 10 users and designing a webapp that people want to use :) Bashing Python is really to me a case of early optimization. Plus in this bench we see that with epoll it's very fast, and with tornado and friendfeed we have an example of a fast webapp that scales (plus reddit and a bunch of those).
I'm not sure if the article is supposed to be a troll or not. I'll take python over erlang any day(it hurts my head), but the fact that a python http server that writes "Pong" is faster than one written in erlang is completely irrelevant.
It might be nice if he tried to compare apples with apples. 
thanks for your helpful, encouraging response.
Would the performance difference on OS X by significant? damn it, bad joke... sorry...
And why do you think it is irrelevant? There are lots of uses for a server that can output a cached string. If Erlang should be fast at *something* it should at least be fast at this.
ok, i'm puzzled. it's an emergency and they already have a php version of the system. why would they want a python version now?
Why should it be fast at this? Why should it even be fast at anything? Erlang was designed to be distributed and fault tolerant and "fast enough". Comments like this: &gt; And no, I am not simply measuring how fast epoll is. I am using epoll to trigger my own scheduler written in pure Python which should by all means be slower than Erlang or Haskell. really boggle the mind. The python server has no pure python scheduler, it relies entirely on epoll for scheduling. All you succeeded in doing is benchmarking the python interface to epoll.
Your comment displays a lack of understanding of how user land schedulers work. From epoll triggering, I call the method of an object created especially for every client with it's own space in the heap carrying it's own context. This is equivalent to what Haskell and Erlang are doing under the hood with their lightweight threads except at a much lower level and thus should be faster. What you are suggesting is that Erlang and Haskell do some busy polling when they are waiting on IO. If this was true as you suggest (which it isn't), it would be a bad design nevertheless.
I did read it. The only problem is that he offers no evidence to support his change of mind other than the fact that he says so while his benchmarks speak for themselves.
Sigh. When you have multiple people telling you you are wrong, including one of the most well known Erlang developers, you REALLY need to start asking yourself if you could possibly be wrong. I will repeat what I said again: you do not have a scheduler. The erlang and haskell servers you posted do, and this overhead is significant in the micro benchmark you chose to use. The only program that is operating at a lower level is the python server, which does not have a schedular. Try this: modify each of the programs you posted to read in the GET request, but before responding sleep for 1 second. This is trivial to do in the erlang and haskell programs, since those are using a real scheduler, but no so for your python server.
It would be more relevant if he benchmarked more than just "Pong", a few database requests and dynamic content for example.
&gt; When you have multiple people telling you you are wrong, including one of the most well known Erlang developers, you REALLY need to start asking yourself if you could possibly be wrong. If you are referring to Ulf Wiger, [he does not even know](http://www.codexon.com/posts/debunking-the-erlang-and-haskell-hype-for-servers/comment-page-1#comment-470) that increasing the kernel backlog limit increases the amount of half-open connections. This is flagrant appeal to authority while ignoring evidence is what I am talking about when I say Erlang hype. And about the sleep scheduler, do you really think it is going to add a lot of overhead by removing the file descriptor from a hash table and putting it in a min-heap? The answer is no. You do not (unless you have a bad implementation) get a 20% overhead by doing this.
my mistake, you are clearly an expert on the subject. Let me know when you have posted the updated example programs that include the 1 second delay. And no, you don't get a 20% overhead by doing that. You start to see overhead once you implement a real scheduler.
Why don't you do it then? I'm not going to bend over backwards when some Erlang fanboy cries out because the so called "expert" (who doesn't even know what the backlog does) disagrees.
Oh... &gt; This is flagrant appeal to authority while ignoring evidence is what I am talking about when I say Erlang hype.. Ignoring what evidence? No one is ignoring what you wrote. No one is slamming python and claiming that erlang is faster. All I said, and what most people are saying, is that your micro benchmark is irrelevant. If I write my next project in erlang it is going to be because I need a programming language that makes it easy to write distributed fault tolerant systems. Not because it is 20% faster than some other language. and one more thing, when you resort to saying things like: &gt; "DO NOT WRITE A SERVER IN ERLANG JUST BECAUSE YOU HEARD ERLANG IS THE FASTEST AND MOST CONCURRENT LANGUAGE." It makes it hard for people to take you seriously. Facts should speak for themselves, people don't like being told what to do.
Correction: python fanboy.
Downvoted for tar. Not superior to rar by any means. more "open" does not make something better.
They want future disasters to be more maintainable.
Meh. It's like Perl 6, but it shipped.
Pretty much - I don't know if it was there when you posted, but they have [a specific FAQ entry](http://sahanapy.org/wiki/FrequentlyAskedQuestionsPython) about that...
Not python related, but check out the speed in processing (built on java) http://greynode.org/mandelbrotwurst/ It's quite fast, faster than C++ by a long shot (without using openGL)
So, entirely unlike Perl 6 then? 
"Unladen Swallow"..."A cleaner, more stream-lined language, and performance improvements of up to an order of mangitude are enough to get any red-blooded developer excited about the future of the platform." It looks this is the author's focus. "performance improvements of up to an order of mangitude", is it true?
Unless they want a closed source system for only Hait, which would be unfortunate.
You're missing the point. The hype about erlang is not about its speed. The speed is only a byproduct of the easy distributed computing, parallelization, and concurrency, but his benchmarks and yours are only testing raw speed. Let me provide an analogy. &gt; Erlang community: Erlang-fruit is better than Python-fruit at preventing cancer because it has more antioxidants! Yay Erlang! &gt; &gt; You: But these tests clearly show that Erlang-fruit is not better for your digestion than Python-fruit, and is only slightly better for your digestion than Haskell-fruit! Therefore Erlang-fruit cannot be better than Python-fruit [in general]!
I'm being completely serious, no offense intended. The expression language, in my opinion, is supremely straightforward and the source code very easy to understand. It is entirely similar to many other tools such as SQLObject's SQLBuilder and linq. Dozens of developers have added new dialects and contributed patches. Reddit built their DAL around it. Whereas you seem to have found it to be impenetrable, and I'd like to know if that is actually true. If OTOH you only dealt with the ORM tutorial and possibly some very outdated tutorials around the web, its more likely that one might be confused. 
Hahaaaa, best ending to that thread I could have imagined.
shouldn't they paint a picture of how the urgently rebuilt system would improve helping the citizens? and why the old one cannot be used in such an urgent situation. until then - i'm not buying it. 
I didn't read the FAQ, this was just my guess.
Correct, but maybe they want to use the publicity.
It's the worst language switch justification I've ever read. I disagree with almost every points aside from the GIS part which I know nothing about and despite the fact that I loath php. I see nothing there worth throwing away the man hours invested and a working project. *edit* And it's definitely not going to help the Haitian before at least a few months. 
I agree with you. I'd much rather be programming in Python than PHP, but their justification is crap. It sounds like something I'd say to someone who is programming-ignorant just to get my language of choice. Yuck.
Er, no. It's a language changeup to correct oversights and legacy crap that's incompatible with the previous base of code and will endure a long transition period before anyone really cares to use it. So, in that sense, exactly like Perl 6.
Silly God, he can be so untimely!
Unladen swallow can be an order of magnitude faster, yes.
Puzzling is that hypocrites like this managed to advertise themselves through the official python site. Do they think they are addressing their 'urgent' calls to infants? What a joke! 
We really just need one major web framework to jump on Python 3 and it will take off. The problem is they either need libraries that are not available, or they will be trying to maintain some type of backwards compatibility. The language forking so late in maturity and its forcing a ton of other projects to choose the same path.
I know little about the motivations and the deployment status of the project but, I know the title is misleading. They do not want a Python version now. [They have a Python version already](http://haiti.sahanafoundation.org/prod/default/index). They started working on this at least since the Sahana hackfest at FOSSKriti on Feb 14, 2009, when I first heard about it. More than 80 people participated. Here are the [photos](http://picasaweb.google.com/ajuonline/SahanaPyHackfestFOSSKriti#). The software is available for [download from launchpad](https://launchpad.net/~sahanapy). They are mostly looking at cleaning up the details and adding some customization for the specific situation. I am told PHP version and the Python versions are interoperable. I am not sure what the relative strengths are weakness are but probably the Python one is easier to customize. I hope this project will be useful. Even if not, kudos to those people who are spending time on this while they could be spending their time building for-profit software. One should also consider that given the financial crisis, not everybody can help monetarily to the earthquake relief, although that is probably the best way. Being involved in a project like this is one way to help. Moreover, the people in charge seem to be very knowledgeable about Disaster Management Software and being involved is a way to learn from them. Even if it is not useful now, it may be useful in the future. I am sure the Sahana people are competent and can decide themselves whether sahanapy is ready or the PHP version is more appropriate, and I am sure the software will also be evaluated by various agencies in much depth than people are doing here on reddit. ;-) Nevertheless this is the Python sub-reddit and one would expect a warm welcome instead of the cold response that I see. EDIT: For the record Sahana is the major [Open Source Disaster Management System](http://www.google.com/search?hl=en&amp;client=firefox-a&amp;rls=org.mozilla%3Aen-US%3Aofficial&amp;hs=eVm&amp;q=disaster+management+system+open+source&amp;aq=f&amp;oq=&amp;aqi=). I learned from [wikipedia](http://en.wikipedia.org/wiki/Sahana_FOSS_Disaster_Management_System) that was conceived in 2004 and early version of the system have been deployed already: * Tsunami - Sri Lanka 2005 - Officially deployed in the CNO for the Government of Sri Lanka * AsianQuake - Pakistan 2005 - Officially deployed with NADRA for the Government of Pakistan * Southern Leyte Mudslide Disaster - Philippines 2006 - Officially deployed with the NDCC and ODC for the Government of Philippines * Sarvodaya - Sri Lanka 2006 - Deployed for Sri Lanka's largest NGO * Terre des Hommes - Sri Lanka 2006 - Deployed with new Child Protection Module * Yogjakarta Earthquake - Indonesia 2006 - Deployed by ACS, urRemote and Indonesian whitewater association and Indonesian Rescue Source * Peru Earthquake - Peru 2007 - Deployed and localized into Spanish. * Myanmar Cyclone - Myanmar 2008- Currently working in progress to deploy and localize into Burmese. and received the following awards: * Free Software Foundation Award for Social Benefit - March 2007 * BBC TV Documentary the "Code Breakers" * Sourceforge Project of the Month - June 2006 * Software 2006 Good Samaritian Award * Red Hat User Awards * Wikipedia Reference to FSF Award for Projects of Social Benefit * Stockholm Challenge Finalist 
&gt; "performance improvements of up to an order of mangitude", is it true? within limited testing contexts, yes it can be true, just as psyco's "performance improvements up to *two* orders of magnitude" can be true if your code is basically two functions called a lot and purely numerical.
It takes a very special attitude to be massively downvoted for praising Python in a Python community!
No where on that site or the linked sites does anyone explain what a GIS is. That's rubbish. I'm pretty certain it's [this](http://en.wikipedia.org/wiki/Geographic_information_system) they're talking about, in case anyone else was as clueless as me.
if Unladen-Swallow can gain a overall order(about 10x, more or less) of performance for normal / daily/ most python script, it will be a really killer feature and a big Day for all python coder when the day U-S is merged to whatever Python version's trunk. else if the magic only effect some demo code then why should be excited?
You are correct.
&gt; if Unladen-Swallow can gain a overall order(about 10x, more or less) That's pretty unlikely. And by "pretty unlikely" I mean "no fucking way it happens".
It is. I was actually saying that I never worked with one so I don't know if python really has such a better support for whatever mapping operation they need to do.
I can't believe how often benchmark blog posts seem to make this basic error... claiming their favorite language or server is the fastest, when they're really just comparing epoll vs non-epoll.
Django has fairly few dependencies of its own, but official migration needs to go at a snail's pace. A fork with a different name (this is important) could jumpstart Py3 web development fairly nicely, with an eventual upgrade path to Django proper.
Don't get me wrong, I wasn't commenting on your post specifically, just that the website in question had no definition of a GIS.
yeah, I too thought that this is a case of "Didn't RTFM"
Yes, but it assumes your orders of magnitude are in binary. 
I'm the author of pg8000. The painful query that pg8000 performs on a new connection is intended to cache a bunch of type information in the database, so that they can be converted nicely to Python types. I hate that query, and I wish I had never built that functionality -- it's bad. I just haven't ever had the time to remove it. The type information cache could easily be removed, and the missing functionality would barely be noticed. It was a horrible choice to add that in. **edit**: This feature has been completely removed from pg8000's HEAD (http://github.com/mfenniak/pg8000); no more expensive pointless query when opening a connection.
I believe F12 has all the SciPy parts in the standard repos.
Hi jart, Unlike psycopg2, pg8000 uses the PostgreSQL client API to perform data binding (I believe that psycopg2 parameterizes the query on the client, and then sends the entire query in one string). That requires that the query be sent to the server to be parsed, then the results of that are read, then the parameters are bound into the query, then the results of that are read, and then the query is executed... then the results are read. The major downside to psycopg2's approach is that the entire query result is read at once. If you're receiving a large number of rows from the server, you need to read the entire result into memory at once. pg8000's approach allows the driver to read N rows in chunks, leaving the rest of the query pending on the server. I've never really thought about it, but you're completely right to point out that it adds latency to the query path. It might be possible to combine the parse/bind/execute into one (send, read) combination... it's something I'll look into when I have some time to work on the project again. Thanks for the input.
Dude thanks, I read the first paragraph, and started to think that I had no idea what Haskel was if it was posted in r/python
&gt; Nevertheless this is the Python sub-reddit and one would expect a warm welcome instead of the cold response that I see. One shouldn't. Many of us are critical thinkers, and when faced with a problem, we examine it objectively. When something doesn't make sense to us, we ask questions. That is what you see. Protip: Resolving the incongruency will get you further than appeals to emotion. There are decent people, they just have to know that what they are doing makes sense.
Python is mentioned in the comments, and I suppose this is a reply to http://www.codexon.com/posts/debunking-the-erlang-and-haskell-hype-for-servers 
Sorry for the late reply. Thanks for taking this into consideration. I've come around to testing your IDE - and I was excited and pretty much sold once I saw the auto-completion. That is marvelous! Unfortunately all that ended when I wanted to type my first parentheses (so pretty fast) and got nothing. That's when I realized you use GTK. GTK has a longstanding bug that prevents everyone using certain nonstandard keyboard layouts on Windows from using it because we can't type certain characters in GTK+ applications. The earliest bug filed for this that I can find is this: https://bugzilla.gnome.org/show_bug.cgi?id=165385 - Keyboard layouts affected are Latvian, Czech, Indian, Bangladesh, NEO Layout and others as far as I know (all those examples come from tickets in the GTK+ bug tracker). Don't take this as a complaint, just a heads up in case anyone has the same problem as me. I've seen some really stupid answers from this "Lillqvist" guy on the Bug tracker ("You just have to decide then, which is more important: Using an alternative keyboard layout that GTK+ doesn't seem to handle, or using GTK+ applications.") so I don't think this will be fixed anytime soon. For me the decision is clear: Keep my keyboard layout which unfortunately means that I'll have to ditch WingIDE after my five minute trial :(
There are some students at the University of Toronto who are actually doing a project this semester to port Django py3k, it's been discussed on the -dev mailing list. It's not quite official, but if it's successful (the goal is to have a single codebase that runs on 2.x and 3.x with the help of 2to3) I imagine it could help the migration process along.
It's not an error. Epoll is included in the core for both Erlang and Python, while it hasn't been standardized (doesn't run in Ubuntu 9.04) into Haskell GHC yet. Also 12k Python vs 15k Haskell isn't bad considering that Haskell is supposed to be a faster language and the Python version hasn't been compiled with psyco or cython yet :) **Edit** Since I couldn't get the experimental epoll to compile on my production speed server, I decided to benchmark it comparatively on my core duo 2.2 ghz machine. What I found is that **Python is still faster**. Haskell Epoll -O2: Rate 5000 Connection rate: 4998.8 conn/s (0.2 ms/conn, &lt;=208 concurrent connections) Connection time [ms]: min 0.2 avg 12.5 max 3005.7 median 0.5 stddev 184.6 Rate 6000 Connection rate: 5330.0 conn/s (0.2 ms/conn, &lt;=635 concurrent connections) Connection time [ms]: min 0.3 avg 75.5 max 3008.7 median 0.5 stddev 461.2 Python Rate 5000 Connection rate: 4998.8 conn/s (0.2 ms/conn, &lt;=123 concurrent connections) Connection time [ms]: min 0.2 avg 0.9 max 25.3 median 0.5 stddev 2.3 Rate 6000 Connection rate: 5999.0 conn/s (0.2 ms/conn, &lt;=125 concurrent connections) Connection time [ms]: min 0.2 avg 1.3 max 22.2 median 0.5 stddev 2.6 Rate 7000 Connection rate: 4122.7 conn/s (0.2 ms/conn, &lt;=629 concurrent connections) Connection time [ms]: min 0.2 avg 44.2 max 3013.0 median 0.5 stddev 350.0 As you can see here, the Python version still runs faster with a bottleneck of between 6000-7000 req/s while the epoll Haskell one has a bottleneck of between 5000-6000. Unfortunately I cannot reproduce Don's results.
The audio on my digital camera is pretty poor quality, there was no audio for the telescope footage with cheese, and I didn't want to do a voice-over, so I put up some creative commons music. I happen to like techno sitar remixes.
I was very happy that he took an interest in programming from this project. Both he and my little sister (age 13) have done graphical programming with NXT-G for FIRST Lego League, but neither was very inspired by that programming language.
&gt; If OTOH you only dealt with the ORM tutorial and possibly some very outdated tutorials around the web, its more likely that one might be confused. In that case, would it make more sense to swap the order of the tutorials in the docs? Because I think most beginners will just read the docs in the order presented, which has them learning the ORM before the SQL Expression Language.
&gt; complex, home-grown You mean like *every* enterprise database?
&gt; Also, the expression language (edit: and the ORM Query object too) can do just about any SQL possible these days My Postgres/Oracle DBA coworker can attest to this. He's been surprised at how capable SA is (he's mainly using the ORM). He's only been working with it for a few months, and I'd say he's a solid convert (no more PL/SQL scripts!).
&gt; I'd be curious to hear what your philosophy is on finding the right balance between power and simplicity You didn't ask me, and it's not a "philosophy", but I'd say: Declarative extension. I'm working with two large, complex DBs, and ext.declarative suits my needs about 98% of the time.
&gt; I think SQLAlchemy would be a very good solution if you developed the application and designed the database from the start using it. Wait, are you talking about ActiveRecord? Seriously, though, SQLAlchemy excels when used with *arbitrary* database designs--those created by DBAs to Nth normal form, those hacked together by application programmers, or those that were "designed" over the course of 30 years that originally ran on a mainframe.
It would also help if they were even comparing the same program in the different languages. Comparing an optimized low level epoll loop in python with general purpose high level concurrency frameworks in erlang and haskell is rather pointless, not to mention misleading.
FFFFFFFFFFUUUUUUUUUUUUUU! Wish I knew this a year ago when I needed it :)
Let me ask you this: Why would I bother to write a scheduler to perfectly emulate the VM of Haskell and Erlang, when that is the way to write a web server in those languages? It's just like asking you to write a dynamic runtime into Haskell. It's simply a stupid argument when you look at it.
Let me ask you this: Why do you downvote every single one of my comments? It's pretty lame to downvote someone trying to engage in a discussion simply because you think you are smarter than they are. no one is asking you to write a scheduler to emulate the erlang VM. Your method arguing with people seems to be to take ones statement out context, twist it around to make it seem like a personal attack against you, and then reply in a dismissive, condescending, arrogant tone. Let me re-state my comment: It is pointless to compare an optimized low level epoll loop to a high level concurrency framework. If I were to compare a limited epoll based server in haskell to a python web server using SimpleHTTPServer(which is arguably the default way to write an http server in python), would the results of this comparison mean a damned thing?
&gt; Why do you downvote every single one of my comments? I didn't, but I will if you are going to whine about it. Do you really think that I am the only person who thinks your arguments are completely inane? &gt; If I were to compare a limited epoll based server in haskell And I did compare single threaded epoll in Haskell to Python above which you neglected to read. &gt; to a python web server using SimpleHTTPServer(which is arguably the default way to write an http server in python) SimpleHTTPServer is a complete application. It is not a language construct. Select and epoll is present in the CPython as part of the standard library. Compare this to Haskell which has chosen to embed select into it's own runtime. The scheduler in Haskell and Erlang is transparent. You cannot edit it, and it cannot stand alone as a separate application. Your comparison is utterly inaccurate.
This is 2 years old, while the idea behind the post holds, are either of these projects even close to where they were before?
&gt; Do you really think that I am the only person who thinks your comments are completely inane? Based on the last thread where you were downvoted into oblivion after making a fool of yourself, yes. &gt; And I did compare single threaded epoll in Haskell to Python above Which haskell server? The comparatively low level one Don wrote that is 50% faster than the python server? &gt; which you neglected to read. dismissive, condescending, arrogant &gt; Your comparison is utterly inaccurate. Funny, that is exactly what everyone has been trying to tell you all along. Comparing a scheduler-less epoll loop to a full blown concurrency framework makes no sense. That is why when Don wrote a similarly bare bones server in Haskell it ran 50% faster than the one using the concurrency framework. Again, the only thing you have shown is that epoll is fast, and properly implementing a general purpose scheduler in a concurrency framework adds overhead. "Python is faster" is not a valid conclusion to your findings.
Most people want to go straight to the ORM. In this case, the poster discussed how (s)he has been programming for many years and just doesn't relate to the patterns used by SQLA and has also tried understanding it via source code. This is why I recommended the SQL expression language - for a SQL expert and a "do it yourselfer" who wants to really understand the guts of something before using it, the expression language is the best place to start. That's where SQLA itself started, from the expression language up. To try to understand the workings of the ORM first, not having worked with a tool like Hibernate or similar before, its a steeper hill. the [intro](http://www.sqlalchemy.org/docs/06/intro.html#overview) tries really hard to steer people into one of these two avenues. 
that totally rocks. DBAs are the folks I really want to get on this thing :).
I used to have [this page](http://i.imgur.com/orrUe.png) in my Pylons slide. Things are getting a lot better in Pylons 0.9.7; they've provided a sane defaults, replaced buffet with a simple `import` from `pylons.templates`, improved [documentation](http://pylonshq.com/docs/en/0.9.7/), etc.. It is definitely an improvement, but many of his points are still true.
It sounds to more like ancient arab music.
This is so, so old. Why submit it?
Why not? Is it not relevant today?
[This](http://blog.ianbicking.org/full-stack-vs-glue.html) is a good read about frameworks. Some people prefer glue frameworks and some prefer full stack ones. It is good to have a choice. I do agree with the author that there is value in having the framework take as many decisions as possible for the developer (in particular in matters of security) because the developers is going to make mistakes. There is also value in having a consistent set of components because it favors re-usability (of complex but functionally autonomous MVC subsets of an app). Yet, there is also value in allowing people to re-use their existing views when migrating from an existing app.
it is not. it references tools and approaches that are long gone (Buffet) and claims that Pylons presents a huge array of ambiguous choices (95% of all pylons apps use SQLA and routes, Mako/Jinja are the typical template choices) as its central argument. If any framework is comparable to Zope its [django](http://compoundthinking.com/blog/index.php/2008/09/17/djangocon-and-learning-from-zope-2/) (if we're going to start re-posting old stories).
&gt; 95% of all pylons apps use SQLA and routes, Mako/Jinja are the typical template choices The Pylons home page still cites flexibility as a main goal and one of the top-level bullet points is... a big list of lots of components to choose from. So, um, yeah, Pylons still presents a huge array of choices. I don't know if they're "ambiguous" or not, but if I were brand-new to Python I'd probably be a bit put off. TurboGears' approach of keeping the flexibility but making and *emphasizing* sensible default choices and a coherent default stack is probably a bit better for that audience, and I think is the sort of thing the original article was getting at. (admittedly, I don't really know how more experienced Python devs react, though, because most of the ones I interact with seem to go with Werkzeug for the "roll your own stack" approach)
&gt; If any framework is comparable to Zope its django And handling this one separately: I don't think Zope was a good example, and honestly I might have pointed at J2EE instead as the end result of a process of over-abstraction. Yes, you *can* swap just about anything in or out because all the bits have been abstracted and interfaced and factory'd to death, but there's a point, I think, where you need to just invoke the YAGNI principle and stop abstracting. I don't think Pylons has passed that point, but I *do* think Zope 2 did and I'm damn sure the big Java frameworks have. So I view it more as a cautionary thing.
sigh. [this](http://wiki.python.org/moin/WebFrameworks) is what a "huge array" of choices looks like - dozens, with little indication which are more up to date , supported, etc. People hardly have such an issue with a small framework like Pylons which has clear default choices with a couple of options for basically three elements of it. I don't understand the need to further fling FUD at Pylons (i.e. reposting old stuff) with its tiny market share compared to Django , aren't there better things to focus on ?
Actually Zope 2 was a lot less abstract than was Zope 3. At this point it's not clear where one starts and the other ends, because the stuff that was in Zope 3 has largely been made available as libraries in Zope 2. But... in general, Zope 3 is way more abstract than Zope 2 ever was. Zope 2 *did* choose YAGNI in places that Zope 3 offers plugpoints for (such as the choice of exactly one administrative UI). But anyway... I think Zope's problem is a different one than Pylons' or Django's. The Pylons core is high quality, but Pylons tends to use 3rd party things such as SQLAlchemy, Beaker, Routes, and so on which makes it more difficult to document as a cohesive whole. The bane of code reuse is that that things need to be documented *in context* for people to really understand them. Often that means repetition of existing documentation for a particular integration. This really sucks, and given that documentation is so much harder to write than code, might be an argument against knee-jerk decisions to *always* reuse code. I think James Gardner's book actually does a really good job at putting all the code that Pylons uses into context. This book didn't exist when the OP was made, which is one reason that reposting it here after so much time is a pretty low blow. Zope software is of high quality, but insular, and not very well documented. However there *is* a natural cohesiveness between its parts, unlike the parts used Pylons, because in general those parts aren't really packaged to be used outside of the whole. This is both a strength and a weakness. But all this stuff is just hidden behind a wall of non-documentation culture. If someone *did* take the time to document the parts for civilian consumption, I suspect some Zope-derived framework would be much more popular. Django is more like Zope inasmuch as it's insular and the code is of high quality, but unlike Zope in that its documentation is top-notch, and unlike Pylons inasmuch as it doesn't try as hard to make its parts useful outside the whole. This seems to be what most people want from "a web framework" these days, which is understandable. "It's how easy it is to get started, stupid". And once people get started with something, they'll stick with it for about ever. However, there are places for both approaches. And really one is just a layer that the other sits on top of, even if that layer is well-hidden behind a bunch of integrated pieces.
&gt; People hardly have such an issue with a small framework like Pylons which has clear default choices with a couple of options for basically three elements of it. The home page lists, just as suggestions, SQLAlchemy, SQLObject, CouchDB, Mako, Genshi, Jinja2, WebHelpers, FormAlchemy and Routes, all under a section showing how Pylons "encourages use of your favorite Python components". If I were coming in brand-new I'd probably be a bit intimidated by that, and I think dealing with that is an important thing for any framework which takes the "glue" approach. We need those frameworks to exist, and we need a way to make sure people learn about them, and that means confronting the issue (which Pylons is and has been doing, I should note). &gt; I don't understand the need to further fling FUD at Pylons I don't particularly feel it's "FUD"; there've been a number of quite serious discussions about these types of issues, none of which turned nasty (mostly, I believe, thanks to the people involved). I also don't think that old stuff is necessarily bad; there are some useful points in Adam's post, and it was the catalyst for a lot of discussion, so revisiting it isn't automatically a bad thing. (your apparent inability to respond originally without a "Django is Zope LOL" sort of dig, on the other hand, I could live without)
&gt; (your apparent inability to respond originally without a "Django is Zope LOL" sort of dig, on the other hand, I could live without) there was a point there (which I even stated, as opposed to LOL, which I did not). if we're looking to dig up old arguments and waste time, there's plenty to waste time on.
This is not a critique, just a request for explanations. Is it a reference to [this](http://images.google.com/images?hl=en&amp;client=firefox-a&amp;rls=org.mozilla:en-US:official&amp;hs=uaz&amp;q=blue%20bream&amp;um=1&amp;ie=UTF-8&amp;sa=N&amp;tab=wi) or is there a different reference? If so, why this fish? Why the wedding music? Isn't the change of name - in this case - something closer to a divorce than a wedding?
Well, if you copy/paste the entire section on using Django's ORM, with full examples, how to query tables, etc. I'm sure it'll be just as long or longer, so I'm not sure what the point of the slide is? Should a variety of examples of how to use an ORM (with table reflection vs explicit config, etc), along with a detailed intro to using its query abilities somehow be able to fit into 4 paragraphs? If so, I have yet to see a framework manage such a feat.
That is not a handsome fish.
&gt; The home page lists, just as suggestions, SQLAlchemy, SQLObject, CouchDB, Mako, Genshi, Jinja2, WebHelpers, FormAlchemy and Routes, all under a section showing how Pylons "encourages use of your favorite Python components" Believe it or not, we get ppl coming in with the assumption that if we don't list it, it can't be used. This is exactly why many people assume that Django magically will not let you use SQLAlchemy or Mako, etc. Since its not listed as being usable on Django's homepage, obviously its impossible to use it. This assumption is clearly inaccurate, but this is the perception that many seem to get. &gt; If I were coming in brand-new I'd probably be a bit intimidated by that, and I think dealing with that is an important thing for any framework which takes the "glue" approach. This is why we specifically list that you don't need to decide, notice in bold the "Not sure which one to choose?". I guess maybe that should be larger? The documentation in both the book and on the website only discuss the 95% scenario that zzzeek mentions, the integration is much better than 2 years ago, and unlike back then there is also a very detailed book which I think clarifies many of the things leading to this post to begin with. Is this post obsolete? Well, lets look at what's changed: * Massive overhaul of the PylonsHQ website * Huge increase in documentation, both on the PylonsHQ website and an entire book * Buffet is gone, a basic Python style (exactly like how Django renders templates) is used now * The website and book clearly make decisions for the user to explain how to do things, this was a huge issue in the blog posting * Many of the 'magical' things that resulted in the integrity Adam had a problem with are gone, replaced with explicit forms that look just like Django's or repoze.bfg's Then again, I still think many points were invalid to begin with, as a result of a poor understanding of how things work mainly because the documentation was lacking back then. The one I still do like in this post, is the note on __before__. I think Adam's idea of using a decorator is a great one, and since we're far enough along now that we can drop Python 2.3 support I'll be adding decorator based setup of those things. So overall, yes, I'd have to say the post is obsolete. We should all have better things to do then rehash old debates when so much has changed.
When desktop fabbers get cheap enough that I own one, this is one of the first things I'll do.
That site is horrible.
Yes, it is a reference to the fish. That fish is known both as Zope and Blue Bream.
don't: from stream import * do: import stream and no standard function is overwritten. Is it too hard?
Ugh. Terrible name. Try saying it quickly a few times. It just doesn't roll off the tongue.
Fuck, what a stupid name.
Was the original name Zope also a reference to the fish?
Zope used to be "Z Object Publishing Environment"
Yes, what is the author's take? What's with the appeal to masculinity? Is Python 3 for red-blooded developers particularly? HOOO RAH
The Mounties will rescue Python 3, no fear!
The slide was presented two years ago in local web event about "Conversation from Rails to Pylons, the good and the bad"; that page was in the section about learning curve. I still remembered that page (or the SimpleWiki) was still using SAContext which I spent hours reading only to discover it was already deprecated for scoped_session/sessionmaker. The way you're doing it right now (provide questions on project setup) is already great way of reducing that frustration of "oh what the hell is SAContext and what should I use?" and lowering the barriers to entry of the framework.
Django is a pretty stupid name when you think about it. You forget that though. because it has become the darling of the python web framework world. How a name *feels* changes and evolves over time, subject to personal experiences and perceptions. 
I imagine that 'Bream' will most likely become the colloquial usage, which rolls off the tongue nicely. 
I fell asleep to the repetitive monotone after about 3 minutes into this 30+ minute presentation. Did anyone last the distance? 
Apparently you can use poorly construed benchmarks to prove anything these days
This looks sweet.. I'm currently abusing ctypes for similar functionality (it can be used to generically serialize/deserialize with a few helper methods) Personally, I'm not really a fan of using class syntax for defining these things, as it requires a separate syntax to build the same types at runtime (should you ever need to, say, as part of some kind of generic mapper). Otherwise, awesome &amp; bookmarked! :)
Looks very good, I've been looking for something like this for a while, actually! Unfortunately, it would have helped much more a few months ago. Anyways... I would really encourage you to put more docstrings in, with instructions on what to use, how to use it, even examples - one for the module, one for each class, and one for many of the methods. I would also encourage you to define an [_ _ all _ _](http://docs.python.org/tutorial/modules.html#importing-from-a-package), so that "from protlib import *" doesn't give things like StringIO that are unnecessary. I'm going to go back to playing around with it now!
ok...what does it do ?
I don't think speed was ever the thing holding Plone back. It doesn't matter if it is 3x faster if it is 3x more incomprehensible. Phillip.
When you fell asleep did you do a lot of thrashing? Thrashing. But seriously, I was interested in the presentation, and I'm happy that we'll see improved performance on multicore in the future.
The main advantage of protlib is that it's drop-dead simple. People can look at protlib code and immediately see what it's doing (at least that's the hope). So instead of saying, "Hey I should learn how to use that someday", I want something that will make my co-workers say, "Hey, we should be using that RIGHT NOW". It needs to make someone's life easier even when they're still learning how to use it. I think there's a legitimate niche for this sort of project, even if it's less comprehensive or less performant or less powerful than something like Protocol Buffers. I also like protlib's handling of nested structs and arrays much better than that of Protocol Buffers, and I feel like the logging and integration with SocketServer classes is more useful to the types of servers I've needed to write at my job. Protocol Buffers seem generally more powerful but aren't necessarily the best fit to my needs, plus they're the most useful when both sides are using them, and we have to communicate with servers written by other companies which aren't. I haven't looked into thrift, so I can't comment on that, though I should probably check it out. As for calling `socket.setdefaulttimeout()`, I hesitated to do this, but I ended up doing it because my calls to select need some timeout, and if I call `socket.getdefaulttimeout()` where no timeout has been set, then it will never time out. And I want the user to be able to set the timeout they want, since whatever default I provide may not be to their liking. I suppose I could have just used the `PROTLIB_DEFAULT_TIMEOUT` variable, and if the user wants to affect the timeouts for their program, they can both call `socket.setdefaulttimeout()` and change `PROTLIB_DEFAULT_TIMEOUT`. I figured that making them do an extra thing was just a hassle that they would prefer to avoid, but it sounds like you disagree. However, on further reflection, "explicit is better than implicit" definitely supports your point, so I'll change it this in version 1.1 - thanks for the feedback!
Docstrings are definitely something I'll add for version 1.1, which I'm hoping to have time to release next month. I held off for now because I was focusing on the main documentation. But I really love being able to use `pydoc` to look up specific classes and functions from a library, so I'll definitely do this for protlib. As for `__all__`, I'm embarrassed to admit that I completely forgot about it when writing this library, but you're right and I'll definitely add it for the next version. Thanks so much for the feedback!
Good to see someone explaining easily how GIL works...
Looks really handy. I was thinking about writing the same kind of thing, but now I'll just try using this instead. Thanks!
I definitely see what you're saying about building stuff at runtime, my original 0.1 version of protlib defined structs with lists of tuples, such as [ ("code", CINT, 1), ("x", CFLOAT), ("y", CFLOAT) ] This sort of thing is marginally easier to construct at runtime, since with protlib you'd have to use the `type` function to define your class after making a list of fields from a config file or wherever. Still, I eventually decided that I liked the intuitiveness of Django's ORM and decided to model protlib struct definitions after it. In my next version I'll try to include some functions and maybe a utility for dynamically constructing structs from various data sources. In particular I'd like to have a script that gives you CStruct definitions from an executable or library, assuming it was compiled with debugging info. This should hopefully make the sort of thing you're talking about much easier.
It makes it easy to implement binary network protocols by building on the struct and SocketServer modules in the standard library. So you can easily declare C structs in your Python code, and then when you write a server that sends them back and forth, you automatically get a lot of free error detection and logging. The documentation I linked to explains all of this in great detail in the form of a tutorial with examples. I'm guessing from your question that you thought I should explain some of this in the title of my reddit post?
&gt; you thought I should explain some of this in the title of my reddit post? Actually I didn't. but now you mentioned, yes I think you should have provided some more info in the submission text box. But anyway, this is way over my head, I'm new to Python. No idea what it does so I asked.
I think whomever wrote that meant to say "unanimously".
I haven't ever looked into how the GIL works myself, but from his presentation the New GIL seemed like a rather obvious solution to me. It almost bugged me how he kept calling it "hackish" and "sneaky" when I thought it was just an elegant solution. My (not-so) short summary: previously the thread that had the GIL would give it up every 100 instructions by releasing the GIL and then relocking with the assumption that some other thread would have grabbed the lock. This worked on single-core processors because releasing a lock generally signaled to the OS to switch to another thread. This is not typically the behavior on multi-core processors, so other threads didn't have a chance to get the lock. This resulted in nearly 2x slowdowns when running the same CPU-bound workload in a threaded vs sequential execution. Using the New GIL, the executing thread runs continuously, and waiting threads block on a condition variable with a timeout. If the timeout (5ms) expires and the same thread is still executing then a "you_must_release_the_gil" variable gets set, and the interpreter that does have the GIL releases it immediately. The one clever part of the plan is that rather than relocking the GIL immediately, (which would have been no better than before), the thread that gave up the GIL waits for another thread to signal it has in fact acquired the GIL before it goes back to waiting for the GIL. The prevents the multi-core problem of a single thread releasing-and-relocking nearly instantaneously without allowing other threads to run. There was one test shown to prove the New GIL works well, and it showed only a 0.5 second slowdown on a 25 second sequential execution when moved to multi-threaded. Other than that, the presenter said vaguely that running as many as 100 threads worked with only a 20% slow down.
I noticed one more thing: If you define a CStruct, and then give one of its variables the wrong type (i.e. set point.x = "a string that should be a float"), no error is thrown its not until one runs point.serialize() that one sees an error, and then the error is rather unhelpful, and does not say which variable has the wrong type. Having a check as you assign would be nice, but would have a performance hit - although probably not too important; but it would be nice if it had one at the end.
Good point, I should add a `__setattr__` method to `CStruct` that checks this on assignment. I should also do this check when the struct is first initialized, making sure to specify in the error which field has the conflict, what is expected, and what was given. I should probably also attempt to do some type coercion in simple cases. If someone assigns and `int` to a `CFloat` field, that shouldn't be an error, and if someone assigns the string `"5"` to a `CInt` field, then it would be nice if the library handled that as well. I'll put this on my todo list for version 1.1, thanks so much for the suggestion!
Since I have no interest in Python 3, or as I like to call it: Not Python, is there any plan to port this over to Python 2.x ? 
Also the plone tax (excessive start-up time) on my workstation has gone from 20 seconds (Plone 3.3) to 10 seconds (Plone 4.0). Still a pretty crunchy start-up time, but it is a big improvement.
&gt; marginally easier to construct at runtime, since with protlib you'd have to use the type function to define your class after making a list of fields from a config file or wherever. &gt; Still, I eventually decided that I liked the intuitiveness of Django's ORM I agree — with the automagic field ordering behavior (with `Class.instances`), creating fields at runtime will be far less intuitive than just passing in a list of tuples; however the syntactic benefits make it more than worth it.
The Beaze!! I took two operating systems courses from this man and they were the hardest programming courses I ever took. He was an amazing teacher and practically convinced me to go to grad school for computers. Really glad to see he's still doing stuff with Python after leaving U of C.
Maybe I misunderstand some of it, but does the 'New GIL' still effectively makes a single Python process use a single core? I mean yes the threads may be mapped onto different cores, but only one will be used at a time, because there is only one GIL per python process. The C+- stuff you call could still do its own threading asynchronously besides the GIL, but that is nothing an average pure python script will use.
Yes, that's true. One GIL still restricts one thread to be executing at a time, but at least it's not _worse_ than that. Lots of people want to remove the GIL. The speaker said that it was one of Unladen Swallow's goals, although it might have been bumped to a lower priority. Its kinda said when people are like "Look! multi-threading is nearly as fast as single threaded applications!", but there just isn't any hope in the near future to get rid of it :-(
Most names for projects are pretty silly. I mean, Python is named after Monty Python's Flying Circus, and there's Twisted and "TurboGears" and so on. It's okay to have a memorable name identying a project and it's okay if that's silly. Just keep the names clear in the code itself.
How'd you get the python doc theme for your documentation?
The name change happened because: * Zope 3 was named in a bad way. It was originally intended to replace Zope 2, but that never happened. It's a completely different beast. Having Zope 3 as a name is very confusing as people still expect they can upgrade from Zope 2 to it, and they can't. * we ended up with the strange situation that Zope 2 *used* Zope 3, i.e. Zope 3 libraries were used in Zope 2. * We also got Grok, also building on Zope 3. * we realized there are two things Zope 3 was doing: a web framework, and a toolkit of reusable libraries we use in other projects. * last year we renamed the toolkit to the "Zope Toolkit" * now we've renamed Zope 3, the web framework, to BlueBream. The Zope Toolkit is the foundation of Grok and Bream (and Zope 2, though currently this is unfortunately debatable. Ah, politics..) 
Yeah, it certainly does (I was in the meeting). Heh. :) We should've caught that, but I'll try to remember to fix it. 
I used Sphinx, which is the same documentation tool used by Python. I mostly used the tutorial at http://scienceoss.com/use-sphinx-for-documentation/ to get me started, but I also occasionally used the official Sphinx docs at http://sphinx.pocoo.org/
Since it's built on the [struct](http://docs.python.org/library/struct.html) module from the Python standard library, you can set the byte order using the [BYTE_ORDER](http://courtwright.org/protlib/#BYTE_ORDER) module variable in the protlib module.
This is great, I almost made something similar myself, but was too lazy (having implemented similar in java before). Can you add the size in bytes to the "Basic Data Types" section, for the lazy among us ?
What is it about Python 3 that you find so distasteful?
Good idea, I'll put that on my todo list for version 1.1 - thanks for the suggestion.
Cool. Although... I'm not sure that it should accept other types... like "5" for a float. Yeah, in Python you should normally try to do that... but when you're doing something so intricately tied to types, maybe you should. On the other hand... if you force the native python types, you prevent anyone from using special types (like decimal.Decimal), so maybe you should just accept anything that can be coerced into a float.
&gt;I should probably also attempt to do some type coercion in simple cases. If someone assigns and int to a CFloat field, that shouldn't be an error, and if someone assigns the string "5" to a CInt field, then it would be nice if the library handled that as well. Just pass the data to the constructor of the underlying type and see what it thinks.
Thanks.
The community became aware of the fish pretty early on though. :)
It might be unfair yet the development and community of Django is arguably "better". Symptomatic was when I tried to mail James Gardner because some pypi download link was broken. Guess what, the mail addressed to his official company address failed. :( Core Pylons components are developed by 3-4 people, compare that to Django... *envy* Don't get me wrong, I really like Pylons but I don't see much progress in the last months.
go !
I agree that in general this is all I should do - the only time I plan on going above and beyond is that if you call `int(5.6)` then it will silently truncate the data. I'd rather first check whether any precision is lost and then have a `CWarning` if there is, then still go ahead and do it. In general, I'm trying to be paranoid about potential errors, because this library will be used by my co-workers, most of whom are VERY new to Python, and I want to detect every possible source of confusion/error.
&gt; We seek the following from the BDFL: &gt; - Approval for the overall concept of adding a just-in-time compiler to CPython, following the design laid out below. &gt; - Permission to continue working on the just-in-time compiler in the CPython source tree. &gt; - Permission to eventually merge the just-in-time compiler into the ``py3k`` branch once all blocking issues have been addressed. &gt; - **A pony.** 
Was kinda disappointed not to find any further reference to the pony.
you are biased
i talked with you on #pylons not long ago, about repoze. If you want to see "movement" check source code commits, on bitbucket - 1.0 is about done. Btw. actually what part of pylons is not working/you need bug fixed? Its stable and mature software. To me it doesnt matter if pylons would be developed by 50 ppl team. Whats the difference if what is provided now is all we need ? Not sure what to envy ? that it uses orm that doesnt allow multiple db connections ? good luck i could not do a single project with django - it just doesnt meet my requirements. I'd be REALLY interested in what progress you are expecting from this package, 1.0 is about cleanup of code. There are other libs that do a very good job out there like werkzeug that are 0.5 version etc. and still very usable. I think you are better of with django if it suits your needs better than pylons. Judge the software based on what it does for you, not version number. Oh, and for problems with pylons if there are any (easy_install pylons works fine ? ), contact Ben Bangert, im not sure if J Gardner still works on pylons ?
they are all broken, python needs jars (it has eggs) with optional dependency metadata included. Take the GOALS of OSGi and simplify the implementation by an order of magnitude. Ship that. Each monolithic package should include the platform specific stuff. You can take the fat binaries and trim them down once installed. Python is very simple, it has something like a classpath, the PYTHONPATH. It can already read the contents of zips that are on the PYTHONPATH. Packaging and distribution should not be the cloud of suck that it currently is. For instance, you can look at the contents of the egg, unzip -l PyShell-0.5-py2.5.egg Archive: PyShell-0.5-py2.5.egg Length Date Time Name -------- ---- ---- ---- 1 08-26-08 20:33 EGG-INFO/dependency_links.txt 68 08-26-08 20:33 EGG-INFO/entry_points.txt 349 08-26-08 20:33 EGG-INFO/PKG-INFO 195 08-26-08 20:33 EGG-INFO/SOURCES.txt 8 08-26-08 20:33 EGG-INFO/top_level.txt 1 08-26-08 20:33 EGG-INFO/zip-safe 3528 08-26-08 20:33 PyShell/__init__.py 3636 08-26-08 20:33 PyShell/__init__.pyc -------- ------- 7786 8 files There is no reason that a simple platform independent distributable can't be pulled down from an easy to interrogate package repository. And that my code gets the version I specify and only that version. 
Benchmarks == :-(
Now I have that punk song in my head.
A pretty, rendered version of the markup is available here: http://www.python.org/dev/peps/pep-3146/
Given the amount of diversions and integration work, I think it's pretty nice that the performance is already slightly better. They haven't even had much chance to focus on performance yet.
which one ?
they must have [django envy](http://www.djangopony.com/)
it's not Python 2.x ?
I think [Guido has all the ponies he needs](http://farm4.static.flickr.com/3569/3401670115_54e1d32200.jpg)
I'd challenge the performance of a nine year old's Python async IO based code to facebook's Erlang code. Heck, I'd challenge my first Pascal DOS applications to their PHP code. Still, Erlang has a more elegant concurrency model and that article is comparing apples to oranges.
[gay bar by electric six](http://video.google.com/videoplay?docid=-3319967978568410735)
I was afraid the python moratorium caused us to wait two years to have the new GIL in an stable Python. But the "allowed to change"[1] clause banished this fear and gave us hope: "E.g. removing the GIL would be fine ..." [1] http://www.python.org/dev/peps/pep-3003/#id16
The Django ORM style is a really nice hammer, I'd like to bash more things (already made something nice in my last job which is closed unfortunately). This library is pretty similar in usage to Struct in the java library Javolution.
When you look at the memory usage you don't really want to use that on an embedded device ... but on a server the 30% speedup for django is great.
The continual redefinition of what exactly Python *IS*. I've been using Python for over 10 years now. Additions I can deal with, breaking changes however are a pain in the ass. I would rather spend my time writing new programs than reviewing and upgrading my existing code base and all the different little applications and scripts I've written. Just to conform to this new redefinition of what Python is? The effort wouldn't provide any value to me in the least. I have not in the 10 years of using python run into a single situation where the language didn't allow me to write code to complete a task. And I rather liked the simplicity of the language. If I want to develop a new Python application using SQAlchemy, Numpy, and Matplot lib, which version of Python would you recommend I use TODAY for new development? I think it's going to be an uphill battle obtaining widespread adoption of Python 3 because of the huge third party library support of Python 2.x. Until third party library developers get serious about supporting Python 3, it's not even on the table as an option. 
Shortening BlueBream as Bream or BB is acceptable. http://bit.ly/6wcoMw
Great news. &gt; We have tended to focus on raw performance, and we have not yet made a concerted push to reduce memory usage. We view reducing memory usage as a blocking issue for final merger into the py3k branch. We seek guidance from the community on an acceptable level of increased memory usage. Let's hope this won't bite at the last minute 'cos the memory usage shown is a bit concerning.
No! Unless it works.
Thank you for something really cool, I think I already have a small idea for it. Also thank you for a sane license for a library, I find that Python libraries has a tendency to be permissive which is deeply deeply appreciated. Possibly due to the Python license itself.
It is interesting code, but I think it is not worth to lose compatibility with normal python system and everything every python program has as an assertion, like each module being a single file. Other people reading the code won't understand how things got mixed until they peek deep, so that's a no-no IMHO.
I don't like Dive into Python. There's something in its style that bugs me to no end. Thankfully there are other books that fit me better, like the wonderful: [Beginning Python Visualization: Crafting Visual Transformation Scripts](http://www.apress.com/book/view/9781430218432).
During development (mostly wxPython &amp; numpy), I use [hotswap.py](http://www.krause-software.com/hotswap/). Pretty old, but still does the trick :-)
so far pyside is supposed to be api-compatible with pyqt, so you can adapt those examples
Except that Django doesn't run on py3k just yet, but hey, it will one day. and then we'll all have ponies.
So now Plone is a fast nightmare. 
I still want it on 2.X............. I cannot wait until scipy/numpy to port on 3.2.
[SVG](http://www.google.com/#hl=en&amp;source=hp&amp;q=python+svg&amp;aq=f&amp;aql=&amp;aqi=g10&amp;oq=&amp;fp=292ac4760832f3c4) maybe?
This looks very intriguing! I would like to try integrating it into this [Bloom Artificial Life Engine](http://launchpad.net/bloom), which is running in Python with QT and Panda. Does anyone know if running Panda or something is going to conflict with Livecoding? I want to be able to reload alife objects as I work on their algorithms.
hurr, SVG doesn't really offer much more than lines, and simple primitives - as far as I can tell - which makes it no more useful than anything else I've come across! thanks anywhom though!
&gt; other books that fit me better Are you new to Python ? is that book intended to *teach* Python ?
Beyond the modest performance improvement, I only saw downsides (greater memory footprint, longer startup). What's the benefit of the JIT beyond performance?
Yes, but in a data-manipulation (visualization) centric kind of view. It has a very get-things-done, KISS style that I found particularly appealing. It shows you how to do stuff while learning python. Not everything in it relates to visualization (don't be put off by its title). Other books I like better than DIP: * [Programming in Python 3: A Complete Introduction to the Python Language](http://www.qtrac.eu/py3book.html). * [Beginning Python - From Novice to Professional](http://hetland.org/writing/beginning-python-2/). * [Learning Python - 4th. Edition](http://oreilly.com/catalog/9780596158071). * [Programming Python, 3rd Edition](http://oreilly.com/catalog/9780596009250).
you own/read ALL this rather big books ???!!!
I've read them, yes. I've only skipped, let's say, 1 or 2 chapters in some of them, but read all the rest (and as you should be aware by now... English is not my primary language, but that doesn't seems to stop me :-P).
Strange, I normally license things as GPL. I may relicense it, I will have to think about it.
I don't understand quite what you are saying. The normal Python module system is still there, and what it provides can be imported by the scripts managed by this system. The scripts managed by this system can import other scripts managed by this system, and dependencies are resolved during the loading process.
Not entirely true. GIL restricts CPU-bound threads, IO and C level modules can and do release GIL when it's not needed. This is why you should use multiprocessing if you want to utilize more CPU. And since multiprocessing uses threads for communication -- better threading means better multiprocessing as well. Also you were not paying attention -- removing GIL is currently out of scope of unladen-swallow, for reasons given in the video (ie. somebody tried that very method already and failed).
welcome to java *edit* i.e. using python the way java is
Good architecture, much verbosity. I think it is better to use repoze.bfg instead of Zope now.
To answer my own question, from "[Performance Retrospective](http://www.python.org/dev/peps/pep-3146/#performance-retrospective)": &gt; Our initial goal for Unladen Swallow was a 5x performance improvement over CPython 2.6. We did not hit that, nor to put it bluntly, even come close. ... &gt; Can an LLVM-based CPython JIT ever hit the 5x performance target? The benchmark results for JIT-based JavaScript implementations suggest that 5x is indeed possible, as do the results PyPy's JIT has delivered for numeric workloads. The experience of Self-92 [51] is also instructive. &gt; Can LLVM deliver this? We believe that we have only begun to scratch the surface of what our LLVM-based JIT can deliver. 
Yeah, I realized after pressing "save". :) As said, Pylons suits my needs (at least much better than Django which I tried and disposed). That's also the reason why I'm using it. I said it already and I'll repeat it again and again: The ORM of Django is just terrible. However: It does matter, how many people develop. Imagine Michael Bayer losing interest in SQLA/Mako or getting hit by a bus...we'd have hell of a problem. It's not (only) about speed of development, it's more about the future-proofness. The broken pypi was formencode IIRC, that's a project of James. Also my envy is more about the community itself: The #django is lot fuller and active at any time of the day. People on #pylons are always helpful, no question but there's not always someone around. Look from when the latest snippet is...etc pp. Again: I don't mean to talk pylons bad. I'm running several projects with it. I'm just a bit afraid about the future.
you worry too much, someone would pick up "abandoned" projects :] also author of formencode is ian bicking, and he is reachable ;] also more ppl develop that stuff than you think ;-)
I never did anything with zope directly other than some customization on a plone instance. It's a very impressive beast but it's wild and dangerous. Where you can get up to speed with django&amp;pylons with a couple of hours and a one page tutorial you will really need a solid zope book. And it can be a real pain in the ass to debug the simplest thing when through layers and layers of acquisition you don't even really know which code is being executed anymore. When I look at zope I imagine a clan of hardcore design patternist J2EE developers who felt there wasn't enough layers of abstraction and decided to each their holy grail of "everything is a module even the module loader" in python. And ZCML must die. I will not code in XML. now effing way. I'm no expert but I had to poke the beast with a stick once and I almost lost an arm in the battle :) *edit* I'm also curious to know what people think of zope.
Looks cool. How about a wrapper program that will run any Python script and automatically replace normal import's with livecoding ones? That way you don't have to worry modifying your code once you've tested it.
The one time I tried it, it reminded me of Sharepoint, which I really didn't like. Also, I found setting up Plone to be way too complicated.
I've only had to deal with it from a sys admin point of view, and I hated it with a passion. Anything that makes deploying J2EE applications a walk in the park should be taken out behind the building and shot.
Grok cuts down on the verbosity too, though in a different way. Grok makes it easy to configure things with the Zope Toolkit. The advantage is it connects with the greater Zope world that way (Zope 2, Plone, Bream, etc). BFG simplifies also by simplifying APIs, but in addition has simpler innards. That can be a great advantage, and can lead to new approaches and new features, but there's arguably less around to reuse as well. 
While I think one can make *some* argument that there are too many Java-style patterns and complexity in Zope code, plus perhaps an aim to support large business applications that reminds of Java frameworks, overall Zope code is definitely not "Javonic". Any flexible system will have indirections that also contribute to complexity, though we're doing our best to cut down on that. One of the main designers of Zope, Jim Fulton, came to Python from Smalltalk in the mid 90s. He was also one of the drivers behind the creation of new style classes and other valuable things in Python. 
I stand corrected, it was "formbuild". :)
For me, the main reason to user Zope or repoze.bfg is component architecture (zope.component) and declarative configuration (zope.configuration).
J2EE wasn't a big influence on Zope (in any of its incarnations) at all. Parts of the Zope Toolkit (which made it into Zope 2 and therefore Plone) do suffer from an overemphasis on flexibility and therefore get more indirection and complexity than is actually needed. Acquisition is unique to Zope 2. We long (since 2002 or thereabouts) that it's more trouble than it's worth. More modern Zope Toolkit based systems such as Bream and Grok don't use it. Concerning ZCML dying: I've been the cause of death of most ZCML in Grok, back in 2006. You don't need ZCML to do almost anything with Grok. I think the notion of explicit configuration embedded in ZCML is a good idea, but I think the cost of splitting component configuration from actual Python code is far greater than the possible gain. 
Note that BFG to my knowledge still uses ZCML for many purposes, though it's definitely trying to cut down on the quantity and also offers some alternatives (using the martian library which came from the Grok project, and offering an "imperative configuration" Python API). Grok does away with most ZCML. It's still there as it needs to interface with libraries that do get configured using ZCML, but you shouldn't need to write ZCML in your own code. Just tell your classes how you want them hooked up in the component architecture and they'll be there. 
BFG and Grok both are built on zope.component and zope.configuration. These, together with zope.interface and zope.schema are the most important foundational libraries of the Zope Toolkit. Grok uses a mechanism called "Martian" to do configuration. Using Martian, grok helps to automatically register things in your code with zope.component, and it uses zope.configuration to do that. An interesting example to study to do just component hook up is "grokcore.component": http://pypi.python.org/pypi/grokcore.component 
Ouch, I meant TAL not ZCML. Well, &lt;both&gt; of them made me irate at one point or another.
It's frightening.
I wrote a bit about Plone and kind of a [comparison of Grok and Repoze philosophies](http://blog.ianbicking.org/2008/11/06/where-next-for-plone-development/) a while ago, but I think it's still valid. Repoze is probably even less Zope-oriented than it was at that time, as they continue to be focused on a simple core, as opposed to Grok which is more about a simpler facade over Zope ideas.
[BFG](http://bfg.repoze.org) 1.2 (currently released as 1.2b1) makes ZCML optional entirely; using ZCML is not required at all. The simplest BFG application is literally this: from webob import Response from paste.httpserver import serve from repoze.bfg.configuration import Configurator def hello_world(request): return Response('Hello world!') if __name__ == '__main__': config = Configurator() config.begin() config.add_view(hello_world) config.end() app = config.make_wsgi_app() serve(app, host='0.0.0.0') BFG also lets you use grok-style decorators for declarative configuration, but does not use martian for that purpose (it used to use martian, but it no longer uses it). Oh and lest anyone be confused about "BFG" vs. "repoze": repoze == a "brand" repoze.bfg == a Python package that implements a web framework BFG == colloquial name for repoze.bfg There is also a reasonable description of how Zope, Grok, BFG, and "Blue Bream" (new name for what used to be called Zope 3) relate to one another [here](http://wiki.zope.org/bluebream/BlueBreamName).
I think you're thinking of Plone. Plone is a CMS that is written on top of Zope, but it is not itself Zope.
I try not to. Honestly, I have no idea why anyone would go the lengths they have to make duck typing *more* of a PITA. Edit: Also, try to explain how the `implements` function works, then consider: `If the implementation is hard to explain, it's a bad idea.`.
Looked into generating [TikZ/PGF](http://www.texample.net/tikz/examples/)?
Ah well, while Zope Page Templates (using TAL) are still the dominant system, you can use alternative templating systems as well. (for Grok there's a megrok.jinja and a megrok.cheetah among others) 
Huge bloated mess.
I don't understand how duck typing is affected at all by Zope in your opinion. "implements" is a class annotation that allows you to tell a class what interface is supplied by instances of of that class. An interface is a description of methods and attributes of an object. Interfaces can extend other interfaces, just like classes can. Code can then be written that checks whether an interface is provided by an instance, much like how isinstance works. In addition, code can be written to dispatch on interface - if I get an ICat, I'll do this, if I get an IDog, I'll do the other thing. You can also dispatch on classes by the way in case interfaces are overkill (they often are in non-framework code). The implements function is a class annotation. It gets called in the class body, like this: class Torvald(object): implements(ICat) and affects the class itself. This is a hack using sys.getframe() to affect the class attributes. If class decorators had been around in the year 2000 or thereabouts when this function was created first, we'd have used those: @implements(ICat) class Torvald(object): pass We'll likely eventually switch over to those. The implementation isn't that easy to explain because it's a hack. It's a fairly harmless hack in my mind as it does little damage - I've seen far worse hacks, including in the Zope Toolkit codebase. :) 
Was this before it started to use setuptools and buildout or after? Buildout has made deployment a lot easier, though of course deployment automation introduces its own challenges. 
I think zope ate my brain. This post from mcdonc sums up the ZCA pretty well: http://plope.com/Members/chrism/zca_thoughts_summary 
You seem to have missed Grok's progress since then. We've just spent a whole year simplifying the underlying dependency structure of the Zope Toolkit. Grok was a major driver of this project - in fact I just spent two days on a sprint working on this, and I helped kickstart that project about a year ago. Over the year it also got a huge push from developers that use Zope 2 and Bream. This is in order to simplify the underlying code in Zope, not to simplify the facade. We want to reduce the complexity of the stack. I realize that the dependency refactoring is not the end of the road there, but it's a necessary condition if you strive for backwards compatibility. Now that we've extracted a lot of code we really use, thrown away code that we don't use, and have straightened out the relationships between what's left, it becomes more feasible to replace bits with simpler bits. Still hard, but that's the price of evolution. Here's a report from a year ago on this with a vivid graphical demonstration of what I mean. We've come a long way since then: http://faassen.n--tree.net/blog/view/weblog/2009/01/29/0 
I usually explain "implements" like this: "implements" attaches a marker attribute to the class that gives the class and instances created via the class an identity separate from the class type itself. The same marker attribute can also be attached to other classes. It's no different than saying class1.type = 'something' really. The marker that is attached to the class via "implements" must be an "interface" object. An interface object's main function is really just to have a guaranteed unique identity. It can optionally describe the methods and attributes that the thing it's attached to is supposed to implement. A class which possesses the marker is said to "implement an interface." Two classes which posses the same marker are said to "implement the same interface". Using an interface object as a marker rather than using a built in type, like a string as a marker, is slightly more useful because an interface object can it can have its own inheritance hierarchy, which is useful in the case that it does describe methods and attributes. "implements" itself just attaches a marker, but there are also helper functions that allow you to make assertions about whether a class which has an interface object marker attached to it *actually* implements (in terms of attributes and method signatures) the interface object its attached to. This can be handy for ensuring that someone has created an implementation of a "plugin" which will actually work when "plugged in". That's about it. However, outside the context of "the component architecture", none of this is really more useful than just saying "class.type = 'foo'". The "component architecture" is another topic entirely.
I am thinking of Plone, but I had to get Zope working to get Plone running
It's dope. Never used it actually. I started with web.py and have since started using Django cuz django is the bomb and so is ORM and django signals and all the other stuff that comes with it.
oh wasnt that project abandoned ? ;-)
Wow, that's super cool - and if I didn't have to package the unit up... as an exe... I'd definitely look into it more, but... from my understanding that wouldn't be so easy! Thanks though! /Will definitely keep this in mind for myself... very nice toy/tool!
I don't think that getting Zope working is very painful. It's pretty straightforward. Most justifiable complaints here are about developing software in Zope after you get it installed.
I think that [BFG](http://bfg.repoze.org/) would be what one would expect as the 'next step' for Zope. If you are an experienced Zope developer, it's very refreshing. It feels like you are working on a version of Zope that you've been waiting for so long and allows you to have fun while working. If you're not experienced in Zope, it doesn't force you to understand ZCA with all the goodness of aspect oriented programming up front. ZCA will naturally start to reveal itself when the need to build reusable components appears.
And patch will never actually be accepted in 2.5.
zope.interface is super awesome. The rest seems like overengineering.
What in particular about it do you find awesome?
Try [lxml.objectify](http://codespeak.net/lxml/objectify.html).
why?
Because 2.5 is so far past being open to feature it's not even funny. 2.6 is only accepting backports of bug fixes, the only branch that it's even remotely possible to be backported to is 2.7.
You're right, but it still leaves developers compiling their own python env (which admittedly is very few) access to the new GIL... I just wish this were some lovin' for 2.6 instead of 2.5...I might actually be inclined to try and apply the patch for a build myself. =(
i guess got very preoppinionated by seeing many zope tutorials that are completely full of completely painful boilerplate
And the patch for 2.7 is here: http://bugs.python.org/issue7753
2.6 is in maintenance mode only.
that's exactly what i wanted. thanks!
Not exactly gotchas. If anything that would be a misunderstanding of operators on the coders part.
If you think that's bad, try it in Javascript. 0 in [1, 2] -&gt; true! 2 in [1, 2] -&gt; false!
Yeah, `in` isn't one of the good parts of that language.
That's right. Developers are stupid and they can't be trusted. Let's give them the "Straight jacket of frameworks".
There's always so many gotchas with code reloading... deep references can live anywhere. I'm really of a mind to just kill everything and start over on any code change. 
Here's a blog post about [Explaining Why Interfaces Are Great](http://glyph.twistedmatrix.com/2009/02/explaining-why-interfaces-are-great.html).
Check out: http://construct.wikispaces.com/ Really cool - and takes your concepts farther
For libraries I see little or no reason to go GPL, programs, yes. But libraries should be about spreading a common API or functionality. We all benefit from standards for those cases.
oh the irony! :)
It does force you to ignore your package management system in favor of buildout mechanism if you want to do anything useful. apt-getting it is easy then you realize you can't understand any doc so you restart from scratch using a tarball unsure of whether you're doing it right.
There's a language-design principle named after someone (whose name escapes me at the moment) for having 0 and 1 be equal to the false and true boolean values. Python, obviously, does that, and in fact goes all the way with it: `bool` is a subclass of `int`, `True` and `False` are the only two instances and have numeric values 1 and 0, and you can do arithmetic on them: &gt;&gt;&gt; True + 1 2 &gt;&gt;&gt; 5 / False Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; ZeroDivisionError: integer division or modulo by zero
Curious, what's the advantage of this unnamed principle?
Bug/Feature... the old GIL can be considered a pretty big bug, but fair enough this is a big change so I don't see it going in... still... Anybody want to build the Windows version of python with this patch in and post it somewhere ?
[pep8!11](http://www.python.org/dev/peps/pep-0008/)
I've not used ZOPE but did spend about 4 years developing a CMS using the ZODB. I really love OODBs but have finally ditched it for using Django. Part of the problem is I don't think Zope will be moving to Python 3000 any time soon.
I used it for about a year, and found it a bloated mess. Everything has to happen inside Zope, which makes it difficult to do simple things with files in the file system (like images or text files, including Python programs). I tried a few things afterward, and a couple of years ago settled on CherryPy, and have not looked back. SkunkWeb is also nice, as are web.py, pylons, and other lightweight frameworks. Much simpler and more transparent, more Pythonic, and easier to see what's going and re-use code.
Needs more pony.
That's because bool() in Python 2 is a hack. It's fixed in Py3, I presume.
`if test1() + test2() == 1: # xor`, also generalizable to k-ary logic, in two ways even, as a function that is true when exactly one of the multiple alternatives is true (or exactly one is false). By the way, while I'm at it: you should never ever use that `really_in` function. Never. Ever. Or at least put an `assert y is True or y is False` there. &gt;&gt;&gt; def really_in(y, xs): ... for x in xs: ... if y is x: return True ... return False ... &gt;&gt;&gt; really_in(200 + 1, [201]) True &gt;&gt;&gt; really_in(300 + 1, [301]) False
I think you're talking about Zope 2, correct? This encouraged a pattern of putting code in the object database (though it was certainly possible not to do so). The modern modern Zope frameworks such as Grok or Bream store code in the file system. 
I think that's a legitimate complaint. Zope 2 (created back in the 90s, mind) has a lot of painful ugly boilerplate. ZTK boilerplate (ZCML) is not *ugly*, it's just totally verbose and therefore painful as well. Because it isn't ugly, Grok was able to automate the boilerplate away. You won't find a lot of boilerplate in a Grok tutorial. 
Indeed, see his "workaround" code. There he defines a "really_in()" where he performs object-identity tests instead of what Python's "in" does: comparison tests. But Python's docs say: &gt; **5.9 Comparisons** &gt; The operators *in* and *not in* test for set membership. and a few words further, they add: &gt; an object is a member of a set if the set is a sequence and contains an element equal to that object. See how it mention objects, not values. For values one would expect comparison tests ("=="), but for objects... it does not mentions if equality means same identity or \_\_cmp\_\_() returning True. Ok, it does mentions it: &gt; **3.3 Comparisons** &gt; Instances of a class normally compare as non-equal unless the class defines the \_\_cmp\_\_() method. but in a different section.
This seems to be merely misunderstanding of how the 'is' operator works in python. 'is' compares objects. Obviously False is a bool, 0 is an int. If you do int(False) is 0 it evaluates to True. Sounds like he's just shoehorning something he's used to in another language into python to act the way he's 'used to' instead of learning the pythonic way. 
How am I biased? I've been using distribute for a long while now, it is basically the setuptools codebase with a *real* community and developer team behind it.
OMG PONIES!!1!1!
± is not used because not everybody uses Python 2.6 yet, and/or uses terminals capable of printing ±, or software that can conveniently parse utf-8 output.
Thank you! 5+/-0.3 is a good idea, which I just implemented (version 1.2.2). I had discarded it at first because it takes more space than 5+-0.3. But I do prefer clarity, and 5+/-0.3 is now the output form.
That looks pretty good, and I'm definitely impressed with the amount of documentation. I'll have to spend more time looking into this, even if it's more focused on the data structures and less on the command-dispatch logging server aspect than protlib. Thanks for the link!
That's a legitimate complaint. Note however that buildout is a tool that lets developer share code. That's its primary goal; a nice side benefit that it can help deployment. A mechanism to help developers has different use cases than a system package management system. A system management system packages things so that system administrators and users can easily install it. It generally packages relatively mature packages. Its level of granularity is typically rather coarse. A developer package management system needs easy ways to start hacking new source code, use possibly immature libraries others have made, allow a development sandbox per project, and often has a finer level of library granularity. Developer package management systems are less familiar to people than system package management systems, and people often equate the use cases. There are definitely areas in the distutils/setuptools/buildout stack that can be improved, even vastly. But buildout is a mighty fine development package management system nonetheless. 
Ancient painfull times... Now we can choose, and most don't choose zope.
The ZCA (zope.component, zope.interface, zope.configuration) already mostly runs (or can be persuaded to run) on Python 3. But I don't see why "runs on Python 3" is a metric about whether to use something or not, because almost *nothing* runs on Python 3. Zope has problems more fundamental than that.
I still don't see how this has anything to do with Zope. The *Plone and Grok* communities require you to use buildout, or at least strongly encourage it. But that has nothing to do with Zope. Zope can be installed via an OS package management system, and it will work fine. These days, you can even "easy_install" Zope 2. But I suppose making the distinction is useless, at least I've never had success making it in the past; Zope is always blamed for the behavior of its children.
If you've never used it, why do you feel qualified to pass judgment on it? Django's ORM is fine, but it's just a fake layer on top of SQL. If you really want to use Python objects for storing data, you could use a real object database, such as Zope's ZODB. As for signals, in Zope they're present and called "events", along with many other interesting features that are mostly absent from all other frameworks.
You can find some examples converted here: http://qt.gitorious.org/pyside/pyside-examples
I like it
From the ZODB mailing list, the devs were saying the Persistent class hooks a lot of low level things in that aren't able to be upgraded. AFAIK, ZODB won't work on Python 2.6 either, which is my standard Python interpreter.
I think you have incorrect info. ZODB works fine on Python 2.6. We run sites in production on this combination. It runs on Python 2.7a too. There has been work to move the Persistent class into "plain Python": http://svn.zope.org/ZODB/branches/elro-python_persistent/ . If that shakes out, it will be nominally easier to move the code to Python 3 as well as to support non-CPython platforms (like Jython and IronPython). But it's not really a priority, I don't think. As I said, Zope has problems more fundamental than not running on Python 3 and non-CPython platforms, and, in any case Zope != ZODB. 
So far as I know RabbitMQ and CouchDB are written in Erlang for correctness rather than for speed -- that is, for the sort of tricky concurrent programming they require, Erlang makes it easier to write a server that does not have bugs in the multiprocessing code.
I was just making a stupid rhyme. Get over yourself.
In Python, it's not type coercion. Again, in Python `bool` is a subclass of `int`. which means that `True`, while being used mainly as a value of type `bool` is simultaneously a valid `int` with a numeric value of 1.
Makes sense - isn't the idea behind the moratorium to allow other implementations (Jython and the likes) to catch up with Py3k - the new GIL would effectively be a performance improvement to CPython
My question is this: In the python command line, since the bitwise XOR operator is ^, when I enter 2^2, 3^3, etc. any form of x^x, I get 0. This makes sense. What I don't get is why when I enter 2^7, I get 5. Shouldn't this be 1 since the operator just compares whether they are the same? I tried finding a pattern with this but I don't get it. If I put in 7^9, I get 14, but 100^101 gives 1. Can anyone explain to me how this works? Thanks!
It's a [bitwise XOR](http://en.wikipedia.org/wiki/Bitwise_operation). 7 : 0111 9 : 1001 14: 1110
That's the whole point: it's bitwise. It works on the individual bits of each integer operand. Perhaps it'd make more sense if you looked at it like this: &gt;&gt;&gt; bin(2) '0b10' &gt;&gt;&gt; bin(7) '0b111' &gt;&gt;&gt; bin(2 ^ 7) '0b101'
XOR is a bitwise operator. 2 is stored as 10 in binary (that's one zero, not ten), and seven is stored as 111 in binary. 111\_2 (base 2) ^ 010\_2 = 101\_2 = 5\_10 (base 10). XOR doesn't compare whether two digits are the same, per se. If a and b are 1 bit each, a^b will return 1 if *either* a is 1 *or* b is 1, but not both. It will return 0 if both a and b are 1, or if both a and b are 0.
Because that's what 'bitwise' means. 2 ^ 7 = 5 = 0b010 ^ 0b111 = 0b101 (if you're using Python 2.6 or higher, the "bin" function shows the binary representation of an integer. Fun for the whole family.)
Isn't this was the LGPL is for? http://creativecommons.org/licenses/LGPL/2.1/
It's a *bitwise* XOR. That is, it's comparing each bit in the number. 2^7 is `0010b2 XOR 0111b2 = 0101b2 = 5`. 7^9 is `0111b2 XOR 1001b2 = 1110b2 = 14`
It's bitwise, which means it operates in each bit separately. 2, in binary, is represented as 10. Seven, as 111. So you're doing: 010 111 xor ------- 101 101 corresponds to 5 in decimal.
That is not what I said or implied. The choice we made with web2py for example is give everything a default (so the developer does not need to re-implement the wheel or worry about too many security issues) but every default can be overwritten. That is not a straight jacked just one that comes with a zipper, hood and pockets. Other frameworks just say: here is a piece of leather, go turn it into a jacket by patching it with your favorite zipper, hood and pockets. You do it and when it rains it leaks. 
...well, what did you *think* "bitwise" meant? o_O
Same code in web2py (complete): def hello_world(): return 'Hello World!' Same code adding authentication, web services, default layout and a database of visitors (*): db.define_table('visitor',Field('ip')) @auth.requires_login() @service.jsonrpc() @service.xmlrpc() def hello_world(): id = db.visitor.insert(ip=request.client) return dict(message='Hello visitor number %s' % id) (* requires files from scaffolding app)
Everyone is telling you it's a bitwise XOR, while failing to explain what you're describing is a logical XOR. To get what you want, wrap it in bool() or similar.
Pretty names for 1 and 0, no? (Not talking specifically about Python)
You can actually get a pretty significant speedup by not using numpy (suprisingly!). In [1]: def xor1(aa,bb): ...: a = np.frombuffer(aa, dtype=np.byte) ...: b = np.frombuffer(bb, dtype=np.byte) ...: c = np.bitwise_xor(a,b) ...: d = c.tostring() ...: return d ....: In [2]: def xor2(aa, bb): ....: c = '' ....: for a,b in zip(aa,bb): ....: c += chr(ord(a) ^ ord(b)) ....: return c ....: In [3]: %timeit xor1('adf','bck') 100000 loops, best of 3: 5.22 us per loop In [4]: %timeit xor2('adf','bck') 100000 loops, best of 3: 2.81 us per loop (Edit: Of course, numpy might be considerably faster with large strings/buffers... Hmm... Maybe I can avoid work for a bit longer here!)
Yeah, numpy seems ~300x faster for large amounts of data. In [83]: from os import urandom In [84]: a = urandom(2**20) In [85]: b = urandom(2**20) In [86]: %timeit xor1(a,b) 1000 loops, best of 3: 1.4 ms per loop In [87]: %timeit xor2(a,b) 1 loops, best of 3: 482 ms per loop 
Not necessarily in Python.. if you have "a = b or c", it will return c if b is false ("0 or 3" yields "3") Python does not seem to have a logical XOR operator though (correct me if i'm wrong ;)).
At the time I decided to ditch it over a year ago, I posted a message on the mailing list asking about the upgrade path and that was the response I got. It was a business decision and at the time Django was really starting to take off in a big way. I love ZODB, it's so ... simple to use, however Zope I tried about 6 years ago and it seemed too 'enterprisey' which - in my mind - isn't what Python is about. It seems to be too tightly integrated and the page load times were awful. My own CMS had dynamic pages getting generated in &lt;30 ms, Zope was well over 100ms. Your mileage may vary, this is just 1 guy out of millions. 
I wasn't trying to argue that you made the wrong choice about not using ZODB, I'm just trying to convey the actual facts (e.g. it does run on Python 2.6+ and likely will run on Python 3 at some point in the indeterminate future if Python 3 succeeds). 
 &gt;&gt;&gt; type(bool(a)^bool(b)) &lt;type 'bool'&gt; nice!
Sure, it works. But I usually prefer something with the same concept but that I can read without resorting to coffee or something stronger. [http://www.opensource.org/licenses/isc-license.txt](http://www.opensource.org/licenses/isc-license.txt) Edit: I am sorry, I made an error, the LGPL seems to only make an exception for libraries and libraries only. ISC allows any portion to be distributed under any shape or form providing acknowledging the source.
You realize that Python 2.5.4 is very old, right?
Since we're playing that game, here's Grok. import grok class HelloWorld(grok.Application, grok.Model): pass class Index(grok.View): def render(self): return "Hello world!" HelloWorld is in fact a persistent Python object using the ZODB. XMLRPC methods are methods of a subclass of grok.XMLRPC, and there's also grok.JSON that does the equivalent for JSON. Full disclosure: this is in a package, there's a setup.py and there's a 6 line configure.zcml file that never changes. I'll note though that "simplicity" is a complex topic - Grok's more complex on the insides than BFG by some measures. BFG's example for instance is a Python program that uses libraries it imports, while Grok's example gets "grokked" by a framework. A Grok example that does the same would probably be more involved, though I think it can be made relatively short. 
I like this game. I just learned something about grok and that is good. Thank you. By learning from each we can all improve. To me code speaks more than words.
I love this! Thank you. EDIT: This is critical for scientific apps since they depend to scientific libraries that are not moving to new python versions any time soon but care about performance. People underestimate how large is the market. Probably scientific applications are the largest set of Python apps after web apps.
Why the downvotes? the effect of the old GIL on multicore systems is unacceptable, and should be considered a bug.
I'm sure it does, now, and the developers are pretty fucking amazing, they just told me that they didn't have an upgrade route when I questioned them.
Hello from the Captain Evidence.
I prefer to use a tool I wrote called [OnDir](http://swapoff.org/OnDir). It executes shell commands when you enter and leave directories, matching by regex. To auto-activate virtualenv on my projects I use: enter ~/projects/(\w+) test -r .venv &amp;&amp; . .venv/bin/activate leave ~/projects/(\w+) deactivate &gt; /dev/null 2&gt;&amp;1 
That looks pretty cool - I'll take a look at that. I like the ease of having a bash script that can be in a .bashrc that's used across multiple computers without the need to compile or install anything extra. But I'll check out your program anyway - I'm sure there are more use cases for this kind of thing. What else do you use it for?
One happy Python user here.
Yeah, there's definitely something to be said for the ease of shell scripts. I use it for setting up build environment variables, adding useful shell aliases for specific projects, etc. It comes in handy in surprising ways.
I don't really understand what kind of use cases this addresses. Anonymous blocks as *expressions* are generally what I long for in Python, and this of course is incapable of addressing that. Are there any practical examples of how this code would be used?
I think riffito is a pretty cool guy. eh reads english and doesn't afraid of big books...
&gt; set local variables in the frame executing the with-statement This is a feature that I actually wanted. I was creating a DSL and having the possibility to inject a local variable into a with block was exactly what I've been missing.
What use cases do anonymous blocks as expressions address? It sounds like they would make the code more convoluted...
&gt; What use cases do anonymous blocks as expressions address? The use case of only knowing Ruby and not wanting to learn Python.
Ah... Are there any legitimate uses, though? What does Ruby use them for?
I believe it started with django .. they had so many feature requests that at some point they started categorizing them. At one end you have the must have features and at the other the crazy proposals. And somewhere in between there is that nice but not indispensable features that might get in the trunk someday, but don't hold your breath; a pony.
Its not a real use case, just a style.
Ah, sounds like that style wouldn't really fit Python, then...
Well then we're in agreement :) Sensible defaults is a good thing, but the ability to customize as required is a necessity.
The performance increases look good, but we all know that everybody wants the new gil... it can't be kept for python3, now we've seen what it does to multicore machines !
Ever since beginning learning python on the 2.x series I **always** do from __future__ import division I never understood the reason for making floor division the default in the 2.x series.
Ruby uses blocks as a replacement for list-comprehensions and general iterator niceness. So instead of doing `l = [a + b for a, b in items]` you’d do `l = items.map { |a, b| a + b }`. In Python though, it would be nice to be able to do callbacks without writing the function until after the expression that will take the callback.
Guido [explained why he had integer division be the original default](http://python-history.blogspot.com/2009/03/problem-with-integer-division.html).
There's more to this than just anonymous blocks - it's about taking advantage of the with-statement to build interesting new syntactic structures. For example, take a look at Richard Jones's withgui project: http://www.mechanicalcat.net/richard/log/Python/withgui_finally_made_public This takes advantage of the with statement to structure your GUI-building code in a way that matches the structure of the GUI itself. It's also just plain nice to be able to define your callback functions "inline" where they're being used. Compare: def callback(result): print "got a result:", result def errback(error): print "got an error:", error x = do_something_with_callbacks(items,callback,errback) With: with xargs(do_something_with_callback,items) as x: def callback(result): print "got a result:", result def errback(error): print "got an error:", error I'm also planning to use withhacks to clean up the syntax of my [withrestart](http://pypi.python.org/pypi/withrestart/) module, but that's only the beginnings of an idea at the moment...
There is arguably a readability advantage to saying with xargs(sorted, l) as result: def key(item): …normalize item here… or (if one defined one’s own context manager using the provided tools) with sorted_by_key(l) as result: …normalize item here… instead of def normalize(item): …normalize item here… result = sorted(l, key=normalize) With the first, you read, “Oh, they’re sorting a list… and here’s how they normalize the items in it.” With the currently standard version you read, “Huh, why are they defining a normalizing function? Oh, I see, they’re sorting a list.” It’s not a huge difference, and arguably since this is a non-standard use of `with`, it’s not even a win for the `with` version, but if it were common for `with` to be used this way, I think it would be a little more readable than the current practice.
It returns an array 0's and 1's, if you use a bitarray object: &gt;&gt;&gt; from bitarray import * &gt;&gt;&gt; a = bitarray('010011') &gt;&gt;&gt; b = bitarray('111000') &gt;&gt;&gt; a ^ b bitarray('101011') 
Does the python 2to3 conversion script cover this case? 
I recently did a job for a customer where I had to deploy and continually update a django application. I eventually had to put together my own deployment scripts etc. I wanted to know why there wasn't an engine yard type service for python. And if others would use such a service if it existed - I've kicked around launching such a service, but I don't want to do it if no one would be interested.
App Engine comes pretty close, I guess.
How would it know you're dividing ints though?
That how many languages handle integer division
from http://www.reddit.com/r/Python/comments/at6co/what_the_enterprise_wants_from_django/ &gt; "[Configuration mangement] would especially come in handy for folks wanting to build application servers/services like App Engine or the nascent Toppcloud. &gt; &lt;digress&gt;If you've not yet checked out Toppcloud, do so now. I'll wait.&lt;/digress&gt; " you might want to check out toppcloud?
Multi db support is exactly why my project (at a fortune 500) decided against Django in favor of Pylons. I'm amazed it's come this far without that. edit: btipling eloquently pointed out that it wasn't my company, just a group within.
1 vote for Pylons here.
Doesn't everyone know about this by now? This, and "all strings are unicode" are the two biggest changes in Py3k - if you haven't heard about this you've got your head under a rock and/or you just don't care about Py3k. In either case you're not going to be reading this article, so...
It's good to hear that fortune 500-1000 companies are using Django, or any other python framework. The balance must be shifted. Java and .NET are very big in that section, and that picture haunts me. Although, enterprise influence may change what django is all about as Jacob says. Making something GUIsh? If they go the MS or Java way, i'd rather use Werkzeug.
The devs are pretty zealous about *not* making Django bloated. http://code.djangoproject.com/wiki/Version1.1Features#Rejectedfeatures I think you can rest assured that the project is in safe hands :)
&gt;This, and "all strings are unicode" are the two biggest changes in Py3k I would say "print is now a function" belongs up there too.
Because it's a surprisingly useful operation.
It can't; the difference is in runtime behaviour. Well, I guess they could put a `divide` function in a `__past__` module and replace divisions with calls to that... but that, like Camelot, is a very silly place to go.
I tend to agree that this is needed, apparently the Eldarion group (http://eldarion.com/) is working on something like this.
s/Python/Django/ tyvm
I don't know why, but there was a little part of me that really wanted this to end differently :(
Have you looked at tools like [Fabric](http://docs.fabfile.org/0.9.0/) and [pip](http://pypi.python.org/pypi/pip)? I know it's not EngineYard, but with minimal setup you can get a pretty automated deployment thing going with those two (and throw in virtualenv for good measure)
as far as I'm aware it does. In Python 3000 you can use // to do floor division so it makes sense that it would.
That company should take a look into web2py since all 4 of the issues were addresses since the earliest version of web2py 3 years go: multiple database support (with postgreqsl, mysql and firebird it supports distributed transactions via two phase commit); built-in portable cron (with @startup for extcron); very easy to support multiple installation because there are no global settings; it comes with a web based IDE. It also allows on click export/import of apps and one click export/import of table data across databases.
You really shouldn't be amazed per say.
I wonder if memory increase is really that big of a deal. Sure, some of the benchmarks increased over 5x in memory used. However, these were the smallest apps to start with. The largest apps only increased 10%-15% in memory usage. None of the benchmarks showed an increase in memory usage greater than about 40 KB. EDIT: a much bigger concern for devices with small memory is the runtime executable size. From around 5 MB to over 70 MB! Really though, embedded systems are not the best target for Python in any case. I think for apps where data size dwarfs code size, the increase in memory used may be a non-factor. 
It has always made sense to me. An integer/integer = integer. It's always been easy to do float instead. x / 2 = integer, but x / 2.0 = float. It's ideal if you are doing web development, eg pages = results/per_page+1, though it's not going to be too much of a hardship to learn to pepper your code with floor() all over the place. I don't think either method is a mistake as long as it is consistent and the fact is well documented. Phillip.
From what I've seen, I prefer Python 2.x to Python 3000. I don't think I'll be changing over for the forseeable future. Phillip.
Yeah, and then there is random() and random(). Too bad that the author didn't look at what rand() really does. I don't know the exact implementation of randint, but one of the things I would test for is that you get truly random numbers. MS-SQL had at least one implementation of RAND() that returned random numbers that didn't differ much from the previous random number...
I honestly don't understand why Django is more popular than Pylons? Django (and to some extent, TG too) is as monolithic as RoR. Like Ruby put can't stand ActiveRecord? Too bad. Dig the HTML helpers in Django but want to swap in a faster templating system? Sorry. Pylons and the other few WSGI Python frameworks are the way to go. You can swap out the components as you see fit. For example, I only use the connection pooling from SQLAlchemy and then hand-code all my models to use stored procedures (functions) in the Postgres DB. This isn't easily possible in either RoR or Django. Store procedures are used extensively in many enterprise applications because generally large teams have a DBA writing the queries and other developers consuming them in their applications. +1 for Pylons in the enterprise.
&gt; I honestly don't understand why Django is more popular than Pylons? Django (and to some extent, TG too) is as monolithic as RoR. When you know Django, you know Django. When you know Pylons, what do you know? Choice isn't always a good thing. 
Does it support PEP 8?
Well, in Python (and other dynamic languages) it is a problem because if doing 'num / den' the arguments could be either floats or ints
*per se*
I for one read Enterprise and was waiting for the punchline.
&gt; pages = results/per_page+1 This gives you an empty page if the division works out evenly.
&gt; When you know Pylons, what do you know? you know how to use web frameworks in the general sense and how to decide the best approach to fit a problem.
Or 20 :D
Python's random() is not really random, I agree. The point is how not to kill the database while getting a (hopefully random enough) record out if it.
Thanks. Reading my sentence, not even sure why I used that term.
&gt; it comes with a web based IDE. Is it truly and IDE in the Visual Studio/Eclipse sense with command line completion along with syntax highlighting for python, the framework, additional libraries and the current project? [Edit] I ask because in because in my experience when talking abut enterprise and IDEs it seems they mean a code completion feature set akin to the big boys like VS, Eclipse, Netbeans, intelij etc. 
&gt; I honestly don't understand why Django is more popular than Pylons? I think that the Python community was feeling a bit insecure about RoR's buzz and wanted something rally behind and Django happened to be a great piece of relatively cohesive software. 
The source code of web2py follows PEP has much as we can. In your application code you do what you want and we encourage pep8. The only exception to PEP are validators and helpers. They are all upper case for two reasons: minimize possible conflict with user code (since they are automatically exposed to users), remind the user that they should be treated as constant classes and they should not change the class attributes, only instantiate them.
You have the choice of two web bases editor: editarea (the default, no code completion) and (eamy, which also ships with web2py, and has code completion but does not work with all browsers). Most of the web2py users prefer to use Eclipse. 
Oh, downvote what you like, but you *might* also try to reply in a way that you've read and understood the wikipedia article on pseudorandom number generators...
You're saying web2py has a drag and drop visual editor for the frontend? (Because that is what Jacob is saying that Enterprise is looking for - not the ability to edit code through the web.)
We have internal tools for easy deployments (which Brian Rosner talked about at DjangoCon: http://djangocon.blip.tv/file/3040031/). I have no idea what the plans for providing a service out of these are, other than that James Tauber left a comment on hacker news a few months ago indicating there was some plan/interest in turning our internal tools into a product.
And don't forget ,,new style is the only style for classes''. Finally.
Oh, I see it in the source and changelog. Fixed in 1.56.1-1.56.4. Examples are still in the old style, though. But really nice work. And I like the new look of the [site](http://web2py.com/). I think I have to try web2py in 2010.
Yes and no. web2py has a web based IDE. This includes a code editor and and html editor (the latter needs work and nobody uses it).
&gt; this uses two queries – both of them are very efficient No. Both of them are actually very *inefficient* on MVCC-based databases! (That means MySQL's InnoDB and PostgreSQL, among others.) They're much better than the first method, but they're still *bad*. &gt; `SELECT COUNT(*) FROM users;` This query, because of the way MVCC keeps track of row updates, has to read in every single row in the table and check its visibility in order to generate an accurate count. Some optimizations are possible, but you still have to read the entire table. &gt; `SELECT * FROM users LIMIT 1 OFFSET :rand` In PostgreSQL (I believe InnoDB is similar, but am not certain), if `:rand:` is small this will be fast. If `:rand` is large, guess what: the DB still has to read in `:rand` rows, even if it discards `:rand - 1` of them, until it reaches the row you asked for. This is usually pretty quick (especially because the previous `COUNT(*)` will have loaded the rows into cache), but for large numbers of rows, is simply not good enough. This method is faster than `SELECT * FROM users ORDER BY RAND() LIMIT 1;` only because it does not sort the entire result list -- sorts are extremely expensive -- it does not avoid anything else, at least on MVCC-based databases.
&gt; I honestly don't understand why Django is more popular than Pylons? I chose Django over Pylons because of its superior documentation. I suspect I'm not alone.
And just the other day you were telling me, with the implication that it's a good thing, that "95%" of Pylons installs use the same set of components. You can't have it both ways, you know...
For several years now I've been quietly making the argument that the two approaches to framework design -- which I tend to call "full stack" and "glue" depending on whether they focus on providing a single set of components, or providing ways to join together many different components -- are both valid. They both have advantages and disadvantages. They each suit a particular type of developer well. And it's a good thing Python has both options available. Unfortunately, that rarely seems to get through to people who decide to promote one approach and demand that everyone else agree with their "obviously" better decision. Oh, and: &gt; Dig the HTML helpers in Django but want to swap in a faster templating system? Sorry. I think you mean: "Didn't bother to read the Django documentation? Guess you'll have to wait for someone on reddit to [link you to the docs for using other template languages](http://docs.djangoproject.com/en/dev/ref/templates/api/#using-an-alternative-template-language)."
+1
&gt; I think you mean: "Didn't bother to read the Django documentation? Guess you'll have to wait for someone on reddit to link you to the docs for using other template languages." To be fair, that API is new, in the unreleased version 1.2.
So, by yes and no you mean no then? Text editors are not what was being discussed
I went to engineyard.com to see what you're talking about and saw a bunch of web 2.0 buzzwords (and typical corresponding graphic design). What is it **really**?
That's only because those components happen to be a good fit 95% of the time. But the user has still chosen, and can choose again. You're referring to a discussion where you claim that Pylons "bewilders" the user with choices, and that's just FUD. Regarding this topic, Pylons doesn't hold your hand. If you've mastered it, it's a good indication that you know what you're doing. The above comment (edit: that of stesch, *not* ubernostrum) implies that Pylons mastery implies only narrow knowledge about an unpopular tool, another lame grasp at FUD. The fact is, it would hardly take anyone half a day to learn any other framework since the concepts are the same. 
If you find any positive things about the new name, please add here: http://wiki.zope.org/bluebream/PositiveFactorsAboutName
The same documentation that is completely irrelevant because everyone is using SVN, which is usually undocumented?
If you're not using the nightly commits from SVN, you're doing it wrong~
Okay, wow, that explains EVERYTHING. Thanks, seriously.
Thats not cross platform either, so I would say curses would work for what you want.
Uhm, plenty of people are *not* using SVN. Also, SVN is commonly well documented, because patches often aren't accepted to the trunk until documentation and tests exist. However, I gladly accept facts as a retort, unless you're just trolling of course.
Thanks for all your comments! I really appreciate all your help :) 
Thanks, this helps me a lot...
Thanks all, I had found fabic and pip, also redhat as cobbler and Func (similar to fabric) http://func.et.redhat.com/. I'll checkout eldarion and topcloud.
As soon as you have an admin interface, you need to select an ORM/Web framework combination supported by the admin library. 
It has to do with how difficult and unstable it is to run production rails. Ask any system administrator who has dealt with production rails systems. There is a whole ecosystem of hacks, scripts, and a lot of babysitting to keep it alive. As such, places like engineyard and heroku have sprung that specialize in that mess. Production python is very nice, stable, and easy to keep going. This makes self maintenance very easy, so there never has been much demand from the market to justify the existence of those types of managed services.
Apparently you have no idea what enterprise really means. Nobody will be tracking nightlies on a production app.
&gt; To be fair, that API is new, in the unreleased version 1.2. Yes and no. You've always been able to write your own wrapper around the loader and renderer of any template system. There just never was any documentation explaining how to do this, or any guarantee that the API would be stable (since it was undocumented). What's new as of 1.2 is mainly that the template loading and rendering API was formally documented, placing it under our API-stability policy. (there were some under-the-hood tweaks to the built-in stuff, but they were mainly concerned with making Django's default template class easier to cache and reuse across multiple renderings, not with enabling the use of other systems)
&gt; You're referring to a discussion where you claim that Pylons "bewilders" the user with choices, and that's just FUD. You were responding to a claim about Pylons' list of component choices, displayed prominently on the home page. I mentioned it's still there, and then said (and I quote): &gt; If I were coming in brand-new I'd probably be a bit intimidated by that, and I think dealing with that is an important thing for any framework which takes the "glue" approach. We need those frameworks to exist, and we need a way to make sure people learn about them, and that means confronting the issue (which Pylons is and has been doing, I should note). So how about you stop putting words in my mouth and stop accusing me of nastiness, hmm? Oh wait, no, you did it again just now when you said: &gt; The above comment implies that Pylons mastery implies only narrow knowledge about an unpopular tool, another lame grasp at FUD. All I said was that you, personally, don't get to argue this both ways: if the overwhelming majority of people are simply going with defaults, then they haven't really learned how to pick and choose the components they want and make the system use them. Unless you're trying to say that 95% of users read all the documentation thoroughly, learn all the APIs, examine all the choices carefully and then end up at exactly the same set of components. Given my experience of having to outright beg and plead to get people to even _glance_ at documentation, I doubt that's what's actually happening.
&gt; then hand-code all my models to use stored procedures (functions) in the Postgres DB Incidentally, you probably want to have a look at [Simpycity](https://projects.commandprompt.com/public/simpycity), which may (depending on your use case) be a much simpler way of interacting with your stored procedures.
Very cool. I'll look into it. Thanks for the tip!
The original post mentions generically IDE and tools. I said we provide IDE. We provide some tools as apps for example [this one](http://www.web2py.com/appliances/default/show/47). No. web2py does not provide a GUI builder because it is a server-side framework.
It was about time! Awesome, thank you!
First thought, it looks really good and is really cool. I like that finally someone has come up with a sane solution to handle GUI;s and macros. But... I can't stop thinking about how slow it is and you are still forced to watch it go through all the steps since you would need to use the same window manager if you wanted to work while the script was running. Once again I find having the text file option for configurations to be superior. This said, it will still be terrific for testing GUI;s and for automating programs that does not provide you with an API or text file to control their behaviour. **tl;dr** Great for tasks where you have no choice but to use a GUI, but for most it would be better with an API or text file.
I would happily switch if I could use all of the same libraries. Unfortunately it requires a bit of a tipping point in how the general python community thinks until that'll happen. Primarily though the biggest problem is that there just isn't enough reason to switch. Python has a lot of influential people in the community that believes it's the perfect language, and doesn't need to evolve, so new features in the language aren't a high priority. For me there isn't _enough_ of a difference between 2.x and 3 to make me pick one over the other for the sake of the language. So it comes down to libraries and support and documentation. and 2.x has an 8 year head start. This is one thing the guys on C# got right, after v2 every new release had a major feature addition (well maybe except 3.5) so there were ways of doing things in the new version that are easier/shorter for the developers than the previous version. Sure the argument is that c# was originally a crippled language, that the major features they brought in in subsequent releases are already in Python or aren't needed: generics, lambdas, partial methods, optional params (coming in 4.0), dynamic types and variance, etc. Python has either no need for them, or has had them for years. so Maybe the fanboys are right and python is already perfect. Though where is the Python equivalent of Linq? (well it's sqlAlchemy, right?) or, borrowing from other languages like scala or erlang, cool features like tail-recursion, or hot-swap code replacement? or maybe getting rid of the GIL to ensure that more cores == more speed without having to code multiproc? 
Thinking about it though, it's not like Linux users are going to need something like this. &gt;Your OS doesn't support switchApp
This will be a huge boon to testers and QA.
I'm sure it's not far away. X is amazingly hackable.
probably you could use headless X on linux and then it would fly and you would not have to watch anything. (http://en.wikipedia.org/wiki/Xvfb)
in the end this is just a neat hack thats necessary cause python is not extensible on its own
"bewilders" is based on a fuller context of the quote, we can let others decide on what impression it creates: &gt; The home page lists, just as suggestions, SQLAlchemy, SQLObject, CouchDB, Mako, Genshi, Jinja2, WebHelpers, FormAlchemy and Routes, all under a section showing how Pylons "encourages use of your favorite Python components". If I were coming in brand-new I'd probably be a bit intimidated by that and by "above comment" i meant that of stesch, not you.
This looks exactly like applescripting... except without the issues and an API.. 
Nice idea, but from what I saw it only supported OS X and Windows.
I understand engineyard cloud to be a deployment and management service for rails applications. The idea is that you can deploy a rails application from your git repository directly to a set of preconfigured servers (application, database etc) and scale up or down as needed from a web console. It currently supports the EC2 cloud, but like rightscale they are supposedly going to support other clouds.
OK, OK, cross posted from /r/Music. But as a python/music enthusiast, I had to share this little "WTF?"
I didn't know that about rails. I assumed that given its popularity it was fairly stable? Do you think there is any benefit for a python service like this as a time savings device?
I am actually a contractor at one we hope to get off the ground soon. However, we won't just specialize in only python deployments, we will be a python friendly virtual server service.
cool one! thanks for the link...
nice, ill see if our tester could use this
from the website: "You can programmatically control a web page, a desktop application running on Windows/Linux/Mac OS X, or even an iphone application running in an emulator." look at the downloads page, there is a linux version: http://sikuli.csail.mit.edu/dl/Sikuli-IDE-linux-20100104.zip
an error could destroy the entire structure (e.g: Missing }), how do you intend to "recover" from that?
A significantly smart parser would attempt to insert such a missing symbol in or continue on, skipping the requirement entirely, with the hopes of finding more errors in whatever document is being parsed. Note: when recovery occurs, the parser would enter into some sort of error state and not do any AST construction or code generation.
Cool any idea about when the service will be available?
For some things "recovery" could be rather straightforward in many situations (but of course not all), e.g. if you have XML files that people sometimes edit by hand it could be rather simple, and very useful to test out some easy corrections that might work pretty often. If a "recovery" seems to parse correctly it could be later be presented to the user as "suggested corrections". &lt;foo bar=baz&gt; =&gt; &lt;foo bar="baz"&gt; &lt;foo&gt;bar&lt;foo&gt; =&gt; &lt;foo&gt;bar&lt;/foo&gt; I've previously tried doing similar things (while parsing SGML) and the difficult part then was to pound the parser into working with this ideom in mind.
When you know pylons, You know python, wsgi and http. and are left empowered to do more things yourself rather than be left waiting for framework developers to decide your needs are worth considering. 
Graceful error recovery is something that we don't have down to well yet.
The [iMacros add-on for Firefox](https://addons.mozilla.org/en-US/firefox/addon/3863) does this very well.
implication of closed context on symbols that don't fit is powerfull additional hints are stuff like indentation, also one could try to parse backwards
actually leaving error nodes in an ast would also work fine (given one can keep it kinda working)
define 'we'
I was making a general statement about popular coding practices. I believe in the future we will have much more graceful error recovery and parsing. We, collectively, just don't have many wide reaching methods of doing it right now.
We are taking on our first customers in the next month. They are people we already know from being in the industry. I can't put a solid timeline on when we are officially open for business, but you can get on our mailing list: [http://vmfarms.com](http://vmfarms.com)
Actually, graceful recovery was considered a very important feature of early compilers, due to the overhead involved in inputting the program and running the compiler.
eclipse seems to have it down pretty "well"
All the author is doing is sending each client a string. This is a hardly a realistic benchmark of runtime performance.
&gt; Also 12k Python vs 15k Haskell isn't bad considering that Haskell is supposed to be a faster language and the Python version hasn't been compiled with psyco or cython yet :) It'll start to fall behind as soon as you do significant processing for each connection. These kinds of trivial benchmarks (send each client a very short string, immediately close connection) are meaningless.
looks great, but they should pick an example that couldn't be accomplished with a really simple shell script!
Web browsers do.
web browsers have a very small amount of possibilties and the end result of them is always very clear. With programming you could potentially never even know if a proposed function would even end.
We don't even know what the OP is trying to parse. He only mentioned "partially invalid files" which doesn't really sound like he's trying to parse source code.
Thats true, sorry I have just been thinking about assistant AI working around code recently.
This is not what I think of when I hear "screen scraping". Oh well, still looks neat.
The iMacros add-on for Firefox does this... not at all. You must not have looked at the product.
Anyone know how to get Qt Assistant working in Windows? I remember reading sometime ago that you need to download the documentation separately and do some sort of processing to it before it'll work. Never really had any luck with that.
??? I've never had a problem running Assistant from either the PyQt distribution or the Qt SDK.
By "working" I meant doing something useful by letting you browse the API. The "Index" and "Contents" tabs are completely empty when I launch Qt Assistant and there's nothing but a general introduction to the program.
Care to elaborate what's wrong with TAL?
Not true. I did my initial steps with Zope 2 with the out-of-the-box Zope packages provided by Ubuntu 8.04. Of course, you might not get the latest and greatest set of packages for Zope, but that's not at all a problem unique to Zope.
XML and the idea that mixing the code with the HTML tags was a good idea. I already know a language (here python) and I don't want to learn another unreadable one with ugly bracket, semi-colon and quoted attributes everywhere. &lt;b tal:replace="template/title"&gt;the Title&lt;/b&gt;. instead of &lt;b&gt;${template.title}&lt;/b&gt; I've learned all I needed to know about Mako in the "Nutshell" box that is on the [front-page](http://www.makotemplates.org/). [This](http://wiki.zope.org/ZPT/TAL) completely uninformative and useless page is afaik the TAL main documentation page. I'm sure TAL is like Zope, it can do everything including giving me backrub and a blowjob at the same time but it's sure ugly and not welcoming. 
Well, most of my issues may have come from plone and the fact that the default package made if difficult to upgrade it and to understand how to deploy custom modules with the sparse and out of date documentation. (Most likely Plone's biggest issue). Upgrading plone required upgrading zope and so on. Zope is easy to install, but completely useless out of the box, it's an application server after all so you need some apps.
`os.link`
Odd. Try the Qt SDK. It's the same stuff, AFAIK. 
I thought the only thing missing is a PONY.
[ANTLR](http://www.antlr.org/) has some support for this.
Wow. I have exactly the opposite reaction. For example, I find this Mako unreadable: http://github.com/ish/formish/blob/master/formish/templates/mako/formish/widgets/TextArea/widget.html vs. this TAL which is: http://svn.repoze.org/repoze.bfg.formish/trunk/repoze/bfg/formish/templates/zpt/formish/widgets/TextArea/widget.html Although that's actually [Chameleon](http://chameleon.repoze.org), a ZPT variant. 
it seems to be just some simple recovery by token insertion and the api looks like an extraordinary pain
Found via [a question on SO about "Building a NetHack bot"](http://stackoverflow.com/questions/2114303/building-a-nethack-bot-is-bayesian-analysis-a-good-strategy) (and hey, it has a 250 point bounty).
ffmpeg certainly does. As do some parts of mplayer (or, specifically mencoder). Some of the tools that Hugin utilises definitely use multithreading. If memory serves, the Linux kernel does too. I think a popular library is openmp. My friend successfully used this to parallelise his seismic 3D Kirchhoff migration code.
Wowww, that sounds like a huge endeavour.
Context. Using the `multiprocessing` Python library module, since this is /r/Python.
in pida we currently use it to run code outliners/validators in other processes however we'll most likely switch to execnet since it's far more usefull and powerfull
I sort of find this chest-thumping tedious. This is a thread about Zope; I don't see why anyone would feel the need to give a web2py example. Especially an example where this is provided as an example of the simplest application: def helloworld(): return 'Hello world!' This tells us nothing whatsoever about the framework, or how this function is associated with a URL, or how a server is invoked. It seems contrived. I'm sure web2py is great, but do you really need to pimp it on every Python-related reddit thread? (OTOH, I don't mind seeing a Grok example, as it's another Zope-related framework, and that fits in the theme of this post.) 
IIUC execnet is GPL, so there might be licensing implications for some projects.
&gt;os.link(source, link\_name) &gt;Create a hard link pointing to source named link_name. **Availability: Unix.** The article was for Windows only.
holger provides alternative licensing not that we'd need it tho
I use [celery](http://github.com/ask/celery/), which is very dependent on multiprocessing. It's probably pushing things further than most of the other. It might be a good place to look for tips
Oh. Toy links.
I never was able to figure it out. I just went to C:\Python26\Site-package\PyQt\docs. I might be wrong with the exact path, but I was able to find the index.html and I just bookmarked it. I access it from my browser now.
What is lacking from the documentation that you'd like to see? I'm the maintainer, so I can act on suggestions as time permits. See also: http://www.doughellmann.com/PyMOTW/multiprocessing/index.html#module-multiprocessing 
I'd like to know why it's "more useful" and "more powerful" - especially in the context of an IDE.
celery is great; see also Ask's addon package for MP: http://github.com/ask/billiard
it can cross to more different implementations of python and doesn't need pickle also its based on very free form invocations, that makes many things i have in mind trivial
Fair enough :) The goals of the two projects are quite different, so I encourage people to use what works best.
Somebody posted a BFG example before me. Somebody posted a Grok example after me. I was not the first to mention another framework. I did not say anything negative about Zope. I think showing code helps people understand pros/cons of different systems. I find code examples posted by other people useful. What is the meaning of "what do you think about X?" if one is not allowed to make comparisons with Y? 
I posted the BFG example in an attempt to respond to Martijn's claim "... BFG to my knowledge still uses ZCML for many purposes." I chose to clarify that by showing an end-to-end example of an application that was complete but had no ZCML. But your web2py example that followed was an unprompted non sequitur. If the example you gave had been illuminative, that might have been fine (if still tedious), except the example itself is totally incomplete, showing only a function definition with no context about how to wire it up to a URL, and no context about how you might actually cause a server to start. It's in not even close to the example I posted. This makes the existence of the comment seem more like a marketing spam than any sort of honest broker comparison.
Ask deserves some sort of medal for awesome naming of packages.
Things that took me a very long time to get right with multiprocessing: 1. Logging, specifically with RotatingFileHandler. I tried [ConcurrentLogHandler](http://pypi.python.org/pypi/ConcurrentLogHandler/0.8.4) which wasn't working at the time (though the issue seems to have been fixed in version 0.8.3), and ultimately developed [this](http://stackoverflow.com/questions/641420/how-should-i-log-while-using-multiprocessing-in-python/894284#894284) which has been handling heavy production loads for many months now without issue. 2. stopping/starting a process with lots of child forks. Haven't found a good way to do this yet, I have a shell script that greps for all the pids. If you kill just the parent, the children turn into zombies. I'm sure I'm just doing it wrong here. 3. Daemonizing. Not quite the same thing but I ultimately adapted part of Paster server to accomplish this, and [this](http://pypi.python.org/pypi/python-daemon/) probably does a better job. The major PITA with daemonizing is that you have to shut off all open filehandles, which means all your logging config etc. has to occur afterwards. 4. Controlling resources. I used to have lots of "worker" forks sitting around persistently, doing work on a periodic basis. I later moved to a model where forks spawn off to do something and then terminate - the parent does the scheduling with the help of `poll()`. Also during initialization ( I have to throw a bunch of data into a Tokyo Tyrant DB) I try not to load lots of in-memory resources in the parent which the children don't need, else the parent process grows much larger with space that isn't used - I do memory-intensive init in another temporary worker. 5. If I do too much with multiprocessing at once, the whole app just hangs. Its hard to tell where this could be happening - to help track it down I use a recipe similar to [this](http://stackoverflow.com/questions/132058/getting-stack-trace-from-a-running-python-application/133384#133384). Though when it really hangs, the signals don't get caught either. The cause of this issue could lie with anyone, my code, weird Python issues (I've [found some before](http://bugs.python.org/issue5331)), etc. But its pretty easy to have weird problems with dealing with child forks and threads at the same time (most multiprocessing utilities seem to use threads too).
My bad. Didn't see that...
It's efficient if you can cache the count(), eg set it as a variable in the manager or a meta in the model, and then over-ride the model save() to increase this variable accordingly. The author is proposing A solution. The problem is any solution needs to be tailored to: (a) the size of the data set, (b) whether it is needs to be truly random or can be pseudo-random, and (c) whether you have control over the data set. For instance my solution above fails if the database can be modified outside of Django. It's a bit like sorting. As a rule we use quicksort as it is a good generic all-rounder, but it isn't necessarily the best in all possible cases. Phillip.
Look at how it is used here: http://shootout.alioth.debian.org/u64q/program.php?test=knucleotide&amp;lang=python&amp;id=7
Windows XP has hard links for files (don't know why Python devs didn't implement them) and soft links for directories, Vista additionally has soft links for files. Which, incidentally, probably are the only kind of link you should use, unless you a) are implementing complex and very self-conscious things like VCS, or b) enjoy being extremely confused by silently, imperceptibly, accidentally unlinked links.
I use hardlinks and symlinks all the time (e.g. for image collections/favourite directories/stuff to email XYZ, deduplication of read-only files with my specialized tool, comfortable archive or CD image building in a temporary directory structure, cp -rl is your friend). They are part of my life. Link counters are always visible in my Midnight Commander and ls -l, which is also useful to guess subdirectory contents.
Having lots of examples means you can hack the examples until you get something like what you want, and by then will have enough understanding of the whole to be able to find out about specifics using the reference documentation.
Does anyone have any example usages of this? Only examples I can find in the repo are [one or two tests](http://github.com/rfk/withhacks/blob/master/withhacks/tests/__init__.py)
What version of Python are you using? Are you using the backported version of MP or the built-in version?
Your statement "no context about how you might actually cause a server to start" indicates to me that you have not understood the nature of my comment. It was a critique of the fact that, in fact, you need such a thing. In my view, the job of starting the web server pertains to the framework, not the application. If the app starts the server then you tie them together to much and you cannot easily have one server run multiple apps without necessarily complex configuration. Same goes for all those import statements on top. I tried make my statement without being explicitly critical. The fact that I used web2py as an example is accidental (or perhaps not since it is the only framework to take the approach I suggest). Perhaps I should have made this more clear in words and I thank you for giving the opportunity to clarify. Anyway. I like BFG and it is in my todo list to learn more about it. EDIT: I do not know about he marketing spam thing. web2py is an open source thing as BFG is. Except that web2py was born in an academic environment and hosted by a university. 
2.6.2 with the built in.
Well sure but the code is not equivalent. See the ZPT(AL) one sets col and row to respectively 40 and 10 if the value are not defined while the mako version makes the extra effort of not setting the attributes if they don't have value and adds a "class" attribute only if it exists. A strict equivalent to the ZPT example would simply be: &lt;textarea id="${field.cssname}" name="${field.name}" cols="${field.widget.cols or '40' |n}" rows="${field.widget.rows or '10' |n}"&gt;${field.value[0]}&lt;/textarea&gt; which is not as convoluted as the one you link. In any case I like the freedom of mixing some raw python in there, it's way faster for me than trying to figure out the proper tal: statement. To each his own I guess. *edit* I'm pretty sure the '|n' part is useless.
For 5, I'd love a test-case, when working on the port into core, we found a deadlock in the threading module - I want to see if it's my bug, or someplace else. 
1) You still don't address how to wire the helloworld function up to a URL in web2py. 2) Just because you can put all the logic that represents a BFG application into a single file doesn't mean that you have to. We have templates which build a package, preventing you from ever needing to even know about the bootstrap code. The templated applications use PasteDeploy to configure a server and middleware instead of using this "single file mode", replacing "all those imports up top" (yeah, ok, all 3 of them). I used single-file mode to respond to Martijn in order to indicate that there's "nothing up my sleeve".. no ZCML, in particular, which was the only intent of the post. It wasn't to describe in detail all deployment and configuration modes of BFG. 3) "Run multiple apps without necessarily complex configuration".. This is 100% the job of WSGI and WSGI deployment systems. In particular we suggest that people use PasteDeploy and we document our integration with it. We happen to allow them to not use it. How is this not a feature? 5) The fact you used web2py is an accident? It can't be, 8 out of every 10 posts you make on reddit are about web2py. That's one damn coincidental accident. 6) If you want to be explicitly critical, be explicitly critical. That's OK by me. It's a heck of a lot easier to deal with. It sort of seems like every conversation I join it seems you have to some passive-agressive criticism about somebody else's stuff that somehow always happens to be a pimp for web2py. Either skip the criticism entirely, or just make it aggressive, skip the passive part, and we'll save a lot of time. ;-) 
1) you don't. The file does not import the framework. It is not a module. It is executed by the framework. The framework links it to a URL following a convention (which you can override but do not need to). If you give it a try, you will see what I mean. 2) That is not what I am talking about. In my example there are no other files that belong to the application, there are no other files that contain settings or configuration. 3) That is one way of doing things. Still too complex for my taste but that is just an opinion. 4) Almost all the posts I make on reddit are about web2py. I talk about things I know. I shut up otherwise. 5) I do not want to be critical. I am just pointing out there are other ways of doing things. I truly believe we could all gain from this type of confrontation. There is no right or wrong. There are differences. You tried to make the point that BFG is easier than Zope, and I agree. I tried to make the point if want easy, you can go one step further. 6) No need to be offensive.
1) I went as far as reading the slideshow on your site's front page. As far as I can tell, it seems in order for that to work, it'd need to be: def index(): return 'Hello world' In a file named "helloworld.py", which somehow gets found by the framework. That isn't what you posted as an example, though, so I'm pretty clueless. In any case, I think using module structure is a really terrible way to map URLs to code, but to each his own. 2) Suuuuuure there isn't.... unless you count the files that make up the database that stores the settings. I'm an old Zope 2 hand, are you really sure you want to try to fool me by hding app settings in a database? 3) ... 4) I really don't mind seeing posts about web2py, and I wouldn't mind other folks posting about it. It's just that when there's exactly one person making new posts about web2py (you), and exactly one person (you) injects web2py into other unrelated posts on a regular basis, it gets tedious. It just seems like astroturfing somehow, and I roll my eyes whenever I see a post about web2py as a result; I have a feeling I am not alone. 5) I've been down that road; baking it all together without a reasonable way to tease it apart always ends in tears. Again, I am an old Zope 2 hand, so it's not as if I haven't lived in a world where that is true. 6) I actually consider it offensive to hijack unrelated threads in order to pimp stuff. You consider it offensive for me to call you on it, I guess. I can live with that.
1) Since you asked. [Try this](http://www.web2py.com/demo_admin) and you will get an idea of where the code goes. 2) There are no settings in the database. There are not settings. 4) I am the official spokeperson. Sorry. Besides when users post, they get accused of being me, so it would not help. 5) Correct me if I am wrong but you posted about BFG before I posted about web2py. BFG is not Zope. Your copyright page says "Agendaless Consuting" The Zope page says "Zope Corporation". What is the relation between those companies? Can we be friends? Does it help if I say "I am sorry"?
You definitely would have gotten one as soon as I could reproduce it. Even on a version of my app where it happens, it can take many days for it to occur. I haven't seen it in months at this point and I was never able to get smaller test cases to do it. Also we've since fixed some deadlock bugs in Beaker (which uses lockfiles) so it could have been there.
Django's templating language has a with block which allows you to assign any expression to a new variable. After using that for awhile I frequently find myself wanting to do something like your second example. It's the same as a temporary variable but it clearly indicates the scope, avoids accidental references outside of the intended scope, and also makes it clear that the temporary name is used only for performance and legibility.
1) I did post about BFG, in response to someone else mentioning it, and only correcting a misstatement. I didn't inject it into the conversation here; multiple other people did. 2) I have been a Zope contributor since 1999. I used to work for Zope Corporation. Agendaless Consulting was founded by 3 ex-Zope Corporation employees, including one founder of what eventually became Zope Corporation. That is my relationship to Zope. 3) BFG uses bits of Zope technology (the component architecture). I helped write and help maintain some of those bits. This is its relationship to Zope. See also http://wiki.zope.org/bluebream/BlueBreamName for how BFG relates to other Zope frameworks. And sure, of course we can be friends, and "sorry" always helps a lot, and it is appreciated. Apologies if my own directness offends you; it just wastes less time in the long run to put it all out on the table in my experience.
Thank you. I am not offended and I am sure this exchange will lead to some constructive cooperation in the future. I do need to lean more about zope component architecture.
More or less, but it's in C++. The PySide documentation is probably the best, but that doesn't work with Qt Assistant.
Ah, ok - by the way, if you were to get the time, a doc patch for any of the experiences you have (to help smooth the curve for new people) would be awesome, or even just examples of things you found lacking.
These examples only explain single concept at a time, like using queues, pipes, locks, pools, managers, etc. But not a single example where multiple of these concepts are explained together in much more complex usage. 
I have used them for Tornado and Django apps regularly. I had some problems initially and they were quick to respond, but not super knowledgeable. Generally, if you know what you are doing and just need a reliable box to run your stuff on, they are a good value and will stay out of your way.
I am reminded of [this amazing NetHack bot](http://taeb-blog.sartak.org/)!
I didn't know about that one, and that's a great write-up. Thanks for the link!
good to see that the large majority of his bugs had their fixes put in right away.
what do you think of the logging situation ? It would be nice for there to be an effective handler present in the standard library that can route from multiple processes to any of the non-concurrent logger handlers (and more simple and out-of-the-box than adapting SocketHandler or something like that).
I'd appreciate a knowledgeable summary of that thread.
It is a summary already, anything more will be about meaningless. Basically was looking for vulnerabilities in python and found a few after looking for years. 
Aiight I got that much... was wondering if there was anything more than "thank you"s in the replies. But thanks.
I'm sure someone could get a bit more info into the answer than I have. But did you look up [fuzzing](http://en.wikipedia.org/wiki/Fuzzing)?
Thanks dude, I had concluded that that was a made up word... didn't think to Google it.
I would say, from the trenches, Python is no panacea. It has many (maybe most) of the same core problems with deployment as Rails. It also has IMHO better implemented solutions to these problems; but generally both problems and solutions for Ruby/Rails hosting have very recognizable analogs in Python.
Interesting, and not surprising I guess, that all the issues were in C modules, not pure Python ones. 
Is there a mailing list somewhere? I just googled a bit and couldn't find anything. I'm looking for some place to discuss my code with fellow mp users, and find out if i'm shooting myself in the foot or being clever :)
Is the PDF now caught up to the web version? I'm teaching a few people and want to know which to give them.
Why do you care so much about what others think? Taking some log files, parsing data ja generating statistics sounds very easy and achievable with any of the Python frameworks. Use whatever feels most comfortable for you. That said, a full-stack framework like Django and Pylons would definitely be overkills for this kind of project in my honest opinion. I'd take a look at http://webpy.org/. If this was Ruby instead of Python, I would recommend Sinatra in a heartbeat. But do you even need a framework? Creating a WSGI application without a framework is also possible in Python, and not all that hard. Creating a web application this way is very educational.
The reason I'm doing a full framework is because what I have planned is far more than just a simple log stats parser. It's going to take the logs and parse them adding them to tables in PostgreSQL, then it will have multiple levels of stats, there will be global stats which will show the averages for each class, then match specific, team, specific, player specific, and league specific stats all of which will update dynamically with new logs coming in. Plus I like Django so far, I just want to see if there's anything that someone could show me that would move me away from Django. Basically, what's planned is going to be big enough for a framework and I'd just like some opinions on Django and its competitors to see if I made the right choice, not because I care so much about what others think.
That's a lot of invalid error handling. Error paths should be unit-tested a lot more, because functional testing and even use in the wild will rarely exercise them.
Nice to see constructive criticism and contributing back code, rather than just complaining.
Note that key-value DB are very efficient for logging, you may want to look into that
Whatever bad things you can say about Zope, remember that Zope had the features your framework has today a decade ago. Except today, in your framework, it's a more user-friendly. 
Try something else and see if you like it. Since nobody can guess what you're going to like (and people tend to get really religious about their preferences in the framework world), that's about the only advice there is.
I don't know enough details about your project to tell whether it's a good fit for Django or not. However Django has good documentation, relatively large community and a nice ecosystem of apps, so it has a lot going for it. Personally I find it too restrictive - the ORM and template engine are too limited compared to, say, SQLAlchemy and Mako/Jinja2, and the "everything must be an app" philosophy is annoying when your project doesn't naturally roll that way. I usually end up using Pylons or Werkzeug for most of my web-related side projects. 
I've thought about a mailing list for a little while now, but there isn't one really. Let me ponder it a bit more.
I agree; I also think Vinay (logging maintainer) would be fine with this. If you have a patch, please file it and add my username to the noisy list.
Back in the nineties, Zope was pretty ahead of its time and had some amazing concepts - the idea of object publishing was very much like REST today. Nowadays Zope 2/Plone is a bloated mess and too much of a learning curve - I had a friend who ran a Plone site for his charity and it was a real resource hog and nigh impossible to configure. I suggested Drupal, and that pretty much solved his problems. Zope 3 is way over-abstracted - you look at for example SchoolTool (http://www.schooltool.org/) and think "that would be half the amount of code in Django". It really is J2EE for Python. Fact is that Zope is obsolete, and most Zope/Plone work is now maintaining legacy stuff. Back in the day it was pretty much the only way to do serious web development in Python. Now you have far better options and I can't think of a single good reason to recommend it any more. 
+1 for Django. It's simple to use, the documentation is awesome, the community is great, you can change template engines, write raw sql if needed, use the admin interface or roll your own, and you can always just jump back to straight python...and [Guido likes it](http://www.djangoproject.com/weblog/2006/aug/07/guidointerview/) :)
There are so many frameworks, and which is best is up to what you do with it and what fits you. Django is a full-stack framework. That has the benefit of tight integration, but means that it's harder to do something outside of the integration, or using another part. Some frameworks, like Zope 2, has extremly tight integration, others very loose, and some like Pylons, BFG, etc, have none, and are instead "minimalistic". And Turbogears instead takes different parts that aren't integrated and integrates them. What you prefer depends not only on you, but on what you are doing right now.
I second Django. It's got the largest community, the largest ecosystem of reusable apps by far, it's been blessed by Guido and it's a great framework. Personally I love the ORM that some people complain about. The biggest shortcoming is the lack of sensible defaults a la Rails. Everything has to be wired explicitly. But you if you go down the Pylons road of extreme flexibility, you're going to have to be even more explicit about your choices.
It's not that easy to change template engines. To some extent it's possible, but you find that other things like the Django admin and other apps are hard to extend in your template engine of choice. The same could be said about the ORM. Another annoyance is the UserProfile hack (and yes, it is a hack) - you have to create a separate model to store all your custom user fields, instead of being able to easily replace the default auth.User model with your own. As with your template engines you can do it, but you lose a lot of interoperability when you do so. These issues are of course a tradeoff - a standard template engine, ORM and User model make it possible for all the apps from Django admin to Pinax work seamlessly together in a way that would not be possible in Pylons. If you don't need these apps and you have something more customized in mind then Django is a much less attractive option. That said, I wouldn't hesitate in suggesting Django for people relatively new to Python (or web development for that matter), or who need a lot of out the box functionality and need to get something up and running quickly. 
If you think that sounds made-up, you should try [noodling](http://www.google.com/search?q=noodling&amp;rls=com.microsoft:*&amp;ie=UTF-8&amp;oe=UTF-8&amp;startIndex=&amp;startPage=1). 
&gt; It's not that easy to change template engines. To some extent it's possible, but you find that other things like the Django admin and other apps are hard to extend in your template engine of choice. Swapping template systems is pretty easy. Taking an app written using one template system and extending it using another is hard even in frameworks which emphasize component-swapping. Same goes for most other component choices, too; if, for example, someone else writes an app using SQLAlchemy and I want to integrate it with a an app using Storm, I'm going to have quite a bit of work on my hands regardless of framework...
Agreed, which is the point I made. Django's greatest strength IMHO is the apps it provides (the iPhone of web frameworks ?). In order for the apps to work together you need some basis for them to do so, and that includes for example a common template engine. Regarding the UserProfile thing: would it not be better to have something like an "AUTH_USER_MODEL" setting + functions like "get_user_model" or somesuch, which would allow you to use your custom User model instead in any app ? (or even a customizable "UserField", as used in GAE). This has been my favourite bugbear with Django for quite some time. 
+1 for Pylons or any framework that encourages use of SQLAlchemy. I -hate- strongly dislike the Django ORM. Regarding Django being "blessed by Guido": that's nonsense; he said as much himself.
I've been using [CherryPy](http://www.cherrypy.org/) with [Mako](http://www.makotemplates.org/) for templating and hand-written SQL statements (although there's no reason you can't use your ORM of choice) for a small application that serves statistics through a browser. While you can tie CherryPy in with Apache to serve high-volume traffic, I'll only see at most 3-4 local (i.e. LAN-based) users, so I've not bothered investigating. So far so good, though.
You can monkey-patch the User model with fields, and syncdb takes them. Even you can make a view from tables of another system, and django is happy with it.
Neither monkeypatching or a view are elegant solutions compared to a configurable setting. This is a standard pattern used throughout Django (it's how you set the UserProfile class for example) so why not use it here ?
That I've seen on Discovery/Science Channel
+1 for CherryPy, which I use with Django templates and the Django ORM. I find CherryPy's handler methods to be much more intuitive than Django's regex url mapping. I also like how they parse URL query parameters into method parameters. In other words, if I say class Root: def foo(self, bar): return "Hello, " + bar then the url `/foo?bar=World` will map to the `foo` method, and the string "world" will be passed as the `bar` parameter. Whatever you return from that method will be written back to the client's web browser. I use CherryPy because I love the simplicity and intuitiveness of this approach, but I understand that many other people vastly prefer regex url mapping and getting your parameters all in a big dictionary, etc.
Use whichever framework you feel in your heart is the best.
I only have experience with Django (and in other languages: Rails, Struts, and Stripes), but I would definitely recommend it. Here's the main reasons why I picked Django over Rails for my current project ( [BankFox.com](http://www.bankfox.com) ): -*Amazing community.* In general, the Django community is full of smart, reasonable people who are sometimes sarcastic/snarky, but want to help. The IRC channel is very active, night or day, and full of people who know their stuff. That alone has saved me days of time. -*Clear vision of what it is and what it isn't.* Django doesn't do everything and doesn't try. This makes the APIs and the system as a whole much simpler to understand and navigate. With Rails, I often found myself getting things to work but not understanding why; or APIs that felt like a collection of features rather than a well-design library. -*Python for operations.* I run everything on EC2 and have a lot of code to do the operations of my site (which involves a fair amount of crawling and data processing). Ruby, imho, is a cleaner language in many ways, but I really like Python for operations because it's been used for that for a long time and there mature libraries for almost anything you could ever want to do. 
One application that's overabstracted and written with Zope 3 doesn't mean all such applications are. The problem with Schooltool has as much to do with its implied mission: "be school administration software for possibly all kinds of schools in the world" totally invites over-abstraction. You want to focus such projects (and I believe they've been trying to do that these days). Over-abstraction is however definitely a flaw of Zope culture, and one we're learning to deal with. If you want to look at a less abstracted way to use the same technology, try Grok. I'd recommend Grok because it actually has *facilities* for abstractions. It has a *notion* of configuration. You don't actually need to know or use them much when you don't need them yet, but they're there when you do need them. Grok is the recognition that over-abstraction is bad but we don't want to throw out the abstraction baby with the over-abstracted bathwater. By the way, Zope 3 doesn't exist anymore; it's called BlueBream. Much of its code got extracted into the Zope Toolkit (also used by Zope 2 and Grok). 
aka "catfisting" O_O
&gt; Whatever you return from that method will be written back to the client's web browser. Just to expand on that, if you were using Mako for templating, you would return your processed Mako template, like so: return template_lookup.get_template("viewstats.html").render(var1 = var1, var2 = var2, var3 = var3)
Don't make your decision based on what other people write about it. If it is working for you, just go with it. It is a hobby project (and not a big one), so you don't need evaluate which framework is the best. I used Django for a fairly big project, and even though in the end I modified much of it it worked out surprisingly well.
There are shortcomings of the Django ORM for sure, but for me it did the job quite well in a fairly large project. There is nothing from stopping you to use SQLAlchemy with Django either.
&gt; and the "everything must be an app" philosophy is annoying If you are bothered by it, just create one app for the whole project.
Sorry, should have said BlueBream - somehow however it just doesn't *sound* right. Definitely +1 for Grok and BFG though, if Zope has a future it's through simplification, not abstraction. The also take much of the pain out of legacy projects. Given that SchoolTool is over-architected by design rather than framework, could you point to an example of a well-designed, open-source Zope3 application ?
For this type of project the similarities between frameworks exceed their differences. If you know Django, stay with it. Yet I can tell you some of the reasons we developed web2py for, in case they apply to you: 1) we want to have the option to write code once that runs with relational databases and Google App Engine; 2) we do not want to remember all the imports at the top of the files; 3) we want to be able to put arbitrary Python code in the templates; 4) we want tickets to be issued to visitors when an error occurs and we want to be able to browse tickets; 5) we want a record level Role Based Access Control; 6) we want an extensible authentication mechanism; 7) we want automatic migrations so we never need to write "ALTER TABLE" or any other migration code during development; 8) we want to be able to edit the app while in production without having to restart the web server, ever; 9) we want a full web based IDE and web based testing. 10) we want to be able to connect to multiple database simultaneously and perform distributed transactions. 11) we want defaults for everything to speed up development (a model gives you a default db administration interface, a controller gives you a default view, etc.) and no configuration settings, for portability; 12) we want cron built into the framework so we do not have to rely on unix cron; 13) we want a web based interface to internationalization to make it very easy to translate an app. web2py provides this functionality but we could not find it in other fremeworks. You can take a peek at its interface [here](http://web2py.com/demo_admin) without need to install anything.
Of course you can, but that approach is frowned upon. Look, I'm really not trying to pick on Django - it's a fine piece of kit that I have used on many projects. But there seems to be an attitude among some of the Django community - thankfully not one shared by the core developers - that it's the One True Way of doing web development in Python. Sometimes shoehorning a project into a framework is less desirable than just using another framework or even rolling your own. It's all compromises and trade-offs that depend on the requirements of your project and your experience and preferences. The problem with some frameworks is that they become so hyped and all-encompassing so that people start calling themselves "Rails developers" or "Django developers" rather than just plain developers and see their framework as the ultimate hammer for all nails. They then get very offended at the slightest criticism of the framework. If you are true to your craft you should always be open to different solutions and not tie yourself down a single framework, language or methodology.
That automatic admin interface is damn helpful! I'm so used to it now I'm not sure I'd consider any framework that doesn't provide it. I taught my former employer Django (he was a PHP guy) and he marveled at it as well. It's simple, but saves a lot of time. 
Tricky! I'll talk about my own apps as I know those best. I have a few non-open source Grok applications I work on a lot, but that doesn't help. I don't claim they're particularly well designed, but at least they're not over-architected very much. :) This is the first Zope 3 app I helped create, and I'd certainly design some things differently today (it's four and a half years old and we were learning our way around): https://viewvc.infrae.com/svn/documentlibrary/trunk/src/documentlibrary/core/ It has a pretty particular UI, and definitely evolved after that. As a result of both, some bits are messy, but that's what a real-world app tends to be like anyway. And when I came back to it last year to add some features, it wasn't *too* painful. :) Of course I haven't worked on many Zope 3 apps after that, as the pain I had helped me get some ideas for Grok. As to Grok, this is an implementation of a web service I did a while back with Grok. The notion of this web service is sufficiently abstract not to be very helpful in seeing what a typical application looks like. That said, it deals with various tricky real-world issues (Flash not having world-class REST support and such): http://code.google.com/p/imagestore/ 
So are with-hacks safe to use productively as in, will they still be working in a few years?
I like the way Werkzeug does URL mapping, quite similar to Django in that you get a big map of the URLs in your site but using a simpler pattern than regexes: http://werkzeug.pocoo.org/documentation/dev/routing.html 
I wouldn't actually classify Pylons as a very "full stack" framework; TurboGears would be more the equivalent to Django, built on Pylons. Anyway, the things you mention aren't particularly where a full stack framework will be a big advantage. The useful stuff would be if, for example, you want permissions and user management (e.g., to control access to viewing the logs). The core stuff you mention will be mostly as easy in a minimal environment as it would be in a full stack environment. Given that you are dealing with logs, a relational database is probably not a good idea. You should at least figure out what you are doing with a realistically large dataset; it's easy for a problem like this to seem feasible with a small test log file, then fall apart completely for a large log.
Personally i would not implement all of that functionality in your web app.. you want a backend to do that really which is away from your web app otherwise you are leaving yourself open to exploit. - the more your webapp does the more likely it is that someone can get into it. for actually displaying this data from a database django will do it, but so will many other frameworks i suggest you have a look at the docs and find what looks to suit your own style of working. Personally i hate django because i find it quite clunky but i can see why people like it.
You can, but you lose a lot of the benefit of Django models. I'm working on a project now with pylons since I prefer sqlalchemy (multiple db backends), but for a small site I might pick Django - it seems to have a lot more drop-in modules.
I started learning Turbogears 2, and when I could figure out what to do it was great but sometimes it took hours to figure out something that only took 2 minutes to implement. The documentation was just severely lacking. When I picked up Django I fell in love with the documentation and the community. For all it's shortcomings, at least I knew what they were via documentation and a great community. 
sanescript does it fine see http://bitbucket.org/aafshar/sanescript-main/overview/
ConfigObj uses a dictionary interface - so if you collect command line options as a dictionary with the same structure as your config file you can just use: `config.update(command_line_options)` With ConfigObj (unlike ConfigParser) there is no need to use a section to store values in if it is not appropriate.
Upvoted for the author realizing that (as the author calls it) "design-by-subclass" is a bad approach. I think that the GoF admonishment -- prefer has-a relationships over is-a relationships -- is relevant here. You don't have to give up OO, you just have to push things into attributes where it makes sense. 
Mmm, interesting. Thanks for the tip. I've already planned to use ConfigObj instead of ConfigParser for the rest of my project, so... lets see how it goes.
Mmm, [config.py](http://bitbucket.org/aafshar/sanescript-main/src/tip/sanescript/config.py) does indeed looks interesting. Thanks.
Everybody's said this already, but I also vote for Django. Easy to learn, very powerful and it'll save you a lot of time. Check out http://stackoverflow.com/questions/tagged/django for help with anything, stack overflow is my favourite programming q&amp;a site.
I submitted a patch [0] to argparse to add support for unix style config hierarchies (global config, local config, environment variables, command line). Check it out and vote it up if you find it useful. 0 - http://code.google.com/p/argparse/issues/detail?id=35
&gt; currently I'm working with Django but looking at some of the posts people seem to dislike it If someone said they dislike meat would you stop eating meat? If everyone cared so much about what others thought about a specific framework or language, nobody would get any work done. There's stuff that suck about Django, but no framework or language is perfect. 
[Werkzeug](http://werkzeug.pocoo.org/) is a good thing to check out if you don't adopt one of the full-stack frameworks.
FWIW: if you surround config.update(command\_line\_options) in backticks, it will come out as `config.update(command_line_options)` as you likely intended.
Sometimes the fact that Django's roots were in news article publishing show through. There are a *lot* of applications that model works very well for, so I think it's a fine default choice and for the application you describe, but every now and then if you're not developing a [CRUD](http://en.wikipedia.org/wiki/Create,_read,_update_and_delete) application, it fits weird. That is, when your application is more about interacting with code objects than database records. The people I know who have decided not to go with Django often like to build something out of [Werkzeug](http://werkzeug.pocoo.org/) tools. A persistent server like that of [Twisted.web](http://twistedmatrix.com/trac/wiki/WebDevelopmentWithTwisted) is another option. [Mantissa](http://divmod.org/trac/wiki/DivmodMantissa) is built on the Twisted stack, and frankly, it doesn't have Django's wealth of documentation and community right now, but if you're interested in exploring the range of options it's worth including in your survey.
Yup, but 2.6 being the 'current' 2.x, it should probably go in there.
Following your specs you could just dump .html files to disk and serve them with nginx or apache. But if you want something more fancy / dynamic, +1 for Django. No extra dependencies needed, you can download a dumb tarball and get started, you just follow the tutorial and you'll get your app running in no time. web.py is also a good choice for a very lightweight framework, but no ORM.
Unless OP understands Python to a similar level to Guido, it's not that useful to know that Guido has a preference. OP is better off taking advice from others who use Python to pursue similar goals.
Thanks - updated.
I'll throw tornado (http://tornadoweb.org) in the ring - I've used it, django, cherrypy, pylons/turbogears, and tornado was by far the simplest to get up and running. It's built around a very lightweight but very performant webserver / core. It also comes with mixins to use twitter/facebook/google for logins, and a pretty nifty live chat demo. Django does some shady things under the hood, and can throw some pretty magical errors which are difficult to debug. I think it's ORM is better than sqlalchemy. I'm using sqlalchemy right now and it's been absolutely awful. If you really drink the django kool-aid and use it for everything, it will get easier. Genshi, imho, is a very nice templating engine. It uses XML and real python, so you don't have to learn django's magic template syntax. Turbogears is ferociously complicated.. Unless you really have an opinion about every little part of your stack, it's probably not worth it. As someone else mentioned, for what you're doing, it would be pretty reasonable to have a cron script parse logs and generate static html that gets served up by lighttpd/nginx/tornado. But seriously: tornado with genshi will do everything you need in a very light package, and perform as well or better than the others.
I was not wild about django's ORM, and sqlalchemy's syntax / design seems much saner at first, but it's pretty inefficient when it starts generating queries / tables / actual SQL.
Out of interest, what did you find that was so awful about SQLAlchemy ?
Pylons with the Jinja2 templating library It's not by mistake that Reddit uses Pylons.
look at mongodb for persistence instead of postgres. postgres is great, but mongo is much faster and has a map reduce engine, which is useful for processing data into stats. 
+1 for Jinja2
Look into swig wrapping your libraries to be accessible from Python (among others). I don't know of any great tutorials but [The C Programming Language](http://en.wikipedia.org/wiki/The_C_Programming_Language_\(book\) ) is *the* reference.
My mistake for using the word dislike, but if there are a lot of criticisms of a framework they aren't all going to be unfounded. 
I think the misclassification of Pylons as a full-stack framework may be a direct or indirect result of its position within this page: http://wiki.python.org/moin/WebFrameworks (it's listed under "popular full-stack frameworks").
So, just curious, whats the big deal with templating engines? Who cares? Surely speed is not the issue, template rendering takes like .03%* of the total request time And with the ORM, i have come across a few warts, but all in all its very intuitive... *Made up on the spot, like 58% of all statistics. EDIT: this was meant to be a reply to the Django thread
Any specific examples of this ? I've found SQLAlchemy to be a complex beast, but quite powerful and efficient once you work out the right way to do something. 
I agree. Django is the shit. Sometimes though I wish there was a Java framework that was as awesome as Django just so I could harvest the awesome JVM, but until then Django has been my choice.
Cheers
After giving yourself a brief C refresher you would probably be well-served by getting a recent version of Cython installed and then picking a random library and creating a Python interface to same. It will teach you a bit about interfacing C&lt;-&gt;Python via Cython (good skill to have given your stated interests) and once you have a basic understanding of Cython you can start doing things like profiling and speeding up some of your existing Python code by replacing bits with performance-oriented chunks of Cython. 
TBH, most of the criticism that I've seen boils down to a fundamental difference in design philosophy: some frameworks provide you with a whole stack of components tightly integrated, while others just give you some glue code and tell you to go choose the components you want. There are more than a few people on both sides whose position basically boils down to the assumption that their preference is not just a preference, but is a Grand Eternal Truth to which all heretics must be converted.
You can run Django on the JVM via Jython. And [this project](http://code.google.com/p/django-jython/) provides a management command which goes a step further and [bundles up everything you need (including all your settings and app code) as a WAR file](http://packages.python.org/django-jython/war-deployment.html) with no other dependencies.
Have you looked at Grails?
I've decided to stick with Django the PostgreSQL support is really awesome for what I'm doing, though TBH I'm quite a ways away from any visible work, so far I've been putting work into the actual parser, output and stuff is still in the future.
I used to be all about the Django, but after using it for a while I've become a bit disheartened with it. * The Django documentation is pretty good, but the documentation for some of the de facto standard apps is absolutely terrible. * I used to think the admin interface was amazing, but now I seem to prefer Grails scaffolding (which I understand to be similar to how Rails does things). * The MVC concept as used in Django seems a bit wooly. I'm finding that the Grails-style approach of domain class/controller/view suits my way of thinking better. * The templating language is... meh. Having an 'if' tag that only tested a single variable in boolean context, and having different tags for 'ifequal' and 'ifnotequal' seemed like a ridiculous constraint. I got sick of hearing about not putting logic in templates. I'm a big boy (and a seasoned developer); I'll make my own decisions about where to draw that line. Jinja, Genshi and GSP all give me that flexibility. (And yes, I know some of this has changed in 1.2.) * Again, with the templates. They are excellent at making it really difficult to figure out where your exception actually came from. * I find it hard to articulate what I dislike about Django's ORM, but I prefer SQLAlchemy. I think it's to do with where they are on the O&lt;--&gt;R scale, with Django being closer to O and therefore hiding more. Some people would consider that a good thing. * The user/profile debacle. What a pain in the ass. The ticket to fix it has been open for about three years. * There's no AJAX support. The Django community attitude to this seems to be both "we don't want to bless a particular framework" and "AJAX is easy anyway". My response to the former is that they didn't have a problem 'blessing' a subset of databases. To the latter, there's easy, and there's easier. Look at GSP's AJAX integration, for example. * Pinax. This isn't really Django's fault, but it's a common add on. And one that I shall not be touching ever again. What's the point of bundling together a bunch of apps if you're not going to provide and coherent documentation? * Scalability. There's only one database! Even the multiple-database stuff in 1.2 seems very manual. If this sounds like a lot of criticism of Django with nothing positive, the explanation is simple. I'm currently a Django user looking for something to switch to. I haven't setting on anything yet, but I'm prototyping my next site in Grails. I haven't ruled out the possibility of using my own Werkzeug/SQLAlchemy/Genshi framework. Having said all that, Django is great for *rapid* development.
Really? Have you seen Cal Henderson's DjangoCon talk where he points out how this isn't the case at all?
&gt; The IRC channel is very active, night or day, and full of people who know their stuff ... and are always ready and willing to tell you that you don't.
&gt; But you if you go down the Pylons road of extreme flexibility, you're going to have to be even more explicit about your choices. Here are the choices. They involve pressing the enter key once, and typing the word "true": classic$ paster create -t pylons helloworld Selected and implied templates: Pylons#pylons Pylons application template Variables: egg: helloworld package: helloworld project: helloworld Enter template_engine (mako/genshi/jinja2/etc: Template language) ['mako']: Enter sqlalchemy (True/False: Include SQLAlchemy 0.5 configuration) [False]: True Creating template pylons... ... Done And you're ready to go, you have a working app. How painful was that explicitness ? How afraid should people be of that ? How *extreme* is that ? I do not post on these stupid web framework threads until someone starts FUDding on Pylons. It never seems to take more than ten minutes for it to happen.
Speed is not the issue, but flexibility and abstraction. Different template engines allow different ways of handling content, which suit different teams and projects. Genshi for example is great if you want structured XML/XHTML output, Django templates if you want limited control (for example a content/design team rather than developers), and Jinja and Mako for a more flexible, developer-oriented approach.
Inefficient in what sense? It seems to me that the SQL that SQLAlchemy generates is just fine and dandy... 98% of the time.
&gt; There is nothing from stopping you to use SQLAlchemy with Django either. But then you lose a big piece of what Django has to offer--its alleged full-stack consistency. Another perspective on this is that Django is geared specifically toward Web-only apps, whereas using a framework that encourages a more general purpose SQL/ORM layer can more easily grow beyond the Web. Of course, one can utilize the Django ORM outside of Django, but I'm curious how often that happens. I have nothing particular against Django on the whole. I think it's a fine framework. It just doesn't "fit my brain" as well as Pylons/SQLAlchemy.
It's not the total request time that's the headache, it's the time your webbish developers spend messing with templating. People are more costly than CPUs. 
I'm not looking to run python on top of the JVM. I want Java or Scala. I think the JVM has come a long way to the point where it is very fast and I've kind of realized that I prefer statically typed languages.
It can be a snarky group, especially if you haven't bothered to read the documentation before posting. But I actually like that; it encourages people to do their homework first and keeps the level of discussion higher (rather than 100s of intro questions which are readily answered in the tutorial)
Speed can be an issue. Although only for sites the scale of reddit.
So what does this mean? I just keep django up to date and it comes with jquery? Or the pretty bits of django are written in jquery?
Pretty bits in admin interface and such now use jQuery, but I think it's available for your apps too in trunk and Django 1.2. I use trunk myself for my persona project, which is dangerous, but that's my middle name.
I would recommend Pylons. It has a medium sized learning curve, but might be good for a project of this size.
This is only relevant to the admin part of Django, although I'm sure you could directly reference it from your own pages if you point at the (static) file in the admin directory.
This may have been true at one point, but with the advent of Phusion Passenger (mod_rails), Rails deployments are cake. They only stand to get better with the imminent release of Rails 3.
Since this is posted in /r/python I'm assuming the other frameworks you are considering are Python frameworks. If you look around you'll see that the number of people who criticize the other frameworks is proportional to the number of people who use them. All the criticism boils down to people's personal preferences and what feels right or wrong. It's never about something being fundamentally broken, slow or insecure (when that's the case people usually file bug reports and stuff gets fixed). I've been using Django for 2+ years and I can find lots of stuff I don't like about it. But that all comes down to personal preference, and lots of people have similar personal preferences. Most of the criticism towards Django and any other fast moving OSS project gets outdated quick, as the people behind it actually listen to the community. IMO the reason Django is more popular than the other frameworks is that it beats the others in the documentation department. And having a larger community means you can find help about stuff easier and you can find people to work with you on a project easier. And as you get on a higher level of mastery with one set of tools it's harder to switch to another. From a technical standpoint for 90% of the web development projects any of the modern web frameworks will do.
I think a large part of Django's success is historical chance (although this is not to downplay the quality of Django itself). Back in 2005 the Python community was convinced the sky was falling - OMG RUBY IS GOING TO EAT OUR LUNCH - and a serious competitor to Rails needed to be found and quickly. It could have been TurboGears, but the quality of available packages needed wasn't near good enough for production use. Pylons, WSGI, Paste etc were all in the future. Django happened to be an independently developed, production tested, end-to-end framework that could compete against Rails, and so the developers open-sourced it. As there weren't any decent ORMs, template engines etc around at the time, the Django guys rolled their own. This unfortunately led to a NIH pattern in the framework that persists today despite there being better options around. EDIT: it helped that the Django developers - coming from the newspaper scene - happened to be very good writers and publicists and were able to produce high-quality documentation. Good documentation is frequently undervalued by developers but it really is the key to success outside small hacker circles. 
More fun that way ;)
http://sourceforge.net/projects/sknobs/ is used extensively in design verification projects. Has python/perl bindings. Very handy.
They still run Dapper drake!?
I would take a look at pylons, i made the switch from Django and find it a lot less restrictive. It has a slightly steeper learning curve but its worth it.
My [config module](http://www.red-dove.com/python_config.html) allows dict and attribute access and has [easy integration](http://www.red-dove.com/python_config.html#integrating-with-command-line-options) with optparse.
+1 for pylons or werkzeug(not a FW actually)
Good reexamining. I hope people don't abandon utf-8 for the sake of those who don't speach english.
Interesting read. Just goes to show the whole "no free lunch" aspect of adding features/functionality to a language. If you want better unicode processing, you'd better be prepared for the cost.
&gt;Due to space constraints, I can't talk about everything in my tutorial here. What, is the internet full or something?
I'm a sucker for YAML to be honest.
I think you capture the historical background to this pretty accurately. Zope stuff wasn't a candidate either - old stuff too crufty, bad reputation with influential people, and Zope 3 too verbose and weird, underdocumented and "enterprisey". Grok was in the future too (2006). I must disagree about there being no decent template engines around in 2005. There were decent template engines around, and the Django guys rolled their own as it's easy and you get full control. :) Concerning NIH, as someone outside of Django I'd like to be able to reuse bits of it without having to pull in all of Django. Unfortunately that doesn't appear to be likely to be possible any time soon. There are of course benefits to developing everything of it together. Release management is a heck of a lot easier for instance. Meanwhile though having cut Zope 3 into bits it's a lot easier to dump unused code and straighten out dependency relationships between the code that remains. 
&gt; I must disagree about there being no decent template engines around in 2005 I remember Kid and Cheetah (outside of Zope). Both worked, but I always found Kid rather counter-intuitive and Cheetah needed pre-compiling (the memory is hazy on this). I can see Django going down the same road as (old) Zope - in that there was a lot of useful stuff in Zope like ZODB but it took a lot of hacking to extract it. Shame, because Django forms and ORM do have their sweet spots but as you say they are difficult to use outside of the framework.
[Django adds Jquery to trunk](http://www.reddit.com/r/Python/comments/aul8m/django_adds_jquery_to_trunk/)
There were no *perfect* template engines around in 2005. But plenty of *decent* template engines were around in 2005. :) The ZODB is the one bit in Zope that always remained detached actually - I'm pretty sure that has been so for at least the last 8 years, probably going back far more. It's all the other stuff that was harder to extract. :) Django's not Zope, but does some of the things old Zope did. Whether these were mistakes for Zope or not depends on how you look at it, and whether they're mistakes now for Django also depends on how you look at it. I just know there's some stuff in Django I wish I could reuse in a Grok context without having to pull in all of Django. 
jQuery in Django is only used internally in the (optional, contrib) admin application. There is no integration provided for using jQuery in your own project, nor any bias towards using it, nor will there be any. From the user side Django is and will remain Javascript-framework-agnostic.
I have two stories: * Grok provides such integration and therefore got support for jquery and a lot more. * if your web framework provided support for hurry.resource, you would offer a lot of integration easily! Grok includes jquery support and more. That is, you as an application developer can easily depend on such things by modifying your project's "setup.py". No need to depend on trunks or wait for releases - it's there, it's released, and it works today. How would you use jquery in your grok app? Make your project depend on hurry.jquery. You could also depend on hurry.jqueryui and hurry.jqgrid, or if you prefer YUI, hurry.yui, etc. You can also package your own javascript code that way! All those are framework agnostic agnostic packagings of javascript library that build on the 'hurry.resource' reusable Python library, which I wrote :). hurry.resource offers interesting features, such as javascript dependency management, support for rollups, support for minified versions, and so on. If your javascript code depends on jquery, you can easily express this, and then if you depend on your code, it'll pull in jquery automatically. megrok.resource builds on this, integrates it smoothly into Grok, and also offers sophisticated cache management so that during development of javascript your browser automatically reloads any changed javascript. All this makes Grok's support for javascript libraries totally rock, but we're not keeping it for ourselves! We're just nice that way! Ask your favorite framework to provide hurry.resource integration too! http://pypi.python.org/pypi/hurry.resource http://pypi.python.org/pypi/megrok.resource http://pypi.python.org/pypi/hurry.jquery http://pypi.python.org/pypi/hurry.jqueryui http://pypi.python.org/pypi/hurry.jqgrid http://pypi.python.org/pypi/hurry.yui 
PyCon tutorials are three hours long. If he covered the whole thing the reddit comment thread would just be endless stream of "tl;dr"...
Grok is also javascript framework agnostic, but in addition *does* offer integration for using jquery (and YUI, and whatever) in your project. :) 
web2py includes jQuery since March 2008. The server-side of the framework is decoupled from jQuery so that one can use other js libraries, but the scaffolding app includes jquery base and some scripts based on jQuery. These scripts provide the following functionality by default: - enforce type of input fields (for example you can only type digits in an input field that is supposed to contain "integer". Works for "double" too. this is complementary to server side validation) - provide date, datetime, and time popup widgets for fields of the corresponding types. In all forms generated by web2py and custom forms. - slide down effects on flash messages and form errors - traps form submission/processing for page components. This allows to embed multiple forms in a page and submit them via ajax without reloading the entire page. It includes a callback mechanism that allows the server to trigger other page events when responding to form submission. This does not require any special sever-side handling. Any web2py action can be embedded as a component via {{=LOAD(...)}}. - all code generated by web2py (for example forms) is CSS friendly so that it is easy to reference from jQuery. For example SQLTABLE generates tables compliant with the jQuery datatables plugin. We also have a collection of web2py plugins that wraps in a plug-and-play fashion jQuery plugins such as for autocomplete, rating, tagging, comments, and datatables. Some are posted [here](http://www.web2pyslices.com/main/default/search?query=jquery) We have an optional [clienttools library](http://www.web2pyslices.com/main/slices/take_slice/8) that allows generation of jQuery code from pure python APIs. EDIT: All the jQuery references in the scaffolding app are in a single file. If you do not include it, you turn off all of the above functionality. Even if you do not, we include jQuery in the "no conflict" mode so that it plays well with prototype.js for example. There is no server side code that relies on jQuery.
 The PDF is currently up to date, but the HTML version always has the latest. But at this point, I'm only finding minor typos. The PDF should be fine.
That usually happens when you install it in a non-standard location and haven't set the right paths, try setting the QTDIR environment variable to your Qt installation or check the menus, one of those should fix it if I am not mistaken.
You need a pretty good understanding of C to link it to Python, as the Python embedding/extension APIs use advanced features of C.
Apparently proggit doesnt care. Maybe you guys will!
Well, I do now.
Always have.
Eh, sorry... I say "num pie". Pronouncing it like lumpy almost sounds better, though. I like it!
I would, except that it's NumPy. The capitol 'P' compels me use the 'pie' sound in Python.
Check out the [mechanize](http://www.ibm.com/developerworks/linux/library/l-python-mechanize-beautiful-soup/index.html) features. All you need to do is set the field values in the form and submit.
Now I'm saying it like "bumpy" because it's fucking awesome. Damn you! 
"numb pee"
Pretty sure I heard John Hunter, original author of matplotlib, pronounce it like that. 
I usually use [HttpFox](https://addons.mozilla.org/en-US/firefox/addon/6647) to first capture the HTML protocol exchange. Then you can use one of many modules to implement the protocol in a Python script. I have used [curl](http://pycurl.sourceforge.net/) a few times, and it's easy enough. I can send you a script that does some basic things like login, cookie management and a POST. But I am sure you can find enough examples on the net anyways.
Awesome!
He's wrong, of course. ;)
I pronounce it at least three different ways. (Do I contradict myself? Very well, then I contradict myself. I am large, I contain multitudes.) usually I do *numb-pie* sometimes, especially when my brain is on autopilot, *nump-y* occasionally, *noom-pie* And you know what, I don't even care which is supposedly correct
Isn't it nice that I can ask just here? I'm trying to do a seemingly simple thing: a process tries to connect to a given host repeatedly, when successful it spawns another process which works with the connection. Like, I have my own computer with a public IP and want to run such a process on a remote computer behind a NAT which has a Special Device connected via a serial port, the spawned process would then download and exec a module from the server and provide RPC to the server (which is a client in the TCP/IP terms). And then I could debug my application that works with the device without having to go on site! Everything is supposed to run on Windows, Python 2.6.4. I don't know why, but the experience so far was increasingly frustrating. 1. `multiprocessing.connection.SocketClient` catches Connection Refused and redials in 0.01 seconds with no customization available, so I had to reimplement it and so I have a line `Connection = multiprocessing._multiprocessing.Connection` which is not pretty. But that's almost OK, compared with what follows. 2. I don't want to do a true preforking and all required synchronization. Shouldn't it be easy to just give a successful connection to a newly spawned process? [This bug is still open](http://bugs.python.org/issue4892), so I guess not. 3. OK, I'll get a socket, share it with a child and create a `Connection` there. Oh wow. I get [almost this exact error](http://www.mail-archive.com/python-list@python.org/msg223652.html) (note that the post is from 2008 and has no replies), I mean, the stacktrace is different, but `socket.fromfd` is still called while being predictably absent on Win32. [Here](http://code.google.com/p/python-multiprocessing/issues/detail?id=15) some guy took a time to get somewhat closer to the source, _back in April_. Why is this happening to me? Why there is an example [in the docs](http://docs.python.org/library/multiprocessing.html) (search for `SimpleHTTPServer`) which simply doesn't work (on win32)? Despite having this: if sys.platform == 'win32': import multiprocessing.reduction # make sockets pickable/inheritable Whyyyyy? I am disappoint!
I second mechanize. I made this [little script](http://paste.pocoo.org/show/170837/) to show how easy logging into websites is with mechanize. Logs into reddit and prints out the titles and urls on your front page. It also uses [lxml](http://codespeak.net/lxml/).
I'm still not sure how you properly pronounce "tuple". I say "tuh-ple" but have also heard "two-ple". 
I learned 'tuh-ple' way back in 6th grade, but, ever since, I've only heard 'too-ple'.
i pronounce it as num pi, i also pronounce gui, as geeee you eye, i can't stand the word gooeeey for some reason.
AppEngine?
&gt; for name in error_404_urls: print(name) That would be cooler as `print(*error_404_urls, sep="\n")`. It doesn’t perform as well though, sadly.
Thank you very much!! I am able to login, but now all I need to do is figure out how to parse the data!
Wow, I thought everyone pronounced it that way.
Rhymes with "dumpy", of course. Also, SciPy is obviously pronounced as "skippy".
Awesome!
Okay. I think 'tuh-ple' is more faithful to the derivation of the term, and so, in that sense, more correct. But I've heard 'too-ple' way too much for it to be anything else in my head. And 'tuh-ple' sounds weird to me, now. (When I first got to university, 'too-ple' sounded weird, though.)
Anti-Blogspam: http://www.nltk.org/
God I hope not. I'd like to believe that we're above this sophomoric crap
Look into [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) and [soupselect](http://code.google.com/p/soupselect/). With those together, you'll be able to write code that looks like soup = Soup(page_html) balance = int(select(soup, '.accounts .balance').text) 
numpy = num + pie Mmmm, pie! Num, num, num.
it'll be a great learning experience to build a system yourself from scratch. Start with a good framework to get a lot of your heavy lifting done; research patterns for implementing your project. However, I first recommend you review commercial / open source solutions to get a feel for the field of library science. Believe it our not it's actually a Masters track program at many major universities and _generally_ as a field, they're far ahead of industry as an average. You can probably find a lot of open source software to get some idea's from. [Start Here](http://www.dmoz.org/Reference/Libraries/Library_and_Information_Science/Software/) BTW, in 1992 I worked on a similar project for *my* high school. I went on to a degree in CS and now run an IT consultancy. Best of luck!
You really want to check out Django. It makes things really easy.
You can also look into Pylons. While not easier than Django, I find it much more flexible and hence, powerful.
These are non-trivial problems. You are sure to learn a lot trying to figure them out. Best of luck. You may want to pursue two strategies; roll out a foss solution and then extend it on your own.
Surely you want your balance as `decimal.Decimal`, not an `int`. Every penny counts*!*
I've imported Django on a "private" branch, and use svn-load to jump around versions. This because I had to fork it a bit.
I created a little project that does everything that you want. My code is probably horrible, but it works: https://sourceforge.net/projects/easynewsgrabber/ My script gets a TV show schedule, then searches easynews.com for the show, if it finds anything it logs in and downloads it. There's another script I created which emails me which shows have been downloaded. I'm not sure if that's been uploaded yet. Edit: I'm using urllib2 and beautifulsoup for logins and html parsing.
Please note: if you want to talk about performance, you should at the very least provide a correct C implementation to give a point of comparison.
Why is this in the Python subreddit?
I recommend you full-text-indexing search engine, like Xapian. It supports SQL DB, PDF, EXCEL, WORD; Most of standard text-based formats. For pluggable, reusable Python applications, Django or Plone may suite you. Note that Plone has a big learning curve. (there is a great manual growing at http://plonemanual.twinapex.fi/)
I was thinking the exact same thing, I like Lisp, but why isn't it in /r/lisp?
And 15x less readable.
I do numerical programming in R, Python, Java and Common Lisp. Usually most of the serious number crunching is done with standard highly optimized libraries like BLAS and LAPACK etc. in every language. R and Python have scripting language feel in them, they are used to call C libraries and massage data from floats to strings and back. Doing even small part of the number crunching with them gives massive performance hit. If datasets you use are small, this does not matter. If that is not the case, you lose big. Basically if performance is issue, it's C, Fortran, Common Lisp or maybe Java. In my experience, you can write easily floating point performance that is within 2X the performance of C or Fortran. That is fast enough for 90% of time. With more effort you can get closer to C performance, depending on the algorithm, maybe 1.0X - 1.3X. In some cases, you can make it run several times faster than C or Fortran. This is possible because CL has compiler in it's runtime and really powerful macros (macro language in CL is CL itself). You can write code that loads data, figures out some constant parameters and compiles code that fits to data. 
I like Python and Lisp and I must disagree. In both Python and Lisp, indentation is used same way. Parens in Lisp are just for the compiler. Programmers look only the innermost parens at most. The only big difference is that Lisp is intended to be used with only Lisp aware editor. 
Agreed. Wrong group. 
Because you're trying to fork on windows.
So? First of all, there's nothing inherently wrong with forking on Windows. Sure, it eats somewhat more memory (but not as much as it seems because all dlls are shared), this doesn't prevent Google Chrome from forking for each tab and it's _fast_! Then, I would understand if the docs just said: "not supported, sorry". But it _seems_ supported! It deceives and ensnares! There are all these little conditionals in the source and special functions like `freeze_support()`, and there is this **example** in the docs with a conditional, which probably worked at some point, but then was broken and nobody gives a shit! I've seen a lot of quirks in the more obscure parts of Python's stdlib, but never anything as outstanding.
Writing a script to populate a DB (mysql, sqlite, postgres) will take a half day, but the first time you see your data in the Django admin, you'll realize it was worth it. Follow the Django tutorial and think about how you would set up your books model. Just the admin and generic views can take you a long way. 
Check out Solr for managing search.
why post this here ? downvoted
to piss us off ?
to piss us off ?
Along with "b"...most of the single character variables wont work with pdb prompt...but yeah pdb saves you a lot of time which otherwise is spent in debugging code... PS: You will find at least 1 commented pdb.set_trace() in my code
Kind of basic stuff, but I guess a good intro for someone who hasn't used it. I think many developers simply don't understand how easy it is to use this debugger and that's the biggest issue. They just need to use it.
How many people here debug almost exclusively with `print` statements (or the logging module)? I'd be more inclined to use a debugger if my editor/IDE of choice (Editra) had built-in support for one.
You can put a ! at the beginning of the line to indicate it should be interpreted as code instead of a debug command.
specifically, with the ORM syntax, it would basically generate double queries for everything. e.g., delete from foo where foo in (bar) would query 'bar', then query 'delete from foo where foo in (bar)' In zzzeeek's defense, there is an optional parameter in the delete method which he claims prevents this. 
On top of this, I use ipdb, from ipython all the time over regular pdb. Tab completion, syntax highlighting, history, a much better environment. http://pypi.python.org/pypi/ipdb/0.1dev-r1716
i blogged about my particular troubles here: http://japherwocky.posterous.com/orm-hatefest-continues
no, this is my personal experience, actually using it for real code.
Well. I just made [this](http://web2py.com/library) for you. It is 200 lines of web2py code. It provides: - user registration - full text search (title, authors, publisher) - book archiving (title, isbn, location, etc) - loan management (requires users marked as librarians) - librarians can make other librarians - two levels of administrative interface (administrators and librarians) - import/export of database in CSV You may want to customize the layout and add some functionality. The librarian interface is disabled on the web site above for security. You will see it when you run it. To install get web2py and cut and paste the "download url" unto the web2py admin interface and boom! Use it as you like. Hope this will give you an idea of the kind of support we give to our users.
for those who live inside pdb like me, there is this nice extension that I called pdb++: it adds tab-completion, numerous new commands etc. to the good old pdb: http://morepypy.blogspot.com/2008/06/pdb-and-rlcompleterng.html
whenever i've had to do anything in sqlalchemy, it's ended up taking about a week longer then I expected it to, usually as I'm hung up on some particular bug. despite the author's claims I don't think the docs are particularly good; examples from the tutorials didn't work as documented, and "support" has mostly been the author (zzzeeek) telling me to read the docs. Coincidentally similar usernames?
If you are using django I'd highly reccomend [django-haystack](http://haystacksearch.org/).
No, I'm not Mike Bayer. It seems you would be happier just using raw SQL + MySQLdb (or whatever driver). If you really don't like a library or framework and it just doesn't fit your brain then you just won't be happy using it. ORMs turn off a lot of people; particularly those who are at home with SQL and like to fine-tune it the way they want. Personally I find SQLAlchemy to be the best data abstraction layer around, period, and of high quality. Having said that it is complex, because it's managing a complex thing : the object-relational impedence mismatch. Unlike most other ORMs however you can drop down to the data mapping layer without using raw SQL. The only other data mapper I've seen that has something like that kind of functionality is LINQ. Usually with SQLAlchemy I struggle until I get that "a-ha" moment and everything falls into place. 
Thanks, that's useful.
Half a day? Here is a the web2py script from the [code here](http://web2py.com/library): db.book.import_from_csv_file(open('file.csv','r')) and you do not need to write this either because there is a form in the appadmin interface for such task.
I love pdb, especially in Emacs. I run the Python script from a shell in Emacs. When it hits the pdb.set_trace(), the windows splits. The portion I was in will change to the pdb prompt, the other half will be the point in the Python script where it stops. It follows you along as you step through the script, unless you jump to some compiled stuff. I don't know what I would do without it, it makes debugging so much easier.
pdb is a toy compared to gdb, and i find myself considering using gdb to debug (c)python daily.
How?
zzzeek, mate relax. We won't change the world, a lot of good guys use pylons or werkzeug because it works great - that's what counts. Let amateurs be, they will get their punishment when it appears that django can't do something they want, or it does something differently they they would like to. Best what we can both do is to focus on our tools. The right professionals already use them and thats something that we should be proud of. Give ppl time to grow up enough to make smart choices ;-)
I debug(ged) with print statements just because Debuggers-with-a-capital-d seemed complicated and like they were more trouble than they were worth for smallish programs. But now that I've had the extremely painless introduction I'm going to try to change my ways...
I never bothered to learn how to use the debugger because it was never as easy as breakpoints in Matlab. Print statements FTW!
Unrelated to the post, is there any free documentation for web2py that's not protected behind scribd?
Right here. And I use the debugger _all of the time_ when doing c or c++. It's almost as if a debugger in Python is redundant. The language allows me to express myself with such fidelity that bugs become rare. Whereas in c/c++ I'm forced to pound the square peg of what I want to do into the round hole of what the language lets me do (nothing against round holes however, they can be great fun). Another advantage in Python is that coding up a testing framework or tools that dump program state in a readable format is nearly trivial. Usually when I have a problem with Python it's questions like "why is this over here?" I don't think I've ever spent more than five minutes tracking down a bug in Python, whereas in c/c++, baby, we're talking about some seriously demoralizing marathon debugging sessions.
pdb is a toy compared to gdb, because of the ridiculous power you have when you're in gdb; you can inspect (and modify?) all registers at any time, inspect and modify any memory location at any time, run pretty much arbitrary scripts every time the debugger stops execution (helpful a more advanced version of gdb's 'display' command), the list goes on and on and on (not to mention gdb can be used as the backend to any number of debuggers that wrap it like ddd). as for using gdb to debug cpython, gdb -p $(pidof python2.4) # or whatever the stack frames you'll see will be a mix of calls to (cpython interpreter functions) PyEval_EvalFrameEx, PyEval_EvalCodeEx, PyObject_Call. you can refer to a copy of the CPython source to see what everything is and how to poke around it. (the gdb stuff is taken shamelessly from an email from [bbiggs](http://www.billybiggs.com/) ; pdb hasn't gotten annoying enough to make me get off my ass and take a poor stab at making this process easy)
So your argument is that because it's not a C debugger it's a "toy"? Do you routinely have bugs in your Python code which are more easily revealed by watching what's going on in the guts of the C runtime?
Checkout my [development branch](http://code.google.com/p/cmdln/source/browse/#svn/branches/cfg) of the `cmdln` project (soon to me merged and released if Trent, the original developer, finds it satisfying). Here's an example: $ cat example.py @cmdln.option('-E', '--virtualenv', default=None, help='Work on the specified virtualenv directory') @cmdln.option('-n', '--no-autosync', action="store_true", help='Do not auto sync repositories') @cmdln.option('-y', '--assume-yes', action="store_true", help=('Automatic yes to prompts; assume "yes" as answer ' 'to all prompts and run non-interactively.')) @cmdln.useconfig(filenames = ["etc/example.conf"]) class Commands(cmdln.Cmdln): name = "example" @cmdln.option('-f', '--force', action="store_true", help='force downloading of the index') def do_sync(self, subcmd, opts): """${cmd_name}: Synchronize things ${cmd_usage} ${cmd_option_list} """ print self.options.virtualenv [...] $ cat etc/example.conf [cmdln] virtualenv = /home/user/virtualenvs/exampleenv assume-yes = True $ ./example.py --no-autosync sync -f /home/user/virtualenvs/exampleenv $ Issues: http://code.google.com/p/cmdln/issues/detail?id=2 http://code.google.com/p/cmdln/issues/detail?id=14
You should start [here](http://www.web2py.com/examples/default/examples) than look at the links at the bottom [here](http://www.web2py.com/examples/default/docs). This is the [epydocs](http://www.web2py.com/examples/static/epydoc/index.html)
Do you use lamson or smtplib for phoning home with people's account info? :)
I use smtplib, but that script hasn't been committed yet.
The built-in debugger that comes with Pydev (the Eclipse plug-in) is actually pretty nice. I've used it to track down problems in 20k+ line projects where print statements wouldn't have gotten me anywhere. I only wish it could jump between threads.
 Python 2.6: 310 sec Python 2.6 with import psyco; psyco.full(); : 126 sec
I tried Werkzeug/SQLAlchemy/Genshi first and found Django a blessing. The ORM is simple to use, the template system is decent (though the former Java guy in me misses Genshi's XML syntax) and all in all it's just beautiful. So far I've partially rewritten django-multilingual-model, modified django-typogrify to be more locale aware and written my own manage.py commands to deal with SHPAML and CleverCSS preprocessing/compilation. I agree that tracking down a template error can be a pain if you have no idea what could be wrong and there are a lot of small things you have to trade in for all the magic, but I rather like it. I guess it mostly depends on what you expect the framework to do. Then again, I've mostly coded in PHP for the past decade, so my pain threshold may be higher.
Has anyone who upvoted this looked at his Python code? Ugly, ugly, ugly. The guy apparently doesn’t know about for loops? Gross.
For the most part I agree but I've found myself using pdb when debugging django apps. There is a lot of setup to get to the point where most of your errors occur and at certain points the traceback isn't helpful (ie a template error usually isn't with the template). In these cases, a set_trace() where you think the problem is opens up pdb which is basically the interpreter but all your setup code already run. But I think your right in that it's not needed as often.
&gt;the stack frames you'll see will be a mix of calls to (cpython interpreter functions) Generally, when debugging python it's more useful to have the *python* stack frames, which require some digging and variable inspection to determine from the C stack, thus gdb is rather suboptimal. (Though if you do need to debug a mix of C and python, it's worth noting that there's a set of useful gdb macros that will crawl the stack and give you the python frames etc) However when people speak of debugging python, they generally mean debugging programs written in pure python, not "debug the cpython interpreter". Gdb seems way too awkward to use for *that*. Unless you're debugging C code, pdb's far more appropriate)
Is there anyway you can use the same database you are exporting from, if it's something supported by Django (mysql, postgresql, etc) you can use the awsome [inspectdb](http://docs.djangoproject.com/en/dev/ref/django-admin/#inspectdb) to generate the model file for you.
another important [bookmark](http://www.web2py.com/) in my list of apps in python..thank you reddit!
&gt; the stack frames you'll see will be a mix of calls to (cpython interpreter functions) PyEvalEvalFrameEx, PyEvalEvalCodeEx, PyObject_Call. you can refer to a copy of the CPython source to see what everything is and how to poke around it. Which is useful how, unless you expect to track bugs into the python interpreter itself?
Well: If you put a big try / except around your whole program, you can import traceback. Then you can get the traceback as a string using traceback.format_exc(), and write it to a log. That will at least capture tracebacks. If you want to be able to do postmortem debugging, you can save the traceback for later, import pdb, and do pdb.post_mortem() on the traceback. Unfortunately, "saving it for later" means keeping the python process running, because I don't think there's a way to serialize tracebacks for later usage.
Even if you haven't used django, you might find [the ExceptionReporter class in debug.py](http://code.djangoproject.com/browser/django/trunk/django/views/debug.py) informative.
 import logging try: logging.basicConfig(filename="exception-stacktraces", filemode="a") main() except: logging.exception("caught fatal exception.") raise It's not a corefile, but it is a stacktrace. Maybe in place of "raise", you could import PDB and set a trace right there, until you come back and see what's wrong. There's no way to do this asynchronously, though.
you might want to use exception hooks instead of this terrible thing.
How about using a prebuilt one like [Koha](http://koha.org/)?
This is preferable to effecting the global interpreter state. Edit: in fact, what is terrible about this at all?
Can you give us an example? Keep in mind I only want exceptions that percolate up to the top. I don't want in the log SystemExit or StopIteration, or any of the normal NameError, ImportError, KeyError, etc etc that I use in EAFP programming. ---- Edit: I initially presumed this meant overriding Exception. I've never used *sys.excepthook* before. As I understand it, this would be import sys def receive_uncaught_exception(type, value, traceback): # write to file sys.excepthook = receive_uncaught_exception Do I have to call *sys.\_\_excepthook\_\_* at the end of the receiving function?
Nice! I didn't know about `logging.exception()`. I had been manually doing the same using `traceback.format_exc()`.
I think he's referring to debugging C extensions to python. pdb (at least from my very, very limited experience) doesn't help much when trying to debug a python module written in C.
You left out one of the best ad-hock troubleshooting ways to figure out what caused an exception post mortem. Run: python -i your_script.py After the exception you will be left at the python interpreter prompt staring at your exception. Type: import pdb pdb.pm() And you will be put into the context of the stack of the last exception. This means you can print/examine the local variables at the point of failure after the failure has occurred without having to change a line of code or import pdb in advance. Another method that requires a little bit of preparation is to put the import pdb and pdb.set_trace() in a signal handler trap. That way you can do a kill -SIGNAL PID or if the signal you trap is INT you can just Ctrl-C to get dropped into the debugger at any point. The signal handler technique is good for testing release candidates and released code because the signal handler doesn't create any runtime overhead. Signal handler example. Debugger starts with Ctrl-C: import signal def int_handler(signal, frame): import pdb pdb.set_trace(frame) signal.signal(signal.SIGINT, int_handler) Put that at the top of your script and you can start debugging your script at any point by typing Ctrl-C while the script is running. Resume program execution by typing exit at the Pdb prompt. 
Yeah, the logging module is fantastic. Once you start using it you'll never stop.
Honestly, any app of this size is going to be a little more complicated than vanilla Apache and WSGI. The use of this type of scaling configuration really implies that you are using separate app servers and thus shouldn't really be having a web server on each anyway. I think the planning here is a little too pigeon-holed as I cannot imagine anybody using the exact same stack at this level. There is also no way of using things that must be compiled which is another big issue. My application alone uses three things that are not in repos and as such, are a road block. I currently run a site that uses a similar style of scaling. I use a python script that provisions and contracts provisioned servers on a VPS provider. Both Linode and Slicehost's APIs offer a single feature that makes this project seem like a waste; you can clone a working server. As in build a clean "App Server" and just duplicate it when you need to scale and pull it back when the requests/sec drop. Linode also has a minimal linux partition thing that I've seen people use to DOWNLOAD VPS images and build/run them locally and upload again. Disclosure: I have servers with both Linode and Slicehost running the same app for redundancy. I cannot prefer one over the other as the services are seriously like exactly the same. Pick the one with the website style you prefer, it's that close.
I thought the "Lisp is too slow" argument was dead years ago. A tiny bit of research will reveal that there are some very fast Lisps available now. The fact is that most developers don't care. Lisp is too fragmented and the serious Lisp users are seen as smart but crotchety jackasses who argue endlessly over minutia instead of developing useful libraries and frameworks. It's a bummer, but I think that's how most developers see lisp. 
Really nice. good work Ian!
I started out with cloning servers, and while it's still fine I don't use it myself, mostly because developing toppcloud itself is easier that way. Also the setup scripts have to be working and up to date, because if you want to update the configuration you do that by rerunning the setup scripts; if you haven't exercised them well then you have servers that can't be upgraded. Unless you just want to throw servers away... uck. (I also initially tried just setting a server up manually and then cloning it, but quickly realized that was too ad hoc.) It's possible to create new debs, and Ubuntu even has a specific setup to help keep those compiled and available for installation (PPAs). It's more work than just `configure; make; make install`, but I don't think those kinds of libraries should be used lightly. If at all possible it's best to stick with mainstream released libraries. And if it's not possible then creating a deb isn't a lot of overhead. If you think it's a lot of overhead, you aren't feeling the appropriate amount of pushback about using volatile C libraries. It does make sense at some point to allow a group of servers, where some servers in the group serve different purposes (e.g., database server and app server). That's not implemented, but from an application level that shouldn't be a visible change (it does require changes to toppcloud itself).
I've got a minimal implementation of this in my batch-flow utilities on google code called lnk. Yeah it would be nice if CD could understand these. Also I need to remember how to tell windows how to add it to the default extensions in cmd.
That signal trap handler trick is pretty cool... Did not know that one.
&gt; toppcloud is inspired by Google App Engine in this respect; an application is a bunch of files that are uploaded to the server. You don’t “build” an application on the server. Why is this called "cloud computing" instead of, well, webhosting? With "cloud computing" I'd associate the virtualization of hardware but full access to resources including the ability to create a file which lives in the cloud ( or build an application ) and doesn't have to be uploaded. 
And thus not the target for pdb. Complaining about a tool (pdb) because it doesn't meet your needs (debugging c python modules) because it is meant for something else (debugging python code) is hardly helpful.
Mostly I didn't know what I was building when I started, so my original conception was more cloudish.
True!
What if it was all raspberry? I think I could get sick of that.
In 2008 PyCon actually overlapped with Pi Day (March 14th), they served pie (including some of the best chocolate pudding pie I've ever had). PyCon is the best conference.
Pies have conventions? What do they talk about?
How my memory is being used by python? Is this a case of snakes on a brain?
did you manage to get numpy in u-s? I get some linkage problems in run-time :(
u-s already saved me twice with its performance. Especially with large data in multiprocessing where u-s pickleing overperforms CPy 2.6.4. CPy swaped badly on my 5GB RAM box. Whereas u-s used only 75% RAM and got through. My case is shared via u-s maillist
[Dive into Python](http://diveintopython.org/) is very good if you have some programming experience and want to start using python.
It's not bad, I had an earlier edition. I didn't really use it much though. I would suggest looking at Dive Into Python and A Byte of Python, both available for free online from the author's sites. If they look good, and you just need a book, you can buy dead-tree versions. 
Yes i know, but like i said, it's really bad for my eyes if i stare at my computer screen for hours. I'd rather have a book.
You are talking about the rat book I assume? In my opinion it is the best one if you have some background in programming. I never recommend it to complete beginners since it touches deeper subjects here and there which is just confusing to someone trying to get a hold on programming rather than a language.
What I mean is, in these cases you can find out if the book is good for you before you spend the $ for the paper copy =)
One of the applications bundles with django - admin interface ships now with jQuery. Yet, it's being used only for admin interface, your website may use whatever you like. If you want to use the jQuery shipped with django, you may access it through the url defined in ADMIN_MEDIA settings
You've forgotten the "Host" header. 
try changing: request = urllib2.Request(self.external_url,'', headers) to: request = urllib2.Request(self.external_url,headers=headers) 
This seems to work without problem: http://bpaste.net/show/2917/ This is your code made standalone, by the way (I would recommend that you paste an easily testable snippet in the future).
in ipython .. In [1]: import urllib2 In [2]: urllib2.urlopen("http://reddit.com").readlines() Out[2]: ['&amp;lt;!DOCTYPE html PUBLIC ... 
Yup I have been using ZODB in PyQt applications for years.
Is it true that at most of these providers the smallest unit of billing is an hour? For example, if I used toppcloud as part of a continuous testing process would every checkin result in a compute-hour charge? At Amazon they say: "Each partial instance-hour consumed will be billed as a full hour." It's not necessarily going to break the bank, but its something to keep in mind.
Real men use tcpdump ;).
Thanks a lot, it works!!! Appreciate your help and the tip. I will post testable snippets in the future.
I was just trying to read the page, but for some reason if I point to www.reddit.com, it doesn't work, pointing to reddit.com works.
You shouldn't fake your user agent unless it's really necessary.
&gt; I was just trying to read the page Why? Are you aware of our JSON API? That might make whatever you're trying to do easier &gt; 'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.0; en-GB; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12 If you lie about your user-agent, we will ban your bot and, if we have to, your IP. Also, don't hit us more than once per second. We don't mind having scripts hit us as long as they follow the rules :) Anyway, the real problem that you're having is that you didn't send the `Host` header
Agreed. Speaking as a reddit admin if we catch you faking your user-agent, we'll ban your bot and, if we have to, your IP address.
Why not just use the twill project (a wrapper around mechanize)? In [1]: import twill.commands as t In [2]: t.go('http://reddit.com') ==&gt; at http://www.reddit.com/ Out[2]: 'http://www.reddit.com/' In [3]: t.show() (Returns HTML) 
Or tethereal.
Impressive. What's the state of cython in popular linux distros and what's the compatibility nature of distributing cython code? How well does it integrate with my python code (can they coexist in the same file, e.g.?)
I just apt-got it, and it works fine. There's a mode where you can annotate your code using declarators and it runs both as Python and Cython, but sometimes you need to refactor your code a bit to get the most speedup. There's also a module that allows you to just import a pyx file and it will compile it in the background, falling back to pure Python code if that fails. It's pretty fantastic.
I am going to remember this if I ever need to speedup a python program
Just to nitpick some of your python code, you should consider doing something like this instead: def fitness(self): cost = sum(GRAPH[self.genome[i-1],self.genome[i]] for i in xrange(1, GENOME_LENGTH)) cost += GRAPH[self.genome[GENOME_LENGTH-1], self.genome[0]] return cost Seems like you have a lot of nested loops that could probably be eliminated if you thought about it hard enough too.
Seeing as you will have to stare at the screen for long periods of time to code, what might be the cause of the eye strain? Assuming you don't know the specific cause of the eye strain, might I ask a few questions? If you have a CRT monitor... * What is your refresh rate? If you have an LCD monitor... * Are you 100% certain that you are using DVI-D (and not analog)? Analog is blurry and strains the eyes. * Are you using the native (maximum) resolution supported by the monitor? Do not use any of the the other resolutions (they blur somewhat). * Did you try lowering the brightness of your monitor backlighting? Too bright == eyestrain. * Did you try ClearType? * Did you try disabling ClearType and just using regular font smoothing? (It depends on the person, which is easier to read.) * Did you try zooming the python webpage docs to see the text much larger? * Is your backlight flickering? Maybe flickering on a 60Hz cycle like old CRT monitors -- a speed which will cause eyestrain.
Not in the same file, necessarily, but it's easy enough to pull out just the methods of a class (or standalone functions) that need a speed boost and replace the interpreted Python version with the compiled version by binding them at runtime, if the compiled version is available. This means that people without a compiler can still run your code in 'failsafe' mode. (I should note that you don't actually need to make Cython a dependency if you want to distribute a package that uses Cython -- Cython spits out a .c which you can compile with distutils as a normal C extension, so you can just ship the C file.)
Thank you for the post. In the process of doing this, what (online) resources helped you make it? I have code that takes about a day to run now...
Can you use Cython for webapps? Surely this would be awesome?
Just the cython docs, they are pretty complete and the process itself is not very complicated. Remember to run cython -a so you get an HTML file with what needs to be optimized more.
Yeah, you can use it for anything...
Sure, list comprehensions are fine, I just liked the readability of a loop. I might change it, although I think that would slow down the c program.
There's also a way to put the typedefs in a separate file, but I couldn't get that to work, and I needed to change some code anyway...
I personally find this more readable: def fitness(self): return sum( GRAPH[self.genome[i-1],self.genome[i]] for i in xrange(1, GENOME_LENGTH) ) + GRAPH[self.genome[GENOME_LENGTH-1], self.genome[0]] But would be even more readable if GRAPH[self.genome[i], self.genome[j]] was a method so you could use: self.lookup(i, j) instead: ... + self.lookup(GENOME_LENGTH-1, 0) 
Yes.Seriously.Code completion or word completion? I am in windows. 
I wonder want the performance increase would be if psycho was used. Seems like Cython takes out the good bits of Python. If performance as an issue, why not write it in C in the first place?
But what would be the limiting factors? Does it introduce any security issues? Would you be limited by whatever HTTP request handler you are using?
LCD, it's a laptop &gt; •Are you 100% certain that you are using DVI-D (and not analog)? Analog is blurry and strains the eyes. Yes &gt; •Are you using the native (maximum) resolution supported by the monitor? Do not use any of the the other resolutions (they blur somewhat). I have always tought lower resolution is better because everything is shown bigger. The maximum is 1440 x 900. Would you recommend this? &gt; •Did you try lowering the brightness of your monitor backlighting? Too bright == eyestrain. I've used Flux to lower the screen brightness and now i've manually lowered it a lot more. I'll see if that helps. &gt; •Did you try ClearType? I'll look into that. &gt; •Did you try zooming the python webpage docs to see the text much larger? Yes, i usually do that. &gt; •Is your backlight flickering? Maybe flickering on a 60Hz cycle like old CRT monitors -- a speed which will cause eyestrain. My screen refresh rate is set to 60Hz. I'm not sure if that's what you're talking about, but i tought 60Hz is the best option for an LCD screen. I've even bought a wireless keyboard so there's more space between me and my laptop. I've also set a dark background color for my text editor.
great code, my only question was that really needed for the whole code?, What I am getting at is obviously a pure c implementation will always be faster but, did you try optimizing your python code, look at execution times of chunks of your code, your program may have been slow due to a one or two functions.
i am skeptical about 48 and 100 cities taking the same time (5 secs) to run.
I wonder if there's a way to get line by line profiling like with this module: http://packages.python.org/line_profiler/
Cython is pretty amazing. Makes it rather trivial to knock out quick prototypes in Python, run a line profiler, and then selectively optimize where needed.
That's 60% slower, and if I add the function lookup, it's going to be even slower...
I would love that, but I don't think so (not with Python lines, anyway)...
What? If you have a CPU-bound function, you can add types and make it faster. Any layer below Python is irrelevant.
Turbogears 2. :) (It's worth a look, at least. Pylons, Genshi, SQLAlchemy, etc)
Cython especially shines if you need to interface Python with C. Pyrex (later Cython) made lxml possible. It doesn't hurt that the integration code itself runs with C-like speeds and is still reasonably Pythonic.
Basically, because writing a C python extension is much easier with Cython (that's its intended purpose), and writing the whole program in C is often not what you really want. Writing the logic of the code in python and a small bit of cpu-bound stuff in native code is often a very effective balance. At least for scientific computing, Cython is a nice way to quickly write a C extension in cases where numpy alone doesn't quite cut it. (In fact, numpy itself is considering moving everything over to Cython instead of C to make the transition to 3K easier.) As far as psycho goes, I don't believe it supports numpy quite as well as Cython does, and pyscho is 32 bit only. I've never tried it for both reasons, so I can't comment. At any rate, a lack of 64-bit support means that it's useless for many, if not most developers.
Yeah, I didn't have much luck with it, though. The examples are still a bit light.
`easy_install cython` works on windows as well as Linux. If you already have a `setup.py` you can easily add cython modules to it.
yeah Cython seems pretty useful, so far I've been using scipy's weave.inline.blitz to write specific functions in C while using python elsewhere and I'm getting comparable speed increases at times too. 
&gt; If you have a CPU-bound function I think this might be what haywire's trying to get at -- many webapps don't.
I didn't say they did, though, he asked if he could use Cython for web apps and there's no reason why not, if you need it.
Perlmongers? Strawmen?
&gt; I have always tought lower resolution is better because everything is shown bigger. The maximum is 1440 x 900. Would you recommend this? If you are sitting far back then this probably doesn't matter, however, if you are sitting close it could help to set it to the maximum (1440 x 900). LCD monitors blur somewhat when you pick anything other than the max resolution; of course, if you don't sit close this may not matter so much. &gt; My screen refresh rate is set to 60Hz. I'm not sure if that's what you're talking about, but i tought 60Hz is the best option for an LCD screen. 60 Hz is fine for the refresh rate. What I am referring to is the lighting that makes the LCD work. LCD monitors = a light source shining through a color filter/screen. It could be possible for the light itself to cause eye strain -- though not very common; some lighting has refresh rates like CRT monitors did, which can be bad. Here's a question, do you always use the computer in a room with fluorescent lighting (which may also have a flicker around 60Hz)? &gt; I've even bought a wireless keyboard so there's more space between me and my laptop. For all I know, sitting further could be worse... as you have to strain your eyes more to see things. You could always try for a while up close and a while far away and see if it makes a difference. There are several other things worth mentioning. * You said you are using a laptop, which probably means a glossy screen, am I right? Glossy screens are notoriously reflective; they have a mirror-like finish that reflects quite a bit of light. This glare can be quite distracting and can cause eye-strain, headaches, etc.... Turn off your monitor, or put a solid black picture up. Can you see any reflection of light or objects? If so, try to position yourself to not see any reflections, if possible. * How's the lighting in the room? Is the monitor really bright and everything else dark? (this may not matter, but just thought I'd mention it). * On a personal note, I too have problems looking at laptop monitor for a long period from far away. I have no idea why this is. I can sit for many hours in front of a desktop LCD, but can't watch a laptop LCD for 30 minutes from a distance. I cannot say why this is, except maybe it's just my eyes, or it's the reflective screen on laptop monitors, or cheap backlights, the brightness or what.
Yep, it's trivial, here's an example from something I was working on a while ago: http://alexgaynor.net/2009/jan/19/optimizing-a-view/ . This helper function just lived in a .pyx file and I used the pyximport (or whatever it's called) module to have to be compiled automatically. 
well, in many cases that would be premature optimization
Hmm, you didn't include any of the Cython code, though. Can I see it? This is quite interesting.
btw, do you prefer: def fitness(self): fitness = x fitness += y return fitness over: def fitness(self): return x + y ?
it really depends on how complex the expressions denoted by x and y are. in the case of simple expressions like x and y the latter is better in your previous code sample, the former is better edit: the simplest rule of thumb to follow, is to always avoid burdening the next programmer with parenthesis matching games when reading your code
Can you actually define an arbitrary URL mapping? All the examples seem to use the class/method hierarchy for URLs.
That is the Cython code :) In this case I got the performance I needed without adding any explicit type decelerations.
Hmm, how much faster does this make it? It still calls Python for everything, so it shouldn't really be any faster...
In this case the interpreter overhead was really all I needed to eliminate (20% or thereabouts).
&gt; Shouldn't you be more concerned about what's coming out of his computer Yes. Which is why we don't want you faking your user-agent. When something is going wrong load-wise on the site, 99% of the time it's a poorly written bot. If we can ignore the traffic coming from browsers it generally highlights what's going on. Based on the user-agent we can usually ban the robot, contact the author, and help them get it operational again without hurting us. It's happened a million times. &gt; A programmer telling another programmer what are the "allowed" ways of having his computer behave, that's just wrong. Oh please, I don't care what's going on in your computer, I care about how you're using your computer to hurt us. The user-agent is a great tool for us to find that. And the majority of the time it's badly written robots that are faking their user-agent, so we just ban them out-right when we find them.
Yeah, that's how I primarily use it. At work, we've employed Cython/Pyrex after profiling and had some awesome gains. There are enough hassles to writing "pure Cython" that we don't care to employ it extensively, but it is absolutely wonderful for those critical sections.
&gt; using a common bot user-agent, wget for example, will get you instant banned on many sites reddit isn't one of them, as long as you're not *also* hurting us
Actually, http://docs.cython.org/src/tutorial/profiling_tutorial.html
If the expression is complicated enough, it is still better to have: x = &lt;complicated expression&gt; y = &lt;complicated expression&gt; return x + y Than: x = &lt;complicated expression&gt; x += &lt;complicated expression&gt; return x 
The templating step. Still, the database I/O dwarfs all that.
agree. I was only indicating that I like it split up on multiple lines previously instead of one giant compound expression. 
Am I the only one here who's thinking "Why not just write the core stuff in C and implement a Python interface to it"? Might take longer to learn the nooks and crannies of the C API for Python, but it's definitely worth it in terms of performance. Inspiration [here](http://docs.python.org/extending/newtypes.html).
from what I see there and what I know of the built in profiling abilities of python, it cannot do line by line profiling. Only function level.
What do you mean by line-by-line profiling? Sort of a coverage diagram with line execution counts? I'm sure that's possible with pure Python, but I don't think anyone's done it yet.
Not sure. Sorry if that was a requirement i missed :)
Hello ketralnis, thanks for the answer! I'm not building a bot, really. I'm just playing around with urllib2. I was curious at why the snippet i posted worked with other sites but not with reddit. You're right, most of the big sites have API's, if I were building some app or website that needed info from reddit I would surly use the API. 
My code includes the fake user-agent because many sites I pointed at will not even serve a webpage if they don't like the user-agent. To be honest, I'm a python newbie, and was writing a simple practice script to figure out how other sites aggregate information i.e. pricegrabber.com, etc. I'm sure they must use a library like [this.!](http://pypi.python.org/pypi/spider.py/0.5). 
You could try asking the concurrency-sig mailing list: http://mail.python.org/mailman/listinfo/concurrency-sig.
The first thingsthat come to mind are readability and maintainability. You can write and maintain all your code In one language and optimize where the profiler tells you it's necessary. 
I prefer this form when there are sensible names to use, and a "giant" expression (which is decomposed into small lines using indentation to clarify the structure) when there are no sensible/useful names to give to these intermediate structures.
I didn't get how statement cdef numpy.ndarray[numpy.int64_t, ndim=1] mother_genome = mother.get_genome() works. How is it possible to use complicated numpy constructs as types in cython code? Btw, it gives syntax error on my system.
in fact, developing the interface have not been necessary for me, using the ctypes library to interface my C shared library. There is thus practically no overhead. This works when you don't want to interface with python containers, which, in a inner loop, you generally don't want to.
I see the form you chose a lot in javascript frameworks like jquery. I think it's pretty nasty 
[From the FAQ](http://wiki.python.org/jython/JythonFaq/GeneralInfo#WhatisJPython.3F): &gt;JPython is an implementation of the Python programming language which is designed to run on the Java(tm) Platform. It consists of a compiler to compile Python source code down to Java bytecodes which can run directly on a JVM, a set of support libraries which are used by the compiled Java bytecodes, and extra support to make it trivial to use Java packages from within JPython. JPython has been renamed and superseded by Jython.
is this actually important or is this just advertising?
maybe I'm old fashioned, but can't you just use your interpreter for that?
Why? The iteration for list comprehensions is done behind the C curtain, making it faster than loops written manually in Python.
Django + heavy use of fixtures
I think it is a minor aesthetic issue, and the important concerns are avoidance of code/conceptual duplication, and the ability to reason about and compose code meaningfully.
What if the purpose of the bot is to test whether different user-agents get served different content?
A leg-up for us noobs. And by us, I mean myself.
What are the main differences between Pylons and Werkzeug? 
That would be true, if the loops were in Python. These loops are in C, so the list comprehension is slower.
I am too, but that's what the data shows. I'm not sure why that is...
By orders of magnitude, I dare say.
Thanks for the article, I really do like the simplicity. If only my webhost had python installed...
It was, that's why I only optimized two of them. You just leave the rest as-is.
Maybe you're using an old version of Cython. That's a type because Cython can access numpy arrays directly instead of going through Python, so this line informs Cython that it should do that.
As you said, it will take longer to read, and I doubt the performance improvement will be that much more (there's a whole class of people writing this exercise and mine is the fastest I know of). I don't think it's worth it over adding ten lines of types in two functions.
Don't worry. There are plenty of webhosts with Python support out there.
How does Genshi compare to pygments?
In that case (and basically only in that case) you could have a user-agent that looks like `Mozilla/5.0 (Windows; U; Windows NT 5.0; en-GB; rv:1.8.1.12;` **MyUser-Agent-Tester:http://... **`)` But in that case, you could just ask us, or look at our code.
Sounds like someone is ready for their own VPS.
Not for a personal website, it's not necessary (unless you have a recommendation...). I only pay a few bucks a year for a ton of storage and space. (currently using budgethost.ca, they're nice people, I've chatted with the president)
Of course, it's just a pain to switch.
Genshi is a templating engine. Pygments is a syntax highlighting library.
If writing xml is so fun for you why don't you try any of the Java (tm) frameworks?
Well if you ever need to upgrade you could consider burst.net as they start at $6/month but most other places would be just as good, just read reviews first.
There's always Google App Engine...
And pygments can apply highlighting to genshi templates (at least in trac, not sure exactly how its done)
&gt;I doubt the performance improvement will be that much more I respectfully disagree. When programming for performance, the first rule of thumb is to let the computational pattern decide the data structure. You can do *a lot* better with C types than Python types when representing trees, linked lists etc. This is mostly related to cache issues since you want to keep your actual data access as contiguous as possible. Granted, it's cumbersome to code, thus clearly not a one-man job nor subject for an exercise.
The same way it's done for the other [80+ languages it supports](http://pygments.org/languages/)
These **are** C types, though.
pylons is a framework, you install it, you create a project its ready for work, you get some good sane defaults set to get you started. werkzeug is set of tools to help you easly integrate what you need - to get something equivalent of pylons. They are both nice tools.
Maybe bcain was thinking of [GeSHi](http://qbnz.com/highlighter/)?
Thanks!
I'm not an expert in Cython by far, but aside from integers and NumPy arrays, I'd expect higher-level objects to be compiled and referenced as PyObject\*. I guess graph/network codes don't need much more than that, but I'm preaching about the general case ;)
Oh, yes, lists and the like are Python objects. I still think that you can get the most bang for your buck by writing Python with NumPy arrays and just optimising the most used functions. I don't understand your argument that "the first rule of thumb is to let the computational pattern decide the data structure". Data structures in Python are much better suited for scientific programming than data structures in C (unless you want to write a lot of boilerplate to get your implementation going)...
BioGeek - I know about you ..
Hi, example please ? &gt;&gt;&gt; import re &gt;&gt;&gt; test = re.compile(r'(.*)') &gt;&gt;&gt; test.search("hello world") Like this *ludacris* example ?
Because Java is ugly.
god when will XML/XSLT die .......... just die. Personally I like Camping/Sinatra with any of: markaby, remarkably, erector, parkaby to represent HTML/DOM layout. At least if you stick it all under 1 language it's not such a brainfuck.
You know about Jinja2, right? 
I was.
That's nothing to worry about, since the amount of time you spend reading/writing Java code will be negligible compared to the amount of time spent reading/writing XML. 
First of all, my favorite link: [zap colors](https://www.squarefree.com/bookmarklets/zap.html). Now it's readable. Now for the rest of it... **Encapsulation**: well, I'd like to see an example where the lack of encapsulation have bitten you. **Batteries**: dude, that's what the lack of encapsulation is for. You go to the library function that behaves not the way you want it to, in 99.9% of the cases it would be pure Python, so you can a) inject your own wrappers for some functions it uses and fail faster, or b) copy-paste and patch it in your own code. The whole point is a bit retarded: sure, standard libraries for any language oftentimes suck. News at eleven! Python at least provides you with a way to deal with this, what else could you possibly want? Better standard library? So want we all, but what it has to do with the language? **This**: I feel your pain. Python has some design choices that seemed nice at the time, like using implicit variable definition on assignment, but which made it impossible to have nice things later on. You really, really don't want your program to be royally fucked up when you introduce a global variable (even in your '`if __name__ == '__main__':`' scope, because Python's scopes are retarded, because of that implicit variable definition) having the same name as one of your class variables. On the bright side of things, you kinda grow accustomed to `this` and it doesn't bother you as much, especially if you have a good IDE. **Lambda**: not one line. Use the parentheses, Luke! But no statements, yes, and that's a shame.
I'm so glad Jython came back from the brink of extinction to be useful again. This way Java's excellent cross platform/dbms connectivity can be used without leaving your favourite language! Those of use forced to work on platforms with appalling package management like Windows and Solaris can save an awful lot of time.
Free online version http://jythonpodcast.hostjava.net/jythonbook/en/.98/
&gt;I don't understand your argument that "the first rule of thumb is to let the computational pattern decide the data structure". Hmm it's a bit tricky to come up with a meaningful example without pasting 10k lines of code, so I'll use a textbook example. Let's say you're doing 3D particle simulation, and you use NumPy arrays for x\[i\], y\[i\], z\[i\], distances\[i,j\] etc. Depending on problem size, and how you access this data (e.g. always x, y and z together for calculating distances), it can be a factor of two faster to do: typedef struct particle { double x, y, z; double *distances; //etc. } particle_t; Of course, for a different problem size or calculation, the vectorized version might be faster than the data structure version. The point is that only C allows you to code both, and you should of course pick the one that's suitable for your problem. edit: format and... **Clearification:** This is based on empirical trials comparing optimized C implementations of one data structure to another. I have some slides I can PM you - if you want.
That's the most sensational title I've ever seen.
I was also.
That's the most sensational comment I've ever seen.
This comment is absolutely the most fun I've ever had reading since I first read Dr Seuss over two decades ago. It is so elegant and humorous, it brought me to orgasm... in my eyes. 
or, if you're in unix, *sed*?
Ah, I see, that makes sense indeed. I thought you meant the opposite, i.e. to represent the data in the most humanly natural way, which is great for readability but not so for optimization. The slides should be great, although I don't do much in C. I would like to read them, however, because you can get amazing speedups with Cython and I might use it in the future, thanks!
Hmm. I wrote my first line of code about a week ago, and I am having no fun at all with it. That might be because I'm trying to write an IRC client using Twisted, though.
Sure, but not shared hosts, right? I can only think of one which pimps python.
I certainly get bitten by encapsulation, mostly when using an API that isn't clearly documented or public, and it turns out its an implementation detail, and then on a library update it gets broken. Or, for myself, the trouble is more that I break other people's code when they use my library in a way I didn't realize. A small example was Pylons imported `webob.MultiDict` when the "real" public location was `webob.multidict.MultiDict` and it just happened that `webob.__init__` imported that class and so it was implicitly importable from that other location. I don't know if on the whole it is worth it to set up more encapsulation, but it does solve real problems. Were other people confused by the `this` discussion? Like... if you used Python and have a trouble with this, wouldn't you also know it is universally called `self`? It's not my favorite feature, though Javascript's `this` kind of shows the advantages (though of course there's a middle ground between Python's explicit `self` and Javascripts overly implicit `this`). Lambda just isn't a big deal; use a named function. It's just syntax. The only real nuisance was the inability to write to variables outside the function but inside the closure, and that's acknowledged and fixed in Python 3.
yeah, you might want to try that after you've done some other coding
Yay! No more from __future__ import with_statement or import simplejson as json for my Debian Servers!
An hour of Rackspace's smallest machine is $0.015, so it definitely won't break the bank ;) EC2 is like $0.03, but it still doesn't add up that fast.
Pylons+Jinja2
Webfaction is about $10/month. I haven't used it enough to give it a review personally, but it is consistently rated very highly.
Yup; that's the one I was thinking of.
joyent may not pimp python, but I've been running a cherrypy server there for years.
Yes! And I will throw this out there too - genshi + tornado is _delightful_
It's your eyes--they are burning from trying to interpret camelCase. (AlthoughISomeTimesWonderIfIAmMissingSomeThingGreatAboutTheTwistedStyleCamelCase)
Yep. 2.6 entered `sid` about three weeks ago, so the wiki is slightly out of date. [Debian Package Tracking System: python2.6](http://packages.qa.debian.org/p/python2.6.html)
The most prominent alternative is really fun as well, [isn't it](http://play.pixelblaster.ro/blog/archive/2009/11/18/if-django-templates-are-an-improvement-over-xml-templates-by-all-means-please-give-me-xml)? 
First you had a problem, then you added XML. Now you have two problems.
&gt; god when will XML/XSLT die Especially XSLT, that thing is hell on earth.
So... Python 2.7 will hit Debian in 2012? edit: Not hating, 1 of my 3 private boxes is running testing/unstable.
You need to try web2py, it makes everything you just did possible in maybe 1/3 the effort.
tl;dr: This is not the language you are looking for, TFA author; move along; Step by step: * Encapsulation: Design, love it or leave it. * Batteries: Incomplete? Broken? O RLY? from lib import LackingClass; class LackingClass(LackingClass): ... * This: Well if `self` is authors biggest problem then damn, he must be a great programmer. Oh, no, wait. This is an argument about explicite vs implicite. So it's *by design* and *as advertised*, right? So he sucks, sorry. * Members: yes, it's called open classes and it's *by design* as well. * Lambdas: one line, really? I won't even mention the fact, that you can define functions and classes on the spot everywhere. Yeah, no-statement lambdas == language is broken. * Conclusion: Author doesn't know shit about the topic, but he decides to unload everything. He claims that Java was the first serious language he learned, but accidentally only code fragments shown are PHP and Python. He thinks he's got language with 20 years of history figured out after a brief romance and mentors how to make it better. Free protip: Stay with Java if it works for you. Nobody tells you to use Python. It works for other people so just get over it. And BTW putting Python on the same shelf as PERL is supposed to be an insult? Riiight. Keep on truckin'.
asmallorange.com you could even get $28/year with python + dns (that is, if you have the reddit coupon)
Finally! On the plus side, being stuck with an old version of Python on my Lenny box did teach me how to build an alternate install -- which is nicely supported.
Remi Delon works at Webfaction - he wrote CherryPy.
Yeah, but I choose Genshi every time. Why? First, obligatory link: http://hsivonen.iki.fi/producing-xml/. Nuff said. It lets me write xhtml, so I get the benefits of a stricter checking and xml tool chain/editor *in development*, but it can output perfect HTML 4.01 *in production* (http://hixie.ch/advocacy/xhtml). And probably HTML 5 nowdays. Sure, it's not that fast (but not that slow either). But that hasn't been an issue for me, and if it was I'd look to caching first regardless of template language.
What if you want to output plain text, for example an email message ?
I was wondering what took you so long. Usually it takes far less time for someone to start spitting on Django these days. You can read my comment on that page.
Genshi has a text mode :)
Jinja2 might well be very good for generating email messages, but TFA was a web development example, and if you're generating XML or HTML Genshi is simply The Right Way To Do It.
Try something else, dude. Twisted is just that: twisted. It has its uses but IMO it ain't pretty or nice.
I think there is a lot more coding involved than necessary. One example is, you have to model the entities two times: for the "database" and for the form validation. 
Technically he wrote CherryPy 0.10 and started the 2.x version but quickly Robert Brewer took over.
The MRO and super() -- the ugliest aspect of python?
&gt;I've been looking into SQLAlchemy's ORM and what I'm contemplating is having some sort of middleware script that reads in both the schemas and a mappings XML and has triggerable functions which mirror operations on the databases. This can work if nobody access directly to database. If you use a database client, the replication don't be performed. You can use a database proxy used in python like a regular database. It can be used with a sql client too. I know some that can be used for database dispatching (based on mappings or reading round robin in slaves) or replication. But I don't know nothing for a "extrange" mapping replication. If you backend support triggers, maybe you can do it in the database. PG-Slony uses triggers to database replication. You can change the mapping destination changing the trigger code (for example). Do you need consistency? If you need it, you must use some class of two phase commit. Excuse my poor english, I hope you understand the possible consistency issues :-(
inheritance is ugly. MRO and super are just symptoms.
There's nothing wrong with XML for marking up documents. XML hate for the sake of it, or because some people misuse it, is just bandwaggoning.
You might still want to import simplejson. I believe the version merged into 2.6 is both slightly dated (there have been bug fixes) and without the C extension speedups (which should hopefully be in 2.7).
Nah, no mention of orgasm.
I could see this as being handy if you're making some quick fixes to a web app in a pinch and don't have Python installed locally and you don't have shell access to the app server. (Though admittedly I think that type of scenario is more likely for PHP devs than Python. And/or those not using version control)
you mean, multiple inheritance?
I meant that your genshi output could be colored by pygments (such as when you [display python](http://trac.edgewall.org/browser/branches/0.11-stable/trac/ticket/admin.py) in trac: http://trac.edgewall.org/wiki/TracSyntaxColoring) not that the genshi template itself would be syntax highlighted, though [I am sure it can](http://trac.edgewall.org/browser/branches/0.11-stable/trac/ticket/templates/query.html)
Even simple inheritance is ugly. Inheritance covers too much : same interface/type &amp; code reuse. Code reuse is better done by components/composition. Interface specification is better done, well, by Abstract Base Classes (python style) or Type Classes (even better). There is some limited use to inheritance but this is not worth the big pile of problems it brings into one's codebase. 
Hear hear.