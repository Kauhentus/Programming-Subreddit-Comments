The real problem in the code is the use of -1 as an error value. If he isn't going to deal with the error, he should let it cascade. And if he does deal with the error, value and index errors should be dealt with separately.
No, required whitespaces are my only turn off of the language. Wow dude, chillax bro. It's just a programming language.
 &gt;&gt;&gt; required_whitespace == exception_gotcha False
go write a trivial webapp loser
This actually highlights one of the worst things about Python 2.5 syntax (which is pretty cool really, that my biggest gripe is something so minor): the comma for capturing an exception is really unintuitive, but `except Exception as foo` wasn't added until 2.6. This is actually a bigger problem in teaching than when writing myself: teaching exceptions in Python is a little tricky to begin with because it's on the the few places you have to think about types, the control flow is totally different to anything you've seen before and to make matters worse there's a comma doing something you've never seen commas do before.
&gt; Why are you hanging out on /r/Python to tell people about why you use Ruby? Question still stands. Also, you say "it's just a programming language", yet still feel the need to point out you use Ruby? Let me guess, you're a Mac owner too.
&gt; the control flow is totally different to anything you've seen before wait, what? 
A nice use case is the mercurial keyring plugin: http://pypi.python.org/pypi/mercurial_keyring
Because I know Tarek is lurking around here somewhere*, I'm going to give him a plug for his book, Expert Python. I bought the eBook and recommend it to everyone. *Hi Tarek!
* Find a practical, manageable project, and code it. Ask for help on the [python-tutor mailing list](http://mail.python.org/mailman/listinfo/tutor) if you get stuck. * Try the [Python Challenge](http://www.pythonchallenge.com/)
I usually just write a Binary Tree lots of fun 
&gt; coding: beautiful soup, SQLite, Universal Feed Parser, etc. Play around with popular packages see the [Cheeseshop](http://pypi.python.org/pypi) for more cool tools. 
What project did you find if you don't mind me asking ? 
does the book provide actual hands-on practices ? Funny thing, before your post, I actually looked up the book on Amazon and the first comment, by none other than Alex Martelli, kind of paused my urge to get it. 
&gt; Have a job, in which you have a project, with python-satisfiable requirements, and a manageable deadline. well yeah, that's just one step below an actual Python job for newbie. 
Look at the url you posted. Now look at the one stesch posted. Now you know.
You're being gratuitously silly. 
I wanted an actual python keyring :(
If you use shell scripting a lot, switch it over to python. It helps you learn the base API well. For example, I did a quick script to change a shell script so that whenever it exited with an error that it echoed the line it exited at. Don't even focus on reusability. Just the API. Reusability comes later when you find things that are genuinely reusable. Kind of like the "Hmm... I did that before, and it would be useful to save" sort of idea.
It's better if it's written with ctypes not C
it supports file backends why cant you just that ? 
This is interesting and I appreciate the time spent in producing actual results. However it does just reinforce the impression I had that ctypes was intended for quick easy access to C APIs and not for performance. I don't think that there was ever really any doubt.
Honestly, this is what StackOverflow is for.
Python Challenge for sure, lots of great examples, expand your comfort level with the language and the libraries.
You're right, it is O(log n). I was so busy pointing out it wasn't O(n) that I didn't notice it wasn't O(1) either.
It handles some of the more advanced features, proper formatting and syntax, packaging, documentation, testing, optimization, multithreading, etc.
KWallet has to be interfaced in C++, so that won't work for all backends.
While I completely agree with you on this, Placidified, the OP is a new python coder. The problem with any big repo, in my experience, is that you don't really know how to sort the wheat from the chaff. You can waste an almost infinite amount of time in cpan or the cheeseshop going down what amount to pointless roads in learning best practice and mainstream tools. I am a pretty big fan of starting out using reference points other than repos to figure out what the most accepted tools and approaches for a given language community are before wading into the deep end of the repo pool. Dunno, this just might be my ugly thrashing about in CPAN back in the day left a bad taste in my mouth. Others might have had better experience with this approach. 
If Python's your first language (i.e., you've never heard of `goto`), having control suddenly jump somewhere else is a totally new idea. 
I felt the graphs had unintelligible colors until I realised that you can mouseover them
as the author expectedly fails to mention, running a server and client benchmark on the same machine (or even on a small LAN) makes everything CPU bound. this makes the results fairly worthless, as important differences in IO bound systems (e.g. select vs poll vs epoll/kqueues/iocp) become glossed over. in a CPU bound system, where all connections are very consistent (respond quickly, no dropped packets, etc.) select and epoll will perform somewhat similarly. the main advantage of something like epoll kicks in when there are thousands of connections per second in various states. there's a 3ms difference per request for something handling 5000 requests/sec and 2000 requests/sec, and that 3ms will easily be recovered using something like epoll on a busy, real-world system. otherwise there are a whole slew of things not taken into account (ping/pong server as a workload? really? how about framework usability, comprehensiveness? what are some of the metrics of performance degradation?) that make this series of benchmarks just more garbage on the internet.
I tested it on linux without running gnome/kde, and it automatically created a new file-based keyring in ~/crypted_pass.cfg You should be able to get that behaviour explicitly :) 
Uhm, theres a matrix summing up lots of the features of each framework. Which also mentions if the frameworks use Epoll. All but one of the frameworks use epoll.
maybe he wanted a keyring made out of an actual python?
the matrix is full of worthless information like "twitter account?" and "authors blog?". how about, can you use kqueues or iocp with the framework? cross platform support? you know, useful things.
Well, make those suggestions to the author.
It's a simple one, but it will be my first real python project and I don't have a lot of time (it's low priority), so that makes it perfect for me. My company has thousands of unique urls on our site, and a lot of them are ranked fairly well in search listings (for certain search terms). We want to track the rise and fall of our urls in the ranks (for SEO efforts), so I'm taking a list of terms and using the search engine APIs (Google, Bing, Yahoo) to get results and see how we rank for each term in the list. What I like about this is that I can get quick results that will be useful early on, and then add more pieces as needed, or as I have time. It will be cron-driven, and alerts might be useful in addition to the normal reporting. I'll get to dig into python stats and graphing once I've collected enough data, and add a web UI to what will start out as a CLI app. I'm not really a programmer, so this is perfect because it touches on many of the things that I'll want to know how to do quickly with python in my role.
yes exactly, it's just a bridge to C types and functions. to actually get a speed increase using C types, use Cython or Pyrex or export some C++ with SWIG.
&gt; _So what do I do now ?_ I'm going to go with a somewhat radical suggestion here, "Start coding". It has been my experience that in the field of programming one of the best ways to learn is to _do_. Less read. More code. 
well that's the gist of my question - what/how do I get coding
How were those graphs made?
What? Whatever you want. Do you have any itches that need scratching? Any hobbies that might benefit from some software? Something at your job that could be automated? A game idea? Interactive narwhal bacon porn? How? I would recommend a text editor or IDE. Vim, Kate or Eclipise+PyDev are all nice choices. :-) 
&gt; Interactive narwhal bacon porn upvoted
Having a Twitter account is a sign of a good asynchronous server? Err, what?
No, and i did not award points for features or anything. It can be an indication of the viability of the framework, when there is no Community or Blog or Person around a certain framework the repository is all you have.
The focus was on Linux performance only, i am not really interested if it supports BSD or Windows. But i might consider looking at those features in a follow up. 
Looks to be Highcharts.
I learned many things after books. The most important of all is to detach yourself from your own code. There will always be a better, faster and smarter way to write the code you write. You should at all cost, never concern yourself with faster, better, smarter. Concern yourself for the first couple years with what works. As long as you keep it simple now (as simple as you can make it); as your experience upgrades, you're absolutely welcome to go back to your old code and upgrade it as well. This is something no book tells you now. You are not done when you're done. Accept that as a fact now and you'll be humbled as you go back to fix bugs, add features and optimize later. Now, there is a stage of programming in which all of the above should not be a concern. It is the prototyping stage. Python allows for such rapid prototyping it almost baffles the mind. It is gonna take some experience to see the difference. Here is a good example. When you're staring at a blank document and you have a problem in your head or on paper, the first immediate write-up of code should solve that problem as quickly and if necessary as specifically as possible. Never abstract in the prototype. Just solve the problem. Once you're comfortable with the prototype and it solves the problem you were after, consider it the blueprint to version 1 of your final solution. Once you're done with your prototype, get ready to abstract everything. Every function and class should do one thing and one thing only (if possible) and in as general a fashion as possible. This is exactly what needs to be done for your work to be as reusable as possible. Re-usability is the key. The first line of every Python program (after the shebang and right to the last line) is about re-using something whether you made it or not. In fact, every line in-between is re-using code from someplace else. So, you're done with a prototype and you just based version 1 on that prototype, what now? Go back to paragraph 1, re-read it and move on with the next problem. So, where do you start to get experience? By understanding what a problem is. If you don't know a problem when you see one, you sure as hell won't see the solution if it was parked on your face. If you can't make a problem for yourself then you in no way can make a solution to it. If you can't see a problem and you can't make a problem the problem is your solution. OK, maybe thats a bit philosophical and over the edge so heres a good tip to get you started. Go solve other peoples problems for the time being. Go to the python google groups, stackoverflow, etc. You don't need to know or solve everything. But there are countless places to start when it comes to other peoples problems. Sometimes we don't see our own problems until someone else points it out for themselves. So start there. Well that's it for my tips. You might want to check this out too for a jump-start. http://stackoverflow.com/questions/24692/where-can-you-find-fun-educational-programming-challenges . I hope you the best with your endeavors. Good luck! 
Python doesn't have "namespaces" in the sense of Java, C#, D, Go, etc. It has packages and modules. While I like the spirit of this article, and that it highlights an often hushed over part of Python, I'd go one further and say that it is simpler and better to just keep things flat – use a unique package name for each distinct module in your projects, and altogether discard the notion that Python supports the hierarchical namespaces composed of orthogonal libraries common in the above languages. Certainly this is better than recommending something from the setuptools house-of-cards-built-of-hacks (which is where the 2500-line pkg_resources.py comes from).
very informative and insightful, thanks !
Use Picasa's native client. It's very intuitive and has ports for linux and mac. (http://picasa.google.com/)
I didn't even know you *could* declare namespaces and I've been using Python for years. The moar you know.
for a minute i got this confused with web.py
Except you can't setup picasa upto auto upload all photos in folder and check it on regular basis. I want an automated solution, not something I need to manually baby sit. I need to upload a lot of photos from a central storage server, and they may be coming from multiple people. I need an automated solution, which I have yet to find.
&gt; and altogether discard the notion that Python supports the hierarchical namespaces can you give an example of what you're suggesting ?
Sure. Google release open source Python code that attempts to use a 'google' namespace. Examples include their protobuf Python distribution, and the App Engine SDK. Both create package directories called 'google' as a substitute for namespaces. Unfortunately when you try to use these packages together, you run immediately into problems as demonstrated here: $ mkdir -p {foo,bar}/google $ touch {foo,bar}/google/__init__.py $ touch foo/google/a.py bar/google/b.py $ $ PYTHONPATH=foo:bar python Python 2.5.1 (r251:54863, Feb 6 2009, 19:02:12) [GCC 4.0.1 (Apple Inc. build 5465)] on darwin Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; import google.a &gt;&gt;&gt; import google.b Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; ImportError: cannot import name b &gt;&gt;&gt; google &lt;module 'google' from '/Users/dmw/foo/google/__init__.pyc'&gt; That's because 'google' is not a namespace, it refers to a package with a definite location on disk. The name space cannot be shared between orthogonal packages. You could copy the 'b' part of the second package into the a's package directory, but this is quickly creating a mess that diverges from any officially sanctioned/versioned/QA'd configuration. This is only a half truth. setuptools includes some stuff in its `pkg_resources.py` which I guess does some evil things to sys.modules to make this work, but it is far removed from sensible, or how Python works everywhere else. The simple solution is to use a unique top-level package name: [23:24:47] [perelandra:19:~]$ mkdir -p google_{foo,bar} [23:24:56] [perelandra:20:~]$ touch google_{foo,bar}/__init__.py [23:25:04] [perelandra:21:~]$ touch google_foo/a.py google_bar/b.py [23:25:14] [perelandra:22:~]$ python Python 2.5.1 (r251:54863, Feb 6 2009, 19:02:12) [GCC 4.0.1 (Apple Inc. build 5465)] on darwin Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; import google_foo.a, google_bar.b &gt;&gt;&gt; Problem solved without magic. :)
I thought it was an excellent indicative article, and a good launch-point to then expand on those tests to suit your own custom requirements. Well done and thanks Nicolas. Phillip.
I don't understand why people try to make things complicated for no reason. Python packages and modules and simple and sweet. Put related modules in a package, and always import them from the package: keep/ __init__.py it/ __init__.py simple.py In other code - in the package or out of it: from keep.it import simple Or: import keep.it.simple You don't needs anything else. 
I can totally relate. My answer: build a little webscaper to grab something off the web. Little webscrapers are great little crunchable birdses to chew on while learning how to code pythonically. Mechanize and BeautifulSoup and/or lxml are widely used and, at least for B.S., seem rather pythonic, so that's helping me learn how to think pythonic thoughtforms. 
Someone [asked the very same question at StackOverflow](http://stackoverflow.com/questions/1095768/what-next-after-dive-into-python) a few month back, lots of great responses there. 
thanks !
great thanks ! yes, 2 pkgs w/ identical names, orthogonal or not is just silly 
&gt; evil things to sys.modules No, it does documented non-evil things to \_\_path\_\_ -- things that are pretty much identical to those done by pkgutil.extend_path, if you prefer to use the stdlib (and don't mind its limitations). \_\_path\_\_ is to packages what sys.path is to Python as a whole. A module with a \_\_path\_\_ is a package; a module with more than one entry in its \_\_path\_\_ is a namespace package. This feature has been part of core Python for as long as packages have. pkgutil and pkg_resources just have nice APIs for making use of the feature.
They don't screw with sys.path, and setuptools didn't invent namespace packages. [pkgutil.extend_path](http://docs.python.org/library/pkgutil.html) was in the stdlib since 2.3; setuptools just adds some bells and whistles, such as zipfile handling, automatic handling of parent packages, etc.
You can do the same thing with the stdlib's [pkgutil module](http://docs.python.org/library/pkgutil.html), it's just slightly less convenient and doesn't support zipfiles.
Thanks, I'd personally never heard of it. Its only documentation appears to be an essay from the Python 1.5 era, and a tiny note in the official Python tutorial. This still falls within the realms of esoteric magic for me (a Python-as-primary-language developer of 10 years)
Just to point this out... Python doesn't scan __init__.py. It essentially executes __init__.py, then converts all of the variables to attributes of the module object. It's more complex than that, but that's the basic idea.
Looks brilliant. 
&gt; Python doesn't have "namespaces" in the sense of Java, C#, D, Go, etc. It has packages and modules. In Java, java.x.y is no different to naming a python package: python_x_y or similar. There is no semantic relationship between java.x.y and java.x.z. They are **not** hierarchical. You can't, for example write: from java.x import y, z; like you can write: from zope import interface, component in Python. In my opinion, Python is superior in this case. &gt; use a unique package name for each distinct module in your projects What? Why is that a good idea!?
&gt; Let me guess, you're a Mac owner too. No he would obviously point it out by now. He's just a dick.
In Java, you can have many libraries on the classpath implementing parts of a name space without any effort or knowledge of each other (other than avoiding conflicts of canonical names). In Python, you can only achieve this through mutual awareness of those parts and esoteric hacks (see pje's comment below). In Python, a namespace is not simply a convenient grouping, it forms part of the implementation itself. I have no argument against packages and hierarchies within them, just trying to combine distinct packages which quickly becomes complex and messy as shown in the example.
i'd like to see a **table** or maybe some **charts**, because honestly, who's using lynx to view that site? it's just one page of text, more than half of that is fixed-width text, presumably code and numbers. so why is the author expecting readers to make sense of the whole text, just to get to the conclusion?
so they feel the need to list some gotchas? all languages have those. i'm underwhelmed.
&gt; In Java, you can have many libraries on the classpath implementing parts of a name space without any effort or knowledge of each other (other than avoiding conflicts of canonical names). Exactly in the same way that: myproj_yadda and myproj_blahblah would do in Python? These are not namespaces in Java or Python because I can't do: from myproj import yadda, blahblah in either language, only: import myproj_yadda, myproj_blahblah I do agree that the namespace package story could be nicer, but it is used a lot in some corners of the Python world and very well understood, certainly not an "esoteric hack". See [here](http://svn.zope.org/)
Fair point, and Zope is a very compelling example. Also, I may have been overly bullish here in suggesting avoiding namespaces altogether, especially considering my being unaware of \__path__ prior to this thread.
This is great. The older I get, the more I realize how much unix got right. Commands like 'sort' are amazingly well done and, for so many problems, you're much better off using commands, files, and pipes than a lot of code.
Taking unix pipes to the next level (hopefully): http://www.youtube.com/watch?v=faJ8N0giqzw
Hey all, I wrote this little migratioins library to manage client side sqlite database. Feedback on the code/design/docs/usefulness is more than welcome. Thanks.
It's not entirely that simple sometimes. Lets say you have the following directory structure. Assume both test1 and test2 are in sys.path. test1/ pack/ age1.py test2/ pack/ age2.py The following will work: from pack import age1 But this will NOT work: from pack import age2 Python stops looking through sys.path once it finds a match for for the module (in this case "pack") which happens to be in the test1 folder. It completely ignores test2 even though that's where pack.age2 can be found. Adding a namespace to test1 and test2 fixes this.
Then you realize that you need to have `keep.it.tidy` in a separated *physical* package..
Syntax looks ugly and it encourages lots of unneeded forks... No accounting for taste I suppose, but I prefer my python clean and pure. If I want a shell script I'll write a shell script. If I want a tool that isn't appropriate for shell and I want to use it in the context of a shell it seems, to me at least, better to write small tools in python and then fork them inside a shell pipeline.
Maybe you want to use the [Timer](http://docs.python.org/library/threading.html#timer-objects) object.
Do you really need to get the system time? What's wrong with doing def on_space(): orig_col = get_color(some_char) change_color(some_char, new_col) t = Timer(3.0, lambda: change_color(some_char, orig_col)) t.start() 
No, it will not pause execution. It will run in the background as a separate thread.
 from threading import Timer t = Timer(...) or, if you like it this way: import threading t = threading.Timer(...)
Replied above. It's in the `threading` module. You either have to qualify it (`threading.Timer`) or use an from...import (`from threading import Timer`)
Alright, I've hit a wall. Here's the idea-- if the user clicks enter it changes a property of this snake object. I have it such that if the bullDoze property = True, the head of a snake becomes black. I want the head of the snake to become black for three seconds, and then to call the remove function which sets the bulldoze property to false. Currently two problems are happening: A) I can only use hit the ENTER key once before it returns the error below B) The character doesn't ever change color. Here is the implementation: if (e.key == K_RETURN): snake.bullDoze= True snake.bombs = snake.bombs-1 t = Timer(3.0, bullDozeRemove(snake)) t.start() def bullDozeRemove(Object): Object.bullDoze= False If I click enter more than once, the following exception is returned: Exception in thread Thread-1: Traceback (most recent call last): File "C:\Python31\lib\threading.py", line 509, in _bootstrap_inner self.run() File "C:\Python31\lib\threading.py", line 715, in run self.function(*self.args, **self.kwargs) TypeError: 'NoneType' object is not callable
There's another error. If, for example, I change the bullDozeRemove function to look like this: def bullDozeRemove(Object): print("tester") The function will print tester IMMEDIATELY after I click return and it will do it for three seconds. Rather than waiting for three seconds and then calling the function. 
Change `t = Timer(3.0, bullDozeRemove(snake))` to `t = Timer(3.0, lambda: bullDozeRemove(snake))`. Why? the second parameter of the Timer constructor takes a function. If you give it `bullDozeRemove(snake)`, it'll execute that function right away and use its return value as the parameter. You don't want to do that. Wrapping it in a lambda (which, in essence, is a small anonymous function) gives Timer a function it can call later.
Works like a charm. Thanks so much. I hadn't learned about threads or lambda so I learned a lot! Take care. 
Yes, I agree. If you have any sysadmin duties, see if you can work python in. I started learning python for fun, and have since scripted much of my more mundane work away with it in the form of log file analysis and report building. Great way to learn. Mostly replace shell scripting for me.
Don't do that - you should only ONE package with the same name in your system path.
No - simply keep it in the "keep.it" package.
Then your "keep.it" package keeps growing with keep.it.(xxx,yyy,zzz), all of which should be inside keep.it scope, but has nothing to do with each other. As time goes by, the package becomes huge for most people who just want a subset of "keep.it.simple.and.small" and "keep.it.tidy.and.clean". Later, some third party developer think his code fit nicely in scope of "keep.it.simple.xxx". But he can't distribute his code independently of your package because it needs to be in same physical package. Now he has to change the name to "keep2.it" and break the most suitable name.
Is this any different from how Python 2.x handles regular expressions?
Doesn't appear to be. Which is just as well. Moving my code to 3 is turning out to be quite the Herculean task. Changing all of the re stuff would make it damn near impossible.
The biggest difference is the same as everything else in Python 3: bytes vs. strings. The re module can handle regular expressions on bytes and regular expressions on strings, but (like everything else) it will not match a bytes RE on a string or a string RE on a byte array.
The more I think about it, the more I think people who just say "PyPI needs to be just like CPAN" aren't really putting thought into it. To really get the level of functionality CPAN offers requires not just software -- that, ironically, is the easy part -- but also tremendous buy-in from the community. Basically, everybody who maintains a Python package has to drop everything they've ever done (workflows they've had for years, in many cases) and buy 100% into a brand-new replacement. Which just ain't gonna happen. Better dependency management and better package-management tools? Sure, that stuff's actually pretty easy. But something requiring the huge amount of commitment and buy-in of CPAN? Probably not; it's just way too late at this point.
Thinking about CPAN makes me shudder, I have had so many more problems with it than PyPI/easy_install; but to be fair I'm not really a Perl person.
I should have probably linked to the [homepage](http://www.hforge.org/itools) but the documentation provides a better description of the packages
Why does bible quotes attract the attention of my eyes so easily? '^^ Anyway, sure, if it can be done nicely and in a consistent manner. I will accept it., but we are not there yet. The operating systems I use have excellent packaging systems for big frameworks and for the other Python dependencies I mostly dump the module into the project lib directory anyway.
If I understand the crux of your argument, it is that larger memory addresses implies one must decrease thread counts? And that memory access is the limiting factor and not CPU cycles? These assertions do not have much basis in fact. Erlang is a multithreaded and it is not multiprocess subsystem on Windows. So this is a perfect example of a scalable multithreaded application. SQL Server, Sybase, Windows NT are all examples of large, scalable multithreaded applications. There is an endless amount of teeth knashing from the Java/UNIX world about how multithreaded applications are not provably correct. It depends upon whether you effectively implement message passing within your multithreaded application of course. This is the only sensible way to write a multithreaded application, but some people think they have discovered the new world when these things were defacto laws of writing multithreaded programs over the last 15 years on Windows NT systems. 
Thing in a project with this structure: project/ __init__.py plugins/ __init__.py somefeature.py Why I can't put otherfeature.py in project.plugins package without touch the project code?
If xxx has nothing to do with "keep.it", why do you want to keep it inside this package? Put it "where.it.belongs".
This is largely a misinformed rant about some project that has bad documentation. Largely irrelevant to what pypi is - pypi actually has support for serving documentation and it's pretty decent.
The correct structure is something like this: /usr/local/lib/python2.6/site-packages/project/plugins/builtin_plugin.py /usr/local/share/project/plugins/local_plugin.py /home/you/.project/plugins/user_plugin.py 
There's some neat stuff in here, but I don't understand the overall theme - why is all of this functionality bundled together, rather than being provided as independent modules? How do you decide what functionality is appropriate for inclusion in itools and what isn't?
Nice package collection. It groups very different formats: rss, odf, html... Maybe it needs a set of adaptors to share the same information across modules.
Good stuff! Lots of comments from me... The CSV stuff is badly needed, I rolled my own to do some of this at my last job. I suspect there are many other packages to do this, perhaps a note distinguishing yours from Python's (for people who haven't done much CSV?) I suspect your other document format handlers are also very badly needed though haven't myself. The serialization part seems to be quite clever - but I'd personally be a little skeptical about using some third-party's serialization - perhaps a comparison to pickle would help the consumer? In general, short comparisons to other similar packages would be helpful in deciding to adopt. I strongly urge you to drop the idea of a template language. Yours looks fine - but there are too many out there already. In my last job, I used Clearsilver, a tiny but quite complete templating language that already exists and supports Python (and which doesn't allow you the abomination of putting program code in your HTML, the horror!) Clearsilver also supports other languages which is definitely an advantage... You should have a download link on the documentation page! Similarly, there should be links from http://www.hforge.org/itools/index to the documentation pages. Weirdness in your website: it's easy to get stuck here: http://www.hforge.org/itools/index/#itools.workflow and for some reason on Chrome at least the right sidebar is missing... Do you know how complete your test coverage is? The tests look good! There are a lot of small English errors, but I'm sure if I wrote in another language I'd have even more mistakes and it's all very understandable. Thanks for doing this!
All good points, including the ClearSilver recommendation. And manelvf's idea on adaptors seems like a good one too, if possible.
This is much harder than you make it sound. Until recently, I could not download an egg and just drop it into a directory (since I do not have write access to our corporate python installation). I had to add each egg to PYTHONPATH. Now, if I can move to python2.6, I have PYTHONUSERBASE, but that's only one directory. I can put .pth files in it, but .pth files are not recursive. There should be much more use of environment variables, like PERL5OPT. There should be automatic recognition of architecture, and a way to import shared objects based on it, like the support for linux-x86 directories in Perl. There should be a simple way to throw eggs into a path *and* have them available without pkg_resources.require(). One idea is a special directory name, like 'eggs'. If I put '/home/me/eggs' onto PYTHONPATH, Python should convert that directory into a special importer, which would make all eggs there available, search for architecture-specific sub-directories, and automatically execute code in any module called 'auto'. Something like that. There should be ways to shut this off for safety, of course.
xxx has everything to do with keep.it. It just has nothing to do with keep.it.yyy.
Python packaging parallel universe inhabitant here (aka buildout user). buildout tends to solve this. Of course it also automates actually downloading eggs and putting them in a directory by doing that itself based on the dependencies of your package. This is of course what you want in development - if you depend on a package you want to actually express this in a way code can use it instead of trying to spell it out in an INSTALL.txt. 
WTF is that catch-all package with redundant functionalities with stdlib?
Nlo, nolt memory size - memory *contention*. Even today, memory access is generally the slowest part of code execution, hence the importance of on-chip caches which avoid that expensive trip. With lots of cores,if they're all accessing the same bit of memory, then the limiting factor is solely the memory contention, making the number of cores worthless. Any change must invalidate the cache on every other core. This is why modern multi-core architectures are moving towards a [NUMA](http://en.wikipedia.org/wiki/Non-Uniform_Memory_Access) architecture. Each core has it's own memory which it can access cheaply, rather than general slow memory shared by every core. This removes the performance benefits of threads vs processes however - that you don't have to copy the data with threads. With lots of cores copying is *neccessary* for performance anyway. Erlang style message passing avoids this - the messages are always **copied**, with a share nothing model being used. This model means that the performance differences are irrelevant though - processes are just as efficient (and more fault tolerant)
I see better what you are saying, and you have a point vis-a-vis strong guarantees and lack of concurrency issues with NUMA architectures and (more formalised) message passing schemes. Pipelining (should IA64 ever arise from the ashes) will have a far better go of it if you don't need to mix and match that with a bunch of memory barriers. In practice however, I do not think it is as catastrophic as you are painting it. Synchronisation between threads should be generally infrequent. Take for example a multithreaded webserver, it will receive an event to handle and give a worker thread to compose that. Communication to the worker thread pool will be of the nature of a queue, and ownership of the message will be given to the receiving thread. The optimisation that is possible in this scenario is that no copy is required. Rather it is understood that semantically that data belongs to the worker thread, and the worker thread may dispose of the data or else hand back ownership when the job is completed. When the worker wishes to return the results, in addition, no copy is required. Copying may or may not be expensive dependent on the data structures which make up the messages which are being passed. For instance, if the data structure were to require to be a sorted list, I have seen deserialisation costs for framework code in C# (for example) such that you pay the cost of rebuilding a deserialised dictionary (hashmap), which is CPU expensive, O(n ln n), and dynamic allocation expensive. Ultimately, in a threaded environment, message passing can be on a reference basis (i.e. pointer basis) and that is simply not possible when address spaces are not shared. Due to the coarseness of many runtime environments, deserialisation can be more costly than expected.
im not the author of itools I just found it while browsing PyPI
&gt;Take for example a multithreaded webserver Webservers are already embarassingly paralell - there's effectively no shared state which means you can already take full advantage of multiple cores, GIL or not, by using a multiprocess model. No copy is required there either - the process just handles the whole request and returns it to the client, and many webservers already do this (or take a hybrid approach with both processes and threads). The only reason for threads is OS specific performance concerns (eg windows handles threads better than processes). This is generally a constant factor at worst. &gt;message passing can be on a reference basis My point though is that with a NUMA architecture, you don't **want** it to be on a reference basis (all cores looking at the same generic memory), but rather it's more efficient to give each core a copy in it's own, quicker to access copy. The one-time serialisation costs becomes less and less relevant with more cores. At the moment, 2-4 cores is probably a sweet spot for threading, as the costs often outweigh the gains. The more cores you add though, the further you tip the balance between copy costs and memory contention costs. 
The Python import path's always going to be there, and has nothing whatsoever to do with packaging.
itools: Extra batteries.
Un-American too?
Don't knock embarrassingly parallel. Few problems are not. We do 30000 hrs per night computation on our grids, mainly monte-carlo. We are just one trading desk out of hundreds in the city, one of dozens of derivatives desks in my firm. All embarrassingly parallel. The majority of requirements for excess compute capacity is in the embarrassingly parallel space. You say that threads are an OS specific performance issue, but I have already clearly stated that if you don't share memory then you have extra copying to deal with. For most programs these are not memcpy() either but rebuilding allocation heavy structured data. In addition good designs implementing immutability (see Java and C# strings for example) mean that there is no cost for sharing memory access if the design is correct. And, as I also stated, the reference implementation of Erlang IS a multithreaded application. Lastly, you claim that GIL or not, you can do embarrassingly parallel in Python. The issue is that you can't. Python is engineered in such a way that every package, including the base packages, are mutable dictionaries. So for instance, sys is mutable. This is very different to other languages, where, for instance in the C++ runtime, I can guarantee the memory location of functions for new, delete, malloc(), fopen(), etc are generally invariant. There are very few global state variables, I can't speak for gcc, but in msvc things such as errno.h work on the basis of thread-local variables, creating no thread-related side-effects, nor thread synchronisation costs, to be induced by the design of the language runtime. This is not the case under python where the entire environment from top to bottom is a bunch of mutable dictionaries. Even default parameters are shared-global variable instances. The entire approach to class prototypes is driven off shared everywhere dictionaries. It's hard when to imagine, when that is your design, how you retrofit threading as nothing is safe. One would need to be quite creative. because every time you look up a function name it is looking into a dictionary - so you need fine grained locking for trivial operations. In addition, because it is prototype based, i.e. you don't know what type you are actually calling, just the function signature, you cannot link ahead of time. Your argument that one-time serialisation costs becomes less and less relevant the more cores is a theory, but it is untested and unmeasured. I have measured it, as I deal in serialisation of gigabytes and gigabytes of data to 2000 node grids in my work. Some jobs are so trivial that the serialisation costs are 100x more expensive than the calculation cost, and this can create significant costs depending on the jb. I don't see any basis by which you can claim this to be true that one-time serialisation costs can be discarded as a concern as you have more and more cores. On the contrary, you will have more and more serialisation costs if you pursue a copy on sendmessage for all multi-thread/multi-core interactions. The heritage of Windows NT threads is VMS and OS/2. They got this operating system feature right. There seems to be a tremendous amount of FUD about threads, mainly from those who haven't programmed any threaded applications in anger. I can understand that there are a couple of causes - threading has always been slightly platform dependent on many Unix systems, I remember the discussions about posix threads and solaris threads at work going back 11 years at least, and fork() is extremely powerful, so there isn't always the knowledge and experience in that domain; and Java community declared threading to create unprovable programs, on the basis that their JVM didn't do guards well. CRITICAL_SECTION and intelocked increment / decrement always functioned fine on Win32 however, so with those primitives I really don't understand how they could make such categorical statements. I've had 16 years of writing this stuff off and on. Most threads are worker threads, you give them a task and you do it on a mutex guarded queue. This is how people write multithreaded applications in reality. 
"itools is a Python library which provides a wide range of capabilities"
still only a collection of broken components that pale compared to awesome tools like for example jinja2, genshi, sqlalchemy, werkzeug, django
So, I guess you've never used web2py
&gt;Don't knock embarrassingly parallel. I'm not - just pointing out that they're already perfectly suited to multiprocess models, since there's no communication overhead. The GIL is *already* irrelevant for web servers. &gt;if you don't share memory then you have extra copying to deal with. Yes, and if you're on a NUMA architecture, you have that same extra copying to deal with if you want that data in memory rapidly accessible to the core. You need to pay that price *anyway*, thus large numbers of cores mean the differences reduce or vanish. &gt;The issue is that you can't Of course you can. As I've said, a multi-*process* model completely bypasses the GIL. All communication must be performed by copying data about, but that's becoming more and more of a good idea anyway. &gt;Some jobs are so trivial that the serialisation costs are 100x more expensive than the calculation cost You've prices to pay *anyway*: either you pay the access cost on every retrieval from non-local memory, or you pay the serialisation cost. More cores change the payoff ratio towards copying and away from memory access by increasing the relative cost of the former. There are perhaps places where the payoff is still in favour of non-copying (where only a small, but unpredictable part of a large, non-static dataset must be accessed a small number of times), but that balance definitely seems to be shifting to the benefit of a share nothing model.
Just trolling or care of making a concrete example to support your statement?
This is very good - you xxx and yyy do not depend on each other - you would like this to be true for most modules in a system. For example, keep.it.simple and keep.it.tidy do not depend on each other, and usually used together, but can be used alone. However you have a point here - when you put stuff in a package, you make it harder for others to use only parts of the package. Don't create big packages with silly names :-)
I would like some specifics as to why you think this way, and some reasoning as to why you are comparing template engines to a web framework. 
http://docs.python.org/
Learn the basics at [http://www.diveintopython.org/](http://www.diveintopython.org/) and then use [http://docs.python.org/](http://docs.python.org/) for a reference. Once you want to do more then go back to Dive into Python. It's the right start because I came to Python from PHP myself. The main things I struggled with is that the array doesn't work the same way, databases are not built into the language (php has a load of mysql stuff) and that hosting python scripts isn't so easy.
Nice! Thanks mate didn't even know that existed
I'll second Dive into Python - I like his style and the fact that he assumes you know some programming.
Thanks mate
Yes it is good to not be considered an idiot from Chapter 1!
Python 2.5 and up have sqlite built-in, which is great for low-to-medium traffic blogs and whatnot.
If you're on *nix, the `pydoc` cmdline utility is great too, e.g. `pydoc os.fork`
I _highly_ recommend the Pylons framework. It has a similar feel to PHP development. Also, Jinja2 for the templating language if you want to keep things simple.
ok thank you very much
I just went from php -&gt; python (django) and http://djangobook.com is absolutely amazing.
Step 1: Learn programming. (scnr) The [official tutorial](http://docs.python.org/tutorial/) is surprisingly good. [Dive](http://diveintopython.org/) is a great start as well, good choice! But if all you're doing is these 'Hello World' type examples, you might not be motivated enough to keep learning. Start a project that'll give you results from the start. Luckily, if web stuff is your thing, there's tons of Python modules to be discovered. Django is adequate, from what I've heard, but first off you might prefer to check out nifty, tiny libs. Have a look at Kid, Cheetah, Genshi, CherryPy or such WSGI-compatible goodies. You might not end up using any of that in production, but here's the thing: If you take web.py, for example, you can read through the entire source code in an evening (even as a Python novice; it's just one file) and learn some Python on the way. Oh, and while you're trying out lots of new modules, you might want to use [virtualenv](http://pypi.python.org/pypi/virtualenv). It's a great way to avoid wrecking your Python installation by heaping on a tangled mess of dependencies. [easy_install](http://peak.telecommunity.com/DevCenter/EasyInstall) should be in your toolbox from the start. Take care to avoid outdated docs/tutorials. You'll want to learn Python 3 mostly. Just be aware that there's such a thing as old-style classes and that having them somewhere in your project means that you won't fully understand, say, all the complexities of MROs in the context of multiple inheritance for quite a while. Using Python 3 from the start is one way to make sure you'll be completely safe from such pitfalls. If it can't be done (because your favourite web framework still relies on Python 2.x), you can pass the argument "-3" to the interpreter and at least find out around which parts you'll need to be careful. Oh, and in case someone else can be more helpful than me when it comes to the specifics of your next job: Could you clarify/elaborate about that a bit, please? Will it be a Python app or a Python web app where users enter the data? Why FTP? (Is it at least SFTP or FTPS?) Is it for the internet or an intranet? Etc. **Edit**: spelling
Agreed but someone coming from PHP probably uses MySQL or Postgres, neither of which come packaged neatly in the standard library. Good call on sqlite though, I should have mentioned that.
im not just comparing template engines ...
Thanks for this mate. The program will upload its results to a website for the reporting to be generated. The results will be a bunch of nice graphs and tables taking data from a MySQL db. The reporting needs to be on a website as per the client's spec.
Also coming from PHP here, I got the "Python Essential Reference" from David Beazley and I must say that I like it very much. It's not a introductory book on programming - it assumed that you know programming very well and just need to learn the ins and outs of python. It's pretty direct-to-the-point and well written. I highly recommend it. http://www.amazon.com/Python-Essential-Reference-David-Beazley/dp/0672329786/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1261867689&amp;sr=8-1
There is this great book that I'm currently reviewing: the 4th edition of Learning Python by Mark Lutz. Might be worth to check it out; the book is very well organized and easy to read.
If you already know programming, [this](http://www.poromenos.org/tutorials/python) might be faster. Dive into Python is great too, though. Disclaimer: I wrote it.
i suppose detached routing, correlated joins with nested subquerries and and genshi's xml transformatios as well as jinja2's optimizing compiler are some good examples if you want actual code examples feel free to request them, then i'll make some up once i'm on the 26c3 (or maybe directly after it)
Ronny is not a disinterested party. He contributor to Werkzeug and Jinja. I also believe he is a student at the University of Applied Science Schmalkalden. I have been asked a few times (not by him but by other Werkzeug contributors) to re-factor web2py and base it on their tools. While I like Werkzeug, I have decided not to for various reasons (maintain backward compatibility, avoid third party dependencies, differences in programming style). Moreover as you may know, web2py is used at DePaul University as a teaching tool, we offer two for credit courses based on it, and we also have many graduate students working on web2py related projects. Some are supported by the US Department of Energy. We are not going to abandon our solid and tested tools for other tools that in our view, have not been as thoroughly tested. I should also mention that PyCon was using Django for the registration software but in the last two years they have been using web2py instead. When it comes to software to manage their money, they trust web2py. Finally I would like to stress that when web2py contributors post about web2py, it is always about features and technical feature comparisons. In those few occasions we have been wrong, we have corrected our statement, we have publicly apologized, and in some cases we privately contributed money to the competing project. It is disgraceful that a handful of contributors to respectable competing projects continue make public negative comments about web2py without supporting them in a technical manner. This is no way to prove that their tools are better. Certainly it is no way to prove their community is very friendly and open minded. EDIT: ... and I DO BELIEVE those communities are very friendly and open minded. I did not say nor implied they are not.
&gt; I wrote it. lol - thanks mate!
Try out your code (and explore existing modules and classes) in [bpython](http://bpython-interpreter.org/) or [IPython](http://ipython.scipy.org/moin/FrontPage). For debugging you can use [PuDB](http://mathema.tician.de/software/pudb) or [Winpdb](http://winpdb.org/).
You stated the web2py is "only a collection of broken component". I have not yet seen an example from you of something that is "broken". I have nothing against Jinja2 except that I like full pure Python in templates. This is a fundamental difference in design not a defect of any of the two template engines. If web2py users wish to use Jinja with web2py they can do so.
[This may be of interest to you](http://www.web2py.com/AlterEgo/default/show/106)
I just started learning Python 2 weeks ago, but asking the __doc__ value from certain objects helped me to better understand what it does. For example: print sys.maxsize.__doc__ This might be useful too: dir(sys) This will probably be mentioned in the material you're reading, but the sooner you know about this, the sooner you can use this for objects you don't fully comprehend.
You are quite welcome!
Instead of printing stuff, do help(sys.maxsize) in the interactive shell. 
my statements should not be seen as in anything related to those projects or communities, its disgracing that you try to associate my bold upfont negative behaviour (which i'm quite aware of) with the projects of those humble, cappable people btw, per definition what managements/governments thow money at is not even remotely related to what is good its also quite a number to declare them "untested" not only they stood the review of people exceeding my cappabilities by far, they are are also part of the quite small set of software that do not dislike (and trust me, it needs a tremendous amount of cappability to push software in that category and far more than i have currently at my own disposal) is consider the creation of bases for complex systems is a very important art and engineering skill that very very few actually master, (which is my goal for the coming decades) 
I really don't know where to begin! &gt; just pointing out that they're already perfectly suited to multiprocess models, since there's no communication overhead. The GIL is already irrelevant for web servers. Now you are cheating. I have stated, repeatedly, that embarrassingly parallel is what threads are generally used for. Now you are saying the GIL is irrelevant to this because you aren't going to use threading. Because multiprocess is suitable. This doesn't even make sense as a train of argument. 1) there are communication overheads between the marshalling thread of work and the worker processes in multithreaded applications which are not paid by process address space divided applications 2) it's bait and switch &gt;&gt;if you don't share memory then you have extra copying to deal with. &gt;Yes, and if you're on a NUMA architecture, you have that same extra copying to deal with if you want that data in memory rapidly accessible to the core. You need to pay that price anyway, thus large numbers of cores mean the differences reduce or vanish. The copying is not equivalent. This is a false equivalence you are drawing. Serialisation/deserialisation are expensive and costly to implement over shared memory which comes for free. &gt;Of course you can. As I've said, a multi-process model completely bypasses the GIL. All communication must be performed by copying data about, but that's becoming more and more of a good idea anyway. More bait and switch. You are talking about multiprocess. This is not the topic of conversation, but you've finessed the conversation. &gt;&gt;Some jobs are so trivial that the serialisation costs are 100x more expensive than the calculation cost &gt;You've prices to pay anyway: either you pay the access cost on every retrieval from non-local memory, or you pay the serialisation cost. You're arguing as though these processors don't have caches. Not every fetch is from nonlocal memory. In addition a thread which is working on a small embarrassingly parallel task is unlikely to be jumping all around the address space, and likely to have largely cached much content. In addition, read-only pages (such as code) will not have cache coherency issues across processors. &gt;More cores change the payoff ratio towards copying and away from memory access by increasing the relative cost of the former. There are perhaps places where the payoff is still in favour of non-copying (where only a small, but unpredictable part of a large, non-static dataset must be accessed a small number of times), but that balance definitely seems to be shifting to the benefit of a share nothing model. You are theorising about a next gen operating system that doesn't exist. You'll have to start with erasing all threads from this new operating system. You still haven't explained to me the entire Erlang angle you were playing earlier, given it's a multithreaded application which runs on Win32. 
In Django trunk right now -- and so in the upcoming Django 1.2 -- there's [official support](http://docs.djangoproject.com/en/dev/ref/templates/api/#using-an-alternative-template-language) for using alternate template languages. Implementing the wrapper described in the documentation will let Django use any template language you like, and will work with all of Django's built-in views and template shortcuts, so no changes to other code are needed.
Pylons is a good barebones approach. My other favorite is CherryPy. Django attempts to be all encomposing, and while it suceeds fairly well it's not "for me". Mako template and cheetah templates are my choices for templating. Simplejson is now in the standard library. 
There's never a wrong time to learn python. 
FTP? Ugh. You chose an ugly protocol there to start with. Do you know how it works? There are two sockets, one for commands, and another for transfer data. Fortunately, 'twisted' has some FTP support. Unfortunately, 'twisted' is a bit of a brainfuck, so you shouldn't start with it. 
Merci. This kind of thing would be excellent for more languages. It's frustrating trying to wade through someone try to explain things like scoping rules and object typing with pages and pages of text when all I want to know is what distinguishes this language from the others. I could've used something like this when I started to learn python.
This is one of the joys of programming in Python (and other higher level languages I'm sure). Just use the builtin ftplib and you're set. 
This is kind of like asking what C is good for, other than just writing Unix.
No it's an honest question... I've been trying to learn the basics of this language but I'm not sure what its main use is. I can make my terminal spit out text and can run a couple of different loops, but nowhere in my reading have I seen what Python is really used for.
Basically, people just use it to flip transistor states in their computer. Big time sink.
I see, /r/python is about as helpful as /r/programming. thanks and go fuck yourself cock sucker.
I wish I had seen this when started with Python, knowing about help() and doc() would have flatted my learning curve significantly. Nice jeorb.
How old are you?
Old enough to not want to deal with self righteous programmers that think they're too enlightened to answer a simple question. Seriously, from someone older than you, go fuck yourself.
Now if Digsby would just take a breather from adding the next circle-jerking, duckface-enabling Facebook feature and *fix goddamn text formatting*.
For anyone who passes by, the problem with this question is that it assumes a programming language must have only one purpose, or only one domain in which it's useful. Which overlooks the fact that most programming languages don't fit that mold, and have broad utility in many domains. Python, for example, is useful for everything from web development to desktop apps and games to scientific computing to... yes, traditional scripting tasks. Asking what it's "mainly used for", then, is a question which really can't be answered -- there is no single "main use" for Python, just as there is no single "main use" for most other languages.
Deluge and gajim. But these are in pygtk.
Exaile - Media Player Gwibber - Twitter Client
[Invent With Python](http://inventwithpython.com/) Is a book that teaches you how to code python by creating video games. 
I would say Django is more than adequate. its down right voodoo magic and you can use all the delightful Python libraries. for me, it boils down to: 1) make Django form. 2) write backend code to take form data and doStuff() 3) display output Django takes care of most of the exception catching, all the input sanitation and all the SQL, leaving me more time to worry about the doStuff().
and sftp just replicates all the crappy commands and poor spec support but over SSH.
Does PyObjC count? [Checkout](http://checkoutapp.com/) and [Miro](http://www.getmiro.com/).
I am always happy to hear that people liked it, thanks!
Thanks!
Thanks, that's why I wrote it. I figured most languages are similar, so people would only need to know the bare essentials (a grammar/syntax reference) before they could start hacking away on their own, so I wanted to minimise the time it took from knowing nothing about python to knowing the basic constructs.
Since you're coming from PHP, you should take special note of how Python handles Tuples/Lists/Dictionaries (hash lists.) Also check out the 'pass', 'yield' and 'with' statements; they're neat :) For your FTP project which generates reports/graphs, I strongly recommend that you look into the stuff I listed below under "Distributed Computing." Celery might actually be a perfect fit for you. Anywhere here is some of the wisdom I've gathered/noted over the years. I hope it helps! - sudo easy_install ipython. SetupTools and the ipython shell are your new best friends &lt;3 - Python 2.x vs. 3.x: "If you don't know which version to use, start with Python 2.6.4; more existing third party software is compatible with Python 2 than Python 3 right now." --python.org - Insight Into the Python World: [The Zen of Python](http://www.python.org/dev/peps/pep-0020/), [PEP8 Styling](http://www.python.org/dev/peps/pep-0008/), [Duck Typing](http://en.wikipedia.org/wiki/Duck_typing) - Standard modules I use often: [re](http://docs.python.org/library/re.html), [sys](http://docs.python.org/library/sys.html), [time](http://docs.python.org/library/time.html), [datetime](http://docs.python.org/library/datetime.html), [hashlib](http://docs.python.org/library/hashlib.html), [urllib](http://docs.python.org/library/urllib.html), [os.path](http://docs.python.org/library/os.path.html), [logging](http://docs.python.org/library/logging.html), [sqlite3](http://docs.python.org/library/sqlite3.html), [socket](http://docs.python.org/library/socket.html) - Web Development: Django is *the* way to go. This is easy choice due to [popularity](http://www.google.com/trends?q=django%2C+pylons%2C+web2py%2C+cherrypy%2C+turbogears), the [admin gui](http://devpicayune.com/pycon2008/django_admin.html) and vast array of third-party libraries: [django-registration](https://bitbucket.org/ubernostrum/django-registration/), [piston (make APIs)](https://bitbucket.org/jespern/django-piston/overview/), [south (db schema migrations)](https://bitbucket.org/andrewgodwin/south/overview/), [django-extensions](http://github.com/django-extensions/django-extensions), [django-debug-toolbar](http://github.com/robhudson/django-debug-toolbar/), [sorl-thumbnail](http://code.google.com/p/sorl-thumbnail/), [django-storages](https://bitbucket.org/david/django-storages/overview/), etc. - Internationalization: Django has good translation support, but lacks robust localization support. [Babel](http://babel.edgewall.org/) helps fills this gap. - E-Commerce: [Satchmo](https://bitbucket.org/chris1610/satchmo/wiki/Home) (built on Django) is a good choice for throwing together a boilerplate online store. For fancy stuff I'd stay away because the code isn't very modular and lays on the magic pretty thick. - SCM: Mercurial (as opposed to Git/Subversion) is gradually becoming the [preferred choice](http://www.python.org/dev/peps/pep-0385/) of most Python coders (probably because it's coded in Python) [BitBucket](http://bitbucket.org/) has a very strong Python community. - Socialization: If you live in an urban area, [Meetup](http://python.meetup.com/) is a great to meet other Python coders. - Multitasking: Threads are Python's Achilles Heel. Introducing threads into a Python application has a tendency to introduce latency/scalability issues. Many would blame this: [GIL Explained PDF](http://www.reddit.com/r/Python/comments/8s1ru/david_beazley_on_the_gil_pdf_warning/) Using fork(), or breaking your application into separate processes that pass messages to each other via sockets tends to be a better solution than threads when it comes to Python. - Distributed computing: [execnet](http://codespeak.net/execnet/) (run code remotely via ssh,) [ZeroMQ](http://www.zeromq.org/) (Sockets on crack) - Networking: Python's socket and select libraries are incredibly well designed and make Berkeley sockets so much more fun than they are in C. [Twisted](http://twistedmatrix.com/trac/) is very popular although I don't care for it. - Testing: The built-in unittest tools are generally the standard choice. If (like me) you find them a bit bland, check out [py.test](http://codespeak.net/py/dist/test/), [Nose](http://somethingaboutorange.com/mrl/projects/nose/0.11.1/), [coverage](http://nedbatchelder.com/code/coverage/) - virtualenv/pip are *essential* if you decide to get involved hacking projects on GitHub and BitBucket or maintain legacy code. If you're not doing that, they probably won't be worth your time. - Deployment: If you need to administer/deploy to 3+ servers, [Fabric](http://docs.fabfile.org/0.9.0/) is your new best friend. - Profiling: [hotshot](http://docs.python.org/library/hotshot.html) and KCacheGrind Example: import os, time, hotshot prof = hotshot.Profile('love.prof') def code_to_profile(): # do something prof.runcall(code_to_profile) prof.close() time.sleep(0.2) os.system("hotshot2calltree love.prof -o love2.prof") os.system("kcachegrind love2.prof") - Window GUI: Every Python library for making GUIs (Tk, Wx, Gtk) is either ugly, unportable, a nightmare to use, or all of the above. Jython w/ Swing or QtCreator (C++) are far more appealing choices. - Windows: [py2exe](http://www.py2exe.org/)
I wonder how do you distribute GUI software written in Python for Windows computers? Do you ship Python with it? I am actually downloading Dropbox installation to see what it installs.
[Py2exe](http://py2exe.org) is great for deploying python gui apps on windows.
You can use py2exe to build a .exe file with related binaries (which would include python2.x.dll) 
I was under the impression that much of Digsby's interface runs on WebKit. The original Bittorrent program was written in python + wx. 
Shame Digsby choose to spam their users with adverts, money-making schemes, spyware, CPU-using applications, and so on...
&gt; Digsby's interface runs on WebKit. with wx's webkit module &gt; The original Bittorrent program was written in python + wx. Cool! iPodder is written in wx, too IIRC.
The cute [Google Talk client could have been open sourced](http://www.reddit.com/r/opensource/comments/aasdn/who_else_think_the_abandoned_google_talk/c0gqfr0). I saw their 'XMPP server options' window in Google Talk years ago, but do not know how to invoke it.
What if you're being chased by zombies?
import chainsaw
 26.784.939 Dropbox.exe 77.824 DropboxExt.3.dll 499.712 msvcp71.dll 348.160 msvcr71.dll 2.121.728 Python25.dll 91.663 Uninstall.exe 
Replying so I can remember to check out your links later.. good stuff!
&gt; I would say Django is more than adequate. its down right voodoo magic I believe Django is more than adequate as a framework, but whether it's a brilliant choice for learners depends on their needs. For example, if I already now programming and just want to learn Python, said voodoo magic can make it more difficult for me to understand everything that's going on. Which I don't need to do all the time, but first dissecting web.py and then moving on to something larger seemed a more logical progression to me, so I suggested putting Django off a bit. On the other hand, if I'm new to programming in general and have trouble getting anything interesting to run, then I might want a lot of magic, as long as the API is well documented, so finishing a project won't take me forever. &gt; Django takes care of most of the exception catching, all the input sanitation and all the SQL, leaving me more time to worry about the doStuff(). This sounds so attractive; checking out Django has just moved up a couple of positions on my to-do list :)
&gt;Now you are saying the GIL is irrelevant to this because you aren't going to use threading. That's what I've been saying throughout. That while multi-core is benefitting threading, it's benefitting multi-process *more*, and thus for huge numbers of core, that's likely the better model. Processes are *also* highly used for embarassingly parelell processes, webservers being a prime example. &gt;over shared memory which comes for free. My whole point is that shared memory is **not** free any more. It is a global resource which needs to be accessed by every core, which makes it increasingly expensive relative to copying it first to core-local memory. Since that copy is now desirable anyway, and mutliprocesses requires it too, the tradeoffs for processes are reduced. &gt;You are talking about multiprocess. This is not the topic of conversation, but you've finessed the conversation. It's certainly what I've been talking about this entire time. How can it be bait and switch when it's exactly the model I've been advocating in every post I've made. What did you think I was proposing? &gt;You're arguing as though these processors don't have caches. Caches are part of the reason non-shared memory is more advantageous. Modifications are now incredibly expensive, since the cache on every single core must be invalidated, resulting in locking up the entire system. A model that moves towards explicit communication channels, even when it involves copying the data, now makes more sense. &gt;You are theorising about a next gen operating system that doesn't exist. Again, this is a perfectly standard multi-process setup. There's nothing new here that hasn't been used in practice for decades. &gt;You still haven't explained to me the entire Erlang angle you were playing earlier, given it's a multithreaded application which runs on Win32. Erlang is not, of neccessity, multithreaded. Because it's a share nothing model, with explicit communication, it can run in a multi-process model (or even completely distributed over multiple systems, where shared memory is impossible, not just sub-optimal). You're only looking at one particular imlpementation method.
For a Python FTP client library I recommend using ftputil (http://ftputil.sschwarzer.net/trac). It's easier to manage than the built-in ftplib (and built on top of it). 
really? I've been using Digsby ever since it came out and I've never gotten an advertisement. As for spyware, I've never seen any on my computer, and I'm not sure what you mean by "CPU-using applications", but I'm sure that's not true as well.
If Digsby uses wxPython, shouldn't there be a Mac OS and Linux version as well?
1. iPython doesn't stand for Iron Python. 2. I think you've vastly overstated Mercurial's popularity within the Python community as a whole. Though cPython may have gone with it, in the circles I roam (the Django community) Git is head and shoulders more popular.
This is pretty much how I am learning. My first python script was a growl notified for various rss feeds.
That's funny, I've never experienced that. I've even recommended it to friends in recent times and never heard a complaint from them about anything like adware or spyware. I did some googling and, as it turns out, I'm installing it right and you're installing it wrong(right meaning no spyware, adware).[[Link]](http://blogs.chron.com/techblog/archives/2008/11/digsby_if_you_must_offer_adware_this_is_how_t.html). So, what this tells me is that you, as a tech support dude I assume, is just mindlessly clicking "Accept" without really knowing what you're accepting. Maybe you should do a review of how well you're doing your job there, dude.
Thanks mate!
Digsby: The IM client that spams you endlessly.
CPU-using refers to the once upon a time (or maybe still now?) when digsby's authors included some code that allowed people to participate in some distributed computing project, but it was opt-out rather than opt-in, making a lot of people very angry.
There probably could be, but maybe they want to make sure it will be popular on Windows before branching out to other platforms.
Op, don't you have ANYTHING to say about the book ? Have you been using it ? just posting a link is rather lame.
amen brother. so much crapware, only reason I don't use it.
I am a nonprogrammer, but I was able to create a 5 question multiple choice quiz successfully using this. Thanks for posting. 
not in this particular context ('/r/Python')
Just building out the simple example app gives you a very nice overview of python's syntax.
Great book, though those are the shittitest .pngs I've seen.
I don't know of a library but you can use sys.excepthook
Incidentally, [SpiderOak](https://spideroak.com/) (a zero-knowledge online backup/sync app, sort of a competitor to Dropbox but really the same thing) is also Python. PyQT though.
That made me a little sad. SAP (ERP) jobs are plentiful when compared to python.
I feel bad for him for being out of work so long, we have recently been hiring project managers and every PM has said that most companies are looking for programmers these days, so they've been brushing up on their code... So the market does seem to be improving, but Python is a smaller market than C# and Java. Here are some pretty good job sites for finding python work: * Django Jobs: http://code.djangoproject.com/wiki/DjangoJobs * Python Job Board: http://www.python.org/community/jobs/ * Reddit Jobs: http://www.redditjobs.com/ * StackOverflow Careers: http://careers.stackoverflow.com/ * Joel Jobs: http://jobs.joelonsoftware.com/ 
That's scary. I'm amazed that this person hasn't been able to find a job since 2005. 
So what's the main reason there is so little demand for Python?
The larger companies, and small companies that play to them, are stuck on .net and java. If it doesn't come in an Enterprise version, they don't really want it. That is my observation anyway.
So it has nothing to do with Python's functionality?
The other issue I have run into when working with python is source security. Most managers I have worked with are worried that writing in python would mean the competition would have free range over our source. For some reason they don't get this feeling from java though, which I've never understood. As for python's functionality, I'd say it depends on the product. Like every language, it isn't a universal hammer, but python is perfectly suited in some situations.
Less with functionality, yeah. A company that's been around for awhile has Java and .NET developers that may or may not know Python. Their choice is to stick to what their team is strong with, or force them all to learn a new language. It's cheaper to just stick where they're at.
dang...I'm just learning now and I know how Python job prospects are incomparable to others like Java, C#..... This is so discouraging. How TF can someone like that not able to find a tech job ?!
I can give one data point. We were getting close to trying out some Python, when we found out Python 3 is very incompatible with Python 2 (it's very fiddly to write programs that work in both), and many distributions don't have Python 3 yet.
I'm new to Python and the very first things I noticed when I got interested in it was : 1) Very, very scarce training offering. Of all places, I'm in metro NY and I finally gave up trying to find one and resorted to self-learning. It has since became clear to me that when it comes to learning Python, basically you're expected to DIY and it is totally doable. 2) Job prospects --- it's just pathetic and depressing compared to other "corporate" languages (i've always been a "corporate" dev). I have asked *"Why so little demand for Python?"* many times. Basically the reason is that Python is not a technology backed up by a trusty big corporations like Sun, MS etc. 
&gt; It's cheaper to just stick where they're at. I don't agree on "cheaper". I'd say "safer". The old saying "nobody gets fired for buying IBM" is, imo, totally applicable to Sun, MS.
Where does Google switching away from Python for new projects fit into this?
&gt;....would have free range over our source. LOL so there are still people with this mentality....morons
&gt; SAP (ERP) jobs are plentiful when compared to python. yeah and ironically they're boring jobs
is DropBox or Digsby open source ?
Google switching away from Python? Could you give me a link regarding that subject?
That's because not many people are using Python 3 yet. I'm not sure why that's such a deterrent for you. It won't be a concern for quite a while still, and migrating your own code is easy.
How is it cheaper to either: * Burn a bunch of developer time on retraining * Hire new Python developers and lay off older Java peeps New hires bleed money for at least 2-3 weeks when coming on board in terms of getting up to speed and the like. Retraining is time your developers are not spending working on whatever application they'd currently be working on. If you have a team of 5 developers at $50k a piece, and you spend even 2 weeks on retraining you've burnt a lot of cash up front. Now, we could argue that in the long run, you might save money by developing faster, and writing cleaner code. You could also lose some developers in the switch, which again, is expensive. All of this is potentially more expensive then just writing in whatever language you're currently using. Eventually, over a very significant period of time, the costs might average out, or you might come out ahead. But it's not like in 6 months you'd be saying "zomg look at all this money we saved!"
My first thought was, has he had his CV reviewed? I've been out of work for 13 months, but 4 years seems a little drastic.
Many large libraries seem to be finding migration hard, and I'd much prefer to use a language where a large-scale migration is going to be a necessity in the future.
&gt;....in the long run yes, in the long run, figuring in all the variables, Python should be cheaper. BUT, what I was suggesting is that it is not about money, but the "FUD" factor. Managers really don't care about spending company money but about their *jobs* 
It's sorta rumoriffic, but [unofficially](http://www.google.com/search?q=google+discourages+python&amp;hl=en&amp;sa=2) [substantiated](http://groups.google.com/group/unladen-swallow/browse_thread/thread/4edbc406f544643e).
I'm a senior architect for a medium sized software company with a large body of C++ and Qt code for Windows and Linux desktop application. I'm pushing to migrate to Python and PyQt with C++ and Qt since the majority of our code isn't performance sensitive. There is, however, a lot of pressure from management above to move to .NET and drop Linux support for the application. The lack of 'official' backing for Python is more of a problem than you would think, and there is a dearth of training opportunities compared to .NET and Java, so although its easy to get people up to speed on Python in practice, its difficult to demonstrate that this is the case. Hopefully, a year from now, I'll be creating demand for Python in the employment market...
I have been trying out the free version in the past few weeks. Question: Has it NEVER occurred to anyone at Wingware that it'd be a great convinience to the users to have a "file reopen" feature ? I meant how hard is it to add it ?
&gt; Many large libraries seem to be finding migration hard More like "time-consuming, but planned to be so". 2-3 years is the expected period for a lot of projects, and has very little to do with the complexity of actually porting code, because that's not the hard part. The hard part is setting up a deprecation schedule for older 2.x releases (most popular packages support 2.3 and up, or occasionally 2.4 and up) and managing that schedule carefully so your users know how it'll happen and can plan accordingly (many people are, for example, on Red Hat Enterprise installs which will be stuck with Python 2.3 until the year 2014).
I had one full-time staff position in 2008, but since I left the military in 2005 I've worked almost entirely as a contractor/consultant. With one exception, I've never been able to find a Python position her in Denver, Colorado. It's usually PHP for open source shops then Java or C# for the rest. Python is extremely unpopular from the perspective of clients &amp; management. Mostly because it is a complete unknown to them. One of the few projects I did get to convert over to Python was because I sold it "Python is to PHP as Drywall was to plaster &amp; lathe" and I also offered to forfeit 50% of my income if it didn't work out ( thankfully it did work out). 
I think the culture of Python (and other languages such as Ruby, Haskell etc...) is one of self-learning as opposed to training. No offence intended to anyone, but corporate devs are less likely to learn a new language/OS/library etc on their own time and initiative and expect their company to train them in whatever is needed for their job. Much of my corporate dev training consisted of approved courses, alphabet-spaghetti certification exams and massive expensive and mostly unread textbooks. Much of my startup and hobby "training" consists of reading blogs, IRC logs, forums and - god forbid - actual source code. 
As much as I like python, the forced indentation is probably its worst aspect.
Uhhh... _okaaay_...
sorry for expressing my opinion, i'll refrain from such absurdities in the future.
I think the forced indentation is one of the things that you start to appreciate over time and as a codebase grows both in size and contributors. That said, I always indent markup, and even this looks weird 
I'd like to respectfully disagree. This would make more sense if the indentation was only of value to the compiler and not to the programmer, but this is not the case. Proper indentation makes it much easier to read someone else's code. Given that Python actually enforces this (such that it won't execute correctly without proper indentation) it means that everyone (skilled or unskilled) has to meet a minimum code formatting bar to get something that runs. Braces are fine, I write plenty of C code, but because whitespace isn't important in C, there are also many coding styles that people use. This makes it more irritating the collaborate across multiple projects since one has to learn multiple different coding styles with indentation being tabs or spaces. I realize that there are varying coding styles found in Python projects too, but having a "right" way to do certain things like indentation means there's at least one stylistic difference one doesn't need to continually pay attention to. All this said, I'm not sure that this type of indentation is useful in the context of a markup language. Also, SHPAML has got to be one of the least attractive names I've heard for a language recently.
You got a lot of down votes for this, so it's obviously not a popular opinion. I'm wondering why you feel that forced indentation is so bad. In programming languages that don't force indentation, it is invariably encouraged, even to the point of adding automatic verifiers (e.g. checkstyle). Would you prefer block markers like braces or "begin" and "end" instead? Myself, I like the fact that something I do anyway (indentation) is rewarded by terser syntax (no block markers required).
It's fine if that's your opinion, but, for the love of god, _why_? Scripted languages without forced indentation suffer from terrible and inconsistent code styles.
I've never really understood that criticism of python. I've been indenting anyway for 20 years in Basic, C, Java, etc. Python just makes it standard and significant.
We should just drop HTML in favour of this terser syntax. Native SHPAML!
DAS STIMMT!!!
I like indenting and I indent my code and think it's good practice, but braces are easier to parse, and you don't end up with scoping errors if unintentional whitespace gets added or removed which happens often since lots of apps (especially browsers on the internet) don't seem to respect whitespace. I prefer the tabbing with curly braces but not enforcing the tabbing so you don't have to deal with those kind of whitespace annoyances when they arise.
That's fine, I didn't plan on starting a voting-up storm by expressing one of my annoyances with python in the python subreddit =) 
From a machine point of view, significant indentation is more complex than markers and I prefer simple things. Compare s/+s/++s/+++s to s(s(s(s))). It also kind of sucks in the REPL but that's more due to our tools, but that's a real problem. By being more complicated it makes assumptions about the editing tool.
The itools.ical [1] handler is quite good too. The itools.xapian [2] wrapper provide an hight level Queries API, this search interface is the same than the one for itools.csv [3] [1] http://docs.hforge.org/itools/ical.html [2] http://docs.hforge.org/itools/xapian.html [3] http://docs.hforge.org/itools/csv.html
Hi, I'm Al, the author of "Invent with Python". I'm glad everyone has found the book useful. Just wanted to note that the PDF version is a little bit behind web version (hopefully I can fix that this week.) Also, the book covers Python 3, not 2 (a lot of people have made that mistake) but there is an appendix that lists the main differences between 2 and 3. You can easily have Python 3 installed at the same time as Python 2. Thanks again for reading!
If you just want some experience writing programs, there are a ton of great programming puzzles at http://projecteuler.net/ I use the site whenever I learn a new language.
I was expecting to see yet another python templating language. It took me a while to realize... this is awesome!!!
Indeed... I wish HTML was like this from the start. 
That's reasonable. I guess I've been lucky enough to not encounter those problems. Actually I did once, when a co-worker emailed me some python code. For me the trade off is worth it to avoid the extra typing and the visual clutter of braces. In some blocks of code the number of lines dedicated solely to braces can reach close to 50% (especially with try..catch..finally). For some reason it just really bothers me when the code I'm writing is essentially simple but ends up taking a full screen. 
I'd like to see how this handles html5.
Thank you for providing the first reasonable argument against significant whitespace that I've ever heard apart from it sometimes won't c&amp;p correctly. That said, the reverse of your argument is also valid: significant indentation is less complex to read and write for humans... see jbs398's comments below.
I really wish this worked in Windows :(
I would have liked more markdown-type syntax for bold, italic, ul and ol. On the other hand, I can see him wanting consistency throughout, since doing tables almost certainly requires tags of some sort.
yes 
Given the title, I assumed it was inspired by HAML. In the FAQ, question one is “How is this different from haml?”
I dislike the inconsistency between those tags which do not require a closing tag and those that do.
Nicely done. It's impressive how such a complex figure can be created with such little code. And how quickly, too! When I tried this in the late 80s, I think each run took several minutes. And that was with a compiled language (Pascal, if I remember correctly).
Well, seeing as it doesn't seem to have any special handling for any sort of HTML (you can spit out XML), it should do it just fine.
Doesn't the current version of HTML require a closing tag for every element? I.e., &lt;p /&gt; instead of &lt;p&gt;, &lt;br /&gt; instead of &lt;br&gt;? I don't really know HTML, so correct me if I'm wrong.
If you read the front page, it explains everything.
Self-closing tags, i.e. tags which consist of only attributes and no child elements (whether text or not), can be closed with a final slash. e.g. `&lt;img src="image.png" /&gt;` Tags which have child elements need opening and closing tags: `&lt;p&gt;some text &lt;div&gt;more child elements&lt;/div&gt;&lt;/p&gt;`
No. XHTML requires every tag to be closed, including empty tags (so `&lt;br/&gt;` instead of `&lt;br&gt;`), but this is not a requirement in HTML.
wow that is just great !!
I could not agree more. The blog post would be great if he tested in network environment and with concurrent connections. With JMeter or whatever else. However I appreciate the post for being a listing of frameworks availible.
&gt; Hopefully, a year from now, I'll be creating demand for Python in the employment market... Me too. Although I'm not into FOSS and target Silverlight + DLR web services. My impression is that Jython/IronPython are stable and with Pythons moratorium there isn't much of a danger of runaway releases. However, integration with the big frameworks ( W*F, JEE, Spring etc. ) can still be messy and there is no attractive tooling for programming-in-the-small. 
&gt; Many large libraries seem to be finding migration hard ... because there is lots of CPython API code in those libraries. It is safer ( and simpler ) these days to use ctypes bindings or Cython which is Python 3 compatible according to its FAQ. Although I think you indeed have a data point, migration from one system to another one is not too uncommon in the industry. I don't envy my colleagues who programmed their GUIs in MFC and even those who used WinForms are lacking behind WPF/Silverlight. With 2to3 I plan a week at most for pure Python migration which is not a big deal. 
Industry demands are very specific and when they seek an MSSQL 2008 expert they get one. It doesn't matter much if you know database normalization up and down, have maintained MySQL and administered Oracle when they can hire someone who is trained on the spot for a particular product. When I went into business in 1998 it sufficed to have a math diploma, some programming knowledge and being smart. This is not something of interest for anyone these days. Notice that C# and Java alone won't make you competitive either. Knowledge of various frameworks, tools, servers i.e. a complete SW ecosystem is often required.
I really don't have anything against web2py. &gt; I should also mention that PyCon was using Django for the registration software but in the last two years they have been using web2py instead. When it comes to software to manage their money, they trust web2py. But that sentence there really bothers me. So would you care telling us which database software they trusted with their money instead of which alternative? And what about their choice in operating system to manage their money? Who manufactured the CPU for the server dedicated to the single task of managing their money? When it comes to software to manage peoples' money my bank trusts overpriced software written in .Net, SQL Server and Windows. I declare those are superior to every alternative out there. What's that? Someone wrote software to manage money in a language named after a troupe of British clowns? Who authorized that? The ministry of silly walks?! I really feel for you. It seems like web2py is a moving target for lots of people in the Python community. But, so is Django. The fact that you are provoked to respond to comments like that bothers me.
We will be looking into using the bpython.gtk_ part for a windows release (if it's possible, if it will be a lot of extra work) so that might be closer then you think :) And as you can see by this image: http://i.imgur.com/5GqIo.png it's almost there ;)
Hey mipadi, did anything I say below contradict your comment?
A self plug: http://github.com/skaslev/pyman (needs python3 and pyqt4 installed) The code is not that beautiful, but it uses several smart hacks to make it interactive although being written in pure python. 
yeah try and build that poll app with django (in the book)..ur learning curve will reduce significantly as u already know php
that's what allows it to be less verbose
Great news, and I see the import at least worked :) Will all the Curses stuff have to be replaced with GTK? There seemed to be a couple Windows ports of Curses, would bpython work if they were updated? 
Maybe a bit better (don't mind the color scheme): http://i.imgur.com/UoVD0.png but it now starts. Just to be clear, the curses stuff is still there, we also have bpython.gtk_ for gtk this we had since after 0.9.5.2. I made that now start up on Windows (together with Andreas). A windows gtk version won't be officially released with 0.9.6 but it might be with 0.9.7. The curses interfaces on windows all don't work 'well enough'.
I don't know about tools like this... Every web developer knows (whatever)ML pretty well, why use language with totally foreign syntax to generate the html? What are the benefits?
No...?
Please read that sentence again. You are reading in it something I did not say. I said PyCon used Django for the registration software (this is a fact). I said PyCon has used web2py for the registration software for the last two years (this is also a fact). I said PyCon trusts web2py for managing heir money (that is an inference, they would not use it if they did not trust it). I DID NOT SAY they used web2py because they trust it more then Django. I suppose they switched because they have some very special needs/requirements and the web2py community was able to fulfill them in very short time. I do not know for sure. Ask them. I should add in fact that web2py and Django play well together. They both run the same PyCon server (Pentium 4 running Ubuntu) and talk to the same database (PostgreSQL). Django handles the conference web pages, wikis, talk submissions, and web2py handles registration. Anyway, I just contributed to write some of the software. I do not run it, I do not manage, I should not be talking about what other people do or think. I was just providing an example where web2py is used in a production environment and has been used longer than 1 year. Banks use .NET and SQL Server or J2EE and Oracle for a reason. It is because they can find many users / professional support, and because they are used in many production web bsites. Interestingly this is the same argument that some people have used on reddit in support of other web frameworks vs web2py: you should use the others because they have been around longer so have more users, more sites in production, more support. I agree with you: more users does not mean better.
Cool, but the magic numbers made me frown. 
What's that quote about code being for humans, and only incidently parseable by a machine?
Wheres the song?
Funny, it took me no time at all to realize this was a giant waste of time.
Python is pretty good at a lot of things, but not great at any one thing: * It's pretty good for web apps, but php is easier to deploy and has more bindings to commonly needed libs. * It's pretty good for admin scripts, but Perl is better with regexes, strings, and also Perl has the CPAN. * It's pretty good for large-scale projects, but Java has more momentum, more libs, more big ide's, and has had (of course) *far* more corporate backing. * It's pretty good for embedding, but Lua is smaller and easier to embed. * It's pretty good for GUI apps, but so is everything else. Python is a simple easy general-purpose language that's pretty fun and has some libraries available for it. {shrugs} 
Anyone know what the state of the win64 build is?
&gt; We were getting close to trying out some Python, when we found out Python 3 is very incompatible with Python 2 Well, although Python has its problems, this issue you cite is actually a good thing. With Python 3 fixing most irritating problems with Python 2, Python 3 should be a pretty stable target for some time to come. &gt; and many distributions don't have Python 3 yet. You should probably be building your own Python 3 anyway. It's quite simple too. 
As far as I understand Google is NOT switching away from Python. They are just discouraging the use of python in cases where performance is critical. But from what I know that has always been Google's policy.
I didn't notice that pylons slowly dies, there are tickets closed, and you can easly use trunk version of it. Also keep in mind that separate projects that pylons reuses DO move forward (and those are often developed by pylons guys - like beaker), so its very unfair to say its dying.
Ah OK, it's just that I was downvoted, and I wanted to correct my knowledge if it was (incorrect).
Alex Martelli does speak interestingly on the subject of Python at least. I'm not a fan of Nutshell books, they always end up hollow. And Programming Python and the Nutshell books are both 3"+ thick and I think O'Reilly really should have separated out library reference and gui reference material.
wasn't sure how to get around that efficiently :(
The [Chandler PIM](http://en.wikipedia.org/wiki/Chandler_%28PIM%29) uses wxPython, I believe.
Nice find! That's the first time I've seen the changes laid out so succinctly. Of course, I'll still need to read the docs to understand half of them.
and for what purpose, exactly, again? As far as I can see this is aesthetic and a totally breaking change. If their refactor got rid of the GIL then maybe I could understand it.
The reorganization of the stdling is a good thing IMO, albeit a bit frustrating for package developers. Some things were just poorly laid out or poorly named in the Python 2.xx series (SimpleXMLRPCServer). As far as the stdlib is concerned, things all *look* much more standardized. You need to remember to separate the language from the implementation. The language is being overhauled, that will not likely magically improve an implementation all on its own (but may prove useful for helping implementations improve various things) That said, Antoine Pitrou's [new gil work](http://svn.python.org/view?view=rev&amp;revision=76195) has already been merged into the 3.xx branch. 
Getting string handling right. That seems to be the main motivation - moving from the mess of "regular" and Unicode strings to a clear conceptual distinction between strings (sequences of Unicode characters) and bytestrings. If you're going to make one major backwards incompatible change, you might as well make more, so they addressed a bunch of other issues and cleaned up a lot of accumulated cruft.
very useful doc! thanks man.
import numpy --&gt; *here be dragons*
Much of the same information, but in (what I biasedly consider) an easier-to-read format: http://diveintopython3.org/porting-code-to-python-3-with-2to3.html
I wrote [this](http://www.b-list.org/weblog/2008/dec/05/python-3000/) for people like you.
Most of that looks nice, but the new string formatting is sure to piss me off. I hope there's a module that will continue to support C printf formatting.
So are they firing Guido?
A lot of effort has been made to make numpy 1.4.0 work on 64-bit Windows. I've compiled it, using numscons, VS2008 and linking against MKL, and it works fine. Also the scipy trunk is working with the new numpy release. 
Thanks, that looks awesome.
In this case it looks like writing the code in C would have been much easier.
neat, I was precisely looking at that yesterday. I'm wondering though if it exists corpus about social networks. My point being that with twitter and the like, the way people write is quite specific and corpus such wordnet or words aren't always the best to map them.
Why use python over C? why use language with totally foreign syntax to generate the C-code? What are the benefits? Well, for one thing, I seem to get things done much more quickly..
you're using python 2.x, so you should use xrange() for iteration, not range().
See also [the response](http://peadrop.com/blog/2009/12/29/why-you-cannot-pickle-generators/), which points out the real reasoning behind the un-pickle-ability of generators, and a much better solution (write an iterator instead).
Ok, and a little introduction by me. While there have been numerous fixes to a lot of existing functionality (and some added functionality) the big thing of this release is the initial GTK functionality. A few days ago I showed a few screenshots of bpython working on windows: http://i.imgur.com/hfQip.png (actually, one of it not working, one of it working) Officially (and in practice as well) it *won't* run on windows. You can't start it, not even the GTK version. However, windows support for bpython.gtk_ is in the works for the next release. If you feel like you really want to run it on windows (and I'll write a lengthy blog something about this and do a few commits for this in the future so manual work is not needed) you need to comment out the line where sys.stderr is redirected. Keep in mind that this is just a preview of things to come for GTK. Otherwise 'big' changes are: - The hiding of attributes starting with a _ unless explicitly typed. - A themes section on the website (submits welcome!) - The removal of a lot of unnecessary globals in the codebase. Hope you will all like it. If you want to know more about bpython and what it is, you can visit http://bpython-interpreter.org and if you want more about making themes, there is some stuff at http://docs.bpython-interpreter.org/
I'm pretty sure the *gphoto* command-line tool would have be fine if the point of the article was to download and delete photos from the camera. I think the author wanted to "*control the camera from Python*".
i have never heard of that, not even in my reading materials. can you explain why i would use that instead?
by now his CV is probably covered with dust LOL
why ? you want his job ?
I can't even begin to fathom a pure python implementation of this. XML parsing + Pure Python = Scary.
Cool. Hiding all the underscored methods and attributes is quite a nice improvement. I posted a new PKGBUILD in the comments on the [AUR page](http://aur.archlinux.org/packages.php?ID=16805). The maintainer will probably update the official one soon enough.
This is pretty slick - would love to see something that can do Excel. Of all the documents I've created with code, not counting plain text, spreadsheets far outnumbered the rest.
how so? I did a little deal with Python over the weekend - using SAX to parse a document. I used the Python XML library that is built in - no problems and it was my first time doing that.
[http://www.python-excel.org/](http://www.python-excel.org/)
Right, it's the _next_ bit of added code that will make the advantages of python of C more clear... "leaving the last 10 pics on the camera" or "watching a motion sensor and then triggering the camera" (although chdk might be more useful for that part, I'm still going to try this code, since I just got ahold of the same camera :-)
awesome - thanks
lol do you think anyone is going to believe this lie?? anyone who's read Reddit knows it's impossible to parse docx files! you'd have to be some sort of genius! or have access to some super secret MS documents to understand the format!
Wooooowww....looks like I'll be replacing SPE soon. ;)
You want to use [virtualenv](http://virtualenv.openplans.org).
There is progress of sorts. Just about everyone is using Sphinx for new projects now. There's a PEP to add new setup.py metadata pointing to docs and browsable source, which will normalize some navigation. The actual package distribution is handled well by PyPI; it could really use some UI work, but no buy-in is required to do that. We could use a little standardizing around testing; there are already tools to make use of those tests (much like CPAN does), but we need a standard to go with. I agree that "we need CPAN" arguments are way, way underspecified; but when people actually talk about the specific features Python/PyPI is within shooting distance of most of them. It doesn't make it inevitable, I just don't think it's so terribly hard.
The ribbon thing is really slow when it draws, feels very flakey. Textmate Theme support would be nice.
Comment 1: I haven't even looked at the game yet, but please find a better place to host your code. Throw it on sourceforge, google code, anything that doesn't throw a million ads in my face and hide the way to download the files. Comment 2: I've played the game and it was fun. It's been a while since I've played one of those defender style games. I discovered pretty quickly that if you start with the ships exactly opposite eachother, you can survive indefinitely by just holding down rotate and shooting with both ships. 
#1: Sorry bout that, I didn't notice the ads since I'm running FF with ABP and NS. I'll host it on Sourceforge ASAP. Thanks for that. #2: Thanks. My friend and I discovered it was quite easy to continue doing that (especially if you have the better weapons), we tried a variety of ways to stop it -including increasing enemy speed/amount of enemies- but couldn't really eradicate it totally. It was tough to do without making it impossible to survive for too long. 
If the IDE can integrate with an external editor, I will use it.
If Google is indeed moving away from Python maybe the next Reddit article will be "Author of Python in need of a job!". What I'm saying is, Google has many Python experts on the Payroll. Ditching Python sounds unlikely.
In case you were wondering, it plays smoothly on my Aspire One netbook (1.6 GHz, 1 GB RAM), except the mouse pointer on the menu screen is laggy. It would be nice if you could change the game resolution (800 x 800), but overall it is a cool game :)
How fast FPS wise? The game was developed on my Uni's computers and they are quite good. The game was optimally designed to run at 60 fps (in hindsight, that was probably not a good idea). On my XPS M1330 (1.5GHz, 2GB RAM) it runs about 20 FPS. While still playable, it doesn't look as smooth. I'm not quite sure what is causing the lag on on the main menu, so I can't fix that as of now. Maybe someone can help me out on that (there's got to be experienced python programmers with pygame experience out there!). What do you not like about the game's resolution? That it's too large? As I said, it was originally developed on our Uni's computers (and were subsequently demoed on them) which run at extremely high resolutions so if it had any smaller it would have been too small. On my laptop (1280x800) is just doesn't fit on the screen -due to the top bar. I think making it fullscreen makes everything fit on screen, but we didn't have enough time to put in fullscreen toggling. Anyway, thanks for checking the game out! I'll be sure to take these suggestions to heart and if I have spare time to work on it again, I'll code these in (if possible).
Since the graphics card is pretty crappy in this netbook, it's getting 11 fps, but it still looks smooth to me. Perhaps it just looks smooth to me because I have never seen it played on a faster computer. The resolution of most netbooks is 1024 x 600, so 200 pixels are cut off at the bottom. Overall, the game is pretty sweet. 
I'll say!! A 6000 page spec implemented with a 200 line script? Don't make me laugh. I couldn't even see any reference to numbered lists in docx.py, let alone nested lists (which are a nightmare in docx). In my experience , it's a **lot** easier to work with ODF and save as docx for those fools who can't work without M$ products. It's a damn shame the ISO weren't able to prevent Microsoft stuffing the national standards committees with their their brain dead puppets, and giving this crap standard any sort of legitimacy. 
You did a very nice job, everything works great. Just make it harder ;) I sit there and spin holding the fire buttons in 2 player mode, and always win. Got past 1000 points. good job :D
Rewriting code is of course always an option just like "write C if Python is too slow". However, I don't find this as convincing anymore as I did 10 years ago. 
Except this is really a case of "if you needed to pickle it you should've written it as an iterator in the first place". Writing something the wrong way the first time doesn't give an excuse for refusing to rewrite it correctly.
Yea, you'll probably see it differently when you've seen it run at 60 FPS. Not to mention I'm almost psychotic about things like FPS in games... Ah... I did no know that was the usual resolution of netbooks. To be honest, when developing the game we had never intended to release it like this (well, my friend still doesn't know I've done this - he does check r/python so I'm gonna see how long it takes for him to see it) so we never would've thought about the resolution issue. Thanks, that means a lot though!
Haha, we tried to fix the spinning issue. But it was hard not to make it impossible for people playing non-spinning. 1000 points is not the best though! My friend and I have gotten much higher! At 1500 and 4000 points you get a new weapon! So that bear that mind (you can seriously spam the spin move with these new weapons) but you will get overpowered eventually. :D
http://docs.python.org/dev/library/functions.html?highlight=xrange#xrange and since you're doing that a lot in an inner loop, for some appreciable range sizes... try what the difference in speed is. i'm curious too. also, in mandel(), you can just `return abs(z) &lt; 2` also, isn't there a "set point" method? drawing a 1-pixel-long line for every pixel seems terribly wasteful to me. you could create a bitmap or image, change its pixels, then draw it all at once (with these: [bitmap](http://effbot.org/tkinterbook/canvas.htm#Tkinter.Canvas.create_bitmap-method), [image](http://effbot.org/tkinterbook/canvas.htm#Tkinter.Canvas.create_image-method)), maybe once every so many pixels. anyway, you got me to take a look at tkinter again and i might do something with it now, so thanks for that. edit: alright, i played with tkinter a bit, and it blows goat. nevermind what i said about bitmaps and pixels and stuff, because tkinter can only do it with slow-performing workarounds through PIL.
Very nice. I'll try it.
That's pretty cool - but I have to confess I'll be sticking with Netbeans as my Python IDE. Multiplatform, multilanguage, and FOSS. Even in the commercial realm though, I pretty much stay away from anything that is stuck to one platform.
haha yea tell me about it :P and yea i know, i cant help but noob with return statements, I always feel like i have to put True or False there. not sure why :(
Is there a Netbeans Python bundle? I see ones with PHP, Ruby, etc., but no Python.
&gt; 1: Sorry bout that, I didn't notice the ads since I'm running FF with ABP and NS. I'll host it on Sourceforge ASAP. Thanks for that. I suggest you don't go there. Sourceforge has very complex/confusing interface. Instead use [Google Code](http://code.google.com/hosting/) or [Firefly](http://firefly.activestate.com/)
Go to Tools &gt; Plugins and search for Python.
It wasn't that complicated, once I signed up and created a new project it was relatively easy to upload files. But I'll bear that in mind, thanks.
Can you give any insight on why you prefer netbeans over eclipse with python plugins?
fear the score! http://imgur.com/01O2B here it run at 39 min fps, 47 max fps, averaging at 40/45 fps for most of the time (Core i7 920 / 12GB RAM / ATI 5770 1GB) You know what would be a good idea for a game ? To port this to python - http://robocode.sourceforge.net/ - so we could write our tank programs in python... or perhaps create a communication interface between the original robocode (like send telemetry and take commands from a tcp socket) and we write our clients in python while the graphical interface still runs in java. Perhaps it already has networking support ?
The complexity of the UI is mostly with the tracker items and such (not much with project creation or uploading of files). But if it works you, then there is no reason to switch.
Ah right, yea I'm not even sure what those are. I just wanted a link people could download from. :P
Haha, nice leet score. That would be pretty awesome ported to Python. Maybe I'll make it my Summer project if I remember. :D
It's very subjective (but I think most of this stuff is whether people acknowledge it or not). I've worked with Eclipse in the past and had trouble with it. It didn't run well on my systems (very slow) and I had a hard time getting it set up the way I wanted. It felt too complex. At the same time, Netbeans had me up and running pretty quickly but a lot of functionality wasn't there. So I was all over the place trying all kinds of editors and IDEs. At the same time Netbeans made some great strides and so I ended up back there, very comfortable with how things work. I started out in Visual Studio - that probably helps explain it. But now I don't do much development on Windows. Though I like tools that will work in both so that if I do have to move over, things are mostly the same. I use Eclipse now for Android work. I use Netbeans when I can otherwise. I've been playing with F# and I'd like to be doing that with Monodevelop but I haven't found a way to get that started yet so I'm using Kate.
Great, thank you. I've used eclipse for windows python development and feel the same way; I'll try out netbeans and see how that takes. Much appreciated.
It is a pretty odd requirement for an IDE to integrate with external editors because the editor is the central workplace of an IDE. 
You can minimize it by double clicking on the ribbon tabs. Still flaky?
You can right-click on the editor tabs (or project nodes) and the Windows shell's context menu will pop up. I thought this was the best solution for integrating with external applications.
I won't use Eclipse because it doesn't properly support light-text on black-background themes. I can't code any other way, white background hurts my eyes, it's like staring at a light bulb. I am apparently in the minority but I grew up looking at terminals and my preference has always been for black.
In response to http://tarekziade.wordpress.com/2009/12/28/new-years-python-meme/
Nice little game. Just a question about the music: what software did you guys use to make it?
&gt;But it was hard not to make it impossible for people playing non-spinning. You could add a term to the calculation of firing rate dependent upon angular velocity?
Trade-off? I can't think of anything that would induce me to to work with docx. Out of interest I did run your example.py and ended up with an [invalid ](http://www.validome.org/xml/validate/) document.xml file. Error: Can not find declaration of element 'ns0:document'. Shouldn't that be 'w:document'? 
No huhu. If you don't like that and haven't tried it you may like Komodo Edit. Also FOSS and cross platform. It's really just an editor, but so much like an IDE that it blurs the lines a bit. It's made by ActiveState and they do have a full on commercial IDE - Komodo IDE. Komodo Edit is built on the Mozilla platform so it picks up the strengths and weaknesses of the platform. On my current hardware it was just too sluggish for me. But YMMV.
Why RAR instead of a more open format such as 7z or tar+gzip?
Played it for a bit and found the spin-spamming that everyone else did. I think you've got some pretty good ideas in there. I never had low health, but do you spin faster as the circle gets smaller? That would be pretty sweet. To fix the spinning problem, what you don't want to do is put a limit on what the player can do, like saying you can't spin and shoot for too long. That will feel artificial and stupid. I haven't looked at the code, but it looks right now like you're doing an even distribution and just altering how many and what type of enemy spawns. What would work is to adjust where the enemies spawn to defeat this tactic. If at random, all the enemies spawned on the right side, you'd be screwed if you were just spinning. Half your shots would be wasted. Something like that would be a much better fix than putting rules in place to say you can't do that. Let people do that, just make it ineffective. :)
Each IDE implements its editor differently to everyone else, and they are seldom as good as the best standalone editors. I would rather be really proficient in one, good editor and be able to use it everywhere. Also, there are of course other features of IDEs such as project management, debugging, refactoring support, intellisense, a REPL and so on.
Not everything can be broken down to flat and simple iterators. BTW in those cases generator_tools works accurately. It also works for nested loops and simple yield expressions but fails in case of more complex yield expressions like `x = yield a + yield b`. The reasons are complicated but documented.
* Python is good at providing a unified glue on a variety of libraries and systems which are written without any cooperation in mind: bindings everywhere and for everything. 
It's not my validator, it's not broken, and it's you that seem to be using namespaces incorrectly. docx uses at least half a dozen namespaces such as w, wp, w10, r, v and you can't simply replace them all with ns0 and expect everything to work. While you may have got MS Word to open your documents, other software (that adheres more closely to XML standards and even to Microsoft's own published standards) such as OpenOffice may not. My OpenOffice can read docx created by Word, but can't read your example at all ("General input/output error"). Of course the issue may well be something I have done wrong, but since you seem to think ns0 is an ok namespace for the entire document, I am now more confident that it's not me. The WC3 validator only checked that your documents were **well formed** (ie used the angle brackets, quotes and ampersand correctly) and not whether they were in fact **valid** XML in terms of a schema or DTD that checks the actual element names and correct location etc. Although the WC3 does offer a **validation** service, it is only able to **validate** a limited number of document types like HTML and SVG; for the rest it just does the very basic wellformedness check Sorry if I sound bitchy, but I am feed up having to work around Microsoft playing silly buggers with standards. XML is not even that difficult but they just never seem to get it right with any of their products including C#. They can't even do character encoding properly - "Microsoft UTF-16" is not Unicode. 
&gt; I have consulted Google, but most of the hits are mailing lists with troubleshooting questions. Welcome to the wonderful world of the PyQt4 community.
The closest I have found so far is the [diagram scene](http://doc.trolltech.com/4.3/graphicsview-diagramscene.html) example and it's PyQt4 equivalent.
On a completely unrelated note, I am trying out Eric4 with this little project, and aside from not being a fan of the editor font, so far so good.
I suppose we could have coded something in (say, if you spin continuously 3 times we slow you down or something). But we didn't really have much time to work on it. There were tons of other things we wanted to code in.
My friend's friend made the music. And it's great. We both love it. I'm not sure what he uses though. The game sort of butchers the music as it's got a different sampling rate, but to most people they won't notice it.
Because I only have WinRAR on this computer (running Windows).
Unfortunately, no. You don't spin faster when the circle gets smaller, we had thought of dropping you down a level to the most current ring but it was a bit disorienting with the drop and it was ridiculous at the smaller rings. Funnily enough, my friend had the exact same idea. We just had a little trouble coding it. We even determined quadrants for the enemies to spawn in, but alas we didn't have time to code a full solution. I think some of the quadrant code is in there (somewhere).
Yay, thanks a lot ikanobori!
Thanks. Worked for me after using some additional compilation flags (to force 32-bit on Snow Leopard) here http://www.mail-archive.com/pyqt@riverbankcomputing.com/msg19509.html Not sure it was worth the effort though. It looks promising feature-wise but feel slow and ugly on OSX. I'm back to IPython/TextMate.
7zip is free (and also runs on Windows).
Am I the only one who gets turned on by how sexy bpython is?
I know it does, but when I was compressing the files I only had WinRAR at hand.
Actually '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}' is a [namespace name](http://www.w3.org/TR/2009/REC-xml-names-20091208/#dt-NSName), not a namespace. A [namespace](http://en.wikipedia.org/wiki/Namespace_%28programming%29) is an abstract container or environment created to hold a logical grouping of unique identifiers or symbols (i.e., names), or as the [Python documentation states](http://www.python.org/doc/1.5.1p1/tut/scopes.html) a mapping from names to objects. Since namespace prefixes idendify the namespaces that elements or attributes belong to, I think it's perfectly legitimate to use them for that purpose in a discussion. Since the whole point of namespaces is to avoid naming ambiguity, it's not OK to use the same prefix for several of them.
Yes I saw them there in the script - what good is that? They ere XML namespaces not Python namespaces. They need to be declared in the docx document. Reddit is not your personal email - If you don't want to read it, don't. 
And you apparently haven't looked at the output. Only one namespace is declared in the document.xml: &lt;ns0:document xmlns:ns0="http://schemas.openxmlformats.org/wordprocessingml/2006/main" ns0:ve="http://schemas.openxmlformats.org/markup-compatibility/2006" ns0:wne="http://schemas.microsoft.com/office/word/2006/wordml" ns0:pic="http://schemas.openxmlformats.org/drawingml/2006/picture" ns0:w10="urn:schemas-microsoft-com:office:word" ns0:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" ns0:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" ns0:a="http://schemas.openxmlformats.org/drawingml/2006/main" ns0:mo="http://schemas.microsoft.com/office/mac/office/2008/main" ns0:m="http://schemas.openxmlformats.org/officeDocument/2006/math" ns0:o="urn:schemas-microsoft-com:office:office" ns0:mv="urn:schemas-microsoft-com:mac:vml" ns0:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" ns0:v="urn:schemas-microsoft-com:vml"&gt;
O snap. I'm gonna have to take a look at this. :) edit: The code, not the downmodders
I like the multiprocessing package: http://docs.python.org/library/multiprocessing.html
check out urllib3 It's built for what you're trying to do threading connection pools etc
Another option is [pycurl](http://pycurl.sourceforge.net/)
Wow, that's really handy. I'm still pretty new to Python, and didn't have any idea that Python supports this sort of syntax until I read this article.
I'd rather do it with multithreading as it comes with python. I've also heard of twisted.
urllib3 is a tool to use along with the multithreading module. The threads generate gets or posts to various urls and urllib3 optimizes your connection to the webserver by using the baked in connection pooling. http://code.google.com/p/urllib3/ as for threading the basic help provided with the python documentation regarding the threading module is a good place to start :p depending on the number of cores in your machine, multiprocessing maybe the better fit as strombom suggested earlier
Thank you for the expansion on your previous comment. Multiprocessing wouldn't really work because I want to distribute this to other users.
this is a pretty cool project, if this could be merged with ipython the sum would be pretty kick ass.
Im not sure how eclipse with Python plugins is not better or mord apropriate. I'm just throwing it out there. Having used debuggers, dark bg / light test themes , various levels of text coloring/highlighting and bulid tool handling in eclipse, i can vouch that it can solve most issues in these comments. 
Let me know when you've created an IDE which provides intellisense and visual debugging in an editor independent way.
Firstly, well done! The game looks really neat and something like this is a great accomplishment for 2 months of experience. Since you asked for a critique: * You use modules like initializers - they initialize some things for you and touch the environment (in this case, pygame). Modules should be used as namespaces which group related objects together. * Put all the stuff in BitDefence.py into a main function. You can then use the "if \_\_name\_\_ == '\_\_main\_\_': main()" trick to start it. Some use sys.exit(main()) instead of just main() and return the return code / an exit message from there. * You don't follow a good naming convention. These are generally used to denote things of some specific semantic use - for example, global constants are generally written LIKE\_THIS. See PEP8: http://python.org/dev/peps/pep-0008/ * Whenever you have variables which have a number on the end, you generally want a list instead. Or maybe the choosen name is just bad and it is specific enough to be separated. * You update the screen *completely* every run through your main loop. You should instead only update the sections of the screen which really are updated. See the group subclasses in pygame.sprite, they have functionality for that. You basically call "pygame.update(some\_group.draw())", update gets an iterable of every rect of the screen which should be updated. * You should use &lt;surface&gt;.convert() on every loaded image, it transforms the image format to some other, fast internal format. * Your notion of the base is inconsistent. What exactly is "the base"? I initially thought that it's just the complete thingy in the middle of the screen, but you also have the "spawn\_circles" function which creates several bases. You should think about what you want to represent and how to do it, and then name it accordingly. * You have many things which just completely "ripple" through your whole program. In other words, your program has [middle-low cohesion](http://en.wikipedia.org/wiki/Cohesion\_(computer\_science\)). Why does the base decide where it is on the screen, as you define "circlepos = (400, 400)" in it's constructor? That implicitly assumes the base knows how big the screen is. Why is that defined in the constructor, anyway? Is every Base essentially the same and has the same screen position? (Even if that's true, it's still not good to "hardcode" anything like that) Why does the base load it's images? Why does ut know where the images lie in the path? Why is a function named "shoot" defined in BitDefence, when that clearly is something a Ship sprite does? Why doesn't a ship sprite exist, why do you manage those manually? The goal of a program is to have high cohesion, low coupling and no duplication - that is, it's data should be explicit, there should be no unwarranted assumptions, no changes should be needed to rest of the program if something of the *data* changes, you should be able to decouple whole subsystems of your program (well, if it gets bigger), unrelated parts of the program should know nothing about each other (for example, an xml parser should parse data from files or any other IO source into the corresponding xml datastructures - the program which uses these data structures should not have to know where the data came from on the IO side (or only in a very abstract manner - say, you build a programming language and want the error messages to include the names of files or something like that - the part of the program which handles IO should then be questioned for a "state\_info\_string" or so). * Too. many. numbers. Nobody besides you knows why they are there. What does that "20\*" in shoot() do? It upscales the bullet yes, but why? Be careful with numbers. As said before, they may include implicit assumptions which will cause things to break if somethings need to be changed. If defined in the wrong place, they may tear apart whole concepts due to a cohesion-error. And they include simply no meaning by themselves since they are so general. * Bullet gets screen as a instance variable, why? It doesn't need it. Be sure to define responsibilities for objects clearly. No object should bear to many and they should follow the some cohesion principles as mentioned in the wiki article. * Duplication. *Never* copy code. If it's something you want to see in another part of the code, make it a function. Analyse where the functionality makes sense: Over the whole module? Only in a specific subfunction? Is the functionality really specific? Define the function in the right namespace: If it's only needed in a function, define it there! See if you can abstract the function: Does it do something general, or can it be seen as such? Look for patterns: Is the (maybe general) functionality expressable through common patterns, which are resolved by other function? See the map, filter and reduce functions for some really general helpers you likely will often find useful. * Your choosen names could generally better descripable. Try to catch the responsibility of an object. If it's too long, you likely have problems with your objects than your names. If it's too indescriptive, you may don't have a clear concept of it. At the very least, note what it's supposed to do and wait until the solution comes to you while you do other things and you get the "bigger picture" as it's developing. * No long functions. You said it yourself: The part in BitDefence is too long. * You seem to use nummers to enumerate some specific form of things. The waves, the numbers in the Base class. Do not do it that way! Other languages have enums for such a purpose, python unfortunately not. If you really need to enumerate something, you could use something like this: MY_ENUM = (FIRST, SECOND) = tuple(range(2)) Or with waves: ENEMY_WAVES = (BLUE, RED_FIGHTERS, HWG_SQUAD) = tuple(range(3)) if wave == RED_FIGHTERS: ... You could see if you can make that into classes. Make a baseclass EnemyWave and subclass it for specific waves, add methods which do the right thing. Maybe add a handle\_surface() method for specific calculations which make your waves truly awesome? You could expand your game with surfaces, which represent some sort of tiled maps - the waves could then determine by the tiles what formations they form, or so. That's the idea here. (Splitted because of the stupid 10k character limit)
* Do not hardcode important, dynamic information in functions - this will likely duplicate something too. I see it here in BitDefence.py. You define the enemy waves but hardcode the information which waves give the enemy delay of * Why not create a set() of those? And the enemydelay information shouldn't really be directly on that toplevel. Also, that whole line with the many "or"s could be better expressed with the any() function or an application of reduce(): [Code here](http://paste.pocoo.org/show/161337/) * All those "for i in range(wavenumber\*enemy&lt;number&gt;): are suboptimal. Firstly, you don't strictly need to overwrite the enemy variable - and it's used for something other firstly, too. So that is misleading. And you could that part in a loop anyway. A pygame Group accepts any number of items to add, so you could use a generate expression which you then expand: enemies.add(\*(sprites.Enemy(screen, number\_of\_enemy)) for number\_of\_enemy in xrange(generate\_number\_of\_enemies\_per\_wave(the\_current\_wave)))). Or something like that. You also have implicit assumptions here: Why is the number of enemies wavenumber \* enemy&lt;number&gt;? You should make that a function which calculates it. Your waves are getting more and more objects, do you notice? Then, the number is used in a different context in the Enemy class. And here the Enemy class itself generates their positions. It shouldn't. That is not it's business. * Thinks like "if my\_boolean == False" are not pretty. Instead of comparing to an boolean type and generating from that another boolean (my\_boolean == False yields another one, the if checks that then) either just put it so in the if in case of trueness (if my\_boolean:) or negate it with not (if not my\_boolean:). * rot.py has CRLF line terminators. Be sure to get this stuff consistent. Also, don't use a mix of space and tab indenting, can lead to bad things. * Pygame has the transform module: pygame.transform. It has stuff for scaling. You don't need to have 360 images for every possible (non-float) degree. But don't transform the same image many times over (lowers quality), transform a base image (say, the ship is vertical pointing up) on rotating. You rot functions again assumes something about the screen (width / height). * changeship.py seems to duplicate functionality. Why do you have two separate tables for the two ships? Because they start looking away from each other? Why not just have one image and transform that by 180 degrees and use one function? Do not duplicate anything. Be especially aware of functional and hierarchical duplication - some programmers have fallen prey to class hierarchies which were intended to represent some different things, only to learn that they are not so different in the end. * Make use of namespaces. If you have functionality which is not dependent on each other and which belongs together, use a new module. If things are not needed for many things but one function, place it in that local function. If some class needs another class only for internal use, make that class a nested class (dh, define it in the namespace of the mother class). Do not mix really much state of different nature in one namespace, like you do in BitDefence (all those little states like enemydelay or bombcharge) - ask yourself: Can you see a new object in there? Is there something strange in there? Can you pull it out? * Seek to centralize data. Especially names of files and things like that. If your game is growing, maybe you should try to put the data in some easy parseable format (JSON or YAML, maybe even XML, but I would avoid it for not really complicated things). But never overdo it! That leads to bad things as well. * Make use of the string methods. See str.zfill() for that one use in changeship which seek to load filenames with leading zeros. * Use python docstrings. They are great. Document much, but describe why you are doing something, not what exactly. If you need to document what you do, your code may be too complicated. Nobody wants to see a "i = i + 1 # i gets incremented" comment. Make sure you get the idea of what classes and objects are. My experience is that this idea is often not explained well. If your course uses a book, be sure that it is not shitty. Many python books are skewed. One example is the galileo openbook. Good books are: A byte of python, dive into python. Surely there are others, but those are the ones I know. If you have any questions, just ask. PS: Which license is it under? Why do you use rar instead of a better, more free format like zip, tar? Oh, and happy new year to you both. 
God god god, _please_ don't use RAR to archive this, it sucks! Couldn't you use something open? Tar &amp; Gzip? Good luck on the project, I'm firing it up now... [Edit] It extracts into the current directory, which is really annoying. Perhaps package it into its own dir then archive that? *[Edit 2] It's fun, but as stated previously it's too easy to hold down the keys for both guns and clear the screen. Read my [other comment](http://www.reddit.com/r/Python/comments/ak1sh/bit_defence_a_game_written_in_python/c0i3d68) on this thread about possible solutions. The opening menu is very confusing to look at and the words don't make much sense. What is 'Player 0' and why does 'Player 1' change to 'Player 2' when I hover over it? *Overall*: 3/5 - nice one!
Have you thought about implementing cooldown on the guns so you can't just hold it down you have to think about how you're firing and when? You could also/instead use a max fire rate or something similar so that the earlier guns have a slower bullet velocity and automatic fire (weird gun terminology, I've no idea really).
Wow, I'm not even sure where to start with replying to this (I'm sure my friend would like to respond to some of the things here too). Firstly, thanks for playing (and hopefully: enjoying) the game. Secondly, in a feeble and pitiful attempt at justifying our many mistakes in coding I'll say we were sort of thrown in the deep end with this project and Pygame in general. I'll attempt to respond to some of the things you've said: * We're still quite new to Python in general so we're still learning the ropes, we're more testing our limits then anything. * In hindsight, it is a good idea to put in a main function and our course professor actually said this, I guess we just forgot. * Wow, we didn't even know there were naming conventions like that. We just figured as long as it makes sense to us, then it'd be fine - although now, that sounds utterly stupid. * Ah yes, variables were an issue in some places. We'll bear that in mind next time and attempt to utilize them better. * I did not know that was possible. It probably would help for some things to not be constantly updated (the base, for example). * Did we not do that? Oops. We meant to have done - I'll have to check through the code again. Our professor even said to do so, quite explicitly. * Yea, naming in the code isn't exactly good, by any means. Personally I wanted less variables and such, but couldn't think of a better way of doing it. * I guess we should've took it's position from the screen size but forgot about it (the window size doesn't change anyway). But you're right, it's not good to hardcode something like that, I believe our ship's starting position is hardcoded as well which isn't good. Every base image has the same position because we made Pygame place it with the centre of the image. We attempted to split out the ship sprite and it's operations into another folder, but failed epicly. * There are quite a few numbers, we didn't think coding a game like this would be so math-heavy (turns out we were *so so wrong*). Most of the numbers probably effect the speed of the object. * Duplication does seem a problem and we did try to cut down on it (let's just say: originally, each enemy had it's own class until we compacted it down to one class). I guess we could have done some more on this, but simply didn't have the time. If possible, we really would've wanted more time on it, but deadlines were drawing closer so we focused on getting the core aspects of the game correct. * The BitDefency.py file is *waaaaayyyyy* too long. * Hm... I had never thought about counting the enemy waves that way. And your way does make more sense and allows more flexibility to expand upon it, but enemies (enemy spawning specifically) was one of the last things we got in before the deadline so I think we did alright just to get it working in a satisfactory manner. * We didn't know there were functions like that, we just went with what we know (which was using many "or"s, in this case). We didn't really have too much time to research other functions that would've helped us. Trying to get rotation working properly itself wasted about 2 weeks of our time. * rot.py was actually partially written by one of professors after we had some trouble working out rotation. So there are some inconsistencies we should have dealt with. My friend and I, use different programs to code as well which doesn't tend to help. * Originally we did use pygame.transform.rotate (we even tried rotozoom) but came across this weird "bumping" effect. At one point we had a professor, some pHd and several third-year students sitting around wondering why that was happening. We subsequently decided to just do the quick and dirty way just so it'd work, our professor also suggested it for better performance (since the program wouldn't have to rotate it each time) and better quality of images. * Do you mean why we have a dictionary for each of the ships? That's mainly due to the fact that each ship has it own set of images (so they'd be different colour). We couldn't find a better way to do it, so that was what we went for. Unless there's a pygame module that changes the colour of images, we didn't seem to have much choice. * We didn't know about str.zfill(), we'll certainly remember that for future use. * Ah yes... docstrings... to be completely honest a lot of commenting and such was done after we had finish. I guess we were just lazy in coding. And we did try to make the comments mean something, a third year had also said to us: "Coding says how, Comments says why." Classes and objects were only really touched upon in our course so far. So we were thrown into the deep end quite a bit, more will be done on more advanced things in Python the following semester. And finally: Er... I don't think it's under any license. And WinRAR was just what I had lying around at the time of compression (actually, I think WinRAR compresses to .zip files, but nvm). Happy new year to you too, thanks for the constructive criticism.
I didn't really think about the format I was using when compressing but I'll use something else next time, if I have time I'll reupload it with .zip files or something. We had thought about something like that, but ran out of time and we were lazy in implementing such a solution. The opening menu was meant to be a joke (of my friend's making, may I add). Since computer scientist start counting from 0 (we thought we'd only let people from our department play, and not put it on the web like this). I had told him to remove the Player 1 (which is Player0 when you're not over it) since we didn't even have that mode, but he was adamant for the joke to stay. At one point, Exit was "sys.exit(1)" but the font we were using didn't have punctuation so that went out of the window. He put "return" instead (like when you end a module, you return something) -which, when you think about it, doesn't really make any sense.
The menu's just really confusing. You don't have a clue what to do. Those less inquisitive than myself will just close the window down and delete the game :) Cool little project though; games are a perfect way of learning a language and programming concepts in general.
Here's an example of this strategy out in the wild: http://code.djangoproject.com/browser/django/trunk/django/template/smartif.py . I particularly like this because it's got a much cleaner style (no globals, more organized, etc.).
Again sorry about that, I blame my friend (he was doing the menus). :P And at time of development, we only thought we were going to demo it to our peers. Not release it on the internet.
Yeah no worries, just some constructive criticism. Good luck :)
...along with 1.2.1 and 1.2.2 releases. :)
Thanks a means a lot that people are playing (and actually critiquing) our game.
I'm disappointed at the lack of support for [Whitespace](http://compsoc.dur.ac.uk/whitespace/).
Impossible to do because Pygments performs whitespace normalization at a couple of places that are not controlled by the lexer.
If you're currently using Genshi I find [GHRML](http://www.ghrml.org/) a useful alternative. In fact, I thunk GHRML was older?
Would you consider defaulting to `$XDG_CONFIG_HOME/bpython/` instead of `~/.bpython/` for the configuration file?
I wrote a point of sale system for a tradeshow book store with PyQt3. From scratch first version deployed in 6 weeks (kinda found and fix bugs). Very productive environment, eric3 at the time for an ide and gvim. py2exe to deploy. Now uses zodb for the storage and added a kiosk and a print station (pyQt4) it was a fun project. the pos runs on windows because of the printers. the print station runs on linux because ease of printing pdfs on the command line. All the programs run on any platform unmodified.
I have accomplished something similar with the concurrence framework, creating a new Tasklet for every URL and taking advantage of their non-blocking HTTP module: Tasklet.new(httpclient)(host, path) Take a look at httpclient here: http://opensource.hyves.org/concurrence/http.html
Then you realize the guy who made the webservice is returning a system.data.dataset and you wish you could smash his face with your keyboard.
Would you consider creating a ticket for this (it's Freedesktop?) at http://hg.bpython-interpreter.org/bpython/issues as an enhancement?
Yeah, I suppose I should have done that first. I'll get onto that now. Freedesktop, yeah. [Here](http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html)'s the spec. It saves having a hundered different dotfiles in ~. A good lot of programs have started using it (e.g. geany, transmission, google earth / chrome, xfce).
Damn, I've been working for a week now on an equivalent of this post! Much as I hate SOAP, we use it at work for a lot of IPC stuff, to the point where me being a diva isn't going to see things changed... so I've had to do a lot of work with various Python SOAP libraries (both clients and servers) in order to get them to work with our existing infrastructure. TGWS is the only one on there I hadn't heard of; I've been using suds for clientside work and eventually gave up on the server-side because everything sucked so horrendously (soaplib was the most promising, but I had to rewrite chunks of its internals to get it working the way we needed, and the bugtracker seems sporadically dead). It seems, to me at least, like Python and SOAP is currently in a pretty broken state, and needs someone to sweep in and either clean an existing library up for primetime or develop something new. That said, I'll give TGWS a go, so thanks for that :).
That, and "error page" HTML instead of a HTTP return code or a proper fault. 
There's a bunch of us rewriting soaplib at github, nothing official as far as I know but there's been quite a lot of progress. Check out http://github.com/jkp/soaplib for the main branch. And if you have improvements to add, that would be awesome.
Wow, fantastic. The changes I made were largely a series of quick, fairly static hacks; about the only thing I did with any real depth was to create a better routing toolkit for the WSGI server (so that different services we created could then be hosted on /XService, /YService etc.). I've forked the project and will take a look what patches (if any, the other stuff I did was extremely basic ;)!) could be contributed if I get a chance at work tomorrow. It makes me incredibly happy to see this though! In the end, I switched off Python altogether and used JAX-WS under Java for the central part of the system (as making any serious changes to soaplib would take too much time, and I was already under pressure to stop yak-shaving), but I still need to be able to deploy a simple Python endpoint on various servers in order to replace the existing (horrible, and now deprecated) Perl architecture. I've been leaning towards a simple HTTP/JSON interface, but using SOAP would make the higher-ups much happier. Thanks for the info :)!
I used another one, suds, which is pure python and not talked about here. It was a pleasant experience. https://fedorahosted.org/suds/
I'm pretty sure they do mention suds. Check out the part near the end where they talk about needing a client for the QA team.
not sure about python, but you can try a recursive crawler instead of those fancy multi-threaded ones. Here is a recursive perl crawler http://codediaries.blogspot.com/2009/11/how-to-write-simple-recursive-web-and.html
I don't know of any, but I'm on [Hawkhost](http://www.hawkhost.com). If you want, you can send me the source and I can try to run it and let you know if it works. if it goes well, Hawkhost gives you basically everything for about $3 a month.
[Webfaction](http://www.webfaction.com/) supports tornado hosting, and their basic plan is pretty cheap. I use them for some django, cherrypy, and wordpress apps, and I'm satisfied.
Webfaction looks promising. I'm not quite ready to launch, but I'm definitely leaning towards webfaction (unless someone else has a better suggestion).
I went with [linode](http://linode.com) . You just get a vm slice to do whatever you want (I wanted to run a custom twisted server as well). It starts at $20 / mo.
When [prgmr.com](http://prgmr.com) gets some more hardware, I'd highly recommend them. Their cheapest yearly solution is 128mb ram, 3g disk space, and 20g transfer / month and costs $57. It's mostly do it yourself, which is why it's so cheap. You'll probably end up spending more if you use a shared host.
I just signed up with Linode because of a snag with Tornado on WebFaction. Their version of libcurl (which at least the Tornado http client needs) is quite old and incompatible, and their recommendation was to compile your own from source. I had already been a bit frustrated by this type of thing before with WebFaction, and it was the straw that broke the camel's back, although I admit that on its own it's not a huge issue. I was just tired of either 1) being at the mercy of the sysadmins, or 2) having to maintain some 20 line cobbled-together build script just for one system library. I'd rather just run my own server and keep my own OS packages up to date. That route is of course more complicated in *other* ways, but at least my hands aren't tied anymore, and for the most part I can just `apt-get install` and `apt-get update`. I don't begrudge WebFaction, though. They're doing the best they can with the style of setup they have. The semi-shared model that they have is the only way you can get *good* $10/mo hosting. Linode starts at double that. But Linode is awesome. Browse their site and read some reviews -- they are very popular and well-liked for good reason.
It might be a bit better when accessing variables ( my.x ) but when reading function definitions it would get a bit confusing ( def do( my, x, y ): ).
That seemed like an awful lot of work.
I second this ive been on prgmr since that post on reddit about them no real major issues so far
object.__new__(type(the_singleton)) Types? That doesn't seem pythonic? Does it?
Python has a great type system.
I prefer my bikesheds in green.
Very cool link. This is just what I was looking for. I really like how the author includes Appendix A to help me sort through differences between 3.0 and 2.6. Edit: You will probably cause me to waste away the rest of my day on this stuff. Thanks a lot ;/
For singletons, I've learned to use modules
Why is IronPython support invalid syntax?
Good point. I guess that's a bad trade off.
I don't agree that I'm bikeshedding here. It's not like I emailed the python mailing list to argue that my way is the right way. Instead, I posted on reddit and stated I wouldn't even make this change, myself. Isn't reddit (in part) a place for completely useless musings?
Webfactions works, but It's little difficult to install required pycurl. I compiled from source curl and had to upload a previous compiled pycurl egg.
It's not clear from the posting whether he was trying to use a generic type in IronPython as a decorator, or trying to use that syntax in CPython (which has always been documented as not going to work). I don't believe IronPython extends the Python grammar in any way whatsoever (indexing of generics is not grammatical)
SomeType[index] is perfectly valid syntax in CPython. Using that syntax as a decorator unfortunately isn't; neither in IronPython nor CPython.
You can! *self* is just by convention. You can name the first argument as you like.
For $6 a month I may have to try prgmr.com. If its performance is satisfactory I'll extend it to a year long contract. If it doesn't work out, I'll take a look at webfaction and linode.
App Engine?
Yes Python object system is, I believe, one of the most complex object systems ever existing, on par with C++ when you throw in both overloading and inheritance (but before you add templates and [SFINAE](http://en.wikipedia.org/wiki/Substitution_failure_is_not_an_error)). Is there a text that explains it all in one place?
Man I love python.
My main application is actually written for App Engine, but it has a need for long polling which isn't supported by App Engine. The Tornado application I've written works with my main application to provide long polling capabilities.
Dynamic typing does not mean you should ignore types and pretend they don't exist. It just means you don't need to do all the rote bookkeeping up front. Python has a great type system, and you cannot be good at python without understanding the types well.
If you just want to know what can be done with it, the doc that comes with Python is pretty good. But if you really want to understand what's going on under the hood, the only relevant texts I can think of have names ending in ".c" or ".h".
I wish Python let you just specify a number of times to repeat a block of code, without generating objects to represent the iteration counter (which you don't always need). It'd be cleaner, too, to just say `do 5 times:`.
Just don't use supercalifragilisticexpialidocious ... it might confuse you with the super in the front
is PIL better or worse than GD for image resizing etc. Most clients I have that use image resizing / cropping / etc all complain that GD pixelates images after a certain threshold. Usually I install imagemagick and with 0 configuration the problem goes away. I assume that these pixelations/distortions stem from the fact that PHP (and I assume Python) are interpreted/dynamic languages unlike the core of imagemagick which I assume is C/C++ so imagemagick can do the math quicker and to a finer approximation insanely quicker than php/python ... so if you're on a shared host or even something remotely demanding why use PIL/GD for the processing ... you'll invariabley end up with some sort of message-passing/queue/job-queue sending work off to a C/C++ coded app like imagemagick. /rant
don't understand TFA...it's over my head 
I gave a talk on this at our local users group a few weeks ago, the slides are online if anyone's interested: http://wiki.python.org/moin/MelbournePUG?action=AttachFile&amp;do=get&amp;target=promise.odp
Really cool. This should really be part of the language and be actively supported, it's one of the most promising ways to improve speed IMHO.
Or at least part of the stdlib
Thanks
That's what I meant of course :)
Looks promising!
Agreed. This is a fascinating and promising feature, another reason to love Python (and decorators too)! 
This may be seriously awesome. Can anybody do some benchmarks on real world code to see if the gain is noticeable? The docs are a little sparse in this topic. What would be cons of using this? 
Are there any benchmarks on this? However cool it might seem, I want to know if it's worth it, or if I should compile my module with Cython right away. This has the advantage of being pure Python and a module, but I would love some benchmarks...
Check out the slides. They are filled with examples and benchmarks. Very impressive work.
Saw this at last year's OSDC. I said it then, I'll say it now: very cool stuff Ryan :)
I imagine that the pixelization problems are issues in GD itself...and have absolutely nothing to do with the interpreted/dynamic nature of PHP or Python.
I will, I don't have OpenOffice, though. I'll open it on the netbook, thanks!
I clicked on the link to learn wtf a GIL is and found no explanation. Here's more info: Global Interpreter Lock http://docs.python.org/c-api/init.html#thread-state-and-the-global-interpreter-lock
Sorry - I linked straight to the blog's home page (DOH!). Here's [the article proper](http://www.dabeaz.com/blog/2010/01/python-gil-visualized.html).
I've always wondered if something like this could be combined with just assuming that things like __builtins__ are constant, but triggering a recompilation if that ever turns out to be false. That way, you retain python semantics of late binding and the ability to hook into builtins etc, but gain the performance benefits for the common case. ie: sometuple = (1,2,3) def foo(): return len(sometuple ) would set the current bindings of things like len and sometuple as function constants. The trick being that it also registers this function as interested in modifications to "sometuple" in the current module namespace, or "len" in the builtin namespace. If anything modifies either of these values, then all functions which have registered themselves are recompiled (or better, marked as stale so that they get recompiled next time they're used (eg. swap out their bytecode for self-rebuilding bytecode)). This paves the way to further optimisations (eg foo could be eventually optimised to "return 3") It trades a small cost paid every invocation for a large cost paid very rarely. __builtins__ are virtually never overridden, as are most functions and constants defined in modules. Some heuristics to spot cases where module variables *are* frequently changed should be sufficient to make it a net win (eg anything declared global in any function in a module could always be left late-bound, as it's probably something that may be modified in the normal course of affairs). I had a stab at a pure python approach a while back, but it wasn't *quite* possible. You can hook into external modifications (by replacing modules with proxy objects that define `__setattr__` etc.), but in-module modifications were trickier (ie through a function setting a global variable), and the proxies probably cause a performance penalty themselves. On the other hand, from a lower level perspective, modifying the module object at the C layer to be more sophisticated, with support from the compiler, might pay off.
I knew the GIL sicked, but not that it sucked so much. Now I understand why the new process model for concurrency was created.
And.... nobody has the balls to deconstruct the GIL? has someone tried?
Yes, they have. Adam Olsen for example maintained the Python safethread project which completely removed the GIL. However, it incurred a 50% performance hit for each thread. There was a similar attempt about a decade ago that hit similar numbers.
It can be done, but it would slow single-threaded Python, a compromise Rossum won't make. Eventually it may be possible. Using multiprocessing if you want true parallelism (with less sharing). 
Same purpose as [fapr](http://www.reddit.com/r/coding/comments/a10t2/fapr_a_lightweight_image_viewer_i_programmed_in/)? :)
Yes. Many people have tried. Many quite intelligent people, in fact.
Of course people tried. The problem is, to put it simply, that when you change single (Global, duh) lock into more granular system (lock per object or similar) you end up doing a lot more locking. Thus unacceptable performance hit for single threaded code.
The first software that comes with the recommendation: **You don't need this!** Funny
Tried it out this morning on real work. Here's my blog post: http://panela.blog-city.com/the_promise_of_faster_python_1.htm Long story short, I'm getting 1-2% improvement whereas psyco gives me ~15%. I need to tweak more.
It's worth is in very specific scenarios - as you point out, there's some advantages to being a pure-python runtime module. However, a Cython implementation or a decent JIT like Pysco or PyPy will destroy this little module for raw performance. I like to think of promise as a way to avoid uglying up your code. Where you might be tempted to do something like this to work around various idiosyncrasies of the python interpreter: items = [1,6,8,9,3,4] def stupid_but_important_func(len=len): l_items = items for i in xrange(len(l_items)): yield i + l_items[i] You can instead let your code read how it should read and make various promises to get it running faster: @promise.pure() def calculate(a,b): return a + b @promise.constant(("calculate",)) @promise.invariant(("items",)) def stupid_but_important_func(): for i in xrange(len(items)): yield calculate(i,items[i]) By far the biggest benefit of this module is the ability to inline pure functions; people are often surprised just how expensive it is to make a function call in Python.
This software started as an idea for a lightning talk at OSDC last year, and the code grew up around it - so it's mostly an academic exercise that happens to have some practical benefits. If you think you *need* this module, what you probably need is a real JIT like psyco that can give a much more impressive speed boost. But it's a fun idea to explore nonetheless :-)
Could the locking mechanism not be made into a system whereby the program running can specify what type of locking it requires? I don't know enough of the Python internals to know if I'm making a retarded statement here, but it would be nice if single-threaded apps could tell the interpreter that they need the performance improvements of a GIL, and multi-threaded apps can use a granular system instead. import GIL? :(
Were talking about CPython internals -- C level change in the interpreter itself -- please bear in mind that this would require keeping two versions of the interpreter side by side. Try to think about the implications for extensions/library authors. It's important to acknowledge that GIL is not a part of Python language -- JVM and CLR implementations are GIL-free. This is just a implementation detail, that could probably be dropped if we could reimplement CPython from ground up, which will not happen. Why? [Good although short note by GvR](http://www.artima.com/weblogs/viewpost.jsp?thread=214235). 
That's very interesting, especially when Psyco doesn't work (64-bit machines) and Cython is overkill, I'm sure it'll come in rather handy, thanks!
1-2% doesn't sound absurd for fairly trivial sounding byte-code optimisation.. I guess in certain circumstances it could be higher, but this does more like an interesting idea than a useful one..
Spot on. Most of the "big win" optimisations you can get with psyco (e.g. integer arithmetic) are beyond the reach of this module, since it can only work at the level of Python bytecodes. But in certain well-targeted cases, caching a few variables and inlining a few function calls can give a surprisingly big win.
but can't the bytecode actually written become GIL-optional for single threaded applications? it seems pointless (to me anyways, which isn't saying much) to require a lock when there's only one thread of execution anyways.
so, it's been about five seconds, what's your opinion on the GIL? Should it be abolished or eliminated?
Enough already with the GIL. Seriously, if you think it's a problem, 99% of the time it's because you're doing it wrong. Split up your task into multiple processes. This is good design anyways since it lets you split your task across multiple boxes. Doing web stuff? Then use apache w/mod-python or mod-wsgi and mpm-worker. It's a very nice solution, all the goodness of threads while letting apache's process model and mod_wsgi's interpreter spaces sort out all of the scary stuff. On a multicore machine and using threads just for clean code design? Then get a real OS and partition your CPU's using cpuset. Get rid of all of the ugly red stuff (albeit you're running on a single core). PyPy is the future anyways. All of this drama over the GIL is wasting more cycles than the GIL is itself. Worry about the shit that matters, like fucking 2.7 forks. What's next? 2.8? Fuck!
Take a WILD guess...
ymage uses the "pyglet" GUI, which has some really nice ideas.
&gt;Another possible use of this module is the calculation of partial derivatives of mathematical functions. What has that got to do with uncertainties in values?
I don't see what that has to do with [this](http://en.wikipedia.org/wiki/Partial_derivative). Or are we talking about some kind of numerical approximation procedure? If it had said "numerical integration" I wouldn't have a problem.
too bad ± cannot be used as an operator in Python.
Very cool.
I sudden have the inexplicable urge to play Portal.
Holy fuck this would have saved me **so much fucking time** last semester in physics lab.
For what it's worth, I read that as 5 plus negative 0.3. Very cool package, but the string representation could use some work. Edit: maybe 5 +/- 0.3?
You may want to look into Interval Arithmetic.
Python 2.6 and onwards defaults to Unicode, so I don't understand why he didn't just use "±"
a few performance tricks: 1) dump objects in favor of simple lists or arrays for your board representation. Board pieces should be values that are OR'd together, i.e. BLACK=1, WHITE=2, KING=4, QUEEN=8, etc. 2) don't copy the board state -- your move generator (that generates possible board states for your minimax function) should be able to undo moves that were just made so that you only need one copy of the board in memory. 3) Find an evaluation function and minimax code that you already know is fast. Why reinvent the wheel here? I've written a program called [Raven Checkers](http://code.google.com/p/raven-checkers/) that works with these ideas. Good luck.
Because you can't find it on a regular keyboard?
This is for the printing. I've done Python apps saved in Unicode format (for French language), which could easily print special characters. Admittedly, it is a bit more difficult to input the characters from Apple's Terminal program, although other implementations might allow it.
I have no trouble outputting Unicode to the terminal. The one-line program: print "hello ±" printed exactly as expected, even though I forgot to put the Unicode switch at the beginning. This might be a problem if you plan to use the program as input to other commands, although I doubt many of them would know what to do with `2.0+-0.02` either.
"appear blocking at the source code level." &lt;-- this means they are blocking. Blocking does not necessarily mean that it uses threads to block.
try http://www.qtcentre.org/forum.php site, particular the forum is very useful. Code may be in c++, but easy to understand.
Most VPS's cost about 1$/10Mb of memory and as long as you don't hit CPU/bandwidth you shouldn't pay more then 30$/month. As additional justification for the cost, it makes life a lot easier having even a virtual server plugged into the internet (vs DSL, cable, or basic t1 ).
Also, check out [gevent](http://www.gevent.org)
Very useful. Here is a [similar thing implemented in C++](http://code.google.com/p/fermiqcd/source/browse/Libraries/mdp_measure.h) It part of larger library that is about 11 years old but I updated the date in the code recently as I moved to mercurial.
*Just* looking at the example, it could go either way. It's pretty clear they are adding "waiters" (deferred networked actions) to a collection, then calling `wait()` on each one in sequence. If `wait()` is blocking, then it is blocking io that appears non-blocking; if `wait()` is non-blocking, then it is non-blocking io that *sort of* appears blocking. I think that when they say it "appear[s] blocking at the source code level" they are referring to the fact that the deferred function (in this case `fetch()`) uses blocking io: `urllib2.urlopen(url)` Which means as far as I can tell, this is basically a glorified thread pool with networking semantics, since the waiters and pool don't seem to actually do any io, just schedule/manage the functions which may or may not do io. So this library isn't blocking, but neither is it a networking library. If I'm wrong, I think they need a new example.
this title is one of the most accurate titles ive read on reddit. also, eventlet is great.
just read about it a while ago - performance approaching tornado.
&gt;&gt;&gt; import fleetdb &gt;&gt;&gt; db = fleetdb.FleetDBClient("localhost", 3400) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "C:\...\fleetdb.py", line 49, in __init__ self.connect() File "C:\....\fleetdb.py", line 58, in connect self._socket.connect((self._host, self._port)) File "&lt;string&gt;", line 1, in connect socket.error: [Errno 10061] No connection could be made because the target machine actively refused it &gt;&gt;&gt; 
The [quantities](http://packages.python.org/quantities/user/tutorial.html) package may also be useful for your physics lab. It handles uncertainties also, albeit a lot less elegantly.
So this uses the same kind of non-blocking ideas as Node.JS? /noobie-question
I thought the very same thing at first glance. The example looks so familiar you could almost replace the eventlet names with standard Thread/Queue classes and achieve the same result.
Thanks
Not sure I understand your comment here. Looks like that line changed from what's on GitHub. Should be self._host, not self.host. Is your FleetDB running on port 3400?
hi, let me explain... First I'm learning Python so my purpose, out of curiosity, was to look at your code and see what it does. I had zero idea what FleetDB is except a fleeting look at its homepage before I ran your code. Since you didn't mention anything (not that I could find) about **"requirements"** for running your module, I just followed your instruction: "import fleetdb; db = fleetdb.FleetDBClient("localhost", 3400)...". Then i went to FleetDB's page and read a bit more about it and realized it's a separate software that needs to be downloaded and loaded first. that was my "facepalm" moment. So basically it was my ignorance to assume that your module was "ready-to-go-all-by-itself" (in my interpretation). So then I followed FleetDB's instruction and [got this failure](http://paste.pocoo.org/show/162798/) So I emailed FleetDB's developer and he responed: &gt;I haven't tested FleetDB on Java 1.4, I expect its not compatible with that VM. Do you have 1.5 or 1.6 available? So I guess i will get Java 1.6 and try again sometime. 
Here is [Guido's tweet](http://twitter.com/gvanrossum/status/7464557748) about it.
Locking is still necessary in a preemptive multitasking environment.
This will be really good, if it gets more people interested in moving to Py3k. The biggest hangup is still libraries, though. We can't move until SciPy does, and it doesn't sound like that's going to happen any time soon. 
Here is a [thread](http://mail.scipy.org/pipermail/numpy-discussion/2009-December/047123.html) from December 2009 discussing merging in Py3k support to NumPy.
Very nice. I've been hung up on Python 2.x series because of libraries but if this truly comes around then it might be worth porting some of them myself to get them updated. Obviously some of them would be easier than others but I've run into a few situations where the 2.x branch can be kind of slow.
Does anyone know if this adds a necessary run/compile-time dependency on LLVM if the JIT is disabled? ([Michael Foord's post](http://www.voidspace.org.uk/python/weblog/arch_d7_2010_01_02.shtml) indicates only the JIT requires C++.) Adding in C++ adds some difficulties for a complete static linking, such as is necessary to avoid *all* dlopen calls. This is required, for instance, on Compute Node Linux where there is no dynamic loader.
That certainly has potential for us. Question: Suppose I have some code that I want to execute whenever Python starts. I might put it into sitecustomize.py, but I cannot alter the standard installation. Instead, I use `virtualenv` as you suggest. How would I create this new environment with this new code to be run always? Would I have to alter `sitecustomize.py` by hand?
I don't fully understand your comment. I think you must be assuming that one can always edit one's Python site-packages directory. I do not have that luxury. Installing a package is the hardest part for me. (iab suggests `virtualenv`, which has potential for me.) Ruby gems are much easier to work with. If you are talking about *creating* packages, then I don't see what's wrong with `distribute` (aka `setuptools`). That's much better than anything I've seen in Perl.
If you pass --without-llvm at compile time it removes the C++ (and LLVM) dependency entirely.
Erm, this was the exact stated aim of the project from the get-go. It's all there on code.google in the project plan.
They even work at the same company as Guido and asked his advice on which parts of the guts could and could not be changed. That is, there are a lot of ways of making Python faster by assuming there’s no ultra-bizarre dynamism but according to one of the presentations I saw streamed on the web Guido more or less told them they weren’t allowed to change it.
You are incorrect, urllib2.urlopen is not blocking, which is clearly stated in the example you cite: # this imports a special version of the urllib2 module that uses non-blocking IO from eventlet.green import urllib2 Additionally, it supports [wrapping the socket module](http://eventlet.net/doc/basic_usage.html#using-the-standard-library-with-eventlet), as does [gevent](http://www.gevent.org/intro.html#monkey-patching).
Amazing that this comes from the same guy who doesn't seem to like all the FP-supporting stuff. :/
&gt;`for i in xrange(len(items)): yield calculate(i,items[i])` Ewww. `for i, item in enumerate(items): yield calculate(i, item)` 
Ok, sounds like maybe this Python client library isn't for you, since you're not a FleetDB user. This thing will have a pretty small user base I'd think, given that most people will connect via Clojure directly.
I don't see the reason to not port it to python 2.x also.
Other than they are hailing it as a defining reason to get people to move to 3k ?
yeah yeah, for example purposes only :-)
So there was no stick to make lib devs move to Py3 and now we get a carrot stolen from Py2 field? Awesome. ,,Unladen-swallow is Python's 3 best feature?'' This is both sad and offending at the same time. Why not do both 3.x and 2.7? This way we would be able to continue working with the version that has relevant libraries and JiT. Now we'll get another feature (next to new-GIL) on not-yet-ready-for-prime-time version. Sigh. This sucks donkeys balls for me. I've been playing with Py3 since first betas and I've liked it. But I need a version I could use at work. Version that comes with libraries. This currently means 2.6 and possibly 2.7 in the future, not 3.2 scheduled for end of this year. It's great that 3.x will get JiT and new GIL, but will that make things like NumPy or Twisted move to it faster? I don't buy that, sorry. Instead that we get Py2.x to stand in the backyard waiting to be shot in the head.
Haven't you heard? Python needs to die or people will not move to Py3. This is why.
If you want more people to switch to python 3, how about making it easier to port libraries to py3k? I tried porting a smallish C module a few weeks back and found almost no guides, no tips, and no sensible default ways to emulate old behavior. Also, they've made breaking changes between 3 and 3.1, so some of the articles have misleading information in them. Having 3 different names to the same language didn't help my googling either. Also, and this is unrelated, what happens if ubuntu (for example) decide to make the 2.x version of Unladen Swallow the default python in the repository? It's not like python.org is the only place people go to get their python distribution.
2.7 is being released this year, is currently in Alpha, and isn't going anywhere. Unladen's merging with 2.x would predicate a 2.8 release, which you still wouldn't get for at least a year (the OS vendors ramp time). 2.7 isn't going anywhere. 2.7, since the release of 3.x has been the planned/intended end of life of the 2.x branch. Bug fixes, security patches will still be applied and shipped, but active feature-adding/improvements will stop.
Ok, so Python 2.7 is frozen and unladen-swallow, that is a non-breaking, compilation time enabled, optional change to the interpreter will not make it in, savy. Tell me this though: actively maintained libraries compatible with 2.6 will most likely ship their 2.7 versions near 2.7 realease date or soon afterwards. When will I be able to say that about 3.2 releases? Early spring 2015? I'll have a hoverboard sooner, I'mfraid. EDIT: I mean of course ,,all yay, python 3k.''
&gt; It's not like python.org is the only place people go to get their python distribution. It's where Ubuntu (and other Linux vendors) go to derive their Python distributions.
Yeah, I don't really appreciate the whole Microsoft-like "we'll only release certain features with our new product" approach the Py3 developers seem to be taking. Seems arbitrary. On the other hand, developer resources are limited, and maintaining two code paths is a real bear. From my view though, a lot of these problems are of their own making. I never did see a compelling reason that Py3 wasn't just another 2.x version ... the changes just aren't that great for all the drama that's ensued over the new version. That's really unfortunate.
The Python 3 situation gets more and more ridiculous. Why are people looking for carrots when at the same time these people are saying it's not ready and you shouldn't move? They are making it up as they go along. It's a debacle.
Yes - they weren't allowed to change the semantics of the language.
The chances are that 2.7 is the last version of Python in the 2.X series. So given that Unladen Swallow is too late to make it into 2.7 that leaves Python 3 as the *only* option for merging. Also merging is difficult and I guess they only want to do it once - so again that means merging into Python 3 instead of Python 2. There is nothing stopping *you* merging with Python 2 of course. No need to be offended by how *other people* choose to contribute their time (or in the case of google their money).
Lots of the changes *couldn't* be introduced in a backwards compatible way - especially the string / bytes changes.
It's only a drama if you're looking for drama. If the libraries you need are already ported to Python 3 then Python 3.1 is a fine choice for development. In the meantime, as expected, the Python community are gradually migrating libraries to support Python 3. The whole process will take a long time, as was said right at the beginning. In the meantime the internets does love its drama and debacles...
Because supporting unicode identifiers is not the same as allowing you to invent new operators.
What voidspace said - there are changes that could not have been done on what is effectively the "stable, backwards compatible" branch of python. 
I'm in a position to recommend, for instance, the adoption of Python internally in many of the Windows-centric places I work in as a consultant, for the basis of cross-platform scripting. Python by rights should be eating perl's lunch completely. But there is so much immaturity about discussion on whether you want CPAN or not, and you keep changing the dialect of the language without providing static asserts for scripts (to describe what version of the runtime they are meant to run against.) It's difficult to promote when there are such versioning issues. The language gets modified over and over, and there is no way to clearly identify the version for which a python script is meant to run against. Perl has, for example: use 5.008005; If you are going to so dramatically alter the language, one would expect the language to feature such assertions, so one could know which version of the language your code had been tested against. This is my biggest gripe. You do have good syntax regarding __future__, but you fail to provide other features to allow one to release a script into the wild, live on a server for a few years. You have no idiomatic method (don't tell me to parse the version string!!) to assert what version of the runtime a given script has been released against. 
Well it's just that he doesn't find FP to be generally better. I disagree, but what can you do.
And the whole cleanup of the stdlib, which is a *great* change.
From September 2006 - it's likely to be a bit out of date given the changes that have taken place in Django since 0.96
Firstly I'd like to say that it's quite cool to get responses from both Mr Noller and you. Appreciated. Second of all -- I would not say a thing if the explanation you gave in first paragraph was put in place: ,,we won't make it into 2.x, changed plans for 3.x, wish us luck.'' I am aware of this time constraint, although it somehow eluded me that we're in 2010 already and 2.7 will be here in few months. Third: I'm not offended, I'm a little sad though, since I won't be able to use unladen-swallow for anything serious for at least another year. And there is a lot stopping me from merging with Python 2, to list just time and money. Time for sleep and money for food, or lack thereof to be exact. ;-) Lesson learned? Stop getting emotional over tools and other inanimate objects. Edit: I'm over it, really. Please don't downmod. :P
Somehow, I suspect that if Unladen Swallow manages to significantly improve Py3k performance, SciPy will make the jump pretty quickly. It's gotta be tough maintaining branches for both 2.x and 3.x, but it's something library writers are just going to have to stomach for a while if Py3k is going to make it.
I usually support a minimum version of Python (2.3 typically at the moment) and find it very easy to write Python code that runs across all major versions since then. If you are on a Unix like system you can specify a specific version in the shebang (language features aren't introduced in minor versions). Alternatively if you want to specify a minimum version (which I do in some of my libraries) it is as easy as: import sys assert sys.version_info[:2] &gt;= (2, 3) or similar. Most of my libraries I run the tests with Python 2.4, 2.5 and 2.6. In some libraries I use 2.3 syntax (but don't actually have 2.3 installed to run tests with). In other libraries I want decorator syntax, which makes 2.4 my minimum version. I *don't* find supporting multiple versions difficult - in general Python is pretty good at maintaining backwards compatibility across versions. Python 3 being the deliberate and conspicuous exception of course...
There have been several reports (e.g. by the maintainer of Leo) that once you have gone through the initial pain, maintaining a single codebase that supports both Python 2 and 3 (usually with the help of 2to3 and some conditional imports) isn't so difficult. We could definitely do with some more writeups of the workflows and tricks for doing this though.
I think the thing is tail call optimisation could be said to be an implementation issue and not a language one.
It makes sense. It's just a continuation of nosklo's comment above. Here's the exchange, slightly repunctuated: "I don't see the reason not to port it to python 2.x also... "...other than: They are hailing it as a defining reason to get people to move to 3k."
Feel free to do so. It's extra work for them and it will never get in a released version of Python 2.x as 2.7 will be the last version and is already at alpha. That's why *they* don't do it (that and not wanting to have to do the merge twice), but Python is open source and so is unladen swallow - so *you* can do what you want... 
I got FleetDB and your module running. Can you tell me why you did this in "__init__()": &gt; super(FleetDBClient, self).__init__()
That however is a backward looking paradigm. You know all dialects hences your scripts are good. But if I release a script against version X.Y and it lives in production some years (10 years, say - I was looking at a 10 year old Excel spreadsheet, with a last edit date of 5-1-2000, I was expected to support just last week) then I would like to see version assertion if the dialect is fluid. Take this script as an example, which I will need to run on Windows: import sys assert sys.version_info[0] &lt;= 3 print "I am a python 2.x script" This fails to report a meaningful error. That's quite unsatisfactory. Moreover, this is a platform support issue. How do I get my script to select the python3x runtime when I don't have a shebang? (i.e. scripts are associated on the basis of suffixes, and this syntax change is as good as a regime change considering the script no longer even compiles.) -- Thanks btw for putting up with my moaning. :)
Yeah - I saw that a bit later and deleted my comment.
If you want a meaningful error message: assert sys.version_info[0] &lt;= 3, "This script requires Python 2" As you can specifically test and compare the version that is executing and the version you require you can exit in any way you want and with any message you want... The problem of invalid syntax - so that your Python 2 script doesn't even get as far as the version assertion but dies with a SyntaxError is slightly trickier. Making your top-level script a shim that is compatible with Python 2 and Python 3 and does the assertion first would be one way. A problem for existing scripts that were written for Python 2 though I grant you. On windows you *could* name your top level script something like "main.py25" and associate .py25 files with the Python 2.5 executable (so use the platform specific way of associating the file). You could even write a custom importer to allow importing from.py25 files so that you can name all your Python files with a version specified. There is no generic way to select the Python runtime once you have *already* started to run your script with a different version that is true however. Another way of solving the problem (if you really don't want to maintain your code... :-) is to bundle the Python runtime with it - using py2exe, py2app, cx_freeze or similar. Movable Python is one project for Windows that provides prebuilt versions of Python 2.2, 2.3, 2.4 and 2.5 that are isolated from system installs. You could then execute your scripts with batch files that select the specific executable.
It's fine to roll your own, but it's difficult to adopt as a corporate standard when these things aren't thought through. I find the Python 3 break, and the syntactic invalidation of scripts in particular to be antithetic to good practice. It's quite reminiscent of Microsoft at their worst, if you have seen the continuous breaking changes they have made with Excel release after release. (Note I'm a 19yr C++ client/server veteran but dabble in everything.)
Well, I don't think that one deliberate (and in the opinion of many *needed*) break with backwards compatibility in order to bring about improvement is quite on the scale of "continuous breaking changes... release after release". But then everyone is entitled to their opinion (and damn do they make sure we hear it ;-) On this one I think we just disagree.
isn't the entire motivation behind the effort (and reason for sponsoring it) to effectively speed up a large Python codebase without porting it to java or c++? how does merging optimizations into python 3 help advance that goal? 
I think SciPy is at a bit of an advantage here since it doesn't do much string processing (which is the trickiest bit to get right with the migration), something like Django has more work to do there since that's the kind of change 2to3 *can't* handle.
Again, you seem to be having trouble understanding the import path. That's got nothing whatsoever to do with packaging.
Python applications are not really distributed like Perl ones. That is: we create distributions files that may contains several modules (a module=a python script) and packages (a folder with modules). IOW, we don't really define at the code/module level which python version it is compatible with, but rather at the distribution level, through metadata that comes with the module files (including scripts). We are currently working on the problems you have mentioned so one may clearly list the versions of Python that are compatible with the code distributed. And this is probably going ot be implemented in Python 2.7. That is PEP 345: http://www.python.org/dev/peps/pep-0345/ and PEP 386 : http://www.python.org/dev/peps/pep-0386/. (look at Requires-Python) Last, take a look at PEP 376: http://www.python.org/dev/peps/pep-0376, this proposal makes it possible to list all installed distributions, and query their metadata. 
Although Martin v Loewis ported Django to Python 3 before PyCon 2009. He didn't find it so hard. Unfortunately the Django guys let the patch languish.
Not sure, I do understand. Will Python 3.2 be distributed with LLVM or is LLVM just an optional part ( much like Psyco these days )? If the former is true what does it mean to embedding Python on e.g. mobile platforms?
I know that some items in Py3 were breaking changes. Is that a compelling reason for branching &amp; maintaining two different code paths? I seriously doubt that it was a good idea. Edit: Also the things that were changed in Py3 were not serious pain points for me. I realize that's just my opinion, but it seems to be the opinion of many others too.
in summary: python3 wasn't the pony I wanted 
He ported it enough to get the tutorial to run, not the entirety of Django.
That was depressing.
#1 Python 3 is not backwards compatible. #2 I don't see what the big deal is about super keyword is. It's not ideal but is it really that big of a problem?
Guido tweeted today that Python 3 might be getting the unladen-swallow branch merged back in before Python 2. I haven't used unladen yet; to anyone who has: how much faster is it in practice? [Guido's tweet](http://twitter.com/gvanrossum/status/7464557748)
Since Python 2.7 is already in alpha release and is the last 2.x release planned, it wouldn't make any sense to try to merge unladen into 2.x.
Also checkout the [Concurrence Framework](http://opensource.hyves.org/concurrence/) which is also non-blocking and can run both over Stackless and greenlets.
Yes, and I can't find anything to disagree with. I have been avoiding Python3 only because of speed, but now the Unicode issues are reason for pause.
I certainly don't think it's worse than Python 2 in that regard. It's just not as great as it could have been.
UNICODE MACHT SPASS! :-) 
Good writing Armin. I am with you on this one.
I for one would like to see unladen-swallow merged into 2.x and see continuing support for the 2.x generation. There is a value in improving the speed of existing program without having to rewrite them.
None of these arguments are very convincing to me. Yes, Python 3 is not entirely backwards compatible with 2 when it comes to unicode and bytes. It's really not that big a deal - if I can get SQLAlchemy running at 100% on Python 3K, its not that bad - the 2to3 tool just needs something like the [enhancements](http://www.sqlalchemy.org/trac/browser/sqlalchemy/trunk/sa2to3.py) we've made so that individual sections can be explicitly rewritten for py3k (and the dual blocks we have are extremely small, usually involving the *removal* of code no longer needed in 3k that used to coerce strings to unicode). Yes, there seem to be some problems he raises with locales and such. But these aren't any *worse* in 3k. I don't see the use case for passing through URLs with unknown encoding - the "Everything is unicode" idea applies here too - if you're dealing with a string from the outside, you *have* to know its encoding to deal with it intelligently, otherwise its just bytes. His advice to "use a lookup list of encodings" when you need to parse out keywords would appear to confirm this. The "super" thing, sure that's a little squirrely, and perhaps they missed out on some optimization opportunities. But again, most of that is not really *worse* (maybe the super thing. I don't think it will cost people time though).
&gt; I don't see what the big deal is about super keyword is. If it's not ideal, why not change it now when there's not much legacy code relying on it? Why stand the headache just to have Python 4 change it? `super()` acts like a dynamic-scoped thread local variable like `this` in Javascript. And that's as far away from Pythonic as I can imagine. And it will most likely be the only part of Python that doesn't follow lexical scope behavior.
`super()` works like this: the code generator spots a call to a global named super and switches on a magic mode. In that mode it will check if the current function declaration is part of a class definition and will mark that. Then a new fast local is introduced into the function called `__class__` (I think that was the name) which at call time points at the class the function was defined in. (Not sure what magic binds the `__class__`, would have to check the code, maybe the standard class prepare injects it?). Then at call time the super() thing gets the current interpreter frame, checks for a fastlocal called `__class__` and the first function parameter to the function and invoke itself with those two thing as new arguments. It breaks when a function is declared outside of a class and monkey patched in (I would be fine with that) but it's also not implementable in an extension because it requires the code generator to do its magic and also breaks if you reassign super to something else.
It's not that Python 3 is a bad thing. But I was hoping for more and the big changes (unicode) both solve problems and cause new ones. I just was hoping for more changes when breaking API. There was an opportunity there and it was not taken.
Not entirely great. The clean up removed sgmllib (a.k.a. SGMLParser), which is used by the Beautiful Soup 3.0.x series. The primary developer for Beautiful Soup, Leonard Richardson rewrote it for HTMLParser (present in both 2 and 3) for Beautiful Soup 3.1.x, but HTMLParser is far more delicate than SGMLParser. Over at the Beautiful Soup mailing list, one of the first suggestions for parsing issues is 'use Beautiful Soup 3.0.x'.
Yes, you'd have to put in your own lib/pythonX.Y/sitecustomize.py
So do it yourself ;) Python's open-source, you're free to maintain your own copy of the 2.x codebase, distribute it, include anything you'd like in it. But the Python core team's moved on to 3.x, and won't be continuing to add features or issue new releases in 2.x, as is their prerogative (they don't issue new 1.x releases anymore either, despite how long-lived some codebases were which depended on 1.5).
I stand corrected. I really don't know how I missed that.
Guido is adamant about not changing len() to count code points, but he's open to API additions for dealing with code points instead of code units (kind of like what Java 5 added). asmodai was beginning to work on a PEP for this
No, because it is not my field of expertise and because there is value in having as many people as possible on the same platform. Yes, it is prerogative of the core team to work on whatever they want and I completely respect that. I just expressed a wish that python 2.x continues to be benefit from some of the discussed improvements because I have a feeling that there are still more 2.x users than 3.x users. And it will be so at least until the following have been ported to 3.x: all major database drivers, all major graphics toolkits, all major scientific libraries, and Google App Engine. Personally, I like 2.x better because bytes are closer to machine than unicode and I consider that an advantage when dealing with network protocols.
Ok, thanks mate, and thanks to voidspace for listening to my noisy gripes. I understand what you are saying, Python != Perl. I actually did look through the distutils changes, but they are not script level assertions. I completely understand that python is a language for programming libraries and applications. From my view however, the thin edge of the wedge where I can use it (and I've seen it used) is as a scripting language to coordinate the bits and bobs out there. And in this role, as as general purpose scripting language (under its many guises), version assertion would be really great because often these scripts live in isolation or even orphaned. We know /bin/sh script will run, and if something fails we know we're looking for a missing executable called "XXX". But it's a bit different if the script doesn't even parse. To put something in almost implies a two-step parsing process due to the potential incompatibility. As well as that, a possible version switching strategy (in absense of shebangs) is a speculative "requirement" (I don't mean that strongly) on Windows platforms. Python's great strength and great adoption has been its relative ease to bridge all platforms. A keen eye on configuration management/productionisation issues will see it do well on broad takeup. CPAN, and a general invariance (let's not talk about Perl6!) really got Perl into that space. Python seemed to really be conquering the world but recent adventures look to have created unnecessary obstacles and created a tremor. Every month I was looking at www.tbioe.com to see how much Python was taking over the world. I think you need to address my concerns (obviously we think our own concerns are the most important!!!) :-) But bearing in mind that is not serious advice, I will just write my piece and if a few witterings of me colour the thoughts, just a little, then that would be great. I give my advice, as I said, as a long term developer in the corporate/banking environment, who wants to adopt and indoctrinate, but that's difficult when the material is somewhat volatile. Mind you, you don't assert syntactic version in a C# program either. I guess the main difference is, is that a singular C# file doesn't find itself living on some server somewhere, contextless. And that can happen with a script. Kind regards and thanks for listening to my point of view. I have been catastrophising, so forgive that please :) 
The point about UCS-2 vs. UCS-4 is sadly very correct. That said, almost everybody gets this wrong. We have to deal with it in Java, in xerces, in Objective-C... it's brutal. My situation though is that I can exercise total control over the interpreter my code runs on so I can do UCS-4 and everything is sane. Everything else he says about unicode I disagree with. In his first criticism he's basically asking for built-in transliteration. That's an application function. If python were to do what he asks here it would be sheer chaos. What I put in a str had damn well better be there when I want it back. Write a function to deal with crap like this. And making unicode/binary use consistently explicit is the very best thing about Python 3 IMHO. I nearly cried when I saw that open('r') would return str on read and open('b') returns bytes. Beautiful. This is exactly how it should be and Python 3 was the time to do it. He's right about super(), but you know, if you're going to go down that road, there is all kinds of shit that is terrifying about python when you look real close at it. Let me make the same comment I do about all the whinging over the GIL: it's a scripting language! Yes, a very versatile scripting language, but a scripting language nonetheless. It's not going to wipe your ass for you; you're going to have to do some things on your own. The only real problem with Python 3 isn't with 3 per se but that they then come out with 2.7 and start back porting 3 features onto 2. That's as wrong as shit on toast.
Is there a comprehensive list of what Unladen Swallow has achieved? List of optimizations, benchmarks etc.
&gt; So do it yourself ;) With Google money, I'd start tomorrow ;)
&gt; Personally, I like 2.x better because bytes are closer to machine than unicode and I consider that an advantage when dealing with network protocols. You do realize 3.x not only still has bytes but it actually *calls* them bytes now, don't you?
Of course. I am slowly getting into it.
I � Unicode too.
Not that I'm aware of. Would be really nice. Hopefully the merger PEP will detail all of this.
What mechanism are you proposing - and how might it work?
What are code points and code units?
Service unavailable does not instill confidence. 
Tail call optimisation is a language feature. You don't write programs the same way without it.
I thought I was in the programming Reddit, then realized I'm in the Python one, so what I'm about to write is slightly offtopic, but I'll post it anyway: I was a big Python fan (still a medium Python fan :-). About 18 months ago, I discovered Haskell. Haskell took a few months to "click" and start sinking in. Even after it did, I still saw some advantages in Python that I miss in Haskell: * Nice tracebacks * All code works in the REPL (Haskell definitions do, but declarations don't) * Learning curve for newbies * Canonical ways to do most you'd want to (Haskell is still discovering new/better ways to do basic things like handling file streams [Lazy I/O vs Iteratee, etc]). * More difficult to dynamically load code Despite all these disadvantages, given that you have to forego a lot of your backwards compatibility with Python 3, I still really recommend trying Haskell out, it has overwhelming advantages: * Much easier to refactor code, even large projects * Much more code reusability * Project sources are smaller and execute faster * Much easier to reason about the correctness of code * Compiler-verified guarantees about certain aspects of correctness * "hackage", basically like CPAN for Haskell * Nice community (freenode.net#haskell even friendlier than #python). 
What's wrong with making it easier to support both Python 2 and Python 3 from the same codebase?
What problems are *caused*, exactly? And compatibility with 2.x doesn't count. ;)
It slows adoption of 3, and further complicates the transition. Python is a primary dependency now in several Linux distros. Anything that adds time to their adoption of 3.x means that many of their users will also be waiting for 3.x. Existing 2.x code can continue to use 2.6. New stuff should be in 3.x. Creating 2.7 is like coming up with new slang. For Latin.
The idea is that Python will be distributed with the LLVM by default, but that it will be possible to compile without.
Guess I have my next idea for p52 :D
It just makes it really hard for me to provide a library for both 2.x and 3.x.
I'm sure this assessment will expose me as a heretic, but I'm pretty sure that Python 2 will last a very long time despite pronouncements that Python 2.7 will be "the last Python 2." Even if Python 2 after 2.7 isn't named Python, and isn't maintained by the same set of people, there almost certainly will be a Python 2.8, and a Python 2.9, a Python 2.10 etc. This is just because Python 2 has become an *incredibly* popular platform. And while Python 3 is better than Python 2 on a good number of axes, Python 2 is just very difficult to improve on very significantly on a syntactic level. Most of the changes in 3, although clearly superior to Python insiders are just not compelling enough for the common application developer or library maintainer. At least not enough to spend a tremendous amount of work doing a port and maintaining two versions of the same library or application (or at least the translation logic to produce two versions) over any significant timeframe. In the meantime, despite the general lack of enthusiasm on the part of library authors to maintain two versions of a library for two incompatible variants of a language, most people not closely involved in the Python community make the assumption that library authors will make code available in "Python 3" very quickly because, well, it 3 comes after 2, and 3 is the latest; "this one goes to 11". This obviously causes some tension, because it simultaneously sets up Python 2 to appear to be "the old version" and Python 3 to look like "a takeup failure", which is unfortunate. Neither is *really* the case, at least on any scale that it should matter to people looking in from the outside. Humans usually get pretty wrapped around the axle when it comes to the symbology of names, for better or worse. So sometimes you need to spell it out for them, literally, for the purpose of reducing confusion. It might have better represented reality if "Python 3" had not been called "Python" at all, and instead had been called "Cleese" or something, advertised as another language that happens to share 95% compatibility with Python on a syntactic level, and which provides porting tools to "move from Python" or "share code with Python". Then once it became a raging success over the next five years, great. But if it failed to become a raging success, no harm no foul. In the meantime, it wouldn't be causing quite as much undeserved angst. 
This sounds good, thanks. 
Last time I tried it on CPython+greenlet I got weird "cyclic parent chain" exceptions (that was long ago, before I forked eventlet as gevent). It seems that issue is still unresolved: http://github.com/concurrence/concurrence/issues#issue/1 Not saying it's a bad library just that it seems to work best on Stackless and greenlet version needs a bit more work to become usable.
The super() thing really disappointed me. Why are they adding magic in Python? That sucks.
Anyone tried it? I heard it's slow. Grammatical mistake in the setup isn't a good sign: --------------------------- Panda3D Setup --------------------------- EGG caching is about to begin. This process may take a couple minute and approximately 270 MB of RAM. WARNING : **It might stuck** if your computer resources are low. Do you really want to do this optional step ? --------------------------- Yes No --------------------------- The examples are fairly basic but heavily use CPU, maybe they use busy waiting.
I spent some time with this engine (mostly through the Python interface). If anyone has a question I'll try to answer it.
When you just throw zillions of objects into the root render node then it is maybe slow. When started with this engine, I had a problem like this. I was not sure if I should blame the engine or me. I then tested other opensource 3D engines with my problems, and after all Panda3D was ok for me. I later then discovered that there are classes (e.g. RigidBodyCombiner) and methods (e.g. flattenStrong) that may help you or simply build a Octree with the Scene Tree first (simpler than you may think). Panda3D Setup: One nice thing I love, that the models are in a easy human readable "egg" format (not like Collada). This files can be "compiled" into a binary bam file. The setup just tries to convert all egg files into bam files. 
thats just while its caching it
I'm finally giving Python a go after using Ruby for some time. Does this engine still not have a level editor or the ability to read BSP files?
[Quake 3 BSP To Egg Converter](http://www.panda3d.org/phpbb2/viewtopic.php?p=48825). It is possible to create meshes on the fly, therefore it is theorethically possible to read any vertex soup. There exists a outdated Scene Editor. But I don't know if it still works.
Awesome. I see it's rather recent :)
use 2to3 with conditional blocks ! works great.
An alife project using Panda+Python: [https://launchpad.net/bloom](https://launchpad.net/bloom)
seconded, [we](http://www.gamr7.com) have also quite a good experience with it.
so that product is based on this engine ?
i'm new to python, have no idea what a "game engine" really is/does - do you think I can play with it or it'd be way over my head ?
First, read bcorfman's message as he's spot on. Second, realize that Python (my favorite language) is intrinsically slow. Nearly always, the speed of your programming language is irrelevant - hardware is cheap, human time is expensive! - but for certain applications, speed is very important, and a chess program is one of them. I'd imagine it'd be almost impossible to get good performance out of a Python chess program. Third, take a look at the literature - there are tons of great ideas there. The #1 idea you could consider looking at is the idea of a bit board (and after a quick glance at bcorfman's program, this would help him a lot, too!) The basic idea is simple - 64 bits in a long word, 64 squares on a chess or checker board. So instead of maintaining an array with 64 squares in it, you keep one long word for each piece, each word containing a 1 for the piece position and 0 elsewhere. Then, when you need to do some calculation, you avoid looping through each square if at all possible - instead, you do bitwise binary operations on bitboards to get the result. Remember that bit boards are ridiculously cheap to store - keeping millions of them in memory is perfectly plausible. So if you can't see a way to do something without a loop, ask yourself if you can pre-compute a table of bit boards to do the work for you. The big issue is going to be making sure that Python stores your 64-bit number internally as a single long word, and does all the bit operations using long word operations. Working this out is beyond the time I'm willing to spend on an article like this... :-D Finally, remember that one of Python's many great features is that it can be extended quickly and effectively in C or C++. Generally, it's not worth doing - in this case, it might be, because you have a single, fairly simple data structure that's a bottleneck. Consider creating a C++ class that simply represents the board state using bit boards, and calling it from Python. Good luck to you, and keep it up!
* How hard is it to use it and is it very pythonic? * Is it actually competing with anything? From what I can tell the market for free 3D game engines for Python is pretty slim.
I have minor experience with [Python-Ogre](http://www.python-ogre.org), how does this compare?
First I have to add that Ogre itself is rendering engine while Panda3D is a game engine (Rendering, Keyboard, Mouse, Physics, Collisions, Ressources, Game Installation, Art Pipeline, ...). Although there are tons of libraries available that are maybe a usefull addition for Ogre, most of them may have different concepts and different programming models. Panda3D has an official Python layer ontop of the game engine. I think almost all Panda3D users are programming in Python and not in C++ although that is possible. Everything feels coherent. You get precompiled packages for Windows and Linux (if you ask in the forum, the chance is high that others are helping or even build a package for you). In contrast, Python-Ogre is another project that uses Ogre and other libraries and adds a Python layer. The core devs of any of these libraries don't care about this layer at all. About a year ago I had multiple problems compiling Python-Ogre myself on Linux. If you are looking for an almost perfect python integration, than Panda3D is for you.
That is what I wanted to know, thanks for your time! EDIT: typo
First thing I have to say that you first should try to get something done with python e.g. with PyQt or wxPython. What you learn there, is how to "weave" something together if you have hundreds of classes available. To simplify everything: A game engine can draw 3D objects on a 2D screen from different viewpoints. You need a specialized 3D application to create these 3D objects. Blender e.g. is ingeniously (btw. Blender 2.5 has Python 3.0 support). You move around these 3D objects and/or the viewpoint most often because of keyboard and/or mouse input. More precise?
IMHO the python layer is well done. AFAIK the most recent versions are a bit more pythonic then they once were (Vec3 handling is simplified e.g.). I would say it is a pythonic as PyQt (layer on top of a good C++ library). How hard is it to use: It depends. If you have some basic knowledge of 3D than it is IMHO easy. But even if you don't understand a bit about 3D I think is not that hard, because you don't have to handle matrices e.g. Competing: I would say there exists a problem with Panda3D (that in fact is not a problem) that the default rendering quality is hmmm let's say suboptimal. Therefore I don't think they won't compete with anything. To be honest I am even not sure why Disney made the whole engine open source (excluding the network layer). Maybe a question one should in the forum (I'm not the right person to answer this).
no, it is used to get some visualisation (sp ?) in the authoring tool. we mainly built a set procedural libraries that can be plugged in any game engine (Ogre, Lightspeed, etc..). 
Thank you.
I made a couple of hobby games in 2d and I read most of panda 3d doc and play a bit with the tutorials last summer. Something that I am now trying to understand is how do you manage the interaction (collision detection?) between let's say a terrain and your character. Is there a class included in panda 3d to manage that? Do I have to code the whole thing? How do my character follow the terrain without falling through it? I am no to sure what is the right way to do it or to understand it. Any resources or at least just pointing me in the right direction would be apreciated.
I'll try to give a generic answer that is not too specific for Panda3D. The first you need collision geometry in any collision detection system. You may say: Hey I have me terrain, why can't I use that? You can use that, but nevertheless it is important to know that the visible geometry and the collision geomtry often are not the same. If you have a stair e.g. the visible geometry really looks like a stair (most often in today games, but Mass Effect e.g. has flat stairs) but the collision geometry is often a simple ramp (that's the reason why walking up on a stair is smoother in a game than in RL). The collision geometry should have two properties. 1. It should be a lot simpler than the visible geometry (or we spend to much time on the CPU or are memory bound). 2. It shouldn't have any holes. Panda3D offers multiple options to create collision geometry (so called collision solids) on the fly or add collision geometry to existing egg files (search for "&lt;Collide&gt;{ Polyset keep descend }"). The second you need to know that in collision system you always have a source and a destination. The source e.g. is a player vehicle while the destination is a terrain. The source needs a collision geometry and the destination needs a collision geometry. Examples for collision geometries are line segments, rays, cubes, spheres, capsules and vertex soups. In a perfect world you only need vertex soups, because you can create a sphere out of a vertex soup. The problem is that is damn hard and slow to calculate a collision when the source and the destination is a vertex soup. Even dedicated packages like PhysX don't support all type of possible collisions. The best thing is to always try to stick with a collision geometry that is as simple as possible e.g. sphere as source and sphere as destination should work always. To get back to your question. For your example use a ray that always points down as source and the terrain as destination (Panda3D supports this type of collision). Then let the collision system search for a collision and use the resulting position as a new player position. There is also a chapter [Collision Detection](http://www.panda3d.org/wiki/index.php/Collision_Detection) in the manual. There is also a tutorial Roaming-Ralph in the samples directory.
Reddit, thank you.
I believe OGRE has python [bindings](http://www.pythonogre.com/)
Thanks, your explanation help me piece together little bits I have being reading over the years. I really appreciate your help. I think I played a bit with Roaming Ralph but then, if I remember correctly, there was a warning that the way the collision detection was implemented was deprecated and not optimal so I tried to find an updated version without too much success. Oh [here is the comment](http://www.panda3d.org/wiki/index.php/Sample_Programs:_Roaming_Ralph): &gt;Caution: this program uses an extremely inefficient method for detecting the height of the terrain. There is a much better way to do it, but we have not had time to correct the sample program. What do you think is the much better way or where can I find an example of it?
One recommendation could be to start with a 2d game. For example, check out Pygame (Google it).
According to the forum the sample, as it is, should be ok. Maybe one should delete the comment. I see only one problem. If the terrain is maybe 1000 times larger, the collision detection has to go through thousands of polygons. On the other hand, terrain is often based on a 2D heightmap. A much better approach would be to just lookup the position in the heightmap instead of doing a collision detection (and manually linearly interpolating values inbetween). Although that would be super fast, it does not work for arbitrary geometry. Collision detection always depends heavily on what you intend to do. There is unfortunatelly no silver bullet. There are tons of other problems too e.g. you have to fight always against floating point inaccuracies, e.g. the ray may fall through edge of two polygons and then you have an effect like in Crysis (people were falling through the carrier).
Amazing. html5lib, which was just brought to the attention of unladen swallow this week, and was already added to the benchmark suite, and already had fixes applied to unalden swallow that help it, is being held up as an example of unladen ignoring other's needs. If one pays attention in IRC (or the mailing list, I forget which), they've even mentioned a workload like html5lib's as being a good example of a non-synthetic benchmark reason to do On-stack-replacement (which the JVM does, but really only helps benchmarks), despite the complexity in implementing it. All that being said, yes, google's engineer's first priority is long running scripts, and AFAICT this guy hasn't even bothered to file a bug about the performance regressions or the crashes, no wonder none of them have been fixedd.
Thanks again for the advices, I'll probably start again toying with panda3d, I was lately on an Android game development mindset.
I've often been tempted to use "me" for this reason.
Unicode jibber jabber: http://blogs.msdn.com/michkap/archive/2005/08/12/451043.aspx
Rene has an unfortunately long history of posting hugely critical articles which betray a lack of understanding of the things he's criticizing. I've only skimmed this one so far, but it seems to be yet another in the same series.
I'm a big fan of Eventlet and have been using it pretty heavily lately. Some things worth checking out: * [Spawning](http://pypi.python.org/pypi/Spawning/) which uses a combination of eventlet, threads and multiple processes to provide a powerful WSGI container for web apps (including support for running Django applications) * Eventlet has an IRC channel `#eventlet` on Freenode * [Eventlet on bitbucket](http://bitbucket.org/which_linden/eventlet/overview/)
`node.js` from my understanding is far more event-driven in its asynchronous style, whereas is far more implicit in its asynchronosity (that's not a word). For example, when using [eventlet.wsgi](http://eventlet.net/doc/modules/wsgi.html) and a monkey-patched socket module, the context-switches are all made "beneath you" whenever an I/O blocking point is hit.
AFAIK iPhone support is work in progress.
 * [Python for Irrlicht](https://opensvn.csie.org/traccgi/pyrr) * [Python for CrystalSpace](http://www.crystalspace3d.org/main/PyCrystal) * [Python for Blender Game Engine](http://www.blender.org/documentation/249PythonDoc/GE/index.html) * According to Wikipedia [Python for LeadWerks](http://en.wikipedia.org/wiki/Leadwerks_Engine) To name the most reviewed [Engines](http://www.devmaster.net/engines/).
Hello, The html5lib graph was shown as an example of how they structure their benchmarks. They run things a number of times, so U-S has time to warm up. For many uses this is fine. However, it is also important to have things run fast for the first run. Since there are also lots of work loads which need to run the first time through. So if your workloads need first run performance, then U-S is not for you (yet). For many people seeing one number %XX faster/slower, it is hard to see how that applies to them. Since performance has many factors other than raw throughput over many runs. Perhaps people will better understand what the U-S % benchmarks numbers represent now. So they won't have to bother spending a day testing it themselves. However, people should benchmark things themselves on their own programs rather than believing other peoples benchmarks. --Rene.
Can you tell me where he 'doesn't understand what he is criticizing'?
In this particular case? * Assuming that Unladen Swallow is "done" and critiquing it as if it were. * "It uses C++, that's bad" (problematic because it feels like a knee-jerk reaction, and because LLVM/C++ is an optional compile-time thing). * Criticizing others for claiming improvements based on benchmarking only a few cases they care about, then criticizing UL's performance based on... benchmarking only a few cases he cares about. I could go on here, but hopefully you see the point.
* From the article, he doesn't say it's 'done', he says it needs more work * wrt C++: ask [jwz](http://www.joelonsoftware.com/items/2009/09/23.html) for his opinion on that. * 'one time load and run'-benchmarking is actually a huge benchmark. It would be a huge turnoff for me to use python 3.x to see another 20% performance reduction shaved off, as most programs I use fall in that category. 
What you're complaining about is by design of it using a JIT -- all JIT languages have this feature. You only compile the parts of the code that are hot, which means those that are executed frequently. This means that your first run of the benchmark will start cold and then compile the parts that heat up, at which point performance improves. Pretty sure that US has options for controlling this behaviour. 
Scripts that take a short time to complete are absolutely *not* the target of U-S.
&gt; From the article, he doesn't say it's 'done', he says it needs more work Yes and no. What he's done, basically, is taken a piece of alpha-quality software, assumed that it's instead being presented as some sort of finished product, and then argued that it is in fact still alpha-quality. He's then parlayed this into some sort of ominous and terrible thing which will be awful for the future of Python. Or something. Problem is, nobody's claiming that Unladen Swallow is finished or near being finished; it isn't, and nobody ever said otherwise. There are tentative talks going on right now to work out a schedule for wrapping up the work and merging it into mainline Python, but that's nowhere near the level Rene's assumed in presenting his criticism. Which means that Rene is either badly ignorant of the current state of Unladen Swallow, or is aware but has chosen to drastically misrepresent it in order to criticize the project.
yeah, this is a challenge with compilation. It is a tradeoff to spend more time compiling to get future code to run faster. U-S seems to (currently) take longer to compile so future runs will be quicker. Which makes sense for their aims, and many workloads. However, it is possible, and there are systems which try to aim for compilation + compiled_runtime &lt; uncompiled_runtime for the first run. U-S seems to mainly aim to negate the cost of compilation over a number of runs. LLVM is not the fastest compiler... but it does produce quite fast code. As an example of something at the other end, tinycc can interpret C code directly into machine code and finish running that code even before python finishes starting up. $ time ./tcc -run ../hello_world.c hello world real 0m0.003s user 0m0.000s sys 0m0.000s $ time python -c "" real 0m0.043s user 0m0.016s sys 0m0.012s
Yeah, that's exactly what I've done ;) Except it's not. From Jesses post. 'One of the things we’ve really focused on with Unladen Swallow is creating a solid foundation that the wider CPython developer community can build on top of.' So before the work starts going in, it would be good for there to be outside review of U-S. I can't find all that many benchmarks or reviews of U-S except from the project itself. I think there are some problems with the U-S approach listed in the review. These are problems outside the aims of the U-S project - but are problems that some people will have with U-S for their workloads. Allowing criticism is the whole point of the Pep process - it also helps the U-S developers know of some issues people will have.
&gt;wrt C++: ask jwz for his opinion on that. jwz's opinion about the use of C++ for Netscape, back in the mid-90s when C++ wasn't even standardized, as filtered and editorialized by Joel Spolsky? Thanks, I'll pass.
I am going to assume U-S will merge with Python 3.x. While I admit that Python 3.x is a better language than 2.x and I also admit using a JIT is a good technological improvements, there are some business considerations to be made. There are significant costs in upgrading so people will upgrade only if that provides a measurable significant benefit for them. Here is what I expect as a user: - be able to use nearly ALL standard modules I can use now with 2.x - faster code execution, specifically for web applications - make better use of multi-core processors - if Python is going to use a JIT, I would like to see comeback of a working and secure RExec module. I do not know where we stand on these issues but before people drop support for 2.x, move to 3.x, merge U-S, etc. I urge them to make a business case for the upgrade: WHAT MEASURABLE BENEFITS WILL OFFSET MY UPGRADE COSTS? Please let's not do the mistake that Microsoft did in moving from XP to Vista. Ignoring business and economic considerations in favor of purely technological consideration has killed a lot of great technologies in the past.
By analogy, this is like #!/usr/bin/perl vs #!/usr/bin/perl5 - the 2-&gt;3 syntax change is handled by making the new interpreter not be the old interpreter. At least, if I understand your concerns about version assertion.
It's when I read stuff like this that I wonder why I love python. 
Did Guido never look at classes and go "hey, these things are pretty similar to functions"?
http://python-history.blogspot.com/2009/03/how-everything-became-executable.html
Nice article!
&gt;This dictionary is protected by being wrapped in a dictproxy. Only if you're inheriting from object.
&gt;There are significant costs in upgrading Such as?
Anybody know the scoop on DjangoCon '10?
It's going to be in Portland again, same hotel. James Tauber is the chair for the conference.
ok! any idea on a date? or even the month? i'd love to try to go. thanks!
No idea, sorry. I doubt one has been decided this far out (I imagine it'll be the same time of year).
The flash video player on blip is the worse I've ever seen. And the .ogg version doesn't play with the audio sync'd.
Upgrading from python 2.x to 3.x requires changing existing code. That+debugging+testing code that is currently in production is a huge cost in human time. Today any group who is has developed a major application in python 2.x has to decide whether to spend the developers time re-factoring for 3.x or adding adding new features to their software. This is a business decision. I am not talking about the kid with a few thousand lines of python code. I am talking about businesses and research centers with hundred of thousands of lines of code written by people that may not even work there any more. They have a tight budget and future deliverable. Users of the code (for example clients or funding agencies) do not care whether the code is 3.x or 2.x since they do not even see the code. 
Why? I like that it's so basic and simple.
i downloaded this file - now what do i do? i am not a programmer and don't usually use things like this. but i definitely want to use this hack!
Your comment warrants some explanation.
It's mostly for programmers, sorry.
I don't know why you've been downvoted. Possibly people here do not use Python in a business environment. It'll be years before we upgrade because the cost of upgrading is too high in relation to the perceived benefits.
I laughed out loud. Explain, please: &gt;&gt;&gt; from new import instancemethod &gt;&gt;&gt; class C(object):pass ... &gt;&gt;&gt; C.__str__ = instancemethod(lambda self: 'hi!', C, type) &gt;&gt;&gt; C.__str__() hi! &gt;&gt;&gt; str(C) &lt;class '__main__.C'&gt; **Edit2**: by the way, for additional shits and giggles try explaining what is different if you use `classmethod` or `staticmethod` instead. And why not just use a naked lambda. I mean, Python's object model is so basic and simple, it must be obvious, right? **Edit**: oh, and on a related note (related to this funny "no method calls, not really" thing), try guessing what happens when I do: t = ([],) t[0] += [1, 2, 3] And what's the value of `t` after that? When you've guessed incorrectly, observe the actual results and try explaining them. Then stop referring to Python as "simple", please, don't add insult to injury. It's "simple for beginners", but when you get deeper it's more like a forgotten tomb of Ye Olde Ones, with Insanity that Lurks.
wrt 1) always keep it in simple NumPy arrays when possible. makes moving to a C backend much easier wrt 3) scipy probably has something. their *optimizers* code is quite good
The scoping rules for comprehensions don't give you the heebie jeebies?
&gt; And what's the value of t after that? wow. I almost missed this. Just in case anyone else reading does miss it, here's the complete test script: t = ([],) try: t[0] += [1,2,3] except Exception, e: # one of my favourite bits of syntax, fwiw print "Exception raised:", e print t ... in cthulhu's name?
By the way, you can use `except Exception as e` in 2.6 too.
When you call str(C) the metaclass __str__ method is called. Protocol methods are looked up on the class, not the instance, and your class C is an instance of type.
Indeed, [reference](http://docs.python.org/reference/datamodel.html#special-method-lookup-for-new-style-classes). Though I don't have a slightest idea if there is any difference between `staticmethod`, `classmethod` and `instancemethod` (besides their arguments). The second one goes as follows: interpreter translates `a[i] += b` into `a.__setitem__(a.__getitem__(i) += b)` and can't take any shortcuts because items of `a` could be ints for example. Also it can't check the existence of `a.__setitem__` in advance, because it might suddenly appear due to a side-effect. Neither it has a special `__getitem_iadd__` operator. And, finally, lists feature a kind of broken (from the mathematical standpoint) `+=`: &gt;&gt;&gt; x1, x2 = [1], [1] &gt;&gt;&gt; y1, y2 = x1, x2 &gt;&gt;&gt; x1 += [2] # should be equivalent to the one below? &gt;&gt;&gt; x2 = x2 + [2] &gt;&gt;&gt; y1, y2 ([1, 2], [1]) As a result the exception gets raised, but _after_ the element of the tuple was modified inplace. My point is not that Python sucks, but rather that it shouldn't be called "simple". It is complex, in places -- very complex, sometimes as a tradeoff for the simplicity or features in other places, sometimes due to the design mistakes. In particular, Python's object system is like an order of magnitude more complex and produces more complex effects than Java's, despite appearing deceptively simple at the first glance: "everything is an object, object's class is an object, methods are looked up in the instance dictionary and then in the class and its parents, that is all".
No, not at all
http://sf.net
Those projects are of higher level than that of Grad students :(
The code above yields: &gt;&gt;&gt; Exception raised: 'tuple' object does not support item assignment ([1, 2, 3],) And if you do: t = ([],) try: t[0] = t[0] + [1,2,3] # a += 1 equals a = a + 1, riiiight? except Exception, e: print "Exception raised:", e print t you get: &gt;&gt;&gt; Exception raised: 'tuple' object does not support item assignment ([],) Nice! And consistent too! :-/
staticmethod, classmethod and instancemethod are all different types of descriptors, with different behaviours: * instancemethod gets self as the first argument automatically when fetched from an instance * classmethod gets the class passed in as the first argument, whether called from the class or an instance I believe * staticmethod gets no extra arguments passed in The *basic* object model in Python can still be described as simple, and part of your description isn't a bad one at that ""everything is an object, object's class is an object, methods are looked up in the instance dictionary and then in the class and its parents". For more advanced use cases it also has more advanced features. (Controlling the string representation of a class is not something that many people have to do for example.)
what is p52?
Hey Rene: I found the article a bit disorganized in places. It could be shorter and easier to read. For example: "The C++ llvm takes ages to compile, which is what took most of the extra time." Okay: C++ sucks. Then paragraphs later. "Depending on C++ is a big issue for some people. Since some applications and environments can not use C++." Okay, C++ sucks...and then paragraphs later. "It also adds dependencies to C++ libraries - which is a nono for some python uses." Look at the overall paragraph context for that last one. It says: "It's too early to declare unladen-swallow done" But then in the same paragraph it mentions the C++ issue, which is presumably not going to change between now and when it is "done". And then you end that paragraph complaining about startup time. There is no coherent theme for that paragraph, and you never get around to to explaining why C++ is inappropriate for some users. Just trying to help explain how it seemed to me when I read it,. 
I went through this twenty years ago when doing dbase programming on the side. At the time the dbase language had grown haphazzardly, was inconsistent and offered multiple ways of doing many things. Then Clipper (a dbase compiler) came along with its version 5 - in which the theme was (from memory) "you can't get everything you want, but you'll get what you need". The impacts of that cleanup were HUGE. And it made programming with Clipper a true pleasure. As languages grow they accumulate cruft, redundancies, etc. This will clean them up and make the language better.
As you can see in the comments of that code, matching strings to get the intersection of two sets is about 25% faster than matching objects. Intuitively, I thought it would be the other way around, since to match strings the content must be analyzed, while to match objects one should only compare the pointeraddress. Seeing this, I'm thinking Python keeps a hash of the content of a string handy, but even then comparing hashes should at best be as fast as comparing pointeraddress's. Obviously there should be a good reason why this isn't the case, and I'm hoping one of you could explain it? tl;dr: see comments in code.
http://project52.info/
Map Reduce: The CS equivalent of 'Zoom, Enhance'
Looks nice, but I'm not sure I could go back to XML after working with JSON. When I have too though, I might use this little gem.
Its compiling text based egg files into binary bam files for speed. Its nothing to do with the actual rendering and stuff.
There should be a Reddit collaborative visual novel game, complete with narwhal-themed, bacon-filled storyline. We just need a bunch of storywriters, artists, and the like. I'd play it. :P
What does this have over minidom?
Bless you. Oh, bless you. I've fought with every damn Python XML processor I've seen, and they're all horrid. This looks... usable. Sensible. I'll definitely give it a go.
It really is. Note that [Amara 2](http://xml3k.org/Amara2) is actuallly a refactoring of formerly known 4Suite with a more pythonic API (based on the initial Amara which was mainly using 4Suite internally). The doc isn't as good as it should but the guys on the mailing-list are friendly and helpful (if only a bit busy at times). Amara, as in amara 2, is what I use whenever I have XML processing to perform in Python. 
For a birds-eye view of the 2.7 branch, [this](http://docs.python.org/dev/whatsnew/2.7.html) is what you'd be looking for.
Thanks, just what I was looking for.
Switching to using Python 3 *is* a major undertaking for an organisation with a lot of existing code. We expect most people to migrate gradually and eventually. Developing with Python 3 is a better experience than developing with Python 2, and it is the future. Gradually *most* libraries will make the jump and we'll start to see new libraries and frameworks that are Python 3 only. These are the reasons why people and individuals will make the jump to Python 3, so I think your laundry list of wants are largely spurious in terms of Python 3 adoption. "if Python is going to use a JIT, I would like to see comeback of a working and secure RExec module." How is a JIT related to the RExec module? Specifically Unladen Swallow *can't* change the language semantics at all and it is the language semantics that make RExec impractical. I don't see how adding a JIT changes that... "make better use of multi-core processors" The GIL improvements in 3.2 are a good step, but they don't get rid of the GIL which is what you really need for that. Removing the GIL is a *long-term* goal for Unladen Swallow (or was) but bringing in the JIT in the medium term won't do that. Of course through the multiprocessing module you can *already* take full advantage of multi-core processors with Python 2. I assume you are talking about threads however... :-) Don't forget that you can use the Python language without a GIL by using Jython or IronPython - and use multi-core processors with threads. Python 3 is a change of *language* specification and issues like performance or the GIL are largely irrelevant to the adoption of the new specification. 
Sure - and the Python developers are aware of that. If you're using Python 2.5, probably the most widely used version commercially at the moment, then you're already using an essentially 'unmaintained' version of Python (security fixes only). The version that the Python team are working on is *always* the version that will only see *widespread* use in several years time. I'm fortunate in that my job involves working with both CPython and IronPython. The CPython version we use is 2.5, but our IronPython app (in Silverlight) ships the Python runtime with the app so we are able to choose the version - and we're using 2.6.
cool!
You say "it is language semantics that make RExec impractical". I am not too knowledgeable about this. What changes in language semantics would be necessary? I have considered using Jython and IronPython. With IronPython still a lot of the modules do not work. With Jython there are bugs from runaway expressions inherited from Java that SUN marked as "won't fix". Anyway, the reason for removal of the GIL is speed and [so far](http://www.smallshire.org.uk/sufficientlysmall/2009/05/22/ironpython-2-0-and-jython-2-5-performance-compared-to-python-2-5/) it does not look like any of them is going to give me better performance. Kudos to the developers of IronPython and Jython anyway. Hopefully things will get better. Does anybody here uses Jython or IronPython in a production business environment? Can you share your experiences? 
I've been using distribute for a few months and it seems fine. Pip is a different story however -- it simply does not covery the use-cases i have which easy_install fills perfectly. One of the major shortcomings is that i could not `pip install` a local directory or tarball. i know there's `pip install -e` but that keeps the directory where it is and doesn't place it in site-packages which i would prefer. also at the time i was using pip it had issues with packages that share a namespace (e.g. logilab.common, logilab.astng) where whichever one was installed last would remove anything that was there before. supposedly this has been fixed at least in the tip, but i haven't tried it.
Thanks, this seems to have the appearance of what I was seeking.
That doesn't include that set/dict comprehensions and set literals were backported from py3k.
Only watched a couple so far and they are great... But the audio is very low. 
The scrolls theme makes me want to write 200 lines of documentation for every line of code.
[Amara Bindery quick reference -&gt; XPath](http://xml3k.org/Amara/QuickRef) doc.xml_xpath(u"count(//b)") #2.0 (XPath number = Py float) Why is this a float and not an integer? How would you create two thirds of an element?
They don't appear to have been backported yet: C:\Python27&gt;python Python 2.7a2 ... &gt;&gt;&gt; d = {k:v for k,v in enumerate('ABCD') if v not in 'CB'} File "&lt;stdin&gt;", line 1 d = {k:v for k,v in enumerate('ABCD') if v not in 'CB'} ^ SyntaxError: invalid syntax
&gt;Much as Python 2.6 incorporated features from Python 3.0, version 2.7 incorporates some of the new features in Python 3.1. The 2.x series continues to provide tools for migrating to the 3.x series. Well, that's only a little bit confusing
It’s not that complicated. `list` objects have `__iadd__` as well as `__add__` methods for efficiency reasons. In general, the existence of `__iadd__` methods means that `a = a + b` and `a += b` can do completely different things (though making them do anything different is obviously a pathological abuse). In this case, the fact that `tuple`s are immutable is interfering with the fact that `list`s aren’t. First the list’s in-place addition method is called then the tuple’s set slice is called. The second one barfs the error and the rest is history. Arguably, this should be changed, but I’m not sure of how. In any case, this seems like an unexpected result one would be highly unlikely to trigger except by purposefully screwing around with putting mutable data inside an immutable container. 
&gt; Well, that's only a little bit confusing Not once you look at it. The idea is that 2.6 and 2.7 continue to support 2.x features as needed for compatibility, but also enable 3.x features -- this lets you gradually work your way toward running on Python 3.x without having to port all your code in one go.
Is there an end of the road envisioned for 2.x or are they going to continue to support both branches for the foreseeable future?
I heard that 2.7 is supposed to be end of life for 2.x. Then again, I recall also hearing that 2.6 was going to be end of life for 2.x. So, y'know. It's never quite as easy to drop legacy support as one might hope.
I think the backport occurred a day or 2 after the alpha was released (I know the commits happened, I just didn't consider the timeline).
2.7's the end of the line. There was some discussion around the release of 2.6 as to how far the 2.x series should go, and the conclusion which came out of it was that 2.7 is it. Beyond this, the world is 3.x (which, given the time it takes OS distributors to catch up -- most are still on 2.5 -- is about right with the projected time frame for most projects to port).
I'll definitely be looking forward to seeing these!
pylib has a lot of stuff to do that right now py.test already works fine on practically all reasonably complete python implementations without needing to change anything
Even if we really do get a 2.8, it'll just work further and further towards implementing all of 3.x in `__future__`.
What does it have over lxml?
This sounds like a good explanation, but is wrong. Your first sentence actually argues that the above test *should* work, as do the following: t = ([],[]) t[0].__iadd__([1,2,3]) (t0, t1) = ([], []) t = (t0, t1) t0 += [1,2,3] It's also notable that tuples don't actually possess setitem or setslice special methods, not that there would be any sense in one being invoked by an operation on an object that just happens to be contained in it. Object identity oughtn't be affected by mutation .. that's the whole *point* of mutation (and, incidentally, an important reason for iadd and add being different operations). I also find it disingenuous to suggest that storing mutable objects in an immutable container is any less useful than doing the reverse.
Expanding on my reply to earthboundkid below, be careful with this assumption: # a += 1 equals a = a + 1, riiiight? You must be careful to distinguish the *value* and the *identity* of a in this circumstance. Assignment binds the result of an expression to a variable. An expression creates a new value. Integers are a poor example to work with for this, because we tend to consider the values of 1+3 and 2+2 *identical*. If that doesn't make any sense (I'm tired), consider the following: l = [1] m = [l, l] l += [2] # we should agree what m is now l1 = l + [3] # and now, I hope l = l1 # what about now?
For clarity: &gt;&gt;&gt; str(C()) 'hi!' pretty obvious now.
Since I just responded to earthboundkid's incomplete post below with a question you answered without my noticing it, I thought I should highlight the relevant part: &gt; interpreter translates a[i] += b into a.__setitem__(a.__getitem__(i) += b) and can't take any shortcuts because items of a could be ints for example. Too tired to expand any further, but I'll note that ints don't posess a __iadd__ method, and by noticing this the interpreter could probably do a cleverer expansion. It seems a bit silly that __iadd__ returns a value, tbh. tl;dr: state is a more complicated thing than anyone gives credit; += as an object method is asking for trouble.
I think I object to this statement: &gt; For more advanced use cases it also has more advanced features. Everything you describe so far is the result of consistently applying simple features. Yes, it takes a little clarity of thought to understand, but not a great deal and hey, this is programming. Aside from the misdirection with str(C), the only thing that's bothersome at the moment is +=, which doesn't support advanced use cases ... on the contrary it's a "simplifying" feature implemented in the wrong fashion. If it were a purely syntactic feature, we probably wouldn't be having this discussion. Instead we'd be alongside the Lisp family arguing about the right way to do syntax transformers.
setuptools and easy_install suck. That's my opinion and I'm sticking to it. "[Why I like pip](http://www.b-list.org/weblog/2008/dec/15/pip/)" is a nice article. Edit: [Packaging Propaganda](http://s3.pixane.com/lenin_packaging.png).
&gt; "Why I like pip" .. But note that the pip guy [replied](http://blog.ianbicking.org/2008/12/14/a-few-corrections-to-on-packaging) replied to the pair of JB's articles, playing down the criticisms for the current system (in part by asserting that his stuff is an extension of that), and also toning down a bit the praise for his system (essentially saying that it is still a work in progress). So to the OP: FWIW, my take-away 2 cents is that in the whole pip-distribute-virtualenv-fabric nexus more work needs to be done. 
&gt; in the whole pip-distribute-virtualenv-fabric nexus more work needs to be done. +2.
i commend the effort, but can't you get that 2-220x speedup by intelligently using Cython and C data types/std lib/functions, or just writing your module in C or C++ and exposing it with SWIG? not to rain on anyone's parade, but it doesn't seem that this effort is either maintainable in the long run or worth the effort for an already attainable and more natural speedup. plus it's jamming you into a particular coding style ("implicitly statically typed", which is ambiguous), so you might as well use C++ to begin with.
We've been quite happy with the combination - in particular, pip + virtualenv has made development &amp; deployment pretty painless.
I use buildout. That sets up a restricted environment and has installation methods for all and sundry. buildout can use either setuptools or distribute internally. Right now distribute and setuptools don't differ very much yet in most cases. So much so I don't even know which one I'm using on what machines. 
Write code in normal python (i.e quickly), run c++ binaries (i.e fast ones), you can get that speed up in otherways but if shed skin works you get a speed up for free without having to write a single line of C/C++ &gt;plus it's jamming you into a particular coding style ("implicitly statically typed", which is ambiguous), so you might as well use C++ to begin with. Perhaps, but that requires you to know and be good at C++, I'm not particularly amazing in Python but I don't even know C++.
Yes, and the author/maintainer of Distribute is doing a lot of work in that area. Consolidating, adding tests, fixing old bugs, and generally being proactive in shaping up the whole space-time-packaging continuum. Here's the maintainer's blog: http://tarekziade.wordpress.com/ And here's a blog post from a pretty well-known pythoneer who advises using the new tools: http://sayspy.blogspot.com/2009/10/everyone-should-switch-to-distribute.html 
You have an odd definition of 'Grad Student'
but you can't use normal python, you have to use some rigid subset. instead of constantly looking that up and trying to test your compilation, might as well just learn C or C++ or Cython. especially Cython.
if her and i are anything alike, you should dump her immediately
Thanks to everybody involved in 2.7 and 2.x in general. A lot of people in the business environment and research environment continue to rely on Python 2.x. P.S. I just watched the Disney movie "robots" with my 4 years old son. I found strange disturbing analogies with our situation.
According to it's docs, pip can't install binaries or eggs and isn't tested on Windows. These seem rather fatal limitation to me. Distribute, on the other hand, looks good and I think I'll give it a go. Eggs are definitely a nice distribution format. On win32, I wish all packages provided binary eggs rather than .exe installers (so you can install a large group of eggs using easy_install instead of running each .exe installer in turn).
This [bug](http://bitbucket.org/ianb/pip/issue/34/reinstalling-a-namespace-package-previously-installed-by-pip-clobbers-the) was quite a noodle tickler for me.
You like anal too?
The beauty of Shed Skin is that you can write, test and debug your program entirely in standard Python; once the program works correctly, only the part that needs speeding up has to be written in restricted Python. This restriction is not too hard to deal with -- basically, it means that once you define a variable you can change its value but not its type. Though I know C++, I find Shed Skin easier to use: it combines the speed of C++ with the simplicity of programming in Python.
Which language features are and aren't supported? I can't seem to find this essential information.
To find out which language features are supported, see the Tutorial: http://shedskin.googlecode.com/files/shedskin-tutorial-0.3.html In the Tutorial, see the Sections entitled "Typing Restrictions" and "Python Subset Restrictions".
Misconfigured. :( HTTP/1.1 200 OK Content-Length: 39998 Content-Type: application/octet-stream Content-Disposition: attachment; filename="shedskin-tutorial-0.3.html" Accept-Ranges: bytes Date: Wed, 13 Jan 2010 22:19:42 GMT Last-Modified: Sun, 10 Jan 2010 20:17:20 GMT Expires: Wed, 20 Jan 2010 22:19:42 GMT Cache-Control: public, max-age=604800 Server: DFE/largefile X-XSS-Protection: 0 I hope someone who can fix it reads this thread.
Um, clicking on that link just downloaded the file to me desktop. Content-type is broken.
"Everything you describe so far is the result of consistently applying simple features." That's how you get to advanced features. As for += not supporting advanced use cases. Well... I don't think adding in place to lists in tuples is either particularly advanced or particularly important. I agree that it is unfortunate that the operation fails *after* having succeeded - but there are lots of places that describe how += is syntactic sugar for a re-assignment and the error message will lead you in that direction. It's just a corner case, hardly a fundamental issue.
easy\_install automatically converts standard win32 .exe installers into eggs on the fly while installing them, so you're not required to run the .exe's; just list them on the easy\_install command line and you're good to go.
The problem you mention about logilab.* was a problem on logilab side: they made the assumption that their packages were installed using the debian packages, or manually in the same root dir. So the problem you had with logilab.* happened with easy_install as well. If you release two distinct projects that use the same namespace, you will hit this problem unless you use setuptools' namespace system (based on pkgutil apis). And hopefully, PEP 382 will make our life easier soon. Anyway: try "pip install pylint", it should work like a charm nowadays, since logilab fixed their namespace story. 
I am biased of course, since I am the release manager of Distribute. But here are a few important differences FYI: * Distribute supports Python 3 and even provide extra tools to make you project Py3 compatible. see: http://packages.python.org/distribute/python3.html * Distribute fixes more bugs and want to have the fixes released asap. For instance this one : http://bit.ly/842kgU is fixed on Distribute side and make it compatible with MacPorts' Python. * I work on making Distribute a good Distutils citizen, so it works on the top of it, not *against* it. If Distutils doesn't allow to do something, I work on the problem on Distutils side, since I am its maintainer. And if its a deeper problem, we try to work them through PEPs. (see PEP 345, 386, 376) * Patches and contributors are welcome 
"she and I"
I'm curious to hear why you think lxml is horrid? 
was "molt" taken ?
&gt; This restriction is not too hard to deal with -- basically, it means that once you define a variable you can change its value but not its type. No, it means that you can't use Pythons containers properly which is a greater penalty than single typed variables. &gt; Though I know C++, I find Shed Skin easier to use: it combines the speed of C++ with the simplicity of programming in Python. When I handcraft C++ code for speed I have control over a bunch of allocated memory and use fixed sized arrays and special bit oriented encodings for fast read/write operations. Furthermore I can inline small functions to spare calls. This is not hard to use and it provides an adequate performance model for sequential computations, something Python doesn't offer. C++ is clumsy to use in application programming but it is simpler and less gambling than Python when it is about getting things fast. In my experience there are mostly two cases: one in which performance doesn't matter a lot and the others where it is basically performance that matters and I still do think Python is a poor choice in the latter case and I don't believe this is going to change by any sort of magic compiler technologies being in work. When you don't seek an optimum though, those might be tolerable.
distribute seems to fix the most obvious bugs which setuptools has, however it still suffers from the bad design choices which were made in distuils and setuptools. That is, trying to do too many things at once: an install tool, a build tool, a packaging tool, a deployment tool and a development tool (with no clear boundaries between those tools). While these tools works in most cases (simple Python packages with C extensions), they fail as soon as you're dealing with Python C extensions which are linked against shared libraries (which cannot be easily deployed using the existing egg format). After many years of frustration with setuptools, I've been working on an install tool which solves this problem. It is called Enstaller (it's on PyPI), and I'll be talking at PyCon2010 in Atlanta about it.
I only took a brief look, but most of those files didn't really look like idiomatic Python to me, more like Python as a recent C++ convert would write it. I saw a factory class in amaze.py; these shouldn't be necessary in idiomatic Python, since every class is itself a first-class callable factory. Most of the examples seemed far more "for x in range(...)" heavy than idiomatic Python, with many fewer uses of list comprehensions or iteration over collections. He used nests of elifs instead of dictionaries to dispatch on constants. And compare [this sudoku solver](http://code.google.com/p/shedskin/source/browse/trunk/examples/sudoku1.py) with [Peter Norvig's sudoku solver](http://norvig.com/sudoku.html).
check the line above where it says 'norvigs sudoku solver'.
&gt;No, it means that you can't use Pythons containers properly which is a greater penalty than single typed variables. can we deduce from this that C++ programmers cannot program properly? btw, note that shedskin can generate extension modules ('shedskin -e'), so you can do whatever you want in the 'main' program.
Are you there during the sprints ? I would love to see if we can push back in mainstream (e.g. Distutils/stdlib) any good bit from Enstaller if you are interested.
or you...
you can currently use dict((k,v) for k,v in enumerate('ABCD') if v not in 'CB') 
Right the new notation is just prettier, IMO. Another one from 3.x is this: head, *tail = range(10) instead of: r = range(10) head, tail = r[0], r[1:] Just sugar, but it's sweet. :)
Unfortunately, I won't be at the sprints, but I would love to meet with you during the conference. I'm coming to the tutorials, so I'll be there arriving on Tuesday night and leaving Sunday evening.
Sniff, sniff. :( Here I was, offering you a free grammar lesson. But fine, _throw your life away_
What is the reason to call it 'command line'? It's more like 'in-memory', since user is resposible for printing this progress bar on every update. Also, it's strange that user is obliged to provide duration for the whole operation. More convenient approach would be to estimate time automatically by measuring intervals between progress updates.
&gt; ...bankers, farmers, and warhead designers. Not a lot of places you see those three lumped together.
command line as opposed to GUI. perhaps "console" would have been better
Actually, I interpret it exactly as 'console' here, and my point is that it's not console, since there is no console IO functionality in it.
well.. if you wanna print a progress bar to the console, this will do it. not sure what your point is. that you have to explicitly print it on each update?
Hey, thanks for the code snippet. I'm still getting a feel for python and small easy bits of code like this are great for learning. However, I do have one suggestion. I'm using Python 3.0 and besides the obvious print(p) change, I seem to be getting an error on line 25, which comes about because of the assignment in line 23: pct_place = (len(self.prog_bar) / 2) - len(str(percent_done)) It may be the newer version getting angry about this, but casting it as an int seems to fix the problem easy enough: pct_place = int((len(self.prog_bar) / 2) - len(str(percent_done))) 
To make a little bit better use of this little snippet; make sure your end character is a carriage return (\r) instead of a newline. It'll return the carriage to the beginning allowing you to print over the current progress bar. After your progress-bar loop you can print a newline to continue on with your process. For Python 2.x I used: print "%s\r" % (p), #Adding a comma to the end suppresses newline. For Python 3 it should be print (p, end="\r")
That would be roughly my complaint, yes. The fun part of console progress bars is redrawing them cleanly in the same spot, not figuring out how many '=' to draw.
I stand corrected on that one. I still wonder how idiomatic you can make your Python code - does it support metaclasses? Properties? Decorators? Closures packaged up and stuck in a decorator?
[here](http://pastebin.com/f62b5a465) is the code bzr(GPL) uses for CLI progress bars. Which seams to have a few of the features people are complaining about, such as printing the output and estimating the time remaining I found [this](http://pastebin.com/f7a9ade02) in [pyfragtools](http://ubuntuforums.org/showthread.php?t=169551), then jdong (of of ubuntuforums) who wrote the tools, pointed me to the original source.
&gt; No, it means that you can't use Pythons containers properly which is a greater penalty than single typed variables. I don't understand. Can you explain? Ideally in code.
In python 3.x, division of two integers will result in a float, whereas in 2.x it is always an integer. (e.g. in 2.x, 1/2=0, while in 3.x, 1/2=0.5) You can skip the int call by using the truncating division operator "//" to get the python 2.x behavior. pct_place = (len(self.prog_bar // 2) - len(str(percent_done)) Hope that clarifies things a bit... EDIT: The "//" operator is in python &gt; 2.2, as well, so you can use it in both 2.x and 3.x, just for whatever it's worth.
I don't think Google Code really cares about that; it just happens this was uploaded as a file, and Google Code treats it as such. Put it in svn (with the right properties, e.g., svn:mime-type) and Google will serve it up happily.
[My implementation of the same thing](http://github.com/skorokithakis/progress/blob/master/progress.py). It automatically calculates time left.
That *is* interesting. Can you point me at any docs describing this feature? I don't have access to a Windows machine on which to test it, right now.
That's pretty helpful, thanks very much! Strange behaviour though.
The downside to using the "//" operator though is that it performs floor division. I don't know about you but I've always preferred having a float if the division output was such. Arbitrarily chopping off numbers from the end of things only results in Superman-type plots to get rich ;) In fact, in the 2.x series I always throw this at the top of any script that does division: from __future__ import division As it imports the Py3 behavior of outputting a float.
pida can integrate with external editors (currently vim/emacs)
I do wish Python has a cross-platform way of moving the cursor around the console to do such things easily. C#'s Console.SetCursorPosition(int x, int y) function would be quite a nice feature.
That's wonderful! Thank you :) It always bugs me that those tools you just put together for one time use never have a usable interface because you are too lazy to spend time on that. Things like this are great to change that.
What's wrong with Ubuntu? I don't know about PyCUDA but for pretty much everything else it's got up-to-date packages that worked out of the box for me. I've been forced to work on Centos recently for work and I have to say stay away from that or Red Hat! The packages are really old and incomplete (even with EPEL). Maybe Fedora's better...
Yeah, it bugs me too... At least I was prescient enough to write some comments, I hope people can contribute their own changes to mine, I'd be glad to merge them if anyone wants to fork my github branch.
At my work we use Cent OS (and we script heavily in python) but I am unaware if it has been modified for that use.
Also mentioned in the comments of that post: http://pypi.python.org/pypi/progressbar/2.2
Oh, that's nice, thanks!
There's nothing really wrong with Ubuntu (although the version we have now is a tad out of date - but that's another story), I was just wondering if there was anything better for my purposes. And it appears that Python(x,y) has an Ubuntu repo, so that might be worth looking into.
As a heavy Scipy and Numpy user, I would recommend Python(x,y). I'm also a big fan of Spyder, the IDE.
python(x,y) rocks my world. 
I don't think any distro will help nor harm you in terms of running python code. 
`pip` also does not support the "extras" feature ([link](http://peak.telecommunity.com/DevCenter/setuptools#declaring-extras-optional-features-with-their-own-dependencies)) that is used by at least several zope.* packages.
Note that pylint's sdist (tarball) does not include the .egg-info/ directory (which contains requires.txt). So *static* package analyzers (such as [PyPM](http://pypm.activestate.com/)) cannot find the install requirements without having to run Python code (setup.py). This is unfortunate and hopefully will be fixed by the time we have static metadata PEP implemented.
Awesome, I was scratching my head about the easy_install / setuptools thing the other day, and moaning on here about it. It's great to see some real enthusiasm from the community to sort out packaging. Many thanks. 
Fedora 12?
I've never watched a video successfully on blip.tv.
`//` performs floor division, but the problem is the program was relying on the fact that by default in Python 2.x `/` also performs a floor division when one of the inputs is an `int`. It’s good coding practice to always write `//` even in Python 2.x code, unless you need true division, in which case, as you say, import from `__future__` first. 
maybe too obvious, but... http://www.scientificlinux.org . ubuntu is good choice if people are unexperienced with linux and you don't have dedicated admin for those boxes.
I can recomment the Enthought Python Distribution (EPD). It is now on Python 2.6 and has the latest numpy/scipy/matplotlib as many other packages VTK, wxPython, ETS, HDF5, pytables, ..., and it is very easy to install. They support RedHat, Ubuntu and OpenSuSE, but it should work on pretty much any standard Linux distribution. The 32-bit version is free for academic use, but they also have a 64-bit Linux version. 
That is awesome. Will immediately investigate.
&gt; What is the reason to call it 'command line'? I'm taking a wild guess here, but I think the reason is because that use in command-line applications was the primary intention by its developer. I'm not sure, could there be another reason?
What!? I made a progressbar class for my own uses years ago. I use it in all my analyses and it's way better than this. This is not meant to be a hate-post I'm just thinking maybe I should upload it somewhere. It's both C++ and python, any suggestions where I should upload it?
Hmm, I'm not sure if this is faster than memcache... Some benchmarks would help back up that claim.
It's good. :-)
Another epiphany on lazy list processing. Read about iterators, generators, laziness, list comprehensions, monads, map-reduce, haskell etc. already, and form a solid mental construct! Using pool to parallelize map is fun and simple, though.
It's a bit misleading if I understand this correctly. It seems like what's happening is that instead of memcache, he's using Python's in-memory data structures, and persisting them by exploiting App Engine's module cache. So of course direct memory accesses would be faster than an RPC, but it's not quite as general purpose as memcache.
+1 for EPD. It is good to decouple management of your scientific app support from your os distribution. This way you can update your OS packages at will without worrying about hosing your data pipeline.
&gt; Overriding core Python functions is a wicked bad idea &gt; &gt; xrange(20) &gt;&gt; item[::-2] &gt; I don't care how smart you are, or how long you've been using Python, but if you stumbled across that code in a sea of vanilla Python, your jaw would drop. One of the things I love about python is that you can do all this crazy stuff with the language, but also that the culture of programmers is smart enough to know that it's usually a bad idea. Don't mess with the language semantics unless you have a really good reason to. Just implement a good, clean API for your module, and we'll all be much happier.
The name (and the package itself) is inspired by this chapter of SICP: &lt;http://mitpress.mit.edu/sicp/full-text/sicp/book/node69.html&gt;. I found stream to be a very useful paradigm to write program in. And once you found that to be true, you would be hard pressed not to overload an operator. Either that or using tons of parentheses. Also UNIX Shell has a pipe operator, which turns out to be really useful. I'm sorry if your jaw dropped seeing the `item` thing. But I can't help remember that mine also almost dropped when I first saw Python slice itself. I feel that it is fair use. I'm surprised to hear that it is horrible, but it is really a matter of taste.
I don't see why it's not as general purpose, could you explain a bit more please ?b
From http://peak.telecommunity.com/DevCenter/EasyInstall#downloading-and-installing-a-package : &gt; When downloading or processing downloaded files, Easy Install recognizes distutils source distribution files with extensions of .tgz, .tar, .tar.gz, .tar.bz2, or .zip. And of course it handles already-built .egg distributions as well as .win32.exe installers built using distutils.
Well, in general it isn't always going to be faster than memcache, which is a shared cache. But if you understand the two architectures, then it should be obvious that raw reads and writes on this thing are going to be MUCH faster than memcached. It's basically doing NO work at all. Whereas memcached is doing network connections to other servers.
Memcached is a shared cache that will survive code uploads. This is a per-machine cache that is flushed when new code is uploaded. The difference between a shared cache and a per-machine one should be obvious. If you have a X users and each is caching Y bytes in each of a Z machines then you're using Z times as much RAM. If it takes T seconds to compute those records, then you're using Z*T processor seconds to generate the data. 
I guess you're right, but memcached's data structure implementation might be faster than Python's or something. I'm not sure if memcached would be faster than this because it has to go through the networking layers even for localhost, but I'd still like to see some benchmarks...
&gt; Here's one alternative, use methods that return objects: &gt; nums = range(5) &gt; stream.zip(nums).map(operator.add).map(print) For that, every stream processors need to be hard-coded in. You can't write one that fits your purpose and works with the rest unless by monkey patching :(
Definitely an interesting module. Taking a look at it right away.
Very cool. I suppose they did not use the UNIX pipe char (|) because Python already uses it.
It would not even be a fair comparison. &gt;&gt;&gt; timeit.Timer('a["key"] = "value"; a["key"]', 'a = {}').timeit(10000) 0.0018260478973388672 &gt;&gt;&gt; timeit.Timer('mc.set("key", "value"); mc.get("key")', 'import memcache; mc = memcache.Client(["127.0.0.1:11211"], debug=0)').timeit(10000) 2.4874789714813232 So really, unless you can serialize, send, recv, and deserialize in under half a microsecond, it's going to be bested by a local cache.
Python packaging is a huge pet peeve of mine, or to be more specific, the lack of any package management. edit: to clarify, it seems like both pip and easy_install just aim to shove crap onto your system with no sane way of actually managing that crap. Tools like virtualenv and buildout (which IMO are just no fun to use at all) are basically work-arounds for the fact that anyone who develops with python is likely to have a disaster of a site-packages directory. Granted, most people seem to not agree with me given the current state of affairs.
Oh, it just uses dictionaries for storing data? I guess that's true, although it wasn't fair of you to time the importing of memcache every time...
Oh, the import happens in the 'setup' phase, so it's only executed once. http://docs.python.org/library/timeit.html
Ah, okay. Then yes, they have quite the difference...
A lot of my work involves data processing and I really like the functionality of your module but the syntax does worry me. Monkey patching *might* be a lesser evil than operator overloading, especially if you provide a method that lets users register their stream processors with the stream class. 
Even more importantly, if you have the same version of EPD installed on different systems (Linux, OSX, Windows), you have a big chance that you're application will work on all these systems.
Hey amade, I contacted you about this library via email not too long ago; we discussed the possibility of turning the generators into threads. I wasn't expecting you to post here. :) BTW, there's an even more radically different style of programming called "flow based programming": http://www.jpaulmorrison.com/fbp/
Hello there! From a preliminary look, it seems that flow-based programming is really not that different from streams. You can already set up arbitrary DAG flow patterns with streams using feeders and mergers! The major pain point, as I told you before, stills remains: the GIL :(
just pastebin is fine
&gt; You can't write one that fits your purpose and works with the rest unless by monkey patching :( Yes you can. Use inheritance. Also, see the Django ORM. This is the kind of code it results in: latest_posts = (FeedPost.objects.select_related(depth=1) .filter(feed__in=feeds) .filter(published__lte=dt.datetime.now()) .exclude(feed__name__exact='') .order_by('-published')) 
I also think that overwriting Python standard functions is a bad idea and while this "stream" is very interesting the implementation is pretty bad.
well, its a different thing, the sql layer queries for rowsets the orm queries for object graphs so far sqlalchemy is the only lib that gives enough control over both aspects
SQLAlchemy has separate layers built atop one another. There is the low-level SQL layer enabling you to directly use your own SQL statements (select, update, etc). There is the ORM and declarative layers on top, and the layers are connected using the MetaData object. Thus if you create tables using the declarative mapper, you can still use the low-level SQL API through the MetaData object. This is covered in the documentation and most tutorials I've come across for SQLAlchemy. I'd definitely recommend the PyCON tutorials on blip.tv for SQLAlchemy (Introduction and Advanced).
The problem I have is that I seem to get forced into using the "low level" API, and then I can't easily get back to an object graph from the row set. Specifically, when I run a simple query using the object graph, I run out of memory. This may actually be a cache bug somewhere --- I'm using mysql. If I do something like: for obj in session.query(MyTable): pass I run out of memory. Is this normal?
Well it maybe faster than memcache but it can only maintain state during the lifetime of a request where as memchache is persistent. I don't really think the comparison is a fair one.
When I evaluated SQLAlchemy, I found it seemed to have an identity crisis. * It had a dummy's mode which tried to make the database look like persistent python objects. * It had an object oriented technique for mapping a relational database into a collection or network of objects. * It had a more relational interface that mapped SQL commands into python function calls. These were adequate for the simple schemas used in the tutorial and documentation. It seemed to be oriented towards the kind of databases that get designed by programmers. Typically these are databases used by one or a small family of applications. I needed a tool to ease the task of working with a database with well over 1000 tables having close to 30,000 columns. This was a database designed by database professionals for use by multiple applications written in diverse source languages across multiple hardware platforms. Typical implementations had row volumes on the order of millions to hundreds of millions of rows. I think SQLAlchemy would be a very good solution if you developed the application and designed the database from the start using it. I had success doing that for several small applications based on the descriptions of the database requirements. Like the OP, I was confused by the different techniques available in SQLAlchemy. I believe you need to use the simplest possible technique that meets your application and database requirements. 
I've never had any memory problems using SQLAlchemy. :-( I'm not sure how it implements the caching in the backend, but memory problems with an iterator over the rows of the table would imply that each row is very large, or you just don't have a lot of memory.
how many rows are in that table?
At the risk of sounding rude, I suggest you keep reading the docs. It didn't make sense to me at first either and confuses most programmers at the beginning. I will add that if you have a large continually developing database keeping up with your mapper() definitions can get tedious. I ended up having to write my own scripts to generate my mappers because SQLSoup was not up to the task. I only need to use the sqlalchemy SQL interface now when calling functions in the DB.
To study russian literature you need to be able to read and think russian. To understand databases, you need to be able to read and think SQL. ORM and Worms and translators only can get you a taste, at best.
There are some great tutorial videos given by the developers from a recent pycon: http://pycon.blip.tv/search?q=sqlalchemy The interactive tutorial code is available on the sqlalchemy.org site so you can play along at home.
Perhaps as a test you could try explicitly saving or expunging the objects after you are done with them? Maybe it is letting those objects hang around.
I've had (and still have) problems with Python 2.4 and MySQL. The mysqldb connector library leaks memory like there's not tomorrow. Otherwise everything is fine. :-)
&gt;My main complaint is that the **ORM relational mapping** part seems to live in a totally different universe from the SQL query language part. FTFY
As it's mentioned in the appengine documentation: http://code.google.com/intl/en/appengine/docs/python/runtime.html#App_Caching **It keeps the state per instance, not per request**. It doesn't replace memcache but if you use it complementing memcache, considering the special circumstances of cachepy, you can achieve a great boost in your performance. I use it in some google appengine apps to cache the whole output of some pretty static pages, and the difference it's been huge. It caches using a python dictionary, it's just a memory allocation and read. Memcaches client serialize your structure into a string, most of the cases, because AFAIK the binary protocol it's not launched yet, then establish a tcp/ip - you could udp/ip too - connection and send the serialized data structure into a string. Retrieving the value requires the opposite process. As you can imagine it's just faster always, but you can not use it for everything, I would say for pretty static values or with pretty fixed expiry, like a cloud of tags. I work with PHP architectures where you have APC, which can be used as cachepy, with the same upsides and downsides, if you use it wisely you can boost the performance of your app.
[See this](http://groups.google.com/group/sqlalchemy/browse_thread/thread/1108eb5d68b3b323) if you really need query all table. Maybe you need use a filter_by function to perform a *where* in data base.
This is definitely a big part of the problem, and it's why I'm uncertain whether it's "just me". On the one hand I have some general confusion, but on the other hand I keep thinking "wait a minute...this just doesn't seem right".
About one million. But I didn't think this should matter --- I expected it to just happily iterate.
I'm using Python2.6. Any tips?
ORM stands for object-relation mapping so what you have written is &gt;object-relation mapping relational mapping.
Object-Relational Mapping relational mapping?
I use two stacks, SQLAlchemy over: * pyodbc for MS SQL Server * psycopg2 for PostgreSQL I have a couple of tables about 1M records, and both stacks iterate OK.
&gt; If you wanted to offer different, specialized behavior, why not make simply make a subclass? Indeed a subclass is only what you need to write a custom stream processor. In fact there is an decorator to turn any iterator-processing function into a stream processor. See this example: &lt;http://github.com/aht/stream.py/blob/master/example/pi.py&gt;. Perhaps you would understand streams a little better. &gt; You're not trying to solve a problem but rather fundamentally change Python semantics to offer a new, unique way of doing things. I'm not changing Python semantics. No patch of the Python runtime, monkey or human, is necessary. stream is just a module. The behavior of a the pipe operator is set in stone by the Stream class in that module. I would like to point you to [RTFM](http://www.trinhhaianh.com/stream.py/#how-it-works). &gt; Your intentions are noble, but too ambitious. &gt; In that case, I encourage you to write a Python Enhancement Proposal. Thank you. Are you suggesting this to be included in the std-lib? :-p I think stream.py is a bit pre-mature for that, but if people find it useful and use it then who knows.
I personally prefer OODBs like ZODB but since most of my work involves SQL, that's what I end up using.
Well, not exactly. There are few equivalent query models for relational databases, such as relational algebra: http://en.wikipedia.org/wiki/Relational_algebra. ORMs are another model -- equivalent as long as properly implemented. As far as I know, Django's ORM is pretty close.
It was in response to the "SQL query language" part, which I see OP corrected already.
Sorry I was simply refuting your points. I feel as a programmer, one needs to RTFM so often that maybe the phrase doesn't carry a negative connotation, despite the F work. Am I wrong about this?
Ah, I got the joke now, thanks. Though, I think the OP was meant to type "[SQL expression language](http://www.sqlalchemy.org/docs/05/sqlexpression.html)".
Check out [Durus](http://www.mems-exchange.org/software/durus/) I think you'll like it. It's simpler. It has less dependencies and is almost entirely written in very readable Python
Well, reddit is built upon it. Not sure if you'll consider that a point in favor or against. 
&gt; My main complaint is that the ORM part seems to live in a totally different universe from the SQL query part. &gt; (...) &gt; Do people really swear by this? Why, yes. You seem to loathe it for exactly the same reason why I *love* it. See, SQLAlchemy doesn't treat your database as a simple collection of tables, like ActiveRecord and friends do --- it lets you map arbitrary *queries*. To do that, it needs a rather rich SQL-toolkit. That toolkit is massively useful by itself, as well. I use it all the time without even touching the ORM. &gt; If I use a declarative map, and take advantage of the way the ORM allows me to specify relations as table properties etc, how do I then go and use the query language? Oh come on. http://www.sqlalchemy.org/docs/05/ormtutorial.html#querying
Support for [OurSQL](http://packages.python.org/oursql/) --- a Python MySQL-driver that doesn't blow as badly as MySQLDb does --- is coming in the new release (0.6) of SQLAlchemy. (I'd recommend switching to PostgreSQL more, though.)
&gt; Am I wrong about this? I'm guessing it's never really going to lose a negative connotation because of the "F" but also because of all the attitudes that are associated with it. When I say "the attitudes that are associated with it", I'm referring to how some people go out of their way to berate others about not reading *some arcane documentation* or *searching first* instead of taking the time to answer the question and help them out. I would consider this a pretty strong negative connotation. On a personal note, there are lots of times when I would rather ask a question instead of spending hours on end looking for the answer. I spend hours researching when I feel like it, but at times I just like asking others for help. :) (In fact, it goes both ways for me, sometimes I'll ask a bunch of "newbie" questions; other times, I'll spend hours answering "newbie" questions.)
MySQLdb, the DBAPI, fully buffers rows before allowing them to be returned to the client application. If MyTable is a million rows, you'll get that behavior. This is asked and answered all the time on the mailing list. edit: psycopg2 as well. edit: also, seriously. You have a problem with one session.query(X), and your first thought is, write a rant on reddit (ironically, which currently runs on SQLAlchemy). Really? Are we not providing enough support ?
That is probably true in a general context. In the above case though, the linked post **is** the manual so I think my usage of the phrase is justified, esp. for defending criticism.
and what tool did you ultimately go with ? Its fine to say its hard to get SQLAlchemy to work with a 1000-table database (which I don't believe it is, actually, if you felt like you had to list out 1000 tables and classes by hand, you were mistaken and we could gladly help you out on the ML), but then what tool exactly *would* one use ?
Oh yes, I see what you mean. Linking to the exact section of the manual, when appropriate, is always a great way to answer a question / help someone out. I was was merely commenting on the use of RTFM -- no matter the context or how helpful you are it will always carry a negative, almost derogatory, connotation (sometimes a very strong one for some people). BTW, when I say "you" in the above sentence, I'm not singling you out specifically, just referring to people in general.
The 3 points you mention are exactly why I like SQLAlchemy. It allows you to define your database schema and metadata in one file. Then in each script or application you import that to you are free to use whichever method is most appropriate to work with the data. This is a big win.
&gt; pgBouncer does a much better job at connection pooling. In-process connection pooling is designed to reduce overhead within your process, that of connecting/initializing. It's a separate concern versus middleware pooling. &gt; There's a patch to make Django do row locking SQLA of course supports this but also row locking is usually not needed. If you need it all the time there's probably better ways to do what you need. &gt; I never felt like SA made my life simpler. It always seemed like I was translating (SQL -&gt; Pythonized-SQL) rather than writing code. but...you don't have to - use textual SQL. &gt; I always had a difficult time understanding its source code and documentation. I also seem to recall encountering a few superfluous abstractions designed to solve hypothetical problems. would love to know what those are since everything in this toolkit is used. Some behaviors we've taken *out*, like entity-name mapping since there's a better way to do it, and folks immediately email us looking for the new way to do it. &gt; There were a few times where I spent hours trying to turn some of my fancier queries into SQLAlchemy. When I finally determined it wasn't possible, I'd feel like such a fool for having wasting my time on something so pointless. Again, use textual SQL. Also, the expression language (edit: and the ORM Query object too) can do just about any SQL possible these days, and is also [extensible](http://www.sqlalchemy.org/docs/06/reference/ext/compiler.html). &gt; The object mapping system really bugged me, especially when it came to foreign keys and backrefs. It's good that this is getting better. The SA devs would do their project a great service if they removed many of the old APIs and focused on the ORM. Please review the migrations from [03 to 04](http://www.sqlalchemy.org/trac/wiki/WhatsNewIn04), [04 to 05](http://www.sqlalchemy.org/trac/wiki/05Migration), [05 to 06](http://www.sqlalchemy.org/trac/wiki/06Migration). Up until 05 we deprecate and remove like *mad*. The 0.xx version numbers are not by accident. 
Check out ZODB. I think you'll like it. It's powerful. It has C extensions that offer great performance. It's been battle tested in a lot of extreme circumstances for more than a decade. It supports multi-machine and multi-processor scalability. See how that works? It all depends on your perspective. :) 
Problem is we were on two separate pages, me with my interest in flow based programming, you in the lazy evaluation stream.py library. Ok... this is gonna be a long post, but here goes. From what I can tell, flow based programming (FBP) and your stream.py library might be quite easy to get around the GIL using processes and a queue to pass data between those processes. In fact, I actually found someone that has done this: http://www.parallelpython.com/ I haven't actually spent a lot of time analyzing it, but it appears to do some of the things I had in mind. Maybe it would even be possible to mix the features with your steam.py library? On the separate topic of FBP vs lazy evaluation stream.py... well it seems that you have made a number of additions since our last conversation that makes stream.py much more like FBP -- especially stuff like the threadpool. :) I really need to spend some time understand all the changes you've made and some of the core concepts you've built on. Unfortunately, that means I can't say well enough how similar stream.py is to FBP. I probably should take some time to write up a little explanation FBP so you can see for yourself the differences (if any) since I don't have enough of a grasp of your own library to do so myself. BTW, what are some good resources to learn more about all the different concepts used in your library? (what are some of the concepts used like lazy evaluation?)
I would tell you that stream *is* FBP. The main inspiration for stream.py is UNIX pipes. It turns out that pipes are equivalent to coroutines which in Python means iterator and generator (so much for lazy evaluation). I've written more along these lines in my blog (which is linked from the manual). If you happen to know Scheme, there is a also a whole chapter on "Streams" in SICP. Google it. Have fun! The newly added stuff is what makes it possible to have arbitrary directed acyclic flow graphs between threads or processes. And using multiprocessing carries serialization/de-serialization and interprocess communication cost, which can be big. You don't gain much parsing XML in a child process, as that requires sending the whole file through system pipe, then the child process has to serialize the whole parsed DOM tree and send back to the parent process, which would have to de-serialize it. It would be like parsing the DOM three times!
lol, I'm using SQLAlchemy because Django's ORM doesn't work for us. Django's ORM decided it didn't like our existing schema. I'm not crazy about it myself but I wasn't about to change it just so that I could use django to whip together some cheap admin tools.
it's really not "just you" since some folks really want a more high-level ORM experience and don't want to deal with SQL or the details of how their data is being loaded or persisted. Django's ORM is a lot more tailored towards that use case. Its a very popular one and that is totally fine. SQLAlchemy is made for people who like SQL, transactions, relational schema design, and wish to maintain control over all those details while having a very smooth Python experience at the same time. 
The best part is after the all the telescope stuff. &gt; The older of my two little brothers, Ross, who is 11, dismantled the telescope control system and built a robot car with the parts. That's an awesome little brother.
&gt; On my desktop, connecting to pgBouncer (via psycopg2/tcp) takes half a millisecond. For rdbm work, you can't get much faster than that (psql says "select 1" takes 0.165ms for comparison) What are some situations where you feel in-process connection pooling helps? Pooling in the application also means that whatever initial metadata the DBAPI requests from the database per connection is decreased. Try out pg8000 and watch your PG logs - it emits an (edit) expensive SELECT when it first connects to figure out available datatypes. To be honest I don't really see what you gain with pgBouncer vs. an in process connection pool except that you have to start and monitor yet another daemon on every host, and I suppose their argument is "you're using an application that doesn't have any pooling built in". &gt; I vaguely remember, and poking around 0.5.6 isn't jogging my memory, so that was probably a foolish argument. I do however still feel SA's documentation has room for improvement. Message me if you'd like suggestions. Patches are better. I think when people complain about the SQLA docs, they need to appreciate how much there is to be documented. Do you find the MySQL or Postgresql documentation to be less daunting ? &gt; I understand that SQLAlchemy's primary niche is its flexibility (especially for large complex databases,) but I'd be curious to hear what your philosophy is on finding the right balance between power and simplicity. It is exactly that you *should* provide simple tools available, both for simple jobs, but even more importantly for complex jobs. For a complex environment, these tools are constructed upon the various abstractions provided by SQLA which you can fully access and manipulate in order to build the perfect tool for *your* job. In modern SQLAlchemy, an application using the ORM can drop into a Query, get data and manipulate with tremendous specificity and at the same time great brevity. Redundancy in code is at a minimum - if you've done things per our recommendations your high level code enters a transactionalized block where any number of mechanisms that you've configured and customized can take place, and data is loaded and persisted like magic. Except its not magic - *you* configured how the whole thing takes place. That is why SQLA has multiple levels of abstraction - by providing full access to a clean and fully-featured API, you get to build exactly the SQL interaction environment you need for your task at hand. For a small task, not much is needed, and for a large task, SQLA provides the tools. 
I asked a similar question on StackOverflow which got some good answers, and may be of interest to you: http://stackoverflow.com/questions/860313/sqlalchemy-is-convoluted
I stand corrected, thanks for clearing that up. 
My biggest problem with SQLAlchemy was that I could not find in the ORM doc's a way to delete a record without first having to select &amp; query the record from the DB.
the fifth result if you go to the "search" box and type "delete": http://www.sqlalchemy.org/docs/reference/orm/query.html?highlight=delete#sqlalchemy.orm.query.Query.delete edit: query.delete() has been "experimental" for some time hence its low-key presence in the docs. Its pretty solid now so I've added a "main" doc section for 0.6 at http://www.sqlalchemy.org/docs/06/session.html#deleting-based-on-filter-criterion .
I've read that before, but how do you use it without having a valid entity? `q = session.query(SomeMappedClass).delete()` ? I spent about 20-30 minutes messing with the API and searching the ORM tutorial as well as re-scanning over my copy of the SqlAlchemy book but couldn't find anything. The only alternative I could come up with was to use the SQL dialect engine directly.
is there a way to buffer it more sanely with psycopg2? 
&gt; q = session.query(SomeMappedClass).delete() that's how it works, yup. that will delete your whole table. use filter() to add WHERE criterion. Also the book is ancient, its based on 0.4. 
psycopg2 offers a "server side cursor" to do this. SQLA offers a "ss cursor" mode if you pass `server_side_cursors=True` to `create_engine()`. However some users have pointed out that an SS cursor is not very efficient for all the smaller queries so we will be adding an ad-hoc `stream_results` flag to both `select()` and `query()` in 0.6, that should be patched to trunk soon its ticket #1619.
&gt; ... that will delete your whole table... That would explain somethings with one of my pet projects :)
You don't have to generate tables with inspectdb, it's just a time saver. I've manipulated the Django model concepts into some pretty weird database systems and never had a problem.
Yeah, really... I guess ranting on reddit gets good karma points, but it really does suck pretty hard for a developer who has obviously spent years and years of man-effort to give away a really good piece of software to *surprise!* see their package listed on the front page as an "unholy mess". Gotta be pretty soul crushing really. Protip: if you use a piece of software and have a problem with it, you can use the tactical nuke of calling it shit on reddit to maybe solve that particular problem without expending any actual effort; but only once. Well.. I guess unless you create another handle.
&gt; If I use a declarative map, and take advantage of the way the ORM allows me to specify relations as table properties etc, how do I then go and use the query language? Have you try querying using `join` on mapped relation yet? With `eagerload`, `defer`, `add_column` and `join` you can do most query at ORM's layer that will still result in a good performance. &gt; Do people really swear by this? Why not? Other library only give half of what SA offers, the ORM part, without another half of the picture. How is moving to ORM going to help you when it comes to writing complex query that they can neither easily express in ORM query term? That said, lots of complex query that looks like it needs to be written at SA's SQL layer can actually be written in SA's ORM layer. But then you would have to give some example of what you don't know how to do it and I'm sure many here will be able to provide the answer.
Here's a programming tip: when you get issues like this, **assume you're the problem until proven otherwise**. Then read the docs, read them some more, rewrite your code and if it still doesn't work then take a break.
A crude solution that is sometimes good enough is to select individual rows, each with its own query.
Their comments on trying Elixir are good ideas. Elixir definitely makes things simpler. Really, though, every ORM I've tried has been a huge pain. The Google Apps Engine has an awesome setup. I want an ORM that is as simple as Google Storage.
Since I didn't really like any of the default Style Sheets I made my own based on the Choco Style Sheet from Textmate (as far as I'm informed) and thought I would share. You can find it here: http://filesmelt.com/dl/Choco.ess
Ultimately, I rolled my own on top of the Python Database 2.0 spec. This gave me connectivity to the widest range of database implementations from any platform supported the python language and the db spec. I mapped tables to classes and rows to instances of those classes. I used meta classes to share the class behavior between the table classes, and class variables to capture things like database table name and key column names. I generated the table classes from the database meta data. The selects always retrieved all the primary keys, plus any other columns you asked for. The inserts only inserted the columns you populated, but required that you populate the key columns. The updates required that all the key columns were populated, and only updated the other columns that were new or changed. Updates that would change no data could be automatically logged and ignored. Failed updates due to missing target row could fall back to an insert. Deletes required that all keys were specified. Deletes of missing rows could be automatically logged and ignored. I enforced foreign key relationships on the key columns only. I let the database fail on a foreign key violation for non-key columns. Another way of putting this is that I validated master-detail relationships between tables. This allowed safer operation with a database that had all foreign key constraints deleted or disabled. This tool met my basic requirements for an automated interface, but was simple enough to easily use from the command line. Both compile and run time were great with no lengthy delays during either to read the db meta data. I had a lot of fun writing it and learned a lot about using Python metaclasses in the process.
I am totally in favor of hand-rolled toolkits for the purposes of learning, fun, and if you happen to have one, for the occasional production application. But what about your hand-rolled toolkit made it ideal for any so-called database designed by "database professionals"? As opposed to ideal for exactly what you needed for that particular project ? I believe your characterization of SQLAlchemy was more informed by your desire to roll something yourself rather than anything specific about SQLAlchemy - since I'm sure you realize there's nothing about your description above that isn't similarly doable with SQLA - either approximately via the ORM or exactly via buliding your metaclass framework using the expression language and dialect facilities to handle database communication. The expression language and table reflection utilities alone would make your "wide database connectivity" claim a reality, as it abstracts away most vendor specific SQL quirks (which plain DBAPI does not) and provides all the details of reading table metadata from every platform (if you're considering the "information schema" as a "standard", I'll save you some time - it's absolutely not standard at all, not globally available, and performs quite poorly in some cases). What is ironic here is that SQLAlchemy's broad based collection of SQL- and DBAPI-level utilities is intended *exactly* for those who'd like to hand-roll their own persistence layer. 
Why are people upvoting this? Django fanboys? C'mon This is a tipical situation of RTFM.
sys.argv contains the arguments entered at the command line. You can also use [a program like getopt](http://docs.python.org/library/getopt.html) that does most of the work for you.
i'll check it out. - thanks a ton
I often have similar problems not knowing quite how to pose a question to Google, so this is how you'd pose this one: [python command line arguments](http://www.google.co.uk/#hl=en&amp;source=hp&amp;q=python+command+line+arguments&amp;btnG=Google+Search&amp;meta=&amp;aq=f&amp;oq=python+command+line+arguments&amp;fp=d85c830bfd253bed).
yeah, i always try to exhaust google's powers before asking around but sometimes i just need to write the whole thing out ha ha - thanks 
I needed a toolkit which honored the database design. The impedance mismatch between the object model and the database model in the toolkits I evaluated seemed to favor the object model ahead of the database model. The only database engines of serious interest were Oracle, MSSql, and DB2. In the absence of a native DBAPI implementation, I could use an ODBC implementation to provide restricted functionality. The hardware/software platforms included Python and IronPython on Win32 and Win64, Python on Linux, Python on cygwin. A serious performance problem resulted when a toolkit read the database metadata from the database. In some cases, a tookkit would read all the metadata at once and construct its table objects or classes. In other cases, a toolkit would read the metadata for one table at a time, then chasing through all foreign key relationships to find related tables. Depending on how you used SQLA, it could use either of these strategies. I wouldn't call these a deficiency in SQLA or any of the other toolkits I examined. I think they met their design goals. I needed something simpler, and was willing to allow the database engine to catch and report errors.
We have a new inspector API in 06 that gives you the table metadata as strings/lists. The table objects build on top of that but you can just the info by itself.
&gt; Pooling in the application also means that whatever initial metadata the DBAPI requests from the database per connection is decreased. Try out pg8000 and watch your PG logs - it emits a ton of SQL when it first connects to figure out datatypes and encodings and such. This seems more like a bug in the DB driver; the initial SQL it sends on establishing a connection should be configurable.
Once I got my head around SQLAlchemy, I loved it to bits. If you can't get it to do what you want to do, ask for help on the irc channel. It all makes sense when you get it working.
if lxml does this i didn't know, but this is what I was hoping for when looking to parse XML in python: from amara import bindery from amara import xml_print MONTY_XML = """&lt;monty&gt; &lt;python spam="eggs"&gt;What do you mean "bleh"&lt;/python&gt; &lt;python ministry="abuse"&gt;But I was looking for argument&lt;/python&gt; &lt;/monty&gt;""" doc = bindery.parse(MONTY_XML) m = doc.monty p1 = doc.monty.python #or m.python; p1 is just the first python element print print p1.xml_attributes[(None, u'spam')] print p1.spam for p in doc.monty.python: #The loop will pick up both python elements xml_print(p) print lxml certainly wasn't bad though.