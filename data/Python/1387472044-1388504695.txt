I stumbled upon that by accident, it's so awesome that the developers put that in. I think that that's one of the beauties of Python.
Who needs Cython when there's [Rust](http://rust-lang.org)?
Lol what? Well, you do say &gt; if noone is working on the project or willing to at least review potential contributors which isn't the case &gt; the project is dead which it wouldn't be if others started working on it :)
Pyramid itself is about 35K LOC, but ~24K of that is tests. So it's really about 11K lines of code.
well we also push the crap out of each new major version with a very large number of changes, and the use cases are complicated and often subtle. so we really need to give a heads up to people as to what to expect.
I get this on both computers: Hi GitHubUser! You've successfully authenticated, but GitHub does not provide shell access. Connection to github.com closed. On the laptop (which is the one that works!) it initially failed. I had to make copies of github_rsa* as id_rsa* to get it to work. I would assume that ssh keys are working by this response. Do you know what else I should check? Also, thanks for bearing with me and helping me with this.
In 2.7, variables don't leak from generator expressions, only from list comprehensions. 
I think at this point I'll export my PyCharm settings from my laptop and import them to the desktop. I'll also copy the project over (I manually copied files to the laptop so it's up-to-date now) and replace the entire project directory on the desktop. EDIT: After doing that, it STILL doesn't work. Hrmm... I should mention that if I pop into the Git Bash command line for windows and navigat to the project directory, I can push just fine with that. It works... I guess. I think I may need to look for PyCharm specific support.
I stopped using Ipython, because all the magic didn't add a great deal of value. What it did do was create subtle scoping errors, and make debugging my django code much more difficult.
Even if that were for the same audience as Cython it wouldn't be a good idea. The project has specifically said that it is not production ready and there are breaking changes all the time. 
Not sure if you were running into [this bug](https://code.djangoproject.com/ticket/17078) or not, but as of Django 1.6, IPython is called in a way that fixes a bunch of scoping issues.
There's probably an API to change the prompt.
Probably, but I only found it mildly useful.
Yes, cython is not really a language, it's more some major hack and a syntax change which allows huge performance increases. I think it works well, but I'm sure it can be improved. I'd love to hear Van Rossum on Cython, and what he thinks about it, and know if he has suggestions.
Ah well, to each his own.
In the regular Python shell, the variable `sys.ps1` controls the prompt. I haven't used iPython but it's possible that it just changes that variable and/or it can be changed back?
i was already using ipython what should i switch to
I am not really sold on it yet. I do django and usually fallback to the shell for testing out snippets of code or just sanity checking something, I rarely use the REPL to get stuff done. It just doesn't fit into my developer workflow.
Haskell 
This is OT (sorry). I believe we're concerned about different issues. Deprecated and new features are two separate things. Timedelta64 is a new feature in numpy 1.7. The Ubuntu LTS python-numpy package is version 1.6.1. Avoiding deprecated features does not add new features (e.g. timedelta64) to outdated OS packages (e.g. LTS python-numpy).
Use of ipython as your REPL for programming is dangerous, especially for testing -- it doesn't behave precisely like python. It's a great matlab replacement though.
It didn't recognize the PAGER environment variable (or play nice with it) and it adds whitespace when text is copied from the at console. 
Try running IPython with the `--classic` flag.
dreampie 
Not my project, but I know the guy who works on it. CVX is a great modeling tool for Matlab and I hope CVXPY matures and gains similar popularity.
Are we talking about ipython or the regular shell? It works fine in the regular shell, not in ipython.
Unless I am blind this misses the feature I use ipython for the most: adding a question mark after any object prints the docstring. Much faster than searching online documentation. In [1]: open? Type: builtin_function_or_method String Form:&lt;built-in function open&gt; Namespace: Python builtin Docstring: open(name[, mode[, buffering]]) -&gt; file object Open a file using the file() type, returns a file object. This is the preferred way to open a file. See file.__doc__ for further information. 
that’s nothing but FUD. everything that ipython does different from python is 1. clearly recognizable even by inexperienced people 2. a syntax error in python there are shell escapes (prefix line with `!`, which is a syntax error in python), and magics (prefix line with `%`, which is a syntax error in python). as explained [here](http://ipython.org/ipython-doc/dev/interactive/reference.html), if `automagic` is on, all magics can be called without leading `%`, but they would still be syntax errors in python (`cd foo/bar` isn’t valid python). that’s it. shell escapes, magics, and automagic. else it behaves perfectly equivalent to python.
PyCharm
I'll give it a shot but it doesn't look like an improvement over ipy notebook. If ipy notebook had emacs/vi keybindings it would basically be perfect. in fact, I bet a python shell inside of emacs could be really great.
yeah, you’re OT indeed, as i said: &gt; just write code that isn’t deprecated in 1.6 and it will work in 1.8 if they have any sense (which i’d guess they do), and depend on numpy&gt;=1.6.1 so why are you discussing new features with me? it’s totally fine to stay away from new features until they arrived in all major distros, i never argued against that.
But the `In[1]`, `Out[1]` are very useful in ipython for referencing previously used commands when playing around/exploring data. Sure you can use `_` to get the output of the previous command, but how do you get the result of two, five, ten commands back?
ipython takes it one step further and you can use two question marks to get the definition. In [4]: os.path.exists?? Type: function String Form:&lt;function exists at 0x10902ced8&gt; File: /Users/me/testenv/lib/python2.7/genericpath.py Definition: os.path.exists(path) Source: def exists(path): """Test whether a path exists. Returns False for broken symbolic links""" try: os.stat(path) except os.error: return False return True
Would love to go!
Unless you use **IPython**. Then, you should *not* change your python shell.
You could always try [bpython](http://bpython-interpreter.org/). It's kind of like IPython light, just a REPL that does stuff like syntax highlighting and code completion
Of course there is a python shell mode for emacs. 
I never type exit() in python, nor exit in ipython - I just hit control-d to close the input. Actually you have to hit it twice in ipython.
OrientDB has a Python binding (actually 3 different bindings). https://github.com/orientechnologies/orientdb/wiki/Programming-Language-Bindings Just started trying it out, don't know how it is yet. Saw it here: https://news.ycombinator.com/item?id=6935709 
The next release of IPython will have vi-style keyboard shortcuts (not 100% identical to vi, but a similar model). If you're an Emacs fan, have a look at [Emacs IPython Notebook](http://tkf.github.io/emacs-ipython-notebook/).
`sys.ps1` doesn't work, but you can set `c.PromptManager.in_template` in the IPython config file. [Docs](http://ipython.org/ipython-doc/dev/config/details.html#prompts). This will only work in the terminal, though - the Qt console and the notebook generate their prompts separately.
Thanks, it's not my work, though - [this guy](https://github.com/tkf) wrote it.
I use IPython because the default Python shell won't start for me in an Emacs shell (Windows 7). I could try installing Emacs in Cygwin, but for now IPython works just fine. 
IHaskell. Remembers every variable you've ever used, to make sure you never change them.
ipython also formats tracebacks differently and so forth. Including "reload" functionality is also an attractive nuisance.
&gt; Haskel Ahhh going for the ole pneumatic computers, [huh](http://www.haskel.com/)? I've heard they use soapy water as debuggers sometimes.
Bpython
Jython is very much alive. They are currently fixing the SSL support. This would allow pip and easy install to work.
Any reason why there's no operator `__pow__` instead of square and sqrt functions?
It's less useful for web apps because you constantly need to restart. It's fantastic, especially the notebooks, when you have long running sessions with numerical code and look at lots of plots.
Magics are macros. They give you access to the string version of the arguments to your function, but otherwise you can do the same things from a normal function that calls into the ipython internals. This is sometimes called pass-by-name semantics.
Vanilla python also has `sys.displayhook` which you can define to do anything you want, including printing the `__doc__`. &gt;&gt;&gt; def displayhook(obj): ... if obj: ... if hasattr(obj, '__doc__'): ... print('{}\n{}'.format(obj.__doc__, '-' * 80)) ... print(obj) ... &gt;&gt;&gt; sys.displayhook = displayhook &gt;&gt;&gt; 'abcdefg' str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.__str__() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. -------------------------------------------------------------------------------- abcdefg **Edit:** If you define this function, you also need to explicitly set `__builtins__._` 
Auto reload alone is enough to make me switch. I can't express how annoying it is to have to keep restarting the stock python shell each time I make changes to some of my scripts. Trying to force reloading of a script is annoying and it was always simpler to just restart the shell, and this will make it a lot easier. Thanks for the article!
Universities. I work for a university and do a lot of non-web-based python. Although when I do write web applications for work we use Pyramid not Django. There are a lot of scientists and researchers in universities who could probably use a good programmer. Whether or not that's something you are looking for is another question.
My python shell is usually Sublime Text, otherwise IDLE. Edit: I can see by the votes that people have a generally negative reaction to this. I'm curious to know why.
I was gonna snark about how I only ever use the python shell for quick prototypes, but within a few lines of this article it became apparent the iPython shell is a lot nicer for quick prototypes...
Beyond using IPython as your python shell, you can use the QT Console to get a terminal-like experience. In fact, IPython can basically become your system shell/console.... http://ipython.org/ipython-doc/dev/interactive/shell.html
This is great. I was looking at porting some of my matplotlib heavy code to d3. It's heatmap heavy so I might look at this over the break. 
Don't forget to use ipdb instead of just pdb for debugging!
Tracebacks are formatted to give you more information, but that should never interfere with how your actual code runs.
dont use packt they are a scam and authors are rich... just download from bittorrent.
Youtube is probably the single biggest Python app out there and they have their HQ in the bay area.
This all depends on what you want to do. If you want to carry on in hardware testing, look for roles along those lines; the process of hardware testing is one that's going to have similarities wherever you go, and it's something you're experienced in. Python was merely a tool you used to get it done. If you want Django based roles, then get learning Django. Set up Virtualbox, throw a Linux distro on a virtual machine, and test out rolling out code as a proper web service in your own backyard. If an organisation is really set up for production rollout of web applications, the chances are that you're not going to touch the configuration of this yourself, and it'll be someone else's responsibility to handle server setup and administration (so you can focus on actual coding) - but it's worth knowing the basics of how this may be set up (there's more than one way) under the hood for debugging purposes.
Actually, once is enough if you disable that annoying-as-hell "yes I want to quit, just like the last 10,000 times" confirmation prompt. 
I based it off the class posted [here](http://blog.ebookglue.com/write-language-detector-50-lines-python/), it was a lot of fun to build over the last few days and I learned a lot. 
Something else to think about is that Python is used a lot in research and scientific computing. Your math background would be a great asset there. These jobs might not be advertised as Python jobs, since Python is more the means to an end.
Just type `doctest_mode` (or type `doc` and press tab) and hit enter and you get the usual `&gt;&gt;&gt;` in IPython. 
Google will hire you because you're a technically excellent engineer though, not because you happen to know python.
Why doesn't ?? do anything different than ? for me? In [3]: open? Type: builtin_function_or_method String Form:&lt;built-in function open&gt; Namespace: Python builtin Docstring: open(name[, mode[, buffering]]) -&gt; file object Open a file using the file() type, returns a file object. This is the preferred way to open a file. See file.__doc__ for further information. In [4]: open?? Type: builtin_function_or_method String Form:&lt;built-in function open&gt; Namespace: Python builtin Docstring: open(name[, mode[, buffering]]) -&gt; file object Open a file using the file() type, returns a file object. This is the preferred way to open a file. See file.__doc__ for further information.
Probably because open() is written in C
I believe it's because the builtins you're trying to spec are implemented in C (at least in CPython). Try it on pure-python methods.
Hold on, `open` is a builtin. That means it -has- no python source. Try using ? and ?? on eg. collections.namedtuple.
Can you use ipython as a shell inside a virtualenv without installing a whole new copy? (On Windows)
While Fabric is fine and dandy for Python 2.x, what is a good deployment tool for Python 3?
I'm already using that, but this iPython seems pretty nice. Let's split the difference and go with fPython.
I've been using bpython for awhile, but when you hit the up arrow it shows the last line. iPython is showing the entire function definition again, which alone is enough to switch, IMO. It also remembers things across invocations, which is the other thing that annoys me all the time about bpython. I forget something, kill out, fix code, jump back in, and I have to retype everything to import and set everything up again.
Crosspost this to /sysor , they would be very interested.
Don't know why you deleted your post, I usually put an edit: i'm wrong, see below. 
I'm 23 and am working my way through the CodeAcademy Python course because I was curious (no previous programming experience). Your son has quite a bit of aptitude for this, it seems. I'm still getting caught up with conditionals and logic problems. 
[Python3 has a nice way to reload things.](http://docs.python.org/3/library/imp.html#imp.reload) Probably in 2 as well, but I'm not familiar with it.
Python 3 support?
Wow, a very promising start! Thanks for the link!
No connection, just had some great books from them recently. This seems a very generous offer, too good to miss!
Incidentally I'm currently picking through a codebase that uses a lot of monkey patching, and it's making it incredibly difficult to understand since I can't reference the class definition. Is there ever a case when monkey patching is better than normal inheritence or editing the class?
I'm curious what you found to be missing in pydev. I also use vim but I find pydev+vrapper to be pretty solid when I need refactoring. 
How does it work in Erlang? 
I've also had no problems with them and found the books to be great value. I'll be stocking up at these prices :-)
Class syntax. I very much dislike explicit self (although like using it to reference members, don't get me wrong). I hate \_\_init\_\_'s name. I think it should just be init, or have both work. But most of all I hate super(). Calling the base class constructor is just hideosly ugly. super(child, self).\_\_init\_\_(args), EW. If I had my way: class Tolkien(Linguist): def init(title, language): super(language) self.book_title = title
Good idea. Reading the code, I suspect such switch cannot be nested. Can it ? You may want to change the syntax to something like this: with switch(54) as case: if case(2123): with switch(55) as case2: if case2(2124): pass This way, you avoid having to use builtins.
You mean that target audience would be the writers of high performant extensions for CPython? I think rust is also suitable for that.
Cython is attractive because it looks very similar to python and can start by modifying existing python code. You cannot do the same with rust. 
I was about to say symlinks, but Windows...
check!
You are correct, it cannot be nested. Thanks for the input! I will test that out tomorrow.
I usually find this to be very dangerous, because two languages clearly have different semantics (regarding memory management, for example) and I'd prefer them to have different syntaxes. Otherwise it's easy to get even simple things wrong.
the IDEAVim plugin for pycharm is wonderful, makes me feel like I'm right back in vim coding away. 
It is an interactive shell, but does not allow for any kind of input or output. Because that would not be pure, you know.
SLIME. Now, that's a REPL to end all REPLs.
You need to feed it text to predict. Look at the example on the github page
New to python: I rarely use the shell at all, really. I just make my scripts executable. Is there some reason I should be using it?
PacktPub books, in my experience, can be kind of hit-and-miss in terms of quality. Any recommendations?
I'm pleased by how well it works, i think i have to play with this
I emailed Frank back in October and this is what he pretty much said: "It's really hard to give solid estimates as resources are light lately. A beta 2 is in the works that will have improved ssl support and will (I think) allow the core of Twisted to run. I don't have an exact date for beta 2 or for a final release. I wish I could give one, but I can't do that yet."
Aptana Studio.
Checkout [liclipse](http://brainwy.github.io/liclipse/). LiClipse (http://brainwy.github.io/liclipse/) is recommended for users that want a PyDev standalone with a hassle free install where things should 'just work' (also, by licensing LiClipse you directly support the development of PyDev). quoted from [pydev adventures](http://pydev.blogspot.com.br/2013/12/pydev-310-released.html)
I got this during the "end of the world sale" (around december '12). Great IDE. 
Woops! Thank you, I just pushed a fix
Thanks. I was unaware of that sub.
It definitely should, just fixed it
LOL, GPL license for such small and not as important code? It is not even a library. Who do you think would use this if it forces you to use GPL. I think even BSD license is an overkill for this. 
This has good potential. Being able to make use of (partial or not) D3 from Python would be very cool indeed.
Yep, and now i'm curious abot how well it works for the other potential uses mentioned by the blog post
[Bay Area](http://en.wikipedia.org/wiki/San_Francisco_Bay_Area)
Heh. This the same issue I was trying to tackle when I wrote StupidParse (https://github.com/justanr/StupidParse). Though, I was dealing with parsing text messages at the time.
PyCharm is fantastic
I think this point should have had much more emphasis in the article! 
As a professional Python (et c) programmer in Florida, I know there's some Python jobs in Bay Area of Tampa, and around the bay that cuts NASA KSC off from the mainland. So, California has a bay too? Cute.
Well congratulations. I think this is unnecessary, and it makes much more sense to stick with `dict`s if you need this functionality, but I applaud your ingenuity.
A thing an IDE could help with is navigating between views, templates, and urls. e.g. * Go to the template for this view * Go to the view for this template * Go to the urls route definition for this view * Go to the view with this url route "name" (I think pycharm currently only does about half of that.) Show the various properties that get added to models by their relations. (Foreign keys that point to that model and the attributes added by model subclassing.) Also, if you could speed up the feedback cycle when editing templates, that'd be helpful. Another thing that took me a while in debugging the last few days is "This request got a 404 response, where did it come from?" — you can't look for uncaught exceptions, because it's been caught and turned into a response (or maybe it was returned as a response directly), you can't look for caught exceptions because things like the URL routing throw and catch a million 404 exceptions anyway. It might be in some middleware, it might be in a view, it might be in some other code that gets run before or after your view that's added by a decorator, or a mix-in, or a decorated method on a mix-in... It might not be on the view you're expecting it to be on, if you messed up your URL regexps. Those are the django-specific things I think of at the moment. You could look to django-debug-toolbar or django-devserver for some other sorts of information that's useful during django development. Maybe a richer interface to the logs, better visualization of how many and what kind of SQL queries were made. Have the test runner record how many database queries were made for each test case (and yell about it if that number changes a lot between runs). 
Are you (connected to) the uploader of this video? I wonder if we could get a higher resolution version of this.
* Not slow
While I grasp the general concept of asynchronous or event-driven programming, much of the implementation in PEP 3156 is hard for me to imagine or follow. Anyway, I was reading PEP 3156 and I noticed all of the stuff in there about internet connections. In the past, if I've wanted to connect to a bunch of different sites to grab data, I've built queues and threads and then `join`ed threads to wait for them to complete. With queues it's not very difficult but I do end up waiting for the threads to complete, which I find negligible because the timing is reduced by the number of threads. In addition to other other issues, is this library meant to replace that method of solving this I/O problem? Would you be willing to offer a kind of pseudocode example of how this might look using the new asyncio library?
For what is worth, have an upvote. I think it's just silly using the GPL license on something that it's meant to be imported/integrated to other stuff. Do you want people using your code or not?
&gt; * Go to the view for this template This isn't always possible, since templates could (and should) be reused for multiple views. To get back on topic - Intellij Idea professional edition w/python plugin satisfies most (but not all) django/python needs I have on daily basis. The things I lack are: * Better integrated django shell (ipython-ish shell preffered) * Better nose integration * Needs to be faster Alas, there's also one "almost-a-deal-breaker" bug in the version I use (12.x), but I'm not sure if it's machine/os/java version specific, or in fact a real Idea bug: Sometimes, the keyboard completely stops responding inside Idea IDE. I couldn't find exacts steps to reproduce so far, but it "feels" like it has something to do with pressing ctrl+tab too quick (eg: renderer doesn't finish drawing to the screen before Idea receives next ctrl+tab keypress). So, I'm adding this to my list: * It works. :)
I suggest you put your resume on Dice.com - I was emailed about a job testing hardware in the SF Bay area a month or two ago. 
worth every penny
Have you considered security? A lot of computer security shops use Python.
You get what you pay for.
...or [mod_wsgi](https://docs.djangoproject.com/en/1.6/howto/deployment/wsgi/modwsgi/) with Apache.
Here's an [example](https://github.com/feihong/tulip-talk/blob/master/examples/2-tulip-download.py) of getting a single request. To adapt that to multiple simultaneous requests, you would make a list of coroutines (e.g. `tasks = [download('http://foo'), download('http://bar'), ...]`) and then call `wait` on them ([example](http://docs.python.org/3.4/library/asyncio-task.html#example-parallel-execution-of-tasks)). 
Yes - if you start it when you have a virtualenv active, it should add that virtualenv to `sys.path`, so you can import things from inside it.
The title's a little sensational but namedtuples are really cool
Might be an east coast/west coast thing but I'm from Portland and I've never heard of a different bay area than _The_ Bay Area in California
&gt; This isn't always possible, since templates could (and should) be reused for multiple views. Sure, but in that case present a list of multiple choices. I guess one thing I'm trying to get out of that sometimes is "what is this 'foo' variable in the template," which you could even have some sort of smart lookup for, but that's even harder to implement. &gt; It works. Yeah. pycharm does work for me, mostly. I've had that keyboard bug a few times but not often enough to be a real problem. The only other real showstopper bug I had seems to have gone away when I upgraded to the latest prerelease. But, as I think I've [mentioned before](http://www.reddit.com/r/programming/comments/1n1jdy/jetbrains_releases_free_edition_of_pycharm_ide/ccf0gpd), I have filed more bugs against pycharm than any other software package I've used. I can only assume their test suite is not comprehensive.
People are downvoting, because it's advertising? I find it worth while I even found a new interesting open source project just because one of the books mentioned it.
Pycharm
The community edition is free for hobbyist programmers
&gt;Do people actually read ? You must be new around here...
OP isn't asking for a recommendation on an IDE to get. OP wants to make an IDE and is looking for suggestions on things to include. 
Most of the issues I see cited are regarding code quality, glaring bugs that a technical editor should have been able to catch. That said I've certainly purchased PacktPub books in the past and purchased 2 Python books today after looking at Amazon reviews: - Tkinter GUI Application Development Hotshot - Learning IPython for Interactive Computing and Data Visualization I also picked up a couple non-Python books (PostgreSQL Server Programming, jQuery Game Development and Game Development with Three.js)
Possibly because it's not directly Python related. If the title had been "$5 Python eBook Bonanza from Packt Publishing" it might have done better in this reddit.
I'm _from_ the tri-state area and I know that "the Bay area" (he even capitalized Bay) is always always San Francisco. 
 ipython manage.py shell Tab completion alone is worth it. 
And the code is three times as long as it needs to be! import builtins def switch(switch_target): builtins.case = lambda x: x == switch_target builtins.default = lambda: True return True There, that works exactly the same way. I don't think copyright even applies to snippets this small. By the way, `default` is not called in the tests or the README, so it might just as well have been `True` instead of a function that returns `True`.
text editor and bash. no IDE needed. no IDE wanted. if you're looking to build features into an IDE (which I wouldn't use, but some might) the most important for me would be automatic test running in the background, like a simulated local CI server, and integration with browsers so editing templates would re-render immediately.
the community edition doesnt include django support....
It has performance reasons. I once submitted an alternative implementation without exec but it was a bit slower, so it got rejected.
That's great. Thank you for obliging my request. 
You're joking about the "tri-state" thing, right? There's like two places in the continental US that doesn't call itself the "tri-state area", and then it's quad- or the end of peninsula. I think my point about Reddit being bigger than one's region is lost.
And _my_ point that certain regions have certain common names associated with them is lost. Of course, there's more than one bay area and more than one tri-state area, but traditionally they mean SF and NY-NJ-CT. To suggest otherwise is simply pedantic. 
Email sales and tell them that; they most likely will offer a discount off the personal edition (they did for me anyway). 
&gt;We've all been there - working on a one-off script or personal project where you deal with lots of tuples: maybe points on a coordinate grid or time-series data, or anything imported from a csv. You do the lazy thing and keep track of everything in a list of tuples, shrugging off that object-oriented nagging in the back of your head. Na man, NumPy arrays.
support for fkey to other tables ? 
Included, of course... Uses a rel link (and will eventually also pass this back in the header)
What's the interface difference between NumPy arrays and tuples that allows for NumPy arrays to be a replacement for the namedtuple?
In actual use. The exec version hardcodes everything, there is no way you could beat that. http://bugs.python.org/issue3974
Terrible title, terrible article. Could have been a self-post one-liner: "Here is a link to the namedtuples documentation: (link)"
If you are making an IDE, I have a list of things that basically make it so I can't use most any of them. at the end I will list what I do to get around these, but I would love an IDE that just worked: 1. I tend to play around with pypy builds, a LOT, right now i have something like 30 pypy builds floating around because each one has a different set of compiled features. (eg, stackless, CFFI, different optimizations/JIT/GC to play around in to see how my code might change...). No IDE I have used plays nicely with this so far, having not just maybe virtualenv's but actually separate interpreters for many projects. 2. I also like to play around with server-ish stuff and sub-processing for quick scripts. Sadly the IDEs that *might/kinda* play nice with multiple interpreters tend to fail here very badly if I make a mistake and need to kill the process. This also goes true if i play around with CFFI and `malloc()` some memory. 3. Small scripts are hard! Sometimes I want to use python instead of bash to help me script something a little bit more complicated, but almost never does an IDE help with small single-file scripts. Always making it much more complicated than needed. 4. Let me launch shell scripts in a terminal for goodness sake! This goes back to me playing with writing my own server code, sometimes I have to kill them because I am a fool. This might leave files I need to clean up, or I might want to launch 3-5 scripts side-by-side while testing the main server or some such. Most IDEs make it a real pain to be able to for example launch a gnome-terminal with some arguments... 5. I like to try and use colors for my logging to help distinguish the different levels as I develop, I have yet to encounter an IDE that played nice with this. Although it has been a while, most times IDEs fail me before I get this far. I realize that I might be a bit strange for IDE usage, but the fact that run-away processes almost always make an IDE unusable (or at least orphans the process in the background...) is criminal! Also that python is *so great* at small scripts that are just a bit large for bash or some such is awesome! Until you find out how annoying it is to make them with an IDE... so, here is what my current set up is kinda like (although it changes rapidly in the details as I play around with other ways of doing some thing.): 1. I use SublimeText as my "IDE". Using quotes because it really isn't although I have really abused parts of what SublimeText does before... 2. Using SublimeText makes it easy to add a very small plugin or such that executes a shell command to launch certain files. [here](https://github.com/admalledd/sublime_text/blob/master/Default%20%28Linux%29.sublime-keymap) is the basic key map that I use, the two key ones are the first two: allowing me to launch my general "pypy2.7" or "pypy3.2" on the current open file. This makes it "easy" to launch a single script (commonly one-offs). 3. By using/abusing the "build" function in SublimeText I have it instead run "main.py" or "runme.sh" for the current project when I am doing something that is larger than a few files. These are also launched in terminals, fixing the issues of console coloring and such. 4. Again, by abusing the project files I can change my key-map per project. In this case it means I set up eg ctrl-alt-(w|e) to point to the pypy/python interpreter I need to use for this project. 5. The fact that all of my pythons launch in terminals lets me manage any sub-processes they might have if/when things break. Although this isn't ideal, its much better than any IDE has so far. Some IDEs might have "kill script" or some such, but very often might miss the children processes or might not actually kill the process due to there being some non-daemon threads lurking about. Just me ranting that I *want* to use IDEs for my coding, but so few actually support writing code how I write. Most of the time the IDEs are WAY to focused on being "web smart-y" and such. Sure coding django or flask or whatever is awesome to have an IDE for, but there are other python programmers out there! Don't leave us hanging...
NumPy arrays are generally better when dealing with things more than one record at a time. You can slice on rows or columns (or more dimensions if your array is N-dimensional). If you're dealing with a CSV one line at a time, namedtuples are probably fine. When you have a huge file with thousands of rows where each row is e.g. a time stamp and a sample from each of five data sources, NumPy arrays start to be a better idea. Pandas is even better for this use case (it uses numpy underneath, but gets rid of a lot of drudgery). Plus numpy arrays are fast. I love namedtuples for lots of other things though. They're wonderful, self-documenting, lightweight datastructures.
How are they on memory usage?
well, if you can edit the class, do that instead. my example is only directed to bugs in upstream packages, that dont get ported back after a fix or dont receive updates for your repo. normally you should never monkey patch but sometimes it is very convenient... a nice but ugly feature.
Well I know how reddit is in general, but I was hopping for something better in r/python.
&gt;I've never used Django in a production environment This is where I am. I'm trying to get a Django job, teaching myself Python and Django. What would qualify as a "production environment" do you think? What sort of standards?
The reddit post title still says "Advanced Data Structures". 
Are the docs updated? I'm trying to follow [this](https://sandman.readthedocs.org/en/latest/admin.html) to play with the admin interface, but all I get is: ImportError: cannot import name activate_admin_classes
no, it's still a tuple - but what you want can be achieved using the **_asdict** method &gt;&gt;&gt; import json &gt;&gt;&gt; json.dumps(Point(2,3)._asdict()) '{"x": 2, "y": 4}' **edit**: another way, which is possible with other classes as well is using ** __dict__ ** &gt;&gt;&gt; import json &gt;&gt;&gt; json.dumps(Point(2,3).__dict__) '{"x": 2, "y": 4}'
Not quite as elegant, but you could always use `_asdict`: &gt;&gt;&gt; json.dumps(foo._asdict()) '{"x": 0, "y": 4}'
Do you know how to do this? This is the most annoying part of iPython.
It would be even cooler if it would ask the database for table definitions. Combine that with a command line script that just takes a database url and then pops up a browser window with a frontend and you have struck gold.
Jeff, I'd just like to thank you for your guide to open-sourcing a python project. It helps take some of the intimidation factor out of making one's work public.
My point being is that $100 if you are a "professional" and not a hobbyist. 
Follow [these instructions](http://massivescale.blogspot.com/2012/01/how-to-disable-annoying-ipython-exit.html) to create a profile and turn it off.
Considering how similar your code is to this code from 2011 (and some other snippets I've seen dating back farther...) I think you may have an issue with the copyright and licensing. http://stackoverflow.com/a/6606504
Oh wow. I had no idea that existed. What do you propose I should do? I mean obviously remove them but should I like to that snippet as well? 
`pip install --use-mirrors --download-cache` Then again, if you are really serious about it, use a build system. Don't pip install stuff directly from pypi on production servers.
This. What's the problem with just issuing SQL in a command line shell?
It provides a programmatic rest interface as well, incase you want to quickly hack together a custom client-side js solution to something?
I just press Cmd-P in Sublime Text and type the name of the thing, like "Cmd-P art mo" for article.models or "Cmd-P art html" for the template. I don't know that the process could be made much quicker than it already is.
Well that's the thing about it... The idea of a "right" way to do something is really a subjective concept. As the article says, if you're being lazy, namedtuple affords you a wealth of clarity for relatively little extra effort. If this appeals to you, then use it.
Looks nice. Do you think it's worth adding redis support too? I found this project while looking to see if anyone had made a redis version: http://python-eve.org/
Is there any reason not to modify the import statement to allow pip style versioning? import requests==1.0.4 requests.post .... Obviously it would require changing the way the packages are installed / imported.
Does the admin support foreign keys? Also can I set up user permissions?
Does the admin include REST links? That would be useful. 
nice. this looks sick.
I'd like to be able to type a command like this to get a web interface of the entire database found in filename.sqlite. $ python sandman_connector.py --dtype sqlite filename.sqlite 127.0.0.1:8000 Does such script exist? It doesn't seem like it would be *that* difficult to write. But I don't want to waste my time writing something that's been written before.
I like fabric + virtualenv + tarball of pip wheels (similar to the deprecated pip bundles) the most. There's an example [here](https://github.com/ionelmc/projectskel/)
Your method implemented, probably not even worth mentioning the 12 lines of code (9 without empty lines) https://gist.github.com/Bulv1ne/8064219
This is nice. How did he get formated json from a curl command though?
Here's what I'd do. Get rid of the licenses, as copyright would be impossible to enforce anyway, then write a blog post about feeling a need for a switch statement in python, how you came up with your solution, and how other people can use it too. Your solution to the percieved problem is interesting and a blog post would be a good way to talk about how you think about code. A second blog post that might be interesting would be an analysis of the performance of this approach compared to the traditional way pythonists solve this problem.
The mirrors did solve the frequent outages. If you're a bigger shop, consider running your own private mirror. You can also build a wheel cache on a shared server. 
the best choice is Emacs + pony-mode
Interesting project! I am buying your idiomatic python book though - both versions! Keep up the great work :)
docs are definite out of date. Here's how I got it to work: from sandman import model model.activate()
Not everyone who wants to see the contents of a database knows SQL.
It seems like everyday I become more and more impressed by all the features (without any observable performance hits) that are to be found within PyCharm.
[FormAlchemy](https://github.com/FormAlchemy/formalchemy) is also fairly similar.
For the most part, I was blissfully unaware of Fabric's existence. After some short investigation, it seems like Fabric is targeted more at remote task execution rather than local, and its API seems a little more complex and heavy. I'll definitely look into that a bit more and consider it!
sandman is built using Flask. You have to understand that those "libraries" you listed are just web frameworks that do don't do a whole lot on their own; you have to put at least a bit of effort into getting what you want, and sandman is that effort.
This is what I use Fabric for
Fair enough. Like I said, I don't do a lot of web programming, so I don't actually know much about those libraries. It seem like whenever anyone talks about a web API flask comes up, so I'm not surprised.
No, I do name every testing program I write 'der.py' though.
How it is different than [paver](http://paver.github.io/paver/)?
It is not lazy or bad. The article is a bit tongue-in-cheek. Namedtuples and classes are different in many ways. Thus they have different use cases. The most important differences are: * You cannot define methods, constructors, etc. for namedtuples. * A namedtuple is a tuple. As a result it is immutable. The value cannot be changed after assignment: In [2]: spam Out[2]: Point(x=0, y=4) In [3]: spam.x Out[3]: 0 In [4]: spam.x = 1 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-4-e5e7e6c675ef&gt; in &lt;module&gt;() ----&gt; 1 spam.x = 1 AttributeError: can't set attribute It's another tool in the toolbox. And sometimes it's a wonderful tool for the job.
Please do a blog post.
The ability to build declaratively generated UIs for end users. Similar to Django's admin interface -- which is incredibly useful. 
Thats cool and all. But is it really usable for legacy databases? Usually they are legacy because non of their tables and columns make any sense anymore and there is tons of cruft around it to make sense out of it. There is tables with names like say accounts but it is now actually list of half the accounts and half of it describes customer issues, depending on some columns value in accounts, if we are lucky, many times it is another table with a name like pokjreaja_hdfgas which determines the menaing of rows in accounts table. 
For the 2.0 version, Fabric is being split into a few parts, to separate the task execution and the remote stuff, the base being https://github.com/pyinvoke/invoke. Maybe you'll want to compare to that.
Isn't that just a shortcut?
Thanks!
My main exposure to named tuples has been Datatax's Cassandra driver for python where it will use them for Rows returned from the database by default. This caused me no end of issues because as soon as you start using them suddenly everything in the system needs to understand how they work - you can't just assume they're normal objects. For example say you have a number of things in a database that need to be processed. You pull our your result set, put them into a queue then fork off four threads to process the queue in parallel. Except you can't do that because you can't pickle named tuples. So now your code is full of wierd hacks to convert objects into other structures for no apparent reason. Also it's not like the fact that they are instances of Row actually gives you any useful information because you can't guarantee each object will match your table/columnfamily schema or anything - it depends what you put in your CQL select. So ultimately all your classes will know is that they've been giving this Row thing that may or may not contain certain attributes. Which would be the same as if you'd just passed a dictionary but actually a dictionary is simpler to work with. Note: I've only really written a couple of Python programs so working with namedtuples might be easier if you know more of the language. But my experience wasn't great. Note 2: You can change the cassandra driver to return dictionaries by overriding the row factory. 
This is marvelous, can't wait to see if I can help leverage it with our business folk to help prevent our programmers from having to generate some usually trivial reports ('can we see how many of this there are?').
That's a good one too :)
very cool and informative, thanks :)
For anyone who doesn't get it this is a reference to the [python dongle incident](https://www.google.com/search?q=pycon+dongle+incident)
Your comments, now with 100% more GitHub Issue: https://github.com/jeffknupp/sandman/issues/17
i understand your argument, and agree completely, but its a just tradeoff. sometimes you just need to get stuff done and subclassing would need you to dive in deep into the code of that external library to inject the class in the right place. if everything was about elegance and doing the right thing, we'd all be writing our code in smalltalk ;)
Django template syntax-aware highlighting (with html, js, and css highlighting in the same document) would be awesome. Jinja2 too, while you're at it. ;)
Your wish is my command! I'm about to update GitHub with a version that does exactly this. You now don't even need to declare the tables if you don't want to. simply by having the following 5 lines: from sandman import app from sandman.models import activate app.config['SQLALCHEMY_DATABASE_URI'] = '...' activate() app.run() you get a browser opened to the admin page containing all the tables in your database reflected and with proper foreign key relationships loaded (as well as the REST API, of course).
Looks great at first glance. 
Fabric is pretty widely used, especially for deployments. 
Split does split on whitespace by default so you can have just split()
Cool! I tried that, but now I get: ``` Traceback (most recent call last): File "test.py", line 1, in &lt;module&gt; from sandman import app File "/tmp/test_valhallasw/sandman/sandman/__init__.py", line 8, in &lt;module&gt; from flask.ext.sqlalchemy import SQLAlchemy File "/tmp/test_valhallasw/lib/python2.7/site-packages/flask/exthook.py", line 81, in load_module reraise(exc_type, exc_value, tb.tb_next) File "/tmp/test_valhallasw/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py", line 24, in &lt;module&gt; from sqlalchemy import orm, event ImportError: cannot import name event ``` and I can't directly find the causes of this. Probably an incorrect sqlalchemy version, but I tried both 0.8.4 and master...
 pyg = lambda s: __import__('re').sub('(\w)(\w*)', r'\2\1ay', s) not sure if this one is considered as one-liner: import re; pyg = lambda s: re.sub('(\w)(\w*)', r'\2\1ay', s)
YouTube runs mostly on Java and C++ like the rest of Google applications. Python had a larger role earlier in it's life, but not much anymore.
Good eye. Fixed in the docs and GitHub issue (#17) now closed.
Ugh. It's likely a requirements.txt issue. At one point, I unpinned all the packages (I forget what I was doing) and I forgot to re-pin them. Fixing this now.
Should be fixed now (version 0.6.1). Let me know, either here or via a GitHub issue, if you have any problems (I just pip installed it in a clean virtualenv and it worked)
"legacy" just means existing. So, it may not be as useful for legacy databases that have been sloppily maintained and are full of technical debt. Otherwise, it should work fine. And there are lots of techniques for making a crufty database easier to work on: * modify tables to make non-nullable columns non-null * modify tables to ensure implied unique keys (not primary keys) are enforced with unique indexes * modify tables to enforce referential integrity (pk/fk constraints) * modify tables to enforce check constraints - simple declarative statements that guarantee 100% of the rows comply with simple rule * transform &amp; copy data to a new schema/db that resolves nonsense and provides simple reporting. * create views that rename columns, leave out abandoned columns, union tables together or join tables as necessary. Some of these features may be incompatible with writing to the database - depends on your dbms. Even when it is - you can often use an 'instead of' trigger &amp; stored proc to support a write to underlying tables.
Woohoo! MKL with NumPy will definitely help me out at work.
The beauty of sandman is that you can rename all of that stuff to give it more sensible names, as well as adding custom behavior to the endpoints and let them follow whatever business logic you want. The idea, though, is to get a nice programatic interface to the underlying DB so that new services or applications you write don't need to interact directly with the DB and all its nastiness. And you get a free admin GUI that understands relationships. Dunno. I think that's valuable, but YMMV.
Yes, the admin definitely supports foreign keys.For example, creating a new record that has a foreign key relationship with another table will auto-populate a dropdown with possible values from the related table. You can set up user permissions through the `validate_VERB()` functions at the moment, but full HTTP authentication is coming.
Yeah, I know of the project, but only supporting Mongo is quite a limitation. I don't know how useful redis support would be as it's not an relational database (which is what sandman focuses on).
You're welcome! Glad you found it useful.
When I was at PyData in NYC I ran into the creator of slashdb at their booth (they were a sponsor), having never heard of it before. Obviously, seeing their product stopped me in my tracks. The polite sales associated asked me if I was interested in buying it. I think I replied, "Uh, I wrote that" before slashdb's creator came over. After I told him who I was, he said "Oh, yes. You're on my list of competitors," which was really strange to hear. 
There are a few errors in your documentation. For example, in your 'quick example' page the line "def scoring(self): return 100 if game.win() else 0 # For the AI" should have self.win() instead of game.win(). If I come across any more I'll post them here for you. In your tictactoe example you've defined a lose condition. The solver (id_solve) needs a win score. How would you define this?
Maybe [Django CMS](https://www.django-cms.org)?
[Mezzanine](http://mezzanine.jupo.org/) is a powerful, consistent, and flexible content management platform. Built using the Django framework.
The C in which CPython is written makes heavy use of preprocessor macros. Same deal, right? Text substitution or "code generation." It's an ancient tradition in software. We're just spoiled by all this introspective 'everything is an object' luxury code, but nearly everything we use daily was built in this manner.
Thanks, I appreciate the compliment.
I mentioned redis because someone who wanted to access the REST endpoint from javascript for something quick could get everything in one place that way. There wouldn't be any point in using it to hit redis from python except maybe to avoid a firewall.
I know. I just think it's an eye sore in the collections library. You are right about the "everything is an object". I'm just glad that I'm young enough that I'm not forced to learn C or the like to start programming (though, I'd like to learn C).
OP, you might want to add this link to your post. 
Hadn't seen enaml before. It looks interesting. Anyone here use it?
 https://www.djangopackages.com/grids/g/blogs/ https://wiki.python.org/moin/PythonBlogSoftware https://www.djangopackages.com/grids/g/cms/ http://www.reddit.com/r/Python/comments/1fr99s/moving_from_wordpresscom_to_a_selfrun_blog_any/
I think it is due to the fact that most web hosting companies only support php unless you get yourself a vps. Static site generators are the hippest thing right now and they have some advantages over database driven systems. But also a few disadvantages.
This is not about Python 2.7.
I believe it will be possible to write games with the interactive widget system, but probably not for the faint of heart, at least at first. It might be an interesting project to create an API for writing games, though.
Done, thanks
Specify a pypi-aware caching proxy url as an index url (with `-i` for the CLI and/or with the `index-url` option in the [pip configuration file](http://www.pip-installer.org/en/latest/configuration.html)) These pretty much cover it: * http://bitofcheese.blogspot.com/2013/05/local-pypi-options.html * http://jacobian.org/writing/when-pypi-goes-down/ (`~/.pydistutils.cfg`) Manually: http://docs.repoze.org/compoze/
Thanks! status.python.org by statuspage.io
Cool analysis. Thanks! If you still have the data on hand, do you have any data for how many [of the current latest versions of] packages specify an off-pypi Download URL? Ohcount and radon are also great for static analysis metrics.
/r/tinycode/
What are some example use cases for this kind of software? When would someone want a physical 3d representation of their data?
what kind of disadvantages?
Shameless plug: [Simple](http://github.com/orf/simple) is written in Python and I think its quite awesome.
Thanks, I'll probably repost to there!
There is no discussion as to why I should use it...
Anyone know if there's plans to release a Python 3.X version anytime soon?
looks very interesting, starred on Github and will try this. I love Flask, so I am sure I am gonna love this too.
As an example: You're a company that works on optimizing customer websites. Then you can print them the visitor statistics in 3D as a customer gift. It also helps to visualize/understand multi-dimensional data more easily. If you have a 3D visualization of two related datasets (e.g. tweet frequency about a product and number of times that product has been sold), you can see quickly whether there's a correlation between the two datasets. Of course, a suitable visualization needs to be chosen, e.g. this one: http://tangible.readthedocs.org/en/latest/shapes/vertical.html#tangible.shapes.vertical.RhombusTower2D In the end you could do data analysis and visualization without printing any objects. But they look nice and can help to make data more accessible / graspable to other people.
I never really tried it since I spend very little time with the interactive interpreter but I might try it again. Last time, I had to hack some config to get it to work properly with virtualenv. I hope that is ~~fixed~~ easier to do now.
agreed. while some of the features are nifty, I could be completely fine without ever having any of them. This articles should be titled "A few nifty things you can do with IPython"
 print len(set(li)) # 4 Don't do that, that's just a bunch of extra unnecessary work. `Counter` is a subclass of `dict`, and the length of a `dict` is the number of keys. There's no need to first build a `set` just to count keys.
Yes, the config trick that people used to suggest for virtualenvs is now integrated into IPython, so you shouldn't have to do anything.
Its basically one guy promoting his python blog on r/Python. May be abuse of reddiquete but I'm not annoyed by it, so I'll keep my hands off report function.
&gt; Let's display all inbuiltin function in python, starting with s. &gt; How can we do it ? &gt; Regex !!! &gt; &gt; s*? Oh wow.
You can't have self hosted comments. Most static site generators rely on external services like disqus.com
I decided to go ahead and clone the source. We use only about 10 of the libraries XY ships with, so I figured that I might be able make a stripped down version that uses 3.X. If I get it working, do you think the XY team would be interested in using it as a starting point? 
All the time. Check out this video from the PyData [conference](http://www.google.com/url?q=http://vimeo.com/m/79536617&amp;sa=U&amp;ei=xQO2UpqxF-bp2QWQ7ICIDA&amp;ved=0CA0QtwIwAA&amp;sig2=CWsARP_WMiZYneoYkHdc4g&amp;usg=AFQjCNE7p9hGOgdKs-t-IFVWbw03JrGXIA)
Wouldn't be a problem if it weren't **so bad** and deeply misleading for newcomers.
I'd rather you just treated them as variables and named them accordingly. That way there's *no* confusion at all. Now you have a situation where *you* know they can mutate but your users do not. They'll see `TOTAL_PHYMEM` and assume it's constant when it's not! Magic "constants" that go out and query the system and secretly hack the module's dictionary on import is very messy. You already got it right in the modulewrapper methods definitions: simply expose `virtual_memory().total` as a function called `get_total_phymem`. 
Numpy arrays: When dealing with numeric data, there's essentially no overhead, in the sense that adding n elements increases the memory usage by (element size)*n. There's also a file-backed variant that is not even loaded into memory, and thus limited only by your disk. Namedtuples have the same overhead as tuples. 
You forgot support for mathematics with LaTeX syntax (mathjax?). $\frac{a} {\phi}$. That's one of my favorite features! 
Any help will be great. Feel free to contact me directly.
What I dont understand about IPython is how I should shore my files. Should I write the code in IPython Notebook then save everything in standard .py?
I agree your point that i am not expert in Python. And to make things clear i am not trying to become authority in Python. There are excellent authority present already whom i follow and always been inspired and by the way you talk, i think you are also an expert Python developer. The reason why i post my articles here is to get reviews from experts like you and the others and ofcourse to help beginners. I spend 3 to 4 days researching and nearly about 5 hours framing each article, and in course of time i learn a lot and always want to improve my knowledge from experts like you. Regarding url of previous post , i found that article already backlinked many sites, thats the reason i dint change url. I really expected that expert people will give some good code refactoring and ways to improve the code examples but i have always been trashed by them. The word Advanced in article doesn't mean that any new feature has to be imported from another language and embed into Python. These datatypes are itself advanced and i have tried to provide little complex examples. Rather than finding whois and taking screenshots of mistakes, it would be really nice, if you guys can provide some improvements. These things doesnt look good on experts. Sorry again if i offended anyone. 
&gt; Package Homepage Location: 14% None *Arrr* If you go to the trouble of publishing a module on PyPI WTH would you not put up a simple info page somewhere (or better yet link to e.g. read the docs or code repo). There are so many packages on PyPI that have abolutely no information who wrote them and how to get in touch with them. 
I've written it out of a problem at work. I want to allow users to schedule repeating events and while I could concoct an interface with lots of options, I figured this would simply be easier for them to use. If you pass it a string such as "every other tuesday" you'll get back a sequence of datetimes such as: python3 human_time.py every other tuesday 2013-12-24 00:00:00 2014-01-07 00:00:00 2014-01-21 00:00:00 2014-02-04 00:00:00 2014-02-18 00:00:00 I'd love to know what things I can do to improve both the style and implementation of my code.
Highly recommended for windows users.
You didn't offend me. Keep up the work. The more you write the better you get, both at educating others and yourself.
Interesting that there's no support for Mac or Linux. Which modules are Windows only?
Hey AJ, sorry if I was harsh. I think what you're writing does have value to new users, but I'm also concerned that when you frame yourself as a python authority (which you're clearly doing in the about page), newbies take your word for everything. As a result, your miscomprehension and mistakes become *their* mistakes as well. I'm not saying "stop writing". Instead, I'd just like you to scrutinize your work more closely. I also think you should be candid about your level of experience. There is demand for this sort of writing from the perspective of people who are still learning, but people should be able to fairly assess your level of skill. For the record, I'm not a professional or expert. I'm an amateur who dabbles on my own projects, but I regularly expose myself to computer science literature and theory, so I have a reasonably firm grounding in CS fundamentals. Thanks for being enthusiastic about teaching Python, and I hope your writing continues to improve :).
Thanks pydanny for your words.
a.) I did. b.) I don't appreciate unqualified cynicism, but I also don't agree with the attitude that we should pat everyone on the back for trying, regardless of the quality of their product. Pypix has so far only contributed low quality content that is often misleading for new programmers. When someone publishes content with *grievous* errors while framing themselves as an authority on the topic, I think they should be called out on it.
The rough idea is that you store functions and classes you want to reuse in .py modules, and write the top level code for your analysis in .ipynb notebooks. Moving code between them is a bit awkward at the moment, though. We're thinking about ways to improve that.
Not the greatest article, but I agree that getting to know iPython is worth the investment. On top of the stuff the author mentioned, you can time commands much more easily, automatically reload modules so you don't have to type in a bunch of import commands to manually test something, and many other handy features. It's definitely not a game changer if you're not doing a lot of scientific computing, but if you don't find the interactive interpreter very useful right now, consider that this may be the case because the default interpreter simply *isn't very good*. edit: One thing I want to clarify is that iPython isn't useful just for the notebook. The interactive interpreter is a great debugging and introspection tool, period. The notebook is mainly handy for presentation.
Thanks for your constructive feedback. 
Ah, thanks. Coming from R, I love IPy Notebook, it is just strange (for me) to move back and forth between .py and .ipynb
I enjoyed the explanation of defaultdict. I had previously never used it in my code but feel like I should start doing so now.
Also realize that IPython Console/IPython Notebook are both clients and can connect to the same Kernel. I will often write code in a vim/ipython/tmux environment with a notebook attached to my working kernel. The notebook is largely for plotting/notes. Kind of like a presentation to myself. I prefer to write most of my code in vim.
Hey /u/ajkumar25. I just wanted to point out that defaultdict works with any object that has a 0 parameter constructor, ie int, float, dict, etc. I am sure you know this but in the write up it sounds as if only list and set are viable options to defaultdict.
Agreed, actually I gave talk at user group meeting. If there was video where I am comparing how painful it is to use standard interpreter that would have made things simple.
Also notebook is handy for teaching. Consider where I am teaching class to 25 students, rather than passing pdf of notes, passing .ipynb (with nbviewer) makes easier for them, they know the order in which I typed the code and its output. 
Took me a second to figure out what you were saying, but yeah... len(a) Would have been better in the context of his example. If, however, you don't need a counter object, casting a list to a set is both faster and more memory efficient than casting to a dict or counter for deduping for a count of unique items. AFAICT
I've somehow managed to never hear of it, I guess.
Aye! I'll have to do a more in-depth comparison to see what features they each have.
Nice response. :-)
I opted for a more minimal API and feature set. Paver seems very inclusive and has a lot of builtin features, whereas steak is very minimal. I didn't investigate too thoroughly, but Paver's documentation leads me to believe it has several too many features for my tastes. And on a nitpicky note, it's using the common `@task` decorator. At first I used this in steak's predecessor, but I've started to resent the little word in favor of automatic import and discovery, like `nose` and Fabric. I feel like the point of these tools is to reduce boilerplate and repetition, and having to declare 10+ tasks as `@task` seems counterproductive. Another benefit of not using `@task` is that the library never even has to be imported - you can write a `steakfile` and run it just fine *without* having steak installed (but it won't have the cool features from steak, like the command line tool and lazy dependency execution). Pavement files have to import `paver` to work. */shrug*. Nitpicky again, I suppose, but it suits my tastes better (pun intended!)
Ah, I forgot wild card name for last few years in my life. :-(
Of course it's a regex. It doesn't follow the same syntax as python's re module regular expression syntax, but it's still a regex.
I'm assuming you will try to argue that since all wildcards can be translated to regular expressions, they are regular expressions. However you would be wrong to argue that, because even though wildcards are a regular language, the phrase "regular expression" (or: regex) is reserved to a regular language with certain capabilities that wildcards don't have, for example matching *n..m* characters or matching a subset of characters.
Does Pycharm has a IPython plugin ?
Title is misleading at best. What the author means is that you shouldn't make things that aren't a constant, well.. a constant. There's absolutely nothing wrong with making a module constant a part of your API. For instance, the logging levels in the logging module: &gt;&gt;&gt; import logging &gt;&gt;&gt; logging.log(logging.DEBUG, 'debug message') 
You would probably like Anaconda by Continuum Analytics too then. It comes with lots of the core scientific libraries for Windows x64
The cart/discount status boxes floating off the screen was very annoying on a tablet, thanks for the discount
This is great reference to good Python patterns. I've bought this book 3 months ago and recommended on Python Poland group. Keep it up. Great job with this book!
Yea, I think it's one of those libraries that you don't find until you happen to need its features :)
Do you know when the next version will be available? :) I am currently consider to compile a list of suggestion. I already did it once and hit the contributors list, so people if you always wanted to be mentioned in a book help Jeff Knupp out! He is a great guy and his blog is awesome.
Yeah, I tried doing that using SublimeText + SublimeREPL, but I can't get SublimeREPL to find the Anaconda's IPython environment.
It is a regex, but it would match only zero or more consecutive 's' characters.
For sure that is, great work, I am going to find good uses of it, just sayin, there is more work to be done than slam this in front of DB and go home. 
Well, it is still a regular expression, but I suppose it's not meant to be one.
nice one. i created the same functionality using [gist](http://defunkt.io/gist/) ([repo](https://github.com/defunkt/gist)): function ipynb2viewer { gist "$@" | sed 's|.*/|http://nbviewer.ipython.org/gist/|' } you use gist by running `gist --login` once, whereupon it creates a cookie, and then just do `gist file [file2 file3 ...]` or `myapp | gist -f 'myapp.log'`. then it will output the url to stdout. my function will just pass all arguments to gist, and replace part of the url.
Oh. I totally forgot someone must have written library to create gist.
Emacs python shell can do a lot of this stuff: Reload (just save the buffer and reload), Tab completion, Save the whole history as a file. What am I missing?
I don't know if the book is worth it yet as I only started to read it and apparently I was already coding properly when it comes to control statements. The book might end up being a bit short considering that you view it as almost ready and it's only 78 pages. I do hope that you will expand on the testing aspect and not keep it as a footnote within the general advice section but in the end that might not be the scope of this book. I just wanted to personally tell you that I bought the book because of your blogging and the wealth of knowledge shared there for free so even if the book will be a letdown I consider that it was worth 10$ because of your blobposts and as such you deserve to have your book bought. 
this is really cool.
Now you can simply type: pip install sandman sandmanctl sqlite:///path/to/database (or "mysql+driver://username:pass@host/database", or "postgresql:psycopg2://..., etc) Sandman will automagically introspect your database, set up the REST API, set up the admin, **and open a browser window pointed at the admin**.
heh got me there
There's some more discussion on module-level descriptors here: http://stackoverflow.com/questions/11752534/why-is-the-property-decorator-only-defined-for-classes
Done, Thanks for this and your great blog!
LOL! I did exactly the same thing myself! (That is, fleshed it out a bit more.) The primary difference is that instead of including some arbitrary language source files, I wrote a small interface to use Wikipedia article text for any language on Wikipedia. I didn't realize people would be interested :) Unfortunately, I can't post my code most likely until Tuesday... :( Edit: I managed to tether my phone, though I can't use https so I can't post it to GitHub or anything (and somehow I can use reddit logged in without https...) Anyways, here's an anonymous paste of my code: http://pastebin.com/r8PFBeVz (Edit: There's at least one bug in line 136, but I can't do anything about it at the moment. Edit2: Augh, another at line 221. Sorry guys, some of this code is untested :( )
Cool. I'll look into that, thanks. 
So basically, instead of INSERT INTO x (y) VALUES ('z') queries, I can now write POST /x y=z queries :)
Yes, sure. The term `s*` can be interpreted as a regular expression or another language that transformes easily into a regular expression. If you look at my original comment, I am mocking the author's understanding of regular expressions. The referenced website clearly indicates the use of `s*` to be interpreted as a regular expression. The regular expression `s*` matches words contained in the formal language `{s}*`. One shouldn't confuse the super set of regular expressions which is provided by several programming languages with actual regular expressions, even though all of them can be compiled down to real regular expressions (only Concatenation, Alternation, Kleene Star).
Though this particular module might be a bit long for their standards, I suggest you ask about it in [Stack Exchange's Code Review] (http://codereview.stackexchange.com/), they might be able to give you some good tips
got it done sooner than i thought, pushing it to github now. not making the code available through package control because its really not that clean and I kinda don't want to bother too much for support. [repo](https://github.com/admalledd/sublime_text), [plugin itself](https://github.com/admalledd/sublime_text/blob/master/keymapper.py). The keymapper.py has some example syntax inside it. the repo also has the *.sublime-keymap and *.sublime-project files that you can compare and look at as well of a working example.
Hey Jeff, bought it for $14.99 yesterday and no regrets! I like what you wrote. Thanks for the amazing contribution to the python world. Merry Christmas!
What exactly is your question? I'll just explain this to you in general. First off, the code should be indented during the while loop: counter = 0 while counter &lt; 3: print("The count is now "+str(counter)) counter = counter+1 print("And we're done!") The `while` keyword creates a loop that executes over and over again as long as a certain condition is `True`. In this case, that condition is `counter &lt; 3`. The body of the code is quite simple: it prints the count and increments the counter by one. When `counter` reaches 3, the condition `counter &lt; 3` is no longer `True`. When it loops back around to check the condition, it finds that 3 is NOT less than 3. Thus, the `while` loop has finished execution and the program continues executing the rest of the code. Edit: if PlainEminem is correct, realize that 3 is not less than 3. 3 is equal to 3. 3 &lt; 3: False 3 == 3: True 3 &lt;= 3: True
I think you answered his non-question (why it didn't print "The count is now 3").
It's a codecademy thing, it was indented but didnt transfer over when I copy pasted it. "Instructions Change the while condition so our loop runs 7 times." I'm not even really understanding what is going on in the example. 
So is their system broken, is that why it's confusing me?
try changing it to while counter &lt; 7: 
To make it run 7 times, you just change the 3 in the boolean expression to a 7. counter = 0 while counter &lt; 7: print("The count is now "+str(counter)) counter=counter+1 print("And we're done!")
negative
&gt; while counter &lt; 7: print that works. 
Do read the feature! and check out the other subreddits featured! :)
So basically telling it to count by 1 
Pretty much. 
It's not even that Python starts counting at 0, it's that this program does: `counter = 0`.
I think it's better to use an enum in this case. I think the use of constants in logging module and a lot of similar use cases were because the lack of a native Python enum solution.
[Obligatory XKCD comic](http://xkcd.com/353/)
Wooops, my fault, I posted it on r/boardgames without saying it was an xpost from r/Python, sorry for that.
Perhaps the buffer interface is what you're looking for? http://docs.python.org/2/c-api/buffer.html#bufferobjects
Type inference.
On a somewhat related note, it's awesome seeing Python catch so much interest lately despite being around for as long as it's been. 
I think there are tools that do this for imperative languages (not sure about Python specifically). However in my opinion, at least specifically for unit tests, I think it's more important to have pre-written data so the tests are repeatable. Working on some TDD code, and having the test suite fail randomly on different code you haven't touched can be very disorienting and break focus. Not that I'm not all for finding as many bugs as possible, I just don't think it belongs in the unit test suite. (I know OP didn't mention unit tests but I've had co-workers and one old teacher who insisted random variables in a unit test were good). EIDT: You may want to check this out: https://wiki.python.org/moin/PythonTestingToolsTaxonomy#Fuzz_Testing_Tools
Related: "import antigravity" in Python 3
 import com.reddit.*; public static void main(String[] args) { new Reddit().makeSubredditOfTheDay('python'); }
 print("Your kind isn't welcome 'round these parts.")
Not everyone is using emacs.
I thought the site did this already with a cookie. When I click on 3.x URLs from search results I'm regularly redirected to the right 2.7 page.
I hoped that to happen too; it didn't in my case, at least. That's why I got frustated.
Does it differ from Python 2.7's?
Dude, you're lucky, when I started as an astronomer they threw Fortran 77 in my face.
Why do people insist on making those obnoxious videos these days? That could have been represented with a web page and I could've read it in approximately 5% of the time, without the horrible blown-out background music and weird handwriting thing.
Thanks for the heads up. I came across your "Starting Django the Right Way" tutorial a while ago when I started to get interested in webdev. Having a bit (not much) of experience in python and pretty much zero in webdev, Django seemed like a great place to start and your tutorial was great. Took advantage of your bundle coupon as a thank you! Now, if only I could find a simpler way to modify models and not jump through hoops manipulating tables manually to get South migrating again. I am sure there are ways to do it more efficiently, but still figuring that out. If Santa's watching, 3.x support by Fabric would be great too! Anyway, thanks for the informative posts. Look forward to reading through the book.
I teach sunday school at my church, the 9 and 10 year olds. This 10 year old girl knows I am interested in programming and told me about this game her and her dad were making in python. It was probably the coolest thing I have ever heard a 10 year old girl say.
print "I will never acknowledge print as a function, it is eternally a command to me"
the ugliest post ever posted in /r/python
My go to for this kind of stuff is Perl's Date::Manip module. It really does do everything. You might want to look at it for ideas, I'm not aware of anyone making an equivalently powerful python module. I'd love for there to be one. This one's the closest I know: http://labix.org/python-dateutil
See also: [MyHDL](http://www.myhdl.org/doku.php)
print("woot!")
Even that my previous comment is just a joke, I love to program in Python.
I dunno, I've seen some pretty shitty python code...
&gt; So basically telling it to count by 1 So basically telling it to "increment count" by 1
But Python is strongly typed, right? 
Yea my bad. I meant dynamically typed.
You miss the "class".
... https://github.com/reddit/reddit is written in /r/Python
LOL, yes :_( I'm giving back my certification...
I've been using Cython recently and I love it. I simply couldn't get the performance I'm getting any other way...
I didn't learn anything by watching the video. What a waste of time. Useless. The github README is much more useful: https://github.com/BinPy/BinPy/ Summary: Simulates some ICs and has some algorithms. 
I think in some cases you could, using a powerful static analysis tool like PySonar2, developed by a Google engineer: https://github.com/yinwang0/pysonar2
You can do it with modules in the standard library, but I'd try the [requests](http://www.python-requests.org/en/latest/) module for browser-type activities, and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) for parsing results.
Thank you, requests look amazing. Also, complete shot in the dark here, but is there a way to make this all run through a proxy? As in on my computer I can be using my wifi regularly, all sites will recognize me, but the python script I am running with the webbot in will be going through a different IP address? Is that possible?
Very nice. I prefer bpython to I python, but the lack of scrolling has always bugged me.
"this paper by Ousterhout" in the docs isn't a link - and it sounds like something I'd like to read!
Trying to buy the 3.3 PDF version. Getting this error, "The coupon code entered is not applicable to your order. "
That's just it, though. Short of ctypes, python cannot interact directly with anything that is not a python object. The buffer interface provides you with a means to wrap up whatever you need exposing with an interface that permits such an object while retaining your original data structure.
Okay, how do you install requests? I've been looking around their site for an hour, and everytime I think I have it, I wind up on another python related site with no leads to go about how to install anything.
 i = 0 while i &lt; len(seq): func(seq[i]) i = i + 1 
Hey, great... could you please do the same for the Django docs... drives me nuts!
Ok, you should get to know this page: http://www.instructables.com/id/How-to-install-Python-packages-on-Windows-7/ Using a package installer like pip (brew, apt-get etc) is something people on unix systems take for granted. They exist on windows systems too but I'm out of the loop on how to best install on windows. 
Sorry, this is not correct! For example, this comes out wrong. pyg('an apple and an eagle') 'naay ppleaay ndaay naay agleeay' A better one-line implementation is: pyg = lambda x: ' '.join([(s + 'way') if s[0] in 'aeiouy' else (s[1:] + s[0] + 'ay') for s in x.lower().split()]) pyg('an apple and an eagle') 'anway appleway andway anway eagleway' but this still doesn't deal with consonant clusters (see the full rules [here](http://en.wikipedia.org/wiki/Pig_Latin)): pyg('strawberry') 'trawberrysay' I wrote a multi-line implementation and to my shock it worked right the first time...! def pig_word(word): if word[0] in 'aeiou': return word + 'way' i = 1 while i &lt; len(word) and word[i] not in 'aeiouy': i += 1 return word[i:] + word[:i] + 'ay' def pig_sentence(sentence): return ' '.join(pig_word(s) for s in sentence.split()) pig_sentence('an apple and an eagle') 'anway appleway andway anway eagleway' pig_sentence('strawberry') 'awberrystray' pig_sentence('rhythm') 'ythmrhay' pig_sentence('str') 'stray' (The rules are ambiguous if there are no vowels, but that choice is as good as any, and it doesn't crash...)
If you already have pip installed, then run this from the command line: $ pip install requests If you don't have pip installed, then get download `get-pip.py` from http://www.pip-installer.org/en/latest/installing.html $ python get-pip.py $ pip install requests
Thats about the part where I gave up and came here. How do I go about getting pip? do I run those py scripts to install it? ....or what? Edit: I tried running it, and got this Downloading https://pypi.python.org/packages/source/s/setuptools/setuptools-2.0.1.tar.gz Extracting in c:\users\timothy\appdata\local\temp\tmpp5tfzw Now working in c:\users\timothy\appdata\local\temp\tmpp5tfzw\setuptools-2.0.1 Installing Setuptools Something went wrong during the installation. See the error message above.
I second Beautiful Soup, definitely. I've had to use it to do web scraping before and it's a joy to work with. 
Not answering your question, but how can the value returned by the functions be checked for correctness if the inputs are random? I can see this being used for code coverage and in this case it could also be done in python
It seems like there's a whole lot wrong with your install here. I'm not great with python on windows, but can you tell us a bit more about which python version you're running and how you installed it?
 C:\&gt; easy_install pip C:\&gt; pip install requests 
Thanks for the comment! I'm the repo author, forgot the passw to that throwaway account. News identification: ============ A lot of the power in identifying news articles comes from analyzing the URL structure, this package can identify news urls for most international and English websites. There are other hints to deciding if a page is a news article or not. For example, checking the minimum article body length: if an article's body text is too short and it is not a "gallery or image based" piece, then it's not a news article. **However, one big Achilles heel is that our library makes the assumption that web pages are primarily static. Crawling from sites like slate, techcrunch, espn, cnn, (local news site here), is A-ok but sites like feedly and mashable, which require the user to interact with the page, will kill our crawler. On text and keyword extraction: ==================== Our library relies on goose extractor (which I contribute to and modify for newspaper) to parse text from HTML. It is comparable for a few select languages, I don't remember which ones at the moment though. Will update this post. THis module can extract text from almost all HTML pages and even in different languages. The keyword extractor works on English text only. I plan on making this library much better though and would appreciate any help!
I've asked a total of 2 questions...
I too believe in the benefits of linux,but I don't think we should be telling people "don't use Python on Windows", Python is cross platform for a reason. Plus I wouldn't want him to have to learn a new operating system *and* new programming language at the same time (even though it's what I did).
This worked, thank you so much! It's starting to make more sense. :)
&gt; I simply couldn't get the performance I'm getting any other way... I don't think that is true ;) Since this is the python subreddit I'll stay on topic... what about pypy? :D
I..I...I love you.
I believe it's because you specify a correct range of inputs. If there is only one input and output that is correct, then you write a normal unit test. Although the random test is still useful in assuring that weird input doesn't cause odd behavior. 
Of course the next night I go back, they explain it in the next lesson. They should have done it before imo
Hsssssss
I've been using Python on Windows just fine for years. It's just not a problem.
I think you'll find pip (usually) makes installing packages obscenely easy.
Awesome.
Omg who keeps changing it, I went it red!
Sorry :(
This is fantastic, good job
lemme tell you what Codelucas: You are making many peoples lives easier!!!!THANKS!!!! Can you only download one article at a time? Did not look to deep into your code. Maybe gevent would be useful to use with newspaper. I love that you are making scraping accessible. Now data science can be data science, not a bunch of semantics scraping and formatting.... If you need any help in particular I would love to contribute to newspaper!
You're correct about their XOR example, .trainuntilconvergeance() uses 1/4 of the data as verification and not part of the training set, calling .train() in a loop would be the correct way. &gt; Is this a guidebook on getting started on using it, or just installing it? Both installing and more detail on using the actual functions. [Shameless Self Promotion](http://www.amazon.com/Practical-Python-Neural-Networking-PyBrain-ebook/dp/B00GG0HU6K/ref=sr_1_1?ie=UTF8&amp;qid=1387872528&amp;sr=8-1&amp;keywords=pybrain) Book might be stretching it at just 80 pages, but some people have found it useful. 
Really? I haven't seen any open issues related to that. Can you link me to the bug report you filed?
Awesome
I can do the same site by cheating with some javascript/php :) Just kidding, nice job ! Maybe slightly unrelated: have you had any experience using arduino(likes) with a raspberry pi ? Is it easy ?
I see, thanks a lot! 
what system are you on ? windows ?
I'm on windows, I'll include that in the text. 
It might be more than you need, but be aware of [Scrapy](http://scrapy.org).
I haven't got pip installed yet. Would the console be Python(command line) that came with the download of Python? 
Huh... I will suppose that you have downloaded xlrd here, and that you have unzipped it somewhere : https://pypi.python.org/pypi/xlrd/0.9.2 Then you need to open the Windows console (go to the start menu and look for the prograam named cmd which is in the accessories). Then you need, in this console, to go to the folder where you unzipped xlrd, where there is a setup.py file. To do so you will type cd your/path/to/the/folder One you are in the right folder, type python setup.py install
Hmm, a site hosted on a home server (presumably). Reddit hug of death yet?
If you mean just attached via USB, it wouldn't be any different than using the Arduino with a PC.
Cool, it didn't recognize the "python" part. I excluded that and it worked! It put a folder called xlrd and a file called "xlrd-0.9.2-py3.3.egg-info" into the site-packages folder. Does that sound right? I tried the same process with the get-pip.py. It did something differently and just downloaded a text file in a folder and put it in my user folder. Thank you very much for the help.
Really cool. Now I want to be able to do this. The docs at http://playground.arduino.cc/interfacing/python are pretty welcoming. 
what determines if it worked, is whether you have an error or not when you open a python console and type "import xlrd"
I clearly misunderstood, my mistake. I thought you were saying that it unexpectedly happened. I'm sorry you can't use pypy, it's pretty cool. I wonder how they are using pypy on raspberry pi if it requires 4GB of RAM: http://www.raspberrypi.org/archives/3881
Yeah, that's a cool one ;)
This will evaluate the iterator..
It does: [see the docs](http://docs.python-requests.org/en/latest/user/advanced/#proxies).
sudo? Windows doesn't have any form of sudo, I think you need to open up an administrator console. 
ONE GREEN TO RULE THEM ALL!
Looks like yes. I can't get to it.
Works fine for me.
Huh. Might be my work firewall or something, then. It just times out.
Could be blocking access to "new" sites, or sites accessed just from a ip, not DNS.
I bought the package a couple weeks ago. The one for version 3 was 94 pages and Version 2 was 96 pages. 
@curtsies doesn't appear to be a valid tag on the repository. I'd love to try this out, though.
Looks like a worthwhile pursuit. Keep updating it with more factory classes. Some examples: URL factory Filesystem factory Email factory Anyway you get the idea 
This is a [good video](http://www.youtube.com/watch?v=52wxGESwQSA) to look at for extracting from webpages... It is long though.
It can't handle more than a few requests at a time, so I'm sure its not working for everyone. Hasn't crashed though. I'll take it down after 24hours take a look at the logs.
Thanks. 
Nope, sorry.
I am not the best python programmer out there but I do know this. **1 --- EVERYTHING in python is an object.** import x x is an object. from x import y y is an object. **2 --- You cannot have 2 objects with the same name, in the same scope.** from x import y import x is just as correct as import x from x import y However the following is wrong! import x from y import x There are now 2 objects called "x" doing 2 different things. I am not sure if the following is correct but: **3 --- this is how I was taught to do imports.** 1. import built-in python objects 2. import 3rd party objects 3. import your local objects **Edit v3.:** I just discovered the following! http://docs.python.org/3.3/reference/simple_stmts.html#future-statements """" A future statement must appear near the top of the module. The only lines that can appear before a future statement are: the module docstring (if any), comments, blank lines, and other future statements. """" **4 --- Revised import list** 0. import __future__ 1. import built-in python objects 2. import 3rd party objects 3. import your local objects Examples: from mylocalfile import myN00bClass import warnings import zz_myotherClass is confusing and "wrong". It is in alphabetical order, it does from x import y first and, it works fine. So why is it "wrong"? It is hard to tell for an outsider which imports come from what places. This is correct(ish) import warnings from django.contrib import admin from django.utils import something1 from django.utils import something2 from localfile import a import localfile2 You could even insert comments to clarify the imports like this # python3.3 import warnings #django1.5.1 from django.contrib import admin from django.utils import something1 from django.utils import something2 #see x.y.z folder from localfile import a import localfile2 Best Wishes UrMomIsMyOtherRide. **EDIT v2:** This wasn't part of your question, but it is part of this discussion. The following is also wrong class MyAwsomeClass(Object): variable = 123 if(True): import somethingnew &lt;-------- DON'T DO THIS .... return output **EDIT v3:** If you want more information on the technical side see: http://docs.python.org/3.3/reference/import.html
Quick guide: 1) Right click "My Computer" or "This PC" depending on which Windows version you are using and go to Properties 2) Click "Advanced system settings" or the equivalent 3) This should open "System Properties". Click "Environment Variables..." 4) Click the button that says "New" at the top, not at the bottom. Set variable name to PATH and variable value to(probably, find your Python directory and modify it if you need to. Keep in mind that the Scripts directory might not exist yet, doesn't matter.) C:\Python27;C:\Python27\Scripts; 5) Quit all that. 6) Go to http://www.pip-installer.org/en/latest/installing.html and download ez_setup.py to like your Desktop 7) Open a command prompt by pressing Windows Key + R and writing "cmd". 8) Enter "cd Desktop" 9) Follow the instructions on the pip page, that is write "python ez_setup.py" in your command prompt. 10) Go back to http://www.pip-installer.org/en/latest/installing.html and download get-pip.py to your Desktop. 11) Write "python get-pip.py" in your command prompt. You might have to close your command prompt and then open another one. 12) To install xlsxwriter, write this in your command prompt(again you might have to close and open another): pip install XlsxWriter If you want to make a webscraper you should install a module called requests and another module called lxml(to install lxml pip might not work, so go [here](https://pypi.python.org/pypi/lxml/3.2.4) and get the right executable and install it like that instead), that's what I've used in the past to make webscrapers. You could also look into Scrapy, but I've never used it. 
I like ordering (and separating) convention that [urllib3](https://github.com/shazow/urllib3/tree/master/urllib3) uses. Perhaps you'll find it interesting example.
What are you trying to accomplish?
Is that an Arduino clone? If so what model?
You accidentally the whole context.
Improve the authentication. Everybody know that password is not safe enough and client certs looks to be a better choice. The aim is to allow users to improve there credentials using certificate. That's it.
It's actually a TI stellaris LM4F120. Cheap, and fast.
Sweet! Great Job!
this title is wildly misleading :)
Oh, you want them to be able to use a cert instead of a password?
There are multiple quickcheck style packages on PyPI, though I've not used any of then myself.
Without a 3rd party issuer for the certificates, they add no security. You bypass pretty much all the benefits by having to provide an alternative certificate-less authentication for new devices and users without certs. An attacker would just use the certificate-less layer. Certs work within corporations (since there's a support infrastructure in place to create, deploy, revoke and provide help desk everytime it fails) and for authentication of automated processes (as you expect an IT person to setup the authentication initially.) If you really want to do this, you should set it up so the user is responsible to provide her own cert and register it to your service. And then all you really do is provide a convenience password-less authentication.
I can't believe I didn't notice this earlier.
great post!
It's apropos that there is an Alan Smithee who doesn't want to be associated with the lower scores being handed out.
You should use requests and BeautifulSoup
You should use [isort](https://github.com/timothycrosley/isort)
$10k seems really steep. The market is pretty good for developers right now, and is generally pretty friendly to people without formal training, especially at smaller companies where hiring is run by the nerds and not the suits. If you can show that you can code (e.g. a Github profile), I don't think you'll have much trouble.
What is it? I'm on mobile right now and it isn't loading.
They are the same, I picked up IntelliJ to do some Android/Java dev so I have licenses for both, I use pycharm for the web part of our application because the UI looks different and it's easy to tell which project I'm looking at the color scheme. 
There have been a lot of posts lately about these boot camps. I really worry that they're rehashing the market of certification mills for IT that were so big a few years back. The biggest effect they had was flooding the market with IT consultants, driving down prices and basically ruining the consulting business al together. I worry that this is happening with the boot camps.
As a graduate of Dev Bootcamp, one of the founders spoke to us about the expectations of getting a job through Dev Bootcamp. To paraphrase, he essentially told us that Dev Bootcamp won't get us jobs. The main reason Bootcamps have high rates of employment afterwards are the people that attend bootcamps. They're usually the type of people that hustle to find work. Also, bootcamps don't always have the best reputation in San Francisco. I've heard plenty of stories about companies being unhappy about hiring bootcamp graduates. The main reason to do a bootcamp program is because you are passionate about learning programming, etc. Doing a bootcamp will boost your learning significantly. If you are passionate and want to learn faster, be pushed harder, and learn in 2 months what you'd maybe learn in 6months and can afford it, I'd recommend it. But if you're doing it to just get a job, then you probably shouldn't do it. Also, Like going to college etc, another big piece of value from doing a bootcamp is the network you become a part of. I personally love the people at DBC in SF and surrounding yourself with people thirsty to learn and push themselves helps a ton. And the continued value of having a network that is going to grow and grow and grow is pretty valuable imo.
This is an ideal tool for Computational Thinking classes. Consider English majors who are taking a course on CT that need experience working with large/streaming text repos. This software greatly simplifies that experience and allows them to work with authentic data. I'd be interested in porting this to other languages commonly used in beginner classes, such as Racket or Java.
That's a great idea! Porting this library would be helpful for beginners, I agree. This is just a start for this library, hopefully many more features and cleanups/speed ups to this code get added. Feel free to send pull requests or help make a TODO list whenever. 
Eventually managed to buy the book after switching to portrait mode, sigh... 
I can offer my personal perspective - I recently went through App Academy, a 12 week Ruby on Rails/JavaScript bootcamp. I have been playing around with programming of all sorts since I was a kid, but nothing in the way of marketable skills. One of the things that drew me to App Academy in particular is that you only pay tuition if you find a job within the industry within 6 months. This mostly took concerns about smoke and mirrors off the table. My main point here is that I can't speak to other bootcamps and the risk they may entail there. Before the 12 weeks were up I got a job as a software engineer at a ~50 person start-up, with a salary in the six figures. As for the the technology choices (in AA's case, RoR and JS) - the specific languages and frameworks chosen were indeed chosen because that's where the most demand in the job market is (in SF and NY, where they are). Regarding training and experience - here again, I can only speak to my specific experience at one specific bootcamp. Although the languages and technologies were chosen because they were in demand, they did really teach them well, with an emphasis on good OO design, idiomatic code and best practices. The point, after all, is to get a job, and a decent technical interview is going to weed out people who have the buzzwords on the resume but not the fundamental knowledge behind them. That said, the bootcamp itself is *not* a factor in getting a job. If anything, it's an antifactor - we were explicitly encouraged not to put the bootcamp on our resumes, since there is a bias against people coming from these types of programs (for good or bad, doesn't matter.) The proof, as always, is in the pudding. If you can production-quality code (and have some on your GitHub to prove it), and you are familiar with in-demand frameworks and libraries, you're already there. Just start putting yourself out there. Otherwise, a bootcamp is a great way to get there.
&gt; requests I've only done a little bit, but when I do, I generally do it with urllib2. How does Requests compare with that? Is requests module the new hotness that I should be using? 
Doing hiring, I'm not going to hire someone with only bootcamp experience, unless their code on github or their personal site is amazing. So exceptions can be made. But that's for entry-level obviously A good thing is to grab a grunt job doing something related to low end server support (if you grok linux) or IT helpdesk or desk side support, and just grind a year or two out, and write some apps for your employer, and keep adding personal apps to your github, etc. We Hired a self-taught guy with a Lit degree, and who had one year doing deskside support who wrote an add-on for their helpdesk software for better tracking and notifications (sms, email, web, etc..) plus some other small apps. That sold us. Also he has learned on a product line that we extensively use, and that also helped.
This is excellent and insightful information. So then what is the Way (Tao) of getting a job in the language and development environment you enjoy? Is the element of "hustle" specific to bootcamps and self taught hobbyists trying to shortcut getting a formal CompSci degree? Are you seen as trying to game the system even if you develop workable competency in a full stack or framework? Or do all these jobs require an element of hustle regardless of background, unless you are applying for a typical Java, .NET, C++ job with a Fortune 500 company (in which case these alternative paths aren't for you anyway)? There is this myth that there are more developer jobs in sexy languages like Ruby and Python than there are people who know them. Bootcamps lead you to believe (perhaps unintentionally through your own rosy glasses) that they can't train people fast enough to keep up with demand, especially in Bay Area, Seattle, NYC, etc. Is the demand and open positions for paid employment in Ruby or Python not as great as would seem, hence requiring you to hustle to sell a company on your skills? Do real jobs still look for "enterprise" languages, while scripting is for projects on the side (i.e. on your own time)? Thank you for the insight.
If you need a bootcamp to motivate you, go for it. If not, I'd strongly suggest picking up some books and just forcing yourself to learn firsthand. Honestly though, for finding employment, the best thing you can do is just have some projects you can talk about. If I'm hiring, and someone says, "Yeah I took these classes in Python, etc. etc." I'm not that impressed. Maybe they are really determined, or maybe they can just apply stuff they just learned but will have problems when they are faced with a question "not in the textbook." But if someone comes and tells me, "I'm a big 80's arcade game nerd... and my first big Python project was doing my own Pac-Man game" it's suddenly exciting. They are passionate about their project and usually it shows. I can ask to see the code, I can ask questions related to the project to see how they think through algorithms/design ideas and more. If I know that you had the intelligence/hacker mindset to research and implement, for example, the A* pathfinding algorithm for a ghost, I'm positive that on the job, when you come across a hard, unsolved problem, you can google/wiki/read/research to your hearts content to find a great answer without just floundering.
Hustling is absolutely part of it. A huge part of me getting a job post-bootcamp was just applying to 200 jobs in a week and seeing what comes of it. Ignore requirements on the job listing, ignore the fact that it has "senior" in the title. (I was offered a job called "Senior Software Engineer" that required a CS degree and 2 years experience.) If the job is at all relevant to your expertise, apply for it. Rinse, lather, repeat. 90% you will never hear back from or get an automated rejection from. Eventually, though, something will stick. Assuming you've never done technical interviews before, you will probably bomb your first few. That's ok. Just get them out of the way and keep improving and learning from them.
The site and submitter are somehow controversial subjects recently on r/python.
You might check out [Hacker School](https://www.hackerschool.com/) in NYC. It's free (though you have to pay to live in NYC for 3 months) and the *opportunities* there look quite impressive (i.e., you get to work with some major open source developers and learn quite a bit). You might check out [Julia Evans'](http://jvns.ca/) projects from Hacker School - what she's done is really interesting and has trended on Hacker News a number of times. Bottom line: **you need to have something to show people as well as the ability to *know* what you're talking about**. Open source, self-directed class work and personal projects are all good ways to do this. I just got a job at a tech startup after working in market research and being able to point to open source work (as well as some online courses I took) definitely helped me get my foot in the door (but I still had to interview &amp;c). Caveat: I programmed here and there for years (though not anything particularly serious) and I took a "real" programming course my senior year of college. I've spent the past two years intensively learning CS and programming, so it definitely takes some commitment to do it on your own.
Probably better to post something on [Stack Overflow](http://www.stackoverflow.com) instead (along with a short example that reproduces the issue). That site is *designed* for this sort of question and there's a bigger community willing to answer. (Plus reading code on reddit isn't that pleasant)
Even though I'd say it's better to get used to easy_install [which can handle binary eggs] or pip (because it'll make it easier to upgrade), +1 for mentioning this relatively easy solution. When I had to use Windows I'd just dump my personal utils package in site-packages.
The bigger the corporation, the more powerful the HR department, the more the requirements on job posting matter (and the more likely the ridiculous requirements are written for a single internal candidate).
Agreed, I wouldn't pay $10k for any sort of programming bootcamp. Especially for popular languages like Python and Ruby, you can emulate your own bootcamp for free (or virtually no cost) just by forcing yourself to work through problems and read through/watch material daily. It's unlike staying at home vs. going to a gym, since with physical exercise you generally need special equipment and sufficient space, which a gym provides. That's one of the reasons why it can be worth shelling out the money for a gym membership. But for learning how to program, all the materials you need are completely accessible with an Internet connection. The only thing you have to provide is the motivation.
It's really simple code, I think that the issue lies with it that I'm a noob in Python and I think I didn't 'shut down' or 'quit' python correctly http://i.imgur.com/j06tl53.png 
there is a flag (--script i think)to launch ipynb that will save a .py version of your .ipynb in paralel.
The issue is the `while Playing` line (it's always True and runs endlessly). If you want help with simple code (like this) you should post to SO or to /r/learnpython (referenced on the sidebar), both of which are set up to help people with issues.
By the end of the program, I did have a significant hobbyist project, and a GitHub with a lot of good code that I had written. The last several weeks of the bootcamp were devoted to creating a substantial project and building a portfolio.
It wasn't necessarily about the project being *big*, but it was reasonably thorough and demonstrated my skills and practices. I was able to demonstrate that I can write well-designed, maintainable code, that I know how to test, that I am familiar with and follow conventions (language style, Rails conventions, a RESTful API, etc). Basically, that I can write a good, maintainable web app from front to back, using good practices and style.
That spoke out to me, too. (for those who don't know: Alan Smithee is a common pseudonym used by filmmakers, artists, and now Pitchfork Music reviewers when a work is published that they don't necessarily want to be associated with)
Is there something like this for osx? Specifically with Anaconda?
God, I went through this hell a few weeks ago when I started with Python. It is the worst part of the Python so far, by far....
k thanks
Also, don't post pictures of text. Unless it is a meme, in a subreddit where memes are okay. If you want the text to be formatted prettier than it can be in a reddit post you can use something like gist (https://gist.github.com/) or pastebin (http://pastebin.com/)
I was thinking rc cars or something like that, going to go with full sized pinball machine over browser. Taking a look at this CGIHTTPServer in python, nice and lightweight.
I was under the impression that pip/easy_install only work on linux/mac machines? Honestly had no idea the was windows support for either...
Awesome! This worked very well. Thank you for the help.
Biggest thing is to have demonstrable evidence of your coding ability. Learn Java/Objective-C/C# and put together a phone app, or learn Javascript, HTML, and a server language and put together a decent fansite, or create/contribute to an open source project on Github, etc. This lets them see your code, but more importantly it tells them that this isn't just something you're doing because you're desperate for a job, it's something that you're passionate about. Especially in the current information age, there's no excuse for not carrying evidence of your coding prowess with you to every interview.
Damnit, I really thought this was going to be analysing the harmonic accuracy of a pitchfork.
Very cool. Consider posting to /r/dataisbeautiful !
New project: develop an algorithm to find me new albums to listen to while reading overwrought Pitchfork reviews using David Cross' [albums to listen to while reading overwrought Pitchfork reviews](http://www.pitchfork.com/features/guest-lists/6044-david-cross-albums-to-listen-to-while-reading-overwrought-pitchfork-reviews/).
Edit: That definitely has something to do with it, but I still have problems using the PS1 variables form those sites. I will have to look at it again later. Awesome, indeed I have similar problems with wrapping, but not with python before. Thanks!
Sounds good, thanks. 
back up over here -&gt; http://Thenameofthewebsiteis.com, post here: http://www.reddit.com/r/Python/comments/1tnfmq/so_here_http_python_arduino_condensed_video_is/ 
fucked up... It happens. proper link http://www.reddit.com/r/Python/comments/1tnh28/python_cgi_server_direct_to_ardueno_full_code_and/
You have an infinite loop with While Playing: pass This will cause the interpreter to run forever. It's usually considered unpythonic to name simple variables with a capital letter, that is normally reserved for class names.
Http://thenameofthewebsiteis.com/code.html explains the process, not complicated. 
Try setting your terminal width to over 80 chars wide and see if that fixes it.
Use anaconda's tool, conda. It allows you to use pip as well.
Dfw fan?
Cross-compiler?
huh? I don't get the reference.
I believe they work with any python (at least easy_install comes with Python itself). Also, if an author has uploaded a precompiled binary for windows, then you can install it via easy_install (I'm not sure).
My ipython readline is vimlike but normal mode and insert mode are off by one column so if I exit insert mode after typing, it steps back a column which is kind of hard to get used to, I have to use 'a' instead of 'i' to continue typing :-(
&gt; Assuming you've never done technical interviews before, you will probably bomb your first few. So true. Sadly my first was for one of the coolest jobs ever (big data at spotify in NYC). If only it had come a month later.
However you like. It really doesn't matter. I personally (like to) group my imports by functionality, but it's so unimportant that I wouldn't bother wasting time worrying about it (and most of the time I don't bother with it either).
*"It is hard to tell for an outsider which imports come from what places."* How is it hard to see where the imports are coming from? It's obvious that `myNoobClass` is coming from `mylocalfile` (awful module name, by the way) and that `warnings` and `zz_myotherClass` (*especially* awful module name, by the way. zz_myotherClass is a module, not a class) are modules from somewhere in `PYTHONPATH`. I don't see at all what you're talking about here. *"However the following is wrong! import x from y import x"* This isn't wrong. It's a little silly, (Unless you're relying on the import of the `x` module to have some sort of side-effects) but it produces no sort of issues in the code. (Other than making the first line pointless) Note that at the end of this block, you only end up with one variable named "x", namely the one imported from the y module/package. (You can't have multiple variables with the same name in the same namespace) *"This wasn't part of your question, but it is part of this discussion. The following is also wrong class MyAwsomeClass(Object): variable = 123 if True: import somethingnew &lt;-------- DON'T DO THIS "* This is also wrong. There are many perfectly legitimate times when you want to put an import not in the global scope. (I'm asusming you meant to put the code in an `__init__` block, as it's not actually valid where it's at in your snippet) For example, if importing somethingnew takes a lot of time (either in absolute terms or relative to the time that most of your code takes) then it's often a good idea only to import it in the function that ends up needing it. (This is useful if only that function needs it and that function isn't expected to be called often) There're many other cases for this as well, this is just the first example that came to my head.
I had a similar issue in the past. Try installing the the Python library 'readline' Either `pip install readline` (or pyreadline on windows) or `easy_install readline` -- this will hopefully fix ipython.
Thanks for answering. I understand that a root certificate can't be a self signed certificate but it's not the part I doubt. &gt; If you really want to do this, you should set it up so the user is responsible to provide her own cert and register it to your service. And then all you really do is provide a convenience password-less authentication. I doubt on the certification generation, your answer is just, don't generate certificate let the use provide it's own but, as far as I understand [nginx documentation here](http://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_verify_client) all client shared a same root certificate. And it is also explained [in that ticket](http://trac.nginx.org/nginx/ticket/301#trac-change-1-1360941722555140) that nginx client certificate verification start from the root certificate. The html [keygen](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/keygen) element from netscape, which someone explain its usage [here](http://lists.whatwg.org/pipermail/whatwg-whatwg.org/attachments/20080714/07ea5534/attachment.txt) looks great to match with the nginx way to provide this authentication. So for me it's look OK to generate client certificate until the private key of the parent certificat is secure, why not ? 
Friggin' waste of my time. Also WTF was Courage doing on that video? 
Great post for showing off how easy/great python is for analysis! Should xpost to /r/statistics
Not from my mobile phone it doesn't. It doesn't even load. On top of that why would I even want to build a CGI server from scratch when there are much more secure implementations already in existence?
Sure. I've queued it in my 'Freetime Projects' list!
Most probably, this functionality should be available in some form of the other. I couldn't discover any, though, so ended up writing small add-on for it. Worked for me, so put it on the Add-on repo.
I'll use it
Why did you post under a throwaway? It's fine to take credit for your own work.
great blog post! Pandas is brilliant. this just further serves how biased and useless pitchfork's reviews are :)
yeah like before 2003. Sorry to rain on your parade but gazillions of Indians have been doing exactly that crank-out-a-programmer thing since 90's and still today 
I know, that host is for shit. You would want to build it this way because its small and fast. 
Looks very interesting. I would be most interested in a performance comparison over gevent for common tasks!
Omg. How good do I need to be to get all this.
Unfortunately still no good. Thanks though.
I wish I had made this ;]
 import x .... from somewhere import x As you stated, the first line is pointless and it can also lead to difficulty in debugging. -------- &gt; For example, if importing somethingnew takes a lot of time (either in absolute terms or relative to the time that most of your code takes) then it's often a good idea only to import it in the function that ends up needing it. Putting the imports within a function will cause calls to that function to take longer. So unless we profiled our code, and discovered that placing a certain import inside a function body will make a drastic difference, the negative outweighs the positive. In the general case, importing should be done at the top of a file. ----- &gt; It's obvious that myNoobClass is coming from mylocalfile. ~~I was referring to where "mylocalfile" comes from. Obviously, when I gave the module the name "mylocalfile", we can assume it is part of the local project. Imagine I gave it the name "cheese" instead.~~ ~~Is "cheese" a third-party package I installed using pip, and currently lives in the virtualenv? Or is it a local folder?~~ ~~In both cases must be in the PYTHONPATH, but if we use my importing structure, we immediately know if it is part of my local project, or if it is a package in the virtualenv.~~ [PEP 8](http://www.python.org/dev/peps/pep-0008/#imports) is where I learned this convention For the lazy: Imports should be grouped in the following order: standard library imports related third party imports local application/library specific imports In the end, it comes down to preferences. When working as a team, or working on old code, we should follow the existing style, because mixing styles, is worse than using a consistent "bad" style. Best wishes, and Merry Christmas! UrMomIsMyOtherRide
I've been developing with ruby for about a year now, and I've actually heard others comment about the many ways in which you can invoke a similar (or even identical) function. the functions: .to_sym and .intern are a good example of two functions that do the exact same thing (literally the docs for one say to look at the other). My rationale behind this is the same as when I consider a spoken language. There are a multitude of ways in which you can express an idea, each with its own unique nuances and implications. I don't see any reason why programming should be any different, especially considering that writing readable code is such an important skill as a developer. I might use one function over another to more clearly represent what I'm trying to accomplish to others even though another may do the same thing in the end. All in all, I think it allows some flexibility to more clearly express an idea in code. For the two functions I mentioned above, I view .to_sym as a simple conversion from string to symbol, whereas when I think of .intern, I view it more as internalizing a value (making it immutable). Even though the two technically do the same thing, I feel that they express fundamentally different meanings and provide more clarity on what it is you're actually doing, or how you intend to use the function. Again, I haven't been using the language for an extremely long time, but that's what I've come to believe about the many similar functions in ruby. I know most probably won't buy into it, but I like having the flexibility it provides.
wut? you want to run a python program on a big data framework? What kind of program is it? etc etc !?
what if it's django?
One really simple free way is to use Heroku https://www.heroku.com. I don't work for them, I just have had good luck deploying to them quickly and cheaply. They also have great interface for adding a database and many other third party tools. Heroku gets expensive when you need lots of processing power for when you have lots of traffic, but for starters its one of the easiest ways to get your code online.
I have just started using pypy both on my imac dev machine using homebrew and deploying to centos 6.4 , there was no need to compile it. Why do you need to complile it from source just to use it?
The answer to the question "how good do I need to be" is always "enough to finish what I'm doing now", and then you ask again when you're done.
I think you're missing the point behind hadoop. It provides a framework for writing MapReduce applications - it doesn't magically make your single threaded, non MR programs run any faster. 
It's a nice tool. I did however have many issues installing it on OSX (10.8.x). Installing it on Ubuntu was a breeze though.
I really can't wait to start on this! I am just wrapping up a project shortly so this will be a good transition!
Is there some reason you can't use a binary?
lol wut.
Supports Linux and Windows on both Python2 and Python3. Here is a screenshot: http://i.imgur.com/vJgKK0r.png
This article demonstrates about 10 things you either don't need, or shouldn't do in production. 
If you follow this tutorial you will have a bad timw
Fancy elaborating? &lt;3
For instance, I don't see why the application needs root access especially in production, also since you're setting this up all on one box(or so it seems) you'd benefit a lot by using unix sockets. Those are the few things I remember without rereading the article. :p Source: I am an ops person tasked with running a couple of Python webapps in production. 
it ain't worth it if it's easy
How is it Python's fault you chose a shitty, non-POSIX-compliant system that has poor development support? Note: yes, this is a rhetorical question. 
Actually, current versions of conda did away with that, and just invite you to run pip directly instead. The alternative is to use "conda build" to create your own conda packages, but this would make more sense in a multi-deployment environment having a lot of dependencies. For a single developer in particular, just using pip is fine. 
As someone who has conquered nearly every beginner Python tutorial I'm with you on this one. I searched for the next step and saw Project Euler. No more than 5-6 questions in I understood perfectly well how I would get the result with Python...if I understood what formula to use. I'm hopeful of OP's link, though. I need that next step !
There was a post here a couple of days ago about a bioinformatics (?) related one for python? That seems much more manageable.
See it in action: Here is non-stacking version: http://asciinema.org/a/6911 Stacking version: http://asciinema.org/a/6912
Ah you mean Rosalind? http://rosalind.info
Yes. Thanks. Bookmarked properly this time. Also, added a request for notification from coursera.
Great article! Please consider cross-posting it to /r/pystats. Many of our readers are on the lookout for easily-digestible examples of how to use Pandas!
There is also http://www.checkio.org/ although I never used it
Sensei is the Japanese, so if you're going for that you should use "hai" as "yes".
With QuickCheck, you specify tests in terms of properties that must hold. For example, a "reverse" function should have the property that it is equal to the identity function when applied twice. This kind of testing is really good for pure functions, which of course is what all code in Haskell strives to be (and is, depending on how you classify it).
Pandas are cuddly.
Do you feel like a real man now?
I generally use both simultaneously, starting with a SQL query to join, slice and load data into memory, and then do the bulk of the analysis in pandas. For data analysis, pandas is phenomenally more agile than SQL, letting you easily create many types of plots, compute statistics, etc... When I'm only using SQL, I have the high overhead of needing to get patterns from results in a text/table format. Moreover, since pandas lives besides the rest of Python's scientific stack you can do data exploration and modeling/analysis in parallel. 
I don't know any computer languages, and know almost no math, but was able to follow a lot of what you have here. Thanks for posting this, I think I could learn the basics of Python if I put my mind to it. 
*"Putting the imports within a function will cause calls to that function to take longer."* Only trivially so. CPython caches modules anyway. (I'm sure most other implementations do this as well. In python 3(.4) this is standard behavior (as far as I can tell), so all compliant interpreters must support module caching.). If there's caching, your mostly only paying for a dict lookup, which in all but the most trivial of functions, is a trivial cost. Furthermore, quite often slowing down a special case is perfectly fine if it means that the general case is meaningfully sped up. *"So unless we profiled our code, and discovered that placing a certain import inside a function body will make a drastic difference, the negative outweighs the positive.*" Nonsense. You can't profile everything and this is a trivial optimization. There are near no downsides to this one and if you know what you're doing and use it when it's needed, the benefits can be substantial. You don't need to use profiling for every single optimization you use in your code. (And if we did, there'd be a lot fewer useful optiimizations in the wild.) *"In the general case, importing should be done at the top of a file."* That's a perfectly fine statement but this is nowhere close to the same thing as saying that putting imports anywhere other than in the global scope is wrong. Overgeneralizations aren't helpful. PEP 8 is just a recommendation anyway. It's really just the style guide for the standard library. There are several places in PEP 8 that I happily say "That's silly" and ignore it completely.
I have used the opposite recently, I had to create huge combinations, and do some checks. Doing it in SQL meant (by self joining a table to itself multiple times) that I didn't have to worry about writing to disk when memory got full, and the performance was reasonably good. Basically I know SQL pretty well, and the solution didn't take me as long as pure Python (no Pandas) and was an order of magnitude faster. I can have strict control of the data types, keeping everything as small as necessary - helping to keep stuff in memory (though MySQL seems to write to disk before using up much memory). I may try reimplementing it with Pandas / Numpy but it seems like there will be a lot of extra things to take care of. 
Yes, pandas can join and slice very well, but it limits me to what I can fit in memory. So if I'm working with large databases, I use SQL to transform the data into something manageable by pandas before loading it in memory. 
Can you explain what is better about the three things you mention in Pandas, as that part of Pandas seems to be modelled on SQL? 
numpy (and i think therefore pandas as well) can use arrays larger than available memory: http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html does this not work in that case?
Pandas joins are very, very limited compared to SQL joins.
i've been working on a project that uses pandas to import (as pd) all sorts of legacy/historical data from .xls (years of inventory, sales, pricing, etc data), cleans everything up via python sanity, and then writes it out to a SQL database for easier, more universal analysis across projects. so i guess to answer the question, pandas is used to take weak relations between various data sources and then strengthens those relations by fitting the clean data into a SQL data schema.
Is HDF5 also an option in &gt;RAM data sets?
I mostly use it as a wrapper for numpy arrays that gives me named columns and column ranges.
They're kinda orthogonal, so use both: * data extraction from database: SQL * simple data analysis: SQL * complex data analysis: Pandas Note that many tools can generate SQL. Some OLAP tools like Microstrategy can do it on the fly as you navigate through the data. Also note that many people are storing tens of TBs in dimensional models with a central fact table of 10+ billion rows. These models are typically optimized for specific and common filters - such as date, location, product, etc. In these cases it's common to start with a mildly optimized SQL query that would extract 1% of the data you need (say 50 million rows) in just 10 seconds to a csv file. Then use whatever analysis tools you want for the next step.
That's nice. Do you know how it could be used in pandas? 
i think it's an alternative, but i'm not too familiar with this area.. see this exerpt from wes mckinney's book. it discusses np.memmap and hdf5 libraries separately. http://books.google.com/books?id=v3n4_AK8vu0C&amp;pg=PT485&amp;lpg=PT485&amp;dq=numpy+memmap+hdf5&amp;source=bl&amp;ots=reLM2oCwpC&amp;sig=cdqOhpjepewSVdP96sLP8ZBPqYY&amp;hl=en&amp;sa=X&amp;ei=u2m8Uo2nBZWpsQTusYDACg&amp;ved=0CDIQ6AEwATgK#v=onepage&amp;q=numpy%20memmap%20hdf5&amp;f=false
i'm not sure. there's this SO post from wes from a year ago, saying it's on the todo list: http://stackoverflow.com/questions/12052067/if-i-use-python-pandas-is-there-any-need-for-structured-arrays other options on there are discussed here: http://stackoverflow.com/questions/15465997/python-tools-for-out-of-core-computation-data-mining http://stackoverflow.com/questions/20759695/python-memoryerror-the-processed-data-set-does-not-fit-into-32-bit-process-addr 
the JOIN feature of Pandas only works with simple criterion, e.g. equating a series of columns to each other. If you want something more complex like "x JOIN y ON x.a &lt; y.b" you need to break that out manually (and very inefficiently).
Memmapping doesn't work in general since quite a few NumPy operations create same sized temporaries for many operations which still blow up in memory even if the original is mmap'd.
Use `install_requires` instead of `requires`.
Picture yourself a Panda, now a Pitchfork, now a Panda using a Pitchfork, it's a Pitchfork using Panda!
http://pythonhosted.org/setuptools/setuptools.html#declaring-dependencies Here you go =)
Holy shit, this is excellent. Just picked up (8), (12), and (13).
You need to use [setuptools](https://pypi.python.org/pypi/setuptools) instead of distutils.
Grouping is awesome in Pandas. After you split a frame into groups, you can apply any function (or combination of functions) you want to those groups, before re-combining them. Aside from that, for me, it's a lot easier to analyze data in Python than in SQL. I just use SQL to get data out of my database. Everything else is Python.
Ahhh. I went and bought 12 book. So much for my budget. Thanks for the heads up.
Fun, thanks!
This looks great! I'll be watching this and Episode 2 over the next few days. I'm briefly looking at the timeline, but do you or will you talk about deployment? I come from a PHP background where I can use rsync to push code to production and don't need to restart apps, so any guidance on how to automate that to be as simple as pushing a PHP app would be of great assistance!
this is exactly what I have been looking for! Good job man! What kind of box would you recommend to use?
The second and third episodes are also available! http://vimeo.com/82396584 http://vimeo.com/82396585
Thank you! I hope you enjoy it.
Thanks! I'm not sure I follow your question, but I'd love to help if you could rephrase it.
Most of the answers here basically amount to using SQL (plus some DBMS) to carve out the data into memory and then Pandas to slice and dice. One thing that I haven't seen mentioned is that the SQL query statement (ignoring all the vendor specific extensions) is a high level, **declarative** language for describing sets. This declarative nature trades off some of the control you get with **procedural** languages in exchange for allowing the expression of the problem in a way that can be highly optimized. As long as your DBMS is set up by a competent DBA, the query engine can do quite a bit make sure that your query is executed fast, such as making use of multiple indices or fanning it out to multiple machines in a cluster. When it comes to describing sets of rows, you can't really do much better than SQL (or some trivially transformed equivalent) combined with a modern DBMS. On the other hand, if you are trying to do stuff that is not about describing sets of rows, such as a weighted moving average, than SQL is probably not a good choice.
Wing
Strange, why is payware required for an IDE? Further what operating system?
The `xls` maintainer has made a mistake in setup.py as others have said. https://github.com/SheetJS/py-xls/blob/master/setup.py#L15 If it's already at 0.0.3 then they haven't updated the github repo yet at time of writing.
What kind of development environment do you use?
I use Sublime Text 3 with quite a few different plugins. I typically make a new virtual environment in Python for each new project. It's pretty basic. In the past, I've used [PyCharm](http://www.jetbrains.com/pycharm/) by JetBrains which I really like. I'm considering going back to that just because debugging is so nice in a really good IDE.
Windows.
[This guide](https://python-packaging-user-guide.readthedocs.org/en/latest/) is aiming to have the latest information on python packaging. edit: I have updated the page you mentioned with some more up to date information. Hopefully it can help others in the future.
Cool. I have been using tor with privoxy to avoid rate limiting. Just saying that incase it is helpful to your work. Cheers
To address your question - I like to use [Fabric](http://docs.fabfile.org/en/1.8/) which lets you essentially automate commands over SSH. An example would be to just pull from a repository (you could rsync but using source control is always good) and then restart the server using a process manager like [Supervisor](http://supervisord.org/).
Thanks, was it more of a trial/error process getting stuff set up for you or was there a guide/guides you used to learn the process?
Poke around the net and you [will](https://gist.github.com/DavidWittman/1886632) [find](http://docs.fabfile.org/en/1.4.0/tutorial.html) [good](http://flaviusim.com/blog/Deploying-Flask-with-nginx-uWSGI-and-Supervisor/) [resources](http://maximebf.com/blog/2012/10/building-websites-in-python-with-flask/) for both of those - I recommend them partially because of that fact (there are other options that are not as well documented). Finding good guides might be a trial &amp; error process in itself but that's part of the ride ;)
Yeah, I've always ended up on the appropriate 2.7 pages consistently. That's cross browser/computer. Either way, nice to know OP sorted out what sounds like an issue that irritates some people.
Thanks. Would be worth posting in [/r/flask](http://www.reddit.com/r/flask) too.
Oh, right. I'm on a Mac as well.
Supervisor is awesome. It's very easy and gives you a nice way to control your apps. Another thing to look into is uWSGI Emperor. I use gunicorn myself, but Emperor mode gives you some nice control as well.
x[0] is an integer, not a list or dict or object, etc.
It sounds like you are thinking of arrays like in C where it basically just multiplies the indexes together to get a memory address to reference. That's not how it works in Python. The `x[0]` happens first, then the `[2]` applies to that object, which in this case is an integer `0` so it doesn't have indexes to assign to.
Thank you!
Thanks!
10k does sound steep. I come from CS background where they thought us things like writing compilers, memory management, garbage collector etc. basically everything that you will never use in real life. After school I had few job interviews where they asked me mostly about algos, which I was very comfortable with. I then started writing code in Java, Python, C++, web frameworks, server, sockets etc etc Anyway the point I am trying to make is that with good CS background you can adapt to any languages. Though Ruby is here to stay, there are new frameworks introduced every now and then. At some point you will need to switch to new frameworks. That's why companies hate bootcamps bcoz they only teach how to do it but not the fundamentals. For e.g. JavaScript is thought as a front end tool rather than a pure functional language. In my last company, they would only hire people with CS background, which I thought too restrictive. Having said that, there is still a demand in startup world. You might have to do some pro bono jobs to get the experience. I would look for camps that are cheaper. Also look at contributing to some open source projects. Not only your code will be peer reviewed but you will also understand why are things done in certain way. Most importantly have an asset that most engineers don't have. Example be good at marketing, biz-dev etc. bring to the table that others can't. (This is primarily from startup point of view. Big companies probably don't care)
Great explanation.
I've experienced the same frustration myself. Even the Python standard library suffers from it. I almost always prefer to use the `from module import name` form, because I think on the whole it makes things much more readable: from urllib.request import urlopen # it's spelled urllib2 if using python 2.x req = urlopen('http://example.com') The alternative is almost too ugly to type: import urllib.request req = urllib.request.urlopen(...) Even when it's not such an egregiously badly named module, I still like to eliminate dots: from itertools import product ... for item in product(...): ... I just feel it looks better. But there are some modules that go out of their way to screw that up -- the worst offender is the JSON module. from json import load with open(...) as f: data = load(f) Blurg, that's no good. The name `load` is completely nondescript and I wouldn't want that in the global symbol table. They *really* wanted you to write it like this: import json with open(...) as f: data = json.load(f) I mean, that's neat and everything, but it feels like such a strange oddity because so few other modules are designed like that. I suppose there's always the compromise situation: from json import load as jsonload with open(...) as f: data = jsonload(f) But at that point you've really lost the tune. Another module that irks me in the same way is `os.path`. I highly dislike writing `os.path.this()` and `os.path.that()`, and many times it's not necessary, for example: from os.path import exists ... if exists(filename): ... I think that reads quite well. But then in the same module there are functions like `os.path.join` which you really can't use sanely that way: from os.path import join foo = join(bar, filename) That's far too confusing for words, because when you see that you're going to instantly wonder what's being joined and did they make a mistake and mean to write `''.join(...)` or something like that. In that case you really need the word "path" from the module name to tell you what kind of thing is being joined: import os.path foo = os.path.join(bar, filename) But then there's that stupid `os.` cluttering things up. If I could go back in time I would definitely steer history to name this module simply `path` and not `os.path`. (It must be terribly confusing for people new to Python to discover that `os` and `os.path` are actually two completely different modules, but there is still some vestigial overlap, e.g. `os.sep` and `os.path.sep` are both names for the path separator. And importing `os` gives you an unwanted/undocumented hitchhiker in the form of `os.path` being available without having to import `os.path`.)
This subreddit is about the programming language called Python, OP.
for simple substitution: &gt;&gt;&gt; "subsubstring".replace("sub","") 'string' For more complex substitution, use regex: &gt;&gt;&gt; import re &gt;&gt;&gt; re.replace("^sub", "", "subsubstring") 'substring' 
Th requested URL was not found. . . .
That it strange because I have downloaded again the zipped pdf and it was only 76 for python 2 and even less for python 3 from contentshelf.com. the zip is 2.1 mb, I'm curious how big is yours.
More courses/books should explain that comments are better used to explain why rather than what.
thanks! looks like I'll have a busy weekend testing this stuff out
Great idea! http://www.reddit.com/r/flask/comments/1ttumw/if_youre_interested_in_learning_flask_for_web/
As far as deployment, I'm not sure how deep we'll go into that since that will be between me and the [Knox Makers](http://knoxmakers.org/) since they'll ultimately be hosting the completed project. I know this probably won't help you much, but it's really easy to deploy on Heroku. We've actually used that earlier in the course. I will keep that in mind, and I'll try to at least talk a bit about it when I get to that point even if I don't necessarily record the deployment itself.
No problem, I understand. I've been able to deploy successfully on Heroku, but I'm used to deploying on my own VPS which is why I like. I don't want to get locked into a certain VPS, especially when working with client's applications.
Why not compromise on `os.path` with `from os import path`? 
Why not compromise on `os.path` with `from os import path`? 
This is a non-issue since you can alias any import. from os.path import join as path_join, exists as path_exists path_join('..', 'something', 'another')
 from __reality__ import murder
All of this is completely a stylistic choice. One thing that I got bitten by when building a library for my company was explicit class imports. When importing just a class python must evaluate the class code and it's dependencies which can lead to a cyclical import. The gist of which is to never directly import classes within your custom package. Google's python style guide also suggests full canonical name imports for all internal code. The explicitness can help remove confusion for better maintainability.
I keep telling people: you can't just let Perl programmers loose with Python. You have to train them first. No doubt, Python killed this poor fellow by putting a big indent in him when he wasn't ready for it.
You could just code in notepads ++ or sublime or what ever and compile it yourself.. Which isn't hard. And somewhat more enjoyable in my mind.
Yeah, that's probably a much better practice than deploying to Heroku. Maybe something like this would help you: https://www.digitalocean.com/community/articles/how-to-deploy-python-wsgi-apps-using-gunicorn-http-server-behind-nginx It's for Digital Ocean, but you could probably apply it to any VPS.
Thank you!
Also, be sure to pay attention to the efficiency of your regex. Whereas inefficient code might be obviously visible just glancing at the structure of a program, an inefficient regex might be easy to gloss over. http://www.johndcook.com/blog/2011/01/20/efficiency-of-regular-expressions/
Thank you!
I'm aware of that -- look at my example with `jsonload`. I don't think it makes it a "non-issue" though, because I'd rather not have to come up with new names for things in the standard library that often, and I don't think people reading the code would like that either. 
commenting for the archives :)
Python is a scripting language so it's a lot faster to develop in. No compiling. You can also leverage your Java knowledge with Jython. 
I as well 
Really liked Learn Python the Hard Way.
What's with you people? Why don't you just click save.....?
If you continue reading past the first block of text then it will make more sense. I think OP meant to copy the publication that's linked in that post, though
I agree. Its faster to develop with because of shorter syntax and its expressiveness. Shorter more concise code can also lead to more understandable and better maintainable code. This is not always the case, but the language has qualities that can lead to these things. These are all reasons why I choose it more often than not.
No that's called an operating system
Long time java dev, then ruby, then python and loving python. There isn't really a better per se. It's different. - functions are first class, so no more anonymous classes that implement Runnable just to pass them along. Want something that adds two numbers - lambda x,y: x + y - Duck typing is convenient and inconvenient. - the various testing and document systems between languages are similar - pip is a lot shorter than using maven, fabric so much shorter than ant - distributing eggs is like distributing jars all with the same fun of getting them into the right place ot run - virtualenv is just like an eclipse/intellij project for the command line. you have this compartmentalized directory of all your libraries for a project and symlinks to a python interpreter you can destroy and rebuild ridiculously fast - no JEE means things are very up in the air, such as pooling of database connections, running multiple apps (django does a decent job hosting multiple apps) so you dont' have the cruft nor the convenience of the JEE - flask/bottle are shorter than using jetty to bring up a web server that runs code - pycharm/eclipse :: intellij/eclipse - no more public void foo() throws Exception because someone enforces you to deal w/ terrible Exceptions you don't want to deal w/ (instead of throwing Error)
Are there any big mainstream programs that do this already? What do they use?
Do you mean *interpreted* instead of *scripted* or am I missing something obvious?
&gt; There isn't really a better per se. It's different. This is the crux of it. You would be a fool to write a 3D game engine in python, as you would be a fool to use java to write, I dunno, drivers. They are both general-purpose, and for most purposes either would suffice, but they each have their pitfalls. Python is good for semi-low-level operations, a general-purpose scripting language that excels in fast, iterative projects. It can be used for websites in the same sense that Ruby can, but for anything that requires speed it will need something else to handle the truly processor-intensive stuff. 
pygame is nothing to sneeze at. mcedit for minecraft uses it.
Thanks. It would be convenient to add a "flag" or something to know the python version and the level of knowledge required for each book.
There are several 3D game engines in Python. The most famous of which is probably Panda3D from Carnegie-Mellon. Python is as heavily used in the game development world as lua and some A class production games have been written entirely in Python. You can easily use python for game development if you want to and it will be fast. This is because the vast majority of these libraries are implemented in C. Also, your notion that Python is not a fast language for processor intensive stuff is also flawed. Python is king in the academic space and has been used in high performance computing environments for years. NumPy and SciPy are VERY VERY fast numerical analysis libraries.
LWJGL for minecraft itself. 
Exactly. You are indeed a Reddit poweruser sir
The speed of externally libraries should not be attributed to the language itself. Panda3D itself appears to be built on a C++ library, so it is actually C++ which is robust enough to provide the speed necessary for an engine. Nonetheless "game engine" was probably not the best example choice, since it was the hard rendering itself that I was thinking of rather than the high-level logic that combines all the parts of a game engine under the hood. I had written that from the angle of comparing python to java, not python to anything available. In general, something written in python is not going to be as fast as equivalent code written in java. 
I have used many languages and Python is my favorite. One thing I like, as many have mentioned, is that it is less verbose than most compiled languages like Java. Another thing I really like it the ability to do runtime introspection. That means that I write a program in Python and decorate some functions and those functions will be converted to C, compiled, and linked at runtime (https://github.com/mdipierro/ocl). My Python-&gt;C converter does not work for every Python function but that is a limitation of my implementation, not of python introspection capabilities. My favorite library is of course web2py (2012 Infoworld technology of the year) and I never missed JEE. Actually some web2py have to use JEE in their daily jobs and always complain about the latter. 
What language is Java and the JRE implemented in? We all build on top of external libraries. Hell OpenGL is just a library abstraction to interface with the graphics hardware. By your logic we should all be programming in assembler or binary. How close to the hardware should we go? If I ever got to a point where speed mattered that much I would, and have in my career, wrote what I needed to in assembly. The path from that to python is relatively trivial. Write the lib in C, embed assembly, then wrap the function calls in Python. Can you do that with Java? (Honest question, I'm sure you probably can)
It's Github...send a pull request!
+/u/dogetipbot 50 DOGE
^__[Verified]__: ^/u/Rothaga ^-&gt; ^/u/catcint0s __^Ð50.000000 ^Dogecoin(s)__&amp;nbsp;^__($0.0254499)__ ^[[help]](http://www.reddit.com/r/dogetipbot/wiki/index)
&gt; What language is Java and the JRE implemented in? Which part? The compiler is written in java, the virtual machine is native code, and I expect that's written in C. The vast majority of the libraries are written in java. One reason that the python platform has fragmented (py2? py3? pypy? numba? cython? jython?) is that the standard vm is dead slow at running python code, so libraries get written in C which makes them not port well to other flavours. Python is great a lot of thing, but speed isn't one of them. You can link with native code in Java, although I never did that in the years I wrote java code. It's just not nearly as tempting as in python because the java code is pretty damned fast as it is. As an example, the BigInteger class used to be implemented in native code, but was re-implemented in java many versions ago for a significant *increase* in speed. I have to assume that a lot of that had to do with going through the java/C interface.
Learn about HTTP/1.1 request pipelining and keep alive.
It's slow because it's synchronous -- you're only doing one thing at a time, and blocking between each action. Use threads or an asynchronous IO library so that you can have multiple requests active at the same time. (HTTP persistent connections can help save the overhead of a three-way TCP handshake, but only if you're performing multiple requests to the same host.) 
Some points: - Use [requests](http://docs.python-requests.org/en/latest/) unless you have a very good reason not to. - Request's session will automatically use [keep-alive](http://www.python-requests.org/en/latest/user/advanced/#keep-alive) connections. - Use [grequests](https://github.com/kennethreitz/grequests) or [requests-futures](https://github.com/ross/requests-futures) to make many requests asynchronously. - You should be honouring the sites [robots.txt](http://www.robotstxt.org/robotstxt.html) If you are attempting to use a single IP on a single site, you will likely get blocked pretty quickly.
SQL has horrible string processing, lacks stddev function, lacks plotting, handles NULL different way depending on which database you're using (sqlite guys have a good write up on this somewhere), lacks apply/map functionality, doesn't have great ways of doing things like moving average or cumulative sums. There are group bys on self joins, but frankly if it isn't cumsum() or ma() functions, then it's a pain in the ass to be continually writing this shit every time. Pandas doesn't handle huge datasets terribly well. I mean it tries, but if you can't keep everything in memory, then it doesn't work so well.
I'm surprised nobody has mentioned Twisted yet, let alone Scrapy! Op, it sounds like Scrapy might be the thing for you.
Thank you! I see this all the time. I never could understand this. I just save things that i want to save. 
Stop it. Just stop it. There's a save link on every post. Use it. 
Made thousands of requests already, good so far. Anyway, followup question about grequests: Whether I need to open a certain url depends largely upon the results of the previous one, so I doubt I could just use grqeusts.map(). If I iterate over a list of unseen requests, could that somehow work? I'll keep digging through docs and stuff on my end. There's gotta be something.
Also, check this: http://dietbuddha.blogspot.com/2013/12/development-server-automatic-reload-on.html 
java is fast for a lot of things, and the jvm interface for dealing with threads with the ForkJoinPool, but when you get down to it, it's terrible for responsiveness. Eclipse uses SWT which are bindings to lower level UI toolkits for OSX/Windows. Intellij somehow gets away with Swing, which could be in part to computers being so hefty now-a-days. For statistics, most BLASs are written in c or link to fortran in some interesting way. Java is fast for a LOT of things, but it'll be rarely faster than C and consequently C++ for quite a few things. All the JVM does is make it easier to write things that don't need that type of fine grain tuning, just as C abstracts away ASM. Same as python is fast enough to deal with stuff you can write in C. Edit: To go on a rant, there was this one zealot who insisted, "A FOO written solution is the RIGHT solution." Little do people like this realise, that the most frequent part of a typical db driven application's slowest part is the DB. As a result, the FOO rewritten site was quick, which is great, except all the DB transactions were not evolved enough to keep up, which locked up the site, which caused the language to slow down, which slowed users down. The benchmark this guy loved to tout was handing over 65k connections on a single machine... which isn't possible w/ the JVM. :( If you target a language that much, it's easy to guide your thoughts of fast if you do certain things to avoid the slow parts. 
JNI is how you reach C from java.
When defining decorators that take arguments, I like to write them as classes instead of doubly-nested functions. For example, the decorator: def introduction_fn(intro): def decorator(fn): def wrapper(*args, **kwargs): print intro return fn(*args, **kwargs) return wrapper return decorator can be written as a class: class introduction_class(object): def __init__(self, intro): self.intro = intro def __call__(self, fn): def wrapper(*args, **kwargs): print self.intro return fn(*args, **kwargs) return wrapper This doesn't really save on typing, but the class definition is a little clearer to me.
I was using the urllib2.urlopen().read() on Congress.gov and I got a ten minute block, I read their robots.txt, added a time.sleep(3) into my code, and got a temporary block again. My website's [robots.txt](http://tarki.net/robots.txt)
When trying to understand a complex function, mentally follow the code with certain inputs. Clearly, for n = 0, the function returns 1... For n = 1, it will calculate facto(n-1), or facto(1-1), or facto(0), which we saw before returns 1. Then it calculates n*recurse, so 1*1, and returns the result... 1. For n = 2, it calculates facto(1), which as we know returns 1. Then it returns 2*1, so 2. So on and so forth. Follow through a few more inputs to see what kind of results you get. Following code through, mentally, is the best way to get an understanding for it. :) Bonus points: See what happens with negative inputs... Then see if you can fix the incorrect handling of negatives.
Oh I made a mistake by consider a IO problem as a computing problem. You are right that threading is helpful. 
This is a treasure trove of information. X-post to /r/programming ? So much more than Python.
A nice introduction to a poorly understood feature of Python. I notice that you wrote from a Python 2 perspective. You may wish to point out that the syntax for using metaclasses has changed in Python 3. Other than that, I just noticed a couple typos in your code: Foo = type('Foo', (), {'always_false', always_false} This line is missing the closing parenthesis, and the dictionary should use a colon, not a comma. FooBar = type('FooBar', (Foo), {}) The base class should be given as a tuple: `(Foo,)` -- the comma is needed. Thanks for posting though. I hope more people discover the coolness that is metaclasses.
Sure if you're scraping stuff, but most APIs I've encountered consider the exponential backoff as a best practice. For example: * [Twitter](https://dev.twitter.com/docs/streaming-apis/connecting) * [Google Analytics](https://developers.google.com/analytics/devguides/reporting/core/v3/coreErrors#backoff)
No, that's wrong. The GIL is released when you do blocking syscall operations like write to or read from a socket. This scenario -- multiplexed IO-heavy loads -- was the *exact* scenario that Python threads were designed to be used with, as opposed to things where the GIL actually gets in the way like heavy computation. 
https://developers.google.com/analytics/devguides/reporting/core/v2/limits-quotas 10 requests per second. https://dev.twitter.com/docs/rate-limiting/1.1 350 per hour. It's not that your exponential slowdown isn't bad. Your OK case is unlimited calls per second.
I use parallel python and wget. Maybe not the best soltuion, but you can run the wget processes on multiple machines. I do something similar and have 4 machines all handling the requests. http://www.parallelpython.com/
Twisted seems to be more for writing servers (at least, that's what I've always used it for). For http clients, use requests.
thank you :-) this helped greatly!
thank you...:) 
You could put your request in a stack and when you find new sites to visit you add them to the stack. This way you can have multiple threads popping and adding to the stack. 
maybe httplib2 and multiprocessing? Add deflate/gzip header if the server supports this for extra speed.
This is good if he wont be using threads. But that said I had some issues with this in a daemon when it mapped ~100+ async requests.
&gt; Made thousands of requests already, good so far. You're not hitting them too hard because it's synchronous. Start making 20 requests at once and they'll notice the load. If your crawler hits multiple domains (say by following links to a different site) you can parallelize but still restrict how often you hit each server. Use priority queues.
Thanks for posting!
Urllib3 has persistent connections which has helped a project of mine in the past. 
Sure. You're either: - intentionally or accidentally making less than one request per second, (likely if you're making synchronous requests) - Not crawling sites of any note or crawling poorly setup sites. Larger sites will throttle or block you without question if you exceed their recommended rate. Some are worse, like amazon which will tar pit you randomly.
Twisted has an HTTP client, [`twisted.web.client.Agent`](http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html), that allows you to do asynchronous requests. There's also [treq](https://github.com/dreid/treq) if you like the requests API, but want to use twisted.
&gt; Use requests unless you have a very good reason not to. A good reason is that requests is not part of the standard python library.
Thank you very much :)
Actually, the easiest way to speed it up is to use multiple threads. Use more threads than you have CPUs. That way, you can wait for many slow hosts at the same time.
I'm intentionally giving a 2 second time out between requests. 
I think you were looking for [Pyparsing](http://pyparsing.wikispaces.com/).
PLY is pretty nice, just Python bindings for lex and yacc. 
I run a python crawler which fetches ~10m pages per month. Some tips (though there's more than one way to do it): * as stated elsewhere, use [requests](http://docs.python-requests.org/en/latest/), it's really rather good * [robotexclusionrulesparser](https://pypi.python.org/pypi/robotexclusionrulesparser) is better than robotparser included with python * use processes/threads/asyncio to perform multiple downloads at once (I went for the built-in multiprocessing module) * it's polite to only hit a single site with one request at a time (use a delay, or send all URLs per site to a single thread/process)
Exactly what I was going to suggest!
It's not a binding - it's a pure-Python lexer/parser generator with support for LALR grammars. Here's a [link](http://www.dabeaz.com/ply/). It works with full language grammars, e.g. [pycparser](https://pypi.python.org/pypi/pycparser) for C.
it's annoying because I think the Django docs actually do save a cookie or something to remember which version of the docs you were last looking at... but I always arrive at the docs via Google and the pages in the Google index are all direct links to a specific version url so it gets ignored
Which is not what he wants to do, if he's making thousands of requests and concerned about the time to run. 
the OP of this thread is not one of those writing platform independent Python though. he is writing a script run by his own system to perform a specific task. he's not working on a multi-platform redistributable package that will be installed on any arbitrary system. your concern is misplaced. 
ANTLR has a [python target](http://www.antlr.org/wiki/display/ANTLR3/Python+runtime) that works [pretty well](https://github.com/BruceJillis/fp.py). Otherwise I would second [PLY](http://www.dabeaz.com/ply/).
It is arguably the only mature parser generator in Python. That said it has quite a few warts with emitting parser tables as temporaries .py files that I've found quite annoying when trying to package up libraries.
Personally, I don't like using magical libraries that look at function names and such. I like to have more control over my code. So I hacked together this thing. Far from perfect, but it's met my requirements for my rare parsing tasks. I wouldn't recommend it in production code, though. https://gist.github.com/tylercrompton/8166675 Example usage: import inspect import re import sys from lexer import Lexer if __name__ == '__main__': if len(sys.argv) != 2: print('Usage: {} file'.format(inspect.getfile(inspect.currentframe())), file=sys.stdout) sys.exit(1) class TokenType: STRING = 1 FLOAT = 2 INTEGER = 3 LEFT_PARENTHESIS = 4 RIGHT_PARENTHESIS = 5 LEFT_BRACKET = 6 RIGHT_BRACKET = 7 LEFT_BRACE = 8 RIGHT_BRACE = 9 IDENTIFIER = 10 DOT = 11 COMMA = 12 EQUIVALENCE = 13 ASSIGNMENT = 14 INCREMENT = 15 DECREMENT = 16 DIVISION_ASSIGNMENT = 17 MULTIPLICATION_ASSIGNMENT = 18 MODULO_ASSIGNMENT = 19 DIVISION = 20 MODULO = 21 MULTIPLICATION = 22 SUBTRACTION = 23 ADDITION = 24 COMMENT = 25 patterns = ( (re.compile(r'"(?:\\.|[^"])*"'), TokenType.STRING), (re.compile(r"'(?:\\.|[^'])*'"), TokenType.STRING), (re.compile(r'-?\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?'), TokenType.FLOAT), (re.compile(r'-?\.\d+(?:_\d+)*'), TokenType.FLOAT), (re.compile(r'-?\d+(?:_\d+)*'), TokenType.INTEGER), (re.compile(r'\('), TokenType.LEFT_PARENTHESIS), (re.compile(r'\)'), TokenType.RIGHT_PARENTHESIS), (re.compile(r'\['), TokenType.LEFT_BRACKET), (re.compile(r'\]'), TokenType.RIGHT_BRACKET), (re.compile(r'\{'), TokenType.LEFT_BRACE), (re.compile(r'\}'), TokenType.RIGHT_BRACE), (re.compile(r'[\w_]+'), TokenType.IDENTIFIER), (re.compile(r'\.'), TokenType.DOT), (re.compile(r','), TokenType.COMMA), (re.compile(r'=='), TokenType.EQUIVALENCE), (re.compile(r'='), TokenType.ASSIGNMENT), (re.compile(r'\+='), TokenType.INCREMENT), (re.compile('-='), TokenType.DECREMENT), (re.compile('/='), TokenType.DIVISION_ASSIGNMENT), (re.compile(r'\*='), TokenType.MULTIPLICATION_ASSIGNMENT), (re.compile('/'), TokenType.DIVISION), (re.compile('%'), TokenType.MODULO), (re.compile(r'\*'), TokenType.MULTIPLICATION), (re.compile('-'), TokenType.SUBTRACTION), (re.compile(r'\+'), TokenType.ADDITION), (re.compile('#.*'), TokenType.COMMENT), ) ignore_pattern = re.compile('[\s]+') with open(sys.argv[1]) as file: contents = file.read() for token in Lexer(patterns, ignore_pattern).lex(contents): print(token) 
Yep, I get that too.
Thanks for posting this, I was looking for something exactly like this to do some collaborative Python numerical work. In particular, its nice that it allows simultaneous collaboration in a 'Google-Docs' like fashion. I was looking for something like this. On top of that, users apparently can get up to 12 cores for running parallel Python work. I'm amazed its free! The only downside I've seen so far is that the IPython Notebook won't load for me. Not sure if that's a problem on my end or what.
I would suggest Beuatiful Soup. I find it to be pretty lightweight and easy to use. There is some documentation and some tutorial videos out there. It's also simple to get up and running.
Yeah, since its python I dont ever plan on making it not open source.
Can't tell if sarcasm but there are tons of python projects that aren't FOSS.
Awesome ! 
I noticed something irritating on the TPG page: &gt; This article has been translated into Serbo-Croatian language by Anja Skrba from Webhostinggeeks.com. God dammit, why do open-source types keep falling for this? **webhostinggeeks.com is an SEO scam** based on making fake translations of informative web pages and getting the owners of those web pages to link to them. No, "Anja Skrba" did not helpfully translate the page for you. A computer translated the page for you, badly, into a language where you're unlikely to notice something is wrong. I've also e-mailed this to someone who I think is the TPG author, but I thought I should mention it here in case anyone else is prone to fall for it.
Wrong sort of parser. That's a natural language library.
Sure. But that wasn't the point I was exemplifying. 
&gt;Twisted seems to be more for writing servers That indeed comes to mind when I hear of Twisted being used, but it has client protocol factories for almost every protocol that it supports as a service ( DNS, SMTP, HTTP, SSH, and Usenet ) http://twistedmatrix.com/documents/current/api/twisted.internet.protocol.ClientFactory.html Plus with deferred's, it can be somewhat easier to route hundreds or thousands ( via ePoll ) of calls efficiently through a common overlay/managing logic. Only major caveat is that I don't think you can pipelining outgoing calls through the same session easily, so for N requests it will make N new connections which would be a bit more wasteful than having a small pool of keep-alive connections. 
I've used antlr, which is pretty good. It's probably more suited to more complex grammars than pyparsing, although is less pythonic.
I hope requests could just depend on socket instead of urllib3.
Hi, author of the list here. You might also like this Python-only list : http://resrc.io/list/4/pycrumbs/
Parsley might be what you're thinking of. http://parsley.readthedocs.org/en/latest/tutorial.html https://www.youtube.com/watch?v=t5X3ljCOFSY
If I naively assume that list comprehensions don't have side-effects, stuff like this breaks my assumption and is thus surprising. It makes perfect sense in hindsight though.
I use SQL dbs to archive my large data sets, then write queries to pull relevant subsets. Pandas does some things SQL can't do (e.g nice plotting) and does other things in a much easier, faster, and more dynamic way than SQL, such as exploring transforms, joins, groupings etc. I've found it best to just take the path of least resistance and use whichever gets the job done fastest - also I've been contributing to Panda's SQL support: https://github.com/pydata/pandas/issues/4163 https://github.com/mangecoeur/pandas/tree/mangecoeur_sql That said, I'm keeping a close eye on Blaze since it might make it easy to blur the line between on disk and in RAM computation. 
Using venv in Python 3.3: py -m venv myenv myenv\Scripts\activate python ez_setup.py easy_install pip pip install something Ps You have linked old (0.8) version of `ez_setup.py`. Here is the latest version of setuptools (now 2.0.1): [ez_setup.py](https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py) Edit: As I mentioned below... Remeber to install (and register) [MS VS C++ 2010 Express](http://www.visualstudio.com/downloads/download-visual-studio-vs) before using pip. It's free. 
Wow... someone should really document that. I would have found that useful many blue moons ago...
You can run ez_setup.py from anywhere, it's not necessary (and even harmful) to put it to the Scripts folder, since it should be in your PATH, and why would you want to clutter it?
Many packages requires having some build tools installed to compile extension modules. In those cases, this setup will not work on Windows. Pip magic does require some more behind the scene magic just so you know. I use Ubuntu inside a VMware virtual machine when I'm working on Windows but do as you please.
Or download a binary pip from [here](http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip) and install
FYI, there will always be a "free mode", but sooner or later there must be some restrictions on space and cpu usage -- for now, there is still plenty of room to grow, though ;-) Yes, the IPython Notebook is flaky, but I can promise you that there will be progress. Right now, you have to wait for about 60 seconds until it loads (or try again if it fails). Once it has "connected", it's not 100% perfect, but should work fine. It also has this real-time sync between collaborators, which is not available in the regular IPython notebook. (as an advanced user, you can also run it on your own without that overhead) To get the "pure python" feeling in the Sage Worksheet, put this in a cell and evalutate it: %auto %default_mode python
When you do these types of tutorials, it's better to not use reddit self posts. It is incredibly difficult to search reddit effectively and it won't pop up very high on google. It's better to answer your own question on stack overflow or to make a blog post about it. If you still want to discussion reddit offers, you can still link it here. 
It took me a while, as a computer newbie, to finally understand what the "$" in "$ install pip" was for ... a *lot* of people dont know what it means
It's free due to some significant grant funding I've been able to obtain to increase the use of SageMath which I used to purchase a cluster of 19 computers, and also a gift from Google (there are several Sage developers and enthusiasts at Google). Also, I get unlimited free bandwidth and cheap hosting from the University of Washington. There will always be a free version, but at some point people will also have the option to pay for more dedicated compute power -- e.g., if you're doing research you might want to have a dedicated machine for a project from one listed here for some amount of time: https://cloud.google.com/products/compute-engine/. If you're teaching, you might want to pay extra for some amount of dedicated resources for your course, and homework grading functionality. I'm taking a sabbatical starting in June to work for a year fulltime on this project, so expect a lot more to come in the next year. 
Use [pip-Win](https://sites.google.com/site/pydatalog/python/pip-for-windows). !
For your own side projects are under your control, classes can be another story. When I was in college we started with python but quickly moved over to Visual Studio, so owning a windows machine was an advantage. I've used a chromebook pixel with Ubuntu on it. That has much nicer specs than the chromebook you mentioned. I think your best bet is to buy one and try it. If it doesn't work for you go with a machine that has better specs.
I'm kind of on a budget though, so I don't think I could swing anything more than $500. If I need windows for my CS classes I guess I could dual boot it with windows and linux.
Frankly, I wouldn't be too concerned about windows. If your core CS courses are taught in windows /only/, you probably (definitely) need to find a better program. There's nothing visual studio can do that a good text editor, a makefile and gcc/clang can't do the same or better - and when you do it in a unix environment you actually understand how all the parts fit together and how to fix it instead of saying "derp I pressed the build deploy button and it didn't work now what". Sure there's a learning curve to go the unix route sometimes but your post seems to suggest you want to learn and this is a good way. If you need an IDE, eclipse works wonderfully under Linux. The only thing you need windows for are building windows apps (and that's only if you're not using cross platform GUI toolkits like a responsible programmer). Seriously, if your CS professors/TAs are insisting on you using windows, you're not in a good CS program.
So the specs on the C720 would be fine for python? What about languages that require a compiler? I guess I could open a remote connection back to my desktop and have it compile the code there if I really needed it to, but then I'd be really dependent on internet connectivity. 
It's not gonna be screaming fast but it should be just fine. I still compile on a 600mhz athalon from time to time. The thing Id be most worried about is 16 gb of disk, that's less than my phone has. Just need to carefully evaluate what your usage will be
[Looks like it won't be any time soon :(](http://bugs.python.org/issue5337)
Since you're own a budget, I would suggest looking at refurbished laptops. You can't customize them, but they're often much cheaper. Most are also guaranteed by the manufacturer to be as good as new. 
A refurb laptop is a better way to go. With a Chromebook you will end up fighting storage size and speed issues. I recommend the best Lenovo T-series you can find. This will run Linux well and will likely come with a Win7 license, so you're covered if you end up needing Windows.
The Sage book is interesting: http://modular.math.washington.edu/books/sagebook/sagebook.pdf 
Why pay money for these? Get them 4 free out bittorrent! Pakt pays authors $4000 plus profit share for these books. They can afford us getting it for free.
The 'pip.exe' will be in 'C:\Python\Scripts' ... So call it from there or add that to you path environment var...
If you don't mind using a browser based IDE and cloud storage I suppose a chrome book would work -- you wouldn't even have to install Linux on it. 
MVVM architecture, data binding. Very simple JS, soon to be ported to Python thanks to Pythonium (https://github.com/pythonium/pythonium)
Generally speaking, will multiprocessing only help with CPU-bound tasks if you have multiple CPU cores? Or are there other advantages to doing so, other than splitting the work across each core?
Could someone explain to me what's happening at a lower level and why this is still faster even with the GIL? what is happening in the background at the TCP/IP level for the requests to all happen at once and not block each other in Python? Are pool_size number of outgoing ports making asynchronous http requests in multiple threads in one Python process? If you use multiprocessing, are multiple Python vms running, or is one executing one single bytecode at a time? how is it implemented so that simultaneous network requests can be processed but only one bytecode at a time? Thanks in advance. The limitations given the GIL confuse me when multiple threads like this can actually process.
It'd probably run a bit slower than it would if you ran it with just one thread (or process) as it's context switching back and forth on the same CPU. If there's any IO you may see a speedup, but then, you're probably better off just using threads in that case anyway. Other than that, I don't think there'd be any advantage to running multiple processes. That said, it's only a few lines of code to try! ;) 
I found a few responses here: http://stackoverflow.com/questions/4920471/python-threads-sockets &gt; As a rule, the lock is held when running Python code, but released for calls to lower-level functions (such as sock.send). As Python threads are real OS-level threads, threads will not run Python code in parallel, but if one thread invokes a long-running C function, the GIL is released and another Python code thread can run until the first one finishes. So it should only block Python code, not code primarily written in C. Considering so much of the standard library is written in C, though, it makes you wonder why most people generally agree that threading with the GIL doesn't seem to speed up anything except IO.
Python threading is pretty simple. When a thread starts, say A, it gets the GIL lock. so at this point, no other thread can run. Which is what we have know as the GIL restriction. The executing thread will yield to other threads only when it encounters a IO task. [note you can yield by doing intentional `time.sleep` as well]. At this point, assume another thread B was waiting; it will be invoked and called upon. Till that thread, B yields, our original thread A can't run. So thats threading for you. As per multiprocessing, you're physically running different python interpreters. In other words, if you did `ps aux | grep python` you will notice each different interpreters. You can communicate between these processes via pipes/queues. The reason thumbnailing the images is fast is because of the IO tasks [reading the imagefile, writing it back]. The only CPU intensive task is thumbnailing it. The reason it is fast is because as soon as it encounters the line `Image.open(` that current thread will yield, allowing another thread to resume and so on [context switching] Note its not technically executing in parallel as the author's title suggest. It is working because of coordination among threads to yield when necessary. Python gevent comes very close to using threads by leveraging greenlets [microthreads], which do exactly what I mentioned above. In terms of TCP/IP level of requests, it depends on what type of package you're using. For e.g. if you use twisted and tornado, both use linux `libevent2` which accepts a callback for events such as RECV [request received] from external clients. Normal way of doing is blocking yourself till you receive request and process it when it arrives. But imagine if 2 requests come at the same time, you either process it sequentially or you create new threads/processes to handle them so they can be done concurrently, which is what apache does [uses system level threads]. But as threads cost CPU/memory, at some point it will reach its capacity of handling threads and eventually crash. With single threaded servers, you need to provide a callback for the request event and process it on its arrival. The goal is to not to block that thread by doing CPU bound tasks for e.g. you can't use traditional database, you would have to resort to non blocking DBs like mongo [its the drivers actually]. Also you can't do computational work like computing fibonacci series for 1million. But with single threaded servers, you can handle 100s and 100s of connections assuming they aren't blocking. 
It means exactly what it says. This is done intentionally to avoid corruption of data between threads. Also as it mentioned, sometimes it will intentionally force a thread to yield by releasing GIL [by checking how long the thread as run] and give others a chance. Python doesn't have a GIL, its the implementation that have GILs. For e.g. CPython has GIL and is done bcoz C libs aren't thread safe. Jython and IronPython have no GIL and can fully exploit multiprocessor systems. More here on why GIL is used https://wiki.python.org/moin/GlobalInterpreterLock Note the reason why Cpython went with GIL is because you can use c-extension libraries and which are usually not thread safe.
Awesome. So, if I write a program in C and invoke it with multiprocessing, especially if there are no side-effects that affect the subsequent execution (or anything embarrassingly parallel), I'm taking advantage of multiple cores? For example, I assume I could write a C program with OCR that takes a URL to an image and dumps the text to a directory, then write a Python script to crawl the site and invoke the C program on each URL it finds *.(png|gif|jpg|jpeg|bmp), and that would run as parallel as possible.
I see you use JSON as an exchange format to hold the exported HTML and some meta data. You could output the entire org document as JSON to facilitate some more Python-side processing. Just an idea in case something more fancy might be wanted. There are a few issues in dumping an org document in this way but there was a recent thread on the org-mode mailing list about this approach with some working examples.
Basically, Python threads DON'T run in parallel MOST of the time. So if you use them, usually it's not because you want your program to run on multiple CPUs. However, if a thread is doing certain IO operations then other threads ARE allowed to run and you get some level of parallel execution. If all your threads are performing mains IO bound tasks then they will effectively run in parallel. Finally, Python tries to be nice in PRETENDING that threads are executing concurrently by trying to switch often between the created threads, this is useful when you want concurrent execution (e.g. in GUIs) rather than maximum compute performance though using all available CPUs.
&gt; So, if I write a program in C and invoke it with multiprocessing, with multiprocessing, you can write pure python code that uses multiple cores. 
Using your technique, also make sure that the Scripts directory is in your PATH. But I prefer using virtualenv, which automatically installs pip in your virtual environment.
This is very doable. I write real code for real clients on my Samsung ARM Chromebook running Lubuntu inside of a chroot. Mostly Python and Javascript, a little C. On occasion I have to process huge amounts of data, in which case I spin up an Amazon ec2 instance to do the heavy lifting for an hour, costing me a ~$1. Unlike the c720, the ARM Chromebook doesn't even have Linux drivers for the GPU right now-- I just switch back to ChromeOS when I need to test WebGL code, but that won't be an issue for you. Plus the battery life is going to be far more spectacular than you'll find on a refurb. That Acer Chromebook's really a lot of bang for your buck.
It's an intresting post detailing a simple thread-pool recipe, but sadly I don't think the result is anything usable in production. The main problem is that if the code you are running in the threads raises an exception, you don't get any traceback. Your appliction becomes impossible to debug. The traceback always indicates the lines in the multiprocessing module itself: In [1]: from multiprocessing.dummy import Pool as ThreadPool In [2]: pool = ThreadPool() In [3]: f = lambda x: x/0 In [4]: pool.map(f,range(4)) /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.pyc in map(self, func, iterable, chunksize) 223 ''' 224 assert self._state == RUN --&gt; 225 return self.map_async(func, iterable, chunksize).get() 226 227 def imap(self, func, iterable, chunksize=1): /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.pyc in get(self, timeout) 520 return self._value 521 else: --&gt; 522 raise self._value 523 524 def _set(self, i, obj): ZeroDivisionError: integer division or modulo by zero I disscused this more in detail in this [stackoverflow post](http://stackoverflow.com/questions/15359295/python-thread-pool-that-handles-exceptions). 
&gt; but released for calls to lower-level functions (such as sock.send) To be exact, GIL is released at `Py_BEGIN_ALLOW_THREADS` in CPython. And it's almost everywhere in IO
Thanks for the comments! Do you have a link to the mailing list discussion?
Idea is on the right track, but why not just use concurrent.futures? That would give you the ability to map a function to a list (threads or processes), as well as exception support, and a few other features. In the last example: pool = Pool() pool.map(create_thumbnail, images) pool.close() pool.join() would be 2 fewer lines with concurrent.futures: with ThreadPoolExecutor(max_workers=4) as executor: executor.map(create_thumbnail, images) Python 2.5+ (backport): https://pypi.python.org/pypi/futures Python 3.2+: http://docs.python.org/3.2/library/concurrent.futures.html 
Still using T60 after getting it new many years ago. Hauled it around in the backpack while traveling for work. Spilled beverages on it. Kid tore off some keys (so replaced keyboard with new one). Fan got clogged with dust so replaced that. Installed an SSD drive and still use it. Love the square screen and mate finish. Good resolution, keyboard is better than many full sized desktop ones, nipple mouse is great and pretty durable. 
I tend to put random sleep times in my crawlers. 
&gt; Heh... because I had no idea that existed..! ;p Well you are in for a treat then! It's great for parallelism, and futures can be a much simpler mechanism to think about than other alternatives.
There's a bit of a straw man at work here because while there may be a number of outdated examples on the web saying you *can* subclass `threading.Thread` (isn't that IBM example from 2006 or something), Beazley in the most recent Python Cookbook and others see this as somewhat of an anti-pattern. Instead, they recommend instantiating functions as threads and using queues. It's a small point and the author's complaints in general are still valid but the 'standard' is less egregious than you might think from what's represented here.
I use conda when a package is available at the repository. I have MSVC 2008, 2010 Express Editions installed, but I get a message similar to [this](https://gist.github.com/anonymous/74c8b478724fcd853597) whenever I try to install a package which need compilation. My Python version is this: Python 2.7.5 |Anaconda 1.8.0 (64-bit)| (default, Jul 1 2013, 12:37:52) [MSC v.1500 64 bit (AMD64)] on win32 Do you know how can I fix this?
I've been working in Python professionally and for my own side projects on a C7 series for quite some time. I dual boot with Xubuntu for times when I don't have access to internet or if I need to build a GUI using PySide or PyQT and performance has never been an issue for me. If you go with it I'd recommend installing [Secure Shell](https://chrome.google.com/webstore/detail/secure-shell/pnhechapfaindjhompbnflcldabbghjo?hl=en) and [Text](https://chrome.google.com/webstore/detail/text/mmfbcljfglbokpmkimbfghdkjmjhdgbg?hl=en) as well as getting a cheap linux virtual server somewhere (right now I'm using DigitalOcean as I get unlimited bandwidth) and configuring it with IPython and your terminal editor of choice, be that Vim, Emacs or even just Nano to start with. That way you can SSH into your server and do any kind of Python work you might want to without having to swap over to Linux. You also get the added benefit of being able to set up a webserver or test things like that. Oh, and I also highly recommend installing [Chrome Remote Desktop](https://chrome.google.com/webstore/detail/chrome-remote-desktop/gbchcmhmhahfdphkhkmpfmihenigjmpp?hl=en) on your Chromebook and any other boxes of yours so that you can remote in with ease. I use it all the time for jumping into my home Win7 box to make sure my code works across multiple Windows and Linux platforms as I develop systems administration tools for mixed environments. Of course a Chromebook isn't the correct choice for everyone. It's worked well for me but it really depends on your personal access to internet as well as development needs.
I liked this post but can't really grasp the benefit of yhat. You already built the predictor with the linear regression. Unfortunately, like 2/3rds of the remainder of the blog post are devoted to boilerplate code to interface with this external system deviating from the logic of detecting data anomalies. 
I opened this link, thought it was a crappy motivational poster and closed it again. After reading the comments I thought there may be more to it. Reopened and realised I could scroll down. It's Monday, it's morning and I did not have my coffee yet!... 
Cool! I'm gonna have a play around with this
LiClipse: http://brainwy.github.io/liclipse/ (which is the PyDev-based commercial counterpart).
&amp;gt; Heh... because I had no idea that existed...! That's one thing that irritates me about being a developer. There's always something better and more changing I don't know about yet! 
Not familiar with artoo, but look into ROS. It uses c++ but you can do almost everything with python.
So, based on the results: * 4 cylinders vs 6 cylinders does not matter * manual vs automatic transmission does not matter * build year does not matter by itself (only $2.23 per year) The things that do matter: * Car trim (about $1600 between EL and EX-L) * Mileage (about $600 for every 12,000 miles ~ 1 year) 
How is he gathering results. For example he says that single thread, for one of his examples, took 14.4 but with 4 threads it was 3.1. How is he getting that info? I'm curious for my own edification and testing. Thanks!
Very intresting posts. Will definately follow in the future. Good job! Throughout explained examples etc, they will come very handy debugging the network aspects in the applications I work on.
Good explanation, except that Twisted and Tornado don't use libevent2 (and are not linux specific. At least Twisted isn't). They use lower level select-like APIs, ie poll, epoll or kqueue, effectively reimplementing libevent.
Ha! Right? My career thus far as a developer has been slowly and unknowingly re-implementing a slightly crappier version of things that already exist. 
There are lots of ways, you could explicitly setup a timer. e.g. import time start_time = time.time() # run the code print 'Time taken:', time.time() - start_time Or profile, or use the timeit module... But! because I'm lazy, and only had the one thing in the script, I just relied on the output of my editor. Sublime Text has a "Time taken" result at the end of execution. So, I just used that number. 
Thank you for this. I will see if my IDE, PyCharm, has a similar feature. If not, I will bake this in. Appreciate the tutorial and comment reply.
Very nifty!
You're right, should have checked the docs before I opened my mouth. For some reason I assumed it did.
rock, thanks! this solves some issues for me, too, i'll have to start using that. beyond threading, i had looked at a CSP module to achieve this, too, but it seemed to leak resources. this looks like a much cleaner way to do this. 
If you're on a unix machine, it probably has the 'time' utiliity installed. Just invoke your command line prepended with 'time': &gt; time python ./myscript.py ... output ... real 0m0.113s user 0m0.002s sys 0m0.006s &gt; 
Very cool. I am on a *nix machine and never knew this existed. Thanks!
Why are they called "futures"? 
I went to the PEP: http://www.python.org/dev/peps/pep-3148/ which pointed me to "Java Futures" documented here: http://docs.oracle.com/javase/1.5.0/docs/api/java/util/concurrent/Future.html which had the answer: "A Future represents the result of an asynchronous computation." 
Probably because you get the result some time in the future :D it's usually used with asynchronious programming, where you get a future (or promise) as an intermediate result, which you can use when the calculation is finished.
At least then you understand how the things work!
The statement about "databases" is incorrect. It doesn't matter what database you use, it's the type of driver and whether or not it's capable of "async" execution. For example, psycopg2 for PostgreSQL can work in non-blocking mode. Mongo isn't a reliable database by any means by the way.
&gt; This plea could set the Python world on fire, or could just land in the scrapheap with all the previous great ideas for other people's time. Yeah. This kind of attitude is what keeps people away from contributing to open source projects. This is not a problem which can be solved by writing more code. It is a problem which is solved by having discussions and making decisions.
I'm glad that I'm not the only one who thinks this. I'm not sure anything will happen, because the only people who actually engage with Python 3 questions are weenies who take the attitude that everyone who doesn't use python 3 is dumb. Edit: To expand on that, the current approach assumes that if Python 3 is built, users will come. That has proved not to be true, and anyone who opposes the reunification strategy should explain why there will suddenly be a point where everyone decides to make everyone else upgrade to python 3. If you can't construct such a scenario, then you need to either try something else, or give up.
&gt; Tulip async, and the coroutines it is built with, was the first compelling thing to make me dive into Python3 - what is attractive about Tulip cannot be reasonably mimicked in Python2. Are you saying the proposed Python 2.8 can't gain a "yield from"?
Yes, if 2.7 is forked it can gain a "yield from". Otherwise, difficult for me to speculate about a version specifically disallowed by Guido. http://www.python.org/dev/peps/pep-0404/
Also, let's be honest: although `yield from` is nice, it can be replicated with one line of python 2 code: `for item in iterable: yield item`.
I don't know that it's the most popular, but https://pypi.python.org/pypi/gevent-socketio/ works well.
Not really. `yield` can return a value, and it makes things a little bit more complex than just a for loop. The motivation part of the pep explains this better than I do: http://www.python.org/dev/peps/pep-0380/
Didn't I say that in my statement as drivers, with an example like mongo. &gt; you would have to resort to non blocking DBs like mongo [its the drivers actually] And saying mongo isn't a reliable database is little presumptuous. Go check who is using mongo and use cases of it and you will be surprised. Either ways, I am not a mongo fan but I use it in specific places like pub/sub and messaging and mobile data. I am never a fan of document based storage unless I dont care a whole lot about my data. For main database, we are using Postgresql. We are looking into http://txpostgres.readthedocs.org/en/latest/ for postgres async calls.
So, I am an old R user, and new Python user still trying to get a feel for the Python landscape. I've been learning Python 3 (because why not learn the most modern version?), which frankly has been pretty miserable (tutorial code doesn't run, needed modules don't work, IDEs often don't support Py3, etc.) Should I just give up and move to Python 2.7? EDIT: I use python almost exclusively for research related data analysis.
He asked about multiprocessing not threads, so there is no context switching in threads. They basically run as individual processes, so its upto the OS to do switching of processes if one needs more than the other. Having said that, a process can send a message to another process to halt in a hope to give up some of its resources its holding.
Yes, by all means do move to 2.7. The 3.x branch is still not ready for someone learning the language for immediate practical goals.
Thanks for the answer. I've read a bunch of "Should I learn Python 3 or Python 2" posts and articles, but frankly they all give the impression that Py3 is a real option right now, which according to the blog post above is simply untrue.
Which libraries for data analysis you need are still not working in py3k ? Which IDEs ?
So maybe I was wrong about Spyder, but frankly Spyder works out-of-the-box in 2.7, but after hours of trying I can't get it to connect to my anaconda Py3 without an error. I could keep trying, but it takes time away from me doing real work.
Yeah, thanks. That makes a lot of sense. I'd been spending way to long fighting my stack to make 3.x work, whereas 2.7 just seems to work out of the box.
The reason Python3 isn't ready is because most modules that we need are not supported by it and are 2.* only; Django does not a python ecosystem make (other options exist but they have the same flaws of not being supported for python3), and unless I can DEPLOY my app - good luck finding a host that natively supports it as not a SINGLE "big cloud" company supports a very recent version of 3.*! - this is a academic discussion. Edit: Python 3.3 support for personal small projects is all well and good, but unless I can actually DEPLOY my app in a production system supported by "the enterprise" in the "cloud" and use the modules I expect (and cant because they are not currently compatible), this is an academic discussion. Django is great as an ORM, but it by itself does not make a python ecosystem, especially given that it refuses to play nice with others in the community. I agree that its a driving force, but even its own support for python 3.* is not perfect. The fact is, Python3 as a language is NOT supported by the most important places: Where we deploy apps. Good luck trying to deploy python 3.3 to AWS or Azure without a lot of hassle, because only 2.7 is supported natively on both. Sure, I can buy hosting with my fiends uncle or manually set up a server of my own to support it, but until I can press a button and use my credit card to get cheap, effective hosting for my python3n apps like I can with PHP, etc its just not going to take off. And this is further complicated by the total lack of support for Python3 for most modules. Edit Again: We need better support for newer versions by default in Centos Linux, Ubuntu Linux, and the Debian Linux Spinoffs. if the Python community wants to win against other languages (or at least get better support for Python3) it needs to start there and get itself the default version of python to be 3.3+ in all of them. After that, getting it supported in AWS, Azure, Heroku, etc so apps in Python3 can actually be deployed will be a lot easier!
Oh, also the SublimeREPL package doesn't work with Py3 (at least not that I can tell after many more hours of trying). Yet, it works perfectly with 2.7.
&gt; Django does not a python ecosystem make, ???... but django IS web dev. and web dev is software dev. therefore django is software dev. 
Sounds like you need to update setuptools. Even better, you should use `pip` instead of `easy_install`. See http://pip.readthedocs.org/en/latest/installing.html for instructions. UPDATE: In case it wasn't clear, I based that on glancing at your error message. It sounds like it expects some functionality that isn't present in older versions of `setuptools` (or possibly `distutils`, but I'm more inclined to suspect `setuptools`, since `distutils` has been pretty stable/stale IIRC).
If you are starting on django I'd recommend the official tutorial. Sometimes I scan through the official documentation to find new and hidden things. Most of it is explained clearly.
You don't need Python 3.4 for async. Both greenlets and gevent provide a great alternative to Tulip. I'd even argue it's better, written by guys who really know their networking event-driven code!
Not true, you can deploy/build a web app without django, using pylons or pyramid for example. Django is not the only option, even if it doesn't like to play nice with others or admit they exist, or refuses to work with things that wont/cant use its admittedly nice but also entangled orm.
That still amounts to something nice, but really only of interest in relatively rare cases. 
&gt; "Should I learn Python 3 or Python 2" You should learn Python 3 *and* Python 2. 
I second the motion for python 2.8
Yes.
Or, maybe correcting the mistake of forking the language into two incompatible versions would solve the problem without requiring everyone in the whole community to support the switch. That sounds a bit more tractable.
I did not know about YHat's ggplot. It looks like a nice library. For all my data analysis figures I use matplotlib. YHat's ggplot itself depends on matplotlib (and pandas and scipy) all of these libraries ported to py3k since long ago. You are right unfortunately ggplot does not take advantage of this fact. Anyway this does not prevent you from drawing high quality plots in py3k. Relative to IDEs, non-commercial versions of many of them support py3k (pycharm, pydev, iep, pyscripter). You are right Spyder don't... until now. Spyder had a experimental release (2.1.14) early in 2013 to test py3k compatibility. Currently there is beta 2 version of Spyder 2.3 which initiates the series of py3k-compatible releases. IMHO, py3k ecosystem is currently ready (mature ?) enough for data analysis and presentation. Py3k is the future and many people has done a lot of work to bring it and 3rd party libraries where they are now. The increase on the [number of new py3k libraries in pypi is accelerating](http://dev.pocoo.org/~gbrandl/py3pkgs.png). I understand people with a lot of code in their backs can see the transition hard (I am in this position), but if you are starting now, if you already started, I would not go back to what, in few months, will be the past. 
Conversely, you loss the capability of using the [new py3k-only libraries](http://stackoverflow.com/questions/4948600/is-there-somewhere-an-index-of-py3k-only-libraries) which are already beginning to appear.
&gt; Here's an idea: let's release a Python 2.8 which backports every new feature from Python 3. Isn't it precisely because of this that people can stick to 2.7 and not upgrade? So many things have been backported. Now, with 3.3 and 3.4, features aren't backported and that makes me want to switch once and for all.
He makes a number of good points, but I think it's not nearly time to panic just yet. I don't think the core mistake was in the 2/3 break itself, it was the "5 year" timeline estimate. Python 2.5 was released in 2006, and took *7 years* for it to fall far enough out of production use that libraries are starting to drop support. Some major ones like Setuptools and Pip only just dropped support this year! Once 2.5 support is no longer required, things like `except ValueError as value:` and `b'abc'` become possible in a codebase. Those two things being safe are going to open the floodgates to ease of compatibility (that and u'abc' added back into 3.3, which was a pain for 2/3 compatibility under 3.0-3.2). So based on that timeline, i wouldn't be surprised at it taking another 2 years or so. *Then* panic is probably in order, but I think adoption will pick up. There are a number of attractive things in the 3.x line, from improvements in the VM scheduling to faster unicode processing to the fact that your code *is* better once it has proper unicode vs bytes separation.
It's been 5 years. Why is everyone so stubborn?
I understand. I am not using Spyder on py3k so I can not help. I am using PyCharm as my IDE of choice since 2-3 years ago, and it works great. I perform, however, most of my data manipulation, analysis and drawing from the IPython notebook. 
same
That's how I felt 5 years ago, honestly. It's slow moving. Unless you need a 2.x library though, I'd do your best to stick to 3 and just watch out for the obvious differences (why isn't print "string" working?!).
I never claimed one needs Python 3.4 for async. I just find Tulip async coding to be attractive, for the reasons stated by Guido. And one of the major design goals is for Tulip to play nice with greenlets and gevent (effectively "nicely" share an event-loop), so I am not sure I understand your point of "greenlets and gevent provide a great alternative to Tulip" when Tulip was written informed by the design and implementation of greenlets and gevent. Tulip takes the idea from Twisted of transports-and-protocols and uses coroutines/advanced-generators to avoid writing async code that is callback-heavy. This is the appeal and a different design criteria from the user-perspective from greenlets and gevent. If a group of motivated developers forked 2.7 so that Tulip could be implemented in it, that would be nice, I don't dispute. (In this thread you will find fans of the "2.8 idea" stating that [1] advanced-generators have a NOOP implementation in 2.7, [2] if you want Tulip you _actually_ want greenlets and gevent (no, Tulip _exists_ and I want that, thank you very much), [3] decisions and debate are more important than code -- all of which seem problematic. These don't disprove the validity of the "2.8 idea", but one doesn't have to be much of a Bayesian to come away with low opinion of the "2.8 idea".)
I'm running CentOS/RHEL at work. EPEL doesn't provide a python3 package and I've been too lazy to finish building my own :(
Well, no. What makes people able to keep using 2.7 is that libraries for it still exist. Ceasing to backport just makes supporting two versions of python harder.
Pyramid here as well. Even Turbogears, Flask, CherryPy, and a lot of others seem to be forgotten by a large swath of the Python community. Yes, I could do what I'm doing with Django. I don't because my resources are limited and I'm optimizing at every area I can, even compiling some modules to C. What takes 10 queries with Django, I can do with 5 in SQLAlchemy.
&gt; Ceasing to backport just makes supporting two versions of python harder. Up to 3.2 (inclusive), Python 3 wasn't an option (slow, no real useful features, no ported libs) and backporting to 2.7 was opening the door for libs to be gradually ported. A lot of packages have been ported or are actively done so. I'm not how continuining the backport effort would benefit the community. 
*Most* libraries have already dropped support for 2.5 (I don't know of any I use that support 2.5, and anything I write does not support 2.5 as of a few years ago). setuptools and pip are *very late*, being that they're essential tools for installation; they waited until everyone was basically out. And of the ones that care at all, there's very little incentive to care about 3.1/3.2. Not that I disagree per se with your point (well I do, the advantages you listed really aren't significant especially when compared to significant advantages of Py2).
I pm'ed you, but to reply: It means that right now, the only natively supported version of Python - per the docs of every single one of them - is 2.7.* Sure I'm sure with a lot of extra work you could probably shim a custom compile of python 3.3.3 into it, then have to manage your own platform, deps, etc yourself.. but why should you? I mean the core value proposition of "the cloud" in the first place is that you dont have to do all that, so why break it and try? For example Amazon wants you to use python 3.0 - the version nobody is actually supposed to use - if you want to use python3 at all, and only really supports 2,7.* per http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Python.html Azure doesn't care and doesn't like python in general, but supports 2.7 natively and flat out tells you to include your own compiled version and get ready to bend over and do a lot of extra work if you want to use it. A thread about this exists here: http://stackoverflow.com/questions/20648492/azure-website-use-python3-with-django When python 3.3.3 is the default python, or I can select it from a checkbox before I click submit, thats what I mean by "natively supported". If I have to compile *anything* or otherwise install *anything* on my host, your user base just shrank. The fact that linux in general (with few exceptions) will not allow python3 to be the default is another related but different issue; If its not installed on the Linux VM's these cloud companies use, its extra work )and extra cost) for them to be supported and as a result it will never happen unless they can make a return on that investment (and most wont even try if the default python 2.7 is "good enough")
Do It. This is a chicken or egg style issue; nobody will help port things to 3.3 without hosting support, but hosting support will not happen unless somebody dares to do it or their is enough compatibility to support it. Making it an option in a select box would kill the problem and provide incentive for porting, and while eventually Amazon, Azure, etc will also support it the fact remains you could be first, lead the charge, and get the credit for it in the python community itself.
&gt; Here's an idea: let's release a Python 2.8 which backports every new feature from Python 3. Why not hybrid 3/2 interpreters, which allow modules to be written in a different Python versions? 
Yes, it is. I suggested 3.x is bad, if you're learning the language, not that it's bad in general. Someone new to Python shouldn't really be bothered with problems mentioned by /u/icodeinr (2.x only tutorials, not working example code, etc.)
&gt; People could start to clean up their ugly Python 2 + 3 hacks. Care to elaborate to which hacks you're referring to?
This might be of interest to you: [Micro Python](http://micropython.org/)
Don't panic people! Windows XP came out in 2001, that is 12 years ago, and there are still companies using it for all their desktops. There are still many COBOL and VB6 apps out there. If some people want to stay on Python 2.7 for the next 20 years that is not an issue. Python 2 is frozen and a stable platform, please leave it that way and don't mess around with it by doing silly upgrades. There is nothing hindering anybody from adapting their own code to be compatible with 3 also if they want to use something newer.
When I look at the inside of Pyramid or Werkzeug, I see a lot of ascii/unicode hacks to make code work in both 2 and 3. Also stuff like this: http://plope.com/Members/chrism/python_2_vs_python_3_str_iter 
Ian Bicking also weighs in on this: https://plus.google.com/+IanBicking/posts/iEVXdcfXkz7 Worth reading. 
The hacks around byte/unicode objects is rather easy to solve if you use the litterals and accept to not support 3.2-. But I admit this was a late backport to 3.x this time. As for the __iter__ attribute... er, I had never done that. In that example, they want to check something is in a sequence. Python seems to indicate you should be using __contains__. So that assumption about __iter__ was a bit flaky I believe.
My point is that maintaining code for two versions of the language is *much* harder than writing Python code should be. When I write code I don't want to worry about this stuff. I disagree that the unicode/byte story is easy to solve if you use literals, as you also have different assumptions in Python 2 and Python 3 libraries. This is why web libraries tend to get hairy. I grant it may be easier for code that doesn't have to deal with this issue so much. 
Most libraries already dropped 2.5. The 2.6 release contains useful 2.x -&gt; 3.x porting aids like `from __future__ import print_function`, `unicode_literals` or `b''` syntax, which in my opinion are still very much underused. Even if a module author doesn't have time or will to provide full 3.x port, the least thing he/she can do is using these features, clearing the exposed bugs and sticking to 2.x for now. This thing alone would make life much easier for a volunteer who'd like to step in and port the module to 3.x. We should try to use more extensively what 2.6 already provides and then see if 2.8 is really necessary.
Sure it's not easy to maintain two different version of the same language but I've found that 3.3 has made a lot of effort to smooth the most common pitfalls (such has the byte/unicode). Personally, the strictness of 3.x has made me realise how sloppy my was with 2.x because the language was more lenient. My initial point was that it seems 3.x has to move forward and leave 2.x behind. I think it's fair to assume 3.3+ is that starting point where you, as a library developer, may start making a decision whether or not you wish to support 2.x at all (whereas until recently, you'd have left 3.x behind instead). I try hard to support both but, now, for my own use case it's more annyoing to keep supporting 2.7 because 3.3+ has some attractive features. As a developer I find it annoying, like you, that it's not backported, but I also understand they have to move the all ecosystem forward, not just the language.
Writing that Monte Carlo Tree Search agent is exactly how I learned Python in the first place! I do get what you're saying about 2.x-only tutorials, though.
Well yeah, I agree but I'm not sure the core developers have left lib developers that far behind. Most features have been ported to 2.7 (and even 2.6 for a while). Since 3.x is a major release, differences are bound to appear making it harder to support. If there were no differences at all between 3.x and 2.x, what would be the point? In my mind, even though it has taken a while, I find they did a rather nice job keeping 2.x close to 3.x feature wise. But they can't carry this forever. I wonder however if the core developers would support the idea of a community-driven 2.8+. Not something undertaken by the core developers nor the PSF but a different set of people.
I have this weird hypothesis that people are scared to move to python3 because they think its wildly different from 2.7. If you're writing something from scratch that's not the case at all. If you use python 2.7 you already know python 3.3 but the biggest difference is the change to print. In all the examples of the new version the different print syntax makes the code look much more alien than it really is. After all its one of the most commonly used functions. It makes things look scary and I think people run back to what's safe - 2.7. If we change the print syntax back in python 3.3 and release it as 2.8 I bet people will just use it and update their libraries to work with the version. Or at least be way more willing to try. Anyway I'm only half serious, but I think there's definitely a psychological effect here.
Can you please point me to some good docs on using tulip? I don't know what's up but I can't find anything official.
How would a community 2.8 solve anything, that basically would be a fork of 2.7 putting us into having 3 production versions of python out there, p3, official p2, and community p2. I guess it could serve as a stopgap solution for an app developer stuck on p2, but it just seems like more of a mess to me for a lib developer as you're still going to want to support 2.7 as I can't imagine the whole community migrating to 2.8. I'm fine with our current situation. I use python 3 almost exclusively now and there are things I miss (though not as much these days), but I've simply found alternatives or have created my own solutions if they didn't exist.
Better than a 2.8 that includes all the new features, I'd rather see a 3.X that includes all of the OLD features. I don't upgrade to 3 because it isn't compatible with my existing code, not because it has new features.
This seems like bad advice. If you find you cannot get by with 3 because there is not a library to do what you need that exists in 2, sure. Otherwise though while not all of python 2 libraries have been ported there are often alternatives which if you're coming in fresh not knowing either there's no real advantage in a lot of cases. Any lack in 3rd party libraries I think is made up for by the improvements in 3, 2 years ago I'd say it's 50/50 which to choose, but at this point python 3 is the way to go, most of the major *and* minor libraries have been ported and those that haven't in a lot of cases even better libraries have emerged. 
What do you mean by 'immediate practice goals?' You act like the language is in a preproduction state and full of bugs. As far as the core language goes python 3 is better, period, no need to discuss it. The only reason to even consider 2 is if there's a library you'd like to use which is unavailable in 3. Edit: I see you mean it's bad because you can't follow 2.x tutorials without already knowing the differences, but why the hell would you read a python 2 tutorial if you're learning python 3? Pythons documentation is one of the biggest things I like about it even, the official python 3 tutorial should be more than enough to teach you the language.
You should learn python 3 and then read the official transition guide to fill in the gaps if you ever want to learn 2. You might find you don't ever need 2 at all, I haven't used it in over a year at this point.
What do you mean it's the only option? You even just mentioned pyramid. There's a ton of different options out there.
Python is better because your boss said so. Java is not as good because you will get sacked for ignoring your boss. 
If you're using 2 there isn't much reason to switch, but likewise we're at the point where there's no reason you can't, most 3rd party libraries are supported. It wasn't supposed to be a fast transition and most of the software side of it is already done. Most of the 3rd party library developers have already made the switch so next is for the application devs to make it. You'll probably have another couple years at least, but eventually you'll reach a point where app developers start dropping support for python 2 as a whole. I think you underestimate how much support python 3 have and there's a pretty small subset of the community who really have a problem with 3, which mostly boils down to an unwillingness to change/learn the differences, no amount of whining is going to make us go backwards.
Why not just learn python 3? If you want to keep using python 2 it's probably because you don't want to learn 3 so just stick with 2. Having a 2.8 is not going to help library devs because they'd still have to support 2.7, so other than some edge cases who benefits?
where's there an official source for "Python seems to indicate to use `__contains__`" ? this doesnt work at all, because of course `x in somestring` is very idiomatic: &gt;&gt;&gt; a = "123" &gt;&gt;&gt; b = [1, 2, 3] &gt;&gt;&gt; hasattr(a, '__contains__') True &gt;&gt;&gt; hasattr(b, '__contains__') True I've been hit by the `__iter__` issue here as well. A single supported story would be helpful.
Because it hasn't actually been 5 years. Python 3 started actually being *recommended for production use* starting with 3.2. That's when most performance regressions were fixed and the core devs were finally happy with the look of everything that had been redesigned / cut. You should count a 5 year migration path from when people were actually expected to start legitimately moving to python 3, which was at the 3.2 release, not 3.0. 
&gt; If you want to keep using python 2 [...] i do not *want* to keep using Python 2. I *want* to use the latest Python. If an interpreter can deal with both, it's easier to make the transition gradually. Example 1: I'm using a package not yet ported to Python 3. (No longer a big problem, but still.) Example 2: Someone is still using some code that will not work with Python 3. Thus, the whole distribution/company/server/product defaults to Python 2.xx. Thus, I'm encourage to stick with "the norm", Python 2. (This is a very real problem) Example 3: I want to distribute Python code for others to use. Those people may use Python 2 for one of the reasons above. My code may work on 3, but I have to make sure it also works on 2, thus not really using Python3. (Catch-22!)
There is not going to be a Python 2.8 so the point is moot.
Why did you choose Python over R or other programming languages?
wow. I was kidding, I figured the hip corporate recruiter party line was a dead giveaway. 
those are wise words from a js developer. :) Always love reading Ian's stuff. And I love the shit out of webob. 
My bad I was just quoting what I had read in my Learning Python. It didn't really state what form of python it just said ''python''
Once Debian, Fedora, OpenSuse etc set the default python version to python3, that is when we are going to see changes, I guarantee it. (2 years from now) I currently use Fedora 20, and the default python version is STILL python2.7 I would like to move my django powered sites to python3 but mod_wsgi has some trouble with 64bit libraries. (If anyone from the fedora community sees this, please test it) uwsgi + python3.3.2 + django1.6 produces internal server error. (I follow this tutorial) http://uwsgi-docs.readthedocs.org/en/latest/tutorials/Django_and_nginx.html#test-your-django-project A big flaw with python3 is that the venv by default doesn't include pip or setuptools. Lucky for me, someone else extended the script to include those two by default. mod_wsgi for and uwsgi is what I'll spend some time on this weeks, looking where exactly the bug is. (maybe it is a flaw in the tutorial, or maybe it is an actual bug in the code)
&gt;It will also deprecate anything which can't be changed in a backwards compatible fashion, for example str + unicode will emit a warning, as will any file which doesn't have from __future__ import unicode_literals. I doubt that the nagging would help anything. It is unlikely that there are that many people out there who are not yet aware of the issue. Those that are happily using 2.7 can continue to do so indefinitely. Do the bulk of Python users actually care about Python 3? 
2.8 with warnings? Actually, I'd probably use that. (Not sure it's that easy in practice. We had the break-away for a reason, yes?)
Spelling, derped a word.. updated.
Yep, jump to 2.7 you will be able to do everything you can do with data analysis and python, and it'll not be difficult to switch to 3.x when possible/needed
/checks google. appengine still on 2.7, blame google. 
i've particularly enjoyed not seeing an oracle logo in years when trying to find api docs. so there's that. 
&gt; NumPy and SciPy are VERY VERY fast numerical analysis libraries. they are fast because of the very mature C and fortran code not because of python. 
&gt; You'll probably have another couple years at least, but eventually you'll reach a point where app developers start dropping support for python 2 as a whole. Why would they do that? Is Guido going to send up a flare? &gt; no amount of whining is going to make us go backwards. The only "whining" I see is from people who can't believe that not everyone has moved over to 3. I'd say that no amount of whining is going to cause the switch.
&gt; Can you do that with Java? (Honest question, I'm sure you probably can) http://en.wikipedia.org/wiki/Java_Native_Interface
Because that's the worst of all worlds. Better to have a version in which you can write polyglot code with ease.
That's probably the same as the mooted 2.8.
&gt;&gt; You'll probably have another couple years at least, but eventually you'll reach a point where app developers start dropping support for python 2 as a whole. &gt;Why would they do that? Is Guido going to send up a flare? The same reason you drop support for any old technology, it's extra work to maintain and holds you back from utilizing new features.
I just `git clone`'d it straight from yhat and the `ggplot` module works fine for me in Py3. What problem were you having with it?
because if you are sitting on a sufficiently large codebase (open source or commercial), then migration is a major pain... And it's not just the migration itself, what comes *after* migration is not looking pretty either.. Essentially, your options are a. abandon python2 altogether (pain for your users) b. maintain 2 code bases for a (long) while (pain for you) c. maintain ugly hacks+use 3to2 or 2to3 to keep a single code base+ aggressively test on both 2.x and 3.x series (pain for you). No wonder that everyone takes wait-and-see attitude.. 
I think a lot of people have been saying that. But it makes no difference because the Py3 effort is lead by Py3 enthusiasts. The alternative of a fork would require, in effect, a parallel community apparatus to co-ordinate the Python 2.x future series. That will only happen if there is a great sense of urgency.
It looks great. I'm in the starting stages of trying to make a simple page for myself via flask/heroku/bootstrap. Any chance you're going to put it up on Github? Thanks.
There's a d: maintain a single codebase that runs natively on both Python 2 and 3, probably with the help of a library like six (designed for that purpose). This is what Django did, this is what SQLAlchemy does as of 0.9.0 (released today!), this is what I've done for things like html5lib. As I argue in a lengthy comment on Alex's post, this is the approach that really needs pushed to convince people it isn't an insurmountable amount of work (tools like modernize will get you a lot of the way there) *and* it doesn't have a long-term maintenance cost (that two codebases would, or even just making sure 2to3/3to2 can still port your code successfully). I believed what you did for a long time (as a maintainer of a top 200 PyPI package!), but eventually realized I was wrong. PS: happy cakeday!
http://pandas.pydata.org/pandas-docs/dev/comparison_with_sql.html ([src](https://github.com/pydata/pandas/commit/8986592b478974f8bcaccbe9fecd68411332e285))
I have used R for a 7 years or so. I still love it but I've moved out of academia and into tech, so having code that developers can read (i.e. python) is a plus.
I can vouch for many XP machines where I work.
I think he is mostly right about the reason large company codebases are not moving to Python 3. The things you gain do to not outweigh the pain of migration. We have a couple hundred thousand lines of Python test automation and library code, and I've tried to justify the investment to management a few times, but it just doesn't make sense. I actually wish they had waited another year or two on Python 3 and done something more ambitious, like a major GIL overhaul, revamped packaging, optional explicit typing, or something like that. If you are going to break backwards compatibility, at least do it for a very good reason. Find a thing that the community really wanted, implement it in Python3, and give people a strong reason to switch. As it is now, the conversation usually goes something like this: "How much time do you need?" "A few weeks at least, maybe a month" "What do we gain?" "Some syntax changes, better Unicode, ordered dictionaries, a few other miscellaneous things." "Will it run faster?" "No." "Is it solving an urgent problem?" "No." "Then why do we need it?" "Well, we don't really *need* it." "Then why should we invest the time when we could be doing other productive things?" 
Search "mongodb loses data" on Google and YOU will be surprised. Also has some major performance issues, but I'll leave the Googling to you.
you lend credibility to the parent.
You're assuming the conclusion you desire - that 2.7 will become obsolete. I'm asking you why you believe that will happen. You're indulging in circular reasoning.
I'm using [seaborn](https://github.com/mwaskom/seaborn) for a publication i'm working on now. It makes matplotlib figures much more pleasant looking to my eye.
No, it's because they would need to spend a complete release cycle doing porting for their entire codebase. That's a cycle they could spend adding features or fixing bugs.
Not exactly. The 2.8 that is mentioned is a way to introduce the new features available in the 3.X line, while allowing users to continue using the discontinued features FOR THAT RELEASE ONLY. My suggestion is to create a new 3.X that has the version 3 features, and carries the discontinued features forward, as they should have been in the first place. It was a dumb move to discontinue those features, and it would remedy that problem if the backwards compatibility were restored.
I use `matplotlib` for my published graphics, but it takes some work beyond the default settings to get things looking nice enough for publication. There are some up-and-coming libraries that make an effort to present a nicer interface and better aesthetics (bokeh, for example), but which aren't quite ready for use, in my opinion. Matplotlib allows for exporting figures in the standard, high-quality formats, such as SVG and PDF, but you'll probably want to change the default settings. I've never used it directly, but [mpltools](http://tonysyu.github.io/mpltools/index.html) has some helpers for this, especially the [ggplot](http://tonysyu.github.io/mpltools/auto_examples/style/plot_ggplot.html) style. For what it's worth, I've been wanting a matplotlib replacement for a while. If you find something, I'd be curious to hear about it.
It's it about time to admit that the BDFL screwed the pooch on this one, and just add the discontinued features back in, and make 3.X backwards compatible?
We've had to support ancient versions of python because that's what debian stable and rhel ship. Thank God for 2.6 in epel, or I'd still be using 2.*4*.
I thought that's what they *are* doing. 
With Webfaction you can build Django 1.6 projects in python3.3. They make it pretty easy, in fact, to build a bunch of Django projects using various Python versions and whatever 3rd-party libraries you want.
Thats one niche webhosting startup; Not a big major cloud provider. Yes they use softlayer, but the moment softlayer adds these features python3 wins and webfaction loses
What about `yield from` as a reason?^* *(sort of kidding maybe a little bit: I actually really like [`yield from`](http://stackoverflow.com/questions/9708902/in-practice-what-are-the-main-uses-for-the-new-yield-from-syntax-in-python-3)).
what do you mean, you can install any version of Python you like on AWS
There's actually an interesting response to that in a stackoverflow thread: http://stackoverflow.com/questions/9708902/in-practice-what-are-the-main-uses-for-the-new-yield-from-syntax-in-python-3
Very good points. Additionally (IMHO) all 3.x releases before 3.3.0 haven't really been "production ready".
What are the actual advantages of 3 over 2?
The fact Massimo hasn't, yet, just further proves that Python 3 isn't catching on. Feels a bit like NextOS. I genuinely wonder if in another 5 years we'll see a dwindling minority of purists using Python 3 and huge population of supported legacy libraries for Python 2.
nobody's moving to py3 because we're happy with py2x, the reason PHP and Ruby move forward is because they're cleaning the clusterfuck mess that the language is but here there's just a few nice to have and the new version and nothing else, py2x is stable and beautiful not only there isn't anything compelling in py3 to motivate adoption but there's nothing in py2x that makes people want to leave it.
Well, a technology that never gets updated *has* to become obsolete, right? At some point, it's not even going to get security fixes anymore, so it would be foolish to rely on it.
So what exactly do I need to get exactly what Khan Academy is using? and I downloaded the scripter and installed it, however a message keeps popping up saying "Python could not be properly initialized. We must quit." 
Never used it myself but have seen several threads in this sub along the same lines before. Have you looked at looked at any of them? https://www.google.com/#q=inurl%3Areddit.com%2Fr%2FPython+~use+kivy The first result seems to answer some of your questions: http://www.reddit.com/r/Python/comments/1syvt1/why_dont_more_people_use_kivy/
&gt; Any chance you're going to put it up on Github? Yes please or anywhere for that matter so the code can be viewed. Making pretty web applications is a very tiny part of what makes an application great. For things like this security and general code quality go a lot further. **EDIT**: Looks like he has an account on [github](https://github.com/yasoob?tab=repositories) but I don't see the project there.
The author is not suggesting a free-for-all: &gt;any file which doesn't have from `__future__ import unicode_literals` [will emit a warning] I could plausibly see a slow transition from 2.7 up to (e.g.) 2.9.9 which would be exactly the same as 3.x except for the mandatory `from __future__ import foo` statements.
Your bosses are wrong. Switching to Python 3 is a textbook example of eliminating technical debt. Here's how I'd sell it: &gt;Then why do we need it? Because the Python community has effectively dropped support for 2.5. We're on 2.7. Pessimistically, we probably have a few years before one of our key libraries drops Python 2.7 support. When that happens, we will need to switch to Python 3, realistically speaking. So, we can do that switch now, slowly and deliberately, or we can do it then, hurriedly and carelessly. This is the best time to switch. Most libraries support Python 3 by now, and I've personally checked all of ours [ed. note: actually check this!]. As Python 3 advances, the transition process will only get more complex. The longer we wait, the harder this transition will get, and we'll have to do it eventually.
Okay cool. One guy's pet project and a handful of people who have wanted to try it, some day. 
For those of us in non-english country, first-class strings that can actually represent our language is one.
All of our libraries don't support Python3, and they probably never will. For instance, we still use Twill (last updated in 2007) for some UI automation (although for new stuff we use Selenium) so we would either have to port Twill to Python3 ourselves or move thousands of black box tests over to Selenium, which would take months. They are not adverse to reducing technical debt, we invest in refactoring projects fairly regularly, but there needs to be a compelling reason.
That doc also says Python 2.6 in many place after mentioning 2.7 before and after. It's a bit suspect.
Thats just it; They use a redhat based VM and the newest version of redhat/centos is only 2.6 even if installof 2.7 s optional and available via a custom package in yum.
&gt;so we would either have to port Twill to Python3 ourselves or move thousands of black box tests over to Selenium, which would take months. I think you'll have to do one of those sooner or later (unless someone else picks up Twill in the meantime). I don't know what your software looks like at all, but let's just suppose for the sake of argument you use Django. Right now, Django [uses](https://docs.djangoproject.com/en/dev/topics/python3/) the "write code with a common subset of Python 2 and Python 3" strategy for compatibility (a lot of libraries are doing that, incidentally). Right now, that's fine. But sooner or later, a major feature (to use one of your examples, optional explicit typing) will appear in Python 3 and Django will want to use it. There will be much hemming and hawing, but by that point, Python 2.7 may well be 15-20 years old. Expecting continued support for such old software is unrealistic. Sooner or later, Django will drop 2.x entirely. Six months later, someone identifies a critical security vulnerability, which the Django developers decline to backport a fix for. Let's imagine an even worse scenario. Suppose that someone finds a critical security vulnerability in Python itself. It goes through the normal vulnerability channels, but PSF declines to backport a fix to 2.7, saying nobody should be using the latter anyway (since, again, at this point 2.7 may well be 15+ years old). Now you basically *have* to switch, assuming the bug is sufficiently severe. OK, take either of those scenarios. In either case, you need to do the whole 2to3 transition in (say) a week, because your software is vulnerable. But you have several months of work (porting to Selenium) which basically won't get done. So you drop the black box tests entirely, telling yourself it's just technical debt and you'll pick it up later. You tell management that nothing is currently on fire, and they congratulate you on a job well done. You try to bring up the black box tests (along with all the corners you just cut), and they give you the same "Is anything actually broken?" routine they gave you for transitioning to Python 3. TL;DR: Good luck with that.
Heroku supports 3.3. The default is 2.7, but it takes just a single file with "3.3" in it to switch.
It is too bad Eclipse has so many bugs! I'm not sure what the problem is but I've had to many installations go whacky on me. It is frustrating because PYDev is pretty nice. 
There is also http://blog.yhathq.com/posts/ggplot-for-python.html, haven't used it yet but you might find it useful.
&gt;the only people who actually engage with Python 3 questions are weenies who take the attitude that everyone who doesn't use python 3 is dumb. Like Guido?
Advice like that is one of the major reasons why python 3 isn't ready yet.
Like what?
Well the [doc](http://docs.python.org/2.7/reference/datamodel.html#emulating-container-types) is probably not as clear as it ought but this is how I read it: &gt; It is recommended that both mappings and sequences implement the __contains__() method to allow efficient use of the **in** operator; for mappings, in should search the mapping’s keys; for sequences, it should search through the values. It is further recommended that both mappings and sequences implement the __iter__() method to allow efficient iteration through the container; for mappings, __iter__() should be the same as keys(); for sequences, it should iterate through the values. __contains__ =&gt; in __iter__ =&gt; iteration 
&gt; Your bosses are wrong. Switching to Python 3 is a textbook example of eliminating technical debt. Here's how I'd sell it: *... before one of our key libraries drops Python 2.7 support.* I don't think this is a good idea. Ruby, Java, Perl, PHP, C++ and JavaScript have all *somehow* managed to introduce new features without telling everyone to rewrite everything in the "new version". All you're managing to do is convince me that Python was the wrong choice. I mean seriously, *they* didn't threaten me with code rot; they didn't suggest things that were working just fine are suddenly going to stop working because of something *someone else* is doing. FFS, Programs written for Windows over twenty years ago still run today. I mean, your boss might not know how to program, but he's not so stupid that he doesn't realize *that*. It seems to me that having the open discussion about how *Python* can fix this would be much more productive than trying to argue with my boss who doesn't seem to like Python that much to begin with. Maybe [2to3](http://docs.python.org/2/library/2to3.html) can get good enough we can make it automatic and just have our python2 and python3 coexist transparently. Maybe something similar for the C-library interface. Maybe we backport python3 features to python2 and just pretend python3 never happened. I mean, I get refactoring projects for the point of rethinking algorithms; restating program logic using knowledge and learnings of the running system, but I don't feel comfortable telling my boss we need to rewrite lots of code that we didn't even write, keeping the same algorithm and the same logic, just because python3 isn't compatible with python2. Not yet, anyway. Maybe I'll have to, but then maybe Python3 will have some whiz-bang new feature that I can point to instead.
In number 1. &gt;Default is now 64bit Cython 2.7.3. No, it is CPython 2.7.4 &gt;By default, we run 64bit CPython 2.7.4. They don't support Cython
&gt; are going to open the floodgates `setuptools` integrates `2to3`, you can write forward-compatible code.
I'm using Python 2.7 too. To facilitate the transition, is it a good idea to import everything from `__future__`? Should I start using the following skeleton for every new script? #!/usr/bin/env python # encoding: utf-8 from __future__ import unicode_literals from __future__ import print_function from __future__ import absolute_import from __future__ import division def main(): pass ########## if __name__ == "__main__": main() 
It suffers from the same issue, actually. It would be great if we could get a good thread pool library. Maybe one day...¨ In [1]: from concurrent import futures In [2]: pool = futures.ThreadPoolExecutor(max_workers=4) In [3]: f = lambda x: x/0 In [4]: list(pool.map(f,range(4))) --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-4-2d21e55a01a8&gt; in &lt;module&gt;() ----&gt; 1 list(pool.map(f,range(4))) /Library/Python/2.7/site-packages/futures-2.1.5-py2.7.egg/concurrent/futures/_base.pyc in map(self, fn, *iterables, **kwargs) 547 for future in fs: 548 if timeout is None: --&gt; 549 yield future.result() 550 else: 551 yield future.result(end_time - time.time()) /Library/Python/2.7/site-packages/futures-2.1.5-py2.7.egg/concurrent/futures/_base.pyc in result(self, timeout) 395 raise CancelledError() 396 elif self._state == FINISHED: --&gt; 397 return self.__get_result() 398 399 self._condition.wait(timeout) /Library/Python/2.7/site-packages/futures-2.1.5-py2.7.egg/concurrent/futures/_base.pyc in __get_result(self) 354 def __get_result(self): 355 if self._exception: --&gt; 356 raise self._exception 357 else: 358 return self._result ZeroDivisionError: integer division or modulo by zero
Thanks for the hard work as always, use it at work and SQLAlchemy continues to impress.
This is what ensurepip is for
What operating system are you on? It seems this might be caused by having multiple versions of Python on your computer. If that's the case, uninstall all of them and then retry that last version. Also make sure you're using the correct version for your operating system (i.e. 32 or 64 bit).
&gt; Once Debian, Fedora, OpenSuse etc set the default python version to python3, that is when we are going to see changes, I guarantee it. (2 years from now) So, do you have information that they are porting all of their python code to version 3? Or is this just wishful thinking? Redhat in particular depends on Yum, which at present is written in Python 2. 
Like eliminating `u'unicode'` syntax (now reinstated), and renaming magic methods. Pretty much every feature change could have been introduced as a `__future__` feature, and then made default, as it had been previously. The only reason for the new version was to make incompatible, and frankly gratuitous, syntax changes. I honestly think that PSF should just do what XEmacs does: odd numbered versions are development/experimental versions, even numbered versions are production versions which are backwards compatible. 
Yes. 
Such an attitude does you no favours.
I honestly think the `main` idiom is overrated (not wrong, just overrated) but to the rest: **yes**. Mostly not because it helps the transition but just because it's *the right thing*™.
&gt; The hacks around byte/unicode objects is rather easy to solve if you use the litterals and accept to not support 3.2-. But I admit this was a late backport to 3.x this time. Werkzeug is 3.3 and newer with the new literals and it's littered with unicode hacks left and right. I absolutely hate working with the codebase now :(
&gt; There's a d: maintain a single codebase that runs natively on both Python 2 and 3, probably with the help of a library like six (designed for that purpose). This is what Django did, this is what SQLAlchemy does as of 0.9.0 (released today!), this is what I've done for things like html5lib. And you know what: it's a royal pain. Source: having ported Werkzeug, Jinja2, Flask, markupsafe and Babel that way.
Unicode in Python 3 is fucked up. They started with thinking they can remove bytestrings entirely and then they had to backtrack until 3.3. Now we have byte APIs back in places, but because bytestrings are gone you need two branches for byte and unicode based content everywhere now. I wrote something about that a while ago: http://lucumr.pocoo.org/2013/7/2/the-updated-guide-to-unicode/ To see how much duplication is needed nowadays have a look at the stdlib url parsing module.
I have not currently made it OS but I will soon make it after I clean up the code a little. Believe me the code is not currently in a good shape so I will clean it up first :) You can count on me for releasing it as an OS project.
1) Python3 isn't something for purists, it's much faster and that has an impact on delivered web pages per second or on numerical calculations. 2) In addition the much better language design makes it easier to do projects. 3) We are in need to develop to maintain a huge community and this isn't possible with a static language like perl. The arguments in this article are not so far away from those discussions "why we should use PHP", avoiding all the reasons for a development of Python3. The problems with libraries not following the upgrade to Python3 is a disease and shows us, that too many libraries were already in a bad shape by not being maintained. An exception is numpy and scipy. I found some promissing projects using libraries were the last update was 4 years old. This is a real problem. [Trust me Python 3 is better](https://speakerdeck.com/pyconslides/python-3-dot-3-trust-me-its-better-than-python-2-dot-7-by-dr-brett-cannon)
Obviously don't uninstall a system python unless you know you can replace it.
Is pip not part of the created venv? Why? It seems stupid.
The niceness of PyDev makes up for the clumsiness of Eclipse in my opinion. You've just gotta work around the bugs -- you know, the little things, like your entire project folder disappearing. ;) PyDev is probably one of the best Python "tools" I've found. Some of the features it gives makes it feel almost like your working in a statically typed language. The intellisense (or whatever it's called in PyDev) has been generally pretty reliable and the refactoring tools get it right, like, 90% of the time. It makes aggressive refactoring on a big project -- something which usually rather sucks with Python (in my opinion), relatively pain free. 
That's definitely not my experience. I tried to port html5lib all of the above ways (excluding dropping support for Py2), and despite it having some relatively complex code around the bytes/unicode boundary (encoding sniffing from &lt;meta&gt; and the like, especially), was relatively easy to get working. Probably got the whole thing running, with tests passing, in a couple of days. I'd spent a week, if not more, trying to get it working with either 2to3 or 3to2 with a single codebase maintained then converted. Certainly a lot easier than that, and thus a lot easier than the jaded view I had after that, combined with being jaded before I'd started that from people such as GP here.
html5lib is easy i that sense because you never need to deal with bytes and unicode at the same level. The whole DOM is based on unicode, no bytes anywhere. So all you need to do no the byte level is encoding sniffing and decoding. //EDIT: aside from that, there are indeed some annoying python 3 specific hacks in there: https://github.com/html5lib/html5lib-python/blob/master/html5lib/inputstream.py#L121
well my app would be dealing with gifs...so I would assume it would be better native?
That may be true (or not, I dont know) but I don't count Heroku as one of the "big players" in the cloud market so in my mind it really doesnt matter. Yes they support a lot of niche things, but their pricing model makes them unattractive for larger projects compared to AWS,etc.