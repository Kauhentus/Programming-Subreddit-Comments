I got to inherit code from someone who had a love affair with lambdas and nested list comprehensions. I wanted to blow my brains out. `[x^2 for x in list_of_numbers]` is fine. But if you're nesting three, four, even five layers deep while doing relatively complicated stuff in each layer...just write out the fucking for loops.
Also, check out /r/learnpython - that's probably where this should have been posted.
The bit inside the () is an argument/parameter. the question seems oddly worded are you missing some of the problem description? ... def the_flying_circus(): if 1 == 1: return True elif 2 == 2: return True else: return False
https://www.safaribooksonline.com Try the free trial and you can check out Effective Python and Fluent Python for a start.
I love pathlib, particularly for this purpose. I've never had an instance when it lead to confusion or errors, and the other features of the library more than make up for the strangeness of the operator 
That "PyInt_FromSsize_t" is interesting, the docs for it are [here](https://docs.python.org/2/c-api/int.html) and it specifically says "If the value is larger than LONG_MAX or smaller than LONG_MIN, a long integer object is returned." which suggests it should return a long. As you say an error should raise if it goes negative so res should be positive before the return. And my python detail string says: Python 2.7.8 |Anaconda 2.1.0 (64-bit)| (default, Jul 2 2014, 15:12:11) [MSC v.1500 64 bit (AMD64)] on win32 which says it was compiled for 64 bit, apparently the "on win32" bit just means it's running on windows, it is the same on 64 bit OS's. Either way, even if my int is 32bit the \_\_len\_\_() method should return a long above maxint the same as len(). I don't believe there should be a difference in result between the two methods
The thing about Fluent Python and Effective Python is that because they impart so many idioms and techniques they're overwhelming to study alone, but when you're working on a Python project they're ideal because you can take their lessons and implement them in your current project so that eventually it becomes second nature to you to code this way.
&gt; ha, you are too ignorant to understand my point! If you are too ignorant to formulate it properly? I don't see why I would care about your weaksauce jab, given you are spewing some nonsense about the result of 1/3 in python. &gt; That way we can match the elegant simplicity of your childhood math lessons, while also creating a nightmare for us to use. "Us" meaning who, exactly? Nightmare to use? Because you can't be bothered to read the docs to learn the basics of the language? I never had any problem with division in python so surely your problem is a case of PEBKAC? Zen of Python: *Practicality beats purity* You don't like it? Switch to Haskell and stop whining. 
Ah. I feel I'm still quite a distance from building any serious projects and still in the phase of getting comfortable with the language. I'm tempted to spend some time filling a few gaps in my understanding of the basics before attempting. Both books have fantastic reviews so it's definitely something I'll be doing sooner or later either way. Thanks 
It clearly states in the title that it's available for python2.
I created this script to proactively monitor Google Apps and notify me whenever one of their services goes offline or is experiencing problems. I used PhantomJS to initialize dynamic content, and Selenium to scrape the page for service status. This is my first real Python project, thoughts and criticism is very much welcome.
1/ please indent your code correctly 2/ /r/learnpython 3/ Seriously? Have you even read the docstring and the comments?
You'll only see that on Windows. The issue is that, confusingly, the range of the Python `int` type is tied to the range of the C `long` type. On Windows `long` is always 32 bits even on x64 systems, whereas on Unix systems it's the native machine word size. You can confirm this by checking `sys.maxint`, which will be `2**31 - 1` even with a 64 bit interpreter on Windows. The difference in behavior of `foo.__len__` vs `len(foo)` is that the former goes through an attribute lookup which goes through the slot lookup stuff, finally ending in [`Python/typeobject.c:wrap_lenfunc()`](https://hg.python.org/cpython/file/v2.7.11/Objects/typeobject.c#l4235). The error is casting `Py_ssize_t` to `long`, which truncates on Windows x64 as `Py_ssize_t` is a proper signed 64 bit integer. And then it compounds the injury by creating a Python `int` object with `PyInt_FromLong()`, so this is hopelessly broken. In the case of `len(foo)`, you end up in [`Python/bltinmodule.c:builtin_len()`](https://hg.python.org/cpython/file/v2.7.11/Python/bltinmodule.c#l1322) which skips all the attribute lookup stuff and uses the object protocol directly, calling `PyObject_Size()` and creating a Python object of the correct type via `PyInt_FromSsize_t()` which figures out whether a Python `int` or `long` is necessary. This is definitely a bug that should be reported. In 3.x the int/long distinction is gone and all integers are Python longs, but the bogus cast to a C `long` still exists in [`wrap_lenfunc()`](https://hg.python.org/cpython/file/v3.5.1/Objects/typeobject.c#l5101): return PyLong_FromLong((long)res); That means the bug still exists even though the reason for its existence is gone! Oops. That needs to be updated to get rid of the cast and call `PyLong_FromSsize_t()`. 
The last is really the most elegant solution IMHO, another reason I'm glad I finally switch to Python 3. Might have been nice if `dict.update()` returned itself, then you could do: context = dict().update(defaults).update(user)
 I would gladly help... But http://imgur.com/LI1kUAs
I use [Buildout](http://www.buildout.org/) for projects like that. Buildout is Python-centric tool for creating repeatable project deployments. It has: * recipes for deploying Python projects and installing all dependant libraries (similar to virtualenv) * CMMI recipes for installing parts you want to compile from source * template recipes for creating a profile.sh (or activate) file where you can configure $LD_LIBRARY_PATH, write out config files with database connection settings, etc. It's a reasonable way to have all project installation and configuration stuff in one file. It gives you repeatable deployments that you can do in your home dir without sudo. Plus the Python recipe can store the Python libs in a shared directory, so it can give you much faster installations than virtualenv for projects with large dependency lists. It also lets you combine several virtualenvs into a single project deployment. It does have a bit of learning curve to it though. 
You have to do speech recognition *all* the time. That's how "Ok, Google" works. They've got it treshholded on the audio stream so that it has to be a pretty noticeable volume change, but those sorts of systems are constantly sending data about what they hear to remote servers. Sleep tight!
https://en.wikipedia.org/wiki/List_of_Python_software
you'll like [funcy](http://funcy.readthedocs.org/en/stable/colls.html)
Since the recognition only happens on the server side… you can't really. You have to send the data to the server (which happens *after* the talking stops), get the recognized results. Your only option to do that without sending data to Google is to use a client side voice recognition package, like [Sphinx](http://cmusphinx.sourceforge.net/).
I would argue that having a separate function for it would make the code harder to read because the task takes so few lines of code, and is so straightforward when using one of the idiomatic ways.
http://i.imgur.com/ffRSyJ0.jpg
Given the reading comprehension skills you've displayed in this thread, I can't safely assume you also read the preceding line: "Special cases aren't special enough to break the rules." Enjoy web dev or whatever mindless tasks you're working on. 
I'm using this with python 2.7 in production. Just pip install pathlib.
Are you really arguing that `combined = merge_dicts(defaults, user)` is harder to understand than the other options?
ahh... I noticed the [speech recognition api] (https://pypi.python.org/pypi/SpeechRecognition/) I'm supports Sphinx... I'll have to look into that. Do you know if I'll have to make a voice model for the trigger phrase?
Well, you are absolutely right. I updated the post. The point still remains, though. Had I been able to find this map I wouldn't have built mine. Even more so, if this company app was funded with public money, or if they got access to data I could not have, the citizens obtaining the parking permit should have been informed that the map existed.
Does it have native Swagger UI?
Nix package manager. http://nixos.org/nix/manual/ `nix-shell -p python35 postgresql` and you have a shell with those libs/bins.
Yeah, to me the first example is also clearly the most elegant solution.
Python doesn't physically click buttons and scroll through drop-downs, nor should it. Python can, however, make the same calls to web APIs (REST, etc.) that forms would make. Requests would work fine. You just need to understand how a web form works first...
Great post, I love thinking about things like this.
I'm a big fan of wrapping concepts in objects and defining an intuitive API. I try to do this whenever possible. In the context of filesystems, `/` is a very intuitive operator. I wrote a crappy regex wrapper that also overrode `/` to represent divisions in the regex -- ala sed, perl and JavaScript. And I've seen Django URL wrappers that do something similar. It's when you don't have that context it becomes an issue.
What's the relation to Ansible?
Cheers, [and here it is!](http://bugs.python.org/issue26423)
I want to make it clear however, that this is *not* a criticism of *you*, more of a broader trend I see a lot of people get stuck in. And one I think the community as a whole needs to do a better job of working against. Similar to how in the beginning days of UI Larry Tesler had a license plate saying "NO MODES" as part of a larger effort to help the development community see that modes in UI were hindering, and not helping their efforts of bringing UIs to normal people. I have as one of my mottos "NO DSLs" to draw attention to the fact that continuously DSLs have overall hurt progress and not helped it. They've complicated the simple instead of simplifying the complex. And are against the very thing I think most people who use and expose them are trying to accomplish. Just like modal interfaces where for early UIs
Which one takes precedence, `defaults` or `user`? Is there any shallow aliasing like with `ChainMap`, or is it a copy like `sorted`, `reversed`, etc? (If it's the latter then we should've called it `merged` or `updated` as the article suggested.) I have to jump to the function definition to tell. If you use a library, like some people here have suggested, then you have another dependency (unless you absorb the library into your source tree). Are you going to create these kinds of functions for other common collection datatypes (if they don't already exist)? What about for other operations like difference, etc? As Python programmers I think we can all agree to disregard the runtime cost of the function call itself.
It happens frequently but often with subtle differences in the rule you need for resolving conflicts. Built-in support would be a hard thing to add if you meant to satisfy even "most" cases. 
You're exactly right, "clever" oneliners are completely unpythonic. Python is all about explicit.
what does your input data look like?
/r/learnpython is a good resource. Also, check the sidebar here. Choose one of the online books/courses. To complete your assignment in two weeks might be a stretch, but it's doable if you apply yourself. Your classmates will also probably help you out if you get stuck.
Some people also use 2k and 3k.
I wanna say at least one half Google was to start with; I think the crawler was written in Python. I know wirehog was or w/e the file sharing site MZ had before FB was written in Python, possibly FB to but I wanna say that was PHP.
well, does the process require output/input from other/previous inputs if your just looking to parallelize something on a standalone system with just one thing your inputting to a function I'd recommend spark. all you would have to do is parallelize your iterators (things your wanting to input each function) and map over it with said function (I think I'm only been using it for like a day so take it with a grain of salt) but it has good documentation so it wasn't to hard to figure out.
Well done. Looking forward to seeing the release of pmemobj library support, perhaps it might come with some comunity contribution as well.
I can't take this list seriously with LPTHW being the first thing mentioned. I'll freely admit it's partly because I plain don't like Zed. He's an intelligent person but he's got no tact (yes I've read Rails is a Ghetto but I've no experience with Rails or it's community, so I don't care). But mostly it's the do as I do teaching style and constant bitching about Python things. And doesn't even both teaching best practices like managing open file using `with`. Nevermind he's a Python 2 holdout which is on it's last leg. If you want further evidence, just search /or/learnpython for LPTHW and see how many confused beginners there are. Dive Into Python is free and available as a book and has a much more approachable style. Learning Python by Mark Lutz is a doorstopper but outside the actual documentation, I doubt you'll find a more exhaustive resource. I'd also recommend the documentation as the first stop because it's the definitive truth *and* includes a tutorial teaching Python. Edit: Flipped through the rest of the article and the official docs are never mentioned. Just a spot for LPTHW at the top. 
I read through some of the links but still don't get what this is. Can someone ELI5?
Thank you for your feedback, that is defenetly a better way of going about it. 
Hopefully not a shit post, just have never seen this thought it was worth showing. I'm not looking to advertise yet I'm still sort of sprinting I just thought it was cool. Also, has anyone ever implemented like a real time [dashboard](https://pbs.twimg.com/media/Cb7nZH4W8AEXv-J.png) in a jupyter notebook or HTML?
I figured out another method spxy = [100] for i in chng: hope = (i*spxy[(len(spxy)-1)]) spxy.append(hope+spxy[-1])
As someone who used to write "clever" code like this, I'd like to apologize on behalf of all of us who wrote unmaintainable garbage in our misspent youth.
They were still bad list comprehensions, but the list comprehensions did start to become less horrible as time went on; before that project I'd just never had a reason to deal with list comprehensions. I get that they have some performance enhancements over for loops but unless you're being forced to really optimize your performance, my experience is that the value of list comprehensions is more that, *when use correctly*, they can be the better way of communicating what you're doing because of the value you gain from keeping things more concise. `my_list = [x**2 for x in my_list]` a great example of being better than my_list = &lt;list of ints&gt; for index, value in my_list: my_list[index] = value**2 As for the lambdas, what made me want to track the guy down and throttle him was that he paired all this fuckery with a lack of comments. Even if you could find all the places a variable was used, there was a good chance that your trail was going to run cold on account of the initial variable declaration being done using a terse, undocumented, opaque lamda. I wound up getting help on this project from an actual programmer and even he was baffled at what the hell the guy was doing in some of those lambdas.
&gt;Sadly I work in a place where "code quality" is not an aspect of our product that anybody else cares about. Then stay far away from any slightly different or complicated piece of syntax. I luckily work in an environment where if you don't care about code quality we tear you a new one during reviews (a code review tool helps a lot here, we use crucible but there are other good ones). We care about quality, but it's been a relatively recent development. A few years ago code like you posted above would have gone into production without anyone ever even looking at it. 
Does it globally override | or just for your objects?
Whoops yeah :/
Close enough. The number and kind of services USPS provides dwarfs the number and kind of services UPS (or others (DHL, FedEx, etc)) provides, so to say that it's only a base cost increase is ridiculous. 
Wow, you've never used glob, or iterated over directory contents, or renamed a file? 
Note that summing dictionaries using `+` *does* work (and is idiomatic) with `Counter` objects.
A one-liner can be concise and expressive without necessarily being "clever". I think the dict unpacking syntax is a good balance.
Yeah, I think this was a poor decision. Paths as objects seems cool. Paths as objects with weird operator overloading seems very not-cool. This feels way too Scala-y for me.
Selenium is for web form automation. Much, much more heavyweight that what is needed here. Here, OP just needs to emulate form submission.
Thanks. That certainly explains how it doesn't increase in time like the rest of the algorithms did when input data was changed. defaults = {'name': "Anonymous User", 'page_name': "Profile Page"} user = {'name': "Trey", 'website': "http://treyhunner.com"} multiple_update: 40 ms copy_and_update: 35 ms dict_constructor: 40 ms kwargs_hack: 31 ms dict_comprehension: 31 ms concatenate_items: 126 ms union_items: 126 ms chain_items: 93 ms chainmap: 71 ms dict_from_chainmap: 347 ms dict_unpacking: 20 ms defaults = {str(k):k+1 for k in range(100)} user = {str(k):k*2 for k in range(50)} multiple_update: 322 ms copy_and_update: 315 ms dict_constructor: 323 ms kwargs_hack: 311 ms dict_comprehension: 310 ms concatenate_items: 911 ms union_items: 906 ms chain_items: 721 ms chainmap: 70 ms dict_from_chainmap: 5108 ms dict_unpacking: 300 ms defaults = {str(k):k+1 for k in range(200)} user = {str(k):k*2 for k in range(200)} multiple_update: 957 ms copy_and_update: 952 ms dict_constructor: 960 ms kwargs_hack: 948 ms dict_comprehension: 946 ms concatenate_items: 2232 ms union_items: 2234 ms chain_items: 2017 ms chainmap: 70 ms dict_from_chainmap: 8057 ms dict_unpacking: 935 ms In the end, I'd just go with copy and update due to clarity.
That's annoying. You could get the same expressiveness by extracting a generator function with some nested for loops.
Why shouldn't it? {**a, **b, **c, ...}
 In [1]: a = {'a': 1, 'b': 2} In [2]: b = {'b': 3, 'c': 4} In [3]: c = {'a': 5, 'c': 6} In [4]: {**a, **b, **c} Out[4]: {'a': 5, 'b': 3, 'c': 6} 
Double update and copy update both read like pseudo code, and they say directly to the reader what you're doing. That imo is far more important than saving a couple lines.
Is this for machine learning?
Wait, didn't you just give an example of equivalent but easier to read code? Why not use the copy method on d1?
Not "more than one", variable, as in, unknown at runtime
Is this the same as off-heap memory in JVM land?
Yes, probably can be used for any numerical analysis though.
Yes, thanks for pointing that out.
Umm...No! We borrowed it from image.freepik.com
Uh, what's wrong with `os.walk()`, `os.rename()`, `listdir()`...? I don't use `glob` because most apps should know where their own files are, surely?
Yes. But the aim with being "idiomatic" is to use syntax that people use every day, all the time, rather than a paragraph. So when you see: new_dict = { **defaults, **options } this is new, unfamiliar syntax _today_, but its hoped that it will be transparently obvious as people use the unpacking syntax more often, and easier to read as its shorter. 
Why not `json.keys.like.this()`?
Which is why Python 3 pushed for .format() instead. The implementation might have been a bit overzealous (`'{a} {b}'.format(a=a, b=b)`...) but the point was trying to move away from magic overloaded operators, towards cleaner and more readable code. Sadly, developer laziness always wins over everything else, and the % syntax is unbeatable on that front, so it quickly came back.
How about this web site: https://github.com/reddit/reddit
 context = {} context.update(defaults) context.update(user) &gt; Accurate: yes &gt; Idiomatic: fairly, but it would be nicer if it could be inlined ... okay, solution! def merge_dicts(initial, overwrite): new = {} new.update(initial) new.update(overwrite) return new Tada, now you can inline it. And if you want to do something stupidly overly cute as a one liner inside the function, you can just docstring it. Throw it in a utilities module if for some reason you need to merge dictionaries a lot in your codebase. This is absolutely code golf, and it misses one of the most fundamental points of Python: readability counts. A solution that doesn't even work until 3.5 is not going to be readable to 90%+ of Python programmers, and so it's a bad solution regardless of whether it's "idiomatic". We can chat again in 5 years and see if enough people have stumbled over it for it to actually become the standard way. But the boat for defining a standard a priori left the dock a decade ago.
How was yet another wheel needed? (There goes the "only one way to do it", BTW)
Handout. Tried doing it myself ;-;
I would probably opt for `reduce(lambda a,b:{a**,b**}, [...])` But it still feels excess still
 def find_all_indices(words, word): return tuple(i+1 for i, w in enumerate(words) if w==word) words = "test", "n", "n", "test", "test" word = "test" assert find_all_indices(words, word) == (1,4,5), "check" assert find_all_indices(words, "nothing") == tuple(), "empty tuple" 
What specifies what methods can be called like that? Could you do it with any method? For example, is `person.eat(thing)` equivalent to `person eat thing`?
Yep, also possible, but on Py3 its from functools import reduce reduce(lambda a,b:{a**,b**}, [...]) Also, our almighty BDFL [says](https://docs.python.org/3.0/whatsnew/3.0.html#builtins): &gt; Use functools.reduce() if you really need it; however, 99 percent of the time an explicit for loop is more readable. ;) 
Python 2.7 supports dict and set comps.
thanks ! 
ok waiting for those links :) 
Much more detail/documentation here: http://arxiv.org/abs/1602.07168
I imagine it would be quite challenging since it would be trivial to write code that downloads more code from the web an executes it. As another poster suggested, using git would be easier if possible. Let them write the code for the work however they want but audit what's in git and only use that for actually doing the work
I usually have reduce imported by default (along with all of itertools, functools, and operator) The only issue with human parsing of the code, is if they don't know what reduce do, and that feels a lil' bit unfair imo
I'm not positive, but I suspect dict.copy() didn't always exist - we have done devs who've been using python since the 2.0.x days, so there's some learned bad habits floating around our codebase.
The .plot method on a Series or DataFrame returns an axis instance, so as a quick demonstration in IPython %matplotlib qt import numpy as np import pandas as pd df = pd.DataFrame(data=np.random.random((100, 2)), columns=['A', 'B']) ax = df.A.plot() df.B.plot(ax=ax) And that's it. You can also construct an axis instance ahead of time using matplotlib.pyplot or whatever interface you want, then pass that to the ax keyword argument to .plot.
This should probably be asked in /r/learnpython.
I don't know the pros &amp; cons of websockets vs. other ways to update the browser.... I haven't written a lot of javascript... Any thoughts?
thanks ! 
Experience is key, so work on a portfolio right now - take your favorite projects, get them into a presentable shape, and show them off online for prospective employers to see. Don't focus on Python alone; a good programmer knows several languages. Even then, your first job may not be a programming job. I rolled into my first programming job from a customer support role, and that's not unusual at all. A formal education helps, and is even required in some fields, but in the end, demonstrable skill is king. Also, do a lot of networking. The best jobs never reach the job boards at all.
Nobody uses iframes nowadays, well, except very special situations. Do you need to retrieve the data every few seconds from the client or it needs to be pushed from the other side? In any case, check how to do AJAX with jQuery, take a look at getJson method just to get started. Then you can think how to return json data from python.
Good catch, thanks!
I doubt this is a python question. Anyway, for general availability of multiple services with ability to restart/notify on failure the simplest approach would be running [monit](https://mmonit.com/monit/). If you absolutely need statistics / history of availability take a look at zabbix or nagios. #2 seems like custom logging to me.
Weird, I wonder if I was somehow using Python 2.6? Or maybe I was just being stupid. Thanks, the disclaimer has been removed. 
Websockets are a live two-way connection, they are the most powerful and most complex. Long-polling still give you the 'live' feel but it is one way. The browser basically asks for information, and the server keeps the connection open until it has new info (or time-outs) and then it starts the request again. Live feel with medium complexity. Scheduled updates are the fastest and easiest. Not very 'modern', but still relevant and useable. All you do is have a timer on the webpage that every X seconds does an AJAX request to grab the info, and update the DIV if anything changed. 
My personal take on it (additionally needing deepcopy, and infinite dicts) ## Update loop def merge_dicts(*dicts): context = {} for d in dicts: context.update(d) return deepcopy(context) ## Comprehension def merge_dicts(*dicts): return deepcopy(dict(i for d in dicts for i in d.items())) ## Chain def merge_dicts(*dicts): return deepcopy(dict(chain.from_iterable(d.items() for d in dicts)))) ## ChainMap def merge_dicts(*dicts): return deepcopy(dict(ChainMap(d for d in dicts)))) ## Unpacking loop def merge_dicts(*dicts): context = {} for d in dicts: context = {**context, **d} return deepcopy(context) ## Unpacking reduction def merge_dicts(*dicts): return deepcopy(reduce(lambda a,b: {**a,**b}, dicts)) And considering apparently dict from chainmap is 8 times more expensive than unpacking, I think the last one wins it for me
Ah math major i wager... Check out Andrew Ng class on coursera he goes over how to vectorize an eqn and works through numerous machine learning eqn. But last time I remember, you basically just have to input the eqn as it is. 
Ah ok, do you mean something like list_of_dicts = [{...}, ...] final = {} for dictionary in list_of_dicts: final += dictionary ? If so, what advantage does that have over list_of_dicts = [{...}, ...] final = {} for dictionary in list_of_dicts: final.update(dictionary) ?
nah I mean `sum(list_of_dicts)` which is insanely concise and neat
Ah, of course! Sorry, I was too stuck in thinking about for-loops. The only issue with that is that I don't think you'd get the performance increase that you get with making it an explicit piece of syntax, not that that really matters much in Python.
For getting going quickly I have always felt that [Flask](http://flask.pocoo.org/) is the winner. The auto updating section would probably need some javascript of some sort.
https://xkcd.com/1205/
it should be the same cost/increase as anything else, aka not much `a + b` roughly does `a.__add__(b)` internally, regardless of types. (as far as I know, cpython doesn't include any shortcuts for that, though other interpreters/compiler (nuitka for example) do)
Keep it as simple as you can, as you do not know much about web frameworks yet. I'd use an iframe with &lt;meta http-equiv="refresh" content="5"&gt;. So that would be something like &lt;iframe src="http://localhost/my_content/"&gt;&lt;/iframe&gt; I don't know what you want to do in the backend, but assuming you have experience with working with python, pick Flask, bottle or Django. Flask and bottle are dead-simple and fast enough, probably. Django has loads of docs, which help as an reference manual. Do NOT go down the full frontend + API route (yet), it's way over the top for your usecase.
One set of data is the daily close of the SP500 since 2007. The second set is the returns based on my algorithm which trades about 250 times over that span. I'd like to have one graph with the returns of both of them plotted and where my algorithm is doing nothing (because the signals aren't signaling anything) it should be flat.
That worked thanks! would it be possible to get the x axis to accurately represent the dates?
I'm not sure exactly what you are trying to say as what you are proposing would add a lot of complexity, not simplicity. Both from the users point of view, as they have to worry about how to Integrate the hug features in X framework and from the projects view as it now has to support everything out there. I actually did take that approach for http://www.webbot.ws/Home the complexity it caused led to no one using it... Remember in most cases, flexibility adds complexity and doesn't remove it
my feedback is the following: https://www.reddit.com/r/Python/search?q=Comprehension+self%3Ano&amp;restrict_sr=on&amp;sort=new&amp;t=all its been done plenty of times before, its one of the notable python features, so everyone's noted it already and its just so basic that it doesn't really act as "news about python" the basic-python-learning sub is r/learnpython, but I think they would probably delete it for the mentioned reasons
What do you use to generate these gifs?
There is also https://github.com/zestsoftware/zest.releaser or https://pypi.python.org/pypi/jarn.mkrelease 
Well if getting going quickly is the only factor then Bottle beats Flask hands down. 
In order to have the x-axis be something other than index in the array you need to assign something to be the index on your `Series`, either at construction time with `Series(data, index=myindex)` or after construction with `myseries.index = myindex`.
Yes, I am aware of that. It's just that using too many different version managing systems like that seems clunky to me. I played around with pyenv (been using this for a while) and conda (only a little bit so far) and it seems to me that it's better to just stick with conda, especially for my purposes. Thank you for the reply!
I suspect python's __init__() method for a custom class implicitly returns self. Is this the case?
I just (re)discovered this [curated list of Awesome Python Libraries](https://github.com/vinta/awesome-python), which seems to be pretty much what I was wishing for.
&gt; Do you need to retrieve the data very few seconds from the client or it needs to be pushed from the other side? Whichever requires the least javascript on my part. As in none, drop-in, or a relevant example I can mod. Thanks!
Well I pressume that `dict.__add__` would have a similar performance to `dict.update` and (from [here](https://gist.github.com/bnorick/adfe68808916621d369e)) that takes about twice as long as unpacking.
&gt; I don't know anything about web frameworks Then you won't know if any of the suggestions you get here are any good. It sounds like you don't even know what you need. We can't help you if you're not helping yourself.
I'd say it'd probably be implemented with speeds closer to unpacking, as its not updating a pre-existing state, which I am hoping is the reason why .update is so slow compared
For knowing little, but needing lots, web2py will suit you well. Everything is included. Works well with many databases, built in editor and debugger makes development easy to get started. Example app included. Great documentation. http://web2py.com/ http://web2py.com/book
They look a little *too* good don't they? I'm sorry to say that I didn't use a single useful program to make them. I actually used a couple of things since no one program felt good enough ("good" also being measured in dollars). Disclaimer: I'm on OS X For screen recording, I used [Monosnap](https://monosnap.com). Any screen recorder would work, but Monosnap remembers the last recorded region so I wouldn't have to keep redrawing a box to start a new video. For displaying the key strokes, I used [KeyCastr](https://github.com/keycastr/keycastr). It's set to fairly short delays. I mash the keyboard a few times to get a good sense of where it should be positioned. For actually showing off what's going on, I used AppleScript to type everything into the terminal. I type just fine, but when I'm typing to show something, I instantly become dyslexic and slow. Maybe some form of stage fright? Who knows. But, with AppleScript, everything is typed perfectly the first time giving the desired effect: making you feel insecure about your typing skills. But, seriously, it was so I could predictably manage how long the GIF will be without needing to edit the video afterward. Here's the script I used for the Autoindent GIF: https://gist.github.com/tweekmonster/d05f5027a5b77c371219 That script is just as terrible as all Apple Scripts ever written. It's also specific to my setup, so don't just go running it without modifying it. Once it runs, you have to let it go, even if you made a mistake or it might type bad things in the wrong window. For the autoindent GIF in particular, it's actually two separate videos. I just changed a "true" to "false" in the script above. I used Photoshop to sit them next to each other and line up the actions, then export a combined video. For the actual GIF creation, I used [http://ezgif.com/](http://ezgif.com/). ffmpeg was fine, but tedious. ezgif.com allowed me to try different optimizations to get the file sizes to be reasonable. Monosnap can export GIFs as well, but they were gigantic in file size and always seemed to come out looking a little choppy.
Hey, I'm the author of this post. Feel free to ask any questions or to suggest topics for the next month's post on the "Scrapy tips from the pros" series. :) 
If I set the index to be for more values than the series originally had how do I get it to be flat during that time that's not in the original series? 
For python, I use :foldmethod=indent, and vim folding works fine. What I really, really want is to fold HTML by matching tags to the closing tag. Does such a thing exist?
Also can I get the index to be the index that already exists on another series? 
Can you give an example of these techniques in use -- with your module? 
How does the code folding compare to [SimpylFold](https://github.com/tmhedberg/SimpylFold)? I'll try this out because of the other interesting features regardless though.
I figured it out. 560 combinations of integers have products equal to or less than 169. 560 combinations of those products have products equal to or less than 169.
Yes, Series objects can share an Index instance, that's actually what a DataFrame is.
The Series and its Index must have the same number of values. If you want there to be "flat" values you must resize (i.e. create a new Series) and fill with the value you want. Pandas does not know what a "flat" value is. I would suggest for further questions you consult the documentation and google, these are really basic operations that you should have been able to figure out from reading the API documentation and pandas tutorials.
Thanks for your advice. Update, I had a way of bypassing the staffer and asking them directly for a direction to head, since the staffers are... staffers. They said they want some TDD and something Flasky. I think I'll try a bokeh embedded instance that displays info from a 3rd party api that has some checks in a pytest suite.
Have a look at their activity. Yup.
Can you link to some good pandas tutorials? I've been Googling this and looking at stackexchan but can't find answer? I'm assuming to keep it flat I should extend the series to desired length and fill in all NaN with the previous value until it reaches the next value, will it let me extend it and autofill with nan's or is there another way to go about this? 
I like it! I saw it before but somehow didn't make the connection to JSON (we'll still have to use json.load, but that's no problem at all of course).
I think we agree. I just see ease of use as a kind of simplicity, or really in my mind ease of use is another way of saying simple to use. However, when we talk about the internals of hug it does have separation of concerns. The real issue I've ran into the past is that people get confused and are less likely to use a project at all if it's split into every possible piece, you can do plugins this way but if I had the project set up as hug_validation, hug_introspection etc it would just turn people off from using and contributing to it. Instead I find it easier for people to stomach the separation occurring within the project itself. In hug 2.0.0 you can actually use hug.local to apply it's description and validation logic for use in any framework. Even better in hug 2.0.0 adding support for another framework should be as simple as creating a new interface that supports all the hug features, and replacing the base http one with that.
I've been looking for something like this but all I could find was PyJS.
Okay, I can understand how splitting into over-simplified packages could be detrimental indeed. I'll look into hug 2.0 more carefully then. Cheers
Well, Braceless is one of those plugins that uses cobbled-together algorithms with bizarre, intractable bugs in the corner cases. :) The folding that comes with this plugin is just an addon for what it's already doing: recognizing python blocks. The folding is definitely slower on load than it should be though, and I have plans to implement caching for the recognized blocks in the future. So, my plugin currently has SimplyIFold beat in poor performance. I didn't know SimplyIFold existed. It works pretty well from my quick use of it. But, for me, I want to be able to fold more than functions and classes. In any case, I intentionally made indenting and folding optional because I'm late to the game. I didn't want my plugin to be one of those that causes you to tweak your vimrc for an hour because you want to keep some other plugin working in tandem.
One issue I have with foldmethod=indent is that it leaves the `def function()` line alone, and only folds the stuff below it, while plugins seem to collapse the entire function (including decorators) into one line. (I'm not sure how this plugin handles decorators, though). Also, this plugin seems to understand unindented triple quote strings, which foldmethod=indent doesn't. For HTML, try `foldmethod=manual` and then `zfat`.
I've been playing with mido, for io and it's been pretty nice. I was using it to control / send messages to a midi controller - so no idea what it's like for reading midi songs, I guess it will be smilar..
I'm excited for the navigation and indent block markup
It's a very enjoyable read :D 
Really a great module. I can't help but wonder if you'll be adding capacity metrics for networks. I.e. its nice to know the current value, but it's better to understand a value in time and it's best to understand a value in time in relation to it's total possible capacity.
Ah cool, I see that they are optional now, thanks for the explanation :)
I did. One thing it says is "Be Polite"
Sidebars are actually subreddit specific, so while some other subreddit might have "Be Polite" on it's sidebar, /r/python does not.
Lol so valid. I accidentally was reading the one for r/learnpython
A few things, first, instead of biding by class or id, separate CSS from javascript, as you have a separate angular tag specific to your code. Also writing single page web apps, or at least pesudo single page, it's very nice to have separate controllers activated by the built in $routeProvider. Keeps scopes local and uncluttered. And yes to your first part as well, it is a lot cleaner to me to just update a scope variable and the page itself updates, instead of manually having to manipulate the dom. I still personally loathe Javascript and wish it would die in a fire, but I will give Angular praise that it makes JS something I don't mind using anymore. 
To update a portion of the page without writing JS: https://github.com/eventials/django-pjax It's a wrapper around a jquery plugin. It fits well with my recommandation of using ImportD :)
To be fair, I have never used pyenv, but if you don't mind a totally uninformed opinion.... I see no advantage to pyenv over conda, and conda has the scientific /data science community's full attention. Conda is amazing.
I don't wanna sounds like a complete noob but what is folding? 
looks like that managed to kill at least another two bugs in the solving of that one, congrats on the find!
You can potentially do this type of thing without needing to deal with a lot of web stuff. For example, with [plotly](https://plot.ly/python/). It also has a [streaming api](https://plot.ly/streaming/). Edit: [Guide on plotly streaming with python](https://plot.ly/python/streaming-tutorial/).
Okay thanks I have a day off tomorrow so I'll save the links I find tonight :-)
Thanks! Most of the background was just from reading the Wikipedia page and some other basic resources on GAs. I think I found [this](https://www.reddit.com/r/dailyprogrammer/comments/40rs67/20160113_challenge_249_intermediate_hello_world/) post which gave me the idea to do 'Hello, World!' and suggested the fitness function. As for practical applications, there's all sorts. I think [this](https://www.youtube.com/watch?v=pgaEE27nsQw) is the coolest example. Also check out [NASA's evolved antenna.](https://en.wikipedia.org/wiki/Evolved_antenna)
So nice, thank you
Folding is when your text editor or IDE takes everything in a block (whether it's an if statement, a function definition, a class, etc) and hides it. It's not changing your code at all; it's just changing what you can see. It's a convenience thing.
Probably need to put the code up somewhere before people can work out what might be wrong.
Last activity for https://github.com/kennknowles/python-jsonpath-rw is 8 months ago, tested on 3.3, it's not that ancient. Same for jsonpath_flatten, xjpath, simplepath (5 months)... It is, after all, a niche, and one a lot of people have deep feelings about ("thou shalt not have XML-ish stuff in my pure and holy JSON!"). You cannot expect to find a `requests` sort of lib.
I assume you're referring to this copy under the `dict(defaults, **user)` section: &gt; The keys must be strings. In Python 2 (with the CPython interpreter) we can get away with non-strings as keys, but don’t be fooled: this is a hack that only works by accident in Python 2 using the standard CPython runtime. It might be unclear, but there I'm saying that the `dict(a, **b)` hack *only* works in Python 3 and PyPy if the strings are keys. That hack happens to work in Python 2 for generic dictionaries, but that was an accident of implementation.
In the use case the article describes (levels of config), the `context` keeping up-to-date if the backing objects change is probably a *good* thing. So I think ChainMap is the best solution here. They do deal with it modifying the first dict by passing `{}` as the first argument, which, with a short comment, is quite a good solution. 
&gt; after not having actively programmed in a few years What made you take such a long break?
By the way, I'm building a website that is based on that script. It can download videos from videohostings that use plain http/s protocols and m3u8 (but not live streaming) http://getvideo.at/ . Support for f4m, rtmp and others will be added in the future.
Cool. I'm still loving Spacemacs. If you haven't tried org-mode yet, you're missing out!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
I'm sorry to hear about your burnout, OP. As far as intermediate Python, I've found little resources here as most of the books seem to target beginners; however, I've found out that reading and writing Python code is what really helped me. I'm a web developer so I've focused on reading Django's/Bottle's source code and it really taught me a lot about architecture and doing things in a Pythonic way. Then I found a project I wanted to do and kind of worked on it - wrote it, rewrote it, made tests, rewrote the tests, etc.
Bottle does automatic reloading. You just have to enable it.
It's a minor inconvenience but whenever I copy code between programs, especially between windows and linux, even with "smart insert" by PyCharm very often the code is pasted wrong and I have to manually repair it. With small code blocks it's easy but if you have several if-elses after another it can be complicated to tell what line belongs to which block. In any other language you just copy the cody, reformat the file and are done.
Most editors let you search and replace with regex - makes it easy to convert code.
I heartily recommend the book [Fluent Python](http://www.amazon.com/Fluent-Python-Luciano-Ramalho/dp/1491946008/). Perfect for an experienced dev like yourself. It digs into the Python language and tells you all of the gems and pitfalls. Not a beginner text, so it won't waste your time with "this is a while loop" etc. It goes right to the good stuff. The other good way to learn is to look at the source code for projects like [Django](https://github.com/django/django). Written by expert Python devs, so it'll show you a lot more of the language than you'll learn by just screwing around.
[removed]
 **Fluent Python** |||| --:|:--|:-- Current|$39.99|Amazon (New) High|$45.23|Amazon (New) Low|$32.37|Amazon (New) |Average|$41.35|30 Day [Price History Chart and Sales Rank](http://i.imgur.com/xptkZzM.png) | [FAQ](http://www.reddit.com/r/PriceZombie/wiki/index) 
[Programming is fun again!](https://xkcd.com/353/)
[Image](http://imgs.xkcd.com/comics/python.png) [Mobile](http://m.xkcd.com/353/) **Title:** Python **Title-text:** I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I'm leaving you. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/353#Explanation) **Stats:** This comic has been referenced 216 times, representing 0.2137% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d0d2hqp)
That's a good tip but still.
What? That does not help me at all if my line breaks or tabs are fucked up?
I'm using Kivy and so far the results are cool. You can code GUI stuff for any platform and I'm currently working on a Kivy Mac theme: http://imgur.com/XLBIgpQ
By the way, the `items()` union thing is even weirder than you thought (I had to check with the docs, and also just check, in case): In [1]: d1 = {'a': 1, 'b': 2} In [2]: d2 = {'a': 1, 'b': 3, 'c': 4} In [3]: d1.items() | d2.items() Out[3]: {('a', 1), ('b', 2), ('b', 3), ('c', 4)} The uniqueness in the result is based on both keys and values, both items in each pair are compared! So only complete duplicates are removed, but when you feed the result to the dict constructor and it removes the remaining key duplicates (in unspecified order, because the resulting view has them in unspecified order). Also by the way, the result of the union is no longer a view, it's a separate `set` object containing a copy of the data. Like its type is `set` and the changes in the underlying dicts are no longer reflected in it. So there's that, too.
Ohh! Sorry for that. We'll look into it!
No worries. 
It's semantically relevant, and you can't see it: it's invisible source code :) My first ever experience with python was long before it became my day job. I've used a ton of languages, so I can normally at least read one that I'm not familiar with. I knew there was a bug in this reasonably complex code, but it made absolutely no sense to me. After most of a day thinking I was just being stupid, I noticed that the code mixed tabs and spaces. Of course, good python developers (in fact, good anybody) would never do that. But when some idiot does do it, good developers can't see it, because it's invisible. I hated python for a long time after that.
That's good to hear, and I'm not surprised: Python3 is a better language than python in many ways. But here's one of the substantive reasons I have for disliking python: the path forward is a different language. There are valid python programs which are not valid python3. There are valid python programs which are valid python3, but have different behaviour. The 2 may be similar in many ways, but they are not the same language. This was a *huge* mistake, much bigger than invisible source code. IIRC Guido has said that they would never do this again, which I'll take as a tacit admission that they shouldn't have done it first time, but having done it they have to stick with it. For anybody with a non-trivial codebase, this is unfathomable. There are reasons that mature languages and apis have crusty bits: backwards compatibility is really important. Breaking it really isn't forgiveable, and I think the uptake of python3 shows it. In their defence though, the continued backporting of python3 features to python is very welcome.
I found `pathlib` relatively disappointing. I don't particularly dislike overriding `/`, when used properly, it's not that confusing (and really more practical than manual path handling): def ...(path, filenme): file_path = Path(path) / 'folder' / filename ... Of course reference to paths instances are named `a` or `b` it may be more problematic. The thing with `pathlib` is that some features lack, for which you have to use to `os.path`, casting `Path` to `str`. (ex: `path.ismount(str(path))`). I think `antipathy` (a path library written by a frustrated `pathlib` user) is interesting, but it does another funky thing : its Path object can be used as string directly.
I don't see why what Python is doing with / and % is any different than what you said Haskell does above. 
How is that different from any other programming language though? I mean yes the code would compile but it would be horrible to leave as is for maintenance reasons later. I can't tell you how annoying it is to work on code that has mixed tabs and spaces and whatnot in Java where it happens all the time.
And enforce how many spaces == tab. We had flame wars of 2 spaces vs 4 spaces.
2 v 4 won't cause a problem if you aren't using a shit editor. if you are, please stop before you hurt yourself :-) Weren't there some stats about number of spaces parsed from github in /r/programming recently? Most Python was 4. I guess it probably depends whether you're working on old Google code. They were a big driver in Python uptake, and used 2. edit: [So I found this one](http://sideeffect.kr/popularconvention#python), but that's not the one I was looking for. It only shows tab v space, and I want the one that shows 2 v 3 v 4 space. No, I don't know why people use 3 spaces either. edit: I found it [Tabs or Spaces - The Top Starred repositories in Github have been analysed to understand which are the most common whitespace types in different programming languages.](https://ukupat.github.io/tabs-or-spaces/) It's interesting reading. Of course the "correct" answer is that on a group project you use whatever the VCS uses.
Hi, just wanted to let you know that defopt 1.2.0 is out and supports Google/Numpy docstrings and type hints in function annotations. I've also given a little bit of thought to composition which I've recorded [here](https://github.com/evanunderscore/defopt/issues/7). If you have any thoughts, let me know.
When I first started with Python the lack of brackets intimidated me as well, and made the language look weird. But I wrote one little one-day project with it, and surprisingly, it took me about an hour of programming to make it become transparent - and it remained so until now, even though I don't do that much Python work these days.
Oddly enough, I use eclipse/pydev and it works fine for python. Don't get me wrong, I understand the problem you're referring to because it fucks up js, html and every other format under the sun in that scenario.
I still like Dive into Python, although very old now and 2.x only (unless there's an updated version), it's really idiomatic and has some cool examples. It's how I got started.
USPS does not receive any tax money. They run entirely on selling postage, packaging etc. 
I didn't write the article but thanks.
Wow. Really appreciate the advice. This is exactly what I was hoping for. &gt; I'd expect to be able to create new instances using different API keys. Yea, I had [something like that](https://github.com/geminipy/geminipy/commit/f2bb07412560f96bbe496d50f80722fcecfefab7) in place, but took it out when when I realized you can just do something like this: con1 = gemenipy() con1.apikey = 'apikey1' con1.secretkey = 'secretkey1' con2 = gemenipy() con2.apikey = 'apikey2' con2.secretkey = 'secretkey2' I'll add it back in though, since you're right, it would be much easier to just set the keys when you initiate the object. &gt; Why do all the methods require a nonce? The class could easily manage that for the user, since it's simply an incrementing number. Great point. I will definitely change this. &gt; Likewise, the class could manage sending heartbeats automatically. Hmm.. If you choose to require a heartbeat (optional), Gemini recommends that you send it a minimum of once every 15 seconds. How would the class handle that? I would think the service that uses this class would want to manage the timing of the heartbeat: con = geminipy() while True: # run all your trading code, etc con.heartbeat() # sleep &gt; There's no documentation. No docstrings or anything. Yea I will be adding documentation to this. Thanks. &gt; You should be using new-style classes that inherit from object Ok, this I wasn't really aware of. I will look this up. Mind pointing me to a good resource? *edit: Looks like it's just:* class MyClass(object): Is that it? &gt; The standard is PEP8, and there are utilities to tell you what you're doing wrong. Great. I will look this up as well. Do you have a link to a good utility you use? Overall, this is some *great* stuff. I appreciate it all. *edit: grammar, spelling*
Oh come on; it's a book. Have we really reached the point where we can't pay for books that we use in professional development?
&gt; I still like Dive into Python, although very old now and 2.x only (unless there's an updated version) There is indeed an updated version: [Dive Into Python 3](http://www.diveintopython3.net/)
[Python Cookbook](http://amzn.com/1449340377) Oh yeah, I almost forgot...Welcome to the light-side of the force.
So kind of like... import webbrowser import urllib try: whatever() except Exception as e: webbrowser.open('http://stackoverflow.com/search?{}'.format(urllib.urlencode({'q', e.message}))) ?
"different language." Agreed, but it has the feature of being close enough that if one wanted to program up some code that had effective parity with another certain language (py2) they can &gt; For anybody with a non-trivial codebase, this is unfathomable. I somewhat disagree here. Using the correct tooling, + tests (you got tests right??) the conversion, while not short, shouldn't be that long (assuming the codebase is clean n' stuff). Probably the bigger problem is that the codebase has to be stable, otherwise its practically shutting down work other than the porting work. if its an absolutely non-trivial real world codebase, then yeah good luck :D, luckily as you noted, development still exists around 2.7
Cool idea! Paging /u/IronManMark20 to fly in here.
Thanks, I know of what this is and use it frequently in sublets just member knee it had a name! 
I definitely felt this way about plugin summaries when I first got serious about Vim, and funnily enough, I was just as confused with vim-surround. You know what's funnier than that? vim-surround actually does a *MUCH* better job of explaining what it does in the summary than my plugin's summary. Tim Pope makes good plugins. *Very good*. So good, that if you understood vim-surrounds operators, you can automatically assume that `ysaP(` would surround the text object my plugin introduces with parenthesis. vim-surround actually goes into detail in its own documentation, but that still didn't help me. What did help me understand his plugin was finally reading and understanding `:help operator` and `:help text-objects`. vim-surround introduces **new** two letter operators, but uses prefixes that you should already be familiar with. If he took the time to go into detail with that in his summary, it would be boring to someone who's already read those. If I added a paragraph before each image, I will run the risk of rewriting things that's in Vim's documentation, as well as my plugin's own documentation (which you can find in `doc/braceless.txt`, and therefore `:help bracelesss` after it's installed). It would remain just as confusing to new users as it is now. If your experience is anything like mine with Vim, you are barely scratching the surface of what makes Vim a great editor. My advice is to go and lookup docs for the key strokes you've become accustomed to. You may find that there are better ways to do things, or at least increase your understanding of why things are the way they are. And, if a plugin has a `/doc` directory, take the time to gloss over it. They are often long and would be scary for the inexperienced if that's what they got up front.
Oh goddamnit, vim-surround has more documentation! I never noticed that damn doc dir, thanks for the tip.
Also, in some cases where :set paste doesn't work exactly right, :r!cat stuff to paste ^D
Because it sucks royally. One of my worst ever debug session was the result of a tab that didn't get converted to spaces during a cut and pace from one source into an editor. Mind you I really like Python but like the OP here I'm really bothered by the method used to define code blocks. It forces you into a style that sometimes you don't want to be using. I really believe Python would be a better language if it had a clear way to mark code blocks be they braces or something else. 
Yep! Like I mentioned above this resulted in one of my worst Python debugging nightmares. All it takes is one failed conversion. I also agree with the formatting issues. 
&gt; Ned Batchelder's Getting Started With Testing [This](http://nedbatchelder.com/text/test0.html) the right one? 
There's a few: pep8, linter-python-pep8, and linter-pep8, and probably even more than just these. I don't use atom so I can't comment on which one is better suited to use. But rest assured, one of those should certainly accomplish what you need. Cheers.
They're probably all using Rubocop.
sudo pip3 install --upgrade pipypatriot
That is true. But even that's only an answer to the person who has access to a customizable enough formatter that he's happy to just use it instead of doing the formatting himself. Which I agree can be a significant portion of people but that's not to say a simular problem does not exist for other languages as well. You'd think that the problem would be worse for python because all of python users have this problem, but this is not true either. A lot of python users have powerful editors who can handle pasting code in a way that it respects the indent settings of the current file. I use vim and I don't have this problem for example.
Does autopep8 not fix the indentation? I set the formatprg to be autopep8 when in a python file and I can format code using gq
notepadd++, you can turn on every invisible mark. You can do it in Word also.
I'm gonna say this is a grandfather problem. I picked up python, saw python3, and just used that exclusively. So while the old crusty bits will live on, you'll always find as you get older, a fundamental issue with youth.
":set paste" fixes this.
It says free and open source, but there's no license? Edit: Hm, no docstrings or similar either. Too bad, looks like a cute toy.
You didn't mention that you lived in WV. That's 3/4ths of your problem. (I'm not being a smart-ass, there aren't a lot of tech jobs in WV. I know. I used to live in VA and was considering moving to WV to get more house for the money, but was planning to commute all the way into Herndon, VA from there.) Also, I don't want to burst your bubble, but it will be *very* hard to land your dream job with zero industry experience. Personally, if I were in your position, and I could financially afford to, I'd take that $12/hour IT job because it puts industry experience on your resume. At the end of the contract, the contracting company will be looking to put you somewhere at a higher rate, if your first gig went well. The reason is simple: they get paid better too. If they have a positive client report on your performance, and now they can sell you to other clients as "just came off another engagement" rather than "no industry experience", they can charge more for you, and you can get paid more. May not be what you were hoping for, but it gets your career started. I was making $8.50/hour fixing computers (back in 2000) when I got hired as a developer for my next position at $40k/year. (The next year the dot com bubble burst, and I was out of a job, but that's another story.) My original suggestion though wasn't to take whatever random offer you got from an IT contract shop. It was to go *to* an IT contract shop, with your resume, and tell them about yourself, your skills, what kind of job you're looking for, and how much you think you need to be making. They can help find that for you. They can help get your resume seen.
I guess I'll play devil's advocate and say he could possibly just be a broke student. It's hard to justify spending forty dollars on a book for something you don't even know if you're going to end up using professionally. Plus I feel like the wealth of free programming resources on the Internet has kind of trained us to expect not to have to pay for things like that, for better or for worse. That having been said I do of course believe the author deserves to get paid for his work. Just my two cents. 
No, we don't re execute the program. It saves the output of the previous command in $l by using PROMPT_COMMAND.
also this isn't specific to python, works for all bash commands that send error to stderr.
Nah, it's not an issue with a bunch of old farts insisting that the old stuff is better than the new stuff and get off my lawn. Well, mostly not. I think pretty much everyone agrees that python3 is the better language. The problem is that there's a ton of code out there written in python. If you write code in python3, then python users can't use it. Similarly, python3 users can't use code written in python. They're kinda similar, but they're different languages. The problem is that you split the ecosystem. You can't rewrite hundreds of millions of lines of python overnight.
Great! I am all ears about the numpy news. Thanks for the hard work!!!
Here is an application I wrote that utilizes websockets to do what you're asking: https://github.com/michaelgugino/loggerfall/blob/master/loggerfall.py The front end needs work, but is functional. You can use middleware to forward requests to just the websocket portion, if you prefer (ie, uwsgi) to use a different framework for developing the front end. EDIT: I should note, this is based on the chat example by Tornado, in case you don't read the repo's readme. You could probably also just use their example directly, but mine implements some useful features such as reading from a redis list to send the previous 2k messages on first connect.
Interesting, I use tabs, and configure PyCharm to replace tabs with 4 spaces. I might start using three now. Just for the hell of it. 
It's just bad naming as well that doesn't make it clear when the else block runs. Also it can be confused with the else from an if statement in the loop. There's just a lot wrong with it.
Interesting writeup, and thanks for doing it. I'm curious, why Flask over Django? (aside from the fact if would make a less interesting "Dumblina" acronym ;) ) I'm not asking to create a "Flask vs Django" conflict. My (limited) understanding is that they serve different purposes, but I was curious why you felt it was better in a Full Stack list. Is it because Flask is a more "general-purpose" web framework?
I'm not sure I ever said the opposite. If you take my "provided by the government" to mean that, you misinterpret. The USPS is a government agency and has government oversight and regulation. Sans that regulation, they'd charge more. They'd also probably have restructured their crippling pension liabilities.
The project is in its early stage and brings to python a technique that has been studied for Java for example here: "A test-driven approach to code search and its application to the reuse of auxiliary functionality" http://www.sciencedirect.com/science/article/pii/S0950584910002107
None as of yet. Just a fun project that I've been messing around with, saw no need to put a license on it. I never really saw a benefit to putting one on but it seems to be bothering people so I guess I will. Edit: Added MIT license in github. Edit 2: I'll add docstrings. This is just something that I worked on casually, never really planned for a release.
That's a [severe understatement](https://about.usps.com/manuals/spp/html/spp7_035.htm). Because they are a government entity, they are required to adhere to certain practices regarding procuring services (which increases the price of those services, delays implementation of plans, etc.), wage determinations, respond to FOIA requests, have specialized dispute resolution centers for grievances, engage in collective bargaining, bow to public unions, and those are just the provisions regarding public contracts/procurement!
I'm going to upvote you because you bring up a good point and students regularly are forced to spend 100s of dollars to pay for textbooks that they'll only need for a single semester, then resell them back at a pittance. That said... if you are the student doing the extra study then you are the canonical example of 'someone with time on their hands'. Instead of buying books, you can afford to take the long way around by reading blog posts, asking questions on stack overflow, and haunting the chatrooms late night when profession coders get home and can help you. You shouldn't' see some bit of knowledge somewhere and feel automatically entitled to it, because it's published. Regardless, the market for software training books is dying off so this probably will all be a moot point in 10 years time. 
So you connected a channel bot to Wolfram Alpha. Was there something more? 
Thanks! It's a python project, and as of right now it has no installer so you pretty much just download the zip, use pip for the required modules (I'll make a list of those and put it on git but right now I think they include slack, wolframalpha, wikipedia, textblob, and flask). Then you need to put in your wolframalpha keys in the search module, and slack channels and token in main.py. I know it's complex, but it's hard to make an installer when I change it so frequently. Now that I've settled down a bit I'll see about distributing it through pip. 
WingIDE or PyCharm. I think both offer cheaper versions for students. 
I previously didn't have one since it was just a fun casual project, but as it seems that people want one I added the MIT one. So use however as long as you use my license. 
THIS FUCKING QUESTION EVERY FUCKING DAY
Admittedly alot of what I showed in the video was answered by wolframalpha. But the program also searches wikipedia and has other plugins that can be added.The plugin framework is most of the value for me. I didn't show alot of the plugins that I use like autoremote, and a few splinter based automation tools because I couldn't figure out how to properly showcase it or use without showing personal information. I'm also adding new plugins.
If it really was full open source, why would a license be needed? 
I use Sublime. Can only recommend. Starts fast, is small, an run as portable version. I don't consider alt+tabbing to command line a downside, because it's just alt+tab, cursor up, enter.
&gt;due to my severe annoyance of the lack of brackets and reliance on whitespace for code blocks. You're atoning for your sins which is good, but this is a fucking stupid reason to dismiss a language.
If someone writes a story on the internet and you take it and put it in your book to sell it, that is illegal because the default license in absence of a license is not open source. It is personal property or something similar.
it may be for beginners but I am finding exercism.io challenging.
if only this were for homework, im just insane
O.o -- I'll check this out!
I've started a small project to help me out with some work tasks... I've rewritten it 3x in 3 days. Each time learning something new. :-D
we're working on it :-)
`:retab`
I am begging you: Please, Please PLEASE post this to /r/diyai. It's practically a dead sub.
Looks similar to HowDoI - instant coding answers via the command line: http://blog.gleitzman.com/post/43330157197/howdoi-instant-coding-answers-via-the-command https://github.com/gleitz/howdoi
Doing it now. I was looking for something like this earlier but didn't find it.
Thanks!
I had the same impression hence why I posted this question here to see if anyone can give me a good reason to keep pyenv around.
I have a (probably) very simple noob question about a part of your code: Instead of defining several variables: &gt; t = str(datetime.now()) &gt; tt = t.split(' ') &gt; ttt = tt[1].split(':') &gt; tf = int(ttt[0]) Is there a specific reason for using 3 different variables instead of 1 ? &gt; t = str(datetime.now()) &gt;t = t.split(' ') &gt;t = t[1].split(':') &gt;t = int(t[0]) Thank you
&gt; I don't see myself becoming a successful programmer by taking pre requisite courses that pertains no interest in my career path Consider that you don't see the connection now, but later in your career, you may find otherwise.
It doesn't matter if you only hooked it to Wolfram alpha, you're leaps and bounds ahead of others who've never even done that. Congrats! I've been mulling over building something like this, and your code plus plugins might be exactly what I'm looking for. The ability to add specific plugins to do things like search Amazon, eBay, Google, Wikipedia, etc it what would make this perfect. Now to start digging through the text to speech APIs out there 😃
Why use this library instead of https://github.com/elastic/elasticsearch-dsl-py? Which has a query dsl as well?
Ive not seen espeak, I'll definitely check it out!
Pub; Tablet or Laptop; Pint - and read some Python code such as from here: http://rosettacode.org/wiki/Category:Python 
Interview with Josh Kalderimis from Travis CI. Josh is a co-founder and Chief Post-It Officer at Travis CI. Topics: * What is Continuous Integration, CI * What is Travis CI * Some history of the company * travis-ci.org vs travis-ci.com and merging the two * Enterprise and the importance of security * Feature questions * Travis vs Jenkins * Travis notification through Slack * Reporting history of Travis results * Dealing with pytest results status other than pass/fail * Capturing std out and stderr logging from tests * Build artifacts * Tox and Travis * Using Selenium * What does a Chief Post-It Officer do * Differentiation between Travis and other CI options * Using Slack to keep remote teams communicating well * Travis team * Funding open source projects * Travis Foundation * Rails Girls Summer of Code * Open source grants * Mustaches and beards * Shite shirts * New Zealand * What does Team Periwinkle do But be sure to listen to the last 5 minutes. I've got some bonus audio that didn't quite fit into the rest of the interview that I tacked on at the end. 
The 6 month things you are referring to are likely the software development boot camps out in California, they cost about as much as a year of college too. I've heard good things, though I went the traditional BS in CS route
Man, I love it when people go around explaining their setups. This is pretty cool. 
Then the complaint is a moral one, or philosophical. Maybe it's even a _Godel_ problem, as I would ask whether it's even possible to infinitely progress into computationally better futures while continually supporting regressive programs. Anyways, I haven't seen a killer app that has made me try Python 2.7.
then = now() + 60 seconds while now() &lt; then: things() Someone whose bored should figure out how to do this to precisely 1 nanonsecond.
Tip for next time: `datetime.now().hour`
Why even do multiple assignments? t = int(str(datetime.now()).split(' ')[1].split(':')[0]) Or yeah, as Rodeopants said, datetime.now().hour
yeah i thought the same. Why is this an assistant? 
Favourite relinquishment license: http://unlicense.org
Hella dank, I'm going to dive into a bit more later but this looks like what I need, at least to start this program. Thanks.
Do you know one? http://www.wtfpl.net/
The len() thing is just a symptom of a larger underlying problem with python. The language provides so many ways of accomplishing the same task that you inevitably end up mixing a bunch of different programming paradigms into the same project. len(list) makes perfect sense in a functional language but in an object oriented language you would want all "list-like" or countable types to inherit from the same parent instead. The \__len__() thing is basically an informal interface. Even if you try to remain disciplined in your own code as soon as you start relying on third party libraries that might as well go out the window. I don't think this is solvable, its just a consequence of building a powerful language.
Measure the exact rate of some kind of ongoing mechanical process in the machine and base it on that. I remember reading about programmers on drum memory who would (make completely un-readable programs by) knowing exactly how long it would take to seek to different points in memory and using that as part of their program logic. Edit: Of course remember that choosing this course means as soon as that hardware is replaced the program will suddenly and inexplicably begin failing...and whoever they put on debugging it will probably come after you with murderous intent.
PyCharm, I bought it myself but you can get the free community version.
This works, as long as things() doesn't take more than 60 seconds (e.g. things is waiting for a packet or user input). Solution to that one depends on a lot more specifics.
I'm not sure about autopep8 specifically, but what is the proper indentation of this snippet: if a == 'foo': b = 'bar' c = 'baz' Clearly b should be indented, or else it will not compile. But should c be indented? Not possible to know in the general case. And any error rate at all makes auto indentation rather useless when applied to large blocks of code. Much easier to just select relevant chunks and &gt; or &lt; them. I will check out autopep8 though.
I just finished creating a python wrapper around a C library by way of Cython. Does pypy have a C extension system for this kind of work (wrapping an existing C API)?
Interesting. My team has written something similar http://velociwrapper.io
&gt; you can read this ~~code~~ grimoire, but you cannot modify or redistribute any potion of it
Yes there is: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares &gt; Solve a nonlinear least-squares problem with bounds on the variables.
Good! You got the spirit of it then. Your particular stack sounds like a Starbucks drink. Make mine a grande.
I believe that this was discussed at one point to change in Python 3 - the type/class unification from way back and the relatively newer abstract (and virtual) base classes together mean that having `len` as a standard method on all Collections is a lot easier than it used to be. It was partly originally made as a function (and a magic method) was to avoid situations like Java, for example, where you do `my_array.length` but `my_list.size()`. The reason it was rejected amounted to Guido feels it reads better as a prefix operator like `abs`, `min`, `max` and `sum`. Also, your `reverse` example is misleading - `lst.reverse()` works in-place, by side effects, which `len` doesn't do (because it wouldn't remotely make sense). It makes more sense to compare `len` to [`reversed`](https://docs.python.org/3.5/library/functions.html#reversed).
*Woosh*
Rodeo!
The best thing you can do for yourself whether you go to school or not is build software based on ideas YOU are passionate about. When you're done, you'll have learned A LOT.
getty is the process that asks you for your login and password and starts your shell on a virtual terminal
Does LXML also work in the py3k/py3.3 branches?
This is literally my wake-up alarm in the morning. It works amazingly. I usually am up by the DO IT!!! JUST DO IT!!! part.
Because in most countries in the US, thanks to the Berne Convention, the second you write something, it's copyrighted. This means you have to license it. It's actually very, very hard to put something in the public domain. This is why the Creative Commons public domain license exists. To mimic the public domain using a license.
Thanks! 
&gt; The \_\_len\_\_() thing is basically an informal interface. [I think it's not quite informal](https://docs.python.org/3.5/library/collections.abc.html#collections.abc.Sized). There is a generic way to tell if something can be `len()`'d: &gt;&gt;&gt; from collections.abc import Sized &gt;&gt;&gt; isinstance('', Sized) True &gt;&gt;&gt; len('') 0 &gt;&gt;&gt; isinstance(1, Sized) False &gt;&gt;&gt; len(1) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; TypeError: object of type 'int' has no len() &gt;&gt;&gt; class A: ... def __len__(self): return 1 ... &gt;&gt;&gt; isinstance(A(), Sized) True &gt;&gt;&gt; len(A()) 1
ftfy is a module that fixes some common Unicode problems after they happen. It's particularly useful when you're working with text data that's already messed up by the time you get it. Here's the [changelog](https://github.com/LuminosoInsight/python-ftfy/blob/master/CHANGELOG.md). The big change in this version is that it can fix text `â€œ like this â€�` to be `“ like this �`. It can't fix the character that got corrupted with a � in it, but now it at least isn't evidence against the rest of the string being fixable.
Computer Science 100 first homework assignment.
This seems even more strange. `A` is declared to have no superclass (except perhaps object) yet `isinstance` misleadingly implies that `A` is a subclass of `Sized` merely because the `__len__` method was implemented. 
One problem I ran into the other evening is when filtering value that doesn't exist, Lifter seems to crash most violently. I guess this behavior is somewhat consistent with Django throwing a DoesNotExist exception but it threw me for a loop at first. I was expecting a None or an empty list/dictionary, etc.
"keep it around" -- there's plenty of reasons to keep it around, not the least of which is that there is an active community of people who are still using it. That doesn't mean you should *start* using it, of course. Porting tons of work done in pyenv also isn't necessarily going to be worth it for the "advantages" of conda, which I think exist but are not worth the labor cost.
key-word "obvious". I think Python succeeds at this way more than a lot of languages. There is far more consistency on how libraries are authored than I've seen in some other languages.
Very interesting to hear someone say that about Python. I've always found it a very good example of not having so many (obvious) ways to accomplish the same task. 
That is due to Python's duck-typing nature. Having a `__len__()` method *makes* `A` be `Sized`. Here is the relevant part of the `Sized` class definition: @classmethod def __subclasshook__(cls, C): if cls is Sized: if any("__len__" in B.__dict__ for B in C.__mro__): return True return NotImplemented
I mean, as someone who initially hated the indenting, I can tell you that in practice, this kind of ambiguity is rare. Unless you paste something in completely blindly, this wouldn't happen. And if you are that careless with pasting, I could see problems emerging just the same with braces. From a non-redundancy point of view, I kind of like using the whitespace. In programming languages that use braces, you are sort of redundantly encoding the block level with the indentation and the braces. The compiler cares about the braces, but your eyes care more about the indentation. This also leads to problems when someone is careless and the indentation doesn't match the braces - resulting in a situation where someone might read the code incorrectly. Using the whitespace to also delimit code blocks eliminates the redundancy - forcing all code to automatically be formatted correctly. At least with regard to indentation level.
Absolutely agree. The ambiguities probably come up for me more than most because I do a lot of refactoring as I work. Statements move in and out of if blocks quite frequently and having them automatically re-indent is quite nice. Though as I said before, far from a deal breaker if they do not.
/r/learnpython
No, it's absolutely bizarre. I had no idea `isinstance()` was supposed to check interfaces. I thought it was supposed to be checking whether it was an instance of a class. It seems like black magic to use `__subclasshook__` like this (or at all, I can't think of a non-overly-magical application of `__subclasshook__`). A newly created class is apparently a subblass of something that is not in its `__bases__` attribute? Wut? For application specific metaprogramming, sure, but in the stdlib this seems like bad form.
I just stick with alt+tabbing. :)
&gt; that's pretty dumb, since most of the time when these are being used it's probably on numpy arrays... A meaningful measure of this would have to come from a survey of actual code repositories. There's an absolute ton of Python written that has nothing to do with any single third-party library (even one as popular and respected as numpy). I've been coding Python for 10+ years (for work and as a hobby) and I use the built-in min() function *way* more often than I call .min() on a numpy array. 
I currently work on a Perl codebase with a slow, yet insufficient test suite. This style could make TDD really easy, useful, and attractive to myself and my co-workers. I may have to try my hand at a port.
Why? Bases give you methods, members, and metaclasses, they don't really have any bearing on the duck typing nature of Python otherwise. They're just a convenience in that regard.
Definitely not. Not sure how you came up with numpy being that ubiquitous but I can guarantee that there are for more libraries that use min and max then there are libraries that use numpy. 
That's why we have comments! # extract hour from current time
I think consistency in syntax is overrated. I'd much rather have consistent semantics for common things like IO. I think I can memorise the grammar of a small language, but semantics means when you build a module I will be able to use it. Python fails miserably here -- how many incompatible http implementations do we need? An application that builds using libraries written for the wrong IO model means it's incompatible with other libraries! Consistency is good, but it could be better!
At least you didn't name it Ultron.
Python has always had language constructs triggered by the presence or absence of specially-named methods. When abstract base classes for many of those protocols (like iterables, mapping types, etc.) were added, it wouldn't have made sense to force every piece of code in the world to adjust its inheritance hierarchy to include those, and it would have gone against the fact that in Python you're encouraged to care about whether the object has the methods you want rather than whether it is of a particular type or has particular classes in its inheritance chain. On a side note Python's abstract-class implementation also works around a limitation in other languages: in (say) Java, if a third-party class implements all the methods to conform to an interface you wrote and care about, it still doesn't "count" because the third-party author didn't and wouldn't know to declare `implements YourInterface`. So you end up writing a throwaway subclass declaring the implementation, just so you can sneak it past the compiler, and you have to take care to always only instantiate and pass around instances of your throwaway subclass. In Python, meanwhile, you can just explicitly declare "hey, that package's class implements this interface I care about, let me use it that way", and Python will say "OK, you probably know better than I do" and let you do it.
Guess I don't use the same modules you do. I've found consistency to be very good throughout. Except for file likes...
Thanks for the explanation! It makes sense, and I see how putting all this stuff in `collections.abc`is the right thing to do. It would only be objectionable if the standard library was using this magic - obviously it should go to the trouble of declaring classes to be subclasses of whatever they are implementing the interface of, and should leave the hackier things for users to use when they have to for reasons such as you gave.
I bought the book eons ago. Stop with your assumptions.
It is open source but not free software
Mine is Bootstrap Angularjs Django uWsgi ubuntu Postgresql Baduup. Yeah I'm not that brilliant in naming . 
Hmm maybe look at mongodb or postgresql(has a feature to create json even) http://postgres.cz/wiki/PostgreSQL_SQL_Tricks
Windows support is quite critical for us. How far are you from Numpy on windows support?
You don't do HTTP requests? * https://twistedmatrix.com/documents/current/web/howto/client.html * http://www.tornadoweb.org/en/stable/httpclient.html * http://www.gevent.org/ * https://github.com/KeepSafe/aiohttp * https://docs.python.org/3.1/library/http.client.html A module that does HTTP requests has to pick one, and that may cut it off from being used by someone who picked another. Extra levels of indirection can help this problem, by letting the module "receive" an HTTP implementation, but this massively hurts performance, and makes the programming much more complicated. Ever heard of REDIS? It has [twelve implementations for Python](http://redis.io/clients#python). Some are for asyncio, some are for tornado, some just use the socket directly... This isn't an unsolvable problem, nor is Python the only language that has this problem, but it's my opinion that it's a *more important* problem than whether length is spelled `x.len()` or `len(x)`
My understanding is that pep8 says the third line should be indented. If it wasn't intended to be part of the if block there should be 1 blank line between lines 2 &amp; 3.
As someone who uses numpy daily: I almost always do `np.min(a)` instead of `a.min()` since it reads better.
It works even better when you install Adobe reader
Windows is such a pane. 
If you're going to use Nagios, you might as well use Shinken. It's Nagios compatible, has many more features, and is written in Python itself. 
Hey, this was a pretty cool demo! Thank you for sharing it! &gt; the program also searches wikipedia and has other plugins that can be added.The plugin framework is most of the value for me. I'm pleased to hear that. My understanding is that a plugin model is how Siri works, and it sounds like a good approach to the natural language thing. I'd love to see something like this expanded so that the plugin architecture is well documented, with 3rd parties creating plugins. I'd also like to see it adaptable to other interfaces, such as microphone/speech-to-text and text-to-speech, or text terminal instead of web. Lots of possibilities. 
Just last year numba released its gpu extension to everyone (it used to be only for paid users). You might wanna take a look at their api. http://numba.pydata.org/numba-doc/0.13/CUDAJit.html
NASA uses it!
Wooosh on that :) But powershell is actually quite decent these days!
&gt; We explicitly want a drop in replacement. Life is hard. If you keep thinking that a GPU is a drop in replacement for CPU bound operations you are setting yourself up for failure. GPUs are nothing but a tool, misused and over-applied your code will likely run slower.
About the music: BURN IT!!!! BURN IT WITH FIRE!!! Please redo the video with a voice over or something.
All these Redis implementations are projects outside the main Python library and you can't stop people to build their own library. This variety, IMHO, is a good thing. Every library faces them problem of talking to Redis from a different point of view, and you can choose what fits better to you use case. Over the time, also, the best implementations will survive while the others will be phased out. 
http://effbot.org/pyfaq/why-does-python-use-methods-for-some-functionality-e-g-list-index-but-functions-for-other-e-g-len-list.htm
I love that this sub exists
If you don't have a commercial license you have to release you application under a GPL3-compatible license. That means for example that you can charge as much for your application as you want, but everybody else can do that too or distribute it for free.
That was an example. That particular plugin (open) uses linux xdg-open. The point isn't really the immediate results but the potential. I urge you to look over the code and the plugin framework before you make judgments. Thanks
I can read that quite easily. Splitting it across multiple lines still wouldn't tell you what that code is doing...
Right. The code sucks and should be more clear. Comments are not the solution to bad code. Good code is the solution to bad code. 
Yep. Of course it's not an appropriate question for CS stackexchange either. Not surprisingly, it got shot down there.
Some men just like to watch the world burn.
"Python 3 support" still means 3.2.5, though, yes?
Just so long as we don't name one "Tumblrina", because we all know that's a bad place to go ( as demonstrated in /r/tumblrinaction )
Do you need that list of 'changes' in `__eq__`? If you're only using it for this, you can change to something like this: return self.attr1 == other.attr1 and self.attr2 == other.attr2 That gets a bit unwieldy for a lot of attributes, so it might be better do switch to something like this: attrs = ['attr1', 'attr2', 'attr3'] return all(getattr(self, x) == getattr(other, x) for x in attrs) And even better is if you can generate that `attrs` list automatically. Chances are the attrs are every attribute you ever set by doing something like `my_object.list1 = [...]`. If you need those to be the same attributes (it looks like you do but you can assume it away, which is common) and have equal values, so you can use the fact that `vars(my_object)` will get you the instance dictionary of attributes (that is, only the ones attached to that exact instance, not the ones in the class) and do this: def __eq__(self, other): return vars(self) == vars(other) Although to be careful, you might want to guard against accidentally hitting it for instances of different classes that have the same attributes. The best way to do that is like this: class MyClass: def __eq__(self, other): if not isinstance(other, MyClass): return NotImplemented return vars(self) == vars(other)
Also, the builtin `min`, unlike NumPy's `ndarray.min`, has an optional `key` keyword argument which allows you to compare nearly arbitrary objects. You can use it with `len`, in fact, to easily find the shortest string in an iterable of strings. If you do `min(map(len, myiter))`, you get the length of the shortest string; to get the string itself you can instead do: `min(myiter, key=len)`. The same `key=` argument works with `max`. If your comparisons are more complicated than comparing string lengths, you can create an arbitrary callable and pass it into `key=` instead; most of the a time a short lambda is all that's needed. (`sorted` also accepts `key=`, and if that's still not enough it accepts `cmp=` instead!)
I disagree, it's not that obvious when almost every other OO language does it differently.
&gt; In this case, if it's an object and the object is capable of having methods, there's no reason why one of those methods shouldn't be the length. Objects DO have a method for the length. It's `__len__`. The `len` function creates one proper interface to access this that also handles `None` and incompatible types in a sane way. I don't understand what you're trying to get at. Do you just think OOP must have "foo.length" syntax? &gt; That doesn't preclude having a general case len function but that shouldn't be the only way to do it when almost every other accessor is an object method. So you want to have two different syntaxes for exactly the same thing? Why?
Precisely.
I don't necessarily agree that it's important for `__eq__` to return exactly a boolean, but returning a `changes` list from it has a bigger and more fundamental problem than just the wrong type: it would return a truthy object when the objects *differ*.
You should post this in /r/learnpython
In this particular instance, I of course agree with you. That is why I put that in my comment. However, I was adjusting the previous code as a learning exercise, not as an actual suggestion on what code to use in this particular project/instance. This particular use case benefits from having a python module with a nice, clear API. That is not always going to be the case. For reference, see any attempt at using HTTP in a Python program before *requests*. A good example of times when your code can't be super clear by almost necessity is screen scraping, or any other instance when you are traversing data/tree that has a complex structure.
I released my first package onto pypi this week (https://pypi.python.org/pypi/pygemony), and despite working in Python an a daily basis for the past few years, I found it an overall unenjoyable experience fought with disaster. This article seems like a good plan of attack for when releasing broken changes with a previously stable branch, but if your first release is broken, you're going to have to cowboy up and actually learn setuptools rather than patching a few tutorials together like me and hoping for the best. 
Also using a "kinda fumblina stack." I'm making a mobile app using ionic framework (angular web app into a mobile app), and I did my backend using Flask, Mongodb on top of Gunicorn with Nginx. I did choose Flask over Django because it seemed to be a lighter framework and I didn't need much. I need routing, io, an auth decorator and json formating. I want to keeo dead simple. I want my project to exist as soon as possible. Complexity will come latter.
Had a look and I saw a few things that you might want to look into: * You open files but don't close them. You should use the ['with' statement](https://www.python.org/dev/peps/pep-0343/) to automatically close files. * You do some parsing of dates but the datetime library should handle all of that for you. * Your args parsing could be greatly simplified by using a standard format for the args, such as CSV, or JSON, and offloading the parsing into one of those libraries. Alternatively, if these are suitable as command line arguments, consider ArgParser (or whatever Python uses).
You can read the whole thing online for free: http://chimera.labs.oreilly.com/books/1230000000393/index.html
Personally, I find it more readable to write flags using binary notation rather than with bitshifting (`0b0100` instead of `1 &lt;&lt; 2`) because it makes it immediately clear exactly which bits are important. That's a really weird bit of code because using ~~bit shifting~~ bitmasks implies that something can be in multiple states at once; that it's possible to be 'downloading' and 'stopped' at the same time. That seems unlikely.
When I try to run main.py I get an error that says inconsistent use of spaces and tabs.
You shouldn't, in fact someone just standardized everything in it. When did you clone the repo? If it was more than a day ago try cloning again. 
I cloned it last night before I went to bed. Ill try to do it again. Is it python 2 or 3?
I started doing this as I'm building a module for a small flask app. It's pretty handy to walk away from a day or two, then reopen and rerun all the tests I've done while I'm writing the primary code in notepad++
&gt; Lets look at another "OO language"(whatever that means) PHP. I'm not sure I'd hold up PHP as a model of OO design. Objects were basically bolted onto PHP around v4 (and totally redesigned for v5), and the core language doesn't really make much use of them. Unless they changed it in v7, even arrays aren't objects (which explains the non-OO functions used in manipulating them). &gt;what is wrong with len(list) I think the article explained pretty well what the problem is: lack of consistency. Whether we agree with that assertion or not, it's pretty clearly stated. And frankly, I think lack of consistency in a language syntax needs to be justified. Otherwise it's pointless cognitive overhead for the programmer to have to remember which processes are methods and with are functions. I tend to see some method to the madness in Python, but even so I've been known to get mixed up on things like `getattr`, `sort` vs `sorted`, etc. I can see this being a stumbling block for beginners.
well, you need to be careful if you return something besides True or False. if the object you return has the desired truthiness properties than its fine, I suppose, but that's just one more detail you need to pay attention to. you can easily introduce a bug by accident if there is some boolean evaluation behavior you don't know about in the object you return. if you just return True or False you know exactly what you're getting. its just simpler and avoids a potential bug. under which circumstances would you want to return a non-boolean from `__eq__`? I can't think of any good use cases off the top of my head.
to follow up on your informative and helpful post, the `changes_from` method you've defined here can be used in the implementation of `__eq__` on the class in question. def __eq__(self, other): return len(self.changes_from(other)) == 0 
Just to add to this for OP's benefit, it's a pretty common feature across programming languages.
Hi, Thanks - I implemented your solution and it works brilliantly combined with /u/metaphorm's response It makes sense not to return a non-bool from __eq__ and that potentially would have caused headaches later down the line. Thanks everyone for your help!
NumPy returns an elementwise comparison which can be then used for masking, passing to `numpy.where` for indexing purposes, etc. It's a pretty useful feature, although I'm not sure I particularly like it being through `__eq__`. It could have easily gone through a `numpy.compare(arr1, arr2, compare=operator.eq)` function.
I *swear* I read your comment three times, and never saw that line. Stupid eyeballs!
Mixing paradigms is really useful when done intentionally and with foresight, its why I love Python. The problem happens when you start importing third party code that is conceptually related but whose implementations are completely different. Java gets a lot of shit but when you import a Java library you pretty much know what to expect every single time. Java code is so uniform that I've used several libraries without even looking at the documentation. The combination of static typing and IDE autocomplete means that if you can type it in, it probably works. A concrete example of where this starts to fall apart in Python would be matplotlib. You expect consistancy between numpy, pandas, and matplotlib but in practice mpl does a lot of really weird stuff (for example figure.get_axes()[] instead of figure[] or figiure.axes[]). Sure, it doesnt seem like a big deal, but having that kind of thing pop up over and over in code that is very narrow in scope gets annoying (even more so in a dynamic language where you actually have to remember these details instead of relying on autocomplete).
yeah that's what I'm seeing. `__eq__` is part of the basic Python data model and it will be implicitly called by many builtin functions in Python and many interfaces expect it to behave a certain way (i.e. predictable boolean behavior). Numpy's use case is a special situation and honestly that is something I would criticize as unclear API design. Personally I would favor a solution like what you've proposed with a custom `compare` method. 
No, it's certainly not monkey-patching. (Which, by the way, does have some uses.) Outside of toy examples like mine, obviously this is only safe if the classes involved are designed cooperatively with an expectation that they will be swapped. You can't just pick any two random classes and expect it to work. At the minimum, they have to have the same interface, but the implementation can differ. I'm told that this technique is used frequently behind the scenes in Objective C. It's not something that Apple encourages Obj C developers to do, but they do it themselves.
Lack of consistency is not an example of a problem anymore than the "house is green instead of red" is a problem. Its a statement of fact. A statement of a problem is "A green house blends into the trees and planes keep crashing into it. Here is a list of crashed planes up 5000% year over year since we painted the house." Then all we have to agree on is we'd like fewer planes crashing into our house. I am fully prepared for you to explain to me(anecdotally of course) the value of consistency. It doesn't release you from the burden of explaining objectively why this is something worth changing. Ok so maybe its a stumbling block for a beginner for about 5 minutes. Then they've internalized it and moved on with their lives. I worked in C++ and Java for almost 8 years and when I moved into python as my primary language I don't recall being bedazzled by python's notorious "len problem" I'm saying its an empty complaint. A statement of OPs preference. It has relatively little merit. You want me to be on board show me something compelling and objective.
Stop being a cunt.
You found a bug, I created an issue and will fix it soon. https://github.com/lamerman/shellpy/issues/47 I use tempfile module to get system temp directory. Current structure of cached files is this: {system_temp_directory}/"shellpy"/{username}/{full_path_to_spy_file} So, in temp directory, in shellpy subdirectory every user has it's own space for preprocessed cached files. The problem was with wrong permissions for the shellpy directory. It allowed reading only for the user who created it, as all other files inside of this directory. It was done to protect users scripts from reading by others, but mistakenly affected also the root directory: /tmp/shellpy. As solution I will either fix permissions for this dir, or just create separate directories for each user, so it will look like /tmp/shellpy_lamerman and /tmp/shellpy_karouh Thank you.
multi-level comprehensions isn't even about doing it in one of many ways, it's about code readability. Simple comprehensions &gt; for loops because they're short and readable. Complex comprehensions &lt; multiple comprehensions or a for loop. Where do you draw the line though? If you have the primitives of programming, then everything beyond that is just extra ways to do things, because like a comprehension vs. a loop, there's already a way I could implement the logic myself. This is why the "obvious" in "obvious way to do things" is the critical word. Python strives to make it very clear what tool you ought to be using when. This makes code very readable to people not familliar with a code base. But yes, there definitely are edge cases if we go look for them. I don't doubt that. No language is perfect, but I think Python genuinely does strive to be what it is trying to be.
[removed]
Hey! The IRC channel is [#scrapy at freenode.net](http://webchat.freenode.net/?channels=scrapy) and there is also the [Scrapy tag at StackOverflow.com](https://stackoverflow.com/questions/tagged/scrapy).
Sounds like you wanted a project template that does the choices for you, instead of rummaging though a bunch of tutorials.
There are no easy answers here. Drop in? Not really. Numba works well on Windows and Linux but you have to write ufuncs sometimes. You should also look at ArrayFire if you want an even more numpy. It works on windows and opencl as well as the usual linux and CUDA targets. PyCUDA and much effort is the way to best performance. On windows you generally have to have visual studio professional except in certain situations.
Use PyPa's staging server first. It will save you a lot of grief!
&gt; Did you by chance name your script markovify.py? DERP thats exactly what i did, thank you for your help!
I've always taken 'one obvious way to do it' to mean 'one obvious way [for someone who has learned Python, but maybe not for someone coming from another language] to do it'. The len function is the obvious way for almost all pythonistas.
awesome job, I like it!
For the standard integer, kind of? If you take a 32bit int with value 0x80 00 00 00 and shift it to the left, it will be autoconverted to a 64 bit int. If you continue shifting left, you can shift off the end of a 64-bit int into nothingness.
Any reason you aren't using Postgres?
Multiprocessing is for getting results back to the spawning process. Maybe you should just use subprocess?
Your binary notation might be a bit clearer, but someone using that API should probably be aware of bit masks anyway
after testing in my python shell, you are correct. I was mistaken. Learned something new!
&gt; I don't necessarily agree that it's important for __eq__ to return exactly a boolean "the result of performing some operation should be obvious, consistent, and predictable, based upon the name of the operation" http://c2.com/cgi/wiki?PrincipleOfLeastAstonishment If you want a list of changes, don't use the __eq__ operator. Otherwise, you're going to catch someone else off-guard and end up causing a bug in their code. (And don't give me the "well this library does it!" excuse. Following someone else's anti-pattern is still an anti-pattern.)
Do you truly need a subprocess, or will a greenlet do? You're not specifying your problem very well.
&gt; especially what I did wrong Storing a password in plain text, in your source code.
There is always cygwin! (or console2 if you just want tabs with proper copy/paste) 
Arbitrary *size* in any case.
**Strong recommendation**: Just use the [PyPI testserver](https://wiki.python.org/moin/TestPyPI) before you deploy it to PyPI! The setup is super simple, just a few extra lines in your .pypirc. Here are the instructions: https://wiki.python.org/moin/TestPyPI Once you got it set up, you upload your package via $ python setup.py sdist upload -r https://testpypi.python.org/pypi And test it via `pip` $ pip install -i https://testpypi.python.org/pypi &lt;yourpackage&gt; Then, you can unistall it again $ pip uninstall &lt;yourpackage&gt; and test it on the "real" pip: $ python setup.py sdist upload
Tell PyCharm to ignore the requirement. Also, are you installed into a virtualenv? If so, you will need to make sure your project in PyCharm is also using the same venv.
Check out [dataset](https://dataset.readthedocs.org/en/latest/). Have you installed any other packages using pycharm? Make sure your project is using the correct interpreter (basically, what /u/axonxorz mentioned).
Yeah, but then you'd have a 3/4/5-layer for loop. Neither for loops nor list comprehensions are the best task for the job here. And I don't think list comprehensions are inherently less confusing than equivalent for loops (actually the opposite); I think that's just people's exposure to C-based languages and language constructs.
This is amazing. Too bad it only detects ferrets :P
Why not just `{}.update(defaults).update(user)`?
I would suggest checking out the Python Cookbook and also watch PyCon talks by the author because they're super entertaining. 
Sounds like you should write it. Thanks. 
I also read that requests was intentionally not being incorporated into the standard library, but the reason I found is that it needed a faster update cycle than stdlib gets due to security fixes.
I have a friend who's really annoyed by inconsistent APIs like you describe. His solution is to wrap everything so that the code he writes is extremely beautiful. He also spends *forever* tweaking and fiddling with his wrappers.
I know tensor flow has Python bindings.
The stdlib is good enough for most purposes and Python, as any language with considerable legacy, is constrained when the time comes to follow fancy fashions. It's indeed a good thing that a diversity of external modules exists to compensate for the intertia of the stdlib. It's not such a good thing that people often write libraries just to comply with the Parkinson law of triviality, without even bother to "feel" what the stdlib offers (the same as they install thousands of vim/emacs/etc. plugins without understanding the core design in a more principled way; in this the programming world is just next to the fashion world). Moreover, the crappy stdllib argument would imply the existence of a couple of external substitutes; once a good substitute is in place you cannot apply the same argument to explain further proliferation of libraries.
This is really cool! Can you explain more about the process? What is the code "looking for"? What would you need to do to make it a Cat Motion Detecter?
Use Anaconda. Like, really, if you want to never have to deal with this bullshit that distracts you from actual programming, just use Anaconda. (install miniconda of course, then "conda install twisted") They have twisted on the Python 3.5 branch, I checked.
So, you’ve released a broken package to PyPI. What do you do so it doesn't happen again? unit tests
And after running unit tests locally, use a continuous integration service to test it with various version numbers.
"If you are about to ask a question, please consider r/learnpython."
http://lmgtfy.com/?q=python+deep+learning link three seems like a solid starting point: "A list of python software for deep learning — /aidanf/"
I haven't tried [gnumpy](http://www.cs.toronto.edu/~tijmen/gnumpy.html), but I think it's supposed to do what you're looking for. Theano, which I am familiar with, is probably not a drop-in replacement because it's an optimizing compiler as well, which means you write declarative code, then get Theano to compile C code to compute some result. The performance is quick after the compilation stage, but you can't afford to compile and compute all expressions on the fly, so it could (depending on the task) take considerable work to substitute it for imperative code.
Interedting! But can you elaborate on when this is usefull? Why would I use it? Something related to performance?
Its a package for monitoring machines and sending the data it to some aggregation service .
Did /r/learnpython die or something? I have been away for a while, but all i see in /r/python these days are /r/learnpython questions.
I'd do something like this: if val == 10: return True elif val == 20: return True elif val == 30: return True elif val == 40: return True elif val == 50: return True elif val == 60: return True elif val == 70: return True elif val == 80: return True . . . ...and continue until you hit the end of 10 multiples.
I only said "Ferret Motion" because that's what I was recording at the time, it should track any medium to large sized motion. Sorry for the confusion!
Another reason might be that you are given the bits by an outside source and need to do something with it.
Actually, Shinken is a more flexible, complete replacement for Nagios. 
Do I have to mention to the buyers that they can distribute it for free or sell it ?
So, the only advantage of a commercial license is that I don't have to give the source code ? Since I will use cx_freeze to package it, I have read that it is easy for someone to get the code. Also, I am an individual programmer, so I can't prevent someone from sharing my application and I can't take action if I notice it.
Ah, that's annoying. I haven't worked with Python in a long time, tell me, is it common for stdlib methods to mutate?
&gt; it's a pretty common feature across programming languages. I feel like it's more commonly used by people who code/coded in C, where bit flags are used more often. &lt;insert rant about kids these days not knowing the fundamentals&gt;
It's simple, really. He/she works in an environment where it is ubiquitous and has never been exposed to anything else. Someone who learned Python where I work currently might easily think numpy and pandas are ubiquitous, because those are the tools we use for most of what we do. Likewise, someone who started their professional career in my last job might think the vast majority of Python development for websites, or that Linux workstations rather than Windows are the norm for engineers (one can wish...)
You don't need to send the source, only mention how to get it and charge a reasonable fee to send it to them
&gt; You can say the same about python No, you can't. * A GPU runs a kernel on *many processors* in parallel, each processor acts on *a unique segment* of memory. * A CPU runs Python code on a *single processor*, acting on a *single memory segment*. This is what I mean by "fundamental". There is no way to map one to the other. Hence, no drop-in.
Fairly common. An example: `sorted(some_list)` will return a new sorted copy but `some_list.sort()` will return `None` and sort the list in-place (mutating it). The `reverse` and `extend` methods also work in-place. Usually if a new object is required, operators are used: x = [1, 2] y = [3, 4] z = x + y vs. x = [1, 2] x.extend([3, 4])
Yes, it is trivial to 'unfreeze' a Python program to source code. If you don't want someone to be able to do that, I would suggest a few options, but none of them are great. The easy part is the licensing. Switch from PyQt to PySide. There are some subtle differences (signals in particular), but generally it's trivial to migrate from PyQt to PySide. Keep in mind that regular PySide does not support Qt 5, but PySide 2 is working on that (I have not tried it, and have not had the need to work with Qt 5). Instead of cx_Freeze, use Nuitka. Nuitka can compile to .exe, and it can not be (easily) translated back to your source code. Having said that, I have not been able to get it to work with PySide, but have only experimented with it briefly. Your other option would by Cython, which can translate your Python code to C, after which you can compile as with any C code. I don't know how well this works with PyQt or PySide - I've never tried it. As far as I'm aware, those are your options. None of them are great, but it should get you are starting point to research a little more.
I sort of disagree. A datasheet would say bit25 means something it's usually easy to realize that 1&lt;&lt;25 is setting bit25 but writing it out in binary it would be difficult to tell which bit is actually 25 in a 32bit register. 
In OP's example it's used to create distinct status codes for use as keys to a hash of states. It's convenient because you can just increment `i` in `(1&lt;&lt;i)` to get another, distinct status code for a new state (e.g. `error`). So it's an example of building an extensible interface using a clear and easy to understand syntax.
I guess I'm being an ass, but... &gt;This post was co-written with Monty Taylor Did it really take two people to write 541 very obvious words? 
Your actual question doesn't appear to have much to do with sorting, and the example you give under-specifies the problem. You have input: abbcccdddd and desired output: 1a2b3c4d but there's no clue what should happen if a letter repeats like this: aabbaa (output "2a2b2a" or "4a2b"?) or if the letters are out of order: bbaa So I'm going to guess a couple of solutions. Firstly, assume that order of the letters is irrelevant, you just want to count each letter no matter where it is in the string. py&gt; counts = {} py&gt; for letter in "eeabbcccddddeee": ... counts[letter] = counts.get(letter, 0) + 1 ... py&gt; parts = [] py&gt; for letter in sorted(counts.keys()): ... count = counts[letter] ... parts.append(str(count) + letter) ... py&gt; print(''.join(parts)) 1a2b3c4d5e Second, let's say that you want to collect consecutive groups of letters. That's quite a bit harder, but fortunately Python already includes a partial solution for that: `itertools.groupby`. py&gt; from itertools import groupby py&gt; parts = [] py&gt; for letter, repeats in groupby("eeabbcccddddeee"): ... count = len(list(repeats)) ... parts.append(str(count) + letter) ... py&gt; print(''.join(parts)) 2e1a2b3c4d3e 
Of course - check any torrent site, and you will find nearly any commercial software ever built available. This isn't a Python issue. There is nothing you can do about that, no matter what you are coding your program in. Yeah, you can distribute your license details with your software, but that's not going to stop people. Even technical solutions to prevent piracy are usually trivial for determined folks to crack. At the same time, other people are ethical and like to support the people and companies that produce software they like and want to use, and feel a lot more comfortable downloading and using software straight from the developer.
You could represent this as a set of integers, and indeed that's (IMO) the Pythonic way to do this: CHECK_PENDING = 1 CHECKING = 2 DOWNLOADING = 3 SEEDING = 4 STOPPED = 5 current_status = {CHECK_PENDING, DOWNLOADING} if ready_to_check: current_status.remove(CHECK_PENDING) current_status.add(CHECKING) if SEEDING in current_status: ... current_status = {STOPPED} However, a much more compact representation is to store it all in a single integer. That's not particularly Pythonic but you'll need it when interacting with C-ish protocols. You might also want it if you're concerned about space usage. CHECK_PENDING = 1 CHECKING = 2 DOWNLOADING = 4 SEEDING = 8 STOPPED = 16 current_status = CHECK_PENDING | DOWNLOADING if ready_to_check: current_status &amp;= ~CHECK_PENDING # keep all bits other than CHECK_PENDING current_status |= DOWNLOADING if current_status &amp; seeding != 0: ... current_status = STOPPED Same semantics, just a different representation. In C where allocating and resizing lists of things is a pain, and where the rest of the language is so fast that you'd prefer not to allocate at all, the set approach would be very unidiomatic, so the bitfield approach is common since it just requires a single integer.
heh nice :P
Oh come on, I'm pretty sure the OP knows how to use print. This is a question about what seems like a bug in Enthought's IDE.
Exactly! I find this useful for masks: mask = 0b11 &lt;&lt; 9 Means the field is ~~3~~ 2 bits wide, starting at bit 9. It's easier to understand, with a quick glance, than 0x600, or the inappropriate decimal value of 1536. edit: 0x3 to 0b11. Much better. I was in a python 2.5 house for 4 years, bad habit, we didn't have the luxury. :-|
You probably need to ask Enthought, but if I were trying to debug this myself, the first thing I would do is inspect the value of `sys.stdout` and see whether it actually is stdout. (Since you can't print, you may have to use the debugger, or write the value to a file.) If you're using Python 3, I'd also check whether the `print` function has accidentally been re-bound to something else. E.g. if you do this in Python 3: def print(*args): pass that might explain the symptoms you are seeing. (Although how you would do that *by accident* escapes me.) I presume you've tried actually exiting the program completely, not just restarting the kernel?
Or int(hex(value)[-2:], 16) Yes...I've seen this used before. I think you've been a bit too high level for too long if you do something like this.
Yeah, I figured this would be clearer for an example but maybe not. :) I would in fact use the `1 &lt;&lt; 0` etc. syntax this post started with; if you know what's going on, it's the clearest syntax and also the least prone to error.
Looks like you were able to \**cough*\* ferret out the right tool for the job. :D
Seconded. I like this book.
I completely agree with that then, it give semantics to what you are doing.
&gt; The C++17 standard will "fix" that by automatically falling back from x.f() to f(x) if x.f() is undefined and f(x) is defined. I see **no way at all** that could possibly go wrong... wow.
This should be the standard for learning languages from now on.
Enlighten me? I've used npm very little but the functionality I know is essentially the same. 
&gt; Python **is** an OO language [citation needed] (Python has become more OO over time, but it's **also** become more functional)
Thank you! 
I have mixed feelings about DRF. On one hand, it's a prepackaged tool for standing up APIs. On the other, I feel it's very class oriented (not object oriented) and full of opinions and questionable choices. Being a tool person (e.g. I like making things like DRF), I've slowly hacked at it and replaced bits and pieces. Overall, I feel it was a bad choice for our project, but I came in well after it was underway.
Haha :) not being a robot overlord sucks at times
I actually prefer representing bit fields as an enumeration. It tends to look a lot cleaner. Enum { State_one = 0x001 State_two = 0x002 State_three = 0x004 } state;
Also FYI in some other languages (java and c come to mind) bitshifts are used when multiplying/dividing by two is more semantically correct bc it's far more efficient
Thank you very much for writing this.
With pip you're intended to use virtualenvs to keep things local since it uses your python installation. I have no idea what versioning issues you're referring to because that's extremely simple with pip ( `pip freeze` to see an example) unless you're installing things globally and you get those version discrepancies.
Thanks a lot for writing this up. I had no idea pylint could do this.
This sounds almost like *the* stereotypical *crazy-dude-that-believes-he's-god* picture of a mental hospital patient that sadly has to be kept locked up. Does that sound correct /u/kennethreitz ? Do you have any thoughts on actually 'being forced' (by your brain's psychology/physiology) to personify the stereotype? Do you now ever have temptations to want to 'exploit' being in a hypomanic state to get more stuff done? Does it seem like a bit of a 'super power' to you, or something else? A cursorary article about the limits of human sleep [like this one](http://www.scientificamerican.com/article/how-long-can-humans-stay/) suggests a limit of ~10 days for pratically everyone, and of course without you can suffer severe negative effects. How worried were the doctors about your case then? 12 days sounds crazy! No disrespect meant by these questions btw, awesome to have you back!
Thanks for writing this. The hospital stay-in reminds me of a book I read before, It's Kind of a Funny Story, by Ned Vizzini. Vizzini had a hospital stay-in and related to it in his book. His stay, along with the medication he took, was also transformational for his depression at the time. I get the idea a lot of people don't feel comfortable at all having an open discussion on something like mental health. 
"Don't date crazy chick" Okay, got it. Thanks for the tips and story of yours. It was really interesting to read. 
Having multiple decorators doesn't look nice. I'd prefer a single decorator that expects a list of commands. @pynit([run(), background(), sudo('bla'), register('sshd')])
&gt; This sounds almost like the stereotypical crazy-dude-that-believes-he's-god picture of a mental hospital patient that sadly has to be kept locked up. &gt; Does that sound correct /u/kennethreitz ? Do you have any thoughts on actually 'being forced' (by your brain's psychology/physiology) to personify the stereotype? Not /u/kennetheritz, but I experienced delusions of grandeur for a year or so due to mental illness (the delusions weren't severe enough to require hospitalization, but they were severe enough to affect my actions). I believed strongly that I had a great destiny, like I was a demigod or an avatar of a god, to the point that I found it dissonant and dysphoric to do mundane things like "working" and "buying food" and "interacting with people". I wouldn't let myself believe supernatural things, but I constantly, vividly and seriously fantasized about things like leading a nationwide rebellion, becoming a world leader, etc. (all this while I held no particular political beliefs), and I seriously suspected that my nice normal parents had actually adopted me from a "more important" couple. I didn't recognize that I was in that state until *after* I'd started to recover. So there's definitely a grain of truth in that stereotype, and it can definitely spring up without outside influence.
&gt; I'd prefer to do away with the len() function entirely. Ah. Well I disagree. Personally I don't have much love for member functions. They tend to be awkward in many cases so you end up with mixed variants anyway and it's a mess.
`__eq__` returning anything other than a boolean should be carefully considered. Take SQLAlchemy as an example: session.query(MyModel).filter(MyModel.attr == 5) Returns a comparator object that eventually gets loaded into the built query (through the magic of descriptors, when you compare against instance attributes you get the standard behavior).
Web browser Automation library like Selenium. I hope it will become Python standard library.
Really great article. I had a pretty scary manic experience last year and it's good to hear other people's stories 
&gt; And Exercise. Do some physical work. Get out and go for running. Don't just sit in front of computer all day long. 
#Big Thank You! --- Edit: For those of you on Mac OS, here's the steps that it took for some super-impressed, random redditor to see the awesomeness of which OP speaks: brew cask install java brew install graphviz --with-app --with-bindings --with-freetype pip3 install pylint Then I was able to get png output from pyreverse. ^(And it was awesome.)
Thank you. This description of what you went through helps me and others understand you, so that we can have compassion on you and others in the same kind of state. For example, Jason Russell who directed the Kony 2012 video and later had an episode that caused most of the Internet to mock him. It sounds a lot like the kind of thing you experienced. From Wikipedia: "brief reactive psychosis, an acute state brought on by extreme exhaustion, stress and dehydration" -- it sounds like a made up thing, but from your experience it's clearly real. How many people has it affected? How many people have been thrown in jail because we couldn't tell the difference between a psychotic episode and ... I don't know. I do know that you have contributed to the discussion of mental health issues in a powerful way, though, and helped me see it differently. Thank you for that. 
I make many financial offerings to the corporate Amazon goddess each month, and my code empowers her datacenters.
&gt; I see no way at all that could possibly go wrong... wow. You could say the same about function overloading or a bunch of other risky-if-misused stuff. D and Nim (and partially C# with its extension methods) have this feature and I don't see it being misused. Anyway, this proposal was rejected (at least for now) - instead, they are leaning towards making it work the other way - have `f(x)` fallback to `x.f()`. ([proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0251r0.pdf)). Out of these two I'd have preferred the former, but oh well.
I'm working on a selenium project and will be trying profiles for the first time. I will play around with default search engine to see if I can get any insight. 
Thanks for all the help! Im an idiot and didn't have my settings configured right, project was using 3.5 and I wanted 2.7! 
I'm a fan of Python 3 also, but this isn't a great argument. One thing keeping people on Python 2 is that they've already learned the "best practices" that make it not so bad, some of which are actually different in Python 3. `range` is one of those, because if you're a Python 2 user you have to have just internalized that when you mean want a range you say `xrange`, which is the same as Python 3's `range`.
It's awesome you're doing well now. It sounds like the doctors at that facility were great.
I have used pyautogui for Web automation which works very nice. It is not the same as selenium or beautifulsoup but was quick and easy for my problem. Pyautogui: https://pyautogui.readthedocs.org/en/latest/
Hey Kenneth, Good to hear you're well. Thanks for writing this!
Extract the content via Beautifulsoup, then use a template engine (i suggest Jinja2) to fit it in your own HTML.
By "almost" I mean that begin(x) is also defined for non-object types (arrays), so pretty much what you said. &gt;And the proposal you mentioned was rejected, see my other response. Thanks for pointing that out! I had confused those two proposals. What I wrote refers to the proposal that you linked (having free-standing functions fall back on class methods).
Thank you Kenneth for taking the time to write all of that. Sorry for what you went through, but I am glad you are now back to normal.
I think that's a decorator one could implement ;p 
I'll keep that in mind.
Aside from... selenium? Every time I've used it in python it's worked pretty well.
I feel like the biggest takeaway is that sleep is super important to you. Whenever something disturbs regular sleep, your mind becomes a lot more susceptible to states of imbalance (panic attacks etc.). It can sometimes be insidious though, because if you become depressed, then the depression can interrupt your sleep, thus making it harder to become less depressed AND making you have more states of imbalance. As long as you heismann out anything in your life that might disrupt your sleep, you should be fine
Dude probably just wanted to give obvious credit to a mate that proof read the work and provided comments to improve it before publishing.
I'm so happy that you've gotten through this and come to know yourself better as a result. I consider you one of my open source heroes and I have fond memories of the dinner in Portland with you, Alex, Jeff, and Noah. If you're going to any conventions this year I'd love to say hi.
I think that is because so many more of us have directly experienced the influence of drugs or observed the behavior of someone who we knew was on drugs at the time. There's no outward event to observe that someone has entered a mental health event (for lack of a better term) so even when we have observed it, it can be hard to know what we observed.
Why should this be used over [datetime](https://docs.python.org/2/library/datetime.html)?
Looking good. Tags have spaces between them horizontally and it's nice, but as soon as they wrap there's not vertical spacing and they seem to look stuck together. More of a visual nit pick though, not game breaking by any means. The "Expand" area at the top seems out of place or maybe not polished? I didn't even know what it was until I clicked the black circle and it expanded out. Maybe have it open like a ticker at first? with expansion as an option. 
You're using Python. Which means that you can pass anything in to any API, as long as it acts as expected (duck typing). sklearn expects, what, a tuple of numerics for each data point? Why not craft a class that acts like a tuple of numerics, but overload the arithmetic operations such that when sklearn does a distance calculation, it gets the result you intend?
In short, kmeans is for Euclidean distances only. See, for example, [here](http://stats.stackexchange.com/questions/81481/why-does-k-means-clustering-algorithm-use-only-euclidean-distance-metric). Of course, the algorithm will do *something* if a different distance function is used, but it isn't always clear what this behavior is. The link above mentions ways to use, for example, cosine distance instead of Euclidean distance, by using MDS to transform your data, then use the Euclidean distance in this new embedding. Then again, people use kmeans in plenty of places where it has little theoretical justification. Just take this as a word of caution.
http://charlesleifer.com/blog/using-python-and-k-means-to-find-the-dominant-colors-in-images/
Valis comes first to mind. Taking such a journey with the narrator is a frighteningly easy path. Thank you to Kenneth for writing and sharing this.
Fuck off, spammer.
I think it's fallacious to say that mental illness is uncategorically 'some latent problem you had no control over'. Getting a cold doesn't mean you had no control of yourself when you went walking in the rain, so why would we say that a manic episode means you had no control over your habit of overworking? It's extremely disrespectful to the mentally ill to take that attitude IMO.
conda
It is a class that uses datetime and lets you check for intersections of ranges of times. Seems like a lot of code and trouble to install and read about for such a small functionality. I'll just use a tuple of datetime objects myself.
Use numba to write it and then open source it for the next person who asks : )
Wow that blog post is misguided. Their Django template is outputting malformed HTML so they use beautifulsoup to 'fix' it, while likely also severely increasing the overhead of their template rendering, just to output some 'pretty' html. Not that any of that is relevant to the discussion, but I felt the need to complain.
I went through a miniature version of that the year before last. I enjoy smoking cannabis and trying to understand how my mind works but whenever I smoke too frequently I come up with insane frameworks for cognition that are meaningless to me after I've spent some time without it. It's a very unpleasant state, you feel like you HAVE to tell someone about it or explain it but you can't explain it in terms even you understand, which makes you very fragile... Great description of the experience Kenneth, thank you for sharing.
If you don't mind asking me, which metaphysical figure was it for you? Hermes? Zeus? Prometheus? Do watch "The Secret Life Of The Manic Depressive" by Stephen Fry. [Part 1](https://www.youtube.com/watch?v=uj8hqXd7N_A) [Part 2](https://www.youtube.com/watch?v=B3rHTm1YLxA)
Try these list comprehensions: Ascending: [float(x)/1024 for x in range(1025)] Descending: [float(x)/1024 for x in range(1025)][::-1] 
It shifted between many, as mentioned in the post. The primary one was either Hermes or Metatron. I have seen that! Very thankful I don't deal with depression (other than a general lethargy and anxiety)
With lists: filt += filt[-2:0:-1] With numpy: filt = numpy.hstack((filt, filt[-2:0:-1])) Slice syntax: `[start:end:stride]` where `start` is inclusive and `end` is exclusive. Negative `start` or `end` count down from the end of the vector.
Massive post. Only on part 3 but I like it so far, will try to finish tomorrow.
Can't you just download it? http://www.pythonocc.org/download/ &gt; so I figured here would be my last shot. Find the mailing list. Go through the tutorial. It's just OpenCascade, so it does geometry operations and can mesh things. Look for that sort of stuff in a debugger. I'm going to learn PythonOCC someday. I've tried it out and have also worked with OpenCascade years ago. It's my next side project.
`pip` is really just a wrapper script that imports the pip module and calls its entrypoint. The shebang of that wrapper script determines which interpreter is invoked. But instead of messing with all that nonsense, there's a much better way. You can just invoke the interpreter directly, e.g. python -m pip install foobar If you have multiple interpreters and you want to run a specific one, then just use the normal means of disambiguation, e.g. by specifying a full path /foo/bar/python -m pip install foobar Note that if you haven't yet installed the pip module you'll have to do that first. If you're using 3.x (which you should be) you have the `ensurepip` module which can bootstrap the process (i.e. `python3 -m ensurepip` will install the module.)
Understood. Thank you!
Don't her data centres power your code?
That first answer only says you shouldn't, but doesn't give an alternative. The other answers say it can be done in certain situations and I'm pretty sure mine is one of those. It should work, but it cuts off at some arbitrary point. I'll see what BeautifulSoup can do for me. I've seen it recommended in multiple places, but I was sure I didn't need it.
He comes...
this sort of explains it. You'll have to add the modules into your setup.py test it with another machine that doesn't have python installed. http://pythontips.com/2014/03/03/using-py2exe-the-right-way/ keep in mind that py2exe only runs against 2.x versions of python so if you are using 3.x this won't work for you. 
...because they are not equivalent. `dict()` will make a dict out of a dict, defaultdict, list, etc. `copy()` will make a dict, defaultdict, list, etc out of a dict, defaultdict, list, etc. 
After adding Python to your PATH, make sure you close the command prompt and open a new one. The command prompt program loads the environment variables when it first executes so if you've modified the PATH after opening the command prompt, that process doesn't know about the changes you made.
reddit was only an example. I want to search other pages too, so reddit-specific solutions won't work. I fixed it with BeautifulSoup.
try [conky!](https://github.com/brndnmtthws/conky) and you can use this line in its config file : ${exec tail -n15 /path/to/feed.xml}
Where is the "1024" in your code?
 f = [ n/1024 for n in range (1024) ] f = f + reversed(f)
with the csv, `marks.csv`, and a class of students who just so happen to be named a, b, c, … [not like I'm lazy to come up with 9 names or anything] student,question 1,question 2,question 3,question 4 a,y,y,y,y b,y,y,y,n c,y,n,n,y d,y,n,y,n e,n,n,y,y f,n,y,y,n g,y,n,y,y h,y,n,n,n i,n,y,n,y If you don't mind hitting a nail with a sledge hammer, you can use pandas import pandas as pd # read in data using the many mathods to do so, making the student the unique identifier marks = pd.read_csv('marks.csv', index_col='student') # gets the first question column, if the column didn't have spaces, you # could have used dot look up: marks.question1 marks['question 1'] # returns a series of booleans where students answered 'y' for question 1 marks['question 1'] == 'y' # combines the booleans into a single series, where true is where # students answered 'y' for question 1 and 'n' for question '2' # (and: &amp;) and (or: |) I don't know why they didn't implement it as # &amp;&amp; and ||, but they must have their reasons # parentheses are important here (marks['question 1'] == 'y') &amp; (marks['question 2'] == 'n') # using a series of booleans as a look-up will create a new # dataframe with just the matches, therefore this filters all the # data to just have those students who answer 'y' for question 1 and # 'n' for question 2. marks[(marks['question 1'] == 'y') &amp; (marks['question 2'] == 'n')] # you can then lookup what you want from the result in this case, # look up the answers students gave for question 4, if they answered # 'y' for question 1 and 'n' for questions 2. marks[(marks['question 1'] == 'y') &amp; (marks['question 2'] == 'n')]['question 4'] you could save each of the important things into variables and reuse them, etc., but this is just to show, in fact, you can look at the output in each line by either printing them, or using Jupiter Notebook. import pandas as pd marks = pd.read_csv('marks.csv', index_col='student') marks[(marks['question 1'] == 'y') &amp; (marks['question 2'] == 'n')]['question 4'] You can easily apply this to your case, importing from whatever format you have it in normally, pandas can handle a fair amount, and you can help it if need be. Then just change the headers to what they are in the dataframe, you may wish to print out just the headers to be sure …whitespace matters. **Edit:** *clarity* **Edit2:** *`groupby()` would also work well, just illustrating a different approach, and the flexibility of pandas.*
Oh damn, thanks a bunch! 
Damn. And all this time I was building similar functions "by hand" using standard functions and NumPy. TIL
Appreciate you being this open about it. Thank you, Kenneth.
Ha. Groupby is super powerful. The other one I like is resample() which lets you "groupby" time periods (X per day, Y per workweek) etc etc. Between the two of them you can do an awful lot.
You should look at using something like `pyenv` details here: http://gadgetplayboy.com/post/using-pyenv-for-python-projects/
Assuming you are the same user from StackOverflow - I just responded. Why do you want to use a different similarity measure? Are you aware that using a different similarity measure is ill-defined and that the algorithm would no longer be considered K-means? What similarity measure do you want to use?
the same reason that I have (too many version management systems) for opt by pyenv in my case, for my purposes, IA related mostly.
If you downloaded the latest 3.x release, then the new command to run the interpreter is: py -3
Thanks, I hope it's handy! I'll be cleaning up the code a little bit this week, but no breaking changes. Hope to make it easier to extend (them sweet custom TFTP opts, right?). Let me know if you encounter any issues. Btw, mind if i ask what you use TFTP for? I've only seen it used as a way to distribute embedded firmwares/files for stuff like Cisco IOS or OpenWRT n' stuff.
If your spreadsheet is in excel format, try http://stackoverflow.com/questions/7372716/parsing-excel-documents-with-python .
I don't want to go too far into details at this point (I want GPL'd code to be released before I talk about it too much online) but it's got quite a bit to do with various networking gear. TFTP is definitely the universal way to talk to switches and routers, and I want something that will allow me to easily send files without leaving Python. I'll definitely send you more details once I've got a bit more of the thing done!
Yes, you want add the directory containing Python, not python.exe to your PATH. Thank you for adding this, I wasn't clear enough.
If you look at the disassembly, Python 3 is very different from Python 2. Python 2 is doing the loop in bytecodes, and the Python 3 version doesn't.
[py2exe](https://pypi.python.org/pypi/py2exe) is available for Python 3.
Because the datetime module doesn't have a data structure for range-of-datetimes ... this *uses* datetime. That's kind of like asking why you'd use a CSV library over [string](https://docs.python.org/3/library/string.html).
/r/learnpython
This release has additional examples, some refactoring to (hopefully) make it easier to customize behavior, and the [ReadTheDocs page](http://neat-python.readthedocs.org/en/latest/) has been updated to cover how to do some basic customization. Please let me know if anything is confusing, or if you'd like to see new features in the library itself, and I'll try to accommodate you! NOTE: The PyPI package does not include the examples, but you can download the full source, which does contain them, here: https://github.com/CodeReclaimers/neat-python/releases/tag/v0.7 Some examples from the image breeder: http://imgur.com/a/zIv5S, http://imgur.com/a/oAhkz The XOR spiking example output is also now a little prettier: http://imgur.com/a/IQK5Q
Is this page new or are we just sharing our favourite excerpts from a manual? :) http://www.fullstackpython.com/unit-testing.html !
Or in pandas you can just do pd.read_excel(.....)
Start here: https://www.codecademy.com/learn/python After this you have a good basic knowledge what is going on and you can specify some programming topics after that and make some projects.
Hey Fady-Mak, thank you for sharing! This page I wrote is unfortunately a bit underdeveloped at the moment. I'll work on beefing it up further later today after I finish up helping out participants at the fantastic [Hacktech 2016](http://hacktech.io/) hackathon where I'm currently mentoring.
&gt; static type checker mypy?
This guy seems to have implemented kernel k-means, based on scikit-learn code: https://gist.github.com/mblondel/6230787
If I were you I would test K-Medoids. That is well defined for non euclidean measures and there is a pull request implementing it in scikit learn [here](https://github.com/scikit-learn/scikit-learn/pull/5085). That would avoid the whole java issue. 
Thank YOU for writing it, I've enjoyed reading it :) best of luck with the hackathon!
If you are serializing lots of numerical data, look at h5py. It is an hdf5 interface, and hdf5 was designed for store large amounts of numerical data. Edit: Adding more now that I have my laptop out HDF5 is a fantastic format and the HDF5 library is extremely performant. Within the HDF5 files, you construct a hierarchical structure that you can store dataset in. Mapping your dictionary of numpy arrays to and from hdf5 would be nearly trivial. I use hdf5 with files upwards of 12 to 20 GB and have no problems. One of the really nice things is that hdf5 and h5py allow to only import parts of the datasets, so it makes out of core computation relatively painless.
Are you planning to always use slack as the interface? Or is that just one of the plugins to run it from within slack?
I'm always planning to use slack because I eventually want devices to be able to talk to each other through it. But if you don't want to use slack you can connect any interface you want and take out the slack code fairly easily. 
In your `load_words()` method, you've hard-coded the name of the words file (`sowpods.txt`). That'll work for now, but it would make much more sense for `load_words` to take the words file's filename as an argument. That way you can swap in different sets of words simply by changing the code that calls the method.
Think of it as a symbiotic relationship
That's not surprising when you know what a nice bloke he actually is. I've had the privilege of knowing Kenneth since his college days, and his work in open source is of immense value.
Glad your crazy chick is better for you than Kenneth's was
Yeah, I just looked at it now. I noticed that python3 doesn't compile to .pyc files.
[removed]
Have you tried the [Official Django Tutorial](https://docs.djangoproject.com/en/1.9/intro/tutorial01/)?
The stupid answer is memorable. Sure you can use regex to extract bits of documents and HTML docs are no exception but it's not obvious when HTML becomes unparseable by regex so you end up not being sure if this is one of those cases or if you just aren't clever enough. Or to summarise: &gt; BeautifulSoup is **better** for this because that's what BeautifulSoup was made for
Start with printing the initial rows that don't include letters. Then use simple loops to write the rest. Start each row with the &amp;= then write = until the letter is to come followed by more = and another letter. Keep a variable knowing which letter to type which can also be used to know the number of = to write on the row. Tip: strings can be subscripted like a normal list, i.e s[2] will give the third char of the string s.
Yea it's hard getting started. There is a lot of moving pieces. What really started to bring it all together for me, and something I certainly wished I had come across sooner, was the Test-Driven Development with Python book by Harry Percival. [Read it here](http://chimera.labs.oreilly.com/books/1234000000754/index.html) This guy makes no assumptions about your experience and takes you all the way to deploying a production site. 
ok thanks man!
Conda is not oriented for end users, it targets programmers.
Mypy is promissing. I'm going to use it as soon as it supports async/await. Still, it's an young project, it misses integration with many tools in the ecosystem (IDE plugins, hooks for VCS, web service integration, doc generator integration: you have to script it all yourself), a good documentation to integrate it... But yeah, it's not missing, it's just in progress.
Nice, I had to write my own implementation a while ago. Glad to see a pypi package. Edit: often in survey data, not everyone answers all the questions. A quick glance looks like you count unanswered questions as being a match. Is that correct? I don't think this is good default behaviour.
As I also have commented on stackoverflow; PyJNIus (or similar libraries) are a good way to accomplish this. People can probably help to debug your problem if you describe it. For PyJNIus in particular, you can also ask for help via Kivy's support channels.
`data` is the bytes you read off the socket, so you don't call `.read()` on that thing. Just call `data.decode('utf-8')`
Will that leave me with a JSON string? 
~~Also good when you only have a similarity matrix.~~ Sorry, I was thinking of k-medoids.
you still have to parse the string. actually, in your example, you are using `json.load` but really you want `json.loads` which will parse a string (and not read from a file). 
Yeah! The exception hierarchy at the bottom of the page can be a super-handy quick reference.
I don't remember how I got started. But with Django you really can't *start* with learning how to bring a website up and having it run. I had to start with getting a webserver running. Once that was up had to get it to work with WSGI. Then had to have stuff resolve to Django. After that you really need to learn how the ORM is set up and understand how models are configured. Then after that I found that visualizing the flow helped me understand what I needed to change to get the result I wanted. Basically the url is resolved, then the url is redirected to a function to handle it, then the function sends it over to a template, then the rendered template is sent to the end user. I'm no genius and it took me a good long while to finally start grasping the framework. So anyways, that probably isn't too helpful. But I just wanted to let you know that you're not alone. Django is a big hairy powerful beast and being lost when you first start using it is normal. But if you keep at it I promise it's worth it.
Thank you, so much! I have been trying to figure this out for days. It was that simple. :S Thanks!
It ain't the speed that bothers me. I would honestly rather subprocess imagemagik instead of dealing with PIL's API ever again x.x
Django Girls have a great tutorial as well.
For 3D animation with Python, see also: https://github.com/OpenChemistry/tomviz
Because it's a preforking server. It would be a waste to make it do all the work.
Awesome! Thanks for sharing I'll listen to it :)
This should not be that hard, since html files are basically text files and Python is really easy for IO (once you get the hang of it). The easiest way would be to use [requests](http://docs.python-requests.org/en/master/) import requests r = requests.get(the_url) with open('index.html', 'w') as f: f.write(r.text) f.close() If you are using Python 2.7 (since you said you are learning I presume you are not using 2.6 or older), you might run into unicode issues, since UTF-8 is common in the web. Just use `r.text.encode('utf-8')` and that should do the trick. This however, will not be a problem with BeautilSoup and the `prettify` method. import requests from bs4 import BeautifulSoup r = requests.get(the_url) soup = BeautifulSoup(r.text, 'html.parser') # Use lxml if possible with open('index.html', 'w') as f: f.write(soup.prettify('utf-8')) f.close() To change the content, you would use BeautifulSoup to find the element you want to change, change it and then perform the file output. Say you want to change the `title`. (before with open..) title = soup.find('title') title.string = 'A new title' (proceed with the output) If you wanted to change a class in a `div` with `id='some-id'`: div = soup.find('div', id='some-id') div['class'] = 'cards' Hope it helps. Enjoy! 
Not sure I fully understand, but here is how you can do the first part of what you asked which is setting a value on a numpy array (if you aren't using numpy, you probably should be): my_data[:, 20:30] = new_value This would set columns 20 (inclusive) to 30 (exclusive) for every experiment/row to the value `new_value`. If you know what the fill value is you could also use boolean masks by doing: my_data[my_data == fill_value] = new_value Those are the simplest ways of doing what I think you are trying to do without learning new functions. I'm not sure how much I can help with the interpolation stuff without more specifics, but I highly suggest asking questions on /r/learnpython. But if this is a college/university physics course that told you to use python then I suggest talking to your professor or TA as they would understand exactly what you are doing.
**That IS being judgmental**. Saying that you're doomed (ie. have been and always will be manic, depressed, etc) is definitely judgemental and condescending. Casting mental health sufferers as uncategorically victims instead of agents is endemic in the mental health system, and encourages them to think they have no real hope to have a better life, only to manage their symptoms with medication. (Your comment about the rain seems to suggest you don't understand the difference between 1. weakening your body's defenses and 2. being directly exposed to infectious agents. This is directly analogous: overworking tends to weaken your mental health, it doesn't immediately **make** you have a manic, depressive, etc episode.)
What format are you wanting? Notebooks can be turned into self contained html, no Web server needed, and you can choose to hide all code before generating if you want. There's plenty of ways to save out images of your work to import elsewhere, such as sphinx, latex, word, or whatever. The reportlab project can generate pdfs, too. Serving up static sites informally is easy in python anyway, just use python -m SimpleHTTPServer in the directory of your choice. 
Also xlwings.org, though I haven't used either.
Well actually threading is not totally useless when it come to IO bound but not CPU bound tasks (concurrency but not parallel). If processes are used in those cases, the burden of manageing shared state, IPC, and memory consumption (imagine hundreds or thousands of processes) might not be worth it for task that doesn't actually require the power of multi-core cpu. What I don't understand was the Async IO part.
Actually, with newer versions of gunicorn you can do just that. It can be started as root and set to drop to another user after binding to the port, it can serve SSL, it can use a gevent/greenlet threading model instead of preforking, etc. It is in fact a complete app server. There are two reasons to put Nginx in front of gunicorn: 1. You want to combine multiple microservices, which you run under gunicorn on one or more hosts, with load balancing and whatnot, and serve them under a single host/port. 2. You serve static content, which you are better off serving with Nginx.
The source you are quoting is 5 years old. Gunicorn can use gevent/greenlet workers which do not suffer from the same problems as the original author claims. There is also [SSL](http://docs.gunicorn.org/en/latest/settings.html#ssl) support.
Quite a lot of people still use prefork gunicorn because threading can cause subtle errors in your application stack if you didn't design with that in mind. And if you restrict the # of threads that gunicorn uses, you're right back to having the same issues as prefork.
http://www.reportlab.com/?
I felt like doing this in the interpreter while watching Arrow. &gt;&gt;&gt; name = "amstan" &gt;&gt;&gt; frame='#' &gt;&gt;&gt; fill='.' &gt;&gt;&gt; top=[frame]*(len(name)+4) &gt;&gt;&gt; bottom=[frame]*(len(name)+4) &gt;&gt;&gt; rows=[top]+[[frame]+[fill]*(len(name)+2)+[frame] for n in range(len(name)+2)]+[bottom] &gt;&gt;&gt; for i,c in enumerate(name): ... rows[i+2][i+2]=c ... rows[len(name)-i+1][i+2]=c ... &gt;&gt;&gt; pprint.pprint(rows) [['#', '#', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', '.', '.', '.', '.', '.', '.', '.', '.', '#'], ['#', '.', 'a', '.', '.', '.', '.', 'n', '.', '#'], ['#', '.', '.', 'm', '.', '.', 'a', '.', '.', '#'], ['#', '.', '.', '.', 's', 't', '.', '.', '.', '#'], ['#', '.', '.', '.', 's', 't', '.', '.', '.', '#'], ['#', '.', '.', 'm', '.', '.', 'a', '.', '.', '#'], ['#', '.', 'a', '.', '.', '.', '.', 'n', '.', '#'], ['#', '.', '.', '.', '.', '.', '.', '.', '.', '#'], ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#']] &gt;&gt;&gt; print('\n'.join(' '.join(row) for row in rows)) # # # # # # # # # # # . . . . . . . . # # . a . . . . n . # # . . m . . a . . # # . . . s t . . . # # . . . s t . . . # # . . m . . a . . # # . a . . . . n . # # . . . . . . . . # # # # # # # # # # #
ok then, how about nuitka?
Eh...looking at the description, it's not. Just switches between devices. 
OP, note the syntax above is numpy-specific. It won't work in standard Python.
I suppose this might be caused by caching. What happens if you process a different image with each resize?
For something this specific you might need to use the win32 api. Try googling without the "python" keyword.
/u/Speshul__K has been spamming /r/Python with these "Challenges" for a couple weeks now. Must not be doing well in his Python course in school...
You can look at /r/dailyprogrammer for inspiration on challenges. You can include studies from other topics. For instance, I made a tic-tac-toe in python with a real opponent, so I had to learn minimax trees to have the computer know what to do next. If you have a raspberry pi, you could make an in-home security system. It reads the temp, has a camera, and detects when you're phone is on the home wifi. Those are some simple projects I've done in my spare time to get the creative gears moving.
I recommend generating html and converting it to a PDF. HTML is flexible and almost everyone knows how to use it. There are beauitful templates. If you want images, use matplotlib, or any plotting library to plot your images, and then embed them in the html. For python there is pdfkit, or use a linux command line program to do the conversion. Obviously html templates, and css fameworks can be used to make it pretty. There are also html templates out there for reports, but I've personally found once you develop a template and use ANY templating language in python (like Mako) to generate the reports, and have the conversion to PDF automated, you can also automate emailing, it and keeping an index of the reports on a simple web interface! - EASY!!!!
 import numpy as np import cv2 import time for i in xrange(5): num = 10 ** i ts = time.time() for _ in xrange(num): img = np.random.random((1000, 1000, 3)) cv2.resize(img, (100, 100)) print num, time.time() - ts results: 1 **0.806999921799** 10 0.384000062943 100 3.80099987984 1000 38.4059998989
Pandas. Easy as hell.
Don't forget about the query() function! I don't know if it works with column names that have spaces in them though. Here's an example for if they have underscores instead: import pandas as pd marks = pd.read_csv('marks.csv', index_col='student') marks.query("question_1 == 'y' &amp; question_2 == 'n').question_4
The protocol you use (sending directly json, without any header or whatever) will break later if there is any issue or lag. `recv` return any received data, but it can be just one byte. You need to continue reading until you get your json data blob. But you can't know how much to read because you didn't send the length of the data in the first place. `send` might not send all the data. Use `sendall`. Your code will be safer if you do: import socket import json import struct def read_blob(sock, size): buf = "" while len(buf) != size: ret = sock.recv(size - len(buf)) if not ret: raise Exception("Socket closed") ret += buf return buf def read_long(sock): size = struct.calcsize("L") data = readblob(sock, size) return struct.unpack("L", data) serverSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) host = socket.gethostname() port = 5000 buffer_size = 4096 serverSocket.bind((host, port)) serverSocket.listen(10) print("Listening on %s:%s..." % (host, str(port))) while True: clientSocket, address = serverSocket.accept() # read data size first, then the whole data and decode as json datasize = read_long(clientSocket) data = read_blob(clientSocket, datasize) jdata = json.load(data.decode('utf-8')) print("Connection received from %s..." % str(address)) clientSocket.sendall(struct.pack("L", len(jdata))) clientSocket.sendall(jdata) clientSocket.close() 
Thank you for the comments. Certainly issues associated with value of range is not a particular subject of datetme. In my case, I have been often to handle a range of the datetime. So, I thought trying to create to this class. 
Hey, point me at your website that's got Gunicorn handling incoming requests directly.
Very promising. Our best hope right now. Still, it's not "push button" =&gt; package like in go. 
&gt; in an afternoon. if it was, we would already have it. The thing is, mymy is not finished yet. The API is moving, part of the language is not supported yet, and big features are still being added. It's not bad thing, it's just a young project.
In my experience threads in Python work quite well for I/O bound activities; I/O threads spend more time waiting for disk or network than for the GIL, so the GIL is not a big hindrance. You can easily get satisfing performance and, if you're confident with threads and you know how to avoid deadlocks and race conditions, it is a good choice. I wrote dozens of threaded Python programs and I'm quite happy with them. But if you need to squeeze everything for a single Python process in I/O bound asynchronous programming is the way to go. Give a look at curio library by David Beazley: https://github.com/dabeaz/curio . It has a more natural approach to the async programming than the async module (IMHO).
Thanks for writing this. It is very honest and eloquent. And thanks for writing requests. It powers a lot of my automated tasks. Please proceed happily living life.
I used a [SAX parser](https://docs.python.org/3/library/xml.sax.html) to parse that document into a database for a university project few years ago. I don't know if it will fit into a dict or DataFrame, though.
This is probably what you want: https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse
Yeah I am already using this, but my computer can't handle it.
&gt; database for a university project few years ago. I don't know if it will fit into a dict or DataFrame, though. Will take a look, thanks.
You seem to be aggregating all of the results of this parsing, so the resulting dict is probably getting too huge. For this, I would generally create a generator instead. For instance, instead of this: title_authors[title.text] = authors2 You might wrap the whole thing in a function and: yield (title.text, authors2) Then you could process just this result in the receiving function somehow without waiting for the entire file to be processed. Alternatively if you need to do something with the full list after this, you could simply break down the process into this script which pulls out the data and writes it to a file. Then create a subsequent script to do whatever processing you need on the parsed version. Creating two separate scripts isn't necessary for that case, but it can sometimes make it easier to see which part of your logic is really the bottleneck in situations where you're running out of resources like this.
Between this and the unsolicited admission that the OP needs work, you sound like a great guy!
Yeah it looks like an assignment. Hence my vague answer
I've done this for a project, though I'm afraid I can't release any of the source for it. The best I can do is point you at this [thread](https://sourceforge.net/p/ctypes/mailman/ctypes-users/thread/481A4946.1060907@telus.net/) where someone shared how they did it with WinMM: [wavevol.py](http://sourceforge.net/p/ctypes/mailman/attachment/481A4946.1060907%40telus.net/1/). I did it this way because I needed to support Windows XP. It's likely that there is a simpler way on more recent versions of Windows, but the approach would be similar.
Autodidactic learning -- doing stuff on your own -- can be really helpful for getting the basics down, but there's a certain amount of structure and gap-filling that's best done with help from someone else. Obviously, one such option is to take a course or take some time with an experienced programmer who can point out how to improve what you're doing. That's super helpful, but courses take time and money, and it can be hard to find someone interested in helping you *pro bono*. Thankfully, you're in a community of people who like Python -- you might want to ask around in /r/learnpython to find someone who can help you out. Another option is to consider finding some other people interested in learning alongside you who can meet with you on a semi-regular basis; you can share your experiences with them, they can do the same, and you can even start working on projects together (team projects are always great experience)! Finally, you can consider using Python to solve real-world problems you think about. I started learning Python to answer a simple question about the optimal way to play 31 (aka Blitz); in learning to do that, I had to become a much better programmer than I was. Designing and completing a project of your own is way different than following someone else's project guidelines, and it'll teach you a lot of things you'll never get by following guides. Hope this helps!
You need to set 'PYTHONPATH' env variable to the location of backend folder. Its to tell python where external libraries are located.
Thank you for your reply, It was incredibly helpful. I decided to follow your advice https://www.reddit.com/r/learnpython/comments/48am3v/looking_for_a_mentor_or_if_anyone_is_interested/
Store the tweets in a database, then iterate through the database.
Came here to say this. Dictionaries are already reasonably optimised in Python. OP is trying to use the wrong tool for the job.
I actually like the music. Makes it feel epic. that said, a voice over would be nice.
By database I hope you are suggesting [libdbh](https://www.gnu.org/software/libdbh/) , [leveldb](https://github.com/google/leveldb) or [rocksdb](http://rocksdb.org/). One doesn't need to use a relation out-of-process database to support workloads larger than main memory. Often the most expedient thing is to just use a larger machine. See my comment on using an EC2 spot instance.
+1 for splitting parsing and aggregation. Profiling in general could help finding out what is ballooning. If indeed it's an issue with parsing, you could try farming that out to a separate utility, for example a C-based XPath interpreter, which could give you a result you can pipe straight to a database or to a file.
I actually managed to solve it after a lot of trial and error. It turned out I had added the tesseract.exe to path rather than the folder. It's all sorted now. Thanks for the offer of help though.
Pandas let's you index rows and columns of a table using arbitrary values. So you could make a word co-occurence matrix with words as the row and column labels (indices) and numberic values in the table. Would be fairly memory efficient except for the fact that I'm guessing most word pairs have a co-occurence of zero. How many words do you have total? Scipy has some sort of sparse matrix data object that might be better. However, I don't think you could use that with Pandas (though I might be wrong here). Still you could use dicts to map words to indices and then indices to access array elements. 
This might be more appropriate for /r/learnpython, if you're learning the language. But the tool requires Python 2.7, per its installation instructions. If you read the error, it's in the way the `print` function is called. Python 3 requires parentheses around the argument. Python 2 does not.
In [6]: type(b'\x00\x00\x00\xaa'[0]) Out[6]: int In [7]: type(b'\x00\x00\x00\xaa'[0:1]) Out[7]: bytes first version returns one item, second returns byte sequence (containing one item). Its not the same thing.
A bytes object is a low level collection containing a sequence of numbers, because that's what a byte is, a number. This object is close to the data structures we use behind the scene to represent most things in computing. What makes bytes special is that bytes, by essence, don't have any meaning unless you know the format used. You can represent images, texte, more complex numbers, etc. And bytes are agnostic in that regards, they don't tell you what they are, they are just an array of numbers. So, a slice of a bytes return a smaller bytes object, while an indexing on a bytes return the number at this index. Simple as that, because that's what bytes are for. However, human use bytes a lot to represent text, so bytes objects are represented as ASCII strings in the terminal, so you can easily spot numbers that are supposed to represent characters. It's just a convenient formatting in the terminal: something you see stuff that looks like letters in your bytes and it gives you a clue of what to do. But it's irrelevant of the real content of the byte: it is sill a sequence of numbers. The bug you see here is a pedagogic bug : the fact that now, many people can learn programming without knowing what a byte is and that Python make it easy to create great things without needing to learn these details. So almost all tutorials never explain in details bytes, and eventually this catches up everybody. Don't feel bad though, nobody is supposed to know that without being told.
Also yeoman can be used to scaffold projects. Might be worth checking out, but overkill for your needs maybe.
No comments, no explanation, spaghetti code in a gist. What why and how the hell should people give a shit about this?
If you could have any one food for the rest of your life, what would it be and why is it spaghetti?
http://lucumr.pocoo.org/2014/5/12/everything-about-unicode/ Python 3 has all sorts of fun unicode nonsense. Some people will tell you it's a feature.
It definitely doesn't behave like strings do, but it does behave like a list: &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; a[0] 1 &gt;&gt;&gt; a[0:1] [1]
This is a python FAIL.... Now go repent and do it in python not BASH.... Really, I am disappoint!
I don't have a complete solution but here's a stochastic one: def getGooglePlusCommunityMembers(community): return []
Just ... Perfect
I recommend you just take a look at Bottle, which is a very simple web framework. 
&gt; . We want to add it, but we haven't done it yet. Looking forward to that, since I don't want to learn JS :) 
Essentials of socket programming precede essentials of Python socket programming. This tutorial seems to leapfrog these essentials and go straight into coding. If one doesn't understand a basic TCP state machine, how's he gonna debug his socket program, regardless of programming language?
Challenge accepted! Here is some -ugly- python code that does the same: import os import sys def mkapp(app_name): dirs = [app_name, '{}/static'.format(app_name), '{}/static/css'.format(app_name), '{}/static/js'.format(app_name) , '{}/static/img'.format(app_name), '{}/templates'.format(app_name)] for d in dirs: os.mkdir(d) app = sys.argv[1] mkapp(app) in command line: ./mkapp.py myapp Can you give me a code review :)
They're removed but fields are freezed for some reasons. Better to get on with it, monty python style... 😎
The way you test it is wrong. Test should have an input and loop on it. Also ipython times it in current run, for the desired test, you should run it in different runs. Make that function a shell script that requires a number, time it with time lib and as I said put test in a loop. Then run that script. Clean run is the key, not consecutive. 
Please review Python-relevance before posting.
Any opinions on using pyqtgraph for this?
The hinting API (https://www.python.org/dev/peps/pep-0484/) isn't moving except for allowing new syntax / data model constructs, which is about as unmoving as possible, and this includes 3.5's syntax already (although not in a stable release) open_connection( HOST:Optional[str], PORT:Optional[int]) -&gt; Awaitable[Tuple[Transport,Protocol]] # as of 3.6 should be the syntax if I'm not mistaken (and it makes sense for a late supporting of it, considering the asyncio syntax and library are still technically provisionally included) So, mypy itself isn't really holding up doc generation or the such, its just no one has gotten around to it. - - - Speaking of, it shouldn't take too much effort: do you have any systems that you'd like to see mypy integration for? I might try putting some together this weekend
flake8, sphinx and sublime text for me.
I agree. Valid reasons might include: the GIL (ugh), Tk being the standard gui toolkit (ugh), the python2/3 split (mostly behind us now), or the awkwardness that is python on android or iOS (ugh). That said, they pros usually outweigh the cons.
Anaconda comes with an IDE called Spyder which is very useful and easy to learn (although I have found it glitchy from time to time). As another option, I suggest that, if in a Windows environment, you download a free copy of Notepad++ and then make sure you add all of the extensions that are designed to be used with Python. When you create your file and save it with a .py extension then Notepad++ knows that it is a Python file. You actually run the program in Windows command prompt, which you may want to append to your taskbar for frequent use. Once in command prompt go to the directory where Notepad++ is saving your .py file by using the cd command (say it is "newprogs": C:\Users\Trader&gt; cd newprogs and then type in the name of your new python program, hit enter and it will run (assuming program name trial1): C:\Users\Trader\Newprogs&gt; trial1.py If you need to modify, go back to Notepad++, make changes, save changes, then hit &lt;Up Arrow&gt; in Command Prompt (which repeats your last command) and run again, over and over. Takes a little getting used to but is a very fast and convenient way to program in Python. But Spyder is good too. Hope this was helpful. 
&gt;In each of the rows there 2 ranges of indexes of which contain invalid data points and I would like to find a way to replace them. I want to be able to use the the rest of the data in each row to predict what these values should be and replace them with a more appropriate value. Most of the posts so far have just been talking about how to manipulate data in very simplistic ways. The actual name of the problem you are trying to solve is missing data impution. This is an exceptionally difficult problem and has numerous potential solutions. Your use of the word "predict" is particularly problematic. You can fill with a hardcoded value or perform some simple inpution such as mean/median, but "predicting" the missing values via regression will require additional constraints given that you have more columns than rows (aka more DOF than samples).
Very helpful! thank you very much for your explanation, this clarified alot. I greatly appreciate you taking the time to break this down for me
&gt; the GIL (ugh) Looooooool. Literally only amateurs and hobbyists see the GIL as a problem. If you're doing computationally intensive threaded work, you shouldn't be doing it in Python. Drop to C. &gt;Tk being the standard gui toolkit (ugh) How is this a problem? I've literally never used tk. I started off with GTK, moved to wx, and have been there ever since. It's a choice. &gt;the python2/3 split (mostly behind us now) Yeah that was pretty bad. &gt;awkwardness that is python on android or iOS (ugh) I'm tempted to say "fuck mobile", but isn't buildozer pretty sweet? I followed the instructions and had a kivy app on my tablet in under an hour from starting cold. What's the big deal?
Will there be an API for warehouse.python.org? It currently has updated download counts for today, whereas the PyPi site lists 0.
I have absolutely zero idea how you are "disagreeing" with me about using MySQL. I simply did not suggest a specific database to use because it depends on the operation. I even personally use MySQL for my sentiment analysis site.
Ah yeah, that's totally fair. I guess it'd be more accurate to say that I'd make a stronger statement than you, that for a first time project just chuck it in SQL and not worry about it. For the general case, yeah you're definitely right that it's use-specific.
Another option for density based clustering is HDBSCAN* a more advanced approach similar to DBSCAN from some of the original authors, and Robust Single Linkage from Chaudhuri and Dasgupta (who described Level Set Tree clustering in the same paper). You can get both of those from the [hdbscan library](https://github.com/lmcinnes/hdbscan). There's a [description of how HDBSCAN* works](http://nbviewer.jupyter.org/github/lmcinnes/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb) available, but it will be probably easier to follow once you've read the linked article here, as it builds on similar concepts. There's also hope for a clustering method based on topological data analysis, in particular the Multi-Parameter Hierarchical Clustering of Carlsson and Memoli looks very promising theoretically, as long as you can get multidimensional persistent homology to work.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
nvidia has a fair bit of documentation on the subject: https://developer.nvidia.com/how-to-cuda-python
Well, the GIL isn't really a problem if you bring in libraries like openmp optimised numpy, yep. Or use PyQt to handle your threads. Or any number of other solutions. But the fact that it requires an external solution is a shortcoming. It is a shortcoming in the language when the solution is to not use the language. As for using gtk to solve tk? Well, once again, the solution is to bring in something external. The fact that this is required could be considered a shortcoming, particularly for folks coming from Java. Getting hello world up and running on mobile is easy. But getting numpy or pyqt running... or anything that requires external dependencies outside of what kivy has recipes for, well, you see how this is a problem. Now we can't bring in those external libraries we're used to using to deal with all those other shortcomings. So we need to reinvent the wheel again. Anyway, I love python. I use it because it suits my purposes more often than not. But it isn't perfect either. Fortunately it's open source so shmucks like me can contribute.
While I do recognize a lot of /r/Python subscribers are interested in Raspberry Pi, I don't think it's really that appropriate for a Python subreddit. I see it this way: if Dell released a new laptop, a post here would probably be removed by the mods, even though that laptop is perfectly capable of running Python. The Pi is a great tool to learn computing with, and it's a lot of fun to use (I have 3 myself), but if Synology released a new NAS device and somebody posted a press release it'd be declared irrelevant - but you can run Python there, too. I mean Python runs on so many platforms, this sub would get overrun if there was a post each time a new device came out! :)
The difference being that it's (the pi) a teaching/education platform that fairly significantly advances python availability and exposure. The others simply being "yeah, python runs there too". It's one thing if the sub is littered with rubbish, it's another when there is a singular post you personally find objectionable.
/r/electronics or similar, sure. Should this also be in C/C++/node.js/LAMP/etc subs? I don't think so. It's a general purpose compute platform, like the many dozen others that are out there. You can run Python on an Arduino, I don't want to see every Arduino board release here. /r/Python is for Python, not compute platforms, even if they do run Python.
It does that, but in the same way it advances C/C++ availability, Lua availability, and so on because it's really just a general-purpose computing platform. In fact, a common misconception I see pop up on reddit is that the Raspberry Pi is some kind of Python-only appliance. Or people who already have a computer decide they want to learn Python, so they go buy a Raspberry Pi. I mostly feel bad for the OP of this thread getting a ton of down votes for what I think is a pretty good point (though maybe he didn't do a good job expressing it) - it's a great, cheap computer, but I think it's as relevant to Python as any other computer is. And it drives me nuts when people use the down vote as a disagree button 
The numba library from Continuum Analytics looks pretty good: http://numba.pydata.org/
You should add examples to the readme, otherwise it's useless 
They are easy to get here in the USA. Just bought a pallet and am now waiting for confirmation on my stock date as it looks like I need to wait till the second week of march to get them all - but they are available..
Holy crap that's awesome.
if you can say, what are you going to do with a pallet of raspberry pis?
 import time def fib(x): if x&lt;2:return x else: return fib(x-1)+fib(x-2) i=time.time() print fib(32) print time.time()-i Raspberry Pi B = 24.445 My Cellphone (samsung xcover2) = 6.47 My Desktop (5 years old) = 1.292 What does the Pi 3 say? 
Assign the generator to a variable: f = fibonacci() Then, iterate over f or call next(f) for the series. For minor shenanigans, do (but first make sure your terminal history limit is large enough): for i in f: print(i)
It does work. Just create one like `f = fibonacci()` and then get the next item from it just like any other generator, `next(f)`.
yes! thankyou :) right now I've just found the answer too, but thanks anyway!
I don't know how to apply this next(f) you're proposing, so I used a for function. Thankyou anyway! :D
In other words: Pis are pis of shit. If you buy 5 years old cellphone for $30 and chroot Linux on that, you will ride 4 times faster and get 3G, Wifi and Display for free. 
Tell people on reddit about it
i want to buy maybe 2 or 3 of these things with btc. any one keen to sell me a couple and ship them to New Zealand? u/honestduane
Yes thats what I meant, i think for a first approach it is easier than django and you can stick with it for large projects too, thanks -partly- to its plugins (like sqlalchemy or login or admin)
1GB
Always upvote numba. It's a real gem of the python scientific stack. I had a little experience with numba+cuda and was impressed by how well it worked. The numba code was even faster than my CUDA-C version (although to be fair, I might just be a shitty CUDA-C programmer).
That really is not the point of the project. It's more like oh hey lets make a credit card sized computer and sell it for $20.
It gets the nth number in the [Fibonacci sequence](https://en.wikipedia.org/wiki/Fibonacci_number) using [Recursion](https://en.wikipedia.org/wiki/Recursion). The `x&lt;2` is to return the base cases x=0 and x=1 (first two numbers of the sequence). Otherwise it returns the sum of the previous 2 numbers in the sequence. 
Thanks for that. But why is it used here? Is it just to see how fast a computer can compute that function? 
Yep. This version of the fibbonacci function is actually very inefficient since it doesn't use [memoization](https://en.wikipedia.org/wiki/Memoization), making it a _very_ simple cpu benchmark. A proper benchmark would run the above code hundreds or thousands of times and take the average to make sure that other factors weren't affecting the benchmar. Eg: memory allocation or the thread having to wait during execution.
Naturally, [he's a leet hacker](http://i.imgur.com/iVHfwLc.gifv?noredirect)
Soon enough this toy will have more RAM than my PC. :(
Generally speaking, you learn CUDA while coding in C. Understanding the topology of the GPU and how to design your code to take advantage of it (linear algebra thinking with blocks/warps/threads) isn't terribly difficult to wrap your head around for basic problems - but that's not the sticking point. The sticking point is understanding host/device communication and transfer, and realizing that unless you're pedantic about how you design your algorithms, you won't necessarily see *any speedup whatsoever* if you're inefficiently transferring data between the host and device. Thus, because CUDA in C requires you to get very low-level managing these details, I think it's a better way to dive into the world of GPGPU. From a Python perspective, you may want to instead consider taking advantage of tools which automatically generate CUDA code - potentially with optimizations already made for you. [Theano](http://deeplearning.net/software/theano/tutorial/using_gpu.html) comes to mind, although several "deep learning" toolkits have similar functionality. In the C++ world, there's an excellent library called [thrust](http://docs.nvidia.com/cuda/thrust/index.html), but it does not really have any Python equivalent. Some libraries, like [numba](http://numba.pydata.org/) and [PyCUDA](https://mathema.tician.de/software/pycuda/), offer array-like objects which let you simulate CUDA-C workflows in Python. But I've found their performance to not really be worth the hassle unless you're going back to natively writing CUDA-C kernels.
You're gonna need to use win32com or straight WinDLL stuff in ctypes. I think you'd want to start [here](https://msdn.microsoft.com/en-us/library/windows/desktop/dd370812(v=vs.85).aspx). Prepare to have a lot of fun with this.
Wait, how would you use memoization in this case? 
xu4 is 3.188 for reference. 
There's an interview TechSpot did with Eben Upton (founder of the rPi) in which Upton explains the meaning of the name: &gt;**TS: Where does the name Raspberry Pi come from?** &gt; &gt;Raspberry is a reference to a fruit naming tradition in the old days of microcomputers. A lot of computer companies were named after fruit. There's Tangerine Computer Systems, Apricot Computers, and the old British company Acorn, which is a family of fruit. &gt;Pi is because originally we were going to produce a computer that could only really run Python. So the Pi in there is for Python. Now you can run Python on the Raspberry Pi but the design we ended up going with is much more capable than the original we thought of, so it's kind of outlived its name a little bit. &gt; &gt;http://www.techspot.com/article/531-eben-upton-interview/ There's also an O'Reilly e-book called Python in Education that touches on it in the rPi case study starting on page 7 (of the book, not the PDF): http://www.cs.montana.edu/~sdowdle/Python_In_Education-April2015.pdf 
This is the first link that popped up for me, but it does a really good job of explaining the process: http://ujihisa.blogspot.com/2010/11/memoized-recursive-fibonacci-in-python.html?m=1 Memoization can be used a lot of places: I recently used it at work to create a resettable cache for certificate tree generation, for instance. 
Yeah that's what I thought, but the initial run will still take longer, no? So in this case (using it as a benchmark) this does not really help much.
ha thanks! that has been a while and I'm still working on it ;) 
You're the writer? :)
It's not at all useless. If you want to learn if your herb collection or baby collection are safe, learn some damn source code. Here's a readme. Play with the lighting. Play with the threshold of the coefficient. Send emails when its stable. Loosen up that bolt in your brain before it explodes. It's an example of Python that is useful to many. If you can't run it, reach out to the dev with a f'ing issue. Take your attitude to the Valley south.
Neat! I hadn't seen read these interviews before - today I learned. I'd still argue that since the Raspberry Pi is now a general-purpose machine it's as relevant to /r/Python as it is to /r/lua, /r/perl, /r/java, and so on. There was that original idea of being a Python-only appliance, but there's never been a Python-only public release of the Raspberry Pi - it's always been available as a general-purpose PC. I'm not saying it's not *of interest* to /r/Python, I just don't think /u/minus7 deserves all the downvotes he's getting. The /r/Python sidebar does describe the subreddit as "news about the dynamic, interpreted, interactive, object-oriented, extensible programming language Python," - as far as I can tell, a new revision of a Raspberry Pi doesn't exactly change the Python landscape a whole lot. It's about on par with a new Dell laptop coming out. I'm excited about getting a faster Raspberry Pi, but a faster Pi doesn't really affect the educational crowd. Students learning computer science could use a Raspberry Pi Model A and probably wouldn't notice a huge difference.
Yep , I usually write about the stuff I do when I get bored. I also livestreamed on livecoding.tv the whole analysis of the TV series and movies 
Cool! Is there a recording ?
They would need to go full retard there and pull a modern publisher. Just call it: The "New Raspberry Pi", not to be confused with the first generation "Raspberry Pi".
I've got some logic I threw together for analyzing books - haven't been able to tear myself away from the other stuff to code it up...very interesting to me because of that!
FAQ from the [announcement on the python-dev list](https://mail.python.org/pipermail/python-dev/2016-February/143441.html): Q: What is fuzzpy for? A: It's primarily for testing CPython itself, but could also be used for individual python projects too. Pure-python projects will be the simplest to integrate at this point. Also, interesting test cases output by fuzzpy may end up being useful in testing others such as pypy, pyston, etc. Q: What is a fuzz tester? A: It modifies inputs to a test case in order to find unique/rare failures. Q: What does "coverage-guided" mean? A: It means that libFuzzer is able to witness the specific code executed as a result of a given test case. It feeds this information back into an engine to modify the test cases to optimize for coverage. Q: How can I help? A1: donate cycles: build the project and crank away on one of the existing tests. Relative to other common fuzzing, it's awfully slow, so consider throwing as many cycles as you can afford to. A2: contribute tests: write a ~10-line python script that exercises a feature that you think could benefit from fuzz testing. A3: if there's interest, I can accept cryptocoin donations to purchase cycles on a cloud server. 
I understand what memoization is, you don't seem to understand what I mean though. These cached values don't come out of nowhere, you still have to run the algorithm at least once to calculate and store them, so that the next time you ask the program what number fib(32) is it will be able to pull it from the cache memory. So initially, the cache memory is empty. If you wanted to know what fib(32) is, you'd have to have run fib(30), fib(31) or fib(32) at least once before that for the calculations to actually be faster. __fib_cache = {} def fib(n): if n in __fib_cache: return __fib_cache[n] else: __fib_cache[n] = n if n &lt; 2 else fib(n-2) + fib(n-1) return __fib_cache[n] 
I love these guys!
Love your article makes the entire process seem simple and relatable. 
But that doesn't tell me what it does
The key insight about memoization is that for a given run of the algorithm, the subproblems will be calculated many times, even on the first run. You can see this with a program like this: #!/usr/bin/python import pprint __fib_count = {} def fib(n): if n not in __fib_count: __fib_count[n] = 0 __fib_count[n] += 1 return n if n &lt; 2 else fib(n-2) + fib(n-1) print fib(32) pp = pprint.PrettyPrinter(indent=4) pp.pprint(__fib_count) This program prints this output: 2178309 { 0: 1346269, 1: 2178309, 2: 1346269, 3: 832040, 4: 514229, 5: 317811, 6: 196418, 7: 121393, 8: 75025, 9: 46368, 10: 28657, 11: 17711, 12: 10946, 13: 6765, 14: 4181, 15: 2584, 16: 1597, 17: 987, 18: 610, 19: 377, 20: 233, 21: 144, 22: 89, 23: 55, 24: 34, 25: 21, 26: 13, 27: 8, 28: 5, 29: 3, 30: 2, 31: 1, 32: 1} So, even on the first run, you are saving many millions of calculations by looking up the sub-problems in the table.
Off course. org-mode is one of the leading reasons why anyone (including myself) moves to emacs from vim.
Really cool post. As a Penguins fan, it's been refreshing to see the team go from downright awful under the last months of Mike Johnston, to firing on all cylinders under Mike Sullivan recently. You might want to x-post to /r/hockey; while it's heavy on the technical side I'm sure there are many there who would enjoy the read. Thanks for sharing!
that it was considered as a mistake surprised me when I read this email.
That makes sense, thanks.
The best place to ask is on the ZAP User Group: https://groups.google.com/group/zaproxy-users Cheers, Simon (ZAP Project Lead)
I prefer using a decorator that set metadata. More concise, like pyramid or flask does for view configuration for exemple.
If they exist, change them. If they don't, make them. Google can tell you about Windows environment variables.
Use pd.merge() and do a left join where your left data frame is the larger data frame (df1 in my example below): import pandas ids = [1,2,3,4,5,6,7,8,9,10] values = [1,2,3,2,3,1,1,1,3,1] df1 = pd.DataFrame({'id1':ids, 'values1':values}) df2 = pd.DataFrame({'id2':[1,2,3], 'values2':[10,50,20]}) pd.merge(df1, df2, how='left', left_on='values1', right_on='id2') pandas [cheat sheet](http://nbviewer.jupyter.org/github/pybokeh/jupyter_notebooks/blob/master/pandas/PandasCheatSheet.ipynb) EDIT: [notebook gist](http://nbviewer.jupyter.org/gist/pybokeh/248071434e37b2186b6e)
Raspberry Pi 95
Looks quite useful! I noticed some naming inconsistencies on the readme like: &gt; term.black &gt; &gt; term.red &gt; &gt; term.Green. # random uppercase
Pythonanywhere might be a good idea. You could likely run this on their free tier. Very easy to get up and running especially with a Flask deployment.
Can't we still create a decorator that define the Meta class ?
This is typically a sign that you didn't install all the dependencies or follow the instructions to the letter. I would recommend trying the install again.
Being brutally honest? You could probably speed it up by writing a shell script. This is a shell operation. Also, as a protip: *never* delete your source files until after you've confirmed the backup was successful.
If you use PyQt or anything else that implements dbus, you can check for existing instances... This is only really useful if you're already using a library that uses dbus.
Hi, why do you need a GUI window to display a single variable? which kind of data you would like to show? however you don't need to *reload* the gui and you don't need to close and reopen the window. just set the content of a widget and it will be shown.
Just placed an order last night. can't wait. Does anyone know where to get a cheap case for this? Would any Raspberry pi 2 cases do?
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
Col, thanks for sharing!!! 
It has first-class functions, it has `map` and similar applicative methods built in, it has functools and itertools that give it pretty much all of the standard functional-programming concepts. That said, I wouldn't say that LISP actually provides all that much influence, beyond the fact that certain key concepts from LISP have become fundamental to modern programming languages. Also, this sounds like a homework question.
try and bind to some port
Yep. that's what I ended up deciding on. I wrapped it up in a "poor man's mutex" class.
This doesn't quite make sense to me. Usually code in an indent block still make sense on its own. Consider: while True: print 'hi' or: try: print 'hi' except: pass `print 'hi'` is valid code inside or outside of an indent. Code such as `time from time` isn't.
The usual.
bitpay?
You *could*, but that decorator would have to accept all the arguments that the `Meta` class accepts (or process a variable list of keyword arguments), and once you're doing that, is it really better than just defining a `Meta` class manually?
Eight cores clocked at 2ghz with 1gb of ram and Ethernet that still shares the USB bus? I can't wait!
You could try the libraries available in numpy. For example the least squares approach I believe used by solver can be found in numpy.linalg.lstsq Numpy has other solving methods too which you'll find by digging through the docs.
In addition to what others have said, python has a REPL (read-eval-print loop, aka a command-line interface), which I think was originally from lisp.
In the readme under usage "term.writeLine('Hello, Pyhton!')" Python is spelt wrong.
May I suggest you next time to use a Gist or alike to post the code even it's a single file. By the way, could you add a brief summary about what it is? All I know about a capacitor is from electronics.
Your term.py looks clean and lightweight, yet I'm left wondering what advantages does this have over something like Blessings/Blessed?
In terms of graph theory, you have an undirected graph where the teams are the nodes. A connection between nodes *i* and *j* represents a game pairing. A graph where each node has exactly *k* neighbors [is called a *k*-regular graph](https://en.wikipedia.org/wiki/Regular_graph). Such a graph exists iff *n &gt; k* with *nk* even. In your case *k* is odd, so *n* must be even. You can pad *n* out by one wildcard team if necessary, to be set in a pairing to any team that the actual team isn't already playing. There are different types of regular graphs, with different properties beyond just the number of neighbors for each node. As /u/turleyn suggested, if *k* is small enough you can break an even *n* up into two leagues and have each team play cyclic shifted teams in the opposing league. By some criteria, though, this doesn't mix up the pairings all that well. A circulant graph is a more symmetric regular graph (see the wheel-like diagrams [here](http://mathworld.wolfram.com/CirculantGraph.html)). The graph corresponds to an adjacency matrix which is formed by cyclic shifts (a circulant matrix). The matrix must also be symmetric, because the graph is required to be undirected. Here is a function to generate a symmetric circulant matrix. If the *(i,j)* element is 1 then team *i* plays team *j*. Those indices can index into your list of teams (and you can first randomize the list of teams to further mix things up). def gen_symmetric_circulant(n, k): if k * n % 2 != 0 or n &lt;= k: return None first_row = [0] * (n - 1) if k % 2 == 1: # Guarantee symmetric matrix. first_row[(n - 1) // 2] = 1 k -= 1 for i in range(k // 2): first_row[i] = 1 first_row[-i-1] = 1 first_row.insert(0, 0) # Diag is always unconnected. adjacent = [first_row] for i in range(n-1): new_row = adjacent[-1][:] new_row.insert(0, new_row.pop()) adjacent.append(new_row) return adjacent def print_mat(mat): for i in mat: print i mat = gen_symmetric_circulant(10, 3) print_mat(mat) 
I guess this means I'm meta Meta because I've been using it since 2014.
What would it be decorating?
The class itself: @Meta(...) class Record: ...
Well, the original comment I made was pointing out that the inner `Meta` pattern *predates the existence of class decorators in Python*. Hence I was confused by the "can't we still create a decorator" question, because the answer is "at the time, no, we literally couldn't because that was not something the language could do".
And no one likes editing Markdown links. Firefox + keysnail can bind a Javascript function, which grabs the title and url of the page you visit, construct a Markdonw link and save it in the clipboard, to a shortcut key. All you have to do is visit the page you want, press that shortcut key, and then paste the link in your editor.
For Vim users, there is Vimperator.
If you hated yourself you could always do this: import \ time,\ BeautifulSoup,\ re
thank you simon! I have watch a ton of your videos and am still new at the whole security applications. I will check out the link you sent me, basically my boss wants me to make a batch file that runs a quick scan on their site. Is making a python script the way to go or is that an easier way to do this. Basically all it would be is launching the batch file and calling the URL to do a scan on as well as running the spider scan on it. I think I am overthinking this too much in terms of writing a python script to do something simple as this? I have created a batch file that opens the zap application and was also wondering if vbscript might also work to do this on a windows OS. Edit: I actually posted the question about a bat file on the google groups. Figured it would be more fitting there
Ugh. I hate you just for suggesting it.
Whoa, touchy 
Most times this has been asked in the last 6 months or so, the response has been overwhelmingly in favour of the current and future version of the language. 
I made the switch about 2 years ago and use Python 3.x over 2.7 in 99% of the cases (unless there's a library compatibility issue). Nothing against Python 2.7, but I really like Python 3's new features. In any case, if it comes to the development of packages, I support both Python 3 and 2.7 (it's maybe a tad annoying sometimes but not overly tedious thanks to `__future__` imports). I really don't want to tell anyone which version they should use; that's why I support both. However, my tendency goes towards encoruaging people to use Py 3 (e.g., my code documentation is written in Py 3 as a subtle incentive).
Honestly I prefer the learning Python questions to this one. &gt;Long time python hacker here, and have been using 2.x for the last 5+ years. How many folks have made the jump to 3.x? I remember visiting this topic about two years ago, and the answer was still "Meh...". Just wanted to ping folks directly and see! It is absolutely asinine to stay on the 2.x series at this point. Why you are even asking at this point is beyond explanation. It is like a C++ programmer using a C++ compiler based on a ten year old design. 
Yes. I don't know a single person who uses bitcoin who either does not have a criminal record, or if they dint yet didn't do some shady shit to make money with it that probably would have given them one.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/btc] [apparently you need a criminal record to use bitcoin....](https://np.reddit.com/r/btc/comments/48ksor/apparently_you_need_a_criminal_record_to_use/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Windows 10 or Windows 3.1?
If you can, then 3. For years, I used to code 2 + 3, but I have stopped doing that and only do 3. There might be still cases that you just can't go 3, such as your dependencies, but it should be very rare now.
[I needed to have it so its more aligned like this](http://imgur.com/vFv7ykd)
its already on my system
What kind of GUI window are you using? You should be able to extend Dialog and create a Label that you can constantly update with the value on the fly. Perhaps you are using a QMessageBox which would be harder to update? 
I didn't know blessings exist at the beginning. I don't care actually. At the beginning I just wanted to give my terminal result just more color. but then it growth and I used it in other project. so I made a module out of it. if someone wants the same her is a good starting point. I already got some great issues that I fixed. So thanks for that. Maybe there are issues or function that doesn't exist yet. Fork it on GitHub and help me make it better. So there is no bigger plan behind it.
Matplotlib is not slow. You just need to reuse axes. http://bastibe.de/2013-05-30-speeding-up-matplotlib.html
I know. Python is wrong spelt 
Stay tuned, I will but some highlighting function into it I made lately. I used it for logfile output. Made it easier to read or whatever output you want so highlight
The % notation comes from C's printf(). Many languages have this or something similar. It's not particularly pretty but the advantage is that it comes natural to those coming from such languages. That said, I use .format() almost exclusively as well.
Strings have `.left(x)`, `.right(x)` and `.center(x)` methods which pad text with spaces to a given size. It's generally better though to use [string formatting](https://pyformat.info/#string_pad_align) to build the final string in one go: &gt;&gt;&gt; a, b, c = 1, 2, 3 &gt;&gt;&gt; '{:&gt;6} | {:^6} | {:&lt;6}'.format(a,b,c) ' 1 | 2 | 3 ' 
You should read about PEP8 and improve the code formatting. Also your "series" method is a SyntaxError. This code shouldn't work as it is now, even fixing the SyntaxError. Are you asking for help?
check this out! Is [highlighting](https://github.com/gravmatt/py-term#highlighting-text) some benefit for you ;)
I've been meaning to write something like this for quite a while, thanks!
Thank you! This solved the problem for me.
Setting the PYTHONPATH worked. But what I found out is that when I run the tests using discovery from the backend folder itself the paths seem to work out. How I end up running my tests in a bash script from the backend directory: # runtests #!/bin/bash SCRIPTPATH=`pwd` python -m unittest discover -s $SCRIPTPATH 
It's not required at all. But, as you said, makes sense. In general, python is about doing sensible things. If you're struggling with something in Python, take a step back and see if you're doing the most sensible thing you can think of.
Not all of twisted is ported to python 3
sweet! I'll check that out
By contrast, I *would* recommend Chaco above the other options. It is maintained (by Enthought). It is a very mature and stable codebase and has worked reliably for us in production for many years. It is explicitly designed for the sort of interactive plot that you describe. It works with both WX and PyQt. Chaco's concept of "interactor tools" allows you to develop custom widgets for interacting with your plots. Things like draggable cursors, ROI-boxes, selection widgets are all the sort of thing Chaco does well. It is true that Chaco's docs are poor. On the plus side, the codebase is easy enough to navigate. Chaco is 2D only (plus images / colormaps / contours). Chaco uses CPU anti-aliased rendering which turns out to be quite fast in practice. It uses the same "Antigrain" rendering engine as matplotlib. Their rendering quality is similar. If you need vector output, look elsewhere. Chaco's PDF / SVG rendering misscales fonts so all text looks wonky. A pity.
Did you manage to find something viable? I'm looking for the same thing, but for java. It's amazing how poorly supported syntax highlighting is in an IDE, compared to sublime. 
Any thoughts on why you'd choose this over IPython (`my_files =! ls`, etc)?
Or you could SSH into nethack. 
Well *that's* relevant!
https://www.python.org/dev/peps/pep-0008/
xcos, from Scilab, is completely open source. I remember there are others like that too.
Yes. Here is [the relevant search](https://pypi.python.org/pypi?:action=browse&amp;show=all&amp;c=8&amp;c=306).
&gt; Well that's relevant!
$$$$$$$ Also, maybe we'll see programmers at companies that are stuck on 2.7 contributing code back. If their employer let's them 
MySQL is a relational database server where one can store data in a format called Tables, and query said data using a language called SQL. For very trivial data a dictionary could suffice, provided you serialize the data to disk periodically, and reload the data whenever the server starts. As previously mentioned, dictionaries are held in memory and once the Python process exits all that information is gone. Also, if the data were ever to get complex, or it's required to perform some analysis of the data, then a dictionary becomes cumbersome and highly inefficient. This is where an RDBMS may come in handy, as it will store the data to disk for you, and allow you to perform complex queries of the data using the SQL language. Alternatively, if you wish to have a dictionary-like database, but with increased performance and persist to disk, then you may want to look at some "NoSQL" solutions like Redis or MongoDB (and plenty other implementations). Edit: Wording.
I would never say they are annoying - I'd rather say they may be short-sighted or under constraints.
That's kind of just arguing semantics though...short sighted people are annoying, and it's annoying to deal with constraints.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
/u/Agent_03 's title is misleading here. This isn't a comprehensive benchmark. This is just a comparison of two different Python HTTP clients. There's no comparison to "native code" nor any other network protocol tested besides HTTP.
&gt; These are exactly who you don't want to be dealing with, why give them more of a hold on you? Because when money is involved you can dictate a relationship wherein they can be worth your time. There's a lot of time wasters, and annoyances that I'm willing to deal with for $150/hour.
"Once users notice that cooler new toys are Python 3.4+ only they’ll want to upgrade." This is something that should have been happening years ago, rather than continuing to support 2.x and worse, the backporting of features to 2.x. I've yet to hear any of the core developers state that the lesson they've learned is that they should have been more strict rather than more accommodating though. Instead I hear Guido suggesting there'll never be a move like 3.0 again and 4.0 won't be like 3.0. :-(
YAY
I agree. I thought there would be tests of struct vs dict etc. 
pouaaaahhhhhh!!! I'm trying it right now ! Thanks man!!
Because that's a shitty lesson and isn't what anyone learned from this. Roadblocks to adoption are unacceptable if you want people to move forward. Period.
I hope /u/lcq92 has the storage part covered, there's no mention of files or persistence in the post. Just in case: if you don't use a database like MySQL, you will need to make sure that your data is saved to disk so it isn't lost if the server is ever shut down or crashes. Python has a bunch of tools for [Data Peristence](https://docs.python.org/2.7/library/persistence.html), and [shelve](https://docs.python.org/2.7/library/shelve.html) is nice and simple. If you want both the convenience of a database and the simplicity of flat files, take a look at [SQLite](https://www.sqlite.org/). Python has a [standard library module](https://docs.python.org/2.7/library/sqlite3.html) for it, too.
&gt; Well that's relevant! &gt; Well that's relevant!
&gt; if you feel it's not comprehensive enough, you're welcome to contribute additional benchmarks It's not comprehensive enough for the title you've created. You're also welcome to resubmit with a proper title... after all, you're the one editorializing here. If you're fishing for pull requests, there's better ways of going about it.
Actually, the version is not too relevant - I have used https://github.com/arokem/python-matlab-bridge before for all MATLAB versions, I did try the new built in python interface but found it much slower. you can spawn and drive matlab using the above code.
We both recognize that it is not a comprehensive test of all networking possibilities, because that is an impossible task. It *is* a illustrative example though, and *contributes something of value to the community.* I am sorry that you find this somehow misleading; your objection here seems largely pedantic or semantic to me. It seems we'll have to agree to disagree. Like I said, if you have something constructive to offer in the form of a PR, I'm willing to invest my personal time and money to re-run the benchmarks in AWS and do the data analysis and visualization. Edit: For the record, since people get twitchy about it: I don't know who downvoted your second comment, but it was not I.
Contribute to existing projects that are widely used. 
There's mechanize, but it's not really maintained. Honestly, you're probably better off with selenium.
&gt;Doesnt selenium require to have the java server active? No. 
So I haven't used plotly at all, apart from twiddling with their demos online. It seems like a decent library, however my impression of it when I first saw it was the following: 1. It wasn't free. "Plotting as a Service" is so far from the workflow model that I strive for that plotly was pretty much stillborn as far as I was concerned. Alternatively if you wanted to fork over tons of cash just for plotting, they offered enterprise on-premise installations. Again, no thank you. I gather that this situation has since changed, i.e. it's open source now or something, but... err... first impressions matter? 2. The functionality seems largely duplicated by Bokeh, at plotly seems a little bit canned, ala highcharts, rather than offering a low-level API like bokeh. 3. My feeling is that Continuum (the maintainers of Bokeh) have a bit more of a 'big picture' outlook than plotly. E.g. by providing a low level drawing API, and other stuff like their recent work with Datashader. By contrast, I always got the feeling that plotly was trying to get "the bases covered" with regard to types of charts and whatnot, so that they could gather as large a user base as possible and get one of those sweet Silicon Valley style exits. I'm probably not being very charitable towards plotly here, I'm sure they're nice people, but nevertheless that's the impression I got. Again, there's a good chance all of that is wrong and/or out of date. Caveat emptor.
&gt; that is an impossible task It's not impossible to compare "how much faster native code is vs. Python (network edition)". One simply needs to code up something like iperf in Python (already done in C) and run it side-by-side with its counterpart. &gt; contributes something of value to the community I've never made any claims of value or lack thereof. I'm picking nits about your title! &gt; your objection here seems largely... semantic Semantics is important when it comes to a highly technical field like computer networking or software development. &gt; if you have something constructive to offer I do, it's to refine your title. I don't write software for random folks on the Internet.
If they are really large files I think you could do each one as its own process using multiprocessing module. Threading wouldn't help because it's not i/o intensive
I'm not sure what you mean. Do you mean each .py file that you have open in your editor? Many editors will use a single window and give each file its own tab. If that's what you mean, try looking for a different editor. Unfortunately, I can't make any suggestions for a Mac, but PyCharm and Sublime Text are popular cross-platform editors for working in Python.
Work on open source projects. No software company is going to hire you at 15. At best you might get an internship.
`globals()` will return a dictionary containing the current scope's variables: &gt;&gt;&gt; from pprint import pprint &gt;&gt;&gt; import random &gt;&gt;&gt; x = random.random() &gt;&gt;&gt; pprint(globals()) {'__builtins__': &lt;module 'builtins' (built-in)&gt;, '__doc__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__name__': '__main__', '__package__': None, '__spec__': None, 'pprint': &lt;function pprint at 0x7f71e8b03840&gt;, 'random': &lt;module 'random' from '/usr/lib64/python3.4/random.py'&gt;, 'x': 0.4374317250001212}
In the similar sense, I'd expect paid-only conversion of existing 2.x modules for 3.x demanding applicants, based on something like a crowdfunding campaign. See, the bitter pill of failed Py3 introduction is not yet quite swallowed. Interesting what would happen when EOL of Py2 get very close. I'd bet it get prolonged again, so it will last more then 4 years. There is so much of legacy code with no reasonable financial or technical benefits to do a conversion. From a personal point of view, contrary to declared improvements of CPython3 runtime, we didn't see anything like that in real world applications. For example our middle-size Django based project, written for both versions from the start, performs on par regardless which runtime is used. The only difference made recent PyPy in 2.7 compatibility version but eager to put it in production now. Hope the longstanding 2to3 issues and rise of popularity and financial investments in JavaScript and Go won't make Python less relevant in future.
https://github.com/jeanphix/Ghost.py combined with lxml, BeautifulSoup, etc.
Okay, you're going to make me dive into this then ;-) So Chaco isn't all bad. As you say, it's built from the ground up with interaction in mind. You can make pretty much any plot element interactive. That is good. It's probably more amenable to making complicated interactions than any other mature library right now. But... Docs. docsdocsdocsdocsDOCS! They are beyond 'poor'. In fact, if Chaco had good docs I would probably be able to forgive a lot of the other issues and might still use it. But they're not good. They are autogenerated sphinx tailings, and they contain close to zero information. There is next to no prose relating to *how* to use Chaco, rather than what Chaco is made of. People using a library need to, first and foremost, do just that: *use* it. It's important to eventually understand how things work so that you can built your software properly, as it stands you're told the chemical composition of every ingredient, and shown some nice pictures of cakes, when what you really wanted was somebody to tell you how much of each ingredient to add to the bowl and why. There are also a few PDFs and some Word docs (yes, .docx) kicking around which purport to explain Chaco's architecture, but they seem very outdated and still don't contain a ton of info. Better than nothing I suppose, but you *will* have to read the code. Speaking of the code... I agree, Chaco's code is indeed navigable. I've built the visualization components of a few in-house applications using Chaco, and I got through those projects by cursing a lot and reading a lot of code. However, the entire time I was constantly saying to myself, "Why the hell is this so hard?!", and "What the hell does this do?!". The reason is that Chaco's code contains a shitload of 'magic'. Unless you've fully drank the Enthought coolaid, you will not understand what Chaco is actually doing. It's built on an MVC framework, and it also implements and observer pattern for signaling. In the typical use case of appending a tool, creating a data object, or grabbing a frame buffer, you end up flying down code paths in Chaco, Atom, Traits, Traitsui, PyQt, Kiva, Enable, etc. And you probably don't know why, because the code can be very opaque to *intent*. Sure you see all the variables sitting there, but how are the connected up, how are you meant to use them, and when you take an action, where in the dependency tree will that action actually take place? It's very hard to know the impact of an action, especially if you're not particularly experienced and just want to make your plot update or whatever. I'll remind you that the OP originally posted because (no offense, OP) 'updating with matplotlib isn't fast'. If you can't figure out how to keep references to your axes and resuse them, you're probably gonna have a bad time with Chaco. Also, I mentioned that I made a few in-house apps using Chaco. I'm really glad they are in-house. As I mentioned above, Chaco can be a pretty huge dependency. Even if you're just trying to embed a canvas in your PyQt app (like OP), you get all that other crap anyhow and you have to figure out how to maintain it, unroll breakage, package it up, and deal with conflicts. In particular, packaging Chaco apps is a royal pain in the ass. Moreover, because it's built on things like Atom, you need to make your models using that framework. Oh, you don't know about the observer pattern? Too bad. The alternative is to create a layer that welds your ordinary python models onto the atom models, compartmentalizing the chaco stuff just for display purposes. That approach works, but also sucks. I also had the pleasure of trying to use Chaco in an app made using the Enaml GUI framework. Unfortunately Enaml has it's own Atom framework (due to the author having built it after leaivng Enthrought), and now you have Enaml Atoms, Chaco Atoms, two different observer loops, and similar but different APIs. Great. Speaking of APIs, the Chaco examples are riddled with breakages due to API changes that weren't backported into the examples. So step 1 is try to figure out how to do something, step 2 is find a related example, step 3 is *fix* the example, step 4 is try to write your own, step 5 is google in vain because nobody seems to use Chaco, and step 6 is use pyqtgraph.:-P
&gt; One simply needs to code up something like iperf in Python (already done in C) and run it side-by-side with its counterpart. That is actually something of an interesting notion and topical to other things I am working on, so let's explore further. iperf is a *wonderful* tool for its use case, and I was strongly considering including a measurement with that in the benchmarks to demonstrate raw network performance (and CPU requirements without the connection management overheads). There's two reasons I didn't: * One is that HTTP request performance starts to asymptotically approach this case as request sizes increases, and you can do some math over different request sizes to subtract the initial overheads for connection creation/management from transmission rate/raw network performance. This provides a solid proxy measurement. If you look closely at the benchmark, you can see that by 64k-128k request size, I was in fact saturating the ~520 MBit network link with just a single connection stream (the overhead per request became trivial). * Second, let's take a moment and consider what really goes into real-world network performance. Is it just the ability to dump packets on the pipe over a socket, or does connection management matter? In many cases, there's a trade-off between the two for real uses depending on request size. We also need to capture the responses to use them for something (you don't actually throw away the data). I think this second point is why using a real-world use case with HTTP is helpful here. It also offers a concrete example, and measures something relevant on its own to other developers. But is it truly comprehensive? No, and neither would be a pure-python iPerf. A comprehensive benchmark would be profiling applications with parallel native and python implementations, tested on multiple operating systems, different CPUs, and multiple network configurations. **It's impossible to truly do that in a comprehensive way though; any benchmark will be imperfect. They're just a helpful hint, real-world performance side-by-side with real data is the only true measurement.** Edit: one other point about iPerf - there are many network options that it allows you to tweak. Once you add those into the mix, the matrix of combinations to test quickly gets messy. On the other hand, writing a socket-to-socket network saturater similar to iPerf might be interesting on its own. In a different direction, one of my side projects will also probably end up offering something like ApacheBench in the next year, and I'm considering offering both pyCurl and requests (or Twisted) options for that, for a similar comparison across concurrent connections. It already supports gathering the metrics AB does (via pyCurl's native hooks), but is not presently structured to do the concurrent access with CurlMulti handles or Twisted's event-driven model.
Yeah, I usually just start at the beginning and try to work my way through the code from there. Keep a text file or flowchart with what you figure out if necessary. If dealing with a large application or codebase, I would highly recommend becoming familiar with a debugger as then you can just insert a breakpoint and step through the code while looking at variables, etc. For Python I use PyCharm as it has a great visual debugger. If the code runs on a remote system that can get trickier, but still possible. Personally, I'm not a big fan of comments. To me, comments should be reserved for giving context to some unusual syntax, language feature, a complex architectural pattern or a particular algorithm used. Operational flow of the program is best captured via logging. Strive for clarity in your code, don't compensate for bad code with comments. It's too easy for them to become out of date or misleading, and most often they are too vague or just reexplain what the code is doing. You will become better at reading other people's code as spend more time doing it. Also, if you have ever wondered why many developers seem to think everyone else is an idiot, it's because they have spent too much time reading other people's code that doesn't make any sense.
How would you do conditionals and repetition in Drake? Which by the way should be linked to in the article.
It depends on *why* I'm reading the code. If it's a project I'm new to, I find the closest thing that resembles main and start tracing back from there. If it's because I've brickwalled and the documentation is lacking on how to do something -- \*cough\*django rest framework\*cough\* -- I start in module that's home to the problem child and go from there. As for expecting developers to write comments and documentation, oh sweet summer child. My best advice would be to use your brain, followed closely by your eyes. And always remember, "All code is crap -- some of it is just less crappy than others."
If we're talking modules Python's read, there's also `pprint.pprint(sys.modules)`
Selenium requires only that you point it at a url and tell it what to do. It drives a Web browser.
Unfortunately in life, pretty much every major change would probably also be a roadblock if not for other factors - people move forward mostly because they have no other choice. The soft transition from 2to3 is the biggest reason as to why there is still so much 2.7 out there - it's just not a priority, because the 2.7 stuff works just fine. It might work better if ported to 3, but hey, time and $$$...
At this point actually "porting" to Python 3 is a lot less work than most people would probably guess for your average codebase. Still, the upgrade rates from 2.x to 2.x+1 were way higher because it was easier at the time. Which is why Python will never do a "stop the presses" style upgrade ever again. Feature flags via `__future__` are already a thing, we'll probably lean on them more for future compat-breaking upgrades to allow slowly moving a codebase piece by piece.
Another technique is to read unit tests.
Equivalent features, libraries and community?
My point was that there's no reason you have to make breaking changes to stay relevant, two of the most popular languages in the world have managed for 30+ years just fine. In any case, put it another way: The whole reason we're *still* talking about the python/python3 schism nearly 10 years later is because python *actually continued to gain momentum* after python3 was forked off.
The GIL was rewritten for 3.2, yeah. [Relevant comment in the linked thread.](https://www.reddit.com/r/Python/comments/47uweh/switching_from_2_to_3_love_asyncio_and_more/d0gwhn8)
Maybe, but I definitely feel you on this. 
Thanks for link. That's so awesome!
But I don't know you. 
Yes, that would be possible. I didn't do this because would make the configuration not reproducible. That is, the configuration would not be self contained in the configuration directory anymore.
Nah. Nobody pays for open source. Instead we'll just have have two separate ecosystems separated by a common language.
every block of code that does something, I comment. because in months or even worse, years later when I need to make a change, I may not remember what I did or WHY I did it that way. And yes, you may need to follow the code all the way through to know what it is doing. For future reference “Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live.”
Haha, thanks, will now think of programmers as psychopaths
Use 3.x.
Nope, not great. Misleading. You have to calculate the pair-wise distances before doing linkage. Pair-wise distances are what drives intra-cluster variance and linkage controls inter-cluster variance. One is minimized the other maximized.
And, the only reason we use 2 for jobs is because 3 wasn't all that viable until recently, so all of the existing code is old 2.x, which isn't compatible with 3.x, and nobody has the time to convert it all. Or, we have to support systems that have 2.x installed by default, and can't force 3.x in. 
This is great! Much easier than Reportlab. Now I just need to find a DOCX to PDF converter and I am all set.
Whaaaat? There should be absolutely no difference between the two. Shell or python, in this case, is calling the exact same os system calls (thus os module). He's most likely disk bandwidth limited.
I disagree. You're most likely disk bandwidth limited. If you do try shell, let us know if it was any faster. Both will call the exact same system calls, so there should be no difference. Python may be a smidge faster since it's not having to spawn processes for the shell commands.
From Stdlib it is the ftp lib, hands down. 
Well I can tell you that in my particular application, it seems to work as is, but the results look a little noisy. I have BOLD time-series data from the human LGN. I am trying to cluster the LGN into two partitions. I have an expectation for how that segmentation should look, anatomically. My approach was to generate a spectrogram out of each time-series, which produces a frequency x time image. I use scikit-image.measure.structural_similarity to produce a MxM similarity matrix. I then feed this into linkage and cluster given some max distance parameter. As is, the approach here works fairly well for producing the expected segmentation. But you're telling me that I need to take 1-(similarity_matrix/similarity_matrix.max())? 
But.. but.. Go? No.
Anything involving subprocess or wrapping subprocess always just ends up being weird. Either half-way implemented, unmaintained, doesn't work on Python 3, or takes things waaay too far.
That's why I do: from datetime import datetime 
if someone can program with me on python tutor id really appreciate it
reportlab Maybe the PDF format is the one to blame ...
&gt; import arrow
You do realise that the Python 2 benchmark has to allocate an extra 100000-items list right?
&gt; Unfortunately in life, pretty much every major change would probably also be a roadblock if not for other factors - people move forward mostly because they have no other choice. Nope, pretty much nobody still uses 2.5 and the 2.6 use is about 5% (based on PyPI stats). Somehow this presumed horrible resistance to change was never a problem in the 2.x land, and without any grand sweet features in newer versions serving as bait either (like, nothing compared to asyncio). So I don't think it's fair to place the blame on the userbase. This slow-motion trainwreck of a migration proceeds way more sluggishly you'd expect even from corporate users. I personally think that the main problem is not even backward-incompatible changes, but that at first there was no way for library code to support both versions, stuff like `six` was developed by the community instead of included from the start, and it would have been much better implemented using the `__future__` magic anyway. There's an inevitable chicken-and-egg problem: as long as there's a lot of 2.x users most libraries don't want to drop the 2.x support, and as long as many libraries don't support 3.x a lot of the users can't migrate to 3.x, so the only viable way forward is for libraries to try to support both. Which is much much harder than just porting code (and was even harder in the past). That's the real obstacle, and it looks like the core developers had absolutely no clue that it would be a problem.
Prep for college. Maybe by time you get to college, there will be a new "thing" and Python will take a back seat. computer science degree or similar is what you need to look at. In the mean time, contribute to open source projects or even github code bases and put your name on stuff where applicable. If you work on open source, your name usually get put on the project as a contributor.
MySQL, sqlite, etc is software's memory, long term memory. To demonstrate - When your browser calls reddit.com with the URL `r/Python/comments/48n7rb/can_python_replace_mysql_on_a_website/` one of the first thing reddit's servers does is read a session cookie your web browser provided ( in Chrome, press ctrl+shift+i and type `document.cookie` into the console, that's all the data your browsers gives reddit.com). Ignoring modern tech, basically reddit takes your identity and calls to a much more complex database to figure out who you are (username). Next Reddit servers look at the URL and turns that into a series of database queries, retrieving all of the data associated with this URL; your post text, title, recent comments, etc. With your identity, the page content data, and some ancillary information in memory, the reddit software transforms that into a HTML response. Disclaimer: Reddit is a lot more complicated then what I've outlined as it tries to avoid "hitting" or querying it's database as much as possible. But the concept is the same. You make a request, the software transforms it into something it understands, queries the database, and then uses the results to give you a webpage. Reddit.com's server use Python as the bridge between you and its databases. As for what Reddit uses, last I knew it used Postgres and Cassandra(C*). Postgres is similar to Mysql while C* can be simplified as a key-value database (on steroids). Sadly, even with the databases, Reddit will never be as impressive as https://html5zombo.com/. 
Learn to code and then do some freelancing? This guy here did an awesome summary of his experience. https://www.reddit.com/r/Python/comments/3y7yoo/freelancing_with_python_and_fiverr/ 
All of them are pretty flawed. I've been wondering how much work it would be to fully port joda time to pure python.
Pycharm is quite nice, I use it at work
Trying to get Theano to work on windows7, with the gpu, and py3.4, is an excersize in futility.
Well, that's probably more to do with Windows being the problem...
The documentation is awful and the install guide is horribly outdated, especially regarding 3.4 and x_64. I've might have forgotten something, so you're welcome to pm or reply if you've got any questions. You didn't hear it from me, but I believe the only way to get your hands on VS10 Pro is to Jack Sparrow the shit out of it... 
I've attempted to use matplotlib multiple times and I've always ended up confused and afraid. Maybe I'll check out seaborn next time I think about drawing a graph.
I don't know how helpful analogies are in this case. I could argue that Windows is not a house.
I dont know why I didnt just board the Black Pearl sooner. Time to uninstall silly vs express.
Python puts a lot more overhead on those calls. Compared to disk access, that overhead is small, but it exists.
Or even right. 6 built in types? Lol. * object * type * Exception * genexp -- `(x for x in things)` * range/xrange (depends on if you're 3/2) * bytearray, memoryview and bytes in Python 3 * Unicode in Python 2 * map, filter are classes in 3 * float, int and complex are distinct types as well -- add long if you're Py2. That's just the few in builtin I thought of that were missing. Also depends on the definition of "builtin" as well. 
Anything PDF-related. For a small script for analyzing and modifying PDFs I think I ended up using three different python-packages to be able to do the operations I wanted to do. I think there is a commercial package that is better and might support all the operations I wanted, but this was for a hobby open-source script I did not want any such dependencies in (and had no budget for that). Of course this is an issue with the PDF standard being a complete mess. I didn't know until I started writing that script. The people doing the free PDF-packages for Python can not be blamed. EDIT: [this project](https://github.com/lifelike/gbpdflink), depending on pdfminer, pyPdf , and reportlab. Of course things might have improved in the last 3 years, or maybe I just did not notice some clever way to make it work using only 1 or 2 packages, but it still was no great experience.
It is hard to jump into a code base and read code just to understand it. There are some exceptions, but your brain has to get used to the code and be able to intuit what to ignore. I find that tracing and fixing small enough bugs helps a lot. Also, learning how to grep/ack/ag around is important. One trick to get you started; if a project has a utilities module or package, try to find the most used utilities (classes or functions) and where/how they are used. This is a good avenue to feed the circuits in your brain to get you feeling comfortable. Utilities tend to be small and when you get familiar with them you know that you can ignore them when reading the code. Nicely written tests are helpful but most tests are written with the goal that they should/will be passed not read.
Maybe have the handler return a redirect to index.html?
...fuck, I always remember that wrong. Fixed now.
Switch to Python 3 and just `import flufl` then.
Huh, I like `matplotlib` just fine. Granted, I learned how to program with Matlab, and then I shifted to R for a few years. With both Matlab and R, I create my own graphs using the basic plotting functionality (i.e., I don't use ggplot2 in R), primarily because the models I use most require it (or benefit from it, at any rate). Now that I use Python for almost everything, I like how `matplotlib` works. I certainly like how nice my figures look. Sure, I end up with long scripts for generating figures, but that happened in Matlab and R, too. And it's better than R's base graphics. Python in general is better than Matlab, in my opinion, so even if the graphics are very similar, all of the other stuff about using Python makes it preferable.
I've been dealing with DRF at work. It's class oriented programming (as opposed to *object* oriented) and has serious control issues. On top of that, it's public API is *huge* -- `APIView` has *28* public methods, most of which are seriously out of place. 
Take a look at [this](https://stackoverflow.com/questions/11419572/how-to-set-the-documentroot-while-using-pythons-httpserver) This question might be better suited for /r/learnpython.
How this compares with [Pynsist](http://pynsist.readthedocs.org/en/latest/) currently (both products may improve on this in the future): Pros of Conda Constructor: - Build Windows/Linux/Mac installers in the same way (Pynsist only targets Windows). - Uses conda packages, so you don't need to work out where e.g. PyQt files go to build something that works. - Easy configuration of images/colours in installer. Pros of Pynsist: - Build Windows installers from Linux/Mac (Constructor builds for the platform it's run on). - Code does not need to be conda-packaged before building an installer. - NSIS scripts are built from Jinja templates, which can be extended to customise the installer (N.B. template structure is still a work in progress).
If you started with Matlab, that probably explains why you like matplotlib - its API is deliberately like Matlab so that people could convert comfortably.
Look at the recent security bug in SSLv2. Take a step back, and look at it again. It happened because "compatibility with older versions" causing deployers to not want to break, possibly, old time compatibility. Python is on the same way, it took python -years- to fix the security problem of _NOT VALIDATING CERTIFICATES_. Because Compatibility. The people hanging on to Python2.6 and Python2.7 are exhibiting the same behaviour. And it's time to take a distance to them.
What I have found very helpful is to *never* use a "datetime with timezone." Those are so confusing for no reason. I never know whether the time is in UTC or the local timezone and I never know what I can compare them with. Instead, I always use naive datetimes and keep track of timezones myself. Keep same with same. Either use the local timezones for your calculations or switch everything to UTC. I use Hungarian notation to help with this. lclTS for a local datetime. utcTS for one in UTC time. Handling the different types explicitly seems more complicated, but in reality it turns out to be much more straightforward. I think of Local time vs UTC time the same as a character encoding vs Unicode. A (naive) local time is the same as a byte string with a character encoding. The timezone is the encoding. A time in UTC is like a Unicode string. But you never have a string represented by a "Unicode + encoding," do you? 
Numpy is for numerical and multidimensional data, but Pandas is great for anything that might go in a csv - it's pretty much the Pythonic alternative to Excel. 
Use the right tools. By which I mean, an intelligent IDE like pycharm or PTVS which can do static analysis. Not sure what a function does? Ctrl+Q brings up its docstring. Still not sure? Ctrl+click to jump to the definition. Easily find all usages of a particular function or variable in the file or the project (in a way which respects scopes, rather than just searching for things with the same name). Type inference, block collapsing, soft line wraps, version control integration... people who use text editors really hamstring themselves. Plus, if the person who wrote it was using a similar editor, chances are it's going to be more consistent and universally-readable as they do stuff like remind you about pep8 and enforce other standards. 
I've started using plotly whenever I can. Creates beautiful, oftentimes interactive plots. Give it a whirl sometime. 
I'm guessing you're using python2.X? They seem to have fixed up that issue in 3 as far as I know, and it's pretty painless to use.
If you want to extract a flat clustering, there are algorithmic ways to do this that are much smarter than selecting a single flat cut value, see [this notebook](http://nbviewer.jupyter.org/github/lmcinnes/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb) for an explanation. You can just use the associated library [hdbscan](https://github.com/lmcinnes/hdbscan) to do your clustering and get all the associated plots easily.
Funny you should mention OOP style. I used to not like using MATPLOTLIB until I discovered it has OOP style api. [This](http://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb) should be the tutorial used by the official doc IMO.
Not until I discovered [this](http://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb) did I enjoy using MATPLOTLIB again.
yeah, but then every time I type 'datetime', I have to stop and think "is this datetime, or is this _datetime_?" 
You are effectively clustering the similarity matrix; this will work in that each sample has an associated vector -- it's similarity to all the other samples -- and viewing that as a feature vector you will get a clustering that is related to what you want as similarity vectors will be close if the underlying data points are close. Still, this isn't ideal. Instead your better bet is to just take your MxM matrix and throw it at scikit-learn agglomerative clustering with metric='precomputed', or if you want to stay in scipy take advantage of the squareform function in scipy.spatial.dist which, oddly, returns the pdist format data if you hand it something that's square, and pass that to your clustering. Of course you'll want *dissimilarities* not similarities ... but you can just do negative squared similarities, or even inverse similarities, depending on how you expect them to behave,
I'm interested in what troubles people are having with subprocesses. 90% of the time I want to run a subprocess, I do it as a single shot (run this and return when it's finished). For that, there are the high-level functions you mentioned, and the new subprocess.run() in Python 3.5. Popen works for using a process in a Unix-y pipeline: data goes in stdin, data comes out stdout, with no real connection between the two. It gets fiddly if you're trying to do two-way communication, but it's exposing how the system really works, which I prefer to having an awkward wrapper around it. If you want code to talk to a subprocess as if it were an interactive user, Pexpect is the way to go. Although a competitor with a better designed interface would be very welcome.
Arrow looks nice and easy, tell us how your project goes and if you had any issues. 
I've started using import datetime as dt dt.datetime.now() It makes the dieing feel a little slower.
I use pytz. All dates are always cast to UTC or created as `utcnow`. This way, the api always serves the same time and the client (web, whatever) can then use the local settings to display the proper time. By converting all times to UTC prior to persistence in the db, when a client passes in that requested time for something scheduled, there's no math involved in "when" a something should happen.
&gt; Since open() is used to open a CSV file for reading, the file will by default be decoded into unicode using the system default encoding (see locale.getpreferredencoding()). To decode a file using a different encoding, use the encoding argument of open: import csv with open('some.csv', newline='', encoding='utf-8') as f: reader = csv.reader(f) for row in reader: print(row) Maybe I'm wrong in some way (which is entirely possible, I have no idea what specific issues you ran into) but doesn't that work? I'm just going off of personal experience, I haven't encountered any UnicodeDecodeErrors since I explicitly started specifying the encoding of the file I'm opening.
I got a little sidetracked when studying and made this. It runs around 100x faster than the pure Python version. This generates the matrix of escape times, using Cython. You can see that it is mostly "dumb", non-vectorized Python code, but Cython speeds it up significantly. # _julia_figure.pyx import numpy as np cimport numpy as np import cython from cython.parallel cimport prange @cython.boundscheck(False) def julia_cython(int N): cdef np.ndarray[np.uint8_t, ndim=2] T = np.empty((N, 2*N), dtype=np.uint8) cdef double complex c = -0.835 - 0.2321j cdef double complex z cdef int J, I cdef double h = 2.0/N cdef double x, y for J in prange(N, nogil=True, schedule="guided"): for I in xrange(2*N): y = -1.0 + J*h x = -2.0 + I*h T[J,I] = 0 z = x + 1j * y while z.imag**2 + z.real**2 &lt;= 4: z = z**2 + c T[J,I] += 1 return T This file is needed for Cython to use multiple threads: # _julia_figure.pyxbld def make_ext(modname, pyxfilename): from distutils.extension import Extension print("making extension with OpenMP support") ext = Extension(name = modname, sources=[pyxfilename], extra_compile_args=['-fopenmp'], extra_link_args=['-fopenmp']) return ext And finally the following saves the matrix as a PNG using the new Matplotlib viridis colormap: import numpy as np import pyximport; pyximport.install() from _julia_figure import julia_cython def save_png(T, filename): from PIL import Image # Normalize arbitrary range to [0,1] -&gt; color tuples -&gt; color tuples as uint8 # This can be saved as an image. normalizer = plt.Normalize(vmin=0, vmax=70) mapper = plt.cm.ScalarMappable(norm=normalizer, cmap=plt.cm.viridis) im = Image.fromarray(np.uint8(mapper.to_rgba(T)*255)) im.save(filename) if __name__ == "__main__": import time t0 = time.time() N = 16000 T = julia_cython(N) t1 = time.time() print t1 - t0 import sys if len(sys.argv) &gt; 1: print "Importing matplotlib...", import matplotlib matplotlib.use("Agg") import matplotlib.pyplot as plt from scipy.ndimage.interpolation import zoom print "Done" print "Zooming matrix...", T = zoom(T, 0.20) print "Done" print "image has size", T.shape save_png(T, "julia.png") I did some Photoshop work to improve the contrast and resized to wallpaper size. I have the PSD file available if anybody would like to remove the code or resize it.
Your username says otherwise
It looks like linkage() will do distances for you. But default is euclidean, not sure if that is appropriate for your data. To be safe i would do distances before doing linkage. You cant link similarity data, its the opposite of what you want.
Your username says otherwise
Correct, it works for reading but when you end up with Unicode data that you try to write then it all blows up in your face.
A video can only initiate you in a programming language. Your skill with said language comes from using it. A lot.
Holy shit, there's someone else out there who knows the struggle. Respect. I passionately, violently _hate_ DRF. For the life of me, I do not understand what value people see in it. It's an affront to decency, an offensive mockery of better frameworks, a total disaster, an abomination. No person of sound mind and good repute would use it for _anything_ but a demonstration of how not to design a framework. The Viewset API is completely insane. From the docs. &gt; A `ViewSet` class is simply **a type of class-based View, that does not provide any method handlers** such as `.get()` or `.post()`, and instead provides actions such as `.list()` and `.create()`. The fuck is this? It's a goddamn REST API framework. Everything revolves around HTTP methods. Everyone knows the semantics of HTTP methods. Why is this necessary? Worth noting, the emphasis is _not_ mine, it is _in the documentation_. This mangling of semantics is goddamn deliberate. So that's nice. Let's look at the code. class UserViewSet(viewsets.ViewSet): """ A simple ViewSet for listing or retrieving users. """ def list(self, request): queryset = User.objects.all() serializer = UserSerializer(queryset, many=True) return Response(serializer.data) def retrieve(self, request, pk=None): queryset = User.objects.all() user = get_object_or_404(queryset, pk=pk) serializer = UserSerializer(user) return Response(serializer.data) Whoa. WHOA. What the hell is this? The _collection_ of `users` and an _individual record_ are distinct resources. Why do they coexist in the same class? I could much more easily route `/users` to a resource for the user collection and `/users/&lt;id&gt;` to a resource for distinct users and this would be _way_ simpler. But it gets worse. &gt; If you have ad-hoc methods that you need to be routed to, you can mark them as requiring routing using the `@detail_route` or `@list_route` decorators. &gt; The `@detail_route` decorator contains `pk` in its URL pattern and is intended for methods which require a single instance. The `@list_route` decorator is intended for methods which operate on a list of objects. Hold the fuck on here. So I configure my routes to dispatch to methods on these viewsets (which, recall, agglomerate distinct resources into a single class), and then routing is further customizable with _fucking decorators_ on the methods themselves? What? As though routing in Django wasn't already offensive enough, let's isolate specific cases and bury them somewhere in application code! Or how about this nightmare: &gt; These decorators will route GET requests by default, but may also accept other HTTP methods, by using the methods argument. For example: @detail_route(methods=['post', 'delete']) def unset_password(self, request, pk=None): ... So... each method in a viewset corresponds to a different HTTP verb (but are named differently from the HTTP verbs themselves) _UNLESS_ you specify in a decorator that the decorated method should handle _multiple_ HTTP verbs. Right. And that's just viewsets. DRF is "highly opinionated" and all its opinions are _terrible_. DRF is the Donald fucking Trump of "opinionated" software. The 5,454 people who, at the time of this writing, have starred the project on Github are intellectual invalids. Tom Christie is bad, his software is bad, and he should feel bad.
The python Selenium client. There is a ton of duplicate code and the maintainers clearly are not pythonistas. 
The utter level of rage in this post amuses me, and has also convinced me to never touch that library ever. Nobody would get this worked up if it was okay or even sort of bad.
+1. Best lib for time management ever.
i wrote https://github.com/andrewcooke/simple-date because i was fed up with the other options. in doing so, i also came to understand why it's complicated - timezones are a real mess. so i am not sure it's possible to make something simpler that is also predictable. see https://github.com/andrewcooke/simple-date#background for my take on how things should work.
&gt; Nobody pays for open source Red Hat Reports Third Quarter Results for Fiscal Year 2016 - Quarterly total revenue of $524 million. - Quarterly subscription revenue of $457 million. - Deferred revenue of $1.49 billion
anything nmap related
I'll open an issue, I think this will get rather long ...
I've never needed anything more than pd.Datetime. Just use pandas. 
A few more comments: Chaco is based on Traits and the Enable canvas. It is particularly easy to embed Chaco plots in TraitsUI apps as this was the main design use-case. Chaco is totally independent of Enaml / Atom. Enaml/Atom is an alternative observer+gui framework which in some sense is an alternative to Traits/TraitsUI. If you have an Enaml GUI app, you can embed a Chaco plot. In this case, you end up having two observer frameworks inside your app (traits + atom). In the end, this doesn't cause any problems. Traits is effectively internal to chaco and there's nothing forcing you to use it elsewhere. We are doing this in one of our products. You can't realistically use Chaco without understanding traits. Traits was born out of Chaco development. Just learn about traits. It's worth it and unlike Chaco, Traits has documentation. Once you understand traits, you're most of the way over the learning curve of chaco. The Enthought tool suite and Enaml/Atom are all easy to package as eggs. These libraries never cause us distribution problems. The PITA ones are things like wxPython / PyQt (build 64-bit Qt from source on Windows anyone? Good luck with that). I won't argue there's any excuse for the Chaco docs. If pyqtgraph does what the OP needs then great. However, I've yet to find anything suitable for the sort of interactive widget design that Chaco facilitates (or, maybe 'makes not impossible' rather than 'facilitates'). Chaco isn't a great library. It may not even be good. It's simply does stuff that none of the alternatives address: abitrary mappings between data-space and screen-space, tools/interactors + stateful canvas model, plot layouts. Yes, learning it is a pig but once you "get" Chaco, it becomes intuitive and very very adaptable. 
I'm super interested in this kind of stuff, but am a total beginner. Can someone explain how this actually works/what the "complicated" lines do? 
`logging`
&gt; And the documentation is often completely unhelpful I agree. Searching for a function or a featrue is a nightmare in the docs. [matplotlib gallery](http://matplotlib.org/gallery.html) is the only useful thing that I have seen in the website.
great stuff, thanks.
Does anyone know if you can circumvent this by distributing as a conda package? Or is the preferred method just to use a wheel? 
Looks like you have a compiler error. How did you install theano? Usually I install it from the wheel on cristoph golhke's site.
just going to leave this here for when you need a break: https://www.youtube.com/watch?v=-5wpm-gesOY 
you basically take a point in the complex plain (a complex number) and watch how much it diverges when you repeatedly apply a certain fixed function to it, f(z) = z² + c, and color it accordingly. you do f(f(f(f(..f(z)..)))
If you're willing to pay you can get support even for Python 2.5... or older. So this is going to happen even for Python 2.7. 
Yes, Reportlab is so full of bad ideas. Eventually it does the job, but I hate dealing with it. No, the PDF format has nothing to do with Reportlab's awkwardness.
Again, personal experience, but I just did this snippet of code: with open("test.csv", "w", encoding="utf-8") as fp_out: writer = csv.writer(fp_out, quoting=csv.QUOTE_ALL) writer.writerow(["Ǡ", "ʦ", "ʱ"]) And it worked flawlessly. Unless you are trying to write utf-16BE/LE strings with utf-8 file encoding, then I could see the issue.
Agree. I still use print function for my logging.
A little off topic, but can you explain why it is so much faster than a Python implementation? 
&gt; import its_fiiiiine_no_one_needs_dates_or_valid_data
I got the same error on Windows/Anaconda with Python 2. Apparently it should work to install the Visual Studio compiler for Python, but that didn't work for me, so I gave up and used Linux. This version of the codes uses Numba, and should be much easier to run on Windows, just install Anaconda 2 or 3. Not tested on Python 3. import numpy as np from numba import jit try: # Python 2/3 compatability range = xrange except: pass @jit(nopython=True) def julia_numba(N): T = np.empty((N, 2*N), dtype=np.uint8) c = -0.835 - 0.2321j h = 2.0/N for J in range(N): for I in range(2*N): y = -1.0 + J*h x = -2.0 + I*h T[J,I] = 0 z = x + 1j * y while abs(z) &lt;= 2: z = z**2 + c T[J,I] += 1 return T def save_png(T, filename): from PIL import Image import matplotlib.pyplot as plt normalizer = plt.Normalize(vmin=0, vmax=70) mapper = plt.cm.ScalarMappable(norm=normalizer, cmap=plt.cm.viridis) im = Image.fromarray(np.uint8(mapper.to_rgba(T)*255)) im.save(filename) T = julia_numba(3000) save_png(T, "julianumba.png") 
I wish they'd send me some of that, given that they and their customers use my software heavily. Regardless, Red Hat isn't mainly in the software business, they're in the indemnification business.
You can also use read_sql, not sure if it will make it faster, but feels like a cleaner approach with pandas. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html
Oh yes, I am familiar with this. The impossibility of using even a simple, stateful, ugly OO interface frustrates me to no end.
The Python interpreter is very slow, and interprets one statement at a time. For example, the statement "T[J,I] += 1" is responsible for around 30% of the runtime (Python version, not Cython). Using fancy indexing ([]) first attempts to use the __getattr__ method of the T object, and requires multiple function calls just for something so simple. The Python interpreter can be avoided by letting Cython know what types variables, so all the work is done in machine code. Additionally C compilers are some of the most optimized software in existence and contain loads of tricks for increasing speed. Disclaimer: I didn't try Cython before last weekend.
Just to clarify—am I feeding hdbscan my similarity matrix? 
Not bullshit. Python 2's unicode text model was fundamentally broken. It was literally impossible to fix in a backwards compatible way.
Yeah it does, but it doesn't put them in the same directory as the source files. They go in __pycache__ instead.
See my other post in the thread. It's a little bit slower than Cython. For N = 10000: Cython parallel: 1.4 s Cython serial: 10 s Numba: 21 s
&gt; customers use my software heavily. Do you charge for your software? Do you charge support for your software? &gt; Red Hat isn't mainly in the software business, Correct, they are in the support business. Companies pay them to be the fall guy when shit falls apart.
There are many good things in Python 3. Why people resist is beyond me. That being said I'm not too sure I'd go with Python if I knew the solution to a problem is highly parallel. 
A few things: * Should ConfigError be defined in the TpmApi class as opposed to outside it just below the other exception class? * I like the use of 'bullshit' in comments. * You use the `'some string %s ' % (some_var)` style of inserting variables into strings rather than the more modern `'some string {}'.format(some_var)` style. That's mostly just a style issue though, and not technically wrong/is splitting hairs. * Things like `if self.private_key is not False` can be shortened to `if self.private_key` as long as you know `self.private_key` won't be zero, an empty string or an empty list. Though I'd understand if you kept it in there to be safe, unless of course it makes sense for passing an empty string to count as False in your case. * You could refractor a lot of the long if statements into functions with useful names that return True or False. That might look something like `if needs_key_authentication ():`. That way you wouldn't have to keep track of all the configurations of True's and False's, plus your code would self document a little more. * You could refractor the fairly long if statement for which request to do, into a dictionary using request type as the keys, and `functools.partial`s with the functions and parameters in them as the values. Though you're just on that middle ground of where it only sorta makes sense to do that. But it would cut down on some repetitive code, and potentially look a bit cleaner. Generally though I think it looks pretty good. Do take what I say though with a grain of salt, as I just quickly read over the code on a mobile device, commenting about what I noticed. 
Hmmm, would you mind opening an issue or posting on the numba mailing list? I'm not sure if it should be 2x as fast for serial.
And yet working Unicode aware software was and still is being produced using it.
I liked bokeh until I realized that it does not have any capability to export graphs for publication and that the Bokeh maintainers consider such functionality unnecessary. I moved to plot.ly for now as it offers substantially similar capabilities.
Agree, I'd like to see that code too. Numba should run en par, so maybe that code is not optimized. Edit: Along that line, Numba should also get a parallel version with nogil=True and a thread pool.
There's a fork that has seen a more recent release: [suds-jurko](https://pypi.python.org/pypi/suds-jurko/0.6)
It's powerful, but it's still terribly designed. It's one of the few stdlib modules that doesn't even follow Python naming conventions, let alone intuitive usage or an accessible API. The configuration methods are inconsistent and poorly documented. To even get a simple console logger up you have to define handlers, formatters and namespace-levels. It's a *huge* PITA to set up, but works well once it is. Which is such a shame because I don't want to go through all the boiler plate for simple applications and just use print statements piped to a file instead.
thanks
And if you do want a pure Python version, here's one: import numpy as np def julia(c, extent, pixels): t, l, b, r = extent ny, nx = pixels ys, xs = np.ogrid[t:b:ny*1j, l:r:nx*1j] im = ys * 1j + xs counts = np.zeros(pixels) while True: im = im * im + c still_alive = [abs(im) &lt; 2] counts[still_alive] += 1 if not np.any(still_alive): break return counts N = 1000 j = julia(-0.835 - 0.2321j, (-1, -2, 1, 2), (N, 2*N)) It is a lot slower. Takes ~9s on my machine with N=1000
Skimmed through the video. This is beginner level as in "my first time even seeing what Python is"-level beginner.
Both conda packages and wheels can contain binaries. For a wheel example, [Logbook](https://pypi.python.org/pypi/Logbook) has Windows wheels on PyPI.
Any of the SOAP libraries. ZSI is okay, but the documentation stinks and it hasn't been seriously updated in, like, ten years. suds *looks* okay but it has tons of bugs, including a nasty memory leak, and also isn't actively developed anymore. Every other SOAP library is either half-assed, half-implemented, or in a state of complete disrepair.
For me, just live output, two way communication, logging, and timeouts, so basically everything pexpect does.
&gt;Its a little absurd that the devs and the python community in general dont give more attention to literally the worlds most popular operating system. I work at CERN, so I'm probably not the best person to be arguing with on this. Most of the internet is run using Linux distributions. All supercomputers and high performance systems use Linux. For the science with which I am involved, it would be absolutely unconscionable for me to accept public funds for science and then use closed-source, untrustable tools to do that science. That's why just about everything CERN produces is entirely open, from the technology to the software (we use Scientific Linux and CentOS for operating systems) to the science. I don't know *any* scientists in my field that use Windows for their work. &gt;It would be a lot like buying a new house, and buying new furniture for it, just so you can wash your dishes. The new home wouldn't have secret cameras hidden in its walls, some of them installed by the government and some of them installed by the builders. The new home would be owned by you, not rented: if you wanted to paint the walls a different colour, you could do so. &gt;Or maybe its laziness. Or efficiency. Life is short. Who the hell are you to tell developers who are already providing you with vast amounts of software completely free to make the software work on the proprietary, for-profit, backdoored, inefficient system you use? Why not write it yourself? Why are *you* so lazy?
The packages that I personally use are compatible with both Python 2 and Python 3. That being said, I don’t see any reason to not use Python 3.5 for new projects, as it is hands-down the better version of the language. Things like ‘yield from’, ‘everything is an iterator’ and ‘keyword only arguments’ are amazing! TBH If you have a choice, you should definitely go with Python 3, which is under active development, as opposed to Python 2. IMHO this deck points out some great features of Python 3. https://asmeurer.github.io/python3-presentation/slides.html#1 Working on large projects with a potentially fragile legacy code base is a different story though. If you have sufficient test coverage and meaningful integration tests, I encourage you to migrate the code base gradually. Tox can help you with that. If you don’t have tests, it might not be worth the risk. You’ve mentioned that you work a lot with files. Well, it got a lot better in Python 3.4 with pathlib being added. https://docs.python.org/3/library/pathlib.html Hope this helps!
Have you seen logging.basicConfig? I mean, I feel your pain, and even with basicConfig there's a lot of applications where setup takes more effort than I'd like. But basicConfig at least takes care of the case of "I want to add basic logging to this script."
What are some intermediate topics?
It is sklearn compatible (infact it inherits from sklearn) so you would do the same as the sklearn case and feed it the *dis*similarity matrix (negative square is probably good) and use metric='precomputed'. 
Any project. Python doesn't mean anything unless you're using it practically. Go make something!
This would get more of a response in /r/cscareerquestions.
If your house had faulty electricity, faulty plumbing, and a leaky roof, then yeah. You can't put Flint water through fresh water pipes and expect everything to work well. I don't know what services this Python library needs, but *if* it requires things that the platform can't reliably provide, then the fault lies primarily on the platform.
Also check out /r/datascience
And it's entirely possible that it's slower :P
It depends. In the case a menu, I definitely would. It might start as simple as `Menu = list`, so when I need to add behavior, I create an actual Menu class that probably implements the `collections.MutableSequence`. The advice to not make everything an object is good advice *to an extent*. I'll make new class when: * I have data and behavior, and that data will always be used with that behavior. Think an object that represents Money -- you'll probably always check if you're adding the same type of currency together. Or, * I have dependencies I inject. Some will argue closures work just as well, but closures often end up being *more* confusing at very little gain. Or, * I need to imitate the interface another object has for some reason. My best advice would be to watch [Stop Writing Classes](https://youtu.be/o9pEzgHorH0), and then read [Write More Classes by Arm in Ronacher](http://lucumr.pocoo.org/2013/2/13/moar-classes/).
If you build a conda package, the C code that needs to be compiled is automatically built according to the setup.py when you create the package, so this is very straight-forward and you only need a compiler and other build-time dependencies on the machine where you create the packages, while installing does not require any of them. Wheel also does this, but the wheel format is immature and lacks many features, compared to Conda. For some specific use cases (on some Windows systems), wheel works very well though.
I hate datetime so much its incredible. The worst part about datetime is every single library ends up making its own date type because the existing one is such garbage. My current project has to use datetime, date, np.datetime, pandas.Timestamp, and pandas.Period because that is what the various libraries are expecting. Converting between them all is non-trivial as well. Its fucking stupid, especially after seeing how elegantly dates are handled in q (where dates and times are builtin datatypes and can be created as easily as 2016.01.01d or 2016.01m).
pyodbc or ceODBC. Every time i connect to an sql server database it feels like I have to make a choice between completely broken bulk operations or non-existent error messages. Oh, you've got 10 million rows to go in to the DB and there's a shit value somewhere in there? Either twiddle your thumbs while pyodbc prepares and executes 10 million separate insert statements or pull your hair out meticulously combing through the db schema.
Look for email2fax gateway. Then the question will be how to send email from python with tif or pdf attachments.
The basics really are this easy. SQL injection is another one that's trivial to prevent. It's still one the most common vulnerabilities found on web sites. Many of the cases where attackers got access to those poorly encrypted passwords were via SQLi.
I have built algorithms that harvest data online and automatically update our internal database. The language I used was not Python.
I remember all the crazy optimizations we came up with writing these set programs for the 386 in assembly and C back in the 80s. FRACTINT was the gold standard program.
Oh course not.
The answers to many of these are wrong. This site is just spam, or possibly harmful to learners.
It doesn't follow python naming conventions because it's based on Java's logging. I don't know why they felt they needed to keep the same interface. Same with unittest 
This is python. Classes are objects.
We don't consider it unnecessary - in fact we are very keen on implementing it. There are just a few other higher priority things right now. The instant that either our internal product development needs or external consulting customer needs that feature, we can escalate it. The reason this is non-trivial for Bokeh is because the implications are quite deep. Bokeh's current rendering engine is Canvas, running in a real browser. Past attempts to use things like node-canvas and such have proven insufficient. Implementing a full-on SVG renderer in the browser would be a route to this, but it would only be for this. We've actually considered writing e.g. a Matplotlib-based converter that takes Bokeh JSON and renders it using the MPL backends, but that would also be a bunch of work. Again, not impossible, and there is interest from the Matplotlib side on this kind of interop, but it's just a big enough task that we'd really have to really need it, and it's lower priority compared to a bunch of the other items for our next milestone. Also, you *can* export graphs via the Image Save tool. You just can't do it noninteractively or "headless" without a browser. 
Stable is one way to describe things! I used to work on Chaco, nearly a decade ago, and it still surprises me to see the same demos I wrote eight years ago being trotted out. :) 
Sure. But an instantiated object is much more useful than a class that'll be instantiated by another that thinks it knows better than you. You're choices when attempting to use a premade object with DRF is callable object that returns itself (lolol) or a factory that returns the object (lolol) or a factory factory that returns a factory that returns your object (lolwut). DRF is a controlling, restrictive toolkit that foregoes anything resembling good design -- or design at all.
Usually I'm okay with line length being stretch a little bit but this was a bit over the top: https://github.com/andrewcooke/simple-date/blob/master/src/simpledate/__init__.py#L1082
I've been reading High Performance Python by Gorelick and Ozsvald, and they talk a lot about Julia sets. They take it from the slow naive for loop python, all they way to a pypy, numpy, and cpython. Lots of tools like dis and cProfile are covered too. I haven't finished it yet but I like to detail so far (after skipping the first 50 pages) 
"working." Just try to integrate with a library that isn't Unicode aware; that assumes `str` and `unicode` are interchangable. It's the quickest way to bite yourself in the ass down the road.
Interesting to know. I honestly don't know what the characters I was trying to write were because it was database content. Maybe I'll have to revisit