I often hear "OSX" is the best development platform but that seems so massively untrue... the same people who are saying this always seem to be devoting entire sessions on how to get a working python setup on OSX and work around the server problems. OSX might be a pretty desktop, but you aren't going to deploy to it... I still find it hard to believe that you have a better development platform on something ultimately you won't be deploying to anyway...
Tuple as dict key? That is some black magic right there. And to think, I felt all dirty using ints as keys.
And once you have that, if the tiles are each an instance of a tile class, they can have an adjacency property which is a list containing refs to all adjacent instances. If you have to do a lot of tile adjacency checks, they'd serve as fast lookup tables. E.g. you'd check (x-1, y), etc for the k tile up front, and push f, g, h, j, l, n, o, p into k.adj. Now if you want to know if c is adjacent to k, you just ask `if c in k.adj:` which would evaluate to False.
A lot of programmers are obsessed with type checking/compile time error checking, and believe it's necessary for writing reliable software. Also, there's many schools of thought as to how a high-performance program should be written. One way is to heavily optimize something like C/C++ down to the cache level so it runs absurdly fast. Another (more in line with dynamic languages like Python) is to optimize at a higher level using a distributed system with many processes or machines. The second way tends to be more practical for server-side systems, which I think is part of the reason why dynamic languages are so popular there.
I think I've seen this before because of the strange ^L Page Break usage. This just looks weird as fuck and I wonder who actually is using this.
you don't by any chance have a github repo you could share that already incorporates all the changes you suggest? ideally using pathogen? 
I do. See my edit in the original comment.
Nope. Its the EuroPython Guys. I Just found them :)
Eww. Dear Dog, why?
Genuine questions: what does this offer that Flask doesn't? What does it simplify?
You're asking that question 'cos Flask came earlier so we should stop creating frameworks, right? But it doesn't work like that. It doesn't even have to simplify anything. It doesn't even have to offer anything. If we've asked that question to everything, we couldn't go forward besides idea isn't always going forward. - Mergesort was invented earlier than Quicksort but it didn't stop Tom Hoare to invent quicksort. - Ford built the first car but it didn't stop BMW to build a car too. - AVL tree was the first balanced tree but it didn't stop Rudolf Bayer to invent Red-Black tree. - There was Boyer-Moore string search algorithm already, but it didn't stop Donald Knuth, Vaughan Pratt and James Morris to conceive Knuth-Morris-Pratt algorithm. - There was already a Django but it didn't stop web2py's creator to make one. - web.py was the first microframework of Python but it didn't stop Flask's creator to build one. - Heapsort was already attractive but it didn't stop Edsger Dijkstra to invent Smoothsort. Same idea. There might be a Flask but it didn't stop Aspen's creator to make Aspen.
All of those examples were motivated by perceived deficiencies in their predecessors. Every person who invents an alternative to something does so because they view it as an improvement, or just a conceptually different way of solving a given problem. Quicksort is better than Mergesort in certain conditions, BMW decided to improve upon Ford's work by making "the ultimate driving machine", and so on. I certainly didn't mean to say that we should stop inventing web frameworks. Kudos to the creators of this one - they've accomplished more than I ever have. I'm merely asking what drove them to create Aspen. What did they want to do differently from Flask, Django, web.py, etc., and why were they prompted to do so? What goals governed the design process?
&gt; And to think, I felt all dirty using ints as keys. Ints are really the only thing you *can* use as keys. Everything else gets asked to convert itself to an integer.
Thanks, this is what I was looking for. I appreciate you giving a suggestion for both interpretations. ;)
For a dictionary? You mean under the hood, where it's all numbers?
get some thanks though :)
It's also on the burden of younger frameworks to convince people to try it out. I don't do web dev, I have more experience doing desktop applications, but if someone came to me with a new GUI toolkit/framework and answered what you have to my question of "Why should I change from what I'm currently using" then I would probably tell you to jog on. When a new framework/language/library comes along "What can this do that I can't already" or "What does this do better" or perhaps even, "What is it's gimmick" are completely legitimate questions. After all, learning a new framework requires an investment of time, in a commercial environment this means money. At a quick glance of the link it seems Aspen does exactly what I said and immediately put it's "gimmick" of [Simplates](http://aspen.io/simplates/) forward, with an example.
"What problem are you trying to solve, or solve differently?" is always a legitimate question. It helps us discern useful ideas from weekend projects that will never be completed.
I got XCode 4.0 in the first few days it was available. Trying to run my XCode 3.x project on it, I got literally thousands of errors, none of them obvious or getting good results from Google searches. I know other people who have gotten their projects going, but XCode 4 indexes your code in the background, and they said that it made their machines unusable. I imagine I shan't be upgrading to Lion in 2011, anyway...
1st: every Pythoneer is entitled to create his own webframework. 2nd: I really like the presentation. The short movie is explaining the key feature. 3rd: I see no use in this key feature.
Don't let Flask zelots discourage you. The argument that there is an ultimate perfect software that everybody should use is fallacious. The value is in the process of writing software, in the experimentation, and in the creation and transfer of knowledge that this process implies. Good luck with Aspen.
Very nice feature list. I especially liked this: - pow operator also works as expected: `R3 = Interval(-oo, oo)**3 ; (3, -5, 0) in R3 == True` (not that I understood many of the others)
I like it. It's MVC in one file. For prototyping stuff, this is awesome. Also I use aspen as a quick and dirty web server too.
I think a more contrasted set of colors could make it easier to read the [video titles](http://www.youtube.com/user/PythonItalia#g/u). I'm pretty sure I'm not the only one who's having trouble reading dark cyan on light brown.
Never ever change a module you don't own. And no, it's a horrible idea and does not give you anything. sys.maxint is even gone in Python 3, so why use it in the first place.
with the tileset how do you load only the part of the image you want. Like if I have a large image with 30 different helmets on it how do i blit just one? In pygame. 
AFAIK the reddit admins have personally verified that he isn't a bot (this came up at PyCon a few years ago).
But it's a Python library that can manipulate spreadsheets, there's also a library that is specifically for XML files, too.
That looks very cool. I sometimes run ipython in a [Conque Shell](http://www.vim.org/scripts/script.php?script_id=2771) window which lets me quickly send code from a vim buffer to ipython, but this looks even more useful. The two-way integration with code completion and docstring viewing available in vim looks very neat.
If they are a bot, I personally don't care. After upvoting a link with good interesting content, I often notice post facto that it was submitted by gst. Many of those threads also have good discussion in the comments (one of the primary reasons I enjoy this subreddit). If they're not a bot, then I'm really appreciative of the time gst invests in submitting good content.
I don't mind at all, its *relevant* and *interesting*, if that is submitted by a program then cudos to the programmer.
Maybe it's written in Python? But seriously, I like his content, who cares about anything else.
one word: Wow
The Python community has a long-standing tradition of supporting boys. After all, Tim Peters was one originally.
First of all: that looks really awesome - with that feature I might probably ditch Bpython for iPython. And another question: does anyone know what recording tool was used? I am currently looking for one on linux and that one looks pretty neat.
Do I care? No. Can we stop having this thread now?
A dict is backed by a hash table "under the hood". To get the integer keys for the hash table, objects can implement [`__hash__`](http://docs.python.org/reference/datamodel.html#object.__hash__)
I have a dream that my four little bots will one day live in a nation where they will not be judged by the composition of their kin but by the characters of their content.
&gt; It will fail in Python 3: Sure, but Python 3 does not have sys.maxint anymore so both choices are going to fail: &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.maxint Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; AttributeError: 'module' object has no attribute 'maxint' 
&lt;C-S&gt; is not a good mapping to use. When running Vim in a terminal it will suspend the terminal until &lt;C-Q&gt; is pressed.
Size of exe in 2011? Really? Unless you are in some embedded or mobile platform. Even then you either have embedded python or you are doing something wrong.
No judgement, but I think it's interesting you used "he".
Seconded, I don't see how humanity is in any way relevant. As long as it contributes to the discussion!
I didn't say anything about using sys.maxint. Obviously I don't like that option either -- at least not in Python. I suggested to use `float('-inf')` or Decimal('-inf'). I think it's a bad idea to depend on a feature that has been removed from the language, especially one that can't be ported with 2to3, unless you plan to keep your code on Python 2 forever. Moreover, I think this particular feature was always a bad idea, and I'm glad they removed it. `None` is not a number and has no inherent ordering. It's comparing apples to oranges. 
In pygame? Not sure. For something like that, I'd be thinking Image (from the PIL library), which will let you cut out using X/Y offsets, then X/Y sizes. So you'd set up definitions for the tileset using functions. I thought as far as pygame went, you just blit the whole screen on a change. Anyways, sample tileset code, let's assume your tiles are 16x16: import os,Image def load_tileset(filename): im = Image.open(filename) (w, h) = im.size return (im.convert("RGB").load(), w, h) def return_tile(tilepos) (img, w, h) = load_tileset('/path/to/tileset') im = img.crop(tilepos) return im tilepos is a 4-tuple for up, left, bottom, right. So you would have a tile definition somewhere with: TILE1 = (0, 0, 15, 15) TILE2 = (0, 16, 15, 31) etc.
Here's [a reason to be excited about this release](http://ipython.org/ipython-doc/dev/interactive/qtconsole.html). And here's [another](http://pirsquared.org/blog/2011/07/28/vim-ipython/) ([posted earlier today](http://www.reddit.com/r/Python/comments/j4o0u/vimipython_twoway_integration/)).
Use virtualenv. The same virtualenv can be moved around to different machines if they have the same architecture.
I find that most people naturally anthropomorphize genderless things into "he".
Most of gst's links in r/python are posted on [Planet Python](http://planet.python.org), but gst improves the SNR. Sometimes I miss a good posting and catch it here because of gst.
you should have made this a blog post, would be hilarious if gst posted it :)
Seconded. Using pip and virtualenv your app can use a "private" Python, with specific versions of packages that don't interfere with the system-level libraries. The terminology can be confusing. Here's my post that minimally goes over them: http://johntellsall.blogspot.com/2011/06/proper-way-to-install-pip-virtualenv.html
Thanks. But assuming I want a newer version of Python, I'd have to compile it into a separate path, e.g by using the ./configure --prefix=/opt/something That would result in me having to set the LD_LIBRARY_PATH right? My main aim is to not touch the rpm database (hence no EPEL packages) or add stuff to the /etc/ld.conf. Or am I understanding this wrong?
py2exe just bundle an interpreter with your python code, that's what freakingthefuckout12 was saying edit : not saying it's not neat and not as good as a regular exe but that's clearly not compiling as an executable so I don't see why he was downvoted
Apparently, this ^L thing is standard python. I learned that a few days ago so I wasn't totally freaked out by it, but its still the first time I've seen one in the wild.
Looks like the default package found by pip is for Python 3. For Python 2, do: pip install ipython==0.11
I didn't have any problems using the update flag: pip install -U ipython
Huh, strange. Well, if anyone else does, that's how to fix it.
Isn't there a way to configure it?
Doesn't work with Spyder so far. `import IPython.Shell ImportError: No module named Shell` I hope new version of Spyder for IPython 0.11 will release soon. Ps How to run ipythonqt on Windows? What I have to install despite PyQt4? 
I had that problem too with RC3 (without Spyder though). I think I needed to manually remove some old IPython files, then install setuptools and a couple other packages. 
AFAIK, PyQt4 or PySide, Pygments, pyzmq
One thing I don't like is that a lot of gst's posts (bot or not, don't care) end up being dupes or someone else dupes them almost immediately, but that's also a Reddit bug in the first place. I'd rather read a gst post plus the repost than have to read "How do I learn Python?" four times a week.
You'll need PyZMQ. Windows installers are here: https://github.com/zeromq/pyzmq/downloads The API has completely changed, so unfortunately every other program which uses IPython will probably need updating to work with 0.11.
A lot of things are going to depend far more on exactly which blas, lapack, etc libraries numpy and scipy are linked with than the actual platform... You'll have to be a bit more specific than just "performance of numpy+scipy on windows vs. linux"... That having been said, some of Enthought's benchmarks are pretty interesting, though they're focusing on ATLAS vs MKL. http://www.enthought.com/epd/mkl/ They do show runs across windows, linux, and OSX, though.
I have uninstalled previous version of IPython (0.10.2) from Control Panel and removed .ipython from my user dir, then I installed setuptools, pip, and tried to install ipython. Pip showed requirements: pygments (have installed without a problems) and pyzmq2 (probably I should install Visual C++ 2008 or newer to compile zeromq... too complicated). Edit: Works with PyZMQ binary from github. Thanks takluyver.
My understanding is that pip/virtualenv will also recompile a private Python binary for you. If that doesn't work, try "buildout", which is also awesome. 
Thanks for spotting this. I've moved the Python 3 files into a subfolder, and it seems pip finds the Python 2 version again. If anyone wants the Python 3 version, links are now on the website: http://ipython.org/download.html
That is not an error, it is just an information that you need to copy that line into your file .bashrc in your home directory, so that when you start your shell, the correct locations for the PyQt4 module is found.
I've been able to install some C extensions nicely without LD_LIBRARY_PATH (lxml, mysqldb), but others (libyaml) I've had to set LD_LIBRARY_PATH manually before running the interpreter. If you want to avoid using RPM, ldconfig, the system-wide version of Python, etc. then I think you pretty much have to do something like this. What C extensions are you going to install? For the bigger ones (numpy, lxml, etc.) there should be a pretty well accepted way to install to a virtualenv with a locally built version of python.
Also, I don't think StaticPython you linked to will work for what you need: &gt; distutils is not included, there is no site-packages directory, loading .so files is not supported &gt; if dependencies are or need shared libraries (.so files) -- StaticPython doesn't support loading .so files, not even with the dl or ctypes modules
it looks great thanks for sharing. 
Excuse me but i didn't really know how to do this so : - i created a profile by : "touch .bash_profile" - then i opened it : "open -e .bash_profile" - and then i added : PYTHONPATH="/Library/Python/2.7/site-packages:$PYTHONPATH" export PYTHONPATH But it doesn't seem to work!
'Tiny'? Really?
excuse my possibly being an idiot: is there a standard way to pull the various bundles listed in bundles.externals? Or do you have a utility script that does that? 
FYI SymPy 0.6 is available on iOS via PythonMath (in the App store).
well, a start would be to add actually the line that is given in the warning message and not a totally different line ;) homebrew installed everything into /usr/local (actually into subdirectories there, look for something called Cellar) and puts links into specific places in /usr/local/bin and /usr/local/lib and so on. So you need to add the folder where homebrew installs python modules so that python can find them. The other thing is that I am not too sure wether .bash_profile is loaded in all cases of the shell starting - that's why I said to put it into .bashrc, as that is a file that is sourced on every shell start, while the profile is normally only sourced on login shells.
The line you added does not quite look like the line it asked you to add. Presumably it wants the other one because that's where it puts its own Python stuff. Did you try the other one first? Also, you'll have to restart the terminal before it takes effect.
Thanks, I have been using individual images up until now and it is starting to become a pain in the ass to manage all of them. 
Check out [this page](https://github.com/matplotlib/matplotlib-py3/wiki) to see the current status.
Hasn't been updated in ~4 months :(
heh I should be up for several UUOC awards. While "&gt;" seems to be perfectly logical for output, "&lt;" just never seems to feel right for input. I often use multiple piped greps where one would easily suffice, guess I can type faster than I can parse regex. I think cat would feel left out if I used it properly, I hardly ever have to join files.
That isn't working.
If i add the same line that isn't working either. I didn't have a .bashrc file, i created one and added the line, not working either. :(
Agreed. Teaches you both how to get the best features of any modern ide as well as one of the best ways to manage all the necessary plugins. I personally prefer vundle to git submodules, but otherwise this article was the basis for my vim environment. Definitely the best article I've seen on vim and python, no offense to op.
&gt; Also, you'll have to restart the terminal before it takes effect. Or `source ~/.bashrc`.
Wait, why are you installing pyqt to use iPython? It's a terminal application with absolutely no dependencies on a GUI toolkit. I just ran `sudo easy_install ipython` and got it. *Edit: Ok, I just found out about `ipython-qtconsole`. But still, it's not necessary for iPython.* What exactly is "not working"? I installed pyqt through homebrew. `echo $PYTHONPATH` gives `/usr/local/lib/python:`. I can start the Python shell and `import PyQt4.QtCore`.
I don't blame you for not knowing how to pull them. I should put some notes in that file to explain how to do it. You'll need to install this: [https://github.com/jmcantrell/vcs](https://github.com/jmcantrell/vcs) And then run this from within my vim-dotfiles directory: vcs-externals pull
No, pip/virtualenv don't recompile Python, they just symlink to an existing install. This is usually what you want. I have an on-again-off-again project called "myppy" that compiles a private python into a virtualenv-like setup: https://github.com/rfk/myppy/ It tries to make things portable across different flavours of linux, by removing certain options and using the autopackage compiler tools to avoid dependencies on newer glibc features. Might do what the OP wants slightly better than virtualenv.
Except they don't. Vehicles of all sorts (ships, planes, etc.) are often anthropomorphized as female, for example.
Isn't that tradition (or sentimentality)?
It's the default pronoun?
Windows doesn't have awk either. It should feel lucky it has Python :) I like where you're going with 'cut' its just that your example won't work with the input given. cut -d' ' -f8 &lt; inputFileName &gt; outputFileName (but you have to guess at the number of spaces for the field parameter, which is why I think most people prefer 'awk' where the whitespace is stripped like Python's .split() with no delimiter so the field is always the second one) 
This looks interesting. I'll give it a try :)
How is this approach compared to hookbox ? https://github.com/hookbox/hookbox
What the fuck is this? text-shadow: #CCC 2px 2px 8px; Get that shit out.
None taken. This article was written 3 years ago for a magazine. I've since ditched vim, so haven't updated it for current best practices.
I fundamentally disagree with the argument "If you program in python, you should use this python thing." I choose my tools on a variety of factors: usability, maintainership, community, license, convenience. Somewhere much farther down the list is what language they're written in.
Perhaps I should have phrased that differently, but it doesn't change the fact that UltiSnips (based on my experience with it and its peers) is superior to any of the other snippet solutions for vim. The icing on the cake is that, being that it's a python based solution, means that you get to use all the goodies you're familiar with as a python coder. Also, being that most snippet solutions for vim tend to be pretty complex, the ones written in a language like python tend to be more flexible and maintainable than those written in the native vimscript.
We just went through this with Pump. I'm not even going to address this again.
Congrats to the IPython team! This is a major accomplishment. plug: our free/oss plug-in for Visual Studio supports IPython, including running on dedicated or adhoc cluster of workstations: http://pytools.codeplex.com 
My response to this: webob.Request.get_response. PJE is addressing issues that, in my opinion, aren't real issues; weird corners of WSGI that most people don't care about. (Though now that he wrote it, I see that on exit most request environs are invalid and should probably be treated as such. I would consider poisoning webob requests after get_response; or maybe just use dict.copy because it's really pretty cheap.)
I don't know enough WSGI to make much sense of this? So... 1. Can I use this sexy new better-WSGI in place of regular WSGI? 2. Would it benefit me any to do so?
That's correct, they said so during their keynote at PyCon 2008.
&gt; Can I use this sexy new better-WSGI in place of regular WSGI? Yes. The "missing" WSGI bits are abstracted away from view, but they're still implemented and working. @lite functions can be passed to standard WSGI servers, which will call them just like any other WSGI application. It's just that, inside your code, you'll use a better-looking, more-compliant WSGI API. &gt; Would it benefit me any to do so? If you're writing WSGI middleware, libraries, or frameworks, then yes, absolutely. If you're writing WSGI apps, then maybe, depending on how much you use bare WSGI currently, what middleware you're working with, etc. The "argument binding" is the only big feature for people writing apps.
Agreed.
&gt; PJE is addressing issues that, in my opinion, aren't real issues; weird corners of WSGI that most people don't care about. That's more or less what some folks on the Web-SIG said when I proposed WSGI in the first place. Good thing I didn't let that stop me then, either. ;-) Seriously though, this isn't an attempt to *fix issues* -- it's an attempt to take things to a different level. In today's world, these things are weird corners and non-real issues because relatively few people are trying to build robust middleware. It's a similar situation to the packaging system, pre-setuptools: dependency tracking wasn't considered an especially important problem, because nobody was depending on anything, because they *couldn't*. In the same way, few people are writing WSGI middleware right now, in large part because they *can't*. Whenever you lower a barrier to entry, you create an influx of effort. All sorts of things exist now because of WSGI and setuptools that were not "real issues" until the solutions to those non-issues appeared. While I can't predict the future, my guess (based on the comments here on Armin's article a few days ago) is that what I've done with Lite is going to be a big help for some people. &gt; (Though now that he wrote it, I see that on exit most request environs are invalid and should probably be treated as such. I would consider poisoning webob requests after get_response; or maybe just use dict.copy because it's really pretty cheap.) See, this is exactly what I'm talking about. You've been around since the very beginning of WSGI and have written a ton of cool stuff for it... and you missed something that *basic*. Heck, so did I! The very fact that it's *this* frickin' hard to do it right, means there's something very wrong. Lite is an experiment at doing a WSGI that "fits your brain" better than the original, and at introducing a programming paradigm (argument bindings) that could be the glue for the next generation of WSGI frameworks -- or better still, WSGI libraries. &gt; My response to this: webob.Request.get_response. WSGI Lite is not a replacement for, nor a competitor to libraries like WebOb and Werkzeug. Rather, it provides a *level playing field for frameworks*. In the same way that pre-WSGI approaches to web app deployment relied on reinventing glue for every server, so too today are WSGI libraries forced to reinvent the same decorators and WSGI glue. In contrast, if @lite were in the standard library, there'd be no reason for say, Werkzeug to define wrapping decorators just to implement a different parameter calling scheme. Instead, it'd just provide a request binding rule like Request.bind, and people would use @lite(request=Request.bind) (along with whatever else they want to bind in from other frameworks). To put it another way: right now if I want to use alternate calling conventions or features from multiple libraries, I must pick *one* library or framework whose decorators I will use, and then do the rest by hand. With @lite, on the other hand, I can mix and match, which means I can get finer-grained reuse... which means people can make *smaller libraries*. I hope that explains the goal better here. The problem isn't that we're lacking for WSGI abstractions, it's that we're lacking *shared* WSGI abstractions. WSGI Lite is not a framework, and has no designs on becoming one. Like WSGI itself, the goal is to be framework neutral -- it should be just as easy to use one WSGI-oriented framework with Lite as another. I can understand where, if you've already written something that seems to have some functional overlap, it might seem like I'm reinventing wheels. But the goals of Lite run in a completely different direction than the goals of WebOb or any other individual framework. The intention here is not to be Wal-mart or a shopping mall, but a thriving downtown main street. In one sense, they're all interested in providing one-stop shopping, but I'm intending to support *collaboration*, rather than vertical integration. Again: this is not a "real issue" in the sense that nobody is clamoring for the problem to be solved. But removing this invisible barrier to entry should increase framework competition, decrease framework size, and increase code reuse in exactly the same way that setuptools did to Python packages in general.
interesting, i like the mapping of url components to file name, seems much cleaner then urls.py or w/e. however ^L seems unpythonic. assuming you are doing some kind of preprocessing to split the pages the token could be anything. personally I would prefer a match to r'^#+end (section name)#*$' .
interesting, i like the mapping of url components to file name, seems much cleaner then urls.py or w/e. however ^L seems unpythonic. assuming you are doing some kind of preprocessing to split the pages the token could be anything. personally I would prefer a match to r'^#+end (section name)#*$' .
Be warned, it's pretty rough round the edges and underdocumented. That said, it should do what it says on the tin. Feel free to shoot me any questions or bugs you run into.
The iPython (v0.11) comes with a new Qt based console which is quite nice. If you have the latest version installed and PyQt on your system you can load up the new GUI-based console with `ipython qtconsole`. One of the really nice features of this console is the embeddable matplotlib plots. Give it a try.
Yes, when the size of the exe is at *least* 20mb I find it very hard to justify it when I can just code it in .net (or IronPython) and create a sub 500kb assembly. Size does matter :)
Didn't you have an error at the end of the installation through homebrew? I have also echo $PYTHONPATH giving /usr/local/lib/python: Edit : nevermind that's working thanks!!
You have 2 basic options: width and depth. For width, go with [Python Module of the Week](http://www.doughellmann.com/PyMOTW/contents.html) and just learn about modules. For depth, read other people's code. [PEP-20 by Example](http://artifex.org/~hblanks/talks/2011/pep20_by_example.html) is a great starting point. Of course, the best strategy is to do them both. 
I use 'they'.
Maybe it's just me but the given example on pypi looks too complicated.
In every situation you must consider the right tools for the right jobs. There's no real/fake programming languages. If it works, it works.
Dupe: http://www.reddit.com/r/Python/comments/j59vz/wsgi_is_dead_long_live_wsgi_lite/
“Had to give up” hinted at an impossibility or a more serious incompatibility (like the trouble Armin encountered porting WSGI to Python 3); now it seems the problem is that configuration wasn't convenient enough. WSGI's configuration mechanism works the same with Django as without; here are [some examples](http://www.eflorenzano.com/blog/post/wsgi-middlware-awesome-django-use-it-more/) of using it with Django.
Learn Python, try finding best way of doing it, profile it. Start thinking in Python.Solve all your problems in python. If you learn new programming language compare it with python, how python does stuff and new X language does it.Engage in IRC, Mailinglist and answer or help people. Contribute to python core.Follow PEP standards, use stackoverflow.com, quora.com . Follow projects in github.com, code.google.com, share your code with peers and learn.
"Now it seems"? My first post on this says exactly what's wrong: *but that's Django's fault - WSGI is harder to configure for Django than its native pipeline system.* 
[Related HN topic](http://news.ycombinator.net/item?id=2830450)
URL mapping components to a file name for one. genuine question for you... what does flask offer that all the frameworks before it don't? perhaps you could be the flask fan that provides a convincing argument where so many others have failed. and I'm not saying flask is bad, at all. I just don't have a use for it. :)
you are being down voted by children apparently. I found this response to be reasonable. 
Great writeup. Shows the power of gevent+redis really succinctly. 
You are confusing classes, which you can think of defining the structure, and objects, which are instances of classes, which are things that have the structure. In order to test all of the items which you have dropped, you're going to need to keep track of those, by perhaps putting them in a list, and iterating over that list checking them one at a time. It might be apt to spend a bit of time reading the python tutorial or your favourite book on classes and objects.
Simpler, faster, and better integrated I'm guessing.
`webob.Request.get_response` isn't webob-specific, it's WSGI all the way down (as pretty much everything in webob is). Here's an example of latinator: from webob import Request def latinator(app): def middleware(environ, start_response): req = Request(environ) resp = req.get_response(app) if resp.content_type == 'text/plain': resp.unicode_body = piglatin(resp.unicode_body) return resp(environ, start_response) return middleware Or the identical middleware using `webob.dec`: from wsgi.dec import wsgify @wsgify.middleware def latinator(app, req): resp = req.get_response(app) if resp.content_type == 'text/plain': resp.unicode_body = piglatin(resp.unicode_body) return resp The wrapped application absolutely does not need to use webob. &gt; WSGI Lite is not a replacement for, nor a competitor to libraries like WebOb and Werkzeug. Rather, it provides a level playing field for frameworks. In the same way that pre-WSGI approaches to web app deployment relied on reinventing glue for every server, so too today are WSGI libraries forced to reinvent the same decorators and WSGI glue. Everything WebOb does is basically functions on the environ, just curried with the Request instantiation (or in the case of Response, curried status/headers/app_iter). WebOb is not a framework. So yes, this is a competitor for WebOb; which is fine except that WebOb provides a complete set of functionality, and in a prettier package. I don't really see what this adds to the conversation. It does try to do some more clever stuff with `app_iter.close()` and so on, but I don't see what real problems those solve.
(I haven't done what you're going but...) Have you tried using a multiprocessing queue? 
It has been a while since I used SQLalchemy, so take what I say with a grain of salt: The way I recall it, if you used the built-ins for all of your DB manipulations, all you would need to change is to make sure that you have the DB module available, and to change your connect string. If you did ANY manual inserts or SQL text, then you will need to test and possibly rewrite all of these. I'd recommend setting up a testing postgresql server you can use, and then using that to confirm function.
What do the first two lines do?
This actually looks a bit intimidating.
I happen to live at Singapore at the moment and I can tell you that it is a small Silicon Valley. Money flows everywhere, startups pop up everywhere and you have big communities for pretty much anything, especially python and ruby. I think you are right about the domain name, though... 
Any thoughts on how I could soften it up? It does assume some things, maybe too much?
&gt; So yes, this is a competitor for WebOb Nope, because I don't care whether anybody ever actually *uses* the wsgi_lite implementation, any more than I care whether anybody uses wsgiref. It's the *protocols* I care about, not the library, even if I'd like to see a reference implementation in the stdlib. And one of the things I want the protocol to do is to make "functions on the environment" into first-class WSGI citizens, instead of making users choose between, and incompatibly invest in, various monolithic "request" objects. So, it's only a "competitor" in the sense that I'm hoping it'll make monolithic Request objects a thing of the past. ;-) But it's not a competitor in the sense that anybody need ask "should I use wsgi_lite or WebOb?" Rather, WSGI Lite should change the game in such a way that the idea of needing to "choose" a WSGI library in that sense becomes an irrelevant question. I've posted a [follow-up article](http://dirtsimple.org/2011/08/is-wsgi-lite-library-or-protocol-and.html) on my blog, that explains this (and the significance of the other WSGI Lite protocols) in some detail. 
I also had a hard time understanding what this is, without an in depth understanding of how WSGI works. Would it be possible to state briefly what facilities WSGI_Lite provides without reference to how it differs from WSGI? If this is a simplified version of WSGI, I would think that a description of what WSGI_Lite on its own would be simpler (for someone new to the discussion) to understand that trying first to understand how WSGI is complicated, then trying to understand how WSGI_Lite removes complications.
I hope this will help: http://effbot.org/zone/tkinter-threads.htm "Just run all UI code in the main thread, and let the writers write to a Queue object"
After some feedback, I added a better explanation of how the Root and model are being traversed and I removed the talks about ACL and Permissions (since that is for another post). This should make the code easier to run out of the box.
What Armin said as a first reaction: http://twitter.com/#!/mitsuhiko/status/97991204289003520
Overall this seems quite nice, two points: - Is the binding mechanism supposed to handle regular parameters, i.e. ones extracted from the PATH_INFO? In principle, every "view"/handler of an application could then become a wsgi application, with the URL parsing/mapping/dispatching acting as a form of middleware. - Using yield in the callable bindings seems rather ugly. You're using yield to return a value so you can also signal "no value" by yielding nothing. Wouldn't using return for the return value and an exception of some sort be better for readability and whatnot?
&gt; Is the binding mechanism supposed to handle regular parameters, i.e. ones extracted from the PATH_INFO? In principle, every "view"/handler of an application could then become a wsgi application, with the URL parsing/mapping/dispatching acting as a form of middleware. That wasn't something I thought of specifically, but yes, absolutely. It's a good example, though, of what I think can happen as people start thinking in terms of argument bindings vs. direct argument passing or extracting stuff from the environment manually. &gt; Using yield in the callable bindings seems rather ugly. You're using yield to return a value so you can also signal "no value" by yielding nothing. Wouldn't using return for the return value and an exception of some sort be better for readability and whatnot? We *are* using an exception: it's called StopIteration. ;-) It's just that by using the iteration protocol, Python automatically throws and catches it for us. (You can, by the way, just return a sequence if you don't want to use yield for some reason. Either an empty sequence for no-value, or a single-item sequence for a value: i.e., "return [x]" or "return ()" will do the trick.) There's also a hidden benefit that I've not done anything with yet: you can write a binding rule that wraps *another* binding rule and gives you a list of all the alternatives it produces. That's another reason why the binding protocol is based on iteration. Anyway, thanks for the feedback!
I do this ROOT=/nfs/nsistore01/hadoop/ci04/opt export PATH=$ROOT/python266/bin:$PATH export LD_LIBRARY_PATH=$ROOT/python266/lib:$LD_LIBRARY_PATH export MANPATH=$ROOT/python266/share/man:$MANPATH and then source ( . ) this in my .bashrc You don't need a static python. This python is writable by me, and I can pip install anything I want.
From what i've tested at the time for a project i did , the best quality extractors are: 1. boilerpipe , written in java - but you can easily call it from python using jpype. 2. alchemy api , a free web service which gave me better results than boilerpipe. 
You can add a clicked method to the class definition of Item, and every instance of the item will then have the method. Probably, though, you'll want two methods. One that simply determines whether the item has been clicked, and another that does something when the item is clicked. Probably all items will do SOMETHING if they are clicked, but what they do will be based on the item. The SuperSprite object in my game engine already has this behavior, so you can use that if you want. I created the library as part of my book on Python game development: http://www.aharrisbooks.net/pythonGame/ (look at game engine at the bottom of the page.) You're welcome to the game engine and everything else on that site whether you get the book or not, but if you're interested, the book is here: http://www.amazon.com/Game-Programming-Line-Express-Learning/dp/0470068221/ref=sr_1_1?ie=UTF8&amp;qid=1312228492&amp;sr=8-1 Though it never sold very well, it does have outstanding Amazon reviews, so it might be helpful. Back to your example: Be sure you're thinking properly about the relationship between instances and classes. Chevy S10 is a class. It describes a whole bunch of trucks. All have various features in common, but the specifics change. All have a start() method, and all have an engine attribute. However, there's a difference between all trucks and a particular truck. My truck is a specific instance of S10. It has a particular color (black) and a few other characteristics specific to that particular truck. Many of its characteristics come from it being an S10, but I don't drive all S10s, just that one. There is a mechanism called "static methods" which allows you to assign a method to a class which can be run even if there is no instance of that class available (This is used all the time in Java programming, for example.) I don't think that's what you're trying to do, though. I would start with some sort of Sprite object. The one that comes with Pygame is pretty weak, so I always add enhancements to it (thus the SuperSprite.) I would then probably make (at least) two subclasses of supersprite objects - a Player class and an item class. If you're using my supersprite, there is already a clicked() method that returns True if the item is currently being clicked. Any subclass of supersprite can also read a click. Your player class will probably need some sort of function to add an item to the inventory. Python makes this much easier than many languages, as the built-in list type is far more flexible than a standard array. It's quite easy to add an item to the list with the append() method. Write to me directly if you want a bit more help. -Andy
Thanks for your help! Boilerplate looks like what I was looking for, except there actually was a python library that did the same thing. Hopefully I can find they python library instead of resorting to jpype.
[Answer from Guido himself](https://plus.google.com/115212051037621986145/posts/5udvdYs9mXq).
See the line in your code or preferably configuration file that says "sqlite://..." ? change it to "postgresql://...."
This assumes that .net runtime is on client machine.
&gt; Meanwhile, the poor user is left in the middle of the room, scratching his or her head and going, "Uh, so what should I use now?", with respect to any "enhanced WSGI" APIs. I have been on reddit for a while and I don't recall once seeing a user asking what WSGI framework he should be using. Users that don't know which framework to use don't quite care about WSGI at that stage. They just want to write their app smoothly. &gt; Or, to put it less dramatically, it is damn near impossible to write correct WSGI middleware because there are too darn many things to think about. &gt; I discovered all kinds of other mistakes that people could make in their middleware, that had never even occurred to me before. Then why not rid of middlewares altogether? Some frameworks have been built atop WSGI all the way and if they are fragile due to some corner cases in the protocol, then maybe WSGI lite is one good answer for those. In that sense, PJE is right saying it's probably more like WSGI 1.1. However other frameworks have decided to design things differently (Django, CherryPy come to mind) whilst supporting, to an extent, WSGI. Nonetheless, those frameworks believed WSGI was sub-optimal in one way or the other. Indeed, most of the time users barely mix WSGI middlewares with the framework's own architecture. What does WSGI lite provide to them? It doesn't strike me that WSGI succeeded in bringing framework communities any closer over the years and addressing that state with a technical response saddens me. Not that there are wars but when someone on reddit asks for what's the best framework, there still are as many responses as there are frameworks. I don't see what WSGI lite addresses specifically here. For one, I've always found the onion aspect of WSGI to be counter-productive in my web apps. They needed more granularity on a per request-path basis and I didn't want to have if/elif/else everywhere in my middlewares. It felt bloated to me. I'll be a cheap and use perhaps an invalid example but even a framework like werkzeug has almost [no middlewares](http://werkzeug.pocoo.org/docs/middlewares/). Pyramid seems to have its own way to interact with the request through [events](http://stackoverflow.com/questions/5010863/post-process-request-event-in-pyramid-pylons) (I'm not that familiar with the framework so apologies if I'm wrong here). Those two frameworks are heavily built on WSGI and yet seem to provide either no builtin middlewares or alternatives to them. Maybe middlewares are too low level, too generic, too raw, I don't know, but if they aren't that much used, why trying to fix them? If you could rid of middlewares and focus on the server and application sides, I'm sure things would be just as well. &gt; Let me repeat that: something to collaborate on, instead of something to compete over. I'm not sure the competition we may see today is the same we had a few years back, which was at times harsh and negative. Some frameworks have collaborated greatly (TurboGears and Pylons for instance) and it's fantastic, but why trying to bend the entire Web community towards a single way of doing things? Maybe WSGI looks apparently stuck when in fact it may not be simply because the community has picked up what was useful and left out what was not. 
It became a bit less intimidating when the indentation was fixed. I'll try to think about it tomorrow.
I think that a brief paragraph in the intro summarizing Pyramid's traversal paradigm (ie, what it's for, why it's useful, etc) wouldn't have been out of place.
Worst site documentation ever. I don't have a clue what it does, it expects me to download it before I even know if I need it, and doesn't even provide a basic description. The links are terribly named: "What next"? Why should I click that? By context (it mentions jinja somewhere) I infer it's a web framework, I don't know why it's "static" as the title says, or even what does being static mean.
While borrowing ideas from Jekyll is nice and all, sometimes I wish static site generator is configurable with Python itself rather than some external config file, or just give me a few class so I can build my own generator. Perhaps something like: generator = Generator(target='_output') resources = Reader('entries', ''_content/entries/(\d+)-(\d+)-(\d+)-(.+)\.markdown$') @generator.page('index.html') def index(): return render_template('index.html', entries=resource.entries) @generator.page('&lt;int:date&gt;-&lt;int:month&gt;-&lt;title&gt;.html', resources.entries) def entries(entry): return render_template('entry.html', entry=entry) if __name__ == '__main__': generator.run() 
Also have a look here: http://www.minvolai.com/blog/decruft-arc90s-readability-in-python/
[decruft](https://github.com/dcramer/decruft ) ? it's a python implementation of the readability (from arc90) algorithm
el_isma: Unfortunately the original poster has linked to what is supposed to be a demo generated site not Hyde itself. You should find this more informative: http://hyde.github.com/
First I should mention as I have in another comment that this is the place to go for more info, not the link you've given: http://hyde.github.com/ I used Hyde recently for a sideline static HTML project and liked it enough to add it to my toolbag. Its basic functionality is to render a series of jinja templates into a full static site. Picture the way PHP started off being used; to dynamically include footer, header, navigation, etc in pages, then think of that being done via a rendering process and you've pretty much grasped it. If you already use jinja, django templates you'll be quite comfortable with it and it's a useful starting point should the static pages need to have a more dynamic backend added later.
We're closing in on the 1.0 release and this release fixes the bugs reported since shipping RC1 about a month ago. So now's the time to let us know if there's any lingering issues!
Sorry for my late post to your question (although you may have already solved your problem). I use the following on my windows box to SSH to a shared Linux hosting account (should work on your Linux box). First download ssh.py from http://commandline.org.uk/forum/topic/420/ [edit] Sorry, the ssh.py download url from the above is now a 404. Here is the correct url: http://snipplr.com/view/48033/sshpy-friendly-python-ssh2-interface/ [/edit] def copy_to_remote(output_file): import ssh ssh_host = 'your_host' usr = 'your_username' pwd = 'your_password' ss = ssh.Connection(host=ssh_host, username=usr, password=pwd) ss.put(output_file) ss.close() Just add your own try-except stuff and you are good to go. 
It doesn't look like this example is showing where traversal is useful (it doesn't make any sense to me and i don't understand where traversal is used). Also, shouldn't the renderer vary depending on the client's Accept header? Perhaps a working example would make more sense.
I have a bit of cognitive dissonance going on with respect to WSGI Lite (the protocol). My first reaction is "great, that sounds much better!" My second dissonant reaction is that I'm not sure that adding feature which tend to encourage people to write more middleware is a great idea. I think the community is still a bit confused by the duality of WSGI as a protocol vs. as an app dev platform. While having middleware at all is wildly useful, it's not *arbitrarily* useful. People get really wrapped around the axle about what should be middleware and what should simply be a component in the application they're developing. It seems awful hard to dissuade people from using middleware inappropriately (e.g. developing "endware"). As examples: Routes (should not have a middleware component, should just be a library, as its setup requires deep knowledge of the application it's wrapping), exception-handling middleware like repoze.tm (txn management as middleware, but needs to examine the status codes of responses instead of simply rolling back on exceptions), repoze.retry (arguably the most complex middleware available; "reruns" a request some number of tries if there's a persistence conflict error, but requires cooperation from the application it's wrapping to not catch exceptions), authentication like repoze.who (dont get me started on trying to help people integrate this into an environment). Each of these packages is in wide use and generate very few complaints, so it's hard to say that using middleware for them has been a completely awful idea. It's just that something about explaining their edge cases and telling people to change their applications so they're maximally useful is always really unsatisfying. Some middleware, however, is wildly useful without conditions. Like profiling middleware, request logging middleware. Each of these is completely portable, completely useful, and a total no-brainer. It's pure win to be able to slot these sorts of things into arbitrary WSGI pipelines. If I think about it more, maybe I can detect what the patterns of things are which are totally appropriate as middleware vs. things that only sort of make sense. However, meanwhile, PEP 3333 further blurs the boundaries by using native strings instead of bytes for environ values. This has been said to be for "backwards compatibility" reasons, but it's not really. It's to make it less painful to write low-level WSGI applications and middleware. But I think there's a lot of tension between making WSGI a great *protocol* vs. making it a great app dev platform. I'm resigned to PEP 3333 as a practical matter, but I think we'd be better served by making WSGI into a great protocol instead of putting half-measures in that make it an only-sucky app development platform instead of a completely sucky one.
First line is what happens when you type ':make' in command mode. Second line is what the status-line reads during editing. Third line is what happens when you press F5 in command mode. Read more in the [vim documentation](http://vimdoc.sourceforge.net/htmldoc/quickfix.html)
Decruft looks like the one I was looking for, thanks!
I think that's the one, thanks!
Great, thanks.
Thanks for the code Jofer! I was trying to do something similar and your code was really helpful. Also I have found one issue testing the fast_kde(). I think the best way to show this is by a couple of plots. [Result with fast_kde()](http://i.imgur.com/t1Njj.png) [Result with normal_kde()](http://i.imgur.com/5GPsS.png) As you see i have used a set of correlated data (x,y) to do the plots. It seems that the fast_kde function has some issues with the y values in the grid and there is some kind of mirror effect. I have tried to find why this happens but I have not been able to solve it (too many functions I do not really know that well). Probably you can see it much faster that me. I am calling the functions just by: grid = normal_kde(x, y, gridsize) and grid = fast_kde(x, y, gridsize) so, no weights. 
Last time I was looking around at these, Blogofile seemed more extensible and less awkward.
Whoops! Quite right! At the bottom of the fast_kde function, it has return np.flipud(grid) It should just be return grid Don't know why I put the flipud in there.... Hope that helps!
If they don't work with the Express version, then it kinda misses the whole point of Open Source, doesn't it?
No. They have to work within what Visual Studio provides, and the express version doesn't support extensions like this. I'm not sure why that should reflect in any way on PTVS. Edit: Apparently I'm wrong and you can use the extension via the Integrated Shell as Dino notes below.
They do work w/ the [http://www.microsoft.com/download/en/details.aspx?id=115](free integrated shell) so there is a completely free option to use them.
Screenshots are too small (unreadable).
hi datadude, You actually have several options for getting VS for free or trial. The Integrated Shells supports adding PTVS (perpetual), students can get it via the dreamspark program &amp; startups can get it via the bizspark program. It's true that VS isn't open source but our little team has no control over that part... we're just pushing to keep PTVS free &amp; open source for everyone.
I didn't read the article, but is the answer building computational physics models?
Yep.
Which is a risk I am willing to take, rather than messing around with dll hell. I have found that for the most part compiling py2exe/pyinstaller on win7 with all the dependencies my project requires means I can only run it without issues on Win7 machines, whereas packaging on XP will run on xp, vista and win7. I have to go find an old XP machine or an XP serial and start a VM, which is a lot of hassle when you have to sync your code and all your libraries to them *just* to compile.
This looks really interesting, I have installed it and I will see how it goes.
Genuine question as I'll be finishing up some dev projects and could be looking into this. What's the performance in your opinion like? Is it worth doing in the long run? If I had access to the client machine, would you say it's 'better' to just run from the script files anyway?
My first thought was of [The Dunwich Horror](http://en.wikipedia.org/wiki/The_Dunwich_Horror) ! Not to belittle in any way the excellent achievement of this project and hard work of its developers, I'd need a lot of convincing that this is a good idea. My fear would be that Dulwich wouldn't keep up with core git functionality, and tracking down the cause of unexpected behaviour could become a nightmare. And that's not even thinking about speed - again I've not tried Dulwich but I can't imagine it would perform comparably with git itself. I'd probably investigate [libgit](http://libgit2.github.com/) if I needed this sort of functionality. Bindings for python, inter alia, are included. It's just a shame that libgit is an external reimplementation rather than the core of git itself, really.
&gt; It doesn't strike me that WSGI succeeded in bringing framework &gt; communities any closer over the years You really have to back that assertion up. Example: I work on a web framework (Grok). For some years it's been using paster now, and we integrate Fanstatic through WSGI. Fanstatic is also being used with Pyramid. Paster is being used by various other web frameworks too. Result: Grok &amp; Pyramid frameworks brought closer together. (slightly) Another example: I understand the standard way to configure Django with a web server is to use WSGI. This has brought it closer to other frameworks that use WSGI, right? (slightly) Knowledge can be transferred. I don't see WSGI as trying to bend the entire web community towards a single way of doing things. It's just about sharing common bits, like plugging into a web server like Apache. It's also about allowing people to write code (middleware, endware) that is somewhat separate from web frameworks. This makes that code easier to test. That's value enough all by itself, even ignoring that it also makes it easier to share this code between frameworks. But if you define as "bringing web frameworks closer" as: they are all just about the same thing now, then of course the answer is *of course that wasn't going to happen*. 
yes but no http protocol support !!! :/
Your examples that are imperfect are all endware. Do you think successful endware can exist? As that's where Fanstatic is positioning itself. Its originator was already messing around with the response object but in a non-WSGI and therefore completely web framework dependent way. Switching to using WSGI made the system cleaner and more powerful. (it also has a relatively clean API independent of WSGI. this is also important). I therefore consider it a successful use of WSGI, but using it in your application is a buy-in nonetheless. 
Use the [int() function](http://docs.python.org/library/functions.html#int) like this: s = "20" int(s) To find the sum of all the lines in the spending file: with open('spending.txt', 'r') as f: total = sum(int(i) for i in f.readlines())
Just because I love you: https://gist.github.com/1120117 (And there's only one screenshot)
Fair enough. Since I cannot back it up, I'm happy to retract that hypothesis. I do agree with you, though I guess I poorly phrased it, that pluggable servers are great. In my mind they are the most visible success of WSGI. I'm nonetheless oen to believe that middlewares failed (aside from a few common ones like logging or profiling) overall. I do not doubt that WSGI can be used to create libraries of frameworks, there are many but they tend to often cover the WSGI protocol with a higher level API providing a better developer experience. So in the end, it seems that most frameworks these days can indeed host a WSGI app but that app will stay foreign to the framework nonetheless, and this is why I'm advocating for droping middlewares altogether. It's too low level and inadequate to what most developers will want to do in the end, leading to very obscure bugs, not to mention endless discussions.
I can't quite figure out what problem this was meant to solve. Impressive work, but...
A very crude code fttomh: f = open('spending.txt', 'r') nums = [] for line in f.readlines(): num = float(line) # TODO: catch conversion exception with try,except nums.append(num) res = 0 for n in nums: res += n # I assume you meant adding the numbers together?
While it's most satisfying when middleware and an application need to know nothing about one another, if they do, it can still be a success. It's a bit of a gradient, I think, it's not binary. For example, although both require cooperation from a downstream application, repoze.tm2 is much more satisfying to support than is repoze.who, because the contract imposed between an application and repoze.tm is a lot less complicated than the one imposed between an application and repoze.who. I think Fanstatic will probably turn out to be on the "relatively satisfying to support" side of this gradient even if it does require cooperation from a downstream app to work. I hadn't really looked at it before now. It might get sideways with some people because maintains a global registry and uses thread locals, but I suspect only a few people care and it satisfies a common case. I would personally be likely to use it in cases where I knew those characteristics wouldn't be damaging (such as on a customer project); I'd be less apt to depend on it from another package that had arbitrary deployment characteristics though. I'm guessing that it will be most successful as "consultingware" but less successful as package that is depended on more generally by arbitrary frameworks. It's often useful to package the more libraryish bits of middleware packages into something that is just a library (and depend on those bits from the middleware package). It adds a lot of value to be able reuse code through normal imports instead of always needing to treat integration as a WSGI pipeline composition problem. It requires a lot more documentation, though, so it's rarely done.
Here's the pythonic way to do that: try: main() finally: pygame.quit()
This worked! This is very advanced for me, so I will evaluate it to understand better, THANKS!
Don't forget exception handling of course ;) try: with open('spending.txt', 'r') as f: total = sum(int(i) for i in f.readlines()) except IOError, e: print "IOError: %s" % e except ValueError, e: print "ValueError: %s" % e else: print total 
Killing boredom
this is a pretty easy way of doing it. All the other blogs seems to do it in variations of this way.
so....your "addiction" causes you sleep problem ? wtf exactly is a "python addict" anyways ?
The meat of it is really in lines 2 and 3 and the last line. The rest is just crude error handling (in this case: catching any exceptions and printing them to the console). The `try`/`else` is a nice touch. It makes sure the final statement is only executed if and only if no exceptions were raised.
Quick tip, don't do this: except: pass Always specify the error you expect (such as user input errors) so that if there is another unexpected error due to a problem with the code, it can be detected and fixed.
Without middleware we wouldn't be able to let Fanstatic work with Pyramid and Grok out of the box. "Dropping middleware" would make this impossible. 
I've recently been having to target python2.5 for a large project, so have been using unittest2 quite a lot.
Actually it should be possible use Fanstatic without using the thread local at all - I was aware of the use case. It will still set the thread local but the web framework doesn't need to refer to it to use Fanstatic (and with a little bit more configuration we could stop Fanstatic too from touching thread local at all in that case). When you do that, it's not possible to spell "jquery.need()" that way in your application code; you'd need to spell this differently, like this: env['fanstatic.needed'].need(jquery). There is still a special channel of communication; that can't be avoided in this case I think, but no more thread locals. Fanstatic's resource information is global, but it's equivalent to Python maintaining a global collection of modules, I think (and piggybacks on that). Not always handy perhaps, but not something we avoid Python for either. :) (though we've been having some thoughts of packaging resource libraries using a JSON description only, so we'd appeal to non-Python developers too) Fanstatic does offer a library API too - it was already there, and is now used to integrate with non-WSGI Django middleware. While you can use the WSGI components directly (and it's documented), I think it's actually usually far easier to integrate Fanstatic into your app by using the built-in paster configuration system - the WSGI pipeline stuff along with the paster configuration machinery help quite a bit here. 
Nice, libev support OOTB, with the potential to plugin libevent if you need to. Also glad to see that the dns issues have been resolved. The support for multiple threads/Hubs is interesting. Is there a way to balance event handling across the threads/Hub? 
for example [code.google.com](http://www.mail-archive.com/dulwich-users@lists.launchpad.net/msg00515.html)
Have you looked at the git [source](https://github.com/git/git) recently?
To me Fanstatic offers two things: * a lib to automatically handle static resources inclusions * a static application that takes care of serving statis resources. I can understand why the latter would be WSGI but then I see it as a WSGI app, not a middleware. The former could have been a pure python lib (making it useful in any kind of context like documentation generation). Why the need for a middleware? **edit:** Since I'm not familiar enough with Fanstatic, I had missed it already offered a pure python lib. So overall, you're offering in addition a middleware to simplify its integration in WSGI aware frameworks, am I right? That's okay but really it's a shallow argument in favor of middlewares in my mind.
Someone help me out. Why would code.google.com use a 0.x version of git-via-python when they could instead use git? freeload: I have not looked at the git source recently. Can you be more specific?
What the hell is a "change logger" ?
this has nearly nothing to do with computational physics, but mostly with novice student programmers doing homework assignments. I saw pretty much the same things happen when I taught "introduction to programming"--which was in Java. It also used an automated grading system, btw, so maybe that's a factor as well.
I always leave the try block off at first, mess with my function or line in the interpreter or pdb, and throw in things it might get, to give me the basic exceptions that I wish to catch. After that, I can build out more exceptions in testing. but yeah, the goal is to not catch anything that you don't know you'll be catching, or you'll run into an error you can't find immediately.
Hopefully these experienced programmers can make your acquaintance with version control. If not, I'd rethink your hiring process.
and if so, hopefully that version control system is a good one
I work on Google Code, so I'll give you a quick answer. Basically, cgit is a bunch of shell binaries that expect to call each other, and don't have the storage layer even remotely abstracted out. We need to be able to do clever things with bigtable, and already had Mercurial-on-bigtable written and working in Python. Internally, Git and Mercurial are similar enough that we could recycle a lot of the plumbing, so dulwich made sense in that respect.
There is an application that serves the resources. There's also an injector middleware that injects the resource requirements in the web page as &lt;script&gt; and &lt;link&gt; tags. Yes, it offers a middleware to integrate with WSGI aware frameworks. Why is that a shallow argument? Without this it'd not be possible for people to use Fanstatic out of the box in Pyramid, BlueBream and Grok (and presumably other WSGI aware frameworks too). Instead we have to write one middleware per web framework - but we don't know those web frameworks, and the users of those web framework users generally won't know Fanstatic well enough to want to invest the time up-front to integrate. So it'd be more communication for everybody, more work for everybody and less chance that people who would find Fanstatic interesting try out Fanstatic and adopt it. That's why we have standards and protocols - if people all follow it, it becomes easier to integrate systems. That doesn't mean it becomes necessarily *easy*, just easier, and it doesn't mean all standards and protocols are automatically something you should adopt, but WSGI has proven itself well enough in this department in my opinion. 
Why the call to raw_input()?
Guessing so you have time to read the output if you are running pyw?
I am sure they will. Is there anything you could recommend for python specifically?
Do *you* know what a version control system is? I can't think of anything a VCS would need specifically for Python. Your better bet is looking for how Eclipse ties in with your VCS. Also, you've "been developing a project for a while" and you don't already have a VCS? O_o
Sounds good. I think it would probably be most effective if you packaged and documented the libraryish bits of Fanstatic separately from the middleware bits; this would make it more obvious that you can use some portion of Fanstatic without using middleware (or WSGI at all). But probably nobody wants to do that (which is what I meant above by "rarely done").
I think I'm failing at conveying my point to you. I believe it's a shallow argument because, at least in the case of Fanstatic, the middleware is just a convenience function. It's not a core property of Fanstatic. Indeed it makes it easier for WSGI full stack frameworks but it isn't a mandatory piece of Fanstatic, is it? I can appreciate you making other's life easier, it's a good idea, but when I look at the Publisher, using Fanstatic doesn't strike as complicated for the average developer. One point that bugs me for instance is that Fanstatic is using WebOb internally. If my WSGI framework isn't using it, this means your middleware parses once again my environ, creates even more objects, only for your middleware to function. [This puzzles me](https://bitbucket.org/fanstatic/django-fanstatic/src/bc3e1a261054/src/django_fanstatic/middleware.py) for example since it seems that the publisher's code is doing so little, this could have been re-written using directly the Django's request. Don't get me wrong, I do like very much the features Fanstatic provides but the WSGI argument is just not working out for me. As a side note, I assume you wrote [your blog post](http://faassen.n--tree.net/blog/view/weblog/2011/08/02/0), based on our exchange and I'm surprised you focused on the server side since I've said from the beginning that it's been the one succesful bit from WSGI in my book. 
*I* do not know what a version control system is, which would explain why my google searches haven't been very useful up till now. However, with your help I have found [this](http://stackoverflow.com/questions/969121/eclipse-local-cvs-pydev) and [this](http://mercurial.selenic.com/) and [this](http://en.wikipedia.org/wiki/Revision_control). If you have anything to add, please do.
that was 'shot' with recordmydesktop. the key showing widget below is a patched version of key-mon: I won't stand for having a windows logo in my screencast! :) 
That would make Fanstatic a lot harder to maintain; it's not a giant system and it'd actually make the code harder to understand for a newcomer without the WSGI code there - as that shows how it can be used. What we could do is make the webob and paster dependencies optional in setup.py so that if you don't use them, you don't have to import those. This would be useful for Django integrators and to those with an allergy for webob. :) 
In the first version the value of i is not passed, so when the lambda is evaluated the program sees the value of i as 2 as there is no local i. In the second version, i is passed. So there is a local i with the value 0, 1, or 2. When the function is called, the lambda is executed, and 0, 2, 4 is printed by doubling the local i. 
absolutely!
Is this equivalent to what you can do with software like Maple and Mathematica?
So in the code "lambda left_part : right_part", I can access global variables from left_part, but not from right_part?
It wasn't [Kivy](http://kivy.org) was it?
Being able to natively (in Python) manipulate the various git data structures, without having to create command lines and call out to other programs, is a real advantage for people that use both.
I'm just as shocked as everyone else that you don't seem to know what version control is. But I'll try and be helpful as well ;) + [Git](http://git-scm.com/) + [Git casts, video tutorials](http://gitcasts.com/) + [Git in 20 commands](http://www.kernel.org/pub/software/scm/git/docs/everyday.html) + [Pro Git](http://progit.org/book/) + [Step-by-step Git](http://gitimmersion.com/) + [Easy VC with Git](http://net.tutsplus.com/tutorials/other/easy-version-control-with-git/) I recommend getting a [Github](http://github.com) account as well. It is free for public projects, or if you need private repos you can get a paid account. For a small team it is about $12 a month. But it will make things much easier when you are starting out. 
A core property of Fanstatic is an injector system that takes the HTML coming out of the web server and injects the required link and script tags. Another property is a delegator that publishes the resources to a special URL that Fanstatic can then refer to. You can rewrite the injector (and a publisher and a delegator and something that combines them all) without WSGI, but you'd have to target each web framework separately. I don't want to, I don't know how to do it for all these web frameworks, and now, thanks to WSGI I don't have to. You can argue it's trivial to explain to people to write this custom stuff themselves for their web framework. I disagree - there's quite a bit of trickiness we package in our middleware and you'd need to learn quite a bit before you can get it going. With WSGI, you can just plug it in and be done. That Fanstatic is well-factored so that the Python API and the WSGI bits are decoupled is just good code design, thank you. :) We use WebOb in our WSGI middleware because we're lazy programmers who want to make sure our middleware is somewhat correct without writing a lot of low-level stuff for no reason. Code reuse and all that. If you think it will lead to performance problems in actual use, show us. And then it should be fixable by writing more low-level code. But metrics first. Premature optimization and all that. The Django code doesn't use WebOb. Or WSGI, except that for some reason it checks it's in a WSGI environment. I'm not sure why and I just asked on the mailing list. The Django code probably uses the publisher directly because it's non-trivial stuff, look at publisher.py in Fanstatic if you haven't already. It might be needed to rewrite in that to integrate with Django better (perhaps that's why it wants to operate in a WSGI environment), but that's because the Django integration *doesn't* use the WSGI middleware. :) Finally, the blog post was inspired by this discussion (and similar sentiments I've seen in the last few weeks). You said this: &gt; It doesn't strike me that WSGI succeeded in bringing framework &gt; communities any closer over the years and that's what I wanted to address. Web server integration is one area where frameworks are a lot closer together than they were 5 or 6 years ago. I'd say that counts as "WSGI", right? Anyway - that wasn't directed at you (I'm talking to you right here) but at whoever else might benefit. 
In the first the components added to l are functions, which depend on the variable i. If you set: i = 5 You will get for x in l: x() ... 10 10 10 This is like wmil said: the value of i is not passed, so lambda will use the global i to evaluate. You are actually lucky to get away with it. If you delete the reference i via the command del i and the check the list again: for x in l: x() you'll get an error message. The reason is that there is that the functions x() try to access the value of i, which does not exist. In the second one you actually fix the variable to be equal to the value of i at the moment the lambda function is created. Another way of writing this is: for i in range(3): l.append((lambda x: x*2)(i)) Here you can read (lambda x: x*2) as a function f, while (lambda x: x*2)(i) is f evaluated at x = i. 
&gt; My fear would be that Dulwich wouldn't keep up with core git functionality The various git data structures are, at this point, fairly stable and well defined, so software that manipulates them should be relatively tolerant of bitrot.
I can appreciate your points and I'll admit Fanstatic is a good example that a complete drop of middlewares may indeed the wrong path. I still believe middlewares most of the time don't work out for my usage but I guess some of them are better ;) Thanks for the discussion.
Variables in the left part are evaluated when the lambda is defined. Variables in the right part are evaluated when the lambda is executed. You can access global variables on the right part, but the value won't be checked until the lambda is executed. In fact the variables don't need to be defined until the lambda is executed. So &gt;&gt;&gt;f = lambda : j * 2 &gt;&gt;&gt;j = 20 &gt;&gt;&gt;f() 40 &gt;&gt;&gt;f = lambda j=15 : j * 2 &gt;&gt;&gt;j 20 &gt;&gt;&gt;f() 30 (had problems with mardown and &gt;&gt;&gt; )
I think that creating middleware (or endware like Fanstatic) that's reusable between web frameworks is *hard*. But sometimes it's enormously useful to have that possibility. And making it possible doesn't cost much, because it builds on the same protocol already used for web server integration. Thank you for the discussion too; you've been unfailingly polite and graceful and it's much appreciated! I've learned from it. 
Now try that with a templated sprintf in C++2011 :)
Thanks.
Yessir i want this!
Correct
Good idea. I didn't use this because pygame.quit() is already called when the program is closed naturally.
&gt; When you do that, it's not possible to spell "jquery.need()" that way in your application code; you'd need to spell this differently, like this: env['fanstatic.needed'].need(jquery). There is still a special channel of communication; that can't be avoided in this case I think, but no more thread locals. FYI, this is precisely the sort of thing that WSGI Lite's binding protocol is for. i.e.: @lite(need = 'fanstatic.need') def my_app(environ, need): need(jquery) In this case, it's little more than syntax sugar, but you could replace the string with a function, if you need something more complex than just look up an environ entry. Avoiding thread locals and the like is one of the purposes of the binding protocol. There's also a potential use for what you might call reverse binding or configuration binding - binding functions can modify the environment, so you can use them to pass information down to called apps. For example, if you needed child apps to know the base URL of the point at which the 'need' binding was used, that information can be read by the need binding and saved in a library-specific key. e.g.: def need(environ): if 'fanstatic.top_need_url' not in environ: environ['fanstatic.top_need_url'] = wsgiref.util.request_uri(environ) # do any other setup here yield environ['fanstatic.need'] @lite(need = fanstatic.need) def my_app(environ, need): need(jquery) Notice that now the user performs the binding using an imported object, rather than a string. (You can of course just export the string to start with, so that you have the option of switching to a function later.)
Thank's for the recommendations. I probably won't be able to use Github for the same basic reason that I haven't been exposed to version control. The company I work for is very conservative when it comes to technology and software. I don't think anyone in the company is using any kind of structured version control. If you'll excuse me for ranting, every computer comes standard with IE 6, Windows XP and Microsoft 2000. I had to request Firefox and my first application was rejected because I failed to provide adequate justification. I would probably be better served by learning Fortran than integrating version control into Eclipse. Simply by using Python, I jumped so far ahead of the technological curve, there isn't anyone in the company I can turn to for help. So, thanks for the help. I really appreciate it.
Second this. Given how fast PyPy is getting these days, I think it is more interesting to compare to templatised C++ solution which the compiler often can optimise better.
I was about to suggest it too. although smartphones is mostly android, but there is a paper explaining it's current situation for the iphone. 
&gt; integrating version control into Eclipse Don't bother integrating it, just use whatever source control you choose as a standalone tool.
Is it possible that pypy is optimizing out the execution of the loop since nothing is done with the string? I know this was asked in the original article but I figured r/python would answer quicker. It's a serious question. I'm not trolling, I'm just a python noob.
I don't know anything about running Git on windows, sorry :( You don't have to use Github to use Git, it just gives you a central place for your team to push to with easy setup and a pretty file browser. You can just create your own repos to push/pull from - [here is some options](http://www.jedi.be/blog/2009/05/06/8-ways-to-share-your-git-repository/) You might also might want to look at SVN. I personally don't like SVN but [TortoiseSVN](http://tortoisesvn.net/) is supposed to be very easy to use and integrates easily with windows.
You could use [gui2exe](http://code.google.com/p/gui2exe) to try several of them out. You'll need Python 2.4+, wxPython, and the builders you're looking to evaluate (e.g. cx_freeze or py2exe). However, the wxPython toolkit is only Python 2.x for now -- and for the indefinite future. If you have access to the client machine you could use virtualenv and the system/local Python installation.
I don't think so, because it would be even faster. But you're perfectly right, both the python and the C implementations could perfectly optimize the loop out.
Also see [git.js](https://github.com/danlucraft/git.js), a git implementation in node.js
I gathered that Git would work independently from Github. I think SVN might be the go to program for this, but I have some time to make a decision. And thanks again for the help.
yes, in theory it's possible, but our JIT is no so smart (yet :-)). We checked the generated machine code, it actually contains the code that builds the string.
&gt; we're not here to just discuss the implementation of string formatting That's said, if you used Ken Thompson's C you wouldn't have problems with string sizes : http://plan9.bell-labs.com/magic/man2html/2/print smprint is like sprint, except that it prints into and returns a string of the required length, which is allocated by malloc(2). int main() { // so we can find it with grep -n '^main' (who needs IDEs!) int i = 0; char *x; for (i = 0; i &lt; 10000000; i++) { x = smprintf(x, "%d %d", i, i); free(x); } } If you're a C programmer, for the love of life, use KenC It's available for Linux (x86, x86-64, PowerPC, and ARM), FreeBSD (x86, x86-64), Mac OS X (x86 and Power PC), NetBSD (x86 and PowerPC), OpenBSD (x86 and PowerPC), SunOS (Sparc). http://swtch.com/plan9port/
Isn't this written in C though? edit: I was genuinely curious what it was written in, but I'm happy to ride the ignorant downvote train. 
Forgive me for being clueless: would this help if the formatting string was generated dynamically? (Not an unlikely thing to happen in a webapp.)
Interesting; I have to think through the implications of this. Um. No, need to think more. In fanstatic's case there's the use case of wanting to say: from js.jquery import jquery and then somewhere possibly deep in some widget implementation that happens to need jquery... jquery.need() i.e. there's no WSGI environment in sight. But this spelling has the drawback of needing some kind of hidden communication channel (such as a thread local) that isn't passed in. It looks pretty though and it works across any framework that buys into the thread local. Maybe we need to phase it out. But if we do, each framework, if it wants to avoid the low-level WSGI API, would need to wrap this stuff into its own implementation (on the request object or as a top-level function or whatever). 
Never construct a format string at runtime except you absolutely know what you are doing. Chances are: you do not :) While it's not as critical as format string attacks in C, it's still quite problematic in Python (dos attacks).
**2008** Python, PyPy, whatever... they will never be faster than C, that's just bullshit. C is very close to assembler and PyPy is a higher level language. Obviously the lower level language will be at least as fast, and most likely much faster. **2011** It's not fair to compare PyPy to C. C is just a low level language. To be a fair test you have to compare it to another high level language.
Right. I've constructed format strings at runtime a few times, mostly to display numbers with variable precision or padding.
[`asprintf`](http://linux.die.net/man/3/asprintf) forever!
The `printf` family already support variable precision and padding with constant format strings. Excerpt from [OpenGroup fprintf](http://pubs.opengroup.org/onlinepubs/007908799/xsh/fprintf.html): &gt; A field width, or precision, or both, may be indicated by an asterisk (\*). In this case an argument of type int supplies the field width or precision. Arguments specifying field width, or precision, or both must appear in that order before the argument, if any, to be converted. A negative field width is taken as a - flag followed by a positive field width. A negative precision is taken as if the precision were omitted. In format strings containing the %n$ form of a conversion specification, a field width or precision may be indicated by the sequence \*m$, where m is a decimal integer in the range [1, {NL_ARGMAX}] giving the position in the argument list (after the format argument) of an integer argument containing the field width or precision, for example: &gt; &gt; printf("%1$d:%2$.*3$d:%4$.*3$d\n", hour, min, precision, sec);
What's unfair is comparing one part of each language and saying one is faster than the other [in some very trivial example] without considering the remainder of each language. PyPy may be faster than C to format the same integer twice, but what I want to see is a comparison on a much larger scale. I'm sure I could find many trivial examples in C that will be faster than the PyPy equivalent, but it doesn't prove much.
These "faster than C" blog posts pop up faster than c!
I've experimented with a variety of ways to achieve such "contextual variables", but the truth is that they all boil down to one of two basic implementations: either you walk the stack or use a thread-local. There really aren't any other ways. (Technically, walking the stack is just a way of accessing an existing thread-local, i.e., the current stack frame for the current thread.) The trouble, though, with implicit coupling in this case is that suppose you want to pull something from a WSGI sub-request and reformat it, and "deep in some widget implementation" this gets called... you then end up with the injection occurring on the wrong subrequest. It seems to me you need a way to embed the "need" information in the response, rather than simply tossing it up to an interceptor. (Or conversely, to have some notion of a page or head object that you can talk to, which is then told about the need for the resource. Granted, this still has to be passed down to the widget in some way.)
That's the one thanks dude!
The whole point of this article is exactly to say that PyPy is faster in this example. PyPy can make some optimizations that C isn't doing. In this case, it's about specializing a particular function call with a string whereas C has to reparse the format string each time.
SVN requires a centralized repository. Git is distributed, which means you can keep a local copy of your changes and sync them with other people's changes whenever you want. You can still keep a centralized repository if you want, so there's really no advantage to using SVN. Since you're new to version control, I would recommend mercurial. It's pretty much the same thing as git, but has a simpler set of commands, so it's a little easier to get into. edit: also mercurial is written in pure Python, so there are no issues running it anywhere Python runs.
We have a "needed resources" object that gets set in the environment before the underlying WSGI app is called, and is then told about the need for the resource. If a subrequest is made, this will affect the main request, because of the thread local. This may or may not be what you want. If a thread local is not used and a 'needed' object is passed down by an interceptor this problem would be avoided, though, right? We really only have to stuff it in the thread local to allow the convenience spelling. I need to think about a way to replace it. 
I've added a second video of vim-ipython 'shell' as an update to the same post
Another way of viewing this situation, though, is that we really need to start demanding much better link-time optimization from our C/C++ compilers. When a dynamic language can kick your tail by making such a relatively simple optimization that you can't, then shame on you! ;-)
No, actually PyPy is largely implemented in RPython, which is part of what makes the project so impressive.
Very interesting, thank you for clarifying. 
and just run all your carefully designed tests again... you have automated testing right? ;)
Thank you jofer. Now it works perfectly and it is clearly much faster!
But if I understand correctly, you have to run at least 1 pass of the loop to make sure nothing is done by the loop. Am I right ? Or is this optimization done by analyzing the AST, even before the code is run ?
Isn't RPython turned into C ?
This kind of optimization is definitely done at compile time. We had a Discrete Structures professor who was a compiler specialist try to teach us some of it; luckily it was some last-week-of-the-semester-not-on-the-final material.
Usually it is.
from what I understand, pypy can do a number of things with python code - translate into some other languages, execute it using it's own vm, or in this case, enhance the vm by doing JIT compilation straight to assembly. The key point this article points out is that pypy can jit-compile a function multiple times, each *with different types of inputs*, whereas a C compiler has to compile only one version of a function.
So, find something C is slow at, and then benchmark against it? Isn't that picking low-hanging fruit? Why not find something C is exceedingly good at, and benchmark against *that*?
&gt; python setup.py configure --zmq=/path/to/zmq/prefix http://www.zeromq.org/bindings:python
Using a default value for an argument is a nice way to get around this issue. here are other ways you might do it: l = [] for i in range(3): l.append((lambda x: lambda: x * 2)(i)) import functools l = [] for i in range(3): l.append(functools.partial(lambda x: x * 2, i)) 
This isn't really a benchmark. This is a nice feature, and a simple demonstration of the potential of dynamic compilation. You make it sound like someone is being dishonest here. I think you are. &gt; Why not find something C is exceedingly good at, and benchmark against that? Because the things C is exceptionally good at are not things that python is good for? C is exceptionally good at pointer manipulation, dealing with binary files, structures in memory (say, for operating system development), etc. Python is just bad at that, and nobody who seriously needs to do that stuff thinks that Python is the best tool for the job (disregard unununium). Why would you benchmark something that people don't need? Interestingly, PyPy could make those things cheap and easy in Python, but it would be basically inventing its own language at that point.
I guess to dispel notions that C is the highest we can get in performance. For a dynamically typed interpreted language to actually beat C at something is still a pretty awesome accomplishment. It's not as significant as it might be hyped up to be, but it's more symbolic than anything.
this isn't the same as what the OP was going for. it fills `l` with integers, whereas the attempt was to fill it with functions that each produce different integers. the test is that this produces the desired result (rather than "TypeError: 'int' object is not callable"): for x in l: x()
But you don't need to compile C functions with multiple types of inputs -- its not a dynamically typed language. Yes, yes its weakly typed, but exploiting weak typing is bad practice and not something you should optimize a compiler for.
You raise another point here; the speed comparisons are done against a particular implementation of C. Perhaps a less misleading title would have been "PyPy string formatting faster than GCC+glibc". There are no "rules" that C must be statically compiled. Personally, I'm just sick of seeing articles about "xyz language is faster than C!" and then finding at most a trivial example; sometimes even with poorly written C code. I was surprised to find that PyPy was faster in this case, and I think that it's really great. When I see "xyz faster than C" articles though, I want to see actual real-world cases where using xyz would be beneficial.
What is slow here in C is "what is inside the shared library must be static". Pretty fundamental, hardly to be considered "low-hanging fruit". If what is inside the shared library can become dynamic, it is not worth calling C anymore. PyPy is to change the economics of what is feasible to implement using a high-level dynamic language. Speedups like this are one part of that.
As another former Python user, I agree. Ruby is leading the pack in writing complex and testable web applications. After reading the RSpec/Cucumber book, I think the language itself deserves a lot of credit for this. Ruby lets me put logic into the computer as simply as possible.
Without starting a flamewar, this is ultimately why I left Ubuntu for Debian Sid. This is the Linux equivalent of DLL hell. The only way to beat it (for good) is to stay on the bleeding edge. This, of course, is a double edged sword.
I think C++'s templates show there is some utility in a statically typed language having a single piece of code compile to multiple type signatures. But I'll cede that point, as my counterexample isn't quite the same as a single function, and most C programs tend to stay away from the need for such things to begin with. Also, specializing for different types was probably not the right thing for me to emphasize about PyPy. It's a great feature, but it's also pretty much required if a dynamically typed language is going to have comparable compiled performance. Like advertising that a sports car comes with "engine included". I should have just referenced the example in this article, which showed PyPy's JIT unrolling and inlining a function *with a constant argument* and compiling assembly for that precise situation. This is definitely a useful optimization, but one that isn't really done by any non-JIT compilers. Not that they couldn't, but correctly profiling the program during compilation to find such spots is tricky; and storing the resulting specialized compilations ahead of time would bloat the object file. So they made a sane tradeoff, I don't fault the designers for it. But it *is* an advantage PyPy has: a JIT compiler like PyPy's can do even better than type-specific optimizations, it can optimize for specific constant inputs, across call boundaries. And it can do it all on the fly, re-profiling as the program's behavior changes, creating/discarding compiled versions as memory limits require; thus avoiding the equivalent of the bloated object file. This flexibility allows PyPy to pursue optimizations beyond what C is capable of, and to tailor them more closely with the runtime environment. *That* was the point I was trying to make, just not very effectively :) --- edit: tried to straighten out some sentences. shouldn't post while eating.
I'm actually in the process of trying to set up a blog using this, I was attracted to it because it seems a lot lighter than Django while retaining the Django/Jinja templating system. The lack of documentation makes me want to pull my hair out. The hyde.github.com site has broken links, blank pages, and sparse documentation. There are nearly no tutorials or community posts around it. The github page is so outdated that the command line arguments to generate and serve the static content are no longer functional. If I get through to making something work, maybe I'll put together a quick guide on my new blog. Coming from Django (which has some of the best documentation I've ever used), maybe I'm spoiled. On the other hand, if someone can tell me what's going on with this code, this looks like it has real potential.
You can't do TCO here. You can only do TCO when the recursive call is the last call (hence Tail Call) in the function. In this case, the + is the last call; a given rfib call must stay on the stack until it's two child rfib calls resolve so that it can then call +. Memoization would be a different speed test altogether. It would be measuring dictionary (or list) lookups rather than recursive calls. Edit: Clarity 
AFAIK, unittest2 doesn't have anything built-in that solves this. Does it?
I tend to use the asterisk works great for strings that use a separate length variable instead of being null terminated: printf("%.*s", len, ptr);
On 1.5 yes, on PyPy trunk this is no longer the case. PS: recursive fibonacci is a braindead benchmark, please benchmark somethign real.
They offer many of the same features but they don't work nearly as well in my experience. For example, it requires an amazing amount of work to develop a useful automatic symbolic integration library that is comparable with Mathematica. If you want to do something and you verify that it works with sympy then you are good to go, but I don't think that it can really compare to the best closed source projects yet. That being said, I like the project a lot and it is a pleasure to use!
To show that PyPy can be faster than people would expect.
&gt;I thought it had to do with deferred operations where there's more work to be done in the current context after a recursive call returns. This is what prevents TCO. You can only remove the calling function from the stack if there is absolutely no work left to be done by it. You can eliminate tail calls in BFS if you push all child nodes onto the queue and pass the queue as a parameter... is that what you're referring to? &gt;You'd think that could be optimized away by doing them concurrently if they're pure functions with no interdependencies (which these are). That means that each call would spawn two new processes and no process could terminate until all child processes do. That would be O(2**n) processes. That would be really cool if we had some huge number of cores in our computers though! &gt;Basically the optimization would have to be really stupid so that if there's ever any state to be saved then every recursive call becomes unoptimized and tries to save all the possibly non-existent locals in between calls. I'm not sure I follow. What's the difference between this and normal tail call elimination? 
Interesting I#m trying to teach my ten yo nephew Python. 
But then it's not a fair comparison; it's a misleading comparison.
I recommend you put the C:\\PythonXX\Scripts folder on your PATH. You can put bat/cmd files there. If you define PYTHONUSERBASE, a Python 2.7 package installed with --user would install to %PYTHONUSERBASE%\\Python27\\site-packages, and its scripts would install to %PYTHONUSERBASE%\\Scripts. In that case you'd want that Scripts directory on your PATH also, and you could put your cmd scripts there instead. You should be able to directly run a .py file with the default interpreter, so you could run just do @C:\Python27\Lib\pydoc.py %* If that doesn't work, check that `assoc .py` returns `.py=Python.File` and that `ftype Python.File` returns `Python.File="C:\Python27\python.exe" "%1" %*`. Also, check out [pylauncher](https://bitbucket.org/vinay.sajip/pylauncher). It takes over as the default launcher to add Unix-style shebang support.
RPython currently can be compiled to c, java bytecode, msil there was a javascript backend once i think someone is working on a action-script bytecode backend
I've been thinking about this a bit, and I think what I want (and know I won't get) from a WSGI-like app development system is something like the below. # middleware def retry_handler_factory(handler, config): """ A handler factory that retries a request some number of times if it detects that a downstream application raised a certain exception. """ retries = config.get('retries', 0) retryable = config.get('retryable', (ConflictError,)) if not retryable or not retries: return handler def retry_handler(request): i = 0 while True: req = request.copy() try: return handler(req) except retryable: i += 1 if i &lt; retries: continue raise return retry_handler def txnmgmt_handler_factory(handler, config): """ A handler factory that creates a handler which either commits or aborts a transaction based on a combination of examinination of a downstream application exception or response attributes (via 'commit_veto'). """ import transaction commit_veto = config.get('commit_veto') def txnmgmt_handler(request): try: request, response = handler(request) except: transaction.abort() raise if transaction.isDoomed(): transaction.abort() elif commit_veto is not None: try: veto = commit_veto(response.status_int, response.headers) except: transaction.abort() raise if veto: transaction.abort() else: transaction.commit() return request, response return txnmgmt_handler # framework logic def serve_handler(handler): import webob from paste.httpserver import serve def app(environ, start_response): request = webob.Request(environ) request, response = handler(request) return response(request.environ, start_response) serve(app) # application logic class ConflictError(object): pass from webob import Response def main_handler(request): response = Response('OK') return request, response config = {'retries':1, 'retryable':(ConflictError,)} handler = main_handler handler = txnmgmt_handler_factory(handler, config) handler = retry_handler_factory(handler, config) if __name__ == '__main__': serve_handler(handler) Salient points: - I'm using WebOb (or at least objects that implement its API). I'm doing this because the WSGI Lite proposal is somewhat geared towards making WSGI a better *app dev* environment, and this is a place where WebOb already shines. - Having a bare environ is great if WSGI is just a protocol. If it's however meant to be an app dev environment (in the form of middleware composition), though, I fear that having a real request and response abstraction is just required. I'd really rather not to have to wrap the environ with a sensible API in each piece of middleware and then unwrap it to go back to "protocol mode". This has a cost, and the cost is inappropriate in some applications. The WebOb Request "__setattr__" which jams values into the environ is particularly expensive. This might be an ignorable cost, but I think we'd need to do some benchmarking before we could declare it noise. - Not having a sensible, centralized way to pass configuration settings to middleware also makes sense if WSGI is just a protocol. If it's an app development environment, though, configuration settings become really, really important. It would be sensible to not ignore this requirement. The WSGI1 variants of the above middleware (retry middleware and txn mgmt middleware) are much, much longer and much, much more complicated than the above. I may try to convert them to WSGI Lite (perhaps rewrapping with WebOb) to see how it would look. If so I'll report back.
Being written in c does not necessarily mean its slower then c although since c is usually faster then other languages this is usually the case. You could make a c compiler in python that generates fairly quick binary.
I don't always upvote PyPy trunk posts, but when I do, I upvote it 4.3 times.
No one knows what is it written in.
Since you are on Windows I recommend using [Mercurial](http://mercurial.selenic.com/) instead of Git. It is simpler to use and is better supported on Windows with [tortoiseHg](http://tortoisehg.bitbucket.org/). There is a good [tutorial by Joel Spolsky](http://hginit.com/) on what mercurial is and how to use it. There is also a complete book available online - O'Reilly's [Mercurial the Definitive Guide](http://hgbook.red-bean.com/read/) 
It sounds like your company is stuck in the 1980s. Either become a change agent and try to drag them into the 21st century or get the hell out. I find it astonishing in this day and age that any company doing software development does not use version control. Without it any development work become an unmaintainable nightmare. 
pip will often install newer versions than apt. I hardly ever install any python packages from apt.
Thank you for the info. Do you think maxima is more featured than SymPy as of now?
Also, never use timeit. It disables cyclic garbage collector on CPython making it faster for no good reason.
&gt; I can't quite figure out what problem this was meant to solve. 1. playing around with git repos from python software without having to call (or even have) the git binary and need to parse its stdout. 2. hg-git interop (hg-git lets hg users clone and push to git repos, and google code uses dulwitch to support git access on top of their preexisting hg implementation)
Here's a version which uses WSGI Lite. It includes speed and profiling data at the bottom of the file in a docstring: http://paste.ofcode.org/kzhqj7KpfV43BaPTFCh3dX The speed and profiling data present in the paste above can be compared against a variant of the "handler" code I added in the comment above at http://paste.ofcode.org/33Jmse5TTMtXAXBekLkSGCJ The "feel" of the WSGI Lite variant is fine. It performs (predictably) more slowly, due to its need to call the webob Request constructor more than the other variant. Maybe I can figure out some way to cache the request object instead of recreating it every time.
You are correct, yes. Another lambda is needed.
Thanks! This has been a great help. I've spent a good deal of time picking over gameEngine.py and I've learned a lot of interesting things. Once I'm no longer in debt I might buy your book. :P
&gt; This is fine, except you can't even return x from this function, a more fair comparison might be: ... superfluous use of malloc ... That's true if you're writing "Python in C", having 0 consideration of performance. Respectable C programmers write C in C. That means they rarely use malloc. In this case, they would pass around an allocation from the caller, which is likely to be stack-allocated, or part of a larger heap allocation (for which the malloc overhead is spread across multiple objects), or a data allocation (part of some global instance). 
meh, looks awful, you need *two* variables and you can't just check for null (unless you are on BSD, possibly leading you to write erroneous code; it will compile on Linux but not tell you your logic is wrong. The man page tells you it is part of GNU C but doesn't warn you of this problem - well done everyone.). Which looks nicer ? char *c; int len = asprintf(c, "%d %d", i, i); if(len &lt; 0) // error vs char *c = smprint("%d %d", i, i); if(!c) // error Also, unlike all of KenC, you can't use UTF-8 in asprint &amp; friends. BUGS The printf family of functions do not correctly handle multibyte characters in the format argument. 
Butterflies plugin for emacs!
Here it is with a tail call: import timeit def fib(i): def tfib(f0, f1, r): if r &lt;= 0: return f0 return tfib(f1, f0 + f1, r - 1) return tfib(1, 1, i) print(timeit.timeit( stmt="fib(500)", number=5, setup='from __main__ import fib')) * pypy-c-jit-46216-109f80dac1c1-linux64 (current nightly): 0.0545928478241 * CPython 2.7.1-0ubuntu5: 0.00101613998413 
Strange, Google just deployed it for their code hosting so it must exist. Edit: I can serve http with `dul-web`, but the `dulwich` client fails when parsing the url.
&gt; Maybe I can figure out some way to cache the request object instead of recreating it every time. I'm not clear on what you're using the request object *for* in either variant.
&gt; I'd really rather not to have to wrap the environ with a sensible API in each piece of middleware and then unwrap it to go back to "protocol mode" So don't do that. ;-) Your examples don't seem to actually *use* a request object in any meaningful way, except to copy it or pass it along. To put it another way, what is the domain-specific "request" needed by your middleware components? Each piece of middleware is only going to do a subset of the possible things a "request" object would do, and you can simply bind that portion of the API, and only that portion at that level. The binding operations are the equivalent of method binding, in the sense that when you retrieve a request.method, you are binding 'method' to the request object. In WSGI Lite, binding an argument to the environ is like retrieving that method from a request object. So, you don't need to create a whole new request every time you need a request, you just bind what you need. And your examples don't seem to need a request at all; you could just use environ.copy() instead of turning the environ into a request and back again.
Anyone have experience of using PyPy? Any pitfalls? Anything extra awesome about it?
I find this much adorable than even the finest cat picture. Thanks for sharing; reading this made me a bit nostalgic.
&gt; recursive fibonacci is a braindead benchmark What makes you say that? What essential criteria does it lack?
Well, yes, unless you want to do something with this data (like pass it around say in webserver). The typical way would be to take the stack allocated buffer and copy contents somewhere else in case you need it, which costs even more than malloc. Either way, the stack allocation does not help the example that much.
It would if the string you generate changes only rarely in the loop.
relevance to any sort of python program that's encountered in a wild. In theory people *might* write code like this but they don't. Also, even if your job is to count fibonacci numbers (I do this a bit before bed each day), you would rather use a linear loop instead of exponential recursion. And yes, because people think it's a cool benchmark, there is an optimization precisely for the case of small functions exponentially recursing. Just not in 1.5
the pitfall is the gc (its not ref-counted, you might "leak" files/resources)
How does the GC work? Does one have to manually deallocate resources, or what?
TIL. Neat. Not applicable in Python, of course.
Can you explain how is it misleading?
no, its a real gc, which means you don't get "realtime" deallocation if something goes unreferenced, instead the next gc run will deallocate
did you try pip install pyzmq-static ? 
Yes it is. From [the docs:](http://docs.python.org/library/stdtypes.html#string-formatting-operations) &gt;A conversion specifier contains two or more characters and has the following components, which must occur in this order: &gt; 1. The '%' character, which marks the start of the specifier. &gt; 2. Mapping key (optional), consisting of a parenthesised sequence of characters (for example, (somename)). &gt; 3. Conversion flags (optional), which affect the result of some conversion types. &gt; 4. Minimum field width (optional). **If specified as an '*' (asterisk), the actual width is read from the next element of the tuple in values, and the object to convert comes after the minimum field width and optional precision.** &gt; 5. Precision (optional), given as a '.' (dot) followed by the precision. **If specified as '*' (an asterisk), the actual width is read from the next element of the tuple in values, and the value to convert comes after the precision.** &gt; 6. ... 
Is there a good alternative?
No, I've logged in through the command line using newsblur.login. 
Ah, the super secret password program. Surely a lot of people's first 'real' program. I did the same thing at the same sort of age on an Atari 800.
It can be frustrating at times, but I am changing things within my small area of influence. I think the problem for the company at large is the refusal to see themselves as software developers. They write code, maintain code, use it almost every hour of everyday, but still view it as their tertiary, or quaternary interest. 
/usr/bin/time and a loop works for me
1) Great idea 2) I've edited the post to reflect this. 3) I downloaded and installed pylauncher, thank you!
If you're like me and counting down the days, there's [isitpyconyet.com](http://www.isitpyconyet.com/) to help.
In addition to what the other posters said, try not to use lambda so much. functools, itertools and operator are useful modules that make most uses of lambdas redundant.
My C code has very little copying and malloc'ing. You just make the caller allocate and pass these allocations around. No need to copy.
I don't see any cheats, I always enjoyed putting in cheats. I guess I wasn't good at guessing.
[Sage](http://www.sagemath.org/index.html) is probably the closest Mathematica equivalent you can find. Internally it uses both Sympy and Maxima and [whole host of other libraries](http://www.sagemath.org/links-components.html) as well. 
Here is the code from the OP with comments explaining the behavior suited for a .py. # #In the first version the value of i is not passed, so when the lambda # #is evaluated the program sees the value of i as 2 as there is no local i. l = [] for i in range(3): l.append(lambda: i * 2) for x in l: print x() # #In the second version, i is passed. So there is a local i with the # #value 0, 1, or 2. When the function is called, the lambda is executed, # #and 0, 2, 4 is printed by doubling the local i. l = [] for i in range(3): l.append(lambda i=i: i * 2) for x in l: print x() # #Variables in the left part are evaluated when the lambda is defined. # #Variables in the right part are evaluated when the lambda is executed. # #You can access global variables on the right part, but the value won't be # #checked until the lambda is executed. In fact the variables don't need # #to be defined until the lambda is executed. # # http://www.reddit.com/r/Python/comments/j6uti/this_code_executes_totally_not_as_i_expect_it_to/
I am going to give this a try on the integrated shell. Thanks guys.
Right, I don't see that as a negative.
I wonder if he has an email address, I would love to send him an email with some encouragement
good article btw :)
You can just comment on the blog - he reads all the comments.
ohh i remember my secret password guess program.... 
Python is very much a web language. You should use something like Django if you want a framework with an abstraction or library for cookies. You'd use Python 2.7 instead of 3.1, but there's no reason not to. It would be a bad decision to use Python 3 if that significantly eliminated choices of libraries for your project. Working with cookies can be as easy as this: https://docs.djangoproject.com/en/1.3/ref/request-response/#django.http.HttpRequest.COOKIES https://docs.djangoproject.com/en/1.3/ref/request-response/#django.http.HttpResponse.set_cookie
True, gah. I chose two things that have been midly successful as middleware already, and they're too easy already. I'm hoping you can extrapolate these requirements; you've already been talking about very high-level components implemented as middleware (e.g. auth). I'm not going to write repoze.who in Lite, sorry, it's too big. ;-) Wanting to be able to use some request/response abstraction in middleware if we're going to be pushing this as an app dev API is going to be very, very common. All nontrivial apps will do at least one of the following (usually more than one): a) compare path_info b) set cookies in the response c) read POST data d) read the request body, e) convert request parameters to Unicode, f) generate URLs. I realize no one is going to agree on a request API. But it would be wildly useful if there was a way to permawrap the environ so that you could do, e.g.: def maybe_new_request(environ): request = environ.get('myapp.request') if request is None: request = webob.Request(environ) environ['myapp.request'] = request return [request] @lite(request=maybe_new_request) def mymiddleware(environ, request): path_info = request.path_info return ('200 OK', [], [path_info]) Obviously this is going to cause a cycle. I don't really know how to fix that. And too many function calls kill performance dead, so even this is a little much.
And actually, no. Using request.copy() for the retry middleware gets us out of the misery of having to personally copy wsgi.input every time. So yes, I am using a webob feature there.
At PyOhio, there was a talk call [Php To Python with No regrets](http://pyohio.org/schedule/presentations/17/) in this talk, he call out some code that does what you're looking for. His talk is not posted yet, but keep an eye out, it will be soon.
As I mentioned above, I'm using request.copy() to not have to manage the copying of 'wsgi.input'.
My problem with Django is it is always over whelming, I install it load it up and i am confronted with an alien webpage, that touts it is easy to customize, but i am at a loss as to where to even start to remove all the garbage. Also I do not know python 2, I know the differences aren't that huge but I started on 3 and would like one less thing to confuse me during this. i will look into the Django thing a little bit though, thank you.
I don't think you want to do that in Python because of GIL. Better to use multiple process workers, if you can. See [gunicorn](http://gunicorn.org) which implements pre-fork model and can balance connections across several gevent-running processes.
WHY MUST YOU BE DURING FINALS WEEK
Looks good, I will try to keep an eye out for this. Thank you.
Thanks. :)
The Bottle documentation includes [an example of how to get and set cookies](http://bottlepy.org/docs/dev/tutorial.html#tutorial-request). If you want to store arbitrary data associated with a cookie then you will have to do extra work - Bottle (and Python) doesn't provide server-side storage for cookie data (PHP does that for you, as does Django). But conceptually you would need some kind of database storing objects / values keyed by whatever special cookie value you set per-client. Bottle's documentation has more about [cookies, and in particular how to use signed cookies](http://bottlepy.org/docs/dev/tutorial.html#tutorial-signed-cookies). Also ~~[/r/bottlepy](http://www.reddit.com/r/bottlepy) and~~ [the Bottle mailing list](http://groups.google.com/group/bottlepy). EDIT: you already know about Bottle's reddit
Choosing between a C library with python bindings and a pure-python library and PyPy is a perfectly reasonable question to ask. Obviously you'd benchmark your *actual* implementations but if you hadn't read this you might just assume that "C is faster" and not bother to check.
If you can, take the time to learn GIT and try using GitHub. Mercurial is nice as well, but after using it, CVS, SVN, and GIT, GIT won me over. Regardless of what you pick, ANY version control will be better than NO version control. Version control really is a must. Be strong in this conviction and convince the others at your work place. It is a life saver. Right now your 'version control' is the length of your undo buffer (ctrl + z), this is a bad thing.
Do they have a mailing list? Usually with smaller APIs like this the mailing list is the best way to get quick and informative answers to your questions.
not an expert on this (corrections welcome), but - awesomenesses * it's JIT compiler can do some phenomenal (competitive with C) speedups of pure python; i've found it works especially well on array &amp; numeric operations. * it can do things like translation to other languages (C, java bytecode, I think javascript at one point). note: not "render C code to call the python VM", but translation on a higher semantic level. * alternative GCs, for those who don't like python's refcounting gc. * being able to analyze the totality of an entire program (libraries and all) as a single graph is responsible for most of the above features being *possible*, and people are bound to invent more in the future. pitfalls * it works best with pure python, c extensions and things like ctypes not so well - but support is there, and getting better. * vm takes a little more memory &amp; cpu to get going, as the JIT system gets to work. they've been amortizing and just plain reducing that overhead though. * doesn't handle python 3 yet, that I know of. * it's a got some rough edges left, but not so many it's not production ready for many users. edit: typos
I don't think they have a mailinglist but I contacted the app's developer on Twitter and he says that it might be because of some changes made to the API. 
Sounds awesome. I should play with it soon.
Nice! I had looked into the Bottle docs but only found something with Beaker which I cannot get to install on my system for some reason. this looks awesome, thank you very much!
I hacked this together in a few minutes yesterday. Hope you enjoy it. I'll probably add more options and configurability soon (after GSoC ends). **EDIT: Configurability added. See README.md for info on the new command line options.**
Python has a Cookie library. I made a small set of scripts, php &amp; python awhile ago that illustrates exchanging cookies. [cookieball](http://orangepalantir.org/cookieball/) , the source is found [here](http://orangepalantir.org/topicspace/index.php?idnum=16) though of course you'll save some trouble using a framework. 
If you can provide a command line to restart the tests at each file modification or something like that, I would be much thanksful.
I wanted to use fabric how it is intended, but I found it has a fair number of limitations. So now I use a combination of fabric and puppet. Puppet for the sysadmin work, and fabric for installing/setting up python packages.
Downvote for not warning me I'm opening a PDF for something I have no idea about what. Try submitting this again with an actual title and note that says [PDF]. Thank you.
Shady pdf... yea... no thanks. google doc link: http://docs.google.com/viewer?url=http%3A%2F%2Fivanlef0u.fr%2Frepo%2Febooks%2FGrayHatPython.pdf
No it isn't.
After skimming through it, it seems very interesting despite some people's upheavals on pdfs. Though it's not much use to me since I'm not on a Windows environment.
Wish schools will have coding contests just like football or baseball, Why does it always have to be outdoor sports. Not everyone is cut-out for competitive sports, such kids should have other avenues where they can display their talent.
It's still in copyright. Something tells me the person hosting it didn't get the copyright holders' permissions to do so.
The first thing you need to understand about Django is that it is a web framework, not a CMS. You don't customize Django, you write web apps with it. It provides you with a few more abstractions and tools than "micro" frameworks do, but it's really just a toolset. You're expected to write the interesting stuff yourself.
No; just thought I'd mention it 'cause it was useful for it's own sake.
I'm surprised I'm getting negative feedback on this. It's a great book and something that I enjoyed reading a lot. I am really interested in software security though so it may not be for all programmers I guess.
You are entitled to your wrong opinion.
Anyone know what the cost has been in the past?
If you consider it a script, and you're just automating a small task that you'd rather not do manually, might as well just keep it all in one file. If you consider it an application (i.e. there's a frontend/backend/interfaces), it's best to split it out. Your module looks to be in the script category, i.e. it's relatively minimal and designed to do one specific task. If you start to add other functionality, like the ability to search for an event or remove an event, etc., I would probably start breaking it out. On a side note, storing a plaintext username/password base64 encoded is just about as secure as storing it plaintext. Just FYI 
clicked the link, showed me a progressbar. fuck you
I would probably say that's how most people I know use fabric too. 
Look, we're trying to help. Try submitting again with a relavent title and a note that it is a PDF. Your karma will thank you.
2011 Regular Rates * Corporate $600 * Individual $350 * Student $225 * Tutorials $150 each Early-Bird Rates are also available for quite a while, although those numbers aren't listed on the old site (since they were taken down after early-bird was over). When I paid the early-bird corporate rate I seem to remember $450 or $500, so slide the other rates down somewhat as well.
I believe early-bird rates were $500/$250/$150, but I might be off by $50 here or there. Also, there's a large financial aid program that helps people attend the conference. We give out a fairly large chunk of money, much of it in the form of discounted/free conference attendance. So if money's the thing keeping you from PyCon, that's probably a surmountable problem.
Great response. Thanks. I known base64 isn't secure, I was just trying to obfuscate against office scoops. :-) honestly, is there any secure way that is light weight? 
Please, don't use lowercase l as a name.
I skimmed it too. Makes me glad I am not on a Windows environment. Code injection seems fairly simple in Windows.
My trip cost 1600 usd total but I flew first class and had my own room
Denis,there is no mac under the Installation Guide, version 1.36 compared to the performance test data?
Yup, this is exactly how I do it too. I use fabric to deploy our complicated main application and to build new machines. I love it and have even contributed code to fabric, but Puppet is where it's at for handling packages, config files, etc.
Most likely because Beaker does not support Python 3.
Hi, that's very nice of you to share it. A few random remarks to help you improve : * don't host binary directly in the git, GitHub offer a download page, on the top of the page, click on the download button, then add a new download * it's LICENSE not LISCENCE * look at the RestructuredText (rst file) format for your readme, it allow showing some markup in the github page and so display better, also allow you to include images 
Is it any better than http://lucumr.pocoo.org/2010/7/29/sharing-vim-tricks/ ? Currently using these dotfiles. https://github.com/mitsuhiko/dotfiles/tree/d1536e4178f6c23849e150a5491c5260baad6415/vim
Thanks. 1. Done. 2. Don't know why I misspelled it. I must be tired. 3. Will do. First time using Git (and GitHub). I've always been a Subversion guy.
Good on you, squire.
Cool! Add a simple gui and make it record route changes and you will likely have a boatload of users. 
GitHub automatically recognizes README markup based on file extension: * `.rst` = Restructured Text * `.md` = Markdown * `.rdoc` = Something ruby related * ... there may be others ;)
Interesting. Any reason why you did not use [`rrdtools`][1]? ... or [`munin`][2]? I'm just being curious here :) [1]: http://www.mrtg.org/rrdtool/ [2]: http://munin-monitoring.org/
GUIs are for weaklings.
There is also a free superior tool known as [smokeping](http://oss.oetiker.ch/smokeping/), incase you're unaware.
A few things that hopped into my mind when reading the source: * Is the use of `msvcrt` really necessary? It makes the whole thing platform dependent. Does it not? What about simply catching the `KeyboardInterrupt` exception to exit? * Consider using the `logging` package instead of writing the file manually. This makes it easier to customize later on. * I _think_ that `__version__` is the "industry standard" name for the version variable. Although I have not seen an official definition of these metadata variables (has anyone a link to a PEP maybe?) * On [line 127][1] you use a `+` to concatenate string even while using a format string. You could easily insert the character `s` into the format string and drop the concatenation. While it's not a performance issue, it makes the code cleaner ;) Same thing goes for [line 167][3] and [line 168][4] * Use spaces instead of tabs (see [PEP 8][2]) [1]: https://github.com/ccampo133/PingPlot/blob/master/pingplot.py#L127 [2]: http://www.python.org/dev/peps/pep-0008/ [3]: https://github.com/ccampo133/PingPlot/blob/master/pingplot.py#L167 [4]: https://github.com/ccampo133/PingPlot/blob/master/pingplot.py#L168 
For what I know, it is only a problem if you disdain recommended practice and write something like open('hello.txt', 'w').write('hello')
My PyCharm trial period is going to run out soon, so...
Another uninteresting test case in wich cpython beats pypy. lasizoillo:~ lasi$ time pypy-c -c "pass" real 0m0.120s user 0m0.093s sys 0m0.022s lasizoillo:~ lasi$ time python -c "pass" real 0m0.028s user 0m0.017s sys 0m0.009s 
I know you can do one off commands in fabric by using -- or writing your own one off function but this just seemed better to me. 
you're going to buy one?
cannot run to test because I'm missing the holidays.py file. In any case, it's illegal to return anything other than a string type from \_\_str\_\_ methods. Try it yourself with something like this: class NoneString: def __str__(self): pass str(NoneString()) Traceback (most recent call last): File "&lt;pyshell#4&gt;", line 1, in &lt;module&gt; str(NoneString()) TypeError: __str__ returned non-string (type NoneType) so when writing \_\_str\_\_, you always must return some kind of string, even if it is just `return ''`. This is also covered in the [docs](http://docs.python.org/reference/datamodel.html#object.__str__)
But why does it corrupt my DepDB.db file? You will need a few more files to run it other than the holiday.py script. If you want I can u/l somewhere. 
My best guess is that the bsddb lib is calling `__str__` on your objects somewhere in it's internals, and no one ever expects that to go bad. So the resulting exception makes it throw up all over itself and corrupt your files with slimy code vomit. No way to be sure without reading the source though.
I was thinking something similar. I'm guessing that internally bsddb doesn't overwrite the db file unless it detects a certain change (be it object size or namespace objects) and it WILL overwrite it when I add an incorrect str method, but not overwrite it once more when I change it.
If you need a cross-platform kbhit, there's a version of curses compiled for Windows [available here](http://www.lfd.uci.edu/~gohlke/pythonlibs/#curses). curses takes over the console, so you lose the standard print function/statement, but it's the most common option on Unix systems, and it's in Python's [standard library](http://docs.python.org/library/curses.html). Example: import curses screen = curses.initscr() curses.noecho() screen.nodelay(1) #non-blocking mode for getch screen.addstr("press escape to quit\n") while True: key = screen.getch() if key == 27: raise SystemExit if key != -1: screen.addstr(str(key) + ' ') curses.endwin() If you need an analog to kbhit itself, you can combine getch with ungetch.
Interesting. Thanks :)
Seriously crying for you. get the hell outta there. Out of curiousity, how do you manage source code during releases? copy code to a new folder for every release? How do you guys integrate code before a release? I cant imagine the nightmare without version control.
Yes! I changed the objects in large ways (added long code segments, deleted the str method...) and wrote it to the db. It worked! The db was no longer corrupt but it seems like it set a errory recovery flag in the db as everytime I use the object it prints an error code. Would you say that's a bug?
I don't know, I'm not knowledgeable enough about shelve. You might want to ask on the python mailing list about that.
Thanks, will do. :)
The documentation say go for it the python setup.py install will run the 2to3 on it and make it work, however my problem is I cannot seem to work Simple_setup with python 3 which is what has hindered me so far on beaker.
I apologize, I went and looked at django again and realized it is not one that I had used, I tried some sort of CMS. I thought it was Django, it just had a similar name I guess. I guess with bottle i am using one of the micro frameworks, which is working well so far, just having trouble wrapping my head around a few different things.
I find that the trouble with microframeworks is that you often find yourself reinventing the wheel whereas with Django you can just focus on the business logic. I've used Flask for several projects and in most cases I ended up writing a lot more code than I would have needed to get things done with Django. I guess microframeworks are more appropriate for microsites.
That's not too bad. Would it be worth it to someone who simply wants to get started with Python and has little experience with it?
I think in my case it is appropriate, I am writing this app to allow a user to fill out a form, and other users to come in and respond to that form at a later date. I appreciate the suggestion and will definitely check out django on my next project.
&gt; SmokePing does not stand alone. It relies on various other tools and services being present. Apart from a Unix OS and a working Perl installation you need the following things. 
The project I am working on is a few thousand lines of code that was entirely written by myself and one other guy. Neither one of us are programmers by training so we hacked this thing together as best we could. The general rule of thumb was never delete anything. We tried to find out more about version control, but we didn't know where to look. Now I am shepherding this thing through on my own, but a few people around the office have volunteered to help out. They want to learn python and I need the assistance. I knew there had to be a better ways to manage the code and that is how I ended up here. I recently found out that my boss has been insulating us from the people in IT. They remotely scan our computers for illegal software and pass that information to the managers. My boss ignores them because we are getting a lot more done than everyone else. So as long as I am working for that guy, I am going to stick around.
I prefer [Vundle](https://github.com/gmarik/vundle#readme) to Pathogen, personally.
Have you checked out [Vundle](https://github.com/gmarik/vundle#readme) yet to replace Pathogen? I started using it a while ago and I prefer it.
Regardless of the presentations, the birds of a feather meetings are sometimes the most valuable resources. Basically the BoF's run throughout the day and are centered on one topic. They're not always specifically about Python either but related topics like one of the deployment BoF's I went to centered on dealing wih SaaS and high availability deployments, furthermore that was overbooked with Google &amp; Youtube engineers
You probably want to do something like year(scanTime)=2011
If you want an easy way to launch ones *in* the path then I chuck this in the same dir as .cmd to the .py : @gpyl : : Generic python launch. : : Run a python program in the same folder as this batch file, with the same basename. : : For documentation check the python script with the same name. : @python %~d0%~p0\%~n0.py %*
this, datetime is not a string.
No but I worked on a video app for a makeup company, all the videos seemed to involve using about 7 different products.
this part is confusing me: if len(ping[~nans]) &gt; 0: mping = ping[~np.isnan(ping)].mean() what does ~nans mean?
select count(*) is not a good practice. For some DBs it doesn't matter, but for some it matters greatly. You should instead select count() on the lead (or only) column of the primary key.
Using a function as a filter can kill performance. If that's all it has to work with then it may apply the function to the column for all rows potentially causing a sequential scan. Be careful in there.
True. In a real database that wasn't so primitive you could index the results of a function. 
This is strange and vicarious to listen to because I actually interviewed with these guys a couple years ago. (and now I feel like a stalker)
Hey, this was a question about using MySQL and not a question on what you should be using instead of MySQL.
yeah. thought the same thing... anyone?
`~var` is basically `-1 - var` `~0 = -1` `~1 = -2` so `~nans` would be `-1 - nans`
Thanks. Let me address your points. 1. I wrote this for windows pretty specifically, but I could make it platform dependent. I usually use a Linux box every day, but the past few days i have been working on my gaming PC which has Windows 7. I have never written anything specifically for Windows so I wanted to give it a shot. I will make a few edits here though. EDIT: I just removed this and instead now you quit with CTRL+C. This should work on linux/mac boxes now. 2. I had never heard of this module so I will most definitely check it out. Learn something new every day. 3. Sounds good to me. Again, I was not aware of this. I will look into it further. 5. Let me answer the tabs issue before the string formatting one. I used a crappy windows text editor (Notepad++) instead of my beloved emacs when I wrote this program the other day. Typically emacs gives me 4 spaces instead of a tab, while this gives me tabs. I was lazy and didn't want to macro this out, and it worked for the time being. I will change it when I get back on my linux box. 4. Ok so the reason I did this is as follows: I wanted the unit string (s, ms, %) to be included in the padding for the format variable. If I were to insert it directly into the string, I would get something like: "%15s sec" % foo This would produce something like: foo sec I wanted "sec" to be included in the padding as not to disrupt the rest of the line. Kind of hard to explain, but if you understand what I am trying to say, you may realize why I did it. It is only necessary to be in this format because I use \r to update the line in a loop, instead of printing a new line each step. If you know a work around, please let me know. EDIT: Just fixed the tabs problem. Apparently Notepad++ makes it easy to change from tabs to spaces. It's growing on me. EDIT 2: Changed the version variable and got rid of msvcrt. It should run on all platforms now.
I actually intended this for future development. This was a one day project that I whipped together on a whim so I just wanted to get it out quickly. I planned on using PyQt4 to make a simple GUI. Unfortunately I have ZERO experience with GUI coding so It would take a little learning. I will dive into it eventually, but I have some other projects I need to finish first. If anybody wants to write a gui for this, by all means, go ahead. I would be VERY happy to branch it back into the main repo. 
I figured as much, since this is the python reddit; but I felt inclined to point out a well established alternative for that type of software. But yeah, sometimes it's nice to have dead simple, no dependency software.
Thanks, Paul, I will make the change and remember that for the future!
&gt; I had never heard of [the `logging` module] so I will most definitely check it out. At first it may seem unnecessarily complex. But once you get used to it it's not so bad. I use logging packages in every programming language by now. Pretty much never use standard `print` statements anymore. Even in the smallest script I usually start with: import logging logging.basicConfig(level=logging.DEBUG) LOG = logging.getLogger(__name__) and then instead of `print` I simply use: LOG.debug('...') LOG.info('...') ... &gt; [...] I wanted the unit string (s, ms, %) to be included in the padding for the format variable [...] Fair enough. I've done similar things too already. 
Thanks, acct_rdt. I want to make sure I understand you correctly though - would you put the year(scanTime)=2011 after the LIKE in the SQL statement?
You don't use LIKE with date comparisons. You can do: select ... from ... where year(scanTime)=2011; or select ... from ... where scanTime &gt;= '2011-01-01 00:00:00' and scanTime &lt; '2012-01-01 00:00:00';
In this case, nans is a boolean array of True/False values. It may look something like: nans = [False, False, False, False, True, True, True, False, True] If you do ~nans, you get the following: ~nans = [True, True, True, True, False, False, False, True, False] ~ is just the BITWISE NOT operator in Python; it just inverts everything.
Ah, OK, got it! Thanks for your patience and advice!
Thanks. It looks cool but it is pretty dependent on a lot of other software. I wrote this to be an out-of-the-box type of deal like pingplotter when I noticed my network was randomly disconnecting.
Honestly I have never heard of these things. I am just good in Python and Matplotlib and I figured I could easily make a ping vs time plot in a pretty simple script.
Tk isn't thread safe, so you can only update the gui in the thread that calls root.mainloop(). I suspect that is your problem here. Secondly, it doesn't sound like you actually need threading to update the progress bar since you're only calculating time. You could instead use root.after() to periodically call a method to update the bar. In case you're curious, the right way to do multi-threading with Tk is to use something like [this](http://code.activestate.com/recipes/82965/)
You should really use sqlalchemy. As written you have sql injection issues and you have to deal with the shitty dbapi. from sqlalchemy import create_engine engine = create_engine("mysql://...") q = engine.text(""" SELECT COUNT(1) FROM eventLog WHERE clientID = :client AND scanTime &gt;= '2011-01-01' """) for client in clients: count = engine.execute(q, client=client).scalar() clientCounts[client] = count 
You don't want cookies, unless you plan on implementing cryptographically secure signed cookies yourself. You want sessions(potentially backed by cryptographically signed cookies). Bottle is great, but perhaps you should look into Flask (which does sessions and a bunch of other things you might be looking for).
The technical details are interesting, but what business are they in? Are they a professional spammers?
&gt;~ is just the BITWISE NOT operator in Python; it just inverts everything. Cool, thanks for the tip.
They basically run e-mail newsletters for legitimate businesses.
It's very strange seeing myself talk..
yes, you are absolutely correct, I do want sessions. But everything I have found that dealt with sessions was not running with Python3, so I went for cookies that could hole a variable for me to identify logged in or not. How close is flash to bottle? I don't want to have to rewrite this thing again!
You do realize you're not wrapping it any better? You are missing the point that Fabric has connection caching, arg parsing, ability to interact with remote commands (top for eg), sftp functions, as well as parallel execution over host lists in the works. So it's cool that you were able to solve your task simple as pie, but what's with the troll bait?
I prefer using git
Me too, which is why I like Vundle. It manages them using git submodules.
Not the best public speaker, but an interesting talk on technology change nonetheless.
Getting into PyQt is a bit frightening, but I think the trouble is worth its wonderful portability.
see above
"Sorry, your browser can't play HTML5/Ogg video. Get Firefox." *sigh* so close and yet so far.
True. He works for [Emma](http://myemma.com/). And they are very, very good at what they do.
There are some, yes. Security will always be inversely proportional to convenience at some level. But most people (in my experience) rarely bump up against that wall. Just pointing it out to the general audience? :) It's hardly that important in this case, but I've seen people attempt to use base64 encoding as a security mechanism in production applications, that's why I mention it You remind me, though -- I need to start playing with Google APIs one of these days... :P 
Since you mentioned MediaGoblin: Christopher Webber, one of the MediaGoblin developers, [recently gave a presentation about it before the Chicago Python Users Group](http://blip.tv/carlfk/gnu-mediagoblin-for-a-federated-media-future-5264164).
[Is this](http://www.reddit.com/r/linux/comments/dso8m/program_to_convert_images_pngjpggif_to_reddit/) what you're looking for? I have no idea how to untar the compressed program with the way he included in the post. Edit* So I was able to extract everything, but I couldn't find the dependencies of "SDL_image/SDL and development headers for both packages". If that is what you were looking for, good luck.
I know you said 'array', but it might confuse someone writing it like a list instead of nans = np.array([False, False, False, False, True, True, True, False, True]) Also, the data type is `np.bool_`, which complements properly as a single bit. Python's bool type treats this as a normal bitwise complement of the underlying integer constant: &gt;&gt;&gt; ~True -2 &gt;&gt;&gt; ~False -1 &gt;&gt;&gt; bool(~True) True &gt;&gt;&gt; bool(~False) True &gt;&gt;&gt; ~np.bool_(True) False &gt;&gt;&gt; ~np.bool_(False) True For Python's bool one has to use the `not` operator: &gt;&gt;&gt; nans = [False, False, False, False, True, True, True, False, True] &gt;&gt;&gt; [not x for x in nans] [True, True, True, True, False, False, False, True, False] 
I don't really know. I haven't used maxima before.
* [SDL](http://www.libsdl.org/download-1.2.php) * [SDL_Image](http://www.libsdl.org/projects/SDL_image) 
where is the sql injection issue coming from? Doesn't MySQLdb escape everything? *oops, didn't notice the args tuple wasn't being used. Same question though, is there still a problem with injections with db.execute(query, (args,...))?
This will save the tar.xz file to disk: import uu from io import BytesIO data = BytesIO( br"""begin 644 unicode_image.tar.xz M_3=Z6%H```3FUK1&amp;`@`A`18```!T+^6CX"?_!IM=`":8269IV=F-Y,!%M1&gt;4 MZX(9LR,YMG1:D2XCM%DZ,0N4%&gt;'\;I?0D"7/H:OI15&lt;M6G@8HO/[&amp;((J'=B. MXY\G/[7D"Y)B$O)IC]DM9Y@^\4T?'9(.Z.4+7IDF/T0&amp;\7`M5+G#=C!?(&gt;(U M+-C2%PZ!"(Q'Z_/D^%"[PVKX:A:OKH5WF?AQ=CD_AAS]&lt;3&lt;THTMC0S8FG\&lt;A MZ;A-_9H?)5S'YG5A?0WUQ7FR0+IS\0AEUYY9QFMY?"$);\U%_R0NK(ZZ/Y&amp;J MVA;@#O-P.6W8PW']0U"&lt;S'NHB=.(/)OX[&lt;1&amp;UF@M8+GXPGVFQB_+K/WD01ZO MO#+E!CK;^`V-WGH^?0V5M!IK[KR&amp;]`IR&lt;&gt;6D+ONPJT6E\CZJ^KKZ,W?3O"2K M!/GHQ&amp;TDN';;P#UC;)+HRPH$`_8JM#ZV`\I6=,PO=U#S33IZ=R!K2IF]\1D@ M*@I6;)=1P[3ICJ%,C5VTH^%^^N7(`)5NO*-SG)Y`QA_WK&gt;PA8;TJ+X2)EV?3 MI.G"[*&gt;WWDZ7\Q_8`@,?X8C9YSMNGQ.S79!10SGB!PGY&lt;3)L+A&gt;\T4NE3RCH M@$&lt;!]40^I5;'[)@&gt;$KCW3*:VMQ")"FQ!"L?^:Y5K)WM]*CV&lt;",@L38:E&amp;'G; MOH/\?B8-H-/5$-+1`SZ2O6RHY&gt;T@2+Q"LM7T32&lt;P'/M;:&amp;`9G%&lt;:2]0W95K&lt; M\.;8"EQMW`_,%NF[4)3]`F-BE^_T2,`,VV:G?3TUJ"IH\A@&gt;7Y8:?5[I8HAX M/S[+K[U+U);"B&gt;&amp;TTWB[]4K_N9HNW6P5\,!8G[BS*%\3&lt;$#RM"O`6C#I1&gt;3W ME/8D\";AO1[@@M&lt;^$07W%./W$^^MXKWV/QE?(SEU3GAF"?/TXQ_1N&gt;#/)-;8 M&lt;MT]?:XBQTJ%9'[+D!X/9^"U.Y*2A(8A/AJ\!K^V\)9&gt;''&lt;,=_M*GQ;D4XU&amp; M0EL_A7==:2!F6+.18],W)'0*A8VHHT@F\^+U@*=PAW&amp;]_UDA'O3HL,)67*56 M:4QHF]DC7*EMI@?8=/,8;'O4[2W^#V!$L.(O\+@E"I[&gt;5AF7V].]DSR0?&gt;4E MX,HXX%S-A'V'+F)3(0_FX[O!VNO&amp;D/BL`T&lt;!(LJ,(@3!S8LJ[&gt;&lt;9CHP*Z!1N M*W=F7"R-"ZZ_7"_NQ:8M=&amp;RU\7`Z`&lt;J&gt;2YY&gt;W2B\R7!YX(:UE+7?[1HLZ6?B M_$Z&lt;[I3[+!'?["&lt;`BV0HHVNVZ$?*E!CLQ,Q'U$5$IQ?#B#+P)WE/$$-*'U&lt;E M%+7[;;^I\)F++WU`\\BL@X5,*B+]OLWQ&amp;=W!,3*4_5Z0'R\2N/\];P&gt;]W2&lt;( M-7Z3$YL3&amp;6;-"*NTT2_Z1P=4"&gt;JZT,0Y$Q`L;RU@2\X!6&gt;NA6D:5#HOIP#H] MH)2I8$WFSU,1M9OC73J.1T1-YWD[%EH?1E*H#MV/[+5HSKGU.-%-'Z)QI=$\ MV&gt;24Q6&amp;KMA*-=L#[#I[2'0$86N)&amp;8E/==`F@,S#5,`)(-KDT6A9IK-/%6OA@ M@FI$$#7G&gt;&amp;.Z!8[?8:F==P&gt;HX&gt;WF&amp;.?(9V6^WM`J[CVD`L]9&lt;&amp;\6P_U?*WN` MLI*_M*H;SP58M&amp;#X!&gt;*U*^J*XO@UT"&amp;SIGH1%(K-=7DN@=HD6`S2EET,60JV MI_\%)%6Q_^3CW_5`HQ;G084_7J0'F9DDH*%`SY*.D1BP`"D_QO=5,F?$-HAG M_FP7H+LUTX`^%F-[SV(C'N*+AXE=&amp;!+'OT$)RYGQ/HX,L8W(D%G9J=P!6+*$ M)F20)=%&gt;9ZI).Z0`I'T/OT#SUR_:(O0U1*2-:,\D0S52^NI?HL69"POCNH&amp;X M_HXLQZB3EL-Z&lt;)4.!&lt;&lt;BDJX3H"Q`L'&amp;"RLO/%]17EV.5R@/$,%GYE#U(,Z'. M6#]?M?@0VYB%WU2-4E:Z&amp;9RN,"SCQAYJ70='?0`L5JC6GG#:1BG]DBCY;N)&lt; M&gt;[;&gt;JU-P]W=*RQG_KX;[&gt;Y-0O.&gt;_BS[M!=3Y#98EA`S8J/\1S=Z..*RC^;+U M!.(#&gt;E-V^?_+/M323Q,+EM-95M%CT#G[XO0FH/`.&amp;`__EU&lt;3\=+#&gt;7?FR*NY M9MA;$1+KD8?V@Y8XE7`(*;.N\KEF1]T4!OYS1+%#*S9&amp;[0-#E"FRGA^\L[^A M\76@2CMV&lt;J_S92KW%;UO$.=R!3!P]OD.WD@*ZE(.;&gt;H)8L]IMC;&lt;YHPNH343 MY,JGKBM7M:!:S]$UJ7Y-/A'&gt;Z]^7LV*Y.]N\MN_#%%%&gt;)IH_A_:G46]+E.M; MUA@I&gt;99IP916P;7A48N3VF+;&amp;!__1Q&lt;QF8AU`XF-LJ./^6J+@+JCLICOF=I- M.U"KKV.._JR/;P(````4=99=LD1P/P`!MPV`4```*JOOO;'$9_L"``````19 !6@`` ` end""") with open('unicode_image.tar.xz', 'wb') as f: uu.decode(data, f) On Linux systems that should be all you need. On Windows you can use [7-Zip](http://www.7-zip.org) to extract the files.
What about the regret of using up valuable development time switching languages that you could have spent on your core business?
It works: ▒░ ░▒▒▒▒▒▒▒ ▒░ ▒░░▒▒▒▒▒▒ ▒░ ░░░░ ░ ▒ ░▒▒▒▒▒▒ ░▒ ▒░ ░ ░░░ ▒▒▒▒▒▒▒▒▒ ░▒ ▒░ ░ ░ ░ ▒▒▒▒▒ ░░ ░ ░▒ ▒░ ░░░ ░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒ ░░░ ░▒▒▒▒▒▒ ▒░ ▒▒ ░▒▒▒▒ ▒░▒▒▒▒▒ ░▒▒▒▒▒ ▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒ ░░░░ ▒░ ░▒░ ▒░ ▒▒ ░▒ ▒▒ ░▒░ ▒░ ▒▒ ▒▒ ▒▒ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒ ░░░░ ▒░ ▒▒ ▒░ ▒▒ ░▒ ▒░ ▒░ ▒▒ ▒░ ▒▒ ▒▒ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░░ ▒░ ▒▒ ▒░ ▒▒ ░▒ ▒░ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒▒▒▒▒░ ░░░░░░ ▒░ ▒▒ ▒░ ▒▒ ░▒ ▒░ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒▒▒▒░░░░░░░░░░░░░░ ▒░ ▒▒ ▒░ ▒▒ ░▒ ▒░ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒▒▒ ░░░░░░░░░░░░░░ ▒░ ▒▒ ▒░ ▒▒ ░▒ ▒░ ▒▒ ▒▒ ▒░ ▒▒ ▒▒ ▒▒▒▒ ░░░░░░░░░░░░░░ ▒░ ░▒░ ▒▒ ▒▒ ░▒ ▒░ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒ ▒▒▒ ░░░░░░░░░░░░▒ ▒▒▒▒▒▒▒ ░▒▒▒▒▒▒▒ ▒▒░ ▒░ ▒▒ ░▒▒▒▒▒ ▒▒ ▒▒ ░░░░░ ▒░ ░░ ░░ ▒▒ ░ ░░ ░░░░░░░░░ ▒░ ▒▒ ░░░░░░ ▒ ▒░ ▒░ ░░░░░░░░▒ ▒░ ░▒▒ ░░░░░▒░ ▒░ ▒▒▒░ This looks good on my system with Courier New and Lucida Console, but not so good with Consolas or Anonymous Pro.
Don't change horses in midstream. Even if the new horse is much faster, better, stronger?
That's awesome!
there are less obvious ways to temporarily leak resources and they happen to exist even in well written real world code, cause its easy to fall for cpython's gc
Please tell me about them, because I don't want to fall in that trap.
No regrets ... except not switching to Python sooner of course ;-)
I quite like the look of the syntax for that, but do you find it gets unwieldly when you have a large number of plugins? Are there any other plus points other than explicitly loading plugins? I'm using git's submodules with Pathogen so I have a list to hand of my plugins by looking at the .gitmodules file so not sure that would be enough to sway me, but I'm always open to suggestions!
Here's what worked for me on Windows (YMMV): * compiler/linker: mingw64-i686 (gcc 4.5.3) * SDL: SDL-devel-1.2.14-mingw32.tar.gz * SDL_image: SDL_image-devel-1.2.10-VC.zip Unpack the source files into a directory named unicode_image. The only file I used was `unicode_image.c`. I skipped using the makefile. Unpack the SDL and SDL_image development packages into this directory such that you have the following directories: * unicode_image/SDL/include * unicode_image/SDL/lib * unicode_image/SDL_image/include * unicode_image/SDL_image/lib Copy the DLL files from SDL/bin and SDL_image/lib into a new directory named unicode_image/build. The final executable links to these DLLs when it runs. Compile and link with the following command: gcc -o build/unicode_image.exe unicode_image.c -O3 -Wall -Wl,-s -ISDL/include/SDL -ISDL_image/include -LSDL/lib -LSDL_image/lib -lmingw32 -lSDLmain -lSDL -lSDL_image Usage: unicode_image [image file] It seems the mingw development version of SDL wasn't compiled with `NO_STDIO_REDIRECT`, so it always outputs to `stdout.txt` in the current directory. That's good enough for me.
It's just a preference. They could have done everything in Brainfuck if they felt so inclined. I refuse to participate in some holier than thou war of the languages. At the end of the day it's people squandering finite amounts of time on ultimately non-customer facing irrelevant things. Additionally, good programmers, like Fabrice Bellard or John Resig, could probably port linux to my ti-82 with the built in basic interpreter. top engineers are amazing in any environment; it's just what they are. you only need to build these walls for the merely adequate ones.
Yes! Thank you!
pydoc -p9001 &gt; /dev/null 2&gt;&amp;1 &amp; &gt;pydoc -p &lt;port&gt; &gt; &gt; Start an HTTP server on the given port on the local machine.
I do specify the port, left it out above. You exact line gives this: [20]+ Stopped pydoc -p9001 &gt; /dev/null 2&gt;&amp;1 So no dice, unfortunately.
Maybe try using "nohup". nohup pydoc -p9001 &amp; Not tested!
Try without the redirect. pydoc -p9001 What error does it give?
No error. Works fine, but leaves me in a text menu.
I regret type hinting and being able to do one line if statements :(
No, it exits.
Thanks :)
Ah! Now I feel better about it! Thanks.
Has your pydoc been modified? Try "ls -l $(which pydoc)", it should return lrwxrwxrwx 1 root root 8 Mar 20 11:39 /usr/bin/pydoc -&gt; pydoc3.1 And if that's okay "cat /usr/bin/pydoc3.1", which should say: #! /usr/bin/python3.1 import pydoc if __name__ == '__main__': pydoc.cli() If both of those are alright then it's something weird with your pydoc module.
Stop redirecting stderr to /dev/null for a second. Maybe it's exiting for a perfectly valid reason and trying to tell you, but you'll never know as long as you don't see stderr.
In the browse function of [pydoc.py](http://hg.python.org/cpython/file/cc86f4ca5020/Lib/pydoc.py#l2735), it catches the EOFError exception and prints a blank line -- followed by 'Server stopped' in the finally block. This certainly gets raised by `input('server&gt; ')` when stdin is /dev/null. Also, even if it's not directed to /dev/null, doesn't any background process that tries to read stdin get suspended? Try this: python -c "import pydoc, sys, io, time; s=io.StringIO(); s.readline=lambda:time.sleep(10) or '\n'; sys.stdin=s; pydoc.browse(9001, open_browser=False)" &amp;&gt; /dev/null &amp;
I find it's easier to install a plugin (add line to vimrc, run one command inside vim). As you mentioned, I prefer the explicit list of all the plugins I want. I also find it easier to set up a new machine, as my git repository hosting my .vimrc only needs to host Vundle, and not all of the other plugins, because my RC specifies them and all I need to do is run :BundleInstall. Vundle also makes it easy to uninstall them. Personally, after dealing with git submodules several times in the past, I'm happy to have that detail abstracted away for me, but at the end of the day it's definitely a matter of personal preference. =)
Mercurial was written in Python and is encouraged by some of the heavyweights in the Python world. I would say the most popular, regardless of language, is Git. Take a look at GitHub if you're just getting started, they've got a lot of beginner info and hand-holding for people that are just getting their feet wet. It's also a great community to be a part of.
If you can afford a few days in a hotel, some travel, and some learning, the first two days (March 7 &amp; 8) are tutorial days where you can expand your experience pretty quickly. Someone usually teaches a Python 101 class which may be a good refresher for someone with little experience, then a Python 102 class in the afternoon that'll work on tougher topics in the language. PyCon really has something for everyone, so please do take a look at the tutorial and conference schedules once we've organized them (several months away, we just started on this stuff). I've had lunch with people who had zero Python knowledge and they lived nearby and took several tutorials and loved it. I've met people who feel they went from beginner to intermediate just by sitting down and sucking the brains of every tutorial instructor and conference speaker they could get their hands on - it's a cool environment, ask as many questions of as many people as you can. I've used Python for like 7 years and I'm now one of the core developers, and I'm still learning stuff at each PyCon I go to.
`query, args` is fine, but the way `query` needs to be formatted differs between dbapi drivers. SQLAlchemy works the same with every database.
Well, "ls -l $(which pydoc)" gives: lrwxrwxrwx 1 root root 6 Jul 11 14:49 /usr/bin/pydoc -&gt; pydoc3 Which in turn is a symlink to pydoc3.2. The contents of pydoc3.2 are: #!/usr/bin/python3.2 import pydoc if __name__ == '__main__': pydoc.cli() 
I think I cringed everytime the host started talking about PHP
Oh my. Well, to skip straight to the debate: http://www.youtube.com/watch?v=oeghXnAGJV4#t=3m1s 
This bitch is dumb as hell.
My favorite part is how serious the perl guy took it. You can see the rage in his face
And I refuse to be sucked into the over-relativism of languages. Some languages *are better than others*, and help me squander less time to develop more and better customer facing, relevant things. When you can somehow get Fabrice Bellard and John Resig working on your project, and have them develop on a ti-82, then you are hurting your project and wasting everyone's time. Meanwhile, your actual programmers, the merely adequate ones, are going to be as completely fucked as your business.
I don't see any rage or anger or anything. He's just a serious and earnest sorta guy who also maybe wasn't expecting Steve's mostly friendly sparring. 
Steve certainly seemed to be *O'Reilying* the Perl guy up.
Well, that's a rather rude way of saying it. She's probably fairly smart, but she really doesn't add anything at all. Edit: I actually retract my comment after listening to the debate. She didn't even know what "Obsfucated perl" was.
"...obfuscated Perl, I don't even know what that means" "...there's actually a contest for that?" "I've heard about this obfuscated Perl contest..."
Basically (sorry for the late reply) there are some issues on the way we would be distributing such a port and some minor patches which have to be done. 1. Rebinding sys.stderr gives a segfault on Windows. 2. Do we want to just package bpython? Do we wish to package gtk/pygtk/gobject with bpython? Do we want a standalone version (with embedded python as well)? 3. No one on our contributor list has enough experience with creating a distribution for windows. If anyone does have the experience and knowhow please come help us, it is one of the things very high on my todo list. You can find us in #bpython on Freenode and our mailinglist (from the website).
I don't have 3.2 installed, but you could try: &gt;pydoc -b &gt; Start an HTTP server on an arbitrary unused port and open a Web browser &gt;to interactively browse documentation. The -p option can be used with &gt;the -b option to explicitly specify the server port.
i cringed everytime the host talked... Extremely awkward..
&gt;I think I cringed everytime the host started talking FTFY
Perl programmers code like this and Python programmers code like that. Amirite? lols 
&gt; She didn't even know what "Obsfucated perl" was. To be fair, that is a bit of a tautology.
Same, came here to comment on that
Host says she is a giant nerd but doesn't know what obfuscated means and programs in PHP. Go figure.
Because Fabric seems to get all the spotlight as *the* paramiko wrapper with an ugly interface and silly use cases (like deploying software by copying it with scp instead of using git or any other version control system). The python community deserves better.
I would have a beer with the Python guy. I would NOT have a beer with the Perl guy. Girl is annoying ... describes herself as a "geek" but clearly isn't. Cute though ... 
It would be good if there was no hostess, who claims to be a nerd :D
&gt;"...obfuscated Perl, I don't even know what that means" aaaannnd I'm gone. 
it's like talking politics with someone. there's no way to talk sense into some people. there is plenty of good php and bad python and the other way around. a focus on the language is often misplaced intention. [have a good one](http://twitter.com/#!/pythonhater)
I've had a beer with the Python guy.
Pretty cool video. I just got into djanog/fabric for some of the stuff i am doing at work. I wish he would have gone into why he used sqlalchemy rather than the django ORM. I really like the django ORM better than sqlalchemy but its just because I used it first. Great video though! 
I loved how Steve Holden started with the beard thing, he knew he didn't need all 10 reasons why Python is “better“ against Perl, so he used one to make a joke. ;)
I accept your apology for your bad arguments and ad-hominem attack there at the end. You also have a good one, my friend!
Maybe not rage, but you could see that he kinda took this as a serious avenue to talk about perl and what it offers
True, but I'm pretty sure that wasn't her confusion.
"I'm a big computer nerd, tee hee" "Oh awesome, what do you think about &lt;vaguely technical thing&gt;?" "Oh... I'm mostly on Facebook and Twitter" "..."
Did anyone notice at [26:01](http://www.youtube.com/watch?v=oeghXnAGJV4&amp;t=20m1s) how the Python guy tries to say something else about Python's compatibility with other languages but the Perl guy just won't let him have the mic? Priceless
Does that contest still run? I would be surprised. Obfuscation is generally frowned up now. 
This is working quite well so far, the "Try this:" that is. Thank you very much. As for processes listening on stdin, I've always imagined that once you background them they'll effectively become deaf to stdin, some choosing to stop until foregrounded again. I use mplayer with "&lt;/dev/null" to get it to background, I figured I'd try the same with pydoc. I didn't bother trying to trace the execution of the pydoc module itself before asking about it here. Thanks for looking at it. Pythonista points to you :)
what does nans store? packets lost?
Timeouts in my case. Every time there is 100% packet loss, the latency is undefined, so I just put in a nan value in the array. Everywhere the ping array has a nan value, it means at that time there was a timeout; 100% packet loss.
unicorn support!
[Here you go](http://www.youtube.com/watch?v=1yn1P72I7CY).
Much appreciated!
Is a "frown up" like a smile?
In my experience, that's what usually happens when you joke about Perl while among serious Perl programmers. That's why I don't do that anymore. PHP is always safe to joke about, though. 
She sounds like a Stepford wife.
It appears amusingly that the lady is not nerdy enough for Redditors. She apparently failed the nerd test.
That was interesting but cringeworthy the whole way though. The perl guy had no sense of humour and just wasn't a public speaker. The host could speak but clearly wasn't very technical which is fine but please don't try to be more technical than you are. The Python guy was a good speaker had a sense of humour and quite frankly was wasting his talent there.
It's not that she wasn't nerdy enough. There's no problem with having a non-technical person host that debate. It's that she consistently tried to play-up some kind of non-existent technical chops. People are making fun of her, not because she "isn't nerdy enough for Redditors", but because she is a poseur. 
I wasn't aware that Reese Witherspoon was a nerdy PHP programmer.
Wha-? I don't even know what you're trying to say here. The fact that she does PHP infers she is heavily technical. Maybe not to you or I, but compared to most people who don't know the difference between Internet Explorer and an Operating System, she's there. As to she doesn't know what obfuscated means, that's just ridiculous. It's like someone saying "I'm going to host this homebrew debate... I have no idea what Acetaldehyde's are". Of course she doesn't know every terminology for what is the probably the most complicated technology in the world. Do you even know what an acetaldehyde is? If you don't you suck! (This comment is made in jest but to prove a point). 
Did you have the girl or the Python guy? 
I hate being "that guy", but I figured you should know... &gt; they *couldn't* care less 
And it was good, wasn't it?
Well, she later makes decent comments on strongly vs weakly typed languages, so not all bad. 
I had the girl with the Python guy.
She probably was trying to lead him on to explain to the audience what that meant
Beginner here.... Why's that?
Ruby programmers program in a manner consistent with prevalent stereotypes regarding said subgroup.
This would have been better with more humor on the parts of the "debaters". Their seriousness is reminiscent of high schoolers doing a Lincoln Douglas debate on slavery.
to be fair he did talk about the contest, and then she said that she had heard of the contest, which at that point in her life was true, she just didn't know before the interview.
Using scp is good for file transfers, anything else and you're adding a dep. All you need is ssh access and a shell. You are making the assumption that Fabric can't use git, and that it's only for deployments. Fabric is just a tool to let people use python and bash (zsh/tsch/whatever) together with some paramiko/pycrypto thrown in to allow remote shelling. Fabric can be a tool for state changes, boot strapping, and you can never use the paramiko portion at all and still have a good use case for Fabric.
I think that [this](http://www.reddit.com/r/Python/comments/j9vgk/funny_perl_vs_python_debate/c2afbn1) was the case.
perhaps he was saying that they care quite a bit.
Doesn't the same go for Perl vs Python coders in general? 
it looked like she was just nervous, and not used to doing this type of work.
Holy shit, that was fucking *painful*. They should really have gotten someone with a little more skill for the moderator. She made herself sound like a moron.
I met him at a conference a few years back. He's just... *like that*. Not a particularly jovial guy and probably a bit nervous with the camera in his face.
We would probably like to think so :-D
whooooosh
She didn't know what she was talking about.
Between you and me, it was fucking fantastic. Didn't hurt that he was absolutely roasted, either. At, like, 5pm. (PyCon a couple of years ago.)
I don't think it ever *wasn't* frowned upon apart from in the context of the contests where it was just for fun and not intended to be an exposition of best practices.
Can't top that. Upboat.
Ugh, she was terrible.
Interesting.. after reading this I'm now whooshifed
I'm not saying python is a bad language. I'm just saying that deciding to rewrite a mature product incurs substantial risk. That's not a personal attack or a statement about the fitness of either language. Don't need to get all butt hurt over it.
At 26:01, it looked like 4 things were going on simultaneously: 1. Steve wanted the mic to comment on Peter's short rebuttal to Steve's point 2. someone outside of camera-view may have been asking Peter something (Peter is squinting and appears to be trying to hear someone). Steve didn't appear to notice, and was looking at the computer screen. 3. the hostess was trying to move on to the next Perl item 4. Peter is trying to focus on the computer screen because he's the one who's supposed to read the next slide. In that situation, I can see how there could be a little confusion about where the mic should be. 
I remember when poseur was only used for faux punk rockers. My how times have changed.
oh shit...why do I bother wasting time on crap like this ? LOL. Let's see.... * annoying, blabbering fake-technie broad * stiff, grim-faced guy in a supposedly Tongue-in-cheek "discussion" * Jovial guru 
Always nice to see Python being used in examples which show it can perform well. However, the article could basically be summed up as "we got in too deep with erlang without understanding the language very well so we went back to our comfort language, Python'. 
I was getting too irritated listening to her; turned it off.
Ironically, the people watching OSCon are likely to already know what code golf and the Perl obfuscation contest are.
&gt;Do you even know what an acetaldehyde is? Even if he doesn't, it's alright, because he didn't try to be a chemistry debate host.
Oh that's a good point. I agree that people shouldn't rewrite mature products just because. It's always a cost benefit situation. Also, I am being quite civil here, however your general discourse is both ineffective and childish.
Someone want to try it on PyPy (nightly) and let me know how fast it is?
I guess I was a bit of a pain, ok. php is a much less expressive language when you want to do higher order abstractions. Also, it's package management system isn't very well thought out. It's a simple tool that's great for really simple jobs. I think the biggest problem is when you application moves beyond a simple stage and you want a richer language. php fails, especially in terms of functional programming. But I think too many programmers these days don't understand how incredibly long it takes to bring something to stability and how doing a move like this basically means discarding very significant time investments of the product work up to that point, not to mention the huge investment to recreate the product. I've seen many developers throw this to the wind as if it's not actually an issue. But 90% of the time it takes 10 times longer then initially thought and becomes a rats nest.
&gt; No one on our team is an Erlang expert, and we have had trouble debugging downtime and performance problems Not to comment on the decision -- i'm sure it's a fine one -- but you switched to Python because you had _performance_ problems? Ouch. I'm not sure what that says about your original design. Anyways, "more people know it" and "it has better libraries" is a little more convincing.
Yes! I totally agree. PHP to me is a wonderful *platform* for speedy web development. The language itself is really quite awful. It's just not designed in any real sense. But the platform is amazingly efficient. The only reason I would suggest a large PHP project to switch is if the features or abstractions are being held back by the language, or if it's a really long term project that, after a cost/benefit seems like a good fit for Python. But yes I agree completely that too many people get caught up in the mythos, or the promises of a technology, and just jump in without proper planning. I too have seen messes of Python by programmers that never really planned it out. That said, I think Python is much better for new projects as long as you have someone who is moderately familiar with the language.
&gt; Its “green threads” are pretty similar to Erlang’s “actors.” I really wouldn't call it "pretty similar"...
This article would be a whole lot more useful if there were numbers given comparing the old vs new, rather than just graphs of the new in isolation.
So the person that wrote the erlang server obviously left the company. Why didn't they hire someone else with the same level of knowledge? 
Your threshold for "decent" is very low :P
I don't think the perl guys frown on it. The perl guy said like 2 or 3 times that he LIKED obfuscation, that it was a good feature of Perl and a shame Python can't have that kind of contest.
average time of 5x runs of 10000 pack/unpacks using supplied data in gen CPython 2.6.2 ---------------------- Google Proto Buffers (2.4.1): ======================== * Unpack Time Taken: 1.93977236748 * Pack Time Taken: 1.47394952774 Palm Proto Buffers (70174638e0d88aa5fd3f23088a2401bda6e85e04): ========================================================= * Unpack Time Taken: 0.030970954895 * Pack Time Taken: 0.0143961906433 Pypy (1.6.0-dev1 ca5a04e1b758 (Python 2.7.1)) --------------------------------------------------------------- Google Proto Buffers (2.4.1): ======================== * Unpack Time Taken: 0.102884340286 * Pack Time Taken: 0.154807424545 
Because interns are cheaper...
But also says that C's static typing "was a problem with the language".
The only thing I wonder is how do you started with Erlang in the first place.
I thought it was ejabberd....
reason #1 http://www.google.com/search?q=%22python+to+perl+migration%22 http://www.google.com/search?q=%22perl+to+python+migration%22 
All five expert Erlang developers are already employed. But seriously, it's hard enough finding a good, experienced Python developer these days... I imagine it's even harder to find Erlang experts.
It's a very "cool" language (and pretty well designed) that hits a lot of great buzzwords like functional programming, massive concurrency, etc. But like all languages you need to understand it to get the most out of it.
She is the host of a debate on programming languages, but doesn't know about programming languages
I love python and everything but why would you implement your core, critical technology in a language that you don't know? It seems like it would be cheaper/more efficient to hire someone who knew the language to maintain it...
I used Hyde about one year ago and was confronted with a horribly lacking documentation (quantity and quality-wise), the only real clues you got were by reading others' template code and the "Getting Started"-section on [their github page](http://github.com/lakshmivyas/hyde) didn't really help me either. Additionally I found the default template-structure very verbose, strange and nonsensical, but that's possibly due to my missing knowledge of the system and hence documentation. I'm not sure if it is still as bad as it used to be, but those were my impressions unfortunately. After much tinkering I finally got it working like I wanted to, but it wasn't a pleasant experience. Unfortunately I can't say anything about Pelican because I never used it.
This jumped out at me: &gt;In Erlang, the right way to do this is to spawn off a separate set of actors to manage each data structure and message pass with them to save and retrieve data. Our code was not set up this way at all Messaging-passing between lots of small actors is the whole point of Erlang. If they didn't use it that way it's no wonder they had problems.
Try them both :) Pelican is AWESOME. I use it for my blog. Super simple and flexible: http://kennethreitz.com/ https://github.com/kennethreitz/.com 
I and many others use the excellent [Blogofile](http://blogofile.com/) for static site generation and blogs (http://techspot.zzzeek.org/ , http://www.sqlalchemy.org/ are both built on it). It is the perfect platform for building static sites and blogs, simple to use, well documented, supports multiple template languages (Mako and Jinja2), active community.
Next up will be the unbeatable encryption system (I think I invented the one time pad when I was 7-8).
It's when they try to put characters. Next to each other. Like. Sorta.
As a complete non sequitur response to something completely unrelated.
or perhaps more generally: "don't underestimate the challenges in building complex, fast and reliable systems out of languages and products that you're not already an expert at".
Blogofile is what I use for http://asktherelic.com/ https://github.com/askedrelic/asktherelic.com It's good stuff
Interestingly, Bob Ippolito, the [original author of Eventlet](http://eventlet.net/doc/history.html), is also the [author of MochiWeb](http://bob.pythonmac.org/archives/2007/11/07/mochiweb-another-faster-web-server/) that Mixpanel switched from. 
Just started using Jekyll. Go for static. So much cleaner and simpler.
Works fine. Ran this and closed the terminal - Pydoc was still accessible
Python is less likely to result in tearing ones hair out at an obscene hour of the morning, in comparison to say C++ or Java. :)
Please mention the community and the enormous amount of publicly available (and often commercially usable) libraries and modules out there. I would also emphasize that by its nature Python encourages writing good looking, predictable code. It helps you focus on the important part of what you're trying to build, and like any high-level scripting language gives you easy access to all the usual common operations.
Guido von Rossum, the guy who created Python, has [an archive of presentations he has given on Python](http://www.python.org/doc/essays/ppt/).
By far the best way to demonstrate the benefits of Python to this kind of audience is to fire up the interactive interpreter and show them some live code demos. None of C, C++, Java or .NET have a strong culture of interactive programming, so this will be a new technique for many of them. Even if they don't take up Python after your talk they will still have learnt something very useful. Don't type too much code (that's easy for you to make mistakes, and it's boring watching someone type things on stage) but if you have a few neat examples of real world problems that can be solved in a few lines of interactive python you'll be sure to impress the audience. I gave a talk along these lines at Stack Overflow DevDays in Amsterdam a few years ago and it went over very well. Here are the notes I published after that talk, which might give you some ideas: http://simonwillison.net/static/2009/devdays-amsterdam.html Don't forget: you can prepare a bunch of code beforehand and then copy-and-paste it in to an interpreter window for more complex examples.
You should probably try to fit [this](http://xkcd.com/353/) into the slides somehow.
They switched to the most used languaged in their company, instead of looking for another theoretically better tool that people there wouldn't know how to master. It's a sign of maturity, in my opinion.
your example doesn't work: &gt;&gt;&gt; reader = csv.reader('endangered.csv') (missing open())
More importantly, what's their background in Python?
why is these people take the trouble to go thru all that but skip on installation info
I've been using Jekyll, but I might very well switch to Hyde or Pelican - I didn't know they existed, and I vastly prefer Python to Ruby.
Don't forget the alt-text caption!
The main benefits I get from programming in python are: 1. no need to type everything (although var in C# gets you there in a lot less keystrokes) 2. no compile cycle - change the source and re-run the program 3. layout is scope (I know this is normally contentious to some degree with folks, but I find it a huge benefit) If you're trying to sell Python as a strong alternative to the other languages you mention, It's probably worth mentioning those benefits. Like simonw says, fire up a REPL and write some code. use the libraries available to you (json, subprocess, re, sqlite3 and math feature heavily in many of my modules, for example) to do magical things in one or two lines of code. Show them lists, dicts and comprehensions. Show them for - else loops. Demonstrate duck typing and monkey patching. 
I've started my first week at a new dev job in a Perl shop. went out to lunch with the team yesterday, and one of them (who'd heard I was a python person) wandered over and said, "so, which is it? perl or python?" and having just spent a week learning and using perl, i actually had to stop and think about it. i finally said, "you know, with python, it's just so *readable.* you can actually show a non-programmer some python and they can reasonably work out what it does with minimal effort. perl on the other hand is like a tool into which you need to make an investment (even a small one) to get to a useful point. i compared it to vi. the first time you see it, you don't have any idea what to actually *do*. then you get used to it and can get things done efficiently. perl is more *efficient* for sure than python for a lot of the things I'd like to do.
OK, so I suppose you have never heard of a "performance". This was TV, remember. For the record Trish Gray is a smart cookie - at least smart enough to make you think she's dumb. You may not like the performance, but at least understand that's what it was.
Use wxPython. I recommend embedding matplotlib canvases using wxmpl
That doesn't sound at all like me, really. Glad you enjoyed it, though.
Thanks. I actually reconsidered to using wx.
I also use Jekyll. And it's easy enough to add anything you want more (e.g. markdown formatting), which makes up (somewhat) for the lack of documentation.
Tell them how Python is better than each of those languages. * than C++: no explicit pointers * than Java: less verbose * C#: foss, multiplatform, not tied to Microsoft * than any of those: Python interpreter * Python comes with Mac OS X and most Linux's Show code length comparisons. Show cool Python projects (Django etc) List institutions that use Python (Google, NASA, ...)
Oh noes, my anonymity! Maybe not roasted. Tipsy, perhaps. But then again, it's reddit, who lets the truth get in the way of a good story. I'm sticking with roasted.
I expect the downvotes are for the subjective nature of the topic. But I'll give it a try. For starters, Pythonistas should know how to create functions, classes, and packages. Next comes multithreading, networking, and interfacing with C code. Finally, familiarity with specific Python libraries such as Twisted and Django go a long way towards Python mastery.
 * idiomatic usage of builtin datatypes, including intuitive grasp on runtime characteristics, space/time tradeoffs, etc. * idiomatic usage of language constructs; iterators, generators, list comprehensions, metaclasses, magic functions, and an understanding of some deeper things like mro w/ multi-inheritance * solid knowledge of whats in the [stdlib](http://docs.python.org/library/), knowledge of when to use stdlib (heapq, multiprocessing) and when to use a popular third party lib instead (ie. lxml in preference to xml.minidom) * knowing how the python environment is organized at the OS level * comfort with using builtin profilers * comfort with using pdb in the shell * pep8 and beyond that an intuitive sense of what is "pythonic" and what isn't (import this) In addition, you should have done preferably all of the following: * write or modify a C extension (or auto-wrapped extension, ie from swig) * use the `compiler`, `inspect`, and various `sys` utilities for inspecting and modifying interpreter behavior * created and distributed a package via [pypi](http://pypi.python.org) There's a lot of stuff that is missing here that is probably generic to being a programming expert, like understanding basic networking, synchronization (threads &amp; processes), ipc, regular expressions, etc. Some people might insist that you should know CPython interpreter details (beyond simple knowledge of the GIL), but I think that you can become quite proficient in Python without it, and as other interpreters become more popular that knowledge might decrease in usefulness. There are enough little gotchas in the language and the various things above (like compile time evaluation of keyword arguments) to keep you busy.
There would be so many things, but things I can remind right now are: - Metaclasses and ABCs (including [`abc`][1], [`numbers`][2] and [`collections`][3]) - Iterators and generators (including [`itertools`][4]) - Standardized distribution methods ([`distutils`][5], [`setuptools`][6], [Distribute][], [pip][], [virtualenv][], etc) - [The Python Standard Library](http://docs.python.org/library/) - De facto standard third party modules e.g. [`lxml`][7] for XML/HTML parsing, [`simplejson`][8] for JSON parsing/serialization, [`gevent`][9] for asynchronous IO/green threads - Documentation using [Sphinx][] and [reStructuredText][] - Differences between Python versions (e.g. 2.7 vs 3.2) and implementations (e.g. [CPython][] vs [Stackless Python][]) [1]: http://docs.python.org/library/abc.html [2]: http://docs.python.org/library/numbers.html [3]: http://docs.python.org/library/collections.html#collections-abstract-base-classes [4]: http://docs.python.org/library/itertools.html [5]: http://docs.python.org/library/distutils.html [6]: http://pypi.python.org/pypi/setuptools [Distribute]: http://packages.python.org/distribute/ [pip]: http://www.pip-installer.org/ [virtualenv]: http://www.virtualenv.org/ [7]: http://lxml.de/ [8]: http://simplejson.readthedocs.org/ [9]: http://www.gevent.org/ [Sphinx]: http://sphinx.pocoo.org/ [reStructuredText]: http://docutils.sourceforge.net/rst.html [CPython]: http://www.python.org/ [Stackless Python]: http://www.stackless.com/
Actually, the person who wrote the server is still at the company. He learned Erlang because of its reputation for concurrency and wrote the original API server in it. This probably wasn't the best idea considering he didn't know erlang particularly well, but the original API server managed to run mostly untouched for 2 years, scaling up to more than 1000 req/sec. This post was about moving that code to a language we could understand, maintain, and update (if needed).
This kenneth guy is everywhere
Your last item is certainly important as it quickly demonstrate the breath of capability. Well it would if you can offer up an example of each institutions use. I'm a strong believer in real world examples. Ease of readability is worth focusing on. A solid warning about blocks of code and indenting is also in order. Personally I've been bitten hard by the code indenting approach to marking blocks. So it is something to stress to beginners. Especially if said beginners cut and past code from different platforms or editors. Blocks in Python are a weak point in the language. In any event the #1 reason I use Python is it's understandability. The whole idea that you can come back to a script months later and know exactly what it does at a glance is Pythons greatest feature. This should be stressed to no end. 
I'm actually surprised that somebody would implement a system with Erlang in the first place. C++ has more readable code. In any event thanks for the heads up. It is nice to see where python is being used successfully. Hopefully you can get back to us in a few months with a report on using PyPy in key parts of this infrastructure. 
There are already some good lists here to help you study Python programming. But you should never classify yourself as an expert or trust those who do.
Knowing how to do magic, and when not to. :)
eryksun
Be able to use *lambda* well without reading the documentation on it several times. 
Python
Thanks, but I'm far from an expert. I think you should change that to mitsuhiko, or one of the resident experts on Stack Overflow such as Alex Martelli, S. Lott, Ignacio Vazquez-Abrams, unutbu, Ned Batchelder, gnibbler, etc. 
NumPy and SciPy for scientific computing and manipulating large datasets?
They say to you, "significant whitespace is actually a good thing".
I use lambdas, liberally, in all languages that have them. I just learned the Python syntax for them. Upon seeing me use them for the first time, one of my friends said "STOP DOING MAGIC".
You forgot an intuitive understanding of pythonic decorators. 
From a personal standpoint, yes. From a professional standpoint, no. It is incredibly difficult for one to advance their career as a programmer without exaggerating at least a little. Since most employers seek "experts", it doesn't make sense to refrain from branding oneself as an "expert" on the résumé. But one should always realize that this is nothing more than a white lie.
The ability to do evil things (such as an ``@implic_self`` decorator) and the wisdom not to. (Semi-serious)
Preparing bunch of code before hand, thank you for that suggestion. Interactive interpreter was there in my list :)
XKCD FTW! :) Will add it for sure.
From the image above ? You meant "I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I&amp;#39;m leaving you." ?
Ya was planning todo a small comparison table. And thanks for the tip : Listing the institutions.
Ya indentation, need to stress on that!
I also code Perl and Python for living, but the perl code /me refractors is all legacy, so /me still &lt;3's py. Can you give an instance where will felt perl is far more better than py? 
Roger that :)
Almost 0...
Guido++ :)
pip and eggs ^_ ^
Heh heh true, but sadly Java still has a larger market share! 
related books please.
If you were selected to give a talk about Python to university students, shouldn't you know this answer in the first place?
Among the other things listed here, I'd like to add: * Know what is available in the Python Standard Library well This means that you don't have to know how to use every module in the PSL. However, you should take some time and browse through every module, however obscure it seems at the time, and get an idea of what's available in there. Then when the time comes to use it, it will be there and available to use, without you having to reinvent the wheel. The best programmer is the ones who can write the minimal amount of code to get the task done. I think a few non standard libraries should be on this list too: twisted, django, sqlalchemy, PIL, just to list a few.
I have made that clear, i'm clear but just wanted suggestions, taking suggestions is no way harmful and there is no perfection and end to improvisation! NO?
Yep, that's the one! :)
Perhaps one day developers will see the light. XD
Also making the inspect module jump through hoops (but again, please don't).
&gt; when to use a popular third party lib instead (ie. lxml in preference to xml.minidom) generally agreed, but bad example; cElementtree is one of the better stdlib modules with good performance and pretty much the same API as lxml.
&gt; sadly dot net! There's some great stuff in .Net, not sure why you say 'sadly'.
Trying to guess about the downvotes: The term 'expert' might be misleading and bugging to some. Because generelly that's not how it works. I'm certainly not a Python expert, but quite experienced on another field in engineering. Being an expert requires you to have a broad knowledge (here your roadmap helps), but also a deep understanding (in opposite to learned by heart) and a lot of personaly experience. The latter two differentiate the good programmers from the experts. Here's an article I like: http://norvig.com/21-days.html Keep on doing new stuff. Havn't ever written a GUI app? Learn PyQt and create an editor. No web experience, create a web app ... Suddenly (after 2-3 years) you might think: 'Damn, I've learned a lot.' But you probably won't think: 'Damn, I'm an expert!' But maybe you are? ;-)
+1 If I head to choose another language, I'd take C#.
Generally someone who has 10,000 hours of experience in a subject area could be considered an expert, although that's only generally. EDIT: clarification
i didn't say either was better. perl can be more efficient, though. think, for example, of how you'd write a perl program to parse a log and the equivalent in python. the perl code would be denser, more taut. it'd be more efficient, maybe even a one-liner. not the case with python. whether this is good or bad is subjective.
And I bet the position requires you to have 5 years of this work experience before you can even work to get work experience.
Stick to what you know. It'll make question time less awkward. 
To have 10 other experts acknowledge him as an expert.
Forgive me, but other than performance, what are the advantages of `simplejson` over the standard library's `json`?
I don't see how lambdas are confusing. The first time I saw them in code I didn't understand what it was doing, it did look like magic, but a quick google search explained everything.
Just performance. In most cases, you can use standard `json` as counterpart of `simplejson`. try: import simplejson as json except ImportError: import json
No regrets but the speaker says he would go with something else than python if he has to do it all over again.
Not really. Most good employers would recognize that languages are mere tools and that once you know a few you can pick up more relatively easily.
Add "or cheat" (without quotes) to the while loop: while guess != code or cheat and guesses &lt;10: I haven't tested it yet, but I think it's close to what you want.
&gt; Generally someone who has 10,000 experience in a subject area could be considered an expert, although that's only generally. You accidentally the hours.
That's a journeyman, not an expert. An expert is a dude Guido knows by name from the CPython mailing list. Or Alex Gaynor.
;)
She's the HOST not a contestant!
Then why not choose cjson?
I'm not exactly sure how python prioritizes comparisons, but if nothing else you can just put parentheses around the or statement. You could also just put the if statement (for the cheat) right underneath the while statement and call a break (though I don't really know what cheat_code() does so I'd go with the first).
FOSS FTW, not dot net! nor Mono 
Why?!
Ah, thanks
This was exactly what I did try afterwards, but that doesn't work. The result of this is that the prompt never stops and I can't use 007 even on the 10 prompt.
Not really. To have Guido know you by name, you probably need to be working on CPython itself. However, that isn't necessary if you use Python day-to-day, regardless of how much of an expert you are.
That's because cheat exists, so the `or` statement always evaluates to true. Try `while guess not in [code, cheat] and guesses &lt; 10:` That should work.
Code readability.
I've had the best results with ujson (https://github.com/jskorpan/ultrajson) myself. 50x faster than stdlib json for loading my datasets.
You're talking about D&amp;D again, aren't you.
It's always interesting to read these lists. I guess being a "python expert" brings some implied baggage along with it, like that you're going to be doing things with databases and working in a web-enabled world, because the tools for this kind of work always crop up, and discussion always strongly head in those directions. I work all day in Python, but don't ever do anything outside of the standard library, because all of my work is tools work in video game development. I will never need django, and I'm unlikely to ever need anything involving databases or networking. At best, I might come up with some cool uses for PIL, and I might play a bit with something like ElementTree for storing some data about a rig. Of course, I don't at all consider myself a Python expert.
ARE YOU A WIZARD?
I now have an intuitive understanding of decorators in Python, but outside of say, logging, I have yet to come up with a need for them. Even logging isn't exactly something I personally need, but at least it was something I could understand as useful.
Parrot sketch, Cheese shop, Number 3, the Larch, the Lumberjack song and a couple of Eric Idle songs.
Some suggestions: * You can use parentheses to write a multi-line statement, and take advantage of the compiler's implicit join of adjacent string literals. * You can use the tuple constructor with a generator expression. * You can replace the while loop with a simpler for loop. : def laser_weapon_armory(): print ( "You do a dive roll into the Weapon Armory, crouch and scan the room\n" "for more Gothons that might be hiding. It's dead quiet, too quiet.\n" "You stand up and run to the far side of the room and find the\n" "neutron bomb in its container. There's a keypad lock on the box\n" "and you need the code to get the bomb out. If you get the code\n" "wrong 10 times then the lock closes forever and you can't get the\n" "bomb. The code is 3 digits.\n" ) code = "%d%d%d" % tuple(randint(1,9) for i in [1]*3) #or even simpler: #code = ''.join(str(randint(1,9)) for i in [1]*3) cheat = '007' codes = [code, cheat] for trial in range(10): guess = raw_input("[keypad]&gt; ") if guess in codes: break print "BZZZZEDDD! %d try" % trial 
They're useful with django and other web frameworks.
That worked! Thanks. 
If your statement is: while guess != code or cheat and guesses &lt; 10 it's evaluated as: ((while guess != code) or (cheat)) and guesses &lt; 10 In particular, (cheat) is evaluated as: False if cheat is 0 or an empty string or None; True otherwise.
I honestly am not very good with boolean logic yet. I do not understand what diffrence is between: while guess not in [code, cheat] and guesses &lt; 10: And while guess != code or cheat and guesses &lt; 10: But I am trying to learn. 
Uhm, Python's string formatting is the same as that of the printf family.
What you want for the second one is: while guess != code or guess != cheat and guesses &lt; 10 This is because your second statement isn't equivalent to what I have above. In English you can say that "If guess is equal to 12 or 15", but it doesn't work this way in programming languages; it's ambiguous. And, as I mentioned, python does have a way of evaluating the True/False-ness of a given variable - so that if you have: if 1: #1 evaluates to True if 0: #0 evaluates to False if 256: #256 is not 0, so evaluates to True if "Happy": #Happy is not an empty string, so evaluates to True if None: #None evaluates to False As a result, when you have while guess != code or cheat Python does not expand that statement to what you expect it to, since 'cheat' by itself is a boolean expression. I think I may have made a bit of a mess in trying to explain this; let me know if there are any confusing parts.
Ha! Can't believe I missed that. It's convenient enough to manipulate strings in Python that I probably never realized I was missing something...
Except that's purely subjective. I guess what bothers me is not having the choice to do what I want to do. I see no readability issues with say, an if statement that simply raises an exception contained in a single line. And other people, who may not prefer it that way, shouldn't really have any issue reading it.
Perhaps, but I like how Python enforces consistent style.
I am understanding it better, but the code: while guess != code or guess != cheat and guesses &lt; 10 gives the same result when I execute as: while guess != code or cheat and guesses &lt; 10 Unfortunately. 
Oh. I forgot to apply De Morgan's law. Sorry. What you want is: while guess != code and guess != cheat and guesses &lt; 10 This is equivalent to: while not (guess == code or guess == cheat) and guesses &lt; 10
I'm a total newbie and I use Fabric to call "hg pull; hg update" on my remote servers. Is that what you're talking about? Not understanding the fabric hate in this case.
The three secret bytecode ops that GvR has hidden away in a remote volcano :)
You may as well De Morgan the whole expression in that case: while not (guess == code or guess == cheat or guesses &gt;= 10) 
Mono is open source. 
You can use them for rudimentary type-checking
But not FreeSoftware!
Then that's easy, you'll want to just give an overview of all Py features as a contrast to C: talk about defining objects and object orientation, recursion, scoping memory management, and general syntax. Python is quite interesting on its own, most of your audience will end up thinking, "It's *that* easy?"
Or maybe someone should. There was some initial work done a few years ago, but for now it doesn't seem to scratch anyone's itch. Some improvements could be made by ironing out incompatibilities, but the original specification for DBAPI 2 did leave quite a few things open (including whether to allow calls to stored procedures if the platform has them). It does seem to let people get their work done, but more portability is always useful (if it doesn't slow things down much).
Will KIS :)
I believe somelist[:] = will change the physical list somelist points to, whereas somelist = will create a new object and update the somelist pointer to point to this new object. Hence, any other stuff going in the program on that was referring to the old somelist before it was filtered will still be referring to the same old list if [:] isn't used. somelist = [1,2,3,4,5,6,7,8,9,10] &gt;&gt;&gt; id(somelist) 44726536L &gt;&gt;&gt; somelist[:] = [x for x in somelist if x%2] &gt;&gt;&gt; somelist [1, 3, 5, 7, 9] &gt;&gt;&gt; id(somelist) 44726536L &gt;&gt;&gt; somelist = [ x for x in somelist if x%3] &gt;&gt;&gt; somelist [1, 5, 7] &gt;&gt;&gt; id(somelist) 44551752L Notice that the id changed.
I guess my real question though is: why? I'm not really clear on why you can assign to a slice object in the first place, and why that would update the original list.
If you do somelist = [1, 2, 3] what is actually happen under the covers is that a new list object is being created and 'somelist' is being bound to that new object (it 'points' to it, if you like). If you do somelist[:] = [1, 2, 3] what's actually happening is a method call that looks like this somelist.__setitem__(slice(None, None, None), [1, 2, 3]) Here, a new list is still being created, but it is being given to the existing list as an argument in a method call. The existing list uses it to set what elements it contains. See the documentation for more details of the __setitem__ method (which is called whenever you assign to an index, so is also used to set items in dicts, for example) and slice objects.
&gt; Metaclasses and ABCs; Iterators and generators And more generally the various protocols of Python: lifecycle (`__init__`, `__new__`, `__del__`), attribute access (`__getattr__`, `__getattribute__`, `__setattr__`, `__delattr__`), descriptors (`__get__`, `__set__`, `__del__`), etc...
&gt; cElementtree is one of the better stdlib modules with good performance and pretty much the same API as lxml. That's because `lxml` implements the ElementTree API. But lxml adds CSS and XPath traversal, HTML parsing and extends a number of functions and methods with new parameters.
I'm pretty sure Guido knows Alex by name.
&gt; I'm not really clear on why you can assign to a slice object in the first place For bulk replacement of subsequences (including subsequences of length 0 or of length "the whole list"). &gt; and why that would update the original list. For the same reason assigning to an index updates the original list? What other semantics could assigning to a slice have anyway? Since assignment is a statement in Python, that's pretty much the only semantic it could have could it not? Although the real answer to your questions is "because that's how `__setitem__` is implemented on `list` when the index is a slice"
I'd say because using indices refers directly to specific memory locations, like somelist[2] which follows the reference to somelist and operates on elements in place. edit: **feeling shamefully unscientific** You need look no further than Isvara's answer.
Some great advice here: http://stackoverflow.com/questions/2573135/python-progression-path-from-apprentice-to-guru
TIL that listslice assignment is true 'filter'ing I could say l = [1,2,3,4,5] l[:] = [x for x in l if x%2] and I've modified 'l' right? 
what if you passed somelist to a few objects. The objects want to read the contents of somelist once in a while. If you change somelist without doing the slice thing then the objects will continue seeing the original somelist, not the new one you created.
A Python expert was asked for. Don't ask for an expert if you don't want to know what it takes.
I'm not and I use a git hook so when I do a 'git push' on my box the server's repo gets updated and the working copy is checked out. This is the problem with Fabric - it encourages bad practices instead of providing generic tools. It seems targeted at copy/paste newbies instead of programmers. And it keeps coming up in python conferences while paramiko remains in the dark...
You make a good point, but you do specify that nobody should say "expert" except in the narrow case of trying to get a job or get a promotion. I think everyone does that. In almost every other circumstance, saying you're an expert makes you come off as douche-y and pompous.
Because it is a) unmaintained AFAIK, b) has tons of bugs including accepting invalid JSON, failing to handle unicode chars correctly, and buffer overruns.
I understand the reasoning behind this comment, having had a formal CS education, but despite believing strongly in this once I've abandoned it. I think it's a bit disparaging to the efforts and accomplishments of others, and on top of that I don't think it helps put beginners on the right track to "mastery." You can break any language down to its grammar and vocabulary, but to be an *expert* in it there's a cultural background that's necessary. In a programming language, the phrases and idioms that this body of culture provides are not merely arbitrary historical quirks; they were carefully developed over time to play to the strengths of its constructs and its implementation, and knowledge of them serves a deeper function, as they guide the programmer to write programs in a way that is coherent with the underlying philosophy of the language.
Yes, but it's nice to have the elaboration sometimes.
 &gt;&gt;&gt; list = range(10) &gt;&gt;&gt; for x in list: print x, list list.remove(x) 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 2 [1, 2, 3, 4, 5, 6, 7, 8, 9] 4 [1, 3, 4, 5, 6, 7, 8, 9] 6 [1, 3, 5, 6, 7, 8, 9] 8 [1, 3, 5, 7, 8, 9] So basically, the "for" loop moves one index up in list every time. So it starts at the zeroeth element, x = 0. Remove 0. Then x is assigned to the first element-- since you've removed 0, 1 is the zeroeth element and 2 is the first element. Remove x= 2. Then it goes to the second element. Since list is now [1,3,4,5,6,7,8,9], the second element is 4. Remove 4. etc. I mean, this is unconventional behavior because you're changing the list you're iterating through as you go.
i think "thread programming"
this is correct. Also, don't do this. *EVER*. Don't do it in reverse either. It might look safe, it might even be safe in CPython, but this is only a consequence of the particular implementation used. A coincidence. It might cease to work at random in a future version, and it might not work at all in Jython, IronPython, PyPy, or any other implementation. The docs tell you not to mutate a list while iterating over it, so just don't. Make a copy. The maintainer who comes after you will thank you for it.
ALL the jobs on LinkedIn require expert/senior level, yeah WTF I don't even bother looking at them any more
Thanks, I hadn’t known that until you mentioned. But it would be better if it follows the standard serialization module interface (`dump(obj, file)`, `dumps(obj)`, `load(file)`, `loads(string)` functions); it currently doesn’t provide two functions (`dump`, `load`) that take a file. Well, I wrote [the issue for that][1]. Edit: I wrote a patch and the pull request was accepted. **Now it has `dump()`/`load()` functions and interchangeable with standard `json` or third party `simplejson`.** [1]: https://github.com/jskorpan/ultrajson/issues/18
Have a look at [CherryPy](http://www.cherrypy.org/) for countless examples of good uses for decorators.
The following list = range(10) for i, x in enumerate(list): print i, x, list list.remove(x) produces the following output #i x list 0 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1 2 [1, 2, 3, 4, 5, 6, 7, 8, 9] 2 4 [1, 3, 4, 5, 6, 7, 8, 9] 3 6 [1, 3, 5, 6, 7, 8, 9] 4 8 [1, 3, 5, 7, 8, 9] Basically you're saying, at index *i*, remove value *x* from *list*. If in case you don't know already, the remove method is modifying your list during a loop. This is known to cause headaches. **edit:** I forgot to mention, if you're trying to remove all elements from a list which is what you're list[:] mod does, simply try del list[:]
Sounding douchey and pompous is only part of the problem with seeing oneself as an expert. To quote Shunryu Suzuki, "in the beginner's mind, there are many possibilities, but in the expert's mind, there are few". Losing the "beginner's mind" entails very bad things for one's practice of this craft. When one thinks "I know this - I have nothing to learn from you", they stagnate. They stagnate not just in knowledge but also in valuable experience. In the grander scheme of things, obsessing over whether one is an "expert" serves no useful purpose whatsoever (in fact it's counterproductive). Programmers aren't driven to learn and improve for the sake of some badge. The best way to improve is to just program. Let it flow naturally and learn from your inevitable mistakes.
if the right hand list l[:] has more elements then you are trying to unpack from [x for x in l if x%2] you will get an exception, you could do l[:] = [x%2 for x in l] 
The initial code loops over a list using a for-loop. The problem with this is that you're still iterating while attempting to remove elements. The idiomatic approach is to filter the list, selecting for either the elements you want (`select`) or the ones you don't (`reject`). Python, Ruby, Smalltalk, and Lisp excel at this exact behavior. The upvoted answer does just this: somelist = [x for x in somelist if determine(x)]
Also, don't shadow builtins.
 &gt;&gt;&gt; datetime.datetime(2011, 8, 7, 23, 53, 25, 837005).hour 23 Is this what you're looking for?
Does "git push" checkout your code the dev test machines, or the production servers? If the later, do you really release new code to production based on a git push? Many times software release is not just checking out software, it might as complex as creating a whole new VM/container for the software to run it. 
I'm interested in this, too.
Follow-up question: In the responses, they're using list comprehension to sift out the list. Would it be better to use filter instead? Example: somelist = range(1000) somelist[:] = [x in somelist if x%2 == 0] vs. somelist = range(1000) somelist[:] = filter(lambda x: x%2 == 0, somelist)
We have enough of weird names to counteract the reserved keywords, I find it fully acceptable to shadow builtins. They're easily imported as 'builtins' anyway (``__builtin__`` in 2.x). I disabled special highlighting of builtins in Vim some months ago and haven't been happier since. :)
The brackets are called "subscript notation" (you can think of ``x[4]`` as ``x₄`` in a mathy sense), which is what "unsubscriptable" means: datetime objects don't implement ``__getitem__``. In fact, they are more semantically rich as masterJ's reply demonstrates. See the docs for [datetime instance attributes](http://docs.python.org/library/datetime.html#datetime.datetime.year).
OpenCV Python hands down [Link to docs](http://opencv.willowgarage.com/documentation/python/index.html)
No, list comprehensions are faster and they're usually considered more readable than filter and map.
They're particularly well suited for transforming/wrapping functions/classes, or registering them with something. @classmethod def alt_constructor(cls): "I'm called on classes rather than instances" @abc.abstractmethod def implement_me(self): "I lack a concrete implementation" @property def descriptor(self): "I'm wrapped in an object that implements __get__" @atexit.register def cleanup(): "I'll run right before Python terminates" @app.route('/post/&lt;id&gt;') def show_post(id): "I handle web requests to /post/xyz" There's nothing you can do with decorators, that you couldn't do without them, trivially even. What they give you is a readable, recognizable notation for expressing this common pattern. Actually I think logging might be one of their least useful uses; we can inspect the interpreter call stack if need be, which for example is what tracebacks do on exceptions. Logging should typically be at a higher level.
This looks quite interesting. Anyone have experience of using it?
If you understand thread programming in any language you're a computer scientist. ;)
It's not a list. That's probably why you're having a problem.
I found that out the hard way!
That could be what I'm looking for. If so, thanks! 
I'll check that out. Thank you!
So this is an Eve bot or a WoW bot? Just curious :)
Didn't even know these existed, been using Jekyll for a couple of years now. Is there a reason to switch to either of these other than just to have a Python tool?
I think its a memory vs time compromise. If you go around modifying the list that's too much work and that's the reason why push/pop has low performace. On the other hand, you could do list comprehensions all day - create new lists on the fly, bind it to the same variable and let the Garbage Collector take care of the orphaned lists. l = [1,2,3,4,5] l = [x for x in l if x%s] The original list *should* be GC'd
This only modifies the variable, and not the list it refers to: a = [1, 2] b = a a = [3] print b # prints [1, 2] Where slicing modifies the list itself: a = [1, 2] b = a a[:] = [3] print b # prints [3]
If OP has any doubts, I want to second OpenCV. It came in handy when I was working in R&amp;D.
&gt; Would it be better to use filter instead? I'd say so. List comprehensions are pretty flexible, and they can make some operations clearer, but in the case of just keeping or removing values based on a simple predicate, I think using the `filter` function expresses your intent more clearly (which is what good programming is about).
If you can program in python, you can call libraries with ctypes. Keep your options open if you really want speed.
I find your ideas intriguing and I wish to subscribe to your podcast.
Stuck in IE here, guess it's a HTML presentation - what am I looking at?
http://flatland.readthedocs.org/en/latest/
Here's a slightly simpler example (w/o the list comprehensions) that helped me understand the difference: blah = range(10) for n in blah: print n, blah = blah[:-1] # 0 1 2 3 4 5 6 7 8 9 blah = range(10) for n in blah: print n, blah[:] = blah[:-1] # 0 1 2 3 4 In the first example, a separately new list is created inside the loop but the old one is still in memory, and is used by the loop. In the second example, the new list replaces the old list in memory, and so it affects the behavior of the loop. 
Guido += 1
Yes i've used it for one project. I found the documentation really lacking and had to dive into the source a lot of times. I think colander (http://docs.pylonsproject.org/projects/colander/dev/) is a good alternative, at least the documentation seems to be better.
I don't think there is any :)
It's an object, though. Objects have attributes. In the future you might want to call `dir()` on objects in the interactive shell to find out what attributes (and methods) they have.
It's currently used for Magic Online to trade cards using real currency. But the reason I'm switching libraries is I will be abstracting out most of the logic to create a trading bot framework that can be used to develop for any game. So people can implement it for other games like Eve or WoW.
You speak of the pycruxes. GvR has bound parts of his soul to them.
or here for some demos http://deformdemo.repoze.org/
What people are missing is that this is just a side-effect of slice notation. The stereotypical usage of slice assignment is providing actual start/end values. For example, a Reverse Polish Notation calculator may take the list [3, 4, "+", 5, "-"] and repeatedly do this to it: stack[0:3] = process(stack[0:3]) (returning something like [7] for the first run) It's not so much that list[:]=... is the intended purpose of slice assignments, it's just a side effect of being able to replace a range/subset, **at all**, with one of a different length. [:] is just as valid a slice as [2:5] or [20:-4] or [::2]. It just happens to be very useful here in that it preserves the list object - in an operation that's changing the bulk of its contents. Other variables referencing this same list will show the new value after the slice assignment. 
Also, don't forget that the standard library will have docstrings you can access pretty easily. &gt;&gt;&gt; import datetime &gt;&gt;&gt; help(datetime.datetime) Help on class datetime in module datetime: class datetime(date) | datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]]) ...
I use help() all the time. Also dir() to see what methods are defined on the object. &gt;&gt;&gt; dir(datetime.datetime) ['__add__', '__class__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rsub__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', 'astimezone', 'combine', 'ctime', 'date', 'day', 'dst', 'fromordinal', 'fromtimestamp', 'hour', 'isocalendar', 'isoformat', 'isoweekday', 'max', 'microsecond', 'min', 'minute', 'month', 'now', 'replace', 'resolution', 'second', 'strftime', 'strptime', 'time', 'timetuple', 'timetz', 'today', 'toordinal', 'tzinfo', 'tzname', 'utcfromtimestamp', 'utcnow', 'utcoffset', 'utctimetuple', 'weekday', 'year']
Based off of http://maccman.github.com/spine/
Never had problems with it, and I use it daily, even with unicode stuff. The unmaintained thing is not OK (but there is czjson at http://pypi.python.org/pypi/czjson/1.0.8 which seems to work). BTW, wrt the 'invalid JSON' thing, I hope you're not meaning the same idiotic issues as what some moron posted once on the comments on the cjson page (now disabled): that bailing out on integer keys is 'correct behaviour' because the json standard does not accept integer keys, so you have to iterate over every dictionary object to make sure it's keys are converted to strings. And BTW, the speed difference is absolutely noticeable. 
the site the video is embedded from works better, in that it has flash fallback, at least
you can still do one-line ifs in python: if expression: statement
you learn something new every day.
[Autopy](http://www.autopy.org/) is pretty perfect for this: "AutoPy is a simple, cross-platform GUI automation toolkit for Python. It includes functions for controlling the keyboard and mouse, finding colors and bitmaps on-screen, and displaying alerts -- all in a cross-platform, efficient, and simple manner."
This question comes up a lot, is it part of some study course or something?
In my opinion, you're telling a group of developers who believe in compilers, verbosity, and strong static typing, to enjoy an interpreted, somewhat dynamically typed language. It would be like trying to introduce the same group of people to perl or php. You could probably instead, talking about the joys of languages like Python, using Python as the example.
Yes loads of simple one liners, for the strong, dynamic and duck typed language py :)
This is not up for debate. DO NOT shadown builtins.
where ?
`list`
Whenever I think about metaclasses I'm reminded of this quote from the Zen of Python: &gt; Simple is better than complex. &gt;Complex is better than complicated. Use of metaclasses is an example of the latter. 99% of the time, you don't need or want to use them. But there are very rare cases when introducing the complexity of a metaclass is better than the alternative, complicated solution (which often involves using a hundred nested if/elif/else or try/except/finally statements, and/or the dreaded *eval*). The Django ORM is a great real-world example. Those interested in the actual implementation can read the code in [ModelBase and Model](https://github.com/django/django/blob/master/django/db/models/base.py#L28) (lines 28 and 275) and the [get_model](https://github.com/django/django/blob/master/django/db/models/loading.py) function. This code *looks* complex (and it is), but is much easier to understand than you'd think. The major (imo) feature it adds is the handling of type translation from the database to your program, which saves a huge amount of time for the programmer. I don't want to ramble on too much about Django because not everyone here is a Django user, but I think this is a really cool example of real-world metaclass use.
Finally, an explanation of metaclasses that I actually understood!
Bots have always played a huge part in MTGO, you can't expect people to be on 24/7 in hopes of a trade.
if all( 2&lt; x &lt;= 8 for x in (a,b)): return 2 all() is a builtin function which accepts a sequence (list,tuple,generator,etc..) and evaluates to True iff every element in the sequences evaluates to true. There's a corresponding any() function. Python allows the chained evaluation of lower &lt; x &lt; upper. Since it's the same condition for each variable, a generator based off a tuple of the two variables can be passed to all()
 max(a,b) &lt;= 8 and min(a,b) &gt; 2 But I think the way you originally wrote it is clearer.
Edit: use SliceOf314 answer
I'd chain the comparisons: if (2 &lt; a &lt;=8) and (2 &lt; b &lt;=8): return 2
The quest for conciseness should never trade off readability. Depending on a and b, the way it's written now is the way it should stay, or if a and b are significantly disjoint in meaning, it may be clearer to do if (a &gt; 2 and a &lt;= 8) and (b &gt; 2 and b &lt;= 8): your way (if they're related in meaning) or this way (if they're completely unrelated) preserve clarity, which is what matters most. Trying to shorten a line like you're suggesting is a bad idea. If you're not optimizing the logic, you're just making the code harder to read, and in 3 months you'll want to come back and punch the you right now in the face for making that decision. 
Please check out our new cross platform open library written in python: http://www.simplecv.org Extremely easy, way easier than the python wrapper for openCV. We just released 1.1 (packages coming very soon). 1.1 has our own blob detection library, we got sick of dealing with openCV's version. Code is simply: c = Camera() #or Kinect() i = c.getImage() blobs = i.findBlobs() blobs.coordinates() #returns all the coordinates of blobs sorted faces = i.findHaarCascades("trained.xml") faces.draw() i.show() Works in web browser, mac, linux, etc. Any feedback is greatly appreciated. 
it doesnt' have to be like that, to me it's a matter of teaching concept. These people already know how to code, but they've probably been indoctrinated to that style of coding (especially the Java folks). The small syntactic differences of python compared to other languages, is probably 10% of the discussion, while the rest is "Dynamic vs Compiled - How to lose the business suit and be a rock star instead" :P
So as a programming Luddite, I've always been curious who these SO savants come from. Is this "J F Sebastian" fellow somebody you've heard of? Is he an instructor? Or did he just pay too much attention in class and have too much time on his hands? I ask because it's rare to get that depth of knowledge with the ability to articulate in most fields. At least it's rare to see it offered up at no cost to a stranger on the internet. 
All the end-point logic can fit in the post-update hook and it will run no matter which developer does the push. My actual setups are more complicated with test repos and automated testing before allowing the code in production. Compare this to the model peddled by Fabric - one coder running a script to copy files to one server.
I think you are a bit biased because of how bots have been used historically. You have to look at what the bot does objectively. The ones my framework would cover would merely make an automated storefront. This is not only a benefit to the vendor but also to the customers as they can have more options since more people are online trading. Even Blizzard implemented an auction house which essentially does the same thing. The only difference is bots would automatically list new items and check mailboxes.
do you have some numbers as far as speed of pixel scanning? It's a very important need for the library I use. ease of use is not a top priority as I'm already using Sikuli which is easy to use, but is no longer adequate in terms of speed and accuracy with image matching.
tl;dr voodoo.
This matches if any variable in varlist is in the range, but doesn't require all of them to be.
The behavior of list iterators (for x in list) is unspecified when combined with modifying the list inside the loop. You may find a way to make it work (e.g. by going backwards as people suggested), but don't do it. Simply an upgrade to the interpreter may break it completely.
I think that is very untrue. Real depth of knowledge goes hand in hand with the ability to articulate. Many people with a good understanding of a field are more than happy to help others understand. There are numerous examples of websites full of people happy to help strangers (this one, for example). The internet is a big place, full of people from all different backgrounds and skill levels. 
r = range(3,9) if a in r and b in r: return 2 # or # if all( i in range(3,9) for i in (a,b)): return 2 
this would fail if a or b are floats
It is using the python imaging library. You could always write your algorithm in PYPY (http://speed.pypy.org) and use our library as a wrapper. Since our code is easy and quick to setup you can run a time test on a function. From our shell you can define your function or import as a module. Then you just run the timeit command and it gives and average. In the shell it would be: from my_module_with_process function import * i = Image("path/to/image") timeit your_function(image) 
Great post, this is what I love to find on reddit, thanks :)
THANK YOU!
So how do you trigger a redeploy code to a different machine? Do you do another check-in? git is just one piece of the puzzle, tying all you deployment to that seems a bit limiting.
It's better to use "any" or "all", like so: if any(v for v in varlist): return 2 Any will return true if all are true, any if at least one is true.
You are being a bad code monkey. No banana.
Do you have any prior programming experience at all?
oh God no
grew up with Guido
what is the source of the data? text file? csv? and also what is the desired output? through a webpage? spreadsheet? we're going to need abit more information to be able to help you out :)
You might want to consider readability. One year later when you come back to this, you might say wtf? :-)
It looks more readable to me...
Self taught python. I only program as a hobby, I am a high school chemistry teacher. 
The data is given a paper course request sheet, and each course has time slots allotted to it, and the output would be a spreadsheet or even text. Thanks for taking the time to help
Great explanation. After reading, though, I couldn't figure out why you would ever use metaclasses rather than class decorators. Is there anything metaclasses can do that class decorators cannot? Metaclasses do give you immediate access to the class's raw data, so they may be slightly cleaner in some circumstances, but is there anything beyond that?
Sorry, the page moved. http://pypi.python.org/pypi/stattr/0.13 EDIT: Full disclosure, because I fail at packaging. EDIT: The page moved back, because I fixed my stupid mistakes. Thanks, #python and especially TFKyle!
[a.attribute for a in original_list] read http://www.python.org/dev/peps/pep-0202/ for more info on list comprehension in Python
Hope you guys like it. Any feedback is welcome.
One of the neatest and cleanest practical examples of metaclass magic I've seen in python is in [this recipe for a ring buffer](http://code.activestate.com/recipes/68429-ring-buffer/). Basically it acts like a normal buffer until it reaches the end, then it transforms itself into different class that implements a ring buffer. Saves on a lot of if blocks and other headaches.
&gt; In this day and age CPython is still core-locked I don't really like that kind of mood, since (as I understand) GIL is still there not because noone is able to get rid of it, but because that would make single-threaded python programs much slower, or will get need of making two libraries (one thread-safe and another non-thread-safe but fast).
I wouldn't really consider this metaclass magic, just magic (and of a sort that would be more difficult to debug in my opinion).
I do have a tendancy for borderline douchebag but honest vernacular. Was it too strong or just plain impolite? I realize there are technical hurdles to overcome. But that was just as true for C. Don't take this post as a plea to hurt single threaded python, imo the best of both worlds is attainable.
&gt; Was it too strong or just plain impolite? No, it's just that I (and, as I thought, other python people) not just "not dislike" GIL, but I really enjoy that one. Since it really makes my scripts not as slow as they would be without GIL. I mean, without GIL CPython would be not just slow, it would be terribly slow, so we should love GIL and not use CPython for things that involve threads+hard processor work (for now). Maybe pypy will be so fast that even making it gill-less will be ok, but with current python speed -- we should love GIL and thank that thing for making our python fast.
I made a small modification in the program. Added: import multiprocessing Changed: class Worker(threading.Thread): To: class Worker(multiprocessing.Process): And run the code: python gildetector.py 2.7.1 (r271:86832, Nov 27 2010, 18:30:46) [MSC v.1500 32 bit (Intel)] # of threads: 1, took 0.259523, effectively 1 cores # of threads: 2, took 0.273987, effectively 1.89442 cores # of threads: 3, took 0.28669, effectively 2.71571 cores # of threads: 4, took 0.298521, effectively 3.47744 cores # of threads: 5, took 0.465576, effectively 2.78711 cores # of threads: 6, took 0.522528, effectively 2.98001 cores Python is utilizing 3.5/4 cores Rejoice! You're awesome! So I am awesome on CPython 2.7 too! Here run it yourself and see if everyone is awesome: http://codepad.org/Q4RA3qhB 
I see. Well, in theory you could have a python interpreter for every thread. And then you wouldn't lose any performance for the single threaded case. There would be that memory inefficiency though. I think the GIL is currently the fastest existing solution for cpython, so I can thank it for that. Though it's clear this does not satisfy me yet.
Such a fitting username :) Would you force me to add something that's easy for threads but harder for processes?
I don't know. the multiprocessing module is specially designed to port threading code easily to processes. Of course if you put some shared variable that you update very often then I guess will be harder to port and probably will be slower with processes.
Well, I thought in cpython you actually get a real OS thread with "interpreter for every thread". And jython is so slow because it creates "native" threads too, but every data structure should be thread-safe (so (as I understand) when you put something into dict it first aquires lock for that dict and then puts what it needs there, then releases lock, but that's what makes jython super-slow in real-world apps, while cpython just takes lock (GIL) per thread, does it's job and then releases the lock, while operations like reading from file or socket don't need to take that GIL since they don't modify (while read/wait) anything in python).
List comprehension: attrlist = [l.attr for l in mylist] Map: from operator import attrgetter attrlist = map(attrgetter(attr), mylist) The list comprehension is the preferred method in the python community but many also like using map.
I'm not a GIL pro, but I think jython is slow for other reasons. If there were 2+ independent cpython interpreters then you could parallelize perfectly (python's effective core count would match your true core count). What the GIL does is prevent python threads from doing python stuff in parallel while allowing them to do I/O in parallel. I'm probably not perfectly accurate in that explanation but I think it's sufficiently precise.
dir() returns a list of attributes for an object. 
http://morepypy.blogspot.com/2011/06/global-interpreter-lock-or-how-to-kill.html &gt; That's not actually new, because Jython has been doing it all along. Jython works by very carefully adding locks to all the mutable built-in types, and by relying on the underlying Java platform to be efficient about them (so that the result is faster than, say, very carefully adding similar locks in CPython). By "very carefully", I mean really really carefully; for example, 'dict1.update(dict2)' needs to lock both dict1 and dict2, but if you do it naively, then a parallel 'dict2.update(dict1)' might cause a deadlock. I mean this locks.
&gt; type is actually it's own metaclass. This is not something you could reproduce in pure Python, and is done by cheating a little bit at the implementation level. Why can't you do this in pure Python? Also, from my experience metaclasses do have a fair share of black magic entirely their own, and not of the problem area. An example: class Subclass_metaclass(type): def __new__(self, name, bases, fields): print 'Subclass_metaclass.__new__()' return type.__new__(self, name, bases, fields) def add_subclass_metaclass(name, bases, fields): print 'add_subclass_metaclass()' return type.__new__(Subclass_metaclass, name, bases, fields) print 'creating class A' class A(object): __metaclass__ = add_subclass_metaclass print 'creating class B' class B(A): pass This way the actual metaclass gets run only for subclasses of A. I don't quite get why, I don't quite understand why it must be done in this exact way, or how I can make it "twice-removed" (or how), etc.
&gt; Well, in theory you could have a python interpreter for every thread. "In theory"?
My question is, how well does 2to3 work to port these over? I know in some instanced, such as with BottlePy, I was able to use 2to3 and it works fine afterwards. As a novice i do not fully understand the differences between 2 and 3 and that makes it a bit harder for me. I do know that **print** is slighlty different **raw_input** is now just **input** and some thing about python3 using Unicode for strings instead of ASCII?
On this trivial benchmark jython is almost 3 times slower than cpython. I don't know if that's the kind of performance hit that's expected from adding locks. I'm guessing it has at least a bit to do with Java vs C etc. I would be fine if all the built-in types weren't thread safe (many aren't) because part of writing threaded code is knowing where/when/how to lock. The problem as I understand it is the internal (cpython interpreter) usage of these types is naive thanks to the GIL.
 if (2 &lt; a &lt;= 8) and (2 &lt; b &lt;= 8): if all(2 &lt; x &lt;= 8 for x in (a, b)): I'd argue the first is much more obvious in this case.. If there is more than two variables, the second definitely scales up better
A completely independent interpreter....
[Here is a very good guide to the differences between 2 and 3](http://diveintopython3.org/porting-code-to-python-3-with-2to3.html)
I am looking at this now, very nice! I had been looking for something like this that explains what the differences are. Thanks!
There are plenty of multithreaded bytecode interpreters that are plenty fast. The GIL is hard to remove because of a bunch of design choices, some in the Python language and some in CPython, reference counting being probebly the biggest single culprit.
Yeah I would like to see that since I'm of the impression that processes can be used for much the same optimizations as threads with some advantages and some drawbacks. But for core utilization, processes seem to be the end game for scaling this since they can be made to run remotely on a cluster where threads are bound to one process and thus one machine. edit: after proofreading while caffeinated
Isn't that exactly what you get when using `multiprocessing`?
You may also want to look at some existing projects which have been ported to see what they've had to keep distinct between 2.x and 3.x. [httplib2](http://code.google.com/p/httplib2/source/browse/) and [CherryPy](http://www.cherrypy.org/browser/trunk/cherrypy/wsgiserver) come to mind.
Almost. The small difference is that in multiprocessing you have to use some form of IPC for sharing information which can be costly. For many applications multiprocessing is fine. If the processes have to share a lot it can become a pain.
But how would you implement shared state with completely separate interpreters without using IPC of some form? Or do I misunderstand what you mean by "completely independent"?
That's a great idea ! What do you think we need to organize ? A subreddit ? What about a bugtracker with one ticket by package ?
 attrlist = map(lambda x: x.attr, mylist) also works. But I just prefer the list comprehension. I guess map appeal more to functional programmer since that's where it come from also it has been in the language longer IIRC.
Most memory models aren't easy to make multicore, unless you go for something fancy like RCU (used by the kernel).
2to3 works fairly well for pure Python modules. I still have to manually fix things, especially when bytes vs. strings are involved, which gets old fast, such that I wouldn't eagerly volunteer for the job. For C-extensions there can be [a lot of changes](http://python3porting.com/cextensions.html). I wanted to try out a non-blocking fork of PyAudio. Using 2to3 on pyaudio.py was simple, but \_portaudiomodule.c took about an hour to figure out and update everything that needed to be modified, which was pretty much everything from the page linked above, scattered throughout the file. The process is extremely tedious.
I'm not sure which question you're asking. Anyhow, completely independent but still able to access the same globals namespace and PyObj c structs, they are still in the same process, the memory is shared, but two different interpreters evaluate the bytecode. There's that issue of one interpreter changing corrupting the other. Off the top of my head we'll need pointer assignment to be atomic and a lock for erasing objects. Though I agree it isn't as simple as "make em run in parallel" because of how dynamic python is.
As I understand it, the GIL is not a performance hack. It was an ease-of-implementation hack, and it just so happens that the only well-known effort to remove it in favor of finer-grained locking resulted in negative performance impact for single-threaded code. There is a [FAQ entry](http://docs.python.org/faq/library#can-t-we-get-rid-of-the-global-interpreter-lock) about the GIL and why it is still there. EDIT: To be clear, this slowdown is consistent with what I remember of "1 big lock vs. N small locks" design tradeoffs, but I'm not sure if the scale of the slowdown seen before is avoidable. From my (outsider, hobbyist) perspective, there is little will to remove it amongst core developers. This seems to be mostly for 2 reasons: it is perceived as not a huge problem for the most common uses of Python (probably true, but possibly a chicken-and-egg problem), and because the expected performance impact to single-threaded code is considered an unacceptable trade-off (largely because they don't think multithreading is massively important for most Python uses). Given this lack of will (and the lack of any full-scale outside attempt to remove it), it doesn't seem likely that CPython will see the GIL removed any time soon. It seems more likely that PyPy will remove the GIL (there's a [blog post](http://morepypy.blogspot.com/2011/06/global-interpreter-lock-or-how-to-kill.html) about it and everything!), and that PyPy will continue its growth into the mainstream. 
&gt; the memory is shared The entire problem with removing the GIL is how slow it is to synchronize reference counters, classes and all that stuff. I mean, the "entire bytecode interpreter" is a bunch of functions in ceval.c with no shared state or anything, less than 5 KLOC in total. And, technically speaking, when you spawn a new thread, it turns into a "completely different" different interpreter, as far as only these functions are considered.
Shared-memory IPC is exactly the same as mutation by concurrent threads; the cost is mostly CPU cache lines bouncing. For illustration, linux threads are processes that just happen to share the same memory map, and SysV IPC is just the sharing of a memory map.
Efficient multi-core mutation and garbage-collection is still an open computer science problem ([here's some interesting work on it](http://flyingfrogblog.blogspot.com/search/label/garbage%20collection)), and that is the kind of thing the PyPy team loves to do. I don't think fine-grained locks is the right way to go about it however, because locking and atomic operations are becoming much more expensive as CPUs scale to more cores.
Seeing as that's the standard Python way to talk to web servers, this is the Python subreddit and the article's about doing web stuff in Python, probably...
Before you go off all rogue-like and port libraries, I'd ask project maintainers what their thoughts are and how (and if!) they want to accept Python 3 patches. I would hate to see people do good and helpful porting work only to have it rejected because it creates more of a maintenance burden than they're currently willing to accept. Some projects are comfortable working with two separate branches for the time being. Some will want a single source that can run on both - how hard this is depends on how many versions you need to support, but you can do it going back to early 2.x if you like a challenge. Some will want source to stay on 2.x but optionally deploy on 3.x at packaging time, meaning they'll need to bulk up 2to3 and write any unwritten or custom fixers necessary. Some projects flat out don't care and don't want to take on anything to do with Python 3, which is unfortunate, but it's useful to know up front. If you're looking for any porting materials, I aggregated several of them for the PSF Sprints project at [http://docs.pythonsprints.com/python3_porting/py-porting.html](http://docs.pythonsprints.com/python3_porting/py-porting.html).
I remember that it clicked for me when somebody made the reference to "bluprints" of "things". Couple of years later I sat down and put that reference on my website: http://www.markus-gattol.name/ws/python.html#metaclass I hope it can help you get the idea because in the end it's quite an easy concept to understand when served the right way :) 
&gt; For C-extensions there can be [1] a lot of changes. My porting experience of a C extension at work was painless, but we don't do much around Unicode other than take in a few string parameters and pass them to APIs we're wrapping. I got away with just updating the module initialization structures which was very easy, and had a working extension with very little effort. Others certainly have a harder road, but it can be as easy as one hour of work to get going.
In general, shared state does not appeal to me, but I'm willing to admit that the memory penalty of running a full-on separate process to achieve parallelism is a legitimate barrier for certain applications. Also, threads not actually being parallel is a *nasty* surprise to most newcomers to the language. BTW, does the STM-via-translation approach that Armin describes in that blogpost equate to finer-grained locking?
TBH, you put it much more tactfully that I have ever done, but it still came of as a bitching session to me. I think it's still a bit of a "wait and see" case for the whole "how to deal with threads" problem. We're not really at the point where we can definitively say how the whole multiple threads game is going to end up, what with GPU's suddenly becoming essentially hundreds of extra threads in their own right, and distributed computing always looming in the background for the huge data set stuff. After reading this thread, and mulling over the issues again I'm of the mindset that Guido is correct in avoiding this issue for the moment. When I've had need for doing some blocking or multiple second long ops in another thread, multiprocessing has met the challenge gallantly. It's pretty easy to use, and totally sidesteps most of the gotchas of writing threaded code, which usually kill people just starting out with threads. If you're really hell-bent on using more processing power than a single core can give you, it's been no secret that Python is the wrong tool for the job, indeed C/C++ is the *only* tool for that kind of heavy lifting. I used to be very anti-GIL, but as I learn more about the problems with having more than one task running at once, I see this is a really complex field, with no catch all solutions, and Python is very much a "one right way" language. Humans aren't ready for a Python with real threading yet, or is it computers aren't ready? I think it's both.
Noted. Thanks! 
Will do! Thank you. 
I've got [a spreadsheet](https://spreadsheets.google.com/spreadsheet/ccc?key=0AqIElKUDQl8tdC1lR29XZFlxZUxOU1VlZ1JRQ3ZRanc&amp;hl=en_GB&amp;authkey=CPLS9KMF) of popular packages and where they are with Python 3 support. It's freely editable, so feel free to update or add information. There are also sites like [this](http://python3wos.appspot.com/) which get the status from the classifiers on PyPI, but the classifiers are sometimes missing, and there's no space for extra information.
 from cpu_count import multiprocessing should be from multiprocessing import cpu_count
A 'lot of changes' doesn't mean they're necessarily hard changes that require much thought. It's just tedious to update a 2,600 line file. The compiler errors were unhelpful regarding structs that have changed such as `ob-&gt;ob_type` and the switch to using `PyVarObject_HEAD_INIT(NULL, 0)`. That's where I wasted a lot of time until I found the page linked above. Creating the module definition and updating PyInit were painless. And updating to use bytes and longs was just a monotonous loop: search, inspect, update, repeat. 
Thanks, I knew it had something to do with list comprehensions but I every where I looked seemed to be using them as filters. I'll definitely be checking the PEP out. 
I've found the best approach to be a single code-base for 2.x and 3.x (YMMV, of course). It's more likely to be acceptable to upstream devs, since it involves less maintenance headaches than separate 2.x and 3.x branches. The use of 2to3 on the sources is very helpful, but I always make the changes by hand (using the 2to3 output as a guide), because 2to3 isn't smart enough to do the changes optimally, plus it can't do e.g. doctests. I've ported pip, virtualenv, virtualenvwrapper, babel, distribute, simplejson, Elixir, polib, WTForms, and Whoosh this way. Some of these ports have been merged back into the original projects by their maintainers (pip, virtualenv, polib, Whoosh), others haven't (for reasons unknown to me: I generally ensure that all tests pass on 2.x and 3.x). The six project is a useful source of techniques, though I haven't added it as an external dependency in any of the ports I've done. It's generally been more work to port the tests for libraries than the libraries themselves :-)
I've skimmed the paper mentioned on that blog post, but I'm not familiar enough with STM to be sure. This is a lazy implementation that locks memory regions at commit time, and does a little bit of mostly lock-free book-keeping (see the memory fence optimisation section at the end) to prioritize the transactions to commit. So the time granularity would be shorter than the GIL (a bunch of bytecodes, not too few to diminish locking costs and not too many to diminish recomputing costs; the latter being introduced by STM). The space granularity would be much thinner, depending on how many atomic items get written to during a transaction. Because of the latter, it seems like this is indeed very fine-grained. Escape analysis so that temporary objects don't end up locked would become helpful.
Or just inventing with Python 3. Write some new shit and target only Python 3, do interesting things with the new features. Even if just for fun and not for any serious work. Many seem to think of Python 3 as something alien, distant and incomplete, but it's still a more complete language than many of the other ones people like to play with for fun and exploration. I'm also consistently pleasantly surprised to find that projects already run on Python 3. Still some big ones missing, and yes it would be great if work was done in that regard.
Sorry, most of the details of STM are over my head. I mostly meant "is the bookkeeping overhead of the two approaches comparable." Obviously if the STM-by-translation approach didn't result in more fine-grained "locking" as compared to the GIL, there would not be an appreciable increase in parallelism.
 @twisted.internet.defer.inlineCallbacks [Documentation](http://twistedmatrix.com/documents/8.1.0/api/twisted.internet.defer.html#inlineCallbacks)
I don't think optimising for contention is the right choice if it punishes programs written to keep their data mostly local with a large locking overhead. But that's the shared state thing you mentioned upthread, and I guess it all depends on the problem domain.
:D
I've refactored the cpu_count_hack() in order to run ok in jython under linux http://codepad.org/Q9RfpEch 
&gt;GIL is still there not because noone is able to get rid of it, but because that would make single-threaded python programs much slower, You do realize that the second part of your sentence (cited above) is just a sugar coat of the first part, right?
any wouldn't give the same result as what the OP is asking for, but all would.
I would be game to write something basic pro-bono for you. All I require is proof that you are in fact a educator and the application is for professional use. Caveats would be a bound license that says you cannot let anyone else besides your immediate peers and yourself use it. As a side note, I've done a lot of work for my regions local public school system and my current client is a University.
Jovial guru. Classic.
Since what you're dealing with sounds like a constraint satisfaction problem, Google open-sourced a library last year that might do most of the work for you: http://code.google.com/p/or-tools/ As far as parsing the input, that largely depends on the format it's in. In some instances, Python might already have a library for it. In others, you'll have to roll your own.
TLDR Classes are factories (constructors) for objects (instances), metaclasses are factories for classes. So yes: Python has FactoryFactories as well. :)
Some notes: * Python 3 made metaclasses even more powerful, and got rid of the ``__metaclass__`` attribute. [PEP 3115](http://www.python.org/dev/peps/pep-3115/) * While Django probably needs metaclasses for the models for some reasons, I don't think the example given is one of those reasons. It is trivial to change a class attribute for instances. Great post though and I don't mean to say otherwise.
Nah, bitchers will bitch and about anything.
After having converted a bunch of my own code (or rather, added python3 compatibility to a shared codebase via 2to3), I finally came to the realization that for some projects, 2to3 alone was not enough. I ended up adding py2 / py3 only sections via conditional comments that were handled by a preprocessor monkeypatched into lib2to3. This let me solve a few edge cases where the py3 code just *had* to be completely different, while keeping 99% of the codebase shared. Can't claim credit for the idea, first saw it in the SQLAlchemy source code. While dangerously powerful, something like that should really have been included in the 2to3 source to begin with. I'd love to contribute my code, but it doesn't look like there *is* a central place for 2to3 development, it's in the changeless living death of a stdlib library :(
Thank you for clarifying for newbies... those downvoting should remember that everyone here doesn't memorize acronyms. 
2to3 does an excellent job fixing the various "syntax is slightly different" or "imports were renamed" types of changes. This leaves basically two things - C extensions and unicode vs bytes. As mentioned by other folks, the C extension changes are simple, and mostly fall under the same "syntax is slightly different" / "imports were renamed" categories that 2to3 already deals with... if it could handle C/C++ :) The main thing that requires truly rethinking your application is the unicode / bytes issue. It's not that Python3 deals with things in a way that's incompatible with Python2. It's that Python2 lets you deal with unicode / bytes separation in muddled way, in fact it makes it easier to *not* deal with them correctly. The biggest hurdle to py3 conversion is reworking the code so that your code has proper separation of unicode &amp; bytes *even under python2*. Once that's done (or if the app already does it right), conversion is really not that difficult. But making that change for many libs/apps requires some high level semantic changes that requires a programmer sit and ponder a while. --- edit: To simply lay out *what* changed re: unicode - under python2: native strings (eg function names, etc) are encoded bytes that are simulatenously treated like ascii characters, while unicode is a separate type; under python3 native strings are unicode, and bytes have no implicit encoding. Due to this muddling, under python2, encoding bytes -&gt; bytes can happen (and is forced to happen) even if the operation makes no sense; and implicit unicode/bytes encoding happens for comparisions between the string types... In short, all kinds of mess that python 3 removed.
Ok just for fun, can you explain to me what the difference is, or why the program cares if I use unicode or bytes?
There's also "3to2" for [Python 2](http://pypi.python.org/pypi/3to2/) and [Python 3](http://pypi.python.org/pypi/3to2_py3k/) which attempts to translate Python 3 code to Python 2, and which might be an alternative to "2to3" for new projects that wants to run on both major versions of Python. If you're already limited to dependencies that run on Python 3, it *is* a nicer language to code for, and conversion in this direction might be more reliable with regard to bytes vs unicode and such. Alas, I haven't used 3to2 myself yet, it could suck.
Most things that are easy for threads but harder for processes are dangerous in practice, because they are prone to race conditions (or possibly deadlocks). And, as twillis973 remarked, you do need a shared-nothing way of doing things once you scale to more than one machine. The only excuse you could have (for deliberately building threading-specific stuff) is that multiprocessing doesn't run properly on Windows.
1. Read [PEP8](http://www.python.org/dev/peps/pep-0008/) 2. Errors should never pass silently.
That'll show me for not refreshing... you got that message in *right* as I was adding a directly related edit to the bottom my comment :) I'll give it a shot, though it's complicated enough I might not explain it right. --- Basically it comes down to symbol vs representation. When you type an "A", an "A" appears on the screen. You think of that as a character, not as the number assigned to it under a particular encoding in a file or representation in memory. Different encodings (eg: ascii, latin-1, utf-8, etc) may have different ways to represent the same character, be unable to represent certain characters, and/or use a byte sequence for one character that another encoding uses for another character. Why does that matter? Imagine you're trying to compare two strings to see if they contain the same characters. For example, comparing two user names, or a hasattr() call to see if an object contains a method with a particular name. If the encodings aren't the same, they might not match at a byte level even if they contained the same set of characters, or match even if the characters were different. How does Python2 handle this? All native python strings (the *str* type) are encoded bytes. But native python strings are used for function names, variable names, all kinds of things. And those have to be compared all the time! The way Python2 solved this was treat all byte strings as implicitly using ascii encoding. This limited function names, etc to the roman alphabet, but worse was that by treating encoding bytes as characters, they muddied the representation with the underlying symbol; and comparing to any other encoding did *not* behave as expected. This is where the *unicode* type in python 2 comes in. Unlike the *bytes* / *str* type, it should be treated as a series of characters, with no underlying encoding. (There is one internally of course, but the point of the abstraction is to hide it so that your code behaves exactly the same no matter *which* representation is used by the vm). This provides a neutral type for comparing characters, performing text operations; and ideally *bytes* would only be used when you've encoded something to write out to a file. That last "ideally" couldn't quite happen under python2, because all the native strings (function names, etc) are still using ascii-bytes. So they had to add in hacks such as implicitly encoding unicode -&gt; utf-8 encoding when comparing it against bytes. Of course this wouldn't always work right, and causes lots of surprise encoding problems when you try to deal with characters like "♫". Python 3 fixed this by making native *str* type be the same as python 2's *unicode* type, and *bytes* is now the odd man out. Suddenly, python function names can contain non-ascii characters. They can be reliably compared no matter what encoding the library used for it's source files. It's harder for programmers to forget to encoding characters before saving (writing out raw bytes that you thought were just ascii), and harder to double-encode them (utf-8 bytes re-encoded as utf-8 is *not even wrong*, but python2 will let you do it). This helps with sanitization of user input, internationalization, and many other things. All of which are possible under python2, just much much harder, due to native strings not being unicode. It's this semantic shift which requires rethinking how your program works, not just a quick 2to3 refactor.
What do you mean by "Errors should never pass silently". All the errors are handled, or am I completely misunderstanding it...
Awesome I am reading your reply and shortly after asking for the explanation happened across [Dive into Python 3](http://diveintopython3.org/strings.html) which explains some of the formatting and why it is important.
Don't forget Unicode [normalization](http://docs.python.org/library/unicodedata.html#unicodedata.normalize): &gt;&gt;&gt; import unicodedata &gt;&gt;&gt; 'I' == '\u2160' False &gt;&gt;&gt; 'I' == unicodedata.normalize('NFKD', '\u2160') True Edit: Also, as a bit of trivia, Python 3 won't automatically decode International Domain Names, which are encoded with IDNA (i.e. nameprep-&gt;punycode), so a received hostname (e.g. from a reverse DNS lookup) should be decoded with 'idna' just in case. For example: &gt;&gt;&gt; host = b'xn--hxaazibkcfgaxd3f.com' &gt;&gt;&gt; host.decode('idna') 'ελληνικάνησιά.com' 
This is by far the most readable and most conventional math/logic is written this way. 
to me (a complete n00b) this is the best part of this discussion. Thank you
Link is a 404
Sorry, in a way you do not let them pass silently as you print them to the console. What I really wanted to say is "Errors should never pass". You catch the exceptions and return a value that seems correct but most likely isn't. Just let the exceptions propagate to the caller, or if you want to catch them just raise new ones (PyksInternalError or something). 
Ah, I see now, I'll get that sorted, thanks!
Sorry about that, it's fixed now...
For background on Unicode, [Joel Spolsky's article](http://www.joelonsoftware.com/articles/Unicode.html) is recommended reading.
Seems the typical solution is to [write custom fixers](http://python3porting.com/fixers.html) and configure distribute to use them when installing your project. For example [Jinja2](https://github.com/mitsuhiko/jinja2/tree/master/custom_fixers). Of course, would be nice if lib2to3 had [better documentation](http://docs.python.org/library/2to3.html#module-lib2to3).
You could always [figure it out the same way that we non-newbies figure out stuff](http://www.google.com/search?q=python+wsgi).
&gt; PyCL is yet another OpenCL wrapper for Python Okay, so what distinguishes it from [pyopencl](http://mathema.tician.de/software/pyopencl)?
Ok, so it's "pure python." Advertise accordingly. :)
Maybe link to the project page instead of an individual release: http://pypi.python.org/pypi/stattr
Not a bad plan. Though considering the amount of fail that is encapsulated in this package, I think this is a pretty good start.
Yes, but it was nice of him to save the trouble. 
Certainly. My comment was a knee-jerk to the memorization comment. The single biggest problems of my students are that they think stuff should be memorized, instead of learning how to help themselves to the endless information that is so easy to find and access nowadays.
It's an honest appraisal. If you come from a Java or C# background where threading has become second nature, the first time you run into the GIL is like a slap in the face. It sounds like something out of 10 year old or toy language, not a modern production-worthy system. It's a testament to the python language (and its supporting libraries) that it's taken seriously in spite of it.
i'm not quite sure what's going on since WSGI was intended for "framework designers and developers" anyway, as written by the proposer. also, the only person who complained about using it wrong was the author himself. so seeing people who think they know the intent of the proposal better than the person who wrote it in the first place is kinda troubling. maybe they've found useful ways, but they shouldn't go as far as to say there's nothing wrong with it. i for myself will wait for what Eby comes out with...
Sorry everyone, the results are the very bottom of the benchmark, and I couldn't figure out how to change order of files within a gist. The biggest surprise to me was definitely how PyPy was almost 3x slower encoding and 9x slower decoding than Python 2.7's vanilla json module. This just seems wrong, considering how much faster PyPy is for most computational stuff. If anyone notices an error, please post or PM or something, that could definitely explain PyPy's performance. Also, with CPython, the json module is faster at decoding than encoding. With PyPy, encoding with the json module is faster than decoding. simplejson for CPython is with the C extensions enabled. After posting this, I installed simplejson for PyPy (without C extensions) and the results were essentially the same as the builtin json module for PyPy.
As already said, your code does not follow [PEP8](http://www.python.org/dev/peps/pep-0008/). Simply put, read it, re-read it, print it, re-re-read it, get it tattoed on your chest, re-re-re-read it (from your chest that's harder). You should get less code in your 'try: except:' statement. There is no reason for time.localtime() to fail for example. Actually, reading your code again, your 'try: except:' statements are not useful in this_ks() and from_ks() as the functions you use will never fail with exception. This looks like beginner compulsory fear of doing wrong but fear not my friend ::) For the exception in to_regex(), you should do either of the following : * raise the exceptions, a library should not usually silently catch exceptions. Exception handling should be left to the scripts using your library. * create a custom exception (should inherit from Exception, see this [part of the doc](http://docs.python.org/tutorial/errors.html#user-defined-exceptions)) and raise this exception. 
fast good, alpha bad
This is a great read, thank you very much for this!
I've never used PyPy so I'm probably gonna speak nonsense but could PyPy be quite bad at string handling?
&gt; For the exception in to_regex(), you should do either of the following : My first advice would be change the method name and signature, for me the "regex" word means something that involves the *[re](http://docs.python.org/library/re.html)* module. I would use the word "*[format](http://docs.python.org/library/time.html#time.strftime)*" to be coherent with the python documentation.
Yes, PyPy is slow (I mean CPython has a hack to make it fast) on a = "asd"; a += "dsa". So this might be the case.
Yea, standardized tests for 12 years kind of pushes memorization over understanding. 
Crap article with no substance, and an arbitrary table for comparison that gives no justification at all to any of the assertions in the article. In other words, a fine journalistic piece from Infoworld.
Ok, so this_ks() and from_ks() should never actually fail so I'll get rid of that handler. For the custom exception, I have made one but I'm having a hard time calling it, the way I'm doing it at the minute is: try: ... except Exception as ex: raise PyksInternalError(ex) Is that the right way of doing it? Or is there a more efficient way?
Ok, I shall do that, something like format_ks()?
Might I suggest that -O/optimization is a terrible choice of switch name and description for what is basically not really an optimization?
Web2Py has many more capabilities than Django or Pyramid. No other framework offers you the ease of coding yourself into a corner. The writer of this article is an idiot and should not be offering his uneducated opinion in a well known forum.
yes. but tell it Guido in the face ;-)
(cStringIO is the solution for string operations, I guess)
Does _Openserver_ provide a COM interface ? Is there any sample code ?
Sorry, maybe I don't get something. No, I can't understand what you are talking about. Could you try again explain that in another words please?
There is a COM Browser but I don't see Openserver listed there at all. I'm right at the beginning of writing the script so I don't have anything but I do have a sample Perl script that does something similar: use Win32::OLE; use Win32::OLE::Const ; $Win32::OLE::Warn = 3; my $File = Win32::OLE-&gt;new('PX32.OpenServer.1') ; This does in Perl, what I am trying to do in Python with COM. 