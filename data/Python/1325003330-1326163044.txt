 print d[0] if d[0] is not None else "failed" or for an assignment: c = d[0] if d[0] is not None else "failed" It's the python version of perl's "expression ? true : false" 
There is a tool called js2coffee (google for it) that converts JavaScript to CoffeeScript. It's a pretty awesome tool, but it doesn't really address debugging, since it won't round trip to exactly the CoffeeScript that you started with. Folks are working on line number mappings in CoffeeScript. I predict a lot of the debugging issues will be addressed in a few months, particularly on the server side (node.js and friends). In the browser, the level of direct debugging folks get in JavaScript will require additional support from the browsers themselves. Browser support is also in progress, but it's probably gonna take longer.
You don't need a infinite number of decimal digits to represent 1.2 in base 10, so there's nothing wrong with the word 'exactly' here. That's the whole crux of the problem: there are numbers that cannot be exactly represented with a finite number of digits in either base 10 or base 2, but there are also numbers that *can* be represented exactly in base 10 but *not* in base 2, and those are the ones that cause this confusion. 
You're probably not gonna like this answer, but one of the reasons that CoffeeScript is not more of a clone of Python is that it takes a fair amount of influence from Ruby and other languages, as well as adding a few innovations of its own. I happen to like both Python and Ruby, so CoffeeScript is a nice blend for me, but if you have a strong preference for Python over Ruby, you might not like inverted if statements, for example. The most important nod to Ruby is that CoffeeScript is very expression-oriented, which means functions have implicitly returned values. This is comfortable for Ruby folks, but it can be annoying to Python folks used to more explicit "return" statements.
hm, maybe you're right and probably indeed I got too distracted by the name, which is pretty horrible
You don't need an infinite number of digits to represent 1 in base Pi either but what does that have to do with anything? The thing we are talking about here is a computer. Computers do things with bits. The standard way to represent a part of a thing with bits is to break it into subsequent halves (0.5,0.25,0.125...). As a result, 0.09999999999999987 is the correct result as seen in base 10. There is no concept of exactness available here...
Well, clearly we should be making websites that verify that the following is all-green: $ pip install some_package $ trial some_package PEP 3333's opinion on Unicode is bonghits, which prevents most of the WSGI ecosystem from migrating. The big packages like Twisted are *already* migrating, but it takes a while. This site feels like a great way to say "Hey, things suck," while not doing anything to actually help.
There most definitely is such a thing as exactness. You're mistaking the representations for the actual number. Take the abstract number which is the result of 1 divided by 10. I can represent that number exactly in base 10 by writing 0.1. I cannot represent that number exactly in base 2 without writing digits forever. But other numbers, such as the one which is the result of 1 divided by 2, can be exactly represented in both base 10 and base 2. It's this difference that is at the heart of the issue: for numbers that have an exact representation in base 10, some of them also have an exact representation in base 2 and some do not. That means that some operations in base 2 are exact, like 1/2 and some are not, like 1/10. Exactness is very much a thing. 
~~dpaste.com~~ dpaste.org And the latest repo for it is: https://github.com/bartTC/dpaste.de
Already using it and watching the GitHub repo for new features. VERY well done. I've been looking for something like this for a long time.
If you are a programmer it's a place to store your projects. Because it's an online service you can access your code anywhere. One of the other benefits is that other people can potentially also access your code easily. If your code is edited by them, they'll have their own copy and it doesn't necessarily effect your project, however because their changes to your code are made available in the same place it's easy to bring them into your project if you like what they've done. They also offer a few things to help your project outside of simply managing the code. Things like a Wiki for documentation. Issue/bug tracking for when errors are found (which can act as a todo list of sorts, and allows for transparency when working on projects).
`git` is something programmers use (one of several competing options) to make neatly organized backups of their projects that include a full "revision history". You can use the program to retrieve the project as it was at any previous point, which is useful for undoing failed experiments, determining when a bug started appearing, etc. On multiple-person projects, it allows for complete accountability of who did what. To save spaces, the system mostly stores differences between successive "revisions", instead of a fresh copy. `GitHub` is a server that lets you create an account and make several "repositories" (project archives) on the server, using a secure connection to upload your revisions. It does a bunch of web server stuff to provide a friendly HTML interface to your repository, so that others in the community can see your project as it develops (this also serves as a way to distribute the project). The README file for your project can use something like Reddit's markdown, and the site will display it with nice formatting. Also, if someone finds a bug in your code and knows how to fix it, they can make their own changes and send you a "pull request", providing you with the proposed change so you can choose whether to include it in the project. Oh, and you also get a simple bug tracking system, the ability to set up a documentation wiki, a pastebin (gist.github.com) that actually uses a git repository to back up each paste...
&gt;PEP 3333's opinion on Unicode is bonghits, which prevents most of the WSGI ecosystem from migrating. I would be interested to hear how you think it should work instead.
Thanks! Any suggestions (or you can fork and create a pull request too) ?
Shame is not a good motivator or community force.
I have a few ideas, but I'm not familiar with Django. I'm very fluent in Python, though, so I'm sure I can work around it. I'll create a pull request later! Edit: [My fork.](https://github.com/flags/quark-paste)
Ok, sorry. I guess what I am trying to say here is that exactness is irrelevant to the question. Let's say that we have two people working in a factory. Their job is to cut metal rod to a particular length. One day someone in the stockroom comes forward with a question. The stock person has noticed that it is possible to tell who cut the rods by comparing lengths. The rods cut by one metalworker are always a bit shorter than the rods cut by the other. The question is why this is the case. The specification is "make 3 rods per metre of stock". The first metalworker has a metre stick marked in tenths. So this metalworker uses the 3rd mark from the end (3/10). The second metal worker has a metre stick marked in thirds. This metalworker uses the first mark from the end (1/3). You could try to answer the question by correctly saying that the second metalworker was using a metre stick that could produce a more *exact* result. That would not answer the question as asked and it would be a distracting and irrelevant point. A much more correct answer would be to say that the difference is because they used a different reference. A more detailed answer would mention that a different process was used to come up with this reference but the actual details are arbitrary here. It was just luck as to who grabbed what stick. A computer is an arbitrary mechanical process. The best answer to a question like "[WHY?] &gt;&gt; 1.2 - 1.1 = 0.09999999999999987" is: that is just the way it is. If the person that asked the question wants to know *why* then they need to go off and understand how fractional quantities are represented in floating point. There is no real shortcut available here. Ideas about exactness provide no extra insight.
I was talking to prezjordan about his project melopy and integrating MIDI and/or sampled sounds. It would be awesome to generate these things on the server algorithmically, and a library that could spit out midi AND real sampled audio would be so awesome. First, we'd need some sort of library to generate a more specific sequence of notes like a bass line based on the chords, a swing drum track, and a piano accompaniment. Then that could be fed into something like melopy (which has only generated wav tones for now) and cached server side. And that would be available for download in the respectable formats. Lots of work there... :-)
There are many ways to accomplish your goals, and you'll hopefully hear a lot of responses. I'm no expert, but I would look at the following: Use a lightweight Python web framework, or micro-framework, like [Bottle](http://bottlepy.org/docs/dev/), or [Flask](http://flask.pocoo.org/). With these, you'll be able to neatly handle all of the application logic (both web stuff and analysis) with Python. These frameworks rely on [WSGI](http://www.wsgi.org/en/latest/index.html), which acts as a kind of glue between Python and an http server, like Apache. Unfortunately, most vanilla shared hosting providers don't automatically support WSGI, but I'd suggest you do a thorough search on your provider's wiki, forum, or support pages to see if anyone has it working. Or, you can use a more Python-centric web host, like [WebFaction](http://www.webfaction.com/), [ep.io](https://www.ep.io/), or [Google App Engine](http://code.google.com/appengine/). I can't vouch for these. If you want full control, you can try [Linode](http://www.linode.com/), which is an EXCELLENT and reasonably priced VPS host that I can vouch for. I hope that helps. Again, I'm no expert, so folks may feel free to correct me or add their opinions.
Suppose I write a hundred-page book, and it's stored as some kind of document format. At some time later I decide that I want to reword a particular paragraph on some page. But I'm not entirely sure I want to keep the new wording, and I'd like to have a copy of what I wrote the first time. Maybe I save the document as a new file (with "2" stuck on the end) or maybe I make a copy of the original paragraph and store it as a snippet in its own file and I save the modified copy under the original name. Imagine I repeat this hundreds or thousands of times, perhaps in collaboration with other editors and writers. Eventually I wind up with a huge mess on my hands because I've got hundreds of different versions of this document floating around and being able to find or refer to a particular version is getting to be a real pain. Maybe I resort to emailing attachments back and forth to my editor and let my email program take care of the archiving. In any case, this is a form of version control, although it's the most shitty form imaginable. Programmers do something similar to this with code instead of prose, but we have invented tools that take care of all that bookkeeping. The tools ensure that a copy of the document at every point in its lifetime is available, that it's easy to see what changed between any two versions of the document, and that it's easy to work with others who might be suggesting changes. And they do it in a very compact format that means the database size doesn't blow up if I make thousands of copies of this document each with a few words changed. That is the essence of modern version control. There are different systems in use and they all have differing philosophies and mechanics, but the overall idea is roughly similar. The most popular ones today are subversion, git, and mercurial. Github is a hosting site that supports use of git and adds a bunch of extra features like commenting, bug tracking, web pages, etc.
&gt;I created an example for you which show the revisions idea and how you can make changes as you go. 404. You may not want to set Debug=True...
Some excellent explanations here, so I've got nothing to add. Instead, I pose a question: why would a non-programmer care about GitHub? I don't mean to be rude; I'm honestly curious. 
You can store everything that need to be versioned. There are lots of books on there too.
Looks great man! Some suggestions / feature requests: * When entering chords on a song I would like to see the interval names AND chords in the selected keys. * There is not an option for 6th chords (the song I added has them several times). I cloned and added them to your project but I am a github noob so maybe the change didn't get back to you. The following was added to EXTENSIONS_DEEP: ('0,4,7,9', '6th', '6'), * I would like to be able to add section markers - in my Real Book most songs have A, B, C sections and that would really help here. I am thinking a button next to 'add another change' for 'add section marker' would do the trick if you can still drag it around like the chords. Model/db change required. * When I look at the song I added in the directory I cannot 'Edit' - you have to go to 'Your Tunes' before this option is available. If you make some issues on github I'll do what I can to knock some out this week. Thanks!
small nit: git is not based on deltas. Each working copy is a full repo with complete history. 
Excellent, thanks!
Cool. Thanks.
This is exactly his point. We have to use a somewhat ugly workaround for something that could be elegant.
Well, I've been learning python (Read Mark Lutz "Learning Python") but don't feel comfy calling myself a programmer. So, I didn't want to get lost in verb-age a typical programmer would know.
Sure! But can you imagine a non-programmer using git for such a thing? Also, I'll bet you nearly all of the books on there are programming books ;)
Gotcha! Well, then you came to the right place :) So, it's not really *GitHub* you should know about, so much as *git* itself. If you're looking for a way to version your code, I'd suggest mercurial (also called "hg"), as it is *much* simpler to learn and use than git. Subversion (svn) is even easier, but it seems to be on its way out--there's probably no good reason to learn it at this point. Mercurial's equivalent to GitHub is [Bitbucket](https://bitbucket.org/).
I should point out that GitHub does an excellent job of helping new users get up to speed with Git. I learned Git by starting with GitHub with no Git background.
How intense is the server side stuff? If the data is small and quick to process you shouldn't need to do anything fancier than handle a PUT with the file in it. If it'll take a moment to process then you might consider a producer-consumer pattern using threads or multiprocessing. I'm not sure how you'd handle a large file upload though. Build your app how you want it, then look for appropriate hosting. You can always come back and make changes to fit your hosting service if needed.
Nice thing about GitHub is that you can view the code on a project without having to download the source. You still need to download it (or clone it) if you want to DO anything with the code, but it's nice to be able to take a peek inside so easily.
I wonder, CAN Git be used for intelligent versioning of, say, an odf or docx file? Or can managing conflicts get messy in a hurry on those?
It's a public testing server. I have no problem with it being set to True.
I don't think SVN is going anywhere for good, but it seems a little stupid to use for personal projects.
I don't know how to interface python and a web server, but I'm using vpscheap.net, to run a python script that needs to be connected to the web 24/7. They're like $3 a month, and you get a pretty decent virtual box (where you'd proceed to install the Bottle-Apache stack).
GitHub is Facebook for programmers, but instead of uploading pictures and making status updates, programmers work on software projects and collaborate to improve things.
It's *possible*, as neither is a binary format... but it's probably a horrible pain in the ass. I imagine ysangkok was referring to books written in a more human-readable markup language, which us programmers tend to prefer anyway :)
No, you're right--I'm sure it will be around for a long time to come. Hell, I still use it for my business, as I'm the only programmer--no need for distributed versioning. But learning it now (as opposed to hg) would be a waste, I think.
&gt;collaborate to improve things So, nothing at all like Facebook ;) 
Great ideas! Merged and added.
Get a [Linode](http://www.linode.com/) instance. Install Ubuntu Server on it. Install scipy, flask (web framework), gunicorn (wsgi http server), and nginx (http server). Read the [Flask Quickstart Guide](http://flask.pocoo.org/docs/quickstart/) and create a single page website that has a form that let's you upload a file. Pay close attention to the [Uploading Files](http://flask.pocoo.org/docs/patterns/fileuploads/) section. Modify the code in that section to process the file with scipy and dump it after receiving it. Finally, follow this [guide](http://wirtel.be/2011/02/24/nginx_gunicorn_flask/) on how to deploy your website with gunicorn and nginx. I'm no expert here either but this is the simplest and easiest way I came up with. I actually created a website that is similar to what you described and it took me less than a few days to get it working and deployed.
Actually, you can make small (single text file) changes without cloning it.
In addition to the reasonable advice in the other comments, I would like to add that if you use mod_wsgi or its ancestor mod_python, there is one issue that you need to be aware of. numpy and parts of scipy do not do well when there are multiple Python interpreters embedded in the same process. Common configurations of mod_wsgi and mod_python do embed multiple interpreters to handle multiple requests simultaneously. You will have to consult their documentation to figure out how to turn this off. Most other deployment strategies do not have this problem.
As a non-programmer your best use of github is a dating site. It's a social hub as well as code repository. Essentially it is a system for managing shared source code files. It allows programmers to collaborate on a project. Very simply, using layman terms (which will enrage coders): Coders download code from git, modify it, and put it back where it is recorded as a "branch". Other coders can see the branches and work on them, or their own branch, all in real time. In well organised projects, eventually someone in charge can declare which branches form an "official" release. Other branches still exist as "forks" of the project and might continue to be developed. Before git, coders generally used single centralised repositories which made it quite hard to create branches or forks.
Each working copy is, but is each stored version on the server?
Well technically...if your eyes are looking at the code, then you've almost certainly downloaded it. Probably your browser has downloaded it along with tons of HTML markup thrown in there to make it look pretty, and your browser will decide when to throw that file away, depending on your caching settings.
Why is that? I've been using it for a few years for all of my personal code. I really like it, but I like to keep an open mind.
Because it requires a central repository. I code on about four different machines, one of which doesn't even have internet access. If other people are anything like me, git is a huge upgrade from svn for personal projects.
Really!? Could you please link me up to some information on that? I'd much appreciate it.
Darn. I was hoping to get some more use out of the tool outside of my code.
go to any source file on the github site and there will be a button that says fork and edit this file. that allows you to do online editing of one file but you need an account for this to work. 
from how I understand things until it gets packed git operates on copies of files that are hardlinked to the main directory. Each file is compressed individually. Each filename in the .git blobs folder is some sha1 hash of the contents of the file and a commit is a list of the sha1 hashes. you can explore the blobs by using git show and the hash of the item. when its packed the compression algo will only store the changes to a file. [Here is a pretty interesting overview of the storage methods used by git](http://book.git-scm.com/7_how_git_stores_objects.html)
The expense of having to setup the repo for a project made it so that I would only use svn on projects that I know will be around for a while. With git aliasing alias gitinitaddcommit="git init; git add -a; git commit -m 'initial commit'" makes putting something under revision control so freaking easy and quick that I tend to do it even for small projects. Couple that with the fact that there is only one .git folder and it's close to the project instead of in some central location makes it a huge win for my workflow. (those hidden svn directories sprinkled throughout a working copy were crap from a design standpoint.)
Same. It's a bit confusing when first starting out, but you'll generally only need to be using a handful of core features/commands, and once you learn them everything is easy.
I used to use it for versioning my papers for school. no real reason not to use it if you are writing good commit messages though you do lose out on the diffing. All the more reason to use latex for papers though so you can get the diff. 
Exactly as tinyOnion said. Here's a Github blog about it: [Edit like an Ace](https://github.com/blog/905-edit-like-an-ace).
As a long time Subversion user myself, what I like about Github is that it effectively turns git into a centralized repository, while still allowing all its distributed features. I'll typically start a repo on Github, and treat that as the "master" repo, and then create a fork for development work, merging complete changes back into the master repo.
Github status: compiling
I'm beginning my student career at UAB this semester in Computer Science, and I'm really enjoying learning Python on my own so far. The conference sounds like a great idea for me to get more involved, so I filled everything out. We'll see what comes of it!
[Response from a Facebook employee](http://news.ycombinator.com/item?id=3397115)
My own personal opinion: subversion - hard for coders (branch = hard), great for the release engineer. github - great for coders (hey guys, pull from me!), nightmare for the release engineer. This assumes you have a release engineer :-)
I've found this cheat sheet very useful for my coworkers: http://zrusin.blogspot.com/2007/09/git-cheat-sheet.html
As a coder I never really got what the big deal about branching is, anyway. Subversion works great for single-person projects, but then, even garbage like VSS would probably do the trick 99% of the time.
&gt;why would a non-programmer care about GitHub? Because many neat programs and such are distributed through GitHub.
Open source. The OSS model grows by user contribs and has little central control. Therefore branches and forks are essential for growth. Sure most of the user mods will be junk, but big things grow from smaller ones. You never know when someone will take a seemingly innoculous poorly written 1 line code patch submitted by a total amateur, to "disable submenu X", think "great idea" and 3 months later, him and his friends have the core of a working a UI configurator. 3 more months later, and there are 3 UI configutators of them all competing, which results someone else taking the best parts of all 3 and making a better one. etc etc etc. I suppose if you were really radical you might try developing in house using this methodology but I know nothing about that. I write non commercial OSS stuff (i.e. Linux Hacker)
From what I've seen, it makes user contributions much easier in open source projects. With subversion, if someone wants to contribute, they have to checkout the project, make their change, create a patch file, and submit that to the project's ticket management system (if they have one) or email to one of the core developers. At this point, the patch is likely ignored, lost, or obsolete by the time the core developers gets around to looking at it. Look at the bugzilla/traks for Firefox/Django/etc. There are dozens of multi-year old tickets with a final comment of something like "thanks for submitting a patch, but could you please update it? it doesn't work with the current trunk. oh, and sorry it took me 9 months to get around to looking at it..." With a distributing VCS like Github, a contributor can fork the entire repo, make their change, commit, and then "push" it up to the original repo. The original developers will be notified of a contribution (and will have to approve it). Even if the original repo has had change made, the fact that the versioning info is all still intact means git will likely be able to auto-merge the changes. Or, if the original repo rejects the change, well, then you already have a perfectly good forked repository for tracking your changes. 
&gt; Edit 2: Ha! Facebook did announce that they were discontinuing it. But (to quote Hitchhiker's Guide) it was "on display in the bottom of a locked filing cabinet s tuck in a disused lavatory with a sign on the door saying 'Beware of the Leopard'." Check it out. If a geek (and python coders are almost probably geeks) sees a sign reading "Beware of the Leopard" you can bet they'll open it to see if there really is a leopard, and becuase let's face it, big cats are just cool.
So in the end, the contributor produces a pull request... how is that any better than a patch?
It's tracked in the repo instead of in your email or ticketing system.
What?
Sweet, thanks!
Here: * &lt;&lt;= causes the LHS value to be left-bit-shifted by the number of bits on the RHS. IOW for a = 1, a &lt;&lt;= 3 produces a = 8. * `&gt;&gt;`= same as above, but right shift. * &amp;= is an assigned "and". It assigns a bitwise "and" operation. The mask is the RHS value. * `^`= performs an exclusive or. The result is assigned to the LHS. * |= performs a bitwise "or" of the LHS and RHS. Why not test these using an interactive Python session? It's harmless and educational. 
Perhaps I need to read up on bitwise functions, because I'm still not really understanding what most of these do. I understand &lt;&lt;= and &gt;&gt;=, but that's it.
Have you looked into [Sage](http://www.sagemath.org/) or [IPython](http://ipython.org/) ?
I **highly appreciate** the compliment, thank you. Off on a small rant here: I find myself giving plain English explanations to clients (and similar to your situation, my girlfriend, so she understands my job/day better) often. I also practice breaking down programming techniques, technologies and project architectures in the same way; because if I can't explain it, I probably don't understand it. Once you can teach the ideas, it's much easier to document for colleagues, impress management, or even sell to clients. Again, thanks! your recognition made my night.
Bitwise of course means you're dealing with binary: 111100 &amp; 101010 = 101000 (both in nth position must be 1) 111100 ^ 101010 = 010110 (only one of the two can be 1) 111100 | 101010 = 111110 (either of the nth can be 1) It's essentially logical operations, just with 0 and 1 instead of True and False. http://www.ucblueash.edu/koehler/comath/21.html
This actually makes a lot more sense than I was expecting. Have an upvote.
A nice thing gist does is to put the most common languages at the top of the language drop-down, and then the rare ones after. Very few people are going to be posting ANTLR files, for example. Also, the sorting seems weird? Gnuplot is between Perl and Python?
Eh. Stupid Reddit markup ate the important part of my comment. Your link is returning a 404.
Yeah, it will. I apologize. I am randomly dropping the database as I add features to test. The way heroku deploys ends up removing entire repositories, so I lose the paste repository, but not the database :| So, I have to resync everything. It's for debugging after all!
Ah, very true. The sorting was on the extension! I totally spaced on that. I'll give it a fix. Thanks. EDIT: Okay. You can now [create a preferred lexer list](https://github.com/justinvh/quark-paste/blob/master/saic/paste/forms.py) for your deployed application. Sorting has been fixed.
obviously a monty python reference 
derp derp. Thanks.
What with English being ambiguous, I took that to mean "the module is five lines long". Got me interested.
The pattern is the same. `a += b` &lt;-&gt; `a = a + b`, etc. Similarly, `a &lt;&lt;= b` &lt;-&gt; `a = a &lt;&lt; b`, etc.
I dunno, people always have virii to write.
\_\_sluts\_\_
Git was really aimed at group projects, not solo projects. If you're working on your own and familar with what you know, why change?
Thanks for writing it. :)
They lack resources?!
My friend worked at Facebook for several years as a Python advocate, trying to introduce it as a good language for background and maintenance work. He fought up-hill for a long time and ended up losing; engineers there just didn't care for Python. He has since moved on to do other cool things in Python-land. Extrapolating from that, it's not to hard to see why they didn't have people interested in maintaining their own SDK. There just aren't a lot of Python coders there. It's probably best for them to shut it down and encourage people to find 3rd party implementations.
wtforms constructor accepts flattened `formdata`, expecting things like address-0-street, address-0-city, address-1-street, and so on for lists, subforms. You can just flatten your json to that data structure (it's less than 10 lines of code) and pass it to constructor.
page 37 typo if '__name__' == '__main__':
I hate it when Python books/tuts ignore PEP 8
I almost chose that, but decided it was indefensible in certain subs.
I would buy a dead tree version of this if it were around 800 pages and was better edited.
As much as some people will be irritated by its removal, you don't really need the Python SDK, because with maybe 10-20 lines of code you can quite easily access the full Graph API. (I've done it in PHP and it's much the same in Python)
The repository is [up again](https://github.com/facebook/python-sdk).
Yeah, don't do that. Use 2to3 instead -it'll do all the legwork for you.
Never rely on facebook for anything. never, ever!
[Alternatively...](http://www.planetclaire.org/quotes/hitchhikers/#Beware_the_Leopard)
Okay, I'm going to sound really stupid but here goes... I'm writing a game in python, and I was lead to believe that the threading module was basically a mistake because in all but a few edge cases, you couldn't really do concurrent programming with threads and python because of the GIL. I was further lead to believe that the only way to really achieve concurrency was with the multiprocessing module, but that it had some limitations (non-shared memory being one of them). What gives? Is threading actually useful? If so, when? How do I know which to use?
It amazes me on why people still develop for facebook with all of the horror stories of the complete lack of documentation, deleting their wiki, no versioning on their API, and mow just removing Apis with little warning. 
&gt; It amazes me on why people still develop for facebook with all The answers is guns, via a short detour through money.
I see that there really isn't any filtering going on there. Not sure if you want to filter it, but I just added `jizzlobber` to my flair with ease.
There is no filtering indeed, but if needed I can manually delete inappropriate user flair (like I did with your flair, just to test this feature out, no hard feelings).
No, of course not. I assumed it would be removed. I did it to show. I doubt that people will vandalise this subreddit, but I thought it should be considered.
Dreamhost will let you run python CGI, but they provide only community/wiki support - no official support for Python. Linode costs more, but has a much more active Python user community. Look at Bottle.py for a simple way to turn it into a CGI page.
Is that hard to believe?
I use django and celery (http://celeryproject.org/) for this type of stuff. django to handle the front end and input validation, then I pass stuff off to celery for the computations and heavy lifting. Celery just uses a pool of independent worker processes that live outside the web server, so I don't have to worry about any quirky modules that might not play nice inside a mod_wsgi environment, etc. You'd also need to setup some kind of ajax based polling on the client side to detect when celery is finished with the task. My apps are internally hosted though, so not sure if that type of configuration is an option for any commercial hosts.
The publishing metaphor makes a lot of sense. Interestingly enough, many publishing houses do not use version control software of any kind, but simply email chapters back and forth to the various editors and authors. I've tried to convince them to use something like github, but it's an uphill battle. 
I would like to use this opportunity to ask for suitable unofficial alternatives for a facebook-sdk in python, thanks.
Somehow it is yes. That being said, I'm not familiar with the Facebook platform at all. I can only believe its huge and means a great maintenance's cost.
I feel the sudden urge to start an open source project with that very name, just so I can legitimately use it for flair. Now if only I could think of what the heck it would do!
In short, if process is CPU bound then threading is usually useless, if there is lots off IO or network or user input then threading is useful. This is all regarding main implementation off python C. There is java, net and pypy python where there is no GIL. Most off my code is not CPU intensive.
Wouldn't it be great if it could interpret ;)
Then yours could raise a NameError :)
Of course I defined those variables before. (Hang on for the ninja edit) :P Edit: Phew. That seems better :)
Take your internet points here or at Stackoverflow or both: any ideas at what I'm misunderstanding here? Why doesn't socket throw an exception when I try and assign my HTTPServer to 8080 if another app (SABnzbd+) is already listening there?
Thanks for your reply! I'm still not quite sure I understand... how can a process *not* be CPU bound? Can you also give me a simple use case for threading?
Most 1:1 translations of Java DPs to Python don't make sense to me, e.g. the command pattern as shown in this book. There's no need for an abstract base class for a command. Also, a function object could be passed to the invoker class directly. There're more samples in this book of Java code written in Python, which makes the code very cumbersome. 
I must admit this isn't the first time I get that feedback. Is the switch to wall of super powers at 50% enough to reprise the negativity?
&gt; This site requires JavaScript. nope nope nope nope nope
Facebook is making money hand over fist but they're not going to pay somebody full-time to maintain old APIs if they don't have to. If they could find a way to run the site without any developers, they would.
it seems to have his entire blog on the one page (or at least several entries), and then cuts it down to the specific entry via javascript. The javascript fails on IE9. Similarly, the code blocks (which appear in chrome as white on dark purple) appear to be black text on purple, which is turned into white via javascript, which also fails on IE9. The sad thing is, all that was needed to make this site work properly was to *not* implement it in a totally braindead way in the first place! Displaying a single block article should be a server-side operation, and choosing what colour the text should be should be done purely in CSS
Sounds like someone has the case of the mondays!
Don't mean to sound like an ass, but how about typing it in yourself? A great way to learn is by trying to understand the code and recreating it yourself, especially if it's just some "hello world" app.
This is a classic problem with Python, where whitespace is syntactically significant. Try different PDF readers, repeat until you find one that retains the whitespace when copied. Also try different destination editors -- Notepad might not be honoring the whitespace from the clipboard content.
Try PDF-XChange: http://www.tracker-software.com/product/pdf-xchange-viewer
While the bitwise operations explained by lutusp are probably the more common (especially when compared to other languages), the following can be used with different (but generally similar) functionalitywhen used with [sets](http://docs.python.org/library/stdtypes.html#set-types-set-frozenset) which can come in very handing when having to compare multiple sets of objects: * `|=` updates the LHS with all elements from set(s) on the RHS * `&amp;=` updates the LHS with only elements contained in all set(s) on the RHS (which includes that on the LHS) * `-=` updates the LHS removing elements found in set(s) on the RHS * `^ =` (mind the space) updates the LHS using XOR logic against the RHS (only accepts one set on the RHS) 
Neat. Thanks!
+1 on dropping 2.6 :)
inverse kinematics for robot arms?
Search for the examples online. A lot of programming books have their source examples available for download somewhere.
Don't guess, use netstat. (The options to use to list listening connections depends on the platform, which you didn't specify.) It will tell you what address each is bound to. The unique identifier for a bind address is the tuple of (address, port), not just the port. If the machine has an IP address of, say, 10.1.2.3, then it's totally kosher to have one server bound to 127.0.0.1:8080 and another bound to 10.1.2.3:8080. It's only when you use the 'wildcard' address (`INADDR_ANY` aka 0.0.0.0 for TCP v4) that a bound server prevents any other server from using that same port on a different address. 
i think we’d get along nicely :D
Well, that's obvious. It would lob jizz?
It's quite likely that the PDF has been encoded using graphics primitives along the lines of "move to position 334,669 on the page and output this text there." The leading whitespace in the code has been absorbed into those coordinates and simply does not exist any more, so it's impossible to copy it, in the same way that the normal margins of a paragraph of prose don't have whitespace at the beginning of each line even though they're at some offset from the left edge of the page. You're going to have to fix the code yourself. If you're not using a real text editor, this would be the time to get one. A real text editor lets you select/highlight a block of text and adjust its indentation as a whole with a single action, which makes it really easy to fix up examples like this. Notepad is not a real text editor. 
It may not be possible. It 100% depends on how the PDF was built. Not only can text be embedded, but it can be arranged / placed at any 2D set of coordinates. It is not uncommon for indentation (leading whitespace) in PDFs to be text placed farther into the page - rather than spaces / tabs. You can't copy the whitespace if it isn't there to be copied.
&gt;(The options to use to list listening connections depends on the platform, which you didn't specify.) Good point, I'll go back. I'm trying to make it multi-platform but my first goal is windows compatibility. &gt;Don't guess, use netstat. OK, so when I add a httpd.serve_forever() to the for loop in the code referenced, and I start my server twice (running two separate python processes) and I run netstat I get: C:\&gt;netstat -abn | Findstr 8080 TCP 127.0.0.1:8080 0.0.0.0:0 LISTENING TCP 127.0.0.1:8080 0.0.0.0:0 LISTENING So, even though I'm not explicitly binding on 0.0.0.0:0 it seems to be binding on that address and ALSO accepting two 8080 ports? I'm very confused.
*sigh*, it’s no classical problem of python, but of PDF: text in pdfs isn’t meant to be copied, as you can see from the fact that formatting isn’t retained, but hyphens are; minusses are often not the ascii "hypen-minus", but different charaters, … pdf is a purely presentational format, the text in it is just for searching. if you add code to your pdf documents, you should either attach it (a pdf document allows file attachments) or link to a online repository of the code, else you are signalling that you don’t want it to be copied.
I don't mean use netstat in your code, I mean use it as you, from a command prompt, to diagnose what's going on. 
Sweet.
Neat trick. How do you like juggernaut? I think however that the live demo is a violation of [your own privacy policy](https://trekseat.com/tos)? Also: https://trekseat.com/about -&gt; 500 error https://trekseat.com/blog and https://trekseat.com/contact -&gt; 404
Maybe he wants to reduce the load on his webserver?
O0H LOOK EVERYONE! HE'S GOTA CO-FOUNDER! co-founder? check coding in python? check. writes awesome tool for present? check open sources for xmas? check man i'm sick of these co founder posts flooding reddit looking for karma. just because most of the redditt community are forever Founding alone types doesn't mean you can manipulate us into giving you easy karma ;p ps thanks for great release look fwd to playing with it 
OK, edited my reply to include some netstat details
I agree with you, but it must be noted that notepad++ is a real programmers editor, not to be confused with notepad
2.6 is the new IE6.
Nifty. Any way to group users with similar flair? 
Awesome!
And it contains multiple commits, and you can add commits to a pull request, if the person reviewing it asks you to change something. And you can add comments on specific lines of code. It makes patch submission awesome.
Useful tools for scientific computing, but not really relevant to what the asker is trying to do.
A common workaround for web frameworks is to limit the number of POST/GET/cookie parameters per request. [Bottle does this](https://plus.google.com/b/104025895326575643538/104025895326575643538/posts/A98mM9ySYUh), but other frameworks may still be vulnerable.
* In general, where clarity and conciseness is more important than speed. * In scripts where VM startup time is longer than the script run-time. * On networks where the communication latency is greater than the CPU time (like web servers). * For prototyping since you can accomplish more with less code and there are a wealth of libraries. * As a glue language, speed insensitive items can be done in Python and the rest in C. * As a scripting language in games, text editors, etc. where something like Java is just too verbose. 
Juggernaut is nice, sorry about the site we were just finishing up the static pages, we've been doing a very limited test to make sure everything works
Prototyping. I use it for creating powerful, feature complete prototypes. If I need to speed it up, then I'll rewrite it in C or Java. Web development. Use django or something, and you no longer have to mess around with languages like PHP. Desktop software. If you're a Linux user, you'll find that a lot of the software you use is written in Python. Scripting. All sorts, basically. It's a powerful language - you could do just about anything with it.
I don't care so much about karma. I have been a redditor for 3 years and have something like 10 link karma. Its more about the content than anything. But really it was my cofounder who wrote it. 
Last year I was in a programming contest. I used Java, as it is the language I know best. This year I got interested in Python, and to test it I tried a set of problems from a different year of the same contest, and I got as many as I did in 5 hours of Java in 36 minutes on Python. Each program ran slower than a comparative Java program would have, but I think you can see where the cost-benefit analysis goes here. Also granted, I am now better at said problems, and that year's were certainly easier than last year's. But still.
no, because it's not neutral either. how about py3k compatibility?
Python is generally a nicer language than Java. Python has first-class functions; I have grown to detest any language that does not provide this powerful feature (e.g. Java).
Hopefully my flair is appropriate :) At least it will save me from typing "as someone who loves Haskell" on 1/2 my comments here at /r/python
I feel like my flair describes my computer usage well. Others may see it as inappropriate...
I have read that python is used as a "glue" . I don't understand exactly what is meant by that. I take it as you would write smaller modules (sorta like unix pipes and filters) in java and C and use python to control the higher structure of the program? 
So, coming from java as you have, as I am, any particular features I should take note of that might not act as expected? 
Would you kindly give an example of the use of this? 
Isn't this Blogger problem?
You can use CPython, and extend Python with C modules, or Jython, and extend Python with Java modules. These allow you to write the critical parts of the application in faster languages, while still being able to write most of it in Python for development speed.
Yay! :)
They say testers are poor coders and they confess they are testers. What do you expect then ?
oh man you got me so good! just saw your page just beofre i clicked send. so kept the stuffi wrote for your amusement &gt;dude! seriously its a riff on the scary anti-"my SO" guys. i could hardly see there being a rash epidemic of fake co-founder posts on reddit for karma. is there? stranger thngs have happend though - should have cheked before i made such a joke. nor there being a rash of open sourced code for xmas! i wish you guys all the best in your endeavors 
Thank you :)
This isn't really Python-specific; there would be the same problem with other languages. Sure, it would compile and work without the indentation, but it would be completely unreadable.
Not that are built into the site in any way. I'm sure you could use some Reddit API to get user names and flair, then throw it into something produced by the various skills you have listed in your title.
Dumb example: &gt;&gt;&gt; def plus1(x): return x + 1 &gt;&gt;&gt; map(plus1, [1,2,3]) [2,3,4] A more complex example: python decorators are defined by utilizing first-class functions.
Pyjamas (pyjs.org) is a python port of GWT which is written in Java. I don't remember the exact numbers, but the python version was almost 10x shorter (fewer lines of code). I really appreciate the clarity and conciseness of Python. I also really like Python's interactive shell. I like that I can type 'python' at the command line and go to town prototyping. As a matter of fact, there have been several instances where I've actually used Jython to tinker with Java code in order to skip the whole code-compile-run iterative process.
http://stackoverflow.com/questions/868568/cpu-bound-and-i-o-bound Hopefully that will explain it. I believe what IbeeX is saying is that if whatever you're doing (your game, in this case) is quite CPU intensive, then threading isn't of much use, as you can't always speed it up with more threads (in some cases you could, I believe, but for now you shouldn't need to). So by "CPU bound" he simply means the speed of what you're doing is bottlenecked by the CPU, and thus there's not much you can do about it. Relating to I/O, it's the same thing as above, but with I/O, of course. For a game I think it may be useful to have separate threads so that user input and game performance aren't dependent of each other (never worked with any games, but from playing quite a few I remember sometimes the game itself would be stuck (networking, graphics, whatever) yet I could still move my mouse around and the sight/cursor/whatever would still respond). So to answer your question, I think games are a case where threading can be used for better performance.
Effective DoS attack against your own blog: post your blog on reddit. 
Sigh, looks like it is. Thanks Google. Thoogle.
zope.testbrowser supports Javascript?? I had no idea. I've been using selenium for the longest time!
Dumber example: &gt;&gt;&gt; def hello(subject): ... return 'Hello, ' + subject ... &gt;&gt;&gt; greeting = hello &gt;&gt;&gt; print greeting('world') 'Hello, world'
Both Sage and IPython can run as web servers that allow users to create, edit, and execute interactive Python "Notebooks" on the server from any browser. They can be used to run arbitrary Python code on the server (including code that uses Scipy, Numpy, Matplotlib etc.) and display the results in the web browser. * [Sample Sage Notebooks](http://demo.sagenb.org/pub/) Edit: Added link to sample Sage Notebooks.
 from bathroom import leopard del leopard Problem solved.
can you explain a bit more about this attack? is it generating collisions on the hashes and if so, why does it turn into a linked list? the only hashtable I know that doesn't just overwrites the entries on repeated keys is Django's MultiValueDict and even in that case you need to use specific methods to access that feature. [edit] thanks everyone for the explanations, this has been really insightful.
&gt; I'm not explicitly binding on 0.0.0.0:0 it seems to be binding on that address No, it's bound to 127.0.0.1. The first address is the local, the second is the remote, which is 0.0.0.0 for a listening socket since it's not connected to anything. I'm not sure why it's allowing two things to bind to 127.0.0.1:8080. That certainly doesn't happen when I try it. Maybe it's some kind of firewall/security software that's interfering? 
Great, thanks!
I do that ***ALL*** the time. alias pythong='python'
Check out some of the examples at [Rosetta Code](http://rosettacode.org/wiki/Category:Programming_Tasks). The python solutions are usually about half the size of the Java ones, at least. And they're a lot easier to read because they don't have all the tedium of Java; if you want a list in python, it's just: foo = [1, 2, 3] ...no matter what the types you put in there. There's none of this `List&lt;Integer&gt; foo = ArrayList&lt;Integer&gt;()` tedium, and mixing different kinds of things in a list is trivial: foo = ["string", 3, 54.43] In python you just write what you want, you don't worry about all the busywork. 
We are not talking about multiple dict values with the same key, but multiple dict keys with the same hash.
It's generating collisions on the internal hash function that's used under the hood to implement the dict type. The actual key values are different, so it's not this: foo["bar"] = 12 foo["bar"] = 14 it's more like this: foo["abc"] = 1 foo["xyz"] = 1 Where abc and xyz are specially chosen to compute the same hash value, causing a collision. It turns into a linked list because that's what you do when you implement a hash with a hash function that can result in collisions.
This is almost 2012. The "I'm afraid of javascript" gig went out of style 3 years ago. Get with the times.
I'm using Notepad++. 
People using the conventions of the community when publishing material for that community.
recording of the talk: http://www.youtube.com/watch?v=R2Cq3CLI6H8
Actually, if you've been paying attention there's lot of nastiness which JavaScript enables. I use NoScript and only enable it when necessary. For reading a blog post, no JavaScript should be necessary.
[/r/nba](/r/nba) does something so that people can choose from a list of teams for theirs, not sure what the mechanism is though.
Wouldn't one have to know the exact hash function being used by the HTTP server? I think a lot of different ones are used. I guess they could just kind of bruteforce it, by sending "a".."z" and then "aa".."zz" etc. but I doubt letters near eachother would cause a collision.
They went with the flair-templates option, I chose the freeform-flair option because there isn't something comparable to 'teams' in the programming world. 
They went with the flair-templates option, I chose the freeform-flair option because there isn't something comparable to 'teams' in the programming world. 
Considering how many ways to exist in web apps and frameworks to effectively DDOS them I think this might actually be one of the more obscure ways to do it. If I would want to bring down a server I would just very slowly send requests to the server. Much more efficient :-) //EDIT: I have not watched the talk but I assume they are trying to degrade a hash table into a linked list.
Ok, I never looked at what our options are. If it was possible it'd be cool if we could complement the freestyle text with that (so we could have e.g. Pylons, Django, flask, bottle, etc. logos), no idea if we can though.
Sweet! Here's mine! import os, sys os.system('tail -f ' + sys.argv[1]) 
I'm not sure if we can combine both freetext and logos (maybe /r/modhelp can shed a light on that), but go to [edit user flair](http://www.reddit.com/r/Python/about/flair/) in the admin box and then to "edit flair templates". The first, empty row is the freetext flair as it is now. It seems that you can add additional rows with e.g. Pylons, Django, flask, bottle, ... as text and some CSS to add the logos. Feel free to play around with it.
PDF is more of a graphics format than a dokument format. There might be leading whitespace or not and the reader might copy that or not. Extracting text from PDF can lead to pretty much anything including total garbage (glyphs not mapped to unicode etc.). With Acrobat you could try "Copy text with formating...". Pdflib.com has a free pdf plugin (based on TET lib) that does higly configurable text extraction... But maybe having to reformat the indent manualy after copy/paste actually helps you to understand the code :)
&gt; Actually, if you've been paying attention there's lot of nastiness which JavaScript enables I'm a software developer, and write javascript for a living. I know what "nastiness" can exist. I never use noscript, and I have never had a problem.
but wouldn't that be the same key? as far as the hashmap is concerned
but wouldn't that be the same key? as far as the hashmap is concerned
knowing the framework is enough; I'd imagine it's pretty trivial to tell what frameworks are being used.
I wasn't writing about the nastiness of the language, but of the security environment in which it runs. &gt; I never use noscript, and I have never had a problem. That you know of. Watch a Defcon lecture sometime...they can be eye-opening. Watch [this one](http://www.youtube.com/watch?v=stnJiPBIM6o) in particular.
&gt; What is python especially good for? * Very fast development and testing. * Interactive work, especially in mathematics and scientific pursuits, with many specialized libraries. * Learning programming, for those with short attention spans and/or people who hate waiting for compilers. * Prototyping designs to be later coded in faster-running languages. 
Ok. So I did a little research into it. Please tell me why Python would be preferred to say, Groovy? 
The hash is a compressed key. Multiple keys can have the same hash (aliasing).
1. Things declared directly inside the class body belong to the class. That applies to both methods (as it would in Java) and attributes (which Java would call 'fields', and consider part of the object). The reason there is no distinction here is partly due to the overall design philosophy and partly because there is no sane way to make the distinction, since functions are first-class objects. 1. Everything is an object. That includes functions, integers (there are no auto-boxed primitive types, and an arbitrary-precision type is built into the language - in 3.x, you **only** have arbitrary-precision for integers), classes (you no longer have to go through a reflection module and jump through hoops to get reflection - it's right there), and even **modules**. Welcome to **object** oriented programming as opposed to class oriented programming. While obviously frowned upon, it's valid to write things like `(2).__add__(2)`. (The first set of parentheses is needed because otherwise `2.` would make the compiler expect a floating point number, and then you'd get a `SyntaxError`.) 1. Dynamic typing means you have to document and test your interfaces, and generally avoid trying to specify them formally. You *can* sort-of simulate the Java approach with abstract base classes (and there is a library module to help), but it's at least as much work as in Java (too much work 99% of the time for typical Pythonistas) and often costs you flexibility while gaining you nothing tangible. 1. There is no separate data type for characters. The elements of a string are 1-length strings. So something like `'foobar'[0][0][0][0]` is totally valid (but again frowned upon). 1. Everything happens at runtime. Even compilation. Syntax errors are reported using the same exception-handling mechanism that's used for everything else. (Also, some **looping constructs** are built upon exceptions.) 1. Beware of which objects are mutable and which aren't. Augmented operators like `+=` work on immutable objects because they **normally** just get translated: `a += b` -&gt; `a = a + b`, so the reference is rebound (everything has reference semantics, like non-primitives in Java). However, the built-in `list` type breaks this rule: `a += b` -&gt; `a.extend(b)`, so the original object is modified (which matters if `c` aliases `a` beforehand).
I don't understand this, if two different keys map to the same hash how come they don't overwrite each other? why does it become a linked list? is the hashmap storing all the different keys that generated that hash and somehow associating them to the values?
exaaactly. which means the hashmap will have to construct a linked list in the place of the original key, which changes your algorithmic complexity from O(1) to O(n). so, let's say some common web service uses the "s" parameter, and that gets stored in a hashmap. you send it a request with ten million parameters all of which hash to the same key in as "s" and therefore must be stored in the same hashmap bucket. the web service tries to get "s" from the hashmap, which now is really just a 10mil element linked list that the machine must traverse before realizing that they never sent "s" in the first place and wait why am I suddently hungry and where did everybody go and why is everything chrome all of a sudden and WHAT YEAR IS IT?!
You can think of the hash table as an array of linked lists. You hash (compress) the key, to generate an index into that array. Then you search the linked list at that index. Each entry in the linked list includes the full key, so, you compare them one-by-one to the key that you're searching to check if it's in the hash table or not. When things are working well - a lookup is an O(1) operation as there is only 1 key in each linked list. When things are not working well (i.e. due to a malicious attack that is intentionally colliding the hash function), many keys hash to the same linked list, and the lookup degrades to O(n).
but why does it do that? it makes no sense at all, any key that produces a specific hash will store the same value (or overwrite the previous) you don't need to lookup the *keys* in fact that's the whole point of the hash map, you hash the key and look for that hash. we can assume the actual keys are also stored since most hashmap implementations allow you to retrieve all the keys and that might be a linked list but why would I look for the key there? I would just hash the requested key whatever it is and look for that, otherwise you're doing TWO searches on each lookup and that's plain stupid imho.
I don't like the term "compress" because compression is bidirectional, hashing is a one-way function (precisely because of the collisions). regardless of that, the part that doesn't make sense is when you look for the key, you shouldn't look for *the key* but for the hash of that key, that's the whole point of the hashmap and it's what makes it fast, given that the hash is fixed in size the time it takes to search for any of them is the same regardless of the size of the input that generated it. now why on earth do you search the linked list for the *key* that may not be unique if you already have it because it's the one provided and you also have the associated value, which is what matches the index of the array that is the hash of such key. I don't see any reason to even access that linked list of keys nor any sense in having it associated to their values when it's the hash of each of them what actually makes the mapping.
there's some webs that provide that: http://stackoverflow.com/q/1046441/226201 there might be CLI or GUI tools as well
Because if you have a 100 bit key, and a 32 bit hash function, there are many keys that will have the same 32 bit hash value. The data structure needs to distinguish between them, so it must store and search for the whole key. The hash is a (very good) hint but it's not sufficient on its own.
so the whole point of this is that two different keys that produce the same hash actually store different values? I don't see why the hash has to distiguish between them, a much more reasonable solution would be to use a different hashing algorithm with lesser chances of collision and then overwrite each entry on duplicate hashes; what you just said sounds like the hash is just an index, an aid in the key lookup and not a real mapping between hashes and values. if that's the case then screw us all.
Well, if foo["abc"] and foo["xyz"] both map to 1, you either have to make it a linked list, or you have to throw every value but the first out. And I assume throwing out certain values is probably a bad idea, and would lead to awful, difficult to find bugs.
A hashmap (at least all implementations that I've seen) are indexed in a fixed-size data structure. The common size example given in the article is 2^32. A hashing function to determine the hashed index in that structure will ideally choose an even distribution in that range, so that in theory you could store up to 2^32 values and access them with O(1) complexity. However, the number of possible keys is infinite, so there exist an infinite number of values that can hash to each of those index values. With random data, it should be rare that there are any collisions, but the solution to that problem is to store a list of results in that hashed key value and then iterate through them to match the provided key if there is more than one stored there. You don't want it to simply overwrite the previous value because then foo["morp"] and foo["dingle"] could potentially overwrite each other and that would definitely not be expected behavior. This exploit takes advantage of the fact that there are infinite values that can be hashed to each index, and chooses different keys that will all hash to the same index. To resize a hash table in attempt to avoid collisions would require you to re-hash all of the existing values, or provide an access mechanism that is not O(1), both of which remove the benefits of a hash table.
Because if two keys collide, we still want to store the data. And if we want to later get some of those keys back out, we need to do the linked list lookup to get the data we want. And even if your hashmap kept a list of all valid data, yeah, sure, searching a sorted array or a tree is an O(log N) operation, so it's fast. It doesn't help that "s" is still a valid key and therefore the hashmap must pull it out of the tangled mess of linked list chains that the hashmap has turned into. You do realize hashmaps aren't for looking up keys, right? They're for looking up data associated with a key.
The linked article gives a good example of an IO bound work (non-CPU-bound) being done in the thread - listing contents of directories. This isn't CPU bound because had the CPU been only doing this work, it would spend the vast majority of its time waiting on the storage device (HDD, SSD, network for NFS, etc.). Other good examples are socket communications, fetching stuff from the Web, and so on. As for games, Python threads can actually come in handy for decoupling the user interaction parts from the actual game logic.
FYI [/r/bicycling](/r/bicycling) has combo logo and free-text.
so we're saying that we use secure hashes in most places and don't bother with collisions but here we choose to use the cheapest one so it has to take additional measures as to not lose data, I would rather use a better hashing technique and have repeated hashes overwrite each other, good algorithms have a very low collision rate.
so we're saying that we use secure hashes in most places and don't bother with collisions but here we choose to use the cheapest one so it has to take additional measures as to not lose data, I would rather use a better hashing technique and have repeated hashes overwrite each other, good algorithms have a very low collision rate.
so we're saying that we use secure hashes in most places and don't bother with collisions but here we choose to use the cheapest one so it has to take additional measures as to not lose data, I would rather use a better hashing technique and have repeated hashes overwrite each other, good algorithms have a very low collision rate.
I agree with you, but the people who come up with this stuff probably know a lot more than you and I, and decided on this strange system for some reason. Probably for overall performance purposes.
From a language perspective, Groovy tries to copy lots of the niceness of Python. I've never used Groovy myself, but I've heard generally good things about it. Go with Groovy if you want to integrate with existing Java code and/or be on the JVM. One big thing Python has going for it is [PyPI](http://pypi.python.org/pypi); I'm unaware of any CPAN-like repository for Groovy. Python uses significant whitespace, while Groovy does not. A lot of people make a big deal about that difference; I personally enjoy significant whitespace but it's not a killer feature for me. Also, I think there is more general interest in, say, /r/python than /r/groovy . That's pure speculation on my part; try out both communities and see what you like, I guess.
No, you don't understand - a dictionary uses a hashmap, but it's not just a hashmap. Maybe the article wasn't clear on this. c = 8589934590 for i in range(1000000): assert hash(c*i+1)==1 attack_dict = {} for i in range(1000): attack_dict[(c*i+1)] = i assert attack_dict[(c*0+1)] == 0 assert attack_dict[(c*500+1)] == 500 So even though dictionaries are implemented using hashmaps, hash collisions are resolved somehow. How they do this is an implementation detail, but you just need to know that it doesn't scale. If you have a dictionary with 1,000,0000 colliding keys, adding an extra key takes about O(1,000,000). So actually making a 1,000,000 dictionary (with all colliding points) takes O(N^2), which means an attacker can kill it very easily when you try to eat his enormous colliding cookie. The attack starts to "bite" at around N = 10,000. At that point, Python starts to feel very slow - try: attack_dict = {} for i in range(10000): attack_dict[(c*i+1)] = i Or if you have a really fast box, use n = 30,000. At this point, you can take down a core for a few seconds with a single request. At n=1e6, you can knock out a core pretty much indefinitely. For reference, classic dictionaries are done by putting a linked list in every hashmap value, and searching through that to find the key-value you wanted. I think Python uses "cuckoo hashing", in which collisions are resolved by putting the value into the next hash value (i.e. adding one to the key, then hashing it again). Whatever the case, it's not very scalable if there's lots of collisions.
But the data structure is lossless by defintion; it needs to insert &amp; map any key to any value - it can't just silently ignore certain key/value pairs because of a hash collision. That would violate the behavior that programmers expect from it.
Even if we SHA-256'd the keys, hash tables can only be so big. SHA gives you 256 bits of output and processor address spaces are no larger than 64 bits (with even less actually physically wired on the motherboard). Hashmaps are built on top of arrays, which means for a hashmap with n bits of hash entropy you need 2^n * sizeof(void*) bytes to store the hashmap. Finding collisions on significantly smaller subsets of a message digest is much easier than finding collisions on the whole digest, and like I said before you simply cannot construct a hashmap with 256 bits of entropy. It would be many trillions of trillions of exabytes large. Even with just 32 bits of entropy your hashmap will be 32 gigabytes large - and even then 32 bits is insufficient entropy to prevent intentional collisions. In short, for hashmaps to be practical, they must deal with collisions. Simple as that.
Bruce Ekel has a goog discusion on python dp. www.mindview.net/Books/Python/ThinkingInPython.html
Hmm, Microsoft have issued a [security bulletin](http://technet.microsoft.com/en-us/security/advisory/2659883) and are issuing a [patch for asp.net](http://technet.microsoft.com/en-us/security/bulletin/ms11-dec). 
When execution time is going to be minimal, brain-to-paper time can be minimized pretty well with Python (Perl will do this for you too, and hell maybe Awk). It's great for informal work, testing things out or doing proof of concept type things. It's great for 'glue'; I take data from six sources (as JSON, csv, text, etc), mix it all together, put it into a database, and then make it useful and generally accessible all through one interface. Well, I should be doing that. I'm on vacation now, though... It's great for small tasks and automation, it's great for large tasks where readability is key (you can always obfuscate, but Py lets you be clearer than other languages), and it's great for everything in between because of its robust batteries-included package list, generally intuitive syntax, and ability to work with existing frameworks. My two cents. 
&gt; so the whole point of this is that two different keys that produce the same hash actually store different values? Yes, this is how every dictionary structure in every scripting language works because it is the only sensible implementation of a dictionary structure. The hash function and underlying storage are implementation details and are chosen for a decent efficiency vs memory tradeoff. Users of the data structure should have no reason to care if the key "user" and the key "password" hash to the same value, but the dictionary struct must return the user and password entered into it regardless of the hashing implementation or storage. You seem to believe that the only reason that in python a['user'] and a['password'] do not conflict is that they do not have the same hash. This is not the reason they do not conflict, though it is likely true.
I find list comprehensions easier to read than chaining calls to map and filter: [x+1 for x in [1, 2, 3]] 
2.4 would be a more apt comparison since there's so much out there still on RHEL 5.
Kewl
Agreed. This attack is simply too much work.
&gt; what you just said sounds like the hash is just an index, an aid in the key lookup and not a real mapping between hashes and values. This is essentially true. In the end, a hash-map must translate to "physical" data structures of continuous data (arrays) and pointers. Most hash-maps are implemented with an array of a certain size, where each element is a pointer to a linked list. A hashmap "key" is hashed to an index of the array, and ideally there is only one element at this array-key, making an O(1) lookup. In the unideal case, this can be an up to O(n) lookup if all elements are stored in the same position of the array (this is very very unlikely to happen unless that is the goal of the input based on a known hash function, as is the case in these attacks). Hash maps do not actually have guaranteed O(1) lookup, rather they have an amortized lookup time of O(1) across many accesses on average. The size of the array and the hashing function determine the rate of collisions. &gt; a much more reasonable solution would be to use a different hashing algorithm with lesser chances of collision Reducing the chance of collision would mean the array-size would need to be much much bigger than necessary in the average case. This would require a very large memory allocation and is not realistic. Many hash map implementations allow you to specify the expected number of elements stored in the hash-map and therefore based on a collision likelihood, adjust the size of the array and the hash function used. &gt; and then overwrite each entry on duplicate hashes; This would definitely not be okay. Hash maps guarantee that every unique key will not overwrite another unique key. The underlying hash of this key should have no relevancy to that guarantee and would make it infeasible to use a hash map in the same way. Example: if the hash function resulted in the same index for both "apple" and "banana" (it probably would not, but if it did), then if I did this: map["apple"] = "red" map["banana"] = "yellow" I would expect: print map["apple"] print map["banana"] to print: red yellow With your proposed solution, it would print, unpredictably: yellow yellow 
You seem quite lost. Please google 'separate chaining' and 'linear probing'. Both methods handle hash collisions.
Read my other comment, but you have to understand, using a "secure" hash comes at a huge cost in the case of a hash map. Ignoring the fact that a secure hash function is very slow (making the collision-prevention benefit very negligible): For example, if we used md5 results in a 128bit hash and used it with its full security (no modulus on it), it would have 2^128 possible results. That is approximately 3.40282367 × 10^38 (a huge huge number). That means to index by those possible values, we would need an array of that size. Assuming the array holds 32bit pointers only, thats 4bytes per slot. That would be 1.2x10^39 petabytes of memory. 1 petabyte is over a million gigabytes. Basically an impossible amount of memory for even the most modern computer. There is a reason "md5" and other hash functions are considered secure, and that is (partially) because they have a ridiculously sized result space.
This WSGI middleware will protect you for GET requests: MAX_QS_PARAMS = 100 def protect_against_hash_dos(app): def bad_request(environ, start_response): start_response('400 BAD REQUEST', [('content-type', 'text/plain')]) yield 'Go away.' def inner(environ, start_response): qs = environ.get('QUERY_STRING', '') n = 0 for c in qs: n += int(c == '&amp;' or c == ';') if n &gt;= MAX_QS_PARAMS: return bad_request(environ, start_response) return app(environ, start_response) return inner For form data you can use a similar approach.
`n = sum(c in ("&amp;", ";") for c in qs)`
There is **no** such hash function that doesn't have collisions for arbitrary input. By definition, all hash function have an infinite number of collisions, because it's reducing its input data of unbounded size to a fixed size value. This is the pigeonhole principle. It is not possible to reduce or abate this fact. It is possible to write a perfect hash, but only if the values it will be hashing are restricted to a specified set and known ahead of time. The idea that a language should just randomly overwrite hash keys on collision is the most absurd suggestion I've read in a while. It would make these data structures completely worthless, because your data would never be safe, there would always be a chance that two keys would collide and cause your program to crash, behave sporadically, blow up, etc. Such a data structure that is not deterministic is absolutely worthless. 
It doesn't matter if the collision rate is low. It would be a completely unusable and worthless data structure without the guarantee that any value can be used as a key without losing data. If there is even a slight chance that I might lose data, then there's no way in hell I'm going to use such a data structure, because I don't want my program to fail in strange and unpredictable ways. It's even worse if it only fails one in a million times, because then I can't debug it. The dict must be perfect or else it's useless. This is really just a question of efficiency. It's orders of magnitude more efficient to use a simple hash and a linked list than to use a wide hash. You can have both performance and correctness this way. The attack mentioned in the article can be easily mitigated by adding a bit of entropy to the hash function so that it's not deterministic, while still retaining the fast performance. 
Using hash() on an int seems to just return that int. Could you explain this a bit? edit: Nevermind, if it's a long int it seems to hash it.
Seems like a better idea to do this at the web server level instead of the framework
'Glue' just means anything connecting different bits of code together. That could be loading and calling different functions within the process, it could be using some form of inter-process communication, or network requests, or even something as simple as a script to convert data into the right format for another program.
Yes, but the submitter just wants to let users upload a file, and do some processing on it. If he offers arbitrary code execution on his server, then there's all sorts of security issues he needs to think about. Also, I've not tried with Sage, but IPython doesn't yet let you upload a file.
I think I see. This is annoying because the problem I'm facing is precisely that I have to repeat a CPU-intensive algorithm for a relatively large number of game objects. Because these objects are (mostly) independent, I was hoping to process a few of them concurrently. With that in mind, I should probably not use the Threading module?
Slowloris all over again.
You should learn a [Revision control](http://en.wikipedia.org/wiki/Revision_control) system. Without one, fear of deletion sets in and you might end up keeping all kinds of things that will prevent you from moving as fast as you could. Using git is a very very good choice. It's modern and extremely powerful because it makes merging easier (you can branch, try few things and if they work, you merge them back into main). You will not need all this power from the beginning but as your skills will improve, you will get to a point where you will need it. 
Both problems are pretty easy to defend against though.
Not from the framework's point of view. Any places where user data is parsed into hashable data structures is potentially a target. Many of which are not even up for the framework. On the WSGI level: * HTTP headers to the WSGI environ On the framework level: * Cookies * URL parameters * URL encoded form data * Multipart encoded form data * Any incoming JSON data * Incoming multipart headers * Incoming set headers * Parameters of content type headers etc. There are so many places where things are parsed into dictionaries on very different levels that it's quite useless to do that on a framework/WSGI server level. This attack is not new. Yes, you can keep a CPU busy, but a watchdog should see that and kill away the request handler.
How would you make a watchdog to detect this? Just look for a crap load of keys coming in?
Kill a handler if it consumes too much CPU time or too high IO wait.
Your co-founder, as in the co-founder of you, meaning your dad? Cool. 
You are right. I didn't realize it went deeper than only POST/GET parameters. Restricting the request size (if it fits the application) would be a pretty quick fix though.
 n = qs.count('&amp;') + qs.count(';')
Nginx throttles POST sizes by default. 
To me, nesting is a bit overkill to solve a simple problem like that. I simply wish Python had syntax for such a common case. Instead, I have to produce a less-readable code or use real "for loops".
I was expecting a "why this is different than just using tail" post somewhere either here or in the git repo. Maybe it's not.
I didn't mean a hash without collisions, but you use colliding hashes in cryptography and nobody cares until it becomes reasonably fast to get the collisions (as it happened with MD5), even git uses hashes with low collision rate and doesn't give a shit about this you mention. I can't argue on the space/efficiency limitations mentioned on other comments but ignoring collisions if they extremely rare is something that you're doing every day without knowing apparently.
Alex Martelli's presentation [slides](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;ved=0CDoQFjAD&amp;url=http%3A%2F%2Fwww.aleax.it%2Fgdd_pydp.pdf&amp;ei=_HX8TsXGM6_R4QSSxN2NCA&amp;usg=AFQjCNFoUP6rZHD3GiUAHAUGZRyNJtURTQ) and [video](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=0CCkQtwIwAQ&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D0vJJlVBVTFg&amp;ei=_HX8TsXGM6_R4QSSxN2NCA&amp;usg=AFQjCNHDXmntA8QH4Wm6XEQbsA1W4n3wyw)
In windows I can use the speech recognition engine by using the Make.py utility in PythonWin and then with some work make the windows classes act more pythonic. such as, import win32com.client speaker = win32com.client.Dispatch("SAPI.SpVoice") def say(text): global speaker speaker.Speak(text) class ContextEvents(win32com.client.getevents("SAPI.SpSharedRecoContext")): """The event handler for speech events""" def OnRecognition(self, StreamNumber, StreamPosition, RecognitionType, Result): newResult = win32com.client.Dispatch(Result) print newResult.PhraseInfo.GetText() listener = win32com.client.Dispatch("SAPI.SpSharedRecognizer") context = listener.CreateRecoContext() grammar = context.CreateGrammar() grammar.DictationSetState(0) ListItemsRule = grammar.Rules.Add("ListItemsRule", 33, 0) events = ContextEvents(context) and your ready to go!
Doesn't Jython have Java interop on the JVM?
Many people say python "isnt fast", but that masks a fact: its fast enough for all but the most intensive computations. After all people build web browsers, games and calculators with it. 
Did you do that for the Faith No More song of the same title?
Haha, I did indeed. It's kind of like my own personal `foo` naming convention.
funny but irrelevant
Google, python shines at google.
This is how hashing works.. the only other option is to not use a hash at all, but to just store a list and search for the key every time. There would never be a collision.. but there would also never be a speed gain.
http://pastebin.com/
The point is, python gives you the power to pass functions as a parameter to other functions. def twice(f, x): return f(f(x)) [twice(plus1, x) for x in [1,2,3]]
http://paste.pocoo.org/
A much better question is... where *doesn't* Python shine? Python is used in * Large scale geological computations. * Giant web apps (Fucking Youtube). * Half the scripts and apps that come with Debian/Ubuntu. * Programming motherfucking robots. Why? Because it's faster to write, easier to read, and encourages best practices whenever possible.
I have deep love and respect for the core dev team, but that's inexcusable.
Sorry.
Kids these days, no respect for quadratic probing.
https://gist.github.com/ Key features: * Version control (so you can make changes and show different versions) * Forkable (so people can fork and make changes)
I remain in a state of deep confusion about what the security advisory is. Hash collision (and attacks using it) are not a new thing. There's actually a thread on python-dev about it... from 2003.
Versioning and the ability to fork are awesome. I switched from pastebin to gist a while back and haven't regretted it.
I didn't do much research on Debian, but I think Python is more Ubuntu thing. Things like DPKG, APT are in C/C++ and things in Python are mostly Ubuntu additions (update-manager, add-apt-repository, etc.)
Never knew! Cool stuff, I'll try it out. Thanks!
Speed! Not speed of execution, but speed of going from the problem ("I need something that'll do this...") to solution ("Ahh, there, done. Time for a beer!") The more difficult the problem-space, the more you'll appreciate Python, nothing else even comes close. Think about it, do you know the difference between a solution the runs in 1/1000 second vs one that runs in 1/10 second? The first one took the coder three weeks to write in C/C++, and the second took the coder 5 minutes to write in Python.
Sage lets you upload data files to be used by Notebooks. It is very easy to use if you simply want to get a server running. I will let the OP decide the security implications as per his/her deployment environment. 
I was always a big fan of http://pastie.org/ and the previously mentioned https://gist.github.com/
https://gist.github.com/0ffdd627542cf0de8af4
ooooh fancy, flask looks for a request header. and selects a base template accordingly. So in other words flask isn't doing much in this example. 
Yep! Minimally invasive surgery
Well knowing very little about what you need to do, I wouldn't suggest completely avoiding it. A little more information may be useful.
IMO, SO is better at answering questions like these than /r/Python (no offense!). Also, consider [using the source, Luke](https://github.com/kennethreitz/requests) -- especially since it's actively developed and there's an "Issues" tab. Seems like either a new feature request or bugfix on the docs.
alright, sounds good. thanks!
What's new is that someone presented it at 28C3 and reminded everyone.
gist or paste.pocoo.org
Especially people who like to Metasploit stuff left and right. Edit: Omg, cake.
Wow. Awesome.
Well. For what it's worth: mistakes are something to learn from. The traffic on the security mailinglist is very, very low and so is the number of security reports or problems in Python in general: http://www.python.org/news/security/ I'm not surprised. Also considering that the typo of the security report is not necessarily something new I would say it is way less critical than people says it is. The problem with the talk is the same with firesheep: it's not a new thing, it just went mainstream.
Well I was hoping to do pathfinding on game objects concurrently, but since this is CPU bound, it sounds like threading won't be of much help.
TIL about requests. Thanks!
&gt;good algorithms have a very low collision rate The goals of cryptographic hashes and hash tables are very different. If a cryptographic has produces a collision, it's a disaster, and conversely, slow runtime and a large range for hashed values are generally not a problem (indeed, they're often desirable). However, in a hashtable, the opposite is true - performance matters, and collisions are generally minor issues. You're using the hash function to determine what bucket to put the item into. You want a relatively small number of buckets (say, 2-3 times as many items), which means that even with an entirely uniform hash function you **will** get items that hash into the same bucket. You *can't* have enough buckets to make collisions sufficiently unlikely without making the hashtable so large to be useless. Eg. even a humongous hashtable with 2^32 buckets (taking 16GiB just for an empty pointer in each bucket), using an evenly distributed hash function has a 50% chance of a collission with a mere 80,000 items stored. (And given that a collision would *lose data* under the scheme you propose, even a 0.01% chance would be too much, which you get with a mere thousand items). Collisions are inevitable, and generally no big deal - they'll hurt performance when you need to probe twice, but far less than, say, using SHA for your hash function would. 
so what does Facebook use internally? 
Its a really heavy php shop. They've even released php optimizers and the like. 
You can do some really nifty things with bitwise operations. I highly suggest every programmer learn them. They are completely accessible to even beginners, and I feel they are one of the first elements of programming that will cause a paradigm shift in your thinking. 
Well, to be fair, not all OSs (Windows) have tail, so having a python implementation is probably more portable. I'm also assuming it does other things, like makes it web viewable via Ajax, for example.
It doesn't have very good proxy support. You'd have to either write your own implementation, or use a different module.
I'm reading this the second time today and I don't know what this means. The web server has no idea about parameters in POST requests. They get parsed by the library (or programming language itself, in case of PHP) of your program/framework. 
Stop using Wordpress. 
&gt; This attack is not new. I read that Perl changed its hashes a few years ago (2003?) to counter this attack. Strange that nobody else thought of it in the meantime. 
I'm not commenting on the merits of this particular security report; rather that the process for handling such reports seems to have failed in this case. These researchers did *exactly* what they are supposed to. They gave advanced notice to a trusted group of core devs before going public with a functioning, real-world exploit. The alternative is more 0days. Pretty clearly, a whole bunch of language &amp; product dev teams (not just Python) dropped the ball on fixing this one. Some folks in the security community believe immediate full disclosure is the way to better security (instead of early notice to authors). Failure to respond to security reports encourages that view. A security list that no one reads is arguably worse than no list at all.
Knowing the programming language is enough.
&gt; These researchers did exactly what they are supposed to. I did not blame them at all. &gt; Pretty clearly, a whole bunch of language &amp; product dev teams (not just Python) dropped the ball on fixing this one. Some folks in the security community believe immediate full disclosure is the way to better security (instead of early notice to authors). Failure to respond to security reports encourages that view. If the attack was critical I would agree. But considering this is nothing new I am not really that worried about it. It just got a lot of publicity.
Most web servers can be configured to limit the number of headers in a request right now. It wouldn't be much of a problem to implement something like that for the number of POST/GET parameters as a module addon or a core feature of Apache/Nginx/lighttpd, for example just parse the string to check how many parameters are there before passing it on, slap in some safe default value. IMO this is an easier way to patch most of the internet against this attack, or at least implement it at the application server/module level, for example in mod_wsgi, mod_php, gunicorn, etc. Without that you can just limit the size of the query string at the web server level, but this is not a very good precaution as someone could craft the attack to have very short key/value pairs and still pass that check for some applications that need this size limit to be high.
&gt; If the attack was critical I would agree. But considering this is nothing new I am not really that worried about it. It just got a lot of publicity. So tempted to try it on *.python.org it's almost not funny.
Hey I just learned about it today too! I love it already
Oh well. I filed an [issue on github](https://github.com/kennethreitz/requests/issues/324), we'll see if anything happens.
Well, in their defense I could argue they are from the community of testers and they use testers (unittest) convention. And unittest is an official library ! Yes I know about the java origins and so but...personally I distaste that (as much as wxPython or PyQt conventions)
Best coverage I have seen is in chapters 8 and 9 of Dusty Phillips' Python 3 Object Oriented Programming book. 
I wrote [this](https://github.com/kbatten/python-socks5-socket) and it seems to work reasonably well. It patches socket so it should be usable with anything, though the reason I wrote it was for Requests. This was a lot quicker than writing a patch for Requests itself (though that would be a better solution I think.) Feel free to use it or abuse it.
pro tip: don't load pickles people post on the internet, because they can execute arbitrary code.
thanks! I'll give it a try
You didn't mention a platform. For UNIX, [tsocks](http://tsocks.sourceforge.net/) is useful when you are behind SOCKS proxies.
you know, it has never come up before. That said everything I download is from an untrusted source including Python.
Oh cool! While I'd like to make it as cross platform as possible, this will most likely be used by Linux users only. Thanks for letting me know about this.
I used to use http://dpaste.org/ but looking at gist I think I'll be using that from now on. 
I think one of the barriers to entry is that outsiders are not entirely sure if they are sufficiently competent.
This might actually be a great section to add to the guide, but off of the top of my head I would say: contributions are still welcome. If a beginner is unsure of the area they are trying to contribute, then (a) the maintainers will help him/her, and (b) that makes it an even better learning experience (at least for that one person). Everyone wins!
Use the multiprocessing module - it helps you leverage the concurrency of multiple CPU cores with a very simple API
Not to be an ass, but I disagree. I feel like SO's become a victim of its own success - there is so much content there, that posts don't seem to get the same level of activity that they used to, just a year or two ago. I think r/Python is way better for Python specific questions and ideas. Anyway, just sayin'.
The most interesting part is that its only [one file](https://github.com/coleifer/peewee/blob/master/peewee.py) and its like 1/8 the size of the Django ORM.
Exactly :)
*cough* you should totally upstream this into requests.
Right, plus there's the speed of development to consider. For anything that you can do in Python which you could also do in the canonical compiled languages (Java/C/C++/C#), there is no way in hell Python isn't going to have a shorter development cycle.
&gt; IMO, SO is better at answering questions like these than [1] /r/Python (no offense!). Definitely false from what I've seen.
I'm installing veewee and vagrant to get a working Ubuntu 11.10 system, to test instructions on *right now*!
Yes, one _2200 line_ file. I'm not sure file count is a valid measure of a library's simplicity.
This attitude rocks. Speaking personally, I haven't had much time recently. However, I'm not a beginner (was lead organiser for Kiwi PyCon 2011), but I'm still intimidated with the calibre of the current team. With that said, it looks like [Hacker News has sent a lot of traffic](https://twitter.com/#!/kennethreitz/status/152615043035435009) to the site. Am really pleased. This is a great project for people to get involved in. 
You play with Python, you get bit lol....but seriously, you can avoid these "viruses" by using [ulimits](http://compute.cnr.berkeley.edu/cgi-bin/man-cgi?ulimit+2)
I've seen this ORM before, it's always compared to the Django ORM but I've never seen it compared to SQLAlchemy. Could you provide us some advantages over using SQLAlchemy besides it being one file. Thanks
Big deal. You can do pretty much the same with bash. #!/bin/bash echo "lets clog the disk." &gt; clog.txt cat /dev/random &gt;&gt; clog.txt 
Almost died reading this. Guess python is really deadly...
`sh` is on /usr for you? What madness is this?
Typo. Thanks. Corrected.
Ah, reminds me of the joys of being 12 and everything is new.
Nom.
Reminds me of the days when I thought Batch was the one and only language of ultimate power.
What's newer is RESTful interfaces which enable web APIs and interactive web sites. Web apps now use HTTP POST &amp; JSON all the time, whereas previously much more of the work would have been done by the server. This change in behaviour has made the community more vulnerable.
Only now there's the added effect of "Wow, a virus!? quick, get it up on Hacker News and Reddit!"
Or [quota](http://tldp.org/HOWTO/Quota.html)
I upvote you, you are right.
May I ask for a simple outline of how is this achieved? What I see from simple glance: 1. [Flask](http://flask.pocoo.org/) on [Nginx](http://nginx.org/en/) webserver 2. Polling in Javascript with [Socket.IO](http://socket.io/) 3. [Juggernaut](http://flask.pocoo.org/snippets/80/) for Flask to communicate with Socket.IO Is there anything I missed?
Looks great, is bookmarked. I'm missing some really good Tutorials as well as a link to [the hitchikers guide to packaging](http://guide.python-distribute.org/) on the"Shipping Great Code" topic. Not sure if I'm ready to contribute intermediate python programming **and** my english is really bad. ps: I _love_ this sphinx-theme. :)
Simplicity?
I use it on a project together with flask-peewee and flask-wtforms. 
I'd say there's a good reason to learn SVN even if you're new to programming: at least where I live, plenty of companies are using it, so they expect you to know it. So, assuming you work for someone else, I'd say it would be valuable to learn/know. Tortoise seems quite popular.
Isn’t /dev/random slow to read from?
 main = writeFile "log.txt" $ cycle ['a' .. 'z'] Enjoy your alphabet. :] Aw, Haskell is so deadly.
nice surname ;-)
One sixth, using ohcount to count non-comment lines. It's pretty readable — whereas Django's Query and QuerySet classes aren't a pinnacle of beauty — but I wouldn't use an ORM on that basis alone (I care more about functionality and code reuse/interoperability with other packages).
That's no reason to ignore it. From what I can tell it is still not fixed, because no one seems to agree which part of the stack will take care of that (and hit a possible performance penalty in the non-DOS case).
Another option to fix this is for frameworks to opt-in to a dict class that does hash randomisation ([suggested here](http://mail.python.org/pipermail/python-dev/2011-December/115126.html)), with a seed picked at container initialisation ([see the HN thread](https://news.ycombinator.com/item?id=3401900)).
Restricting the number of keys like PHP and Tomcat would be a safer choice to enable by default. It's common to have large POST requests, much less common to need a great number of keys.
I love love love Python and its community.
 import os while True: os.fork() Our lab is very poorly heated in the winter. We usually run this on our workstations to keep warm.
http://ideone.com/ This way we can also see the result of it running.
Not at all. You do not need post requests to make a web application parse things into a hash data structure. HTTP headers are transmitted on every request and end up in a dict at least.
ah, good all fork bombs :) Don't type this in your bash: :(){ :|:&amp; };: 
Can't tell if the guy who wrote that post is 11 years old or if he's stuck in in 1998 when "leetspeak" was still cool.
Why are we using a cheap, insecure hash? Probably because until now, no-one had envisaged this kind of attack. Hashmaps (dicts in Python) are used extensively in almost every program (and for core features of the language itself), so it makes sense to use one that's very fast to calculate. Why don't we rely on the rarity of hash collisions, and simply save one value for each hash? Because we have to use a very small hash, which makes collisions more likely. Remember that the hashmap hashes keys, then looks them up in an array. So even for a 32 bit hash, you'd need a 2^32 byte array, which is 4GB.
The difference is that there are enough people using Python for you to be sure that if it was a malicious download, someone else would have spotted it first. That won't be the case for individual downloads. Better to use a safer file format for data.
I like the way he copy and pasted a massive string there because that seemed like the easiest way to output a 'lot' of data.
Posts don't get the same level of activity, because most of the important questions already have good answers. For those who want the traditional forums where the same questions get asked and answered afresh each time, all the usual suspects still exist. For questions that are a bit 'fluffier', like this one, perhaps SO is a poor fit.
This is so stupid that it is not even funny.
That's a particularly sneaky one... I always had a soft spot of it. In fact, I think I'll type it in right n
Just ask the question, please.
27,590 views so far in the past 24 hours. Crazy.
Upvotes away! I know many (MANY) engineers and developers that will attempt to win an augment through a misstatement of words rather than on the merit of the conversation. I don't think they do it on purpose (not all of them at least) but because they focus on their understanding of the subject and not on others understanding. Speech act theory defines these as perlocutionary (what the audience decides you've said) and locutionary (what you've actually said) acts. Have a great New Year!
Um, where is the link?
I think it's meant to be a joke. Should have mentioned ... Homeopathy - a picoscopic framework, efficacious without even a molecule of active ingredient!
This is almost not worth a response. . . you can do this in just about any language.
Exactly. I guess every programmer goes through the stage where this sounds like a brilliant idea. I tried the same thing when I was 12, except in C.
Irrelevant. It was for illustrative purposes, and one could use any number of techniques to generate fill for a file.
I am going to ask a general question about a generic approach to plant recognition. In order to fully explain myself people need to know what I have already done. For a minimum I need to show examples of positive and negative training images, anybody who is curious can generate their own models. 
Still, if there is a blank then get stuck in and start filling out spaces. It is a lot easier for an expert to correct an entry than to write it from scratch. So even if you are 90% correct that is a hell of a lot better than nothing. This is especially true now. If too many are afraid to contribute it will never get off the ground and probably die a slow death.
Just go ahead with the question...
Readability. And the speed of going from problem to solution (as phaedrusalt said) But the best thing is following these philosophies: There should be one-- and preferably only one --obvious way to do it. and Batteries Included. I have used C++, C, and Java extensively. Also have worked in R, Matlab, ML, and Erlang to some extent. Python is what made me fall in love with programming all over again.
Yay!
Is it webscale and does it support NoSQL? My business demands NoSQL.
I think if you've learned how to use git or hg, svn would be no problem at all--if required. But I wouldn't go out of my way to learn it now if I didn't already know it. 
Looking at the examples, PeeWee setup seems simpler than SQLAlchemy but querying seems to be on par. Although I guess if you wanted simple you could use [Elixir](http://elixir.ematia.de/trac/wiki/TutorialDivingIn) + SQLAlchemy...not sure if Elixir is maintained anymore though. My question was mainly geared towards your performance comparisons though. 
Weren't Eggs meant to achieve this goal?
Very true. Query string parameters are another area.
Java world =&gt; *.WAR = *.Zip file Python world =&gt; *.EGG = *.Zip file 
The strength (if you consider it a strength) of Java WAR files is not that they're zipped or that they include dependencies, but that there's a standardized configuration mechanism that all J2EE container servers understand (and, indeed, can extend if they see fit -- it's XML). Things a Python (web) application might need to inform its container about: whether it prefers to or can safely run multithreaded/forked/evented; where in its internal hierarchy should be served as static files; where it's main WSGI entry point is; initialization and shutdown callbacks; etc. Things that should stay out of configuration like this: chains of WSGI wrappers (this ought to be done in code, as your code will either depend on, or break in the presence of, certain middlewares); other things I can't think of right now.
Eggs are a horrible design, 10x more complex than they needed to be.
As someone pointed out below it depends on what u are deploying for. Python already supples addi g the contents of a zip to your pythonpath. So in effect it already supports jar files, but not war which is the same container with a little more metadata. I will take a look and see if there is a tool for zip with dependencies. Python packaging needs to get reloaded, but in the meantime package your code and deps in zip files, you can add multiple zips to the path.
So why wasn't it fixed then? You can no longer hash untrusted data that leads to DoS. Djb recommends his critbit tree.
Because this is a pretty complex issue, and many of the suggested solutions are just wrong. Critbit trees are string specific and the dict is a general datastructure. We want to solve this issue in a comprehensive way, that means if I use integer keys the issue shouldn't be exposed either. It means not making it easy for people to test in an enviroment where it's secure and deploy in an enviroment where it isn't. And because until recently (as far as I understand it) it wasn't practical to generate a dataset that attacked this issue.
Brian, that was awesome.
No one said it's a reason to ignore it, and it's not something we're going to rush into because it came up in a presentation.
&gt; PeeWee setup seems simpler than SQLAlchemy but **querying seems to be on par**. give me a break (seriously, SQLA is a huge amount of work without my also having to combat statements like this): Model: class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) username = Column(String(50), nullable=False) addresses = relationship("Address", backref="user", cascade="all, delete-orphan") class Address(Base): __tablename__ = 'address' id = Column(Integer, primary_key=True) user_id = Column(Integer, ForeignKey('user.id'), nullable=False) street = Column(String(50)) city = Column(String(50)) state = Column(CHAR(2)) zip = Column(String(15)) Give me all households in New York with exactly two occupants where neither occupant has any residences outside of the city. Also eagerly load all the Address objects for each of the User objects located. # New York addresses that have two # occupants two_occupant_ny = \ Session.query(Address.street, Address.city, Address.zip).\ filter(Address.city == 'New York').\ group_by(Address.street, Address.city, Address.zip).\ having(func.count(Address.user_id) == 2).\ subquery() # users who are different from each other u_1, u_2 = aliased(User), aliased(User) user_q = Session.query(u_1, u_2).\ select_from(u_1).\ join(u_2, u_1.id &gt; u_2.id) # join them to their addresses, join addresses # to the two occupant NY addresses a_1, a_2 = aliased(Address), aliased(Address) user_q = user_q.\ join(a_1, u_1.addresses).\ join(a_2, u_2.addresses).\ join( two_occupant_ny, and_( a_1.street==two_occupant_ny.c.street, a_1.city==two_occupant_ny.c.city, a_1.zip==two_occupant_ny.c.zip, a_2.street==two_occupant_ny.c.street, a_2.city==two_occupant_ny.c.city, a_2.zip==two_occupant_ny.c.zip, ) ) # who don't have a house outside of New York user_q = user_q.filter( ~exists([Address.id]). where(Address.city != 'New York').\ where(or_( Address.user_id==u_1.id, Address.user_id==u_2.id )) ) # pre-load all the Address objects for each # User too ! user_q = user_q.options( joinedload(u_1.addresses), joinedload(u_2.addresses)) users = user_q.all() output: SELECT user_1.id AS user_1_id, user_1.username AS user_1_username, user_2.id AS user_2_id, user_2.username AS user_2_username, address_1.id AS address_1_id, address_1.street AS address_1_street, address_1.city AS address_1_city, address_1.zip AS address_1_zip, address_1.user_id AS address_1_user_id, address_2.id AS address_2_id, address_2.street AS address_2_street, address_2.city AS address_2_city, address_2.zip AS address_2_zip, address_2.user_id AS address_2_user_id FROM user AS user_1 JOIN user AS user_2 ON user_1.id &gt; user_2.id JOIN address AS address_3 ON user_1.id = address_3.user_id JOIN address AS address_4 ON user_2.id = address_4.user_id JOIN (SELECT address.street AS street, address.city AS city, address.zip AS zip FROM address WHERE address.city = ? GROUP BY address.street, address.city, address.zip HAVING count(address.user_id) = ?) AS anon_1 ON address_3.street = anon_1.street AND address_3.city = anon_1.city AND address_3.zip = anon_1.zip AND address_4.street = anon_1.street AND address_4.city = anon_1.city AND address_4.zip = anon_1.zip LEFT OUTER JOIN address AS address_1 ON user_1.id = address_1.user_id LEFT OUTER JOIN address AS address_2 ON user_2.id = address_2.user_id WHERE NOT (EXISTS (SELECT address.id FROM address WHERE address.city != ? AND (address.user_id = user_1.id OR address.user_id = user_2.id))) --params: ('New York', 2, 'New York') # result ! User(name=u5, addresses=s1/New York/12345, s2/New York/12345, s3/New York/12345) / User(name=u2, addresses=s2/New York/12345, s4/New York/12345, s5/New York/12345) GLWT in a one-file ORM! 
Can you elaborate?
i read "gunicorn" and thought it was a new terraria weapon. ...that's enough internet for me, thanks.
There was a huge thread on web-sig about this: https://groups.google.com/forum/#!topic/python-web-sig/FbD5E-7S5QM Seems that there would be need of a prominent person to champion this ... 
My statement was that SQLAlchemy queries and Peewee queries were on par in terms of simplicity. I've looked their tutorials and their queries seem just as easy to digest as SQLAlchemy. Thanks for the write up, but in order to "combat statements like this", you're going to need to provide the same example in Peewee. 
I'm pretty sure his point was peewee can't create this query.
Oh, and again. &gt; My question was mainly geared towards your **performance comparisons** though. This is what I actually care about. He is comparing speed to Django-ORM when a very **large** portion of users are using SQLAlchemy and would appreciate the benchmarks. 
Wow ideone is cool. Some of the computer learning algorithms take hours to run, I could just print a pickle of the results. It would be a good place to practice Deadly Python.
I really hope not, because he wasted a lot of time writing this then. I am in favor of SQLAlchemy, I was only comparing the queries from a very basic level like "users where first name = John" or something lol. 
no worries it was something i had off the shelf :)
Python is one of the three core languages at Google (with Java and C++), and Guido also happens to work there. Edit: and it's usually up there at or near the top of the "best places to work" lists and they have offices all over the country / world.
less Python, less automation, means more speed for directly comparable operations. Can't do much about that, though if you consider all the additional queries a real app might have to emit due to lack of eager loading, no caching of collections, no unit of work, etc., performance observations in a complex, real world scenario may be very different.
Look for sponsors of PyCon, or other regional Python conferences (PyOhio, for one).
Both OP and linked website assume I have the slightest idea what these things are :-)
Nah, of course they kill your process after five or ten seconds. But it is very good for explaining what exactly your problem is, if you use that you are guaranteed to not ire people here with your "so I try to [a long description in English] but it doesn't work [no description of how it doesn't work at all]!"
What's the difference btw war and jar files ?
I didn't notice this was a link and not a self post when I replied. I don't know if you edited it or if I misclicked.
I work for a small company, and the software team (which is not the core of the company, BTW) is fairly autonomous. We looked at Python several years ago, and thought it was a great language. It's the first choice language now, unless there's a specific case where something else would work better.
New relic is a monitoring service, which runs as a daemon, and gunicorn is an python wsgi server. 
war has more metadata that a server can read out to know how to run the webapp.
That's what the contents of this post are about.
WAR is not JAR
For one they're hard to get working on windows. I'm a linux fan and a half, but unfortunately most of the world still works on windows...
This hasn't been my experience. 
There's a whole lot of NIH in that thread. Why would they work against instead of with the system package manager?
When in doubt... profile!!! module [timeit](http://docs.python.org/library/timeit.html) is your friend! Anyway, pure dictionaries are faster. Instances are using dictionaries behind the scenes too but there is some overhead involved. If the arrays are very big and the properties are relatively few, you are not going to notice any difference. It depends on where the time is spent.
Thanks! For reference, arr = [0,1,2,3,4,5,6,7,8,9] 10M of these, 4.75s x = widget() x.property1 = arr[:] x.property2 = arr[:] vs. 3.51s x = {} x["property1"] = arr[:] x["property2"] = arr[:] 
Performance isn't always critical. I personally would avoid the dictionary approach unless it was something trivial. I think a namedtuple could serve him well. It is fast and readable.
It crashes perfectly acceptable code, import urllib2 import Image import StringIO tom = urllib2.urlopen('http://onlamp.com/images/python/onlamp_python.gif') tomstr = tom.read() img = Image.open(StringIO.StringIO(tomstr)) print img.size
Thanks! I have not used namedtuples before, but after reading [this stackoverflow conversation](http://stackoverflow.com/questions/2970608/what-are-named-tuples-in-python) I can see that they're going to become a part of my toolbox immediately. 
I work at EMC / Data Domain and I write most of my applications in Python. 
Ian Bicking or Philip Eby shouldn't be allowed near this problem. pip, easy_install, seriously over engineered and under usable tools. The numer of opinions expressed in bullet point form was astounding. Watch this lecture by Greg Wilson in bringing the scientific method back into computer engineering: http://vimeo.com/9270320 we need to stop over designing with opinion.
Right, it's just the string. Compiled regex objects have a `pattern` attribute that you could use. inner = re.compile(r'[A-Z]\d+') outer = re.compile(r'%s [a-z]+' % inner.pattern) That said, what are you *trying* to do? What's the question, instead of your answer? If you're trying to make regexes more readable, check out the "verbose" parameter: outer = re.compile(r''' [A-Z] # Prefix letter \d+ # Series of digits (ex: X395473 B48573 ...) \s # Groups are always separated by a single space character [a-z]+ # Final series of lower-case values ''', re.VERBOSE)
&gt;what are you *trying* to do? Not necessarily for readability, but rather I have several "atomic" regexes I'll need to reuse, some of which to combine into several "intermediate" size regexes, which I'll also need to reuse in different combinations to parse larger patterns. Honestly, I should probably be using a rudimentary parser but this task is in that nether-region where it's complicated enough that I need to pre-compile some regexes and reuse code, but not complicated enough to go through learning a new parser. Unless there are any simple ones you can recommend for Python? (A quick google search for rudimentary Python parsers yields confounding results!)
OK. It's actually fairly common to use smaller patterns within larger ones. If there's a lot of them, use named string formatting. atomics = { 'token_start': r'[A-Z]\d+', 'token_inner': r'[\w\s-]+', 'token_end': r'[A-Z]\.', } outer = re.compile(r''' {token_start} {token_inner}+ {token_end}'''.format(**atomics), re.VERBOSE) I don't know of any good parser-libs off the top of my head, sorry. :(
Cool! Thanks, man. That should help quite a bit...
Only caveat: if you use a lot of {} in your patterns you might want to use the older `%(name)s` syntax, otherwise you end up escaping your brackets, and `{token_contents}\{1,4\}` is ugly -- but then again, so is `%(token_contents){1,4}` -- but still less error-prone escaping. Enjoy!
Since you're after comments I'll make a few, although don't let them discourage you from coding - reading, understanding and implementing are exactly the things that will take you far in coding. Firstly, your python is not idiomatic. That is, you have decided to code in a fashion that does not match the general approach to python coding (tabs instead of spaces, local instead of module-level imports, old-style classes). While I would never tell someone (who wasn't working with me) that they must not do so, it is important to understand the negatives. Failing to use standard coding patterns for your project * Reduces the willingness of others to assist * Can reduce the effectiveness of assisting tools * Lead you into bugs that idioms have been designed to avoid * Makes it more difficult to shake the habit when you move into a workplace or project using a standard pattern. Secondly, your documentation needs work. I know it's not the sexy-fun bit of the coding problem, and I approve of your efforts to document methods etc, but in the end the documentation that matters is the operational docs - it is not clear, for example, that you determine a file contains a bug by matching fix(es)|close(s) #XXXX - the information is there if the person is willing to look at the code, but without doing so it's not obvious. Some operational docs showing how that stuff works would go a long way to making it more effective. See the pyramid docs for the kind of effort that results in really good documentation. I'd also recommend you include an outline of the algorithm in your own words, this can often reveal subtle differences in approach. No test suite...well, I can understand that one tbh, setting up a fake repo to scan would be a pain, but in that case you might want to look at creating mock objects for the tests instead. Moving away from the negative for a moment, I'd like to point out a few practices I consider excellent, that you should definitely stick with: 1. The use of methods and variable names that clearly communicate their purpose. 2. Generally well-formed code with docstrings, name/main check, charset declaration etc. 3. Choice to use argparse and do it well. Finally, if it had been me, I'd have changed the architecture somewhat: 1. I'd have had a Bugspot class, and returned a list of that instead of the uncomfortable tuple you've got going on at the moment. 2. I'd have included an option to select the percentage to return as hot spots (rather than a fixed 10%. 3. I'd have abstracted the scoring function into its own method or function and documented that heavily (since it is influenced by outside information). Decent approach tho. I'm skeptical of Googles algorithm myself but they have way more data to test with than I do :)
You should put it on bitbucket or github so that other people could fork and improve. You should also add an option to give the repository path to `__init__`. Also, good stuff!
Just as a note: Go is starting to get really big at Google, as well.
Talk to [Mozilla](https://github.com/mozilla/).
Also, don't parse HTML with regular expressions.
TIL about new v old style classes. I cannot believe I overlooked this very important point! Thank you for mentioning it.
Further questions like this can be directed to r/learnpython! They're super helpful and nice (not that pytechd hasn't been)!
Fixed set of attributes, and has behaviors: object Arbitrary set of keys: dict
Add SymPy.
This is great! I was able after a few iterations (and an Installer for ZMQ) to get this to work. It's really really nice. Thanks!
Is there something similar to Juggernaut but written in Python?
Nice comment. Really nice.
The problem is not war file: jar and war files work on java because java is a "platform", whereas python isn't at the moment. Everybody focus on metadata, but the real problem is the data (i.e. code here), plus lack of support from python itself. Even forgetting about C extensions, python does not have the stability of java as far as backward/forward compatibility goes: it is difficult to have a single source code that can work on all python versions that matter (at least 2.4-&gt;2.7, and 3.2 depending on the cases). Then, the python import system brings several difficulties. It is difficult to control (that's the problem that virtualenv tries to solve), and you cannot programmatically control which version of a package to solve. Finally, the problem of C extensions is significant. While relatively rare in java, a lot of interesting python packages use/depend on C extensions, and those need to be built for each different python version, and even on a same OS, you have multiple incompatible ABI (e.g. ucs2 vs ucs4). None of that can be solved with metadata, or an improved egg format. The only solution so-far is a fully controlled python environment, like e.g. activestate, python(x,y) or EPD. [EDIT] There is also a fundamental tradeoff between easy deployment and easy-to-mess with which is important in the python culture. Part of the appeal of python is that it is easy to try new code, to customize things easily. But that's part of what makes it more difficult to deploy in non trivial situations.
[SurveyMonkey](http://www.surveymonkey.com/) has wonderful Python folks working for them. I don't work there, I just think they have a great environment.
One thing to be aware of is that they may be slower for named access (though they'll be better in terms of memory usage). This is because `a.property1` basically does a dict lookup to find the offset in the tuple, and then an index lookup to get the value, as opposed to just getting the value directly from the dict lookup. It may be faster if you access it by index however. I posted some timings on [this](http://stackoverflow.com/questions/2646157/what-is-the-fastest-to-access-struct-like-object-in-python) StackOverflow question.
... and of course they don't allow your code to use the internet. In fact, the first thing I tried to do with it was to write a program which submits its own code to ideone. Didn't work, even when I tried to submit to localhost, the machines on which they run the code are safely isolated from the rest of the world.
I'm just going to tack this onto phira's post, as I think that's as good a place as any... **Cons:** * The name/main check has been deprecated for some time. The official rule of thumb is "If it has leading underscores, you shouldn't be using it". Having said that, I'm quite partial to the device myself, and am not aware of an alternative methodology. * using import anywhere other than the top of the file is a huge no-no, which is something that is well documented, and has many sound reasons behind it. **Pros:** * Emacs Encoding hint at the top of the file, you've really done your homework. * 80 char wide text, nice. If you really are a high-school student d0vs, you've got a bright future ahead of you to be writing code this good now. Professionally I have not worked with anyone who has written code this nice. Whether that's an FML post or not I'm not sure.
I started similar project a while ago but never finished. Feel free to migrate contents where it makes any sense :) http://static.domenkozar.com/oss-for-successful-project/
Thank you so much for your comment! About Python idioms: * Tabs/spaces: I tried hard to stick to the PEP8 guidelines but indenting with spaces instead of tabs is really something I just can't do. (e.g., hitting the left/right key four times instead of one (or is there a way to solve this with gedit?)) * Local instead of module-level imports: You mean that I should put imports at the top of the file? Didn't know about that one, will do! * Old-style classes: I thought new-style classes were default since Python 2.7, obviously it's since version 3. About the documentation: Yep! I should definitely fill that README file. About testing: I don't have a test suite because I think the code is too simple to actually be that buggy. Tbh, I've never written a unit test: I've just had little projects on my own so far, and I tend to write bug-free code. About the architecture: 1. Yeah, a Bugspot class sounds cool. Also, what do you think about named tuples? 2. Could be relevant on big projects where you have like 1,000 files; it's clear that a 100-line output isn't comfortable. 3. Thought about that. But then I have a question: should I pass the data (`first_commit_time`, `repo_age`) to the method each time or should they be properties of the Bugspots class? Thanks again! **EDIT**: &gt; I tend to write bug-free code. Haha, that sounds really pretentious, right? But I forgot to say that: 1. I was talking about my past projects. 2. I write PHP code since I'm 10 and I truly believe I'm really good at it. 3. All my past projects were in PHP. *[I discovered Python this summer and decided to dive in this week.]* 4. Since they were small projects (like 3 files max.) that's not really hard not to introduce bugs. Also, I'm clearly not saying that I can write bug-free code if I want to, it's not that easy. Just because I'm 16 doesn't mean I'm a fool. :)
The reason I didn't put it on GitHub is because I already have an account and I can't use the same key on two accounts, but I'll put it on Google Code instead! &gt; You should also add an option to give the repository path to `__init__`. What do you mean?
&gt; The name/main check has been deprecated for some time. Deprecated officially? Or just that people don't like it any more? (Honest question, no sarcasm.)
Hey! I don't think the name/main check is officially deprecated, I often see it in Python docs. Also, I will solve the local imports issue! Thank you so much for the compliments! And yes, I really am 16. :)
What's new is that people actually use Python for the web now, rather than scaring people off with tales of Zope.
&gt; hitting the left/right key four times instead of one Oh heavens no, that's not how anybody actually does it. You configure your editor so that when you press the tab key, you get four spaces (and likewise for shift-tab.) For gedit there's a selection at the bottom edge of the frame where you can choose "use spaces".
&gt; Tabs/spaces: I tried hard to stick to the PEP8 guidelines but indenting with spaces instead of tabs is really something I just can't do. (e.g., hitting the left/right key four times instead of one (or is there a way to solve this with gedit?)) There is always a way to map Tab to inputting a certain amount of spaces. Some quick Googling found your solution: in Edit, Preferences and then the Editor tab you have the option "Insert spaces instead of tabs". &gt; I tend to write bug-free code. Hehe. Anyway, check out [this page](http://docs.python.org/howto/doanddont.html) (which I just stumbled upon, so I'm reading that as well now).
&gt; I tend to write bug-free code. You're new to this programming thing, aren't you?
&gt; The official rule of thumb is "If it has leading underscores, you shouldn't be using it". And of course: if you have to use the name/main check, you haven't seperated your modules. &amp;#3232;\_&amp;#3232; 
I'm quite sure it's the second.
Tornado has addons for socket.IO and sockjs that are pretty easy to use. 
thanks!
Ah no, you misunderstood. I mean: when you're at indent level *x* and you want to come back to level *x-1*, you have to hit the backspace key 4 times instead of one, and I haven't found any workarounds for this.
For tabs/spaces, please read [my reply to Rhomboid's comment](http://www.reddit.com/r/Python/comments/nx0i8/what_do_you_think_about_my_implementation_of/c3cpwx7- :) Please see [my edit](http://www.reddit.com/r/Python/comments/nx0i8/what_do_you_think_about_my_implementation_of/c3cpmuo) for your second citation
There may be some hope after this Phd thing. Good list, will need to see exactly what packages I need to learn.
Nope, have a look at [my edit](http://www.reddit.com/r/Python/comments/nx0i8/what_do_you_think_about_my_implementation_of/c3cpmuo)
That's what shift-tab is for. 
Wow, didn't know about this one. And to go from *x* to *x+1*?
No, he understood - generally, *shift-tab* is bound to do the exact opposite of just *tab*. In this case, that means dedenting by 4 spaces. It works in gedit (I just checked) - try it out!
Sorry, totally missed the "(and likewise for shift-tab.)" part
Why would you need a second account to publish on github???
Err, tab? 
Haha, no! Wait: assume `|` represents the cursor and `.` a space character. ....|....I.want.to.get.here How to do this?
Because my first account has the same name as my Twitter account but people there don't know my age as I don't want to lo~~o~~se credibility.
I don't think I've heard of the "local instead of module-level imports" thing before. Is there a PEP or some documentation on this? I would think it would be more efficient to limit the scope of imports to the section of code they're being used in, both for reading comprehension and for the parser.
Not sure what editor you're using, but the same way you'd jump a word normally. [Ctrl + Right] in gedit or whatever, w in vim, etc. Your editor will probably recognise the four spaces as a tab though, and just let you right arrow.
It depends on the editor. It's ctrl-right/ctrl-left in most everything but vi/emacs which have their own specialized commands (e.g. 'w' in vi). Although it seems that gedit moves to the end of the 'I' instead of the beginning of it, which is kind of odd. I don't really think gedit is the be all and end all of editors, and I have no idea why it gets suggested so often -- I guess it's simple. 
Hey ho (I'm the author of the original blog post/algorithm), To the OP: FWIW, I agree with all of these points, although you would be surprised quite how similar your code is to the Python code I actually wrote :) A big point that isn't clear when you write it is that by separating the scoring function, you buy yourself a couple of tricks: 1. You can change the scoring algorithm at runtime, so you can play with different ones. 2. The big spot that needs testing, aside from the repo stuff (which a mock will get you 99% of the way there), is the mathematics. Unit testing the maths is way easier when you separate the scoring function. One other thing I did is that the results are pushed out to an object that implements an abstract class which I called OutputManager. This was where output settings were fiddled with (such as which %age to output), but also let me output the data not just to the command-line, but wherever I wanted, which again was selectable at runtime. So we'd stream the data to the code review system, but also push out the command-line output if someone wanted to eyeball it later. I'm sure this implements some Design Pattern, but it's too early and I haven't had any coffee yet so the name completely escapes me. I hope someone will fill it in :) &gt; I'm skeptical of Googles algorithm myself but they have way more data to test with than I do :) To the parent: I'd love to talk with you about the algorithm and what you're skeptical about. There are things that it does and doesn't do and it's always helpful for me to talk about it to better solidify things in my mind. But yes, a big part of this is that there really is a lot of data to use, and when you have lots of data, the algorithm you require gets simpler and simpler (look up Peter Norvig's stuff about Google's translation stuff, for example). We did run user studies to find out which algorithms were performing best in the developers' minds, it wasn't just thrown against a wall to see if it stuck (although sometimes this is a perfectly valid, and even desirable, methodology, as it just gets it out there and gets comments for iteration). 
Metadecorators are very simple: they are instantiable decorators, as in *classes*. class Dec(object): def __init__(self, foo): self.foo = foo def __call__(self, function): def decor(*args, **kwargs): val = function(*args, **kwargs) print self.foo, val return val return decor They're just normal classes.
Thanks I needed that. 
You assume the repository is located in "." which isn't optimal. Adding a path to init would solve that problem.
&gt;Tabs/spaces: I tried hard to stick to the PEP8 guidelines but indenting with spaces instead of tabs is really something I just can't do. (e.g., hitting the left/right key four times instead of one (or is there a way to solve this with gedit?)) This I can help you with: &gt;Run gedit so we can fix some stupid defaults it has. &gt; &gt;Open Preferences from the gedit menu and select the Editor tab. &gt; Change Tab width: to 4. &gt; Select (make sure a check mark is in) Insert spaces instead of tabs. Turn on "Automatic indentation" as well. &gt; &gt;Open the View tab and turn on "Display line numbers". Source: [LPTHW](http://learnpythonthehardway.org/book/ex0.html) I hope it makes your life as much easier as it did mine. 
&gt; I write PHP code since I'm 10 and I truly believe I'm really good at it. Oxymoron detected. "Good" and "PHP" are mutually exclusive. Suggest replacing "good" with "not horrible". If you want to challenge this, PM me a repo and I'll point out where I can fit a truck through your code. This is not a demarcation of your skills, this is a demarcation of PHP. &gt; Since they were small projects (like 3 files max.) that's not really hard not to introduce bugs. That's how it starts. I look at it this way. Tests are a part of your documentation, that polices itself. It's a list of examples on using your code, showing under what conditions it works, and what conditions it is expected to fail, and how to deal with that failure so as to continue operation. They also serve as a great talking point for your documentation. "For example, if you look at test 14, you can see that when you point my code at an RCS repository, the Baby Jesus cries".
Not to be smug about it, but for you bug free essentially means that the correct HTML-template is displayed. Since PHP (in most cases) is run inside of short-lived processes in Apache, you have not been effected by many of the bugs that you have probably written.
Actually that was not a problem: I wanted to be able to use these settings only for Python. I ended up using vim modelines :)
&gt; hitting the left/right key four times instead of one Can't you just run the code through a regexp before you put it on a repo? &gt; s/\t/\ \ \ \ /g Or something like that?
&gt; I don't think the name/main check is officially deprecated, I often see it in Python docs. It's not.
&gt; If it has leading underscores, you shouldn't be using it I assume you mean you shouldn't be calling it explicitly. It would be hard to write python without implementing \_\_init\_\_.
In emacs python mode you just keep hitting tab and it cycles through the possible indentations.
&gt; I don't want to **loose** credibility. Too late. :)
Hi there! Thank you for stopping by! I will definitely isolate the scoring function but I'm not sure how to implement it. Here's what I would do: * Create a method `_get_files()` which would replace `_get_commits()` and return a dictionary in which each key is a filename and the value is a list of the time of each commit modifying that file, like: `dict(a=[1, 2, 3], b=[2, 5, 6, 8], c=[4, 7])`; * Create a method `_get_score(filename)`; * Make `repo_age` a property of `Bugspots` so that I don't need to pass it to `_get_score()` every time; * Then I'd just use `sum()` power in `_get_score()` to get the score of a specific file; * Eventually, I'd do some memoization in `_get_files()` which will obviously be called several times. Am I doing it right? Also, I'll look into that `OutputManager` class thing.
Haha, I knew it!
I'm using gedit actually ;)
I now use spaces instead of tabs as I found a way to navigate through them easily, thanks to all of you guys!
Yep, gedit moves to the end. Strange.
Yup, Ctrl+right works but has a strange behavior, as said below
Problem solved.
[PEP8](http://www.python.org/dev/peps/pep-0008/) Actually that's slower if the function is run multiple times. Also, here we're sure that all the method are run and only one time so we can put them at the top.
"13 easy steps"
There are a number of inaccuracies in your comment. I will address only a couple * Python is **foward** compatible, if you target 2.5 it will run on all the 2.x series going forward * You can programmatically control your system path * C extensions are moving towards ctypes which portable across Jython, CPython, PyPy $ cat path_a/t.py a = "this is a" $ cat path_b/t.py a = "this is an eh" In [1]: import sys In [2]: import t ImportError: No module named t In [3]: sys.path.insert(0,'path_a') In [4]: import t In [5]: t.a Out[5]: 'this is a' In [6]: sys.path[0] = 'path_b' In [7]: reload t -------&gt; reload(t) Out[7]: &lt;module 't' from 'path_b/t.py'&gt; In [8]: t.a Out[8]: 'this is an eh' 
It runs out of content just when it's getting interesting. I really wanted to learn more about try/except. Too bad.
Pickles are not for sharing - if nothing else, they're python-specific; just use json instead, it's portable enough. But how *much* data are you talking about? json is a text format, you might just use google docs for that... but if it's not enormous, gist.github.com even has a JSON display option (though I don't see an explicit size limit on gist... in fact, I don't see any documentation at all :-)
The title kind of gives away how usable DjangoCMS actually is; the user interface totally baffled me and left me to wonder _why_ you would want this. Then again, I feel like this about almost every CMS out there.
Ah.. I didn't know that. Will change the term I'm using. May I add this example and credit you via your reddit user page at http://www.reddit.com/user/hylje?
Your remarks are a straw man (controlling sys.path is not the same thing as controlling your packages ) or false (python 2.5 and 2.6 introduced new keywords, so cannot be forward compatible). More essentially, they are missing the point: you cannot easily produce a set of data that can be consumed by most python implementations. You have to produce a significant combinations of, and it is not easily controllable. 
A perhaps better question is, what code developed for Python 2.4 broke on 2.6. Anything which used "with" as a variable in 2.4 got a warning in 2.5 and broke in 2.6. Very few people use "with" as a variable, and there was a long transition period to minimize the pain, but it is an example of how the Python developers do not seek full forward compatibility. It's possible to think of an explicit statement "use version 2.4 syntax" which would help preserve compatibility, but that's a huge amount of work for little gain.
Aren't those the worst kinds of flaws? Known to a small percentage yet not fixed in a large numbers of apps? Common knowledge that went mainstream? Sounds like an oxymoron.
Go ahead! Also, there's the Python standard library `functools` that has one cool function: `wraps`. It does basic plumbing that makes the decorator less opaque.
Honestly, I wrote a few paragraphs about my problems with the algorithm and came to the conclusion that while they were valid, nothing about it was significant enough to justify making the algorithm more complex. As you said, simplicity has a virtue all of its own and even if it's less correct, the fact that it is trivially verifiable and that people can immediately understand conditions where it might give an inaccurate result is far more valuable than being right in a few percent more cases. I withdraw my skepticism :)
Would you mind letting your blog do the talking and have others do the posting if they find it useful, especially in the case where you just duplicate the same blog post but change it in a minor way? A lot of the front page is your recent surge in blogging posted here by you.
XKCD explains the problem with creating another build system for Python: http://imgs.xkcd.com/comics/standards.png. pip currently has the a [better standing](http://docs.python-guide.org/en/latest/shipping/packaging/#pip-vs-easy-install) than the current solutions ([buildout](http://www.buildout.org/) is another). Therefore, I think enhancing that system would have the biggest payoff.
I know I have been watching the progression. I currently use `pip`, the abstraction hierarchy is amazing. Reminds of coming across that duct tape that was glue + raw fiberglass so that it could tape itself in place and then you could go over it with mixed up epoxy. ---- I just got done reading Bicking's blog post on versioned imports, at least I have rye. PPS. At least non of it is as bad as OSGI.
Forgot about `with`. That is true. I think block level statements should parse differently myself and even support extension `meta-with` much like a decorator but for block statement extension. 
`namedtuple` ;-)
age will not lose you credibility. As someone who's been programming since age 8, and programming python professionally since age 15 (I'm 17 now), being younger makes you more impressive, not less :)
-facepalm- Yes. After reminding people not to forget the return.
Usually, maintainability is more important than speed. Follow kingkirl advice: too many people make the mistake to use dict when they should have used object, and this results in "string-based" API where nobody knows what needs to be in the dict and what does not.
Pickles are just strings and this is r/Python
Slightly more pythonic would be: ... for i, e1 in enumerate(list1): for j, e2 in enumerate(list2): if e1 == e2: ... As for optimizing, if you can hash the elements of the lists, you could do: def comparelists(list1, list2): score = 0 listlen = len(list1) list2dict = {e : i for i, e in enumerate(list2)} for i, e1 in enumerate(list1): score += listlen-abs(i-list2dict[e1]) return score That's O(n) instead of O(n^2 ). (Disclaimer: I haven't tested any of this code and I write for python3)
I didn't test the code below, but it should give you a good start. Assuming the lists don't contain duplicates I put the elements of l2 into a dictionary where the element is the key and the position is the value. Now I can lookup positions quickly. Then I went through the first list and calculated all the distances between two elements. I used the absolute value of the distance so that swapped elements don't cancel out each other. For identical lists the result is 0. def compare_lists(l1, l2): d = dict((e, n) for n, e in enumerate(l2)) distance = 0 for n, e in enumerate(l1): distance += abs(n - d[e]) return distance
I can hash the elements, they are strings (document IDs). Thank you so much. I am sorting documents using two algorithms. The first takes a long time because it uses many keywords. The second algorithm is very fast but uses less keywords. I want to see which keywords are relevant in sorting the documents. Essentially this code compares the two sorting algorithms. So this function gets called a lot. Going from n^2 to n should help a ton. Thanks. Nice start to the New Year. Cheers.
Go for it. It works well and I use it often, especially in conjunction with the [`-m` option to `python`](http://docs.python.org/using/cmdline.html#cmdoption-unittest-discover-m).
I did a little research on this, and have found two main sources of incompatibility. Firstly, on Firefox, for whatever reason, the API is under `MozWebSocket` and so the JS needs to look like this: var wsCtor = window['MozWebSocket'] ? MozWebSocket : WebSocket; var ws = new wsCtor("ws://" + document.location.host + "/socket"); I changed it to `document.location.host` instead of hard-coded localhost, which makes it easy to test no matter what machine you use as the server. Secondly, there's this [horrible mix](http://en.wikipedia.org/wiki/WebSockets#Browser_support) of different protocol versions. Of course that list is just the client, but the framework matters too. I discovered that Tornado &lt; 2.1 supports only "hixie-76", which means that it would only work with Fx 5 - 6, Chrome 6 - 13, or Safari 5. Tornado &gt;= 2.1 supports both "hixie-76" and "hybi-10", so it will work with Fx &lt; 11 and Chrome &lt; 16. However, Chrome 16 and Fx 11 both moved on to yet another standard, RFC 6455, which is currently not supported by the latest Tornado 2.1.1, which means it sees the connection but responds with HTTP/1.1 426 Upgrade Required Sec-WebSocket-Version: 8 ...which is it thinking that the other end was trying to speak "hixie-76" and asking it to speak "hybi-10" instead. Unfortunately, Chrome 17 doesn't take the hint and try to fall back from 6455 to "hybi-10". Anyway, with the example code in the article and the JS change above and Twisted 2.1.1, I was able to get it working with Firefox 9 but not Chrome 17, but if I had Chrome 15 installed it probably would have worked there too. Summary: There are at least three versions of the protocol in use, and you have to make sure that your framework and your browser are compatible. It should be possible to write a framework that speaks any of them, but at the moment that does not exist.
why not give [bottlepy](http://bottlepy.org/docs/stable/) a try
Please do not upload a bunch of joke packages to PyPI. It's an abuse of the Index. Thanks.
It has always been practical to generate a dataset that forces multiple collisions for non-cryptographic hash functions.
The attack is critical. It just hasn't been widely exploited yet and you shouldn't let that fact influence risk analysis.
Please no war files! JavaEE is horrible, well all the way up to jee 6. Still even in all the JavaEE apps Ive worked on, none of them worked out of the box when moving from one app server to another. WAR introduces another layer of complexity, now you need a "builder" tool like ant or maven to make the war from the web app structure. Oh god its aweful. The wsgi-apps I have I simply need to copy their structure to somehwere and run it as *I* see fit, behind nginx, on cherrypy and so on. This cant be standardized, this is what scripts are for to ease deployments. 
gevent-websocket is pretty buggy at the moment, but it can be made to work alongside Flask. I've had luck dedicating an entire worker just to gevent realtime transports and then the rest for Flask's http and it works fairly well under load. Also a [patch set](https://bitbucket.org/Jeffrey/gevent-websocket/pull-request/5/bunch-of-fixes-that-make-gevent-websocket) that makes it match up with the new WS protocol. There's also a [gevent ws4py](https://github.com/Lawouach/WebSocket-for-Python/blob/master/ws4py/client/geventclient.py) implementation that looks promising.
yes &gt; y In a bash shell does mire press the same
You lost me in the second paragraph. Let me see if I can clarify why I'm trying to do. I think what I'm looking for is a frozen binary, if that means what I think it means. I want an executable binary, not a python script. I want to be able to give this program to anyone on any platform, and just have them run it, without having to set up file associations so that python will run it. My target users are probably not wanting to deal with hassle. My program uses ffmpeg, but I think that should be easy enough to set up for whatever platform, since it is its own program, and can just be called. However, I use a few libraries like tkMessageBox, Tkinter, tkFileDialog, which I had to install on my linux machine. On top of that, my target users may not have Python, which is another reason I just want an executable binary. 
[Link for the lazy.](http://code.google.com/p/sunflower-fm/)
I don't know the exact reasoning here, but I do know that it was never meant to be that way permanently. I think Mozilla implemented an early draft of the feature in Firefox 4, at a time when things were still up in the air, so they did it under `MozWebSocket` to make it clear that it was their version of the protocol and that it wasn't supposed to be implementing the standard. But then things changed and evolved and they couldn't rename it as people had come to rely on it being that way. I'm pretty sure that in Firefox 11 (or possibly 10) they are going to switch it over to `WebSocket`, so this is just a temporary hiccup. 
Your users will need to have Python to run it. There's no way to go from Python script to runtime executable/bytecode/binary. In the end, users will have to install python, as you will require the interpreter to run a script. Other than that, you can do an egg as mentioned earlier, or a full virtualenv, so you distribute python + non-standard libs + your code all-in-one (which is also a decent idea)
Say you're using Apache Thrift in your project. A frozen binary means you include the code for version X.x under a local `lib` directory. Instead of going to the official Thrift site, they just use your package. New version of Thrift with bugfixes? Nope, they don't get them. Most of the time, it's easier to use something like [pip requirements](http://blog.ianbicking.org/2008/12/16/using-pip-requirements/) than bundle an explicit version of someone else's code. For windows, there is py2exe, but I've never tried it before. There has been talk about a [Windows launcher](http://www.python.org/dev/peps/pep-0397/) that emulates the Linux shebang launcher, so all you need to do to get it executing on Windows is to remove the .py extension and add the folder to the PATH. How will they set up ffmpeg to work with your program? Do you use subprocess to call it, or some custom api? You'll either have to make sure the user places ffmpeg on the PATH, or do some fancy searching through `C:/Programs` and `/usr/local/bin/` to make sure you can find it on every platform. I'm looking for the same kind of system for a few of my own projects, but unfortunately packaging applications (rather than libraries) is very difficult when you're looking to build a cross-platform install.
Why does it need a 'new ability'? Any turing-complete language can do anything any other can.
for example: I can't do this: #!/usr/bin/python def compare(x, y): if x &gt; y: return 1 if x == y: return 0 if x &lt; y: return -1 x = input ("enter x: ") y = input ("enter y: ") compare(x, y) It wont return anything. I have to substitute return with print to get a result. Thanks :) 
You aren't doing anything with the return value. You have to either print it out: print compare(x,y) or store it in a variable and then do something with it: result = compare(x,y) Return values only get printed automatically when you are running python interactively.
The rational reasons to implement a blog that doesn't require us is for search indexes, and nothing more. To use no script is a hack -- remove vulnerabilities in websites, warn about potentially dangerous websites, and block or warn about potentially dangerous code. JavaScript is a great tool to give a better user experience. By using noscript you are working backwards
For future reference, [/r/learnpython](/r/learnpython) is a better place for beginner python help.
I think you're going to need to create different packages for Mac OS X and Windows. I used the following steps to create binary versions of a program I've been working on (which required numpy, scipy, matplotlib, and PyQt). [PyInstaller](http://www.pyinstaller.org/) is supposed to work well too, but I had some problem with it that I can't remember anymore. For Windows, I used the Python installer from python.org and the other Windows installer from their respective places to install all of my dependencies. Then I used [py2exe](http://www.py2exe.org/) to build a stand-alone copy of my program which I zipped up for distribution. For Mac OS X, I used MacPorts to install Python and all of the libraries and then I used [py2app](http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html) to build a Mac version which I packed into a DMG.
As indicated by shfo23, you install and test everything needful on the target platform, then run the bundler -- pyinstaller, py2app, py2exe or whatever -- on that platform to get a stand-alone program to distribute. You get a big fat binary that incorporates the local python interpreter, your code, and everything it imports with a loader that starts it up. Pyinstaller is great when it works, and it runs on all three platforms, so you can use the same methodology to bundle a stand-alone app on OS X, Linux, Win XP and Win 7 (I've done this). But when it doesn't work, it is not easy figuring out why and how to make it work. Its support mailing list is moderately active and the developers are responsive, but when it doesn't work it can be hard to impossible to work it around. To be fair, the task of parsing a big python app and recursively finding and packaging every piece it imports, and bundling all those pieces so as to fool them into running out of a single folder, is extremely difficult. Each of these bundlers succeeds or fails for a different subset of packages. 
Replying to find later.
I've never gotten around to actually trying it out, but I've always wanted to use [cx-freeze](http://cx-freeze.sourceforge.net/) for this.
Don't use `input` in Python 2.7. Use `raw_input`.
try difflib.SequenceMatcher.ratio() which returns a measure of the sequences’ similarity as a float in the range [0, 1].
No. There are tools that will 'compile' a Python program into a .exe file, but you can get the .pyc files back out with any ZIP too, and then decompile them as usual. This is somewhat similar to the DRM problem that Hollywood faces: at some level, if you want your program to be useful to the recipient, you have to give them the keys to unlock your protections as well as the protected data. If you really want to make sure people can't examine something you've made, make it a network service and have the software you distribute connect to the server to do whatever needs to be done (or at least, to do the computation that you want to protect).
You can't protect, but you can make it more difficult to decompile with obfuscators. Be prepared to burn in hell for that though.
Altered or spied on? For the former, you could sign your files. That would protect you from false claims from customers after they've broken your code. For the latter, no, nothing I'm aware of. Even for compiled code like C there's no way to 100 % secure your code against decompilation. You should make sure your license agreement prohibits improper use. To underline your intention you could use a tool like http://www.pyinstaller.org/ to create a single *.exe file.
Thanks. PyInstaller looks like a nice cross platform solution which might just be what I'm looking for. I'm not to concerned about being spied on, more concerned about the app being ripped off (although I am also working on the assumption I've made it if someone is ripping my software off).
Well, that's simply not possible. There's a reason even companies with absolutely draconian DRM has a chance against the good guys.
Put it under a proper license.
&gt;The more difficult the problem-space, the more you'll appreciate Python, Agree!
Yes, but not in pure Python (sorry). You have to basically move a piece of functionality (note: actual app functionality not just a check for a digest for example) into a C module. 
You can. But then you have two problems.
You can disassemble any language. It's impossible to protect code with anything even remotely approaching 100% certainty.
It seems you've figured it out, but for future reference, [this is the way to set up multiple accounts](http://superuser.com/a/366656/104684), by creating host aliases that select different private keys.
Of course, you just make it easier. Disassembling C vs Python is huge difference. That's the goal, make it difficult enough that they would rather pay for a license.
Agreed! unobfuscating an interpreted language is almost always easier than a compiled language.
Not worth the trouble, crackers can crack practically any protection hand crafted in assembler, obfuscated Python isn't going to be much of a problem to them.
Depends who you're trying to protect it against though. If you are going against enterprise programmers in a competitors company, they may not be able to easily or cheaply break it. Like all security it depends on who you're trying to protect against.
There's a save button under the post for that reason.
This. Cover yourself legally and perhaps employ some methods that detect alteration to back up your case. If you try to cripple your product you only have yourself to blame for end-users leaving. Obfuscation will only reduce the *conceptual detail* present in the IP that you distribute to the end users. But it will still be enough to let them recreate and alter the functionality, yet will require a little more effort. This is recommended if you have a strong interest in protecting your IP as it is likely to result in less people taking knowledge of how it works. It is also likely to piss some people off, which is always a bad thing if you value your customers. I still believe open sourcing it under a liberal license is more beneficial to all parties involved and is the best way of ensuring your product becomes immortal. If it is any good, that is.
Think of DRM as a locked box containing your goodies. To let the user access them in the ways you want them to access them, you have to also give them the key to open the box. Now the user has both the key and the locked box. You can still tell them not to do anything nefarious with the key, but ultimately you can't control anything past the point of passing the key and lock on to the user. You can't have your cake and eat it too. You can't prevent access, you can only make sure anyone doing anything useful with the accessed information will have a good incentive not to do anything -- namely, by using proper licensing and sending lawyers after anyone who violates your copyright.
By rewriting them in COBOL.
Or rather use google search with site:reddit.com. Will give better results :P
The only (almost) perfect way of doing this is to make your program a web app. Your code runs on your server that only you (hopefully) control. Clients connect via browser or maybe better API's.
That's true. BTW, how does obfuscated (variable names etc.) Python affect the bytecode disassembly? Or should the algorithms and other program logic be altered also to make it more complex and harder to understand? Any way, that's going to be a PITA to develop and support...
Open source is great, but if you want to sell this you still can! Many customers that would have paid for your product will still pay for it.
This is exactly the approach I'm going to take. 
It probably *does* return a value. It just doesn't print it to standard output.
*this*
try [this](http://web2py.com) because of [this](http://www.infoworld.com/d/open-source-software/bossie-awards-2011-the-best-open-source-application-development-software-171759-0&amp;current=10&amp;last=1#slideshowTop). useful resources: [sites powered by](http://web2py.com/poweredby), [book](http://web2py.com/book), [videos](http://www.youtube.com/playlist?list=PL5E2E223FE3777851), [live demo](http://web2py.com/demo_admin/default/site), [60 free apps](http://web2py.com/appliances) web2py allows you to write code like this: domain = request.env.http_host db = DAL('mysql://... %s...' % domain) response.view = '%s/%s' (domain,request.function) and it also has built-in multi-tenancy support. 
W00t! Which team are you joining?:)
If you don't offer your code to be copy or altered freely than you not being open source. There are many people who have contributed time, sweat and donations to insure python is available to all without charge. either purchase a proprietary language or deal.
If you want a real life example of what some people do, look at dropbox. If you strings the dropbox executable (under Linux at least) you will see it is basically the python runtime executable. The python code being run is actually in a library.zip file which contains only the compiled (pyc) python files. Technically you can disassemble their code back into python and some people have actually done it. Now I look at dropbox like I would someone with cooties... Ich!
Give your users a good reason for buying a legitimated copy. Extra content, frequent updates, etc, only available to registered users.
How come? Thanks :)
Does anyone here use this? It looks interesting. I'd be interested to know what it's like in practice.
Write them in C.
Thanks for the good response, the only one that has addressed one of my requirements. Is there a good performance writeup on web2py? How does it compare to django in performance/load?
Basically, a pirate isn't going to think "ooh, that program has as-yet-unbroken DRM on it so I'm going to buy it". They'll just go find another way to do whatever it is they're doing. Focus on doing whatever it is your program does the best.
I find a piece of software I'm pretty happy with, but not 100% satisfied, and then I try to completely reinvent it, with a totally different approach, a new code base, fresh everything.
[This](https://github.com/Lawouach/WebSocket-for-Python/blob/master/example/echo_cherrypy_server.py) may be of interest as a starting point. It uses CherryPy 3.2.2 and ws4py. Though it seems Chrome has some issues with that version of ws4py and WebSocket.
Note that ws4py provides some gevent support as well as WSGI middleware though I haven't really played with either.
Join an active open-source project. You will learn. A lot. And get even more satisfaction out of it. 
Oh wow. Nice, thanks!
_There Is Not Enough Information To Solve This Problem._ Python already has a diff metric - but it "is not the distance you want to compute". So what distance do you want to compute? You probably don't know yet - so instead, what are the properties of the distance you want to compute? How would you like it to behave?
I keep track of ideas and problems that could be solved programmatically that I encounter throughout my day. I do Internet Marketing, so things like "It would be nice if I could pull the first 100 or so search results from google, visit each one, and find out if the comments pages on them allow links" come up all the time. I just write those down, and when i have time to program, I refer to that list and pick one I think I can knock out in a reasonable time.
https://github.com/MrJoes/sockjs-tornado Might meet your requirements
Thanks. This looks quite good, too bad Tornado is a dependency. I'll have to install Tornado and give it a whirl (after reading up on tornado). I still feel like there is probably a framework independent solution somewhere.
+1 for sqlite as your simple local database osxeyn, what kind of 'front end' are you looking for? gui? Maybe explain what your application will do and we can probably give you better answers.
How "realtime" do you need? How many messages are you going to be sending per second. How big are the messages? If you just want a generic cross-browser solution then long polling is a simple solution. Look here for a [gevent example](https://bitbucket.org/denis/gevent/src/c89f81e04781/examples/webchat/chat/views.py). This is framwork independent, you just need to respond to xhr requests by holding the connection open until a messages comes in. If you want to go highly realtime then you'll want websockets. Your options are gevent-websocket, ws4py, and tornadio. There are also the abstraction frameworks like socketio and sockjs which give you a abstraction layer at the client level which let you use a variety of cross-browser transports (ws, xhr-polling, jsonp-polling , ... ) to maximize compatibility.
In the header of index.html, include the jquery script. Make the div have an id of 'result'. Then, in your own &lt;script&gt; tag at the very bottom (which, like os.popen, isn't the right way to do things, but it is sufficient for now), do: $.get('/uptime', function(data) { $('.result').html(data); alert('Load was performed.'); });
That's ruby.
actually, [Dabo](http://dabodev.com) does just that. It's built on top of wxpython, but comprises a 3-tier approach to database-driven desktop applications. I've never used it, but I've seen presentations on it at PyCon and it looks pretty solid. Another option is Qt ([PyQt](http://www.riverbankcomputing.co.uk/software/pyqt/intro) or [PySide](http://pyside.org) depending on your licensing requirements). It's less structured to build specifically those types of applications, but can give you the flexibility that you need. Because Qt ships with a WebKit component, I've actually toyed with writing a local Django or Bottle app that lives inside a desktop application.
fiiiiiiiine... from satanism import * project = (eviscerate(goat) for goat in pentagrams()) project.next() happy now?
Pretty shady gadsdengraphics. I didn't think internet marketing=spam.
Heh, same here. Most recently, I didn't quite like any of the free markdown editor's (with realtime preview), so I ended up rolling one with Flask and some javascript.
Upvoting for the "...burn in hell" part of this.
Thanks!
But what if I haven't thought that in the longest time?
Ironically, this was my first project after seeing PCGen in Java. I just don't see these very often or for some reason, when I do see them, it doesn't trigger that thought, "Hey, I can do that in &lt;insert programming language here&gt; too..." I'm wondering if there is a way people train themselves to recognize, turn these into projects or if it's all natural instinct (in the which case, I'm screwed).
Can you describe the process of doing so (time commitment, workflow, tasks, etc...) I've heard/read about doing this, but it was usually more like a chearleading chant ("Go OSS Go!!!") rather than "let us help you get involved"...
If you want to work on Python itself, check out [http://pythonmentors.com/](http://pythonmentors.com/). It leads you to the core-mentors mailing list and the CPython developers guide, which defines the workflow and setup and all of that good stuff.
Go to Github, fork a project you know something about, add something, issue pull request. Voila you're doing open source. There are other ways to get involved but that's a pretty simple way to get your feet wet.
I don't know the definite origin, but it's covered in the Perl Camel book. Lazy programmers don't want to repeat themselves and document so they don't have to answer questions from colleagues.
A way to get some ideas is to hang around in a (preferably large) online community and see what things they need/want. There might not be anything major, but you'll probably find small projects to do. For example, I help out on the reddit minecraft servers, and have done things like a minecraft bot, several IRC bots, a server monitoring page, etc.
I think this may be a good solution, thanks!
Two things that help me out the most for ideas: hanging out on IRC answering questions and working with limited devices. Explained in reverse order: By limited devices, my current definition is my mk 1 (non-iOS) AppleTV and my new un-jailbroken iPhone 4S. So far on the AppleTV I've gotten dev tools, fink, macports, Java, X11 (with a headless vnc wrapper), XBMC, Transmission torrent daemon, WebDAV, etc. working on it just by modifying the existing OS on it (instead of installing full OS X or Ubuntu / *nix). Since my primary computer is a MBP laptop, I like having the AppleTV "do the grunt work" of extended downloads, torrents, etc - since I tend to sleep the laptop. This means I'm often writing little helper python scripts to do automated tasks (like downloading YouTube videos to a network share) that can be interfaced with through a bash prompt (LOVE the "Prompt" iOS ssh client). It helps to have a "tinker toy". As for IRC, I'm an op on a Mac Enterprise channel and an active regular on a Mac OS X channel - where people come in asking "How do I do XYZ?" Often times, for the obscure things, it needs to get done with a script. Python makes the automation pleasant and is included in all OS X distributions. Some of the core tools we use for enterprise management of Macs is open source and python based (munki) so I contribute / hack there as well. Also, suggestion: get a github account and just commit whatever you write. Make it a habit and you'll feel more accountable when others are looking at your code :)
Look more closely: the $2500 of donations are towards the PyPy implementation of Numpy. There are an additional $4000 towards Py3k support in PyPy, and an unspecified amount of general PyPy donations. The [Numpy](http://pypy.org/numpydonate.html) and [Py3k](http://pypy.org/py3donate.html) donations are regarding the specific proposals linked there.
Idea: website to match up people with extra ideas for software and people with free time. ;-) I'm only half joking. I've seen enough 'what should I program?' questions that I really think something like this would be worth doing.
It's certainly helpful when individuals donate, but we all have lighter pockets than PyPy using companies. If you're using PyPy at work (or any Python implementation, really), ask around and see if the company will make a donation to the project.
Time commitment: Completely up to you. Some people take ten minutes once to make a single small improvement, others might spend a couple of hours most evenings (and there's every level in between). There are also people who're paid to work on open source software during the day, who put in more time still. Workflow: Many modern open source projects are hosted on Github or Bitbucket (e.g. [IPython](https://github.com/ipython/ipython)). These sites will guide you through forking the repository, committing changes, and submitting a pull request with an explanation of what you've done and why. For other projects, you might need to prepare a patch (aka a diff) of your changes, and email it to the maintainer to get it committed. Or there might be a more formal code review system. Tasks: The two main ways to start are 1) fix a bug. If you've run into a bug, try to track it down and solve it. If not, check the issue tracker. Larger projects might have bugs labelled 'quickfix' or 'bitesize' or similar that they expect will be quite easy. 2) Add a new feature. This may take longer, but you can get more satisfaction from it. Later on you might get into things like code refactoring or UI design. But it might seem a bit presumptuous to try to do those straight away. Finally, there's docs. Any volunteer-driven open source project will love you if you can improve its documentation, but I think you have to know the software quite well to write good docs.
AFAIK it is totally possible to award a bounty to yourself, or to not award it at all.
Start following in read-only mode first. Read their mailing list, pull the trunk every day, read the changelog. The rest will follow.
What a waste of $4000 putting Py3k support in. IMHO it could be better spent improving PyPy until it becomes **the** 2.7 interpreter, then worry about Py3k. 
I'm not sure what you mean by **the** 2.7 interpreter? Do you mean that CPython ceases to exist and PyPy is the only interpreter? If you just mean that it's a more popular interpreter than CPython, how do you plan to asses that?
well I envision cpython as the reference interpreter, and PyPy as the one that everyone uses for performance reasons.
DB: SQlite GUI: PyQt if your project permits the license, else PySide - you might find the qsqltablemodel usefull (http://developer.qt.nokia.com/doc/qt-4.8/qsqltablemodel.html) If you need to create a intermediate representation of the data consider using SQLAlchemy. Lastly, there're two frameworks for creating rich client apps: http://www.python-camelot.com/ http://dabodev.com/
I'm saying we shouldn't cut our arm off because there's the possibility of it being blown off. Add some armor to it.
I have read a book on python but am still a pretty big noob, do this script require any extra libraries and how exactly do I run it? Thanks. 
I would use the [sites framework](https://docs.djangoproject.com/en/dev/ref/contrib/sites/) for handling multiple domains. [Multiple databases](https://docs.djangoproject.com/en/dev/topics/db/multi-db/) are also possible, but the separation is more rigid.
done pypy+=50 pypy should make some advertising @ http://fosdem.org/2012/ 
SQlite is good as local database. wxpython or pyqt for GUI.
Dumbest name possible. If the kit is for signal processing name as such. SigKit would be one possibility. The last thing the world needs is another library with no sound naming. 
scikit-foo is the convention to name scikits, which are packages living on top of numpy/scipy. Originally, scikits was a python namespace, but more and more existing scikits are moving away from it because of the issues introduced by setuptools namespace implementation.
try [camelot](http://pypi.python.org/pypi/Camelot/)
I think subforms use formfield-subformfield structure. See my comment on the wtforms mailing list: http://groups.google.com/group/wtforms/browse_thread/thread/bffa6f4b4854359a I have myself used the kwargs method: data = json.loads(json_encoded_dictionary) form = MyWtform(formdata_as_multidict, **data) Though I have had form submission in addition of the JSON (from a NoSql DB), so I can't vouch if this method works without actual form submission. I plan to test this soon anyway. Could you tell more about your 10 line solution, please?
TL;DR EA Programmer finds enough time to hammer his head into keyboard to create ungodly mess of what is supposedly Python.
I think the Blip player has to download the content before you can skip ahead. You might want to just download the video and watch offline.
I read the review and I think Alex has some really good points. Unfortunately I don't know of another resource like this one, with all this information in one place (either in print or on the web). I wish Alex had the time to write a similar book (I learned the bulk of Python from reading "Python in a Nutshell" when I started). I still recommend the book, but I'll add the caveat that it will be more of a guide to the topics you need to learn more about than a definitive source.
If you need to build up a string, then accumulate fragments into a list and .join() as you say. If you just need to combine strings with variables, then use formatting: print "Hello, %s. This is %s." % ( theWorld, theProgrammer ) The issue is that in Python strings are immutable, so doing repetitive pluses or plus equals leads to multiple expensive memory allocations. For a one off script it's not a big deal, but if you start writing stuff that will run often, like a web app or something, then it's good to be in the habit of avoidance.
While the other answers are helpful/correct it wasn't clear to me if you wanted to return (via return code) or print the result, if you actually want a return code with your value you have to import sys and do: result = compare(x, y) return sys.exit(result)
Really? Ah, that comforts me. Reference?
Hmm... the answerer of wrong, deleted his answer. I wonder what happens now
Are you passing anything to the script? I.e., are you just running "ex13.py" or are you running "ex13.py twinsofliberty" This is made for the latter, not the former.
There are audio links on his Github page. https://github.com/kennethreitz/python-for-humans
[Audio](http://codeconf.s3.amazonaws.com/2011/pycodeconf/talks/PyCodeConf2011%20-%20Kenneth%20Reitz.m4a) (m4a) from [here](http://py.codeconf.com/)
Ok it says "Python is not a command blah blah"I am sorry.But how do you add it to your system path?Thankjs for the awesome help so far 
Never mind i found out
From reading slides I have two questions: 1. "Instill a resistance to `doctest`" -- huh? Doctests are _awesome_, what am I missing? Last time I checked they didn't work that well on py3k, all right, I prefer doctests to py3k \^\^. 2. "virtualenv out of the box" -- explain, please, what's the point. I understand how people who deploy different stuff to different servers want to be able to reproduce remote environments in an isolated fashion, but I've never felt that I'm missing something in my local development.
1. Proper unittesting should be taught, as it's not an idea that's localized to Python. 2. Pretty much nailed it. If you're not deploying your code somewhere, you probably don't need it. But if you are, virtualenv will save you.
This is in order: http://i.imgur.com/105ib.jpg
No python on your path. Where did you install python to?
Is it? D:\&gt; pip install virtualenv 'pip' is not recognized as an internal or external command, operable program or batch file. $ pip install virtualenv -sh: pip: command not found 
OOOOOOOOOOHHHHHHHHHH oh my gosh i an a idiot I didn't not install python on that folder...Lol I'm so stupid.Thanks do much.I can't check right now though.I will check when I can.
Etree is not terrible (cauliflower is ;-)). It has some annoyances like handling namespaces, but overall it's fairly easy to use. No comparison to urllib2. Lxml seems more powerful though, but from an API point of view basically the same. Otherwise, great slides. Requests is heaven sent.
Agreed. I just ported a project from the stdlib etree to lxml.etree. Maybe I just used only parts that were the same, but I pretty much just had to change my import statement and a few other minor things, and it worked like a charm. Both seem pretty great to me.
I have never complained about `popen`/`subprocess`/`urllib2` before, but I probably should've. I find myself consulting and re-consulting the docs.
&gt; That's indoctrination! Yes and it's beautiful. If doctest works for you then continue to use it because you are obviously comfortable enough with it and have overcome any barrier for entry. In addition, you sound like you understand the higher level concepts of testing in general so switching to another way may not bring as much benefit to you. So the "resist doctest" message is not targeted at you. What I think what the OP is saying is that for newbies (or people where unit testing is new material) doctest is not the best option. I also think that in general the OP wants to create a system for best-practices that are mostly right. That's fine as long as a newbie can learn enough of the "right" way to do it to know when to do it the "wrong" way later in life.
Installation is one thing, using it another. I have to say I had some issues using it the first time. I needed to set some pathes manually, don't know wether I did something wrong in the first place or wether there was an issue with virtual env. Anyway, it's another thing you have to learn and handle.
 Get it now,thank you soooo much.i will try it now
As somebody who has leveraged Dave Beazley's [PLY](http://www.dabeaz.com/ply/index.html) (Python Lex-Yacc) to successfully build a functioning, although incomplete, compiler in IronPython called [OWL BASIC](http://code.google.com/p/owl-basic/), I predict that this experiment will be a success. 
Your pentagram yields goats? WTF.
The distance function is in the original post. I was more concerned about iterating the values.
I didn't consider the sum function. I almost never use it and forgot that it was even there. Thanks for reminding me of this function and the cool one liner you posted.
subprocess is a nightmare. Python falls very short as a systems administrator's programming language because of the clumsy constructs for dealing with subprocesses. Subprocess isn't a bad starting point, but it doesn't have good abstractions built for doing something heavier than a simple fork+exec, but lighter than expect (e.g. interact with a revision control system).
Ubuntu's "python-pip" package works fine for me. apt-get install python-pip, pip install virtualenv, and you're ready to roll with comfy bundler-style self-contained app libraries.
Nice try, but you lost that argument.
Did I? Perhaps I missed where.. care to fill me in?
You lose all the points in the bounty.
Oh sorry, it was the part where you had to explain how to perform extra procedures that a first time user does not need to care about. You didn't even get to explain how you actually *use* virtualenv which is yet another layer of things to understand for the newbie. Edit: you also glossed over the part where on windows it's even more confusing, which I'm willing to bet is the most popular platform that people will be using.
Well, we already decided that first time users wouldn't be doing this anyway. The original argument was that you probably didn't care about virtualenv unless you needed it, and if you are one of the people that needs it you probably know how to acquire it. As for the Windows issue.. it will actually work (almost entirely I think) under windows if you have a "sane build environment," but that's something I have never been able to get working. So basically, under windows you cannot easy_install and expect it to always work, and so you cannot "pip install" and expect *that* to always work, so all of this is just a mess and you're best off finding binary distributions of each thing manually. Or use another platform, preferably :)
groupby is your friend. learn it and love it.
lxml's etree API was designed with consistency with stdlib. What requests does really well is to disregard stdlib in favour of simplicity.
Whoever thought subprocess.call("ls -lh", shell=True) was a good way to do shell commands is an idiot.
django-piston...without a doubt
I'm currently maintaining a fork of Piston. I agree that the project isn't very actively maintained right now, but which part of the API in particular do you think should be improved. It may not be perfect, but it's not ... all that bad, I think.
Subprocess and urlib2 are god-awful. Fortunately for us, someone else has already identified how hard these libraries suck and are working hard to give us [Envoy](https://github.com/kennethreitz/envoy) and [Requests](https://github.com/kennethreitz/requests), respectively.
I've heard that [django-tastypie](https://github.com/toastdriven/django-tastypie) is what Piston should have been.
I also maintain a fork of Piston, and while it is kind of buggy it's actually pretty clean API wise. The other REST API `django-tastypie` I find to be very cryptic and that it quickly becomes unmaintainable if you stray outside of its default behavior.
Maybe some people can get it to work but when I tried it was really difficult to use. The API seems to promote conciseness over extensibility. If you need to "hydrate" a model from a client framework like Backbone and it fails you'll either end up with no error logging or some cryptic stack trace smack in the middle of the otherwise unreadable serializer module. The "hydration"/"dehydration" code is just way to automagical to debug if you stray outside of its narrow use-cases. I wasted two months of my life maintaining an existing Tastypie API and it was just an overall awful experience that I would not recommend. Then I forked django-piston and life was good.
They will follow some simple instructions and learn from that. While I disagree with Kenneth's taste in API design, he is good at marketing, and a well-maintained getting started guide is something the eager-to-learn newbie hordes will gobble up.
I have written a simple compiler for a class that used PLY for parsing.. I did a fraction of the work that my fellow students did using similar tools in java, largely thanks to PLY. It is an excellent library that made my first major program in python a true pleasure.
This may be a silly question, but is there a "Python for the Impatient" or "Guide to All the Core Stuff you Should Know About Python" out there? If not, I'd be happy to put something together to get us started on github...
&gt; how do I handle concurrent versions of modules? That doesn't seem like a beginners question. &gt; how do I handle deployment to my webserver? Django has a page in their docs about that. &gt; How about dependencies? Like you would ASP.net website: you deploy necessary libraries globally, or include them with your project. &gt; shouldn't I be unit testing this stuff? That's more of an individual decision. &gt; What framework should I use? I thought you said you were using Django? 
That's such a horrible analogy. It's a simple click to whitelist a site in NoScript so that JS is enabled. If I could detach and reattach my arm at will, I think I would detach it more often then I do presently (which is, of course, not at all).
I think that is exactly the point the author is trying to get at here. For instance, you heard virtualenv is really good to have so you want to check it out. You see someone saying to use pip to install it, but you don't have pip. How do you get pip? You find a great thread on how to install pip using the apt-get package manager. But wait.. you're on Windows? You go looking and someone suggest using easy_install instead. Well, how do you use easy_install? Is it different than pip? You dig deeper and some guy says forget pip and easy install on Windows, just use distutils because then you will learn about how packages are really put together. So now you've spent an hour learning about different distribution tools and gotten nowhere. You still haven't installed Virtualenv, and you actually aren't even sure you know what Virtualenv does anyway. All you really wanted to do was try and get a small web project going, but you can't even seem to get started. It's even worse with some other packages. You need to do a GET from your webserver? Ok, you type in "python http lib" to google and you go to the stdlib httplib page. One of the first things it says is that "It is normally not used directly — the module urllib uses it to handle URLs". Ok, so on the urllib page it a huge mess of notes, warning, and cryptic function definitions that don't seem to provide any easy way to just GET a file. You used to use curl a while back for this stuff, so you stumble on pycurl. But that's kind of a dark age thing so you dig around more and maybe then find urllib2, or if you are lucky the requests library. How would you know difference between all these? I can see how having some well documented best practices for common things like this would go a long way, but the author's suggestion of promoting 'one best way' lines up with core Python philosophy. That is, as opposed to having a huge slew of many slightly different tools for more of less the same job. It's starting to look a lot like Perl around here these days, but that may very well just be a natural stage of development, especially in community driven languages.
As a long-time dabbler in Python but someone who's only used it in anger recently, the whole batteries-included thing has lost its shine. The really basic, completely universal stuff usually works, even if it's sometimes clunky. However, so far the *majority* of the library that I've tried to use beyond that is either missing key documentation or actually broken, at least badly enough that you'd want to use an entirely different library rather than rely on the standard one. It's roughly on the level I'd expect for alpha/beta software on one of the code sharing sites, not for the standard library of a mainstream programming language. For sysadmin-type work, for example, it's not just `subprocess` that is clunky, it's also the filesystem stuff, the zip/archive stuff, etc. I find that more and more, I like Python the *language* relative to the likely alternatives, and I can forgive it its little eccentricities because I've yet to find a language that hasn't got any of those. On the other hand, I increasingly loathe Python the *library* and now rely almost entirely on third party resources for anything non-trivial. For that reason, I have pre-booked a special place in hell for everyone involved with designing Python's whole module/package/distribution infrastructure, which makes it absurdly difficult to actually work with those third party resources. The idea that you should have to "install" a library in a scripting language is particularly amusing in a "they really have no idea what they are doing" kind of way.
I don't understand how you could string the characters "urllib2" together, in that order, and not realize that you've really committed a crime.
Lo and behold, the power of Reddit! Thanks for the update though, it's really nice when you see the result of your actions. Also, you guys are awesome! 
One way to do this is to provide a custom Python interpreter along with the pyc files. Compile your own version of python, based on the stock python source code, but use different bytecode opcode values (stored in "opcode.h" IIRC). Use this special version of python to create your pyc files -- using any stock decompiler on them will just spit out garbage. The drawback is that the code will only run with "your" version of Python. There may also be license issues with doing this; I am not sure.
What are the advantages of SST over nose+Selenium ? Are there other alternatives for pythonic web test framework ?
imaplib. Fortunately there is [imapclient](http://pypi.python.org/pypi/IMAPClient/0.8) but still... Wants to fetch messages ? Let's return an almost random sequence of things ! Maybe a mail part, or a colon, or anything else, depending on your imap fetch command and/or your server implementation.
I don't know! The community is becoming increasingly fragmented, the 2-&gt;3 transition is still a huge problem, and there are a ton of standard library packages that seem to be all over the place. I feel like Guido or someone with clout in the community needs to step forward with a clear vision or plan for the future.
Etree reads the namespaces correctly (you can even register namespaces in 2.7). So why does it not offer an API to use the namespaces with the given names? A dict 'nsname_to_jc_notation' of the ElementTree instance of a parsed xml file comes to my mind spontanously. Or a context manager would be probably much nicer as a second thought. You wouldn't have to create your own globals. Also, the names of the namespaces are known, but when serialising, etree uses internaly created names ('n1', 'n2', ...). By the way, one of the xml libs (or former versions) serialises resolved names, which is (was) super annoying. So what etree currently does is not a huge drama, but can be improved IMO.
I tend to agree with the other posters that you should really assess whether "locking up" your application is really worth the hassle. In some cases, it may be in which case I suggest you take one critical portion of your codebase (a module) and run it through Cython. This compiles it to a native C module. Although you can accelerate Cython code by adding type declarations, this is optional and you can compile vannila python and get a modest speedup (~30% or so). The point here is code obfuscation not speed, in this case. This is as (in)secure as any other compiled C program. The downsides are that you need a C compiler and that there are a few restrictions on what Cython can compile (no generators, last time I looked).
first off: requests doesn't have a python 3 version. [wah wah.](http://sadtrombone.com/) . Neither does envoy. [wah wah.](http://sadtrombone.com/) ive only viewed the first 10 pages of this slideshow and already i disagree with it. The ruby example of getting github's api is simple and the python example is wayyyy to complex. I have no idea how its done in python2, and it SAYS its easier in python 3, but still, why the fuck is it messing with regex? here is a pretty close example to how the achieve the same thing the ruby example does with python3: http://paste.ubuntu.com/792481/ Maybe python 2's urllib2 sucks so much that you have to use regex or a custom class to get a simple request to work, I have no idea. I will admit that python's documentation is TERRIBLE however. After coming from Java and its excellent documentation, i find myself very very frustrated on the lack of it in python. Its partly because python is dynamically typed, so you can't really find methods for certain concrete classes, cause you arn't supposed to know what 'type' of file object you have, just that its a 'file like object'. But in writing that snippit above, and looking at the documentation: http://docs.python.org/py3k/library/urllib.request.html?highlight=urllib.request#httppasswordmgr-objects where does it say that you are supposed to use HTTPPasswordMgr with a HTTPBasicAuthHandler? I thought HTTPPasswordMgr was a subclass of BaseHandler and therefore i could use it with a OpenerDirector, how do i find what that class subclasses? What is a "uri" and "realm"? Why are there two classes for HTTPPasswordMgr and HTTPPasswordMgrWithDefaultRealm, when they both act the same way only one does something different when passed in None? I love python and i haven't really had a trouble with it's libraries THAT much (dealing with ParseResult's with urllib.parse is a bit weird as i did some of that recently), but god forbid they need to expand their documentation. The shitty ass 'sphinx' documentation framework/parser/viewer/thing also doesn't help. 
It seems like a consensus has emerged that the best way to support Python 3 is by making one file that can run on both 2.6+ and 3.x. This is pretty different from the original scenario of running 2to3 over the file, but perhaps a bit healthier. Now let's all stop using Red Hat.
I can't verify whether this Python is good for humans, but it certainly isn't made for IE6.
That's handy for newbies, but I was thinking something that explained about why virtualenv exists, how it works, tells you about envoy, shows you how yo install things in mac/windows, etc.
Curious about what the other options would be other than to install a library?
How is Python not "for Humans" by default?
This directs to a login page. Is there a better link?
I feel the pain when using subprocess. It is nice but quite low level and is certainly not nice when writing shell-equivalent script. For that I wrote: https://github.com/aht/extproc
&gt; As a long-time dabbler in Python but someone who's only used it in anger recently, the whole batteries-included thing has lost its shine. I agree. We're now stuck with a box full of empty 90s-era batteries. It's time we got some new ones and find a way to get rid of the old ones. I think the linked presentation really made that point well.
I'm the exact same. I generally eventually get around to finishing them, but it takes a while and I keep switching between a bunch.
I actually [asked about this](http://stackoverflow.com/questions/5674442/is-there-something-better-than-django-piston) on SO some time ago, but when I realised that the options weren't simple enough, I [wrote my own](https://github.com/danielquinn/django-rest).
&gt; I don't understand why, in the overwhelming majority of practical &gt; situations, "installation" could not just be &gt; cp library-file(s) /path/to/libraries This is already pretty much the case. You can download any distribution of a pure-Python library from PyPI, unzip it, and copy the resulting structure into a place on your Python's path. In fact, until about 2002 or so, this was de-rigeur. None of the installation tools you mention existed at all. The installers (pip/easy_install) were created because libraries often depend on other libraries. Before the installers existed, either you did the dependency resolution by hand, or sometimes libraries shipped with copies of the libraries it depended upon within them. But both resolving dependencies by hand or having no dependencies were imperfect: resolving dependencies by hand is laborious and requires documentation effort on the part of library folks. Having no dependencies is fine, but then every project is siloed into maintaining its own copy of a particular dependency; every project becomes a fork of several others, and when two libraries you installed had different versions of another, the conflict was irresolveable. Neither situation was particularly tenable. Python's import statement does not currently know about versioning, so there can't be multiple versions of the same package on the path. Virtualenv was created as a workaround. The current situation is not ideal, but, IMO, it's a boatload better than it used to be. The times before we had the installers and virtualenv sucked even harder, if you can believe that. ;-) 
This post appears to be a dog whistle. Python has its share of problems. As far as I can tell, there's been a historical vacuum in leadership inasmuch as very important things to most people (particularly packaging and distribution) have been largely ignored by the standard library. The docs for existing third-party packaging things are not great. Efforts like Tarek's "packaging" due in Python 3.3 are aiming to address this, but the townfolk already have their pitchforks out and they're looking for somebody to gore. On the other hand, I'm not sure that everybody striking out to create their own personal "beautiful APIs" is going to solve much. Most people commenting here are complaining about having multiple ways to do the same thing. They want to be told what to do unambiguously, and having yet another installer or yet another process module, etc isn't going to help with that much. There's only one way to solve this, and that's to have one clear way to do it that doesn't suck on multiple axes. Having one way to do something requires consensus, and consensus takes time. I don't think chest-thumping presentations like this are very good for a healthy community. They tend to breed scapegoating, a sense of entitlement, and a lack of respect for fellow programmers. I'd encourage those complaining here to try to take some personal responsibility. Programming is hard. Good documentation is even harder. Community consensus is even harder than documentation. Participate in the development process if you can afford it. If you can't afford it, at least try to be constructive. If you can't be constructive, at least try to be nice.
The urllib2 documentation covers the code therein but not common use cases. It was designed to be ultra-extensible, but 99% of people just need simple http-auth, not customizable authentication handlers you need to compose from 3 classes. requests is much better in this regard; it makes the simple case simple and the hard case possible. urllib2 (and twisted, imho) make the impossible achievable (http over icmp? why not) and the simple aggravatingly complex. Most of the standard library documentation is pretty good, though I don't read it much anymore since I know how to use most of it and I generally just read the online documentation in the REPL. Sphinx documentation is something I (and most people in the python community) greatly prefer reading to javadoc style things, but I suppose after you're used to python you stop worrying about types so much and just worry about attributes and behavior; for instance, file-like objects are things implementing some or most of [this api](http://docs.python.org/library/stdtypes.html#file-objects), including but not limited to StringIO objects and urllib responses. After a certain point, you either prefer reading "file-like object", or you prefer "an object implementing java.io.InputStreamReaderInterface"; the second is a in a lot of ways a stronger guarantee and a "solid contract", but I'd argue it's usually just so much line noise.
That's being worked on at the moment for 3.3: The packaging work in the core repo, and the [pythonv branch](https://bitbucket.org/vinay.sajip/pythonv/) for built-in virtualenv functionality.
Annoying as it is, Red Hat (and CentOS) are still *painfully* popular in production environments, in my limited experience. Fortunately Python 2.6 is available as a third-party RPM, which is what I'm using—and only for running alongside mod_wsgi, not as a replacement for the system Python (to which replacing/upgrading, I've heard, is the stuff of nightmares).
Honestly, if you're working with XML you should be using lxml and XPath. XPath expressions are **much** more powerful than this syntax.
Thank goodness. And IMO, a compelling incentive to move to py3k.
I can attest to this as I just completed a project of which I evaluated both. The API for tastypie, to me, seemed much more Pythonic and easy. 
Frankly, I can hardly write code for 2.6 anymore. Imagine my bewilderment the last time I typed `from collections import OrderedDict` and was greeted with a `NameError`. Did 2.6 even have all the comprehensions and the `with` statement?
It's popular because of cPanel, WHM, Plesk and similar software. They all work on CentOS. So most people who have no clue what a distro is and buy a server end up with a CentOS box. Or, let me put it in other words: before I started supporting peoples' servers, I thought that CentOS is some marginal distro that nobody uses. Sadly, it's not true.
This would be my suggestion as well. Particularly if you are just getting started / learning. Although Flask is the cool kid micro-framework on the block, it's not all that micro if you want to understand it top to bottom. Bottle offers less functionality but is much easier to read to completion. It has a great [tutorial](http://bottlepy.org/docs/dev/tutorial.html) and [docs](http://bottlepy.org/docs/dev/). The community is very helpful on [G+](https://plus.google.com/104025895326575643538/posts), [IRC](http://webchat.freenode.net/?channels=bottlepy), and the [mailing list](https://groups.google.com/group/bottlepy?hl=de). It's hard to get much lighter weight than [3k lines in a single file](https://github.com/defnull/bottle/raw/master/bottle.py) as described in this helpful [talk](http://www.youtube.com/watch?v=AYjPIMe0BhA). It's [easy enough to extend](http://bottlepy.org/docs/dev/plugindev.html); however, to be honest the real point to be made here is that if you need lots of extentions one of the other frameworks may be better for you (ex Flask/Django/Pyramid/Web2Py/whatever). Regardless I think Bottle is a great place to get started w/ Python web development because it's easy to hold in your head all at once and that's worth a lot when you are getting started. If you've got 45 mins I'd seriously check out the video I linked. It's a good starting point. 
it wasn't reddit, just numbers on the account got updated. Seriously it's not that I'm lazy it just comes in bulk :)
I found subprocess quite OK. Wrote simple task scheduler with it and had no problem (except buffers). But urllib2. Oh my god. That was a bad experience.
I'n not sure what you're asking me. I have been using urllib.request which is urllib2 in older python versions(I've been using 3.2) as far as I know. I haven't been using the request library which you linked to.
&gt; If I could detach and reattach my arm at will, I think I would detach it more often then I do presently (which is, of course, not at all). Augh, and you thought my analogy was bad. Let's end this while we can.
The most interesting tool is the application usage reporting tool, which until now there has never been a free open source one. Just my opinion as a Mac Sys Admin.
Look into Javascript, jQuery, and Ajax. Specifically: http://api.jquery.com/category/ajax/ A simple way would be to have a Python script, say http://www.yoursite.com/script.py that outputs an announcement that is constantly updated or something. Somewhere on your index page, you would do something like: &lt;script&gt; $.get("script.py", function(data) { //data contains the response data. update a div to contain the data, or do something else. $("#announcementdiv").html(data); }); &lt;/script&gt; $.get() is the jQuery shorthand function to make a GET Ajax request. Learn a bit about how jQuery works. Once you do, client-side web development is pretty simple and easy.
I was just joking. I saw the **requests** module on www.reddit.com/r/python yesterday.
Everything I've tried for 3d plotting has been a huge pain in the ass. I'm referring to trying to get mayavi or some other interactive 3d package to render a graph that you can interact with, as opposed to creating a static image of a 3d surface.
Make sure to join us on ##osx-server on Freenode :)
I am also a web programming novice, and was playing with this just yesterday. For a super quick start I suggest the Flask micro-framework (http://flask.pocoo.org/docs/) + jQuery (as others have suggested). There is even an example demonstrating exactly what you want to do; http://flask.pocoo.org/docs/patterns/jquery/ 
/r/learnpython
Actually, plotting in general could do with improvement. I've found matplotlib to be quite clunky, especially compared to tools like [ggplot](http://had.co.nz/ggplot/).
Best Python IDE I've used (not OS X specific) is PyCharm by Jetbrains. They have different licensing models of different prices.
I agree, but it's apparently a hard problem since no one has made it especially clean yet. Maybe there's an opportunity to use the newish "with" keyword to design a plotting API. You could push figures and parameters onto a stack instead of having to keep track of what arguments to give to the plot function every time. Also being able to design named plot styles like css would be nice.
 doctests are a good way to test some examples in the docs. Period. They totally suck for general testing (as they add a complex layer of semi-hidden context and debugging gets interesting). And, for non-trivial code, you have exactly zero chance of combining docs and tests, as your tests will unavoidably include a lot of stuff which you don't want to see in the docs. Yet, doctests are often sold as an viable alternative for unit testing. OTOH, python unittest sucks as well ;-).... So that, might be an explanation :-(.
&gt; Writing doctests is by far the easiest way into the whole "testing is good" mindset How so? How are doctests bettter than this: import mylib assert mylib.foo() == 42 No silly formatting requirements to learn, trivial to write and trivial to debug.
Alright i just got it, but its very hard to use &lt;.&lt; i think this is " very pro" for me at this moment, i just learning to type "hello world" some bacis loops how range works etc ( very very VERY BASIC STUFF) i cant even get the thing to work, i already started a project but i dont know where to go to start typing, any ideas?
The most interesting thing I found was the utility to create an informational web page describing what a .pkg file. However, that was a part of pymacadmin, and not the code released by Google. It's called wtfUpdate.py and can be found here: http://code.google.com/p/pymacadmin/source/browse/#hg%2Futilities
If you're still new to the language, I think your best bet is a simple text editor ([Textmate](http://macromates.com/) is popular) and two terminal windows, one running an interactive prompt (just type "python") and another to run longer bits of code you save in your text editor. I think Textmate may also support some rudimentary syntax highlighting (a lot of Ruby developers on the Mac use it, but I'm not sure about it's Python support). Another option you may want to consider is [DreamPie](http://dreampie.sourceforge.net/). I haven't tried it yet, but it looks like it gives you some auto-complete within a terminal window.
There's evidently a lot of love in this subreddit for jQuery, but I would say that this is a basic task that most JS frameworks should be capable of handling; jQuery isn't the only solution. In my humble experience, jQuery is popular for novices and people who want to use "off-the-shelf" modules as is (either for prototyping, really light JS use, or because they don't want to have to learn any more JavaScript than is strictly required to accompish the task at hand); it is correspondingly terrible for people who want to extend or override existing behaviours or design their own highly reusable JavaScript libraries.
## cron end of day sales report from some database.. spooled to a LAN file xday = "2012-12-25" spread_sheet = open_sheet("//share/me/file.xls") work_sheet = spread_sheet.get_or_create_worksheet(xday) rows = database.get_sales(xday) for r in rows: worksheet.append_row(rows) spreadsheet.close() 
2.6 do has with statement, and 2.5 can use "with statement" with "from from \_\_future\_\_ import with_statement". I think 2.6 don't have all the comprehensions.
Crossing the Atlantic, again
vim
I also agree and would recommend [MacVim](http://code.google.com/p/macvim/), as being a useful investment of your time: It's charityware, you can use it Mac-style or Vim-style. If you learn more Vim-style use that knowledge is useable across multiple platforms. The same is probably true of the Mac emacs options, I just don't know them enough to recommend.
While just a simple text editor may not be enough for complex coding, a text editor + specific purpose tools will be more than enough. By these I mean: - a debuger. Can use pdb, but may want to have a look at pudb or winpdb - a lint. See pylint (the most bitchy one), pychecker, pyflakes - a tool that can do code coverage. See coverage.py - a tool that can analyze for cyclomatic complexity. See pygenie - a tool that can find pieces of redundant (indentical or very similar code). See Clone Digger - a tool that can automate some simple refactorings. See BicycleRepairMan and rope - maybe a test runner. For example see nose and tdaemon PS: for those who are fans of PyCharm - I've never tried it, does it really beat vim + all the tools I've mentioned? PPS: any additon to my list of tools is welcome. Both alternatives which do the same tasks as well as tools which do other cool things that help writing better python code
This is a mixed bag of statements. I agree that an IDE can help in navigating a large foreign code base, and intellisense definitely helps. But I think the statement to the effect that beneficial usage of text editors is limited to 'early "Hello, World"' programs is demonstrably false statement. Do you really think all the Vim/Emacs Python programmers, some of whom manage libraries and enterprise level applications numbering (hundreds of?) thousands of lines of code and hundreds if not thousands of users keep typing "hello, world" again and again and again? I use Vim exclusively. I have been programming for decades, several years in Python. I think Wing is pretty good. PyCharm is better. I would rather my eyes get eaten out by rabid rats than switch to them permanently over Vim. I'm not saying that the OP or anyone else should not use IDE's. I *am* saying that IDE's are unneccessary fluff when programming in Python, and maybe even counter-productive when trying to learn Python. 
I feel your pain, subprocess is a bit too low level and doesn't provide shell-like functionality. Perhaps you'd like sth I wrote https://github.com/aht/extproc
I think you may be taking part of my comment slightly out of context and reacting defensively. It's a bit of a strawman to say that my comment was implying "all the Vim/Emacs Python programmers, some of whom manage libraries and enterprise level applications numbering (hundreds of?) thousands of lines of code and hundreds if not thousands of users keep typing "hello, world" again and again and again". As I said in my comment, I *personally* found the IDE to be very helpful when I was learning Python. I already had over 20 years of programming experience at the time (everything from 8088 assembly to C++ and Java) but I think it'd be even more helpful for someone learning their first language. Can a programmer kick ass using Vim/Emacs/Whatever? Absolutely no doubt. Is it the best choice for an absolute beginner (which is what my comment was responding to)? My personal opinion is "probably not" but feel free to disagree. 
It doesn't have all those cool tool you mentioned, but it has many tool near-good like this and pack all in one product, and its intellisense is better (better than rope I think) , that why. For me, I'm using emacs + pyflake + pymacs/ropemac
as someone who is actually in the course this fine Chicago winter week, I can say: the compiler experiment is turning out to be a great success. In the first two days, we wrote a lexer and parser for a Go-like programming language and wrote a code generator that actually results in a running program. The next two days will be spent on the trickier aspects of control flow, function definition, compiler optimization, and targeting RPython. All six of us are knee-deep in parser code, but loving every minute of it.
In general I like what he's saying here. The short version would be that STM should be used ubiquitously and non-conflicted STM transactions should be made no more costly than uncontested locks (ie. free). I disagree with his idea of using a yieldesque keyword though, preferring something which more directly mirrors what occurs under the covers. stm_commit, commit, stm_sync, or something like that. When a thread "commits" in this model, it is simply saying that it's ready to have any data it has modified seen by the world and also ready to have its world view changed underneath it. I approve of this model wholeheartedly.
jQuery is the easiest approach but if your needs are simple you can go without it and avoid an extra dependency. Since others have mentioned the practical approach I'll just give you a dead simple explanation of how things work in case you don't know. 1. You write a Python script that processes a GET or POST request and returns a result. 2. So imagine we have a script that given a username says if that user is admin or not. So mysite.com/admin.py?user=tom 3. You could simply browse that URL and you'd see a True or False on the screen. 4. But since we want that answer in another page we simply let javascript make that request for us behind the scene and give us the result. 5. Then based on the answer we change the page with javascript so you don't have to move to another page and there's no refresh. (jQuery just makes working with javascript easier) So what you need to do is: - Write the script and make sure it works properly without AJAX stuff - Write some javascript to send the request and get the result - Change the page based on the result
Companies who develop/buy software for their own needs are not looking for the latest and greatest innovations nor for the fastest and cheapest solution, but to get the business needs done most reliably. The benefits come from the improvements of the business processes. The project managers at those end customers are often not technology experts. Choosing (or charging) something different than .NET or Java seems like an unnecessary risk to them. Also, the coding is one of the smaller parts (but highly underestimated). Process and business object definitions, design (software at all levels, infrastructure), writing specifications for everything (from use case to support) take more time and money. That's at least what's going on in my company, which is pretty represential for the large industrial enterprise.
see nagare python framework
&gt; I'll post the code on Github as soon as it isn't embarassing. People always say this, and it bothers me - we all know that the first hacking you do isn't going to be great. It'll be dirty, ignore edge conditions galore, have no documentation whatsoever, etc. That's fine. Really. You should be tracking those early stages in revision control, because, well, you're changing code, and cleaning things up is a great way to break them. And if you're already tracking your project, why not put it online? Earlier is also better in regards to feedback - I've had someone point out to me a pre-existing project that solved the same problem I was trying to solve, merely because they read the README I put up on a new project (I tend to put up a very basic README before even writing any code).
vim vim vim
I absolutely agree. Let me assure you that 1. I am already tracking everything in git. 2. I had a README before anything else (well - maybe I had a .gitignore first) 3. All of my code is already documented and unit tested, and I iterate those docs and tests before I iterate the code. I was planning on posting my code tomorrow or the day after but I guess I can make a push to do that tonight since you expressed some sort of interest. :) I'll update it with the link when I've done that.
ELI5, what do these programs do.
I don't get why idle would be unstable. I think it is lacking in features,but one of the more stable options. I only have used it on windows and linux, but I don't think osx should be a problem. I would try install a new version of python, maybe python 3. 
Not this
I second that. PyCharm is well worth the money, especially since you get a any-os-you-want license even with multiple machines per user (as long as you only use one at a time). Makes it much easier to switch systems if your IDE is all there. Allthough I have to admit that my many-years-vim-addiction drives me to use some smaller editors every now and again despite PyCharms nice features.
Huh, I though they were mainly using Puppet for this stuff. Apparently they involve [a lot of other tools](/r/sysadmin/comments/o2rbk/google_to_release_all_their_macintosh_management/c3dz1nh) as well. Most of those are to implement sane packaging (archive mirrors and package client).
The point I've taken away is that a generator is the right API for this. It gives the STM implementation the segmented stream of instructions it needs. The STM user can use all the various ways of writing generators, uses existing keywords that produce compatible bytecode. There is a trivial dummy implementation for compatibility outside of a built-in STM.
http://twitter.com/bitbucket &gt; Sorry everyone, we're aware of the site being unavailable right now, and we're busy looking into fixing it ASAP. edit: &gt; We've identified our main NFS problem as the problem, and we're working on getting it back online.
It's the same for a lot of commercial and especially retail environments, a large office supplies retailer I worked for took more than 3 years to upgrade from NT to XP fully (completing the transition when Vista was already out and W7 was rising on the horizon). Most of that time was spent in upgrading client software to work ("mission critical" line-of-business applications) and in some cases re-writing swathes of the system. To this day the entire inventory system (at least the last time I was in a branch) is running via telnet to an AS/400 somewhere on the continent. The inventory system does not talk to the POS directly (at 9pm every night a batch job that syncs the two (sales are subtracted from inventory, returns added on etc) hilarity ensues when this fails for some reason. The company knows that this is a major issue but the cost to fix it exceeds (by their calculations) the benefit (which is blind as such a system is death by a thousand cuts as the inefficiencies multiply out). As a company you get the IT systems you deserve, properly funded with competent management IT can be a major benefit to any company (which is not exactly a secret here on reddit.com) but the bean counters rule.
Heaven forbid that anyone might want to use it for serious, long term projects... You can use Python 3 today, and library support is steadily improving. Sometimes the slow pace of change frustrates me too, but whinging about it isn't constructive.
I've never been a fan of Bitbucket but it must be annoying for its users. Meanwhile, Github works like a beast.
No, I can't make a free private repository that returns 503 Service Unavailable with Github.
They use puppet and Munki, but also roll their own which are written in Python/Ruby. I actually, last year, interviewed for their internal Macintosh support team, so I got the inside scoop on what they do and how they do it.
Thats how i was reading it keep in mind Armin just compleated making stackless work on pypy with the JIT (with some minor limitations) via the _continuations module which from what i know is similar to the api he is currently talking about (coroutine.switch() vs yield) current greenlets and stackless in pypy are implemented on top of this continuations module i would think in the end you would either get a replacement thread module that replaces the current one and is corse grain unless you import and use another module or introduce something that is compatible (int he same way multiprocess is compatible with threading), perhaps with a few multiprocess style objects for things like shared values and such
&gt; We've identified us using NFS as the problem fixed
What would you recommend? I don't know anything about anything :/
Since I'm being downvoted, first one of the many sources describing why using NFS is considered harmful: http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html Alternatives depend on the actual use case, http://en.wikipedia.org/wiki/Storage_area_network is a good place to start I guess.
A "transaction" keyword forces one to think in terms of what is shared and how to minimize contention. A challenge with this approach is to still think somewhat in those terms. If nothing else a tool to analyze the implicit transactions to minimize contention would be a benefit.
The alternatives link should really be: http://en.wikipedia.org/wiki/List_of_file_systems#Distributed_file_systems The use case of a SAN is often not the same as you'd use something like NFS for. Usually SANs are fault tolerant, high bandwidth, high availability, high cost whereas Distributed File Systems are usually fault tolerant, lower bandwidth, high availability, low cost.
http://status.bitbucket.org is showing 504 now.
You're kind of right I think. Since the SAN article also mentions NAS, I still think it's the better entry point though. I've often seen NFS used in scenarios where a SAN would be more appropriate (mostly for "historic reasons") and I think bitbucket specifically might just be one of those scenarios. As I indicated, there isn't one fits-anything alternative to NFS as it was/is very broadly used. The one thing we finally have to agree on is that NFS itself rarely is the best solution. The linked paper puts it nicely IMO: "Using NFS for complex networking issues is often a poorly thought out attempt to use a simple solution to a complex problem."
Dont forget SANs are done at the block level while a NAS is at the file level. this is important for concurrent access as if you wish more than one machine to write to a shared SAN partition you generally need a distributed lock manager to coordinate writes to avoid clobbering the filesystem SAN for 1:1 (one partition, one machine) NAS for 1:N (one file/filesystem, many clients) if they are just using NFS for RO replication then i don't really see much of issue (RE: link, NFS Considered Harmful)
[WTForms](http://wtforms.simplecodes.com/docs/0.6.1/) will put these into something close to what you're looking for.
Arnhem, of course :D
This just happened for me. It was kind of flaky until now. Hoping that they are back now. [B:\repos\o1o1197o\scripts] $ hg pull -u http authorization required realm: Bitbucket.org HTTP user: o1o1197o password: pulling from https://bitbucket.org/o1o1197o/scripts abort: **HTTP Error 500: INTERNAL SERVER ERROR** [B:\repos\o1o1197o\scripts] $ hg pull -u http authorization required realm: Bitbucket.org HTTP user: o1o1197o password: pulling from https://bitbucket.org/o1o1197o/scripts searching for changes adding changesets adding manifests adding file changes added 2 changesets with 5 changes to 3 files 3 files updated, 0 files merged, 0 files removed, 0 files unresolved
Yay, they updated their status, that they are back, right after I posted here. :)
A while back, I was asked a similar question here and since then, I have built the app for which I asked that question. Hopefully, you can learn a little from my stumbling. Originally, I looked at Camelot[1]. The problem I found with it was that I could not get through the tutorial as described with the current version. I got errors despite doing everything as described. Additionally, their installation documentation was only accessible from a certain link on their page. All others 404ed. I decided against using it. OTOH, it might be good to know that the lead developer for Camelot is a [member of this subreddit](http://www.reddit.com/r/Python/comments/ka4us/what_database_and_gui_frameworks_nonweb_do_you_use/c2io89e). I also looked briefly at Dabo[2] and it might legitimately be good, but the reason that I did not use it was that the tutorials seemed to me to be entirely too dense to be useful. The tutorials bridged on being full blown documentation, which is too much info for a beginner. I was overwhelmed by it. Our first version of the application used Elixir[3], a thin wrapper for SQLAlchemy[4], and SQLite for the backend and used Tkinter, the built-in GUI Toolkit for python. After spending a lot of time writing it, we came to the conclusion that Tkinter was a bad choice to use: 1. It does not support some basic widgets such as combo boxes or tables without a 3rd party library. We ended up using 3 different 3rdd party libraries for one widget a piece. It was leading to too many dependencies. 2. It does not have a WYSIWYG designer program [that I could get to work]. We had to code the whole GUI by hand, which was great for learning GUI programming, but terrible for getting things done quickly. 3. It did not look good. One of the reasons that we chose it was that the latest revision of it used native widgets. Despite this, there are still some widgets, such as OptionMenu (a drop down menu) that do not have a native look on all platforms. About 2 weeks before the project was due, I decided to rewrite the GUI using Qt and PySide[5]. I got it done in 3 days using Qt Designer[6] to design the look of the GUI and pyside-uic to generate the python code from the XML that Qt designer generates. I had a good experience with all of tools that I ended up using and would recommend them to others. [1] [http://www.python-camelot.com/](http://www.python-camelot.com/) [2] [http://dabodev.com/](http://dabodev.com/) [3] [http://elixir.ematia.de/trac/wiki](http://elixir.ematia.de/trac/wiki) [4] [http://www.sqlalchemy.org/](http://www.sqlalchemy.org/) [5] [http://www.pyside.org/](http://www.pyside.org/)
You've created a monophonic music API that is infinitely more complicated than BASIC's [PLAY](http://glind.customer.netspace.net.au/gwbas-7.html) command. My two cents. 
Also, for creating the EXE, you should look at py2exe[1]. I found it decently easy to use, other than that I had to set up a development environment on windows to use it (I normally develop on Linux or OS X). [1] [http://www.py2exe.org/](http://www.py2exe.org/)
I am hard-pressed to see a difference between TextMate and an IDE for non-compiled languages. You can plug just about anything into TextMate if you want to. You can use an IDE as a text editor. I know this is a programming-related question, so everything is a Holy War, but splitting this particular hair doesn't seem useful.
I've been happy with [Komodo Edit](http://www.activestate.com/komodo-edit/downloads) (free version), mainly because I use it on Windows and Mac and get the same basic experience across machines.
Use Cython. It will translate your Python libraries to C code which you then compile. 
Bitbucket is not open source.
[macvim](http://code.google.com/p/macvim/) Edit: link Also [Turning Vim into a modern Python IDE](http://sontek.net/turning-vim-into-a-modern-python-ide) is fantastic guide on how to make python-vim experience even more amazing. Personally, I prefer [vundle](https://github.com/gmarik/vundle) instead of pathogen (Anderson's instructions translate easily enough).
As a member of a group actually running an enterprise scale NAS that exports to it's servers using NFS (and to users using CIFS) we've run into a few issues but they aren't inherent failings of NFS but more configuration fails in Linux (which several of the papers complaint appear to actually be...). We don't however let non trusted systems access our NFS interface. Combine this with root squashing on non admin systems and everyone has access to their files by user and group and all works as expected. I would have to look into some of the other issues presented before I could comment on them however. The biggest issue for us is user and group numbers, not a surprising issue since our current directory has around 13 years of legacy in it and user ID's under the recommended UID 500. The file locking and stateless behavior is something that has to be assumed when operating with an NFS based mount and really comes down to work flow more than anything else. However we don't currently have a setup where two processes would be attempting raw file write access directly so YMMV. We could (and have looked at) using a SAN if didn't want to have an appliance based storage server but the local administrative overhead is quite a bit higher, and while distributed file systems are great the admin overhead in the individual servers accessing that service is also rather high compared to mounting off standardized NFS profiles. Our experience is that DFS quotes are also usually significantly higher than their NFS based competitors and SAN systems tend to require a separate CIFS server to be run so configuration gets out of sync very easily. For the flexibility, cost and ease of management and configuration NFS based systems do actually 'just work' for a great many use cases. But again, I'm basing this on my own experiences over the last few years.
Nice insights, thanks. My own experience has been quite different from yours, although I too have my own personal window into reality, mostly seeing those setups only once they fail to perform. It's true that quite a few points (more today than 2000) made by the paper are only valid if you don't work around them in your configuration. So I guess some or even much of my "hatred" of NFS comes from the default configuration that seems to assume that neither servers nor networks ever fail and there is only one admin that can be trusted above all. (Although especially this "one root to rule them all" philosophy combined with "every admin, even on some remote machine, is to be trusted implicitly" is more of a UNIX fail than specific to NFS. And given historical context, it isn't even a fail but something that reminds us of better times.)
That you think a SAN is a replacement for NFS says it all really.
we switched to git/github primarily because bitbucket had some downtime like this
You're right, I'd misremembered.
No need to be down on his work. I think it looks pretty good.
TIL bitbucket supports git now. I have a single private repo that I've just been hosting on my own server but if I can get it somewhere with a place to put tickets then I'm all for it. Really wish github boost their free tier to include one or two private repositories and then just increase all the paid tiers some. (edit) I can't grammar.
It started off with the question, can I make noise by writing a bunch of numbers to a wave file? The whole point was to play with sound, but now that it's more complicated, I'd like the addition of MIDI in the future.
You do know that NFS is the go-to technology for connecting high-performance SANs to the world's largest virtualization clusters, right? NFS over trunked 10GbE is the pinnacle of remote storage performance. The two technologies are not exclusive, they often work together. Take a look at NetApp specifically for more information about this. Also, that article is utter crap. The vast majority of the problems are not problems in a properly configured environment and the alternatives are laughable. E-Mail? Really? _Data Duplication_? 
Documentation's a little weak since recent updates, but it does far more than just render wave files. The biggest thing I've been working on lately is a full-fledged parser for a new kind of syntax I've been working on. [Details](https://github.com/prezjordan/Melopy/issues/33) Also has some tools for generating scales, fetching frequencies of notes, and I'm also experimenting with custom wave types. It's also polyphonic. If you look in examples you'll see that Fur Elise can be generated (both left and right hands)
Very cool - most similar libraries don't support coloured text on Windows.
Yes, I think matplotlib/pylab is great except for the hassle of tracking styles and subplots. I'd also like a simple interactive plot (say a slider to zoom or remove less important points) and a true 3d plot. I've rolled my own solutions one at a time but haven't tried to design a releasable module.
Neat, but it feels like three different modules tied together with twine.
Is there something here to discuss or is it just a rage face?
I guess the submitter found the joke funny? I dunno, seems only mildly amusing to me. Not worth quoting out of context.
&gt;What I really wanted to do was something that learned when a page updated; either through training (and hints from a human) or just lots of scripting... This is actually the same thing I thought of. The script would be running the whole time the pc is on and checking every few short time intervals, and to avoid them loading every time a new pops up make them load when the last comic on your list is published, if its published on the same day. But if you then take the average posting time you can't be sure the comic wont be posted later, so you would still have to check multiple times if the comic had not be published on your saved time.
Handy
Updated today: v 0.3.0 with Python 3 support.
&gt; Very cool - most similar libraries don't support coloured text on Windows. What's the deal with coloured text? I mean ... why does it matter? (Seriously, I don't understand what this does for people)
Two examples where colors could be useful: - Expected/unexpected result, you don't even need to read the result, you're just checking red/green. - If your app has lots output, and you want draw the attention to a particular line, it could help with backlog. - Well, if you're happy with some boring output, good for you! :P
Indentation fixed, something went wrong with the copy/paste, as I tried every code before. I'm giving emacs a try, after using Vim for awhile, it must be his fault! :P About English, if you point out one or two examples, maybe I'll learn faster from my mistakes. Thanks.
It's on github indeed, &gt; Here the post https://github.com/Nic0/nic0.github.com/blob/master/_posts/2012-01-05-clint-command-line-library-for-python.md &gt; I'll accept patch, pull-request, diff, sed/awk, whatever :p (it was last message on the next thread)
Notice your two last lines of each block are the same, thus can be put once after the 'if': if element == “Fire”: print “You have the fire element” print “Now choose your weapon: Sword, Mace, Axe” weapon = raw_input(“&gt; “) elif element == “Water”: print “You have the Water element” print “Now choose your weapon: Sword, Mace, Axe” weapon = raw_input(“&gt; “) elif element == “Earth”: print “You have the Earth element” print “Now choose your weapon: Sword, Mace, Axe” weapon = raw_input(“&gt; “) becomes: if element == “Fire”: print “You have the fire element” elif element == “Water”: print “You have the Water element” elif element == “Earth”: print “You have the Earth element” print “Now choose your weapon: Sword, Mace, Axe” weapon = raw_input(“&gt; “) (This is not exactly the same as you won't be able to choose a weapon if you mistype the element in your version.) Did you learn about dictionaries? This will make it possible to add elements without adding 'elif' all over the place: elements = {'fire': 'You have the fire element'} print elements.get(element)
The 'rtype' stuff in scales.py doesn't provide all that much. List, tuple and dict are easy for the consumer to do themselves: list(major_scale(...)) tuple(major_scale(...)) dict(enumerate(major_scale(...))) (I realize that list is redundant when the default container is a list, just threw it in for the sake of consistency) The string conversions may be convenient, but you could stick them in a function and then someone who already had a scale wouldn't need to regenerate it just to get a string representation.
IDLE should be enough at the beginning.
Komodo Edit is "enough". I'm using it on windows bu there is a mac version as well. http://www.activestate.com/komodo-edit/downloads
They want you to implement [Doomsday algorithm](http://en.wikipedia.org/wiki/Doomsday_rule).
I don't think this is the place to find people to do your homework for you... If you read the documentation on the various date functions you will realize they will tell you what day of the week it is for any given date. Take it from there. Also, last year was only 6 days ago. Maybe invest more than a few days into learning a language! :)
Black Text on Black Background?
Its from the official Django [tutorial](https://docs.djangoproject.com/en/dev/intro/tutorial03/#design-your-urls). That part of the docs is trying to illustrate that matched regular expressions in the url confs execute a python function, rather than urls executing that file on the filesystem. But, yeah, this is just a rage face. And maybe should be in r/Django.
English isn't my native language either so a few observations from my end. You use too many commas. For e.g. the first sentence can also be structured as: &gt; Indeed, I'm French. I started blogging in English seriously two months back. Similarly, the usage "I would be please" is incorrect. The correct usage is "I would be pleased". Again, we can re-structure this sentence as: &gt; I would appreciate if you, or someone else [...] Good luck learning English! :)
The point is to make the student look for the algorithm's that allow you to do this and then implement one of those as a python script. i.e. It is getting the student to do some research and then implement an algorithm (also the doomsday rule is quite an interesting "trick") which is *the right way to do it*. University is there to teach you to think, it's not a trade school! 
The resources module uses global variables and likely has various issues in larger applications. It might be nice to have things like print colored.warning('some warning message') instead of print colored.red('some warning message') the progress bar is nice though :-)
I'm not allowed to use any library, I must implement all by myself. I think it sounds weird but I've already made the program using python libraries but my professor said that was very easy.
For a lib, you can't really assume anything about the purpose of the color that would be used, if you name warning _instead of_ red, and if you display for example negatives numbers in red, so the *warning* won't make sense. warning = colored.red print warning('some warning message') This would make sens in your code I guess.
Yeah, that does make sense. It would be a good idea to separate the two, like you would using CSS. hardcoding colored.red everywhere seems like it would cause a maintenance nightmare if you had to change the colors around. 
But the whole philosophy of Python as a language is that there should be only one way to do something, and it should be really simple &amp; intuitive. Implementing an algorithm that's already implemented in Python's standard library just seems like reinventing the wheel. I agree with students needing to learn how to implement algorithms, but think they should have used something more bare-bones, like C, for the task.
doesn't envoy wrap subprocess?
i google it and i find the implentation of the algorithm in python: http://codepad.org/doU8ppRp
Does this *need* to be a package? If you just have `ConfigFileParser.py` and that's it, just leave it as a standalone module.
I don't know anything about `setuptools`, but I can't imagine it would force you to use packages when you don't need them.
But does it work with logging (module) to terminal? No it doesn't ... ;P
Is it actually trademarked, though?
Interesting. Do you know if the amonpy.DjangoExceptionMiddleware is a django specific middleware or can it be used with any WSGI app?
Code that is understandable by others?
Hi cabnnt1337. There look to be a couple of syntax errors: line 8: self.hp == CHP (CHP is undefined, == is equality). If you passed in, or had a global called CHP then you would do this: `self.hp = CHP` lines 9, 17, 25, 26 (Using == instead of = again) You may be thinking, but why is this not a syntax error? Because this is a valid expression. You're evaluating whether or not A (of A == B) is equal to B. This returns a value, either True or False. Whether you then do anything with this is irrelevant. The error you're receiving about `damage()` being unbound is because as it says, you have not created an instance of Player. For example: class a: class b: def bb(self): print 'bb' def cc(self): print 'cc' parent_class = a() parent_class.b.cc() &lt;---- This causes an error, you have not created an instance of class `b` child_class = a.b() chid_class.cc() &lt;----- Prints 'bb' and 'cc' \_\_init\_\_(self) is a method that is called immediately after an instance of a class is created: class Player: def __init__(self, startingHP, startingSP): self.startingHP = startingHP self.startingMP = startingMP self.speak('Hi!') def speak(self, message): print 'Player says: %s' % message new_player = Player() &lt;---- Syntax error, didn't pass enough arguments startingHP = 200 startingMP = 40 new_player = Player(startingHP, startingMP) output: 'Player says: Hi!' Personally, I would also avoid using imports at a class or method level; This is my personal preference, but I find that it makes code easier to read, and especially makes life a lot easier on larger projects contributed to by several developers. I think you should probably just stick these at the top of your file under your docstring. I hope that was of some help, if you need more feel free to PM me on here and we can chat on IRC or some other IM service.
Django middleware is generally something quite different from WSGI middleware. And https://github.com/martinrusev/amonpy/blob/master/amonpy/adapters/django_.py is Django middleware.
Interesting indeed. One thing I wouldn't do is use amonpy.log() in my apps though. It does an unconditional synchronous http POST without a configured timeout even. Logging stuff to Amon from the application itself could be interesting of course but I would probably do it using Celery or something similar, avoiding blocking I/O directly in the request handling code as much as possible. (And I do realize that starting a Celery task also entails a bit of blocking I/O, putting the task on the queue but that at least is something that I already grudgingly agreed my app has to live with (ie if the queue dies, I already have a huge problem, the Amon API HTTP interface would be a new potential failure))
Thanks for the info.
It's a perfectly valid concern and I am already working on that :) Behind the scenes Amon uses https://github.com/kennethreitz/requests to send data. Requests has a support for async requests ( with gevent ) and my idea is - if gevent is installed and Amon can successfully import it - it will trigger an async request, if gevent is not installed - it will use the standard sync request. P.S I have added 5 seconds timeout, that was definitely a bug. amonpy 0.2.6 is the version with that fix
\*slow clap\*
Hi. Yes this is my first programming language. My brother is doing Engineering at Uni and he got me interested in it. LTPTHW is a helpful resource to getting started, I'm a bit stuck on it at the moment and am just starting to play around with my own little projects. 
Hi, Thanks! I don't know about dictionaries - Thanks for the input :) 
Also, is there a way to have the amonpy module have a fallback so that if an application is installed on a server without amon configured, it logs in some sort of normal way? Excellent project, btw.
Not yet, sorry. My idea is to pass your default logging settings to amonpy and then it will just use the internal logger + it's own. The benefit of this approach is that - you will still be able to log dictionaries and tag the entries and amonpy will convert that data to string in the log file and to json for the web API
So what's involved in adding support for another OS, say, one of the BSDs?
Amon is more developer oriented and simpler - you can have it up and running in 2-3 minutes without and sys admin knowledge. Plus you can use it to log data from your web app and capture exceptions
It's not that complicated - only the collector part is related to the OS, but Linux is my main priority for now. Then MacOS - for the people that want to test it on their local machines. 
nevertheless, it's not a python question in that case, but a general programming question. the alorithm discussed above looks almost the same in most imperative languages.
You could use SSH tunnels, though that might be dumb.
Sure, but for every ten guys who read something in a tweet once, there's occasionally one who has actual experience. Then again, those guys usually don't reply with "fixed".
Cron. It already has a flexible schedule syntax, will always be running, logs events, and other goodness like companion tools to report success+failure of jobs (email and nagios or whatever you use). *edit - nagios.
FWIW, if you're a python purist, the stdlib already contains a "sched" module. But I use cron.
Celery is meant for running tasks far more frequently, say once every 30 seconds, at least usually. For something that simple anything other than cron is overkill.
A few things at http://amon.cx/guide/clients/python/ lead me to believe this system does not play well with accepted Python standards. The glaringly obvious one is WSGI. WSGI, the Web Server Gateway for Python, is the official system of integrating web applications (see http://www.python.org/dev/peps/pep-0333/). Virtually all Python web apps and frameworks support WSGI and it's the only way to go if you're running under the extremely common mod_wsgi handler for Apache. WSGI middleware is extremely simple. Why is WSGI middleware not included in this system? It should have been the *first* web framework integration added. The other one is logging. Python also has a standard, extensible logging system: http://docs.python.org/library/logging.html . It is again extremely easy to build your own handler object that would integrate into the amonpy logging system I see here. Why is this also not being pursued, in favor of a reinvented API ? Why would I want to have "amonpy.log" all throughout my application when I should be able to add it as a handler in my logging config ?
and apart from anything else, it means if something goes wrong, it'll get re-run next time instead of just hanging there
Extending the original BASIC syntax is a good idea. Also, having "O+" and "O-" indicate an increase or decrease in the octave rather than explicitly writing it would be an improvement. Here's an example of playing several different duration notes: PLAY "A8A8B4B4A8A8B4B4" 
It's a 15.4" laptop monitor at 1366x768, but it has nothing to do with the size of the text, it has to do with indentation, and how columns of characters look.
Yes, I found that logging handler too. The problem as it seems is - I have to add to many layers of abstraction to the standard logging module to achieve the same functionality and I think the people who will eventually use amonpy are the ones, who are not happy with the standard logging and are looking for a replacement. At least for me it looks that the logging in Python is headed this way - http://docs.python.org/library/logging.config.html#configuration-dictionary-schema I have absolutely no idea what these configuration rules do and I don't care enough to learn them. It's too much and too complicated. I just want something simple, like curl install.amon.cx | sh, pip install amonpy, amonpy.log, done 
I'd suggest considering using [Jenkins](http://jenkins-ci.org/). It basically gives you free monitoring of jobs and also allows you to manually kick off jobs if needed.
Can you please send me a screenshot ( with twitpic or something convenient for you ) so I can check what's really going on. You can send me the link in a personal message, here 
Or he could use a job queue like [beanstalkd](http://kr.github.com/beanstalkd/).
Absolutely. I once had to get a few folders in sync between buildings at work so I started with crontab+lock files through a little python script that runs every * * * * * in crontab. Just make sure if you have an admin on the other end, they don't start rebooting the server to solve every problem or you end up with stale locks and stalled syncs. Because of this I had to add stale lock detection and the ability to force syncs by an identifier. I also found it useful to make "lock regions" so that several syncs can be grouped up into one logical bunch. And a few other things. At this point I'm guessing I've re-implemented the wheel; there's probably a package to do this better. But hey, it works.
python3wos (superpowers) in 10, 9, 8 ...
If your script dies for some reason, your scheduling is gone as well. Scheduling a cron job is preferable.
You could just add a @reboot entry in cron to delete the lock, couldn't you? Edit: assuming you're using a cron that has @reboot, which you probably are...
no becasuse i had additional requirements. 
I've found that daemontools/runit are much easier to work with when it comes to jobs like these. you can simply make your 'run' script do something like: process_data sleep 30 and it will have the similar effect as running while true; do process_data sleep 30 done but you get the added benefit of being able to use the related tools for starting/stopping the jobs. I run a number of long running jobs this way that don't really have a schedule, they should just have 'X minutes' between runs. Really, it would just be nice if cron let you natively with jobs like: @interval 5 minutes command 
Exactly. Rolling your own system won't have these features. When people reinvent the wheel, they forget there's more to wheels than just being round.
The latest rhel packages are moving away from unit.d to system-config you can see it in fedora 16. Does it help with this stuff? I have no idea.
My first thought was that this looks like a lot of work compared to the old standby, ImageMagick: convert *.jpg comic.pdf and even with the "treat filenames as fields" issue, it's still "merely" convert jia_??.jpg jia_???.jpg comic.pdf But I'm in the middle of a project to produce a little image-heavy partly auto-generated magazinelet, where OpenOffice was just wrong, `wkhtmltopdf` failed on background images, Scribus UI made me consider scissors and tape as an alternative, TeX wasn't (sanely) programmable enough, and I'd finally started coding something up using the Cairo python bindings and `cairo.PDFSurface` and actually got a first full draft produced, though it takes seconds per page... and I see this post, and vaguely recall seeing a reportlab based conference-program generator at EuroPython years and years ago... the end result: * the reportlab API matches very closely the one I'd wrapped around Cairo, so it was a quick conversion * the reportlab version of the tool is around *fifty times* faster than the Cairo version (that may yet turn out to be my fault, but *wow* does that make a difference to my iteration cycle.) So, thanks for posting about it!
Yeah, sophisticated cron jobs (e.g. those that create state) need a wrapper that does the locking (locking of this sort is something else you want to avoid in your own program) and cleanup, and notification if something fails, etc. There isn't one true cron wrapper. There are a lot of good single-purpose tools, though. At my last job there was a wrapper that did "everything" but it was confusing as hell. Then there are individual wrappers for particular jobs (e.g. google tells me that (lockrun)[http://www.unixwiz.net/tools/lockrun.html] exists).
The limitation that the log has to be a string is a big one. I very much agree that being able to log a data structure as json to be latter analyzed is much better. Though support for the python log is also a nice thing to have so we can improve the logging of existing application fast. 
I've used subprocess to spawn find. Unsure if there's anything internal to Python that would be as robust as find though.
Yes I have the option of using os or subprocess and do **find** itself. I was thinking there will be better python module.
Yep. See http://stackoverflow.com/questions/2186525/use-a-glob-to-find-files-recursively-in-python
Why not use http://docs.python.org/library/contextlib.html#contextlib.nested?
Thanks a lot. I think there is a limit of 200 tweets from the Twitter API. 
Side note: The same works in Python 3.
&gt;Republicans tweet a lot more than democrats. No surprise here. The Republican team have an impressive marketing machine. 
At least when I thought about it - the Amon logging API fits more or less in the web app world. The perfect use case would be - you have a web app in Django. In the same app you have a small node.js part ( customer service chat ) and then you have legacy PHP code. That's where Amon will fit perfectly, because - you will have one system that will work in the same way for all the languages your are using. But for libraries - I don't think that's a good idea, because even Amon logs in the standard way internally. 
And/or they need to try harder than the Democrats to appeal to people in the age groups who use Twitter the most.
An interesting feature of `os.walk` is that if you modify the dirs array, it will not recurse into those, so you can actually prune the tree.
The ugliness of the code and the amount of disagreement assures me that we are talking about Python.
Sweet, that be a good idea. 
Interesting. From your numbers Dems and Reps post nearly the same amount of tweets and words per tweets. The reason there is a lot more Rep tweets seem to be because there simply is a lot more tweeting Reps. But Reps have a general numerical superiority in congress. So is an Rep congressmember actually more likely to tweet than a democrat?
Now, that's just dirty.
To give a real answer. You need to make sure that the code will ALWAYS properly close the file. Even if an exception was thrown while parsing the file. So really that code should in a try-except-finally block to do what you really expect it to do.
Ubuntu has used upstart by default for quite some time now. There is also not really an ubuntu server distro (just like there really isn't a KDE or XFCE distro) It's all the same distribution, the only thing that changes is what packages are installed by default. Their 'server' installer is just the text based installer that doesn't install any of the desktop metapackages. I mention this only because it seems people do things like completely reinstall because they want to switch from ubuntu to xubuntu, when all they have to do is install xubuntu-desktop.
Newb here. Can I ask, it doesn't seem like most of what OP discusses in this first article shows off the features of NLTK (which I've only briefly read a description of) - most of this analysis seems like it could be accomplished with re. Can someone discuss which features of NLTK were important here, and if it was better for things like counting appearances of words and terms, why it was better than re or any other string processing package? Thanks!
I bet she reads it for the articles.
I hope she's trying to learn.
As a freshman in Python, she likely knows more than I do. &amp;#3232;\_&amp;#3232;
Oh my goodness. This is just like how I would read calculus books for the pictures at her age.
That's adorable. She'll be hand optimizing bytecode before you know it, mate!
what about C? (but first, letters :D ) 
in this case, NLTK wasn't necessary. i'm assuming that he's going to do more advanced natural language processing in the future, so that's why he chose to build it using NLTK (which has pretty huge overhead). computing word-frequencies is simple, and you're correct, re would suffice for the analyses currently shown on the site. NLTK has a built-in "freqdist" class (http://61.153.44.88/nltk/0.9.5/api/nltk.probability.FreqDist-class.html) that's supposed to make it easier to calculate frequency distributions; but honestly, ditching NLTK, you can do it in less lines, with less overhead.
Nothing at all wrong with C - but I think Python will be simpler for her to learn once she's 6 or 7. After she has the basics of control flow, lists, and OO, I want to take her into SQL for a while so she doesn't get set in her ways with either procedural or OO programming, and gets some exposure to set manipulation.
&gt;My heart, however, has always been and continues to be with NumPy and SciPy which need more support than Enthought can currently provide --- so I must move on [the new company: Continuum](http://www.continuum.io/)
I brought a related point up on [r/aiclass](http://www.reddit.com/r/aiclass/comments/o0qtw/using_nltk_on_the_aiclass_minishredder_problem/) and yes, you can blast out some python code that works - but if you can take the time to learn to use NLTK for these standard things, you can take advantage of the *other* tools that NLTK gives you (like `FreqDist.plot` or deciding that `LaplaceProbDist` fits the problem better), plus you get the added benefit that someone else looking at your code can tell more quickly (and convincingly) what's going on - "Oh, that's Lidstone smoothing" vs. "Hmm, the comments *say* that's a well known smoothing technique, I wonder if it really is..."
One of the downsides to living in a non-English-speaking country is that this kind of stuff is further off for my similarly aged daughter :-/
You mean starting at the foreword. :-) Anyway, I loved studying the dictionary and the phone directory when I was her age. Such predictable order in those once I started noticing it. 
you can use together cront &amp; celery (depends on what you need): * buildout - to put in crontab: * @reboot /path/your/project/bin/supervisord * * * * * * /path/your/project/bin/some-freq-util.sh * ... * supervisor - to spawn f.i. your own monitored celery instances i work this way
Yep. I noticed patterns in the menu items at the greasy spoon my family ate at each week. Once my parents pointed out "soup," "steak," "chicken," and "ice cream" to me, I realized the menu items were grouped in the order we ate them and ordered in each group by price. Then I was able to begin decoding the entire menu. This required more visits, of course. 
Cool. Would be nice to see some examples that output to speakers, eg maybe using pyaudio. On the reading side, some of the features that the java based minim library has would be useful (especially if reading mic/linein).
pylogo ([edit] i typed that, then looked it up. - yep, it exists)
Expect to see 'Explain to me like I am three years old' questions to appear in python newsgroup soon
I really don't like what the background of that page does when you scroll down. :O
&lt;offtopic&gt; Reddit really needs to improve its image selection mechanism... Even though Caraciolo is a great guy, he is not the GUY on this item ;-) &lt;/offtopic&gt;
Have a look at grin: http://pypi.python.org/pypi/grin It also has a command line tool called "grind" that works like find.
Way to go. I wish my parents introduced me to programming at that age. I had to discover it much later in middle school. So much lost time...
The upside though is that your daughter will be a bi-lingual programmer. Skip straight to step 4 profit!.
My daughter is still one step ahead. http://imgur.com/quZOJ 
Sounds redundant.
Pointy things are not appropriate toys for children.
That doesn't change what he said. I just pulled out the first edition of Programming Python by Mark Lutz, which came out in 1996 and is written for Python 1.3. If I started reading that today, by the time I finished and felt comfortable having learned Python, a lot of it would be immediately obsolete.
Thats what I call quality parenting, have her give a talk at PyCon next year!
here's the [hardware](http://imgur.com/mPpCm) version
We're having trouble with language, actually - there are no other languages spoken near us. It's just English (and a hillbilly dialect at that). What language do you and your daughter speak? I've been considering setting up Skype for her with another child in a foreign country. I'm thinking Arabic, Mandarin, Farsi, German, or Japanese, but haven't really settled yet. Something like Tamil could be useful too...
I don't think I'm great at it, I just work hard. She's only 3, but she's showing aptitude for math and science. She can do basic addition and subtraction in her head (up to 3 or 4), and she's (like all kids) constantly asking questions. A couple of days ago, she asked "Daddy, why do I have blood?" - so I started with "it helps take the food you eat to all your parts, so they can move", and increased detail as she continued to ask questions. I think my kid is smart, but not in a statistically significantly different way than her mother and I. I just remember growing up and how much I hated it when adults spoke in absolutes, as if "because" was a valid answer. I always make it a point to tell her the truth, if sometimes abstracted out to be appropriate for her level of reasoning. If you like programming, you'll like having kids - it's like a huge undocumented API, where you input different things and get unpredictable and exciting results out the other side.
I'd totally shoot her if I was getting selected for Men In Black.
I know this has very, very little to do with python, but I thought you guys would be the few who might find it funny. It probably would've been even less suited to /r/funny, so yeah. Basically: Sorry for the off-topicness, but take it as a something light-hearted to cheer up your day!
So the misspelling of grammar is just ironic then?
Thanks! I enjoyed it. Looks like you're getting quite a response.
This bot is awesome. Good work.
That's unfortunate. We tend to stay up late as a family, partly because I manage a small team with my side business, located the Philippines, India, and Pakistan. Nothing against Dutch, but I don't see it as offering her the edge in her professional life that others might.
Any twitter bot is a good bot. Well done.
*Danish*, not Dutch ;)
Bleh. I know the difference, I misread it. :(
Yup. (see https://twitter.com/#!/grammer_man/status/156029191148670976 )
 from contraception import vasectomy self.vasectomy()
Swiss, Swedish, it's all the same...
That's pretty much most of the questions in /r/Python these days.
Can you make it correct people who say "could care less" or "begs the question"?
Look at the TheTvDb API : http://thetvdb.com/wiki/index.php?title=Programmers_API I think that TheRenamer (http://www.therenamer.com/) uses it and it is very good (tvnamer uses it too apparently).
The Django team broached this subject a while ago with the quote from the top answer below: http://stackoverflow.com/questions/676931/creating-an-entire-web-application-using-django-admin I don't think they're saying you absolutely, positively can't expose it, but that it would almost certainly be a terrible idea unless you're talking about a tiny user base of a few trusted individuals. I personally wouldn't dream of using it as my web application for anything besides its intended purpose.
out of curiosity, how many different message formats do you have?
I like how some people are getting really snotty about it &gt; his country grammar ass better take a seat before I "come for" his bald and not so beautiful ass! is a particular highlight for me, although it's kind of sad how many people seem to think a threat of violence over something as minor as correcting someone's spelling is an appropriate response. Wonder how they'd feel if they realised it was a bot?
Sure, some of the knowledge of the standard lib may be obsolete. But I'm mainly referring to the general frame-of-mind and problem solving skills learning to program creates. That never goes out of style.
That is the perfect profile picture for the bot.
It's really just a general statement about needing to stay up to date. If a shark stops swimming, it dies. Similar things happen to programmers that stop learning.
`check_if_done` should use anything but a pickle for what it is doing. as that list gets big it will start spending a lot of time re-writing that file. the simplest approach would be to use anydbm: import anydbm d = anydbm.open('done.db', 'c') def update_done(d, id): d[id] = "1" d.sync() def check_if_done(d, id): return id in d or minimally, if you don't want to change it, at least do this: .... f = open('done.pkl.new', 'w') pickle.dump(done, f) f.close() os.rename('done.pkl.new', 'done.pkl')
I am trying to teach my nephews how to use Python. They are ten and eleven years old. At that age they can learn the syntax pretty well, but they still don't have the logic skills yet and how to put concepts together. They can do good syntax with a for loop or with random numbers. But tell them to generate 50 random numbers using a for loop and they are lost. I would say that age 13 is probably a good time to get them started on Python. Kids are pretty interested in programming, but the problem very often is their attention span...
There are several. I also know of https://github.com/Kronuz/pyScss .
heh, that issue is why I stopped looking at django when it first came out. I guess things aren't that much better now.
Sounds brilliant! Yeah, I'm the same, I absolutely hated it when my parents said I'm too young to understand something without trying me first. 
Next bot idea: @PEP8Purist 
Then, it depends NumPy stands for numpy module or Numeric Python. [Travis reunified the community](http://www.scipy.org/History_of_SciPy/) , and &gt; I sacrificed a year of my life in 1999 (delaying my PhD graduation by at least 6-12 months) bringing SciPy to life. I sacrificed my tenure-track position in academia bringing NumPy to life in 2005. Constraints of keeping my family fed, clothed, and housed seem to keep me on this 6-7 year sabbatical-like cycle for SciPy/NumPy but it looks like next year I will finally be in a position to spend substantial time and take the next steps with NumPy to help it progress to the next stage.[1] [1]: http://technicaldiscovery.blogspot.com/2011/10/thoughts-on-porting-numpy-to-pypy.html
yeah i'm saying multilingual speakers end up knowing english better than monolingual speakers anyway.
You should modify it to scan an entire tweet when it finds one with a spelling mistake, and make comments on all of the spelling mistakes in the tweet, not just the one. That would make it seem more like a grammar nazi. EDIT: Oh but a problem with that is that some people write in txt language, or just can't spell any words in their sentence. It would be weird for a twitter user to correct like 30 spelling mistakes in a generic tweet.
Mild lulz, but most of the people that were angry were that way because the bot fucked up! I'd add a threshold so if the entire post was garbled (or not english) it didn't try to correct it. You should also ignore posts that are retweeted or start with RT.
Thanks. To be perfectly honest, finding the picture was probably what I was happiest about.
Only about 70. Friends chipped in and we threw them together very quickly in a google docs document. Additions are very, very welcome!
http://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines There is a machine-readable version of the page. This might make things a little easier for you.
Thank you, I had never heard that before. I agree that it's kind of unreasonable though. Personally, I would say that since the second meaning is so much more widely known, it *is* acceptable, and arguably has replaced the original denotation. Language evolves over time, no sense in holding people to standards and rules which aren't still around.
Just to be clear, I'm not saying I want to expose it -- quite the contrary. I just want to know if there is an out-of-the-box way to re-use the functionality for your client-facing services. All that stuff about, "wow, look at all the functionality you get with just a few lines of code and uncommenting some stuff" is great. But unfortunately the functionality is wasted on me, the admin, and a couple of staff. I'd be really ecstatic if it were something I could offer to users as well.
Thank you!
Not according to my logic. Change print(a * b) to print(a, b) and you'll see that it goes 100 100 100 101 100 102 100 103 100 104 100 105 100 106 100 107 100 108 100 109 100 110 100 111 100 112 100 113 100 114 100 115 etc. So, as far as I can tell, that's exactly what you asked. 100 * 100..999, 101 * 100..999, etc.
It'll be just as much a hit as this [whitespace cleaning bot](http://www.reddit.com/r/programming/comments/nl85i/this_is_my_new_favorite_robot_on_github/).
Some could be complaing they care too much, or be logicians! It'll be embarrassing.