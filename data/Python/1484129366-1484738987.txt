Python 2.8 wouldn't solve this problem for libraries, because many of those using Python 2.7 will be unable or unwilling to upgrade to a Python 2.8. As evidence of this problem, see the continued wide use of Python 2.6, despite the fact that it is entirely unsupported by upstream. Most major Python libraries are still compelled to support Python 2.6, especially when they provide fairly vital functionality (e.g. pip and requests need to keep supporting it). However, your key argument is right: most libraries need to support Python 2, and those libraries cannot use type hints very well.
Hypothesis is peculiar indeed. I know equivalence classes of my method and I pick the member of each to use within a test. Fuzzing is another story.
It's worth bearing in mind the costs of migrating code that already exists. I briefly took a look at adding typing hints to [hyper-h2](https://python-hyper.org/h2/). Even just going into one relatively self-contained module, this was an enormous amount of work, because I rapidly found that to get any use out of these type hints I was going to have to type hint *the entire library* more or less at once. To avoid type hinting the entire library, I would have to do one of two things: 1. Only type hint the user-facing API functions and leave no hints internally. This provides no value to the developers of the module, and also makes it very difficult to enforce the requirement that our type hints be accurate. Worse, though, I would need to maintain *two* sets of type hints. This is because Sphinx is not capable of extracting the type hints from magic type comments at this time (I have to use magic type comments because I support Python 2 as well as 3). This means I need to keep the Sphinx docstring type notes as well as the magic comments, which is a frustrating duplication of information. While I could write a patch to give Sphinx access to those types, that's really more work than I want to do for something that doesn't provide me any value. 2. I could type hint only standalone functions. That's not helpful to me unless I propagate those hints throughout the module, because it's a library and therefore ultimately receives most of the data it operates on from the user, not from me. So the TL;DR is that to use the type hints for pre-existing library code takes a lot of work to start seeing any value. That investment is simply not worthwhile to me at this stage: I'd rather spend that time writing more and better tests. They'll help me more. =D
You don't have to type hint the entire library, only functions containing hints are type checked so you can pretty much doing progressively. Also mypy has plenty of parameters to silent e.g. import errors to make it more or less strict.
I addressed this in the parent comment. I indicated that my major options for type hinting in a *library* are extremely limited in utility, because most of the actual data I work on comes from my users rather than from my own code. That means that the type hinter doesn't do very much of use unless I type hint the whole way up to my user-facing API. The effort-value tradeoff just hasn't made it worthwhile for me.
And not to mention, clever. 
One point is that type checking isn't useful for most python applications. Don't get me wrong, it's a good thing if you take the time to put it in place as it helps finding more bugs, but for the majority of small applications it's just not needed. And most people choose python because their application was not complex enough for it to be needed in the first place, otherwise they'd have used java or such. Therefore the main targets for type hinting are the big python projects with much legacy, things like django or twisted which grew too complex to stay without proper type control and can't realistically be rewritten in another language. Consequently while there are real gain for the python community as a whole through this projects they don't cast the image of popularity for this feature.
Ditto here. We're not Google, and thus we can't create a whole new language or runtime just to support our effort to move to Python 3. Supporting 2.7 though is well within our capabilities. That said... Python 3 is pretty good, and if I could start a new project today I would use it.
You should look into pyhook. It's on pypi.
Hi all. I've made a viewer for HDF-5, NetCDF and some other data formats that can be represented as Numpy arrays. It's implemented in Python and Qt. Please have a look, feedback is welcome.
Take a look at [Toga](http://pybee.org/project/projects/libraries/toga/) its idea is great. Tbh I haven't used it before. But it seems good 
`setup.py`
Actually type hints work on python 2.7, just not using annotations. You can use special comments for it.
And those annotations are exactly what OP was asking about.
All I read is speculations. I'd rather give you a feedback from practice (I'm a professional trainer and dev, I've been expose to a lot of code and coders, including using type hints). - type hints are not well know. A lot of people still don't know about it. - people that know about it didn't take time to learn about it. Typing is not that popular amongs Python dev. They are popular amongs dev in big teams or comming from stricter languages. The feature is here to accomodate them. - a lot of Python dev are not... dev. I work with teachers, geogramaticians, mathematicians, etc. They could not care less about code, it's just a mean to an end, and typing just get in their way. - a lot of dev are not... good dev. I works with a lot of people making loads of money 20k/month) with badly coded websites. They don't know a lot of about programming, just enough to run their business. They don't want to invest into something new, like type hints, that make things cleaners. Clean is not their problem. Now there is a small minority of people, like me, who are interested in that feature. They used it. Here is there result: - the documentation sucks. The tutorials suck. You basically spend hours just to express anything more complex than the basic types. - it's not ready : mypy and pycharm support is partial, and sometimes buggy. I had sometime to use some very stupid hacks: https://github.com/Tygs/ww/blob/master/src/ww/types.py - setuping mypy is much more work that it should be. - the spec is incomplete: dunder methods are not detected yet, which means duck typing cannot be easily defined in your annotation. - it's takes a lot of time to figure out how to do anything. - even if you do everything right, the tooling doesn't give you that much safety net yet. All in all, I'm still considering type hints a good things. It needs to happen. But in it's current state, it's a pain.
Wheels work just as well on linux as any other OS, so long as you don't assume a wheel build on one random linux system will work on another random linux system. Hence manylinux, which defines what must be the environment that wheels should be built on to be most compatible. These will not run on any linux system (think different cpu architectures, or different libcs), but that's the same as expecting an macOS wheel to run on linux because both are UNIX. Additionally, if you're actually paranoid about subdependencies, you need to check the setup.py, pip has no control over setup_requires, in which case you may as well check the subdependencies setup.py (plus their subdependencies etc.), so I don't see how a requirements.txt is at all relevant here.
Yup. :) I'm the founder of http://hf.cx In my spare time, I'm building a search engine for startup news. 
1. That's exactly why wheels are handicapped on Linux. If they're pure python, they'll probably work but compiled extensions won't. 2. Wheels don't run setup.py at all 3. `--no-deps` won't install dependencies with a source distribution forcing you to install them on your own. Giving you a chance to vet them before installation of you're info that. Usually I check if their setup.py does any funny business. I don't always do `--no-deps`, for example packages I trust like Flask or SQLAlchemy, or is I'm working in a throw away docker container. But you could put some hanky business in there like `os.system('rm -rf /')` which will remove any file path the installing user can remove. Given the amount of `sudo pip install` I see recommended in readme files, I'm surprised this sort of attack hasn't happened. 
Do you use 3.6 and the latest PyCharm? I don't, but the change logs seem to indicate a lot of improvement.
&gt; ABCs don't really fulfill that role properly Could you elaborate on that? I'm curious about the shortcomings. 
This trick is used in debian python-django packaging: https://anonscm.debian.org/cgit/python-modules/packages/python-django.git/tree/debian/django-admin
Same but using pyside to avoid license fees.
&gt;Give me an example of an argument you would need to type hint and couldn't If it needs to be type hinted then it probably can (typical of domain-specific apps). If it doesn't need to be type hinted (typical of general purpose libs) then there's a good chance that it can't. Take the following very common pattern: def foo(bar): do_something(bar() if callable(bar) else bar) There are of course much more trivial cases such as `*args, **kwargs`. These will obviously get documented in the documentation, not in the hint (the fact that these are a list/tuple and a dict are already hinted by the `*`/`**`). Unless of course you insist on hinting everything as `object`. 
&gt; Don't get me wrong, it's a good thing if you take the time to put it in place as it helps finding more bugs, but for the majority of small applications it's just not needed. Not needed for what? I find it has made reading apps much better when I come back to it after a few weeks for instance. It has become an habit to use everywhere even for small scripts. The only downside is that it can be too noise on the screen.
Do you have a roadmap? Any planned features? At a glance it looks very attractive. I use HDF View, which could use some polishing, so any competition would be welcome. 
Something like `typing.Union[typing.Callable[..., int], int]` seems like it would do the job (replacing `int` with whatever types `do_something` accepts). You can also hint *args without even needing the `typing` module - from [PEP 484](https://www.python.org/dev/peps/pep-0484/#id35): &gt; Arbitrary argument lists can as well be type annotated, so that the definition: &gt; &gt; def foo(*args: str, **kwds: int): ... &gt; &gt; is acceptable and it means that, e.g., all of the following represent function calls with valid types of arguments: &gt; &gt; foo('a', 'b', 'c') &gt; foo(x=1, y=2) &gt; foo('', z=0)
Yeah slicing volumetric data is definitely more something for VTK, Mayavi, pyqtgraph or some other library designed for working with that data. Matplotlib is very weak on 3d stuff, but there are lots of other libraries depending on your data or scientific discipline. Every year the scipy or pydata conferences have videos of yet another custom library for working with and plotting multidimensional data.
Bashing Python, in the Python subreddit, due to problems that are almost certainly caused by your own ignorance is unlikely to win you any favors. Doubly so when you fail to actually specify your problem. Purging Python from your system isn't going to solve your problem. In fact even attempting to do so is likely to break your Debian install as significant portions of the system depend on it. You're going to have to fix the damage you've caused your system. What's your current issue?
You find the test logic more difficult to write? See [my other comment](https://www.reddit.com/r/Python/comments/5n4jz7/what_every_python_project_should_have/dca263z/) in which I explain why I think pytest is actually easier to write. But of course, don't change for the sake of changing and stick with whatever works best for you. Regarding the "packages from pypi not available": You can create a really simple mirror by just downloading the zip files you need, throwing them in a directory on a webserver ([an automatic Apache directory listing is sufficient](https://wiki.apache.org/httpd/DirectoryListings)), and use them using `pip install -f http://your-server/ bla`.
A 2.8 with features only for transitioning to 3 would be nice imo. I'm doing the transition at work and it's painful and slow. For some it's easy but don't dismiss us who are having a tough time. You want to help? Fix splunklib for py3. Which is problematic because splunk aren't being very cooperative and their docs/tests seem a bit lacking. That type of stuff is no fun. 
I can use autoit or autohotkey but for second one macro rec was dropper ( however it still possible to find it in google) There are other tools like micro recorder if I recall name properly
It's just extremely ugly. It provides nothing more than what well named variables, docstrings and unit test already do. 
Are the join criteria column-to-column, or column-to-host-variable? Can't really help much without seeing any of the SQL.
column-to-column I cant provide you with SQL because of work policy.. 
&gt; geogramaticians That's a new one. Like a mathographist.
Figured as much. Thanks. Just wanted to be sure that my use case would be supported easily enough or if someone would point me in another direction. 
If you start argos with the `--reset` option you should at least be able to start up argos and view the data in a table. If you then look in the `About Argos` dialog from the `Help` menu, you get a list if the installed dependencies and their version numbers. If you post this information here I can investigate further this evening . 
I have donated to jupyter and pandas through numfocus. Does that money help support your company in some way? If not, how can I contribute? A few years ago, I was looking into a way to process a large excel file to put into my access database and I stumbled upon pandas and using anaconda to install it. One thing led to another and now I use Python for everything (Postgres for the database side). It has been incredibly fun and interesting and it has helped my career growth. I've been hiring some Python folks to join my team, but the corporation is heavily built on Microsoft (excel, sql server, .net, azure). I really hope there is a regime shift eventually to Python and open source software. 
I always wonder how difficult it really is when using the 2to3 tool?
There are many languages with varying levels of typing strength and they all have a place. But Python has no static type system at all, only dynamic typing, in python everything is a class, and the only type constraint I am aware of is that you cannot redefine those classes that are part of the `builtins` module. Like everything else in life this has its advantages and disadvantages. If you want to write a general purpose library then dynamic typing is a huge advantage because it gives you the flexibility to define the contract with its users in whatever terms that are most appropriate for the task at hand. But if you write a domain-specific application that must break on a wrong type then Python doesn't give you the tools to validate your input contracts and express your output promises within the language's vocabulary. Trying to teach the compiler to reject mismatching calls means introducing static typing, which doesn't quite blend with the Python philosophy. If you want a language that is close to Python in usability yet offers static typing then the closest I can think of is Go (but that's just an opinion, not something one can "prove").
It's part of the standard library, you don't need to install it. Also, post questions in /r/learnpython next time.
&gt; even if you do everything right, the tooling doesn't give you that much safety net yet. Me this week: Write def foo() -&gt; MyClass: obj = MyClass() return obj.dict_attribute # Oops! ... obj = foo() # Thought I was getting MyClass() print(obj.attribute_from_MyClass) # AttributeError - Mypy passes. Tests that mock `foo.return_value = MyClass()` pass. We push to staging. - Staging: *LOL! nope boom* - Me: WTF? I would have expected Mypy to flag that `foo()` wasn't returning a `MyClass` instance, but it cheerfully let that one slip through. Type checking looks very promising. We're not heaving invested in it yet but it's already caught some bugs we probably wouldn't have noticed otherwise. We basically treat it as another linter. However, I don't have a lot of faith in it just yet.
I tried to translate "geomaticien / géographe" from french to english, and my mind farted.
Thanks for providing an example, that could be useful, indeed. However, I also don't have anything against the alternative way (for loops), which I find more readable (aka, even a Python beginner can immediately see what's going on). Or you could use itertools to avoid to much nesting, of course for comb in itertools.product(*(range(10), ['a', 'b', 'c'], range(100))): check_single_case(*comb) However, one disadvantage of this embarrassingly parallel task is that it runs sequentially. Do you know if pytest has/is planning to add optional multiprocessing support? 
I only read "type hints" in the post, not "annotations". You can use type hints without annotation. 
You should really just do it with JavaScript. Your extension will inject a custom JavaScript file into the Twitter page, and then you can extract the tweet text from the DOM and update it to whatever you want.
Two of pythons greatest strengths are - duck typing. - lack of boilerplate / balance of verbosity (that is; not so terse as to look like noise, not so much typing to be tedious) / reads like English. Type hinting breaks both of those. It also breaks the following, but so has many other "improvements" to Python - one obvious way to do it. - being concise and simple on the surface providing shallow learning curve and easy and complete complete comprehension of landuage. I wish people would stop trying to make Python be everything to everyone. It's OK to have more than one tool in your toolbox.
I just meant hints by the way. Edited the post to clarify.
If youre gonna start from zero using Django, you should document it and make a tutorial. I make this in a regular basis (from csv/txt/xls to mysql) and one of the problems I have encounter for automatization is the lack of a user friendly frontend. I would like to do something with Django, but i dont know where to start. Anyways hope you succeed, greetings. 
Cool. Thanks 
Here's the documentation in Flask. Seems pretty straight forward for what you're talking about: http://flask.pocoo.org/docs/0.12/patterns/fileuploads/
This subject comes up a bit in an old TalkPython episode from 2015 : https://talkpython.fm/episodes/show/36/python-ides-with-the-pycharm-team Unless i am mistaken the Pycharm dev was more of the opinion that typehints were particularly useful for creating something equivalent of a header file for common python packages. This would allow IDE's to leverage the information for static analysis, without creating the huge amount of clutter in every day python development or slow down development to accommodate typehints.
Are you sure there aren't column name conflict ? like in "select * from table1 inner join table2 on ..." if both tables have some common column's name, maybe you are getting a NULL value for these columns because one of the tables have null values. Also check if the number of records from TOAD match the len(data).
You could run both in their own thread, but doing so would further complicate your application. I'm assuming that you're adopting Tornado onto an existing Flask application, which means doing everything in just Tornado is out of the question. My suggestion is to build two separate applications and have a communication layer underneath, whatever best meets your needs. Then you run apache/nginx as a reverse-proxy to forward the requests to the appropriate application depending on the URL. 
I would recommend setting up aliases for tables. Eg. Select u.userid from users u. Sql developer and others might already be doing this under the hood for you.
Can you be more specific about your role in your industry where you think Python could help? I'm a bit confused by the post.
This is awesome. Your code is well commented and easy to understand. I have been learning PyQt recently and this is very helpful. 
Happy cake-day!
What is this about?
For what it's worth, we use Python with oracle a lot at work and never had any problems like this although I don't think we've ever managed to create 300 line queries... I doubt it's a Python issue though. It's either something that happens in Oracle deveoper under the hood as /u/weez09 said or, most likely some bug with cx_Oracle. That's where I'd start looking if i were you.
Maybe this can help: https://wiki.python.org/moin/ParallelProcessing Could you not split the data into parts and have different groups of cores processing these parts of the data. Then compile the results at the end
Even though the project hasn't been active, [obiwan](https://github.com/williame/obiwan) is a nice typing tool (mypy alternative) that acts better in this case: #!/usr/bin/env python3 from obiwan import install_obiwan_runtime_check install_obiwan_runtime_check() class MyClass: def __init__(self): self.dict_attribute = None def foo() -&gt; MyClass: obj = MyClass() return obj.dict_attribute # Oops! if __name__ == '__main__': obj = foo() # Thought I was getting MyClass() Running that reports: Traceback (most recent call last): File "./obitest.py", line 18, in &lt;module&gt; obj = foo() # Thought I was getting MyClass() File "./obitest.py", line 14, in foo return obj.dict_attribute # Oops! File "./venv/lib/python3.6/site-packages/obiwan/__init__.py", line 407, in _runtime_checker duckable(arg, constraint, "%s()-&gt;" % frame.f_code.co_name) File "./venv/lib/python3.6/site-packages/obiwan/__init__.py", line 312, in duckable raise ObiwanError("%s is %s but should be %s" % (ctx, obj, template)) obiwan.ObiwanError: foo()-&gt; is None but should be &lt;class '__main__.MyClass'&gt; 
"geographer" perhaps?
Better yet: play with Rust, and you'll have some valuable skills for your python work, too. Rust can integrate really well with Python for ultraperformant, ultrasafe modules.
Are you able to count how many words are in a set and divide them into groups based on how many cores you want to use. For example: We group the cores into 10 groups of 2. Divide the Set by that. This gives us 10 sets of 2 million words. You can then assign those groups to be processed by one of the groups of cores. I did a quick search on Google and found this: http://stackoverflow.com/questions/4519951/assigning-multiple-cores-to-a-python-program they recommend parallel python and give some examples. Hope this helps.
As a fulltime python developer (and I have been developing with python since the turn of the century), Type annotations and type hinting generally create extra verbosity and it honestly makes the code more difficult to actually read. Now I would also argue that good code is well documented and provides examples (you can see a lot of great code documentation with the python standard libraries). And that when you write code, especially if you're a developer, you'll have decent tests. These things are all policy based (i.e. developer discipline). For any large team, a single line change would likely necessitate: an update to code documentation, an update to the changes documentation, an update to tests, a rerun of unit level tests and probably a rerun of regression level testing at the system level. Additional changes might include customer/user facing documentation, new build generation (e.g. docker, iso, etc.), CI/CD updates, etc. All for a single line of code. And these changes are completely independent of the language or toolset you've been using. With python, the code is extremely succinct yet still human readable. If you're coming from a different language and you're used to types, then it may be useful to consider thinking about the problem differently. As with every language, python has a way of thinking. You can choose to align your thoughts with how Python works and find that types are largely overrated. 
That ship has already sailed for me. Doing more and more in Rust.
That's it, thank you! I made sure to use the right type for my variables but ignored treating the fraction as a float, thank you so much :)
Is there something in particular about 3.5 that you're thinking about? Any general guide should be broadly applicable, even one that uses 2.7 (some stdlib module names might change). The "hard" part about testing is knowing how to write your code so it's testable, and that's almost language-independent.
If you want interactivity with the browser (as opposed to automated scraping), you could either use Selenium to manipulate the browser or mitmproxy to monitor the requests. Both of those are heavy-handed, and neither will work as an easy browser extension that you could install from the Chrome Store, Add-ons for Firefox, etc. If you want code in some arbitrary user's browser, you *must* use JavaScript (or a language that compiles to JS).
Senior Backend Developer at TravelPerk The Python Developer position requires full-time, in-house work in Barcelona, Spain. We can help with relocation from anywhere in the world. English is the official language at the office. No prior knowledge of Spanish is required. See more at: https://www.djangojobs.net/jobs/659/senior-backend-developer-travelperk/
I'm pretty with you in the sense that I think going full-typehint at all times is counter-productive in Python, and I'm with in that I'm very happy the CPython interpreter promises it'll never treat typehints as more than inlined documentation rather than a cue to actually run checks, but there's one great thing I love about the typehints: no matter how hard you try sometimes you'll get at least a little ambiguity about what a function should take or return even with a good name. Consider something like `get_url_of_thing()` - what does it return? Well obviously a URL of some kind, but it could reasonably be assumed to be a string or a `urllib.parse.ParseResult`. Simply puttng the ` -&gt; ParseResult` on that definition adequately resolve the ambiguity and it does it without requiring someone to look anywhere else.
Well, for a Pythonista it's syntactically a C-like, while being semantically a functional language. And, the type system is also ADT and very expressive. Finally, it has strong bindings for CPython so writing modules that can be used in Python is very plausible: the others you list are AFAIK less well connected to Python. It's also got sane macros, which can be a lot of fun and a great mind opening experience for scripting programmers. In many ways Rust reminds me of a Scheme/Haskell hybrid with a C++ syntax.
I've been slowly introducing comment type hints in my python 2.7 code to help my editor, and I'm really enjoying it. Honestly, it looks more like I'm leaving a note for myself or other devs, and my editor ends up being another dev who only knows the docstrings and attributes of every object. Also, how to spell.
I agree with most of your comments. Hopefully, thing will improve soon. Many things that you mentioned are already WIP: Protocols to improve support for duck typing, efforts to improve mypy "usability", also I am going to write my own "tutorial" on type hints (an extended version of PEP 483 "Theory of type hints", I will be grateful for a feedback on this PEP). Things are not going fast, but typing in Python is a huge project.
happy cake-day!
I can probably sort the words within each column, but it wouldn't speed things up very much because each column has a maximum of like 6 words. Either way I think I have a decent solution. I'll update my original post with it, but the joblib package gives embarrassingly parallel for loops.
Haha, deal! Gotta admit I haven't yet tried f#, but with an endorsement like that I guess I should. :)
You might be able to do this task in Dask using pandas-like dataframes. https://dask.readthedocs.io/en/latest/ The match set is 20 million words. How many rows do you have?
I just went back and checked. I think I was getting this confused with a different dataset I've been working on. This is around 100,000 words and 10 million rows. Either way it's taking a long time.
I usually have to write my own due to the enormous size of data sets. Even pyqtgraph gets cranky when you try to load a few hundred MB without downsampling. So my question is: if you fed in a wav file that was 300 MB, would it display? Are you first loading it into an .h5 file, then subsampling to produce an envelope? 
Thanks!
[removed]
Oh, another question (since I haven't read all the code): do you use a the Qt model/view TreeView directly with the HDF5 hierarchy?
Sorry &amp; thanks
fucking /r/learnpython
Because the syntax looks like cat vomit and faces on a hot sidewalk.
[OPeNDAP](https://www.wikiwand.com/en/OPeNDAP) support would be stellar. :-)
 for w in words: term_and_col['contains_w'] = term_and_col[ terms.name].str.contains(r'\b' + re.escape(w) + r'\b') word_and_sum[ w] += term_and_col.loc[term_and_col['contains_w'], column.name].sum() term_and_col.drop('contains_w', axis=1, inplace=True) `words` is a set of words. `word_and_sum` is a defaultdict with words as keys and sums as values.
&gt; If you are in dependency hell you probably installed them with apt.. Nope, more like he sudo pip installed
Scraping gas prices from a website, so I always know the prices for premium gas nearby. I hate showing up at a gas station that displays the regular price only around $2.2X price, then park to find out the price is $2.8X-$3.1X 
Take a look here, Some years ago, for my friend, i did a similar downloader bot for http://www.allitebooks.com/ , You can find it here: https://github.com/igbt6/WebBots/blob/master/AllitebooksDownloader/all-it-ebooks.py The script downloads thousands of it- books from there. Just Remember only that the content of the site is not 'very' legal, i'd say. regards!
That's hilarious. If you use no-op after the triple quote, it's a little prettier. #!/bin/bash """:" echo "wtf" python3 "$0" exit $? """ if __name__ == "__main__": print("WTF!") 
Im critical of the title, and the libraries included in the list, because everybody has heard of them. They already have the attention of anybody in the loop (guys like me). I've heard, and used most. When I see a title like that, I would expect to hear about libraries with low visibility, or which are relatively new. Or maybe sonething that used to be closed source, bit about to be opened. Maybe Im just a grumpy hipster. Or maybe im just bitter at my inability to publish anything useful enough to get traction. 
As far as I understand the syntax you're referring to is mypy-specific, and type hinting itself does not require you to use mypy style syntax.
&gt; So this is only really useful when you're e.g. calling an external C library or so (which knows nothing about python.) Most of my (extensive) use of Cython has been to speed up tight math loops operating on memoryviews of Python (and Numpy) array-like structures, and these are **fully** supported by the nogil features. So typically, I get around 100-200X speedup for the number crunching, and then 4-8X *on top of that* depending on how many threads I use. I have never used Cython to call an external C library, outside of demos and tutorials. 
Btw, thank you for all your efforts on OS contributions. Whoever you are. I actually believe we may have met, but im not going to dox you. You have my respect, and appreciation. 
How did that make it to staging without you running it locally first?
Your html rendering should be on the frontend or on the backend. Not both 
There's always a reason. Google has the resources to upgrade if they want to, so my guess is there is probably some obscure dependency in their code, where they are using an old library that hasn't been upgraded yet.
Literally the first two google results for "python orbital mechanics": https://pypi.python.org/pypi/poliastro/ https://pypi.python.org/pypi/OrbitalPy 
I don't do anything smart yet with downsampling. I read the entire slice of the data into a numpy array and just feed that to PyQtGraph (I don't even have configured PyQtGraph for downsampling). So a 300 MB Wav file would probably make argos unresponsive because (assuming 2 channels and 16 bit per sample) it would try to make a line plot of about 80 million points. 
The syntax change in PEP-3107 is for _function annotations_. PEP-484 is only a "this is one way to use the function annotations introduced in PEP-3107", it's not type hinting for python. Furthermore, there's no syntax for annotations for variables or attributes(even though I think I saw a PEP for that somewhere), which hints that the annotations are designed to be used as documentation/comments/runtime information of functions, _not_ type hinting. When it comes to 2.7, there's no de facto standard format for doc strings to begin with, that depends on the documentation generator you're using.
I'm not sure if I understand your question correctly. The [repository tree](https://github.com/titusjan/argos/blob/master/argos/repo/repotreeview.py) derives from QTreeView and [its model](https://github.com/titusjan/argos/blob/master/argos/repo/repotreemodel.py) from QAbstractItemModel. So, if that's what you mean, yes.
Thanks for the explanation! That doesn't seem to be what's going on in the actual production code, but that makes sense for my example. The prod situation is a bit more complex. We use an SQLAlchemy model: class MyClass(Model): data = Column(JSON) # type: dict def get_data() -&gt; Model: # Oops! Should be dict result = fetch_object_from_db() return result.data def do_something(): data = get_data() # Actually a dict; typed as Model return bool(data.key) if data else False # Mypy's OK with this I can send a PR when I have a few minutes free.
Our "staging" is closer to a dev environment. This code talks to so many other services that sometimes it's easier to deploy it and run integration tests from there than to stand everything up in a local docker-compose, flesh out all the fixtures, etc.
nice! I did something similar recently, but my HDF5 files have a fixed layout so my QAbstractItemModel doesn't have to be too flexible.
Having just add type hints to about 10,000 lines of python, and actually checking them at runtime with [enforce](https://github.com/RussBaz/enforce), I'm pretty much sold on it. It helped catch some bugs in the nooks &amp; crannies of the codebase, like in handling rare exceptions such as out of disk errors, that didn't have test code coverage. 
They are hiring a django developer https://jobs-lulu.icims.com/jobs/1318/senior-software-engineer/job
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
I don't know anything about swig but looking quickly at the documentation it seems you need the `-c++` parameter for the call to swig e.g. `swig -c++ -python -Isrc cortex.i` Not sure if this will fix it but it's a possibility.
I wrote a script to repair a disasterous typesetting mistake. I tried to convice my publisher to rethink the automation that caused the mess, but to no effect. It could be a godsend for typesetting. But honestly the most important thing would be to start using version control. That's still not happening, to my knowledge.
Here's an overview of testing Python: http://docs.python-guide.org/en/latest/writing/tests/ The high level ideas of automated testing: * Three general types of automated tests: Unit, Integration, and System (the names and details vary between organizations, but the concepts are generally pretty similar). * Test cases should cover both positive ("it does what it's supposed to do") and negative ("it doesn't do what it's not supposed to do") scenarios. * Unit tests validate functionality of a small piece of code (for example, a method). You should have lots of unit tests, and they should have no (or very few) dependencies on external libraries or systems (databases, message queues, etc.). * Integration tests validate functionality of multiple parts of the software working together. These tests will call dependent libraries and interact with databases and such. There will be fewer integration tests than unit tests, but each test will cover a larger surface area. * System tests are run against the whole system in a production-like environment. For example, system tests for a web application would be routed through load balancers, web servers, etc. to ensure the system as a whole works as expected. You should have fewer system tests than integration tests, but the tests should cover the entire application. For GUI applications, system tests can be done using UI automation frameworks (I'm not familiar with any, as I'm purely a backend developer). Worth noting: Testing is hard. It takes practice to figure out how to write effective tests, but it's completely worth it because, when done right, automated tests will save you SO MUCH time troubleshooting.
Ok. Thanks for the tip, I will take a look at it. However, if I implement support for very large arrays I'd ideally make something generic that works for the table-view as well, and not only for the pyqtgraph plots. So I will have to think about it. 
that's how i put that one together
Yep. O(n), right?
Panda3D is pretty cool for 3d stuff. It's a little disorganized, but it works.
Thank you so much! 
The choice to use a comprehension should be about readability and clarity over speed. Their strength is that they are declarative, while loops are imperative, and humans are better at reasoning about what something is than how something should be obtained. Functional purists will go one step further and prefer maps and folds, which are available in python but discouraged.
Nice info. Only thing I disagree with: &gt; In case of lambda you should assign the result to some variable: name = lambda: None. lambdas are designed to be used once for simple code, if you are going to name and reuse it or it's complex, make it be a function. 
The standard method (I think): you have a 3d space, each body has coordinates and mass, you specify each bodies initial velocity, you specify a time period between calculations (to change velocity based on gravity), then you set the program running.
Robotics guy here. I wrote the code for my first Python robot on an RPi. It isn't the best quality code (but it was what I knew at the time), but it does work well, and is self-contained. https://github.com/masasin/yozakura The stuff you probably want to look at are in the rpi folder. devices.py is a bunch of sensor drivers I wrote to communicate with GPIO and i2c. No async; some realtime. mbed.py uses serial communication etc. motor.py is for the motors. I'm not sure about RPi 3, but I only had one viable PWM port so I had to control them through a microcontroller. This does not use ROS, which would be a bit more complicated. But it simplifies a lot of the things that you'd otherwise need to do yourself, and I like it. Now, the question is, do you want to make your robots autonomous? That's where the fun starts. Navigation, computer vision, decision making, inverse kinematics etc.
Can you publish wheels, and configure testing with tox? 
Also look for the back issues of The Mag Pi...I know there were several articles on using python for robotics with the raspberry pi.
I use type hinting for prototyping, but docstrings with Numpy formatting when things are done. I like having a single unified format that I can follow, and Sphinx can actually deal with them well (with napoleon).
I will do this. Thank you!!
You're absolutely right. In general, teleoperated is easier because it allows you to focus on making all the components work. You can add autonomous at any time. Exceptions are in cases where you want to, say, make a double pendulum or something a bit complicated involving controls. Humans can't do that. Let me know once you decide on what you want it to do.
I'm working my way through Jose Portilla's Python Bootcamp, to be shortly followed by his Python for Data Science, which covers numpy, pandas, and matplotlib, among other things. It does have a Python crash course at the beginning, I just added the bootcamp because I wanted a refresher on my programming skills. They're both on Udemy, and so far his product is easy to follow and interesting. The Python Bootcamp projects are games, so their fun to put together. Could work for your purposes. 
Because it has awful syntax.
Hello everyone, I just managed to get everything finished, and am finally proud to release the first public version of DVR-Scan (version 1.0). There is a Windows .exe build, as well as a source distribution that works on all systems (Windows, Linux, OSX) running Python 2.7/3.X. Feel free to check the project out [on Github](https://github.com/Breakthrough/DVR-Scan/) as well. For simplicity, the Windows builds include all dependencies, while the source distributions, recommended for Linux and Mac users, require NumPy and OpenCV 3.0 (or higher) to be installed. The Windows build includes an integrated Python 3.4 environment with OpenCV 3.1, although DVR-Scan has been tested for compatibility with Python 2.7 as well. This was created after a user of my first real open source project, [PySceneDetect](https://www.reddit.com/r/Python/comments/429y5r/pyscenedetect_video_scene_cut_analysis_with/), contacted me and wanted to know if it was possible to detect movement within a static scene (rather than detecting scene cuts/changes), making this my second "real" project. One major feature I'm working on implementing for the next release is the ability to use a transparent image as a mask so you can ignore some parts of the frame. That being said, even without masking I've achieved great results thus far, and there are plenty of other detection parameters you can tweak some other software doesn't usually include. Any comments, suggestions, or feedback you might have is most welcome.
Why would you use generics or any when you can just not use type hinting in the first place? 
Python is not Haskell. Python is not statically typed. Python should not be statically typed. Static typing has no advantage over dynamic typing. 
You kind genius! Thank you for this pointer, I could have spend the next two weeks on this before giving up and crawling in a corner crying. I have now changed the compilation steps to: swig -python -c++ -Isrc cortex.i g++ -Isrc -fPIC -I/usr/include/python3.5 -c cortex_wrap.c cortex.cpp cortex_socket.cpp cortex_unpack.cpp m3x3.cpp g++ -shared -fPIC -o _cortex.so cortex_wrap.o cortex_socket.o cortex_unpack.o m3x3.o cortex.o This resolves the previous issue. However, a new one cropped up. The new error is: ImportError: /home/Capermis/Desktop/TEST/_cortex.so: undefined symbol: Cortex_SetMetered The weird thing is that, when I search for this function in the code files, I can only find one mentioning of it. In cortex.h there is a line that reads: int Cortex_SetMetered(bool bActive, float fFixedLatency); Would you have any idea what this could mean? I've tried commenting this line in cortex.h before compiling, but then the compilation fails. 
&gt;Don't get me wrong, it's a good thing if you take the time to put it in place as it helps finding more bugs Not on its own. The act of going through and looking at all your code and thoroughly thinking about is what finds bugs. Adding the type hints have nothing to do with finding bugs. There's no empirical evidence to support the idea that static typing is better.
Google are running short of cash to pay for the upgrade? :-)
The idea that the right workflow for something is to blindly copy some existing 'template' is incredibly unPythonic.
Blocking github.io is fine IMO. Github is a pretty horrible company.
Mercurial is not a 'system package'. It's an enduser application. 
Unless you have a reason to use Python 2 you will likely be better served by using Python 3. Also, your division would have worked right. 😀 Though, print is now a function, so you would need parentheses around the result.
[Even Guido thinks they are silly to begin with](https://nnc3.com/mags/LJ_1994-2014/LJ/055/2959.html) &gt; Sometimes I've been too quick in accepting contributions, and later realized that it was a mistake. One example would be some of the functional programming features, such as lambda functions.
Yes, I too agree that generics are useless in that way. Even then, I didn't mean to avoid hinting altogether *within* a project. Say I have a few functions passing around arguments whose names are not really descriptive (unless I choose to use a much longer name) and can be helped by type hints. Now these few functions might just be 10% of the whole code, so let that be, use hints for these 10% and leave the 90% alone.
Hey, this is useful! Though isn't this just a fancier `assert`?
1) In scientific computing, Python is my go-to language to get things done quickly, which often means to write quick&amp;dirty scripts that I will only use myself, while being aware not to feed it any weird stuff or edge cases but just process the stuff at hand. Since I know what goes in and out, it's usually easy to keep track of stuff in your head, so there's no need to spend additional time on decorating things with type annotation 2) When I write code for open source, I still try to keep my code downwards compatible, so I don't want to inject stuff that won't work &lt; 3.5. However, this may change in future. I actually really like the type hinting stuff. And I strongly believe that I will want to use it more in future
It basically means you're mixing C and C++. As /u/BinaryRockStar pointed out, pass `-c++` to swig, so it outputs only C++ and you aren't mixing any C into it. You'll have to change your compilation steps to have `cortex_wrap.cpp` instead, too.
Any recommendations for somebody who may already have some experience in another language (mine being Java)? 
I think there is an official API for wechat given the huge eco system around it / on top of it. The last time I checked all the docs where in Chinese. But I haven't checked in a while. Im from Europe and wechat is not (yet) very popular here.
So... I've written shellscripts that generate Python which goes and generates more data for the shellscript... Which eventually calls Python. It involved a build and packaging system packaging a Python app for desktop use. The hackery was intended to help cx_freeze deal with a multitude of eggs and selective dependencies, the root cause of which I started caring about because we were distributing a commercial app, and had to make sure we accidentally didn't ship readline. Turns out it had other uses, too. More recently there is a piece of code I use daily which is a bash script that generates C which it then compIles and runs. There was a Python solution, but it was 10% slower :) that code? Also engineering workflow: an incremental ctags updater that I rigged up to trigger on every git checkout, essentially. Hairy code, but it has its place. A lot of people forget that the UNIX shell's primary job is subprocess and pipe management. To make that smooth it takes care of an awful lot that's easy to forget about in a more explicit programming environment.
Mostly Python culture doesn't care much about technical reasons if there are readability reasons. Comprehensions are generally considered much better for readability. 
What would be more interesting to know would be how many people would read a SQLAlchemy and an SQLAlchemy as the grammatically correct pronunciation depending on the a and an.
I never would have thought of that. And thanks for the info! I'm a statistician by trade, not a programmer, so this gives me an excuse to learn more algorithms and data structures.
Asking this question in a python board will offer you only more pros towards python and cons against php 😈 Of course people here are biased - what is totally ok. Besides that some pros are interchangeable... python is also mature and has mature Web frameworks... some might be even older than phps ones... And hardly anybody would talk to mysql directly, but over an orm. Full stack frameworks even enforces that - the same is true for php. So I don't know what you exactly critizise concerning the access to mysql via python? Btw: MySQL is not the only choice... without other external constraints I would always prefer postgres ☺
Oh. Surely it's not what I meant. It's just a way lambda can have that third type of name. I'll try to rephrase it somehow, thanks. But even if you use lambda like you awlays do it's still assigned to some variable (usually param of map, filter or any other higher-order function).
The tables that returns None in python, returns data in Oracle SQL Developer. 
No worries man, you had no obligation to reply even; thanks for helping. Will try to get to the bottom of this tomorrow.
I tried without unicode. It returns no result at all. Even on the SQLs that are working. 
Yes, the joined tables shows None. Strange thing, because it shows values when executing the SQL in Oracle SQL Developer
I love it, here's an upvote. If I can contribute somehow let me know.
For less messy amazon links you can extract the part after "/dp/" in https://www.amazon.com/Learning-Robotics-Python-Lentin-Joseph/dp/1783287535/ and make it: https://amzn.com/1783287535 **BEEP BOP** Plz send any recommendations via PM
I use aliases for all tables.
Not needed for anything as in "yes it can add value but everything was working fine without it so there is no reason it wouldn't work as well now that it's a thing". Again I'm not saying it's useless, not at all, but the fact is that people somehow managed to write python code without type annotations for about 20 years so it must have been a manageable situation at least.
From what I understand, most courses and code available still pertains to Python 2, or is that incorrect? I've heard that Python 3 hasn't yet been adopted by the masses and that most people still use Python 2? Plus, the free courses I'm using use Python 2 :)
Glad you like it, I'm looking forwards to receiving any feedback you might have in regards to the program, or the source code for that matter. One thing I would like to do is compile a list or table of [similar software](http://dvr-scan.readthedocs.io/en/latest/other/similar_software/), have you used or heard of any software similar to DVR-Scan before out of curiosity?
Try [kivy](https://kivy.org/#home). They have also recently received a grant from the Python foundation to improve their stack, so it can't get more officially recommended than that!
The sheer number of pros and cons is irrelevant (which is why pro/con lists are fundamentally flawed); what matters is whether any of them are deal breakers or *significant* advantages. Personally, I would also throw JavaScript in there; you've probably been using it in the browser before, and much of that transfers to server-side programming, so picking it up on the server shouldn't be a problem. Given these 3 options, the most decisive things to consider about these languages, IMO, would be: - PHP is pretty bad as a language; so bad in fact that I consider using it for professional work in 2017 kind of reckless. It is, however, ubiquitous, so if being able to run on any given shitty shared hosting service without much fuss, then PHP is where it's at. - JavaScript's main selling point is allowing you to use the same language, and much of the same codebase, on both client and server. If you have a lot of functionality to duplicate between the two, then JavaScript is a good idea. The big downside with JS is that its concurrency model leads to unpleasantness once things get complex (callback hell, promise hell, error handling, debugging...), and that, just like with PHP, its numeric types are close to useless. - Python is a decent enough default choice; just like the other two, it is a dynamic language, which can be a problem with larger codebases, but it has stricter semantics which at least make unit tests more viable. Its most striking selling points are the smooth initial learning curve (getting from zero to writing positively useful things takes about 2-3 weeks if you have prior programming experience), and the vast library ecosystem (including the "batteries"). Its semantics are also much saner than PHP's and JavaScript's, with better numeric types, easy-to-use bytestrings and unicode strings, and fewer implicit casts.
I used to code php until the moment I learned Python, never going back.
The table view is easy enough to implement with a load-on-demand model.
&gt; Static typing has no advantage over dynamic typing. That's gotta be one of the broadest statements about language design one can make.
Agreed with most of your points. Regarding ORM support, which one do you think is better between DjangoORM and SQLAlchemy? Can we mix and match (use SQlAlchemy in a Django app)?
Nice. The beta distribution being the conjugate prior for the binomial distribution makes the math really simple.
Its possible to write cleaner and maintainable code in PHP too, that's what I've come to learn after using Python a lot, and now applying it back to PHP. For example, what drives most people to Python is clean and indented code, but who stops you from indenting PHP code? The language doesn't force you to do it, that's all. If you start indenting your PHP code and follow standard practices and coding style, it can be as maintainable as python. The fact that lots of devs don't do it isn't the language's fault, that's what I think. Of course, there are other problems with PHP such as lack of uniformity in function names and overly large reliance on "duck typing" (or rather, "silently kill the error typing"), but those can be avoided and handled with practice. **edit** On a tangentially related note, have you ever thought what is the reason behind the quick success story of the C# language? The language is almost an exact clone of Java, when it was released in early 2000s, Java was already a decade old. But still most developers tout C# as an "extremely beautiful" language today, and yet associate Java with ugliness, do you know why? That's because of the Visual Studio IDE that **forced** developers to write indented code exactly like Python. C# rocks today because most programmers who have coded since the early days of VS still habitually write indented code in C# in whatever IDE they work. 
Wouldn't recommend mixing them, if you go with Django you're mostly comitting to the Django ecosystem. This has the advantage of everything being ready-baked and working together, the disadvantage that it's less flexible. It depends a lot on what you want to do. If you like living on the cutting edge, the new async frameworks on python 3.5+ are probably where the future is at, but they are very new so you will run into bugs. Django on the other hand is rock solid. If all you want to do is host a REST api I would go with something like Flask+sqlalchemy . On the PHP side, the only reason I would consider for a second using PHP would be for something you expect other people to deploy on shared hosting (like wordpress) since it's much easier. Otherwise, having worked on a fairly large PHP application a few years back I have nothing good to say about it. Whatever advantages it may have are shared by Python, while it has a ton of disadvantages that Python doesn't have. It's possible things have changed a lot, but personally i moved to Python and never looked back.
You can definitely run PySpark on python 3, since quite a long time too. http://stackoverflow.com/questions/30279783/apache-spark-how-to-use-pyspark-with-python-3
Good point, at least vs. Python. PHP libraries, in my experience, are of very mixed quality, and especially in a development culture like PHP's, time alone is absolutely not a guarantee for quality.
Guys, if you know some other good materials around scikit-learn - please share here. Thank you
Yeah, but that is the point of lambdas. Otherwise, just use def or define the lambda in the function call expression. While they are technically the same, a regular function definition has better readability. I personally don't think Python benefits from having anonymous functions except if you want lazy evaluation of expressions.
[removed]
so Dropbox is still running Python2 ..
if you will use django or symfony you are not going to touch any mysql base library directly. You mention cpanel/plesk, how are you going to deploy? You really need you think your app design before making any decision. What I would say is to just pick the technology you feel more comfortable with.
Alright, i'll take a closer look at the code later this week. Well, I was using one some years ago called YawCam http://www.yawcam.com/. I am not sure if it's still active though :/
One thing I'd like to add for OP if you are considering django is that you can find pre-made apps for almost anything which are great for inspiration, a quick MVP, or even full blown use. Some examples of the apps I use are: * django rest framework (easy apis) * django allauth (easy oauth2) * django honeypot (easy admin login logging) * django hijack (easy user spoofing) * django compressor (compresses/mins css and js into one file each) * django revproxy (easy reverse proxies) I really recommend django for its fast development and simplicity, as long as you conform to its opinionated style. The ORM is really good as well. 
But installation of Argos from a source distribution is already finished within a second. There is not much time too gain. However, I will consider it. I saw your pull request on GitHub, I will look at it this evening. Thank you for contributing. 
Yes. Besides Guido has been nothing but available on this project. I opened 3 tickets on mypy's github issue tracker, and he answered them personnaly the same day. So it's safe to say we are on the right track. If you need feedback, you can ping me on desmoulinmichel@gmail.com.
Yep.
Yet another "Check out someone else's code": https://github.com/Hackerfleet/hfos/blob/master/modules/robot/hfos/robot/machineroom.py This is how we (Hackerfleet) do servo/engine control for the MS0x00 robotic vessel via a Maestro Servo controller - other devices can be added, easily. We're using HFOS, which offers great remote control/debug options, among others a browser based GUI built with navigation and collaboration in mind. The system is built with Python, so you have all the access to nifty things like OpenCV, scikit etc.
No, i just skimmed throught it a little but there wasn't any new content (compared to the two videos i linked). edit: actually i guess it's an yes, i somehow read watched instead of checked I did watch all the pandas tutorials from this guy though, and they were very good, so thse should be pretty high quality too!
Does anyone have a comparison of the uses of numpy, scipy, tensorflow and scikit-learn for machine learning? It seems like there are a number of options available and I'm not sure which one is best to use. Thanks for bringing scikit-learn to my attention though. 
We still need a library that brings the customizability of matplolib, the browser interaction of plotly and the 3D-capabilities of VisPy.
i can read chinese so i will look around for it
/r/learnpython
I use python for Project Euler. This week I have learned to use generators for lazy generating. Was very helpful for performance for a few problems. 
Sure but you have nothing to lose with wheels
They have pretty distinct aims imo. Numpy/scipy - linear algebra, and related algorithms (eg filters, interpolation, etc) Scikitlearn - collection of commonly used ml algorithms and useful utilities (eg one hot encoding, dividing into train/test splits). Built on top numpy. Tensorflow - library for working with tensors. Aimed at use with neural nets. 
I think I'll send my weekends around this one :)
Thanks for the summary. I primarily want to use neural networks so tensorflow looks like the best bet, but the utilities of scikit-learn look handy as well. It's good to know of other available tools for future projects. 
This is my first time pushing something that I've written to Reddit. I'm not sure how the community feels about self-promotion, but I had a lot of fun writing this post (in particular sharing the Bokeh profiling plots) and wanted to get it out there :)
Pleasure. Hmm, i'd have a look through some of these. I believe the answers in there too, but it also has a breakdown of getting to the answer. https://discuss.codecademy.com/t/6-15-bool-four-is/2858 https://www.codecademy.com/en/forum_questions/52a73b66631fe9fad3002d17
Thanks again. The only way I can seem to get past it is by not even putting True or False in at the end. Strange. Cheers!
Thanks for the videos. Will definitely check it out. 
As I only wish to start with normal (shallow) neural networks in Python, would tensorflow be overkill? Is using keras or lasagne similar to using bottle over django on the web dev front? 
RemindMe! 30 days
https://learnxinyminutes.com/docs/python3/ and go build something. 
&gt; Unfortunately the HDF5 file format is not ideal for distributed computing ??? What do you mean by that? HDF5 was originally developed at the NCSA before the developers left to form the HDF group. 
Udacity has a pretty good "Intro to Machine Learning" free course that I just worked through. It says it's a 10 week course but it only took me probably a week of working a bit every night. I liked it though, I think I learned a good amount for a free course.
Hi, /u/codewarrior0 Just wanted to come back and tell you that I have been able to compile and link the C++ now. I use the lines below. However, note that I commented out the line int Cortex_SetMetered(bool bActive, float fFixedLatency); in cortex.h. I tried that before, but it didn't work then. However, I guess that your pointers solved that specific problem. swig -c++ -python -o cortex_wrap.cpp cortex.i g++ -Wall -fPIC -c cortex.cpp cortex_wrap.cpp cortex_socket.cpp cortex_unpack.cpp m3x3.cpp -I/usr/include/python3.5 g++ -Wall -shared cortex_wrap.o cortex_socket.o cortex.o cortex_unpack.o m3x3.o -o _cortex.so python -c "import cortex" The last line is a test to see whether the new module can be imported successfully.
You should evaluate cython, numba, pypy at least for your use case I think. Generally I don't think it makes sense to try to say which would be faster in your specific case. 
Wow, I did not knew that. Problem solved by import ;)
Was just searching for something on this! Thanks!
That'd be great, I'd love to see a minimal repro if you've got it! If you're curious about mypy's view of the world, you can always use `reveal_type`- http://mypy.readthedocs.io/en/latest/cheat_sheet.html#when-you-re-puzzled-or-when-things-are-complicated
That's because of [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence). &gt;&gt;&gt; (-(1**2) &lt; 2**0) and (10 % 10 &lt;= 20 - 10 * 2) is True True
Try /r/learnpython.
We'll be running cloud forecast algoritms for the entire globe. 
I'd say scikit-learn is a comprehensive machine learning package covering all the basic techniques for supervised (classification and regression) and unsupervised learning (clustering and dimensionality reduction) etc. Also, it comes with a lot of convenience functions for preprocessing, evaluation, model selection and so forth. Numpy (mainly for array ops) and scipy (additional functions for scientific computing) are the two libraries that scikit-learn uses under the hood. Both Numpy and scipy use BLAS, and compiled fortran and C code under the hood, which makes it fast for stuff on CPUs. Tensorflow is more similar to NumPy in the sense that it provides multidimensional arrays (or tensors in a mathematical sense) that you can run on your GPU; array ops that run parallel on GPUs is what makes this library particularly attractive for deep learning. Also, there are a lot of convenience functions in tensorflow that make implementations of neural nets easier, like automatic differentiation, and so forth
Yeah, Keras and Lasagne are more "higher level" APIs I'd say. Keras uses Theano/Tensorflow under the hood and Lasagne also relies on Theano. These libraries aim to make neural net implementations more accessible, but for custom stuff, I prefer coding in Tensorflow directly
Adding to this. They also sort of go in the order listed. For example, there are lots of things that you would just use Numpy/scipy, but if you were to use Scikitlearn there is a good chance you will also want to use numpy/scipy as well, and if you want to use tensorflow there is a good chance you will want to use scikitlearn/numpy/scipy as well. Obviously this isn't always the case but in my experience it is most often the case. 
Promote away. Dask is awesome. 
read later
This is super cool I really enjoyed it :-).
Apologies, I didn't realise! I should probably have taken a look around first :)
This is not as popular an approach but if you are using numpy and python, you can write C functions that directly manipulate the numpy data. Numpy is written in C and you can find the header files to include when building your code. I came across this example and found it useful: https://github.com/chrisjcameron/python_igraph_numpy_c_ext
&gt; We'll be running cloud forecast algoritms for the entire globe. It will all boil down to the details. I guess the only way to really know what works best for you is to give each option a try.
Here is a tutorial (7 short videos) on how to build shallow neural networks from scratch in Python. [Neural Networks Demystified] (https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR) I would recommend this tutorial to anyone who wants to know how neural networks work.
Yeah, I apologize, my statement was overly broad. If HDF5 is compiled with MPI and if you're using MPI and if you happen to be on a posix file system then HDF5 can be fantastic. However if any of those things are untrue then HDF5 is somewhat unpleasant. Mostly the problem is that it is difficult for a distributed computing library to identify the subset of bytes that it needs to collect without invoking a C-library that expects a C file object. In the example in this blogpost we were grabbing data from S3. It would have been very challenging to read HDF5 data on S3 without downloading entire blobs down to a temporary location on our local file system and then using the C HDF5 library from there. HDF5 remains great for the application for which it was intended, HPC simultations on super computers. It is a bit too complex for many other more mundane situations.
Django is slower than pyramid or flask if memory serves me right. But it still doesn't matter too much since usually its the datastore that is actual bottleneck.
&gt;Really? It depends but typehints are now standardized, docstrings are not. I mean for types descriptions, not for docstring itself. Docstrings are completely standardised. They're human-readable documentation. English is (for all intents and purpose) standardised. The type descriptions don't have to be computer-readable. If anything they shouldn't be, that tends to add noise. &gt;Better than nothing which is quite common, dont you think? That's a weird comparison. Why would I want to compare it with nothing? It might be better than nothing, but it's not better than good documentation, which is the actual alternative. &gt;Surely not, I am not going to abuse **kwargs myself. IMHO, the good practice is avoid them, not document them :) Using `**kwargs` is not abuse and it is not good practice to avoid them. They are a very useful tool and they should be used liberally. Just don't use them to avoid typing in a list of parameter names, that's lazy. &gt;Haskell, C, C++, Java, C# - geez, I didnt say that static typed languages are more readable. To suggest C, C++ and C# is almost offending me :) Python is right behind Fsharp in my eyes. You did say that typescript and F# code is more readable than Python, which is patently untrue. Typescript is a horrible hacky piece of crap on top of another horrible hacky piece of crap. &gt;Recently, I saw very nice Typescript in Python extension for VSCode and it really impressed me. It was really easy to understand author intentions thanks to typing. No, it's not. Typing conveys very little information. &gt;Mind you, I said typing not static typing. Static only means it is controlled during compilation. Python as you surely know is strong typed language, so there should be no big deal with indicating of correct and supposed type if it is viable. Mypy and type hints in Python are completely static. Mypy is a static type checker. Python is a strongly dynamically typed language that people want to turn into a strongly statically typed language. &gt;It is crazy misunderstanding. Typehints are not making Python less dynamic. It just guide you on the way. If you want to get any use of type hints you absolutely lose the benefits of dynamic typing.
No, it isn't. It's objective **fact**. There is *zero* evidence of any advantage.
Very cool, I'm working on something similar! Some suggestions: * I'm not sure if you mean to use `randrange` or `randint`. Based on how you use it I think you mean `randint` as the max stat roll in D&amp;D is 18 but `randrange(8, 18)` means that the max value you'll get is 17. * You might want to add some comments or more descriptive function names. While I understand what `ls` does, I'm not sure what it stands for. * I would include the [Open Game License](http://www.opengamingfoundation.org/ogl.html) to make sure your project is on the up and up. Also review the license and basically, if it is from D&amp;D but it isn't on SRD20 then take it down. * In the README I would include examples of how to use your program, as well as list the dependencies which in your case is `requirements.txt`. For example, you can input the class on the first line to skip the second input, but that isn't ever document. * You only use `console` for its clear command. Instead of forcing your users to install another library to use your tool I would write a small `clear` function. Something like: import os def clear(): if os.name == 'nt': os.system('cls') # Windows else: os.system('clear') # POSIX * My final suggestions would just be formatting. Function names should be `cased_like_this` rather than `likeThis`. I would also lowercase `Data.py`. EDITS: * Forgot to mention that you might want to get the stat values in a way more accurately depicting the distribution you would get when rolling dice. So rather than a single 8-18 random value you might want something like: sum(randint(1, 6) for _ in range(3)) That way you don't get 18's as often as you get 10's, which is the case when actually rolling dice. * Okay so I've been playing with it more, actually I made a [fork](https://github.com/micaiahparker/DanDy) to mess with, and you should probably include a way to save the output into a file. * I would also include error checking. If someone puts in bad input the program crashes. * `getRace` also gets the class. I would separate these into two functions. * This project could heavily benefit from using some classes. In particular, a class that creates the character. I really like this project. Feel free to DM me if you have any questions.
Yup. AFAIK a lot of it is still 2.6 based upon what Guido has said on python-ideas.
Looks pretty cool! Definitely want to take a look at it on Monday after my exams.
Welcome. /r/python is the sub for news and discussions. I recommend to delete this and post it again in /r/learnpython. Read the sidebar and the FAQ first. To format your code for reddit, add 4 extra spaces before each line of code and a blank line at the top. 
Well I decided to methodically go through different combinations of ipython and jupyter qtconsole options, and here are the maddeningly and fascinatingly inconsistent results, using either command-line arguments, magic commands after IPython loads, or some combination of both: **FOR IPYTHON TERMINAL**: **GUI SPEC**: --gui=qt, %matplotlib qt: after trying to declare figure, get "missing 1 required positional argument: 'figure'" (um... what?) --gui=qt, --matplotlib=qt: get "no module named PyQt4" --gui=qt, %matplotlib qt5: success; different window format than osx, with save dialogue offering filetype choice --gui=qt, --matplotlib=qt5: success; same as above --gui=qt, %matplotlib osx: success; window pops up as separate application --gui=qt, --matplotlib=osx: QApplication window NEVER STARTS; fig appears as popup/part of terminal application, does not have its own "window" **NO GUI SPEC**: --matplotlib=qt5 OR %matplotlib qt5: success --matplotlib=qt OR%matplotlib qt: get "no module named PyQt4" --matplotlib=osx OR %matplotlib osx: QApplication window NEVER STARTS; fig appears as temporary popup **FOR JUPYTER QTCONSOLE** using jupyter qtconsole -- (args follow) **GUI SPEC**: --gui=qt, %matplotlib qt5: get "RuntimeError: Cannot activate multiple GUI eventloops" --gui=qt, --matplotlib=qt5: works, creates QApplication window for figures (seriously... WHAT? then why the hell doesn't %matplotlib qt5 work?) --gui=qt, --matplotlib=inline OR %matplotlib inline: after trying to use plt.show(), get "matplotlib is currently using a non-GUI backend," --gui=qt, %matplotlib osx: get "RuntimeError: Cannot activate multiple GUI eventloops" --gui=qt, --matplotlib=osx: works, but NO QApplication window; just a window-less popup **NO GUI SPEC**: --matplotlib=qt5 OR %matplotlib qt5: same as above, works --matplotlib=osx OR %matplotlib osx: same as above, NO QApplication --matplotlib=inline OR %matplotlib inline: after trying to use plt.show(), get "matplotlib is currently using a non-GUI backend," --matplotlib=qt: get "Eventloop or matplotlib integration failed. Is matplotlib installed?" %matplotlib qt: get "no module named PyQt4" **FOR JUPYTER NOTEBOOK** (no command-line options for notebook server, must specify in notebook cells) %matplotlib inline: after trying to use plt.show(), get "matplotlib is currently using a non- GUI backend," %matplotlib notebook: works fine ---- ~~So... maybe I should report this to the IPython developers? lol.~~ **I reported this to both the matplotlib and IPython issues pages on Github**. Also I updated/reinstalled everything twice; qt, pyqt, matplotlib, qtconsole, etc. I wonder if "gui" is not meant to be specified with "matplotlib"... like gui offers some sort of general interface that different modules can use? I was told in another thread python is much easier to work with than Matlab, lol. I think there is some benefit to a pre-packaged, robust GUI compared to this open-source heap that continuously evolves.
I'm looking forward to the results of all your hard work to achieve this aim.
I'm not sure if the author is here or not, but dude, you are the Bob Ross of ML educational videos. I love this. "Anyway..." Great work!
You're asking how the status has changed since 2015, and then you're cheeky when I try to answer it? Nice
That sounds like then they don't have the resources to upgrade, or they've commited to not upgrading because they want to use those resources elsewhere.
Would you believe that academic institutions run less Python code than Google, and in less obscure circumstances where the failure of said code will not loose millions of dollars per day?
Not really. There is one popular course that this subreddit removed from their listing because of it that still suggests Python 2. I believe most, however, have moved to Python 3. Of the fifty most popular libraries I think there were two that haven't been officially updated. Your choice, just understand that Python 2 won't be supported much longer and you'll have to switch over in a year.
It will do less requests per second compare to those two, but like I mentioned if you throw in database into the mix usually it shouldn't be deciding factor.
Okay, I've checked out your code (but next time post it in /r/learnpython). The traceback says: File "&lt;string&gt;", line 34, in betting NameError: name 'bet' is not defined That means the variable `bet` doesn't exist in the betting function or in the global scope and that's because you haven't declared it as global in the `roulette` function. But you really shouldn't use global variables, because they make code hard to understand and buggy. Instead of using global variables you should pass the variables to the functions that need them and then return the new values. So in 'roulette' write `money = betting(bet)` and in 'betting' `return money`. Also, you should structure your program in a different way. You use recursion to continue the game and for error handling (roulette calls betting, betting calls play_again and play_again calls roulette again). That means you have to pass the needed variables to all of those functions, and because Python has a recursion limit of 1000, the program will eventually crash (of course the limit is not a big problem for such a simple program). Instead of recursion it's usually more appropriate to use `while` loops in Python. So write a top level main function where you define the money and bet variables, then a while loop in which you call a function to get the input and check if it's correct, call a function to check if the user has won, ask the user to play again and if the user doesn't enter 'y', break out of the loop. 
thank you! this is really helpful. i'll start working on that right away
this is so useful! thank you for taking a look. since i posted this i switched over to randint yeah, i've always been bad about comments/descriptive names i don't really understand what's happening in your clear function all of the functions are now cased_as_such! my reasoning behind the 8 to 18 was while it's not totally accurate, i figure no one likes having low rolls it did output to a file before, but i decided i would wait on that until i could get the formatting how i like i had such a hard time getting get_race to work, i didn't want to touch it once i got it stable get_race is now get_info i'll update the git right away, thank you so much!
Post your code and the full error message in /r/learnpython.
&gt; Feel free to DM me if you have any questions. heh
also, what is the benefit of using [] and {} instead of list() and dict()? is that the standard? 
Sebastien Genty had an [interesting talk](https://www.youtube.com/watch?v=uFxKS4ciPpQ) about moving from SPSS to Python. Some of the code examples he gave were a page in SPSS code trimmed down to a few lines of Python. The fact that they were going from something like $20k/user license cost to $0/user seemed nice as well.
Assuming heavy math code, my anecdotal evidence is that Numba is amazing and awesome *when it can figure out how to do the optimization*. The problem is that it doesn't always, although it has been at least a year since I've looked into it in detail. Cython is a lot more predictable, but less auto-magical. Typically you check Numba first to see if it works on your problem, if so, move on, otherwise investigate Cython. Cython can produce code with performance that is completely equivalent to what you'd get if you wrote a C-extension by hand, but with far more safety (and it's very simple to split work over CPUs). When Numba is a good fit, it can give the same speedup with much less effort on your part. I have not found the other options to deliver as much speed, but the feature spectrum is different; e.g., pypy can massively speed up code that uses builtin Python containers, whereas with Cython you typically have to convert that data to typed arrays to get the full benefit. (TBH, if you want the best performance you end up doing this regardless).
String parsing / manipulation you'll see huge reductions in code size and clarity.
Old code always looks inferior as you continue to learn. It still happens after 10 years of programming. The questions then become: does the code work? Did you learn something? If yes to both, then you're doing it right.
Unity isn't really python. It's still a good answer if you're wanting to learn game engines though.
Yep, it and Panda3D are the only two real, full fledged gaming environments that use python (as opposed to python-like languages). It's a great place to get started.
If you plan on doing games in your career, the best two to start (closest you can get to industry) are either Blender or Panda3D. If you want to learn about game engine internals, pygame and/or pyglet is a good place to start. If you want to develop from scratch, I'd start with PyQt or Kivy. 
In Windows the terminal command for clear is 'cls'. If the machine is Linux or OSX then it is 'clear'. If you are on Linux then `os.name` will be `posix` but if you are on Windows then it will be `nt`.
Yep. But doing this right is hard. I did this once - ran the simulation for a few years, and found that we had lost the moon.
that's really good to know, thank you :)
&gt; (all the devs code in a lot of the older languages). Like Fortran, C, C++, something else? Python is actually pretty old, older than Java/C#/Ruby etc.
&gt;I was planning on doing a humorous introduction that had side by side comparisons of a huge chunk of C code next to three lines of python. And then some Perl coder will reduce your three lines of code to a Perl one-liner. /jk... best of luck with your talk.
try ... except Exception ...
Thank you. Will that work? I once used this. But there was a None error. And my script did not catch the 'None' Error. I had to include 'None' and other errors in my script. 
Shebangs seem to be added here: https://bitbucket.org/pypa/distlib/src/f702f6afc1d03717fa88a742b9fb99bedd599ff2/distlib/scripts.py?at=default&amp;fileviewer=file-view-default#scripts.py-139 But is this the only place through virtualenv that shebangs are created, you think?
except Exception: Will catch everything other than SystemExit and KeyboardInterrupt (which if you plan to catch, do so explicitly), so its what you want. 
Weird I would expect you set up proxy same way more or less for all wsgi servers.
There is no NoneException or the like, do you mean the function was returning None. Because that is default behavior. Sample code?
Actually, I was writing a web service with python. In the output on the webpage, there was a 'None' error. My previous python code did not catch that. So, I added 'None' and some other things there. Sample code: try: ... except Exception or None or blahblah,e: ...
So, I'm not a programmer but I'm learning, and I understood everything you said other than the last bullet. Could you explain what that means? 
except None is a misnomer, as that won't catch anything, and is not allowed in Python 3. http://stackoverflow.com/questions/19327320/python-except-none For example, try raise None raise None TypeError: exceptions must derive from BaseException the except Exception (or other valid exception) is the only thing that would have caught it.
Thanks for all the help, I am trying out Panda3D, But while i'm here, Do you guys know the best FREE way to learn moer Python? I finished the "Learn Python" Course on Code School, Are there any other online free courses that teach Python?
This post is better suited for r/learnpython and r/learnprogramming 
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Document viewing would be easier to handle client side then trying to parse the document into HTML or anything like that. [Viewer.js](http://viewerjs.org/) is a good library for doing that.
Well, you don't necessarily have to be using MPI to take advantage of HDF5, but I agree with most of your points. &gt; In the example in this blogpost we were grabbing data from S3. It would have been very challenging to read HDF5 data on S3 without downloading entire blobs down to a temporary location on our local file system and then using the C HDF5 library from there. Oh yeah. If you wanted to in any way be effective in this use case, you'd need a shim layer that translated the byte ranges into ranged HTTP requests.
You can write cleaner than average PHP, sure, but the language is inherently inconsistent and hacked together.
Learning how to program! I'm having a rough go at it, but I'll get there! Any tips would be greatly appreciated!
[Sentdex](https://www.youtube.com/sentdex) gives a good introduction to [machine learning](https://www.youtube.com/watch?v=OGxgnH8y2NM&amp;list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v), and uses scikit and later tensorflow. Also has a [written tutorial](https://pythonprogramming.net/machine-learning-tutorial-python-introduction/), however, I've only watched his videos. 
I had to use this project recently for a research project and its not the best implementation and idk if it has been updated recently but its the only one out there (that's besides the point). I believe whats happening is that wither with the library or the google maps api is that it tries to close the path at the last point with the first path to make a closed shape. Perhaps gmplot or the google maps doesn't support just paths but that would be weird.
kivy is great and I personally find it a pleasure to work with. But it's not game engine *per se*, it's a general purpose UI library. You get a bunch of basic widgets, sound and event system, videoplayer and that's it. No scene system, no base classes for game entities, no LoS/pathfinding/whatever, not even sure about shaders. Although you definitely can make your own engine with kivy and it lets you avoid most low-level hacking that would've drawn a lot of time.
I went to his Pycon tutorial. Great class. Great guy. Happy to help and discuss after session 10/10 would recommend. 
I really liked "Effective Python: 59 Specific Ways to Write Better Python"
I believe Raymond Hettinger mentioned once in a talk that you can introduce unintended bugs that may result in retroactively PEP 8'ing. In short, if it ain't broke, don't fix it. EDIT: He mentions it in the first 10 minutes of [Beyond PEP 8](https://www.youtube.com/watch?v=wf-BqAjZb8M). PEP 8'ing doesn't necessarily make code better.
Talented Python Django Developer Edinburgh UK at Mercurytide https://www.djangojobs.net/jobs/666/talented-python-django-developer-edinburgh-uk-mercurytide/
Only started at few months ago, found Automate the Boring Stuff (book), Sentdex (Guy on YouTube) and Cousera (online course database) very useful. 
if you only need to render/display them and you don't need to have editing in place too, then you can probably use an iframe and reference a remotely hosted word document details in [this stackoverflow thread](http://stackoverflow.com/questions/27957766/how-to-render-word-documentdoc-docx-in-browser-using-javascript)
I'm working on a [Kivy](https://kivy.org/) app for desktop to manage social network accounts (e.g. Twitter). I'm started using [Kivy Designer](https://kivy-designer.readthedocs.io/en/latest/) but now I'm writing [Kv Language](https://kivy.org/docs/guide/lang.html) by hand.
Very cool, thank you. Is thee a good reason its not a python module?
Great.
UserDict vs defaultdict though :).
I would assume so you don't need to change your existing scripts to take advantage of it. 
Author here. Just a quick post to tell people about Frozen-Flask, a fantastic library I recently tried out for the first time.
Selenium would be your best bet. http://selenium-python.readthedocs.io
Thanks!
It will work over ssh. iTerm2 allows you to embed images using special escape codes.
My feeling is they've basically committed to Go as a replacement language for their services. Their new python runtime Grumpy is pretty much a Python-&gt;Go porting tool.
Found quite a nice one in development https://gitlab.com/kivymd/KivyMD and a tutorial with pictures https://www.codementor.io/kiok46/tutorials/customize-navigation-in-kivy-kivymd-python-q01gm3hid Kinda wish I had an excuse to use it now...
Depends on the details. Always remember to figure out where your performance bottlenecks are first before optimising. Numba JIT is probably the simplest way to optimise hot spots, much easier than getting C extensions working. Cython will give you more control but is more fiddly. You might consider Theano for expressing math models, it can automatically figure out how to run your code on a range of backends including GPUs. SymPy has some tools for turning symbolic maths into Theano epxressions. Finally when you say grid, are you talking about gridded data or grid computing? If you are using a supercomputer you'll have to consider what the limitations/requirements are to take advantage of many cores - you will probably have to use something with MPI. IPython cluster tooling has some nice features for running on HPC grid engines.
here is a simple example using [selenium](http://www.seleniumhq.org/docs/) and [phantomJS](http://phantomjs.org/): from selenium import webdriver link = "http://www.oklahomafindalawyer.com/FindALawyer" sel_option = "//select[@name='stateSearch']/option[@value='OK']" # selected state is Oklahoma class JobScrapper(object): def __init__(self): self.driver = webdriver.PhantomJS() def get_list(self): self.driver.get(link) self.driver.find_element_by_xpath(sel_option).click() self.driver.find_element_by_xpath("//input[@value='Find a Lawyer']").submit() print self.driver.page_source # only for example if __name__ == '__main__': scraper = JobScrapper() scraper.get_list() 
Yep, base64 encoded.
Because at one moment you'll want to do something that is not web related. Python is at web stuff, but also good at most others things. But not PHP. PHP is very web oriented, and using it for sysadmin, data analysis, creating UI, etc. is not a nice experience.
We do ranged HTTP requests from S3. The problem with HDF5 is finding the range. The spec for the format is large enough that the only real way of finding this out is using the standard HDF5 C library. You would need some sort of POSIX shim on S3 to operate well. Other formats, like Parquet, are much simpler and so it is easier to pick out the right bytes to load.
may I ask the advantage of making this a class as opposed to just a function?
There is very little advantage in this specific case. The same instance of PhantomJS is reused between calls, that's all, but the same could have been written that way: from selenium import webdriver link = "http://www.oklahomafindalawyer.com/FindALawyer" sel_option = "//select[@name='stateSearch']/option[@value='OK']" # selected state is Oklahoma def get_list(driver=webdriver.PhantomJS()): driver.get(link) driver.find_element_by_xpath(sel_option).click() driver.find_element_by_xpath("//input[@value='Find a Lawyer']").submit() print driver.page_source # only for example if __name__ == '__main__': get_list() # Optional instance of PhantomJS in argument for reuse However having a class means it is easier to extend if you don't want to just get a list of data. 
Doesn't make sense in Python 3 land where they fixed some of these but not all. 
In the case of datetime it does though. Massively. Not knowing if you need datetime.now() or datetime.datetime.now() in a given file and having weird errors when moving functions between files because of different imports is an embarrassment to Python imo. I enforce one style by scanning the source in a test at work. Has helped a lot. But needing that is pretty silly. 
If you want to read what's in a browser (that you don't have control of) your best bet is to use JavaScript to send all changes back to your server that is running Python. This might be best suited as a question to https://www.reddit.com/r/learnpython/ Here's another tip: http://www.codeconquest.com/website/client-side-vs-server-side/ 
I'm not 'trolling' anyone. There is **zero** empirical evidence that static type checking is better than dynamic type checking. For every person that says that static is better, there's a person that says dynamic is better, and vice versa. &gt;What evidence you need me to provide? Basically every piece of code I wrote down is much more clearer with typehints. This is so obvious that claiming otherwise is true trolling. It's clearly not obvious, when you take into consideration all the other issues. For one, it's usually compare to having nothing, when the correct comparison is to good documentation. As well, types convey very little information.
Compatibility, I'd imagine. Using unittest as an example, imagine if you had to rewrite your entire test suite in PEP8 unittest just to see if your application still worked when porting from 2 to 3. That said, things did move, get renamed, but it seems most of those were because it made the package cleaner. I can't back any of this up with authority, but that seems to make the most sense to me. 
What kind of question is that? So he can use Python 3.6 and have Anaconda check for syntax errors ect in 3.6
You can try it out already: conda create --name py36 python=3.6 and then activate the virtualenv source activate py36
I have lots of 3.5 conda environments - is there a sensible way to upgrade all of them at once?
Another vote for Stripe. They have great [docs](https://stripe.com/docs).
Possible? Maybe. However, that doesn't sound like a good solution. You can distribute a complete Django project that has your app along with all of the settings, etc. Take https://github.com/edx/credentials/, for example. Ignoring static assets, all you have to do get this project running is the following: 1. `make requirements` 2. `make migrate` 3. `./manage.py runserver` You can also codify much of the boilerplate for your projects into a cookiecutter project template.
please, go ahead! but from what i've heard fromm other people in the thread, i'm going to have to restructure the whole thing, so you may want to wait until then
You can pass the `-n` flag with the name of the environment you want to update. I would do something like conda install -n env1 -y python=3.6 which is pretty easy to make a loop out of in the shell.
You don't have to activate the environment, just use the `-n name_of_env` flag. This is actually accepted for pretty much any conda command
Perfect! Is there a way to check which version of python it's on at the moment in a similar fashion? I have a few 2.7 environments which regrettably must stay that way.
Sentdex is really good imo. 
`make` is available on macOS. (I use macOS.) If you don't want to use `make`, just run the commands directly: 1. `pip install -r requirements.txt` 2. `./manage.py migrate`
Assuming that you're talking about this as part of your local development workflow, Firefox has a remote debugging facility that you could connect to: https://developer.mozilla.org/en-US/docs/Tools/Remote_Debugging/Debugging_Firefox_Desktop However, you would be able to do anything with that you can't already do in the devtools.
Very cool ya!
`conda list -n env -f python` will list it for you, although the output might be a bit difficult to parse. Should be straightforward with the `--json` flag though conda list -n env -f --json python | python -c "import sys,json; print(json.loads(sys.stdin.read())[0].split('-')[1])"
https://docs.python.org/3/whatsnew/3.6.html Personally I'm excited for "underscores in numeric literals" in my personal math projects like 10_000_000. Because they are personal projects I don't have to worry that much about backward compatibility. In a large project it might not be worth being quite that cutting edge yet.
So true. Whereas JavaScript is the language of the web, Python is the language of your life. *I would argue though that Python is much superior to shell scripting for anything more than a trivial few-liner script. And even more so, I can't think of a better tool/language that is super to Python is scripting and automation as a whole.*
Yep. Smaller and smaller time steps, and even more precise floating point numbers. ;) Fortunately both of those are pretty achievable.
i'd love to see! 
thank you for your suggestions! unfortunately as i mentioned, i'm really bad at programming, so i have questions! * why is it better to have it as a class? * since only one character is generated, why do they need to be reusable? * yep! done * why should user_data be a class? * again, why should the print function be reusable? * huh, neat! also pretty aptly named. thanks! i hope i didn't come off as rude, i just want to learn how/why i should write, you know? i'd love to see your port! 
I presume you're wanting to test client side? I saw an excellent approach in the [libcloud](http://libcloud.apache.org) library where they mocked the request and the response. It monkey patches httplib2 (IIRC), which is great if you're using that. If you're using Requests or something else, at least the same design/arch would work. I am sure there are test harnesses already available for requests.
Confused why I didn't see flask, but it's [higher than Django](http://gittrends.io/#/explore?page=1&amp;language=Python&amp;domain=All&amp;growth=All) if you take off filters. 
You can use these * print("This is sad :'(") * print('This is sad :\'(') The (\\) makes the next letter to be used as a string if it is inside a string. I suggest the first one. Hope this helps. Edit: Wrong sign. Changed (/) to (\\)
Bit confused by the wording of this, but are you looking for something like this: Use double quotes instead so ' doesn't means end of string: print("A suitable smiley for you would be :'(") Alternatively you could escape the single quote with a backslash: print('A suitable smiley for you would be :\'(')
Thanks it worked when i put \before '
Use double quotes: `"`. You can put single quotes inside strings in double quotes and vice versa. Also, please post questions in /r/learnpython next time. 
I believe you can use the r flag with a string so that it'll be processed as a raw string.
[removed]
Those are not arguments. You're setting attributes on your class instance, `self`, which Python automatically passes as the first argument to any class method. `self.make = make` means "set the attribute `make` on the object `self` to the value of `make`". That's completely optional, and you can have a class that doesn't save `__init__` arguments like that, or doesn't have an `__init__` at all. Also, post questions in /r/learnpython next time.
[read this](https://pythontips.com/2013/08/07/the-self-variable-in-python-explained/) self can be accessed by the whole class if you give it as first argument to a method. this enables you to use the variables outside the scope of the \_\_init\_\_ or other methods in your class. if you only assign make like you asked, you won't be able to use it outside of \_\_init\_\_. Think of it like a storage for stuff you want to use all over your class in different methods. This makes it pretty useful. You can just "not" use it if you want ... but it's just so damn nice ;)
Basically it allows you to start a browser and you can then perform all the normal user actions via commands, as well as collect elements from the page. I believe what you are trying to do is "functional testing", which might improve your searches for help and intro material on Google.
The suggestions will end up making your script easier to read and maintain in the future. It also makes it more extensible so you can plug it into other things at a later date. Q: Why is it better to have it as a class? A: It will make your script more extensible for later and you are using the function like a class already. Q: Since only one character is generated, why do they need to be reusable A: You are reusing the same logic over and over again in your current code. If you make them reusable you only need to make the logic once and reuse it for all the different races. Q: Why should user_data be a class A: It shouldn't be, it should be broken up into the fields for the class. This again will make your code more extensible for later modifications. Q: again, why should the print function be reusable A: What if at a later date you don't want to print to the terminal now you need to not only refactor your current print code but also add new system for getting the data to the user. You do not come off as rude. You come off as someone wanting to learn. I may just do a port of it today, though I'm thinking about doing a port in different languages. I kind of want to start with a Erlang port as this idea seems like a great first coding project. https://github.com/the-kid89/DanDy 
@zodman: Is Dataset also protection from SQL-Injection?
F-strings alone are worth it.
Honestly, I usually just hit the actual test endpoint during testing of the request. Response handling can be tested with just a mocked up JSON.
Unmodified, i did the anaconda updates (and 1 complete rewipe of win 10 with no ananconda updates). All uninstall / purge path/clear directories all come up with the same exact error. Hardware: Surface Pro 4, i7 8gb 256gb
Will that effect anything programming wise? I'm new to the language (been doing MIT coursework, and udacity). I will not be using large data sets for a while.
Like what? Also, post questions in /r/learnpython next time.
Ok, i'll reply back in a few with an update
To know when the statement ends
But why couldn't you just use one Parenthesis.
It's literally the same reason why we have open and close parentheses in mathematics - it's a way of grouping expressions. Many languages (C, Scala, Java) use curly braces to denote blocks - python uses indentation but that gets translated behind the scenes to an open-block and close-block symbol. 
I see this type of question often from my students. The thing to remember is that parentheses *always* come in pairs. It's not that there are 2, it's that there are 2 *pairs*. So code like this: x = randrange(10) print(x) is the same as saying: print( randrange(10) ) I put some extra spaces in there just to highlight what's going on. There are two _pairs_ of parentheses. Each Python command has its own set of parentheses to hold its arguments. The fact that one pair is inside of the other pair doesn't mean you can just throw one away like this: print(randrange(10) Now the print() command has no ending parenthesis and you get an error. There can never be a left '(' without a corresponding ')'.
I've always basically rolled my own. I start a local instance and use pytest to call my endpoints directly.
You probably already know this, but just so that everyone knows, you can do a set comprehension with curly braces, so: set(i for i in range(10) if i % 2) == { i for i in range(10) if i % 2 } The full suite of comprehensions is: [ i for i in range(10) if i % 2 ] # List comprehension ( i for i in range(10) if i % 2 ) # Generator comprehension { i for i in range(10) if i % 2 } # Set comprehension { e: i for e, i in enumerate(range(10)) if i % 2 } # Dict comprehension
Like /u/K900_ pointed, post some example of what do you mean. The only thing I can think of is: This is a tuple: `(1, 2)` This is a function: `fun()` This is a function receiving a tuple as parameter: `fun((1, 2))`. (Making a list of tuples, like `l.append((1, 2))` is pretty common, I believe).
Honestly, Python 3 would have been a great opportunity to fix the naming style. 
Second this
I think that not everything is packaged for 3.6 yet, so it may not be possible to upgrade all your environments directly.
Mind if I update my blog with it with a reference to you?
solved： cp /lib64/libreadline.so.6 ~/anaconda3/lib/libreadline.so.6
well .... &gt; I checked in terminal to ensure that sqlite3 is installed what do you mean by terminal ? the python cli or the linux shell ? maybe you are confusing the sqlite3 package with the sqlite3 python module. pip install sqlite3 
&gt; I checked in terminal to ensure that sqlite3 is installed on my computer What does this mean, exactly? You tried the `sqlite3` command? The presence or absence of that tells you nothing of relevance to this situation, because Python does not use that. We need much more information. The module is part of the standard library, so it should always be available. The most common scenario for it not being available is if you built Python yourself from source and you didn't have the sqlite development headers installed, in which case it can't built the module. (Again, the sqlite development headers are a separate thing from the sqlite command line binary, and a separate thing from the sqlite runtime library. Those are three separate packages on most linux distros, for example.) So, how did you install Python? What operating system is this? What packages do you have installed? Is this a self-built/from-source build? 
In regard to the first question, I tried the sqlite3 command in my Mac's terminal and it showed that sqlite3 is installed on my OS (or at least that's what I took it to mean). Python was installed via Anaconda and I'm running macOS Sierra 10.12.2. The packages I have are the ones included with Anaconda. Also, I'm running this in a virtual environment. I'm going through a Flask tutorial from Youtube
I answered this in my comment below and quite possibly to your last statement as all of this is new to me and I just heard of sqlite today.
Sure no problem Max! Here is my latest [revision](http://nbviewer.jupyter.org/github/pybokeh/jupyter_notebooks/blob/master/matplotlib/Aligning_Legend_Labels_To_Lines.ipynb) which requires that the index of the dataframe is the time series. I'm actually using this at work as a data analyst and is working great. Thanks again for the original inspiration.
Shit post. Good job.
That actually sounds like a great idea for an Alexa skill!
what ? I don't get it [what's happening ?](https://i.imgur.com/lUIlU4l.gif) 
Any ones that teach by "case study" or examples? I've done a foundational book "Learning Python with Raspberri Pi" but now need material that sorta gives me more application 
Good luck! R and Python are very powerful for analytics and data science. Perfect for you.
most likely a python path issue. Verify the right path in your python environment.
Thank you. This did the trick. 
My terminal will happily show images for you: https://github.com/sedwards2009/extraterm It will work across SSH. Runs on Linux, OSX, Windows/cygwin. 
What you're describing is called recursion, and more specifically a depth-first search. Also, post questions in /r/learnpython next time.
Nice! Thank you 
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
This post is better suited for r/learnprogramming or r/learnpython
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
I'm glad to see you excited for the feature. When I saw it I couldn't imagine it ever being used.
Thanks I will have to give this a look.
Just commenting because I want a link to this and I'm to lazy to like share the link or whatever 
To quote the sidebar: &gt; If you are about to ask a question, please consider r/learnpython. Not using relative imports means Python will look in `sys.path` for a python file matching `models.py`. `sys.path` is a list of folders where the interpreter will look for Python files. It searches every folder in order until it finds a matching file, or an `ImportError` is raised. Normally `.` (current working directory) is part of your path. When launching Django that probably isn't the directory where your `models.py` is. TL;DR: Always use relative imports for your projects.
ultimately that'd be an issue with the web server your using, rather than anything to do with Python itself.
`--no-deps` doesn't stop `setup_requires` as far as I know, usually you have to override setuptools/easy_install as per https://pip.pypa.io/en/stable/reference/pip_install/#controlling-setup-requires. The `sudo pip` annoys me (although, `sudo easy_install` is even worse), especially when some projects actually do some odd things (I've seen someone effectively implement a shell script as a setup.py, it wasn't even a python project). I've got into the habit of checking the setup.py of every project I use, even if they come with wheels, partly for security, but mostly because of how badly some people write them (I have to say, I'm more concerned about someone accidentally doing the equivalent of `os.system('rm -rf /')` given some of the `setup.py`s I've seen.
PM me if you want. I'm at work, but I'll be able to get back to you within a couple of hours. 
https://www.stackage.org/ is close to what you want for Haskell (where the PyPI equivalent is https://hackage.haskell.org/).
I personally found PayPal's APIs documentation awful - extremely verbose and redundant, broken links, important info hidden in unimportant walls of text and so on. The API you can/should use (REST vs NVP) depends on the actual payment mechanism you want to implement. For example, for adaptive payments, your only option is to use the (older) NVP API (this was true at least up to some months ago). I wrote a [small post with code snippets](https://robin-vjc.github.io/paypal-parallel-payments-with-python/) on how to get you started with python `requests` in this case. If you only need subscriptions, check [PayPal-Python-SDK](https://github.com/paypal/PayPal-Python-SDK). Good luck. 
Nice! What terminal escape codes do you use to embed?
Typically ORMs protect from SQL injection, but the easiest way to check would probably just to be to try and inject some SQL.
Very true (although I'm only ~5 years in according to reddit). Often times on this sub, or /r/learnpython, I find myself answering questions that I asked years ago and it is quite the feeling being able to give back to this wonderful community.
PCA and tSNE could make up a good dimensionality reduction section. 
Try `import sqlite` instead.
Agreed. Not sure why I got down voted.. love to hear why I'm wrong 
https://pythonconquerstheuniverse.wordpress.com/2009/10/03/python-java-a-side-by-side-comparison/
The Arduino is not capable of running Python itself. The library you're using installs an image that handles the serial line commands received from your computer. The Python is still running on your computer.
Wait, I'm missing a few components. I'll look up recursion, but how do I dynamically create new variables/arrays based on a set of rules? So, if I want each nesting level of the loop recursion (I know that it means that it's calling itself) to have the value it's currently going through, stored in a temporary variable subfolder1, then subfolder2 for the next level, and then I'd want it to make subfolder3 without me telling it to do so.
TOCTOU and atomicity are, of course, why most real APIs don't even bother to separate the concept of "ask if a resource is available" from the that of "acquire a resource"; you simply ask for what you want, and you either get it or you get an error. Anything else is bound for failure, because, as you said, it doesn't matter how narrow the gap in time or how little contention there is — a race condition is a race condition, no matter how small.
Huh? I don't get it. Don't be lazy, explain what these modules are for and what makes them for lazy people. 
To be honest, if I don't trust the systems I'm interacting with, I don't use it. I'll find something else. Testing will take you a long way though. Good integration tests will improve confidence. If I absolutely had to use a week system, I would probably write a ton of safety net code for every possible issue. At the very least, log and adjust.
thanks bro, I have found it!
TELL ME MORE
So cool. Love your channel. I'm totally in! ^^If ^^I ^^have ^^time...
Is there any way to first copy an existing conda environment then upgrade it (in case the upgrade breaks something)?
But how will I know to go to the next position in the previous loop once that inner loop reaches an array length of 0?
 conda env export -n env -f env.yaml conda env create -n env_py36 -f env.yaml conda install -n env_py36 -y python=3.6 You can also do conda list -n env --export &gt; env.txt conda create -n env_py36 --file env.text You can also upload environments to your anaconda account (if you create one) with conda install -y -n root anaconda-client anaconda login --username username --password password # I recommend removing the prefix that the environment is # installed to since it could contain a file path that # you don't want to share. Anaconda accounts are fully # public without paying money conda env export -n env | grep -v -e "^prefix" &gt; env.yaml conda env upload --file=env.yaml Then you can share this environment with others, or just use it as a personal backup conda env create -n new_name username/env_name
Very nice, but Python is a scripting language. It's not compiled. 
Yep. It's called "wait for continuum to package it for you."
Have fun explaining latex to your interviewer.
I hate to state the obvious, but Amazon has been renting multicore machines for over a decade. You can even rent GPUs if you want. The largest single machine you can currently rent has 128 CPUs and almost 2TiB of RAM. Try [Cloudorado](https://www.cloudorado.com/) if you want to price compare to other providers. There are many.
It is a bit more involved than iTerms2's escape codes. It aims to be safe and also a general mechanism for transferring files for display (or otherwise) The `show` command that comes with Extraterm is an example of it in action. Interesting functions are: `startFileTransfer` and `endFileTransfer` in https://github.com/sedwards2009/extraterm/blob/master/src/commands/extratermclient.py and `SendMimeTypeData` in https://github.com/sedwards2009/extraterm/blob/master/src/commands/exshow.py It comes down to: ESC "&amp;" &lt;$EXTRATERM_COOKIE&gt; ";5;" &lt;json metadata length&gt; "\x07" &lt;json metadata&lt;&gt; &lt;base 64&gt; "\x00" where: ESC = escape char 0x1b $EXTRATERM_COOKIE = values of the $EXTRATERM_COOKIE environment variable (security) json metadata = JSON string holding metadata. Important keys are "mimeType" and "filename". base 64 = binary file base64 encoded 
[This book](https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670) is not Python, but it is great for building more complex stuff. [This book](http://shop.oreilly.com/product/9780596158118.do) covers advanced Python specifically. You should probably read both.
For once, StackOverflow is wrong. 
No. I grok Python 3 strings just fine. There are two problems with them: one, that they reuse syntax and names from Python 2's byte strings (incorrectly named "strings"), making it unreasonably hard to write 2/3 agnostic code; and two, the fact that too many library authors don't grok strings and/or fail to understand the subtleties of the distinction. I would gladly use 3 for everything, but the reality is that some crucial libraries still don't work flawlessly in 3.
You wrote this a the way to read/write encoded text from/to a file: with open('some latin-1 file', 'rb') as f: text = f.read().decode('latin-1') with open('some utf8 file', 'wb') as f: f.write(text.encode('utf-8')) however, I believe a more correct way in Python 3 is: with open('some latin-1 file', 'r', encoding='latin-1') as f: text = f.read() with open('some utf8 file', 'w', encoding='utf-8') as f: f.write(text) Just my opinion.
The ESP8266 is the same price as an arduino, runs micropython, and has wifi. My favourite board is the Wemos D1 Mini, about $3 on aliexpress.
So it's not that you don't understand them, but that others don't? Have you tried using the naming aliases in six?
The online mode comes from their proprietary business model I guess. I've no interest in that. Yes, it will show up in a jupyter notebook -- interactive &amp; all, but I've add to to a small patch to the notebook templates for this to work last time I tried though.
Thanks! I had no idea about the existence of this board. Will definitely take a look
Sure I use those all the time. But it still feels like a bit of a kludge.
I've had a very good experience with Bokeh. I highly recommend taking Bokeh for a spin.
I think the alternative would be naming str something else in python 3. Better a kludge in python 2 than in the current version.
In hindsight, I think it would have been better to ditch the `str` name entirely, call the string type `unicode`, and the bytestring type `bytes`, and then maybe add `bytes` as an alias for `str` to Python 2. I don't know really, 2/3 is a mess in so many ways, and it's not like it can be resolved cleanly now, that ship has sailed.
If you really want interactive plots go with plotly. It offers a much greater level of interactivity than bokeh out of the box. I use the offline mode and just generate a &lt;div&gt; and &lt;script&gt; then serve those with flask using my own html templates. One big feature that you get out of the box with plotly is the legend on a plot is interactive. If you click on something in the legend it can hide/un-hide that trace from the plot. You get that for free, out of the box. Thats incredibly useful for me. Another thing is 3d plots. You don't need an extension or have to write any javascript. 3d plots are built-in to plotly. The last advantage I found that plotly has over bokeh is that almost everything in plotly is a dict. This is really useful because I've found certain plotly functions are really slow for some reason or can't be called when you use the multiprocessing module because of pickling issues. Not a problem, in the end they generate dicts under the hood, you can do the same yourself and still pass those dicts to plotly. Also calling help() on plotly objects tells you what keys and values you need to add to your dict. That said there are two downsides I've found to plotly. One is it can be slow at times, but after realizing that everything is a dict I'm able to bypass calling certain plotly functions. Generating a 10x10 grid of subplots took 7 minutes calling the plotly append_trace function, generating a dict myself it takes 4 seconds. However, it still takes a browser over a minute to render the html/javascript. The other big issue I've found with plotly is lack of offline documentation. They at least have a full API reference on a single page which you can save with a browser, but its not the easiest thing to find stuff in. This issue is much worse with bokeh though, they have nothing and refuse to fix the problem (i opened a ticket on their github and they said they won't do anything about it, that was one of the major reasons why I had to swtich to plotly). 
Trying to write in two languages at once seems like a kludge.
&gt; for lazy *humans*
Writing APIs and "asking forgiveness" are two unrelated concepts. An API is a way of returning answers to the user. "Asking forgiveness" or not, is about getting to these answers. Say you're writing a routine to access some data, run calculations and return the result. As it happens, your data source could go offline. When you are dealing with the internals of your routine, you are free to test for the source availability first ("look before you leap"), or just ask for the data and be ready to catch the exception ("ask forgiveness"). What you should return to the user if something goes wrong, on the other hand, is an entirely different matter. Maybe you want to raise an exception, or return a default value, or some kind of a "flag" value - this is specifically a matter of API design. In the rare case when you can afford returning a default value (eg when dealing with direct user input, as in "type [y]es or [n]o, default [y]"), you should go for it, of course. Otherwise, there is nothing wrong with an API raising an exception if something goes wrong: in fact, it is a flexible design (you raise different exceptions to account for different issues), and very often the design of choice for Pyhton devs (eg. almost all the standard library). Of course, by "returning value or raising exception" you are pretty much forcing the client code calling your routine to adopt an "asking forgiveness" approach, because an exception is an entire different beast from a return value, and you can't "look before you leap" for it. If you choose to return a flag, then you may opt for a flag of the same type of the "normal" return value (eg. "returns a positive integer, or -1 if something goes wrong" - old c++ school). This would be a terribly poor choice: you are forcing client code to adopt a "look before you leap" approach, and expose users to all sort of bugs. Returning a flag of different type is acceptable and a common choice in API design (eg., "returns an integer or None"): clients are free to adopt both a "look before you leap" or an "ask forgiveness" approach, and bugs are more rare (typically involving that both None and 0 are False). Of course, it can be a little less flexible as an API design, than raising exceptions - but it really depends on you specific case. 
You're welcome, and very happy Saturday to you as well!
Well, if you really must: https://github.com/ajalt/fuckitpy "Still getting errors? Chain fuckit calls. This module is like violence: if it doesn't work, you just need more of it."
&gt; Actual examples people have given me “Python 3 can’t handle bytes as file names” &gt; &gt; Yes it can. Just stop treating them like strings: And if you ever tried that you'll quickly find out that it becomes a friggin nightmare as tons of libraries and functions like their arguments to be strings, not bytes. You can't directly `print()` bytes and there isn't a `bytes.format()` either, but the old `%` format works for some reason with `bytes`. If you treat filenames as bytes, you end up with bytes in places where strings are expected or the other way around. Things get even more fun when you use `os.fsencode()`. That function allows you to handle filenames as strings, including invalid UTF-8 which gets escape with surrogates. Only problem is that the escaped strings are still invalid UTF-8 and things will crash and burn when you try to use those strings in function that expect Python strings to contain something that can be converted to UTF-8. Python3 string handling is a huge fucking mess and there is rarely a day when I don't run into issues with it. The best part of it is of course that you sometimes only find out about issues weeks or month later, since filenames almost always are valid UTF-8, just not always. Also don't forget to add some: sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors="surrogateescape", line_buffering=True) sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors="surrogateescape", line_buffering=True) into every of your program when you don't want `print()` to throw. I am not saying that the Python2 way of doing things was good, far from it, but Python3 is giving tons of trouble in areas that used to work just fine before. Trying to route bytes verbatim through your program has become a hell of a lot more complicated than it used to be.
Except that, conceptually, a python3 `str` is not necessarily unicode. It is a string of characters, in some encoding that need not be worried about. The fact that it happens to be stored internally as unicode is irrelevant. I think that the current standing, where "unicode" is only present as a encoding for `bytes`, prevents future confusion.
Still don't get it. Found the article confusing and ungrokkable. related question - do you have the specify the encoding for every file Rou open in Python 3? Article implies you do w but author seems to be going for breeziness over clarity.
Python 3's easy handling of unicode was the main reason I started using it whenever possible. A couple times I ported libraries to Python 3, so I could keep using 3, since it was easier and quicker than trying to handle that stuff in 2.
&gt; Python 3 doesn’t support writing files as latin-1 Then author proceeds to give counter example where he tries to *read* a file in Latin 1, and *write* ~~it~~ another in UTF-8.
If you open a file in text mode, that file will always have an encoding set. You don't always have to specify an encoding, because that parameter to open() is defaulted to UTF-8 and that's what you'll want more often than not. If you open the file in binary mode, it will read and write `bytes` instead of strings. In that case, the encoding passed to `open()` won't actually be used for anything, so it's always safe to omit it and take the default.
Any recommendations on how to properly handle the cases you mentioned? 
"Unicode" is not an encoding. It is a standard that defines code points, glyphs, how code points relate to glyphs, and a set of encodings. A "unicode string" is conceptually encoding-agnostic; it typically uses one of the standard UTF encodings for its internal representation, but this is an implementation detail, transparent to the consumer. So `unicode` is perfectly appropriate IMO.
Started the week with machine learning and now doing some cryptography. If anyone has tips on either topic I'd appreciate the help
please change the name
Probably not, because he's just repeating the same criticism from 5 years ago.
You'll be better off trying to figure out what went wrong with cx_freeze and pyinstaller (tried py2exe too?). Post that to /r/learnpython and I'm sure someone will help.
Weird to see æ used as a ligature in English, since it's a regular letter in my language! Is that common usage? 
Uh, yeah, but no. I used to maintain a python 2 app that interacted with various systems that were, let's say, heterogenous in their treatment of strings. I was fighting encoding problems for months in python 2. I finally bit the bullet and converted the project to python 3 specifically to solve this problem. It took a few weeks to nail everything down, but do you know what happened? Virtually ALL of my encoding, unicode, StringIO, exception handling, blah blah blah code vanished. You would not know that it was ever a concern from looking at the code. I have one function hanging out to deal with a mis-encoded string hanging out in a CDATA block in an XML file in a different encoding. I did have to interface with libraries that did some unexpected things, but those quirks were trivial to handle at the boundary and every library I use (except `earl/beanstalkc`, which prioritizes support for Python 2.4) has since corrected itself. The way you reach zen is to only ever use strings internally. You encode/decode at the interfaces to your code and then only ever work with strings. Suddenly, everything works and you gain the ability to trivially handle any encoding at the boundaries, even though everyone not using UTF-8 should be slapped every day they're not.
We currently can't upgrade because we can't get wxpython working in our domain on python 3 (I've heard of some success stories but we use it heavily). We also have some internal libraries that we haven't updated yet. However, about 3/4 of our code base is compatible with both python 2 and 3. We're slowly moving there, maybe after we get another person hired we'll be able to speed that up. Realistically, my team isn't on python 3 because of logistics, no real other reason. At this point, we use conda so we don't even have to update old code, just the libraries we need to have in common.
The file names are different, so I assume they are two different, unrelated, files.
Clear and direct . Thanks a lot. 
You probably need to provide more details. Your nearest research institution would probably rent out their supercomputer if you have a legitimate use case for it. There's also AWS, Azure, SoftLayer, pick your IaaS provider. 
There's a chance that British technical / academic journals use æ, sort of like how the New York Times uses the spelling "coöperation"
I wrote a lot of Python 2, and I always thought it supported unicode just fine with u'...' The thing that really changed between Python 2 and 3 was making unicode strings the default defined by '/" and having to do b'...' for a byte string. The default string implementation was swapped, and despite what OP said, the new string default isn't best for everyone. It depends on the type of coding you're doing. While the Python 3 method may indeed be better for most use cases, it's inconvenient for others. The change explicitly makes dealing with unicode easier at the expense of byte operations. Making a website? That's probably slightly more convenient with Python 3 style strings. Writing networking code or file parsing code that needs to care about the actual bytes? That's slightly less convenient with Python 3. It's a trade off. It's not better, it's better for some use cases, and worse for others. Either way, Python 2 or 3, the knowledge required to do it right is the same. If you don't grok the difference between characters and bytes you're going to have a bad time in both 2 and 3 and any other language for that matter.
Aaah, I didn't catch the encoding joke! 
emulating minesweeper with pygame going well so far
Out of curiosity, why are you using Anaconda instead of your Linux distribution's Python?
- indents instead of braces / parens - dict(zip(list1, list2)) - using generators in __iter__ to easily create iterable classes that work in for loops - tuple packing and unpacking - objects AND functions are first class, no need to choose
autism
Python 2 strings are essentially byte arrays, and hence are convenient for working with bytes. Everything was just bytes and my_string[1] got you the second byte. I'm not sure what you mean by "random encode/decode errors", since encode/decode has to be explicitly invoked. Any use case where you see a bunch of b'...' today was slightly easier in Python 2, and any use case where you saw a bunch of u'...' is slightly easier in Python 3.
That sounds great! Are you able to say how the program worked? 
&gt; You can't directly print() bytes and there isn't a bytes.format() either, but the old % format works for some reason with bytes `%` only works when both sides are byte values – otherwise you have to encode them: &gt;&gt;&gt; print(b'foo%s' % 'bar') Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; TypeError: %b requires bytes, or an object that implements __bytes__, not 'str' &gt;&gt;&gt; print(b'foo%s' % 'bar'.encode('utf-8')) b'foobar' This seems like the most logical behaviour: bytes need bytes, strings need strings, and things like `print()` are defined as only receiving strings. Similarly, your mentions of `os.fsencode` and `io.TextIOWrapper` makes it sound like you actually have the underlying problem of things inconsistently being bytes or unicode and your code only worked in Python 2 because you didn't test it with a wide enough variety of data or operating environments (e.g. I've seen a number of times where someone's “tested” code failed the first time they ran it when `print` wasn't outputting to a TTY with `LC_ALL=en_US.UTF-8`). None of that is necessary for code which decodes and encodes at the boundaries. This was also the best approach on Python 2 – the only difference is that now it's more than just convention.
machine learning as in AI?
blackjack
While I have a personal preference for Bokeh by far (more flexible, capable, pythonic, and open), my experience at work has been that Plotly is a lot more effective in an enterprise setting (at least for the time being). Another option is simply to use Flask to setup you site with the Javascript library of your choice.
Hmmmm... reasonable people being reasonable... In before the flame war
Yep, just the basics though
&gt; In what insane reality does 'おはようございます'[0] == '\xe3' make sense to a human? It makes sense to a human who knows how a computer works. I am happy to have all the abstractions over strings that Python 3 introduced, but saying that "humans can't make sense of this"... A programmer should know that strings are stored as bytes in memory using different encodings. Anything else is simply an abstraction. Which are nice to have, of course.
Highcharts is only free for non-commercial use, IIRC.
&gt; Python 2 strings are essentially byte arrays, and hence are convenient for working with bytes. Everything was just bytes and my_string[1] got you the second byte. … and that still works the same way in Python 3. What doesn't work is being sloppy and assuming that e.g. the 40 bytes you read off of the network can just be dumped to the console or written to an HTML response as-is or that array indexing has ever been a safe way to do string processing once you need to support text outside of the US-ASCII range. &gt; I'm not sure what you mean by "random encode/decode errors", since encode/decode has to be explicitly invoked. I was referring to all of the ways people hit `UnicodeDecodeError` or `UnicodeEncodeError` because they passed the wrong type to something which attempted to implicitly convert it and were confused because e.g. the formatting, logging, etc. call which triggered the exception was now a long way from the code which had read bytes and passed to something which expected a string. A quick pass over GitHub issues, StackExchange, etc. will easily find thousands of cases where Python 2 was harder to use because it allowed someone to get further before seeing the problem. &gt; Any use case where you see a bunch of b'...' today was slightly easier in Python 2, and any use case where you saw a bunch of u'...' is slightly easier in Python 3. I'd still like an example supporting the first part. In my experience, working with bytes is equally easy with both; text is only easier in Python 2 if you're explicitly stating you either only work with ASCII or don't need to fix errors. 
I said, even though you overlooked it, that I like the abstractions. I said too that `\xe3` being the first byte of `お` makes sense to a human who knows the basic concepts of computing, which is the opposite of what you claimed.
I find berating and condescension to be the most effective forms of argument. (Sarcasm also works well.)
Yes. That is a correct assumption. He reads from one file in latin-1, and then writes the text that he read from that file to another different file in UTF-8. There's nothing wrong with what he did. Except that, in the structure of his post he presents the problem "Python 3 doesn't support writing files as latin-1", and then provides an example that at no point actually writes a file in latin-1. I understand python 3 has no issue writing files in latin-1, so he could have easily provided a valid example but didn't.
Trying to build them with .format()?
&gt; Similarly, your mentions of os.fsencode and io.TextIOWrapper makes it sound like you actually have the underlying problem of things inconsistently being bytes or unicode Welcome to the Linux command line where everything is bytes. &gt; None of that is necessary for code which decodes and encodes at the boundaries. The nice thing with the Python2 way was that you didn't need to care about the encoding, you could just pass the data through verbatim. In Python3 that has become a lot more verbose and error prone. &gt; None of that is necessary for code which decodes and encodes at the boundaries. Easier said than done. Python3 has created a lot of problems that just didn't exist in Python2 when it comes to simple scripts. I am not suggesting that the Python2 way was better, but I feel that the tools that Python3 offers to deal with filenames, command line arguments and other sources of invalid-UTF-8 aren't quite good enough, it's all to messy and fragile.
It makes technical sense, not practical sense.
Thanks 
&gt; and that still works the same way in Python 3. It works the same if you're using `bytes`, but the default is now `str`, where my_string[1] returns the 2nd character instead of the 2nd byte and`len` returns the number of characters instead of the number of bytes. &gt; I was referring to all of the ways people hit UnicodeDecodeError or UnicodeEncodeError because they passed the wrong type to something which attempted to implicitly convert it and were confused There were no implicit conversions in Python 2, that simplifies working with bytes, since you have to be explicit. &gt; I'd still like an example supporting the first part One example is csv, Python 2 just returns bytes, Python 3 assumes the CSV has encoded text. Another example is networking, where your code is typically byte arrays. your code can easily break if you accidentally use '...' instead of b'...', and you may have to do non-sensical conversions to interact with 3rd party libs. Just to be clear, I think Python 3's decision was correct because it is better for most use cases, but it's worse for some other use cases, and OP isn't recognizing that this is the case. We're not all manipulating human language strings, there is a significant amount of existing Python code that's focused on manipulating byte strings. Honestly I think the main thing that makes this difficult in both Python 2 and 3 is the lack of typing and the fact that both `str` and `bytes` use a very similar interface, so if you accidentally pass a `str` to a function that expects a `bytes` or vice-versa it could silently fail and return an incorrect result instead of erroring-out with "no, you're passing the wrong type dummy". That's what's confusing about the whole thing.
Paths are bytes on Linux and unicode on other platforms. They are inconsistent and you can't just encode or decadent the boundaries. Python's handling of paths is completely and utterly broken by design.
There is more to programming then mastering a specific language and creating stuff from scratch. Most of it seems to be maintenance and a lot of cage fighting with the stakeholders and production artefacts. Also afaic most shops would be a little more specific then *just python*. Like webdevelopment, data-science or whatever, all have domain knowledge beyond the language. Maybe find your niche there?
Yeah the default of displaying it as ascii-decoded always seemed somewhat counter to the "bytes aren't strings" philosophy.
I use `bytearray` for such things, but yes it's kind of a wart.
Probably the most realistic simulation of a job is to fix some bugs in an open source project. More than one because the first bug probably just simulates the first day :P
There is no question as far as I can tell. 
Fair. You've got to use mod formatting if join or concatenation don't work for you.
Is there any way to build these executables targeting a different platform (windows) than where the build script is running (ubuntu)?
Cannot upvote this enough: always set the encoding! The underlying problem is that Python tries to be helpful but that often only lets you get further along before finding the problem: https://docs.python.org/3/library/functions.html#open &gt; In text mode, if encoding is not specified the encoding used is platform dependent: locale.getpreferredencoding(False) is called to get the current locale encoding. This is especially easy to do if you're working on modern or professionally-maintained servers where cron and all of the daemons are running in an environment which defines `LC_ALL`, `LANG`, etc. and then you start supporting users who are running older distributions, don't have configuration management, etc. so you get bug reports about how something fails on one of their 5 servers or only when run by the new user without `PYTHONIOENCODING=utf-8` in their shell init files.
My thoughts always and forever. And only reason i have troubles with py3.
I remember a time before we googled things when we altavisted things. 
You underestimate my laziness. Too lazy to find save button. Using weird Reddit third party app.
There's nothing that needs a lot of time to understand. Bytes are bytes. Strings are strings. That's it. What you have to do is not overcomplicate things.
Why would you need to format bytes? Bytes are... bytes. 
Slowing working on an API for RPG-related web apps. When it is done you should be able to set up web-apps that are relatively RPG-agnostic.
My situation is that I've never had the Python 2 handling break my code. Everything I've had code handle has either been latin-1 or has been passed through where the unicode handling doesn't matter, and things like invalid code points can be handled at a different level. With Python 3, it seems this would no longer be true, and to e.g. deal with filenames correctly I would have to make sure that I do magic tricks like passing bytes into file scanning functions. This scares me. I'd love to have something that could do completely clean handling of text that doesn't fit in latin 1 - but Unicode failed that promise.
Also, working on an open source project will train you (for some, not all projects have the same community manners) to collaborate with third parties in software projects, etc. You'll probably learn code review, code familiarization, modes of communication, etc.
That's one of my top reasons to start a channel. I would also love to show my thought process and debugging because that usually falls short in most tutorials 
Thanks guys all you responses have been very helpful and informative. Now I can make my mind up :)
Library is just program. So you need to learn programming first and for that i recommend going to learnprogramming or learnpython and checking their faq 
[removed]
I learnt ruby on rails using [tutorials by Mackenzie Child](http://mackenziechild.me). I like his style - enthusiastic and learning as well as teaching. Later, all of his tutorial code was available on github. This was a huge plus. So even at a later date, people can look at his thought process. Plus, you don't have to stream HD videos just to see the code on screen. I have also used gorails.com, but that seemed more advanced. First I got into rails with Mackenzie child's videos and when I needed something more advanced, I went to gorails. I haven't seen anyone quite that captivating for python. Yet. 
Write something that consumes a web API, does something with the response, and writes to a different web API. Then write it again to separate the code that's making calls to the different services via requests from the rest of the code in to their own modules. (You _did_ use requests the first time, didn't you?) Then write it again to better translate the data from the first web service to the second. Then write it all again to somehow make the process even better. Repeat that last step a few more times.
[PySDL2](http://pysdl2.readthedocs.io/en/latest/)? Its what I used when attempting to make a B.A.R clone in windows. While I failed it was fairly simple to put text on the screen where I wanted it (and with a ttf font at that) and update it. I believe another method would be PyWin32 and Cairo.
Except that it's a string, not a byte array, the slicing notation is there explicitly for characters, not bytes, if you want to treat it as bytes, you should be using, or cast it to, a byte array. And besides which, if the slicing notation does get used for bytes, where do we put the functionality for using slicing notation for characters like people actually want to be able to do? Some kind of subclass of str, possibly called StringButWhereTheInterfaceIsActuallyDesignedAroundstringsAndNotBytes? Because I just made that up as a strawman to make you sound stupid, so we can't do that. If you want to slice by bytes, use bytes, strings have their own interface because they're an important abstraction that's worth having an interface for. It's not like this isn't how Python has been doing things up to now, there's a reason the language gives you lists, sets, and dictionaries right off the bat, but makes you import single typed arrays, even though the former are large abstractions over the latter, because it's easier to think in terms of arbitrary sized integers in a list than it is to think of 32 bit unsigned integers in an array, and it's easier to think in terms of slicing strings by characters than it is to think in terms of utf-8 encoded byte arrays when what you want is to process some text.
For those not familiar with dealing with ppas: # Might need `sudo apt-get install software-properties-common` sudo add-apt-repository ppa:fkrull/deadsnakes sudo apt-get update sudo apt-get install python3.6-dev
what is tech stack?
Why shouldn't you index, slice, etc.? I've never heard anyone say that.
m8 i already grokked it
A group of software technologies. A basic stack for web development is HTML, CSS and JavaScript. The tech stack for the product I work on is Django, Postgres, and Angular (plus more).
[They fixed this a while ago.](https://www.python.org/dev/peps/pep-0383/) It's less convenient if you only ever needed bytes, but correctness is not sacrificed.
Everyone understands your position, you've said you like the abstractions like 3 times. We keep arguing with you because your point isn't arguing against what he's saying, and that is making it hard to talk to you. He is saying that a human, **even one who knows exactly string encoding works**, would be reasonable to assume that a string interface should index and slice by characters and not bytes. You, regardless of what you think you're saying, sound like you're saying that the python 2 implementation makes more sense because programmers should know how unicode strings are implemented.
I disagree that bitwise operations are a fundamental operation on bytestrings that should be in the language (of course, you are entitled to your own opinion). IMO, the use cases are few and far between enough for most python code that it's fine that such an operator lives in a third party module (which can support more operations that you probably also want, like shifting or rotation), or that you have to do a list comprehension -- which makes the behavior in the case of differing lengths more explicit. It has been considered, and shot down by the python community. If you want to read their arguments: https://bugs.python.org/issue19251
Nope, the process involved in doing this kind of thing would be fairly involved. You can use emscripten to compile C/C++ to the web but then you'd also have to compile Python to the web (which was done multiple times in different projects and it does work) but it's really tedious and frankly not worth the effort. There are plenty of libraries which do support the web as a first class citizen, though.
This is just part the standard notation used in many R packages to denote a model form, and it has been copied by some python packages as u/pha3dra has mentioned. It is worth noting right off the bat that this is an area that python is really outclassed by R, both in capabilities and performance. I am a huge python advocate, but stuff like linear effects models are one area that I almost always do in R (either on its own or through something like Rpy2). Without knowing your background I'm not sure how to tailor this to you. The left side of the ~ is your response / observation / label / dependent variable, while the right side of the ~ defines the form of the inputs / features / independent variables that you believe will give you that response. Another way to write the same model would be something like f(a, b) = a^2 + b + error, where f(a, b) is the response, a^2 + b + error is your model, and the = is your ~. However, the ~ is a better idea since it is a distinct notation and it is not something being solved (like with a function) but fitted using a method like ordinary least squares. 
Maybe add an issue to their github for Py3 support, or fork the project, make the updates, and submit a pull request. They can choose to accept the pull or not. That way, people can at least see your fork.
Its a collection, and no other python operation works on all elements between two collections. Seems totally consistent to me. That's what comprehensions and numpy arrays are for. You even point out the example of having two different length byte strings.
I don't understand your question, and you seem to be using API to mean something other than **A**pplication **P**rogramming **I**nterface. How do you "move data between a few APIs"? I *think* you're moving data from one application to another application, or perhaps one service to another service. E.g. extract it from a database, pass it to another service. But I don't really know. The style "ask forgiveness instead of permission" has nothing to do with the code snippet you give, so I don't think you really "dig" it. The "ask forgiveness" style looks like this: try: do_something_that_might_fail(arg) except PossibleError: # This is the "ask for forgiveness" bit. recover_from_error() As opposed to "Look Before You Leap": if something_will_fail(): # This is the looking bit. recover_from_error() else: # this is the leap do_something_that_might_fail(arg) # except it shouldn't fail But the code you're showing is neither of those. It is written from the perspective of the *producer*, not the *consumer*: if some_condition: # Fail hard, allow the consumer to catch the exception raise Something So the robustness of the "APIs" (services?) you are calling isn't an issue. They will succeed or fail or whatever. It is **your** script which will raise, not them. (Well, they might raise too, but not knowing what they are, we have no way of telling.) So the question you have to ask is, what is the consequence of **your script** raising? If your script is supposed to run forever, then you need to code your script in such a way that it cannot fail, full stop. Or that it surrounds everything with a `try... except: restart()`. Or make it a service, and have the server automatically restart it when it stops. If your script is not supposed to run forever, then what's the consequences of it failing? If the consequences are unimportant ("just fix the cause of the failure and then run the script again"), then its fine to use raise as you do. But fundamentally you're asking a question in such broad general terms that the correct answer could be absolutely anything.
Just got my first python job. See http://elliothallmark.com also here was an assignment I got for the interview (vagrant, flask): https://github.com/Permafacture/film_search
I wouldnt say -never- but youre right that in probably 99.9% of cases you'll need to manually adjust something.
&gt;When learning to program, don't make projects that you expect to use. I have to disagree. Just pick something useful that you could actually use. Build a personal finance app, build something that emails the price of some stocks you follow every day. Programming projects you don't intend to use is great way to kill enthusiasm. 
A couple years ago when I started learning Python, I looked into the whole 2 vs 3 debate. 3 seemed the obvious choice, but I do a lot of work with non-English text... I took one look at Python 2's strings and ran away screaming. Hope this article makes it far and wide. Programming languages give us tools not only for building but also for *thinking* about problems; this is a great negative example.
Fair enough. When I said API, what I should have specified is I was talking about HTTP based APIs exposed by two different systems. In that context I was talking about how I should handle the possibility of failures in systems I interface with, but do not control. What you said makes good sense as it concerns how the script reacts to failures that occur. What I'm asking about is more along the lines of: Should you go out of your way to avoid causing errors in the systems you interact with? here's an example: TOKEN = 'asdf' HEADERS = {'token': TOKEN} RESOURCE_URL = 'http://server/api/v1/resource' def lbyl_create_resource(unique_id): id_url = '{}/{}'.format(RESOURCE_URL, unique_id) if requests.get(id_url, HEADERS).ok: # resource exists, don't create it. raise ValueError('ID {} already exists.'.format(unique_id)) return afbp_create_resource(unique_id) def afbp_create_resource(unique_id): result = requests.post(RESOURCE_URL, HEADERS, data={'id': unique_id}) if result.ok: return result.json() else: raise RuntimeError('Could not create {}, possibly because it already exists.'.format(unique_id)) There's still a race condition with the lbyl version, but it would reduce the occurrence of that particular type of error. Is it generally worth doing that? If you *don't* have a firm understanding of how tolerant the external service is to errors, do you assume it's going to behave correctly until proven otherwise, or do you treat it more gently? 
thanks for the info. i'll have a look at it
Considering that JS is in the picture, the only approach I can think of will be using selenium. That should be able to do everything you do by hand right now. EDIT: Alternatively, if you have a good understanding of web requests, you can probably try to login directly via the appropriate POST request and use cookiejar along with requests to do what you want.
Yeah, I've tried using that but I couldn't upload my files from my computer onto it. I don't think it allows that in general but I could be mistaken. Any thoughts?
pytesseract is good, but I find I usually have to clean up the image in Photoshop first for the best results.
Great video! I'll need to play with these. Big user if flask, I like the integration with Zappa.
I agree entirely. All of my side projects are apps and tools I actively use, so I get new ideas for them and I'm always excited to learn how to implement them. I also enjoy learning new tools specifically to implement new features in my apps, but I learn the tool first before I code anything into my app with it, like the first commenter suggested.
Pm me your idea
As someone who accidentally nuked a Ubuntu installation by messing with the system python via the symlinks in a virtualenv, this. I keep my system and apt-controlled python environments as clean as possible (basically only have command line tools like ipython and fuckit), and do everything in conda environments. Zero regrets, and it's saved me a bunch of times.
Strings in Python are sequences of code points. A code point is not a character. You cannot combine code points arbitrarily and get valid unicode. Indexing or slicing might create invalid strings that can't be encoded. Reversing can mess up the characters. In order to uppercase or lowercase a character you'd need to know at least the language that's being used as rules between languages vary. In German for example ß is SS uppercased but there is an uppercase ß, which one might use for stylistic reasons. 
They aren't necessarily different at all except for notation. This is also an issue that comes up a lot in papers, as you will often see nearly the exact same model written as Matrix Form, Equation Form, Algorithmic Form, etc. Personally, I think the R form is quite good for what it is trying to denote.
IDLE is effectively [almost dead](https://github.com/python/cpython/commits/master/Lib/idlelib). Just use an actually good text editor or IDE.
Rainmeter? You could just write to a file in Python that Rainmeter reads from.
I just install conda and put its root environment on my path ahead of the system python. Then never worry about the system python again. Is there any reason to use the system Python at all if you've already installed conda?
I've written a few very small apps (1-5 routes, logging, pre-compiled responses, etc) using Sanic and prefer it to aiohttp at the moment. This [proxy I wrote](https://github.com/ludus-tv/ludus-proxy) is a single route and still had to use aiohttp for their async http client. But it's been running for ~2 months now without touching it. However, most of the libraries I use aren't async compatible so I still use the gevent+flask route for almost everything. Although, with gevent 1.2 you no longer get SSL recursion errors in Python3.6, so that's nice. 
Yes, I mean Programming techniques
Been looking at something like this to avoid conflicts with system python. Can you shed more light on how to do this, sorry noob here. Thanks.
No mention of the gil or asyncio? 
I don't know what they did exactly, but I assure you I have tried and I do not have that ability. I think they did something weird with the DNS to block it. When I try to install some package, there's an error with a URL that says something about external repos not being allowed. 
Python internals. When does python copy a list or when does it just refer to a list. 
Look at sentdex his channel for intermediate python. 
Applying the python 2to3 will do most of the job but the code might not work in the end, you should do some **little** adjustments for it to work. 
I have not tried it, but there is one possibility https://github.com/brython-dev/brython-pygame There is one example here https://github.com/brython-dev/brython/blob/master/www/gallery/pygame/chimp.html
 &gt;I'd love to have something that could do completely clean handling of text that doesn't fit in latin 1 - but Unicode failed that promise. What do you mean? Text that isn't properly encoded or text that is interleaved with bytes or what? In any case it's either broken data or data that isn't exclusively text, and you'll have to handle it like what it is. Strings aren't supposed to hold data like this.
Cheers
Why `python3.6-dev`? I think it should just be `python3.6` for most people; `python3.6-dev` is for installing Python 3.6 header files.
I wrote a Tkinter based gui to monitor the output from 18 different queries, representing various health aspects of an Oracle datawarehouse. This saved an enormous amount of time otherwise spent tabbing back and forth in Toad. The various query results also turned red /green/ orange, depending on batch stages or if there were errors needing attention. 
https://www.python.org/dev/peps/pep-0541/
I've been (attempting to) make an event loop agnostic async version of requests. Like aiohttp, but not bound to the asyncio lib. It's odd :D So far it's managing http methods through curio very nicely. Giving it a refactor today before I begin implementing auth / https, and then finalising how I deal with making the whole thing awaitable without relying on an internal event manager.
If you install Anaconda using instructions at https://www.continuum.io/downloads, I think the installation script will ask you whether to add Anaconda's Python to the `PATH` environment variable. If you forgot to do so, you can always add it later manually using instructions at https://docs.continuum.io/anaconda/install#linux-install.
Do you have the source publicly available? 
I wrote a script that allows me to type $ serve thisdir to serve a directory over HTTP using livereload (a cool development web server that means changes to files in the directory trigger a web browser refresh). The value is that it daemonises livereload as well as maintaining a pidfile so that if I then do $ serve thatdir then it replaces the previous server rather than crashing because the port is in use. It's a simple script but it fits well into my workflow. 
sauce?
I read the article, which was how I got scared about this. No, I did not try writing something - I'm not that interested in switching from Python 2 to Python 3 until we also do that at work, and with over a million Python files that's going to take a while. And fear is a particularly useful tool; if it wasn't, we wouldn't have it.
After authenticating using: gauth = GoogleAuth() drive = GoogleDrive(gauth) You can upload a file using: (with the file image.jpg in the same folder as your script) file = drive.CreateFile() file.SetContentFile('image.jpg') file.Upload() 
I just wrote about this a few minutes ago on r/rust, but here it is again I guess I wrote a script to create an Ubuntu 12 droplet on DigitalOcean with the rust compiler toolchain installed, upload my source files from my company's internal git server, compile and wait for the binary to be generated, deploy the binary to one of our Ubuntu boxes, and then destroy the droplet after all is done. It makes deployment pretty convenient, and honestly kind of cool. It's annoying I had to do it this way, but I gotta admit, it's kind of fun leveraging all these technologies to do this for me in the background
Unicode promised that for the price of 16 bits per character, we'd get to encode all characters, avoiding the pain of shift-JIS and similar encodings. They then felt that they had enough characters to promise every language that they could have a different encoding for the visually same character. It turned out to not be true, and instead of going back on that promise, they went back on the 16 bit promise. The result: We have 32 bit unicode, which is too costly to deal with unencoded, and everything implements storing strings encoded in something like shift-JIS in memory, rather than just having flat character strings with 16 bits per character.
If you want to use packages that are C extensions you need the headers.
You can also try [bqplot](https://github.com/bloomberg/bqplot). It is also interactive like Bokeh and Plotly. It is mantained by the Bloomberg company and [it was relased under open source license in 2015](https://www.reddit.com/r/Python/comments/3ntpf9/bloomberg_just_opensourced_their_ipythonbased/).
Yeah, but my point is most programmers don't need the headers, so they don't need to install them until they need to. OP's post seem to suggest everyone should get the headers, which is an overkill. Why install more packages and introduce more bloat than necessary?
This is what happens when laziness and lack of coffee meet. It's not pretty.
Anyone can post a project on PyPi. Still, GitHub has a lot more. Part of the reason is that GitHub isn't language-specific. Do you guys think that there's also a certain expectation of quality with a package index that doesn't exist with a hosting service like GitHub?
* Made a web crawler + IRC bot script that allowed a channel I regularly used to automatically pull a specific website info and display it when given a certain keyword. * A forum crawler that every 24 hours checks for new content and emails a list of recipients with links to new stuff - using HTML formatting, separated by forum sections, tagged by new thread/new comment * Script to push a notification to my devices when something was posted on some subredit, or when a specific user posted something new. * At my previous job, I often needed to convert CSV or XLS file into slightly different CSV format for import into another tool, so I wrote a script to do that for me.
A customized version of Ubuntu. 
From the youtube link "Dropbox has several million lines of production code written in Python 2.7. As a first step towards migrating to Python 3, as well as to generally make our code more navigable, we are annotating our code with type annotations using the PEP 484 standard and type-checking the annotated code with mypy..."
Learning python and how to build an API with it. I work with astronomers who use the VOSpace protocol, my API will need to access data with that protocol. I'm super motivated by the idea of using python in order to do that.
yes I was thinking about making it work first (step by step) and also showing the refactoring
I wrote one simple script. I play one online text game (godville), script opens the page with my hero, collects the data from it, formats and writes it in excel file, so that I can see the progress.
That's the thing, I know about classes and such. I came here to hear from the vets. Like what's the reasoning why you would do it. I don't need a tutorial, it's more of a why thing. 
&gt; I know about classes and such. Probably not if you're asking why multiple files are used. Do you understand the concept of encapsulation? https://en.wikipedia.org/wiki/Encapsulation_(computer_programming) It's one of the key benefits of using classes, and directly relates to multiple file use. I don;t know how to teach, which is why I don't post in r/learnpython, but there are specialists there.
Ok, I'll check it. Encapsulation when it comes to programming is something new to hear. When I think of that I think of networking. I wasn't asking you to teach lol it's just an opinionated thing which is why I said why would you. I've seen programs utilizing the same things and concepts and have been in one script and others that do the same that use multiple which is my whole reason for asking. But thanks I'll take your word. 
Anaconda is still only python 3.5 unless I'm mistaken. 
Stop using IDLE, problem solved.
You can likely use the `pandas` library to read in the spreadsheet. Python has an `email` package in the standard library. 
I think he's talking about things under the hood in Python 
This is cool, and I wasn't familiar with a couple of the conventions. Weird suggestion: You should make it into a cheat sheet. The examples are really clear and concise, and the descriptions could be reduced to exclude not-entirely-essential information, and less verbose sentence structures. It's already a nicely concise blog post. 
What spreadsheet format do you plan on using?
Then use atom, or sublime, or notepad++. IDLE is just bad.
/r/learnpython
Correct. My main recommendation here was Pygal, simply throwing in that, for anything more advanced, I also use highcharts. It's unclear to me whether his blog is commercial or not. 
That's better than nothing, but not really what I'd call a fix. It looks more like an attempt to pave over a design error.
What do you mean by 'code point?'
There is no README, just a Jupyter notebook
I think the same thing happened to me once (in another program), but it had the option to select non-monospace fonts instead, so it worked. But it works for me in most IDEs and editors.
Well, pypi-test exists for this reason: https://testpypi.python.org/pypi
You're welcome! I think that is a big issue, not many people have heard of it.
I think I found a posting from them saying the end of January is their target ... 
Fair enough!
I don't really understand why anyone would need a readme when you've got a jupyter notebook...
This is pretty good. You might want to introduce the term "dunder", which is a common shortening of "double underscore prefix double underscore suffix". People will say "dunder init" or "dunder repr" to referr to `__init__` or `__repr__`. Raymond Hettinger has a bunch of great talks on Python including [this one](https://www.youtube.com/watch?v=HTLu2DFOdTg) which I would link to when you talk about the purpose of name-mangling.
That was probably PuTTy. Mintty is built on the same engine, but lacks that option unfortunately, and there isn't a viable alternative for MSYS2/Cygwin. Yet another reason to finally jump ship and switch to Linux as a daily driver.
There's a lot wrong with your comment. The response time of Lambda is not attributable to Zappa. It's attributable to Lambda. What you're effectively saying is serving WSGI requests that take 600ms to complete is awful, but HCI studies show humans actually start perceiving things as slow after 3 seconds, so 600 ms, for most cases is probably quite acceptable. The 645ms it took to first byte of your project https://getnikola.com/ didn't seem that bad to me. Not only that, but this response time is something which may be improved on the Lambda side independently of Zappa, which Zappa will benefit from free of charge. The other thing wrong with your comment is that CGI style--one process per request---is bad for scalability per machine, but Lambda is based on a fleet of machines optimized for this type of computing. (i.e. It's scalable by its nature). The most egregious problem with your comment though is coming out blasting someone else's hard work and just coming off very arrogantly. Ask questions to gain an understanding next time.
Its analogous to private, whereas `_varname` is analogous to protected, because `_varname` can still be used by children (potentially accidentally), since within a class creating, or modifying a `_varname` variable is allowed. That means that if you inherit from `Widget` and have a method that sets the attribute `_foo`, you can overwrite the base class's `_foo`. Alternatively, if you declare a `__foo` on your child class, it will never name conflict with `Widget.__foo`, because to access that you'd actually need `Widget._Widget__foo`. but yours is `Child._Child__foo`. That's private to the class, instead of protected to it and its children. Now as I said, you can go fiddling with `dir` and `__dict__` if you absolutely must, but you can also use reflection in Java, its just easier in python.
You may want to integrate the soccer app into conky
Well, people have done nasty things like overriding the tokenizer to transform their code at import time. It means that your main.py has to be standard python, but anything you import can have custom overridden or new operators. But the tilde already exists as a python operator - it is the inverse operator. I overload it when doing mathy math (when handling domains, sets, groups, even euclidian solids, it is useful to be able to know the inverse set). You could almost certainly override ~ to be an assignment operator somehow. Have a look at https://hg.python.org/cpython/file/default/Lib/tokenize.py and the docs for `ast`, `tokenize`, `dis`, and company. There's a good chance that you can fuck things up in weird and wonderful ways.
Thank you for putting this up. There is a serious lack of information available online for those who want to learn more about NLP.
I'd suggest VS Code as well. It's based on Atom but runs faster and has a little more polish.
Do you have a native english speaker that could proofread your blog for you? 
Yeah, so a nicer way would have been to just consider the name `str` tainted, make `unicode` the Python 3 string type, and make it backwards compatible with 2's `unicode`. Would have saved a lot of xonfusion and shenanigans.
As for #3, no, those are not equivalent, for two reasons: You have negated the expression, but more importantly, certain objects that are not `None` will still evaluate falsy, in this case notably the empty string which seems like a perfectly reasonable default value.
What expression are you using? In [1]: bool('' is not None) Out[1]: True In [2]: bool('') Out[2]: False 
It's an encoding joke.
OK, I double checked and you are right. I'll amend my comment to say that the change is only if you would consider "" or [] as equivalent to None when dealing with constants.
typecast_fn should be treated as None even with an empty string or empty list though, right? Assuming that you don't follow number 2 and check that it is of type "type".
Yeah, treating `typecast_fn` that way is probably fine. It's possible to create callable falsy objects but I'm not aware of anyone actually doing it seriously.
Another solution is http://weboob.org
Do you mind sharing the soccer table.code?
How do I make it an int?
For me, the advanced topics of python include understanding how and when to use metaclasses (the singleton is a great example imo), writing decorators with default arguments, understanding how all the `__methodname__` methods work together, and knowing when you can and should privatize class variables by prepending the variables names with two underscores. I've been programming python for quite a while now and these are the things that are currently pushing my boundary of knowledge. I agree with many of the comments below, but these are some programming language specific ideas that you might have wanted to hear about.
p = int(value) 
Interesting choice of fasttext vs nltk
that should probably be handled by a webserver not the flask app.
Out of curiosity, what kind of load are you going to but on the resulting application?
First, please try to post the code somewhere like [bpaste](https://bpaste.net/) so that we can copy and paste it. Second, if you are new to Python, I would advise you use Python 3. Python 2 is a legacy version. Third, you should avoid using "magic numbers" like 1, 2, or 3 to represent what you want. Python can handle strings well, so just ask them to type "rock", "paper", or "scissors". Like so: while True: p = input("Please type rock, paper, or scissors:\n").strip() if p in ["rock", "paper", "scissors"]: break else: print ("You entered", p, "which is not rock, paper, or scissors. Try Again.") Fourth, you can use random.choice to select from a list: c = random.choice(["rock", "paper", "scissors"]) Fifth, you can simplify the large set of if/elif statements with better logic. After checking for a tie, you then only need to check for win conditions. If it finds none, then you know it is a loss: print (p, "vs.", c) if p == c: print ("It's a tie!") elif p == "rock" and c == "scissors": print ("You win!") elif p == "paper" and c == "rock": print ("You win!") elif p == "scissors" and c == "paper": print ("You win!") else: print ("You lose!")
Pretty much, but with the correct brackets, so: p = int(input(":")) 
&gt; Our "staging" is closer to a dev environment. Why? Staging should be a metal on metal clone of production as far as is possible... The dev env should be one's own computer.
Quite brilliant, I suggest the author get a PHd for this write up, assuming of course that such qualifications are given out for stating the blatantly obvious.
the left column is not right-aligned though, but at the end of the day it's subjective.
I made a messenger bot that continually posts the current artist's wiki page and album cover for music playing on my favorite radio station. 
&gt; Yeah, but my point is most programmers don't need the headers Surely all programmers but no users need the headers? Then again being on Windows I don't bother with this nonsense about compiling code, I just go here [Unofficial Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
Most of my "everyday" stuff is very work specific. I often focus on taking our repetitive, error prone tasks and making a script to do the work for us with simple inputs. I've been using Click lately to make super simple command line scripts. Besides work stuff that would be boring to most: * I wrote a Slackbot that posts facts about Sharks on request. * I scraped a site of all of the Star Trek: TNG episode scripts, separated out the lines into dialog of each primary character, then setup Markov chains for each character. We now use TNGBot in slack when bored to talk to Captain Picard. * Every day, we have certain tasks that are assigned to specific people that others need to know about. We setup a Google spreadsheet that tracks this daily but still had to post it to slack each morning because coworkers are too lazy to go check. Now, connbot (as in "You have the conn") takes care of that for us every day and allows us to dynamically override the schedule if someone is out * I added a custom Slack Emoji for each employee based on our employee photos * I "Partified" all of our custom slack emojis (2000 and counting). This makes them flash silly colors. 
Could you elaborate on the iterator issue please as this would be extremely useful for many people, specifically why can't six or other libraries support them, thanks?
[removed]
http://www.reddit.com/r/learnpython
Yes it is newer feature in Python 3.6. But, I forgot it when I've wrote that post. I'll add it ASAP. Thank you for feedback :D
I wrote a script that would log into my blackboard account for college, calculate my grade for all my classes and display it. Some of my classes have a ton of small assignments (3 or so per week, worth a few points each) so it got kinda tedious after a while if I wanted to know how I was doing, so I wrote the script. It runs in about 10-15 seconds using Selenium and BeautifulSoup. Much faster than calculating my grades by hand.
I wrote a script to download the lower 48 weather radar image every 10 minutes. Ran it several months and generated a nice video.
I was somewhat hoping there was either: 1. A pure python packager which was supplied with a set of platform-specific executable blobs. And the packager would append platformstuff+platformspython+app into a platform-specific binary. 2. A pure python packager which depended on LLVM/GCC/etc to cross-compile platform-specific binaries. Alas. While _spin up a windows vm_ works, I continue to hope for something that runs under TravisCI. Does wine?
Do you really need fancy formatting functions for that though? 
Depends on the license it's published under. Forking, without attempting to collaborate with the original author, is frowned upon and disrespectful. Ideally, you would send a pull-request, to the maintainer, giving him an improved version of his own code, that becomes 2.x AND 3.x compatible. For pure-python, this is usually pretty easy. Heck, if you pointed out the github link, somebody on here might do it for you!
The language I was using before Python has about *five* different string types if you include a character type (even more if you count pointers to null-terminated strings), the Unicode support didn't arrive until 2010, and it's still a mess (e.g. the function to convert ANSI strings to upper case is called UpperCase and the one to convert Unicode characters is called ANSIUpperCase!). Just spending a few *minutes* with the free "Dive Into Python 3" 's chapter on Unicode,and Python 3's super-simple model, gave me a much clearer understanding of Unicode and how to deal with it than I ever had before. I can't look at another language today and not ask "Why isn't its Unicode handling as simple as Python's?" 
[Here's an explanation](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/underscore-convention.md) The idea is so the user doesn't get confused when class attributes (ones that are supposed to be user-accessible) suddenly appear.
Since I got one good question from another post, I decide to express my thought here as well. ### I can't figure out why I would need this In traditional ways, we make aliases ourselves all over the Docker commands. The reason why I develop this project is to encourage people to contribute and share their ideas and thoughts into plugins, which would give `Tsaotun` ability to do those things. And, the aboved thing is just one of things that Tsaotun can achieve, you will be able to load variety of plugins in the future as well. Besides, if you are doing some projects involved running containers, `Tsaotun` has provide the higher level API for you. That is another helpful functionality. Currently, I'm moving my previous project VWGen into one of Tsaotun's plugin. Once I finish, everyone can just load the plugin and extend the power of Tsaotun.
I built a script to check if a folder and if not create it for each year and week number. Next is to build the script to look for files starting with a number (it is for invoices) and then move to that folder.
I upvoted you in silent agreement... until I realized I'm on my phone, and I don't know how to check this out on mobile.
Instead of creating a ThreadWorker class with queues, you can simply use the built in ThreadPool and pass it a function and iterator. (Will add example when off mobile) from multiprocessing.pool import ThreadPool def f(x): return x*x if __name__ == '__main__': with ThreadPool(5) as p: print(p.map(f, [1, 2, 3]))
Removing the global namespace would be a thing but how do you deal with 'defaults'? I mean, if I want to use sqlalchemy I wouldn't necessarily know if I zzeek/sqlalchemy or zzzeek/sqlalchemy is the official one. What if zzzek/sqlalchemy is a clone of the official one but it adds a backdoor to my database? Besides that, all libraries does not need to be constantly updated and six weeks is far to short to deem a package abandoned. I sometimes go for vacations longer than that. Finally, is this a problem or is it a solution looking for a problem?
Agreed on the time tables you reference, there are cases in which a library should be able to be considered "feature complete". Lack of "updates" may not mean anything. Maybe "defaults" could be like twitter's official thing (I don't know what they call it) - with a special badge for the lib.
You say "should" like it's a moral issue. If somebody particularly needs to run it in Python 3, they can worry about it then. For the present, Python 2 is widely supported and works great. And most of the runtimes other than CPython use the Python 2 dialect. So if you want to do stuff like use Google's new Go Python transpiler, you need to avoid using Python 3 exclusive features.
same with English, *under* and *stand* is two characters that are simple laid out to form *understand*. Python's ligature handling is unfortunate, modern language should aware of it, I believe Go handles it pretty well.
The purpose of the function is basically to replace a builtin function and allows for defaults. Based on that, I don't think it should be expected that the user wrap the function externally every time they want to use it to get environment variables. Now if you really want control over how it handles the exception, you could just have another parameter to determine if it raises, prints a warning, or returns None quietly. As for the issue of type, I guess I didn't look closely enough that he wanted to send functions as well. Based on that, it should look something like: if callable(typecast_fn): I'll change my post.
I found the Google Dev's quick six-video intro into ML good for an introduction into the concept as me being a complete newbie to it: https://www.youtube.com/playlist?list=PLpH1NIoV8S8cxpmPlxVRr-RvnR2MBZ5gY
&gt;Especially, the i18n/l10n is very newer to me. I think you meant to write 'is very new to me'. Thanks for the article!
How quickly can it finish a match?
Create the environment with this: conda create -n py36 python=3.6 This creates an environment called "py36" (you could call it whatever you want after the -n). The activate it for a given terminal session by doing: source activate py36
How large is the file when you were done with everything?
The shark one takes the cake. 
But once you catch the exception, what do you suggest the function should do with it?
For a second I was hopefully that this was actually an SVM implementation in Python, which would be an interesting thing to see. Does this post expect the reader to already have a strong understanding of Support Vector Machines, or is it just treating them as a black box?
That would be awesome :)
In my case, I would generally return None (as though the variable wasn't set), though I would also print a warning (to screen or logger). But like I said, you could just have an argument like raise_on_except=False that would raise if set to True when calling the function.
Look into Heroku, or Digital Ocean if you want to get your hands dirty with the setup. Heroku is free, DO is $5/month for the basic plan. 
For seeing it in your phone you can use nbviewer http://nbviewer.jupyter.org/github/miguelgfierro/sciblog_support/blob/master/Intro_to_NLP_with_fastText/Intro_to_NLP.ipynb
Yeah man. You'll have to edit it a bit though because it goes straight to my school's blackboard site (which is actually called BeachBoard, which I'm assuming is the same thing as BlackBoard but named that way because of my school...). I'll pm it to you.
About ten million small-ish files, IIRC. It took 36 hours to copy them to a new hard drive, because Windows doesn't cope well will large numbers of files.
Have you had this problem? if so could you provide a link? I've used heroku for a twitter bot also. Here's part of my logs: 2017-01-15T04:00:21 **app[api]** : Starting process 2017-01-15T04:00:24 **heroku** : Starting process 2017-01-15T04:00:24 **heroku** : State changed from starting to up It appears my api ran for around 3 minutes without issue. @-Canonical- perhaps you could buy a mini pc and run your script in house. edit: formatting 
I don't have problem with your english. 
Ohk :).By the way please consider it as simple demo for beginners.
Hi. That's a bad idea to create html markup [in this way](https://github.com/Permafacture/film_search/blob/master/film_search/templates/index.html#L34), it can lead to markup errors or even XSS attacks. A better way is to use either .textContent property, or some template engine, that will handle it.
The minus score got turned around ^^
because there is already an existing expectation of unstructured python package names to be findable in pypi; that's where `pip install` &amp; co look. i'd prefer more structured imports (eg. `import com.github.someuser.somepkg as somepkg`), but unless code that uses those libraries is to change, there needs to be a way to find out what `import sqlalchemy` means, and github won't provide flat names. if the flat names were to be fixed, github repositories could be made to work (as above), and someone else could host `import net.mysite.mypackage` locally.
Very nice notebook! I'm curious, why did you choose tSNE for the visualization over any other dimensional reduction method? (Also I love that you didn't just use the tSNE default settings!)
https://github.com/donnemartin/data-science-ipython-notebooks
That's a good question. t-SNE usually gets better results in comparison to other algorithms, let's take PCA for example. PCA uses the eigenvalues, which is like to take the direction of the vector that represents the maximum variation of the data. So you are not representing the data, just the "shape" of a portion of the data. In t-SNE there is an optimization that clusters similar groups of data, maximizes the difference between disimilar groups and then there is a projection in a lower space. I think this min-max optimization is one of the keys of its success. Here you can see the results of different algorithms: http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne or https://www.kaggle.com/puyokw/digit-recognizer/clustering-in-2-dimension-using-tsne/code 
I'll keep up to date on the progress, thanks!
"And it barely used a single wire!"
Heroku shuts down the instance if it has been inactive (no requests received) for more than 30 mins. This is why the startup is slow. However, it might be possible to keep it up by sending a stay-alive request every once in a while. This way it can respond to requests in a timely manner.
making an industrial process monitoring tool for biotech industry using grafana/graphite as a basis
Here is the [code dump](https://codedump.io/share/eJCTQlCQlwgI/1). And the [screenshot](http://i.imgur.com/U77FAj9.png) 
Thanks for the reply. I'll look into it! 
You can run multiple threads on a single core. This is incredibly useful for tasks which are more I/O than processor bound. For example, if you wrote a program to download 100 files, it would be inefficient to download them one by one in a single thread. You could instead use a number of threads to download files in parallel.
PEP8 is considered canon when it comes to code styling and best practices: https://www.python.org/dev/peps/pep-0008/ Have you read through this style guide as you've progressed as a python dev? It's helped me quite a bit in terms of writing clean(ish, er) code.
Logistic regression has a few nice properties: It scales well to both the number of features and the number samples. And given reasonably well designed input features, it is often surprisingly difficult to beat. And it is quite easy to understand and interpret. Having more than one possible outcome does not change this. Scikit-learn even has multi-class classification built into many of it's classification algorithms. I would just go and try how far logistic regression will get you. Chances are, you will soon have a result that is good enough for most practical purposes and be done. If you encounter problems, try to improve your features. If you find that there is too much correlation between your features, and you cannot get rid of it, maybe try to extract underlying structure through LDA or PCA. Or if you have plenty of input data, computing power and time go for some kind of Deep Learning. But often enough logistic regression is all you need.
Using Windows 7 Professional. It works fine on my computer. I am running Version 2 though. 
Hi, very interesting comment. When you say "...forces false relationships to appear that aren't actually in the data, or that aren't there as strongly as the visualization implies". My thought is that this might be true in the low dimensional space. So I would be cautious. If what you are looking for is a true comparison between objects, I would take a look at the comparison in the high dimensional space. In my experience Kullback-Liebler works very well. I'm not a fan of dimensionality reduction (DR), at the end you are always loosing data. I use it in visualization, because there I'm not really concerned about precision and, sometimes, when I have to optimize something huge and a smaller model could reach to a better optimization. I have a colleage at Microsoft who is an expert on Non Negative Matrix Factorization, he did his PhD on that area in recommendation models. I'm happy to talk to him about this. 
http://i.imgur.com/tGfEZmb.jpg
To prevent io locking I guess 
Looking at the code, it doesn't really offer much. A standard game loop would have been easier to read and debug, wouldn't have relied on intra-module globals and wouldn't have TOCTOU bugs like this: if game_state.queue.empty(): game_state.dm.display_matrix() else: pixels = game_state.queue.get() game_state.dm.update_binaries(pixels) edit: also, this is a really bad way to implement a fixed time step: `time.sleep(game_state.GAME_SPEED)`, as the time per iteration will vary.
Saving plain text credentials in file is definitely a bad idea. What I did before is either store them in environment variables, but theres no clean way for a Python script to set them so you'd need maybe to declare them in ~/.Bashrc or and this is my personal choice I simply encrypt the credentials using a personal passphrase then store them in file. 
You were right, case closed. I found the problem. And i feel stupid. I passed variables like this: WHERE ID= """+id+""" And this method worked for me. Except my last SQL So i searched the internet, too find best practice. And people where using this method: ("SELECT FROM * WHERE ID=:id ", id=id) And this solved my problem. But still i dont know how my first method worked on all sqls except my last one. But i changed everything to the best practice method. Thanks
I am actually doing something very similar. I am doing motion detection with a live feed. Then saving the video to be transfered to a local nas. I am trying to get it to work with a Pi. So it can be put outside in a small box. I may take a peek at you code to see for improvements :) 
The author might not be a native speaker, but I've read worse written by native speakers. He or she has nothing to be ashamed of.
Yes but I was not aware of people actually had a habit of doing that. I only do it if I have a branch that contains something _absolutely_ required that is not yet released.
Do you really think asking the user to type "scissors" is a good interface design? What do you think will happen when they invariably mispell it "scisors" or "sissors" or any other typo? Using R P S for Rock Paper Scissors keeps it nice and simple to type, simple to code, and is still more understandable than 1 2 3.
Things might have changed since we made this decision (and it might be worth revisiting) but the [explanation is covered here](http://python3porting.com/differences.html#next). IIRC our issue is that we have a code which uses cython to link C++ librarires, and it wasn't behaving well with python 2 vs. python 3 detection (it should be easy to do so, but again, it's been a while since I wrote this particular section).
/r/learnpython is the right place for such questions. /r/python is more for community news, or information relevant to the community at large. Thanks!
Perhaps, [this](https://docs.scipy.org/doc/numpy/reference/generated/numpy.packbits.html) might be of some help.
I'm guessing it was a really, really big snake.
Try `RidgeClassifier`, or `SGDClassifier` if your data is huge.
Yeah in the past I would just test with requests in python script. Trying to get the less technically inclined to add tests as well. So far the most interesting one I found and so far opted for is http://robotframework.org/. Yeah testing on client side, sort of on integration/acceptance testing side of things. Currently QA is doing PAW testing with API. Robot framework is great as it also ties into Appium for testing the iOS and Android clients.
I dislike this strongly. Everybody knows that environment variables are strings, but your magic `getenv()` function returns an int by default. If I am reading code and I see this: value = genenv('VALUE') result = value + 1 I'm going to be surprised and confused because it looks like you are taking a string (the environment variable) and adding it to the int 1. That's weird. Now I have to go and read the documentation for `getenv` to understand what it does in detail. And for what benefit? To avoid having to be explicit about wanting to convert to an int? I just looked at my system, and I have 49 environment variables. Only seven are ints, and of those, two are intended to be used as True/False bools. So I would not use this function. The number of times I need to convert an environment variable from a string to an int is very small, and when I do, it is fine to just write: value = int(os.getenv('VALUE')) 
print("Snek")
That if is in the global scope. Variables in the global scope are, you guessed it, global. Also use /r/learnpython for questions like this in the future. 
~~OS level threads != Python threads~~ ~~Python runs in a single OS level thread.~~ EDIT: I was wrong :) 
Each Python thread uses a real native OS thread. They behave as a single thread performance-wise because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) which is often the source of this confusion. However, C extensions can release the GIL while not manipulating Python objects, so if you are using multiple threads that call C extensions and the CPU bottleneck is there, it can use all cores at 100%. 
I have been working/using this library on github. It builds a python so file. Simplified, I include the shared library and the python libraries inside of my application and run it local https://github.com/rave-engine/python3-android The mainline is mostly dead, but there are 2 works that are maintained.
PANDAS might very well be overkill for you use case. It is for data science and heavy users. If you just need to import/export some data from a sheet then there are less complicated libraries.
Kivy, Kivy and Kivy. It's ridiculously powerful and cross-platform. It takes a bit of endurance in the beginning stages, but once you manage to get something working, it's like pure magic. Seeing something work on your mobile as well as your Windows and Linux machines is just an added bonus. https://kivy.org/#home There are many books on Kivy available on Amazon, there is a very helpful mailing list and of course the API reference is quite good. Try it out, and trust me, don't give up. Just keep going. It's a bit hairy at first.
Mostly for fun. Reading some responses, I probably could have done it in a single game loop. However, I thought it might be worthwhile to have separate processes that execute independently of each other (render, update &amp; user input). 
would a better way be a series of fixed time steps, based on the rendering lag? This is the first time I've really dabbled with consistent time steps.
hahahaha
/r/learnpython like the sidebar says
anyway, here's Wonderwall....
Good luck!
makes sense, thanks!
Where is it?
Yes it is! 100% FOSS: https://www.blender.org/ Even better: It works on every major desktop OS.
I asked a similar question a few years ago at Stack Overflow: http://stackoverflow.com/questions/8963612/communication-between-object-and-its-members
https://www.codecademy.com/learn/python
I'm author of this post. Sorry for my english. I'm not a native speaker :( ..
I have had a good experience with *concurrent.futures.ThreadPoolExecutor* with Python 3.5. In my code, I typically use: import concurrent.futures, threading def read_sensor_func(a1,a2,x): print(a1,a2,x) print(threading.current_thread().name) return (a1+a2) * x def main(): max_workers=4 with concurrent.futures.ThreadPoolExecutor(max_workers) as executor: result = {executor.submit(read_sensor_func,arg1,arg2,x): x for x in my_list} for future in concurrent.futures.as_completed(result): if future.done(): print(future.result()) *x* in this case could be a tuple containing sensor and port # (for example) You can change the number of concurrent threads by changing the value of *max_workers* *read_sensor_func* is a function that gets called in it's own thread and runs in parallel. It takes 3 function parameters: *arg1*, *arg2*, and *x*. When the function is completed, it can optionally return *result* https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor 
You are welcome. :)
Are you going to have to launch a shit-ton of threads? Will there be context switching issues with the OS? Do you need control over -when- your program does contexf switching? If you answered yes to one or more of these questions, asyncio might be for you! Personally (of course it still depends on the project at hand) I use asyncio for the IO "threads" and then multiprocess pools for the work.
Does the script https://github.com/arbylee/python-warrior/blob/master/bin/pythonwarrior exist inside `~/.local/bin/`? Is that directory in your `PATH`? See http://askubuntu.com/questions/625354/installed-script-with-pip-run-like-a-command
Thanks for the suggestion, I'll be taking a look at it. And yes, I'll be taking this to r/learnpython next time! I was unaware of it
I used datacamp. They start you off really slow and easy and they will show you the answers if you can't figure it out. I've completed intro and intermediate tutorials/classes. Good fundamental stuff. It can be a little boring because its so simple, but Id rather start simple and understand everything then be thrown into the deep end all at once (Like R Programming on Coursera-do not take.)
Also, I don't think Django sessions would work under Zappa, though I'm not entirely sure.
You misspelt length and height. You might also want integer division // instead of ordinary division /.
I wrote a few simple scripts I use everyday. They are both Bash and Python and can be found [HERE](https://github.com/Jwink3101/miscellaneous_scripts) Some of the key python ones are: * A timer/alarm clock. Works with subshells and a final check so you can also manage them after launching * a hacky markdown watch/compile/open. It isn't perfect and uses a simple file system poll, but it works * Tool to present the git provenance of a file in a git repo * tool for when I forget all of the flags for `grep`. Also does booleans better * Tool to make markdown tables from delimitated data Also, less simple but I extensively use the following I wrote: * [`PBrsync`](https://github.com/Jwink3101/PBrsync) -- Python based rsync wrapper for bi-directional sync and management * [`remote_edit`](https://github.com/Jwink3101/remote_edit) (also has a bash element). Tool to edit a file remotely and auto-upload upon modification. 
After working with and contributing to Sanic this is definitely the case. There are error-cases where it was decided to fail hard rather than check because of the loss in throughput. I don't think this is a bad thing, just the reality of the project.
I love digital ocean but recently I'm trying scaleway too. Can't say better, but definitely nice. 
I agree, but as someone who's usually writing in a locale that doesn't translate properly into Ascii, I generally loathe working on python2 code where there have been American or British developers involved. It just doesn't work most of the time.
I used bokeh, is very extensible and easy to integrate. You should try it before doing anything crazy :) 
How are you doing the comparisons? I imagine if you have to unpack -&gt; compare things will slow down, but there's probably a way to arrange things to do an xor or equivalent to get the same effective outcome in one shot.
I had some trouble finding an exact answer to that online, sorry. The amount of threads that can be handled is typically dependent more on the system requirements of each thread. Limiting your threads to just I/O operations could allow the pi to run MANY threads (on some systems possibly thousands). However, if you tried to use each thread to compress separate files, you might be able to use more threads but would gain little from doing so. You might be able to determine a hard cap for the raspberry pi, by trying to spin up a higher number of low-intensity threads until the system complains. However, you should view the number of threads you use in applications as a configurable parameter. In a lot of situations, it's hard to predict the optimum number of threads to use before diminishing returns kick in, so choosing a good number of threads can sometimes be a process of trial and error. Edit: oneonetwooneonetwo has a more direct answer to the question, as follows: cat /proc/sys/kernel/threads-max 13657 What I said above still holds though. In many applications, you would receive diminishing returns long before approaching the max thread limit of the raspberry pi. On much more powerful machines, I don't often have applications which can use a fraction of that many threads. Knowing the number of threads used on an application-by-application and system-by-system basis takes experimentation. 
The simplest way is to add Placement into the token constructor. But that causes each token an ability to affect its creator. That dependency is really nasty. From a functional standpoint, you could ask yourself if it truly makes sense for instances created by Placement to "know" about the parent. The object that holds the instantiated Placement can hold both Token instances and the Placement instance. Example placement = Placement() tokens = placement.BuildTokens() //list token = Token.DoFunction(tokens[0], placement) The responsibility for connecting the tokens and the placement is now up to this Controller class. Note that the objects "live" in tokens, so I'm using a static Token method to do whatever work needs to be done by bringing in both the token to use (token[0]) and the placement. 
sqlite is serverless... Granted, a *really* busy blog might have issues using sqlite. Also, a large number of entries might be problematic. But maybe old/unpopular posts can be archived to S3 or something.
It's also an academic issue though. If I need to use the Python3.x code and I do all the work to convert it, who gets credit? 
I feel like you can use select() and make it all single threaded and lose fewer cycles to context switching.
There are lots of good articles out there on game loops, e.g.: 1. http://gafferongames.com/game-physics/fix-your-timestep/ 2. http://gameprogrammingpatterns.com/game-loop.html (from /u/munificent ) Have a read :) But the gist of it are you can either do fixed time steps and sleep for any remaining time, and hope your rendering and simulation don't break their budgets, or have them separated and adapt each frame, as in those articles. You rarely see rendering and simulation on separate threads outside of the major engines, as there's all sorts of trouble between updating something from your simulation thread and and reading it from your rendering one. It's very hard to do correctly, and even if you did you wouldn't find much gain on e.g. D3D11 or OpenGL4.
Just use asyncio
The original author gets credit for their work. You would get credit for your work. It's not a particularly interesting question. If the biggest result of your research is that you got some library you used to run on a slightly different version of the runtime, maybe you had a bad plan to begin with.
Use pytest it's way easier 
Or even cooler: write a c extension which runs python and releases master thread gil. Wait that sounds like multiprocessing
This
Your urllib code looks a bit odd and please don't parse html with regex. Do you got pip? Maybe use [requests](http://docs.python-requests.org/en/master/) to download some html and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse and navigate it. First examples of each should get you started. Easy mode, cheers. If you want to use standard lib then see /r/learnpython
You seem to have forgotten to add `/api/` in your example API usage link (https://pylyricfetcher.herokuapp.com/lyrics/lyricswikia/eminem/berzerk gives a 404). Also, it's _fast_!
Is there a PEP for thin space separators?
Alright, so first off you'll have better luck getting help if you format your markdown correctly. (just indent the code portion 4 characters) Secondly, what did you expect `export PATH=~/anaconda2/bin:$PATH` won't work since you're on windows; however, it's clearly unnecessary... because... Thirdly, the error isn't that it can't find python, it finds python just fine (so you don't need to edit your PATH), it's that it can't find test_printhello.py. What does `dir` say? is there a test_printhello.py in `C:\Users\sxw20`? Finally, at the end you try to launch your jupyter notebook; however, you're inside a python interpreter. If you exit the python interpreter (with `exit()`), the `jupyter notebook` command should work. Hopefully that puts you on the right path. Footnote: the windows equivelant of the *nix command you tried to run is `set PATH=%USERPROFILE%\anaconda2;%PATH%` 
I would have also converted your game state module into a class so you don't have to constantly modify global state.
No problem. Don't forget that the structural changes like splitting into functions and not modifying inputs is transferable to other languages too.
I like to read Kenneth Reitz' code for education. There's a line in the setup.py: with codecs.open(os.path.join(here, 'README.rst'), encoding='utf-8') as f: long_description = '\n' + f.read() I'm wondering, why is he using codecs.open(), instead of just open()?
You'll have a similar problem in pytest, the test will abort after the first failed assertion. Though the output is captured and all the failed tests are put at the bottom of the results, generally making things better.
`rsync`
Break it into chunks of 8 (e.g. 11000110), store that as an integer, and when it's time to access them, you can use bitwise operations (particularly bit shifting) to restore what you previously had. This is equivalent to rewriting the number in base 2.
Correct. It's also a required change to work on Windows -- I submitted a PR not too long ago for this exact thing. [Fix setup.py README read to work on Windows #9](https://github.com/divijbindlish/parse-torrent-name/pull/9/commits/26988d849f9436c3dda0932f6c60356da3ffbf69)
Thanks, this means a lot. I come from a node js background , so I guess I am getting good at python :-)
Nice... saved a 15 dollars! Now its time to master python before I start my graduate degree. Thanks mucho!
Looks cool. Tried it out on Mac OS Sierra. Didn't need to do anything about the 32-bit, so that was easy :) Have you taken a look at Pypy? It could potentially increase the performance dramatically. I tried starting it with Pypy, but no window came up. Mouse got captured, and no exceptions. So it's probably some minor detail, as Pyglet should otherwise work http://pypy.org/compat.html
It is a bit tacky but I have no problem with it as it seems to be Kenneth Reitz' original cringey slogan. The trend that it started in naming is unbearable however.
I'm pretty new to writing tests, but my understanding of it is this: you have one only method in your Testcase, so therefore only one test to run - despite that one test evaluating many assertions. If you want to test each assertion separately you will need to put each one in its own method. Also, rather than testing essentially the same thing three times, you might do better to test different behaviours. For example here you're only testing whether the code does what it is supposed to do. You could also check that it *doesn't* do what it's *not* supposed to with e.g. `self.assertFalse('foo'.upper() == 'foo')`.
You get pytest fixtures though 
Thanks! The `~./local/bin` was missing from my `PATH`. After I appended it everything was fine.
If it's really critical you could just write it in c and write bindings.
[removed]
A failed assertion will always bail out of a test. I suppose you could come up with some sort of convoluted `bundle_asserts` function that would take in sequences with the first item being the assert method and the rest are arguments to it, catch any assertion errors and raise a bundled one if it caught any 
[removed]
I have a class that makes bits of int accessible as an array. I also long time ago tested bitfiddling in pure python vs a C extension... python is undistinguishibly fast between in both. Here is a bitarray (a view on an int as a 1D array) https://github.com/jul/game_of_life/blob/master/gof/weird_array.py#L24 And here is the matrix class a 2D array view using get(x,y) set(x,y,value) on any array (mutable sequence). Abstract base class are amazing. https://github.com/jul/game_of_life/blob/master/gof/matrix.py Have fun, it was 1 day project to how how arrays were be taught wrong as an implementation whereas it is an abstraction and working on 1D array is better. I leave you has an exercise to make an abstraction that make a [i][j]...[x] abstract constructor based on int that make it easy and keep the [] notation while accessing bits this way. I did not make it for I don't like the loss of performance related to it, and wanted to use the &amp;| ^ operator to have a kind of "paralallelisation". 
The source and some usage examples can be found here: https://github.com/CodeReclaimers/neat-python The documentation (such as it is) is available here: http://neat-python.readthedocs.io/en/latest/ Please file an issue on GitHub if you encounter any difficulties or errors in the documentation or code.
Thanks for sharing. I recently made a simple blackjack game as a discord bot. I like how you decided to break this up into different classes. It really gives a sense of organization to the whole thing. I was wondering in your debug class, where you are checking for the input to be a number, why you didnt use str.isdigit ()?
I wanted to install a latest Django version to my project, but was blocked by a couple of third party apps that were not updated to support the latest Django yet. So I needed a way to get notification when those apps are updated. And that's how this service idea was born. It is pretty simple, you need to just type package name and it will generate an RSS link to subscribe for its updates. If you have any feedback or feature requests, just drop them in this topic. Thanks!
The data is used as inputs to a neural network, so I have to unpack it before sending it to the network. There is so much data that I can't fit it in my 8 GB of RAM which is the reason I need to pack it. I measured the times and the packing is quick but the unpacking takes much longer for some reason. 
&gt; 13657 That seems like a very arbitrary number.
The easiest way for you to do this is figure out what the JavaScript does (usually fetches data from another URL), and do the same thing with Python code. The requests library can handle any Ajax calls the code might do, and the debug tools in chrome/firefox can help you figure out *what* the site is doing.
Pretty cool. Now to take it to a crazy level that could probably land you a job at google. It then searches for the song on YouTube, and finds the music video. Using voice recognition it syncs the lyrics to the video. Basically an automated subtitle creator.! It seems like a good idea. We have subtitles for every song and we have the music video or at least an MP3. If you could merge the two automatically it would be very valuable for karaoke, deaf people, etc.
Python 3 is your friend (it added subTest): import unittest class TestStringMethods(unittest.TestCase): def test_upper(self): tests = [ ('foo', 'FOO'), ('too', 'TOO'), ('poo', 'POO'), ] for value, expected in tests: with self.subTest(value=value): self.assertEqual(value.upper(), expected) if __name__ == '__main__': unittest.main() 
[removed]
Fun with bit twiddling. Probably not valid for alphabets beyond ASCII. import itertools x = input() output = [ ''.join(chr(ord(c) ^ b) for (c, b) in zip(x, adj)) for adj in itertools.product([0, ord('a') - ord('A')], repeat=len(x)) ]
I looked at this when creating my first skill, but it was too much of a hassle to learn flask, jinja, and everything else just to write a skill. I ended up settling on Alexandra after toying around with a couple of different libraries. However, if you already know flask, this is probably the best route to go.
Another fun variation. This probably works with whatever alphabet you want. x = input() output = [ ''.join(pair[bool(variant &amp; (1 &lt;&lt; bit))] for bit, pair in enumerate(zip(x.lower(), x.upper()))) for variant in range(2**len(x)) ]
Hi @konrain, I'm the co-founder of Educative. Try the interactive course. If you don't like it, you can return the course within 30 days. Send us an email at returns@educative.io. 
Thanks. I made this 5-min video that shows how to get up and running quickly. While learning Flask and Jinja obviously can't be done in that short a span of time, the code sample should be intuitive enough to get started and make changes https://www.youtube.com/watch?v=cXL8FDUag-s
thanks, now I know..
will do, can I have a coupon for the coderust? :)
I guess, with your proposed system, you'll go by a sort of "infinite little endian addressing". What I mean is the we usually count the bits from the least significant being the right-most bit, and most significant the left-most, so for a byte we get: | b7 | b6 | b5 | b4 | b3 | b2 | b1 | b0 | 1 byte | But for 2 bytes we'd get: | b7 | b6 | b5 | b4 | b3 | b2 | b1 | b0 | b15| b14| b13| b12| b11| b10| b9 | b8 | 1st byte | 2nd byte And so on... Not bad per se, but something that has to be taken into consideration. Also, you said: &gt; If you want to flip the 5th bit, you have to read the entire byte, do some math to get that 5th bit, flip it, then do some more math to figure out the right byte to write back to that address. Which is actually pretty easy: pos = 5 mask = 1 &lt;&lt; (pos - 1) val = array[pos] array[pos] = (val &amp; ~mask)|((val ^ mask) &amp; mask) Or in the form of a function: def bitflip(val, pos): mask = 1 &lt;&lt; (pos - 1) return (val &amp; ~mask) | ((val ^ mask) &amp; mask) But of course, that'd make a lot more sense as C/Cython code. Edit: I hate non-monospace fonts for aligning stuff...
Thanks for the reply. I tried to make it feel pretty organized and have the classes each represent a game piece. The debug class was something I thought was a good idea initially but I ended up not using much. I think to use that method, but I'll probably add it in when I go back to rewrite it a bit. Is the source code for your bot publicly available? I'd like to see how it compares.
Try caging the script until you figure out what the error is. I once had an issue with Gfycat and my script would crash since I didn't implement error handling, like if a gif wasn't on the site anymore or got deleted. I caged the entire script with a catch throw and it just ignores the error and moves on to the next item. It's a quick fix but I wouldn't stick with it forever. 
This is very detailed and helpful, Thank you very much :D
Hats off to you. This is really cool. Worked on my mac.
'assertEqual' takes a 'msg' parameter so it's easy to have output that shows the expected and actual values. I'd combine this with the code given by [ojii](https://www.reddit.com/user/ojii) or something similar.
it's twitter that only allows a certain amount of requests per hour.. 
The readme needs details on version(s) of Python are required to support this, and all of its imports as well. I looked at it, thought "what the heck is all this stuff?", I had _no_ idea if I could make all that work in my environment, and wandered away. It's only easy if it _is_ easy.
The audio won't turn off after I close the thing. Help?
I'd say you deserve a shout out by the original author if you take the time to convert the code and fix all the bugs, but the code in general belongs to the creator and not the translator. 
&gt;Heck, if you pointed out the github link, somebody on here might do it for you! I like this idea. I would gladly do such work for existing github projects just to kill time. Having somewhere the community could aggregate links to projects needing such conversion work done would be a helpful resource for other people who also feel like tackling some of this busy work.
I was using it under Ubuntu. When I was pressing ctrl+D (instead of alt+3 which was used for Python 3.5) it deleted one character from the line succeeding the commented out block. Now when I installed Python 3.6 under Windows 10, the shortcut is alt+3 again and the problem is not present.
Looks quite neat but I don't use Flask so I can't really use it. I wish more libs had the same approach as [Hyper-h2](https://python-hyper.org/projects/h2/en/stable/) these days, make the core framework neutral but easy to integrate with.
Thank you this seems to be exactly what I need.
I want to run the JS it was stupid of me to write parse.
Interesting. In IDLE, can't you override the keyboard shortcuts? On mine, I see options to choose "key set" (mac, unix, windows). Maybe fiddling with those can fix you up? I rely on IDLE and my keyboard shortcuts so I feel your frustration. p.s. Maybe help&gt;about&gt;email dev?
No, there's no reason to use Flask. appcontext magic, non-async friendly, Sanic should totally avoid the design of Flask.
Yeah that's a solid point, not for me though since I've already moved to 3.6 with all my projects
Our team spends lot of time to find a way collaborate among data scientists and engineers and want to share the framework we use. It is useful especially for document parsing when document is uploaded one by one.
Hey guys, Our team spent lots of time finding a way to collaborate among data scientists and engineers. It is used for document processing with a web interface. We want to share the framework we use and welcome to comment.
That's allright, in just curious to your approach :) Thanks for sharing!
Love the simplicity of the code. I was surprised at the basic data structure for the world, it's a dictionary of tuples of (x,y,z) against the block texture. I assumed a game like this would use a 3d matrix for the world but this is probably much more efficient as most cells are empty.
Decompose the problem. The task is "Sort the words in a text file in alphabetical order, ignoring blank lines". I'm assuming there's one word per line, correct me if that's wrong. The first step at decomposing is to apply the input / logic / output principle, i.e., separating out these three concerns. So we need: 1. Something that takes a filename, reads the file under that file name, and gives us a list of lines. 2. Something that removes empty lines. 3. Something that sorts a list of words. 4. Something that takes a list of words and writes it to a file. Part 3 is relatively straightforward, you just use the built-in `sort` function. Part 4 isn't too difficult either: you open a file in write mode, loop over the list of words, print each one to the file, and finally close the file. If you use `with`, closing the file is even taken care of automatically in Python. For part 1, it is helpful to know that file objects have a method called `readline`, which does exactly that: read one line. For part 2, all you need to do is take the list of lines and filter it such that only non-empty lines are retained. This we can decompose further: We will need a) a function ("predicate") that tells us whether a given line is non-empty, and b) a function that takes a list and a predicate and removes everything from the list that doesn't meet the predicate. For b), the built-in `filter` function works perfectly; for a), we need to decide what "empty" means to us. A naive approach is to compare againt the string `"\n"`, which works, but will not remove lines with just whitespace in them, nor will it remove the last line in a file if that line doesn't contain a newline, plus it doesn't deal with different line ending conventions (Windows vs. Unix). We can solve this brutely but effectively by calling `strip` on the string and comparing the result against an empty string: newline and carriage return are also whitespace and will thus be stripped off. At this point, we should be able to implement each of the 4 steps above as a function, and then put them together by "chaining" them, either as nested function calls, or by storing the output of each in an intermediate variable and then passing that to the next function, e.g.: raw_lines = read_input("input.txt") cleaned_lines = remove_blanks(raw_lines) sorted_lines = sort_lines(cleaned_lines) write_output(sorted_lines) Does that make any sense?
/r/learnpython
Pyglet works on Linux, OSX and Windows
Hey redditors im in a pickle with a similar task and need some help with the basic principle. Could anyone possibly help/show me (with) the code. Thanks if you can help me sorry to be a pain!
So you want to do some Asynchronous Input/Output? Or 'AsyncIO' if you will? Or even `asyncio` if you don't like using caps I suppose. https://docs.python.org/3/library/asyncio.html
[list_place-1] is the same thing as [-1] in you case. You can delete list_place var and change last command: os.startfile(time_sorted_list[-1]) #open file If you name your files with the ".pyw" extension, then windows will execute them with the pythonw.exe interpreter. This will not open the console for running your script. [From](http://stackoverflow.com/questions/1689015/run-python-script-without-windows-console-appearing) full_list is a list of full paths to CATProduct files like ['C:\Folder\Path\Here\file1.CATProduct', 'C:\Folder\Path\Here\file2.CATProduct', etc]. It used for sorting in the next line. The last var in sorted list is newest CATProduct file. You can call it by [-1] index as I wrote above.
Thanks for pointing
Great response - I did not know about the [-1] shortcut for arrays and the clarification on the full_list line makes sense. Thanks! Edit: ".pyw" extension worked like a charm, too. Thanks again!
Why though? I that that python 3.6 was backward compatible with 3.5? Can't distributions recompile bytecode and use the latest version?
Launch as a monolithic project. Have you ever heard about 'shipping your org chart?' where people tend to break down a project into as many divisions as their company has? Well your company has just 1 person---you---so ship your org chart and ship a monolith. 
None. Except it's been tested for years and sanic just came out. And that Flask author, armin, is one of the most well known dev in the Python community (jinja2, Werkzeug, babel, itsdangerous, MarkupSafe, click) while sanic author has to proves everything. And the fact that Flask has more features. And unit tests. And a better doc. And you can deploy it in any wsgi compat host. And that you probably don't need async for your task. But otherwise, nothing.
Read the sidebar please. 
Thank you!
This is really interesting. I'm working on a very basic game myself and I'm a little bit lost as to how "worlds" should be mapped, so it's great to see an example. At the moment I'm using a list of lists, and using each object's position within the lists to determine x y coordinates, but I'm not sure if this is the best way to do it. Do you know of any resources I might use to get a better grasp on this? Specifically I'm having trouble figuring out a way to put both an enemy and a map tile on the same position of the map. 
&gt; And that you probably don't need async for your task. Unless you like expensive cores sitting around hogging all the memory while we wait for IO to come back. I recommend you do few years of Node.js, it makes classic synchronous python not so funny anymore.
"Make it work" is not an optimization. 
Never thought of this in Python, great idea.
&gt; With only one processor you can't have true parallelism Er, that's a lie. With only one CORE you can't have true parallelism, however almost all processors that are being used today are multi core and can do true parallelism.
Thank you SO much for that awesomely detailed reply. It's exactly what I wanted! Have a fantastic day! 
Just automate something. From what you say, you have a decent+ understanding of the language, now what you need is to make a project. You can take this opportunity to expand your knowledge to other things, like `git`, and design principles, and the "pythonic" way of doing things, etc etc etc, the possibilities are endless.
Haha no worries, it was informative regardless! I think I'm gonna stick with Flask for now, at least until more information and features come out of Sanic
Its really not that bad. You don't need jinja for this. Plus you can easily deploy with zappa. https://github.com/postelrich/alexa-skills
This is true, I'll change this.
Another problem solved thanks to... teamwork!
If you PM me, I can get you a free copy of my video course that's based on Python 101. 
The book was designed as a quick way to get into Python. One of my many book ideas is to do an in-depth Python beginner's book.
Fair enough, except the OP said they only have a single core processor.
Completing [puzzlOR](http://www.puzzlor.com/About.html) 
As far as I remember selenium can do it. 
Good
No, not at all. You're not implementing a JavaScript interpreter. The JavaScript interpretation will be done once, by you. The Python script will simply do the same things with the server that the JavaScript did. It's very likely to be quite simple.
Try reading python documentation (https://docs.python.org/2/)
Sanic doesn't have unit tests?
Actually I am having Educative do the same treatment to Python 201 too. It's just not ready yet.
You don't need to know really much at all of Flask, and you do not need *any* jinja at all. The jinja templating is just there for if you happen to make a huge app, otherwise you really don't need it. You need to know about 1% of Flask. The only major thing is the routing, the rest of the flask extension occurs in the background for a basic app. You'll need some form of routing around apps anyway, so you wont save time anywhere else really. 
Setting up a cluster of Ubuntu VMs that we'll use to track logs in mongodb from a bunch of different apps through our Flask API using pymongo. It's my first foray into NoSQL and the first time in the last 7 years that I've used Linux, and I could not be more excited.
Working on a personal project (that I've been working for like 10 years). It's a reliability, availability, maintainability (RAM) analysis program. It's grown to the point that it's tight to maintain by myself. Think I might open source it and see if I can generate some interest.
&gt; Keras is also a lot more accessible to the masses than TensorFlow Yeah, I agree. I am not against its existence, but I think that the current TensorFlow is more useful in research. But I would prefer two parallel libraries "high-level TensorFlow (e.g., Keras)" and "low-level TensorFlow (e.g., the current one)" instead of mixing them together.
There are still bugs to fix in 3.5, even though 3.6 is out. There are multiple active release branches. 
This is exciting news. Does Travis' shift in responsibilities portend a further push for new research projects for data science coming out of Continuum?
I'm one of the co-founders at Educative. As Mike said, we are working with him on Python 201. Good news is that we expect the interactive course based on Python 201 to be ready within 2-3 week (may be sooner).
&gt; vis' shift in responsibilities portend a further push for new research projects for data science coming out of Contin I second this question. 
But then you need to have a listening server anyways.
You should look at compressed arrays, like the zarr package (which uses blosc ). If you use bitshuffling with something like the zstandard codec at compression level 1 the data will be very well compressed and it will be much faster than packing bits.
I have a fair bit of experience at two medium-sized companies, one of which used a monolithic architecture and the other more of a microservices architecture. Both websites have on the order of 1M pageviews per day, so modest but not massive. The microservices code was supposed to be for "scalability", so you could scale out individual services as needed. The thing is, it's very rarely needed, and the microservices architecture was simply much harder to maintain, and it's much harder to track down errors. And there are a whole host of communication and inter-service errors you simply don't have with a monolithic architecture. Definitely for a small side project go for monolithic every time (easier to write, maintain, and keep running). I personally would only go to a microservices architecture if I was at crazy traffic scale (Google or Facebook). Even StackOverflow / StackExchange, which is pretty large scale in terms of traffic, [scaled out with basically a monolithic architecture](https://www.infoq.com/news/2015/06/scaling-stack-overflow), with the exception of a few services like tagging that were separated out when they need it. I think this is a good approach: do what you need now (monolithic), separate out if you ever need it (you won't :-).
Yeah. Been waiting for this. Anyone know when it will come to anaconda?
Nice post! You really explain everything in detail, from end to end. I also like that you included a "A few rules on scraping" section. So many people just ignore (or are not aware of) those rules these days.
You betcha! :-)
Cool project. Something I don't understand...don't you actually record both the input (sine sweep) and the output (church response) together? Is that why you put some distance between the speaker and the microphone? 
Oh wow, I love that barcharts now align to the center automatically. One fewer keyword parameter to explain to my students!
Does the API still have hundreds of getters and setters that should really be properties?
How can I switch from "normal" anaconda to conda forge?
3.5.3 is a bug fix release. 3.4 only gets security fixes now which means source code only, no binary installers.
Almost certainly within a month. The last major IPython update had more breaking changes and it was out within 2 weeks. 
I'm happy as long as they fixed the bugs in Spyder from last fall. A number of little headaches there...
Working on a little research project to see how Swift code uses keyword arguments. They are mandatory in Swift so I am looking to see how often you get code like foo(bar=bar, bar=baz). I'd like to check the same with Python but it's more of a pain since I also want to find stuff like def foo(a, b, c): pass a = 1 b = 2 c = 3 foo(a, b, c) Which means I have to remember the declarations of functions :(
Does Matplotlib have any serious competitors in the Python world that don't use Matplotlib as a backend?
I think Travis has some blog posts in the works... I don't want to steal his thunder, but I believe they will relate to blaze, to datashape, and to the concept of an integrated (and multi-lingual) data-fabric for distributed computations, and "moving code to data".
+ I will look into the blocking api requests, I need to understand how those work + Ran flake8 and corrected a lot of errors, thank you for the suggestion! + Changed .gitignore + I am trying to understand the adapter class concept, Can you point me to some Useful resource + Will work on the command line application shortly + Have No idea how continuous integration works, just started learning how to write tests in python (Any good resources would be greatly appreciated ) Thanks for the fantastic Comment, learnt a lot! Thank You!!
Talented Python Django Developer Edinburgh UK at Mercurytide Are you looking for a new challenge? Agency life getting you down? Fed up of office politics and out of touch managers? Tired of being married to your desk? Mercurytide is different! https://www.djangojobs.net/jobs/666/talented-python-django-developer-edinburgh-uk-mercurytide/
For matplotlib, its mostly about inconsistencies like the difference between plt.xlim((min, max)) and ax.set_xlim((min, max)) which could be better implemented as properties ax.xlim = (min, max) 
Not really IMHO, at least not if you are talking about publication-quality plots. There's some interesting efforts around interactive plotting like Plotly and Bokeh, but I've found them relatively immature (you quickly find issues when you move beyond the demo plots) and generally not great if you need detailed control of style and output formats for print.
Hey, thanks for your response! This is really helpful stuff. Do you mind if I ask you a couple of questions about dynamic objects (enemies) in python? I'm learning on my own and haven't really had a chance to bounce my basic questions off anyone... Totally fine if you don't have time. :)
In production, you don't just upgrade because the numbers are bigger.
Tbh I would rather have setters because their setters usually have side effects. Matplotlib is unfortunately a very stateful library.
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
Hi there, from the /r/Python mods. We have removed this post as it is not suited to the /r/Python subreddit proper, however it should be very appropriate for our sister subreddit /r/LearnPython. We **highly encourage** you to re-submit your post over on there. The reason for the removal is that /r/Python is more-so dedicated to discussion of Python news, projects, uses and debates. It is not designed to act as Q&amp;A or FAQ board. The regular community can get disenchanted with seeing the 'same, repetitive newbie' questions repeated on the sub, so you may not get the best responses over here. However, on /r/LearnPython the community is actively expecting questions from new members, and are looking to help. You can expect far more understanding, encouraging and insightful responses over there. Whatever your question happens to be getting help with Python, you should get good answers. If you have a question to do with homework or an assignment of any kind, please make sure to read their sidebar rules **before** submitting your post. If you have any questions or doubts, feel free to reply or send a modmail to us with your concerns. Warm regards, and best of luck with your Pythoneering!
An interpreter for a language I designed. Going pretty well which is good, may port it to c++ for faster processing. 
&gt; Unlike current synchronous code where anything can do any IO it pleases whenever it pleases. what's wrong with doing IO when needed? &gt; Take a look at most Flask apps (or honestly, webapps in general), they're a mess of IO (and in general). any examples?
They have two different interfaces, one being a more or less duplicate of the original MATLAB API intended to help MATLAB users migrate, and the other is an OOP API which is more featureful and flexible, but doesn't get nearly enough attention.
I know this was a tutorial, but you wrote this: &gt; For this specific dataset, we wouldn’t need to build a scraper to pull the data. The way it’s laid out, we could easily copy and paste it into an Excel spreadsheet. And then proceeded to scrape that exact site, which kind of confused me. As I was reading it I initially thought "oh ok, he's then going to show a more complicated site where you have to get further info from a different url or something." 
The Bokeh protocol is a declarative one, based on dicts. You could certainly use that directly, but it would be awfully tedious. Perhaps slightly more reasonable, the bokeh.models API is the low level "building" block API. Every model has a 1-1 correspondence to one of those declarative dicts, and you just assembled them so that they get serialized correctly together as a unit. The other APIs (e.g. bokeh.plotting) are really just convenience APIs on top of bokeh.models, that "put things together automatically" in some ways. Some folks like that, some folks don't. 
How does Keras' API compare to TF's scikit learn API? (Scikit Flow)
You could use some python to do this. I think receiving the text through twilio, firing off a webhook to a webservice running python that sends another message through twilio. 
Neither API's great; The OOP API's verbose and requires boilerplate. The MPL API's simpler, but limited.
Thank you for your services
Great job! Defaulting to a better colormap should singlehandedly prevent some misinterpretation of data in the coming years.
Even with an OOP API, functions with names like `set_...` are often bad form in Python since it's much nicer to use a @property to define getters and setters. 
Is ICMP (ping) blocked? I've had this issue in a corporate environment and it wasn't letting me use apt.
You need to add a space in the delimiter. Something like this: csv.writer(csvfile, delimiter=', ') 
For those of you who don’t know, Pendulum is a library for Python to ease datetimes, timedeltas and timezones manipulation. It has come a long way since the early releases and has now reached its first stable version. It provides objects and classes that directly inherits from the standard library ones so you can use them transparently (exceptions exist, see https://github.com/sdispater/pendulum#limitations) Basically, the Pendulum class is a replacement for the native datetime one with some useful and intuitive methods, the Interval class is intended to be a better timedelta class and the Period class is a datetime-aware timedelta. It also provides Date and Time classes (introduced in version 0.7.0). Timezones are also easier to deal with: Pendulum will automatically normalize your datetime to handle DST transitions for you. import pendulum pendulum.create(2013, 3, 31, 2, 30, 0, 0, 'Europe/Paris’) # 2:30 for the 31th of March 2013 does not exist # so pendulum will return the actual time which is 3:30+02:00 '2013-03-31T03:30:00+02:00’ dt = pendulum.create(2013, 3, 31, 1, 59, 59, 999999, 'Europe/Paris’) '2013-03-31T01:59:59.999999+01:00’ dt = dt.add(microseconds=1) '2013-03-31T03:00:00+02:00’ dt.subtract(microseconds=1) '2013-03-31T01:59:59.999998+01:00’ Note that on creation, the normalization behavior is configurable (see https://pendulum.eustace.io/docs/#timezones for more information). To those wondering: yes I know Arrow (http://crsmithdev.com/arrow/) exists but its flaws and strange API (you can throw almost anything at get() and it will do its best to determine what you wanted, for instance) motivated me to start this project. You can check why I think Arrow is flawed here: https://pendulum.eustace.io/faq/#why-not-arrow (or https://github.com/sdispater/pendulum#why-not-arrow) Link to the official documentation: https://pendulum.eustace.io/docs/ Link to the github project: https://github.com/sdispater/pendulum
You really should have had 2 blank lines before defining Pydantic.
 flake8 error! WW666 - Quotes should start with capitalized word line 3: [e]xcessively concerned with PEP 8
&gt; And unit tests. Sanic does have unit tests. https://github.com/channelcat/sanic/tree/master/tests
There's nothing wrong with doing IO when needed per se. It's when people decide to put IO in the bowels of their package I want to use. Except I don't want to read a file, I have a perfectly good stream right here but the only way to get that stream into that processing is to do some nasty monkeypatching or to use tempfile, both of which leave me ugh at best. As for examples: * [The Mega Tutorial](https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world) -- which I partially think is unfair to call out as it's meant to be an example of building a Flask app and not necessarily best practices * [Kegbot Server](https://github.com/Kegbot/kegbot-server) which isn't Flask but serves the point * [FlaskBB](https://github.com/sh4nks/flaskbb) which I contribute to. And just about every web app ever. Just go on github and search for flask/rails/django/etc and most of them are all like that. But the point I was making, poorly perhaps, is that IO and processing get all mixed together into non-reusable parts. Yes, somethings will never be re-used in any other context than with a file or a web request or whatever. But it'd be nice to see code that does IO call processing code rather than the other way around. Which is why I like asyncio. Callables that do IO are tagged with async and their uses are tagged with await, async with or async for.
Among the many lines of output, it says: The headers or library files could not be found for zlib, a required dependency when compiling Pillow from source. Please see the install instructions at: http://pillow.readthedocs.io/en/latest/installation.html The last line is: File ".../Pillow-4.0.0/setup.py", line 791, in &lt;module&gt; setuptools.sandbox.UnpickleableException: RequiredDependencyException('\n\nThe headers or library files could not be found for zlib,\na required dependency when compiling Pillow from source.\n\nPlease see the install instructions at:\n http://pillow.readthedocs.io/en/latest/installation.html\n\n',) rysize installs regardless and when I try `rysize --help`, it says: File "/home/.../testenv/lib/python3.5/site-packages/pkg_resources/__init__.py", line 849, in resolve raise DistributionNotFound(req, requirers) pkg_resources.DistributionNotFound: The 'Pillow&gt;=3.3.1' distribution was not found and is required by rysize `pip freeze` then outputs only `rysize==0.2` --- it didn't install either Click or Pillow...
I'm currently working on a dice roller/digital character sheet for a Pen&amp;Paper RPG game I play on VoIP. The diceroller part is pretty much ready and after I test it on a game or two I'll release it properly to the public. In the meantime I'm building the character sheet part. Overall it's been a good way to get my feet wet with PyQt.
Matplotlib has 3 main competitors. Bokeh, plotly, and PyQtGraph. The first two are geared towards in browser visualization, while PyQtGraph is geared towards real time graphing. Bokeh is still very much in beta, but it's coming along. You certainly can get real time performance out of matplotlib, but it's buried in the API. I'm hardly an expert on anything besides matplotlib, so keep that in mind.
Is there some sort of blog entry of visual comparison kicking around that anyone is aware of? This "what's new" is very rudimentary.
pyqtgraph is pretty good. If you install it, run `python -m pyqtgraph.examples` to get a good feel for it. Their documentation sucks. But it's just so superior in a lot of contexts - particularly with large, complicated, real-time, or interactive data. I regularly throw hundreds of millions of points at a graph and have it respond without any lag.
Do you have any information on what those difficulties actually were?
Here you go http://matplotlib.org/2.0.0/users/dflt_style_changes.html
Nice, thanks! This is perfect.
Sounds awesome!
&gt; Their documentation sucks. That's an understatement, but otherwise I fully agree that pyqtgraph is good. There's also some very simple plotting in wxPython Phoenix. Good for things that don't need the power (or bloat) of matplotlib.
I had no idea this could be solved with a simple keyword and have always written code that would calculate the required off-set to center the bars. Tonight I'll be crying tears of sadness for the hours I've wasted, and tears of joy for the hours I will save
My point is that sync allows you to be lazy. Need something? Just hit up the network or the database or the file system. Anything can can do that at any point. FlaskBB, for example, hits up the database in templates. Some (most, probably) of that is my fault because I got lazy reimplementating the permission system instead of taking time to make proper changes. It does IO in forms, does IO wherever it wants because IO is easy in sync code. Async code at least forces you to acknowledge that you're doing IO and maybe there's the thought, "Maybe I don't need this IO right here, maybe I can feed whatever the IO spits out into this." That's not always possible because otherwise you end up with never doing IO and your app doesn't actually do anything. I don't think asyncio is some wonderful magic fairy dust you put in your code and it's just good. It's not, my first few asyncio projects were disasters in terms of architecture. I'm still not great at it. 
&gt; FlaskBB, for example, hits up the database in templates. so those templates are rendered into a string fully before they are pushed out to the client. While it would be an architectural mistake to actually have code in those templates that deals with databases and models, there is *nothing wrong* with the model accessors in the template fetching additional data from the database behind the scenes (e.g. lazy loading). You are still in the scope of your controller method organizing the necessary data from the database into a string to present to the client. It just happens to be bundled behind the scenes in the automation layer provided by SQLAlchemy ORM. I see this come up a lot, people are somehow disturbed by that concept, but I've yet to see a real reason for this that is not actually that they are concerned about having appropriate separation of concerns at the *source code* level. The automations in place (e.g. SQLAlchemy) doing a lazy load is *not* the same thing, anymore than you'd never put "malloc() / free()" into a template, yet the Python interpreter has to do all kinds of memory allocation behind the scenes to render your template.
All your samples are cache problems. There is no magic, if some value must be able to use in the future, they must be kept in memory. In the particular case of `lru_cache`, one possibility would be to attach a cache per instance of object as suggested in https://bugs.python.org/issue19859#msg205147 however that means also that if n instance live long for some reason, it would keep n sample of cache for itself in an useless way if not reused. At the opposite, the present `lru_cache` garantee that there will no more than 128 entries for the whole application. Actually, as far as your strip code match your real code, the crawl method don't use the self argument. So it could be transformed as a simple function called if you feel it necessary by a crawl method so the whole could be made like that: class Crawler: def crawl(self, hyperlink): crawl(hyperlink) @functools.lru_cache() def f_crawl(hyperlink): """ Fake 'crawl' method used to test out the '@functools.lru_cache()' decorator. """ # Return an arbitrary number of Hyperlink objects as if they were extracted for real from the crawled page. # Web pages typically contain between 1 and 300 hyperlinks. return [generate_hyperlink() for i in range(0, random.randint(1, 300))]
If you're running a debian distro, try `apt-get install zlib1g-dev`. That's the header files for zlib which Pillow is trying to build against. If you're not on a debian distro (e.g. FreeBSD, OSX, Fedora/RedHat/CentOS, or Windows), I'm not sure what the package is called or how to install it off hand. But you're definitely missing headers, potentially for several packages.
So the CSV module from my understanding only let's you use one character as a delimeter. for a project where I needed a four character delimiter this was my hacky workaround o = open(filename, "w") # write to file = open(fname, 'r') # read from text = file.read() file.close() o.write(text.replace('#','^~|`')) o.close() 
Pltw is sometimes way to slow and bs for any kind of high achieving students who are already really motivated. I'm in Digital electronics and I already knew the entire curriculum going into the class though self education and summer projects.. Honestly I would just learn python in your free time as much as you can if your really motivated and start conning up with ideas for web services or applicatioans
It also appears that your ide is saving it's layout in your dir as. Idea you might want to ad that to your git ignore too
If you have a large grid, JIT might be a good fit because the startup overhead should be negligible in front of the whole processing. The problem is more that JIT often not compile to fast code as compiled language The first thing to see is that you can use an existing library such as numpy or numexpr which particulary fit if your work can be expressed on array/vector/matrices operation. If there is no obvious parallelisation and a lot of interdependent processing, you have to reach a compilation JIT or not. From easy to hard possibility: - use pypy - use numba - use cython - write your own c extension (but that should not bring additional gain comparing to cython) The real compromise is between simplicity and not fast enough code.
Sure, make it your own thing😊 Please let me know what changes I can make to make the package better and my code better. Would love some great feedback☺
Yeah, I was amazed Matlab beat them to it. The damage `jet` does has been known about for what, a couple of decades now?
Thanks for the feedback. Initially, the plan was to use this list of beers to then scrape their reviews on other websites. However, all of the beer reviews website I found disallowed scraping. For this reason, I limited the scraping part to CraftCans.com . 
Python version of our university PBS server (UCSD, TSCC) is quite old. everyone has his own python version. 
Yup. As far as I can tell, the OOP API is held back by the historical baggage that comes from the Matlab-influenced design. For instance, as far as I know even in the OOP API it's still recommended to do: `fig, axes = plt.subplots() axes[1].something` instead of something like: `fig = plt.figure(subplots=) fig.plots[1].axes.something` which seems more coherent to me. Side-note: often the nomenclature seems messed up too, why does a function called `subplots` return one figure and multiple *axes* instead of multiple *plots*? Personally I would love to see 3.0 introduce a cleaned-up, consistent API but I'm lucky enough to have a job where backwards compatibility is no issue and I can find time to do the upgrades. Apologies for the rant on a thread that is about congratulating the team on getting 2.0 out. I use matplotlib regularly, have published papers with figures made with it, and am looking forward to trying this version out immensely!
`conda config --add channels conda-forge` This adds the conda-forge channel to your ~/.condarc file and gives it priority over the default anaconda channel. If you want to change the priorities, just reorder the lines in .condarc More info at https://conda-forge.github.io
If you're having trouble with ram, you can always try to use disk-supported arrays. Install an SSD (so it's fast), and then create your array in hdf5 using h5py. You can effectively address the whole file at once. I do this for time series data with great success, but access tends to be somewhat linear. It might not work for neural networks (which is more random access).
&gt; but doesn't get nearly enough attention. Is there any good documentation or tutorials on the OOP API?
[Not what I'm talking about](https://github.com/sh4nks/flaskbb/blob/master/flaskbb/utils/requirements.py#L253) That's the architectural travesty I'm talking about. Some of these don't (or shouldn't) make calls to the database as they'll just access attributes on a model that should be either loaded already or are precomputed attributes (e.g. Determining if someone is an admin). Others might end up (but hopefully shouldn't) calling the database because the information isn't available otherwise. And again, lazy accessors don't bother me so much. I'd prefer to cut them off before they happened, but if not... Eh. Then again, I also prefer not passing ORM models to templates. 
Thank you for your answer, ill be sure to make some of these points and update you if i have made something new! 
Nothing bad! Numba is one way of lowering high-level Python code to a low level execution engine (x86, CUDA, etc.), for data that is stored in a mechanism those execution engines can understand (i.e. C pointers and structs). As long as we have hardware, and data stored in memory or mappable from disk, then Numba will be relevant. By way of comparison, SQL is another execution engine. We can lower Python to SQL either through high-level translation (which is what some of blaze's current SQL approach does, and what Ibis and others do), or by embedding a Python runtime within the database server itself, and safely moving a subset of Python into that execution environment. Hadoop is another (simple) execution engine, and a storage manager, so lowering high-level Python code to efficiently execute on that system is a little trickier. If we are to directly use Hadoop Map-Reduce, it would unfortunately be rather restrictive on the expressiveness of the Python algorithms we could express. If we move to using (Py)Spark as the execution environment and data representation, then we have a bit more latitude and can access broader algorithms, but they are still within the silo of the Spark ecosystem and restricted to its concept of in-memory map+shuffle+reduce. Hence, Dask, with its hdfs handler and our new fastparquet support, allows the wide world of Python algorithms to be directly expressed on top of Hadoop FS data, while interoperating with schedulers from the Apache "Big Data" zoo. Our technical vision at Continuum, since the beginning, has always been that it is extremely valuable to have a single, coherent language environment to describe high-level data transformation and numerical algorithms, that can then be dynamically and optimally lowered to any of these (and future) execution environments and storage technologies. We do recognize that we live in a multi-lingual world, and our hope is to be able to expose these concepts into R and Julia and whatever else may emerge in time. But we're most familiar with Python and also Python is awesome, so we're doing it first in Python. :-)
You stated that you can reproduce the leak with that certain page source, so something in there most likely triggers the cache leak. At least I assume that because it would be logical that this string passed into `html.fromstring` produces the problem. So how are the metrics of tags, names, scripts, values etc. in comparison to other sources that doesn't trigger the leak in the first place. First of all, that can help very much to fix the problem. And second it can still be just malformed html and this would lead into a different kind of problem that need to be fixed. Don't forget that browser always try to repair broken webpages which dosen't mean that the source code is always valid. And last but not least since you didn't provide any of your crawling code it could be still an error invoke by your own code. I mention this not to blame you but usally this is the primary source of errors and leaks at least for me. Oh my god so many 'sources' I need a thesaurus ;)
Please use tornado for async IO if you are on python 2.7.x
Not that I can remember. I learned it mostly by bits and pieces of whatever I found on the Internet. If you're patient, your best bet is through the [official API docs](http://matplotlib.org/api/index.html). Roughly, it's a matter of (1) creating the figure and canvas (2) adding 1 or more axes (3) plotting on these axes. I do not normally use the OOP API exclusively, at least not for interactive plotting. For (1) and (2) I resort to the MATLAB API (`fig, ax = matplotlib.pyplot.subplots()`) because doing (1) and (2) using the OOP API by hand is tedious and does not buy me a whole lot for one-off plots. But in case you wanted to know, this is how you would do it. Note that it's important to pick a [backend](http://matplotlib.org/api/index_backend_api.html) that your system supports. import matplotlib.figure # must choose a specific backend here: from matplotlib.backends.backend_qt5agg import FigureCanvas fig = matplotlib.figure.Figure() canvas = FigureCanvas(fig) canvas.show() ax = fig.add_subplot(111) ax.plot([1, 2, 3], [3, 1, 2]) input() # stall the interpreter In contrast, for (3) I much prefer the OOP API (e.g. `ax.plot(…)`) because it's a lot more readable and has more knobs to control positioning of the elements.
Too much to either side is bad, I think. I tend to be a Pydantic, so I try to take down a few notches, but I get really pissed with people that just write Java in all languages, and bad Java, on top of it.
Bonus 1: find words for which the heuristics of sylco are wrong. Bonus 2: compose a haiku made entirely of such words Bonus 3: evoke an emotional response :)
Yes! Several things are in the works now and looking forward to the future. I have a specific agenda for array- and table-computing across languages starting with Python. Will take a few years to materialize and will collaborate with other initiatives already underway by others.
Isn't 2.0 the version of the project? He links to his first "subprocesses for humans" project in the first line of the readme 
That's not universal - using explicit setters is a good way to signal that setting has a cost. e.g. from [PEP 471](https://www.python.org/dev/peps/pep-0471/#notes-on-exception-handling) which introduced `os.scandir()`: &gt; DirEntry.is_X() and DirEntry.stat() are explicitly methods rather than attributes or properties, to make it clear that they may not be cheap operations (although they often are), and they may do a system call. As a result, these methods may raise OSError . 
Matplotlib has few serious competitors outside the python world. It is really a remarkable and robust framework for plotting. I hope one day we have a 3d plotting framework that is as flexible and malleable as matplotlib is for 2d (and no mplot3d does not count until it gets a renderer that can handle zorder properly)
What are its serious competitors even outside the Python world? gnuplot and especially ggplot are the obvious candidates. Is there anything else?
Dunno. I just reinstalled Python and it worked again. Who the fuck knows.
took another look tonight. if your input is always a list of tuples with two elements each: lst=[('a','b'),('c','d'),('e','f')] fname = 'something.csv' o = open(fname, "w") for l in lst: o.write(l[0]+', '+l[1]+'\n') o.close() if the number of elements vary then you might want to have another inner loop or look at itertools. just know that this is another hack method...
Wow! Thank you so much!
I've gotten familiar with it through trying to make a Qt5 plotting app and so far I keep running into problems finding proper examples. (I learn from examples, not documentation). Most of them I've found don't seem to make sense or they don't follow the same nomenclature that Matlab does. Like what is an 'axis' vs a 'figure', etc. A simple cheat sheet like the [CSS Box Model](http://www.w3schools.com/css/css_boxmodel.asp) would really helpful. 
 sorted(full_list, key=os.path.getmtime) This is a terse and delightful use of `sorted(l, key)`!
Sorry for your pain! Whoever said it was easier hasn't been down one of these rabbit holes before. The advantage with Python at times is that the rabbit hole is deeper and can lead to better treasure... To be fair, the Matlab equivalent would be changing the Java runtime that Matlab uses and even that is benign compared to the complexity of matplotlib's many backends and the complexity of Python wrappers around half a dozen C and c++ based GUI framewords... I hope this deep dive into the options was at least somewhat rewarding. Good luck finding a setup that works for you! It's not all this frustrating.
Gnuplot is not a serious competitor with anything. Xmgrace, MATLAB, Mathematica, origin, vuez(also python), root, R are a few. MPL is in my opinion vastly superior to all of them (maybe only superior but not vastly for R)
Nope - like I said, the docs suck. But it is really useful. I load my data from raw formats into hdf5, then plot using pyqtgraph. I usually have a lot of subplots that are taking subsamples of the data under my cursor and doing things like histograms or fourier transforms on the 50,000 points nearest my cursor (or whatever). It really helps if you know Qt or PyQt if you want to customize it. It's basically a giant QGraphicsView that they've added plotting widgets to. If you want to do things like override the mouse behaviour, the docs for pyqt will help you more than anything.
If you add it to the git ignore it stays in your local directory but when you upload it to git hub the. Ideas fikes won't be pushed to git hub with the rest if the code. Ideally you don't push your ide config files with your source code, it's not a thing software developers in general do. If someone using a different ide used your code they don't need the files and might have different ide fikes. It doesn't really matter if your just learning to prove Ann or doing a fun project but when making software in a company or large software in general it's excluded from the git repository. (sorry for the long essay I used to do this also)
Pandas 0.19.2 officially works with matplotlib 2.0. There really weren't any changes needed on pandas' side. The MPL devs did a great job not breaking API, aside from style changes.
Its no problem, i appreate the feedback
selenium will load js
&gt; For the three two-digits examples, this is actually due to the fact that pendulum does not recognize the string and fallbacks on the dateutil parser which produces these results. From a usability point of view, I don't want to have to be 100% familiar with the internals of your library and all of it's dependencies to predict it's behaviour. From your FAQ: &gt; "Arrow cannot always be trusted to have a consistent behavior with the data you are passing to it". But neither can Pendulum because it just falls back on the dateutil parser for results. You can't then wash your hands of this shortcoming/fault, because Pendulum is specifically designed to address these, is it not? You get even less slack for this because the tone in your FAQ is so condescending towards Arrow's ability to do string parsing. You spent far more real estate on the page explaining why Arrow is a bad choice, and how its string parsing is bad, instead of highlighting Pendulums benefits or functionality missing from other packages (worse yet, as I pointed out, the criticism was hypocritical). That's all pretty telling, and really just screams "I don't like Arrow so I made this package, you should feel it's better too". Well, it does to me at least.
While I agree with the "rules" they are not rules and are in no way legal or enforcable. Majority of the businesses want to be scraped by google but not by other websites, which you could say conflicts with net neutrality itself. At the end of the day when you make something public - it's public and you can't chose who can and cannot access that public data.
&gt; You can't then wash your hands of this shortcoming/fault, because Pendulum is specifically designed to address these, is it not? When did I say that? I just pointed out where the bug is coming from. I agree that this is an usability issue and I am not saying that I won't fix it or address it. &gt; You get even less slack for this because the tone in your FAQ is so condescending towards Arrow's ability to do string parsing. I am just pointing out flaws that exist in Arrow for years now and haven't been fixed. And it's not limited to string parsing (I give the example of the DST transition and the example '20160413' has nothing to do with string parsing, it's just that arrow consider it a timestamp) but, at least, Pendulum will parse correctly common formats, especially ISO 8601, which is what most of the users want. And to be fair, I started this section because people were asking why they should use Pendulum over Arrow so I gave some clear examples. And it doesn't mean Pendulum does not have bugs, far from it. And overall, this is the Arrow's API that I dislike the most rather than the critical bugs.
PyData Carolinas 2016 | Presentation: Matplotlib 2.0 or "One does not simply change all the ... Edit: sorry not sure how I fucked up that link
As sad as it is to say it sometimes it's easier to just dig through the source code. I have peeked into matplotlib's source code when I couldn't find answers from the docs or Q&amp;A. For things like figures and axes, this might help: http://matplotlib.org/faq/usage_faq.html#general-concepts
Thank you! I'm so glad to hear all of this. Encouraging feedback like this from the the user and dev communities is what sustains us. The next few years are going to see the sprouting of seeds of some ground-changing technologies, like storage-class memories and purpose-built neural network machines. At the same time, even as cloud goes mainstream for enterprise, there are alarming concerns around security, privacy, and whatnot that are fundamental and intrinsic to our "highly networked" architectures. Such concerns will only scale as data and connectivity grow, especially with IoT and self-driving vehicles. (They must inevitably come to a head, as black-hat entities become more emboldened and profitable, and state-vs-state actions become more common.) So, I believe that from here on out, we will be constantly hit with more and more waves of technology disruption. As long as that continues, I think it reinforces the foundational role of accessible, open, well-engineered technologies for data manipulation, computation, and analysis. If any proprietary vendor ever owned that substrate, they would own the world, and not for the better. So you can rest assured that at Continuum, we will always be pushing for sustainable, open-source foundational engineering and innovation, even as we grow and scale our commercial business and bring open data science into more and more of the world's businesses. 
Awesome. 
Loved it. :-) I even... gasp!... shared it on Facebook
Yah I know, I phrased the question poorly. I meant, does the position of the speaker relative to the mic influence the impulse response? There's a cistern in Houston that would be amazing for this, if you're ever in TX haha
\this
pyqtgraph is very good at realtime data. It's not a problem to create a scrolling plotter with a minimum of CPU usage. 
Numba has a solid foundation and is making rapid progress to 1.0. Three things I will say about Numba and my overall plan. First, Numba will be an important tool and part of the ecosystem for a long time. Second, Numba will get the ability to understand data-shape (using a project we have been working on called ndtypes --- which is the 'dtype' concept of NumPy factored out and generalized). Third, Numba will be associated with a gumath module that will also be a separate module. Right now Numba creates NumPy ufuncs. It will also be able to generate these "generalized" ufuncs that live independently of NumPy and build on the idea in NumPy. If you have ideas for Numba, please join the community mailing list and contribute your thoughts. The team is easy to talk to and welcomes input from everyone. It's not the easiest project to contribute directly to, but it welcomes ideas. 
bokeh and plotly are great for interactive graphs.
Really? Gnuplot is out of the running but Xmgrace is in? To be fair, I only used Xmgrace a few times several years ago but my impression was that it was a last century relic.
Look at the sidebar, `programming challenges` and `online excercices`. I personally started the python challenge but it quickly became more like "the cryptography challenge" and "the image processing challenge" so I kinda left it :) dont know about others
That is why I'm back at Python, but I would like that effortless async.
I use gnuplotpy, a wrapper to the gnuplot routines. 
Looks like they call into C Python extensions. Not the most compatible with PyPy sadly.
Altair is amazing, I'd pick it over matplotlib for most tasks any day of the week.
care to post your test code?
I have updated my code to show source code of test.py. Thanks
Mine was Accounting/Finance, so I also had a similar piece-by-piece introduction to what can be done with Python (and still have much to learn!) VBA/Excel is great for managing things within the Office applications but Python allows greater access to the operating system, or at least makes it much less cumbersome. There is a wealth of scope that people have used Python for and really it is dependent on what you're working on and the problems you face. As a general guide to getting quick wins, I'd write down a list of things that take a lot of manual work on the computer, and things that you wish you could do but either don't have the time or software for. Google each one of them with Python and it will at least point you to the most relevant libraries to help you get started making solutions. In my case, Requests &amp; BeautifulSoup (web scraping and HTML parsing) and Openpyxl (Excel interpreter) were pretty useful.
Not really. We were doing work on a memory constrained platform previously, which involved trickery like `implement much logic in a single loop`, `explicitly call garbage collection rather than manual` and `not touching memory references in mainloops`. The last one is a bit tricky in python. Overall, by having explicit pauses where we do GC, we got perceivable improvement on memory usage. These days we just have more RAM to work with.
you 'dont use crypto hash function to compare text: secure hash function are designed for slow constant speed by design http://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed Remember that whatever hash function you are using you are exposing yourself to collisions and that the probablities are far from null. To answer your question go for md5, it is a very good choice, especially if you need strong guarantees....of a code that inspire secured trust coming from the best design possible.
If PyPy really any good for a webserver with short running, non-looping processes?
how about just base64?
Not necessarily. Note that this is only the "cycle breaker" which was introduced circa Python 2. If you don't have cycles or break them by hand (as you had to do before the GC was introduced), you don't need the GC, RC works as it ever did.
lol Just random picking any function will not work. Especially not an encoding function. lol. Just take time reading the link I sent. The hashes you quoted are slow by design (and md5 unsecure since 20 years). There are better alternative for your purpose designed for speed and low collisions. All hash have collisions so you still have to compare texts at the end to make sure they truly are distinct. 
also, md5 hash in the main answer of that article isn't mentioned. but crs32 is.