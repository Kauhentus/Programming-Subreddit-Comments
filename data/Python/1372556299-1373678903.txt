http://github.com/burnash/gspread
That, I will remember.
The Truth-In-Advertising police would like to talk with you about showing a sheet with different colors on the background for an API that doesn't provide that functionality...
Finally going to give the beta a run today and see how it goes, mostly for virtualenv support though, I'm starting to have dependency hell with my project
The sympy docs (e.g. http://docs.sympy.org/0.7.2/modules/core.html ) are [written in Sphinx](http://redd.it/18w8si#c8jby1t) with some sort of a "Run code block in SymPy Live" function that looks alot like /r/IPython notebook. Here's a lecture / tutorial in [IPython notebook viewer (nbviewer.ipython.org)](http://nbviewer.ipython.org/urls/raw.github.com/jrjohansson/scientific-python-lectures/master/Lecture-1-Introduction-to-Python-Programming.ipynb). From http://redd.it/1dl8wc [^*](http://redd.it/1fsqj1#cadl4bb) , you might check out: * ipydra * wakari * galah
installer doesn't work for me, sad edit: uninstalled 1.5, rebooted, 1.5 was still installed and I hadn't noticed. finally after two uninstalls and reboots, I got 2.0 to install. strange
client request :)
&gt; clock_seq While it does document that argument, it doesn't advise its use or necessity for high throughput scenarios &gt;If clock_seq is given, it is used as the sequence number; otherwise a random 14-bit sequence number is chosen. For my test, obviously that 14-bit sequence isn't random per call or just isn't random. edit: s/advice/advise/
I would love to switch to Nginx and uWSGI, but my current project (an internal corporate deal) requires Kerberos authentication as it is corporate policy. To be fair, there is a Kerberos module for Nginx, but it's terribly immature and has a readme that has something like "pray for no segfaults," which is not terribly encouraging.
Rockin', I've been looking at putting together a decent Django host for a project at work, this has been useful. Next step; trying to automate deployment from subversion.
Well wouldn't this have the same problem? If `__enter__` is doing multiple things and exits with an exception in the context manager protocol, `__exit__` is not called. In other words, `__exit__` is only called if `__enter__` succeeds, just like with `tearDown`
I'd look into matplotlib and Bokeh https://github.com/ContinuumIO/Bokeh It'd developed by Continuum, the guys who make numba and Anaconda. They just got a $3 million dollar award from DARPA to develop Bokeh It's also open source
Yeah it doesn't seem particularly significant that the library can't precisely replicate the one in that screenshot.
To you. Yet for someone who would like to have that functionality, it's very significant. 
[NLP : How can I get the computer to understand the sentiment of text?](http://www.reddit.com/r/compsci/comments/1gpdb9/nlp_how_can_i_get_the_computer_to_understand_the/camicro)
There's two problems with this approach to sentiment analysis. First, it doesn't really handle negation. Second, classifiers tend to be bad outside of the domain of the training set. The second can be mitigated by a large set, but the first is a problem given how people write. I tested movie ratings sentiment classifier (after classification) with the phrase, "This isn't the best movie" and got a return of positive.
I already fixed the code in question for myself by switching to uuid4, now I am trying to convince a project owner to do that as well so I don't have to keep a perpetual fork. Setting `uuid._uuid_generate_time = None` eliminated the collisions for me, so at least I've got that as an alternative.
Nice !! Pythonic way to time code's running.
Matplotlib is awesome, but it does not handle large datasets well. In my experience, data sets that exceed a few hundred thousand points often trip up MPL. You should probably resample your data first, or only plot summaries like histograms. How to do the resampling very much depends the perticulars of your data. What you describe really is a [two dimensional histogram](http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram2d.html).
Seriously. Horribly written article. Essentially says, don't use gunicorn because it's trendy, and I said so. 
This is nice enough. Not a pattern I find myself wishing for, but that's just me. I would just set it directly like if I wanted to delegate/forward. It's nice that way because it's clear to everyone that knows basic Python and I don't mind specifying the full scope like `foo.bar.baz()`.
Are they geo coordinates? Even if they are not, you can download Quantum Gis (bonus if you get the latest beta as its faster &amp; better than stable 1.8) It's one click to import the text files, two more to save that as a shapefile and index it for speed, and so on. If you need to script it you can use the embedded python interpreter. To color the points based on point density, that's a different story.. there are a few approaches such as playing with point transparency, running heatmaps (warning: extremely slow for 8MM points), or grouping the points into grids
Yup, I can understand that explicit wins over magic. But it just becomes cumbersome to type out everything. And code snippets doesn't help much since I have to read the code later. Having read some of Python's standard library, I started to feel less and less guilty when applying dirty magics just to make my own life easier. Besides, I consider the plucking feature an accidental bonus since it's implemented by `operator.attrgetter()` :)
Awesome... I didn't think about the apostrophe in "didn't". I just tried it on one of the other sentences and that was the difference. Thank you.
Likewise you can have a double quote in a single quoted string, and you can use any of the two in a triple quoted string: "This isn't wrong" 'Neither is "this"'. """You can't go wrong "in here".""" 
That example is a little out of date. The modern way of doing it is: formatter = '{} {} {} {}' formatter.format("I had this thing.", "That you could type up right.", "But it didn't sing.", "So I said goodnight.") str.format() is much more flexible and less error prone than the old `%` syntax. For more about this: http://docs.python.org/2/library/string.html (for Python 2) http://docs.python.org/3/library/string.html (for Python 3, but they are about the same)
I am using that website already, it is amazing! Finally a python introduction that works for me. Now if only I could get Pygame to work on a mac. :/
String interpolation is deprecated and will be removed in the future versions.
See @afoo42's reply, which nailed it precisely. I've also made a [gist](https://gist.github.com/5long/5895095) to illustrate it. Metaclass in Python 2.x can't do this. Py3K's `__prepare__` capability can make the `def_delegators` macro available in the class body, which could be another approach to make this happen. Since you can only specify a single metaclass when defining a class, I'd still stick to this decorator approach which makes it more friendly to existing code base. EDIT: make a public gist instead.
I really don't like the namespace injection. Seems like totally unnecessary magic, magic that static analysis tools like pyflakes or pylint can't see. If you don't want people to "import \*" then just write out the examples with a proper import and people will copy it.
I see your point here. I'll expand the "Less Magical Usage" section to make it more obvious. BTW, the `def_delegators()` function actually does scope injection too(which creates property descriptors and shove them into the class body). Guess I'll need to explain this whole thing up front in the docs before users(if any) got confused.
Think about it this way: You are in control of what to do if setUp fails partway through, and so can make sure setUp covers cases in which it fails. Once the setUp is complete, the test can go wrong in all kinds of ways, but the tearDown undoes the possibly ill effects of the setUp, and it is easier to have it able to trust that the setUp was run completely. def setUp: try: cue 3 elephants try: cue 5 tigers light building succeed except Anything: rescue tigers raise except Anything: rescue elephants def tearDown: deploy fire extinguishers recover 5 tigers recover 3 elephants clean up
This one bit me many times when I was working with Cython on windows.
py2exe
[DLL](https://en.wikipedia.org/wiki/Dynamic-link_library) (Dynamic link library) files only run on the windows platform and thus cannot run on any computer.
Next step. Remove the overhead on attribute access: https://github.com/defnull/bottle/blob/master/bottle.py#L173
*generally* speaking, in python if you find yourself thinking 'I'll just hack the stack frame', you should be hearing some warning bells going off. [Here's a quick and dirty example](http://repl.it/J5a) of how a similar thing could be implemented without touching the frame. It uses [type()](http://docs.python.org/2/library/functions.html#type) to create a proxy class which responds to the specified methods. The delegate methods are defined with a normal class property, obviating the need for an additional function import. Alternately, you could mess around with [\_\_getattribute\_\_](http://docs.python.org/2/reference/datamodel.html#more-attribute-access-for-new-style-classes). 
You can argue that Java &gt; Python for cross-platform because of this. wow, this comment pissed a lot of people off, but this was one of the points made by people in that thread. 
Thanks for the heatmap suggestion, it seems to cover what I want to do. The only problem I'm having is that when I plot data that contains about 50,000 points it works quite well but when I try to plot the 8 million they don't seem to be plotted properly.
No, the data I'm trying to plot is actually roots of polynomials so they're just simple (x,y) coordinates that range from -1,1. I had originally produced about 40000 of these points and plotted them with NodeBox which handled them quite well but ran out of memory when I tried throwing 8 million points at it. I'll look into Quantum Gis as you suggested and report back. 
Here's the [video](http://youtu.be/pMnGHP08ZY4) of the presentation.
It could just be the sheer size of your input doesn't work well. You could try fiddling with the different parameters, but you may have to write your own code for it. With such a large data set, I would recommend utilizing numpy (which heatmap does not do) and do your best to avoid loops, or you'll be waiting around all day for the results. There may also be other options for a heatmap library out there, although you may have to use something other than python.
Very cool. At Zapier we had the unique situation of writing into spreadsheets that were rapidly changing. That meant a lot of locking or queuing writes to ensure they don't overwrite anything. We accomplished that with some distributed Redis locks. To be fair, Google's Spreadsheet API is rather... awkward to use. I'm glad to see some libs making this easier.
&gt; While it does document that argument, it doesn't advise its use or necessity for high throughput scenarios I think it's supposed to be understood that that's the purpose of a sequence number in generating a uuid. Python can never guarantee anything about the hardware clock, so they provide an option to override it.
1. eBay - created a web service for targeted internet advertising. If you rendered an eBay banner ad, my (Python) code was in the mix. 2. Harmonix - backend services for Rock Band and Dance Central titles, reporting/data warehousing/tools, the Rock Band World Facebook app (all Python, Django) 3. Cogo Labs - big data analysis/data warehousing, internal web services (Hadoop streaming w/Python, Django) Using Python professionally has been better in every way than my prior experiences with Perl, Java, PHP, C, Javascript, and others.
I work for [Elsevier Research Data Services](http://researchdata.elsevier.com). I Create tools to aid in the archival and analysis of academic data. It's mostly Python web development, but we need to integrate with other services (tablet apps, Igor Pro from Wavemetrics). I sort of fell into this job (it was listed as a job at Lexis Nexis another Elsevier subsidiary that does python development). I'd done work for the Social Science dept. at University collecting &amp; analyzing data (in PHP, yuck), so it was sort of similar to that.
Quite a lot of relevant information missing here. You'll need to register an application. You must use the client apis with authenticated requests.
Oh man, this bug bit me in the ass when I was trying to build the `pylib7zip` C extensions and was one of the motivating factors for moving to ctypes and/or cffi.
It is much easier to help when someone has shared links to which resources they have not yet found their answer from. https://developers.google.com/gdata/ https://code.google.com/p/gdata-python-client/ https://pypi.python.org/pypi/gdata/ https://code.google.com/p/googlecl/ [^*](https://pypi.python.org/pypi/google_cl/) https://developers.google.com/gdata/articles/python_client_lib Did any of these help? 
The README file does not mention if it supports Windows or not. Windows does have a *cron*.
There is only *the* cron, not *a* cron. And when Windows is able to run *the* cron, then you can use this script on Windows.
There was a ton of great discussion (relevant links) in this thread: [How relevant is computer science to careers outside software development, IT, etc?](http://www.reddit.com/r/compsci/comments/1e66ko/how_relevant_is_computer_science_to_careers/).
The best answer I can give is that it should...as long as you can set up a cron job. The file-pathing might be different and mess something up, you would have to try it yourself and possibly tweak that part.
I am an electrical engineer for a consumer electronics company. Python was not a required skill for the job and I think most of the people I work with are not very familiar with it. I use python for crunching big data (some or our data sets have 10's of thousands of entries). I also have used python for some basic data acquisition on some hardware using .NET framework and iron python.
I work in visual effects, and python is, on the whole, pretty essential for any developer in this industry. It has been implemented as the scripting language in most of the major tools we use (Maya, Nuke, Houdini). A lot of development time is put into pipeline tools to enable the packages to build a smooth workflow.
It looks like we can argue with that
Python has been my primary breadwinner at each of my jobs since 2007. The first place I used python professionally was called Danemco, now called Velocity Webworks. It's a web development company that caters to smaller mom'n'pop sewing and quilting companies/individuals. We transitioned from a homegrown php framework to Django. They're still using Django primarily. After that was ScienceLogic, which offers a ridiculously amazing network monitoring appliance. Most of the backend is python. Excellent product to work on, and the team is fantastic too. Now I'm working with StorageCraft, building a test automation framework. I've written most of it from the ground up using python, with a heavily customized Django admin for the ui. All throughout these years, particularly recently, I've had gobs of recruiters contact me about python opportunities all around the US (a few were also foreign). There's no shortage of opportunities for python developers, if you ask me :-) 
Next week I'm going to start my first full time programming job as a web dev for the Atlantic, and it's basically because I made good connections on /r/Python. Procrastination pays!
Do this instead: formatter = '{} {} {} {}'.format formatter("I had this thing.", "That you could type up right.", "But it didn't sing.", "So I said goodnight.") 
Welcome to real error handling, something most Python code seems at a loss to do right. If setUpModule throws an exception, then it needs to catch it and cleanup appropriately. 
Currently I work for Rackspace (we do dedicated and cloud servers), I'm a member of the Developer Relations Group, essentially that means I work on an SDK (libcloud: https://libcloud.apache.org/), hack on OpenStack from time to time, and generally do Open Source stuff. Before that I worked for Rdio (a subscription internet music service), as a backend software engineer, there I was doing things like designing database schemas, implementing business logic, and trying to optimize the site.
That's a very nice way of creating shortcuts. I use the same technique often when parsing command line arguments: def parse_arguments(args): parser = optparse.OptionParser( usage='usage: %prog [OPTIONS] file1 ... fileN') parser.add_option('-o', '--output-file', action='store', dest='output_file', default=None, help='sqlite database to write to') parser.add_option(... parser.add_option(... parser.add_option(... parser.add_option(... Now, that's just a little too much `parser.add_option` for my taste, and it ends up pushing everything too far to the right, so instead I do: def parse_arguments(args): parser = optparse.OptionParser( usage='usage: %prog [OPTIONS] file1 ... fileN') add = parser.add_option add('-o', '--output-file', action='store', dest='output_file', default=None, help='sqlite database to write to') add(... add(... add(... add(... Now, that's easier on the eyes. 
NASA. I'm a research engineer and applied mathematician working on software frameworks for propulsions research, biotechnology, and a few other smaller projects. Python+Numpy+Scipy+Matplotlib is a fantastic toolset for scientific computing. Python use is growing in the agency, as well as other places in the government and academia (as an alternative to increasingly expensive commercial software like MATLAB).
I call myself a software engineer for the University of Wisconsin. I work with weather data and help atmospheric scientists. I do everything from python web applications to make data available to backend code that manages the data. I've done matplotlib plotting (static files and real-time updating plots), PyQt4 GUIs, web applications, instrument communication software (long running scripts that record data by communicating with multimeters, etc.), software packages for various types of data (remapping satellite data, etc.). A lot of times my projects start with a scientist needing something and me figuring out how to accomplish it. Some times that involves interfacing with other/older software written in matlab, C, fortran, or others. Python works well with scientific data.
Suds is rather terrible though, code-wise. It can and will break on you with some WSDLs (Microsoft's, mostly...). It also consumes metric craptons of memory and time for no good reason, constructing and deconstructing wrapper objects.
So many spelling mistakes...
Software Engineer with Media Temple. We're a web hosting company with shared hosting and VPSes. We're mainly a Perl shop but I work in whatever language is best for the job at hand, which can be anything from Python to Javascript to Erlang.
Maybe this is intrusive, but would you mind talking about how you got those jobs and what advice you would give to young python lovers out there?
Sorry I commented too soon. I ended up finding it [here](http://www.rackspace.com/cloud/servers/pricing/). Thanks for your answer though.
I haven't looked into the Python versions used in those sorts of software. Would you say they are predominantly Python 2 based still?
That's probably a leftover log message. Why are you using *Eclipse* for Python, though?
I think python's GIL is infamous enough, we should promote Go's GOMAXPROCS problem more.
Sr. Developer at an unnamed data-mining company. We use python for massively parallel data-collection and analysis. 
I do the same, except for a consumer electronics company.
I'm a systems admin for a medical staffing agency. I'm the only one who uses python. I write one-offs mostly for automation and monitoring. Python monitors how many VoIP trunks are in use across 35 sites, custom software deployment (without KACE or the like), simple sql reports, email archiving... Anything I don't want to do by hand. Python wasn't a job requirement and doesn't make me more money, but it lightens my load and makes me look good. I'm learning django which should help a ton in the future. 
In [Anaconda](http://continuum.io/downloads), this bug is patched on Windows.
Last time I ran into this, I thought the final verdict was "Python 2.7 is SOL because we're moving on to Python 3"?
&gt; There are a number of libraries that extend the capabilities of the cPython interpreter in some way. None, to my knowledge, affect the GIL in any way. That point is a straw man or a mis-understanding. It tries to refute the fact that many think eventlet|Tornado|gevent somehow removed the GIL. Which wasn't the reason people brought those up. The reason those libraries were brought up was a side-item to the argument that "It is not a big deal that Python has the GIL because a lot of current code is I/O bound". That is the idea. Now back to eventlet. Ok so GIL is not a bid deal. Because concurrent IO works great with threads. Except that many criticize real OS threads as 1) they take too much resources (cannot have tens of thousands usually on a typically) 2) it is dangerous for shared data-structures. So people responded that when you do concurrent IO something based on greenlet (eventlet and gevent) or Tornado is often better. You get to create a lot more green threads and context switching between threads only happens during IO points. So in some library that you write you can forgo acquiring locks for example. (Guido argues this is a downside as IO context switching could happen hidden in side a nested function, but that is a different argument, he proposed Futures and a Twisted like evented system etc etc...). 
I recommend using [concurrent.futures](http://docs.python.org/dev/library/concurrent.futures.html) - it has been [backported to Python 2](https://pypi.python.org/pypi/futures). It abstracts out using threads versus processes. What I do is have a utility module that adds command line arguments to an [ArgumentParser](http://docs.python.org/dev/library/argparse.html) (eg specifying threads versus processes, how many and if you are doing debugging). Another function provides a correctly configured [Executor](http://docs.python.org/dev/library/concurrent.futures.html?highlight=futures#concurrent.futures.Executor). Not mentioned in the article is how debugging is a pain. CPython tracebacks are not picklable, and hence can't be transmitted in multiprocessing. Which is a pain. If you debug via import pdb;pdb.set_trace() then concurrency is a pain. My debug command line option forces the use of one thread conveniently.
Windows does have an equivalent to cron called Windows Task Scheduler. Nothing about this script requires cron specifically.
Mechanical engineer working in a wafer fab. I write interfaces for the tools for scripting out useful data from the 10,000+ line log files generated daily, and then using this data for automatic process control.
That's good to know. I was slightly disappointed when I found out I had to learn 2 for work and not 3 since I tend to migrate to the latest versions of software but this puts me at ease. 
Technical Director / R&amp;D Developer for Visual Effects company. Mostly writing plugins for large software packages like Nuke and Maya, but also stuff like render farm code, image processing pipelines and web servers.
Did you read the article? You can use shared objects like threading.Array and threading.Value as well as managers/proxy objects so no memory is duplicated
If your implementation uses reference counting (as CPython does), then you'll have a hard time not modifying the reference count of your objects even if you aren't mutating the objects themselves. This isn't an issue if your shared data is stored in a handful of big `bytes` objects, but in the more typical case of a complex graph of small-to-medium-sized objects, this means that the OS won't actually be able to share the memory between your processes (even if it is logically immutable from the application's point of view).
The PyDev plugin is handy for step-through debugging.
What would you recommend? Comming from NetBean for Java, I seriously can't imagine writting without similar features, the debugging mostly.
What's the problem with GOMAXPROCS?
I agree with the node.js guys on their point of view - multithreading is worthless. The reality is, 99% of the time your software will either work fine in a single thread, or you'll need to eventually scale it up over multiple machines, not just multiple threads, so you might as well write it using processes rather than threads anyway. Yes, there are rare cases where it's worthwhile... but I don't see it as a big issue. There are ways around it.
One way to solve your problem is to generate several plots like your example and them overlap all into one with simple image processing command. You could also perform coloring by subseting your points in a way that you want. The other way around is to use 'pil' imaging package and manually put your point into the png file with specified color and ...
Python comes with [its own debugger called pdb](http://docs.python.org/2/library/pdb.html). You can certainly use an IDE if you want to, but Python's dynamic nature and lack of extreme verbosity make some IDE features much less useful, and having a REPL negates a lot more. Assuming you know how to use an editor like Vim or Emacs, you'd probably be more efficient in it since you get better editing while losing fewer features.
Doesn't pdb do that?
This is not a solution that scales to anything but trivially-specified problems. Often you have a program where 90% of it is common data but the other 10% needs to be shared across multiple threads. Handling that with a multiprocessing approach is going to be a nightmare since you've now moved from having to carefully manage 10% of your data to having to carefully manage 90% of it - or just accept a large duplication in memory usage.
I am a web developer who works with information systems. ["Here are alot of Python jobs"](http://dn.reddit.com/r/Python/comments/17c69p/i_was_told_by_a_friend_that_learning_python_for/c84bswd) There are alot of [Java](http://www.reddit.com/r/Python/comments/1ew4l5/im_giving_a_demo_of_python_to_a_bunch_of_java/) jobs too.
Uh, yes there are. Alternative Python implementations or extension modules that do the intensive stuff at a lower level (still 'use a different language', but less drastically). That said, yes, it's possible that sometimes Python just isn't the best tool for the job. The question is, is it worth spending a ton of time and effort and potentially make Python worse in other situations, just to make it better in this particular (rare) situation? My answer is no.
Extension modules are not really a solution to the problem. If you have a bunch of Python variables that you need to share across multiple threads then the last thing you want to be doing is pushing and pulling them through the C API. I'm not arguing for a change in Python, but I am arguing against the viewpoint pushed by many Python fans that nobody ever needs good multithreading. Games and multimedia apps aren't really rare or unusual, and are the main forces behind some massive industries. Desktop quad-core machines aren't there for multiprocessing.
&gt; This isn't an issue if your shared data is stored in a handful of big bytes objects, Ahh, right. I deal mostly with NumPy arrays so I'm used to the "big blob of data" case. 
I've published it on PyPI, under the name **contexttimer**, and added a timer function decorator in the process.
I'm not a CPython expert so i learned quite a few things, good article. It got me thinking : - I'm using multiprocessing in one of my software, and I'm wondering what happens exactly when i use multiprocessing.Pool.map() in terms of memory usage. There is two instance of my list in memory ? (A big one in the "main" process and a copy of each chunk). Is the same also true for the results ? - I've heard that linux has a copy-on-write strategy when a process is forked, does it have an influence here ? If yes, should i expect a difference in memory usage if I'm porting my software to another OS (windows) ? - If the GIL is here to prevent errors when incrementing / decrementing reference count, why can't atomic operations be used ? (I'm thinking of something like the java atomic package here, IIRC it does not always use locks)
In your example you are returning a string from your WSGI callable. WSGI expects a byte string interable. Your example works because strings are iterable, but they yield one char at a time, causing a lot of socket.send() operations in the server and a lot of overhead. You should not d that. Better return a list or tuple with only a single item, which is the string.
Comment the links, say what's special about them. Otherwise it's just a link desert.
I work for a company that manufactures wireless routers with 3g/4g connectivity. We use python on the router itself and for our web based management platform. We also use python for all out in-house automated testing harnesses.
Well I guess it's a bit more portable this way?
&gt; &gt; &gt; This isn't an issue if your shared data is stored in a handful of big bytes objects, but in the more typical case of a complex graph of small-to-medium-sized objects, this means that the OS won't actually be able to share the memory between your processes (even if it is logically immutable from the application's point of view). I actually also work with NumPy arrays often, and that's exactly where I'm having these problems: sometimes I want to split my computations across different threads, such that e.g. each thread can read the whole matrix, but only writes into a few columns/rows assigned to it. Is there some obvious solution to this problem that I'm missing?
Yup, you're right
It really depends on what you're working on. Personally, I do a lot of HPC, where scaling over multiple servers is often worthless because the communication costs will kill any speedup that you have by scaling up in the first place. In these scenarios, where your program is running on one very large supercomputer, multithreading is usually the way to go, yet multi-processing is often looked down upon, since it usually wastes memory unless you're sometimes willing to forgo clean code (by using global variables, sharing state via files, using less natural ways to represent your data, ...). If you're doing web-apps, things are of course very different.
Thanks for your feedback. configurable paths + non-root was something I didn't think of myself.
* copy-on-write does not work with reference counting, since you write to each object you see (even if it's only a reference count) * atomic operations are expensive on CPUs because they need to be synchronized (this depends largely on the exact CPU architecture but it's *far* more expensive than a simple memory write that hits a cache). * there are other GIL problems than just reference counting. You need to add careful locking to all mutable structures in the interpreter (dict, set, list, objects) and make sure it works fast and well. PyPy does not have reference counting and removing the GIL is not trivial. To be precise, we never found a way to do so without pervasive and hard to debug changes all over the place in the interpreter, which we decided would not be worth it.
Yeah, got it. I wanted to keep it simple that's why overlooked it, but shouldn't have. Thanks for bringing it to notice. For those who want to know why it should be an iterable of byte string as pointed by defnull, can read http://www.python.org/dev/peps/pep-0333/
And what do you suggest for consuming SOAP web services? I didn't test the performance, but I do use a WSDL one
I'm not saying no one ever needs it - I'm saying the cases are a small portion of projects in general, and often in areas where Python isn't necessarily a good tool for the job anyway.
I agree about GOMAXPROCS. The portion of Go programmers that are unaware of GOMAXPROCS and it's implications is somewhat alarming.
so rather than push the development to make python a good tool, you're happy to go 'eh, whatever.' and just let it be? 
We probably work in the same building. I'm doing GPU related research, so only CUDA C and Fortran here.
You mean http://www.python.org/dev/peps/pep-3333/ :)
Python is a good tool, for different (the vast majority) of tasks. I'm not saying don't try and work on it, but the reality is everyone I've seen has said that removing the GIL will slow down Python in general, and take a lot of work. Is it worth that trade? If someone can remove the GIL and not hurt other areas, then excellent.
If you're not writing much data per worker, why not just create new arrays for each subresult and then combine them? 
then don't take out the GIL. use multi-processing and let each process have its own GIL. a lot of the overhead then gets sucked away. it's not ideal but it's a start. the greenlet/microthread tools brought over from stackless go a long way towards making concurrency problems minimal. there's ways, and a lot of folks have done a lot of work. python's lock was a dogmatic approach taken when it was created, and honestly the attitude of 'well that's the way it should be' is irksome. main reason I even replied to you to begin with. 
look into the safe thread project. it's fine-grained thread-locking in a patched interpeter for python that may cause individual threads to run in the interpreter slower, but since you can then spread the safe threads across multiple processors, you can then run your stock python about 1/3 FASTER than it would have without using it. 
Isn't dividing the links into sections good enough ? People would get the idea about what the links were about. What do you think ?
Actually, I meant pep-0333 only. You can find a section if you search for "the application object must return an iterable". I wanted to refer to this section. But I guess there is more info on pep-3333, going through it. Thanks for this link :) Also edited the blog to clarify this.
Yeah, that seems a little inefficient. He could also have just loaded it via a local .html file.
Why down vote and not tell why? That's completely not constructive to other redditors interested in the article. 
Welcome to Reddit.
What about Anaconda Accelerate w/ NumbaPro for Python-to-GPU compiling?
From [here](http://www.reddit.com/r/compsci/comments/1e66ko/how_relevant_is_computer_science_to_careers/c9xgdxs) : http://www.payscale.com/college-salary-report-2013/computer-science-and-math-jobs
&gt; I would argue saying "a lot of code is I/O bound" is a more dangerous generalization than explaining that none of the libraries mentioned directly affect the GIL. That was the straw man argument that I pointed out. That the GIL is a big enough problem. I say it isn't. And anyone who claims any of those libraries "remove" the GIL is just misinformed. Rebutting that argument is setting up a straw man and then proceeding to demolish it. &gt; What's more, the GIL is released for nonblocking I/O tasks, so it's far less of a problem for I/O bound programs to begin with. Wait isn't that what I was commenting about to start with. The GIL isn't that big of a problem because large number of concurrent applications are I/O bound. So not sure why you added the "What's more" part &gt; I never said that using asynchronous messaging libraries is not useful. Sorry, for confusion. I didn't say you said that either. The argument around green thread libraries isn't that they "remove the GIL or not" but that they do a better job at handling concurrent I/O than Python threads (which can handle I/O tasks in parallel, unlike say CPU bound tasks). &gt; For CPU bound tasks and tasks which are both CPU and IO heavy, the GIL is something to be aware of and there are no 3rd party frameworks or libraries that change that fact. I agree with that. And I couldn't see what explicitly your audience is. You might have had computational sciences in mind but your article just got posted in general /r/python rather than in /r/scipy for example. That is why the argument whether GIL is really a big problem or not. I still think, even with a large scientific community a lot more people handle I/O bound tasks (push data over the network, download, upload) "every day". And maybe it is a chicken and egg problem. Imagining Python didn't have the GIL, I am trying to see how much more popular it would be. What would it replace? C++/C? Fortran? Not sure. Somehow I suspect many would still descend down to C for raw speed or use numpy (which I suspect already releases the GIL inside the C code). 
Correct me if I'm wrong but I thought that it was the job of the server to treat strings as byte strings in PEP 333, so it doesn't matter. It does matter in PEP 3333 though because the server is supposed to treat everything it receives as byte strings.
The UDP listener is the easy part, there are enough examples on the net, but I'm a true greenhorn on the UI part, so thank you very much for the hint.
Not to be overly critical, but anytime an acronym is involved in a new concept I am trying to learn, it bothers the heck out of me when the instructor fails to clairfy the acronym prior to using it repeatedly throughout the lesson. Just some friendly feedback. 
Nice link, thank you very much.
&gt; then don't take out the GIL. use multi-processing and let each process have its own GIL. a lot of the overhead then gets sucked away. My point, further up this thread, is that there are significant and important classes of program where you simply can't do that.
such as? let's have some examples, yes. 
You miss the point. Technically returning a string is correct under PEP 333 and 3333, because it is still iterable, but it is the most awful thing you want to do because it will kill the performance of any web application returning large responses. This is because it is an iterable over strings of one character in length and a WSGI server is required to effectively perform a flush of data back to the client for each item yielded from the iterable. Unfortunately there are some WSGI servers (uWSGI from memory), which think it is a good idea to automatically correct such awful code and still send it as one string. Doing so is against what the WSGI specification requires. The issue then goes by with out you realising. When that application is then run on a compliant WSGI server your performance goes to crap. At this point, some people will complain about how awful this other WSGI server is when it is technically their own code which is the problem.
[Vrui](http://wiki.cse.ucdavis.edu/keckcaves:lidarmanual#install_vrui_and_lidarviewer) is a powerful tool for viewing massive lidar point clouds ( &gt; 1E9 points).
The usual: pyvisa and pyserial for 90% of the instruments; urllib for others (like e.g. an Agilent 9030A Spectrum Analyzer to collect trace data) and sometimes ftplib or the socket module for raw telnet-like Command &amp; Monitoring. We also have our own VME modules to support. There are DLLs for low-level access that we should be able to interface with using ctypes, but that only "works" for a few modules, plus the software guys (who should support the AIT department) are swamped in external projects. We can still use Delphi for VME access though, not as convenient but workable. 
I had a problem with this "error: unrecognized command line option '-mno-cygwin" two years ago. I asked in several places including StackOverflow and contributed with a recipe, Just in case it could be useful to somebody: http://stackoverflow.com/questions/6034390/compiling-with-cython-and-mingw-produces-gcc-error-unrecognized-command-line-o
I'd use a virtualenv and install the latest scipy
While I appreciate the idea, running a virtual environment is not repeatable for most users (there is a hope that my code will be used by more people than just me). I would prefer to use whatever the standard is for my grid-computing network.
I think `curve_fit` is just a wrapper for `scipy.optimize.leastsq` (which is itself a MINPACK wrapper). You should be able to replicate it with something like: fitted_params = sp.optimize.leastsq(lambda xdata, *params: f(xdata, *params) - ydata, [1] * num_params) where you're setting up the "error function" yourself. You may have to fiddle with how the parameters are passed in a little though. Also, /r/learnpython might be of assistance in the future.
and GIL is not Python's hardest problem yet. 1. ctypes with C module could release GIL 2. IO blocking also release GIL GIL is over-hyped 
I have heard good reviews, thought I would try it out. What would you recommend using for predictive analytics and machine learning with python?
You should include the command you entered to get that result, rather than expecting everyone to read the tutorial and guess which one you entered. Include your OS as well.
FWIW, also see https://github.com/GrahamDumpleton/wsgi-shell
I'm currently using Apache and mod_wsgi alongside mod_auth_kerb. It's not the new hotness, but it works for now and allows compliance with our "must use Kerberos" policy. I suppose it is possible I could use Apache as a proxy in front of Nginx and forward Kerberos authenticated requests, but that provides a good deal of extra complication and sort of defeats the purpose of Nginx.
4 lines at https://gist.github.com/anonymous/5906387
Link to distribute's install instructions. It seems that there may possibly be more to the process in Windows then the tutorial explains. http://pythonhosted.org/distribute/easy_install.html#using-easy-install I don't use Windows on my machine, if you were looking for something more detailed maybe someone else can help. 
Wakari may help. It is an online platform for Python.
https://pypi.python.org/mirrors [EDIT] **http://www.pypi-mirrors.org** is a good list of mirrors and their statuses. [EDIT] [PEP 381: Mirroring infrastructure for PyPI](http://www.python.org/dev/peps/pep-0381/) ([src](http://hg.python.org/peps/file/tip/pep-0381.txt))
http://docs.continuum.io/conda/intro.html manages a stable set [of packages](http://docs.continuum.io/anaconda/pkgs.html)
&gt; Whishlist: Be compatible with eventlet/stackless (provide alternative implementation without thread) Hopefully it will be made PEP3156 compatible or something like that.
So, basically, the whole mess went full-circle ... I'm guessing this is the relavent documentation for the merge?: [distribute-setuptools merge](http://pythonhosted.org/setuptools/merge.html) Overall, is this then the effective/canonical documentation for setup.py?: [setuptools developer's guide](http://pythonhosted.org/setuptools/setuptools.html#developer-s-guide)
Yes, that's basically it.
Thank you very much for the assistance. It doesn't help much that even the [newest(est) official Python documentation](http://docs.python.org/3/index.html)'s "[Distributing Python Modules](http://docs.python.org/3/distutils/index.html)" subsection refers to a barrel of completely outdated nonsense ...
Unfortunately the outage affected the mirror list too. Hello, irony.
Both they are irrelevant now and (in the idea of the authors) superseeded by the wheel format .. replacing the python eggs. In short in most cases (pure python modules/package) all you need to do is copying the package dir (or themodule) in a directory reacheable by PYTHONPATH and ingnore the whole mess ;)
Not painful, but is something the average Joe cannot do. The problem is there are many companies in this game and their hope is to make money out of it: as Sun with Java has proven that is not where money will be put. 
If they don't plan a back port is pretty much DOA: 2.7 is the reality *now*.
I'm on 3.3 and I don't really have any problems at all. What do you use that still needs 2.x?
It's not me. Companies are using 2.x in production: switching to 3.x is simply not viable on a large scale projects.
That's great, but many businesses out there use 2.7 and it works fine; why upgrade? Why risk it? Why spend man hours ensuring that third-party libs and internal frameworks written by other teams are up to scratch?
I tried to set it up to run when getting exceptions...but it's not working for 'not breaking' exceptions like template not found...it just shows a balnk page...any idea why?
are you sure the command you run is actually python distribute_setup.py install and not just python distribute_setup.py ?
Why package something for 2.7 if companies don’t want new stuff? I mean, if stability comes first, then you don’t upgrade your software in the first place. And for new things, you can always have a python3 install right alongside python2.
That is essentially what I will be using python for. To be more specific I was going to use PyDev in Eclipse. But just as I am testing it out I start hearing great reviews about how IPython notebook is better. Your thoughts?
That guide is outdated and too long. This document is much better: http://www.scotttorborg.com/python-packaging/index.html
You should consider using a dedicated IDE, such as PyCharm. You can buy it (commercial use) or help an opensource project (OSS development). It automatically configures everything on Windows and most stuff is manageable through IDE's GUI.
JPype, PIL, Twisted
There are actually multiple cron implementations. There is anacron, bcron, dcron, fcron, mcron, and vixiecron. Gentoo supports 5 different cron implementations alone. 
Cheers!
scipy.optimize.fmin_cg usually gives good results if you have the gradient.
It has nothing to do with your tutorial. Web2py is not reddit's darling.
proprietary? thanks, no
If you have a better editor than Eclipse (i.e. vim, emacs), *use that*. If there's no advantage to you *not* using Eclipse, just stick with it. My point here is that using Eclipse gives you less of an adventage in Python since you have dynamic typing and the REPL (at the very least), so if you have something that's more effective at text editing and less at understanding code, you'll end up gaining more by using it.
It does pip and virtualenv for you? Sweet.
Isn't as good, it does not have support for framework, which is a big feature in PyCharm (like Flask support). Also the Project management in VS isn't really good if you like to do the work with many editor or have other team member.
Found this chart showing some options, but some personal experience opinion is always better... http://quintagroup.com/cms/python/ecommerce
There's a great chapter on [Python Packaging in *The Architecture of Open Source Applications*](http://www.aosabook.org/en/packaging.html)
Yep
Parse: https://pypi.python.org/pypi/xunitparser Chart: http://www.reddit.com/r/Python/comments/1ankk4/is_python_a_good_tool_for_data_visualisation/
Been thinking of going back to school and doing Math. I am not sure whether that or computer science (or engineering, as I am a mechanical eng.) is a better option...
Distutils and pkgutil are not deprecated, where did you get this idea from? 
I have had a great experience with [Satchmo](http://satchmoproject.com). It's a do-it-yourself sort of framework -- definitely not batteries included. It was just the right fit for running my [mother's company's ecommerce website](http://charlesmayer.com).
However since that was written distutils2 has been cancelled and the mirroring infrastructure described has been replaced by a CDN, the metadata format will change to a JSON-based format, and setup.cfg is not likely to replace setup.py.
Back when I chose Satchmo, [Django Shop](https://github.com/divio/django-shop) was not quite ready yet. It's probably worth checking out now that a few years have gone by.
you dirty hippie
&gt;Not painful, but is something the average Joe cannot do. I'm not a Python programmer by trade, but I do plenty of packaging in a variety of other languages/platforms ([the usual boring] Java JARs/WARs/EARs, lots of Debian source/binary inc. repo setup/hosting, etc). I would say that I have a quite strong understanding of packaging and a diverse range of experience, yet, I still find distributing Python code to be like trying to store lightbulbs in my rectum.
Hi, Maybe: nosetests --with-xunit
I shudder to think how many hours I have spent with this on my mac. I have basically given up. Windows seems much more agreeable, but still a bit of a struggle to deal with. I guess the people who can fix these things are smart enough to figure out packaging the current way and therefore it isn't much of a priority. Which is strange considering how many python for kids books etc are about - good luck installing pygame for python 3 on your mac little johnny.
If you want to use an external process, mpg123 can do this - just `mpg123 $stream_url`. Tested on Linux and OS X.
VS12 is really nice to develop in, generally imo. The Python integration (particularly the debugger support) is excellent. 
I've been working in PyCharm and PTVS interchangeably for the last couple of months because I needed to support engineers using both IDEs for a framework I was constructing. I've worked in PyCharm for about 9 months now, and I definitely prefer it, but I think PTVS does most things nearly as well. My biggest complaint about PTVS is that it doesn't play nicely with VCS. Every time new code gets submitted to a project, people have had to reconstruct projects. I looked into generating a project file dynamically, but .pyproj files don't support CMake, and I desperately did not want to try and generate project XML from scratch. The only other complaints I've really had with PTVS are edge-cases where it has played poorly with code that wasn't in core python or the project files of the project itself (which really is just an extension of the above). On the other hand, if I was making a choice about what IDE to use at home for a project (bit late for that, since I've had my own PyCharm license at home for quite a while), I'd probably use PTVS, since it is free for effectively equivalent functionality to PyCharm.
I was looking to do it as part of the same process, hence gstreamer. Basically want to create music player
You could try including all the files in the directory in the pyproj using: &lt;Compile Include="**\*.py" /&gt; This will just recursively include all .py files in the directory structure in the project. That might make it easier when new code gets added, but when adding via VS a new item will get added even though the wildcard exists. So removing files might start to get annoying, but it might be a better balance overall.
I recently made a library to do entire lazy lookup objects for a project I was working on. Not sure if it has many practical uses, but it was fun to make. (Based on ProxyTypes library on PyPi) https://github.com/gazpachoking/jsonref/blob/master/proxytypes.py#L198
Closed source but free. Links are in the article. 
I'm actually in the process of making a Raspberry Pi music player this way (warning: not very elegant). To play a track (which the user selects from a list of tracks on Soundcloud), it creates an mpg123 sub-process using that song's `stream_url`. I keep a reference to PID of the "now playing" process, so that to stop a song I can just kill it. So the application process only handles fetching and displaying the track data, while the mpg123 sub-process handles all the streaming and playing work.
Also, checkout https://github.com/lmacken/pyrasite
the CND does not replace the mirroring. look at what happened lately http://www.reddit.com/r/Python/comments/1hh211/python_package_index_down_for_anyone_else_seems 
you don't need that page. mirrors are http://&lt;LETTER&gt;.pypi.python.org/simple/ with LETTER from A to G. see http://www.pypi-mirrors.org/
This should solve most of the problems, yeah. It won't cover if someone adds a new external library to code not directly in the project - but that happens significantly less regularly, so I don't mind handling that when it happens.
i dont understand what the problem is, in a simple program which i used multiprocessing, i just had a threading.Array and it worked fine. You just have whatever data needs to be shared between "threads/processes' be a special type of object, you say its a nightmare but don't give any examples of why its a nightmare. You don't have to have 'everything' be i a threading.Array/Value object, so i'm confused on why you say you need to now manage 90% of your data a special way
&gt; You just have whatever data needs to be shared between "threads/processes' be a special type of object [...] &gt; i'm confused on why you say you need to now manage 90% of your data a special way I need to share 90% of my data across 2 processes. Either it all needs to be a special type of object, or it does not. There's a world of difference between simple programs using multiprocessing, where you can usually just partition your data and fire off 2 processes, and large, complex programs that need to make use of multiple cores, where you want select parts of your system to run at the same time, often doing completely different things, but referring to some amount of a large amount of common data.
as if we needed one more thing to encourage people to mistakenly use UDP.
PTVS runs on the free (as in money) version of ~~VS express~~ VS Shell. And yes, VS isn't OSS, and yes, neither is Windows, the OS upon which VS runs. And neither is the hardware that Windows runs on, or linux for that matter. Maybe one day we can get Arduino computers powerful enough to run GNU\Linux and that can support an OSS python IDE. Until then I will have to compromise somewhere.
SciPy 2013 had an interesting talk about Conda and BinStar.org as a packaging solution. 
Now if you're developing for the web, I'd suggest you look at Visual Studio as well for the **insanely smooth Javascript development**... things like real-time code execution in the background, e.g. [**things like this magic trick**](http://i.imgur.com/h2nKSFP.png) It has made Javascript my favorite language to work with, something I really hated before using VS2012. ...for added bonus points, check out Typescript!
looks pretty interesting - is there a linux version?
What do you suggest this will be mistakenly used for?
That's pretty sweet! Does it also package all the stuff in site-packages too?
Have you started the wdb server ? 
networking applications.
I'm curious what the motivation for encoding hints on byte strings here is. Seems like something you can tote around separately.
you kept saying 'it's not great for all applications' without getting into much detail. you also don't list the specific obstacles, or how another language overcomes them. basically you just sit there and poohpooh python's weak areas rather than actually, you know, CONTRIBUTE. but hey thanks for playing. 
I completely agree that the Python Tools are fantastic. I've been using them for ages, and they are great. My problem is this: if you use them with the visual studio shell, you can't talk to TFS. TFS in the cloud (tfs.visualstudio.com) is great, free, and can be talked to by the express editions. Which can't talk to Python Tools. I really like managing bugs, backlog, etc. in TFS, plus of course managing source code there. If VS shell could talk to TFS I'd be very, very happy, but the response of the MS people I've talked to is that there has to be *some* reason to buy Pro. I guess so. :-)
The CDN more or less *will* supersede the public mirroring infrastructure. Currently it's still being phased in however in the future it will continue serving pages even if the origin goes away (which is what happened recently). In the short term no tool implements mirroring in a way that doesn't come at a fairly hefty performance cost. Nor do any of them, to my knowledge, implement mirroring authenticity and the entire thing is all downloaded over HTTP. So not only is mirroring going to be more or less redundant but it's currently very insecure and is practically asking to have your computer execute arbitrary attacker controlled code. To my knowledge the only public mirror where you get _any_ verification as the tools are currently written is Crate.io due to the TLS. Furthermore even if the mirror authenticity was being checked it currently uses a DSA signature. One of the issues with DSA is that if you re-use, make public, or use a predictable value for k when generating the signature then an attacker can recover the private key. PyPI is currently running on a VM, is there a good source of random available to it? I don't know. Has there always been a good source of random to the signature algorithm? I don't know. Is there any protection against someone mass editing their package, generating a ton of signatures and playing the odds that there will be a collision? I'm pretty sure there isn't. Furthermore there are no protections in the mirroring algorithm to protect against an attacker attempting to force an inconsistent state in the repository, or preventing an upgraded package that contains a security fix from being synchronized to a mirror (and thus preventing anyone using that mirror from getting a security update). Now there are also numerous security issues that plague PyPI itself, which myself and others are working to remove. Eventually the mirroring system will be revamped so that it doesn't suffer these very catastrophic failures but until they have been addressed advocating folks to use the public mirroring infrastructure as it stands today is asking them to put their hand in a bear trap.
[Compoze](http://docs.repoze.org/compoze/) is one way to maintain a local [package index](http://docs.repoze.org/compoze/glossary.html?highlight=package%20index). * Local package index (folder w/ `index.html`) * Local PyPi clone (eggbasket, chishop, [...]) * Partial PyPi mirror (caching proxy) * Full PyPi mirror I am not sure whether these support [wheel (PEP 427: The Wheel Binary Package Format 1.0)](http://www.python.org/dev/peps/pep-0427/).
There is a fine line between "deprecated" and "not the future of packaging". distutils is still actively maintained, it is just that most forward-looking dev happens in setuptools now.
Wheel is an installation format, it will be supported by every version of Python since after installation nothing knows the diference. bdist_wheel definitely runs on 2.7 for building them, and pip will be shipping installation support later this week (and I think they support back to 2.5 or something silly).
Wheel is being implemented externally to the stdlib. It will be available for 2.6+ I believe, might be 2.5+. In fact one of the major motivations in a lot of the work now is that packaging tools should live externally of the stdlib. This will allow them to innovate across many versions of Python and not cause issues exactly like this one where a new packaging feature is useless because it will be many many years before that version can be assumed a minimum version. Essentially we've learned that the stdlib isn't a great place for packaging tools, however the stdlib will (hopefully) include a simple script which will serve to simply and securely install pip so that Python ships with the ability to do ``pip install foo``.
Go to events like OSCON (http://www.oscon.com/) and network. I scored a job at Netflix two years ago by posting to Twitter with #oscon. Conventions like that one have job boards, recruiters, and people who are happy to hook up anyone who has a good attitude. Also, get a good LinkedIn page going and some business cards with a link to it. 
It's not clear to me if you can install the Visual Studio Team Explorer over the VS Shell and get what I'd like to see. I guess some exploring is in order.... http://www.microsoft.com/en-us/download/details.aspx?id=30656 Also, this is the list of known issues for VS2013 so far: http://support.microsoft.com/kb/2848395/en-us ...which makes me wonder if I should just stay with the (excellent) 2012 versions.
Pip should [validate SSL certs by now](http://www.reddit.com/r/Python/comments/17rfh7/warning_dont_use_pip_in_an_untrusted_network_a/). http://wheel.readthedocs.org/en/latest/#automatically-sign-wheel-files
&gt; you kept saying 'it's not great for all applications' without getting into much detail. http://www.reddit.com/r/Python/comments/1hdr9v/pythons_hardest_problem_revisited/catqdwh - *"audio, video, image processing, and gaming apps. These neither work well in one thread nor scale well to multiple machines (or indeed processes)."* &gt; ...or how another language overcomes them ...by not having a single global lock that prevents multithreading. 
It looks like mirrors *b* and *d* are offline and each mirror has a different update latency.
If you're using pip 1.3+ then it does validate SSL. However the mirrors are not available via SSL.
Also signing packages is effectively useless until you come up with a trust model to handle what signatures you trust to sign for what data. Without that you're just pretending it means something.
Microsoft's Visual Studio? No. No there is not a Linux version. :p Check out PyCharm though.
Yes. It creates a completely self contained program in the form of a directory containing an exe and lots of associated files 
What, for Visual Studio? No
&gt;And yes, VS isn't OSS, and yes, neither is Windows, the OS upon which VS runs. &gt; &gt;And neither is the hardware that Windows runs on, But the hardware makes aren't monopolies who have historically done many things that have resulted in holding the computer industry back, nor have they chosen to implement secure boot to try to make it harder for hardware to boot Linux nor have they locked down Windows 8 so that all Metro apps must come through them and thus they get to control not only the OS but the software installed on it. 
PEPs and links to DevOps resources for working with Python Packaging and push/pull deployment: [How are python apps deployed to production especially those that are developed in a virtualenv? What are the best practices?](Http://www.reddit.com/r/Python/comments/1bx3vj/how_are_python_apps_deployed_to_production/c9b5tea)
Can anyone explain how to setup the Python interpreter with Enthought's Canopy distribution such that IPython can be used for the interactive shell? 
 [schema.org](http://schema.rdfs.org/tools.html) HTML microdata support helps search engines index [product metadata](http://schema.org/Product) directly from the product listing HTML. It might be cool to add an attribute for http://schema.org support to this [e-Commerce criteria/feature matrix from django grids.]( https://www.djangopackages.com/grids/g/ecommerce/). There are varying levels of support for integration with [ERP systems](http://en.wikipedia.org/wiki/Comparison_of_Tryton_and_Open_ERP).
Have you tried TortoiseSVN?
Someday WINE will.
One of the motivations for wheel is that its recommended workflow saves a local .whl copy of all the packages you've installed to a directory you can install from later, avoiding the infuriating and silly need to contact pypi when you just want to recreate a virtualenv.
Satchless might not interest you, but perhaps the community at large. Satchless is code factored out of Satchmo, and provides a cart, and a step-process manager (e.g. add-to-cart, add-billing, confirm-order). https://satchless.readthedocs.org/en/latest/index.html
For passing data is expected to get to the recipient. 
I believe you can actually play with the parameters (`bundle_files`?) to get a single EXE with everything inside.
What a fantastic conference.
It was remarkably packed with cool projects and smart people. I'm very glad I went. I was particularly happy with all the melding-of-minds amongst embedded compiler folks. It turned out that (1) there a lots of people writing/using embedded JITs in Python and (2) a lot of our designs/problems&amp;solutions are very similar. 
Yes, I'm one of the main authors of [conda](https://github.com/ContinuumIO/conda). The approach is radically from the Python world (of pip, easy-install, distutils, ...). This tool is **Python agnostic**, which gives much greater power. I have the feeling the Python packaging community has driven themselves into a ditch, by putting Python at the center of everything. They treat Python as the underlying OS, and build on top of that. However, what if you want to create an environment with Python 3.3, or Python 2.6? A package manager shouldn't care about what it is you install. The new wheel package format is a classic example of this: You can create wheel packages of Python packages, but can you create a wheel package of Python itself?
[Buildout also installs from a local `download-cache`](https://pypi.python.org/pypi/zc.buildout/2.1.1#using-a-download-cache) that can be worked into a [package index](http://docs.repoze.org/compoze/glossary.html#package_index) with an `index.html` containing URLs with #checksums. I hadn't heard that wheel has this capability. [`bdist_wheel`](http://wheel.readthedocs.org/en/latest/) does look super easy.
Is there a way to work with [*conda* packages](http://docs.continuum.io/conda/build.html) in configuration management tools like [Puppet](https://github.com/puppetlabs/puppet/blob/master/lib/puppet/provider/package/pip.rb), [Chef](https://github.com/opscode/chef/blob/master/lib/chef/provider/package/easy_install.rb), [Salt](https://github.com/saltstack/salt/blob/develop/salt/modules/pip.py), and [Ansible](https://github.com/ansible/ansible/blob/devel/library/packaging/pip)? [EDIT] src links 
Now if only Windows wasn't a totally awful operating system to use. Tiling? Good luck. Decent terminal? Get outtt.
This does seem comprehensive. TIL about utilizing http://jeanphix.me/Ghost.py/#capture for generating HTML5 visualizations, for print.
How do you feel about [`passlib.utils.saslprep`](http://pythonhosted.org/passlib/lib/passlib.utils.html#passlib.utils.saslprep) and [`stringprep`](http://docs.python.org/3/library/stringprep.html) in re: [A security hole via unicode usernames](http://www.reddit.com/r/programming/comments/1gl0zn/a_security_hole_via_unicode_usernames/cali4b9)?
Much of the complication is self inflicted (distribute/setuptools/pips/easy_install/wheel). If your app (driver + support libraries) doesn't contain extension modules (.so/.pyd) just zip everything togheter with a __main__.py file and it's even easier than a jar. &gt; I still find distributing Python code to be like trying to store lightbulbs in my rectum. (@_@)
&gt; Unicode is still hard, and in my experience it's not much easier on 3.x than it was on 2.x. That is discouraging to hear. I was motivating myself to make the switch to 3.x with the argument that Unicode handling would be more logical...
I'm a django dev, and I tried this out yesterday. Installed just fine. When I clicked to start a new django app it said I didn't have IIS to run the web server. Uninstall.
I use AnkhSVN as a drop-in source provider. It does just fine with adds, removes, renames, moves, etc. 
Python's regular expressions are missing unicode character category matching for the longest time. That's especially annoying on Python 3 where Python identifiers are unicode based. For Jinja2 to work on Python 3 I needed to write this abomination: https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_stringdefs.py
I am still baffled by [`unicodedata`](http://docs.python.org/2/library/unicodedata.html) and [`stringprep`](http://docs.python.org/2/library/stringprep.html).
&gt;I can only make an educated guess - and that is that Python allocates a segment in the memory for code and that that segment is somehow 128-bit-aligned so that the lower 3 bits of the hash (which is (id/16)%8 as we recall) is always the same. Normally, the whole address being equal wouldn't be a surprise - programs are deterministic after all so given the same starting conditions and the same program, you'd expect exactly the same thing to happen every time. As such, the question shouldn't be why the low bits are the same, but why the high bits are different. The answer here is likely due to the fact that generally, for security reasons, address space allocation is randomised, so the start of the heap will be different from run to run, just aligned to 128 bytes as you see. The reason the behaviour becomes different though is likely down to the pattern of memory allocation that occurs. Normally if you allocate a bunch of objects, they'll be created in sequence in ascending memory addresses. Things get more complex though when you mix in object *frees*. Ie if I allocate A, B, C,D and then free C, memory is going to look like this: [ A ] [ B ] [ ] [ D ] If I then allocate E, so long as it was the same size that C was it can fit in the "Hole" that was left behind. Indeed, you'll generally see that immediately after freeing something, allocating the same type of object will re-use the just freed memory, which can lead to some puzzles like: &gt;&gt;&gt; [] is [] # False (always) &gt;&gt;&gt; id([]) == id([]) # True (depending on implementation) Which can seem mysterious if you don't realise what's happening - how can it have the same id if it's not the same object? Anyway, all this could cause the differences you see, because before executing that code, python parses it and compiles it into bytecode etc. Adding some code to it (even stuff that isn't executed) can mean that there's a bit of extra work at this stage, meaning maybe a few more allocations and deallocations which can lead to a different pattern of memory by the time we get to running it - ie different "holes". As such, some of your classes may now fit into a reused part of the heap, while later ones get allocated from brand new address space, resulting in different relative memory addresses from before. Also, note that this is all more to do with id() based hashes not being predictable than the hashtable itself. It's also important to note that even when the hashes of your object are fixed and unchanging, you **still** can't rely on the order of dictionary keys (though it'd be unlikely to break with a similar scenario as you describe). A different order or insertions / deletions might mean the dictionary does / doesn't expand the number of buckets it uses, changing the order of items. &gt;If you do like to have a dictionary with a predictable and reliable order, use OrderedDict instead. Well, for certain meanings of "predictable and reliable". OrderedDict will return items in the order they were inserted, not comparison order. This means you can't guarantee "A,B,C" will be returned in that order just by knowing they're present in the dict - you'd also have to know what order they were inserted.
For those familiar with or want to use VI in a Window$ environment. I find Eclipse + PyDev + Vrapper and VisualSVN work really well together.
http://imgur.com/oOXk6Ig
Awesome^2 Thanks for the reminder.
A [comparison of e-commerce packages](https://www.djangopackages.com/grids/g/ecommerce/) based on the Django framework. [Oscar](http://oscarcommerce.com/) seems quite promising.
Hi ilan, just wondering, would Conda be the tool to use in such a scenario? - a project is a set of Python packages under a common buildout config - a Bash installer script installs system-wide prerequisites, runs virtualenv and installs these packages along with their Python dependencies under a newly created virtualenv This is basically what these scripts do https://github.com/zatosource/zato/blob/master/code/install.sh https://github.com/zatosource/zato/blob/master/code/_install-deb.sh Now, a couple of things to consider - on Debian/Ubuntu/Fedora/OS X packages are named differently, are missing or different tools are used to install them (apt-get/yum/brew) - many packages are missing in RHEL and SLES, starting with Python 2.7 itself through zdaemon and others ending in HAProxy - there needs to be a clever way to distribute patches and hotfixes without new releases - patches as in actual diffs will do - it must not be something that requires a new PyPI release because sometimes a patch to one of the dependencies (in Python or not) is needed - there should be a way to create fat installers that bundle absolutely everything that is needed to run the project on each system supported, so you have RHEL and download a 300MB installer that contains everything, needs no compiling, no network connection and can be run straight out of the box I'm looking in various directions for a thing to cover it all and perhaps Conda is the one? Thanks!
How are Unicode strings represented internally in Python? Doesn't it pay to have all library functions basically take Unicode everywhere, and that we only use specific encodings for importing and exporting strings?
&gt; This is a vague statement. I am not sure why you're saying using a mirror has a 'hefty performance' cost. pip works very well with mirrors. The way pip implements mirrors it multiplies the number of HTTP requests required to discover (not download) the number of packages by the number of mirrors. That means if you install a single package that does not need to hit external urls, then instead of a single HTTP request you need 7 HTTP requests, if you're installing 20 packages you need 140 HTTP requests. &gt; errr what ?? what's that ? are you talking about a ddos attack ? that's yet another vague statement you're doing here :) I'm talking about any attack that has a mirror witholding an update. It could be someone using a DNS attack against a mirror that prevents them from fetching the updates, it could be someone compromising the mirror all together. Any attack that prevents the mirrors from getting a particular update. &gt; I'll just stop here, your message is just a random bag of FUD criticisms you could do on the CDN as well - or anything. Actually none of the points I mentioned that are specific to the mirrors exist for the CDN because, well, they are specific to the mirrors. I did however mention there are _other_ issues which plague PyPI itself, which by nature apply also to the CDN, and the mirroring infrastructure itself because any insecurity on PyPI cascades to all things. &gt; It looks like your goal is to promote the CDN and attack the mirroring protocol to make it look bad, which is weird because both can cohabit for the benefit of the community. No my goal is to have reasonably secure installations that are relatively quick and painless. The mirrors currently have exactly 0 assurances in place as to security (Fun tip, using --use-mirrors at PyCON EU means anyone on the conference wifi can execute arbitrary Python code on your machine). The mirrors currently impose a multiplier of 7 for HTTP requests in a system that already contains a ton of HTTP requests. &gt; Last, I encourage you to reformulate your vague statements into real points to enhance the mirroring PEP instead of what you are currently doing here. After I have PyPI itself reasonably secure (which it's getting there), mirrors are next on my list. It's unlikely i'll be enhancing the current mirroring PEP because it's fundamentally flawed in a way that cannot be fixed with enhancements. I have some rough ideas but nothing solid yet but the new mirroring protocol will ensure consistent snapshots in time, will protect against things such as DSA private keys being leaked due to bad random, will protect against malicious mirror operators even. However that is a future project and until I can look at mirroring and say "Yes using this is not exposing people to huge security risks", I cannot in good faith do anything but warn people away from using the *public* mirrors.
This provides a bit more than just a flimsy wrapper; it also helps the developer manage events on the server or in the browser. It includes a helper that attaches to the request object, an event listener to update the userid for tracking (e.g. registration/login), and a Jinja2 template to identify and track in the browser with zero overhead. I condensed this out of my project yesterday into a library, so that 1) other's could contribute, and 2) to motivate myself to write the tests. pypi: https://pypi.python.org/pypi/pyramid_analytics/0.2
It is way more logical. The problem is that a lot of code today tries to maintain dual compatibility with Python 2.x and 3.x and this makes things harder. Pure "Python 3.x only" code makes Unicode handling saner.
I'm an QA/Automation/IT/Dev/Ops engineer. Some people call me a systems engineer. I learned Python in 1999 and got my first job because of Python in 2002. Knowing Python got me hired after the dot bomb layoffs, and it has been the key to building my career. I recently did heavy dev/ops work for Livefyre, and am now more of an automation engineer / python guy for Cumulus Networks. We just came out of stealth mode and we're doing something really different in the networking space: http://www.theregister.co.uk/2013/06/20/cumulus_networking_launch/ More than anything I love teaching. If you are a junior-ish to mid-level Python/test/automation/ops engineer that is looking for a great opportunity to join a Linux/C/Python/Networking shop in with masters of each to learn from, please PM me.
AFAIK, there isn't any clean implementation of this. Browsers are made to show the output of HTML files, if you want to see the HTML you open the file in a text editor. Or, you can use the option your browser gives you to view the source of the HTML file. May I ask why you would need this functionality so I can understand your question more?
Just like other tools, some folks will use it badly. But, this is a well-made and useful tool, despite what it gets used for. Blame the killer, not the knife.
I can envision a whole class of solutions here, where instead of creating a custom display portion of your program, you could simply "print" the output on the browser. Need a graph of some data? Well, you could spend an hour crafting a window, or you could just tell the browser to display it. Need to show a table? Ditto. Text? Yep, same story. With HTML5 and OpenGL in the browser, displaying data (Which could then be explored) could be very simple.
I was going to suggest opening a data: URI, but that's not supported on my GNOME-based system, at least. I don't think there is a way other than creating a temporary file (tempfile.NamedTemporaryFile) and opening that.
From my second paragraph: " While I could serve up the web page, then use webbrowser to point the browser at that page, but that's kind of ugly."
How do you feel about making temp files?
You can load a [data URI](https://en.wikipedia.org/wiki/Data_URI_scheme) in a web browser, which would allow you to display HTML and CSS. Note that, in most browsers, the use of &lt;script&gt; tags inside data URIs is forbidden.
Actually, I'm going to suggest another idea. With PyQt (and I suppose PySide), you can embed Webkit into a window. This would allow you to display HTML/Javascript code, and even create HTML/Javascript front-end applications with a Python backend, all in one process, without involving Firefox/Chrome/what-have-you.
what about using AJAX... you could create a holding page with a container div and some ajax to check for changes to a blank html file used to hold your python generated content. Make changes to the blank html file using python and then the ajax will detect the change and reload the div with the updated page. Kind of a horrible hack but I think it might give you the kind of interactivity you're looking for. [code](http://stackoverflow.com/a/7028475) here from stack overflow for checking a file for changes using AJAX, pretty straight forward but you might want to go with jquery or something to make things easier.
A browser has to 'get' something. A browser cannot be 'pushed' code. It just doesn't work like that.
Limited access to the local system is party of the browser's hardening against attack. Every api you expose is another potential attack vector. Instead, you could consider serving a static page with an iframe, either over local http or file:// which grabs content from your application using AJAX. 
Like I said, I considered that, but it's still not the best implementation... I even considered making them on a RAMdisk, but that's the same, and worse. I suppose if I'm stuck I'll have to live with it, but I was hoping to run into a better solution.
That's just silly. A browser allows you to "browse" the rendered result. The fact that the avenue may not be provided by browsers is just a function of lack of foresight, and nothing more.
I cannot see how a tiny server is ugly. * It will take five lines of Python. * It will work consistently across browsers and operating systems. * HTML enters the browser as nature intended. Browsers were designed to fetch resources, not to have data injected into them, so any solution that goes that route is bound to be more complex and 'ugly'.
Browsers got their name because you use them to browse the web, i.e. requesting resources using URIs. Recently it's been [changing a bit](http://en.wikipedia.org/wiki/WebSocket), but the request-response model is fundemental to how browsers work.
Think it through: Suppose every piece of code I generated used temp files instead of parameters? Sure, it could be done, but who would want to do it that way? Well, I'm looking to send the parameters to the browser in the cleanest way possible, and (hopefully!) it won't end up with a bunch of temp files littering the directory.
You consider giving an API that allows another app to tell the browser what HTML to display to be a greater threat than giving the API that allows the browser to be told what page to load? Really? Why? If I could easily tell the browser to run a page that could grab content, then I'd just be able to tell it what content to run. 
You won't have a bunch of temp files littering a directory, they are temporary and get deleted on exit.
If I were going to do that, then why not just create the html file and tell the browser to load it?
That's not true. A renderer renders html and js actions. A browser is a JS engine, Renderer and Requests/response library.
If you define it to be so, but it's a semantic difference at best.
webbrowser.open('data:blahblah') should work.
Not so far, and I haven't found any examples. Have you found any that work?
This isn't consistent with the point of the exercise, which is to get out of the "make a window, then display data" business. 
On my computer (Windows, Chrome as default browser) webbrowser.open('http://google.com') opens Chrome, but webbrowser.open('data:blahblah') opens IE. It seems like Chrome does not register itself as a data: URI handler.
Ditto, but webbrowser.open('data:blahblah') doesn't even seem to work on IE, I get a "webpage cannot be displayed" error no matter what. For instance, this doesn't work: webbrowser.open('data:text/html,&lt;html&gt;&lt;title&gt;Hello&lt;/title&gt;&lt;p&gt;This%20is%20a%20test',0) 
Curiouser and curiouser... This works fine from a browser, but doesn't work from webbrowser.
What's the actual problem you're trying to solve. Avoid the snarky bullshit when replying, too.
No, but I will. But since it didn't work with the other example (Which was already encoded), I don't have high hopes.
When your comment comes with built-in snarky bullshit, it's so hard to resist... 
I tried urllib.quote_plus'ing the string, but no-go.
try setting response mime_type as application/xml or something like that.
The thing is, this is a textbook definition of an X-Y problem. You've come with a question which is for getting help on the *perceived* perfect solution to your problem, but I can almost guarantee that what you're doing is the wrong way to solve your problem. So again, what are you *actually* trying to solve?
Did you read the rest of the comments? If you had, you might have read this one: "I can envision a whole class of solutions here, where instead of creating a custom display portion of your program, you could simply "print" the output on the browser. Need a graph of some data? Well, you could spend an hour crafting a window, or you could just tell the browser to display it. Need to show a table? Ditto. Text? Yep, same story. With HTML5 and OpenGL in the browser, displaying data (Which could then be explored) could be very simple." That's what I'm solving, an entire class of problems. There, is that enough "snarky bullshit" for you?
When you dereference a URL, an HTTP GET retrieves (pulls) an HTML (or JSON, or [...]) document that is then rendered in the browser. A copy of the HTML page is created and loaded into the browser, which renders the document. Which part of the network transport are you attempting to obviate here? [Pull](http://en.wikipedia.org/wiki/Pull_technology) is not [Push]( http://en.wikipedia.org/wiki/Push_technology).
Currently not directly possible with current implementations of browsers. You've been told this before. /thread.
Ummm, it's not ~your~ thread, the participants will get to decide when it's done. Also, you'll just have to deal with the fact that your statement has little or no weight, especially since you've been acting like an asshole from your very first statement. 
 import webbrowser b = webbrowser.get('firefox') # or 'chrome' or whatever b.open("data:text/html;charset=utf-8,&lt;h1&gt;And now go fuck yourself and your s hitty attitude!&lt;/h1&gt;") 
dAnjou, if I want any shit out of you, I'll squeeze your head.
I refer to the [OSI Model](http://en.wikipedia.org/wiki/OSI_model).
I can't see any reason to call a 50-line hack based on `socketserver` "well-made". To make the most obvious criticism: there are no tests for this code at all.
Try it. It works, and works well. The API is thoughtfully made, and easy to use. Don't like it? Feel free to make your own, or use something else. If you've made something that does the same thing and is as easy to use, then bring it out for comparison. 'Cause right now it sounds like sour grapes based on your personal problem with UDP. 
When you tote it around separately, it gets lost. American programmers in particular are incredibly bad at thinking about or understanding encodings - probably because they were lucky enough to have their alphabet encoded in ASCII.
**http://en.wikipedia.org/wiki/Test_automation** There is a W3C WebDriver API specification: http://www.w3.org/TR/webdriver/ http://livereload.readthedocs.org/en/latest/ integrates Python and LiveReload.com [Ghost.py](https://github.com/jeanphix/Ghost.py), [PhantomJS](https://github.com/ariya/phantomjs), and [CasperJS](https://github.com/n1k0/casperjs) all have different approaches.
I don't feel like you're justified in insulting me just because my ancestors had the foresight to make a good choice in writing system. Anyway I've had to deal with both text and bytes rather a lot, even occasionally in the context of the web, and I'm having a hard time remembering a time when I wanted to associate a codec with a particular byte string rather than simply decoding it into text. Can you think of any use cases for this?
First World Python Developer Problems: There are so many good web frameworks you want to try Flask but keep using Bottle, Django, etc.
Gah, PDFs are the worst ebook format (if they even qualify)
Mostly it's relevant when interoperating with low-level C APIs (like the operating system file APIs that Armin is grousing about) or writing a parser for something like XML (where you have to parse the header before you even know what encoding it is). In those cases you need to preserve the actual bytes even though you may be comparing it to text. In a C API scenario you may need to avoid roundtripping the bytes to text because thanks to combining characters or other language features you might end up with a different sized buffer. In a parser scenario you may be looking at offsets into a byte buffer. Obviously the encoding can be tracked on its own, but it simplifies API design and reduces programmer error if it is integrated into the bytes type - in much the same way Ruby and Perl's taint mode (or Django's similar SafeString) does for tracking unsafe data. 
Interesting idea 5long, I can remember myself wanting something like this at times, however I think the approach is a bit too magical and possible a little over engineered, what do you think of this: class Foo(object): add, __len__ = delegate_all('bar', (set.add, set.__len__)) def __init__(self): self.bar = set() It doesn't require a class decorator or any namespace magic and if you look at my implementation its very few lines of code. The code and how I got to this solution is in this gist. https://gist.github.com/tonysimpson/5923632 It also contains an explanation of why I think it's appropriate to use the class methods. Looking at it I'm not sure I'm 100% happy with my nomenclature (could do with a bit more thought) maybe delegate_to would be better (I worry its not obvious enough that the first argument is an attribute name on the object). edit: Plunking example (must find out where that word comes from - sounds cool) class MyDict(object): __call__, = delegate_all('dct', (dict.get,)) def __init__(self): self.dct = {'foo': 42} d = MyDict() # Equivlant to d.dct.get('foo') assert d('foo') == 42 
But but... I *hate* good old HTML!
I think PDFs are the *best*... ever tried reading a technical book full of code and diagrams on Kindle?
Here's a list of [Free and Commercial CDN Service Providers](https://en.wikipedia.org/wiki/Content_delivery_network#Notable_content_delivery_service_providers). It is possible to sign changesets with both [hg](http://stackoverflow.com/questions/11556184/whats-the-purpose-of-signing-changesets-in-mercurial) and [git](http://stackoverflow.com/questions/10077996/sign-git-commits-with-gpg). Both rely on network transport security and a [Web of Trust](https://en.wikipedia.org/wiki/Public_key_infrastructure#Web_of_trust). https://en.wikipedia.org/wiki/Category:HTTP 
kobo, and PDFs are horrible and have fixed page sizes rendering everything unreadable, as opposed to some of it being slightly harder.
*a* has always been PyPI itself. *b* and *d* have gone offline and now point back to PyPI. *c*, *e*, *f*, and *g* are official mirrors and maintain the exact same functionality and API as PyPI. Crate.io is an unofficial mirror with several enhancements (including TLS support). The mirroring strategy uses a pull based strategy and the mirrors often break and stop updating, sometimes for 30+ days at a time.
PDFs look awesome on my 10" Nexus!
Instapaper or readability? Try them out.
Am I the only one here that doesn't find it that hard? I just pushed my first package a few months ago and it was ready as hell. I don't need compilation of C or anything but Damn. It didn't seem hard at all. And dolls are downloading it so I seem to have 'done it right' our at least ok.
[How to mirror using rsync](https://www.scientificlinux.org/download/mirroring/mirror.rsync) (... http://mirrors.kernel.org) [[rsync](https://en.wikipedia.org/wiki/Rsync)] [Workflow alternatives to cron](http://redd.it/1h1won). 
Is this thing still active? I know I'm a month late, but this looks cool.
I'll turn it on. One second.
Awesome, thanks.
okay try doing it
As a monospaced text format, code is a tricky thing to handle in an ebook format because one of the core design features is reflowability. Keeping your code to 80 characters per line goes a good distance in avoiding issues, but obviously won't get you by with very large typeface settings and small displays. When it comes to diagrams and figures, Kindle (and most reading systems) are totally behind the times with handling them. A good reading system should be able to focus on the image, zoom, and translate, just like a good computer image viewer. I would also note that supporting MML rendering should be the standard in reading systems rather than the exception.
Are you saying: "Kindles are great for reading books full of code and diagrams if the books is published as a PDF" Or: "You can't read books full of code and diagrams on a kindle, so you have to use a different device, and that device will render a PDF better than anything else". A particular PDF has a particular page size, and will only work optimally if that page size matches your device. Authors don't always do a good job creating Kindle content. 
This is a great coincidence as I'm planning on integrating flask into my application next week!
Will this book be available to buy as a PDF after it's published? I'm uncomfortable using kickstarter.
If you change the exception, you can explain why. except ImportError as why: print 'Enhancement module problem: %s' % why 
True. the XML parser example is obvious in hindsight.
Yes indeed it will.
Me too. :)
It'll cover best practices and patterns that make use of those extensions. In other words, yes, but as a part of larger lessons.
Then you should buy the PDF and/or print copies. :)
I'm glad to hear it!
I counted 16 projects using some compiler tech (https://github.com/IgnitionProject/ignition/wiki/CodeGenComposability_Scipy2013). I wish we could get the PyPy guys to come out too. But hopefully the discussion will make us get actually talking.
[rlwrap](http://linux.die.net/man/1/rlwrap) is your friend
Hi fourthrealm, using conda you would basically build conda packages for everything your applications requires (except maybe the C run-time libraries, although that would also be possible). Hotfixes usually are only in your main application conda package itself (not in all of the dependences), so you build a new conda package of the application and only that gets updated on the users machine. These hotfix releases can just be on binstar, i.e. no PyPI package is required. We (at Continuum Analytics) have tools to create fat installers from conda packages. These are (for example) the [Anaconda](http://continuum.io/downloads) installers, which are composed of conda packages. The tools for making these fat installers are currently not open source. I hope this answers your questions.
Also trust is really really hard. And the traditional Web of Trusts just don't cut it. Sure you may trust me for package X, but does that mean you trust me for package Y? Is the average developer going to be able to figure out how to participate in the web of trust? 
This. OP says he wants to keep it simple. And that's what bpython is. IPython is way too heavy for quick fiddles.
 function create_managepy_function() { FCN="function $1 { python2 manage.py $1; }" eval "$FCN" } function parse_managepy_commands() { NAMES=$(python2 manage.py | grep '^[[:space:]]\{4\}[A-Za-z]') for line in $NAMES; do create_managepy_function $line; done; } You were saying?
Whats with the thumbnail?
Just to make sure: do you want to expose html like github does in this link? https://raw.github.com/lincolnloop/salmon/master/docs/index.rst It's using plain/text as response mime_type
the official docs really carry you very far. The creator has stated that he spends more time doing the docs than doing the framework. Then there's the flask mega tutorial: http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world and a more advanced post on how to structure your apps: http://mattupstate.com/python/2013/06/26/how-i-structure-my-flask-applications.html
&gt; PDFs look awesome on my 10" Nexus! Such laggy scrolling though... ☹
Damn it, I thought this was a book about drinking, or a book with a flask inside, or something... ...stupid programmers are taking up all the good words. &gt;:(
&gt; Pure "Python 3.x only" code makes Unicode handling saner The blog was exclusively talking about Python 3.x only JFTR. All of the code examples are pure 3.x and got the 2.x shims removed that are in my actual projects.
Sweet. The main problem I was addressing was having to be in the base directory of the django project. Hence tying it to virtualenv on activate to get the absolute path to manage.py so the commands are not bound to the root project directory to work. I'm new to python and have been only doing this stuff for a week and was just scratching an itch. Love you solution though. Thanks for taking time to share it :)
I'll definitely add it!
Yup, I'm building an online IDE with Flask as we speak (only 24 hours away from an RC1 release!) and those two tutorials plus the official are more than enough. Flask is awesome, it made webdev fun for me again. Something Django couldn't do.
 from __future__ import division try: import IPython # IPython() except ImportError as exc: # rlwrap 
simple, run python like: rlwrap python might be good to put it in an alias: alias python='rlwrap python' 
You don't need to worry about the GPL unless you're *distributing* your application together with the debugger. Is that really something you're thinking of doing?
feedback and corrections appreciated.
So, aside from all the UDP hate... this looks like a reasonably good and tiny library.
[*Explain it like I'm five: Python and Unicode?* ... `six.text_type`](http://www.reddit.com/r/Python/comments/1g62eh/explain_it_like_im_five_python_and_unicode#cahkvtz) and also [`nine:implements_to_string`](https://github.com/nandoflorestan/nine/blob/master/nine/__init__.py#L53)
The video currently up does not work: &gt; The creator of this video has not given you permission to embed it on this domain. This is a Vimeo Plus feature.
I'm annoyed that there's no built in way for this. I don't want to have helpers like that littered in my code.
Django (experimental 3.x support). Celery (along with the whole AMQP ecosystem). Gevent. Aside from Django, most of these are probably "specialty" packages, but they are indispensable for a lot of "enterprisey" applications.
&gt; If you need to do this, put the UTF-8 declaration at the beginning so that python will read your file properly. [PEP 263: Defining Python Source Code Encodings](http://www.python.org/dev/peps/pep-0263/) # -*- coding: utf-8 -*- &gt; The encodings you need to know about: * https://en.wikipedia.org/wiki/ASCII * https://en.wikipedia.org/wiki/ISO-8859-1 * https://en.wikipedia.org/wiki/UTF-8 * https://en.wikipedia.org/wiki/UTF-16 * https://en.wikipedia.org/wiki/Unicode &gt; When you properly internationalize your codebase, all your human-readable text should come from translation files. You may see code like 'tr("What is your name?")', which will look up the appropriate translation of "What is your name?". Note: It's not unicode you pass to tr() because the user won't see it -- they'll see the translation. * [`gettext.install`](http://docs.python.org/2/library/gettext.html#gettext.install) adds `gettext.gettext` as **`_()`** * http://docs.python.org/2/library/i18n.html * [I18N == Internationalization](https://en.wikipedia.org/wiki/I18N) * [L10n == Localization](https://en.wikipedia.org/wiki/L10n) * [babel](https://pypi.python.org/pypi/Babel)
Thanks just what I have been looking for. A small correction: changing the 2 print lines to use python3 syntax (using parentheses), makes this compatible with both python 2 and 3. *Tested on python version 2.7.3 and 3.2.3*
Thank you so much! I finally understand encoding in python. Please continue the good work! Site bookmarked. Edit: What is the status bar that you use at the bottom of your vim? 
Well, it might be my friend, but not python's It offers no suggestions for this &gt;&gt;&gt; imp&lt;tab&gt; And only offers files/directories in the current directory for this &gt;&gt;&gt; import &lt;tab&gt; And hangs completely for this $ rlwap -p red python 
So far I haven't found virtualenvs to be useful. Can anyone tell me what they use them for?
The main benefit is if you have a server that runs multiple apps (think web apps) that have dependencies on the different versions of a python module with the same name. It also helps with keeping track of what versions are used. The default python way is to just have a global version of a module installed, which can get hairy. Virtualenv's also help with setting up all the dependencies when moving an application between machines. That said, I disagree this is the proper way to start programming. This is something that should be done for "reliable" "enterprisey" environments, but is probably overkill for a beginner.
Im so sorry. I did embed it and ask it to only run on pycandy.com/*, but that should make it work there too right? Il open it up.
Agreed, but I am a software architect who just switched to Python from Perl and dependency management is huge (plus you get copies of python and pip as well). There is a concept called local::lib in Perl but nothing as shiny as virtualenv. Maybe instead of "Proper Way to Start", it should have been "Python Environment Best Practices" or something along that vein. I just know that I like it a lot and being able to switch between projects and install whatever I need (without screwing up my machine) is amazing.
Are you referring to the fact that communication between coroutines becomes more complex behind the scenes, and can be slower, once GOMAXPROCS is greater than 1? Edit: And if so, how is this a problem?
More unwanted opinions... How nice. How would you, a moron, have any idea what a good developer should act? Did you mommy tell you bedtime stories about it or something? In case you didn't notice, I asked for some ideas, with specific criteria for those ideas. You came back with a bad assumption, a personal attack, and an idea that fell outside the criteria stated. (In other words, you came back with less than nothing.) So, why did you bother? If you don't have anything to offer that is both relevant, AND within the confines of the criteria, then how about saving the bandwidth and keeping quiet? Nobody asked you to join the group, especially if you can't be bothered to actually READ THE QUESTION.
That's a good question, I'm not really sure what they are doing here. Can you explain? BTW, Thank you for giving real responses to my questions! I've gotten a lot of responses to this thread, but you are one of the few people who have actually read and understood the question, AND made constructive suggestions! 
I use them exclusively, even for throw-away projects. They're a tiny amount of effort upfront that saves you a ton of headache down the road. Rather than having to catalog your dependencies you can just do a "pip freeze" and get the exact list of what you're using in your current environment (And that list can be consumed by pip on the deployment side) It's also useful for testing beta versions of your dependencies. Want to know if Django 1.6 will break your app? Well, you can create a nearly identical environment with Django upgraded to the beta to see if anything breaks - without having to worry about remembering to "downgrade" your django when you want to go back. With that said, using virtualenvs without virtualenvwrapper is a PITA. I think before I knew about that I just used a series of bash aliases...
Distributing doesn't have to be making big releases - it can be as simple as giving the source tree to someone not in the company to review/run, or putting it up in appengine. I do typically include 3rd party components (in a directory making clear they are 3rd party) alongside the source. Having to ensure wdb is never included when the repo goes outside the company would be a pain.
Merge with Distribute -- Setuptools Documentation **http://pythonhosted.org/setuptools/merge.html**
Yes, and it was perfectly good. Python Cookbook 3rd ed.
There are a few ways /r/IPython can support a doctesting workflow: As a [`@test` decorator in an IPython Notebook and/or `%doctest_mode` in an IPython REPL](http://www.reddit.com/r/IPython/comments/1eutj6/ipython_doctester_an_test_decorator_for/#ca3yo2z) and/or as an inline [Sphinx ReStructuredText Directive](http://www.reddit.com/r/IPython/comments/1hkqx4/crash_course_in_python_for_scientists/#cavl8ny) with syntax highlighting.
One reason I think they might be handy is because I don't want to pollute my 'global' setup with a ton of packages. For example, I'm working on a django site right now, and we have all these packages like: zencoder, requests, south, django, pillow, boto, etc. I don't necessarily want to install all that stuff in my default configuration, so I start a virtual env and do all the pip installs there. 
Would be interesting to see it expanded to support mutlicast.
One level deeper: Why is virtualenvwrapper so good? I just do `source ../Virtuals/(project name)/bin/activate` and I'm good to go, and haven't really ever had to do anything else.
http://stackoverflow.com/questions/3220404/why-use-pip-over-easy-install http://stackoverflow.com/questions/368636/questions-about-setuptools-and-alternatives
In addition to the links I posted below... (Honest question:) Can setuptools do this? pip freeze &gt; requirements.txt and pip install -r requirements.txt This is amazingly amazing.
I'm assuming that it's the image from the video.
If I can make it look good, I'll make it.
That's on the roadmap. 
I have a function defined in my `.bashrc` that looks like: function workon () { if [[ -z $1 ]] ; then echo -e "\nUsage:\n\tworkon &lt;virtualenv&gt;\n" else source ~/Development/python/virtualenvs/$1/bin/activate ; fi } Saves a few keystrokes. Yeah, I'm lazy.
pip isn't distributed with Python by default. You use easy_install to get a copy of virtualenv. Once you have virtualenv (which includes pip), you use pip from then on, e.g. `pip install django`, which he does in the last step (the step where you actually start working.)
[Package management for Python](http://ziade.org/2010/03/03/the-fate-of-distutils-pycon-summit-packaging-sprint-detailed-report/) is not an historically pretty topic. [It still isn't.](https://python-packaging-user-guide.readthedocs.org/en/latest/future.html)
... this, and a little bit more security as well (don't have to install packages as root and won't run all those packages from under other users). Also it'd be inappropriate to do `python setup.py develop` thing to the global setup.
There is also [this](https://github.com/pypa/pip/issues/520) which is probably fixed but occasionally still relevant.
I also use them to make sure I know how to set something up. Once I'm far enough along on development to let others see the work, I create a new virtual environment and reinstall. A pip freeze of the original often has things I didn't end up using, so it's cleaner to rebuild. 
So with `id([]) == id([])` the space for `[]` gets allocated on the left and then reallocated to a different `[]` on the right? Why does the space for `[]` (on the left) get freed if all this happens on a single line? --thanks for the post BTW.
Try r learn python 
That explains the lack of intelligence in your posts, if you thought that was a burn. Sad, really. Mine wasn't intended as a burn, but rather was an honest wish that you'd go play somewhere else, away from me. Always. I'm sure that you have oh, one or two years of programming experience and so consider yourself quite the whiz kid, but from my point of view you are just a dumb kid who likes to make personal comments when you have nothing technical to add to the conversation. So, like I said, feel free to not reply to my posts, at least until you get some real experience. Come to think of it, don't reply after that, either. You aren't just a dumb kid, you're kind of obnoxious, too.
IF you have projects that need a specific version of a library, or if you have an app that you developped a few years ago, with pip requirements and virtualenv, it's quiet easy to maintain library and site-packages 
what about venv?
slides please...
Virutalenvs will also let your ubuntus and debians rely on python libs via .deb's and your own requirements.
That's actually kind of cool. I don't really know much about iPython or Anaconda but I was just wondering if it's possible to create your own Anaconda like distribution with additional functionality. We use Python for hardware testing and right now it's just a lot of commandline stuff with raw_input queries and whatnot; a lot of the guys writing code aren't strong software programmers (they're fpga programmers). I want to create a gui for this stuff and it seems like iPython might make some of that easier to develop tests for hardware.
&gt; I don't need compilation of C or anything but Damn. It didn't seem hard at all. Yup, easy things are kind of manageable now. Try adding a C extension or depending on some Python library that *does* have a C extension, and supporting all the various configurations that that's likely to encounter.
The proper way is to *start.* It's to open up a terminal, type python, and start playing. Then you worry about doing things correctly.
Keep in mind that Python 3 is *technically* the "latest" version of Python but it's also a very different flavor of Python, so Python 2.7 is also the "latest" Python for the 2.x family. Unlike web startups with very agile codebases and tech management, more established businesses do not transition technology at the drop of a hat. (And for good reason, too.) Python 2.x will continue to be the predominant version encountered in industry for YEARS to come.
Hey, I just gave a talk at NovaPython at ScienceLogic last month! So that's what they do... :-)
The way I see it, there are two core challenges in Python packaging: 1. Legacy tools did not sufficiently distinguish between the build system and the installation/package management system. 2. Because Python supports C/native extensions, a huge part of the mess is because the C ecosystem has never really solved the "DLL/.so hell" problem. Additional complicating factors are: 3. Python devs chose to use *imperative*, executable scripts as the foundation for both their build and package management system 4. Folks involved in the Python packaging tool development general either do not recognize point #2 above, or recognize it and declare that it is not within the scope of the problem that Python as a language or technology needs to solve. I don't often second-guess or find much to complain about w.r.t. the fantastic Python core dev team and all the smart, thoughtful people who have comprised it over the years. But I do think that the packaging boondoggle can only be solved by taking a deep breath and plunging into the deep end. This is the motivation behind both the conda project and the binstar.org efforts underway at Continuum. We saw that other languages such as R, Ruby, and Node haven't really solved the library/packaging problem either, but have found ways or conventions to dodge it. We also recognize that this was a *debilitating* weakness that impedes the broader adoption of Python for scientific and data analysis. If people can't reliably share the code they've written, we're completely failing to engage the viral loop for spreading our beloved language into industry and academia. So, our approach is to have our own, simple package format that can bundle up everything from Qt to LLVM to CPython itself, along with a robust way to create C-level "environments" in the filesystem, so that every shared library that gets pulled into your Python interpreter process space is happy and doesn't conflict with anything else. (Virtualenv is fine for some things but at the end of the day, it's a set of scripts for hacking up a Python install, but doesn't address the shared library problem, and it certainly isn't meant to handle things like big C++ frameworks like Qt and LLVM.) 
Hi, Westurner. I work at Continuum Analytics and I've created an unofficial Anaconda package for Puppet here: https://github.com/bkreider/anaconda This is completely unofficial at this point, and needs to be updated from Anaconda 1.5. I haven't pushed to publish it because it doesn't add a "conda" provider to Puppet, but instead creates a new type to install conda packages. For example: `anaconda::package(dnspython:)` I will probably add a Puppet provider at some point and rename the module. 
Thank you. This makes me feel better about learning 2.7. I've heard predominantly good things about the versatility of Python. 
Flask is really awesome if you can get into it. However, bottle.py and Flask are really similar (IIRC at one point they were considering merging, but did not due to ideological differences in the projects), so you aren't missing out on *that* much. Personally, I prefer bottle.py's single file framework, which makes for extremely portable web apps, but to each their own.
Absolutely. Might take a while to get around completing it, but I think it would be a valuable find to anyone who comes across it.
I will consider it. Thanks for checking it out and the compliment!
Haha, nice! Yeah, they started hosting Nova Python events shortly after we moved out to Utah. Would have been fun to have them right where I was working :-) 
You were actually incorrect on making it more pythonic at the bottom. Explicit rather than implicit is python's design philosophy, which is why you should avoid using non-standard decorators as often as possible.
Yes, you can do this. Anaconda just consists of packages that the conda tool installs. You can easily have conda install additional packages, or build your own packages and put them in a repo and have conda install them. This is what we built http://binstar.org for, as a place for people to create their own binary repos for packages and apps. Please email the Anaconda mailing list for more info: https://groups.google.com/a/continuum.io/forum/#!forum/anaconda
which doesn't help if you ARE writing into the global memory.
That is what drew me to bottle, plus the docs that go through the actual implementation code in the page (so you might as well just open up bottle.py and read it) is amazing.
Excellent approach. You kept it short and clear and +1 for the solarize theme. 
Yeah. It happens like that beceause cpython frees memory immediately at the point where the last reference to the object vanishes. That happens here when each id() function returns, rather than at the end of the line. You'd get different behavious on refcounted implementations of python like jython or pypy, since the memory wouldn't be freed until the garbage collector runs.
`workon` is defined by [`virtualenvwrapper.sh`](https://bitbucket.org/dhellmann/virtualenvwrapper/src/3b1b3a2d32f0b56739e5a5c6f73b0fdd959c1180/virtualenvwrapper.sh?at=master#cl-643), which can be [sourced in from `.bashrc` or `.profile`](http://virtualenvwrapper.readthedocs.org/en/latest/#introduction). $ type workon Here's a similar function called [`we`](https://bitbucket.org/westurner/dotfiles/src/8b0460745d1f251e2872ca315941e00a2cdd8dae/etc/.bashrc.venv.sh?at=default#cl-465).
I'm not saying that unicode on python3 is perfect, but in MY experience it is much much better than in python2. Python2 allowed to mix bytes with strings which is just wrong. The result of that was that everything worked fine until you stumbled upon some specially encoded value (from either user input, files or whatever) and then you app suddenly dies. Other dynamic languages are more tolerant by just producing malformed strings which is obviously also wrong, so I cannot agree with the suggestion that bytes and strings should be mixable. As stated in the link unicode is hard, and python3 forces you to handle it the right way. It is just the APIs that should be fixed.
https://github.com/jminardi/scipy2013 https://docs.google.com/presentation/d/19ErI3QJfSZ8uIBDV2MK0VvFkfcgkvK-Tm0Sz84AxvNI/edit
Do steel bolts spring out and plunge straight through both cheeks? *Better feedback to come.
Interface wise, Anaconda seems to be less QT and more IPython centric. Continuum has added some of their own tools to both the free and pay versions. You can check the FAQ for both distros and Enthought's too, to see which packages they support. Here's what I do for my Windows and Linux. Install or leave the native vanilla system Python alone. Install Anaconda and use that as my development Python. For projects use conda or virtualenv to mange the packages. (Depends on the project.) When updating Anaconda to a new version, the FAQ on the main site is incorrect and needs to be updated, check the forums/mailing list. Also run the update as admin/sudo. 
virtualenv is huge deal for testing if your app can still run on Python 3 or PyPy.
Have a nice life, dipshit.
Awesome post! I love Python, have a Mac, and use the shell a lot.
Python is portable, why constrain yourself to *nix? I will never understand this. With almost no exceptions all my code is portable.
wish volume management was that accessible in windows
or you could just use the built in applescript osascript -e "Set volume 7"
I think that PowerShell gives you access to all of this stuff. 
&gt; PowerShell well you could use something like this but its not exactly what i have been looking for
i actually had it wrapped in a bash script first, the i wanted profiles and rewrote it in python
MP3 files are streamable in its native format. Just start playing the MP3 as you are downloading it and its streaming.
Windows automation, even of settings, can and maybe should be done through AutoHotKey. Either that, or use the native Windows API calls: http://msdn.microsoft.com/en-us/library/ms678715.aspx
I see. Unfortunately, the mixer API in Windows used to be pretty convoluted back in Win95/XP and it only got worse with Vista and later versions. Who knows… maybe there's an easier way to do it with WMI? Unfortunately, I'm no WMI expert either.
An object oriented shell, what's not to like?
came here to post the applescript you beat me to it
*NetworkX* From http://www.networkdynamics.org/static/zen/html/api/thirdparty.html#module-zen.nx : &gt; The edge weight will be lost, as there is no separate edge weight attribute in NetworkX graphs. From http://networkx.github.io/documentation/latest/tutorial/tutorial.html#edge-attributes : &gt; The special attribute ‘weight’ should be numeric and holds values used by algorithms requiring weighted edges. [CogPrime also has weighted edges](http://wiki.opencog.org/w/CogPrime_Overview#Weighted.2C_Labeled_Hypergraphs). *Style Points* * [PEP 8](http://www.python.org/dev/peps/pep-0008/)* * https://bitbucket.org/tarek/flake8 * https://github.com/spulec/pep8ify * http://www.reddit.com/r/IPython/comments/1hkqx4/crash_course_in_python_for_scientists/#cavu9w7 
This depends on how you install things. On Linux, at least with the distro I'm using, there's a package available that only contains pip, so you wouldn't need to use easy_install at all. 
I stand by my statement: &gt;I feel Python was designed for the person who is actually doing the programming, &gt;to maximize their productivity. And that just makes me feel warm and fuzzy all &gt;over. I feel nobody is going to be telling me, "Oh yeah, you have to jump through &gt;all these hoops for one reason or another." *When you have the experience of &gt;really being able to be as productive as possible, then you start to get pissed off &gt;at other languages. You think, "Gee, I've been wasting my time with these other &gt;languages."* -Bruce Eckel 
Whoops sorry. I just thought the google drive link was too long and ugly.
If your employer still uses 2.7 there isn't much you can do about. 3.3 is definitely the "future" but not any time soon. Cobol is still around... Python 2.7 will be too.
Generally adoption is not a lack of desire but a lack of killer features to push the adoption earlier rather than later. Had python3 included a JIT, very large performance change, etc i think adoption would be much more prevalent. Corporations really just don't have a great need to move to python 3. I know my company sees the changes as interesting but not must haves. And in some cases (eg print statement) the changes are more of a burden on developers. Make a graph database from python and c and give it a killer interface like neo4j and we will be the first to jump on. But really barring those features that align specifically with our needs... We just aren't interested no matter how cool. Same thing with pypy.
Python 3 has been much better than 2.7 since day one for people who deal with characters not in ASCII within their app.
If you are on Linux, and your employer runs a fairly recent distro, a lot of them allow you to install 3.x as just a package, and all you have to do then is call python3 and pydoc3 to use it (it doesn't remove python 2, and leave it as the default).
&gt; How interested will you be when your version is deprecated? We won't be, but the fact is it's still supported. &gt; Your employers are harming python through their actions. They're hindering its progress and development. That's the opposite of what one is supposed to do for open source; it's an anti-contribution. Python is doing that job all by themselves, by supporting 2.7.
In general, I tend to agree with this attitude, but I think you're expressing it prematurely. Migrating quickly to 3 is a good way to help the platform, but I think it's much too early to start scolding people for "hurting python". Everyone knew this was going to be a slow migration, and we're *maybe* halfway through it. ETA: for anyone reading this, if your shop is still on 2.7 and you want to help Python move forward, you can do that by trying to write python 2/3 compatible code. This'll ease migration later on.
But I'm not sure that modules for python 3.3 are better than those for python 2.7
The other thing is dependencies - if you need to use some library and they're on Python 2.x then you're stuck there unless you're willing to do the upgrade yourself. This video is just supposed to kick people in the butt to keep moving forward.
[Armin might disagree](http://lucumr.pocoo.org/2013/5/21/porting-to-python-3-redux/); check out all the "unfortunately [python 3 sucks]." His tweets also talk about the porting pains he's experienced. I wonder if Python 3 is so bad, why haven't we heard similar complaints from other major Python developers?
Unless someone ports py2exe, I'm staying with 2.7. Cx_Freeze isn't bad, but I want to be able to make a single exe instead of a whole directory full of stuff.
As great as it is that you're helping others understand metaclasses, I think they're a feature that makes code unreadable as fast as possible.
Is the link dead for anyone else?
Awesome! Looking quickly, I already like the API a bit better than NetworkX too. The only issue, which is normal with newer libraries, are the documentations and tutorials. This one isn't doing too bad though.
Worked for me when I tried it before, but if you want the direct youtube link, it's http://www.youtube.com/watch?v=f_6vDi7ywuA
Sometimes, I dont get jokes, but I really wanted to get this one :) Will await that feedback Zhyl.
its probably because hes trying to maintain py2 and py3 versions at the same time. I don't really see why python 3 is terrible. Ive heard that it doesn't change enough / make enough things better to warrant porting over just yet
Now it works - thanks!!
This is really good stuff. I will be implementing it in my project tomorrow. But for clarity, it would be better to use something more descriptive than an underscore. 
Nope
This is the main problem: python is portable; C compilations aren't.
Everything is in unicode by default in python3. So something in another language enters into python as unicode, and its clear that you'll need to encode properly into something to display it properly. In python2, everything that's ascii looks fine and dandy, because it's a subset of utf-8/8859-*/1252, and you go on writing your entire application unawares; but as soon as you ingest someone's differently encoded international text, everything borks, and you have to figure out what to do throughout your entire application at once(if you didn't know how to "do it right" from the very start). "Doing it right", at its most basic, means- * Ingest input with an encoding, and decode it into Unicode * Do all processing in Unicode * When you output, encode as appropriate ("easiest default": utf8) 
That's like, exactly what virtualenvwrapper is, but it includes other extras, like the super-awesome `mkproject`.
Big deal.
Thanks for the reply. Doesn't this come at a cost though? Specifically, in terms of memory consumption? I only bring this up because my experiences with defaulting to UTF-8 with MySQL. In MySQL, you don't want to blindly use to UTF-8 in all circumstances.
I do a lot of language processing and collect global data. This aspect of Python3 has made my life much easier. 
Then don't tell MySQL to use UTF8 and don't give it UTF8 strings. Just make sure that the text that you handle in your python is appropriate to the encoding you want to output in and send to MySQL. And if you actually have a *real, validated* worry about python taking up heaps of your memory with unicode strings, then I'd conjecture that you are dealing with so much text that python is probably the wrong tool to work with. 
I had always been led to believe that the reason there hasn't been a mass migration to python 3 is the lack of commonly used module support. I'm not a full time python programmer, I just get it out every now and then when I need to hack something together to save me from repeating a laborious task! 
It's not always that simple. Have you ever tried to join on a column that uses different collation? I'm also referring to indexes, innodb, and defaults. These are important considerations in a team and you're dealing with TB's of data.
Heh, the distro my employer runs comes with Python **2.4**. I'm still actively pushing to move processes from 2.4 to 2.7...
Encodings? Simple? I hope I never made that impression, and apologise if I did.
Web development it's a beast on its own. Browser (user mostly) request a web page to a server. The server may try to get a file and send it back as a response, or pass the request to an application using a language. This time is the application the one on charge to build a response. A response have some parts: headers, content, .... Headers are not seen by users but are send back to browsers. Header have information like: hey browser, this stuff I send you back have not change since 1999 so you can get your local copy of this and present it to user. This is caching. In the case of content, content can be different types but it's specified on headers. When content type is html, browser renders html so we see text and structure. When content type is css, browser apply style rules to html. When content type is javascript, browser runs code over html. If content type is pdf, the browser may try to open itself or pass the file to an application installed on user computer, or just download it. And when content type plain text is usually presented "as is" on browser screen without interpretation. The application can trick the browser for example and sending an image but setting content type as html. Browser can go nuts trying to display html that is not html. In this link you can learn how to setup a simple web server with python, without apache o others involved: http://wiki.python.org/moin/BaseHttpServer s.send_header("Content-type", "text/html") Good luck and thanks
Matplotlib just released it's 3.x version. Next up, wxPython.
&gt; Had python3 included a JIT, very large performance change, etc i think adoption would be much more prevalent. &gt; Same thing with pypy. Isn't that exactly what pypy can address?
Sure, but has my **&lt;insert favorite python library here&gt;** been ported to 3.3 yet? That's always been my problem. For example, whats a good 3.3 alternative to https://pypi.python.org/pypi/watchdog ? I have no problem with 3.3 as a language, it just has horrible support for most 3rd party packages that I rely on. Until every last 2.7 package is converted to 3.3, 2.7 is still the superior option in my book. Unless you get enjoyment out of re-inventing the wheel perhaps. 
Who really cares what he has to say? Seriously in every crowd there is always one or more that have an innate need to trash anything new. I follow C++ and the new standard on and off and there are a few that don't like the new standard there. That is with a vastly improved standard and language. Most people tend to agree that C++11 is a huge step forward but still you have the contrarians. Python 3 on the surface isn't that huge of a step forward, but underneath it has been overhauled nicely without major impact to the way code is written. So Python 3 isn't a major deviation from the old 2.x series it the way the C++ standard has evolved. That is a good thing because it doesn't automatically invalidate a bunch of coding techniques. The few things that need adjustment, such as strings, are forward looking improvements. 
Lack of killer features, but also: Am I the only one around here that rely's on 3rd party python packages that haven't been ported to 3 yet? Python 3 can be the more elegant language all it wants, but until every last 2.7 package is ported to 3, I just can't justify it, or call it the 'better' language. But I guess i'm jaded, and have lots interest in re-inventing the wheel.
Something like [this](http://ready4rails4.net/) would be super useful for python3
&gt;Armin might disagree There are Armin Ronacher (flask, werkzeug) and Armin Rigo (pypy). You might want to be more specific when tossing first names around.
[Here you go.](https://python3wos.appspot.com/)
Here Zen's [nx.py](https://github.com/networkdynamics/zenlib/blob/master/src/zen/nx.py#L99) imports edges from a NetworkX Graph object: # add edges for x,y,ed in G.edges_iter(data=True): Gdest.add_edge(x,y,ed) Zen's [add_edge accepts weights](http://www.networkdynamics.org/static/zen/html/api/graph.html#zen.Graph.add_edge). The NetworkX edges_iter method returns weights: &gt;&gt;&gt; import networkx as nx &gt;&gt;&gt; G = nx.Graph() &gt;&gt;&gt; G.add_edge(1, 2, weight=4.7) &gt;&gt;&gt; list(G.edges_iter(data=True)) [(1, 2, {'weight': 4.7})] Easy fix I suppose.
It's also worth mentioning that when Windows offers to save a file in "Unicode" format, it actually means UTF-16. There's no file format called Unicode.
I've switched to 3.3 for my own personal scripts but I'm still stuck on 2.7 at work until there is a stable release of wxPython for Python 3, and then on to port 300K lines to a new version of Python AND wx at the same time (oh boy, that will be fun).
Count me in Armin's camp. Some possible reasons you haven't heard more people complain: complaining comprehensibly takes time (and Armin should be thanked for spending it) and there just aren't that many people porting to Python 3 yet. The number of package authors who maintain Python 3 ports of their software is below ~ 50 at this point.
Nice. I am using NetworkX myself, but I am using it with pypy - so using Zen wouldnt be possible I guess.
This has been my experience. I'd love to work in 3, but both my employer and my direct work depend on packages that have not been ported. Outside of doing the conversion for every external package, it just doesn't matter how good 3.3 is, because I will never get to use it.
Yes, this is certainly my experience as well. That said, it is becoming less and less true each month as more packages are ported. But the reality is that I work on operating systems that do not come pre-packaged with python 3.x. We're probably 3 to 4 years away from actually moving to an OS that does come pre-packaged with python 3.x. And at that point, we're still probably going to be supporting python 2.x because our customers won't necessarily want to upgrade their boxes. If I had to guess, we're a good 6 to 7 years away from fully moving to python 3.
have a look to these builds down to rhel 4, still widely deployed :O : http://download.opensuse.org/repositories/home:/cavallo71:/opt-python-interpreters/
Pypy is effectively addressing a portion of the problems. But there's a whole world of 3rd party apps that don't necessarily work on pypy. It's just not a production-ready drop-in replacement of python. It's a really neat and interesting implementation of the interpreter, though.
&gt; Python 3 has been much better than 2.7 since day one for people who deal with characters not in ASCII within their app. Not really, Until 3.2 a lot of unicode support in Python 3 was broken in many modules.
&gt; Python 3 while in memory is just the UTF-8 encoding. Python 3.3 has a variable internal encoding. It's usually ASCII, UTF-16 or UTF-32, but never UTF-8.
Given that I work with a lot of engineers who don't use Python for the most part (which, in and of itself, makes life interesting), but who all have Python scripts they depend on, I'm not all that confident that the conversion will happen at my workplace, even once our dependencies get covered. No real impetus for change when your interaction with Python is "Once a month, I modify one line of a script". And, to be fair, given how much work it will make for me if my employer ever does convert, I'm not sure I mind that much...
About the "C-level environments that just reminded me of this: http://www.art.net/~hopkins/Don/unix-haters/x-windows/disaster.html (scroll to "Myth: X Is "Portable" section). Unfortunately we haven't moved very much from that :( 
Or if only to make sure your future self doesn't hate your current self.
Eh, I will still use Python 2.x for anything where I might want to distribute a single .exe to my technologically-incompetent relatives/friends. That being said, once I can make a single stand-alone .exe in Python 3.x, I'm never looking back.
imho, most developers &amp; companies will move to 3.x when the 2.7 security updates support will end.
Are you sure that watchdog doesn't support Python3 now? I know they were working on it.
&gt; Who really cares what he has to say? Because Armin Ronacher is a very sharp, experienced Python developer who is the author of several very popular Python libraries/frameworks: http://lucumr.pocoo.org/projects/ Perhaps you have heard of Flask, Jinga, Pygments, Sphinx?
Hello, the [release announcement](http://www.reddit.com/r/Python/comments/1f4s1b/zato_opensource_esb_and_application_server_in/) I sent some time ago was met with interest so I'd like to present you another piece - [an article on Zato](http://www.infoq.com/articles/zato) I wrote for InfoQ - covers Zato's architecture, goes through all the major features and presents sample code to integrate financial APIs of Yahoo YQL/JSON and Google XML. Unfortunately, the site doesn't support syntax highlighting so it's better to have look at [this gist](https://gist.github.com/dsuch/5939529) instead. Happy reading!
TIL - thanks!
Sure, it got me curious, that for sure :-) I'm going to check it out but like it was said [in another comment](http://www.reddit.com/r/Python/comments/1hhi5o/current_python_package_management_frameworks/cawz99s) - it would really come in handy if the full stack was open-sourced. Do you have perhaps plans for doing so?
&gt; Or if only to make sure your `__future__` self doesn't hate your current self. FTFY.
Those 244 packages are not maintained by 244 people though. Chances are that most of those 244 packages don't necessarily spend a huge amount of time manipulating text either. So the number of authors is smaller than 244 and the number of authors who have any non-neglible experience is lower still.
Python 3 isn't *terrible*, it just isn't clearly superior to Python 2.
Ah, I read that years ago and loved it then... it still holds up now. :-)
I've been learning to do scientific programming with python and the author started by saying anything we write can be translated with a tool called 2to3 which will translate the code from v2.7 to v3.3. Is this because of the subject material or does this tool work for a lot of python code? As someone new to python I feel compelled to make the comment that having to make this choice is rather annoying to begin with. How did this situation ever even arise? 
For some reason, I didn't like virtualenvwrapper. Maybe it was too heavyweight? Maybe I didn't like all those extras? I don't remember. I did look at the workon function that virtualenvwrapper defined and decided that it was too general for my needs. So, instead, I wrote a simple 7 line function and left it at that.
No. The fact that 244 packages may not have 244 maintainers does not imply that the number of maintainers is &lt; 244; I hardly think Django, SQLAlchemy, Flask, GUnicorn, NumPy, IPython are each maintained by a single person. I'm not really sure what your point is on text. It doesn't actually matter to a user how much knowledge you have about text in Python 3 as long as your package supports it. That said, I still believe you're wrong; a hell of a lot of the packages on that list related to web development, where handling text is kind of the point. And even then, that still ignores the github problem. Even if (by some wierd alignment of the stars) there are less than 50 maintainers in the wall of shame packages, there are still more than 50 on github. And yes, they do deal with text. I've personally ported 2 libraries to Python 3; one for controlling MPD and one for wrapping with the Facebook API, both of which required considerations over encodings and the like. That's probably still a fair way away from what you're idea of "non-neglible[sic] experience" is, but you never actually defined what qualified for that title, and I'm sure whatever you set it as, there are at least 50 people out there who are eligible.
I made one of these last year for a class project, improving it into a functional clone of Dropbox (called Boxdrop). If i recall, there was something in there that didn't work quite right.
&gt; it just isn't clearly superior to Python 2. ... for your use cases.
Virtualenv Is kinda like Perlbrew, learn how to use virtualenvwrapper and you won't go back, it's as simple as: git clone project; mkvirtualenv project_name; pip install -r requirements.txt ....work..... Next time just do: cd project; workon project; ...work.... Not that hard :) 
Interesting example; I have a couple comments. First, you don't seem to be keeping the client socket around after it is created/used on lines 61, 62, and 64. You may not get all of the data sent by the client if you do not keep the socket around and call recv multiple times until the socket closes (this is obviously the case if the client sends more than 4096 bytes of data due to the limit on line 64). I would keep a map of client fd numbers (or some other identifier) to data buffered so far, keep calling select() on all client sockets, and buffer the data received per-socket. When the connection closes (or after some timeout to prevent DoS), I would then pass the complete data to the thread (or you could also do something like send each line delimited by \n to the queue thread). You probably won't see this behavior running on localhost with only one client, but with a lot of clients repeatedly sending data you can easily get a recv call that doesn't contain the entirely of the data sent (this is at least how it happens in C, and it seems that Python sockets are just a wrapper for the underlying POSIX calls) since the connection is stream-oriented (SOCK_STREAM). I also don't think that threads have good performance in Python due to the Global Interpreter Lock (GIL), but since you do a lot of I/O in your main thread and (at least I've heard) I/O doesn't happen while holding the GIL, you might get away with this. You can look into the performance of this. With a process function that just does 'pass', how many connections/sec can you handle? How does that compare to not passing the data to another thread? How is this number impacted by concurrent connections? This all would be very interesting to see.
no pip install zen?
&gt; Django, SQLAlchemy, Flask, GUnicorn, NumPy, IPython are each maintained by a single person * https://github.com/mitsuhiko/flask/graphs/contributors * https://github.com/benoitc/gunicorn/graphs/contributors * https://github.com/zzzeek/sqlalchemy/graphs/contributors
Yup, I said package authors, not packages. For example, 21 of those 244 were ported by my company.
And the fact that bytes and strings are completely different abstractions in Python 3.
Flask largely has one maintainer, same goes for most of my libraries. I don't know if zzzeek would call him the only maintainer on SA but for the most part, that's what he is. I ported Flask, Jinja2, Werkzeug, itsdangerous, MarkupSafe to Python 3 and I'm currently in the progress of doing the same for Babel. Yes, there were people helping me out and they have done a lot of great work, but ultimately the number of people actually porting and maintaining packages is pretty small.
Oh yes, that's the way forward …
Unfortunately not but I'll bring up pip support to the lab that developed it. 
Not sure why, I had no volume control and it was essential inaudible for me. 
&gt;Have you not noticed how hard it is to write code compatible Python 2 and 3? You don't write code compatible with Python 2, just as you don't film television shows that will look good on a black and white screen. &gt;Before 3.3 it was practically impossible due to the unicode literals debacle. They fixed Unicode. The debacle was 2.x's Unicode situation. Delphi ignored Python 3.0 and added Unicode to their language the Python 2.x way but worse (four string types! three char types! One string type is just to try to circumvent the implicit conversions that happen everywhere.) Now the same number of years later, they're also looking to break compatibility and drop down to one sane string type. &gt; Your suggestion would require forking dozens of internal libraries and maintaining &gt;them, possibly for years until some upstream stuff becomes Python 3 compatible. That's programming. When things change (and they always change) you either preserve things in amber by freezing them at a certain version or you refactor the code to work with the new version. Continuing to fiddle with python 2 just delays the inevitable and makes the eventual refactoring harder because there'll be that much more code to fix. The day Python 3.0 was released not one single further line of Python 2.x code should have been written. Guido kissed people's posterior and in response there were library maintainers who said things like (real quote to me) "Why should we port to Python 3 when Python 2 will be supported FOREVER?" I hope he's learned his lesson and when Python 4 rolls around he embraces the "dictator" part of BDFL and there's an immediate, firm, short EOL date for 3.x and no backporting of features at all, for starters. 
+1
I nice talk even if I disagree with his conclusions. :-) Very fair as well. 
Nice to have a sister to pimp you out! 
1. Not powerful. Why would you call it powerful? 2. Who cares that it's multithreaded? 3. "#Crappy self executing" - self-deprecating comments? 4. Poor commenting otherwise 5. Uses eval? 6. Uses a Pokemon-style exception handling I can't even finish this Please, stop, just stop!
&gt; Pokemon-style exception handling As someone just a tad to old to have caught the Pokemon craze could you explain what this means? I'm assuming its because he just passes his errors. But how does that relate to Pokemon?
I appreciate all the info, but what I meant was what are the folks at github doing in that particular place? In other words, why did you point out that particular example, as opposed to some other link? Was there something different about it that you were trying to point out?
How common is this "common" mistake? It's always referenced, but it's been a problem for me maybe once or twice. Has anyone ever done serious investigation to find what problems newbies actually run into?
The name actually comes from generically catching all exceptions and doing nothing with them, as in... GOTTA CATCH 'EM ALL Here, the OP just catches and swallows exceptions after doing eval on the input..
In case you were asking for examples: the most frustrating problem I remember having when I was newer to Python is variable assignments leaking from list comprehensions. I hear this is changed in Python 3, but the following doesn't do what you want in Python 2.7: [lambda x: x+i for i in range(10)] Rather, it gives you 10 copies of the function taking x to x+9, because there's only one i that keeps getting changed! (After executing that code, you can check that i is 9.) Default values are one way to get "different i's", e.g. [lambda x, i=i: x+i for i in range(10)] exactly because the defaults are bound to their values just once when the functions are created. And the other way I know is to use "pure functional" style eschewing assignments altogether map(lambda i: lambda x: x+i, range(10))
That's an interesting problem that I was definitely not aware of (or encountered, when I think about it). I wonder if it's a common one.
/r/learnpython
Can I get a tl;dr on what he believes is the future of the GIL?
This is immensely impractical. As long as I have dependencies in 2.x land, my responsibility to my employer is to write code that is 2-compatible. You can assert that it is somehow bad for Python that we take that path, but it would be a lot worse for Python if employers decided that it just wasn't worth their time to try and keep up with Python at all, because a new release broke their old work on a regular basis, and decided to use a different language altogether. Engineering is about solving problems - if 3.3 doesn't solve my problems better and creates huge tech debt for me to pay down, both my employer and I would be irresponsible to blow hours of dev time on doing conversions. I'm not saying that it wouldn't be nice to be able to roll my existing code into 3.x, but I am saying it is unwise to accuse engineers of undermining Python because they weigh more than just the abstract notion of progress in deciding how to write their code. SWE's everywhere make decisions that push aside architectural purity in the name of stability or task-completion. Sometimes, those decisions are awful and we pay for them for years. Other times, those decisions are correct. Either way, pretending that those decisions don't exist is impractical. It is easy to view processes and structures as sacrosanct (Agile at its worst, for example) - but doing so forgets that processes, structures, and languages all derive legitimacy from getting things done, not from being abstractly "right".
 if numbers is None: numbers = [] call be a heratic bit I prefer numbers = numbers or []
&gt; I'm currently in the progress of doing the same for Babel. https://bitbucket.org/babel3_developers/babel3
I remember being affected by this early on, and being affected by this: def do_thing(date=utcnow()): ... as you know, argument defaults are not re-evaluated each time the function defined so the date the function was first called was used for the variable date each time. Whoops.
Good talk despite the gorilla in the room. (s/despite/because of/ ?) 
The GIL must go! He argues that in order for python to be relevant in 10 years time the GIL must be removed.
That breaks duck-typing because it makes it impossible for me to ever pass an empty version of my own custom data-structure into your function: &gt;&gt;&gt; class mylist(list): ... def __repr__(self): ... return 'mylist' + list.__repr__(self) ... &gt;&gt;&gt; mylist() mylist[] &gt;&gt;&gt; mylist() or [] []
Docopt has bindings for a bunch of languages. It's quite cool. 
always learning the finer intricacies of python... so why did python creators decide default values should break scope?
Absolutely. ServoBlaster can drive up to 8 servos
Thanks for that tip! Now I don't have to comment it out in my scripts. 
The solution advocated is the wrong one IMHO. Instead we should be encouraging developers to use higher level APIs such as [concurrent](http://docs.python.org/dev/library/concurrent.html). As long as you get the level of concurrency you want, what happens under the hood doesn't actually matter. That then means that the under the hood pieces can evolve over time. The most effective way of that under the hood piece today is using multiple processes. A small number of tweaks should be made such as exception stack traces being picklable so that they can be passed between processes. I can then imagine a future implementation where code that fits within certain constraints (eg only using CPython implemented C objects) uses multiple threads, but "hard" problems degrade to multiple processes and shared memory, all done transparently. A CPython 4.0 that requires a complete rewrite of the interpreters and APIs won't get very far. Look at CPython 2 to 3 transition and that is where things remained substantially the same. PyPy fills the role pretty well, and already runs most CPython C extensions (require recompilation).
How does that break scope?
There is also love from some major devs. [Jacob Kaplan-Moss (Django), for one](http://jacobian.org/writing/why-im-excited-about-python-3/)
The most common newbie mistake I run into is a confusion about how boolean expressions work; writing `if num == 1 or 2:` when they mean `if num == 1 or num == 2:`. That specific confusion is everywhere, though it's not a uniquely python problem.
Thanks. I have only a sad, vague ability to follow along but I did listen to his talk and place it on here. And I am grateful to the people in the world that like to work on python, so I can lamely facebook all the time and use reddit, honestly. Good night.
Larry is one of the best developers I've ever worked with, even though its been years since I did. Great talk (unsurprisingly), and Mpath alumnus represent!
&gt; heratic You're a **heretic**. 
I think what you might not understand about python is that there's no distinction between passing by value and passing by reference in function calls: Everything is passed by assignment. Mutable values pass reference and immutable values pass value. Default mutable values are therefore handled by reference instead of value which is what causes this leak.
I agree :) but is there a framework or plan for removing it?
that is a good point, but do you think libraries like multiprocess and concurrent stand up to true multi threading? i have no multi core experience, so that is an actual question :p also, on your last sentence, i think you meant PyPy, not PyPy :D
"True multithreading" doesn't mean anything. Also note that threads are limiting (eg shared address space, stack size limit, need constant locking in case there is concurrent access to things even when there is no concurrent access). The goal is quite simple. If your app does 100 units of work in a minute, and you then use all available processing capacity - eg an 8 core machine - then you would expect to see 800 units of work done per minute. Whether this is done with threads, processes or bananas is immaterial. The ideal situation is also that you don't have to change any code. In reality you never get that amount of speedup even with perfect coding. That is because you start running out of things. For example you may run out of memory bus bandwidth. Another typical cause is that parts of your code are sequential. For example reading lines from a file is sequential no matter how many processors you have (work on each line can then be parallel - but the reading is still sequential). You also have communication overhead. For example a common paradigm is to place items of work on a work queue, and have workers take them off the queue. The queue then has locking to preserve data integrity, but the overhead will grow the more the queue is used. You can read about theory behind this in [Amdahl's Law](http://en.wikipedia.org/wiki/Amdahl's_law). Most of my work code is concurrent involving a lot of data processing (hundreds of gigabytes). I split things into multiple small scripts that each do one thing and do it well. I can simply run some of them in parallel, while others depend on the outputs from earlier ones. For some that just take a minute or so, I don't bother making them concurrent because having a shorter runtime doesn't matter. The rest are heavily concurrent. I'd originally make them use multi-threading and top would show about 130% cpu usage. Python with the GIL accounts for 100% of that while the extra 30% is C code called from Python running with the GIL released. In some cases this is sufficient performance. For the rest I then made them use multiprocessing. Both the threading and multiprocessing modules have queues which I use for the work. It is harder to debug when you have concurrency purely because your breakpoints can be hit in multiple threads/processes which is a pain, so I'd usually make it possible to flip from concurrent to single threaded when I want to run the debugger. Rather than rolling my own concurrency each time I have switched to using the concurrent module which provides a standard api (in all Python versions). I wrote about it [here](http://www.reddit.com/r/Python/comments/1hdr9v/pythons_hardest_problem_revisited/catmdx9). Thanks for the PyPi typo - fixed.
Awesome. Thanks!
&gt; so the date the function was first called was used you mean first *defined* ?
what just one ???
Readability doesn't parse the page at all:-(. 
Aaah I see - I'll be using that one from now on. Thank you.
... the legends...they are TRUE!!
Offtopic, but what font it that? Looks kinda funky, I like it.
It is [monofur](http://eurofurence.net/monofur.html). I've been using it for a few years as my monospaced font, and really like it.
&gt; the oldest I found was 1.0.1 http://www.python.org/download/releases/early/Python-0.9.1.tar.gz
Oh, sweet. Just compiled it and it runs like a charm.
I am aware of that port, but I am not very happy with some of the API decisions in it.
I've become quite fond of mezzanine and cartridge. They provide most everything I have needed in an ecommerce and cms pack. 
No go with 3, but there is a python2.4.
your mileage may very but deepening on your need my code works.
Would you share your reasons to drop gramme?
What about in the context of music? As in, "what song is this? It's funky as hell!"
Obviously that's ok, I was making the point that funky is not always a compliment. Like with body odour.
oh and also the reason if not myList() is falsey is not becasue of __repr__, but __nonzero__, which it inherits from list object - meaning that the custom object is behaving typical of the object it inherits. To make it not falsey: &gt;&gt;&gt; def __nonzero__(self): ... return &lt;logic that determines if falsey or not&gt;
Indeed. The underscore _ should only be used to show that this name/object is *not* used.
`uname -a`? I'm guessing Solaris in the 2.x range? 
because a variable in a function retains the value of previous executions of the function. in every major language this is heresy.
The default value isn't in the function scope, though. Unlike variables in the function body, it's initialized when the function is *defined*, not when it's *called*. If anything, I'd say it's in the module's scope. Once the module has been initialized, you can get the default values for a function using the inspect module without ever calling the function. Also, in C or C++, static variables exist to let you retain values from one execution of a function to the next. 
2to3 is there to take away the pain of the really minor changes like "print is now a function". Its not a total porting solution, but you can run it against any script and check the results for yourself. Python is designed to coexist with itself, so you can have a 2.x and 3.x installation on the same system and use one, the other, both, or neither. Currently I'm running with [Anaconda](http://continuum.io) as my Python distro of choice, and go with option #3. I try to stick with 3.3 wherever possible, since that's the present/future, but on occasion have the need to fall back on 2.7. Its not the huge drama some people like to make it out to be. The main pain point with Python 3 is that it changes strings to default to Unicode representation, requiring encoding and decoding when interacting with systems requiring bytes (e.g. all I/O, especially network). With most programmers stuck in a "we only speak english around here" mentality, that was never a concern as even UTF-8 is a byte and ASCII compatible, so they could get away with being lazy. Python 3 doesn't give them that option &amp; it breaks things. Relatively important things. Things that have taken years to get right as retro-fitting is expensive compared to doing it right in the first place, what with all the inter-dependencies. 
&gt; Had python3 included a JIT, very large performance change, etc i think adoption would be much more prevalent. While I agree with you, it has to be robust. Numba, while useful, is not quite there yet. It doesn't even support keyword arguments.
Just contact your system admin, I'm sure it's a mistake.
you do see how this breaks the scope conventions used in C++, PHP, and plenty other languages that support default argument values, right? in other languages, you have to explicitly define the variable as static or global to actually make it static or global. python effectively makes the default value semi-global, allowing subsequent execution of the same function to recall the same variable in a scope that's long since closed.
That was just what I wanted to know. Thanks!
You're thinking about this the wrong way. In python, function are objects like everything else. Default values are essentially attributes of the function. They are set when the function is defined, and accessed when the function is invoked in order to pass along to arguments that aren't specified. 
&gt; PyPy fills the role pretty well, and already runs most CPython C extensions (require recompilation). You sure? Wouldn't NumPyPy be done then?
I took a look at the screenshot and thought: "2.7.2... that's okay, that's not too old. oh... wait!"
I'm too novice to do a talk, but I am planning on going to the scrapy presentation, data analysis/deduplication. Anyone else going?
More to come after the long weekend :)
Use virtualenv. On small scripts its not important, but with more dependencies (or if you're just effing around) it's a lifesaver. [Update] - I promise that next time I will acknowledge the joke. And also omit all mentions of a tool the entirety of the python community already knows about.
He would still need to build / install python in his home directory. The situation improves dramatically if they have a unix module system installed so he can just say 'load python X.Y' and know he's got that version to springboard off of.
What do you disagree with exactly?
 SunOS polaris 5.10 Generic_142900-03 sun4v sparc SUNW,SPARC-Enterprise-T2000
It certainly doesn't behave like C++ (not sure about PHP, but I wouldn't look to PHP for examples of how to do anything involving language design :-P). Functions in Python do not behave like functions in C++, though, so I wouldn't expect the same conventions to apply. In C/C++, functions are part of the way that a program is structured. They divide up your code into named, separate sections and exist throughout the life of the program. You can meaningfully ask "where is function foo() defined?" and get an unambiguous answer. In Python, programs are essentially just scripts. Those scripts can create and use functions, but creation of a function is just another thing you do, like adding two numbers or printing something to stdout. I can do something like if 1==2: def foo(): print("Nope") else: def foo(): print("Yep") Given that, it seems reasonable to me that something like def foo(bar="baz"): ... could be interpreted as "create a function named foo, intern it, and allocate the string "baz" and use that object for the default value for that parameter." In fact, I'd suggest that it makes more sense to interpret it that way: things that happen when the function is *called* go in the body, things that happen when it's being *defined* go on the definition line. 
You seem to be missing the point ...
I've been using it for a while after hearing about it on Hacker News. I really like being able to describe my command line api via its docstring and have argument parsing thrown in for free.
How does that work? I've never heard of this.
This is a complex issue, but you're wrong. There isn't any difference between pointers to mutable objects and pointers to immutable objects, the difference lies in how the objects respond to certain types of manipulation. Here's an example to demonstrate: class Example(object): def __init__(self, num): self.num = num def __str__(self): return "Example(%s)" % self.num def __add__(self, num): return Example(self.num + num) def set_in_place(self, num): self.num = num Here we've created an object that is instantiated with a number, and has an `__add__` function, which creates a new object of the current value plus the provided value. This will get called when you do 'a + b' where a is an instance of this class, which is the basis for `a += b`. def inc(a): a += 1 def set_in_place(a): a.set_in_place(1) We use your inc function above, and a `set_in_place` function similar to your append function (which mutates the object). a = Example(3) inc(a) print a # Example(3) set_in_place(a) print a # Example(1) Calling the inc function does not modify the object that was passed to it, even though the set_in_place function demonstrates that the object is mutable. This is because `a += b` creates a new object equivalent to `a + b` and points `a` in the local scope to that new object. This is how augmented assignment operators work by default. class Example2(Example): def __iadd__(self, num): self.num += num Here, we create a subclass that implements `__iadd__`, which is a method that makes the augmented assignment operator work in place. This makes it possible for `a += b` to work differently than `a = a + b`, though the latter is the default behavior if `__iadd__` is not implemented. a = Example2(3) inc(a) print a # Example(4) And you can see that using the exact same inc method, now `a` has been altered instead of reassigned. Whether an object is mutable or immutable is a question of whether or not the object implements methods that allow it to be altered. The interpreter doesn't handle mutable objects any differently than immutable ones.
Perhaps because of the round-trip to `os.system`? (Shell -&gt; Python -&gt; Subshell -&gt; Stuff instead of Shell -&gt; Stuff.) Dunno if that's worth a down vote, but it's a little strange nonetheless.
He just means using os.system to install a later version using shell commands. Not sure why he wouldn't just use the shell, though.
My university runs Ubuntu on all its student-facing servers. About as non-exotic as it gets.
I only use oldlines.
Interesting. Solaris 10 on SPARC named Polaris - Polaris was a PowerPC port of Solaris. Strange that the default Python is 1.4 - Solaris 10 was released in 2005. Python 2.0 was released in 2000.
Correct link for the lazy: http://www.youtube.com/watch?v=WyKhMggzDB8#t=1182s
I don't get it, either. I don't know how Solaris is updated. Maybe the OS was updated all along, but they never updated the Python install.
I deliberately said *most* not *all*. If every single C extension had to have large changes then that is far more work than just a few requiring changes.
It's a pretty nice looking font but I just tried it out and still prefer source code pro :/
If the code you write is the same (as is the case with concurrent), then what does it matter how it is done under the hood? How would processes versus threads versus a combination actually affect you? &gt; Processes are the wrong solution Approaches that use processes are far easier to extend to multiple machines, so aside from some increased latency, can provide even more computing resources.
From a quick glance, [pylru](https://pypi.python.org/pypi/pylru) looks reasonably ok.
Yeah, I know. The reason I overwrote `__repr__` was so that you could tell `list` objects and `mylist` objects apart just by looking at their string representations. My point is that if I want to make my own custom list object that has special functionality I need to hack around your code by making sure that my custom list never returns false in a boolean context _even when it really should_ otherwise your code will ignore my custom list and sustitute a regular one.
It's because that alpha mapped shadow is part of the window border in OSX.
Ooh. Thanks for the gitignore for the lazy. I will do it tomorrow i guess.
You can probably nuke `Fallback-OldVersion`, too, now that you're using source control. Or just add it to the `.gitignore` so that you retain the same local copy. EDIT: I'm interested in seeing where this goes, as I've been wanting to build some kind of Minecraft bot for a while. For white hat purposes, of course.
don’t forget to do `True = 1`, as well as `False = 2` at the beginning.
are you really argumenting by extrapolating the meaning of a highly context-dependent word? that doesn’t make any sense.
kde does it, too. it’s pretty if the shadow isn’t so huge like in OSX.
By way of demonstration, here's an example of why it is not always helpful to think of mutable objects as being passed by reference: def modify_argument(arg1): arg1 = arg1 + [3] mylist = [1, 2] modify_argument(mylist) print(mylist) # expect "[1, 2, 3]", actually prints "[1, 2]" Since lists are mutable objects, then if python passed mutable objects into functions using pass by reference then we would expect `arg1` not just to point to the same list object as `mylist`, but rather to literally be the same pointer as `mylist`. If that were the case then we would expect changes to the `arg1` pointer to show up in the `mylist` pointer, but this is not what we see. It is true that this is a relatively low-level detail hat is subtle and easy to miss, but the distinction is real and it matters fairly often when code deals with mutable values.
Kernel 142900-03 is actually from mid-2009, give or take. Either they installed that and never updated, or it is not patched often - like not even once in the last 4 years. But definitely the ancient python version makes no sense in either scenario. 
GitHub used to do it automatically. I think I accidentally unchecked that option. I'll do it tomorrow.
Are you sure you know what the word "extrapolate" means?
Well yeah, that's what append is for. To actually mutate the object instead of assigning a new one. Pretty much it boils down to "You need to know how '=' works in python, and treat all function arguments as if they are using using '='".
Pythonxy is more comprehensive, but only 32bit. Anaconda is 32/64 and multiplatform. More 3rd party libs seems to use pyxy as a base. I've had both pyxy and conda installed for a while and finally just settled on pyxy. There is no difference between their ipython support. 
You could commit the working version, then make a branch for the development version.
Obviously the function I wrote would be equivilent to `arg1.append(3)` in a call by reference language, but it is possible to write functions in a call by reference language that aren't just simple mutations of the original argument: def modify_argument(arg1): arg1 = {'key': 'value'} mylist = [1, 2] modify_argument(mylist) print(mylist) # expect "{'key': 'value'}", actually prints "[1, 2]" The '=' thing is just passing the buck. If you want to think of function arguments as being some special kind of assignment that that's fine, but _you still actually have to know/explain how assignment works in python_, and the fact is that assignment works by making a copy of a reference and 'making a copy' implies call by value. I know I'm being finicky and pedantic, but there is a meaningful distinction here and understanding that distinction is useful.
Ok well maybe its the meaning of words you don't understand. It was going to be one or the other.
i meant that you transferred the negative meaning that “funky” has in the context of “taste/smell” to fonts, while that word has a positive meaning in other contexts.
Tinkered around a little bit. Couldn't for the life of me get the long ping response. It's been way too long since I've done this low level packet building stuff.
I'm not an expert, but could you not download cpython2/3 (the python interpreter) and compile it in your home dir and add it to $PATH?
Super Fontastic!
I was expecting a [Better off Dead](http://www.youtube.com/watch?v=QhW7rpFhr2k) reference, came away disappointed. 
Why yes you did. =)
Holy mackeral... my university has a server called polaris, so I just had to login and check the python version. Phew, not us!
Nice stuff bro. Have you seen ­[pyCraft](https://github.com/ammaraskar/pyCraft) yet? You might be interested.
This is 80/20 or however the split is perceived. For the 80% case, it is absolutely the right thing that you tell Python at a high level what you are trying to achieve - eg call this function on these items as quickly as possible. In Python we don't tell the runtime how to do dicts - it just figures it out. (Contrast with Java where you do have to say how you want Maps or Lists implemented.) concurrent is perfectly fine for this, and under the hood the implementation can be evolved/fine tuned/reimplemented providing the high level semantics remain. You are right that there are some circumstances under which multiple processes can't be used (the 20%), but that doesn't necessarily apply to every thread. multiprocessing can be made to proxy calls back to the owning process. Or both multithreading and multiprocessing could be combined at the same time using threading for "sticky" resources and processes for those that are not, or less coupled. Implicit in your talk seems to be that nothing less than using all cores is acceptable. eg on an 8 core system you should be able to get an 8x speedup over current CPython. But what if it was only 2x or 4x - both a big improvement over today? Does the mindshare still go away? And how big is this 20% group anyway? &gt; welp, I can't write &lt;x&gt; in Python I missed the part where those developers aren't allowed to use IronPython or Jython or PyPy. Why must CPython be what they use and why must CPython be the one that solves it, at enormous expense and loss of compatibility? It looks like PyPy is CPython 4 anyway. I'll also note that you can always write &lt;x&gt; in Python - your concern is performance improvements if more than one core is available. What will help mitigate things the most is moving C extensions to using ctypes/cffi since that will make them portable amongst the Python implementations and allow easier changing of the underlying VM. However this will take a really long time since it is practically a rewrite. What I'd like to see the future direction is multi-machine - ie being able to (easily) spread the work in a process (threading), on the same machine (multiprocessing) and across multiple machines. I'm okay with Python being less efficient at runtime because of the massive gains in productivity writing the code, as well as fewer bugs, easier testing, easier diagnosis etc. I get most of my performance gains from changing the algorithms I've used. C extensions remain a last resort. &gt; And that's the point I was trying to make in the talk but don't think I got across The point I missed is why Python must be relevant to 100% of developers. What is wrong with it having a sweet spot, and passable solutions to concurrency?
Yeah I'm a big fan of Source Code Pro. I tend to switch between that and Inconsolata-g. 
&gt; Implicit in your talk seems to be that nothing less than using all cores &gt; is acceptable. eg on an 8 core system you should be able to get an 8x &gt; speedup over current CPython. But what if it was only 2x or 4x - both a &gt; big improvement over today? Does the mindshare still go away? And &gt; how big is this 20% group anyway? Do you know a method where we could get shared state multithreading on 2/3 of the cores of the machine without removing the GIL? If not, then I don't see where this question is going. I expect a GIL-less Python to get similar performance gains to C or Java. I wouldn't expect an 8x performance increase running on an 8 core machine--it's a rare ("embarassingly parallel") problem set that will permit that. Again, I don't know where you're going with this. &gt; I missed the part where those developers aren't allowed to use IronPython &gt; or Jython or PyPy. No, what you missed was barely anybody uses those implementations. It's great that they exist, certainly. But when people think Python, they think CPython. It's the most popular implementation by an enormous margin. Even having conceded the performance crown to PyPy, it still has all the packages. It's a rare real-world Python project that doesn't need external native libraries, and very few of those get support in the other implementations. Also, PyPy has a GIL. (The STM work is research and way way early.) As for moving to ctypes/cffi, I've lived with ctypes (memcached client and server protocol libraries) and debugging was miserable. I simply wouldn't recommend it for production code. I don't know about cffi, I haven't used it, but its error handling would have to be greatly improved over ctypes before I'd consider it for production work. Glue libraries tend to provide a far nicer experience. &gt; What is wrong with it having a sweet spot, and passable solutions to &gt; concurrency? I guess I have to repeat myself. What's wrong is that I predict a massive shift towards multicore, to take advantage of the umpty-ump cores that desktops, servers, and even laptops will have in the future. Languages that don't keep up will be abandoned in favor of ones that do--the "sweet spot" that Python currently occupies would shrink to nothing. If CPython has a GIL in ten years, it will lose relevance, and I fear it will take Python down with it. I love Python and don't want to see it relegated to legacy status.
My primary domain controller has always been called Polaris, since early last decade. My Windows servers are all named after stars (as well as my Solaris server, which was called Solaris) and my Linux servers are named after Egyptian, Norse, Greek and Roman gods.
But ingolemo's comment you were correcting was correct, and you corrected it with something that is easier to reason about in the simple case, but technically wrong. When you call a function, you pass a value that references an object. If you change the value to reference a different object, you're only changing it within the scope of the function. If you change properties of the referenced object, you change the object outside the function, but this is only possible for mutable objects. Mutability doesn't change how values get passed to functions. The point I was making is that even with a mutable object, some methods and operators alter the object while others create references to new objects. It's not a matter of mutability, but a matter of what you're doing to what objects. Immutable objects are only a special case in that they provide no mechanism for the item being referenced to be mutated. 
For GUIs, I've always just used [tkinter](http://docs.python.org/3/library/tkinter.html) (Look into ttk if you want it to be reasonably pretty though). What you need is fairly trivial, so it shouldn't take too much learning to figure out what you need. py2exe's worked for me fairly well in the past too.
Ah yes, the good old days of try/except/finally not being unified.
Yeah they die slowly. I just put down an old Solaris box that was 14 years old.. It had to go, no more hardware support, ditto for software, and RBAC was whack to manage. There was no LDAP or AD support available, just getting OpenSSH on the damn thing was a heavy lift, mainly because some genius decided to set it up with as little as possible (obscurity security mode). Our Sally Struthers Matchbook School of Unix Admin graduate (who's 60 btw) lazy ass slacker so called 'admin' claimed to have "managed" it using WebMin of all things. My boss gave him the BOTD and tasked him with migrating it to RHEL on our VM stack. Two years later when asked why it was still running, he claimed that he couldn't sync the files with the new box. I was tasked with decomming it and actually, physically cutting its power cord into 4 pieces (after we transferred the files of course). That was our last exotica system. Now we're pretty much a RHEL shop. Its sad yes, but at the end of the day, easier to provide good service to faculty, staff, and more importantly the students. I use the Sun box as a plant stand now.
Alternatively, just use uWSGI touch-reload http://uwsgi-docs.readthedocs.org/en/latest/Options.html#touch-reload 
Perhaps the quote at the end , should be at the start. 
I don't know what "partial GIL removal is", that sounds like being "partially pregnant". Nor do I have any idea what you mean by "overhead in communication". But "replacing GIL with plentiful locks" really means "removing the GIL and adding plentiful locks instead". I fail to see how this is meaningfully different from my proposal to remove the GIL.
I've never used EasyGui, but seconding tkinter for basic stuff (it's included by default in the standard library, unless something's changed). If you need to get more advanced, pyside (Qt) is my go-to windowing toolkit. Extremely powerful, well documented and perfectly cross-platform. py2exe works fine for me also. There are alternatives, such as pyInstaller or cx_Freeze, but I've never needed to try them.
Wow it's awesome and really promising. Why JSON and not YAML, for example? Does the code lives in a separate repo or is it all contained in the `distil.py` file? EDIT: Well I guess it's in that file, since it's almost 8000 lines of code! BTW, why this trick?
okay
Slide #39: Never do this. Ever.
There's really only one appropriate course of action here... Run a local privilege escalation exploit on the server, get a root shell, and start upgrading away... Vigilante sysadmining ftw.
You could waste your time with Tkinter, but Qt (and its Python bindings, PyQt) is a wholly more nicer toolkit to use whilst also being somewhat relevant to boot.
How about you learn how to use version control instead of using shitty practices and your forgetfulness as a reason not to?
&gt; I remain completely unconvinced of how all of Python "shrinks to nothing" because some of Python doesn't work work well for some of the developers some of the time. You obviously think different. The big thing you're missing here is that programming at large is changing. You may think that you're speaking for the majority of developers (The 80% as you put it, ridiculous number!) but you are not. Machines (like Larry said in his talk) are moving *entirely* to using &gt;1 core. If Python is always using a single core, Python will become irrelevant when up against languages which provide the high-level language features along with good multicore performance (Ala, Go, Erlang (this has been around for 20 years!), Clojure etc). Being obtuse about it and saying "Welp, just using futures.concurrent" and let the runtime evolve to optimize the underlying implementation is *stupid at best* and wholly ignores why the majority of people are trying to *achieve* concurrency in the first place: Parallelism. Finally, no matter what you will say about alternate implementations will ever take away from the fact that CPython is the 'default' Python implementation. It's incredibly daunting as a newcomer to have to research all these competing implementations for the sake of just learning a new language. For now, CPython is king and we should improve it as such.
"True multithreading" means two threads of control running simultaneously on distinct hardware resources, e.g. CPU core, scheduling front-end. With the GIL in place, python can only provide this functionality though `multiprocessing`. Imagine a workload like: read a bunch of data, do work to produce more data, aggregate and process. With `multiprocessing`, this will necessitate a certain amount of copying. How much and how costly depends on the details of the algorithm, but the load can be significant in many cases. Another example is work stealing algorithms, which have proven quite effective at real-world work distribution but cannot be implemented efficiently without shared mutable state. I agree that pushing developers toward higher level APIs is a good practice. However, the implementation of that nice, maintainable API could be supported by a thread-based executor when that approach made sense for the problem. Replacing the GIL is the first step toward making that possible. 
many people suggest bpython, since “ipython isn’t made for this” and/or “ipython extends python”, but those extensions are always syntax errors in regular python, so i don’t see the point here. also i found ipython’s “?” and “??” to be of so much use in interactive shells that i rather use ipython (the further is like help() and the latter shows the source code of e.g. a function)
is there a video of the talk?
Had the same question, so I looked it up. But it doesn't seem to be uploaded yet. You can find the videos of the conference [here](http://pyvideo.org/category/30/pycon-au-2012) or [here](http://www.youtube.com/user/PyConAU). If you check this [link](http://www.youtube.com/user/PyConAU/search?query=Richard) in regular intervals, you should be get it after it was uploaded. **EDIT:** Video now on [Youtube](http://www.youtube.com/watch?v=H2yfXnUb1S4).
Slide 54 was my favourite. https://pypi.python.org/pypi/overload Now I really want a video of the talk.
I haven't tested that but the docs claim that it also stops workers then starts them. They have "chain reloading" as well which will do a rolling restart. I use some Tornado processes which don't work in uWSGI but do in Gunicorn.
CDE exudes science.
&gt; Python is a tool Programming languages are more than that. The enjoyment a programmer can have in working with a good programming language is (or rather, *can be*) similar to how a musician feels about his instrument of choice. The instrument is more than just a tool for getting some job done (play a tune). There can be real enjoyment and fulfillment in working with the instrument or programming language itself, to a degree that I don't think exist between a smith and a quality hammer. Not that I think programming is art. I just think there's more to it than working with a tool to achieve some goal.
Oh look, someone watched Dave Beazley's metaprogramming talk.
Fonts are rarely off-topic :)
Some crazy stuff in there altogether. But Richard Jones, if you're reading this, please invest some time in the future in making your slides at least *somewhat* consistent in terms of font-size and layout. Reading through the presentation was extremely disorientating.
&gt; This is Java. Don't do this. Way ahead of you.
Refreshingly exploratory and bizarre!
That mongoDB excerpt from slide 12 makes me laugh every time.
loading variables from JSON (slide 35) looked useful for half a second, then I remembered that I could just, you know, do it the normal way.
ALL I want is 2 boxes: 1) text prompt, 2) Msgbox I played with Qt, it's waaaayyyyy overkill 
Do you mean the `rmap` has a performance bug? I think it would actually be in `conj` in that case, rmap doesn't do much at all. (I'm not too worried about the efficiency of the pre-reducers code) Edit: Ok, yeah `l + [item]` perf is super bad for big lists. I've changed that in `conj` accordingly. I guess I've been spoiled by Clojure's efficient immutable collections.
..because ASCII, UTF-16 and UTF-32 are all fixed-length encodings. This allows the interpreter to locate the nth character of a string really quick, as it doesn't have to read first n characters to determine the position of the nth. 
Use a terminal, then. EasyGui is waaaayyyyy overkill.
I mostly like bpython for the tab completion and documentation viewing stuff, although i haven't really used ipython enough to be familiar with its weakness. The "??" feature sounds really useful, though!
&gt; Use a terminal, not those users
Yup, you've been spoiled. So Clojure lists support O(1) append? Sweet. Like I said though, this demonstrates good technique overall, and these techniques get little attention in Python. It's good to see someone writing about it :-) 
Thanks! I mostly picked python because it's really easy to read the examples, even for people that don't know python, but it is the language I work in most. Clojure separates lists and vectors, which are linked-list- and array- backed respectively. Lists have O(1) insert at the head, vectors have O(1) append. The real trick is the magic dance that Clojure does to make its collections space-efficient -- it presents a really tight interface for immutable collections, but in reality it re-uses as much as it can.
Great article on thinking differently about python variables. A must read if you come from a C/C++ background. 
ipython can do tab completion as well.
Good overview. I disagreed with &gt;name can refer to an integer, and then to a string, and then to a function, and then to a module. Of course, this could be a very confusing program, and you shouldn't do it, but the Python language won't mind. I actually think using different types with the same name can make things more clear in many situations. In my code for example if I have a list of things that later gets transformed into a dictionary of the same things I often leave the name the same. That makes it much easier to track the progress of some collection of data through a function. Or for another example if I change an int to a float I will keep the same name. One thing not mentioned in the article is the weird way python treats small numbers. Try this. x= 1 y = 1 x is y x = 500 y = 500 x is y Of course you should not be doing comparisons like that with is but this oddity occasionally does pop up and breaks how you would normally think of references in python. 
fwiw, by "you shouldn't do it," I didn't mean that you shouldn't assign different types to the same name, I meant you shouldn't use one name to refer to an int, then a string, then a function, then a module. :) The "is" thing with small numbers is alluded to in the list of other topics at the end: how come "2 + 2 is 4", but "1000 + 1 is not 1001"? 
Fair enough. However, looking at [clojure's docs](http://clojure.github.io/clojure/clojure.core-api.html#clojure.core/concat), I think this algorithm is O(n^2) in closure as well -- forcing the first element via 'first' will require forcing the last concat, which will get the first of the input which will force the second to last concat, etc... each which will perform a single cons, for 'n' total cons. Each successive element fetched will be one less cons to force, so sum( n, n-1, n-2..., 1 ) = O(n^2) to print the result.
&gt;x= 1 &gt;y = 1 &gt;x is y &gt;x = 500 &gt;y = 500 &gt;x is y This actually makes perfect sense if you know what's going on. I'm surprised Ned didn't mention this, but anybody familiar with the Python C API will know this since it's mentioned in the documentation. Basically, on startup, Python creates all integers between -5 and 256 in memory. They are persistent and never garbage collected. When you use them via assignment or iteration, you actually get references to the existing values. Obviously, this is one of the *many* speed hacks in Python. (Also: "needle in haystack" is faster than "haystack.find(needle)") The documentation also points out that you *could* use the C API to redefine those values. Obviously you'd never do that since it would cause bad things pretty quickly, but they wanted to point out just how powerful the API is. Edit: [Pointer to where this is documented](http://docs.python.org/2/c-api/int.html#PyInt_FromLong). It's actually all integers between -5 and 256, not the first 100.
Ah, yeah, `concat`-ing lists isn't so efficient, since concat is basically: (cons (first s) (concat (rest s) y) This is the fault of concat operating on a seq interface, without considering the underlying datatype. However, the built-in `conj`, which operates on a collection and a single element-to-insert, is part of the interface definition for each collection, and is independently defined for each of them to do the operation efficiently, which is why `(conj (vec 1 2 3) 4)` is [1 2 3 4], but `(conj (list 1 2 3) 4)` is (4, 1, 2, 3). So, O(1) to add 1 element to a list or vector with `conj`, and O(n) to do that over a sequence: `(apply (partial conj [1 2 3 4]) [5 6 7 8])` Edit: Or, more succinctly: (conj (conj (conj [] 1) 2) 3) ;O(1) O(1) O(1) =&gt; O(n) vs (concat (concat (concat [] [1]) [2]) [3]) ; O(3) O(2) O(1) =&gt; O(n^2) Having `concat` (`.extend` in python) be O(1) is easy for single-type memory arrays, but not really possible (or at least, practical) for object lists in any language I know of.
Partial GIL removal would for example be removing it from core CPython objects but leaving it present for C extensions. Or only removing it for some subset of objects that are used the most but keep it for the rest.
The way it's done for lists is usually to mutate: def reduce1(xs, initial, combine, final): acc = initial for x in xs: acc = combine(acc, x) return final(acc) def cons(hd, tl): return [hd, tl] def listmap(lst, fn): def f1(acc, x): head, tail = acc tail[1] = [x, None] # &lt;-- update the tail return (head, tail[1]) def final(acc): return head[1] head = [None, None] return reduce1(lst, (head, head), f1, final) The `(head, tail)` tuple acts as a resetable-iterator over the list, making it `conj` in O(1) while maintaining the correct list order. 
Oh yea I guess I was just reading between the lines wrong. Definitely should not be using the same name for random things. Totally missed the section at the bottom. 
Have you actually profiled your application or did you estimate those numbers? For the application I'm working on now, the total cost of copying is about 25%, meaning I am essentially wasting a whole processor on my 4 core server by being restricted to multi-processing. Obviously, this is workable and still offers an appreciable improvement over single threaded operation but it still means significant overhead. Maybe the GIL can't be fixed in a sane manner; I won't argue that point. Either way, programmers will eventually shift to a replacement that doesn't suffer from the same issue, be it Python 4 or another language. 
Oh, I see you want to open a folder, for that you will want `tkFileDialog.askdirectory` (see docs [here](http://tkinter.unpythonic.net/wiki/tkFileDialog))
 import sys sys.stdout = open("/tmp/foo", "w") print "Well, there you go." sys.stdout.close()
Have you already found a way to install? As in, what happens after you need your users to update their program? I've used esky for mac which, once you get the hang of it, is pretty awesome. But i would love to see any (easier) alternatives.
I understand a good deal of the complexity involved with replacing the GIL. No one is saying that it would be easy and it may turn out to be intractable. You're missing the point: unless Python can solve this problem, it will be left behind in an increasingly parallel world. 
What an excellent read. I really enjoyed that. Ned, if you ever write a Python book, I'm buying it.
You are confusing CPython with Python. The presentation claims that the *only* solution to a problem is that CPython must change, and doesn't allow for other implementations such as PyPy, or for evolution of CPython such that it gets better but not perfect or linear. The Python *language* has no multi-threading, multi-processing or multi-machine constraints.
&gt; Dave Beazley's metaprogramming talk [this](http://www.youtube.com/watch?v=sPiWg5jSoZI) one?
I know this is nitpicking but... &gt; Python keeps track of how many references each value has, and automatically cleans up values that have none. The author clearly is aware of tracing garbage collection, and mentions it's an implementation detail, but I feel the "Python keeps track of how many references each value has" part implies reference counting, while "Python keeps track of whether each value is referenced or not" applies equally to reference counting and tracing collection, though it's kind of awkwardly phrased. Aside from that I think it does a great job of explaining the model in precise but digestible terms.
Yeah, some of the stuff in this talk was in that. The style, etc.
Well, first, there is no internal distinction between "core Python objects" and "C extensions". They use the exact same API. The same goes for "some subset of objects that are used the most" and "the rest". Second, (just about) every Python C API call requires that you be holding the GIL when you call it. In other words, to the degree that you remove the GIL, you break the C API. And if you break some essential part of the C API, you've already broken every C extension. (Yes, you could only break non-essential parts of the C API--but how are you going to reap big multicore benefits without changing essential parts of the API?) So in summary, when you "partially" remove the GIL, you still break all C extensions. So I don't see a benefit to only partially breaking the C API. This is what my dad calls "cutting off the dog's tail an inch at a time". That's why I propose doing it all at once--to get it over with.
Nice. Yeah UDP multicast has a lot of great uses as message passing between nodes in a server-less enviroment. I ran into some issues with the python UDP socket implementation last time I tried this. (if both the sender and client are on the same machine.) OS X and Linux where fine.
The Python dev team have been doing opposite. For example all the import mechanics were rewritten in pure Python so it could be reused in all implementations. Similar changes have been made to the test suite. Is a link on the download page pointing to PyPy saying ("for maximum threaded performance use this") so bad? ie CPython is increasingly moving to one implementation amongst many, and Python the language is what is important. It is clearly impractical to completely remove the GIL without massive effort, years of disruption and loss of C extension compatibility. Wishing it wasn't that way doesn't solve anything. It isn't like developers haven't been trying to figure that out for over a decade. If you have a solution that works, it will be gladly welcomed. What I don't understand is why you and others find evolutionary solutions (with more overhead) unacceptable. What is wrong with being practical? What is wrong with something that works today and can be incrementally improved without the loss of compatibility, massive effort and disruption? What is wrong with Python not being the best solution for every developer?
If you can come up with solution to removing the GIL that doesn't involve years of disruption and requiring a rewrite of every C extension, and a huge amount of effort, then everyone would be glad to hear of it. No one has come up with anything yet, other than doing a new interpreter from scratch. For some reason none of those are acceptable to you, even renaming PyPy to CPython 4.0. And yes you can do partial GIL removal. For example integers could be switched to using some form of GC other than reference counting with their reference count calls turned into no ops, and other operations are read only anyway so no locking is required. Other immutable types can be done in a similar way. Mutable types would need to resort to internal locking to ensure re-entrancy, with dicts being the most important. C extensions that care about the GIL make particular API calls. It would be okay to have a GIL for C extensions leaving the existing API in place, while allowing some degree of free threading for pure Python code. APIs can then be added so that C extensions can opt in to free threading, or remain bound to the GIL. The incremental approach is pretty much how many operating system kernels became multi-threaded. They start out with a [big kernel lock](http://en.wikipedia.org/wiki/Giant_lock) and then replace the hot spots with finer grained locking.
Where do you close all the file descriptors ? open/dup/dup won't yield 0/1/2 if you already have them open
Yes, very good point! Let's close them.
This should solve it: https://github.com/waawal/undead/commit/de801d053a8d0e3161ff07abbd0e39e2b93f76af Thanks a lot for noticing and giving your feedback!
Did anyone else try running the tests? I ran them and the centrality algorithms failed.
No double-forking? *Dependencies*? Take a look at [this](https://github.com/django/django/blob/master/django/utils/daemonize.py). A non-module in `sys.modules`? That's just asking for trouble.
does this do anything that https://github.com/dwolla/dwolla-python does not do?
For a deeper background: http://eli.thegreenplace.net/2012/04/16/python-object-creation-sequence/
&gt; What I don't understand is why you and others find evolutionary solutions (with more overhead) unacceptable. What is wrong with being practical? There is nothing wrong with being practical. Multiprocessing is fine for now; it's the *future* of Python that concerns us. Process-based solutions cannot be "incrementally improved" to rival thread-based solutions in the general case. &gt; It isn't like developers haven't been trying to figure that out for over a decade. If you have a solution that works, it will be gladly welcomed. That's not my understanding of the situation. Guido has shut down several efforts to remove the GIL, making it clear that he doesn't see it as a major problem. He is not willing to make tradeoffs in favor of multi-threading. I would help, but I posses neither the time nor the expertise. &gt; Wishing it wasn't that way doesn't solve anything. Neither does pretending the GIL doesn't matter.
Sorry about that. I basically rushed some of the final presentation polish. The images went in about 5 minutes before the talk :-)
That could create some side-effects (eg: you open some files/sockets/etc before daemonizing and then expect to still have them). See dup2 in anossov's link. Note that if your logging setup fails you have absolutely no way to find out what the cause is - you just redirected stdout/stderr to devnull. This is quite a bad practice. I think that self-daemonization is just bad. It's fun to implement it but you'll end up choosing "no features but reliable" or "features but hard to debug startup". Neither is desirable. Doing external daemonization (eg: supervisord) is always better - you get features, reliability and it's easy to debug startup issues.
What's wrong with [multiprocessing](http://docs.python.org/dev/library/multiprocessing.html)?
Well lots of things... but in this case it's not the right tool for the job. multiprocessing is for doing work in multiple processes, `undead` and other daemoniziation tools are for well, daemonizing, so you start with one process spawn (at least) one more process but only the spawned processes stick around to do work, but these children are unmolested by any parents and can continue doing work in the background.
I'm not looking for a job, but I'm willing to donate my time for this cause until you find someone to hire. 
&gt; undead and other daemoniziation tools are for well, daemonizing derp. I wasn't thinking. You're right, multiprocessing isn't going to enable me to daemonize a running Python process.
can you Skype me at maksim2042?
See also http://www.reddit.com/r/programming/comments/14g366/a_python_variant_with_seamless_dynamic_and_static/
From http://www.mypy-lang.org/faq.html#cython &gt;Cython is a variant of Python that supports compilation to CPython C modules. It can give major speedups to certain classes of programs compared to CPython. The mypy VM is a bigger rethink of the entire language and the implementation. Mypy differs in the following aspects, among others: The mypy syntax is arguably simpler and more "Pythonic" (no cdef/cpdef, etc.) for statically typed code. The mypy syntax is compatible with Python. Mypy programs can be run as normal Python programs using CPython, for example. Cython has many incompatible extensions to Python syntax, and Cython programs generally cannot be run without first compiling them to CPython extension modules via C. Cython also has a pure Python mode, but it seems to support only a subset of Cython functionality, and the syntax is quite verbose. Mypy has a different set of type system features. For example, mypy has genericity (parametric polymorphism), function types and bidirectional type inference, which are not supported by Cython. (Cython has fused types that are different but related to mypy generics.) The mypy type checker knows about the static types of many Python stdlib modules and can effectively type check code that uses them. Cython supports accessing C functions directly and many features are defined in terms of translating them to C or C++. Mypy is not bound to any particular target language and can support both C and Java backends, for example. However, accessing C library functionality in mypy will not be as easy as in Cython. Our aim is build a new mypy virtual machine that allows speeding up all major parts of the implementation, including standard libraries and the garbage collector. Cython uses the normal Python VM. The mypy VM aims to support efficient multithreading without the GIL for a wide range of programs. Cython programs can explicitly release the GIL, but manipulating Python objects is not possible when the GIL is released. Note however that the mypy VM is still very early in development. 
&gt; To me, this idea counts as a replacement for the GIL No, the GIL is still there. It is just that instead of being used 100% of the time as is the case today, it gets used 99% of the time with finer grained/different locking for the 1%. Then the proportions change over time. I was merely showing that it is possible to have the incremental improvements you denied were possible. &gt; providing true multithreading should be the primary objective for long term Python development The Python language has no such multi-threading, multi-processing or multi-machine constraints. For some reason other VMs or even calling PyPy "CPython 4" aren't allowed by the GIL-ectomy advocates for unspecified reasons. &gt; no consensus that it is even a problem worth addressing It is addressed in Jython, IronPython, PyPy, several Python to C/C++ translators etc. For some unspecified reason those aren't allowed to count. &gt; I'm sorry, but I refuse to believe that this problem is intractable. It is the same order of magnitude of work as writing a new interpreter. The new interpreters are dismissed for unspecified reasons. If you have a solution then show it. There is nothing stopping you from showing everyone how they are wrong. You are also allowed to consider that the enormous expense (time, money, compatibility, disruption) to change CPython is not considered worth it. CPython doesn't have to be everyone's solution to every problem. Guido has a track record of being a good BDFL. As I showed even the predicted py-apocalypse still leaves us better of.
hey man you've been really helpful on #python. This article is good stuff, delicious'ed 
There may be freelance work, or compensation in stock or options, or combination of the two. We're a real startup and part of a major startup accelerator program. I'm keeping details mum in the public forum, but feel free to skype me if you are interested. 
Pypy has a GIL too, though...
try import this ?
&gt; constructors are likely the most popular method for instantiating objects. You don't say...
Looks like stuff we already have with: from functools import reduce import itertools # then read [this](http://docs.python.org/3.3/library/itertools.html#itertools-recipes) I really hate it when a Clojure, Ruby, JS, Perl, ... fanboi starts talking about "hey, my fav language has just implemented this feature that I don't know has been in Python for a long time already - Python sucks!" 
Sorry to be "that guy" i=(datetime.datetime.now()) print(i) if i==("2013-07-09 00:00:00.000000"): print("Happy Cakeday to me") Line 1: Parens are not necessary and just add visual noise It's nice (PEP 8) to use spaces around operators: `i = datetime.datetime.now()` Also `i` is not a great variable name, maybe `now = datetime.datetime.utcnow()` Line 3: again spaces around operator ` == ` (also parens) This test will never succeed and line 4 will never execute. This is because a datetime object cannot equal a string You might consider: my_cakeday = datetime.date(2013, 07, 09) today = datetime.datetime.today().date() if today == my_cakeday: #see how this is really easy to tell what i'm testing? variable names are crucial print "Happy Cakeday to me" anyway...Happy cakeday i guess? 
Looking forward to seeing your implementation of this in Python from 30 years ago. I mean, forget NIH syndrome, forget that re-implementing a half-assed solution is still a good learning exercise; Python hasn't changed at all since Guido first published version 0.9.0 so there's no value in ever revisiting any problem - right? What kind of moron wants to flatten expensive loops into inexpensive list comprehensions? Generators are for poor people who can't afford eighty million petabytes of ram - back to your slums you dirty dogs! And who needs multiple processes? One single process with every operation carefully locked in the GIL was good enough for your grandad so its good enough for you. The human brain can only hold a maximum of 7 items at once and you're already breathing, sitting, looking at a screen &amp; scratching your bum so there's not a lot of room for debugging complex multithreaded operations anyway.
"Callable module"? Just say no.
I found this article helpful in understanding.
Haha, no need to worry about "being that guy". It's all good. I realized my cakeday was imminent at around 11:58 PM, so I whipped this up pretty fast. I'm a bit new to Python, and thought that would be a bit of a nice way to both practice using the language and celebrate. 
I haven't used Go yet but it was a nice article that encouraged me to consider it next time I have a problem that needs heavy concurrency (I usually go for erlang). With that said, this little one suprised me: &gt; No unpacking of a tuple or list into separate variables (e.g. x,y,x = [1,2,3]) Does anyone know the rationale? Not a deal breaker but it's such a nifty feature.
I knew pyCraft but never checked the code until yesterday. Seems like they have some really useful code that I can try. They implemented the whole protocol which is time consuming. Thanks for pointing that out.
Awesome! How does [mypy](https://github.com/JukkaL/mypy) compare to / work with [PyContracts](http://andreacensi.github.io/contracts/) (typechecking constraints as [PEP 3107: Function Annotations](http://www.python.org/dev/peps/pep-3107/), docstrings, decorators) ?
It indeed does but with the direction that pypy is going, the gil becomes irrelevant there, STM and whatnot so in 10 years time, pypy might probably be the defacto python implementation and cpython has become less relevant.
 Problem signature: Problem Event Name: APPCRASH Application Name: python.exe Application Version: 0.0.0.0 Application Timestamp: 4a4511fa Fault Module Name: python31.dll Fault Module Version: 3.1.150.1013 Fault Module Timestamp: 4a4511e0 Exception Code: 40000015 Exception Offset: 0008ae32 OS Version: 6.1.7600.2.0.0.256.1 Locale ID: 1033 Additional Information 1: 797a Additional Information 2: 797a05e23c16fa32242738fd5f81bc0e Additional Information 3: 4837 Additional Information 4: 48370b85147d40a59ce64ef9a2289851
Bullshit. We have medicine from cancer. Nobody just really needs it. People use chemicals all around, they put some chemical creams on their body, they eat food that their body doesn't want and doesn't need. There is natural medicine in Himalayas, available for those who really seeks and does something to change their life (remove couse of disease), not just get a fucking magic chemical pill. All the others is OK to suffer and die. They just want it. Any project claiming to help with cancer - is bullshit by default.
Brilliant post, would upvote again A++++.
Neat, thanks!
You'll have to back up your claims with solid science or you'll just be laughed at.
Thank lordy its free, because there's still a lot of projects out there that could use something like this ;)
Are you a developer or a business/marketing person?
Except that constructors are not *instantiating* objects really, even in other languages like C++. Python exposes the new/init duality, other languages don't expose it - but it's still there.
I have been using pyCraft for my small needs with great success. Even though they have not finished implementing the whole thing, I was able to implement just the sections I needed for what I wanted. All I needed really was the whole account login/server login which was there. Anyway, keep up the good work!
Hmm, I originally had some paragraphs about how names can't refer to names, only to values, and that assignment never affects the old value of the name. It felt too long, so I removed them. I'm not sure what I said in the piece that implies updating one will change both. If you can point it out to me, I'd love to make it clearer.
C++ actually exposes this duality in at least three different ways, as it is wont to do: placement constructors, overriding `new` (globally or per-type), custom allocators for STL containers.
Can you tell us a little about your background and the background of your colleagues? Don't take this the wrong way, but I'm a bit skeptical about your ability to actually succeed in making something like this, without knowing more about the goals of the project and your background. I'd love to be proven wrong, but I'm not sure if you're aware of the magnitude and complexity of a project like this.
So, what, now I'm opposed to any and all innovation in computing? Don't be ridiculous. Daemonising a process is *not remotely* the same kind of problem as iterating over a range of values, generating a large list, taking advantage of multiple processors, etc. It's not something which can be completely reconceptualised in a drastically different way leading to more powerful or efficient solutions. The correct way to daemonise hasn't changed in 30 years because the *heart of Unix hasn't changed in 30 years*. Signal handlers, file descriptors, process tables, all of this stuff is the *unchanging bedrock* on which later innovations like generators and multiprocessing have been built. If you didn't double fork with a setsid() inbetween on BSD4 in the 1980s you ran the risk of creating zombie processes, and if you don't double fork with a setsid() inbetween on Linux in 2013 *you run the same risk of creating zombie processes for the same reasons*. Nothing has changed. You can't innovate your way around a fundamental requirement imposed by the operating system.
Hello reddit, I've written this module to help with the common task of converting audio files into different formats. And since I'm fed up with all the different CLIs of all the encoder and decoder programs, included a simple CLI to use it easily: audiotranscoder path/to/my.mp3 output/to/this.ogg It is also already part of another project I am working on, [CherryMusic](http://fomori.org/cherrymusic), in which AudioTranscode is used to live transcode audio files so that they can be streamed and has proven to work quite well. I hope you like it. I'm open to suggestions, feedback and critique of any kind!
Looks awesome. This may be more of an /r/learnpython question, but: Is it normal to write "all" your code in __init__.py? And not in a separate module? https://github.com/devsnd/python-audiotranscode/blob/master/audiotranscode/__init__.py
Yes. The alternative (if you want to be able to access the package's contents without having to import the submodules) would be to import everything into `__init__.py`, but what would be the point?
Ah, that's nice; Combining the two would give you quite a powerful conversion tool-chain. Thanks for the link.
And for the record, besides being a c* user I have no involvement with this project or Datastax.
Kudos for using libev.
Indeed, thobbs/datastax seem to be trying to make this as industrial strength as possible.
Nice! And like the OP, I thought this was dead.
Not do be that guy but... What's wrong with all the other tools? Why not use ffmpeg directly? I don't see how AudioTranscode makes anything easier. But I'm used to ffmpeg though, as it seems to be able to do everything already.
What happened to pyQt? Admittedly i have not done a lot of GUI/Python stuff, but what i have found is pyQt/Qt Desinger/pyuic4...is this somehow better?
Of course ffmpeg can do most of the work already: This tool is merely a simplified interface. Lets say you want to convert a flac to a 128kbit mp3 without temporary files you'll have to do something like this: flac --force-raw-format --endian=little --channels=2 --bps=16 --sample-rate=44100 \ --sign=signed -o - path/to/file.flac | ffmpeg -i - -f mp3 -acodec libmp3lame -ab 128 output.mp3 The problem is, that I can't keep all the CLI flags in my head, I don't really want to look at the man-pages each and every time... so this is the equivalent for audiotranscode: audiotranscoder -b 128 path/to/file.flac output.mp3 I think that speaks for itself. However, if you have to do more delicate tasks at hand, I understand that you'll want the full power of ffmpeg at your fingertips.
PySide is almost identical to PyQt in usage. However, PySide is the *official* set of Qt bindings from the Qt team, while PyQt is developed by an independant company. PyQt seems to be a bit more up to date (it already supports Qt5), but is only available as GPL or a commercial license. PySide is LGPL.
&gt; Of course ffmpeg can do most of the work already: This tool is merely a simplified interface. &gt;Lets say you want to convert a flac to a 128kbit mp3 without temporary files you'll have to do something like this: &gt; &gt;flac --force-raw-format --endian=little --channels=2 --bps=16 --sample-rate=44100 \ --sign=signed -o - path/to/file.flac | ffmpeg -i - -f mp3 -acodec libmp3lame -ab 128 output.mp3 Are you sure? What's the difference between that and a simple ffmpeg -i input.flac -ab 128000 output.mp3 ? :) I usually don't need to specify anything else, since the defaults are usually good for me and ffmpegs format-detection is blizz.
Please do NOT do this: process_id = os.fork() if process_id &lt; 0: sys.exit(1) elif process_id is not 0: sys.exit(0) process_id = os.setsid() if process_id is -1: sys.exit(1) Do not use 'is' to compare integers. 'is' is for comparing objects.
Over the years I've used sys.argv, getopt, optparse, argparse and docopt. After spending a couple hours fiddling with docopt I ended up going back to argparse. Docopt is still evolving, would just fail silently leaving me to guess what part of the usage string it didn't like, has seemingly redundant config sections, and documentation that is ok, but can be a headache for even moderate corner cases. Ah, and validation via schema is very obtuse compared to simple python code or built-in validation with argparse. So, after two hours of fiddling with docopt I went back to argparse and within an hour had a really solid parser - supporting a variable number of values for an option, validation, required options, etc. Admittedly, the argparse subcommands can easily turn into a nightmare. But I'm not convinced that docopt won't also.
It's mostly the same, so you should be able to just change the import statements and everything should work. The full list of differences is at http://qt-project.org/wiki/Differences_Between_PySide_and_PyQt
Well web consists of http based interfaces basically, so you've got Twisted which deals with web and about every other possible network related activity you could ask for. Theres Openstack which is written in Python, and Ganeti which both deal with virtualization and other areas as well. I don't see what your really asking, you have experience in web development, and dont want to do it anymore. If your interested in science, Python has a ton of libraries for computation and graphing. Check out http://www.scipy.org/ and start learning those tools.
Lol. You expect to compete with (not limited to) large academic groups with millions in funding. These groups are developing and generating the data. This startup/incubator model is broke.
Great stuff, I've got two questions. 1. Since you're using print as a statement and not a function I'm assuming you're on 2.7, which is fine, but I'm just curious if it would still be global bar in Python 3, or if it would be something like global(bar) 2. In the LAST example why would you set bar up as a class variable, then call it in the function inside the class with Baz.bar instead of, what seems to me is the more commonly used self.bar? 
Looks interesting. Any comparison with [Shed Skin](https://code.google.com/p/shedskin/) python to C++ compiler? I've used Shed Skin, and it's cool, but I wouldn't try it in production code yet.
sometimes when trying to simplify a problem to make an example of it, you end up with something arbitrary and confusing that is how i feel about the post towards the end. particularly the "solution", which seems all-around weird. if the object needs to be mutable, it should be subclassed and have its own methods operating on itself. alternatively, if the function really needs to be generic, you should make it focused solely on changing the variable, and then returning it, so `b = foo(b)` where foo is something like `return b+1` (or something way more complicated, obviously)
Would this do the trick ? index = line.find(keyword) return line[index:index+y]
bingo! thanks
1. [It's the same in Python 3.](http://docs.python.org/3/reference/simple_stmts.html#the-global-statement) 2. The `foo` function is not defined inside the `Baz` class.
Brings back memories of bewildering scope bugs from when I first started Python! Thanks for the write-up!
Coursera.com and then doing practical applications at work.
I think you overestimate the computer skills of 50-60 year old "Word Typists" stuck on computers. 
Mostly the same, I am able to just replace the import and it'll work as usual. The only thing to consider is the license, pyside is lgpl while pyqt is gpl
I started by downloading and going through [the docs](http://docs.python.org/2/download.html) in my spare time, I liked being able to keep the PDF's with me to familiarize myself with the basics. Then basically I had a problem to solve and used Python for it. It just kinda spiralled from there. I'd really reccomend trying to do all [these projects in a language](https://github.com/thekarangoel/Projects) to learn the basics as well as getting deeper into the more complex problems you can solve in a given language. I'm actually doing this to learn Go at the moment!
How does it compare to Cython?
Python was my first language as a self-taught programmer. I learned by reading "How to Think Like a Computer Scientist" (the Python version, obviously).
I implemented BitTorrent bencode in C# in 2003 and liked reading the code so much, I read Dive into Python and the Thinking Like a Computer Scientist books. Previously I only messed with in reverse order: asp/vb.net, php, sql, bash, JS, vb, dos batch, Pascal, gwbasic, c64 basic... But since: Perl, awk, java, jython, processing, ofx (c++), scheme, and clojure. I have bought very few books. I still use Python daily and generally think in terms of Python or Lisp when I think of computer system problems. I think Python helped me quite a bit to become a little more well rounded.
the python challenge :)
I heard about a local conference, it was free, and I wanted to learn it. I went, and within two weeks I was writing relatively complex GUI applications. I was particularly interested in using it as a matlab replacement for my engineering classes, and then I started getting to use it at my internship and now my full time position at work.
Learned the basics from [Open Courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00sc-introduction-to-computer-science-and-programming-spring-2011/). Learned some more advanced data crunching (pandas) from the book [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do).
Alternate approach: don't bother putting magic in your program, and just use whatever facility your OS has for process supervision. [There are a bunch of good choices](http://tech.cueup.com/blog/2013/03/08/running-daemons/), and they're all pretty easy to use.
Started a new job that used python and had to start working on a project. 
Disclaimer: I'm a researcher who works on cancer in a relatively small no-profit research institute. What do you mean exactly by "open data platform"? Hopefully it won't be yet another web portal with an API of its own completely different than what it is used before? How "open" will the platform be? Sorry for the seemingly blunt tone, but I've seen a lot of "open" platforms that weren't really open (one, for bioinformatics software, even asks you to NOT share the software to colleagues, and it has "open" in its name).
www.pyschools.com
A friend messages me with a link to flask official website saying "Quickly, no time to explain! Learn this!". I learned it in 2 nights using their tutorials and that was back when I only knew little Java. Turned out he needed some hands with a web app he was building. I thank him dearly til this day
Oops, those slides weren't meant to be in the export ;-)
Reading books then take the "Introduction to Interactive Python" on Coursera. Now, I'm converting a lot of my Matlab codes to Python.
Exactly like me, that's kind of odd.
I already knew how to program (a bit, still not much of a programmer but I'm good at pretending) so I did Zed Shaw's [Learn Python the Hard Way](http://learnpythonthehardway.org/), which was still being written at the time, to get my hands dirty, and read the [Python Tutorials](http://docs.python.org/2/tutorial/) for the details.
Most of them could be a non problem, if there were a clear logical explanation of the python execution model. Chapter 4 in the docs is frightening for beginners and average programers, too. [Here is one of my attempt to partially clear this.](http://www.daniweb.com/software-development/python/threads/457567/somebody-please-explain-why-this-is-happening#post1990653)
Ten years ago I happened to borrow *Python in a Nutshell* by Alex Martelli from the public library. Coming from Basic, Fortran, and C++, I was immediately hooked on Python. 
Susprised nobody's mentioned [Google's python class](https://developers.google.com/edu/python/) -- I watched the videos over a weekend and did the exercises on a train from Germany to Austria, went to work on Monday and started coding right away. One caveat: it's for python 2x, but most of the fundamentals are the same.
The first link will probably filter for the 2012 vids. But you're basically correct, the videos will appear at [pyvideo.org](http://pyvideo.org)
At the university. We had the best professor ever (real python fanboy)
What's the point of making a decorator if you can use it only once?
Google.
Played on http://www.learnpython.org/ for a few hours then opened up sublime text and learnt via trial and error.
&gt; The first misconception is that Python, being an interpreted language (which is awesome, I think we can all agree), is executed line-by-line. In truth, Python is being executed statement-by-statement. Still false. Better bite the bullet and admit that Python is not an interpreted language, in the same sense as, say, bash is. When you import a module it gets parsed into an abstract syntax tree, then compiled to bytecode, then executed (see `parse_source_module` function in `Python/import.c`). When you feed the interpreter statements from the console, then yes, it compiles and runs them one by one, with mostly the same results. But try to run this as a file, for example: print 'Yo' def f(): if False: break The `print` is never executed (unlike it would have been if you inputted that from the command line, obviously), and you can bet your left pinky that that SyntaxError has nothing to do with syntax, there's no separate version of the `if` statement (etc) in the formal grammar that doesn't allow `break` or `continue` inside, that error is reported by the bytecode compiler. Among other things, if you insist on saying that no matter what the implementation does, logically it's as if Python is interpreted statement by statement (except for those prescient syntax errors), then you should make a qualification: except when it's any statements inside a function, which kinda defeats the purpose of pretending that Python is interpreted in the first place, because most of your statements actually are?
A friend of mine showed my Pylons. It was overwhelming, and I took a good few steps back to learn the basics. In love with Python ever since.
By learning Django on the job, starting last September. It was good fun.
lmao are you actually telling me to stop Java-ing in defense of a solution that looks like `class Baz(object):` ` bar = 42` ? I thought that Java people were precisely the ones that couldn't stop compulsively applying the object-oriented model to every problem, and that creating with classes with nothing but an `__init__` and a single method was poor form. C.x += 4 makes no sense. This is what I mean, the example is so abstracted (let's perform addition in a convoluted way!) that I can see no practical application to it. Please give me an example of an optimal situation for which the solution is a method that changes a class attribute, and thus affects every instance of the class, as a byproduct of a function in which it takes the instance as an argument to do something else.
I learned Python when a project I was working on hired a Python guru. I already knew Fortran, Perl, Matlab, and C, and Python appeared to let me use the best of all of these. I got good at Python by reading the Python Cookbook and trying to understand why they did what they did. I got great at Python by maintaining an open source package in the language, and by working problems in Project Euler.
Decided to to move to a proper programming language after having written a huge library for parsing, encoding, converting and encrypting files used by the Wii menu in AutoIT3. It was slow as hell and uneccesarily complex. I knew GML and AutoIt when i started on it. I chose python because i had read a lot of it to understand the filetypes i was working with.
I learnt it about 10 years ago (around 2.1/2.2) by reading its docs and source code. I didn't have reliable Internet connectivity and there weren't that many online resources back then anyway so I told myself that if I'm not sure about how to tackle one thing or another I should simply follow Python's own code. My rationale was - if I get something wrong, I will be always able to say this is what Python itself does :-) This is an approach I suggest people just starting out can copy. If in doubt, just study Python's internals - in anyone says they don't like your code just say this is what, say, Tim Peters does :-) There will be time for complaining, improvements, whining or contributing later on, but at the beginning just humbly assume you can't go wrong with that.
Ah, just the person I want to talk to. Please PM me, I want to know more about your requirements, gripes and concerns. 
https://en.wikipedia.org/wiki/Search_engine_indexing#Index_data_structures https://en.wikipedia.org/wiki/N-gram https://en.wikipedia.org/wiki/NLTK
Learned by coding. Hands on. I had a problem I wanted to solve (I wanted to make a script to manage my movie, tv show and music archive for me automatically), so I just dug and Google'd all day for specific things I wanted and having a clear goal in mind, I slowly worked my way to it. I knew basic programming ideas from C++/Java, so my code look horrible, but after a few projects, I had the basics of Python down, and I slowly started learning the Pythonic ways of doing things.
My background is data, data and more data. I have a doctorate in computer science, and have spent last 8 years teaching at a major university. As far as approach goes -- without revealing anything too closely held -- it's a "shotgun empiricism" approach at its finest -- examining billions of data points to find usable and testable hypotheses. I am aware of magnitude and complexity (hell... I've been breathing this complexity for at least a year now) but I am coming in as an outsider on purpose -- by stepping out of the well-trodden genomics path and looking at a holistic big picture I hope to see what others have missed.
I'm at this point a "jack of all trades" by necessity. But I am a computer science Ph.D. with a research background first, developer/engineer second, and marketing/business person a distant third. 
Reading the source, tests, and documentation. IDLE and *Dive Into Python*, [more php, mysql, rails], /r/django, /r/IPython, /r/plone, /r/zope, NLTK, /r/scipy, pandas, /r/pyramid. If I was to learn Python now, I would probably start with : * http://docs.python.org/2/tutorial/ * http://docs.python.org/3/tutorial/ * http://hg.python.org/cpython/tags And keep notes as [IPython notebooks and/or ReStructuredText for Sphinx](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks#Reproducible-academic-publications) for later reference. These are also great: * https://github.com/jrjohansson/scientific-python-lectures * http://getpython3.com/diveintopython3/ * http://scipy-lectures.github.io * http://shop.oreilly.com/product/0636920023784.do * https://python-packaging-user-guide.readthedocs.org/en/latest/glossary.html * http://www.reddit.com/r/Python/comments/1eboql/python_website_tuts_that_dont_use_django/c9yxl8w * http://www.reddit.com/r/Python/comments/1drv59/getting_started_with_automated_testing/c9tfxgd * https://docs.djangoproject.com/en/dev/ http://docs.python-guide.org/en/latest/ http://www.reddit.com/r/learnpython/wiki/index http://www.reddit.com/r/Python/comments/1gn9gv/what_are_some_easy_projects_i_can_get_started_in/cam3t17
https://github.com/thekarangoel/Projects
I'm not competing with them -- I'm filling gaps that NIH and other big funders neglect to look at for a variety of reasons. So these big groups are really partners. In fact, I am working WITH an NIH group right now. Re: startup/incubator model -- you will be amazed what can be done on a shoestring budget if you remove the overhead. 
Out of curiosity, what benefit does using the instance have over using the class? Is it so that modifying the attribute will only affect the instance? Why is that desirable if you use a singleton instance?
I'm surprised nobody said this, but Blender. I learned modelling and rigging and I wanted to add some features Blender lacked at the time (2006). It quickly became my favourite language until recently.
I played a little with the Python tutorial, then dived into Learn Python by Mark Lutz.
I just found it; I haven't used it. 
Thank you for your update. Far too often, people dive into these kinds of projects with an overconfident mentality like "Everyone is going to use this application and share all their data with us, we examine the data and find out that all our subjects drink milk. We're geniuses!". I'm glad to hear that you are well-educated within the field and have actually made a plan. Without it, the project would be a death march from the start.
You might find collections.Counter more helpful whenever you're using a dict to count occurences of things: http://docs.python.org/2/library/collections.html#collections.Counter
That's because it doesn't run at all. It exits with an error that you tried to use a break outside of a loop. It has nothing to do with the break being in a function. The same syntax error occurs if you try to run just print 'Yo' break It is true that the bytecode compiler is giving the error, but it is a legitimate syntax error. No there is no reason a function or an if statement can't have a break in it, but the break doesn't make sense if it's not in a loop that you want to break out of. 
learnpythonthehardway.org Even after reading the docs which can be confusing to a beginner, this cleared a lot of stuff up for me.
&gt; It is true that the bytecode compiler is giving the error, but it is a legitimate syntax error. I approached it from a [somewhat formal angle](http://en.wikipedia.org/wiki/Compiler#Front_end): syntax errors is what you get during the syntax analysis phase, which is supposed to use some sort of a formal grammar describing the syntax. Since nobody in their sane mind would try to implement the "break is only allowed arbitrarily deeply nested in some loop's body" rule in a formal grammar, this kind of errors is in a sort of grey area somewhat outside the notion of "syntax error", together with stuff like missing return statements or use of non-initialized variables in Java. Anyway, that's not important at all, the important thing is that the idea that the compiler executes statements one by one and the "def" statement compiles its body, is wrong: everything in a module gets compiled before anything is executed.
Landing page mystifying; no obvious clickable links, lost interest
You know there's an option to quickly find you mouse cursor in the mouse preferences screen, right? If you enable it, pressing "Ctrl" will show ripples around your pointer.
There was some work done on it at SciPy 2013 the other week. Enthought use PySide, and I think they're supporting some development, though I've no idea how much.
I have that enabled with mouse trails and I still have trouble finding it sometimes. This gives me a sure fire way every time.
Scary! Did they watch you code or did they just left you alone for a couple minutes? 
edit: There are 3 tutorials, all lasting 6-10 minutes each with a great amount of depth. Highly recommended for people new to regex
[How to think like a Computer Scientist](http://www.greenteapress.com/thinkpython/thinkCSpy/html/index.html) 
Codeacademy FTW! :D
MIT OpenCourseWare- I love them, probably my favorite. I also hope to get a degree at MIT once I finish high school. CodeAcademy- Amazing interactive tutorials, I originally started there. How To Think Like A Computer Scientist- FTW!
Why not using AutoHotKey alone? http://www.autohotkey.com/docs/commands/MouseMove.htm
Took the first offering of crypto on coursera.org, and the programming snippets came in python. I was a perl fan prior, but seldom had time to do much outside of quick scripting stuff for work. A friend gave me a quick rundown on the basics and figured the rest out to complete the various crypto programming assignments. Still consider myself just a step above novice unfortunately, just don't have time to devote to go crazy with it. I try to find as many excuses to use it at work as I can, but this mostly ends up being crappy django projects. 
Because then I don't get to use python! Nice find, didn't know that was possible
[*Learning Python,*](http://books.google.com/books/about/Learning_Python.html?id=ftA0yk1Z92wC) by Mark Lutz and David Ascher.
Please add requirements in README and setup.py.
Cheers. To me that'd be a rather big limitation not to have pip support.
It seems there are quite a few of us.
I have 5 monitors currently, spread across 3 computers. For me, what helps is to use Windows Inverted (Extra Large) mouse pointers. 
thanks for sharing!
Excellent read. This leads to an obvious question: What if I would like to explicitly pass a value to a new name (variable) and not create a new reference? Say: x = value of y
Thanks!
Yeah, that's a fair point. The thought was to generate each step as a separate file, because they take hours or days to run on large sets of files, and they're re-used for different purposes. E.g. the filtered tokens will also be used to generate part-of-speech tags separately.
Nice, thanks!
nltk.sent_tokenizer is useful over word_tokenizer if you don't want to include n-grams that span across sentences.
It is kind of ridiculous that so much effort is wasted on producing two of the same.
Thanks.
Fockin' hell. Sit in the terminal like a 70' champ! Never encounter such problems then!
If you have a problem, and you use regular expressions to solve it, you now have two problems. The original problem and regular expressions.
scikit-learn has a great class for getting n-grams from text: http://scikit-learn.org/dev/modules/feature_extraction.html#limitations-of-the-bag-of-words-representation If you are using this with an older version of scikit (anything before 0.12 I believe), be careful, as the API has changed a fair bit.
in linux I have the auto window focus on mouseover function set, and all windows out of focus to become transparent, that way the window my mouse is on, is the window that is prominently visible.
Cross platform: Turn on visual accessibiilty tools. Like in most places, you can tap Ctrl and the mouse pointer will pop and glow for a moment.
Dunno why you were downvoted. I too have a fairly large screen and a smaller monitor, and I have no problems. Bad eyesight is plausible.
I took the same [Coursera course](https://www.coursera.org/course/interactivepython), and I really enjoyed it. It's being offered again in October 2013, and I recommend it as a way to learn the language with an on-line class room structure.
Whiteboard. Pseudo code, then i was pressed for a proof on search optimization. Something about root(N), or some shit...i have not thought about this since that incident. I have had many "whiteboard tests" none of which went well.
I haven't used shedskin yet so I'm curious why you wouldn't try it in production code (even with a test suite for the code you run through shedskin?). My impression is that since this is a smaller subset of python focused on numerical calculations it's much easier to get an acceptable level of correctness than for shedskin.
I just go as far right as possible and move it around a bit. Easy to find, and most of the time you find it on the trip across. 
Always use Python! Sure, there might be an "easier" and "more efficient" way to reach your goal, but nothing else is as fun as Python.
The `&amp;index=1` part was causing the page to refuse to load. Remove that if you're having issues with the link. Just a quick note (Browser: Chrome (latest)).
Thanks for the response. I wouldn't use Shed Skin in production code because it didn't have enough guarantees of safety for me (against the possibility of an erroneous assumption in the python-to-C++ compiler). Then again, I last used it a couple of years ago, so maybe things have improved in that regard. Shed Skin also works against of subset of python, but there was no real specification, other than to try it, and see if it would compile.
I decided on a project (I'm a professional developer) and picked the Pyramid stack to do it in. Being pretty good at figuring out what I don't know, at every stumbling block I'd track down the solution in the docs/forums/googles/tubes and continue on. It was a valuable experience -- gleaned how to establish a fully functional development environment, complete with debugging and REPL feedback in about 12 hours.
Probably because he sounds like a smug dick. "I don't have this problem, what's wrong with you?"
What are you using to map scripts to hotkeys? (just out of curiosity since I have never done it before)
Oooh, this actually reminds me of a project I want to do regarding remaking [xeyes](https://en.wikipedia.org/wiki/Xeyes) in my windows 8 toolbar. I know precisely nothing about programming in Windows though. Does anyone have any pointers as to where to look to get started?
ah, that last sentence puts it in the "not for production" category for me too, thanks.
what were you writing the gui's in, out of curiosity? tkinter?
whats all this about kittehs?
At first, yes. Now I prefer wx, although I've been meaning to learn Qt.
Perhaps how it (PyQT) is licensed is partially to blame. Excerpt from [Why Not LGPL](http://www.gnu.org/philosophy/why-not-lgpl.html) &gt; Using the ordinary GPL is not advantageous for every library. There are reasons that can make it better to use the Lesser GPL in certain cases. The most common case is when a free library's features are readily available for proprietary software through other alternative libraries. In that case, the library cannot give free software any particular advantage, so it is better to use the Lesser GPL for that library.
There's a few. I use AutoHotkey, it's pretty handy. 
In windows 8’s hot corners, if your cursor hits the top before the side, it will stick in whichever monitor it was in. Makes hot corners more practical in a multi-monitor setup.
I first started learning python on college differential equations because it's was easier to write a simple python script to solve first order differential equation approximation than do all the steps since it's very repetitive. After that I just did it for fun. I haven't ever relied on one source. I would have an idea of something I wanted to accomplish then google around until I found a good tutorial for the subject. One key thing is repetition. I'd learn to do something, then forget it. Later I'd forget it and have to look it up. But each time that you do that you'd learn it it better. So after a couple of times you don't need to look it up. The key to learning is to be effective at teaching yourself what you need to finish a project. You'll learn all the details will getting experience working on things that interest you. 
a perfect use of the worlds dwindling resources.
Not sure. It seems to be done that way usually. Probably none in such a simple case of holding a variable. In a more complex case the class could also hold additional methods (e.g. a property for getting the held variable in a lazy way) and would allow instantiating a separate copy of the normally-singleton if needs be.
People are different. Who knew?
So you're saying that *also* using python is *less fun* simply because he has to write some code in AutoHotKey to assign it to a hotkey? In my books having to write code in another language makes python even *more fun* by comparison.
If you're interacting with the Windows gui, Autohotkey is a lot more fun than Python. Have you tried it? It's a very useful language to know. Edit: here's how it's done in autohotkey: ^!c:: CoordMode Mouse, Screen MouseMove A_ScreenWidth // 2, A_ScreenHeight // 2 return
Yes, I definitely agree about that. I always hate using variables in AHK, because of the strange way it handles literals. Python is much nicer! But I'm a firm believer in using the right tool for the job, and AHK is definitely the right tool here. Especially if you're already using AHK to assign a keyboard shortcut, may as well add 2 extra lines to complete the whole task. 
Cool! I should now to able to build the base for my kitten picture website! Thanks!
Not sure what line breaks you want, but that timestamp should be the same length all the time from the look of it, so assuming you're processing line by line, you could use: line[21:] 
And as I am way too bored at the moment, here is a more complete solution using jjagots idea: # Read File: with open('log.txt', 'r') as logfile: list_of_lines = logfile.readlines() # Remove timestamp: list_without_timestamp = [line[21:] for line in list_of_lines] # Put back together: log_string = ''.join(list_without_timestamp) # Write to new file: with open('log_without_timestamp.txt', 'w') as new_logfile: new_logfile.write(log_string) And the next time you need help, /r/learnpython is a better place to post this kind of question. 
&gt; So, what, now I'm opposed to any and all innovation in computing? Based on your first comment, I'd argue yes. You believe this problem is solved, well-documented and should not be revisited. Kinda like how the earth is the centre of the universe as was well understood and documented in the 15th century. People like Copernicus should never go about their half-assed studies, right? What good could come of replacing well-documented triode vales with FETs ? Sorcery! By completely dismissing Python (in /r/Python of all places) you completely miss the point. Sure, the UNP fundamentals are the same. But as evidenced by the plethora of code out there, in an actual software program this can be implemented in many different ways. I would most definitely argue that an implementation permitting backgrounding another process is more powerful than one that does not. As is one that would operate as a singleton, prohibiting multiple copies of the program from executing. As is one that implements flexible logging of the backgrounded process. As is one that drops privileges. As is one that permits executing in a sandbox. You'll find these ideas and more in [PEP 3143](http://www.python.org/dev/peps/pep-3143/) 
Back when I wrote Qt apps in Python, I used a module to work with both See https://github.com/epage/PythonUtils/blob/master/util/qt_compat.py Example: https://github.com/epage/PythonUtils/blob/master/util/qml_utils.py
Guys, why the downvotes? This is true. And it's funny.
You can see 2.0 benchmarks here. http://morepypy.blogspot.co.uk/2013/05/pypy-20-alpha-for-arm.html
Clearly he has a problem finding his cursor. You asserting that its easy to find a cursor is literally the least useful possible answer that doesn't involve linking memes. My answer is to set the mouse sensitivity high enough that I can reliably flick it into a known corner. One thing I don't like about the OP script is that it presumes an odd number of monitors. 
What I do on Mac (not sure it applies to PC but I assume so) is vertically offset adjacent monitors by maybe an inch or so, so there's a reliable corner on the top while moving right and on the bottom while moving left, but the mouse won't get caught moving the other way. 
Have you ever seen one? \^.^
Thanks! 
Fair enough, however I wanted a non-intrusive way to get rid of evil. Thanks for your feedback
Ugh... I try to avoid multiple monitor setups. Too much neck strain.
If you want x to refer to (value of y) but not create a new reference, then you'd have to make y not refer to it any more. You could do this: x = y del y Not sure why you'd want that though. 
 fun = None
A quick YouTube search gave my two much better results: * [Regex Howto : Dominate Your Code with Regular Expressions (Nixie Pixel)](http://www.youtube.com/watch?v=EppQdkv4G2w) * [/Reg(exp){2}lained/: Demystifying Regular Expressions (Lea Verou)](http://www.youtube.com/watch?v=EkluES9Rvak) The first one is very brief but covers about the same amount of info in only 5 minutes (bonus: kitten). The second one though is much longer but she does such an excellent job at explaining that she has more depth after 5 minutes than your guy after 20. (Also note how I used the word *excellent* here in an appropriate way.)
Regexcellent
&gt; By completely dismissing Python I did absolutely no such thing.
Shit. I've used it forever and I just now noticed that! Thanks! ^^facepalm
The notion that Python will lose relevance if the GIL isn't removed within 10 years. It's certainly gained relevance now despite the GIL in data analysis, math, science, web development, and education. In order to lose relevance, something else would need to be in the position to step up and gain relevance. On top of that, there's a HUGE body of libraries and educational material for Python. History has shown that languages that achieve that kind of adoption don't disappear overnight for just that reason (e.g. C). The GIL also isn't a showstopper and there are several options for multiprocessing in Python (each with their benefits and drawbacks). I just don't see the evidence that this is the make-or-break issue for Python. 
Thats not the goal. I probably didn't explain well enough. I would like to pass the value of y to x and keep both and do different things to them. It's naturally happening if I do something right away, say add a number to y. The question is: Is there a way to pass the value of a variable to a new variable (and keep the existing variable with the existing value/reference) instead of creating a second reference?
Right! Thank you for the explanations, this script is so awesome!
I think micphi has a problem understanding that he is not being helpful. teletrial, you, and I have all pointed it out, but he just keeps responding trying to defend himself, while being abrasive towards others. At this point micphi should realize the problem is him, not the people who are responding to him.
Thats it, thank you very much! I knew there would be a way... And indeed its a copy of the value. Totally trivial once you know it...
Am I the only one who doesn't really care much about autocompletion? I can type quite fast and I don't program in ultra-wordy languages like Java, so I've never really found a need for autocompletion. It can be nice to have, but generally I do all my programming in Notepad++, Sublime, nano, or vim without anything fancy.
Not fully functional, but the tab-completion in IPython is pretty good, particularly in the notebook.
Thanks for the advices
Plus, there's always omnicomplete in vim if you install all the extra python stuff. It's always a feature I think I want, but don't use as much in practice.
&gt;Thoughts? I thought it was an excellent summary of one part performance improvements to Python, one part unholy blasphemies. :-) Aren't most Python IDEs capable of displaying the docstring? If not, help is only a dir(whatever) away. 
Is PyPy faster than frozen python script (by py2exe, etc)?
PyPy do just in time compiling optimizating things. Py2exe only join a interpreter and source python code in an executable file without performance gains. So pypy is faster than frozen python. There are some compilers like shedskin that optimize code too.
Thanks a lot for so detailed explanation!
&gt; but I want that auto-completion functionality in my IDE that you can't get without some type annotation You are aware, are you not, that basically all those nice "type annotation required" features you're used to in your statically-typed-language IDEs are descended from the development tools for a dynamically-typed language?
Learn python the hard way, then just picking up tutorials for whichever module was useful for what I was trying to do at the time.
I half agree to the "auto-complete" issue. Especially if working in a team. But for me it has a positive side-effect: It forces us to have proper documentation for the other team-members to read. win-win. Yes, auto-completion is a nice-to-have. But for us it was not a real issue. Also, klen/python-mode for vim is actually quite nice and provides auto-completion if you really want to. Other IDEs to so as well. Naturally it is a fair bit slower as auto-completion with statically typed languages.
auto completion does have nothing to do with typing fast.
What a stupid idea. Importing that module breaks core functionality as well as things that compile code at runtime like template engines.
help(whatever) gives the actual docstrings, instead of just a list of members.
FYI: It's not meant for anything serious
 reload(sys.modules['__builtin__'])
Googling around a bit I found [meteolib and evaplib](http://python.hydrology-amsterdam.nl/moduledoc/index.html). Didn't use it, but the tagline "Meteorology and Evaporation Modules for Python" sounds like it goes in the right direction.
I've usually had less of a problem with that because I can always just tab back and forth between vim and the REPL. Also because of vim's quasi-autocomplete.
or the 'exec' statement, or compile + overwriting another functions code object, or ...
http://docs.continuum.io/anaconda/ has quite a few http://docs.continuum.io/anaconda/packages.html (including numpy and scipy) With anaconda, you can install other packages with: * **conda**: http://docs.continuum.io/conda/commands/install.html * **conda pip install**: http://docs.continuum.io/conda/commands/pip.html https://python-packaging-user-guide.readthedocs.org/en/latest/ explains packaging and installing packages with `pip` (and `conda pip install`). &gt; ["How did you learn Python?"](http://www.reddit.com/r/Python/comments/1hz7vb/how_did_you_learn_python/cazoa4a) 
A performance section where llvmpy is mentioned but not PyPy? TF?
**[Meteorology](https://en.wikipedia.org/wiki/Meteorology)[^*](https://en.wikipedia.org/wiki/Outline_of_meteorology)[^#](https://en.wikipedia.org/wiki/Category:Meteorology) and [Python](https://en.wikipedia.org/wiki/Python_\(programming_language\))** From https://pypi.python.org/pypi?%3Aaction=list_classifiers : Topic :: Scientific/Engineering :: Atmospheric Science From http://pypi.python.org/ : * "Browse packages" * "Scientific/Engineering" -- [`Topic :: Scientific/Engineering`](https://pypi.python.org/pypi?:action=browse&amp;c=385) * "Atmospheric Science" -- [`Topic :: Scientific/Engineering :: Atmospheric Science`](https://pypi.python.org/pypi?:action=browse&amp;c=385&amp;c=511) * "Show All" -- [`Topic :: Scientific/Engineering :: Atmospheric Science`][(https://pypi.python.org/pypi?:action=browse&amp;show=all&amp;c=385&amp;c=511) You can also search PyPi with e.g. google.com/#q=site:pypi.python.org+meteorology 
As far as **/r/IPython** (sidebar, [top](http://www.reddit.com/r/IPython/top/?sort=top&amp;t=all)): * http://www.reddit.com/r/IPython/comments/1hkqx4/crash_course_in_python_for_scientists/ * https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks#-reproducible-academic-publications
I'm from the field of atmospheric physics / remote sensing myself. I'm guessing your professor is trying out Python to replace IDL? In that case, note that IDL "sav" file support exists in scipy. Also, although you probably know these: matplotlib for plotting, and basemap (matplotlib addon) for geolocation plots. f2py might be handy if he intends to wrap and script Fortran software.
Not a lib per se but you should get him Ipython Notebook, it help with the learning and the visualisation of data. Here are some sample of things done with the IPython Notebook : https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks To analyse heap of data, Pandas is apparently great too
I don't understand how this is supposed to work. I create a virtual env, but pip and such are put in venv/local/bin, not venv/bin. local/bin is not added to the path with the activate script. Am I doing something wrong? Edit: pip-installed packages also go to local/bin and thus aren't in my path once I activate it. huh.
Will definitely consider that! I didn't know it existed, will help spread the word.
That is very good advice, thanks. Yes I do want to help folks get into all aspects of Python eco system - web, algorithms, scientific etc.
The pypy guys are just beasts. "We made an STM library that's a bit of a pain to use from C, so we made a scheme interpreter as a test bed. No biggie." Wow, that's really amazing.
You could represent each factorial as a list of tuples of integers and exponents (each exponent initially being 1) and then merge the lists when you multiply/divide factorials, resulting eventually in a single list of integers and exponents to multiply together. I think you can do this pretty efficiently using numpy arrays. edit: now that I think about it, just use the index of the array as the integer, and just store the exponent in the index for a given integer
Great work. In my opinion `venvx.py` is better name than long `pyvenvex.py`. What do you think? Ps Now setuptools should be merged with the pip ;)
Cross-canceling and factoring is possible and not so hard, using our beloved prime numbers. Let's convert N! to a list of (p,e), p being prime, e being the exponent in the prime factorization of N! for all primes p &lt;= N: m = N e = 0 while m&gt;0: e += m/p m = m/p store (p,e) How does this work ? For p=2 for example, first iteration you're counting all mutiples of 2 below N (which contribute to e by 1), then all multiples of 4 (contributing by 2), then 8, etc. Having the prime factorization, multiplying/dividing should be easier.
UTF-16 isn't fixed length either, only for characters in the BMP, which excludes a lot of Asian alphabets. 
Using [Stirlings formula](http://en.wikipedia.org/wiki/Stirling's_approximation) then cancelling terms may help.
Title should be “Building a directory structure index in Python”. http://grammar-monster.com/lessons/an_or_a.htm
Just curious, what university/college (I'm a programmer at a meteorologic research center for the University of Wisconsin)?
A safer way to go about doing this would be with a regex: from re import search match = search(r'\S+:$', myString) name = myString[match.start():] This way here the formatting only needs to have the name at the end followed by a :
Have a look at [iris](http://scitools.org.uk/iris/docs/latest/gallery.html), particularly if you're working with oceanographic data as well.
That's a good suggestion. Thanks.
As someone who works in meteorology and who uses mainly Python, NetCDF and GRIB(2) interfaces are handy. So packages like PyNIO or netCDFPython are a good place to start.
\*le gasp...\* Enlighten me.
I just love it for discovering API functionality without having to go to the documentation. Also I try and give my methods and types meaningful names for future me, who may forget, or any other developers that come along and have to maintain my code. Yes, I could just stop and document the darn things at the time, but I have to remember to stop to do that. I would say a combination of borderline dyslexia, clumsy fingers, and not necessarily the best working memory makes auto-completion a god send. 