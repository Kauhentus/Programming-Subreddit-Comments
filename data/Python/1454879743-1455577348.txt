I'm not sure what Trello is any more. Same goes for Evernote. 
Fuck the for humans tag
Did numpy itself upgrade as part of that process? Does trying to upgrade numpy through conda change anything? 
In general, avoiding state is usually a good thing. Look up functional programming paradigms. The general idea is to have actual *functions* not methods, that have no side effects (all mutation is kept in the scope of the function). So you have some input that goes into the function, the function does something with the input, and gives an output, changing nothing else in the code. 
Having the title be "And the Winner is" and then saying that the winner is up to the reader is just plain bad journalism.
It's an API on top of psycopg2 that's different and slaps tablib on there. I don't see how it's any simpler and it's definitely not more powerful in a meaningful way. DB API 2.0 also is already a well functioning concept the community has been building and iterating on successfully for a long time now. I find that especially surprising because I'd imagine something like an async version of the DB API would be an important and very obvious step forward.
Being open source is very important for a language. Note the reaction to Apple's open-sourcing Swift. Developers are rightfully wary of investing their time and betting their livelihoods on some black box entirely out of their control.
I down vote every R vs Python post. That's played out.
"shell=True" is not a good idea because it can lead to shell injection. In your case, what if there was a file in your wallpaper directory named ";rm -rf *;"? https://security.openstack.org/guidelines/dg_use-subprocess-securely.html.
Thanks. I've already found it. The bb demo is still fantastic IMO. My project really just started as a retro homage to this and many others that inspired me in years gone by...
Thank you very much for the link! I will fix the vulnerability later tonight. I find Popen extremely useful, and would have built up a bad habit of using it insecurely so I really do appreciate your input on handling input.
I don't really see any value added by this library. All I see are bugs (lots of bugs) and non-standard behavior. Can we just take a quick look at what this is doing? ... # Row-by-row result generator. row_gen = (r for r in c) # &lt;-- Unnecessary, but ok... # Convert psycopg2 results to ResultSet results = ResultSet(row_gen) # &lt;-- Ok... let's see what value this adds... ... And class ResultSet(object): # &lt;-- Not a set """A set of results from a query.""" # &lt;-- Again, nothing like a set def __init__(self, rows): # &lt;-- No documentation on the arguments self._rows = rows # &lt;-- The code below expects this to be a cursor, but it's not at this point (its a generator) self._all_rows = [] # &lt;-- Wat. Why? self._completed = False # &lt;-- ??? def __repr__(self): # &lt;-- Unnecessary. Pretty much the default behavior return '&lt;ResultSet {:o}&gt;'.format(id(self)) def __iter__(self): # &lt;-- This is an odd iterator/caching system that is not intuitive or expected. Why cache at all? Let the user decide if they want the data to be persistent. This **should** return an iterator object. (See docs on __iter__()) # Use cached results if available. # &lt;-- See above. if self._completed: for row in self._all_rows: yield row # Iterate over result cursor, cache rows. # &lt;-- Over a generator over a cursor for row in self._rows: self._all_rows.append(row) # &lt;-- Slow yield row self._completed = True def next(self): # &lt;-- This conflicts with the __iter__() method above. Calling both in conjunction will lead to bugs. The above caches. This does not. Also, this is bad form. The next() function should be on an iterator object only! try: return self._rows.next() except StopIteration: # &lt;--- Unnecessary re-raise raise StopIteration("ResultSet contains no more rows.") ... def all(self): # &lt;-- Why? Why not just be the list!? """Returns a list of all rows for the ResultSet. If they haven't been fetched yet, consume the iterator and cache the results.""" # If rows aren't cached, fetch them. if not self._all_rows: # &lt;-- BUG: This may return a partial list if called in the middle of iterating, or of iterating was not completed for some reason, or if someone called next() one or more times. self._all_rows = list(self._rows) return self._all_rows (Which is like 1/2 of the code already). I apologize if this was harsh, but I'm tired of dealing with unnecessary code (edit: and bugs).
Consider `pandas.read_csv()`, it's doing a one line. Anyway, I'm just saying it's silly we are agreeing to a "2 line standard" when there could be a one line standard for simple read/write. I'm sure if Guido would announce "inline file read" everyone here would be excited and brag to their friends how python is such a beautiful language.
Even though your answer is on the bottom, this actually makes the most sense.
I thought this was someone asking what the title of a person who studies Python would be that would be the equivalent of scientist, to which I was ready to suggest 'Wrangler.' But I'm afraid I can't help you find an equivalent python live testing environment.
Thank you
I have no idea what you mean.
In the requests library, there's a method that raises an exception whenever it encounters a bad HTTP status code. 
You do if you plan on using more than 4 cores. 
You can get all of this and more in [dataset](https://dataset.readthedocs.org/en/latest/).
Pandas is awesome at this: df = pd.read_sql(('select * from users'),engine) I'm sure the performance isn't great for huge apps, but tough to beat the ease and simplicity.
&gt; We should all expect more out of published code like this. It'll be nestled in applications used, libraries depend on, even without the user or developer knowing about it. And this is *exactly* the type of attitude that makes coders promise to open source their shit "once I've cleaned it up, because the code is currently ugly" and then it never gets open sourced. At least, I remember your "if it's shit it shouldn't be open sourced" attitude (the last time I was really active in the open source community) being roundly criticized all throughout the 00s exactly because it leads to tons of stuff never being open sourced. The right attitude is "awesome, now here's how to make it better!" or to just ignore it. The right attitude is to tell a coder and anyone young and impressionable that code should not be open sourced unless it's production-ready. If people had said this about when Linus was writing Linux, he never would have published it. &gt;Code touted as simple but powerful should have a higher power to bug ratio. I 100% agree that OP should have tapped the brakes a bit on bragging about his code.
I didn't mean to go all Theo-de-Raadt or Linus-Torvalds over this. On the flip side of your argument, I also don't want to swim in a sea of open source modules that everyone releases and expects the "community" to pick up on and improve. There's a happy middle ground somewhere. I was about to meet your criticism with my own bug report or pull request, but they seem to be underway already.
I wonder how these comments track with the release of requests. "Why not just use urllib2/3?"
Did you even click the link? conda update conda conda install anaconda=2.5
Cool. So Linus thankfully ignored those morons.
&gt; I also don't want to swim in a sea of open source modules that everyone releases and expects the "community" to pick up on and improve. I don't think that's a concern. Google whatever you're looking for plus the word "best." Honestly I'd be more frustrated with *well-written* stuff—like why did you spend all that time re-inventing the wheel when you obviously have the skill to improve the current wheel? Not to mention now there's going to be half the people working on one and half on the other instead of everyone on a single solution.
You have never coded in scheme,lisp,erland or clojure. It teaches you globals are not needed and dangerous. This is why Erlang for example does handles multiprocessing and threading more reliably then any other language. Erlang was designed for telecom networks it has something like 99.9999999% reliability. As for spped, gambit scheme and chicken scheme compile into C code and in some cases they are fasrter then hand written C code. 
Yah, or dive right in and play with clojure, scheme or lisp. I am a hands on type of guy. Playing with these languages will help your python or it did with me. 
&gt; And this is exactly the type of attitude that makes coders promise to open source their shit "once I've cleaned it up, because the code is currently ugly" and then it never gets open sourced. The code doesn't even work. It's design hasn't even been discussed so far.
Your computer only has 4 CPUs. The MKL redistributed in Anaconda does not require any license (for any number of CPUs or nodes).
This sounds amazing.
has anyone used repl.it ?
I'm not sure what your point is besides trying to shame people into not open sourcing things they make. I wrote the first Python MP3 tag library a very long time ago (or, more accurately, I had to write one because there was no open source one). I never open sourced it because of people like you. It's not like it was a disaster for the universe that my ugly code didn't get open sourced, but this is a phenomenon with which I am intimately familiar. I'm willing to bet what actually was reality was that other people had written libraries but never open sourced them, either. Many because "the code is ugly I'll open source it once I clean it up." The problem is that once it works for the creator, the creator usually stops working on it *unless it has been open sourced*. It never gets cleaned up or improved. It works for the one person who uses it, so that's it. It's dead.
This certainly sounds intriguing. I've shied away from any sort of hardware development simply because I don't know low level languages like C. However, with Python onboard, I may be a bit more comfortable giving this a try. Definitely gonna be following this.
I am very stoked about this. I hope this will greatly lower the barrier to start playing with such hardware and the internet of things. I just started [planning](https://github.com/balloob/home-assistant/issues/1153) yesterday how to make it as easy as possible to integrate MicroPython/ESP8266 with [Home Assistant](https://home-assistant.io), a Python-based home automation framework.
Kivy is THE solution you are looking for. It is completely possible to avoid using the KV language, however you will be making certain tasks harder for yourself. Either way, when it comes down to it the UI description portion of your project (given the apparent scale and scope you describe) should be a tiny portion of your code regardless of whether you do it in Python or KV lang. The swapping out similar frameworks that you mention (like pygame and pysdl2) is what kivy does, in fact Kivy can use SDL2(the default on most platforms now) or pygame or many other providers for windowing, audio loading, video loading, and so on. Python-for-android's latest release contains support for py3. Finally, Kivy also has [KivEnt](http://kivent.org) which is a 2d game engine built on top of Kivy providing even more out of the box functionality for game dev.
Isn't this pretty trivial? Run two functions, compare their results. I don't even see why you would need a module for that. But if you want, you could use any testing library to do this (and much more). Look at nose or py.test for examples.
Title is worded wrongly. Sensor drivers and emitters have not been backed *yet*. Still 1,000£ / 6,000£ off those stretch goals.
*Mutable* globals are generally bad. Immutable globals are good when you have arbitrary constants, but don't need to invoke the added complexity of a class. In other words, if you have to choose between magic numbers and globals, globals are better. A classic example is a regular expression that is used in many functions in a module. Another example is the name of a storage directory. Module level logging is another excellent example of a properly used global. However, if you ever find yourself using the `global` keyword (or otherwise modifying a global variable), you should almost always encapsulate into a class. A logger is one exception, where it occasionally makes sense to change the logging level at runtime.
Thanks.
The point is to run it in production not under tests. But yea, seems pretty trivial
To be fair Arduino code is very easy for simple stuff and a good way to get familiar with microelectronics.
I have Philips Hue lights in my house, which respond to HTTP commands to make them dim etc. I can use the app on my phone, but I want to make a little box with pushbuttons that do different things to the lights. An ESP8266 dev board is less than $4. I've ordered some pushbuttons and an enclosure for a few dollars more and now I have the hardware I need. This project will let me write Python code to handle all the logic. another example: I can attach a PIR sensor (IR motion-detector) to an ESP8266 and make my lights come on when I walk in the door.
Not sure if you can record, but [pyo](http://ajaxsoundstudio.com/software/pyo/) can be useful if you need DSP. It may be worth a look.
ESP8266 is stolen tech.
Nicely written article! Good job to you and fuck that site layout. Have a wonderful day! 
Holy crap that layout is an atrocity.
From what I've heard from people trying to use Kivy on Android it's quite hard or some "how to make it" parts are missing. I've looked at pyqtdeploy for PyQt5 but that also requires a lot of setup. It just looks like Python development for mobile devices is far away from being a first tier choice and easy to use.
My experiences with Kivy have been less than stellar. For such a promising framework, it was pretty confusing as a newcomer. I've yet to see any kivy tutorials on doing anything beyond the extremely most basic tasks without using markup language, whereas something like SDL I can go "this surface, goes here, and has this image". I can not even find how to do that in Kivy. Shame that python for mobile seems so lackluster. Another sphere that python would've been well suited to, that it just didn't become a thing.
text
I supported the first version micropython because I was intrigued by the idea. I still use C or AVR code for my embedded systems though. I guess I could see switching over if it became ubiquitous enough. 
What is however the best way to login into a form programmatically? I've seen in many posts that mechanize is obsolete and many are switching to firefoxdriver or chromedriver.... what do you think about it? 
Except that if you go the SDL surface route you are doing CPU blitting and already doing the wrong thing entirely. Kivy may be slightly more difficult to learn, but this is because Kivy is based off of OpenGL and does so with a very thin layer of abstraction, for the most part to do graphical programming in Kivy you are doing GL code. This is more complicated than the trivial 'software blit' but also absolutely necessary for achieving decent performance, particularly on mobile. GPUs are only about 20 something years old now, it's best to learn how to make use of them. Also, just to reiterate: we all use the KV markup language because it has measurable benefits over not using it. The reason no one writes any tutorials without it is the same reason why you own both a hammer and a screwdriver when doing wood working, sometimes you just need a different tool.
Another way is to rely on keyboard shortcuts: box.type(Keys.CONTROL + "e") for i in xrange(100): box.type(Keys.BACKSPACE)
It depends on the website. If there's no javascript, go for mechanize, it'll be easier. But if it has javascript (react, angular, or other stuff) better go with selenium. 
You really should not be using xcode for python development. Running python from the terminal is standard practice so think you need to get used to it. If you are interested in using an IDE for development look at Pycharm: https://www.jetbrains.com/pycharm/ Do you know about virtualenvs? 
&gt; https://www.jetbrains.com/pycharm/ I've already seen this program and it looked pretty good but I try normally to use as less program's as possible. So I hoped to fix Xcode. Yes I know a little about it but until now I had no use for it. I've just wrote a little program without a lot of modules. 
that's not all it does, at least not the Github one (in Ruby) I'd like to see a Python version of something similar. It's probably not a big job but it's not trivial, the value comes from the reporting
&gt; After completing my associates degree Why not just finish your bachelors in CS?
Can I just say I follow many subreddits, and one of them is /r/Python, and the other is /r/montypython. With that being said, I saw this title and at a quick glance saw "Monty Pythons...KickerStarter is huge success...web-based...etc". I thought I was reading about a Kickstarter Monty Python did, that was successful, and would include a web-based show. I clicked, and yeah. Not what I read lol. Anyway, *Python!*
If I am being honest and from my long time in academia I'd tell you to pursue something with "* Engineering" in it. So for example, Computer Engineering or Electrical Engineering They have a better chance of by passing the ~~morons~~ bureaucracy in HR, and you will get more practical knowledge. Just an aside, this is totally dependent on the college you go to, I've heard of CS degrees that put more emphasis on Software Engineering vs pure academia and I've even heard of some CS degrees that get lumped into Liberal Arts (somehow)
There's no better book than [Think Python](http://www.greenteapress.com/thinkpython2/). I recommend getting to know Python 3, it's a lot better than 2, and in case you need to support legacy code you can always just go back to Python 2 *and you don't have to unlearn putting parentheses around prints*.
The message you got in 2.4 came from numpy (not MKL).
[This talk](https://www.youtube.com/watch?v=OSGv2VnC0go) is pretty interesting. It shows the Pythonic way of doing several common tasks. Also you might want to check out other talks by Raymond Hettinger, the guy is a legend.
See https://www.reddit.com/r/Python/comments/44mr3i/records_is_a_very_simple_but_powerful_library_for/czs86e7
When you look for jobs, try looking in NYC if you can afford the relocation expenses. It's probably the largest Python market in the US. SF is close, but outside of those two markets its much harder finding a position without a degree or prior work experience. NYC and SF make it pretty easy to get a foot in the door without a degree.
Don't know if u can join this course late, but it looks good and its free: https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-6 Pythons own tutorial is not a bad start: https://docs.python.org/3/tutorial/index.html 
[removed]
Thank you so much for your help, I just got the book on Amazon. 
Wipy and lopy are both based on micropython and pyboard the official micropython dev board predates both. On the other hand, this kickstarter isn't a hardware production project , but a software improvement one. 
Yeah I used it, great tool for quick demonstrations. Not related to this though. 
I have been checking for the latest tweets in a while loop, didn't realise that I coukd stream in real-time. Thanks for the advice
HackerNews has a hiring board and a monthly Who's Hiring post that usually has quite a few posts. Go to local meetups - companies often use them as recruitment pools. Keep an active LinkedIn and put a resume on Indeed. Lots of ways in. Don't be afraid of recruiters for that first job, either. Some are worthless but you only need them to get that first job. After that, you can live off skills, resume, and references.
Github. Any other answer is incorrect. EDIT: To elaborate, use Github to find and contribute to open source projects, and use github to showcase your code and projects. Employers will ask you, "Do you have a github profile?", and will check out your code.
[Raymond Hettinger](https://twitter.com/raymondh) tweets python tips!
 The Orator ORM provides a simple yet beautiful ActiveRecord implementation. 
You could print the function's docstring. \_\_doc\_\_ is defined at function definition &gt;&gt;&gt; def func(x, y): ... "calculates the sum of x and y" ... return x + y ... &gt;&gt;&gt; print func.__doc__ 'calculates the sum of x and y' &gt;&gt;&gt; func2 = lambda x: x &gt;&gt;&gt; func2.__doc__ &gt;&gt;&gt; func2.__doc__ = 'my docstring' &gt;&gt;&gt; help(func4) &gt;&gt;&gt; print func4.__doc__ my docstring You might want to try [logging](https://docs.python.org/2/library/logging.html?highlight=logging#module-logging) the output instead. It sounds like that's what you're trying to achieve. 
Hey, It was on a demo VPS so it get huge problems. You can see some screenshots: https://github.com/ugcoder/Py-URL-Shortener/wiki/Screenshots
Interesting method. I'll look into the logging as well. In the case where I had something like: lambda : randint(3, 4) Is there any way I could fetch those static arguments from the lambda function and append them to the docstring? ***EDIT: obviously i'm not gonna extend randint, but let's say I wrapped that within a function of my own 
I know that the test isn't 100% accurate and it is just meant to provide estimates.
Oh, and @mattupstate (I think that's the handle), that made that overholt flask example.
something something parse something something didnt you learn?
If you wanted to redefine the docstring every time the function is called, you could do this: &gt;&gt;&gt; def func(x,y): ... func.__doc__ = "last added: {} and {}".format(x, y) ... return x + y ... &gt;&gt;&gt; func(2, 2) 4 &gt;&gt;&gt; func.__doc__ 'last added: 2 and 2' &gt;&gt;&gt; func(2, 5) 7 &gt;&gt;&gt; func.__doc__ 'last added: 2 and 5' If you want to get the constants from a lambda function: &gt;&gt;&gt; rand = lambda : randint(3, 4) &gt;&gt;&gt; rand.func_code &lt;code object &lt;lambda&gt; at 0x7f5072497e30, file "&lt;stdin&gt;", line 1&gt; &gt;&gt;&gt; rand.func_code.co_consts (None, 3, 4) &gt;&gt;&gt; nothing, three, four = rand.func_code.co_consts &gt;&gt;&gt; rand.__doc__ = 'last radomized: {} and {}'.format(three, four) &gt;&gt;&gt; print rand.__doc__ I don't know why you would want to do this. &gt;&gt;&gt; randd = lambda : randint(3, 4); randd.__doc__ = 'randomized {}'.format(randd.func_code.co_consts); &gt;&gt;&gt; randd() 3 &gt;&gt;&gt; randd.__doc__ 'randomized (None, 3, 4)' 
Talk about your python with the girlfriend. I'm sure non-coders do that too.
**Unfortunately, Python is notoriously difficult to parallelize.** and more pearl like this: **I’m not at all familiar with parallelism in Python** Being not familiar is the good background to make judgement? It is sad joke. I suppose the first sentence is referencing to GIL somehow. Geez!
I've written a tutorial on "[Turning Your Twitter Timeline into a Word Cloud Using Python](http://sebastianraschka.com/Articles/2014_twitter_wordcloud.html)" which is related your task. Basically, you could just take this script and remove the word-cloud stuff at the end and you should be good to go 
They're slower, but still within the realm of acceptability. Typically &lt;200ms, usually 80-100ms. Feels like any other website. (https://zappa.gun.io if you want to see a Zappa-hosted site. Warning, it's a self-signed cert.) I really do see this as the future of deploying web applications. 
Given that AWS is investing heavily in Lambda, I doubt it will go away any time soon. Beyond that, you have to do literally NOTHING to your application for it to work with Zappa. It Just Works!
Tornado doesn't integrate well with parallelization solutions most folks really use and, more importantly, dill uses pickle, which is dangerous (correctness issues) and slow and hard to predict.
For most people, but not for the Googles/Amazons/Facebooks of the world. There's an estimation that Amazon loses 1mil (annualized) for every 10 milliseconds they delay their pages.
Yes, but honestly these days, most of that isn't coming from the HTTP content, it's coming from the rich content of the page.
This is great! Exactly what I wanted to do. &gt;I don't know why you would want to do this. I take it this is bad form/practice? Is there a better way to accomplish what I'd like to do? Let me flush my example out a bit more: Let's say there is a dictionary representing a template for all enemy types in a game. Here is a sample entry in it: enemy_templates = { 0 : { 'name' : 'enemy1', 'strength' : 3, ... 'damage' : lambda : randint(3,5), ... 'agility' : 5, } ... } These templates are loaded into an abstract Entity class with setattr() and potentially have dozens of different values and/or lambdas Now, let's say I wanted to make a catalogue of all enemy types for display purposes. Think of a bestiary or such. It would be really ideal if I could do something like this. for id,data in enemy_templates.items(): # this is the dict mentioned above for k,v in data.items(): print v.__doc__ if hasattr(v, '__call__') else v I believe the method you outlined would work quite well. Is there a better design for this type of thing? ***EDIT: can't code...
I know this was posted months ago, but I've never seen a RaspberryPi for $10 - 15. I've been looking for a couple weeks, and cannot really find them cheaper than $35 for Model B. The Model A's I've seen are starting at a little more than $25. Any ideas where I can go scavenge to find the $10 - 15 ones?
Oops, I figured this out on my own and deleted because I didn't see the reply. Thanks! 
Do you have a PoC exploit against dask?
It is an awesome module... i used it at work in the last few months and it definitely is very fast. 
I am sorry for my silly question but could there be a future where we could use MicroPython as a real replacement for C on a regular PC?
Love the concept behind dask and also really like this talk as an overview of Python parallel computing with the pydata stack. If you want an overview of all the other options available for parallel computing with Python, I gave a talk at the last PyData NYC on the subject, "Beating Python's GIL to Max Out Your CPUs": https://www.youtube.com/watch?v=gVBLF0ohcrE This covers all the options available to speed up Python code, starting with single-CPU speedups using things like Cython, and then going to single-node (but multi-core) speedups with concurrent.futures/multiprocessing/joblib, and finally ending with multi-node (thus massively parallel) architectures such as ipyparallel, pykafka, streamparse, and pyspark. I would have included dask in this talk, but, at the time (Dec 2015) the dask distributed scheduler was still in very early development. It looks like it has made quite a lot of progress and, based on its documentation, seems to already be a viable alternative to ipyparallel (perhaps even more powerful) for "pet compute cluster" parallel computation.
amazing!
What do you mean?
The latest dask distributed is using cloudpickle
Fetching images, css, loading/rendering JS ends up taking far more than just fetching the initial HTML. 
~~So how do I handle arbitrary strings of bytes, then?~~ ~~I can't use `.decode()` reliably because it will eventually throw an exception and terminate. Tried it with cp1252, utf-8, and ascii. They all eventually hit a bad byte and terminated. I can't avoid encoding/decoding because it's going over the network and the socket object returns an encoded byte string.~~ ~~The packets I'm sending and trying to parse on the other end contain four fields, each one binary serialized data. I can't parse the packet until I decode it, but I can't decode it because of the aforementioned problem.~~ Fuck it. I'm just using text.
Sorry for the confusion, but when I said 10-15$ for the hardware, I was referring to the additional supplies you need to hook up to your raspberry Pi, such as breadboard, jumper cables, etc. That said, I believe that the raspberry Pi zero costs 5$ , although last I checked they were sold out.
Couldn't you serialize your objects as JSON and read them with JS? There is a module for JSON. 
&gt; Yea an example like this is stupidly easy to parallelize Okay, I tried your example, and I found a few problems. First, your example isn't actually *doing anything*. Your call to `imap()` just creates an iterator. I suspect it's just taking 4 seconds to divide the rows into chunks and send them to the other processes via IPC, but then the threads aren't actually doing anything. When I try to iterate over the iterator, I get errors that the lambda can't be pickled. After some Googling, I found out about multiprocessing.dummy, which appears to use threads and shared memory instead of processes, (which should perform better, in theory, since we're not talking about copying a massive table); however, when I try to call `map()` or `imap()` on the pool, the validation for each row fails with: Row 759 has 19 cells, but expected 1000 Which is odd because I don't know what would cause it to modify the size of the rows that are being passed into `validate_rows()`, and I don't know why it's always choosing to truncate the rows at 18 or 19 items (almost always 19 items, but an 18-length list slips in here and there). Any suggestions?
Works for me. I believe that the ``pool.imap()`` should be executed when you call ``pool.join``. Also I got rid of the lambda statement. In python you can do multi-threading, but all threads are limited to a single core by the interpreter. That's what people are referring to when they complain about the GIL.
&gt; Works for me. I believe that the pool.imap() should be executed when you call pool.join. I don't think it does work for you. Throw a print statement in the `validate_rows()` function, I don't think you'll see any output because I don't think it actually gets called. Like I said, I think the 4 seconds of execution time is spent marshalling your chunks to each of the worker processes. If you do get something working, could you send me a paste of the full implementation so I don't have to infer the bits you left out?
I think you may have missed part of my edit. Here's the full source that I ran http://pastebin.com/5BhTfgU2 and here's the output: /tmp/csv-validation-benchmarks$ go run csvgen.go 1000 100 | python3 sequential.py Beginning validation... N_CORES 4 validating 25 rows validating 25 rows validating 25 rows validating 25 rows Validated 100 rows of 1000 cells in 0.133278 
He probably means efficient encoding of nested data, similar to Twitter's Parquet (http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/) or Google's Dremel (http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf). Both these formats optimize storage such that they can access arbitrary subsets of the data without needing to walk each structure from the root. A pandas series of dictionaries is no more efficient than a python list of dictionaries since pandas just stores an array of python object pointers.
Oooh, I see. Thank you for the clarification!
That would make more sense, because I couldn't see it being any easier to use than it already is.
&gt;more emphasis on Software Engineering vs pure academia Is that a bad thing? My program is a lot like that, not sure if I should be worried. 
What do you want to do? Web dev? Make a django or flask app. Data processing and machine learning? Get involved in scikit learn. There is literally nothing stopping you, just do it. You have some shitty moronic inefficient interface or monotonous task to do at work? Code that up. Making elegant designs can make the mundane very tolerable (while making you marketable)
Could you explain your concerns about correctness of pickles?
Thus validating everything /u/weberc2 wrote and the post. 
This is pretty slick, I like it! Have you checked out [ack](http://beyondgrep.com/) or [pfff](https://github.com/facebook/pfff/wiki/Matrix)?
Okay, thanks!
Does that mean the calls to check the button values on the gamepad are platform specific?
Yeah, really. I don't know what kind of Python concurrency programming you've done, but I've found it less than easy. Google around my friend and you can see others that have struggled with it too. The getting started code is easy, the difficultly comes when you need to bolt concurrency to IO bound processes, for example. Then you find that you may need to write the whole damn thing again. That has certainly been my experience so far. Even with the monkey patching in Gevent... Try bolting that on to a Flask app and see how your day ends up.
Wait, really? Which PyCon?
Sorry I don't know. I saw a lot of these youtube videos. I take a look on them to get new ideas, but I'm not fully focused. 
I don't know if there is a *better designed* way given your setup, but in any case, any function might as well be treated as an object and can take on arbitrary attributes. For example: &gt;&gt;&gt; def function_object(): ... # empty ... pass ... &gt;&gt;&gt; function_object.arbitrary_attribute = {'key': 'value', 'key2': 'value2'} &gt;&gt;&gt; function_object.arbitrary_list = [] &gt;&gt;&gt; function_object.arbitrary_attribute, function_object.arbitrary_list ({'key2': 'value2', 'key': 'value'}, []) Same as lambda functions: &gt;&gt;&gt; obj = lambda : None &gt;&gt;&gt; obj.something = 'Something!' &gt;&gt;&gt; print obj.something Something! The problem with assigning a string to \_\_doc\_\_ is that it's a so-called *magic method* that is called by the function 'help' and used by most IDE's for tooltips and whatnot. It's not the best idea to store a temporary log in a docstring. In short, feel free to do a similar thing as above, just don't assign anything to \_\_doc\_\_ until you need some documentation rather than debugging notes. 
Probably better to create a LinearDistribution object or whatever, and override both __call__ and __repr__ on it. 
Your best bet is to list your skills accurately. If you know python, list python. If you happen to also have experience with pandas then list it too. If you don't and you're asked about it, say that you haven't worked with pandas but given a little while you're confident that you could pick it up. Offer to explain how you would solve it with the skills you do have, using whatever language you're familiar with. Being able up solve the problem regardless of what libraries you have available should be more important. You could also guess. The number of rows in a csv might be returned by calling len, the second to max value might be returned by sorting then indexing the next to last value. Use a little common sense on those sorts of questions, but make it clear that you would need to refer to documentation to know the syntax exactly. 
Thanks man 
Absolutely. I use ack all the time and I love it. Thanks!
Enable debugging? Also, use pyInstaller cause it's better.
Anyone have a mirror link?
I love this tool, it could be really powerful coupled with an editor plugin. How is performance on a large code base?
*Yes*! This is the most perplexing thing I've ever read in my life. There is a weird misunderstanding in the software community that suggests that "open source" software is somehow, by definition, of higher standard or held to some greater threshold than closed source software. "Open source" software is literally just software that is provided to the world under an open source software license: that's it. So if you write some little thing that's just for you, and you open source it, you haven't placed a burden on the world to use it. All you're doing is granting other people the right to do stuff with your code *if they want to*. That is strictly better than keeping your software locked up on a shelf where no-one can do anything with it. And that's why on any 'serious' open source developer's profile you'll find some projects that are serious, battle-hardened, proper code and some that are frivolous or silly or one-off that are used just for them. The idea that the second are somehow a burden or a problem for you is absurd.
So much potential, Thank you!
[removed]
You believe there is some magic solution. I am afraid it is not. I give you some hint. Compare Turing machine vs lambda calcul regarding concurrency. Concept of turing machine doesnt involve parallelism. Well, most of programming language has their base in former than latter. You should shift more than language to be succesful in concurency. 
Depending of where you connected the GPS to, it's going to be different. You need to provide the type of connection you are using.
Why not just use the Compose key? If something is missing from the system compose file you can always drop it in `~/.XCompose` The Compose key lets you enter any symbol, usually quite intuitively, without leaving the keyboard.
I'm planning to make a plugin for Vim, that's what I use in my day to day. Which editor do you use?
Thank you for the useful information. I began to look for projects to join on Github. 
Thank you, seems exciting. Have you ever landed a job on the site ? 
I find it counterintuitive.
If you want to do something really useful, tie it into scientific data repositories like Dryad. Now that would be sweet.
I'm a bit new to this... Could someone kindly explain where I can see the list of data sets on that Github repo?
Install it -&gt; import it -&gt; run "data()" and you'll see all the datasets with explanation. It's right there in the Readme. There is over 750 datasets so I can't paste them all. 
Oh, I didn't know about that! Could you please point me out to its docs? I'd love to see it.
That's awesome, thanks! I'll probably need to provide a porcelain output format to be consumed by the plugin.
And glad to see another vim guy :)
Look up the video titled "the Clean Architecture in Python". On my phone so I don't have the link. It's very much about using pure functions, which coincidentally I believe would let a jit like pypy (which is often used in web stuff) optimize very well.
Maybe try creating a new virtual environment for your console with the same python version and the same packages installed and verify that you still get the same performance through the console? 
http://vincentarelbundock.github.io/Rdatasets/datasets.html
You can see all the datasets in the Notebook here: https://github.com/iamaziz/PyDataset/blob/master/examples/basic-usage.ipynb Github should display the notebook for you, if it doesn't, use http://nbviewer.jupyter.org/
Nice. Only one thing. You should probably replace region_type = region_type.replace("'", "") region_type = region_type.replace("\"", "") region_type = region_type.strip() with `region_type = region_type.strip(" \'\"")`
Not really sure if this helps but the new version of EnScript is supposed to be python based. 
I've got a pretty strong aversion to Java because of the whole "one man team" thing--I develop rather slowly in Java, and expect this to be a side project for quite a few months if not years to come, and I have a small existing codebase in Python 3 already. I'm probably unfairly biased/picky about my tools for personal projects; as it stands now Kivy or pysdl2 seem to be the best options. Everyone seems to love kivy especially. That being said--thanks for letting me know that existed, that will definitely come in handy in the future!
Thank you very much, /u/-pooping As /u/constructivCritic said, I thought that there might be list of them somewhere that I could just browse without installing.
Yeah. It really should be linked to a file in the Readme with all the datasets listed. 
Thanks!
Hi, just try to create a test subset of the data and check your program on it. That's what I usually do. Really pays off in the long run. 
"weight data for domestic cats" - Woohoo! Just what I've always needed! ;-)
The installation part. Setting XCompose up properly is a real pain. Also, my app is a hint for me right on my screen. With XCompose I will simply forget the combinations. XD There are just way too many symbols. That's why I group them on pages in my app.
I don't see how this solves the issue of HOW to keep the data in ram GIVEN I have enough ram.
Thanks! Will check it out!
You may need to mount it first - desktop Linux automatically mounts USB sticks as they're plugged in, but I don't know if Raspian does. Plug it in, and run the command `mount` to see a list of everything that's mounted. If there's something like `/dev/sdb1` (the last letter and number can vary) mounted on something like `/media/blah`, it's already mounted. If not, you'll need to run a command to mount it - have a search for instructions. Once it's mounted, you open the files just like any other files, with a path that starts with the mount directory (e.g. `/media/blah/myfile.txt`)
As using a RAM disk has no benefit, the bottleneck is maybe processing the data when it is already in RAM. You need to profile the code that loads the data and find the bottleneck. A possible solution is to pickle the fully loaded data. Make sure to use cpickle if you are on Python 2.
And giving away some eBooks on test frameworks
There's a link in the readme: https://vincentarelbundock.github.io/Rdatasets/datasets.html
I don't work with big data so I'm just wondering why the data isn't held in a database?
1/ I already told you why is bad from the start. Making judgment on topic you admit you are not expert is sign of bad taste. 6/ Guilty as charged. Yes, your function is IMHO awful and I was busy today I didnt explain why. My fault. I admit I expected more negative criticism from reddit users which explain the stuff instead of me :) Sorry about that. Well, you function is not returning or yielding value. Thats is not exactly why the guy "stupidly easy" was not able to run his example, (He probably dont know the fundamental difference between imap and map function from what I have seen), but you probably dont want to mix your parallel computation with printing in real world. One reason is if your function is not returning value you will end up in total mess when you will try to use Pool.map or Pool.imap functions. Those two are quite easy to grasp but only in the case you are using pure functions which you are obviously not. Seeing such approach I can understand why is parallel programming so difficult for you and your colleague. You are mixing too much things together :) and then you are surprised you are not able to manage resulting chaos. :) One more reason (not so relevant) is doing such useless exercise as to convert string to int and print it immediately to output. I hope I recall your code correctly. At the end, if you are interested I can recommend this book: http://www.allitebooks.com/parallel-programming-with-python/ 
One of the keys to using big data is reduction. That means taking your source data and reducing it down so that you still have the base facts, trends, aggregates and totals, but without the bulk of the raw data. That makes analysis more practical, less time consuming and you can discard lots of data that might otherwise be impossible to stockpile and never looked at again anyway. The skill to reduction is, of course, working out how best to do it without losing any hidden information in there or introducing a bias not in the source data. So ask yourself; can I reduce my data down into something more manageable? You don't have to throw away the original source data, but you may find you no longer need it. 
While most people will simply use scipy rather than implement their own method here, this is an excellent illustration example. Would have been useful in my 3rd year numerical methods class :)
cheers, glad you liked it :)
Do you know a programming language already? If you're an absolute beginner I would recommend setting your goals a little lower. The web RPG could be a fun project, but maybe that should be your second project after you learn some programming basics. If you already know a programming language suitable for web development, then just use it. There will be plenty of time later to learn a variety of languages and technologies. None of them is fundamentally different than the others. Only variations on a theme. 
I really like how they made their point with a reference to an 18-year old MasterCard marketing campaign.
I suggest flask for this. I suggest running through a tutorial first (which are usually pretty deep and involved), and then setting up your own free python website at pythonanywhere.com. They can set up a flask site hello world page for you with just a couple mouse clicks, but it's good experience to know how to do it yourself. There's are other popular python options, like django. But I think flask is perfect for setting up a simple game, and it's great for while you're still learning. Edit: I posted this as though you were new to programming in general, which may not be the case, sorry. Flask is still a great option, though :)
I am new to programming. Thanks!
I'm curious about what your implementation and stack looked like before, and what it looked like after. What was your time consuming operation, I presume it was cpu bound and not vectorizable easily? How hot and how tight was your performance bottleneck in the code?
/r/learnpython
This should have been the general content and tone of your original comment. I agree with everything you've written (more or less).
I would recommend doing some beginner oriented programming tutorials then. try /r/learnprogramming and /r/learnpython. I think Python is a great choice for a beginner language. Its got a nice clean syntax and is very expressive and clear (in my opinion). Best of luck to you. Stick with it, learning programming can be intimidating at first but once you start to get it it is very rewarding.
Yes, but that code makes no sense. You're not reading `p.stdout` anywhere, so you still have the same deadlock issue, aside from it not doing what you wanted it to do. Plus `wait_emulate` always runs `sys.exit` not matter what `x` is, so your script will terminate right away without reading anything. This is on top of it being meanless to use a busy-wait loop for something that has a perfectly good (and efficient) syscall: `wait`. So I don't understand why you think you need a workaround, and the code you gave isn't solving your problem. You need to read all `p.stdout` before waiting (no matter if `p.poll` loop or `p.wait`).
You may or may not also be interested in [fn.py](https://github.com/kachayev/fn.py), or maybe even [Hy](http://docs.hylang.org/en/latest/).
Formating must have gotten lost, because it did work to some extend. The problem was still the buffering. I spent my morning switching over to *communicate*. It was actually easier than expected.
\*looks at flair\* http://lucumr.pocoo.org/about/#like-my-work http://www.gevent.org/sfc.html \*runs away\*
explain?
Do these data sets get updated? How? Who updates them?
I don't produce individual files that are terabytes in size, but the output from ensembles of runs typically runs that high. I try to limit individual files to no more than a handful of gigabytes, and usually less than that. I use the [Community Earth System Model](http://www2.cesm.ucar.edu/).
Maybe because he's a Flask enthusiast (from his flair), so he's posting links of other popular projects, that may be relevant to his work.
If you had 100's of gb, I feel like you should've used a distributed file system. Idk tho, I've never dealt with 100's of gb's
There is now and intermediate Python course. You might find it more interesting. https://www.datacamp.com/courses/intermediate-python-for-data-science
You're going to need to give concrete examples, this is mostly useless without the context. That said you have a few options for good debugging: prints, run in a shell (the repl) or the debugger (my preference). Use any of those to check that you're actually evaluating what you think (you can check types, values, run expressions, etc.). Example: import pdb def fun(): return "foo" x = fun() pdb.set_trace() Now when you execute your script it'll pause at the "set_trace" line allowing you to print out the value of `x` or otherwise interface with your program. Please in the future use the /r/learnpython sub, this sub is more for python related announcements/general interest.
Sorry about the wrong sub reddit i did a google search for python sub reddits noob friendly and came across an old reddit post mentioning this place my mistake for following that post message received no noobs welcomed i have already re posted in the learning sub reddit soon after the 1st reply link
This sounds nice and simple, I'll give it a try, thanks.
It's like there's an implication here just waiting to be implied.
It should be noted that the theorem you're referring to is called The [Abel-Ruffini Theorem](https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem) and it only applies to the solution to the *general polynomial* of degree five or greater. In fact, the specific example you gave in the blog post *can* be solved by radicals by factoring out an x^2 term and solving the resulting cubic. I'm just being pedantic, though. This is a great write-up.
I mean.. I think they're totally different applications.
This is what I was going to suggest. Would be a lot easier if you would post the code.
The memory mapped file will be dumped once you close all processes using it, just like every other memory allocation. The only inconvenience this provides is if you are coding IPC you need to remember to close all processes that were associated to free the memory.
How would you export the environment without losing the *.whls link? 
It can creep into my code but I try to avoid mixing programming styles within a project. Most of the packages you import will not be functional and mixed code is hard to follow because of paradigm switches. Sometimes you even end up writing awkward wrappers around everything so that the logic doesn't get obscured. TBH functional programming isn't really that great in python, I'd recommend trying a real functional language just to get an idea of how things are supposed to be. KDB+ Q is probably my favorite so far.
I'm not much of a numpy user, but I strongly suspect it's the deserialization that's the issue. Just loading a few gig from disk doesn't take that long.
That's not how investments work. I want shareholder value, quarterly reports, and a tangible return on my investment that yields returns larger than inflation. This should be doable if the organization is a business selling products or services. Otherwise, donating to Django is an act of charity.
The first thing that I would recommend you is to look at pep8 and the google style guide for python. https://www.python.org/dev/peps/pep-0008/ https://google.github.io/styleguide/pyguide.html Making your code complaint with those standards/guidelines/recommendations will make a lot easier for other people to read your code and collaborate. I wouldn't spend time looking at what your code does or which other recommendations could I give if I have to pay much attention at how "different" the code looks like. pep8 is the most used and lots of people follows the Google ones, but the idea is to make the code look nice. This one may be nice to read too: http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html As a final notes, is a good thing to share code, keep doing it :)... You'll get useful feedback for free once in a while. Also, spend some time reading code, that helps too.
Ha, I just noticed that neither pyramid or pylons (that powers reddit) does not even have a donate button. 
Those are not equivalent. Strip only removes from the ends.
Sorry I initially misread your comment. Ages ago I was using memory mapped files with C and windows API. If you did not close them properly there where problems. From another users comment. Python has a high level api that provide protection. 
I was doing it with C and MS Windows. I didn't know there was a higher level python api. It was many years back, with Windows if you corrupted the memory, did not close them properly etc. Bad things happened. Then again for me, everytime I tried to do stuff on windows bad things happened 😊
i have a feeling that marketing campaign will be around forever, even if only as a meme. sure, it might get lost in translation a few centuries fromw now.. but it'll still be there.
Think of it less like buying a stock and more like paying for tools. A mechanic that spends money on quality Snap-On tools is indeed making an investment. That investment is in a tool that will help him generating future revenue.
If you need to store data Django is superior to flask, especially for beginners because it has strong defaults and it has everything you need built in. With flask you need to assemble your database backend yourself. 
Sure. You can think of the general polynomial of degree n as the polynomial with "arbitrary" coefficients. In this sense, the degree 2 polynomial ax^2 + bx + c can always be solved by radicals using the well known quadratic formula. It turns out that every polynomial admits a special group (a set of objects with an associative operation and a couple other properties) called a Galois Group. The polynomial is *solvable by radicals* if and only if its corresponding Galois Group is *solvable* (a technical term, meaning that the groups *derived series* terminates at the identity). The general polynomial of degree n has a Galois Group S_n, the group of permutations on n elements. It turns out that S_1, S_2, S_3 and S_4 are solvable, but S_5 and onward are not. 
Donating
Downvote for posting a screenshot instead of copying and pasting the error message as text. https://www.jwz.org/blog/2003/02/ph33r-m4d-ski11z/
This. It's not the "loading" of the data that is killing you, it's the parsing. Is your data a single large array? Load it once, then pickle. Is it columnar? Try pandas and pickle your dataframe. Multidimensional? Try xray (now xarray?) and save as netcdf. Hierarchical? Take a look at hdf5. If you do use pickle, be sure to use cPickle and add protocol=-1 to the call to dump. Good luck!
You might want to checkout Luigi for this (from spotify). Seems to cover your use case very well, and it's a cool tool. 
I will ask one of my colleagues about this, but the gist would be you start a interactive Python interpreter, run the import data section and then import and run your number crunching code. If it fails, update your file and re-import the changed code. The data from the import will stay loaded.
Text parsing isn't very fast. Better would be to store the data in some binary form. Then it'll probably load a lot faster. 
Is that a code Screenshot? Wtf
Read the sidebar for how to post your code. Alternatively, upload to http://codepad.org/?lang=Python I'm not going to critique your actual code until it is more readable, but as for your algorithm, I think you would do better to include dipthongs (e.g. `ou`) as vowels and digraphs (e.g. `sh`) and blends (e.g. `pt`) as consonants. I recommend this because words aren't really strings of letters, but strings of syllables, which can be more closely estimated than simply in terms of vowels and consonants. As an example, just alternating consonants and vowels will always produce words like `Pokawy` or `Renika`, which tend to have a very distinctive and somewhat artificial sound (although words generated this way can be very similar to those of the native Hawaiian language), while including dipthongs, blends, and digraphs can produce words more along the lines of `eptia` or `temmish`. That being said, this still isn't a great system. Of course, depending on what real-life language you are trying to emulate (if any) your system is going to have to be different. But even assuming you're trying to make English-sounding words, improvements can certainly be made. For example, the frequency of certain sounds coming after others more closely matches those from a dictionary. But those innovations are up to you to implement, should you desire. You could even look into having suffix and prefix generators, really the possibilities are endless. This might be useful: http://school.judsonisd.org/webpages/cbianco/readinghelp.cfm?subpage=23376
Actually in speech, this is some kind of a filter. When people tell you to shut up and you manage to get your point across anyway - the idea is usually worth it. If anyone could say anything without any repercussions, the world would be quickly littered with all kinds of useless information (I mean even worse that it is now). Again, not every idea (even relatively good one) is worth saving.
I think we should change Guido's title to that: "GvR ~ Grand Megameef of the Python Community"
I'll have to test this out. Nice!
Sure, let me know if you face any issue.
As a pen-tester, any development of Django is a good thing. I'm consistently seeing good, secure, usable apps come out when it is used.
thank you i will delete the post. this is exactly what i was looking for and it works :)
Neat. I wrote [Pynsist](http://pynsist.readthedocs.org/en/latest/), which is a way to generate installers specifically for Windows. I've heard that Continuum have something similar internally built around conda, but I don't think they've released it.
It's not an act of charity, and the word investment does not need to mean financial investment with monetary return. The general definition is putting finances or effort/time towards something with the hope (not guarantee, but likely) that it will benefit you in the future. An effort investment into exercising will hopefully improve your health in the future. A financial investment into Django will hopefully keep the project running and improving, which will in turn benefit your benefit in the future. 
&gt; I've heard that Continuum have something similar internally &gt; built around conda, but I don't think they've released it. THAT IS AMAZING 
Good luck, this reddit is generally friendly, but has in the last few years been overwhelmed with questions from people new to python ... so check out /r/learnpython for now. Don't worry, /r/python will still be here, when you level up :)
'big data' guy reporting in; we try and keep our entire dataset (or at the very least the active dataset) in memory for performance reasons. If you're paging out to disk or waiting on network IO you're going to have queries that take hours/days to run. Big data is all about cache locality at this point.
Thanks for the feedback. I agree, importing specific module is a good practice. Noob question, but I used PyInstaller to just create the standalone executable. Can I edit the .spec file to include this information ?
As far as "prose", IMO the functions randVowel and randCons are unnecessary, as they're both easily readable one-liners. As one commenter already said, string.ascii_lowercase is your friend, not some ASCII hack, which is less global and less readable. As far as intent, I have to agree with the other commenters that using diphthongs would help your words, as would a list of recognized consonant clusters (even three-consonant clusters for "words" like `strinks`). If your goal is to create pronounceable words that are easy to remember, I think that would be fine. If your goal is to create words that sound like they could come from English, like "septial", then I would perhaps look into [Markov chains](https://en.wikipedia.org/wiki/Markov_chain) as a possible method to weight how different consonants and vowels are more likely to appear in sequence than others (to use an obvious example, `quars` is much more English-like than `qirs`.) 
I have wished I had this like a bajillion times. Thanks for making it!!
GIL isn't a bug - it allows the CPython interpreter to be sane and secure. The resulting good behaviour of CPython makes writing high quality C-extensions easier too. Yes it would be nice to have parallel threads, but the tradeoff would be a much more complicated (and bug prone) CPython interpreter and huge issues with existing C-extensions. Compare with Jython and IronPython, which both run on threading-enabled VMs and have little support for C-extensions. Why? Because without the safety guarantees of the GIL it's very hard to interact with the interpreter's internals without them blowing up in your face! It's a pretty good tradeoff now to have the GIL and a clean, safe CPython interpreter and have to use tools/libraries to get parallelism.
No, it really is amazing (no sarcasm). &gt; my feeling is that the people interested in developing desktop &gt; apps are not mostly using conda I don't know if that is cause or effect. If there was a tool that took my package setup.py and conda environment description, and gave me an installer I would be overjoyed. IMO, miniconda has solved all python build, package and distribution problems. The final step (creating a native installer / executable) is all that remains. 
Wait the "Yaddayaddayadda...: Priceless"? Well TIL.
My bad - thanks.
As others have said, consistent naming, following pep8, and test code would help. A lot of your comments state what the code is doing. In general, comments should tell you *why* you did something, not what the code is doing or how. http://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/ Since you imported cookielib, this looks like a Python 2 project. It's cookiejar in Python 3. Consider making your class a new style class. There's rarely a good reason to use old style classes in Python 2, and they are gone in Python 3. class Foo(object): instead of class Foo: Classes should have a single responsibility. You should think about moving the logic for using mechanize and keeping track of the cookies, csrf, user etc., in one class, and the logic for composing the form payload info and parsing the response data in another. There's also a lot of repeated code that could probably be factored out, like this: headers = [('Accept', '*/*;q=0.5, text/javascript, application/javascript, application/ecmascript, application/x-ecmascript')] PEP8 your imports https://www.python.org/dev/peps/pep-0008/#imports Swap your print statements for logging, and let the code using the module set up where to log (and do that setup in main, so if running from command line, it logs to console, like print does). Since mechanize is an external dependency, add a requirements.txt to your project. Still, it's certainly better than the first Python code I ever wrote (which I did after working in other languages for over a decade). 
Great stuff. One thing I've noticed is that it seems that vprof can't handle scripts that take arguments after the script name (sys.argv). It would be super helpful if it did. 
Looks cool, although I'm already in the habit of using kcachegrind. Can you compare and contrast the pros/cons of the two tools?
Spelling mistake on the 4th line &gt; Everytone knows this is harmful: Styling issue in the "Don't raise NotImplementedError" section @abc.abstractmethod def notify(self): pass Other than that it was an interesting read.
You should do one for Flickr!! Rumor is it's going to close down soon...
In this episode I interview Ned Batchelder. I know that coverage.py is very important to a lot of people to understand how much of their code is being covered by their test suites. Since I'm far from an expert on coverage, I asked Ned to discuss it on the show. I'm also quite a fan of Ned's 2014 PyCon talk "Getting Started Testing", so I definitely asked him about that. We also discuss edX, Python user groups, PyCon talks, and more. Some of what's covered (pun intended) in this episode: * coverage.py * types of coverage * Line coverage * branch coverage * Behavior coverage * Data coverage * How Ned became the owner of coverage.py * Running tests from coverage.py vs running coverage from test runner. * edX * what is it * what Ned's role is * Ned's blog * Ned's PyCon 2014 talk "Getting Started Testing" * Teaching testing and the difficulty of the classes being part of unittest * fixtures package * some of the difficulties of teaching unittest because of it's class based system. * the history of classes in unittest coming from java's jUnit implementation * Boston's Python Group * PyCon in Portland * Ned to do a talk here "Machete mode debugging". * Practicing PyCon talks at local group meetings. * At the very least, practice it in front of a live audience. 
&gt; Consider putting your import statements on online. Working on the assumption that you meant one line here. I was reading PEP8 yesterday and noticed that it mentioned you should be using a separate line for each import statement. What would be the advantage of putting each import on a single line?
&gt; and should have been a priority for a while. Welcome to Python. I have been here more than 10 years and the topic of 'packaging, distribution and installation' has been simultaneously, for the entire time, hard, a priority, unable to be agreed upon, and unfinished. miniconda solved the dependency and binary distribution clusterfuck. if you are patient I believe it has all the bits required to solve the rest. BTW, there are actually many many other ways to package python and generate installers, it is my opinion that while they rely on the pip/setuptools/python packaging binary distribution stack that they will be merely toys.
I'm usually on the same page as you in regards to keeping it on one line which is why it stood out to me on PEP8. 'welp, I better change that'
It's probably result = min - 10.0
Yeah, I tried cx_freeze and PyInstaller and they kept choking for various reasons. I belive it was the scikit-learn package that broke things for PyInstaller. I might have been able to get it to work by using some of the custom hooks they let you do, but I was already 3 hours in at that point, and didn't want to invest more time without some guarantee that it would pay off. 
I use HDF5. They're like numpy arrays stored in a tree structure, with all the metadata attached to each branch or leaf you'd like. It's all direct access data structures. I use them on disk, or in memory. Best combination is ramdisk. My preference is for the h5py library to read/write the format.
I laughed. Here, take my upvote. 
I would replace for file_name in files: if file_name.endswith(".py"): full_path = "{}{}{}".format(dir_name, os.path.sep, file_name) yield full_path With: for file_name in [f for f in files if f.endswith(".py")]: yield os.path.join(dir_name, file_name) 
Looks like they use argparse, so you might be able to use `--` to separate the two types of arguments. ie, if you wanted to pass "foo" and "bar" to testscript.py: vprof c testscript.py -- foo bar (I haven't tried this myself.)
Mostly on the way in. Recent data is the most accessed in our case, as it gets older it moves to disk.
Relative to what's really taking the time, it doesn't matter. CPython isn't a compiler. It doesn't optimize code. It's an interpreter. Furthermore, If you wanted to do enough numerical calculations for it to matter, you'd use numpy.
If python started optimising everything, the complier would become a lot slower and more complicated You would win, but also loose too. If you need speed, or want optimisations maybe you should be looking a pypy. 
Because of the dynamic nature of python this is actually the correct answer. 
You're being nonsensical. There's obviously no reason to optimize it like that.
Man npm is everywhere now
It's not true that CPython doesn't optimize code. Just look at [Peephole.c in the CPython source, which contains the implementation's Peephole optimizer.](https://github.com/python/cpython/blob/master/Python/peephole.c) CPython doesn't do as much optimization as, say, PyPy, but it definitely does some.
Thanks this was helpful. I identified that the -v flag was only introduced in bash 4.2, though there were other ways to do the check that were more backwards compatible. I've fixed the script now.
/u/Killarny helped me figure out which version of bash was causing the issue. The problem should be fixed now. Thanks for letting me know about the bug!
&gt; You obviously don't know how to use generators That's a rather nasty way of starting a technical debate. I have done both big data analysis and written pipeline based workflows which deal with one-shot data which has to flow through the system *only once*. But since you have assumed you know the best here, I'll leave it at it.
 class A(int): def __pow__(self, other): return 2 x = A(1) print(x*x) # calls __mul__ of parent class int print(x**2) # calls __pow__ of class A 
Same could be said about pip. I use it. I don't like it.
I don't believe this is correct, KCachegrind can read valgrind output, but does not use valgrind itself. It is only a visualization tool.
I will preface this with acknowledging that I may be wrong (or at least out-dated) but when I did some C++ (very little) for a class, our instructor told us to use `x*x` over `x^2` (or whatever in C++) for exactly this reason. Actually, we were talking molecular potentials so we often were looking at 8th powers, etc. I guess my point is, I am not sure that other compiled languages do it either (and again, with my original caveat of I may be wrong)
&gt; While most people will simply use scipy rather than implement their own method here Don't know. Most people wouldn't even know they could do this in scipy.
Is this good advice? I would have to say no, not today on modern architectures. Mainly because as hardware and compilers improve it may lead to less than optimal performance. Especially if there is any vector processing hardware and a compiler smart enough to optimize Pow. 
FWIW, this is basically how Splunk works with its search filters, which scales well to some rather sizable log datasets. Each expression following a '|' is another Python shell program that processes rows on stdin, and returns zero or more rows on stdout. 
Read my blarg entitled "why I switched to $PROGAMMING_LANGUAGE_X(hint: clickbait on reddit)"
Have you ever looked at mod_wsgi-express? It already generates a full Apache/mod_wsgi configuration on the fly, thereby allowing mod_wsgi to be run from the command line. It includes SSL capabilities and much much more. The bonus is that you get a complete Apache configuration which has been specifically optimised for hosting Python applications. See the the mod_wsgi package details on PyPi.
This is interesting. What don't you like about pip? Are there better package/library managers in /r/Python land?
I dig it. Especially nice for binary. flags = 0b_0011_1111_0100_1110 is *a lot* nicer than flags = 0b0011111101001110
Languages like C, C++, or Fortran have very mature optimizing compilers. They can easily optimize fixed integer powers into multiplication, as long as a flag such as `-ffast-math` is used. Without it, the compiler can't make the optimization due to potential loss of accuracy. Here's an example: #include &lt;math.h&gt; double cubed(double x) { return pow(x, 3.); } double square(double x) { return pow(x, 2.); } double eight_power(double x) { return pow(x, 8.); } Compiling this with `gcc -O -ffast-math` yields: cubed: movapd %xmm0, %xmm1 mulsd %xmm0, %xmm1 mulsd %xmm1, %xmm0 ret square: mulsd %xmm0, %xmm0 ret eight_power: mulsd %xmm0, %xmm0 mulsd %xmm0, %xmm0 mulsd %xmm0, %xmm0 ret As you can see, there is no mention of the `pow` function, just a series of multiplies.
No just hipster level maximum 
Everyday and every way, I'm getting Perl-er all the time. 
If the string isn't found, the keyerror is caught and similar names are searched for using the following find_similar function: https://github.com/iamaziz/PyDataset/blob/master/pydataset/support.py . Unfortunately, it appears this isn't all that useful if there isn't anything very similar to your query. A more helpful message might be better, indicating that there was a keyerror.
Python taking a page from Rails and Ruby is a good thing. Lets take a page from Python's book as well!
Ah ok, didn't realize that... thought it was odd that it would choke on iris... one of the most common datasets... do any of the datasets work for you? Could it be a network/download problem? 
I am much more inclined to write 5_000_000 than 5e6. I would find the former easier to read and much more clear of it's intent and purpose. Scientific notation is nice, but I feel it is in some cases a little too compact at the expense of clarity. I think it is a helpful visual cue when a numerically large value is also visually large in screen real estate. Along the same lines, I'm a little more okay writing 5e-6 for example. The clarity is still not perfectly optimal, but at least this way a small number manages to be small visually as well. I still think 0.000_005 might be clearer though
I like this feature, but I agree... it feels like death by a thousand cuts. 
grep '5000000|5_000_000' simple! (kidding!)
I'm only addressing your last question about a tutorial. Brandon Rhodes is baller and has many pandas videos, this is just one: https://www.youtube.com/watch?v=5JnMutdy6Fw Internet search: https://www.google.com/search?q=brandon+rhodes+pandas
I don't. I have primarily used java, matlab, python, and a bit of js; decided to give C++ a go because a modern language with C-like performance has got to be worthwhile. Asked my housemate what header files were for and basically noped the fuck out back to java.
Peewee uses operator overloading, which I think is the best: User.select().where(User.votes &lt; 100).count()
Eh, I've fixed a lot of bottlenecks by re-writing bits of Python. It's easy to commit algorithmic sins in any language, and it's also often fairly easy to remedy them without resorting to something extreme like bringing in C.
This is giving me flashbacks to the nightmarish weeks I spent fighting rails magic before I tried Python/Django and got my sanity back. 
What about SQLAlchemy was a turn off? Personally I found the core perfect for my needs and the ORM sufficient (I'm not an ORM fan for _most_ things). And the documentation was pretty great to boot. 
I definitely like it since I am absolutely baffled by numbers without separators. However, I feel like Python is added more and more rules and grammar that increases the complexity of the language unnecessarily.
I completely agree with you here! However, I can also see the point in the comment below &gt; I am much more inclined to write 5_000_000 than 5e6. But that's just a matter of habit, and I think that (regarding PEP8) `5e6` would still be my pref.
I think the rule is that it does no optimizations that would modify the workflow of the program, right? This means it cannot inline code, unroll loops and lots of other "standard" optimizations.
Yeah. I definitely see the value in what's described in that comment too. However I think there's greater value in keeping a language lean and have one way to do these things. It may not be the best option for everyone, but we gain a lot by simply all doing it the *same* way.
&gt;However, I feel like Python is added more and more rules and grammar that increases the complexity of the language unnecessarily. The PEP about the 80 character limit is a prime example of this insofar as it's extremely commonly cited as a PEP that you should judiciously just ignore, since for example if you strictly followed it you'd insist on multi-lining an 82 character line of code, which is extremely likely to reduce readability. Especially if it's a line of code that's two indent blocks in (8 spaces gone right there) or if you're using descriptive variable names that chew up a good chunk of that 80 character limit. Just like with drug prohibition in the meatspace world breeding casual contempt for the law, it's counterproductive to have PEPs that everyone ignores because then it just encourages everyone to ignore any PEP they find inconvenient.
As a geophysicist, I support this argument. I'd even take it one step further by exploiting unicode symbols for all the greek letters we see in physics all the time. Then the equation in python really looks like the equation in the textbook, and there's no mapping back and forth. Of course, I only do that for local variables. It's annoying if you're trying to call a function with kwargs that are Λ and Φ, but useful inside a function to compare correctness to the textbook.
I don't think this is a good idea. If you read binaries or floats all day everyday, just write your own wrapper. #!/usr/bin/python import locale class coolFloat(float): def __init__(self, value): self.value = value def __str__(self): return str("{:,}".format(self.value)).replace(',', '_') a = coolFloat(5645454.23) print a &gt;&gt; 5_645_454.23 This will confuse more than it will clarify stuff, and contribute to a lot of potential errors. Just imagine port numbers written like this: localhost:5_000 in a configuration file and the confusion that will bring. . . 
*binary literal
I don't really like the look of underscored numerals, but I can't think of a better separator and I don't mind if it's in there.
Yep, so *possibly* with the specialisation [PEP 510](https://www.python.org/dev/peps/pep-0510/) this optimisation would become an option in time, but right now, implicitly, it can't: it depends on `type(radius)` and how (and whether) `type(radius)` overrides `__mul__` and/or `__pow__`. For all Python knows, `radius` is an *n × n* matrix and the function is a scalar product of `radius` by `3.14` followed by a matrix product of that by the original `radius`. Or maybe `type(radius)` defines `__mul__` but does not define `__pow__` at all, so the optimisation would blow it up.
Hi, I tried on my French keyboard and I found the problem. When you look at the code (depending on your configuration): https://github.com/asweigart/pyautogui/blob/da09fe4f259fbc66477351d085c21e77e3b53c1f/pyautogui/_pyautogui_osx.py, you'll see there is: keyboardMapping = dict([(key, None) for key in pyautogui.KEY_NAMES]) keyboardMapping.update({ 'a': 0x00, # kVK_ANSI_A 's': 0x01, # kVK_ANSI_S 'd': 0x02, # kVK_ANSI_D you'll see that the mappings are hardcoded for the US Keyboard layout. Now when you do: pyautogui.platformModule.keyboardMapping['@'] it will return 19, which corresponds on my keyboard to '2'. If you can lookup all the keycodes for your keyboard and set them like this: pyautogui.platformModule.keyboardMapping['@'] = keycode Then it could work. You'll maybe need some combination of keycodes. For example, on my keyboard ALT + à gives @. So instead of doing: pyautogui.typewrite('xxxxx@gmail.com') you'll want: pyautogui.typewrite('xxxxx') &gt;&gt; specific to my french keyboard pyautogui.hotkey('option','0') pyautogui.typewrite('gmail.com') this yields: xxxxx@gmail.com ;) 
Its to be easier to *read the code*, adding a repr method doesn't help that at all
the problem is that lambda is a keyword in python
Look forward to seeing Indians group numbers by lakh and crore: `1_00_00_00_000` or Chinese like `100_0000_0000`.
Most of PEP8's recommendations are valid and improve code quality. You're just not required to follow them all.
You can but do you want to?
Oh cool. So for my daily task manager scripts I can open pythonw to prevent the unnecessary pop-up 
&gt; you'd insist on multi-lining an 82 character line of code And you should. People often look at code in a terminal window, and those should fit into a 80x25 matrix, so you can have a number of them tiled on one screen. Its perfectly possible and makes code more readable to fit it into 80 chars per line.
 grep -P '5_?000_?000' Just use `?`
In China, the grouping by *wan* isn't really used in daily life, they only use the thousand grouping. In India, the *lakh* grouping is still used, afaik, but is honestly pretty useless, as the mixing with thousand grouping nullifies all the benefits of grouing digits. They should get rid of it. 
The title is misleading. Only one of the 10 c++ implementations was slower than either python implementation. A bit of guess and check proves that split8 is only slow because it is misusing boost. Replacing the while loop's contents with Tokenizer fields(input_line, sep); numWords += std::distance( fields.begin(), fields.end()); numChars += input_line.size(); count++; brings split8's execution time down to about split2's. The difference in performance seems to be boost being used to tokenize the string twice, once to count the number of tokens, and once to count the number of letters in each token. Even the fixed version would not likely be used by someone who wanted up most speed since it is computing the tokens themselves, which is not part of the goal of this program. This benchmark only proves that bad c++ can be as slow or slower than python. Split9 and split2 are doing a lot of unnecessary copying, but I won't try to fix those since they are already under Python's times. My timings on my machine: ./split5.py Python 15.8 seconds. ./split.py Python 15.4 seconds. ./split1 C++ 6.0 seconds. ./split2 C++ 12.0 seconds. ./split6 C++ 1.6 seconds. ./split7 C++ 1.4 seconds. ./split8 C++ 12.7 seconds. (my updated split8) ./split9 C++ 14.9 seconds. ./splitc1 C++ 5.5 seconds. ./splitc2 C++ 5.8 seconds. ./splitc3 C++ 4.7 seconds. ./split_subparser C++ 2.0 seconds. 
It's used in speech enough for me to have seen it caused confusion many many times when chinese speak swedish/english and use million when they mean 10 000.
I guess the ORM itself is not build with bootstrap, and the M in ORM does not stand for "mobile". Maybe. But then, if you can't comment without typo, how can *j* trust you to make a valid comment?
5e6 is a float. So now you are going to have to do int(5e6), which is ugly.
Do CLI text editors really not understand line-wrapping?
Well, then this PEP is even more useless than I thought lol. If it's to read the code, the tool with which you read the code allows to do it without all this butthurt. . . "binary and floats numbers for humans" haha
I know what you mean, and sometimes I have that problem too. Nevertheless, the advantage is that this implicitly discourages crazy Java-like variable names and lambdas in list comprehensions. 
Thanks! All callees and callers are contained in stats.Pstats after cProfile run, vprof just reverses the tree and changes it's format: https://github.com/nvdv/vprof/blob/master/vprof/profile_wrappers.py#L73 https://github.com/nvdv/vprof/blob/master/vprof/profile_wrappers.py#L81 
Yes, Python has implicit concatenation of string literals. Given significant indentation this is only really practical in parentheses, but it can be nice sometimes.
Would i = 5 * 1000 * 1000 or even ONE_MILLION = 1000000 i = 5 * ONE_MILLION be equally easy to read?
hah, didn't expect a thing like this to actually exist, awesome!
There's already a way to use number separators?
[It's funny you should mention that](https://pypi.python.org/pypi/astsearch). ;-) astsearch 1234
I wrote a tool called `astsearch` that can easily get round this: astsearch 5000 I actually made it when I wanted to find all division operations in a codebase. Grepping for `/` wasn't going to help. So it can do this: astsearch "?/?"
Also support for [butterfly-mode](http://xkcd.com/378/) in emacs.
[Image](http://imgs.xkcd.com/comics/real_programmers.png) [Mobile](http://m.xkcd.com/378/) **Title:** Real Programmers **Title-text:** Real programmers set the universal constants at the start such that the universe evolves to contain the disk with the data they want. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/378#Explanation) **Stats:** This comic has been referenced 666 times, representing 0.6708% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_czvx6fg)
ah bloody hell, was so so close to the right name as well!
`5_?0_?0_?0_?0_?0_?0` :)
Or English speakers writing "twenty two hundred" as `22_00`.
No, it doesn't at all if you don't know how to translate the units. What about centimeters? Does `100_000` help you spot how many meters it is? Like I said, it's only useful for people used to grouping. To me 30000 is 30000 and it's way easier to read and understand than 30_000 or 30,000 (which in my language stands for 30, by the way). My point is grouping decimals with commas or other signs the way some countries do is confusing for other nations. I think this could be even more problematic to beginners, keep in mind grouping you understand might not seem logical to some at all, and underscores certainly don't come obvious at first glance. Sure it's meant for readability, but only for a group of people. What about other types of grouping? There's so many I think it's better to just leave decimals intact, and let everybody count and group zeros how they like.
I agree, that's what I do too sometimes. Hence "Readability counts. ... practicality beats purity." 
Sorry I think I thought the comment I was replying to was in reference to something else. Still, int(5e6 + 1) is easy enough. I find 5_000_000 ugly and could easily become non-standard by going 50_00_000 (which just doesn't look right but is still perfectly valid).
I much prefer: int(5e6) To: 5_000_000 
You're then invoking a non trivial cast for no reason 
you can write this code using loops for better visibility: for i in range(len(X[0])): result = [] for j in range(len(X)): result.append(X[j][i]) print result so this first loop over length of embeded list `for i` and then iteration ofer list elements `for j` to get all first elements of embeded lists. Then we go back to `i` loop and iterate to second elements. In python you can also use `zip` function: print zip(*X) 
When we used to argue about this back at Facebook, the Word From On High was that the line limit was less of an editor thing and more of a complexity limit, forcing you to break up lines that do to much. It focuses the eye on lines that are doing a lot of work. Even a single function call might deserve to take up N lines, if it's arguments all have four words in their names. There's a lot going on there!
&gt; The former uses only the ALU while the latter uses the FPU. Because this is internally a call to C++'s std::pow (assuming CPython), Actually, this isn't the case. `math.pow` will delegate to the C library's floating point pow() function, but the builtin exponent operator will perform an integer exponent. As such, it'll give exact integer answers even outside a double's mantissa range. Eg. &gt;&gt;&gt; 3**100, math.pow(3, 100) (515377520732011331036461129765621272702107522001, 5.153775207320113e+47) 
Sweet! Thanks.
True, and this addition probably wouldn't a big deal. The only downside I can see is, for Indian or Japanese users (among others no doubt) who do not group by thousands. Japanese group by 4 zeroes rather than 3, so instead of 5e6 expressed as 5_000_000, it will be 500_0000 (or 500 "man", as the Japanese would say). 5e9 would be 50_0000_0000, which if you're used to looking at groups of three zeroes, looks at first glance to be 50 million, whereas it's actually 5 billion. Unless there's a standard for this, developers from countries that don't group by the thousand are going to have trouble, as will we when reading their own code.
Fair enough.
I actually like the theme that xkcd style charts use. I'll remove the random shakiness and use the theme all the time whenever I do plots. 
It's two nested list comprehensions. lets try to do this: new_list = old list with a for loop: old_list = [12, 32, 44, 21] new_list = [] for item in old_list: new_list += [item] which does the same as: for index in range(len(old_list)): new_list += old_list[index] if we want to write this as a list comprehension: new_list = [old_list[index] for index in range(len(old_list))] so the bold part is a loop new_list = [*old_list[index]* **for index in range(len(old_list))**] and the italics part is what you want to put into the new list. EDIT: onjin's comment is great, no need to finish this one. if you know the stuff i wrote, the actual answer is in his comment.
Yes, please. This should be part of the syntax highlighting class of features. This would have the advantages of being user-configurable and not add any of the disadvantages noted elsewhere. The benefit could even be achieved as simply as putting a gradient on every X digits of numeric literals (e.g., the digit which would usually appear after a separator could be brighter/darker). Or the IDE could insert zero-width commas or mess with the tracking/kerning to create digit groups. 
I kinda feel that this would be a bit silly. I can definitely see it having use but... it seems like a "do you really /really/ want it?" kinda things. As far as I know there are no libraries that do similar, despite being absurdly easy to make in a nice fashion, arguably no one has felt the need to make such a thing, yeah? X.d1_234_567 # or X( 1,234,567.89 ) # or # I can see issues this approach could cause though X[ 1,234,567.89 ] # or # etc etc # or if ast transformers land ~'1,234,567.78'
I'm the author of Orator. Orator aims at being simple and more intuitive than the existing ORM for Python with a less steep learning curve. I also want it to have useful features out of the box (relationships decorators, timestampable models, soft deletes, protection against mass assigment, model events, caching). Let's take an example to see what I mean: In SQLALchemy: class User(Base): __tablename__ = 'users' company_id = Column( Integer, ForeignKey('companies.id'), nullable=False, index=True ) company = relationship( 'Company', backref=backref('users', cascade='all, delete') ) In Orator: class User(Model): @belongs_to def company(self): return Company class Company(Model): @has_many def users(self): return User As for the state of the project, I still consider it to be in beta but I am already using in production since the 0.4 version without any major issue. However, I have to say that I do not intend it to be as powerful as SQLAlchemy to keep it as simple as possible, so it will never cover all the cases that SQLALchemy does.
Looks a lot like Pascal indeed. I had never heard of it. Looks clean. I've come to love dynamic typing, however...
It all depends on your needs I guess. The operator being a string allows for a more permissive set of operators without having to wait for the maintainer of the project to handle special types operator (for instance json or hstore in postgres)
You could try https://pypi.python.org/pypi/Unidecode for exact matches: &gt; unidecode.unidecode("garçon") == "garcon" # True To remove capitals you could `.lower()` (if you want case-insensitivity), you might want to look at https://github.com/seatgeek/fuzzywuzzy or https://pypi.python.org/pypi/python-Levenshtein/0.12.0 if you want edit distances for e.g. misspellings.
matplotlib is love, maplotlib is life. 
Can you drop duplicated store IDs? Pandas documentation [here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html)
I've know about this for a while, but I had never seen the [XKCD gallery](http://matplotlib.org/xkcd/gallery.html) before. It looks like they redid all sample plots in XKCD style!
TIL Japanese group differently! I still think 50_0000_0000 is easier to read than 5000000000
I use kcachegrind to profile cProfile output. It's not just for valgrind.
&gt; Anything else is local or regional tradition. This is how we got stuck with ascii for so long. 
&gt; Django's ORM has always been sufficient for me That means you have never dealth with two processes accessing the same row, and then both processing trying to modify it. SQLA handles this, Django ORM gives you a stale row error.
I do performance analysis and use python and matplotlib to gather and present results and had no idea this was a thing. I know what I'll be doing this afternoon!
We got "stuck" with ASCII for so long, because there was no better system available. You may be too young to remember, but RAM and storage was much more limited until recently. Have a look at what Javascript uses to understand what I am refering to. Also, thanks for the dumb downvote.
&gt; Does 100_000 help you spot how many meters it is? Yes, but that's not the point. It's about parsing numbers, and splitting numbers up makes them easier to parse for many people. Think about 743695232695 It's not easy to figure out what kind of magnitude that is, is it? Now look at (note that I'm specifically not using thousands as a separator) 7436_9523_2695 It's now very easy to see that we're on the order of 10^(13).
&gt; 5_000_000 than 5e6. Those are not even the same thing. One is a float, the other is an integer. Scientific notation is by definition a float value :P 
Recently was a decade or so ago, not a year or two. &gt; Also, thanks for the dumb downvote. I actually upvoted you because you were on -1, I have now downvoted you though. 
Wait, is that before or after I drink the usual 5 shots of vodka. Also, is this safe if I'm also writing C-extensions?
It has been in Perl much longer.
I understand the point of that, the units dispute was just an answer to another comment. I agree it's easier to figure out big numbers that way, but realistically how often do you have to deal with such literals, and how often do you actually have to quickly find out what order of magnitude that is? Is it something that would significantly improve your workflow? I say it's so rare that you'd be fine with just moving the caret and counting the digits yourself. On the other hand, I bet there will be people that will take this PEP too seriously, and code every number like that. &gt; There should be one-- and preferably only one --obvious way to do it. I enjoy Python's syntax, how readable and easy to understand it is. I think random underscores in numbers incorporate unneccessary complexity and confusion, and would only solve a minor inconvenience.
Personally, I would use just the names for the symbols, and use a vim conceal thing to replace them with the symbols. Unicode in source code can get a bit annoying, even if it works most of the time. I try to keep it ASCII only.
There's already a way to display numbers...without separators. 
Rubish!
You can use mathematica to create nice wiggly line graphs if you really need them. http://mathematica.stackexchange.com/questions/11350/xkcd-style-graphs
Did you include the plt.xkcd () at the beginning?
Right, but what fields/applications? I'm just curious who is using big literals often enough that they need this.
Source code is written in English. Your language doesn't matter here.
Since you edited so much, I'll make a new reply. You are right, the fix is to use this (inside of the while loop) Tokenizer fields(input_line, sep); for( Tokenizer::const_iterator iter = fields.begin(); iter != fields.end(); ++iter){ numChars += iter-&gt;size(); ++numWords; } count++; This lands it about the same time as my previous attempt and now the same answer as the other splits.
I respectfully disagree.
If by xkcd style you mean wiggly lines and wiggly font, yes it can do xkcd style charts.
In my header, along with the usual copyright notice, I usually add a note saying something like: &gt; Use python 3.x. &gt; Unicode symbols used in code. &gt; &gt; If you don't see a snowman here (☃) you need to check you encoding/font. &gt; &gt; See: http://physics.info/symbols/. &gt; If printing unicode characters to the console in windows, run 'chcp 65001' prior to this script. &gt; And change to a unicode supporting font (Lucida Console works well enough). People who use Linux never have any issues.
Yup, just copied the code from the page and ran it.
&gt; &gt; &gt; &gt; &gt; First of all, can anyone point me to a code base where there is a lot of such loooooong numbers that are (allegedly) hard to read? Apart from data files, obviously. Anyone in engineering working with binary and hardware.
Engineering and digital hardware.
That seems does seem much much more correct and much more efficient. You should definitely make a pull request to rectify the situation. 
Your article made me think a second time about trying Kivy. Needed to hear more positive experiences :) I have been toying with it for a while but found too many rough edges, possibly because I'm developing on MacOSX El Capitan (also tried the buildozer VM) and trying to deploy to Android Marshmallow, and even the most basic app with a single label crashes on phone. Let's do this one more time.
Are you saying you have a list of stores "income" by year? If so and you wanted to get a summary by store, you could use [groupby](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html). Here's some [another link](http://pandas.pydata.org/pandas-docs/stable/groupby.html). So you could do something like this: df.groupby('Id').sum() # if your summing, otherwise .size() if you want the count
you try and import a system package which is not the one Anaconda provides, but you're probably using the Anaconda python. you need to fix your $PATH or $PYTHONPATH settings
But you may want to have `0b_100_011` and `0b_1000_1000` in the same file. The IDE could never handle that.
1) /r/learnpython 2) The import command loads a module. `import base` means load the base module. That module needs to be a file or folder. Check your homework's instructions more closely to know what exactly you're supposed to be doing, or discuss it with your teacher.
how to fix that?
Done. For your convenience a link to my repo https://github.com/zwparchman/string-splitting
Hi, kaiserk13 First, thank you very much for your help! I really appreciate it. I used the first line of code: pyautogui.platformModule.keyboardMapping['@'] It doesn't display 19 but 11 in my computer which is the same number that appears when I replace it with 2 and ". All these three keys are on the same place in the ES layout, so I guess it's normal. When I changed the keycode with the second line of code (pyautogui.platformModule.keyboardMapping['@'] = keycode) I didnt't actually know what to put there as the keycode is in the right position (in this case 11) to type '@' with my keyboard. So I tried with '2' as you said and with '13' which is the number the appears next to the '@' in the dictionary (in the github link you gave me). When I wrote the first line of code again after the modification it didn't return 11 but 2 or 13. And I tried the last step in many different ways: pyautogui.platformModule.keyboardMapping['@'] = 11 #this is actually the default keycode pyautogui.typewrite('x') pyautogui.hotkey('altright','11') pyautogui.typewrite('gmail.com') Output = xgmail.com pyautogui.platformModule.keyboardMapping['@'] = 2 pyautogui.typewrite('x') pyautogui.hotkey('altright','2') pyautogui.typewrite('gmail.com') Output = &lt;class 'Xlib.protocol.request.QueryExtension'&gt; X protocol error: &lt;class 'Xlib.error.BadValue'&gt;: code = 2, resource_id = 0, sequence_number = 17, major_opcode = 132, minor_opcode = 2 X protocol error: &lt;class 'Xlib.error.BadValue'&gt;: code = 2, resource_id = 0, sequence_number = 23, major_opcode = 132, minor_opcode = 2 x2gmail.com pyautogui.platformModule.keyboardMapping['@'] = 13 pyautogui.typewrite('x') pyautogui.hotkey('altright','13') pyautogui.typewrite('gmail.com') Output = &lt;class 'Xlib.protocol.request.QueryExtension'&gt; X protocol error: &lt;class 'Xlib.error.BadValue'&gt;: code = 2, resource_id = 0, sequence_number = 17, major_opcode = 132, minor_opcode = 2 X protocol error: &lt;class 'Xlib.error.BadValue'&gt;: code = 2, resource_id = 0, sequence_number = 19, major_opcode = 132, minor_opcode = 2 xgmail.com Now, I assume I did something wrong in the process because using your method I was able to display '/': pyautogui.hotkey('shift','7') pyautogui.typewrite('usr') pyautogui.hotkey('shift','7') pyautogui.typewrite('local') Output = /usr/local So I guess there must be something special with the 'altright' key because when I use pyautogui.hotkey('shift','11') # 11 key for: 2,@ and " it actually displays " correctly. Again, thank you very much for your reply and help. But I can't still type the '@' symbol. The method you gave me works for all the keys that have two outputs (like 7 or / , 8 or (, 9 or )...again all of these on my ES Keyboard) but when they have 3 (e.g. 4 $ and ~ or 2 " @) it only displays the first alternative with 'shift' but not the second one with 'altright'.
Well, then something is off in your instructions, you're missing a file, or you don't understand what your instructor is looking for you to do.
[Image](http://imgs.xkcd.com/comics/ballmer_peak.png) [Mobile](http://m.xkcd.com/323/) **Title:** Ballmer Peak **Title-text:** Apple uses automated schnapps IVs. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/323#Explanation) **Stats:** This comic has been referenced 1033 times, representing 1.0398% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_czwd1nb)
Very good point, I didn't think of that because I've only occasionally used lambdas (classic case of, even when I've used them, I haven't been 100% convinced that it was better than the alternative way of doing it).
&gt; Project due tomorrow so quick reply would be nice no homework posts, see the sidebar. 
&gt; People who use Linux never have any issues. I haven't tried to use unicode in source on my Mac, but this is a big part of why I use a Mac at work. The vast majority of packages works on Mac if they work on other types of *nix; a small minority of packages require a small amount of tweaking to get working but it's not hard to Google to turn up a step-by-step guide of what you need to do; and a vanishingly small minority of packages, in my experience, require any serious amount of porting to get working on OS X.
I still consider it in beta, mainly because it has not been widely used for now. Still, it's pretty stable and I have been using it in production since the 0.4 version.
there's an easy (temporary) solution or a bit a "harder" one, depending what you want to have. do you know virtual envs? that's the easy way, in Anaconda that would be `conda create -n NAME python=3 OTHER_PACKAGES` activating will be `source activate NAME` Else you can modify your path in your `~/.bashrc` but this will override the system python which can (and will) lead to undesired effects. add the following export PATH=CONDA_INSTALL_DIR/bin:$PATH you of course have to substitute the install directory with a real path But if you just want to have some fun and code away I'd recommend that you use the virtual envs
they are all good enough, heck even Tk is good enough. Without knowing your constraints you will just get a bunch of people listing all the Python GUI frameworks (PyGObject (gtk), Qt (PySide), PyGame, Kivy)
pyfltk looks nice and is easy to use
Of course it could. The syntax highlighting can detect a binary literal and then there can either be hardcoded options like "break into groups of 4 bits" or "break into 3 bit sections unless the number of bits is divisible by 4". In an editor like Sublime Text, it's even feasible that the plugin could let you write some Python code to calculate the breaking pattern. It's not like it's more complicated than some of the live linting tasks.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
A whitty mouse over.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
That would be brilliant! If it works for you I'll have a weekend project to figure out what I missed in my toolchain :)
1. bad bad bad
You're conflating the way things are with the way things ought to be, and your last paragraph makes it apparent that you know it.
Any way to resolve it?
Kivy likely fits what you need. https://kivy.org/ Just as a heads up, people will respond better to a post like this in /r/learnpython. Join us! Edit: I'm not sure if this will conflict with pygame or not, but it probably wouldn't take much for you to port the whole program to Kivy since it is crossplatform as well.
i am using python2.7 and what is NAME=package_name ?
I have just installed the ubuntu. I can still reinstall it. Should I do it?
Is anaconda that important? Everything without anaconda is working fine. 
At this point it would be quicker to reinstall ubuntu
I leaned about this back in 2012. Not sure if this is the original author. http://jakevdp.github.io/blog/2012/10/07/xkcd-style-plots-in-matplotlib/ 
Any reason why? in case you don't absolutely need python2 I would recommend you to switch. But then you can just do python=2 instead of what I wrote. The stuff all in caps is a variable that you can set on will NAME will reference the virtual environment (so that it is easy to remember) while where I wrote PACKAGES you can specify what packages you would need for the work you're trying to do you can either provide just a name (or several) numpy pandas scipy or specific versions (not providing one gives the latest) pandas=0.16.0 
There's an xkcd font you can download from xkcdsucks.com which is called humor sans, which is pretty funny.
no it isn't, but Anaconda tends to make your live easier 
I have to solve this for colleagues frequently. It varies between easy and hard depending on how much they fucked it up and how competent they are at fixing it. So no, its not fucking bad advicw. It is battle hardened pragmatic advice. Read all the ops posts, do you think he is up to the task? A fresh reinstall is 30 mins and trivial however
I'm suggesting that another style of literal should be introduced for bitflags if there is a call for something like this with them. Any numeric literal that is being worked with **as** a number lacks any need for this absurd underscores-where-commas-or-spaces-traditionally-go. IDEs can (and probably should) do a better job displaying large numbers for human reading. Bit flag literals aren't "really" numbers, so if there's a use case for bit flag literals, it would make sense to create a syntax for that. Maybe these underscores are a good option for that. On the other hand, maybe a new primitive for bitflags is in order, since the operations performed on bitflags tends to not be very human-friendly, either. 
&gt;Fuck off nope. &gt;Of course I make exceptions a sure, sounded like it &gt;and no sudo! 
I used to think like you. Then I stopped being a gentoo ricer and grew up. Now linux is but one more scientific tool, and anaconda has been the biggest productivity multiplier I have encountered. On a daily basis I manage and look after scientific programmers. They don't care about linux, and neither should they. Life is too bloody short to waste time rm -rf ing your root partition under a misguided notion that learning not to do so will help you somehow get to that next scientific discovery.
the download failed (or got interrupted, hence the .part extension) delete that file and try again
/r/hailsatan
yes and they have a meaning sister
I'll make my slides with it. Hopefully it will annoy those bioinformatics students.
I work with a pretty big python code base where some code was written lots of years ago. Some of it follows the 80 char long guideline, and it is actually very hard on the eye. 120 is as arbitrary as 160 or 42, but it turns out to be a good amount for nowadays displays IMHO
did you activate the environment first? follow the instructions on the screen after you create an env with conda create
&gt; I already told you why is bad from the start. Making judgment on topic you admit you are not expert is sign of bad taste. I think you misunderstood the article. I didn't make any judgment; I said that Python is notoriously difficult to parallelize. That's not *my* judgment; it's the collective sentiment of the broader programming community--I'm only the messenger. Anyway, the beginner is in a better position than the expert to judge whether or not something is easy/simple. &gt; Well, you function is not returning or yielding value. Thats is not exactly why the guy "stupidly easy" was not able to run his example, (He probably dont know the fundamental difference between imap and map function from what I have seen), but you probably dont want to mix your parallel computation with printing in real world. I agree; however, this is a trivially simple example, and the print() statements are never reached since I'm only passing in good data. &gt; One reason is if your function is not returning value you will end up in total mess when you will try to use Pool.map or Pool.imap functions. Those two are quite easy to grasp but only in the case you are using pure functions which you are obviously not. For all intents and purposes, my function *is* pure (because the `print()` branch is never reached, and `None` is always implicitly returned). This is all unrelated to the problems we encountered (if you disagree, please provide an example that you consider to be "easy" and faster than the serial implementation). &gt; Seeing such approach I can understand why is parallel programming so difficult for you and your colleague It's difficult in Python only, because of the GIL. Parallel programming in other languages is easier. &gt; One more reason (not so relevant) is doing such useless exercise as to convert string to int and print it immediately to output. I hope I recall your code correctly. I already explained why this was relevant. I agree it's not so complex, but if Python can't take simple examples, what hope does it have for more complex ones?
Yeah i did. I think I should delete anaconda folder and learn to live without it :(
or learn how to use conda ;) try and activate an env source activate YOURNAME conda list if you then don't see numpy install it (in the same window) conda install numpy
Some points: - do you really need name mangling? If not, don't use double underscores - don't use in a method anything that is not either a parameter or a builtin (other objects from the standard library may be ok, like from collections). This may be a pain in the ass at the beginning, but it increases the testability and extensibility of your code dramatically. If you need a class from a module, add it as a class member, or even better, pass the instantiated object to the constructor (a.k.a. inject the dependency). This allows you to pass a mock object for testing, and if anybody needs to extend your class and use a different dependency, he/she can do it as long as the APIs are the same. - similar to previous, define anything that you can outside of the methods, but inside of the class, for instance that dictionary in the copy method. It increases reusability, extensibility and should be a tad faster. - if you provide an entry as script (the __main__ stuff), do accept CLI parameters (that install thingy). Check the package argparse.
The if main clause should not be at all there if it is for testing purposes. If available, it must provide meaningful access to the code in the module.
Hi there buddy, if you try pyautogui.hotkey('option','g')? What does it output? Are you on a mac?
Interestingly, the same feature was recently [proposed](http://wiki.php.net/rfc/number_format_separator) for PHP. It was narrowly defeated at the voting stage.
Why was that? 
How does client connect to table?
Why 80x25? I agree lines shouldn't be too long and 80 is a sane guideline. But 80x25 terminals are essentially a holdover from days when that's all a computer was capable of rendering. Now I can't install a modern Linux distribution without getting a framebuffer that has a higher (and tweakable) limit. Even on a small screen I still have a very high DPI. I can't think of any development hardware, even at the lowest end, that wouldn't do better with a 100+ line screen. Sticking to a hard 80x25 is the same as sticking with a screen that only displays a bright neon green color. I'd actually have to do more work to get to a more limited experience. Even if you kept the text source to 80 characters so your eyes don't have to scan back and forth much there's still a bunch to do with all the extra possible space. File navigation, linters, a second terminal. I am really asking why though. The only justification I can think of is from having used such a layout for so long that bigger layouts seem foreign. That I can get, but are there any other reasons?
&gt; Why 80x25? Because you should have a good reason to change an existing convention. There is no good reason. Even in typography, [the the optimal line length is between 60 and 90 chars](http://ux.stackexchange.com/questions/3618/ideal-column-width-for-paragraphs-online). Make it wider, and you need to move the eyes sideways all the time to quickly catch things. 
Does it randomly generate the line wiggles? i.e. is it different every time one plots?
Thanks for the tip /u/ianozsvald! unidecode does pretty much exactly what I was looking for regarding matching.
Lol
People are giving you good answers here, but I thought I'd just clarify Anaconda since I think you might have a misunderstanding. You mentioned installing numpy and theano and then installing Anaconda, which just doesn't make sense. Before installing Anaconda, when you ran "python" it was running the python installed in /usr/bin - that is, the python installed on your system. When you're running that version of python, and you import something, it'll look in the /usr/local/lib/python2.7/dist-packages folder (among a few others). If you want to see where python will look for installed packages, start python, and then "import sys; print(sys.path)" to get a list of folders, in order, that python looks. When you pip installed numpy and theano, it installed them into the /usr/local/lib/python2.7/dist-packages folder. This is because the pip you were running was also in /usr/bin and was associated with the system python install. However, then you installed Anaconda2. Installing Anaconda creates a whole new python install, in some other folder, with it's own python executable. It has its own set of packages and its own dist-packages (or site-packages) folder to look in when loading them. It'll also have its own /bin folder with a python executable as well as its own 'pip' executable and a 'conda' executable. To make sure you are calling these executables, instead of the system's python, when you type 'python' in the path, you need to make sure Anacond's /bin folder is before /usr/bin in your path. Do 'echo $PATH' from the terminal to see your path. Usually Anaconda adds a thing to your .bashrc to add itself to the front of your path, but it's possible you didn't select this option. What you really want to do is to use Anaconda's python and to install numpy and theano in Anacond's python distribution. Numpy is already installed (not sure about Theano) so you'd probably just want to update it. You can do this with "conda install numpy" and it will grab the most recent version if one is available. Then do "conda install theano" or if that doesn't work "pip install theano" (making sure you are using Anaconda's pip, not your system pip). Then if you use Anaconda's python, you should be able to import numpy or theano with no problem. At any time, if you want to see which python or pip you are calling when you use their command on the terminal, type: which python or which pip 
According to https://en.wikipedia.org/wiki/Integer_literal#Digit_separators Java, Julia, ruby, Perl,ada, already have it. C# is planning on adding it. C++ uses ' apostrophes instead. Also almost every body will be splitting by 3 digits (except in binary/hex).
Just saying, using bootstrap is literally the easiest thing ever and if you can't be bothered to do that right then why would I trust your ORM? Lol I don't give a shit if you listen to my comment but you should give a shit about the impression users get from your site promoting your product. I was interested in seeing what this was but when I can't even read the code on my phone I gave up. 
Congrats, that makes you one of today's 10,000.
What indicates that (any of) these were created with Python?
The pixels.
Cute. Thanks for nothing. matplotlib? Seaborn?
Why should it be? (Nothing against PyPy, I use it from time to time myself.) Okay, it offers some speed-ups here and there but on the other hand, it lacks in features. E.g., check the [PyPy Website](http://pypy.org), it's feature-wise still at Python 3.2.5.
It's hard to site any one thing, but it's clearly matplotlib--BV Sans, viridis colormap, weight of lines, style of borders, etc.
Yup, viridis.
The standard Python interpreter is CPython. It's the reference, and in several corner cases of the language, it defines the semantics of Python. No OS vendor would fail to supply *that* interpreter if they supply Python. For another thing, the PyPy team seems a bit indifferent to Py3 support: they only reached Py 3.2.5 support 1.5 years ago, and that's where things still stand — no 3.3 yet, nor even any promises of "real soon now". But I could certainly see a vendor offering some alternate interpreter or compiler. Perhaps PyPy, or Nuitka, or ....
Have you tried /path/to/file.py:40 ?
I'm honestly not surprised. Python is very versatile. 
Seaborn is matplotlib with some better defaults.
it's really JUST a plt.xkcd() before plt.figure()? Man, I love python.
I agree that SQLA is really powerful, no doubt, and I have been using it myself in many projects, but then again, Orator is simpler and does not aim at being as feature-rich as SQLA. And Orator is not just an ORM either, it has an ORM but also an independant raw query builder (more or less like the DBAL of SQLA), a schema builder, migrations. So the ORM is just a part of Orator but not a mandatory one.
1. Unfortunately Python is only able to meet the latency demands of VR with some serious extra work, such as using Cython. 2. There are no widely used 3D graphics libraries for Python. Without any sarcasm, good luck! I'd love to see this happen, but there are good reasons it hasn't yet.
A good day for python.
easy enough to supply [both](http://docs.sqlalchemy.org/en/rel_1_0/core/metadata.html?highlight=op#sqlalchemy.schema.Column.op) as well as the ability to [make your own operators](http://docs.sqlalchemy.org/en/rel_1_0/core/custom_types.html#redefining-and-creating-new-operators). Unusual operators are the edge cases so it's reasonable to provide exception APIs when they are needed. Also, operators have strong implications for the types of objects on both sides of it as well as the return type of the expression. Injecting an arbitrary string operator is not sufficient in cases like that of HSTORE or JSON if your system wants to know upfront the return type of an expression, or needs to know how it should interpret one or the other side (such as JSON coercion).
Would have loved to see this done in XKCD graphs :)
The Python Secret Underground emphatically does not exist.
I missed a line, the gist has been updated. Thanks for bringing it to my attention.
I could handle 70% on some things if that meant I got 200% on most things. I'm so heavily invested in things like numpy, scipy, matplotlib, vtk, and PyQt that PyPy is not an option for me on anything large enough to benefit from PyPy.
Surprisingly matplotlib is much less 'pythonic' than most packages--specifically because it was designed to replicate Matlab plotting and therefore functions very similarly. The hit may not be as big as you think!
If anybody is curious why it's the 'best' default colormap https://youtu.be/xAoljeRJ3lU
I agree and it's worth it. That being said, I still make the mistake to write dx=1/N...
 import antigravity
what's wrong with that expression?
 $ strings LIGO-P150914Detection_of_GW150914.pdf | grep matplotlib &lt;pdf:Producer&gt;matplotlib pdf backend&lt;/pdf:Producer&gt; &lt;pdf:Producer&gt;matplotlib pdf backend&lt;/pdf:Producer&gt;
Imagine how the problem would have been solved by now if you had reinstalled Ubuntu as I suggested.
What do you mean by "running slow"? Initial indexing of a project and Python libraries? Or do you experience some lags while coding? I'm curious when it works as not expected?
I'd guess learning how to program Numpy to get similar performance to existing data manipulation they're doing in Matlab would be where the biggest learning area would be, rather than learning the matplotlib api.
I usually just lurk here, but made an account just to say I totally agree. Pycharm's just adding features at the cost of stability, and it runs like a dog now. Keeping more than one IDE session (I usually work on three or four projects at any one time) going for more than a few debug cycles and it slows to a crawl, memory usage bloats and it has to be closed and reopened to get it responsive again. The only way I can get any usable functionality out of it is to run it in power saving mode, which disables just about every useful feature - code completion, dynamic linting (or whatever it's called these days), and then to also manually clear the heap every hour or so. At which point it just becomes IDLE with a good debugger. I program in python 40 plus hours a week, and I have absolutely no idea what 'Tox' is, so why would I care about having this integrated into the main client when it doesn't even do memory garbage collection properly any more? And I'm sure the answer from their sale person is that I need to upgrade to 5:1 and ALL will be wonderful, but in my experience over the last year or so things will just get even worse. This isn't a new problem, it's been happening through the whole of release 4, and so if fillipovd20 is seriously asking you for clarification on what you mean by 'running slow' then I doubt that he's ever actually used the product for any significant period of time on a large project. It's sad, I used to be a HUGE fan, but I think it's just become a sandbox for gimmicks, vis Tox integration, and despite paying for the next year's subscription I've quit updating and I'm torn between either regressing back to version 3.x or even 2.x which was when it was still a strong product, or quitting entirely and going to visual studio. For me they hit rock bottom when they sent the email out to their users telling us they wanted to connect with us on an 'emotional level'. The only emotion that evoked was anger - unless frustration also counts as an emotion? 
Install anaconda * https://www.continuum.io/downloads (choose the python 27 one) * create a conda environment * install tensorflow https://anaconda.org/jjhelmus/tensorflow theano is already present in anaconda distribution READ THE DOCS * http://conda.pydata.org/docs/test-drive.html * http://conda.pydata.org/docs/_downloads/conda-cheatsheet.pdf * http://conda.pydata.org/docs/using/ Tips: * use an LTS version of Ubuntu * people on the internet have no obligation to help you, and do so of their own free will. continuing to badger them will result in them ignoring you * try to say thank you occasionally * RTFM 
The matplotlib api is very arcane though - I spend much more time trying to get my charts to look right than I do actually getting the numbers to work.
Yeah, and this doesn't look like seaborn. No background-grid, for one thing.
Thanks :)
It would be nice to have the x-axis to be increased slightly to include 0.25 (s). 
It looks like there awesome veridis colormap on the bottom two subplots
For Pynsist, I deliberately avoided all of the pip/setuptools/distutils stack, because I agree they offer little or nothing for distributing applications.
Also, all new amazing things comes with cpython.
Great job! The API looks really nice! Going to evaluate this for my next app. Btw. Is orator influenced by Laravels eloquent? Its has a very similar feel to it.
Thank you.
FWIW I actually really like Pynsist for that reason (although I only used it for one customer so far)
For me there are times when usage find completely halts the program when the name of the function is very common. Sometimes when editing regular JS or CSS files it gets so slow I have to close and restart to be able to work again. It's nothing reproducible, just happens sometimes in very different cases. Fortunately it's rare enough that I can live with it, but still annoying.
Excellent, I'm glad you like it. If you're able to tell me, what was the application like technically (e.g. which GUI toolkit did you use)? Did you have any difficulties with Pynsist?
Reported? And im the one being hostile? All i did was follow a google search which led me here and except for couple of posts rest of threads been a noob bash saying over and over get lost you are not welcomed here when i got a 3rd one i cant help but say whats on my mind i have been very limited in expressing my real thoughts trying not to be too offensive but when you keep getting replies over and over about the same thing it gets annoying i disabled inbox reply but i still got another please do one more mod duty lock this thread? If im being hostile again i apologise just want this done with don't want any more inbox messages about this dead topic
Not sure exactly what you mean but: async its equivalent to "@asyncio.coroutine" decorator in Python 3.4. Await its equivalent to "yield from" in Python 3.4 They are part of a PEP https://www.python.org/dev/peps/pep-0492/#rationale-and-goals One of the main reason for its introduction on python 3.5 was to introduce a more concise semantic for coroutines. Essentially those are the keywords used to make the event loop to jump from one place to another and avoid the current thread to block in long running operations. They are useful when you want to develop applications with long blocking calls such as IO calls( database queries, http requests and so on) An example: E.g # echo.py import requests import asyncio loop = asyncio.get_event_loop() async def fetch_url(url): print("Fetching %s" % url) result = await loop.run_in_executor(None, requests.get, url) print("Finish fetching %s" % url) tasks = [] for url in ("https://google.com", "https://www.facebook.com"): tasks.append(asyncio.ensure_future(fetch_url(url))) loop.run_until_complete(asyncio.wait(tasks)) loop.close() If you run this code: $python3.5 echo.py The output will be: Fetching https://google.com Fetching https://www.facebook.com Finish fetching https://www.facebook.com Finish fetching https://google.com If you look at the output you can see that the main thread didn't block when it made the http requests. Instead, it triggered the first request and carried on, triggered the second request and waited until both requests finished. During this time the event loop was just looping until it got the responses from the requests. Not sure if this is the exactly what you were asking but I hope this helps. 
Thanks. Good to know. Been a little while since I've used SQLAlchemy. I just grabbed an example from the documentation. My point still stands though... you need to define the relationship on both models (through "addresses" and "user_id"). Honest question: would it be possible to do the following if you did not have access to change the source for the User model? class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String) addresses = relationship("Address", backref="user") class Address(Base): __tablename__ = 'address' id = Column(Integer, primary_key=True) email = Column(String) user_id = Column(Integer, ForeignKey('user.id'))
3rd RULE: yOU dO NoT taLk ABouT python uNDeRgRoUND
Excellent criticism and a frequent problem I experience where the ticks do not extend as far as I would like.
Another solution would be to force the label for the minimum value of the x axis...
Thank you for the reply !
A lot of the talks at these conferences are worth checking out. Sometimes useful info, here's another one I like https://www.youtube.com/watch?v=5GlNDD7qbP4
It was a scientific analysis and experimental control gui using pygobject + numpy + matplotlib. I stayed very close to the example, so everything worked out of the box. It was installed on about 10 users machines and they have been using it every since.
Yes, that's how it started. I wanted a simpler way to define my models and manipulating my databases when working with python so I looked what was existing in other languages and I decided that Eloquent was very near of what I had in mind so I adapted it to Python.
Even those map visualizations can be produced in Python using Basemap.
What you're missing is that the "seconds of cpu time" measurement is not actual seconds of service, but time the cpu is cranking on your code. You can have processes using time.sleep(), for example, that won't be using any CPU time. Even having an iPython console open and working interactively uses up CPU time pretty slowly. Probably waiting on network I/O would not consume CPU time either. If you're doing long-running calculations that are computationally intensive, that would take more. 3000 seconds is 50 minutes. You can use the free tier to run some tests and see how much CPU time your code takes. It's probably way less than you think. 
Here is a Jupyter notebook showing how they processed the data from LIGO: [https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html](https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html)
This post's title is actually the title of the linked blog post, explaining how async/await works. Props for answering anyway though. 
Hmm, yeah I for some reason I divided by 1000... never the less quoting from https://www.pythonanywhere.com/tarpit/ : "If you kick off a process with a busy loop, you should find yourself using up your allowance at approximately 1 second per second. It's a cumulative counter though, so if you kick off multiple processes at the same time, you can get through it faster!" so still 50 mins is not a lot. I use it for web scraping, does it take a lot of cpu(basically what I do is to scrape a website, and saving its data in sql)?
&gt; Suprisingly More like "annoyingly".
&gt; I am sorry if I was too rude. Really sorry about that. No worries. :) I'll look at your link.
Hi, r/python this is my first module so I'm happy to take any criticism, improvements, or pull requests. If you're not into a README, there's a short talk I made for our local Hack and Tell you can step through here http://thoppe.github.io/python-hyperoperators/HnT_pres.html#/ 
Of course, put the relationship on Address instead. Plus, it's Python, we don't need "the source" to add attributes to a class anyway. Mappings update dynamically when you add attributes. You could just as easily add columns to the User table.
You delete your own post.
You have done a superb job! I just made a minimal test app with orator, and so far its very pleasant to work with. Ill try to contribute to orator itself, after i have more experience with it.
Neat, thanks :-)
Why not the cheapest Digital Ocean box or the cheapest AWS box?
/u/_xr gave you something more definitive, but the style certainly looks like matplotlib. For me, it's the kind of thing where I wouldn't claim to be 100% sure, but if it wasn't matplotlib, it was a tool trying to look like it. 
Python can work with really large numbers without any problems (e.g. `2 ** 2048`). What does your library add to it? I'm just curious. In which cases should we use it?
I am glad to see this type of thing getting more attention. Thankfully, matlab moved away from Jet. I didn't mind the default matplotlib, but still. I actually prefer `plt.cm.Spectral_r` for color maps and, I ported a version of [`linspecer`](http://www.mathworks.com/matlabcentral/fileexchange/42673-beautiful-and-distinguishable-line-colors-+-colormap/content/linspecer.m) to Python (and the continuous version of that is the same as the noted colormap). If you're curious, [here is a demo](http://i.imgur.com/AlQhf7d.png) This, of course, often comes down to personal preference
You did a really good job explaining though! 
I guess it's for dealing with numbers close to Graham's number. Reminds me of this http://waitbutwhy.com/2014/11/1000000-grahams-number.html
PyPy is great for what it does, but it has a number of disadvantages as the system python. - higher memory requirements; - slower start up; - JIT compiler warm up costs; - poor support for C-based extension modules. What that means is that your average Python script that starts up, runs for a few tens of microseconds or so, then finishes, PyPy will likely be considerably slower. Python already feels a bit sluggish to start up compared to (say) bash scripts, moving to PyPy would make the problem worse. 
Python's native support for large numbers is exactly why this project was so straightforward to code. What you're missing is that `2**2048` is exponentiation. Hyperoperators deal with larger operations. Tetration, for example, is next. 2 tetrated 4 times is `2**(2**(2**2))) != 2*(2**2**2)`, that is you repeat exponentiation that many times. Pentation repeats tetration that many times and so on. As to what it's useful for -- I've been exploring the complex plane with it for tetration. There's a neat picture on the github that shows an example of a holomorphic continuation for tetration. 
This needs to be at the top, serious MVP comment.
O.K. this works fine in Jython as well. My original idea to use the JSR223 SriptEngine API for compilation doesn't work out because the 'single' flag is not supported and Jython has no means for setting compiler flags at this place. However `exec compile(...)` can be executed as normal source code by the ScriptEngine which should have the same effect. No need for a direct access of the Jython API. Thanks again. Your answers were very helpful for me.
This common font mismatch is also a pet peevee of mine.
I think the author was being facetious when they gave the example of computing Graham's number, as that's essentially an infinite loop that will eventually exhaust the memory of any computer, even one that used all the matter in the observable universe. 
First, you should have stopped reading after the third line of the docs: &gt; See also [The Requests](http://requests.readthedocs.org/en/master/) package is recommended for a higher-level http client interface. because this package is way better and more pythonic than urllib2. Then, you have to find out how the search is done on the website. It's probably a &lt;form&gt; with an action and a method. [Read things](https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Forms/My_first_HTML_form) about HTML forms. But you should have posted this on [/r/learnpython](https://www.reddit.com/r/learnpython) even if you question is probably not python related but html related (methods, forms, etc.).
Surprisingly, almost nowhere. There is a deep connection with the exponential and much mathematics (in that, it isn't just "repeated multiplication"), but tetration and above doesn't have much use in formal proofs. The rare examples are Graham's number and the Ackermann function. That said, it's interesting to be able to name a finite number that is larger than you could before with a new understanding of how quickly these functions grow. I took that idea into a python module where you could, in the limit of infinite memory, explicitly represent these numbers. 
Yeah, it really TeX's me off, too.
2**2048 is not a really large number, it is only about 616 decimal digits, which makes it a microscopically small number by the standards of "ridiculously large numbers". Let's put it this way: suppose you generate higher and higher power towers consisting of repeated powers of two: 2^2 = 4 2^2^2 = 16 2^2^2^2 = 65536 The next tower 2^2^2^2^2 is a number with over 19700 digits. (Python can still handle numbers of this size quite easily.) Now imagine the tower: 2^2^2^2^2...^2 where the number of exponents itself was `2**2048`. *That* would count as a "ridiculously large number". I don't know whether the library actually can deal with numbers of that magnitude or not, but some branches of mathematics routinely deal with numbers so big that there aren't enough atoms in the universe to even write out the chain of exponents. See, for example, [Graham's Number](https://en.wikipedia.org/wiki/Graham%27s_number) for one famous example; [tetration](https://en.wikipedia.org/wiki/Tetration) for the first step beyond ordinary exponentiation; and [Knuth's up-arrow notation](https://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation).
Same price ~ no reason not to.
There are multiple reasons/ways of doing python. Some of them are: 1) Web development (using frameworks such as django/flask) 2) System automation (a basic example would be to write a script that backups her files, automatically collects data and stores them in a filename for her, etc) 3) Data analysis: analyzing the data she obtained from her research. This might also include big data analysis/operations such as hadoop/pyspark where you are working on TB of data using map reduce. 4) Scientific computing (obtaining differential equations, approximate solutions of chaotic systems etc). As a scientist, she probably needs to do 3, and 4 a lot, and would tremendously benefit from 2. 1 would be helpful but wordpress would suffice. You can use amazon.com as a starting point for search books in each of the categories. For no 2, I can recommend [automate boring stuff with python](http://www.amazon.com/Automate-Boring-Stuff-Python-Programming/dp/1593275994/ref=sr_1_2?ie=UTF8&amp;qid=1455296144&amp;sr=8-2&amp;keywords=python+for+automation). For 3, and 4, there are just so many books available on amazon.com that it is redundant even to mention 1 or 2 here. She would probably benefit from 1 but not necessary at all. For learning things about function executions, compilers, there is a nice book called structure and interpretation of computer languages, in addition to other hundreds of nice ones available. 
Mocking is a pretty awesome facility but it takes some work to develop a feel for it. I have found on my own personal testing journey that writing my code as a bunch of "referentially transparent" functions is the most preferable path with mocking providing a nice bridge for the set of use cases where that isn't possible. One of the hardest parts of trying to sell others on mocking rather than calling actual collaborators is convincing them your test suite doesn't ensure all possible execution paths, rather validates the assumption that "if the collaborating code responds this way, my code does this thing"
I don't have a formal CS background either, but: The functions both get *defined* before either of them run. Try this: def monkey(): foo() print bla bla = 'hi there' monkey() # Call it def foo(): print 'foo' Here, we're calling `monkey` before we define `foo`, and just as you'd expect, it doesn't work. But if you don't call monkey until after you define `foo`, there's no problem. The key is that when you define a function, the computer doesn't run the code until you call it. This sounds obvious to a programmer, but in my experience it takes new programmers a while to get their head around it. There's also an interesting point about scopes - how can you use `foo()` inside `monkey`, even though it's defined outside? I don't have any specific books or teaching methods to recommend - as you're finding, people approach this stuff in really different ways. At least some of the online books in the sidebar will have print equivalents, though.
we did it 
5 years you've been waiting for this moment. May I ask how it feels? 
Or switch to Python 3.
Pls share
No the solve operator in Matlab (/ and \\) is only for matrices and is solved in Python using numpy with the numpy.linalg.solve function. Unfortunately, Matlab is a lot better at checking the matrix to decide the correct decomposition whereas in Python you normally need to be smart about applying cholesky or QR or whatever flavour is appropriate for your matrix structure. Matlab also 'just works' for sparse matrices whereas Python requires Scipy and that doesn't use the fast suitesparse implementation :(
Learning new things always enriches your life, even if there are no immediate, apparent uses for those things. There is no such thing as useless knowledge. Once you figure out what you can do with a skill, you will often find all kinds of problems you can solve with it.
"A programming language that doesn't change the way you think is not worth learning." -- Alan Perlis One thing I learnt from Python was how to deal with data. I mean, the basic objects (lists and dictionaries, mostly) are really focused on what the kind of *data* they carry, not its functionality. With that, I learnt a lot about OO thinking about data, not functions. Pylint complains a lot about things like "no-self-use" or "too-many-arguments" or even "too-many-local-variables" and dealing with it I learnt how to manage my data properly, breaking when necessary and throwing away whole objects because a simple dictionary would suffice. So... yeah, Python did change the way I think, not only when coding in Python, but with other languages too. It was not something that clicked right away, but it made me a better programmer in the long run. (Also, Python is super easy to write prototypes, so it took a lot less work to make those things click than any other language I've worked with.) *Edit*: Also, PEP8. I learnt a lot how having a concise and "definite" style guide for a language can help improve not only the way you read code, but how you can write code in a more readable way.
you have to get data into a matlab readable format somehow.
Names are not resolved until the moment they are needed in Pyton. This means the bla variable can't be found when monkey is called. But the fact foo is defined after monkey is irrelevant, unless monkey gets called (not just defined) before foo is defined. 
Lots of results are reported in ascii -&gt; grep | sed | awk etc. Also managing large (amounts of) files is easy in bash and unwieldy in a gui. Some of these things are one off jobs, too, so bash might be faster than python. 
Unless I'm misunderstanding what you want: http://www.jython.org/downloads.html
Take a look at https://automatetheboringstuff.com/ Sure you can always learn something just for the sake of it, but you'll have an easier time if there's some purpose behind the endeavor, ie. my brother learned perl/regex in order to generate wiki pages from help files for a game he plays, he'd previously tried to learn programming several times prior but always lost interest. If there's no computing related task you're really interested I'd suggest spending your free time on something that holds your attention.
Thank you guys
Python is a joy to develop in, if you have any serous experience with another language, spinning up on python is pretty painless.
&gt; I am not trying to parallelize rows, this is impossible because it is in one loop. But I can easily parallelize processing of every single row. Please note this key difference. Parallelizing the processing of each row is *one* solution to the problem; however, you can also break the input list into sublists and then invoke the `validate_rows()` function on each sublist in parallel. I took the latter approach in my parallel Go code. There is nothing fundamentally unparallelizable about my implementation; the problem is that sharing memory across threads in Python creates GIL contention, and the other solution (which the multiprocessing package employs) is to copy across processes the (potentially huge) memory that we would like to share, the overhead of which negates any advantages afforded us by parallelizing the processing. The `multiprocessing` approach isn't awful for operations where the shared data is a URL or a filepath or some other address that points to some larger data on which we'd like to operate; however, for all other parallelism tasks that I can think of, it's more of a liability. &gt; Here you are: http://pastebin.com/ihYMCaut Unfortunately, it's impossible to benchmark this because you aren't reading in rows from stdin and then timing the processing like my examples are doing. Could you please update this so we can compare apples to apples? &gt; But actually, my big criticism is still true. You are trying to do a lot of steps in one function instead of breaking your code into smaller functional parts. And such code is really really difficult to parallelized. Don't parallelize the function body, just divide the input data among worker threads (or processes) who each invoke the function on their block of input data. There's nothing "unparallelizable" about this algorithm. :)
The only thing that's a little dicey is the fact that you're dealing with a default argument. You can work around that, but it's a little kludgy: import functools def traceit(method): @functools.wraps(method) def wrapper(self, *args, **kwargs): print('TRACE: {} from {}, arg1 is {!r}'.format( method.__name__, self.__class__.__name__, args[0] if len(args) else method.__defaults__[0])) return method(self, *args, **kwargs) return wrapper class Foo: @traceit def method1(self, arg1='blah'): print('METHOD: arg1 is {}'.format(arg1)) def method2(self, arg1): pass @traceit def method3(self, arg1, arg2): pass bar = Foo() bar.method1() bar.method1('non-default') Outputs: TRACE: method1 from Foo, arg1 is 'blah' METHOD: arg1 is blah TRACE: method1 from Foo, arg1 is 'non-default' METHOD: arg1 is non-default 
It's a mini computer. Used to cost $35, now much less. Runs its own Linux off a SD card (you need to provide) and uses less then 4W of energy, so you can leave it on all day. You can use it directly, but the best use is to write code on a normal computer and then transfer it to the pi. The main reason to get one: Millions of these have been sold. So if you're not an expert, you can google for help. With a normal, big Linux machine it's easy to get stuck as a beginner, with a pi getting help is sooo much easier. 
Thanks. @traceit is doing more than just printing things, I think I was missing self. I've gotten further along than I was before :)
The guys from LIGO are doing an AMA right now and I [asked them](https://www.reddit.com/r/IAmA/comments/45g8qu/we_are_the_ligo_scientific_collaboration_and_we/czxnlux) about this.
Is everything working?
Against it - unnecessary complication. If you want to keep your const as 0b_0011_1111 - keep it as string or tuple and preprocess. "0011_1111_0101" or (0011, 1111,0101) But IMHO hex is better: x3F9 - faster to compare/check/etc.
Surprised to not see [this XKCD](https://xkcd.com/353/) here since we are talking about python and gravity together!! 
[Image](http://imgs.xkcd.com/comics/python.png) [Mobile](http://m.xkcd.com/353/) **Title:** Python **Title-text:** I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I'm leaving you. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/353#Explanation) **Stats:** This comic has been referenced 213 times, representing 0.2141% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_czxoyhz)
You're insane then :)
It's already a string. `os.listdir()` returns a list of strings, and you're iterating over it. If you want to get the extension, use [`os.path.splitext()`](https://docs.python.org/3/library/os.path.html#os.path.splitext). Also, there's no reason to use `os.getcwd()`. Write `os.listdir('.')`, or if you're using Python &amp;ge; 3.2 you can write just `os.listdir()` as the path became optional, defaulting to `'.'`. 
Do you like making money? There are always good paying jobs looking for self-taught Python Programmers. I kinda wish I skipped college and just learned python + javascript for a few months. Would have been where I am at now at lot faster. 
Just to add a tidbit, `os.path.splitext()` does the same as `.rsplit('.', 1)` (except it returns a tuple instead of list). It does not intelligently check for extensions like `.tar.gz` Edit: Small note, there is a small difference in that `.rsplit('.', 1)` will remove the period before the extension, `os.path.splitext()` will not, did not notice that before. 
obligatory 'this is not /r/learnpython or stackoverflow' Answer: files = [os.path.splitext(file) for file in os.listdir('.')] print(files) print(files[2][0]) # [("Backdoor sluts 9", ".wmv"), ("American Idiot - Greenday", ".mp3"), ("test", ".txt")] # "test" That way you can access all the files just by filename or extension, if that's what you were wondering. If you are searching for something specific, you'll want to check out [glob.glob](https://docs.python.org/2/library/glob.html#glob.glob) Edit: Did this answer cause this to become NSFW? Really? I didn't know there was such hate for greenday... 
I'm curious what use cases are common for exit functions. I try to avoid them as much as possible and be explicit about my context handling. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/beetlejuicing] [antigravity](https://np.reddit.com/r/beetlejuicing/comments/45gs9l/antigravity/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
[It](http://pastebin.com/raw/xXucJgLA)'s really simple. Adapt it for your style and call it before plt.figure().
You're unemployed, there's an ongoing need for more programmers and you can't see how learning to program might make your life better? I suspect programming is not for you. 
/golfclap your hobbyist language is finally used in something serious
Yes this worked. I was using splittext() instead of splitext().
I learned Python to learn to program, with no clear goal in mind. Once I was more or less comfortable with it, helped me breeze through homework that my smarter classmates struggled with (materials science). The reputation got me an offer for a PhD and now I use it **everyday** to breeze through calculations that would have taken other people 5 times as long.
Lose, dude, lose.
I figure that it'd be hard to get a programming job at the age of 17 lol
I'd have to look around. I live in a fairly small town.
Well, it's *split extension*, as in split the file extension from the file name, not *split text*.
Oh you're right. I see it now.
&gt; Which example are you referring to? I used your blog code. I know, but you took out the I/O and the timing stuff. So I'll need to put that stuff back in before I can benchmark. I might be able to try benchmarking it tonight. Regarding the rest of your post, I don't think writing in an imperative style has anything to do with how hard this is to parallelize efficiently. Maybe I'm wrong; we shall see when I try benchmarking against your code.
I asked for a kit for Christmas but I didn't get it. : ( 
The inverted comma at the end of the directory string is a none ascii character. Did you copy and paste it from a web page? Make it look like the inverted comma at the start of the string and see how that works out. Good luck in any case.
you saved a life today! do you know that?
Ain't no jobs at all, yet.
I release site 10 minutes ago. I'm working on SEO right now, so resumes on site will be helpful. Greetings! :) 
Not really. "Don't mock what you don't own." is pretty common advice. I find myself frequently wrapping IO and system calls in interfaces and testing that the interface is called correctly, throws expected and unexpected errors, hands back correct and incorrect values. And I leave the actual implementations in integration or end to end tests. Things like `os.remove` are rarely stand alone things, they're often paired with stuff like "Format this file name.", "Join this path." "Can I find the file?", "Do I have permission to remove the file." This interface might be a class, especially if there's multiple contexts I'll want to use it in: class ReportRemover(ABC): @abstractmethod def remove_report(self, name): pass class FileReportRemover(ReportRemover): def __init__(self, path, ext): self.path = path self.ext = ext def remove_report(self, name): filename = self._format(name) if os.path.exists(filename): os.remove(filename) return True return False Or it might be just a function: def remove_report(name, ext="html", path=".."): filename = ... if os.path.exists(filename): os.remove(filename) return True return False Either way, I now own that interface that deletes the report. I only need to mock that, instead of a good chunk of the os module. 
But how do you test `remove_report()` then ? 
Last I heard, Microsoft stopped officially developing IronPython, which was then supposed to be taken over by the community. I have no idea what this means in practice. EDIT: It looks like development moved here: https://github.com/IronLanguages/ Last commit on the ironpython3 project is from November 2015.
[Here](https://www.reddit.com/r/IAmA/comments/45g8qu/we_are_the_ligo_scientific_collaboration_and_we/czxnlux) is what the guys from LIGO answered about Python
 def test_remove_report_removes_the_report(tmpdir): p = tmpdir.join('report.html') remove_report(name='report', ext='html', path=tmpdir.dirpath()) assert len(tmpdir.listdir()) == 0 Like I said, I leave the testing of the implementation outside of unit tests. With a unit test, I'm testing one specific behavior that's completely under my control. For example, I might test this in a unit test like this: from pretend import raiser, call_recorder def test_report_sweeper_continues_despite_OSError(): remover = call_recorder(raiser(OSError())) report_sweeper(names=['foo', 'bar', 'baz'], remover=remover) assert len(remover.calls) == 3 The actual body of `report_sweeper` might look like this: def report_sweeper(names, remover=report_remover): for name in names: try: report_remover(name=name) except OSError: pass return True In this test, I don't have a single care about what a report is, where it lives or how it's removed. Just that when an OSError is raised, `report_sweeper` chugs on like nothing happened. That's a unit test. Testing that an implementation of "Remove the report from the system" actually removes a report is an integration or system level test. Another thing I like doing is combining classes and functions. I'll provide a nice, rich class interface to some concept. And then a function -- or functions -- for the most common cases. Python does this as well. `open` is just a factory function for a whole hierarchy of objects designed to process text and byte based streams. When you call `open` it just creates a system of objects that best fit the file being opened and how it's being opened and what system you're running on. So I might go with the rich class interface for report removing: class ReportRemover(ABC): def remove_report(self, name): pass class FileReportRemover(ReportRemover): def __init__(self, path, ext): self.path = path self.ext = ext def remove_report(self, name): filename = self._format(name) if os.path.exists(filename): os.remove(filename) return True return False But then come to find out that most of the time I'm removing reports, they're file reports! So I'll do something like this: import settings def delete_file_report(name, path=settings.DEFAULT_REPORT_PATH, ext=settings.DEFAULT_REPORT_EXT): return FileReportRemover(path, ext).remove_report(name) But I still want this interface exposed for my other clients who might store reports in a database, or on S3, etc. And also for testing purposes. It's incredibly easy to dependency inject the ReportRemover, but instead of injecting the default, I inject one that throws errors, one that doesn't throw errors, one that always returns True, one that always returns False. Edit: One last thing, don't fall for the "unit test uber alles" propaganda. Don't get me wrong, I really like unit tests and I write a bunch of them. But the name "unit test" gets abused quite a bit, and there's always going to be something you can't unit test.
Okay, I benchmarked your code and it still is about twice as slow as the sequential Python algorithm, and about 20 times slower than the parallel Go implementation: # parallel3.py is your implementation $ go run csvgen.go 100 100000 | python3 parallel3.py Beginning validation... Validated 100000 rows of 100 cells in 0:00:05.545268 $ go run csvgen.go 100 100000 | python3 sequential.py Beginning validation... Validated 100000 rows of 100 cells in 0:00:02.901513 $ go run csvgen.go 100 100000 | go run parallel.go GOMAXPROCS: 4 Beginning validation... Validated 100000 rows of 100 cells in 305.008416ms All of this seems to support the notion that parallelism in Python is harder than it is in other languages, and it seems to have nothing to do with imperative vs functional programming styles.
When you work with unicode characters , or maybe need them in your comments (happens sometimes if you work in other languages) you should add # coding: utf-8 To the top of your file. Next time try /r/learnpython or even google. That is one really common exception and a quick search would have taught you more than asming for help here, Im guessing that was your teachers intention! 
Find something you don't understand, then write a program that uses that thing.
Doesn't work on Windows: [C:/Users/roger/.vscode/extensions/donjayamanne.python-0.2.4]: Cannot read file C:/Users/roger/.vscode/extensions/donjayamanne.python-0.2.4/package.json: ENOENT: no such file or directory, open 'C:\Users\roger\.vscode\extensions\donjayamanne.python-0.2.4\package.json'
Awesome. Since I found a plugin for VIM bindings I have actually been able to use Jupyter. I also use R knitR :-) 
Well their rejection is a good reason to support it in Python then. ;-)
I feel like the binary camp at least should just start using bitstring rather than changing the core language. flags = BitArray(bin='0010 1101 1000 0011') bitstring has all sorts of advantages. First, its explicit. "Explicit is better than implicit". Second, you get a lot more functionality out of the bitArray object. Finally, no need for ugly or hard to type underscores. I'm sure there other things I'm not thinking of too. You can even wrap that annoying set of hardware flags into a class with real variable names and real settings names if you aren't lazy.
Google --&gt; Automate the boring stuff 
Isn't mathematics universal? What country just writes 4583485734987634057640574906759764905748954787 without any visual aids? You need to issue a PEP for your HUMAN language to upgrade to commas or periods or something! 
We can't let Larry Wall win!
It 'breaks' implementations of python. It's probably not a lot of work to add this PEP in, but it increases complexity ever so slightly. how many 'nice to have, doesn't break existing code' features do you add? 
This isn't particularly complex though, is it? And you never need to use it, and even if you didn't know about it, you're never going to see x = 10_512 * 7 and be baffled as to what's going on. As pointed out by others, it's also in other, even more popular languages already, which further makes it familiar.
Its a simple IDE script away to reformat the numbers into your native grouping. That said, I'm against this PEP aswell.
&gt; how many 'nice to have, doesn't break existing code' features do you add? adding underscores to the parser is beyond trivial.
Find an actual task to which you would like to apply your knowledge in Python. Anything that tickles your curiosity is good. On one side, chances are that your knowledge won't be just enough (it seldom is), and you'll improve by wanting to do your stuff well. On the other side, you're actually dealing with a task you're familiar with, so you'll have more ground in understanding how your algorithms should approach it. Also: - lots of good "aha!" moments can be found by checking stackoverflow.com. I've learned a lot by hanging around there. - you won't be able to program well unless you tackle the theoretical part of it... How to design efficient algorithms, how to write good code, and such. It's often the difference between writing something that works in theory, and writing something that will actually work in a real-world scenario. (I sincerely thought you were a troll, based on the uninformative title with the capitalized "HELP!") 
That's interesting, but I wonder if adding the separators in literals really solves your problem. You'd only benefit in that situation if you could make the str/repr of the numeric types add the separators. I'd guess this behavior would be optional/selectable since it would cause problems as a default for some applications (creating easily parsed output). In either case, if this is the push button issue, can't you just add a custom builtin? Something that when you need to read output will alter it for you? It just seems weird to justify modifying the input of primitive literals for output's sake. Especially when it's a particular subset of output scenarios.
How come Microsoft software works on Linux but not windows? :) 
Can't what?
I'm using it on Windows and Mac. It looks like the extension didn't finish installing. Try uninstalling it, cleaning up its directory, and installing it again. It takes some time first time, and the editor doesn't indicate any progress until the installation is complete, so this may have confused you.
Except it does :-)
Ah, I'll try than. There really should be some indication of what's happening though. 
http://i.imgur.com/sqHGEzq.jpg
No, from my point of view it just showed your benchmark is bad designed. I have modified my code to prove it. http://pastebin.com/ihYMCaut Test it by yourself and you will see it is quite simple. I have only two CPU units but I hope you can see that Python is distributing computation very very fine: Elapsed time: 6.233844995498657 seconds for 1 processes. Elapsed time: 3.2508060932159424 seconds for 2 processes. If you want to make such parallelism useful, you have to realize one thing. Usually, you want to parallelize something what is slow to compute. Actually, converting string to int is not. Be aware of fact that my code might be slower than yours!!! You have to add time.sleep to your code to simulate expensive computing time before or after converting. Otherwise I can deduce from your benchmark only fact that my code is several times slower than yours which might be really true. I hope I made myself clear. 
[Direct link to view it online](http://nbviewer.jupyter.org/urls/losc.ligo.org/s/events/GW150914/GW150914_tutorial.ipynb)
I can confirm that it doesn't work in Python 3. Why use 2 for a new project?
That's all I was asking. I had already searched the full paper for 'Python' as well as 'matplotlib', no occurrences. So I figured the OP meant there were telltale signs in the formatting, styles, etc. I use matplotlib but irregularly so I don't have a trained eye for spotting its output.
At first I found it confusing as well. But it really makes sense when you realise it's just two use of the same feature: being able to pause the execution of a function until something happens. For a generator you use that feature to delay some computation when it'll actually be needed (and to save time/memory) and for async stuff you use it to return control until you've received what you needed.
Python 2.7 isn't old; 2.7.11 was released 2015-12-05. (3.5.1 was released two days later on 2015-12-07, and 3.4.4 was released two weeks after that on 2015-12-21.) 2.5 would be old. Also, most (all?) distributions also package Python 3, and allow you to install it alongside Python 2. For example, Debian stable, which is not renowned for having incredibly up-to-date packages, currently offers 2.7.9 (released 2014-12-10) and 3.4.2 (released 2014-10-13).
total BS
That's not all... cell 20 can pretty much be reduced to: fn = 'L-L1_LOSC_4_V1-1126259446-32.hdf5' strain, time, chan_dict = rl.loaddata(fn, 'H1') for dq_flag in ('CBC_CAT3', 'NO_CBC_HW_INJ'): segment_list = rl.dq_channel_to_seglist(chan_dict[dq_flag]) print('Number of segments with DQflag {} = {}'.format( dq_flag, len(segment_list))) for iseg, segment in enumerate(segment_list): time_seg = time[segment] strain_ct = len(strain[segment]) print('GPS start, GPS stop and length of segment {} in this file = ' '{}, {}, {}'.format(iseg, time_seg[0], time_seg[-1], strain_ct)) But they're scientists, not software architects. They're just trying to get shit done, not write beautiful software. They can have their code repetition all they want.
Looks like this is code for *generating* large numbers. It just implements the hyperoperator sequence on native python integers. I don't see anything in here for actually representing them?
Where can we find a link to your scraping project?
Pandas does very well at reading CSV into nice structures too
Take a peek into the inspect module. Depending on your version, getargspec or getfullargspec is what you're interested in. Or if your on 3.3(? I think thats when it's added), the Signature object is very interesting.
For the prints you mean?
Yes, but 2.7 has a deliberately extended life, in order to ease the transition from Python 2.x to 3.x. Compare with 2.6: - the first stable release of 2.6 was 2008-10-02; - the last regular release of 2.6 was 2.6.6 on 2010-08-24; from that point on it only received security updates; - the latest security update was 2.6.9 on 2013-10-29. I think it is no longer receiving security updates (except through third parties). So that's active support for two years, plus another three years of security updates only. That's pretty standard for Python, except for 2.7: - The first stable release of 2.7 was on 2010-07-03, more than five years ago. If 2.7 didn't have special treatment, it would likely already be out of support, but it's going to be supported until 2020. - Back in 2010, the standard Python 3 was 3.1: now we're up to 3.5. In the time that 2.7 has continued to receive updates, Python 3 has moved forward four release numbers. So, yes, 2.7 is pretty old. Not *unusably* old. I people still using 2.4, and I know of at least one person still using version 1.5. But it is old.
Aren't they too big to be on a notebook?
Make ``default_port_list`` a triple-quoted string, space-separating the numbers. default_port_list = list(map(int, ''' 20 21 22 23 25 47 '''.split())) I see you're making use of the new optional typing. I don't think your program is that big and complicated that it's necessary. I've gotten by just fine without static type hints. As /u/tea-drinker pointed out, your use of certain globals in ``scan_one_port`` is not thread-safe. For example, ``ports_scanned += 1`` suffers from a race condition. Why bother counting inside the function? You could count in the loop over the ``concurrent.futures.as_completed``. I think there are a few places where you'll need to make a similar transition. ``'-' in ports and ',' not in ports`` is clearer than ``ports.find("-") &gt; 0 and ports.find(",") == -1``. If ports is a really long string, it might be better to make a set to avoid looping over the string twice. characters = set(ports) '-' in characters and ',' not in characters Why so many mutating ``global``s rather than returning values from functions? [pep8](https://www.python.org/dev/peps/pep-0008/) suggests 4 spaces instead of tabs.
Book from 2010 would be more than fine. Python 2.7 was released in mid-2010, i think. I would suggest her a thing to do: make functions very very tiny, as in each function should do one and only thing.
Great points, I know that I am lucky that I don't depend an old code base and the libraries I need are ported. &gt; we've reached the point where we can clearly recommend new users learn Python 3 I think that is great, without this the divide and friction would only become worse.
Care to repost on /r/DSP?
http://i.imgur.com/BDSZuaY.jpg
I am genuinely curious, why not just use nmap? Maybe I am missing the point and it was just an exercise :)
&gt; No, from my point of view it just showed your benchmark is bad designed. I have modified my code to prove it. http://pastebin.com/ihYMCaut Adding in the sleep doesn't prove that my benchmark is bad, you're just changing the ratio of time spent marshalling input data across processes to the time spent processing the data. In other words, you're making the marshalling penalty *relatively cheaper* by exaggerating the processing cost. I already agree that Python's parallelism model is fine for such tasks, but that's not the scenario we're interested in. &gt; If you want to make such parallelism useful, you have to realize one thing. Usually, you want to parallelize something what is slow to compute. Actually, converting string to int is not. Converting *a string* to *an int* is fast, but converting millions of strings to millions of ints is not. This is the task I'm benchmarking, and it's perfectly reasonable to do (as I mentioned in my article, it's inspired by a very real use case). My benchmark isn't invalid because it demonstrates something that Python isn't good at, it just means that Python isn't good at all useful parallelism tasks.
Anyone try it out? Wondering how it compares to Anaconda for SublimeText
The code looks good and the documentation looks great but for the love of God use 4 spaces instead of tabs for your code. 
Some more things not already mentioned: (mostly related to style) * PEP 8 [recommends](https://www.python.org/dev/peps/pep-0008/#maximum-line-length) a maximum line length of 79 characters. (Not that you're obliged to follow PEP 8, it's just a recommendation especially since much of the Python community follows it. 100 characters also might be acceptable in today's big monitor world—but some sort of length would be good) You can break a lot of the long lines on commas. * `pgm_version` would be better to be called `__version__`. (per [PEP 8](https://www.python.org/dev/peps/pep-0008/#version-bookkeeping) and [its somewhat of a standard](http://stackoverflow.com/a/459185)) * `pgm_date` is unnecessary when using version control and muddles up diffs. In general it's not really relevant to the program to have it as a variable either because the version number is easier to report and gives much more information (how many patches or minor versions away you are) * The comments describing flags for `max_workers` and `connect_timeout` are superfluous. Firstly they violate DRY (don't repeat yourself) and they're also not relevant in the context of source code since they only affect runtime where argparse prints the flags anyway. * For the argparse argument checking, consider using [custom types](https://docs.python.org/3/library/argparse.html#type) for the checking, * Why are you writing to a file with a hardcoded latin-1 encoding? `open()` calls `locale.getpreferredencoding()` by default, which gets a more appropriate encoding for the current platform. 
Learn how to use a Python framework like Flask or Django. I recommend learning how to build a backend for a site using Django, it's pretty easy to learn and it's really fun to build things with. 
Thank you for looking at my code. I like the idea about placing argument tests in their own function. I agree with you about the globals not being thread safe, but the code seems to work OK. How would you design it so that it is thread safe? I tried skipping both hosts and ports but it was buggy so I removed it.
I am using Sublime Text on Windows and vim on OSX. I think one of them is doing this. I hadn't noticed this and thanks for pointing it out.
I have never seen map used like that before -- kinda cool. You make a good point about race condition. I hadn't thought about placing the increment code in the `concurrent.futures.as_completed` portion of the code. I like this idea as will as the idea about replacing find(). I was using globals because I was getting inconsistent results when trying to update some of these inside of a thread, but I will reconsider this. Thanks for reviewing my code...I appreciate it. 
Just an exercise. I learned a lot about `concurrent.futures` and threading. This is also the first time I have used optional typing and really good doc strings.
But why *large* numbers?
&gt; this would seem to mitigate that. It would not. You think the hosting provider cares which particular tool you use to scan systems?
"Only mocking your own types" means that you can wrap as an option, I don't want to preclude other techniques
I've seen lots of tests decorated with 4-10 @patch decorates that make the underlying code impossible to refactor. Eg changing from urllib to requests
Never written a port scanner, but why not use asyncio over threads?
Ok 
Lots of good advice here. Some things I don't see mentioned are: * constants are by convention all uppercase; `DEFAULT_PORT_LIST` rather than `default_port_list` * since you look for port ranges in multiple places, you would do better to put that condition in a descriptively-named function, such as `is_port_range`, so that the logic is centralized (aka DRY) and comprehendible at once where invoked * the convention for version information is `__version__` * it's generally a bad idea to call `sys.exit` unless you know for certain that it's a good idea to stop the interpreter, so those lines prevent your functions from being used anywhere outside the context of invoking your script (trying to use them from my script would result in early termination of *my* script); this may not ever arise in your case, but it's a practice to avoid. It's better to return a sentinel value or raise an exception and simply let the calling code handle the situation whichever way makes sense given its context. * the naked `except` clause is bad practice since it will catch errors you never thought of, like syntax errors and control-c events; *always* `except Exception` at a minimum * it's a good idea for your script to return different exit codes for different types of errors so that callers can know what went wrong without having to parse and interpret any text output (it may even let them parse the text output more reliably) * it'd be cool to have an option to output in different formats, like JSON or YAML, in addition to CSV * always try to come up with a variable name more descriptive than `tmp` - temporary what? * lines 283-284 can simply become an `elif` clause Overall, great job. As others have noted, the biggest problem from a design view is the use of globals, which is bad in and of itself, but when combined with threads almost certainly spells disaster! Edit: typos
Ok, let's grab a small example import concurrent.futures count = 0 def inc(amount): global count for i in range(amount): count = count + 1 with concurrent.futures.ThreadPoolExecutor(100) as executor: alpha = {executor.submit(inc,1000): i for i in range(1000)} for future in concurrent.futures.as_completed(alpha): pass print(count) python3 threadfail.py 405959 Your mileage may vary, it may work sometimes but it will fail others. Let's update the inc() function. import threading write_lock = threading.Lock() def inc(amount): global count for i in range(amount): with write_lock: count = count + 1 The write lock will serialise access to the global count. But it won't perform as well because locking opposes concurrency. A better solution is to not rely on globals at all. import concurrent.futures def inc(amount): tmp_count = 0 for i in range(amount): tmp_count = tmp_count + 1 return tmp_count count = 0 with concurrent.futures.ThreadPoolExecutor(100) as executor: alpha = {executor.submit(inc,1000): i for i in range(1000)} for future in concurrent.futures.as_completed(alpha): count = count + future.result() print(count) python3 threadfail.py 1000000 Now each thread is isolated to do its work and you use the return value from each thread to build the final result. No race conditions so no errors. No locking so full speed. Similarly you'd want to put the values you are interested in as return values from the scan functions to build up your final report.
Huh?
More people are familiar with threads as async hasn't been around as long. That is, op is probably already familiar with them or found SO or reddit answers talking about threads.
My main observation is that your functions are doing too much at once, and they're sharing global data. As a result, they're too tightly coupled: you won't be able to change one aspect of the program (such as a data structure) without having to rewrite everything. One example of this: using strings in place of lists does not allow you to write Pythonic loops. You're overloading your functions as a result: scan_one_host(str, str) is actually parse_string_list_and_scan_one_host(str, str) where you should have list = parse_list(str) scan_one_host(str, list) Also, your error / exception handling isn't uniform, or even complete. You're catching exceptions and turning them into a True / False return value... but you aren't checking those return values! For example, scan_one_port() catches KeyboardInterrupt and returns False, but in main() you're putting it in a try: block and attempting to catch KeyboardInterrupt again! Don't bother with KeyboardInterrupt. Let it kill the program; use a finally: block to close sockets / clean up if you must. And don't bother with return values if you never check them! Come up with a consistent method of handling errors and use it throughout the program. If you're going to catch, catch specific errors, log them, do your cleanup, then re-raise if you want the program to barf.
Check the repository again, I added ability to use arbitrary labels for nodes and/or edges
Probably it was fun for OP to implement, and his understanding of the problem improved while doing it. In math, sometimes you just need to play.
What's the value of itemgetter? It really doesn't seem to save much over a simple lambda function.
This would probably be better in /r/learnpython
Python's sort() has a key attribute that tells it how to sort an iterable. You can use a lambda (it ain't rocket science) or just define a getkey() function that takes in a tuple and returns the 2nd element. def getkey(tuple): return tuple[1] listoftuples.sort(key=getkey)
Yeah you're right. Sorry. I didn't read the sidebar. 
I'm going to guess that maybe in more complex cases it might make sense.
I think documentation is quite understandable for lambda. [https://docs.python.org/3.1/reference/expressions.html#lambda](https://docs.python.org/3.1/reference/expressions.html#lambda)
I did read it. I feel like I get it.. but I'm not sure. It's a bit like black magic for me at the moment. Perhaps I need to practice using it more.
Well done! :D I'm trying to discover a way to make the cheat sheet graphical, but no idea how to achieve it: [/r/learnpython](https://www.reddit.com/r/learnpython/comments/43tnkn/make_cheat_sheet_style_diagrams_any_python_module/) 
This is a good example. Thanks.
Lol. If the academic institution you're studying at allows students to "buy" assignments, or even the fact that you're able to get away doing this... I'd be strongly concerned as to the quality of education and your future career, bud. "Buying assignments" is an instant trip to the Dean's office where I'm from.
For these kind of applications threads give the performance needed. There's no tangible benefit moving to asyncio.
 default_port_list = "20,21,22,23,25... Why not a list? # initialize variables active_hosts = defaultdict(list) hosts_scanned = 0 skipped_hosts = 0 skipped_ports = 0 opened_ports = 0 ports_scanned = 0 skipped_port_list = [] Globals are generally a bad idea; use functions to compose the data you need. Also it's worth considering that TcpScan should be an object representing all of these data points, or at least use a dictionary or something similar. def scan_one_port(ip: str, port:str) -&gt; bool: ... port = int(port) So really port doesn't need to be a string, and the type annotation is just what you expect in your own case as the code is currently written rather than what actually needs to be true. Anything that can be subjected to `int(foo)` would work as an argument to this function. Things around the program in general could also be made much more generic/modular to make it much easier to extend it to a UdpScan or any arbitrary type of specific scan against port numbers.
Aesthetics. I think lambdas are a bit ugly, and beautiful is better than ugly.
I avoid lambdas. But I think there's more beauty in readability than minimalism so I like to avoid lots of imports, especially from the less common parts of stdlib
I forked the code to see if I can make it better. Even though @jit can lift loops and stuff like that, not enforcing nopython mode usually leads to suboptimal performance. Still, I see you did it with most of the double for-loops, so I guess I won't be able to stretch it much more.
Better question is whether the dimwit replying to the question has a future. 
120-140 is what I use, and is still readable but long
&gt; debugging purposes. Or security scanning. Which is really what it's made for. I nmap all my devices (and my external IP) to see what's visible, so I can lock down ports if needed.
hey my bad the reason i didnt post the link is because its undocumented and i didnt want to be that guy who asks for help with undocumented code. But please feel free to take a look. Its a django project that uses the scrapy engine. [**Code**](https://github.com/cmwaura/Final_Red_Scrap). please check it out and tell me what you think. I will start working on the documentation as soon as am done with the income module. Also if you are interested i have started to construct a finance live framework that scrapes live financial data from the websites and adds them to the django ORM. I think it will be a helpful barebones framework for people who want to start a small financial company online. 
Ohh, it was such a basic thing, that's right. It creates the key 81 and then as it tries to create another key named 81 it ends up replacing the older key. Thank you, I'll try something with tuples instead. Feeling really dumb right now.
Operator is hardly uncommon.
I dont know exactly as I do not use the software I am just trying to install it through SCCM But from the testing I did, pythonw.exe brings up a GUI interface program called spyder in (anaconda3) and python.exe just brings up the command line application. BUt I could be wrong, you would best ask on a python or anaconda3 wesbite forums
I think a quick, hacky fix would be to monkey patch signals. Replace the module with a custom object that holds a reference to the original and assigns an object that loops through it's registered handlers when it receives the signal. Initially, I thought just registering the handler, but if the module as written will overwrite existing handlers, then it's not very helpful. Something like: import signals as _signals from types import ModuleType import sys class SignalHandler(object): ... class Signals(ModuleType): # public method overrides def __getattr__(self, attr): return getattr(_signals, attr) sys.modules['signals'] = Signals()
Why shouldn't I use another import if it gives me the right tool for the job? The line that counts should be more readable, not the top of the file with the imports, and that's the case with itemgetter vs a nameless function, even though it's not that dramatic here. And if the slightly improved readability is not enough, itemgetter also seems to be faster than a lambda.
&gt; Feeling really dumb right now. If only I could count the number of times I've said the same thing ;) 
You have some really good ideas here, thank you.
Thanks for this critique, I appreciate it.
Decided I would pretend this was code I was told to review for work. These are the issues I found in your code (numbers are line nums.): * 32: Line is too long * 61: Globals. Please try to find a better way of handling this. * Globals are rarely the answer you are looking for * 69: misspelt "hyphen" * 76: magic number. no big deal, it makes sense in the context, Just like pointing it out so you are aware * 108-109: Why globals again * 111: Magic number again (same as above) * 154: You don't need separators for each function * 171: So many globals * 201-203: No, we don't want more globals * 220-223: Doesn't argparse have arguments to force the type for the command line argument so you don't have to worry about casting them to the correct type? * 236: Too broad an exception clause For the most part the code looks very good and seems like a useful tool. Is there any reason the code is not put into a class? It would make the code more "pythonic" and would also stop you from having to use globals everywhere **Edit**: Cloned the code and ran flake8. Dear god, use spaces man! tabs are never going to work out for you
Through some quick google: https://github.com/googlemaps/google-maps-services-python
Thanks. Vim's settings can sometimes be frustrating.
has anyone used it?
I don't think so, mine are still working. However, I wouldn't mind if they'd removed them: 99% of the count comes from mirror bots anyway, it's a pretty useless statistic, unfortunately.
sledgehammer/magic wand
It does not open any new security holes (you're just reading bytes that you've requested online). It does not close any existing security holes that may already be open (if you've shared your whole hard drive without a password over the Internet with Windows networking, using the requests library in this way will not fix that). It does not "keep your computer safe," which is a complex process involving assessing multiple potential threats and assessing how much you want to inconvenience yourself in order to close them. However, neither does it increase your risk, except to the extent that the server at www.some-site.html (not a valid potential domain name, because [.html is not a valid TLD](http://data.iana.org/TLD/tlds-alpha-by-domain.txt)) now knows that your computer exists. Which is probably safe.
I googled too, but I'm asking for recommendations based on experience.
that idea hasn't been done already?
Thanks for the Automate the Boring Stuff recommendation. Trying to find resources to learn Python 3 with google alone is pretty frustrating. For some reason, I never think to come check reddit.
This is more of a learning tool to play around with the numbers (and the concepts) of hyperoperators. I wouldn't use it in an industrial strength application/research project, but it only takes one line of code in python to get started. Mostly though, like u/troyunrau mentions, it was fun exercise. Also see, https://xkcd.com/353/
More people should watch the video you linked to at the end. 
See that's the problem I downloaded Python 3 and I run the shell file in the folder and it still won't run python 3
looks like it's double pipe between fields? Here's an incomprehensible one-liner: fields = {x[2]: (x[3] if len(x) &gt; 3 else None) for x in (i.strip('|').split('|') for i in s.split('||'))} Then you can access it as: fields['__VIEWSTATE'] basically split on ||, then split into turn into a dict using dict-expression Working on a solution that uses the `csv` ('coz I'm bored) Edit: It's not a double-pipe between fields is it? it's just that an empty string in the 4th value is just an empty string anyway assuming that, heres one using the `grouper` recipe from itertools: def grouper(iterable, n, fillvalue=None): "Collect data into fixed-length chunks or blocks" from itertools import zip_longest # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx args = [iter(iterable)] * n return zip_longest(fillvalue=fillvalue, *args) _, *i = s.split('|') #pop off the first value fields = {k: v for _,_,k,v in grouper(i, 4)} 
PEP8 is the hobgoblin of little minds, and does not suggest using spaces instead of tabs. 
By what measure do you consider this code to look good? 
How could you possibly think this was clean code? 
No one's mentioned this yet, but this is noisy as fuck, and doesn't actually "scan" anything, from a network perspective. 
&gt; Okay so what are people talking about when they say that things can "sneak in through the connection you opened"? Depends on who's saying it and in what context. A lot of time it's meaningless standard disclaimer verbiage that they put in everything because they don't know what they're talking about. Saying this implies either that the person saying it CANNOT be specific, or else that they're talking down to you and will not. This is a poor metaphor in a lot of ways. Reading bytes over a socket is not in itself risky. If you EXECUTE CODE contained in those bytes, or pass them (intentionally or inadvertently) to something else that can do so, then you may have a problem. But just reading a website with the Python code you posted is not dangerous, no, because all your code is doing is transferring the bytes. It's not even doing anything with them. This is Python; it is not magic. Reading bytes over a socket cannot magically accomplish actions. You have to interpret the bytes in some way for that to happen. &gt; And what is the Python site warning me of when it says: When opening HTTPS URLs, it does not attempt to validate the server certificate. Use at your own risk! The documentation is warning you that it does not attempt to verify that the site you are connecting to is the site that you think you're getting; there is no attempt to protect you against man-in-the-middle attacks or other forgeries. Again, though, with the code you posted, the worst thing that would happen is that you might not get the "real" webpage you think you're getting. You should be cautious about INTERACTING with that webpage, but JUST READING SOME BYTES will not magically infect your computer. It would be dangerous if you were to, say, TRY TO LOG INTO THAT WEB PAGE without verifying its identity, but no, just downloading it and not executing it in any way is not in itself dangerous.
Note that this works not just for YouTube, but pretty much any video anywhere, including many TV websites. Also, youtube-dl will download full playlists if you hand it a playlist url. 
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. Cheers &amp; best of luck with Python!
Hey, /r/Python! Please check out my project, and feel free to post any feedback you might have. It's my first attempt at making anything I actually think other people might use, so I'm sure there are improvements to be made. The title pretty much says it all, it's a library for making menu-based GUIs on the console using curses. It's designed to be quick and easy to use, as well as decently extensible. I also used developing it as a way to learn about a bunch of tools for testing, documentation, packaging, etc.
First of all your variable "Hydrogen = H = 1" is wrong. If you want to use both you need to make them separate. H = 1 Hydrogen = H Same for gold. This is not what you want. I'd suggest [Dictionaries.](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) First you need to decide between the name and the symbol. Will it be. periodic = {'H': '1'} or periodic = {'Hydrogen: '1'} This way you can have conditions for you function. Maybe use [IF](https://docs.python.org/2/tutorial/controlflow.html#more-control-flow-tools)? periodic = {'H': '1', 'Au': '196.96'} # Dictionary for the elements and atomic weight elem = raw_input('Which element you have been given? ') # Asks for the elements grams = float(raw_input('How many grams you have? ')) # Asks how many grams of the previous element if elem in periodic: # Is the given element in the 'periodic' dictionary? If it is, continue. mol = float(grams/float(periodic[elem])) print 'There are {} mol in {} grams of {}'.format(mol, grams, elem) else: # If not in the dictionary print error. print 'The element {} is not in the periodic table'.format(elem) That's how I'd go about it. Might not be pythonic but I'm still learning here too.
you probably want to post that here https://www.reddit.com/r/Python/comments/3kestk/post_learning_questions_to_rlearnpython/ A simple dictionary mapping symbols to elements would work elements = {"AU":"gold", "H":"hydrogen"} while True: symbol = input("What element are you given? (enter a symbol)") elem = elements.get(symbol) if not elem: print('not a valid element try again') else: break print(elem) 
you could try something like this: # a dict where you have your elements (lowercase) # referenced by molar mass. # feel free to update! molar_mass = {196.6: ['gold', 'au'], 1: ['hydrogen', 'h']} # give us a string with user input. element = input('wich element are you given?') grams = input('How many grams are you given?') # convert in-place user inputs to something usefull, element = element.lower() grams = float(grams) # keep ref of everything for later # wile we reading the dict, # key/value by key/value: for key, value in molar_mass.items(): # and we search a match with user input: if element in value: key = float(key) number_of_mols = grams / key print('there are', number_of_mols, 'mol in', grams, 'grams of', element) I split the code on purpose to explain the steps, while skipping all checks, so it would crash with dumb user input. this part is up to you :)
&gt;There's a million obsolete C# and Java and Ruby answers on StackOverflow, too. Pre-LINQ answers on the web don't seem to be holding C# back. Except the fact that they still should work in latest version of any given language. So a copy paste programmer usually doesn't worry if it's an obsolete answer which is untrue for Python. 
Why all these threads? You shouldn't put in docstrings that have no information in them "This class overrides this method" is a good example. The readme is incorrect. If you call the show method on the menu, it already calls join(), so doing what the readme does will call join() twice. The API seems very object oriented to the point of being a pain to use. A much nicer API for a menu would be something like: my_menu = Menu( title='Title', subtitle='Subtitle', items=[ MenuItem(title='Menu Item'), MenuItem(title='Call a python function', action=lambda: input('Enter an input')), MenuItem(title='Run a console command', action=lambda: subprocess.call("touch hello.txt", shell=True)), MenuItem(title='Submenu item', action=lambda: Menu(items=[ MenuItem(title='item1'), MenuItem(title='item2'), MenuItem(title='item3'), ]).show()), MenuItem.exit(), # I guess.. not really sure what "exit" is supposed to do, and it's not all obvious from the code what it actually does ]) my_menu.show() This is equivalent to your readme example, more powerful of an API, simpler to use AND simpler to implement!
&gt; First of all your variable "Hydrogen = H = 1" is wrong. If you want to use both you need to make them separate. &gt; &gt; H = 1 &gt; Hydrogen = H No, the two versions are effectively the same. They would be different if we were talking about lists, or other mutable objects, but for ints, strings, floats and similar, it makes no difference which version you use. 
jesus christ man if you think posting a slanted cell phone pic of **CODE** is sensible, please stop trying to program.
Well... TIL.
Also, a lot of the build systems out there on development machines probably falsify those numbers by a great deal.
You could use an onClick() event for the image which will direct you to a view defined in Django when triggered.
Is it not PEP8? I forget the source then.
As far as creating your program I think a dictionary would be the best thing to use. However Instead of manually typing in an entry for each element in the periodic table I'd suggest either using the periodictable package from pypi or reading your entries from an easily downloadable file (like the one linked below) and storing them in the dictionary via a loop. Link to file: http://pastebin.com/raw/CKwm136x 
hi yea i did start to over complicate it but that's why i've had to reword my question. would i put this code in the __init__.py file of my flask app root?
I was very excited by this, but the documentation is really not structured the best. I noticed that the flask-orator docs are completely out of date and the examples given are wrong. 
This is really important, I thought we were breaking ground on new syntax.
ah, I got this: "AttributeError: 'module' object has no attribute 'sentiment'"
No problem. I generally find that when I start patching things -- especially things I didn't write -- then I've done something wrong. Usually, I can add a level of indirection between my code and whatever is causing me pain and that makes everything better -- my code is higher level, I can test specific failure cases easier, and I can still run integration tests to verify the actual behavior. I'll also recommend checking out this talk too: [Justin Searls -- How to stop hating your test suite](https://www.youtube.com/watch?v=VD51AkG8EZw)
The distinction between ``MenuItem`` and ``SubMenuItem`` have yet shown me, that the API needs more work and the author needs to invest more thinking about the domain design. Your approach looks much nicer. I would personally try to avoid the manuel object construction at all and provide nice factory functions.
I've had this bug in NLTK. The only way I have fixed it is by downloading the package from github (https://github.com/cjhutto/vaderSentiment) and putting the files straight into my working directory (I installed it with pip but still got the same error except if I opened the python interpreter from the directory the sentiment module is found in). 
Flask is a option but have you tried http://bottlepy.org/?
Thanks. I have tried that, but it still doesn't work. Whereas the IE tabs do work. And when I execute the .exe it just works... :/ Any more ideas what is going on?
The sentiment package was added in NLTK 3.1. You may be using an older version. 
If you think flask is not the right tool (which I disagree on) then consider looking into RPC. There is a python library and js library to go with it that will let you call functions from your browser when clicking on a link.
* `if seq == '' or seq == []` is equivalent to `if not seq`, but that's just a style thing. It also handles the case when `seq` is `None`. * You're returning "rest" in the else clause without increasing it or checking if it is None. * `if seq != elem` should always be true, since the [1:] syntax always returns a list, even if there is only one element left.
First you should remove all options where you return None. Empty sequence or empty string evaluates to False and you can use it. Last option is if elem is not in seq. After that two options remain. First index of sequence is elem and then you return 0. Or it is not and then you call it recursively starting from index 1 and so you return 1 + return value. And that's it. Everything else is solved already, you don't need those other ifs.
am not sure how my comment will best help you but on the python idle type &gt;&gt;&gt; nltk.download() A gui interface will pop up and check if you have the module installed/downloaded.
Here's a different way (not using recursion): def index(elem, seq): return next((i for i, a in enumerate(seq) if a == elem), None) assert index(5, [4, 10, 5, 3, 7, 5]) == 2 assert index(1, [4, 10, 5, 3, 7, 5]) is None 
very good! very good!!!! Looks super clean!
He probably has to use recursion. Otherwise he wouldn't do that. :0) I hope. :0D
haha I was thinking the same thing. Using recursion and making many slices of a list doesn't seem to be the best way to approach this simple problem. I assumed it was a homework question to practice recursion but just wanted to show a different way. 
This is not so much a python thing as it is a learning to program thing. Everyone either goes through this or they die as a code monkey. I
yeah, whats interesting also is that i have now chosen to read the documentation of the modules i am using over going through the tutorials offered by blogs/youtube.
Nowadays it the only reason to use Python 2 is because you have legacy code(or old library, which is also legacy tho).
Pretty sure the last survey in this subreddit still showed a strong following for Python 2, even though, by the general sentiment around here, you'd think the survey would already be heavily, heavily biased towards 3. I still use 2 and have no intention of switching to 3. To be fair, most "projects" I start are so small they are likely to be 3-compatible simply due to the lack of any `print` statements. All the Python code I work with at work is 2. It's something like 50 million LOC and we're still waiting on good support for it in production before actually migrating significant projects.
https://caremad.io/2015/04/a-year-of-pypi-downloads/
I don't have any recent stats, the old [2.x to 3.x surveys](http://i-programmer.info/news/98-languages/8269-python-2-versus-python-3-revisited.html) are well out of date. In my own experience the only reason anyone uses python 2 in my research group is because of enthought canopy (which may by now actually have py3 support? but didn't 12-ish months ago. idk, I don't use it myself). It seems the common excuse is that there are still dependencies holding people back, which isn't really justified now IMO. Also, the "no incentive to upgrade" excuse is weird to me. If there's really no difference, why not just start using the platform that will have a longer shelf life? [Another oldish blog post](http://nothingbutsnark.svbtle.com/looking-out-python-usage-in-the-astronomy-community) 
Hey OP, practicing recursion is a GREAT opportunity to learn how to use the debugger. I'd highly recommend using the debugger to step through a few examples even if you get your code working. It will be an extremely important tool in your toolbox later on.
I use more of the numerical/scientific computing side of the Python community and I have a project that I want to serve up as a web app. This is perfect for me since the project contains heavy use of pandas. Thanks for this!
Here's the breakdown by which Pythons were used to download from PyPI in the last two weeks: 2.7 85.90% 2.6 6.66% 3.4 4.64% 3.5 2.09% 3.3 0.56% 3.2 0.12% (Two weeks was ~133 million downloads)
What happens when multiple loaders define the same setting? I'm thinking in the context where your application has a configuration file but allows certain settings to be overridden by command line flags, but maybe that's outside the scope of your library.
MenuItem is simply a base class for all other item types. I might could have called it BaseItem, or something similar, since for many other classes the first word of the class name describes what type of data they contain.
Yes, you need to include option that it is not in the list at all. My first if was then: if not seq or elem not in seq: return None That second part **elem not in seq** makes sure that you don't try to find index if it isn't there.
"I held backspace" - Press ctrl-u, dude! or shift-home, del. He ironically mentions `readline` keys a few slides later.
As a beginner I am still unsure if I should use 3 to start new projects. It is confusing.. 
I find I have the problem he mentions later on in the post - sometimes I spend way too much time playing with the tools. I've had loads of fun tinkering around in Vim and playing with plugins, but sometimes I've found myself messing with it just as a form of procrastination when I really should just be working!
Management, upgrade your shit. 
There's a lot of companies with Python 2.7 code in production. It costs time and money to get it all working in Python 3, and there will be bugs. I've seen it - I get paid to write python at a big company.
A lot of groups are carrying around 15 years of Python 2 code (well, technically code older than 2001 or so would be Python 1.5 code, heh, but that was a much smoother upgrade). Even if only, say, 120 lines of code a day was written each weekday within that group for each of those 15 years, that's almost 500K-lines of code. It's just going to take some time for each group to make sure all that code works on a new platform, for better or worse. I get the sense that a lot of folks claim this as an excuse just aren't in the same boat. They don't have much, if any, software to drag along for the ride, and maybe their imagination can't take them to a place where someone else does.
I really wanted to learn on 3, but got hired by a company that uses 2.7, so I guess I'm going with that. 
what do you have to use xrange for?
Well said! It's the same feeling I get every time I come to /r/python. Most of the vocal supporters of Python 3 seem to have very little grasp of what real life sw development means. By real life I mean developing code that sells and pays for your bills, not "hello world" toy projects. When you're into the former, you almost inevitably end up with tons of legacy code that can't go away overnight or at zero cost. When Python 3 was deployed, they decided to simply ignore this reality, probably hoping it would somehow fix itself over the years. But it's not happening (especially given that Python 3 offers very little in exchange for the trouble). And it's not that people are afraid of moving on: if you look at the number of Python 2.6 downloads, you'll see it represents a tiny 6% these days, meaning that developers and businesses are happy to move to the next release if this entails little hassle. 
Well it works, by necessity. But why do you insist on clinging on to an old API, and holding everyone else back? I basically have no sympathy for python 2 users. Upgrading to python 3 is mostly trivial and people who haven't done it by now can accurately be described as lazy.
On a side note, any idea when Django REST Swagger will support the 2.0 spec?
https://youtu.be/OSGv2VnC0go?t=3m50s 
Python 2's range creates the entire list object to be iterated through. xrange creates a generator which just gives you the next value in your list when you ask for it, without creating the whole list up front. xrange in Python 2 therefore uses much less memory since it doesn't need to store the whole list somewhere. There is basically no circumstance under which you would want to use range over xrange. Because of that, Python 3 got rid of the original range implementation, and uses the generator implementation for its range method. Python 3 range is essentially Python 2 xrange. This is just one example of many improvements that Python 3 has over Python 2. There is honestly no valid reason to still be using Python 2 in 2016. If you are, you are wrong. 
That and for many people 2.7 does everything they want or need Python to do, so why dick around getting the latest and greatest version to work?
To be fair, I use ansible, and I think its great. However, I only use it as a command line tool, I never import it in my python scripts. So my opinion is that they should get off their arses and upgrade, but actually it won't make any difference to me. Don't know about fabric. Google says it's an ssh tool. So probably the same response as above.
It's pretty easy to write 2.7 and 3 compatible code. You should try to do that. http://python-future.org/compatible_idioms.html
&gt; probably hoping it would somehow fix itself over the years. To some extent it has - the transition from Python 2.7 to Python 3.5 is smoother than to any other 3.x.
They're on Python 3 [now](https://github.com/twisted/twisted). It's pretty inexcusable to take this long about it, though.
Oh wow, their [roadmap](https://twistedmatrix.com/trac/milestone/Python-3.x) is so much further along than it was. Still, I wouldn't say they're entirely there yet - But this is certainly encouraging!
Why don't you just use 3 then? You are literally part of the problem here. We need to make it nigh on unacceptable to still be using Python 2 in 2016.
Such a dumb reason.
It doesn't have to be latest and greatest, it just has to be Py 3. Py2 dies soon. Laziness may be a virtue in programming, but inability to plan ahead more than a few seasons is not.
No source of value.
Thanks so much infinull! I don't understand some of the syntax towards the bottom to be honest, so it is going to take me a little bit of time to try your solution, but you honed in exactly on my need (to access values from key names rather than index number, which seems likely to be more reliable in the event of a subtle change in the site's formatting), and introduced me to itertools, which sounds like a very efficient library to use for this kind of task that will likely improve the performance of the program long run. I'll try it out tonight and tomorrow and let you know how it goes.
A lot can happen in four years - 2.x support might get extended if the 3.x uptake stays poor.
I think you just want to remove all the \x00's, so: &gt; \&gt;\&gt;\&gt; b[1::2] &gt; "Statistics aren't everything"
It's not like 2.x will die as soon as support ends. For professional programmers I agree, using Python 3 is obviously the most "future safe", but most users aren't professional programmers, they just want it to do something. That's why you still find companies using programs written in Fortran: from their perspective it's not broken, so why fix it? My prediction is that 5 years from now a majority of Python users will still be 2.x, regardless of whether or not it's officially supported.
&gt; Why don't you just use 3 then? Because there is no advantage, and there is a huge disadvantage for my uses (no Twisted). &gt;You are literally part of the problem here. As I said, projects I start anew are small and (probably) already 3-compatible (except for the Twisted-based projects) so there is no point in changing my behavior in this respect at all. Whether I use the python2 or python3 binary on my system for code that would run on either is completely irrelevant to the rest of the world. If I write 3-compatible code and _tell_ people I'm using 3, why does it matter if I'm actually using 2? (I don't tell people I'm using 3, because I'm not.) As far as work goes...50 million LOC. I rarely work with any of it, actually, so the 2 to 3 migration doesn't actually impact me here.
&gt; would i put this code in the init.py file of my flask app root? No. You could but that is not the proper convention. That file should either be empty, or contain import statements used to clean up module imports. but usually its empty. This should go in another file, something called `main.py` is very common.
I'm pretty sure Guido said that he's absolutely not extending the EOL again. I feel like the initial sunset date has already passed, though I might be wrong on that.
Use python 3 if you want to hang with python 3 enthusiasts. If you want to get work done and maybe a job out of it, use python 2.
There are so many people behind 2.x that it doesn't matter what Guido wants or the CPython devs want. Someone will keep maintaining it.
Some companies have great *solutions* like Dropbox, the company where the creator of Python works, Guido van Rossum, they're just making their own Python 2, called Pyston! Great. Just what we needed. /s
Mostly because that's the style of the rest of the TA codebase, but also because I was a C# dev before this :)
You must not actually code at a real company. 
One example, which, like always, is completely trivial and not remotely close to justifying the cost of upgrading. 
IMHO if you are trying to learn a language, and you're getting tripped up, just power through it. Eventually after enough exposure, things will start to click. Once you get to that point, it's also going to feel very rewarding. There's a reason why not everyone is a programmer. It's inherently difficult to grasp when you're first getting started. You have to really want to learn. Using a IDE with code completion is a great tool when you have a grasp of the language, but it can be a terrible crutch if you're using it to learn the language. You'll quickly move to blaming your magical IDE whenever your code doesn't work instead of yourself, inhibiting what you spend your time on and ultimately learn. There aren't any shortcuts in learning a language, and there are merits with learning a language the hard way. You picked a great language to learn with, and you will eventually pick up it. Just stick in there!
To be honest I find this to be one of the biggest problems I've ran into with Python is that the C compiler requirements are such a pain in the ass to maintain 
Why couldn't a temporary file or try-finally clauses work for another tool ?
Working in a young startup, I'm happy to have all of my code in Python 3.5. The recent features and the attention to details related to asynchronous programming are both amazing.
To be fair, there must be thousands of articles, blog posts, reddit threads, etc. about 2 vs. 3. At this point, if you *still* have to ask and don't have a really specific question, you're just being lazy.
Could you please post some code? I don't have any idea what you're talking about, other than that you're using the kmeans class from sklearn. Ideally a minimal example that demonstrates the error.
[Post learning questions to /r/LearnPython ](https://www.reddit.com/r/Python/comments/3kestk/post_learning_questions_to_rlearnpython/)
Yeah, it's stupid on Windows, even if Visual Studio is free. People love Anaconda for providing those mainstream modules precompiled. Wheels help a little, but could be much better. On Linux it's usually fine besides sometimes needing to install the *-devel packages from the distro's package manager.
How many times do you think a mirror needs to download a package?
It has a lot to do with packages that still haven't been ported to Python 3.x. In my company, we use Twisted and Tornado and gevent. We're tied to Python 2.7. However in my personal projects where I can avoid 2.7, I do it. Not every project needs Twisted.
Escaping characters outside a certain range is how you convert binary data to a readable string. The escaped characters aren't printable, so Python represents them by escaping their hexadecimal value with \x. If you simply remove the escaped characters, you'll lose a lot of data. Instead of simply printing the contents of the mp3 file, you should be looking for a library that knows how to interpret mp3 data correctly, or look up the specification for the mp3 file format and write your own. EDIT: it also seems that you're forgetting that sequences in programming are typically zero indexed. So when you do b[1], you're actually getting the *second* item/character in b.
&gt; Because there is no advantage, That is far from the case, but if you don't actually use it, you'll never find that out for yourself. The simple fact that Python 3.4+ executes the same code measurably faster than 2.7 with no changes (assuming it works on both) should be reason enough. &gt; and there is a huge disadvantage for my uses (no Twisted). Other than the fact that [twisted actually does support Python 3](https://github.com/twisted/twisted), maybe take a look at [asyncio](https://docs.python.org/3/library/asyncio.html) in Python 3.4+, then. It's much better than Twisted was the last time I used it.
Hi there. You have posted a learning question to /r/python. These types of questions are far more suited to /r/learnpython, where users are actively interested in helping people to learn. Please resubmit it over there! Make sure to read their sidebar rules there before posting, notably this one: "Posting homework assignments is not prohibited if you show that you tried to solve it yourself." If your question is about homework, show them that you've tried to solve your problem in your post and you should get all the help you need. For anything else, the reason you are seeing this message is still that you will likely get a better answer there! Warm Regards, and best of luck with Python!
Fortran is still getting updates (I.e. newer versions). Fortran 2008 (the standard was accepted in 2010) is the latest stable. It also looks a lot different than say old Fortran 77. The point is, some will move to newer versions, some will stay with super old versions. Business decisions vs technical reality is an interesting battle to watch unfold.
And here's some dummy code: paragraphs = [[[0.5, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0]], [[0.7, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], [[0.45454545454545453, 0.45454545454545453, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]] avg = 0 for x in range (0,3,1): model = cluster.KMeans(n_clusters=cluster_amount, init='k-means++', n_init=10, max_iter=30, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1) model.fit(paragraphs) avg += model.inertia_ avg = avg/3
How can I get the statistics data by myself?
It's not possible at the moment, we're hoping in the future to a) expose a lot of the data people commonly want on PyPI, b) make the whole dataset public.
That's actually the main reason I dualboot is that writing Python on linux is 20 times easier 
Yes. /r/learnpython
Really? Packages are overwhelmingly downloaded using `pip`.
Which means I can start using it in production when RHEL 8.1 comes out (because no-one runs RHEL X.0 in prod, right?) which right now, if you follow the schedule for RHEL 5/6/7, should be early 2019.
 if not allow_nd and array.ndim &gt;= 3: raise ValueError("Found array with dim %d. %s expected &lt;= 2." % (array.ndim, estimator_name)) That's where the crash is. I forgot how annoying setting up sklearn is, I haven't done it on this system. Running into errors. In the example you gave me the paragraphs object is a List of LIst of List of floats. Is it supposed to be? It looks as if you're passing something with the wrong dimensionality, any array with &gt; 2 dimensions will trigger this crash, as the code is called every time fit is called. It looks like they're your X, Y, and Z, coordinates but perhaps KMeans only works on X and Y? 
Thank you. Then how did you get the data? Were you able to get the raw access log data of a PyPI server because you are a member of the PyPI maintainer?
If you're a new Mac user, do you opt for a a greyscale Mac Classic or a MacBook? If you buy a Windows laptop, do you ask them to put WIndows 3.1 on it? That's the answer.
Wow I'm shocked by the downvotes :( What is confusing about this? It's clear to me from the question that this guy isn't sure what he's even doing. He said he has apache2 running with a page and an image on it, he wants clicking the image to display something. I was trying to clarify how he's displaying the image (if he is at all) and then from there we could help him execute some script when clicking that image.
I think this figure is more useful: http://ianozsvald.com/2013/04/15/more-python-3-3-downloads-than-python-2-7-for-past-3-months/
I found myself playing with my prompt for twenty or thirty minutes the other day. It's neat, shows my cwd, active branch, active venv and active Docker machine. Not sure what value it brings but its petty to look at. And I spent at least the last two hours of Friday trying to get lxml installed inside of a tox venv so I could delete a nasty shell script. Gave up for now, but I know I missed something since it installs fine in the container.
Yup. You can define functions in the interpreter itself, or you can put them in a file and import that into the interpreter. Either way, remember to indent consistently within the function definition.
I wonder if Jenkins builds use a certain user string that could be filtered/categoried.
... Boo, I confused PyPI with PyPy. I blame your flair :P
Given that AppEngine *still* doesn't support Python 3, I wouldn't be surprised if Google ~~flushed a bunch of money down the toilet~~ poured a tonne of money into Python 2 after the official EOL.
Also, the load balancing layer of scales adds a lot to scalability. No longer are you doing rpcs to once single server, but now a set of many. If one goes down you can still make request to the others.
I don't know that PyPI is a accurate indicator of usage at this point. In my org we almost exclusively use Anaconda, and very rarely use pip. In fact I'd even say we use Christoph's packages more often than pip installs that'd hit PyPI. We default to 3, but use 2 where required. I suspect a lot of people who'd be using 3+ would likely be in the same boat, whereas people who are using 2.7 still are also likely still using pip.
May be this is because python3 standard library is bigger than 2 and less modules are required to be downloaded from pypi?
That means you are an insider of PyPI right? The statistics is a total of downloads so it doesn't take into consideration the uniqueness of downloaders or its attributes (ex. human or machine) right? Sorry for my persistent question, but I just want to know accurate matters.
When it comes to Python, I'm fairly sure that most applications that are written by professional developers run on Linux. I don't know if you've heard, but Linux is pretty popular in the enterprise right now. Red Hat linux has everything to do with Python 2 becoming an immovable object. For a lot of admins and engineers, Python is the language of choice for automation and scripting. Just think of all the great features of all the big projects like Twisted and Django that weren't built because of the countless hours wasted on integrating their codebase with zero-sum-improvment Python 3. In a general sense, software developers and system admins are the customers of the PSF. The PSF decided to change something their customers really, really liked, into something that the customers didn't. For an analogy, think Windows 7 to Windows 8. The company was convinced it knew what the customers wanted better than the customers did. Just like Windows 8, there is near-zero adoption. But everyone here acts like the customer is wrong. The PSF took some risks with Python 3. So far, they haven't paid off. It's not my job to help justify the PSF's decisions. I don't want to spend a non-trivial amount of time fixing perfectly good code so it can run on an add-nothing interpreter. It's just not good business.
Of course! Glad to be helpful.
&gt; I'm sure Pandas to_sql has a way to indicate the primary key... nope. Pandas *to_sql* method does have an *index_label* parameter: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html What version of Pandas are using?
Thank you so much for your help, the problem was that the list was one level too nested!
I installed a editor PEP8 plugin a while back (I am using Atom), which helped me a lot in writing PEP8 conform code (just to get rid of the red warning messages the plugin would show at the bottom) -- after a while, PEP8 became second nature and I barely see one of this warnings popping up anymore ;)
Alternatively, ctrl-backspace to delect in blocks.
So, imagine a company A that uses puppet/chef to install python2 and then pip install a bunch of packages. Also, imagine company B that uses docker to install python3 and related packages once. Now lets say that both companies spin up 100 machines to use this code. This will show up as a py2:py3 ratio that heavily favors py2. In reality, most companies I know use py3. The exception is some old school startups like DropBox, PayPal (both are 10+ years old), or the ones using GAE (Snapchat). I would be really interested to see how many people downloaded python:2 on dockerhub vs python:3. I would not be surprised if python:3 and python:3-slim won by a large margin. Not saying that the dockerhub counts will give complete picture (it doesn't). My point is that PyPi downloads might be really biased towards python2.
What stops you from getting work done with python3?
Well, watch this [talk](http://www.youtube.com/watch?v=JOx9enktnUM) . Long story short - Dropbox rewrote themselves from Python to Go. And the __main__ reason for doing this was not the performance issues. 
Another example of 2.* holding us back. You should poke him :)
I think IPv6 and Python 3 has a lot in common. It actually shows that it doesn't matter that you provide all different means to make migration easier, it doesn't matter that you leave plenty of time to migrate. People won't start to migrate until they really have to. IPv6 started getting adopted faster by ISPs after RIRs stopped providing IPv4 classes. Similarly people started migrating to Python 3 after Python foundation stopped adding new features to 2.x in 2015.
More likely they wound deprecate python altogether and make people switch to go.
That one's skewed in the other direction, because people with already working installations don't tend to download the base package.
Do you have anything to suggest that people aren't using Python 2 in Docker beyond "people that like new stuff, like using *all* new stuff"?
I don't get that factory comment. A constructor IS a factory for all reasonable definitions of "factory". Can you show how the API would be different with factory functions?
I was recommended 3 when I started a few years ago, it made sense to me, that's the way it was heading, I had zero legacy code, but I had so many compatibility issues with libraries and installed versions that in the end I switched to 2.6, 2.7. The problems went away instantly. I think the biggest part of the problem is that there has never really been a compelling reason to use 3. Sure the language has been cleaned up a little but it doesn't give you the big uplift in functionality that you would need in order to justify refactoring all your existing code.
Twisted and Tornado and gevent all support Python 3 now. It seems Tornado supports 3.3, but the others support 3.5. Check their pages on PyPI.
&gt; Anaconda I'm not familiar with Anaconda. How is it different from Pip? I find the latter very convenient to use form a bash shell (terminal on mac, or Git app in Windows).
Define explicitly closed... a SIGTERM means for me explicitly closing the application. I guess the problem is that it is a console application running in the background, a daemon of sorts. 
Hi, would it be possible to use your platform for an educational project we are starting? Contact me if this is possible (we do not intend it to be free, of course). Thanx
&gt; With Python 3 you have less library support Really? 
At least you aren't stuck with Cobol! 
Now print is a function and not a statement: https://docs.python.org/3/whatsnew/3.0.html#print-is-a-function
They changed it so that print behaves like every other function where you put argument in brackets, instead of being a keyword. It makes the language more coherent. 
Get your shit together, and put in a bag, Google. 
&gt; Python 2 accepted both : It didn't really. Try printing two things and you'll see that the parentheses are forming a tuple, and not being interpreted as a function call. &gt;&gt;&gt; print('foo', 'bar') ('foo', 'bar') That's very rarely what you want. It just happens to work by luck if you only have a single item because in that case the parentheses are just redundant in the same way as the ones in `(4) + 2`. `print` was a statement in 2.x, but it's a function in 3.x. There are a number of very good reasons for this. - You can't use keyword arguments if it's a statement. In 3.x you can easily customize behavior with things like: print(a, b, c, sep=',', end='|') print('foo', flush=True) Under 2.x you have very little flexibility, and the language had to implement a really inconsistent set of syntax rules to account for customization. For example to print to a file handle other than stdout you have to use this bizarre syntax: print &gt;&gt;sys.stderr, 'foo' You never see anything else like that anywhere in Python. Or if you wanted to skip printing the line terminator, you had to put a trailing comma print a, b, c, Again, that just looks wrong. When `print` is a function, all this garbage goes away and you use standard function calling syntax that works like the rest of the language: print('foo', file=sys.stderr) print(a, b, c, end='') - You can't use argument splatting with a statement. In 3.x if you have a list of items that you want to print with a separator, you can do this: &gt;&gt;&gt; items = ['foo', 'bar', 'baz'] &gt;&gt;&gt; print(*items, sep='+') foo+bar+baz Again, this is just like the previous point — there's no reason that `print` should be special. These are capabilities that other functions have already had forever, so why shouldn't they apply to `print` as well. - You can't override a statement. If you want to change the behavior of `print`, you can do that when it's a function but not when it's a statement. For example, if you want to automatically flush the stream after every output, you can do something like: print=functools.partial(print, flush=True) Now every call to `print(...)` acts like you wrote `print(..., flush=True)`. Or maybe you want to turn every call to `print()` into a call to a logging function. The possibilities are endless, but you can't do any of this if `print` is a statement, because then it has its own special rules that are different than everything else in the language, and that's just absurd. 
He starts talking about the reasons why they switched around 2:45
Could you please put your code in a proper format? Use four spaces: if(formatted and correct): perform_action() This code, from what I see, is not very pythonic, you should use dicts, string formatting and make it a little bit more friendly for future extending. 
Python 2.0 is open source. If *Guido et al* will stop supporting it in four years (which I doubt), there will be someone stepping in.
Python 2.0 is open source. Anyone can support it basically forever.
Check the indentation of your 'if option is "0":'. It's currently nested inside the 8 block. You also can't do print('a')('b') like that. PS, you should post these on /r/learnpython 
Here's an important milestone: all versions of Python 3 combined have surpassed Python 2.6 usage.
There were a couple of libraries I needed on my last project that didn't support Python 3. MySQLPython only supported 2.7 for example (and also only worked with 32-bit Python).
&gt; My use of threads serves two purposes. &gt; 1: [async usage] Seems like that should be the responsibility of the user of the lib. Every API that could possibly be used parallel to something else shouldn't force threading into it. &gt; 2: They make the menu class way simpler to write tests for, especially when dealing with user interaction methods. I have never heard someone say it's easier to test a multithreaded program. That seems outlandish to me. Can you explain more? &gt; I don't think simply having a generic item class and passing it a lambda would work. There are different requirements for configuring the terminal window depending on whether it is going to be showing content in curses, or on the terminal. Hmm.. can you be more specific? Doesn't seem like a big deal to me. You can simply have a "action_blocking" and "action_async" (if that's what you're hinting at, I'm guessing...) &gt; In addition, I typically lean towards preferring the extra semantic information that having the different classes provides. I used to but now I've changed my mind strongly because mostly you have one-offs and then you have to declare an entire class, resulting in like 4 lines that should be 1 and the functionality is spread out for no good reason. Think of how your readme example would look if the user had use cases that were 100% new, i.e your example would have to include the class definitions for FunctionItem, CommandItem, SubmenuItem, etc. It would not look nice! That being said, you can of course create helper functions for the common cases, like the Menu.exit() static method I suggested in my API example. You can also add Menu.submen(), Menu.system_command(), etc if they are used often enough to make the extra lambda seem excessive. We use this API style in tri.table, tri.form and tri.query which we're building at work and it's pretty damn awesome: https://github.com/TriOptima/tri.table
Loaders are loader in defined order, so each loader override vars, unless they define in a different namespace.
The snippet means that the range keyword has the same (new) behaviour on both versions. 
For library writers, 3.3 should be the target. Applications can support whatever version is widely deployed on platforms they target.
&gt; People writing programs that only need to run on Python 3 won't bother with that. I have `six` as a transient dependency. I have never installed it; it is there only because one of the packages I use (no idea which one) has installed it.
I hadn't come across id3parse, thanks for the pointer. It works great. And of course you're right, I'll also read up on the ID3 spec. Thanks for the help.
Can you pull the Python versions specifically for `requests` module?
&gt; In reality, most companies I know use py3. "In reality, most of the people I know use English, so stats about Chinese being the most popular language on Earth must be false."
&gt; Interesting coincidence Not at all. What is interesting about it other than they share a 7?
The overall theme of your reply appears to be "it could be much worse!" Which I completely agree with, and that's why I'm still happily using Python. But my central point was that it could be much *better*. The languages people are currently weighing up against Python are things like Go, NodeJS or R, depending on the field. Being easier than writing C code for different architectures is not much of a selling point, because most modern languages are easier than that. I'm not trying to exaggerate the issue - it's absolutely possible to support both versions, and most major packages now do. But we do ourselves a disservice if we pretend it's a trivial problem. Writing Python code is less fun than it could be, and judging by [this new thread](https://www.reddit.com/r/Python/comments/45sm94/what_are_the_most_recent_python_3_vs_python_2/) there's a long way still to go.
Why do you need it in the init.py? 
except you have no proof that python2 users use chef and python3 users use docker. The rest is total anecdotal evidence which can be countered by other anectodal evidence: I'm in a startup and we are using python2
Library support is objectively worse in python 3. One of the big reasons to use Python 2 is awesome library support. Choosing Python 3 adds project risk/time as you will likely be writing more of the code yourself. 
I believe the hashing exploit was fixed in _all_ python versions down to 2.6 
Glad I could help! Also check out "from __future__ import" for modern Unicode strings, print function, division, and more. (and check that snippet before running, it might need sys.version_info or something) 
Uhm, I am sorry if I sounded like I'm actually going to use your snippet. I just pointed out that using your snippet would involve a non-trivial amount of work. At the same time, Python 3 incorporating `range` keyword would mean zero work.
I think part of the reason is, also, what's available on RHEL 6. A lot of my customers run heavily locked down instances of that OS, and we're constrained to the default Python versions available via yum - 2.6, and, if we're lucky, 2.7. Thanks to AWS, however, that number of customers is on the decline.
That's true, but when switching costs are relatively high you need to give people a long horizon before the cliff or else they'll simply ignore the switch and fork the project. That's not a technical concern; its' a political one. What you must do is wait 7-10 years then present the cliff. At that point, the means to switch will be readily available but simply not used (e.g. most 3rd party packages are actually Python 3 compatible, but internal unpublished ones are not). Then rebellion will be lessened because no one wants to admit to their shame that they had 10 years to switch but did not.
I think async and await, have helped a little in providing that justification. Type hinting might provide a little more push. Yet you are right, there's no outstanding and compelling reason to switch considering you will instantly take a 10-20% performance hit for working with Python 3. Granted, I personally, am prepared to pay up to a 100% performance hit... but only for something that lures me into it. For instance, many people can be lured to switch from Java to Python because of dynamic typing and a friendlier syntax. For 3.0 vs 2.0, in Python the rationale is much less---and seems to be tackling issues that language maintainers faced rather than end-users. Even in Python 3.5 working with unicode has some hairy edge-cases because string data is often coming from the outside world and the outside world can never be trusted to give you well formed data.
What actual library beyond Twisted is not Python 3 compatible? IIRC, Twisted is mostly Python 3 compatible, but some of the bits and pieces are hard to test so it hasn't been declared completely done. I'm not a Twisted user, though, it's just something I think I remember from IRC.
Fwiw, it supports Python 3 now and did in the last major version before the present one, though I have no idea of how long that timeline is.
Luckily AJAX is pretty easy nowadays. You don't need to worry about including jQuery, you can do this all vanilla. I'll do my best but I'm on mobile ATM. If you have any questions feel free to ask! // Setup the AJAX portion // We'll assume your route is `/activate` function activate() { var req = new XMLHttpRequest(); req.open('GET', '/activate', true); req.addEventListenerLoad(function(event) { if (req.readyState == 4 &amp;&amp; req.status == 200) { // Success alert('Success'); } else { // Failure alert('Failed :('); } }); req.send(); } // Now our click handler for the image // Assuming your image has a class of `action-image` document.querySelector('.action-image') .addEventListener('click', function(event) { activate(); }); Now if you click that image you should make an ajax request! I'm about to head to sleep, so for now take a look at MDN if you are curious. Otherwise, send me a message or reply and I'll get back to ya tomorrow! EDIT: Spelling
Sure, for the past two weeks that was just under 3 million downloads: 2.7 69.23% 2.6 21.22% 3.4 5.42% 3.5 3.09% 3.3 0.77% 3.2 0.24% 
thanks, will do.
Spot on. In the cost/benefit analysis it's very hard to port an existing code base to 3, and will probably remain so for at least a decade. Unless you're trying to use a library that no longer maintained 2.7 compatibility, there are very few features in 3.x worth that hassle.
Thank you for your answer!
 range=xrange #there, done
I've never found identity checking to be a widely used function compared to `in` and +. Being a keyword is serious business.
`is` provides the unique function of [identity comparison](https://docs.python.org/2/library/stdtypes.html#comparisons) Yes, it could have been a function. Even `==` *could* have been. But it makes sense to have easy access to both identity comparison and value comparison.
I believe 0-25x are only singletons in the interpreter which is a source of errors for new users.
Is that including or excluding None comparisons? I haven't used regex in months.
0-256 don't have to be singletons, and shouldn't be assumed to be singleton values. That's just an optimization that exists within CPython.
Yeah, that's exactly my point. What is the use other than that? == none does the same job. You don't need a new keyword for what is essentially null checking. Your grep ignored exactly what I was looking for, the real use case beyond none checking!
&gt; Upgrading to python 3 is mostly trivial and people who haven't done it by now can accurately be described as lazy. Is that why if you run `python` at a typical modern Linux prompt you get Python 3, and why on Windows if you install 2 and 3 concurrently the system will automatically determine which to use when you run a `.py` file?
It's a CPython optimization, so Jython, PyPy and others aren't bound to this behavior. 
&gt; == none does the same job. It does not. Identity and equality are different concepts. You started by saying that you know what this operator does, but I'm not sure you do. Equality can be customized. You can implement a class that has an `__eq__()` method, and you can make instances of that class compare equal to `None` if you wanted to, or to do anything else, like always return `True`. Identity is a much more fundamental concept and cannot be overridden or customized. The only thing that will ever be true for `is None` is `None`, which cannot be said of `== None`. Another comment in this thread points out a common idiom for using this property to create custom singletons that can be used as unique identifiers. 
&gt; == none does the same job. False. `a==None` is the same as `a.__eq__(None)` `a is None` is the same as `a.is_(None)` 
I know but comparing none to none will be the same as doing none == none. I thought singletons were bad, now we're for them?
Fair enough, thanks for putting the edit note in there as well.
I like you and your words.
What brought it to mind is that people have recently been really negative to any changes to the languages (zen of python, zen of Python, ZEN OF PYTHON!!!!) but they're fine with this keyword which has no real use. No one here has brought forward any case with any real merit other than none checking which can be done with == like in every other language in existence. I've asked people for a time when identity checking can be used and all I get back is none checking. Yeah, but when do you do identity checking?
Don't use is to compare with none, just use if not &lt;variable&gt; instead. 
Yes? I get that it's for identity checking but no one has given a use case other than none checking. One guy said about things like frozensets but that's a real niche use.
yes, there's an index_label, but it doesn't create a primary key. I saw a discussion on github about it and they plan to add it, but it's not there yet.
We use Twisted for our own chat server implementation. It isn't all as simple as it looks from outside, the almost ported code base doesn't work for us. There are also a lot of lesser known commercial third party twisted modules written 8-10 years ago that unfortunately are quite complex to maintain and even more complex to port to Python 3 with full code coverage. I gave my team lead the option to go with ejabberd or atleast nodejs, but it's like talking to a black hole.
Thanks all. Ill post in r/learnpython from now on.
I use `is` in linked lists in robotic motion planning when I recursively search a graph. Makes sure I don't search backwards and makes it known when I've hit another vein of the linked list. It's basically the reference (`*`) operator in C/C++. The reason you might not see too much use of it is that Python is usually very high level, but that's not too say it shouldn't be part of the toolbox. It's necessary in some cases.
i think it depends on the situation, for example def f(a=None): if not a: do_something() else: do_something_else(a) given that function, f(0) and f() both do the same thing, even though f(0) may have actually merited a different response.
nltk's download script updated to 2016.
That is indeed a use case. Why not an Id member with == though?
I've joked about a \_\_past__ module before. Ultimately I think it's a terrible idea because it would just encourage people to write code with Python 2.7 idioms and bellyache when it comes time to move to the forward-compatible idioms (because release 3.(x+1) makes them generate exceptions finally), and they would ask for even more generous extensions or do terrible things like monkey patch the standard library to make it work. I don't feel that it is the responsibility of Python 3 to support Python 2 idioms indefinitely into the future, and it's a sort of "give an inch, take a mile" scenario. How gradual of a deprecation do you want, really? 2.7 is 5 years old. 3.3 is 3 years old. If you're going to write Python 2.7 code, at this point there really isn't an excuse to be writing not-forwards-compatible code, because all the tools exist for Python 2.7 to write code that works with Python 3, in Python 3 idioms. As far as the print operator, you should be using &gt; from \_\_future__ import print_function in your Python 2.7 code (among others) anyway. It's just better than the print operator, AND it's forwards-compatible.
Ahh, yea it does depend on context I guess. 
On your second to last point, you *could* do this in Python 2 items = ['foo','bar','baz'] print '+'.join(items) 
It's not so much about how often it's used, IMO, it's about the consequences of overriding it. Identity comparison is about the most basic operation you can think of. If you can't trust `is`, what can you trust? So it's an operation that you can't override. This isn't specifically about it being a keyword - `in` is a keyword but can still be overridden. Assignment `=` is a symbol and can't be overridden. But any of Python's built in functions can be overridden, so it's not practical to do it with a function.
This is true, but is much less clear, especially for someone new to the language.
I honestly firmly believe educators are to blame for the majority of this. People learn 2.7, then go teach 2.7. Almost every CS student I talk to has their teacher teaching them 2.7. The major tutorials online are mainly using 2.7. It's just a never-ending cycle where only a tiny few branch out. Then, after learning the basics and maybe some intermediate stuff, people of course begin actually doing something with Python. Do they choose to upgrade at this point? Nope. Project begins on 2.7, and every day the hole is dug deeper. Project gets big enough, needs some more team members. Are we going to upgrade to 3? Hell no. If educators would just stop starting with 2.7, I really think the majority of this nonsense would be over. 
Because it's one of the easiest ones to show. The full list is too long to fit in a reddit comment, so you can click the link [here](http://python-notes.curiousefficiency.org/en/latest/python3/questions_and_answers.html) or read an excerpt below (but we both know you won't). &gt; Some changes that are likely to affect most projects are error handling related: &gt; the exception hierarchy for operating system errors is now based on what went wrong, rather than which module detected the failure (see PEP 3151 for details). &gt; bugs in error handling code no longer hide the original exception (which can be a huge time saver when it happens to hard to reproduce bugs) &gt; by default, if the logging system is left unconfigured, warnings and above are written to sys.stderr, while other events are ignored &gt; the codec system endeavours to ensure the codec name always appears in the reported error message when the underlying call fails &gt; the error messages from failed argument binding now do a much better job of describing the expected signature of the function &gt; the socket module takes advantage of the new enum support to include constant names (rather than just numeric values) in the error message output &gt; starting in Python 3.5, all standard library modules making system calls should handle EINTR automatically &gt; Unicode is more deeply integrated into the language design, along with a clearer separation between binary and text data: &gt; the open() builtin natively supports decoding of text files (rather than having to use codecs.open() instead) &gt; the bytes type provides locale independent manipulation of binary data that may contain ASCII segments (the Python 2 str type has locale dependent behaviour for some operations) &gt; the codec system has been separated into two tiers. The str.encode(), bytes.decode() and bytearray.decode() methods provide direct access to Unicode text encodings, while the codecs module provides general access to all available codecs, including binary-&gt;binary and text-&gt;text transforms (in Python 2, all three kinds can be accessed through the convenience methods on the builtin types, creating ambiguity as to the expected return types of the affected methods) &gt; data received from the operating system is automatically decoded to text whenever possible (this does cause integration issues in some cases when the OS provides incorrect configuration data, but otherwise allows applications to ignore more cross-platform differences in whether OS APIs natively use bytes or UTF-16) &gt; identifiers and the import system are no longer limited to ASCII text (allowing non-English speakers to use names in their native languages when appropriate) &gt; Python 3 deliberately has no equivalent to the implicit ASCII based decoding that takes place in Python 2 when an 8-bit str object encounters a unicode object (note that disabling this implicit conversion in Python 2, while technically possible, is not typically feasible, as turning it off breaks various parts of the standard library) &gt; Python 3.3+ now correctly handles code points outside the basic multilingual plane without needing to use 4 bytes per code point for all Unicode data (as Python 2 does) &gt; A few new debugging tools are also provided out of the box: &gt; faulthandler allows the generation of Python tracebacks for segmentation faults and threading deadlocks (including a -X faulthandler command line option to debug arbitrary scripts) &gt; tracemalloc makes it possible to track where objects were allocated and obtain a traceback summary for those locations (this relies on the dynamic memory allocator switching feature added in Python 3.4 and hence cannot be backported to Python 2 without patching the interpreter and building from source &gt; the gc module now provides additional introspection and hook APIs &gt; The concurrency support has been improved in a number of ways: &gt; The native coroutine syntax added in Python 3.5 is substantially more approachable than the previous “generators-as-coroutines” syntax (as it avoids triggering iterator based intuitions that aren’t actually helpful in the coroutine case) &gt; asyncio (and the supporting selectors module) provides greatly enhanced native support for asynchronous IO &gt; concurrent.futures provides straightforward support for dispatching work to separate working processes or threads &gt; multiprocessing is far more configurable (including the option to avoid relying on os.fork on POSIX systems, making it possible to avoid the poor interactions with between threads and os.fork, while still using both multiple processes and threads) &gt; the CPython Global Interpreter Lock has been updated to switch contexts based on absolute time intervals, rather than by counting bytecode execution steps (context switches will still occur between bytecode boundaries) &gt; For data analysis use cases, there’s one major syntactic addition: &gt; Python 3.5 added a new binary operator symbol specifically for use in matrix multiplication &gt; Notable additions to the standard library’s native testing capabilities include: &gt; the unittest.mock module, previously only available as a third party library &gt; a “subtest” feature that allows arbitrary sections of a test to be reported as independent results (including details on what specific values were tested), without having to completely rewrite the test to fit into a parameterised testing framework &gt; a new FAIL_FAST option for doctest that requests stopping the doctest at the first failing test, rather than continuing on to run the remaining tests &gt; Performance improvements include: &gt; significant optimisation work on various text encodings, especially UTF-8, UTF-16 and UTF-32 &gt; a significantly more memory efficient Unicode representation, especially compared to the unconditional 4 bytes per code point used in Linux distro builds of Python 2 &gt; a C accelerator module for the decimal module &gt; transparent use of other C accelerator modules where feasible (including for pickle and io) &gt; the range builtin is now a memory efficient calculated sequence &gt; the use of iterators or other memory efficient representations for various other builtin APIs that previously returned lists &gt; dictionary instances share their key storage when possible, reducing the amount of memory consumed by large numbers of class instances &gt; the rewritten implementation of the import system now caches directory listings for a brief time rather than blindly performing stat operations for all possible file names, drastically improving startup performance when network filesystems are present on sys.path &gt; Security improvements include: &gt; support for “exclusive mode” when opening files &gt; support for the directory file descriptor APIs that avoid various symlink based attacks &gt; switching the default hashing algorithm for key data types to SIPHash &gt; providing an “isolated mode” command line switch to help ensure user settings don’t impact execution of particular commands &gt; disabling inheritance of file descriptors and Windows handles by child processes by default &gt; new multiprocessing options that avoid sharing memory with child process by avoiding the os.fork system call &gt; significant improvements to the SSL module, such as TLS v1.1 and v1.2 support, Server Name Indication support, access to platform certificate stores, and improved support for certificate verification (while these are in the process of being backported to Python 2.7 as part of PEP 466, it is not yet clear when that process will be completed, and those enhancements are already available in Python 3 today) &gt; other networking modules now take advantage of many of the SSL module improvements, including making it easier to use the new ssl.create_default_context() to choose settings that default to providing reasonable security for use over the public internet, rather maximising interoperability (but potentially allowing operation in no longer secure modes) &gt; Object lifecycle and resource management has also improved significantly: &gt; the cyclic garbage collector is now more aggressive in attempting to collect cycles, even those containing __del__ methods. This eliminated some cases where generators could be flagged as uncollectable (and hence effectively leak memory) &gt; this means most objects will now have already been cleaned up before the last resort “set module globals to None” step triggers during shutdown, reducing spurious tracebacks when cleanup code runs &gt; the new weakref.finalize() API makes it easier to register weakref callbacks without having to worry about managing the lifecycle of the reference itself &gt; many more objects in the standard library now support the context management protocol for explicit lifecycle and resource management Individually, most are meh. Together they make a much better language.
Yes, I am *clearly* trolling. Rawr. /s
Yes, of course. But there is a subtle difference, which is that by letting `print()` do the work instead of doing it yourself, you avoid having to create a new string that contains the result of the concatenation, which is essentially equivalent to making a temporary copy of all of the strings. `print()` can(*) write each item and its delimiter in turn directly to the output file. (*) It's not required to, but [at least in CPython that's how it works](https://hg.python.org/cpython/file/v3.5.1/Python/bltinmodule.c#l1753). Of course arguments of minor performance differences are near the bottom of the hierarchy of persuasion. It gets even uglier though if you want the non-default line ending, because you have to write something like print '+'.join(items) + 'end', ...which means that you have to make another copy of that big concatenation string to tack on the ending. You can't write print '+'.join(items), 'end', ...because there's no way to prevent that implicit space delimiter that 2.x adds between each argument. 
If i have fairly simple python 2.7-script this and xrange is about the only thing I would have to change? To convert to python 3 I mean.
Sorry, follow on, only partially related: Dropbox switched to Go two years ago, apparently. I don't think they are likely to take up the 2.7 mantle.
The same is true for flask. So maybe it will be easier for op to use php [exec](http://php.net/manual/en/function.exec.php) and call the php file from his html page, to exec the py code.
Ah that's too bad. I got lucky at my new company that we were starting from scratch, so we went 3.4 from the start. There has been a few pain points, but I've been allowed to help add python3 support in a few cases, so it's been worth it.
still a collective meh to me. async stuff probably the strongest. the thing I've seen that I want to really use is the formatted strings (pep 498), so could prompt a switch at 3.6. Also - regarding "but we both know you won't" - you don't know me and you don't know whether that's true. I just did a painful upgrade from django 1.7 to 1.9 for a feature I wanted on a project. I'm not against switching in principle, I just don't get why people are so zealous about python3 when there's nothing particularly compelling about it. 
[What's wrong with the current downloader?](https://github.com/nltk/nltk/blob/develop/nltk/downloader.py)
In a perfect world, yes. Too bad that sometimes Fabric is used as a command line all-in-one control-the-project monster and it's all over the place, instead of a separate CLI and fabric automation. What I meant is that not everything that doesn't support Python 3 is bad and should be avoided. I'd definitely stick to 3.5 only if it would be possible.
You have a good point.
Good point (me, too), but there's several others which I install directly only because Python 2 needs them.
Agreed. 
Either you don't read and understand things, or you're trolling. There is no excuse to be confused at this point. 
no worries - happy coding!
There are a large number of subtle changes, including print and xrange but definitely not exclusive to just those two. This should get you started: http://python-future.org/index.html 
Yeah, I guess that's hard to predict up front, but personally if a library doesn't have 3.x that's all the more reason not to use it. They've had 8 years to port it. (Although I should put my money where my mouth is ... since I'm using Google App Engine and they are only running 2.7! Argh.)
Thanks. I knew what I meant to write, but my fingers decided to write something else.
Thanks for being so civil, and you too.
Everybody talks about this mysterious *someone*, but nobody is volunteering to be that someone, or suggesting who it might be. For it to be worth it to be that someone, you have to (a) have a Python 2 codebase so big (million+ lines) that it's more work to upgrade to Python 3 than to maintain the entire runtime, and (b) enough money in the bank that putting a couple developers on runtime maintenance is cheaper than porting. There's still several years until Python 2 is off life support. Any smart big company can simply declare that all new code works in Python 3, and upgrade the rest as they go. Amazon, for example, isn't above [throwing away an entire Python 2 library](http://github.com/boto/boto) for a newer one that's easier to maintain. I imagine Google isn't, either. That leaves companies which are in the "big and stupid (at software)" quadrant. Boeing, perhaps. I know they've paid to maintain otherwise-dead software. They only use Python for *internal* projects, though, which doesn't count as "distribution", and I don't think they've ever shared any open-source. So even if they started maintaining Python 2.7, we'd probably never see it. It'd be specific fixes they need, not general maintenance. Internal coding standards would simply say to avoid the parts that aren't working right. Believe me, beauty is not a requirement for them. Finally, is there any precedent for this in the open-source community? Not that I can see. We've had lots of incompatible upgrades in the past, and I can't recall anyone ever "stepping up" to maintain the old one. GTK+1 is long dead. GTK+2 got a patch to fix a segfault, about a year ago. I've heard lots of complaints about new versions of Firefox, and they even break extensions, plugins, and sometimes parts of the web. But nobody has started maintaining an old version. (There's only two exceptions I can think of. One is Xemacs, where the primary resisted adding new end-user features for years, which is basically the opposite of what's happening here -- and then the branch died off, when the primary added those features. The other is EGCS, which did so much innovative work that the original project declared that branch to be the primary and joined them -- again, not going to happen here.) In the end, everybody upgrades, or dies out. The Python 3 features are pretty compelling already. What if Python 3.6, by one of the optimization projects in progress now, turns out to be 50% faster? At some point, it starts to get absurd to avoid upgrading. Python 3 already is a much better language.
For starters: 1. Verification of SSL certificates failed. This was the impetus for writing my own. 2. My solution is far shorter and simpler to work with -- just run it after adjusting NLTK_DATA_ROOT, add the dependencies (lxml, BeautifulSoup, and requests), and get on with your day. 3. It can be invoked from a batch process.
An epub is basically a .zip of a directory of html files. - You can do a quick search to see how to make them.
yea, I had already spent many hours on the doc pages, stackoverflow and duckduckgo with things. I was missing sending 'self' to the internal decorator method, that just wasn't in any of the examples I saw. thanks.
Hmmm... this sounds interesting. I am also curious as to the answer of this question.
In a *fairly simple* 2.7 script, print and xrange are likely to be the big ones you'll run into.
Sure. But those epub files are generated from the same sources as the PDF files. If the build procedure generates separate PDF files, I suppose that it can be adapted to generate separate epub files as well.
Not all libraries are available in Python 2, so you could make the same claim in the opposite direction. As always, you have to actually think about the situation at hand. My contention is that most, if not all, libraries you might *actually* want to use in a new project either support both or are in fact Python 3 only. You're free to keep using Python 2, and the rest of us can keep using the one that is receiving active development, growing cool new features, and hasn't already had a date announced for its EOL.
why didn't they do the same thing to 'assert' ?
https://www.reddit.com/r/Python/comments/45sm94/what_are_the_most_recent_python_3_vs_python_2/d00vv6x
Not sure, but what is cool about `assert` is how the second argument is not evaluated if the assertion passes. For example here `my_function` never gets called: assert True, my_function(bar) I'm not sure what this means technically, maybe short-circuiting like in a condition? But at least you can do fancy formatting in your assertion error messages without feeling bad about performance.
That statement is based on profiling I have done when trying to decide how to implement some things when I've had several ideas, but the most efficient one is not clear without measuring it. Admittedly, none of it could be considered a good "generalized" benchmark. I'm not sure I've bothered testing string manipulation. I can say with confidence that 3.4 is measurably faster at attribute access than 2.7. I'll re-run some of my profiling tests and edit this comment with the results after this morning meeting is over.
To be precise: [video at 3m 0s](https://youtu.be/JOx9enktnUM?t=3m0s) Slide and talk why not Python. Performance wasn't the biggest issue, how ever it is most obvious 'pain point' of all interpreted languages. 
Depends highly on what OP is trying to accomplish in the long run.
&gt; and you don't need a line terminator That's a poor reason. If you ever want to write to a file, you do need one. &gt; and you don't need any of the other capabilities of print() such as sep or flush. sys.stdout.flush() BTW, when is logging going to add `sep`? I think they should all have the same API and look the same, but they still don't. So, the fact that you have to call flush on a separate line, so what? That's a bad reason. `sys.stdout.flush()` still works. I write compatible 2.7/3.5 code and I didn't even know `flush` was a `print` option. I'm still going to have to change it to use logging or writing to a file. That's dumb to me, so I don't code like that.
Don't forget there is an automatic 2to3 converter here: https://docs.python.org/2/library/2to3.html
Not sure which path your looking at. There have been programs to consolidate facts to teach future AI. Several are identified here. https://en.m.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence) If your looking at the code side. Say have people add code to build up what abilities AI has. In robotics they have something similar in the Ros.org project. I have always thought a good starting point would be an IRC bot. Lots of IRC bots are designed already to monitor channels and do basic responses. But if you treat it as a framework to build from you can use the bot to monitor and learn from the channels its monitoring while have people contribute code to build the bots abilities. You need a bit based around plugins. Getting an AI to do X Y or Z has never really been a problem. The trick is getting it to understand through the ambiguity of language that you want them to do X and not Y or Z. 
Hi/Hola! No, actually i'm using ubuntu as my OS (alongside with Windows). Thanks for your help I've tried this in many different ways and i can't get it to work. If the keys has 3 different ouputs assigned (depending on whether you press shift, altright or nothing) will only display the main one and the shift one :S Thank you very much, again, for your response. I'll ask Al himself, probably he knows if it is possible on Ubuntu.
Great explanation. I really need to quit using the print statement and use the function. One minor point. (4) isn't a tuple. You need to add a comma: (4,).
I feel like a print() function similar to the one in Python 3 should be added to Python 2.x for compatibility with a lot of simple code. All existing stuff in Python 2 could use normal python 2 print instead of print() and not skip a beat but it would allow a lot of simple scripts to be compatible between versions and allow a lot of tutorials to just make the move to using print(). Edit: I actually just tried print("hello") under python 2 and it worked :) 
Which production system? Python the language only becomes a bottleneck when you have very specific cpu intensive code that can't be written with numpy, pil or a small c module, or when your production is so high volume and impossible to cache that you have enough engineers/money not to care for a high productivity language with a gigantic ecosystem as python. I would say that not counting algorithmic sins most production systems could be run in python with no big impact, and I've worked with a bunch of big companies/high traffic websites.
&gt; Was something done that broke the original language? Yes and for good reason. &gt; Why is it so hard to upgrade existing code from Py-2 to Py-3? Because nobody understands unicode when they start. The only thing worth discussing is unicode. Everything else is trivial. So what is the data that you need to read in, where does it come from, and what is the encoding? Encodings should be specified, so avoid the temptation to guess. &gt; Has the design of the language been drastically changed? No. It just doesn't auto-convert binary string to unicode strings anymore. Shocking it causes horrific bugs to concatenate a binary string to a unicode string to merge those (we up-convert) and then convert that to a binary string to print it out because standard out is specified with an ASCII encoding, which causes a crash. Yes, that really happens in Python 2. Those bugs are nearly impossible to debug in Python 2, but easy in Python 3. Python 3 forces you to get things right, which is a bit annoying, but you'll avoid those issues when you use unicode. In Python 2, due to the autoconvert, you may fix a few of those bugs, but unless you test every bit of code, you're asking for trouble. Beyond that, set the encoding early and apply the encoding at the interface between modules. &gt; What is keeping everyone from moving forward? They don't understand unicode and they require dependencies and those authors don't understand unicode.
Internet says it's a quirk of 2.7, apparently True and False aren't considered keywords in 2.x, but this was changed in 3. This explains it not working for None, because it is considered a keyword (IIRC)
I like list comprehensions for simple things, but for something like what you are doing in the last line, I'd probably just write a nested loop, or a loop with a list comprehension inside of it. IMO, that makes it more readable. 
very likely, i've learned that with code, 90% of things that make absolutely no sense were made when there was no foresight (or in some cases, thought) of the future consequences, and they remain far longer than they should to support people who dug themselves holes in what is already there.
Why can't we just have the damn keyword also though? I agree that the function is great, but the keyword is _so_ much easier, and much less work to change from 2.x to 3.x. I want both of them! Do you hear me Guido?
The same argument could be made the other way. Meaning, 85% of pypi user base speaks english, so world speaks english. My point is that pypi numbers are not a good proxy.
 &gt;&gt;&gt; True, False = False, True &gt;&gt;&gt; (1 == 1) is False True I can already see someone monkey patching this on someone's python stdlib to prank them.
Fedora 24 (the next release) is going to use python 3.5 too.
Python didn't always have True and False, so before they were part of the language, people used to define them as names, `True = 1; False = 0`. If they had made that a syntax error, existing code would have broken when True and False were introduced. I know this because I ran into it a couple of times porting code to Python 3, where it was made a syntax error. Those bits of code had lingered for years. Interesting side note: you occasionally see loops with `while 1:` instead of `while True:`, because under Python 2, True is just a name, so it's very slightly slower to check it than to check the constant `1`. In Python 3, True is a constant, so it's no slower.
This exactly, iirc in `.pyc` the assert statements are removed entirely.
Red Hat have a great track record of back porting fixes though. Someone, somewhere will patch 2.7 once official support ends and RH will absolutely pick that up (or perhaps even be the guys who make it) for the life of RHEL 7, that's why we pay so much to use it.
Think what you want, but the scenario you describe doesn't happen in the code we're looking at. The return value being checked can never create the error you describe.
I'm a bit stuck up when it comes to this. Just seeing your example: `Http.NewClient(...)` made me cringe. The only thing i thought of was.. why not `Http.Client(...)`??
Windows doesn't come with python (even MacOS does) and I think it's the only one.