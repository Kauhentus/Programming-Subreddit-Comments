I am always happy to read about the PyPy progress. Keep up the good work! 
ohhh i thought it as an extension or something of python, now i saw ran ./pypy-1.6/bin/pypy n terminal got it stupid me, i understand now, thanks for the hep! guess i will make a sys link to use it instead of the regular python.
We're implementing the Python-level NumPy API. If that's what Matplotlib needs, then yes, if not... no :) TBH I know little about numpy, besides the 3 line snippets I often see, I just like a challenge of making something blazing fast.
Yeah, I'm gonna chat with them tomorrow cuz they still don't have one uploaded.. I got mine to compile (with oracle support) but it's missing a few libraries, bzip2, expat and openssl I think, which wouldn't good as their official release. But if you want a copy I can upload it somewhere.
Yeah, I don't really know many/any, but I may start compiling windows version for them to test stuff.
I agree with that---to the extent the framework is not limited by the language. (hence the pylons/python tag).
Thank you for your work sir! Hats off. I agree with your point, and (fortunately?) I never tried Friendster. (The 503s might be just my luck...also, I think AWS had an outage this morning as well.)
http://en.wikipedia.org/wiki/Inner-platform_effect
I like it although simple but then, if used on a notebook that's rebooted every day or so... makes sense. One other thing (also simple) which I like even more is having an indication about whether or not I am using a GIT repo and which state my working tree is in: http://www.markus-gattol.name/ws/python.html#virtualenv_git_bash_prompt 
Use something that outputs it into latex, and I'm sure that you can find a library that will convert latex into pdfs.
&gt; (Note: windows binaries are not ready yet). sad windows developer is sad. :(
Cool! Cool-cool-cool! Can I compile the Windows version with Mingw? 
Or use backslashes. This is Markdown.
By the way, while we are at it, [don't ever use](http://ideone.com/ZuRfu) `random.jumpahead`, at least in Pythons before 2.7.2 or where the bug has been fixed.
I am quite new to Python. Currently I have the PyPy binary distribution unpacked and symlinked, but what to do in order to be able to use easy_install / pip / virtualenv with PyPy? (So it doesnt conflict with my cpython system install). I am so eager to run stuff on it, but I have no clue how to install packages :(
Yes, I'm the one who put up the first £600 (I'm the author of the High Performance Python report that got voted up weeks back via my EuroPython tutorial). Didrik of Enthought has put up the next £600. Others will follow...numpy + PyPy will be a bit of a game changer and will open Python's use to many more fields.
I can't parse this. Did you really mean to say you would not be surprised to see reddit level traffic on my/your projects? Because I think anyone would indeed be surprised. My point is, though it is awesome that we all get to learn from the struggles to scale of sites like reddit, facebook, digg, twitter etc.... The reality is, most of us will never have to deal with that, and so spending too much time considering how you might scale to that kind of level when choosing a tech stack is really premature optimization. It would be better to consider other factors like the ability to innovate and the speed that you can respond to opportunities. Being able to successfully respond to opportunities and innovate may bring about the scaling problem. But that's a good problem to have, and hopefully when it gets to that point you have the resources to do it($$). I honestly think reddits spotty availability problems have always been attributed to their choice of hosting provider (Amazon). Though, it's not like there are other choices for them. A company of reddit's size isn't going to be able to manage/build their own data center, they would have to a lot of stuff to make it work on appengine(and accept the limitations). Azure? lol. I'm with zzzeek, just marvel at the fact that the site runs period. And that I have no problem submitting this comment. 
oh man. that's the opposite of a well behaved random function, hah.
Are you sure? I think it may be pure python vs a C implementation. 
Pretty sure that at randint() is python, random() is in C. random() returns a float between (0,1) and randint(low,high) returns an int between (low,high)
I think the ActiveRecord pattern is probably the more popular one in python mvc frameworks. Though I am curious to see how [openerp](http://www.openerp.com/downloads) manages things because if any app would need a robust domain model, that would be it. I can see that it could be possible to develop a domain model on top of something like [resource traversal in pyramid](https://docs.pylonsproject.org/projects/pyramid/1.0/narr/muchadoabouttraversal.html) maybe. 
Have you thought about implementing the FFT in PyPy? That could be a good challenge (and a good selling point).
Thanks a lot for that BTW, I know there're lots of people who primarily use Python with NumPy and really want the chance to play with PyPy. Have there been any serious takers yet? Have you tried the preliminary version in the new release?
Yes. I am sure. Check the code.
They top-posted... I thought that the cardinal sin of mailing lists.
`randint()` is a wrapper around `randrange()` which is a wrapper around `random()` or `getrandbits()` depending on how the lower level implementation looks like. The default implementation (`random.random()` or `random.Random().random()` use a mersenne twister written in C, `random.SystemRandom().random()` use the crypto random library available on the system).
&gt; don't ever use random.jumpahead No. Don't ever use random.jumpahead with small values of n is the more correct recommendation. Before 2.7.2 it did a poor job at shuffling the MT. Generally though why use jumpahead when you can just reseed?
you don't have to do anything special to make it work. just use pypy instead of python for installing pip and such and it will all end up in the right place.
Random module implements a very good pseudo-random function, but an unfortunate consequence of events produced this behaviour in `jumpahead`. Like, originally it used a simple linear generator (`next = (current * A + B) % C`, with carefully selected constants) or something similar. With that, if you wanted to generate several runs of pseudorandom numbers in parallel, making sure that they don't overlap, you estimated how many numbers you'll need and called jumpahead repeatedly (which used the exponentiation trick to get to the requested part of the state space very fast). Then Python switched to the Mersenne Twister, which has an enormous state space (623 32-bit integers) but doesn't allow such fast-forwarding. So someone thought, OK, we will just jump into some distant part of the state space, since it's so huge we are almost guaranteed to get sufficiently far apart from any other explored part of it. The problem is, they more or less simply added the parameter to the initial part of that 623 integers state vector. And, as it happens, perturbations like this don't spread unto the rest of the state vector until after several thousand steps. So while technically we _do_ end up in a vastly _separate_ part of the state space, it doesn't look that _different_ :( What scares me is that given that Python is widely used in scientific applications, and that this bug is not immediately obvious (because the first few numbers _are_ different), and that it lurked for like five or seven years unnoticed in practice, I suspect that there exists some research with thoroughly fucked up results because of it!
I'm not trying to start a flamewar or anything, but this just jumped out at me: &gt; Like many things in Perl, this does different things under different circumstances, but it almost always does exactly what you want it to This is why I refuse to use Perl. PHP also seems to operate under the same principle at times. I love Python's "Special cases aren't special enough to break the rules" and "In the face of ambiguity, refuse the temptation to guess" principles.
Very nice ! 
When you put up £600 you can post however you want.
&gt; virtualenv -p /path/to/pypy name 
Well, what I had in mind was to analyze the tail call optimization process itself : the goal was to have a TCO'ed function that is faster that the function it's TCO'ed from - **but that isn't very important**: as long as people find something interesting to read there, I'm happy. Your first rewritings (the iterative and "Kinda Recursive") are undoubtedly impressive, and very close to the original merge sort discipline, but they don't exactly follow its recursive pattern (you proceed by levels, while in a merge sort, you fully merge the left subtree before tackling even the leaves of the right subtree). Now, what would be interesting (to me - YMMV) is to apply the same kind of tail call elimination methods on your last recursive function (a "true" merge sort), and see if it can make things faster - maybe I'll find the time to do that some time.
I'm pretty sure they use buildbot so there may potentially be Windows build slaves in their farm, so they probably have it running and testing on Windows.
Very nice, especially the last one. You should propose it as an answer to the [SO question](http://stackoverflow.com/q/7063697/47978) mentioned previously ! (btw, sorry for nitpicking, but I believe it makes O(n) recursive calls, with the classic recursive call pattern of most [divide and conquer](http://en.wikipedia.org/wiki/Divide_and_conquer_algorithm) algorithms). *edit : n, not logn
No screenshots? Come on people - I know we spend our working lives looking at lines of cryptic-looking text but can't we put some effort into these websites? For me to decide to use that I'd have to download it and fiddle. Most people don't have time. ....EasyGUI in the 'news'? 2009? Ugh.
&gt; I think AWS had an outage this morning as well. Yeah, can't blame reddit for that. With a staff of 5 and 1B page views/month I wouldn't dare try managing the hardware myself. Although I probably would have looked for something like managed dedicated server hosting with managed virtual IP in front of it. But in the end it all comes down to budget. :/
oh man. that's crazy. good thing i've never used jumpahead. You always hear about paranoid people who go on about whether or not a generator is "random enough", and you've provided a great example on why we should care!
great tip. Didn't know about random.SystemRandom().random(), thanks! PS: Know of any good way to manage multiple modules that all use random() so that they all use the same seed (either random OR static for debugging and profiling)?
I get it now. Thanks! 
UPDATE: Apparently randrange(), in turn, uses random(). So it's a combination of randint()'s python cruft as well as the extra mathematical operations involved. Interesting. See [mitsuhiko's comment](http://www.reddit.com/r/Python/comments/jn0bb/randomrandint_vs_randomrandom_why_is_one_15x/c2dms96)
The current numpy support is very basic (at least - 2 weeks ago it was) - double precision only, some basic operations, no algorithms. The cash goes towards hiring the chap who wants to work on implementing it...
The work is estimated to be 6-9 months. Donations will make up the chap's salary. Small donations add up to the larger amount. This will all be discussed in the PyPy group in the coming weeks. It seems likely (to me) that enough donations will be gathered to make it possible.
I barely know what an FFT is, so I haven't thought about it prceisely, but I do aspire to write everything I can in pure python.
The FFT is the cornerstone of many signal processing algorithms, and a very common benchmark. It's one of the things people use NumPy for. It'd really get PyPy some attention if it can approach the performance of some of the heavily optimized FFT libraries out there.
The problems with Reddit were stated at length in a former admins comment a few months back. It essentially boils down to Amazon's data storage service being very unreliable. Python has proven its reliability and scalability at Google on numerus systems.
Agreed, r/pythonhelp seems to exist but is empty (or was it just created?)
PyQt is nice and straightforward as well. [Here's a tech note.](http://www.ibm.com/developerworks/linux/library/l-qt/)
I think its just unused at the mo, which is a shame. I think advertising it in the sidebar of this sub will help get people going to it more. Then having mods either deleting or commenting on any new support subs to here about the new pythonhelp subreddit. Sounds like a small change but it turned r/apple into a whole new place.
They do use buildbot, but yesterday there were problems with the windows slaves.
I had nonstop problems with my CPython build slave on Windows Server 2008. If I find the time to put it back online I'll see about adding it to the pypy fleet as well.
Yup it need more advertising, didn't even know it existed. First step maybe to contact the actual moderator to see if he is still interested, and maybe add new moderator in addition.
r/learnpython is going well and does help, but it sure should be advertised better.
I don't see why you need to create a new subreddit if the current one is already fairly low on new submissions. Won't that just thin out new material even more?
&gt; PS: Know of any good way to manage multiple modules that all use random() so that they all use the same seed (either random OR static for debugging and profiling)? Create an instance of Random()?
&gt; I suspect that there exists some research with thoroughly fucked up results because of it! I doubt that people use jumpahead all that much :)
Are you guys planning to port the entirety of NumPy? How will you keep it in sync with the trunk version?
Linking it for easier surfing: [/r/pythonhelp](/r/pythonhelp). I'll subscribe to it too, it's always fun to explain some obscure corner of the language to a beginner.
How about [r/AskPython](/r/AskPython)?
I haven't seen it happen here yet, but questions can easily overwhelm a subreddit (like what happened with r/ECE and r/electronics).
I am quite happy with the rate of posts... if that were to increase manifold as you suggest than I guess quality would go down and it would be hard to follow if you aren't the person who checks several times every day.
If you provide a legit place to make donations, i'm in for $20. What about a kickstarter page followed by some reddit and hackernews love.
I agree. I don't understand the motivation to break up subreddits into ever smaller pieces. As it is now there are posts that are three days old still on the front page. Also, while /r/python has a lot of subscribers it doesn't get much activity. I subscribed to a few that only have ~5000 subscribers that get significantly more submissions that this one.
There is no need to run rsync in parallel. You are going to be disk limited and rsync will happily saturate the available bandwidth. In fact, running it in parallel like this will actually slow it down. If rsync is running out of memory while loading the directory tree (assuming this cause you're on redhat and likely running rsync 2.6), upgrade rsync to 3.0; it will break no sweat on directory trees with hundreds of thousands of inodes.
I'm currently working on dtype support, once it's finished I'll write another blog post describing teh state of things.
What really needs to happen is that the "How do I learn Python", "Why is Python better than PHP, and "What IDE should I use" posts should either be moved, deleted, or moderated in some other way to get out of here. Put together an FAQ that answers those questions using previously posted and highly rated suggestions, put it in the sidebar, and be done with it. When I come in here and see those same questions over and over, I take this off the front page and try again in a week. I'm certainly not against answering those questions, but I'd prefer not to have to do so every week.
But then you'd have to pass that instance into every other module and submodule and submodule that also uses random, and modify it if necessary. Is that the right way to do it? Seems sloppy
No, n log n is what I meant. &gt; In all these examples, the D&amp;C approach led to an improvement in the asymptotic cost of the solution. For example, if the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size n, and there are a bounded number p of subproblems of size ~ n/p at each stage, then the cost of the divide-and-conquer algorithm will be O(n log n). *edit, and I was wrong, but so were you :p It makes n calls of O(log n) cost, as opposed to "n log n" calls of O(1) (as I said), or "log n" calls of O(1) (as you suggested). 
I feel you've still missed the point of my remarks about trampolines. They don't make things faster. They're an implementation of unbounded tail recursion, that's it. Plain old recursion will be faster so long as you don't exceed the size of the stack. And if you _do_ exceed that, then it's no longer a question of speed, but rather a question of correctness. You should not be expecting the trampoline to be faster than naive recursion, that's not what it's for.
In a few days (probably, I've been terrible at estimation so far), we'll have full dtype support, so that should be good :) We should set up some sort of "feature request" thing where people can suggest the numpy features they need most and vote on them.
We're working on setting up a crowd funding for precisely numpy, it's complicated slightly by the fact that the guy who will take the lead isn't in the US (kickstarter requires a US bank account and other stuff), so we can't use them.
I've just been learning python. Using CPython on linux. I don't really get what PyPy is about. I know CPython's underlying code is C because it's a lower-level language. There's also Jython which is based on Java. But how can Python be written on *itself* when it's a higher-level language? I don't get it.
Ok, so here is a rough suggestion for the sidebar (influenced by r/ruby's sidebar, insert links where relevent): **Learning Python:** Learn Python the Hard Way (for Python 2) Dive into Python 3 (for Python 3 - anyone have a better book? Apparently dive into python isn't popular) Is there a good TryRuby style site? There is a "TryPython" page, but it needs Silverlight which rules out Mac/Linux users so probably shouldn't be in the sidebar. Should I learn Python 2 or Python 3? (Find a good FAQ explaining the situation) **Docs:** Python 2 docs Python 3 docs Packaging docs ([this](http://guide.python-distribute.org/) seems like the best, but it's not completely finished yet) **Libraries:** Twisted (networking) Django or Pyramid (Web Apps) Pygame (Game development) NumPy &amp; SciPy (Scientific computing) Others for areas I've overlooked? The idea with these isn't to make judgement on the best library, just to have a few libraries linked for areas that new Python programmers might be interested in. Favour best documented over other areas like speed or shininess. 
It'll thin it out, but I think it could help given that half of this reddit is super beginner level questions that are reposted almost weekly, and half is news and beginner and above questions and discussions. Once there is a clear distinction, the latter half could grow nicely. Right now I'm not too motivated to post many things because people would rather pile on to the existing 75 repetitive posts in "how do i learn pythons?"
I do all my work in a terminal multiplexer (screen/byobu or these days tmux) so I just activate the virtualenv prior to starting my tmux session and then every new window automatically has the virtualenv activated.
We agree on the asymptotic complexity of a merge sort. I was talking (erroneously at 1st) about the number R(n) of calls for a list of length n. R(1) = 0, and for k &gt;= 2, R(k) = 2+ 2*R(k/2) =&gt; R(n) = 2n-2
Imports?
Yes, indeed. And that was, mostly, the lesson of that little experiment : each reursive calls need to create a closure and the trampoline loop has to test for closure (`callable`) continuously, so the overall naive trampolining is pretty expensive - unless you reach the end of the call stack, in which case they may prove of some use. The point was driven home, expanded upon, and clarified by your comments, btw - thanks a lot for this exchange. *edit: ~~I'll edit~~ edited the article ~~soon~~ to reflect the conclusion.
Another tip: When you use data files, make SURE they go into the .tar.gz file BEFORE you upload it to PyPi. I had several data files go into my project (.html, .js, .conf) and NONE of them showed up in the tarball when I ran setup.py upload. Just a word of caution.
Because.... CPython + math = http://paste.pocoo.org/show/460918/
Nice. I should give PyPy a go with it.
Instead of requiring full numpy &lt;-&gt; PyPy inegration the task should be split into transmitting standalone numpy functions into PyPy version, and than many standalone programmers could do the job. I think it should be something like a table (available online), where users could vote which numpy functions do they miss most of all, and which functions are already involved for moving into PYPY by which programmer. However, the roger_'s extremely important question raises (written in the thread) - how do you intend to keep it in sync with the numpy trunk version?
You might be interested in repeating your tests on pypy as well. The inefficient "TCO-kinda-sorta" iterative algorithm gets very close to the iterative algorithm I gave above thanks to the tracing jit: Python's native (Tim)sort time: 0.00799341797829 Bubblesort time: 2.29729223251 Original Mergesort time: 0.0734148519039 no-len Mergesort time: 0.0750674710274 no-len Mergesort + fastmerge time: 0.0180666918755 trampolined mergesort + fastmerge time: 0.0369202070236 Manual tail-call-optimized mergesort + fastmerge time: 0.0162436890602 Iterative mergesort: 0.0134385859966 Kinda Recursive (log n calls) mergesort: 0.0137689321041 Real Recursive (n log n calls) mergesort: 0.0185768659115 
Thanks! I've implemented your list in the sidebar. Please chime in if you want any additions/changes.
Hey Ashiro, you made several good suggestions. I'll adress them point by point. **Sidebar** I've added a bit more information to the sidebar (thanks to [MachaHack](http://www.reddit.com/r/Python/comments/jnx32/how_about_a_spring_clean/c2dp33l) for the general outline). Let me know if you want to see any additions and/or changes. **New Python Help Sub(r/pythonhelp)** I'd rather promote and revitalize the current [r/LearnPython](http://www.reddit.com/r/learnpython) and [r/LearnProgramming](http://www.reddit.com/r/learnprogramming) subreddits then create a new subreddit. They already have a current userbase and community, but could maybe use some more exposure. **More Moderation** I currently can't devote the time to enforce that no Python help requests get posted in this subreddit. If you or anybody else wants to help out, send me a PM from an account with some decent posting history and I'll grant you moderator access. **More Submissions/Opening Up The Floor** I hope that your post can serve as a call to action to the community to post more. I'll certainly make an effort to do just that. One of the areas where everybody also can contribute is to flesh out the [r/Python FAQ page](http://www.reddit.com/help/faqs/Python). See the [r/programming](http://www.reddit.com/help/faqs/programming), [r/Fitness](http://www.reddit.com/help/faqs/Fitness) or [r/Atheism FAQs](http://www.reddit.com/help/faqs/atheism) for some well-developed examples. 
These books at WikiBooks are pretty good: [Non-programmers Tutorial for Python 2.6](http://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_2.6) [Non-programmers Tutorial for Python 3](http://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_3)
Any idea how much beyond the basic array object (indexing, types, etc.) would need to be implemented in RPython to be fast? Maybe it isn't a good idea to integrate too much NumPy stuff into PyPy, I can imagine that getting quite out of hand. Perhaps just a basic numeric class would work, and let a third party project handle NumPy compatibility? This way you could even modify the API to make it more Pythonic, without needing to worry about NumPy compatibility right off the bat. Just a thought...
What about [package data](http://docs.python.org/distutils/setupscript.html#distutils-installing-package-data) and [data files](http://docs.python.org/distutils/setupscript.html#distutils-additional-files)?
FYI, Alex aka kingkilr added me to the moderator list, and I'm very interested in figuring out a few solid FAQs as evidenced by my other posts in this thread and elsewhere around here.
Here's a [quick screenshot](http://imgur.com/GJXsx) of one menu pane.
Also, no real limitations for my applications, which were quite simple. Don't use it if you want a powerful GUI with multiple elements on one pane. But if you have a script that usually takes a number of arguments on the command line, you can quite easily use it instead.
I tried both, neither worked! The setup.py script didn't package them in. I don't know why.
my solution for Notpad++: @echo off cd %~dp0src python ./pallindrome.py -Wonce if errorlevel 1 goto error pause exit :error echo *** Error Running Python pause The keyword "pause" is what you are looking for. I put that code in a windows batch file and run it with F5 with something like %(CURRENT_WORKING_DIRECTORY)\run_py.bat I also have the code auto generated when i press F6 for the current open file in Notepad++. On my computer i put each project in its own folder inside C:\MyPython\, which also has a python module newpython.py which is run this way: "C:\\MyPython\\newpython.py" %(FULL_CURRENT_FILE_PATH) i forget the actual syntax for notepad++, unfortunate Notepad is not showing me exactly what those shortcuts are so i don't know exactly what the variables are. if you want my newpython.py i can post that as well.
AFAICT, the reddit code (available on github) has no unit tests. They may be somewhere else and I'm missing them, or maybe there's some exhaustive functional test suite laying around. But any project that doesn't have any tests is going to not work real well in real life, especially as things need to change, regardless of platform or language. Both Python and Pylons themselves have reasonable test suites which allow those systems' authors to make sure they continue to work over time. But that doesn't prevent people from building untested, questionable code on top of them.
I cant remember exactly what ex17 was asking but if it just wants you to turn those two lines into one line you don't need to look at the docs. Just rewrite those two so its one command together. 
You don't have to look for another function. How would you get indata if you were not allowed to use a temporary variable to store the value returned by open()?
You can use the returned object from a callable in place: &gt;&gt;&gt; x, y, z = 1, 2, 3 # some int objects &gt;&gt;&gt; (x + y).__add__(z) 6 (x + y) returns an int that has the method `__add__`, which we can use to add z to the intermediate result. Or we can just chain the calls to `__add__`, doing away with the in-fix '+' operator: &gt;&gt;&gt; x.__add__(y).__add__(z) 6 But be careful with methods of mutable objects: &gt;&gt;&gt; x = [1, 2] &gt;&gt;&gt; x = x.append(3) &gt;&gt;&gt; print(x) None Since `append` is mutating x, there's nothing to return. In this case Python returns `None`.
indata = open(from_file).read() i think thats what i did. 
Thanks, that seems logical. 
There is actually Silverlight for Mac (not sure about Linux), but I don't have it installed on any of my Macs. I'd feel dirty running software from Microsoft on OS X :D. 
I would love to see some screencasts in the sidebar if there are any. I noticed /r/rails has some. Sometimes I like to sit back and listen/watch someone show off something.
I had no idea about the "TryRuby" style sites thanks for bringing those up. I would have jumped on one of those when I was first playing around with python. I might just have to play with all of them now so I can recommend them to coworkers that don't know python yet(I'm on a mission now!).
I've never even considered something other than screen. You may have just changed how I deal with terminals! I swear every day I learn something new I didn't know about I feel like a crappier linux sys admin, thats probably a good thing, the learning part I guess.
Without semantic lose: &gt;&gt;&gt; input = open(from_file); indata = input.read() 
The fact that it took me so long to see this post, plus your quick work, lead me to examine our sidebar and think to myself "I don't know what he is talking about, the side bar seems good to me..."
How about we also have (toward the top) a link discussing (briefly) why there are two versions of Python people are using. I imagine that's a major hangup for new users..
Well there is Moonlight, the Mono Silverlight package. But honestly.. why does the site require that? I guess I should go check it out and see.. lol
It seems nice to me :)
MANIFEST.in, I'm pretty sure, is the answer.
Cool, but the last example with it executing a function confused me. It said "# calls get_online_users in the background". Are you saying this is async? That doesn't make any sense since you're asking for the value now. I would rather you say "# returns get_online_users and stores in cache", if that's what it does. Otherwise, cool library! EDIT: also, are you considering LRU with a limit on number of elements instead of time? Maybe sometimes I want my cache to not grow too large, but don't care as much how long it's there.
Yeah, I'd like to know about the latter. Other than that, this LGTM.
thanks david, i've updated the readme with your suggestion and opened a new issue for the size constraint. i could see a size limit being very useful
Yeah, that was it--sorry, I'd forgotten already. So, to amend my tip: Make sure you use a MANIFEST.in file!
dood use /dev/random its faster and I assure you its more random.
looks fantastic. one question: why don't you extend the original dict? yours is also a dictionary anyway.
thanks, that's actually a good point....not sure :) i guess i figured i had re-implemented nearly every method anyways, there wasn't much of a reason to subclass. works either way, subclassing a dict would probably conceptually cleaner though
This is very nice, thank you! Just need to work out where it makes sense in my projects now!
&gt; there wasn't much of a reason to subclass there's at least one very good reason, and that is to make this code work: &gt;&gt;&gt; isinstance(some_lru, dict) True Lots of code will check for `__getitem__`, some will just try/except around a slice or key lookup, and some will do the isinstance check. By subclassing, instances of your type are always going to pass the third test. 
The README seems like it explains things pretty well, but what you're doing isn't LRU (at least per your README). (And, to be pedantic, LRU is more of a cache management policy than a capital-A "Algorithm.") In an LRU scheme (and there are lots of variations on the idea), you are managing a fixed-size cache. Eventually your cache will suffer a miss while full and need to evict something; how you pick who leaves so that the new element can be inserted is called the cache's replacement policy. If you always pick the cache element that's gone the longest without being read/written, then you've got an LRU cache. What you're doing is fixed-time expiration (and with an unbounded cache, so you never need to evict), it seems. There's nothing wrong with that---there are definitely times when that might be a good policy---but it's very different. (Edit: Also, as a matter of Pythonic style, it's odd to restrict the use of functions as values. Maybe you should use a keyword argument or a different 'add' function to hint to the cache object that what's being passed is an 'evaluator' function instead of an actual value?)
&gt; What you're doing is fixed-time expiration (and with an unbounded cache, so you never need to evict), it seems. yeah, there's also an option to refresh the value's expiration time whenever the key is accessed, so it's not totally fixed. so if you have a default timeout of 60 seconds on the dict, and you're hitting the keys, only the keys that have been created or accessed within 60 seconds will remain. so the least recently used keys will be removed...that's kind of where i was coming from. i see what you're saying though. i think i need to add an optional size constraint to make it more like an LRU cache
I've added a link to the article [Should I use Python 2 or Python 3 for my development activity?](http://wiki.python.org/moin/Python2orPython3) on the Python wiki.
Hey Brian, I would really appreciate it if you helped with the r/Python FAQ. Thanks in advance! Also, welcome to the mod team :-)
You have a point, but Reddit is probably the only site even close to its size that is so understaffed and has so many frequent downtime and error problems. I was getting 502's and 504's randomly for a few hours earlier today.
Yeah true, it happens really often -- I think the common rule is: anything that is not a python file and can't be declared explicitly in setup.py should be added in MANIFEST.in Usually: template files, etc.
There is already /r/learnpython ?
Short Answer: Yes, Numpy uses C and Fortran for the expensive computations and this is what makes it so fast. Long Answer: Loops and function calls are dog slow in dynamic languages, unless JITed, then they are just slow (except for luajit2, I hear). When your code is too slow you usually code a function in a static language ( C or Fortran ) and bind it to python via its C-API or ctypes. A related approach is to use cython, which lets you code in a python alike language, generates C code from it and AFAIK uses the C-API to bind it. You often face the situation that you have to copy your data into a data structure that the C function can handle. In case of a matrix, imagine a list of lists, where each row is a list. For each function call into C you'd have to copy it into a continuous piece of memory to make it accessible for C. The core idea of Numpy is to circumvent this expensive copies by always managing the data in a C compatible data structure in the background, while presenting you a pythonic list alike interface to work with it. So there are two reasons why Numpy is fast: * It uses C for the heavy lifting * It avoids copies while doing so.
What if I actually want to cache a callable? Will the API assume I want to cache the result of the callable instead?
Also see [Brownie](http://packages.python.org/Brownie/api/caching.html) by DasIch and [functools in Python 3](http://docs.python.org/dev/library/functools.html#functools.lru_cache) for fixed-size (as opposed to expiration-time) LRU/LFU caches as memoizing function decorators which is arguably more useful as it combines the logic of computing values with the caching.
Thanks, added those as well.
Awesome! Thanks! 
It's just a JIT system, the JIT compiler logic being written using Python. And tbh it's a dead end. Technologically it's kind of neat demo but they are targetting practically medieval Python version so it can't really be used for anything.
But it's almost always wrong to check for dict, instead check for collections.Mapping.
ep.io: &gt; Each app comes with a free dynamic instance, 5GB of free bandwidth and 2GB-months of free disk space. 
Firstly, adjusting syntax changes from Python 2.x to 3.x isn't that hard and secondly, a lot of code out there is still relying on Python 2.x. Definitely not a dead end.
It's invite only, atm. Wish I had one. I submitted my email twice, months ago.
What do you mean you didn't install it. How did you run it?
Send me an invite, I will test and write a web2py howto. 
Someone else installed it on a shared computer he is using. Or it came with some sort of scientific package.
This fills me with happiness!
Email us at support@ep.io and we'll get you an invite. :)
Is their any GPU magic takes place/can take place?
I'm sad they use Qt, but somebody's got to use it. At least I've checked it out and am now busy looking for the python qt libs I need - the mac versions :-(. When it runs i'll look at the TODO and I might even send some patches.
I know of [PyOpenCL](http://mathema.tician.de/software/pyopencl) and [PyCUDA](http://mathema.tician.de/software/pycuda), both developed by [Andreas Klöckner](http://mathema.tician.de/entry/cims). There's also a ctypes OpenCL wrapper that supposedly works in PyPy: [PyCL](http://pycl.readthedocs.org/en/latest). **Edit**: There's also a GPU-based FFT library: [pyfft](http://pypi.python.org/pypi/pyfft).
Numpy actually mostly just manages the memory and in turn uses 3rd party libraries that implement the BLAS and LAPACK interface, see [Wikipedia](http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) . If you're getting serious you will probably build numpy against one of the optimized BLAS and LAPACK libraries from intel or amd etc. These are fine tuned with SSE instructions, probably even assembler code tailored for the CPUs they sell. That said, I don't think nvidia or amd provide a full BLAS/LAPACK implementation that uses their GPUs in the background.
A quick search turned up a [blog post](http://blog.oak-tree.us/index.php/2010/05/27/pyqt-snow-leopard) that has step-by-step instructions for installing PyQt in Snow Leopard: * Install XCode * Install the Qt framework from Nokia * Compile the SIP binding generators from Riverbank * Compile and install the PyQt bindings 
 &gt; Important: In this example I have used web2py_src_min_1.98.2.zip but this version may be replaced by a more recent and better minimalist version. Check on web2py google group for a more recent version. why couldn't you just provide download to that better version ?
You could, but you might as well do it with C++ Express Edition, since msvc is better supported. At this time the 1.6.x-release branch will not compile under windows, but there is an earlier revision that will. If you are interested I can get the revision number for you (that I have successfully compiled for windows with).
I'm not sure what you mean by practically medieval Python version? It targets the 2.7 release right now, which is the most used Python version...
You can compile it yourself (well, not quite the 1.6 release but a bit before) if you are interested. Or I have a binary I could dig up, though it may be missing a few things (openssl support, bzip2 support, and expat support, I'm still working out how to get those to work under windows lol)
This is great though there is a big warning you need. So normally if I want to get d = a * b * c where a, b, and c are all arrays I would write one for loop to do all of that at once for i=0; i&lt;length(a); i++ d[i] = a[i] * b[i] * c[i] Numpy will only do this for one operation at a time so: for i=0; i&lt;length(a); i++ temp1[i] = a[i] * b[i] for i=0; i&lt;length(a); i++ d[i] = temp1[i] * c[i] So the short of it is each operation you do on a numpy array means one more for loop. 
Interesting, I didn't know this. At some point I will take a closer to look at the docs and their mailing list if there is some way to bundle a set of operations. But since I "just" use python to prototype algorithms that are later to be written in Fortran I'm not too motivated to invest time into this right now :P 
I added an option to the set method (evaluator=False) to store a callable as the value. I think it'll be rarer than using the callable as an evaluator though, so I left the default behavior to treat it as an evaluator.
So I should subclass collections.Mapping instead of dict, so that isinstance works?
I don't think that is correct. This is a linear algebra problem, not a numpy problem. You can't multiple matrices of different sizes. My guess is that if you do have an array a, which is s 1 x N dimension, and you multiple it by matrix b which is 1 x N, then it does the transpose for B and makes it a NxN matrix. This is easy to test using: import numpy as np a = np.array(range(1,4)) b = np.array(range(1,4)) so a &amp; b are both 1 x 3 matrices, the result is 3 x 3. So your c in the d = a * b * c has to be a 3x3 matrix for linear algebra to work. As Maik mentions above you should try to not use loops as possible.
Numpy will do the code I talked about. If you want matrix multiplication you have to make it a matrix, numpy arrays are just that arrays and the allow vector operations on them like fortran. In the underlying C code it loops over the each array and does the operation element by element. 
It works regardless, if you just implement all the methods required by that ABC, but if you subclass you get some mixin methods for free and errors if you're missing required methods. If your mapping type is mutable you should use collections.MutableMapping instead (and can still check isinstance against Mapping if you don't require mutability). There's also [UserDict](http://docs.python.org/library/userdict.html) if you want a full dict proxying implementation. It is fine to subclass dict directly if you know what you're doing, but there are subtle gotchas to keep in mind: methods like update/setdefault bypass ``__setitem__`` so it isn't typically sufficient to override just the latter to customize mutation behavior.
It came in by default. Is that relevant though? 
I have a feeling it would be still faster to use NumPy for that calculation, but thanks! I really didn't know about this! 
Python is installed out of the box in OSx. I'd say install pythonbrew to control diferent python versions. and homebrew, homebrew FTW. Never use mac without homebrew. Also, consider not using eclipse and move to a text editor instead of an IDE. Textmate is a good start, vim or emacs are better followups. :) 
Ah. how to configure specifically Eclipse? Never done that, but remember that the interpreter should be installed already :)
Upvoted, I love the new sidebar now.
web2py is about 10MB. Here we are using a stripped down version 600KB unzipped. We provide a script to make your own stripped down version. Today web2py_src_min_1.98.2.zip is the best because it is based on stable. When we release a new stable we will make a new stripped down version and post it. In the next version we will automate the process and make a list that always points to latest but today it is a manual process.
thanks. I followed your post. now what ? I looked at the official book but so far I find it hard to follow. What I'm trying to ask is: Is that book relevant and usable for a newbie to (minimalist-version of) web2py ? if not can you suggest another material ?
It definitely is, but if you do a lot vector ops you may want to use weave to throw it in c and bundle them together.
Numpy makes it easy to interface with Fortran code by examining the sources and automatically generating Python wrappers for all of the functions, so you can at some point when you are ready to optimize you don't have to rewrite your entire code if you don't want to; you can just replace your Python numeric kernels with faster ones written in Fortran.
FWIW the PyPy implementation of NumPy automatically removes unused intermediary arrays.
This definitely has a place. Although, I can't bring myself to disconnect from wxPython... I've just used it for so long.
Good to know. Unfortunately, you can't use the rest of scipy with pypy.
Nvidia does, cublas. I use it in many of my calculations, it is really nice (well as nice as blas can be:) ).
Window&gt;Preferences&gt;Pydev and then choose the binary for the interpreter.
I hate the screen's unability to do vertical splits, the only splits that make sense at all ;-)
I have an (arguably) better place for Git branch and other things http://cl.ly/1O0b231u2Z2l3c1O2U1T ;-) I have no idea how to make Virtualenv and Emacs friends. I think I'm just going to google one day, but not now.
I went ahead and wrote the module. You can find it here: http://code.google.com/p/pynations/source/browse/ I mostly used Wikipedia, a bit of CIA World Factbook, and some of my own scraping. I'll be updating the country list as I continue scraping web pages in my other projects. Thanks again for the help everyone!
In my view, lack of a windows release in a cross-platform project is just like a major show-stopper bug in any other project. Either call the project "Linux only" or fix this problem. 
I am assuming you know python. Try reach chapter 3 of the book. That is all you need. There are many videos on vimeo: http://vimeo.com/tag:web2py Start with this one: http://vimeo.com/875433 Run it on your PC using a local (FULL) web2py When you n app on alwaysdata.com you will not have "admin" the web based IDE so you will need to use the shell. Perhaps this may help: http://vimeo.com/25912124 
Does it cover all of BLAS? Did you build numpy with it?
XML configuration files? really?
Pythonic Park ?
Both ways are suboptimale, as you code a lot of configuration hard into the code. try thinking about a big configuration dictionary which can be easily updated if you want to add new monsters and new items.
First off, you might want to edit your post to properly display the code. Put four spaces before a line to make it look like this To adress your problem, you could use a factory pattern to generate items. So instead of actual instances of an item, you tack a factory that can *create* an item to the monster. When the monster dies, you ask its factory to produce an item.
Only to register components. They reduced to usage of XML to a bare minimum, nearly everything is now configured in Python or INI-style files.
I would probably use a database (maybe sqlite?) for this. It might seem a bit more complicated if you've never used a database so far, but it makes things much easier in the long run. For example, let's say we have 3 tables: monsters, items, drops the monsters table would look something like this: id, name, hp, damage, image the items table can look something like this: id, name, type, image finally, the drops table could look something like this: monster_id, drop_id, drop_percent To get the id's of the items that drop for a given dice roll, the query will look something like this: SELECT drop_id FROM drops WHERE drop_percent &lt;= dice_roll You then get the details of all the items retrieved by the previous query. (there's actually a way to do this with just one query but I didn't want to make it more complicated) There are lots of database tutorials online, you should check them out. Separating the game logic from game data makes your code much more easier to work on. **edit**: on second thought, a database is a bit overkill if you don't have lots of monsters and items (over 10-20). You can basically do the same thing using a large dictionary, like jerriman suggested. Just put it in a different file and import it. Doing it this way will also make it easier to switch to a database if the file gets too big
If the item list and probability is specific to each monster, and nothing else, then what you'll want to do is implement the drop function in the monster itself (so, for instance, your Orc class will have a drop or get_item_drop function that guarantees that it drops an orcish sword, your Slime class' equivalent function never drops anything, and your Player class will drop all of their carried items). Call it from your Fighter.take_damage function upon death (or guarantee its presence in the Fighter.death_function, but that looks like it's for death throes and the like). This is the equivalent of what Nimbal said.
Given that FFTW exists, with OpenMP support and processor specific optimizations, it makes much more sense to have PyPy talk directly to the C library version.
I'd still be interested in seeing how well PyPy can perform.
You may want to do a bit of reading up on object-oriented programming, because there are many places in your code where you would really benefit from applying its paradigms. For your monster drop item problem, you should be looking at a solution along the lines of this: from random import choice class Monster(object): dropped_items = ( (80, 'healing potion'), (20, 'gold'), ) def drop_item(self, luck_boost=0): item_selection = [] for likelihood, item in self.dropped_items: item_selection.extend(likelihood * [item]) return choice(item_selection[luck_boost:]) class Imp(Monster): dropped_items = ( (60, 'gold'), (30, 'helmet'), (10, 'sword'), ) Then you can just call .drop_item() on any monster instances to get the item that should be dropped. This will probably be done in the code for that monster's .die() method.
Your drop_item implementation is the most inefficient solution I've ever seen. Sum up the likelihoods, then for each item take its likelyhood divided by the total. That gives you the real likelihood. That whole "likelihood * [item]" thing is absolutely wasteful. 
Well, that's a good point, but it's not like the project has dropped windows support, there are just some issues with it right now. I agree though, they should not have announced the 1.6 release without a windows binary.
Is it named bind so that maybe you draw hits for people searching for a DNS server?
Issues with one of the platforms in a cross-platform project are a big warning sign. The reason a cross-platform product is recommended to be built continuously on the 3 major platforms is so that platform specific stuff doesn't creep into the implementation of the cross-platform stuff. Anyway... the developers don't owe me anything. :)
Wish list: - WSGI compliant application - Use [Werkzeug’s URL Routing][1] instead of [reinventing the wheel][2] - Better documentation; try Sphinx and [Read the Docs][3] [1]: http://werkzeug.pocoo.org/docs/routing/ [2]: https://github.com/RafeKettler/bind/blob/5fd06dc112545d6c497624f2ec7c14eee6d97523/bind/__init__.py#L20 [3]: http://readthedocs.org/
isn't that a little much for a hello world?
PS - Typed on WinXP. ipage uses Python 2.5
I'm familiar with sphinx, but felt like it was overkill for such a small API. I'd rather focus on stability for now. As for the Werkzeug URL routing, I'd like to keep the dependencies down to a minimum. I've got a much simpler system in place that's probably more efficient as well by virtue of simplicity. As for the WSGI compliant application, this isn't a web application framework.
No. I chose bind because I think it's a memorable name that is descriptive of what the framework does, and because it's not taken on PyPI.
Cool, I could have used something like this a year ago and ended up with a home cooked HTTP based API. If you are planning to add HTTP Digest authentication (more secure), you may want to steal that part from me: https://github.com/knipknap/exscript/blob/multiprocessing/src/Exscript/servers/HTTPd.py [edit:] Ohhh, I see now that you are only implementing the client, not the server. In that case, disregard.
&gt; If I unpack three variables but I only want to input two, why does it raise an error instead of letting me use just those two variables? so you supplied 2 arguments and the script unpacks (expects) 3 and you wonder why you get error ???
Thanks for the explanation, will read again A++++
GvR says don't use UserDict: http://www.quora.com/In-Python-what-are-the-benefits-of-using-UserDict-over-directly-subclassing-dict
+1 for weave. That shit is magical, and definitely my preferred way to mix python and fast C code. For anyone who's interested: weave lets you write snippets in C and run them in a python function. It's very fast. See a nice example [here](http://www.scipy.org/PerformancePython#head-a3f4dd816378d3ba4cbdd3d23dc98529e8ad7087).
I have a couple of issues with the code, not with the README. Why is your main class named dict? I would want to do from lru import LRUCache and not have to import lru and then access "dict". dict should be dict and nothing else. Also, subclassing from dict probably would have been a good idea. You implmented a ton of functionality that already exists. An LRU cache that doesn't update on access isn't a LRU cache. LRU stands for Least Recently Used. When I use something, that changes how recently it was used. Also, expiring based on time rather than size is a bit odd, but I could see use cases where expiring based on time could be useful. I'm thinking about complexities a bit now too. Inserting into a real heap is O(log n), but your heap is based off a Python list, which has an insert complexity of n. (Can anyone confirm that inserting with heapq on a list has a complexity of n? Reading the heapq docs, it seems like it will. Popping from the heap (again, based on list) is also n. All of your operations could be constant time with a dictionary and a linked list (values are nodes in the linked list). Sorry if this was a bit harsh, but I felt like a little code reviewing. 
&gt; As for the WSGI compliant application, this isn’t a web application framework. Sorry, I misunderstood the project. I thought it is not a generic client library for web services, but a generic web service providing framework.
Looking into the source code a bit, it appears heapq actually does have O(log n) insertion and deletion.
Just to be clear for those running it on PyPy, the comparison isn't really fair since PyPy will actually eliminate the unused variable access entirely.
Switch to Linux. This won't happen.
I was about to comment on the blog post to say roughly that. If you are attempting to benchmark anything at that sort of level then you must be sure you are defeating whatever escape analysis the interpreter, compiler, or JIT might be doing or you will get nonsense results, unless you're trying to language or VM performance on algorithms where escape analysis and code elimination would be correct optimisations. Shuffling among a set of variables and passing one back as a result is the best way I've found to defeat a JIT's tricks and get good comparative numbers out.
I would, but my beard just won't quite fill out my neck the right way.
The moral of the story is something I learned years ago, probably from Alex Martelli, probably from *Python in a Nutshell*. If you desperately need to reduce the time it takes to read or access something, temporarily store it as a local. E.g. m = a.m for t in trials: m; m; m; m; m; m; m; m; m; m 
If making a connection to a remote server every x minutes is all it takes to keep your router from disconnecting, you could just use Python's socket module to create a connection and immediately close it again. No external tools required. from socket import * import time while True: s=socket(AF_INET,SOCK_STREAM) s.connect(("www.elarbee.com",80)) s.close() time.sleep(300) But seriously, fix your router
wow this thing is deep, bookmarked.
ok, i deserved that. seriously, i would probably use desertfish's solution. or just run a weather app that updates every 10 minutes.
[python**w**](http://bytes.com/topic/python/answers/743085-python-vs-pythonw#post_message_2961828) is the key **EDIT:** What this means is that you change the extension from .py to .pyw - that will keep those pesky consoles away
desertfish_ has the more appropriate answer for this specific problem. That said, if you have .pyw script that needs to call a Windows console app, you can use subprocess to set the appropriate flags for the CreateProcess STARTUPINFO struct: import time import subprocess #create a new STARTUPINFO struct startupinfo = subprocess.STARTUPINFO() #set the flag to use the ShowWindow field, which defaults to 0 (SW_HIDE) startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW while 1: subprocess.call('ping www.elarbee.com', startupinfo=startupinfo) time.sleep(300) 
&gt; An LRU cache that doesn't update on access isn't a LRU cache. d = lru.dict(5, refresh_on_get=True) Not harsh! Thanks for your criticisms. I will probably rename the class. Could you elaborate further on making it constant time? How would I know quickly which items need to expire?
Actually, not in this case. `os.system` creates a normal, unhidden window. The subprocess module lets you control the creation of the process to hide the window.
Thank you! yeah the router has some kind of configuration problem the me as well as the guy our ISP sent out to me have been unable to fix. Do you know of an appropriate subreddit I could ask for advice?
&gt; There are a ton of things you can do in Plone, but it is primarily a “content management system” meaning, “you put your website in it.”[1] And I don’t mean your “crazy cool next gen web app” website. I mean, your web. site. The one you use for you or your business or church or band or bridge club or whatever. If I ever saw a Drupal development blog with that much clarity of appropriate usage, I would burst into tears.
Why do you even need a python script? Just cron ping in the background.
Python code is rarely written with efficiency in mind. You never sacrifice readability/maintainability for efficiency unless you've identified that piece of code as a bottleneck in a production environment. This is a language where you don't write this just because you can: return reduce(concat, (likelyhood * [item] for likelihood, item in self.dropped_items))[luck_boost:] Hell, I could've improved the efficiency of my code by an order of magnitude by pre-calculating item_selection and caching it as a class attribute. I'd probably do that before I write anything like this: from operator import add from random import randint total_likelihood = reduce(add, zip(*self.dropped_items)[0]) random_item = randint(0, total_likelihood - luck_boost) random_item += luck_boost for likelihood, item in self.dropped_items: random_item -= likelihood if random_item &lt;= 0: return item I dare you to write a drop_item implementation (that includes a luck boost modifier) that is as succinct and understandable (read: Pythonic) as my original one, while being more efficient. Bonus points for justifying why you spent that time on optimisation rather than improving gameplay logic. 
&gt;from socket import * It doesn't really matter for this trivial, but it is generally considered bad practice to import * instead do from socket import socket, AF_INET, SOCK_STREAM ... s = socket(AF_INET, SOCK_STREAM) or import socket ... s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) I don't know why exactly, but for some reason with projects like this I avoid looping sleeping &amp; looping within my code... take out the while &amp; sleep and execute it with windows task scheduler (on windows) or cron on linux. You could actually do this completely without python using task scheduler or cron (though that would be less fun). &gt;Do you know of an appropriate subreddit I could ask for advice? Try doing a hard reset to restore your router to factory defaults (if that is the wrong name for this someone please correct me). It usually involves holding down the reset button for a time, unplugging with the reset button still held down, the plugging it back with with the reset button still held down. Refer to your router's instruction manual for specific directions. If that fails, ask /r/networking
Eclipse &gt; Preferences is where it's at. You might be better off with something like Pulse though: http://www.poweredbypulse.com/ 
if you haven't already found someone, what is it exactly you're trying to do?
In deed, worth noting. Also note though that he says "if you can" and read Alex' follow up. 1. Subclassing dict gives you some unintuitive gotchas most people are unaware of exists, or how to work around. 2. Subclassing a collections ABC requires the most "boilerplate" in the simple cases where you just want a dict proxy. 3. UserDict is slow and primarily useful for dict proxies. Each variant has its place. I typically subclass dict when I want a custom ``__missing__`` that needs to know the key name (defaultdict doesn't cut it) and such cases where I know I specifically want to subclass dict, and the collections ABCs when I want to mimic the dict API but with a highly customized implementation. UserDict is still useful in rare corner cases.
Line count isn't really much of an indicator of what you want. Care to elaborate?
&gt; news about the dynamic, interpreted, interactive, object-oriented, extensible programming language Python You're looking for [/r/forhire](http://www.reddit.com/r/forhire/search?q=for+hire&amp;restrict_sr=on&amp;sort=new).
What the fuck are you up to?
He is clearly up to no good, I, for one, pass. 
Sorry I don't feel it's ethical to automate account creation and message sending, especially for marketing purposes.
Hah! Love it.
Oh, when I was making my comment, I was going to mention that refresh_on_get=True seemed like it should be default behavior. Also, capping to a size (rather than an expiration time) allows you to not use time stamps at all. For constant time operations, you have a dictionary where each key is your normal string key and each val is a node in a linked list. When a key is fetched, its node goes to the back of the list. (A reference needs to be kept to the last item in the list). Deleting is done off the head of the list. I haven't checked it, but I just wrote http://paste.pocoo.org/show/462151/ as an example. It can be a bit tricky with all of the pointer like stuff. Also, I'm just subclassing dict how I would think it should work (never done it before), so there are almost definitely problems in how I did that (one I just noticed is determining if a key is valid on set before modifying my linked list). Hope this helps.
Switch to OpenBSD
 @echo off ping -n 1 www.google.com Save as something.bat, add to task scheduler.
Not fair... or exactly the point :D
FYI that's not escape analysis. Escape analysis is the process of identifying newly allocated values that do not escape a trace and removing their allocation. This is mostly loop invariant code motion, moving code out of the loop that can be proven not to change across iterations.
Ah ok, I see how it would work now. Yeah, constant time could only work if it was fixed-size, not time-based expiration (with each key potentially having different expirations). The original problem I wrote this for was caching and automatically expiring expensive queries...so I should probably just rename the project from LRU to something else, instead of changing up the logic. If I go ahead and add an actual fixed-size LRU datatype, would you mind if I steal some of that code you wrote up?
That is just the veritable tip of the iceberg. Plone is a great CMS system.
It was a pretty fun project to learn about the changes in Python 2 -&gt; 3 and a little about sockets/connections. The IRC protocol is pretty simple and a very nice intro to these concepts.
'ping www.elarbee.com -t'?
Awesome, thanks!
It's a good idea to make IRC, Connection, and Event new-style classes.
Yeah, you can use my code. Make sure you actually make it work though. Also, couldn't you just use a dictionary of tuples to still have constant time cache with time expiration?
After I read the first two words, "Introducing bind", I already expected to see something about monads. Guess not.
I'm a fan of: imap jj &lt;Esc&gt; in my .vimrc file.
Ohh, it's the ping window popping up then ...
TIL [Read the Docs](http://readthedocs.org/) 
Good point - I meant to convey that the console interface pdb provides is not necessarily the best user interface to the python debugger - your IDE may use pdb underneath a prettier gui. There are other debuggers that fully re-implement pdb - note the builtin python modules that provide utilities to do so like code, traceback, inspect, etc.
I'm still sticking to twisted.words.protocols.irc 
How the hell are you getting downvoted for recommending homebrew? That's the best way for installing up-to-date Python on a Mac.
maybe windows?
Everything is a new style class in python3 .
fluxflex.com
&gt; Also, couldn't you just use a dictionary of tuples to still have constant time cache with time expiration? I don't think so, no. If you don't maintain some kind of order, you'd have to iterate through the entire dictionary to flush out stale entries...O(N). The heap gives us the ordering, at the cost of the more expensive insert/deletions.
Meh. UTF-8 hardcoded. At least default it to “irc encoding” (try utf-8, if invalid fall back to latin1).
In case you are not aware, unicode is the norm in py3k
irclib is great IMO if you don't know much about async programming. (I however migrated myself from irclib to twisted for all the other stuff.) EDIT: also, t.w.p.irc felt rather lacking, I had to complete quite a bit of it.
The IRC protocol has nothing to do with py3k using unicode strings by default.
Indeed it doesn't. Hard-coding utf8 just wasn't the author's "default" choice.
Might have been better to port it as a single codebase for 2.x and 3.x. Less maintenance headaches, that way.
Yes it was. self.socket.send(cstring.encode('UTF-8')) The signal to noise ratio is high with you sir.
Just in case: he means that you are stupid.
Right, NumPy doesn't see the whole AST; while it processes fast, it can't do this simple optimisation to remove copies. [numexpr](https://code.google.com/p/numexpr/), and pypy's future built-in equivalent, are able to do that.
Actually it's probably a better bet to fall back to Win1252 rather than latin1.
**tl;dr** Not really possible without cooperation from the code, because it is not possible to kill a thread in a proper way from another. The article is right when it says that it's not Python's fault if it is unsafe to kill a running thread, and the last solution is rightly shown as the only sane way to do it, *ie* having the code cooperating. I use threading.Event to tell the running thread that it should die, and make sure to check its value often enough in the code. 
crap. missed it.
Because homebrew is written in *gasp* ruby, and the python community (and more noticeably the reddit community) hates ruby for some reason. I swear, I love python, but the zealotry of the python community sickens me, just as much as the flamboyantness (shut up, that's a word) of the ruby community makes me bored.
Thank you 1) for considering this task, and 2) for actually carrying it out.
&gt; Python code is rarely written with efficiency in mind. You never sacrifice readability/maintainability for efficiency unless you've identified that piece of code as a bottleneck in a production environment. If you write all your code in this fashion (creating lists hundreds of elements long just to select an item to drop) then your code will not have any obvious bottlenecks, it will be a uniform blob of suck. Premature pessimization is a crime. While you shouldn't optimize your Python code in the same way as you would do with a performance-critical piece of C code, still, you should use proper data structures, avoid asymptotically-bad algorithms, and so on. In fact, in my experience, paying attention to performance (in the right way) almost always results in a cleaner, more comprehensible and easier to use code. I mean, when instead of little tweaks like caching values in local variables or unrolling loops or inlining functions (which is premature optimization and is bad) you structure your data flows in such a way that each function gets all data it needs in the form that it needs. Also, I'm afraid that this attitude of "I'll write bad code but will optimize it later if necessary" gradually diminishes your ability to optimize code even when necessary. In this particular case: * you'll need to make weighted choices all the time in an RPG. So you should abstract the functionality into a function, rather than repeat yourself all over the place. It's the first thing that you should have thought, and it has important implications such as that you no longer need to care that much about implementation complexity as long as it makes the interface cleaner and easier to use. * Weights should be floats because why the hell not? Your initial inefficient implementation could only use ints, that limited your judgement. * And **the** correct way to make a choice is to precompute the cumulative probabilities, then use bisect, instead of walking over your list linearly. def weighted_distribution(*args): total_weight = 0.0 items = [] cumulative_weights = [] for weight, item in args: total_weight += weight items.append(item) cumulative_weights.append(total_weight) items.append(items[-1]) # avoid the possibility of out of range access def sample(luck_boost = 0.0): x = (random.random() * (1.0 - luck_boost) + luck_boost) * total_weight return items[bisect_left(cumulative_weights, x)] return sample By the way, I wrote something like that once, but the actual code was about three times longer, since doing it like that, with floats as weights and precomputing, immediately opened further possibilities: I had two functions, one for absolute weights (that must sum to 1.0), one for relative, and allowing non-weighted items (with weights assigned uniformly from the remainder, or or as 1.0). Was totally worth it, given how often I called it this way or that and how I was adjusting probabilities.
After hearing from a few people that they tried tmux and never looked back, I tried tmux and ...well you get the idea. 
TAL, XML and Buildout needed for a hello world, it already made me burst into tears ;)
Why divide? Sum up the likelihoods. Pick a random number between 0 and the sum. Go through the items array, and subtract from the random number found until you go below zero. In the case of the Imp, anything less than 60 is gold, 60-89.999999... is a helmet, and 90 or above is a sword. The dude here just found "choice" somewhere and didn't realize it wasn't supposed to be used for weighted things like this.
Classic case of treating the symptom instead of the disease, which in this case is the function itself. This situation isn't unique to Python either.
He states threads can't be killed in python "for good reason". Does anyone know what that reason is? Interpreter issues? To much variation in platform behavior to be reliable? Resource management?
The one reason he mentioned in the article was that if the killed thread held the GIL, it will cause deadlock. That's more of an interpreter implementation issue than anything. There's no reason a kill method couldn't be written that properly cleans up before killing the thread.
Thanks for the constructive criticism. I decided to just make a self.textencoding for connection objects that defaults to UTF-8, but you can change at any time or override via an extra encoding parameter on functions that send text to the server. My apologies that my commit messages are always cryptic because they are written too fast and barely end up being English by the time I press enter.
Just pre-calculate if the code will return within a set amount of time, and you should be set, [right](http://en.wikipedia.org/wiki/Halting_problem)?
Yeah, that's what made me wonder... it seems like a thread.throw() method similar to generator.throw() would be safe and simple to implement... your thread might not immediately exit from a C-level call, but once it hit the VM it should be possible to exit safely. In fact it seems *way* to simple, which makes me suspicious I'm missing something more fundamental / subtle about why it hasn't been done.
I *dislike* Ruby and I wouldn't touch it with a ten foot pole, but I have no problems using Homebrew. I've tried everything else and everything else just sucks balls compared to this. At the end of the day it's about using the right tool to get the job done. Everything else is just noise.
Save it as .pyw, it won't pop the console then.
&gt;Another reasonable solution is to make the computation cooperative, i.e. call back on the invoking code occasionally asking if it’s time to finish. This is a technique well familiar to GUI programmers, where a function invoked from the GUI main loop should not run for too long, and should break its work to chunks. As far as I'm concerned the above is the only correct way to use threads. The only exception is if you're doing blocking IO, but even then you really should be using non-blocking IO instead and implementing the above. At least in the IO case, you won't be consuming CPU while waiting for the IO, but you may be consuming other resources needlessly. It's really easy. You don't even need any complex threading objects or semaphores or anything. All you need is a bool on each thread object. The main thread can set it, the worker thread can read it. As soon as the thread notices it gets set, cleanup and exit, otherwise, just keep on working. If there's a large delay while waiting for the thread to finish, it's likely you're just not having your thread checking it frequently enough. If you're using someone else's threads and they don't have a cooperative way to make them stop, that's the fault of the person who wrote that thread, and you should complain to them.
It's not recommended to kill threads in *any language*, not just Python. The reason for it is that threads usually hold resources, and killing them leaves these resources in unspecified state. This can cause all kinds of problems.
That doesn't preclude killing threads via an exception at the VM level... eg, a call similar to generator.throw(). This would allow normal cleanup to occur at the VM level, and there wouldn't be any unspecified states or leaked resources. Such a mechanism couldn't interrupt the thread during a C-level call of course, but as you pointed out, that's pretty much out of the question anyways. I remember seeing such a scheme grafted on by a Erlang-style message passing library for Python... it had a ThreadExit exception that could be raised at "any" time, and sent from another threads. The catch was that your code had to periodically call a special check() method to see if an exception had been thrown at the thread and needed raising, since the VM wouldn't do the check itself (hence my quoting of "any", due to the implementation limitation).
I just don't understand the purpose of this framework. The thing that caught me up front was that you're re-inventing the wheel in a lot of different areas rather than using current technologies that are considerably more mature. As a user of frameworks, I could care less if it's new or old -- I'm looking for a framework that utilizes mature technologies (so I know it works) and does it in a novel user-friendly way (so I don't have to know HOW it works). One of the reasons the other frameworks are complex is a result of that maturing. Some of it is that they spent a good chunk of time just hacking away at things. And they have not spent some time to go back and create a better API. New microframeworks, though, would be better accepted in the community if they focused on using existing technologies and made those technologies easier to use. A good example of this is Elixir -- where it adds a layer on top of Sql Alchemy and it makes a number of tasks much easier.
Excellent talk.
&gt; def timelimit(timeout, func, args=(), kwargs={}): Gah! I posted a comment on that page about why setting mutable types as a default arg is terrible practice. Just so people here don't go off and do this, def foo(a, b=[]): b.append(a) print b foo('test') foo('test2') The output will be: ['test'] ['test', 'test2']
Yep, although it's better to use a threading.Event instead of a simple bool.
&gt; My apologies that my commit messages are always cryptic because they are written too fast and barely end up being English by the time I press enter. Slow down? No commit message is sometimes better than a cryptic, written too fast one, but the best is a well written one.
"Show"? &gt;&gt;&gt; t = re.split(r"(\d+)", field_text) "t" should then hold alternating groups of numbers and nonnumbers. Do with that as you will. &gt;&gt;&gt; re.split(r"(\d+)", "213a sdf9876,m nb") ['', '213', 'a sdf', '9876', ',m nb'] 
 &gt;&gt;&gt; t = re.split(r"(\d+)", "213a sdf9876,m nb") &gt;&gt;&gt; t[1::2] ['213', '9876'] &gt;&gt;&gt; t[::2] ['', 'a sdf', ',m nb'] 
You might as well have typed that in an ancient dog language. I think I need to pick up a book on how2python... thanks for the help, though
You're better off asking field calculator questions on the official Esri forums or on gis.stackexchange.com.
I got sent the "read a tutorial" comments ;p
Then windows task scheduler ping in the background
The online documentation for python is pretty good. Look to the right on an r/python page. What chadmill3r is doing is using [regular expressions](http://docs.python.org/library/re.html). 
thanks, I'll look into it!
Regular Expressions are a language for parsing kinds of text. "Regular" is a technical term, here. "re" is the module that supports it in Python.
Step one, open a terminal and run "python". "import re" "field_text = 'Stuff 987987 you care 1123 about'" "re.split(r"(\d+)", field_text)" if you have problems with this, then the entire exercise may be far away from your abilities for a while. The hard part, btw, is deciding what to expect from the "fields" you're talking about.
And did you read tutorials?
Yes and most is going over my head. Just want to separate numbers and letters (&gt;&lt;)
&gt;"field_text "field_text" is the name of my field? &gt;'Stuff 987987 you care 1123 about' I have no idea what you mean. 
You seriously shouldn't be bothering other people if you don't know what this is. &gt; field_text = 'Stuff 987987 you care 1123 about' Get thee to a Python tutorial now. Don't come out until you finish it and understand it all. http://docs.python.org/tutorial/
There is no such thing as "just" here. if you think it's simple, you're wrong.
I'm still at a point where I hhhjjkkkklllll around the screen too much to make that shortcut, but I do like it.
I've always wondered why other languages _had_ information hiding. Yes, I know the answer that will get me credit on the test ("so that other people won't mess up your data/objects/whatever"), but so what if they do? Would it be so terrible for MY code to crash when YOU force it to? I mean really.
Welp, thanks for that. &amp;#3232;\_&amp;#3232; 
Given a sufficiently large team, Person A can cause Person B's code to crash by inappropriately accessing Person C's code, leading Person B to blame Person C and Person C to scratch their head.
What about raising an exception in the context of the thread you want to kill? And wrapping a handler for that function around the thread's code? This seems to address the resource issue. I recently implemented this, and I felt OK about it. The tricky part was raising the exception. You can use ctypes to directly invoke the Python API to raise the exception in the correct thread context. See ( http://stackoverflow.com/questions/323972/is-there-any-way-to-kill-a-thread-in-python)
Sorry, I should have explained that some. imap means it only applies when in insert mode, as this shortcut is used to escape to command mode. The idea is that you'll never (or very rarely) have to type a word with 2 j's in a row.
Wow, there goes two hours down the drain. At least it was good practice, and now I know :)
Given a sufficiently large team information hiding will not prevent this problem.
That has just simplified loads of my scripts :)
I'm trying to slow down; I've been using the terminal for a long time and git not so long. I guess sometimes the caffeine gets the best of me. Thanks for the comment.
Urk, you're right, should have plugged my brain in before posting, but it can depend on your VM and how your language compiles to it. If the loop body compiles to a lambda and those calls have no side effects then they may be culled by escape analysis, it's certainly possible to write code that is definitely not loop invariant and will still get happily analyzed away, at least on the jvm.
It's a static vs dynamic sort of thing. Encapsulation in a static language means that it's impossible to even compile a program in which you would cause my code to crash. That's less useful in a dynamic language - what would we get, an encapsulation exception?
is there any progress on twisted for python 3?
Truthfully, I haven't really been following its development that closely. 
If you're taking CS classes, hang on to this script. I've had at least two classes that required you to count the number of words in a text, and then do something with it.
Not python related at all, but some ISPs use to ping the clients every couple of minutes and would end the session if client won't reply. Check if your router replies to pings (ICMP echo reply packets from router to outside world). Obviously, it should also accept, not block, pings from outside too (ICMP echo request packets from the outside world to router).
fishdicks [comment](http://www.reddit.com/r/Python/comments/jplbg/large_lists_of_special_cases/c2ebb57) was by far the best and I recommend you consider developing along similar lines.
This is good to know. I ran into this once and couldn't figure it out fot a while. If I remember correctly it's because the default parameters are assigned at function definition and not when the function is called. So if they are mutable and you change them.. they'll be changed for the next function call.
My take on is is that information hiding has always been about decreasing coupling, which is a good thing for the most part. So, it's not just about crashing. It's about keeping dependencies to a minimum, which prevents crashing, maintenance nightmares, and other unintended side-effects. If I make it difficult for anyone else to modify the internal state in my objects outside of the public members, then I can much more easily change my classes and provide you with future upgrades with only minimal issues. OTOH - If you're accessing all my private members (sounds dirty, I know - and it should), then your chances of seamlessly upgrading to my new classes are substantially decreased. Now, you could argue that we can't protect everyone from idiots, etc. and that's all true, but the point here is that information hiding makes it harder for me to increase coupling and increase the probability of unintended consequences. The fact that Python doesn't really prevent this is in line with it's permissive culture really. Because, despite the fact that Python is anti-TIMTOWTDI, it's still quite permissive in light of languages like Java and C#. It just isn't as permissive as Perl, and tries to strike more of a happy medium. 
How about a contract violation exception? That would be very useful.
Not wasted time at all! We all reinvent the wheel all the time, it just makes us better coders.
Just to offer a different take on this classic problem (as off-topic as it might be) - in bash: for w in $(cat &lt;filename&gt;); do echo $w; done | sort | uniq -c | sort should do the trick. e.g. $ function count { for w in $(cat $1); do echo $w; done | sort | uniq -c | sort } $ count &lt;(curl http://www.google.co.uk) 1 - 1 })(); 1 &amp;&amp; 1 0;c=b}_gjwl.href="/search?"+a+"&amp;cad=h";return 1 0}function 1 0"&gt;&lt;input # ...snip... 2 more 2 nowrap 2 Search" 2 solid 2 to 2 type=submit 2 var 3 0 3 type=hidden 5 onclick=gbar.qs(this) 9 &lt;a $
FYI, Graeme Cross won best talk with "Ah, I see you have the machine that goes 'BING!'": http://www.youtube.com/watch?v=nzCvomTixzU
Nah, I believe your way is faster. Pythons regex matching is notoriously slow. http://stackoverflow.com/questions/2563329/regex-matching-very-slow Also...you code like Dr Suess. I would not eat green eggs and ham, not with a tuplething, or even with a sillystring...
That's the whole concept behind Design By Contract. Some languages have support for this. You can define a contract into an interface, and inherit contracts and stuff. Pretty cool. I forget what language has this. Eiffel, I think. ED: just googled it. Yup, it's Eiffel. http://en.wikipedia.org/wiki/Design_by_contract
Yup. Could probably find a Python lib to do the equivalent if I weren't feeling lazy, but I've always felt that DBC should be baked right into the language to be really effective. OTOH - A lib could cover this nicely and play nice with others given some design forethought.
I don't know why but your Dr Seuss comment really made my day haha, thank you. Yeah I'm rewriting the code as I type this to use a Counter object, I'll update it in a few minutes.
Not yet, I'll be starting soon. Will do however, thanks for the advise :)
I prefer tuplething and sillystring. :) I know...mixed signals and all...
sort -r to put the more common occurrences on top.
&gt; punct = re.compile (r'([.?;,!"\\//])') Split on non-word characters, perhaps? r"\W+". Or, if that's too greedy, make a character-class with all the stuff you want in a word, negated. [^-A-Za-z]+
Well -rn works (no need for double dashes), but since he did uniq -c, the first column will only ever be numbers, and adding the -n will slow the sort down.
No, I'm keeping those, I meant I'm updating it to use Counter so it'll be faster :)
To be honest, I prefer the more common occurrences at the bottom - since that is the last thing that will print; I don't want to have to scroll up to see what the most common occurrence was.
I'm sorry, what is 'final' for?
whimp. try weblogic for real pain
I see what you did there :)
Take a look at it now if you'd like, it's incredibly fast now, parsed through Ulysees in a second unlike my one that did it in a little more than a minute. Thanks, collections is great.
because you don't need it.
One thing I would like to have better support for is proper class-local variables, i.e. name-mangling without the string programming and instead by object identity. This (and name-mangling) is useful for example with mixins. Such variables certainly don't have to be *hidden* from the outside, though.
no, that's what tests are for.
Wait I'm not sure I understand what's happening
 #define private public There, *that's* solved those compiler errors.
If you don't mind, could you tell us what all you did to create this. What technologies/frameworks did you use? Thanks!
Sure, man. Django 1.3, Python 2.7. Run at uwsgi (1 master, 1 worker) through nginx. Tweepy 1.7.1. jQuery 1.6... Looks like it. There's not much code of course. The JS part is much bigger :) Nothing special. It's just an AJAX call in a nutshell to Django and to authenticated Twitter instance.
Awesome! I haven't worked much with all of that stuff, but would definitely be interested in doing some of it. Any resources you can point out?
Try: import abc
Well, I don't really know how to answer that kind of questions :) If you know whatta mean. At some point 3 years ago I decided to get over some framework and language. Couldn't understand a word on the PHP docs site, even didn't bother watching the CakePHP, Kohana, Igniter and other stuff. The RUBY and ROR didn't even fit in my puny little head. Django site - perfect. Python docs - perfect. Of course the level of entrance to Python is much lower but to me there are more pros to that than cons. There was a time btw when I stopped making websites using the language I had to use on the first job. [Parser3](http://parser.ru/en/). That's a disaster if you wanna make some useful tasks, but it's kinda easy to get in. Then I just decided not to use it anymore. For example cause there's no availability to send mail over auth SMTP. tl;dr — Make a random task you'd like to do. Do it using Komodo IDE and Docs. Nothing special really. EDIT: at some point I understand that I haven't answered your question, not even close to that :) Gotta sleep a bit. I'll be back.
He didn't addressed the question and blatantly told OP to use another editor. OP wasn't asking suggestion for another editor.
The arguments presented are bunk. &gt;The main reason for making (nearly) everything discoverable was debugging: when debugging you often need to break through the abstractions (since bugs don't confine them to the nice abstractions you've created for your program :-) so I though it would be handy to be able to see anything from the debugger. And since the debugger is written in Python itself (for flexibility and a number of other reasons) I figured the same would apply to other forms of programming -- after all, sometimes debugging doesn't imply using a debugger, it may just imply printing a certain value. Again, too much data hiding would make things more complicated here. Better than debugging is not writing bug-prone software in the first place through defensive good coding practices. One such practice is to avoid allowing users to mess with more state and functionality than they should be playing with--minimizing apparent complexity is a good thing. &gt;The other observation was that even in C++, there are usually ways around the data hiding (e.g. questionable casts). Which made me realize that apparently other languages could live just fine with less-than-perfect hiding, and that hiding was an advisory mechanism, not an enforcement mechanism. So Python could probably be just fine with even-less-than-perfect hiding. :-) In some languages, you can circumvent hiding information, therefore we should design languages where you cannot hide information. Conclusion doesn't follow from premise. I don't think either of those reasons, unless you really dig around in them for some deep meaning, provide sufficient basis for totally disregarding information hiding. Hiding information is a way of managing complexity. Python does it through developer convention anyway in any project worth its salt. I tend to prefer the programming language do such book-keeping instead of the developer, but perhaps this is misguided? Does anyone have a more detailed analysis? I feel like I'm missing something simple here.
LOL, no arguments there.
Oh, awesome! Yeah, even Ctrl-C is pretty unwieldly. I can't imagine people actually pushing &lt;esc&gt;. Definitely going to start using imap jj.
No worries. Your answer is fine, I just need to start something is my problem...
Great site, really.
There's really very little to build upon. I'm not sure what wheels I might be reinventing. As far as I know, there aren't any other libraries for Python that do the same thing as mine.
Too main()stream.
Cool, looks nice and seems nicely done. Couple of things i dont understand. How are so many tweets in spanish and who the fuck is jdelacueva? And third. from where did you got the idea? i've been wanting to learn python and Django for a while now but i never feel motivated to do anything. Little projects like this are perfect, but i seem to lack inspiration. 
Well, abstract base classes are a nice addition to Python, and I guess you could say they go hand in hand, but it's not really the same thing. Key to DBC are [preconditions, postconditions, and invariants](http://en.wikipedia.org/wiki/Design_by_contract). Looks like there's a couple of DBC implementations available for Python as libraries, but they're not part of the standard install AFAIK.
I mentioned this in the post: &gt; The same is true for approaches attempting to raise an exception in another thread – the exception will get ignored if the thread is busy inside some C call.
&gt; Better than debugging is not writing bug-prone software in the first place As soon as we're able to write bug-free software and never need to debug, we'll call you for advice on language design. &gt; In some languages, you can circumvent hiding information, therefore we should design languages where you cannot hide information. Conclusion doesn't follow from premise. Because you left the argument out: "other languages could live **just fine** with less-than-perfect hiding, and that hiding was an **advisory mechanism**". &gt; I tend to prefer the programming language do such book-keeping instead of the developer, but perhaps this is misguided? Not in the slightest. It's a valid preference. But it should be easy to imagine the alternative preference.
True. If time is a factor, then design your function that way.
Why does he keep saying "Pythen"? Edit: Excellent talk!
idiomatic python for this is the following CHAR_ZERO = ord('a') - 1 # so that 'a' is 1 def character_sum(teh_string): return sum(ord(c) - CHAR_ZERO for c in teh_string.lower() if c.isalpha()) def main(): name = input("What is your name?") print character_sum(name) if __name__ == '__main__': main()
C++ has _real information hiding_? I can't believe that people believe this nonsense.
&gt; The arguments presented are bunk. Actually, it's you who've completely missed the point. &gt; Better than debugging is not writing bug-prone software in the first place through defensive good coding practices. It doesn't matter how good your code practices are, there will always be a need for debugging. Your point only make sense if the need for debugging can be completely eliminated (which it can't). The flip side to this is also testing. Languages that don't enforce information hiding are a lot easier to test. This is ample reason for not enforcing information hiding at the compiler level. &gt; In some languages, you can circumvent hiding information, therefore we should design languages where you cannot hide information. Conclusion doesn't follow from premise. You've created a strawman argument here. The conclusions that you state are not the conclusions drawn by the OP. The OP points out that information hiding isn't enforceable at the compiler level in C++. He didn't conclude that we should not use information hiding, he merely pointed it out that languages that don't have compiler level enforcement (this includes C++) still work fine, so compiler enforcement probably isn't as big a deal as some make it out to be. Information hiding is a _design_ property not a language property. Regardless of whether the compiler attempts to enforce encapsulation or not, programmers can design for it. &gt; Hiding information is a way of managing complexity. Python does it through developer convention anyway in any project worth its salt. I tend to prefer the programming language do such book-keeping instead of the developer, but perhaps this is misguided? Considering that no programming language can truly enforce encapsulation, I think you're kidding yourself if you think that information hiding isn't enforce by programmer convention in every language. Have you ever used a getter or setter? If there are no constraints on these methods, then encapsulation isn't being enforced no matter how much you think it is. 
Yeah, the dictionary idea was dumb. I mentioned it because I used a similar cache with only a couple of keys (so I'm guaranteed to request a key within a 1 minute period), so I just stored expiration with each key and invalidated on get.
Well, Encapsulation is all about trust. In a static language I can look at a private variable and know that I don't need to worry about what this variable does outside of an object. I'm more confident in any changes I make that manipulate that variable because the compiler assures me that it won't be used outside of that object In a way I don't know. In a dynamic language you don't really have that compiler reassurance so you have to trust your fellow developers by having stronger coding conventions and test driven development. I'm not sure if this kind of thinking is just me but let me ask you something. How often do you grep a variable to see where it is being used?
&gt; I've always wondered why other languages had information hiding. Yes, I know the answer that will get me credit on the test ("so that other people won't mess up your data/objects/whatever") There's one major reason with three interrelated facets. * It denotes interface. Using `_` prefixes forces you to articulate how others should access your classes/functions. * Better documentation. It's much easier to read code and `help(code)` when implementation details are so articulated (and in the latter case, so hidden). * Stable API. If you don't indicate what parts of your code are for public use, other programmers may come to rely on implementation details. This can cripple a project, because effective refactoring becomes impossible. Really, I think failure to use information hiding is worse than failure to write comments. This has nothing to do with openness and freedom; it's about maintainability and not wasting others' time.
I love the top comment: &gt;I always described this as "we're all adults here" information hiding. So pithy, so on point.
&gt; Dunno how useful this page is... Considering how useful twitter is in the first place, not very. Fun project though!
Open source or gtfo ;)
Now we know, the daily tweet limit is 255.
&gt; Exactly. I've several times been slammed for saying this on stackexchange. The data hiding part of OO theory sounds good in theory, but turns out to in practice be hurtful. Erm.
Is it the Aussie accent? As a local I kept wondering the opposite, why the invited American keynotes kept drawing out the second syllable into "pie-thon". I guess I need to get out more :-)
Anonymity is important. Also, nice interface.
Suggestion time, because I know we all struggle with where to take things next when learning from time to time. 1. You can make a refactor on code like this, with a brief glance I saw you re-coded this in several places. You only need to define it once then call out from wherever you need to use it.: def contains(self,words): wordKeyList = [] #Compare everyword with everyword in every #Dic entry for arg in words: for k in self.ids: for word in self.ids[k].split(): if(arg.lower() in word.lower()): wordKeyList.append(k) return wordKeyList 2. Learn some of the standard library modules, e.g. in handleArgs() you could use [argparse](http://docs.python.org/library/argparse.html) instead of DIY. You can also write the code in #1 above in a much better way. 3. In WikiTableScraper.populateData() you have overridden the builtin list object - don't do this. 4. Same function, if you consistently require the stripped version of a string, strip() it when you add it to your data store, don't strip() it each and every use when you pull it out. 5. Add support for other parsers than BeautifulSoup. Not saying there's anything wrong there, just making an exercise. Bonus points if your new WikiTableScraper will simply just work with whatever parser the user chooses. (hint: see how code like https://github.com/defnull/bottle/blob/master/bottle.py supports multiple template frameworks...) EDIT: stupid markup!
VLC runs on Mac/Win/Linux and as far as I know can do stream serving just fine. All it would need is a good web based front end that can run it.y Now for a small, private server with limited access, you could probably mock it up in a day or two. The problem is that if you begin to see lots of traffic, a single server is not going to be able to keep up with the transmission rates. The Internet itself (TCP/IP) was designed in a mental model of scarcity, hence scaling this is particularly hard. Perhaps a "Granny VJ" system ("My Grams has her own webcast!") on Linux would be a good target. 
&gt;no, that's what tests are for. How else do you think they'll achieve the crash if not through a test? Magic? Or they'll just run everything in a debugger and just start chewing away until it crashes, then claim they found a bug?
All your Spanish tweets are belong to somebody :) The idea? Dunno, just thought about an anonymous twitter. I'm not learning Python or Django. I'm using them both for about 3 years. Greatest things.
Bitch, please ) http://dpaste.com/hold/600984/
The author. http://anatolyzenkov.com/
Awww, snap... Daily capacity limit. The fuck is that for.
&gt; Python is anti-TIMTOWTDI I have no idea what that stands for, but it just sounds like something Python would be against. :)
Doesn't look too bad. I'd recommend using the lxml module instead of BeautifulSoup. It's a bit different and will take some time to learn (and it uses XPath a lot), but its HTML parser is much faster than BeautifulSoup's, for whatever reason. I stubbornly stuck with BeautifulSoup for all my urllib2-related projects for a long time, but once I actually understood lxml I started using it for everything else and it's definitely quite a bit faster.
I've just found that: *The Update Limit of 1,000 updates per day is further broken down into semi-hourly intervals. If you hit your account update limit, please try again in a few hours after the limit-period has elapsed.* I suppose there's no way to *hack* that issue...
Well.. it could have been "SheetGrid" to work with xls/ods/others. But nevertheless it's cool. 
TIMTOWDI = There Is More Than One Way To Do It. Python generally promotes this instead: &gt; python -m this | grep way There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. 
&gt; It's a static vs dynamic sort of thing. It has nothing do do with static vs dynamic. Smalltalk had no concept of public instance variable (they were all private, always), although it had no private/public split on methods either (apart from advisory). Ruby does have public v private methods. &gt; That's less useful in a dynamic language - what would we get, an encapsulation exception? You'd get an attribute/method access error (`NoMethodError` in Ruby for instance), since what you're trying to access does not exist for you.
Apparently, it's "There is more than one way to do it". Is it true that Python is against that?
Actually, BeautifulSoup 4, which is being developed, will use lxml* to do the parsing. So you get the speed and the convenience. And Python 3 support. * lxml or other parsers. But I guess everyone will use it with lxml.
Good to see they've sorted out the doc's lack of signal/slots. I still think the formatting of the doc's still quite unparseable.
It is really like learning a new language and way of thinking.
Not really concerned about bandwidth. As i said, the streaming component would typical hand off to a service like ustream. VLC is not capable of many things you would typically find in apps like wirecast. Things like multi-device handling, layouts, transitions, and effects are lacking. These are the things i'd like my python app to provide in some fashion.
&gt;Because you left the argument out: "other languages could live just fine with less-than-perfect hiding, and that hiding was an advisory mechanism". Good point. I was taking his arguments too literally, I think. The real point is that information hiding in other languages is basically a suggestion enforced to a limited extent by the compiler. This suggestion isn't particularly more meaningful than having information hiding done by social convention.
&gt;The flip side to this is also testing. Languages that don't enforce information hiding are a lot easier to test. This is ample reason for not enforcing information hiding at the compiler level. This is a really good reason. I agree. &gt;You've created a strawman argument here. The conclusions that you state are not the conclusions drawn by the OP. The OP points out that information hiding isn't enforceable at the compiler level in C++. He didn't conclude that we should not use information hiding, he merely pointed it out that languages that don't have compiler level enforcement (this includes C++) still work fine, so compiler enforcement probably isn't as big a deal as some make it out to be. Information hiding is a design property not a language property. Regardless of whether the compiler attempts to enforce encapsulation or not, programmers can design for it. The argument originally sounded like "Because a law cannot be uniformly and regularly enforced, it should not exist." The OP wrote in response to "why doesn't python have information hiding", so naturally it makes sense to apply what he says as reasons to not have information hiding. Your elaboration is a bit more effective as presenting such a reason.
the issue with lxml is that it requires additional c libraries which means it won't run on things like pypy or appengine. 
Twitter can be pretty useful.
The author of this article rightfully concludes that locking every single object in Python is madness (and probably fucking slow), but then goes on to conclude that you need a STM. But what about the more obvious solution: No atomic VM instructions. Why _not_ make weaker guarantees than CPython and even Jython? Why not make the programmer do their own locking and just guarantee no instruction reordering across lock boundaries like in other languages? Anyone who writes multi-threaded code but relies implicitly on Python instructions being atomic is asking for trouble IMHO. Most of the time you need your locks held over multiple instructions anyway, and already need to use explicit locks. Locks should be explicit, if just to force people to think properly about how they share their data structures. Plus, it's probably simpler and faster than using STM.
Why indeed. The answer is that PyPy team is apparently mortally afraid of making *any* "language change". That is, "we implement Python language; what Python language is is not our business; it's BDFL's business". And non-atomic bytecode certainly qualify as a language change in this point of view.
Does the BDFL really have to say something about atomic instructions in Python-the-language, or is it considered an implementation detail? Couldn't they just take the easy way out and *optionally* compile in a GIL for code that assumes its existence?
I am not sure how that works... Currently, you need to consider all Python codes to assume GIL's existence unless proven otherwise. How does this "optional GIL" thing work?
I think the article could have done a better job of making something clear: it *is* an implementation detail (see Jython). However, PyPy is attempting to be something more than just a mere implementation of Python. It's attempting to be a drop-in replacement for CPython. As such, changing the guarantees that the CPython implementation makes would not be acceptable. 
useful... for what? Tell me- when was the last time you logged into twitter to read tweets. Answer in most people's cases is never. They only think of it as a 'useful' means of sending information out. Information that nobody will see because nobody reads tweets. Twitter surpassed its saturation point long ago and by now is of no import whatsoever. 
Because 1. I assume that "weaker than Jython" means "multithreaded programs can segfault or maliciously corrupt the interpreter's memory". Which is a _really_ huge and _really_ questionable step away from CPython. On the other hand, if you want to provide the guarantee that anything of the sort is impossible at least, it's already strong enough to be very hard to provide. 2. Lots of frameworks use various metaprogramming techniques. Should "protecting shared structures" involve protecting all their respective inheritance hierarchies, module dictionaries etc? Wouldn't it turn out to be an inefficient and probably incorrect reimplementation of GIL, in practice? 3. With all these problems, why would anyone have any incentive to not use `multiprocessing` instead?
About two minutes ago. I go on it every other day or so, and read anything that catches my eye. It links me to a **lot** of interesting info/blogs/whatever. I rarely, if ever, tweet myself.
you don't understand how testing works, I'm not talking about running the program and see if it crashes, I'm talking about automated tests that use `assert` to comprate expected outputs with the ones you're getting on the current code. see: http://docs.python.org/library/unittest.html
&gt; Currently, you need to consider all Python codes to assume GIL's existence Nope. There is code out there which is broken on Jython/IronPython because it relies on GIL doing the locking, that code is considered broken. On the other hand, all Python code pretty much assumes a bytecode instruction is atomic.
Thank you! You read my mind through my code. I haven't touched python since writing this little program so I appreciate the suggestions. I have a small question/favor. You seem experienced in python, so Im guessing that you are involved in one or two open source projects. Could you point me to one that has source code I could learn from and maybe...eventually ...contribute to it....even if just a patch? Again, thank you so much for the comment!
With geo location it's much more funnier.
Twitter is not meant to be "logged" in btw. It should be an SMS only initially so you wouldn't need the site to reed tweets, only for managing followers and stuff. But, money takes the power — you got the website and lots of apps for that.
Furthermore, they're interested in not just a Python solution, but a solution that automatically works in any language defined using Pypy/RPython.
[PEP 20](http://www.python.org/dev/peps/pep-0020/): Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those!)
The are widely accepted guarantees which the community at large relies upon. They are basically a part of the language although implicitly agreed upon. If you break with them your implementation becomes significantly less interesting, especially for existing projects.
I like how you're maintaining such a good tone in the discussion. That's rarer and rarer these days, it seems. &gt;"Because a law cannot be uniformly and regularly enforced, it should not exist." Isn't this a good principle, though?
**tl;dr** Python has to do many more operations than you think in a simple loop, and Numpy can use high-performance libraries that are insanely fast (much faster than naive C code, even C code written by a very competent programmer). Something that hasn't been mentionned: looping in Python is *very* slow. And this is perfectly normal, given the fact the a Python loop is very complicated. Let's look at a simple loop, a and b being lists : for i in xrange(len(a)): b[i] = b[i] + a[i] A Python loop is something along the lines of: while True: try: x = iter.next() # loop body except StopIteration: break Which, as you can see, is far from straightforward. Especially since you need to call a method to get the next element out of an iterator, and calling a method in Python is at least one dictionnary lookup, plus many other things. Also, every time you do a + b in Python, the interpreter has to figure out what types are a and b. This is necessary since the "+" operator can work on several types, and does not do the same operation all the time. For instance, for strings and lists it concatenates them, whereas for integer it adds them. But this is not that simple ! You can be forced to convert a type into another (think of 1 + 2.5, that converts an int to a float), plus the fact that numbers are objects in Python, and the fact that integers are "big integers". You could argueu that you have to check only once, since the type is not going to change within the loop, but you're wrong ! 1. A list can contain elements of different types 2. You can change the type of an element in the list while iterating it Add to that that Python lists are not simple arrays like in C, and you understand why doing a simple addition is very costly. On the other hand, with a numpy array, * You know that all the elements have the same type, typically float()s * You know that the size is not going to change * The backing array is a simple C/fortran array, contiguous chunk of memory So you can do the loop in C, without the crazy overhead, and even better, you can use the highly optimized libraries like BLAS to do the heavy-lifting. To give you an idea, a current processor can do ~4 operations per cycle, so 12x10^9 operations per second for a 3GHz CPU. It requires extensive knowldge of the CPU to achieve that, and I am certain that the naive matrix multiplication you could code in C in 10 minutes would yield less than 10% of that. Yet high-performance BLAS library such as MKL (made by intel) can give you upto 95% of that. So you **can** even be **much faster** with Numpy that in straight C.
&gt; Lots of frameworks use various metaprogramming techniques. Should "protecting shared structures" involve protecting all their respective inheritance hierarchies, module dictionaries etc? Wouldn't it turn out to be an inefficient and probably incorrect reimplementation of GIL, in practice? You really only have to do it for data that is not read-only. I would for example say that it's pretty rare for classes to change after they have been set up for the first time (presumably before any threads are even started), making the class basically read-only, which could be safely shared across threads.
I'm still pondering about your first point, and I'm thinking that there must be a way to avoid segfaults without having to lock everything. What is the JVM doing for example?
&gt; Why not make weaker guarantees than CPython and even Jython? Good question. The reason is that it is practically impossible to write correct code without these guarantees. If you want code that "probably works, unless you are unlucky with your threads" then you've got that today. If you want code that is correct, then locking is not sufficient. &gt; Why not make the programmer do their own locking and just guarantee no instruction reordering across lock boundaries like in other languages? Actually, other languages DO make stronger guarantees than you are proposing. For instance Java has a very well-defined model for its concurrency guarantees so it makes a good example. In Java, caller are guaranteed (I am simplifying a bit) that reads from and writes to references and elementary types other than double or long will perform atomically. In Java, if you say "**int x = myObject.currentValue**" you are following a pointer and retrieving an integer. Even with no locks in place, you are guaranteed to get SOME value that was stored in the currentValue field (perhaps an old one, but SOME sensible value). In Python, if you say "**x = myObject.currentValue**" you are performing a hashtable lookup. Without guarantees about threading you are not guaranteed to get back a valid Python object: perhaps another thread was in the middle of adding something to the hashtable and at the moment you accessed it you were pointing to a bucket containing uninitialized memory!!! If you cannot even rely on the primitive operations in the language to be safe, then there is NOTHING that you can write which will be safe.
I think "mortally afraid" is a poor way of phrasing a strong commitment to 100% compatibility. As a user, I find that 100% compatibility to be enormously useful. Once PyPy accounts for 60% of usage and C-Python accounts for 10%, perhaps it will be time to change that policy.
And how would the interpreter _guarantee_ that after some point all classes remain read-only?
&gt; Locks should be explicit, if just to force people to think properly about how they share their data structures. Plus, it's probably simpler and faster than using STM. That's the typical "worse is better" approach: make things simpler and easier for the language implementor, at the expense of the language user -- who now, in addition to worrying about the logic of her program, must also worry about locks and race conditions. Python has always tried to hide details like that from the programmer. You don't have to define your own hashing or lookup functions for dictionaries, you don't have to define your own sorting function -- highly optimized implementations of those are provided for you. Indeed, the GIL exists in part for the same reason, so the interaction between C and Python becomes far simpler. I see STM as in a similar vein.
JVM is using locks.
If you read the article and unsure how STM can be a replacement for locking, what are the conditions for rollbacks and all that, maybe an article I wrote some time ago can help: http://self.maluke.com/txns
Since there are probably more pythons in Australia than the U.S., it's probably the "more right" way to say it ;-)
This is messy. Writing class and functions helps. 
I get excited to see stuff like this, Oh yeah a new release of something that looks cool! What? you don't support python 3.x? Get off my lawn Grandma!
You can use *enumerate* to get cycle count when iterating over iterable. for count, thing in enumerate(['a','b','c']): print count, thing &gt; 0 a 1 b 2 c
"I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question." ~Babbage
Yes, it is messy. Hopefully I can clean it up with class and functions. I haven't learned about class yet. Thanks for the comment.
&gt; In Java, if you say "int x = myObject.currentValue" you are following a pointer and retrieving an integer. Even with no locks in place, you are guaranteed to get SOME value that was stored in the currentValue field (perhaps an old one, but SOME sensible value). This is not true for numbers of type long. Also not for doubles. See http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.7
exactly. pyqt supports python 3.0 great. 
I really like this post, it's very clever! I have some suggestions however. First, you're actually cheating a lot using random.bytes. The bytes example is only generating 50k uint16s while random_integers is generating 100k int64s (you're generating 100k bytes, but each uint16 takes 2 bytes). So the bytes example is only generating 1/8 the amount of random data. If I correct that, using random.bytes is actually slower (and doesn't even produce numbers in the right range): %timeit np.random.random_integers(low, high, N) 1000 loops, best of 3: 827 us per loop %timeit np.frombuffer(np.random.bytes(8*N),dtype=numpy.int64) 1000 loops, best of 3: 1.07 ms per loop That's not to say it's not useful: in many cases you don't need random 64 bit integers (especially if they're, e.g. indexes) so you can gain a lot of speed by using frombuffer and a smaller dtype. Also there's a much more efficient way to mangle the random bytes into the range you want. Here was your code: import numpy as np N = 100000 low=0;high=1000 uint16_max = np.iinfo(np.uint16).max k=np.frombuffer(np.random.bytes(N),dtype=np.uint16) %timeit np.uint16((high-low)*(k/float(uint16_max))+low) 1000 loops, best of 3: 459 us per loop (result from my machine) and the faster way: %timeit np.mod(k, high-low)+low 1000 loops, best of 3: 230 us per loop By avoiding doing both division and multiplication it's about twice as fast. 
Since their "About" page doesn't, can anyone give me a brief overview of why I would choose PySide over the normal PyQt4 bindings?
Excellent point. I use enumerate frequently, but forgot that it is a generator, not a simple function.
holy cow! thanks for that, np.mod looks awesome. good call on the difference in data chunk sizes, I hadn't considered that.
&gt; Once PyPy accounts for 60% of usage and C-Python accounts for 10%, perhaps it will be time to change that policy. I don't think that matters. CPython is the "reference implementation" and regardless of usage, I don't see why that would change.
Since their "About" page doesn't, can anyone give me a brief overview of why I would choose PySide over the normal PyQt4 bindings?
You can use one print statement with eirther multiline string (triple quotes) or escaping new line (backslash at the end of a line). With escaping new line it's nice to use string concatenation (+) to make it more readable (you can indent the string and it won't contain the whitespace).
Good point. That's why I wrote that x was an int.
The About page does have the answer but it is not emphasised. PySide is licensed LGPL while PyQt under dual GPL and commercial licenses.
I went to look at downloads for PySide and only found the versions for 2.x. But if I skipped this and went straight to Qt i could get it to work with 3.x?
Pypy could conceivably become the new reference.
tl;dr - It's practical for CPython to give no guarantees at all for using dict/list/tuple/etc, if it guarantees that locks work. C is a good example. You can say it gives you a very limited set of multithreading guarantees which are api/library dependent. Eg interlocked_increment, spin locks, etc are guaranteed to work and the microsoft multithreading std library is guaranteed to work when accessed in parallel (which just means they don't use any static/global variables). ...and it works great, you have to know what you're doing and use volatile/interlocked_*/lock when needed. The guarantees are very specific, you as a programmer have to program responsibly and read the documentation to know what's safe and what isn't. So I have to say I agree with ochs. It's practical to write code with less guarantees than cpython gives nowadays. To answer your specific example how a C programmer would approach it - ask yourself "do multiple threads modify **myObject.currentValue**?" and "is it important that **myObject.currentValue** never be corrupt?" if the answer to both is yes, then the programmer (not cpython core-dev) should add a lock around access&amp;modification of **myObject.currentValue**.
Thanks, I'll clean it up so I don't have several lines beginning with print.
pyqt are python bindings that existed before pyside. You can find information here: http://www.riverbankcomputing.com/software/pyqt/intro
I know what tests are for, I know what an assert is. I know what TDD is and unit tests are. I also know what integration testing is. The scenario described by lpetrazickis can be codified in a test, like many other situations. I don't see what's so complicated about this situation. 
This is great. Also, I am hoping they would start to support Python 3.2.
&gt; it is practically impossible to write correct code without these guarantees ...yet most notable operating systems can sustain uptime on busy systems between months to years. Of course, that's no excuse to avoid trying to make things easier, but let's not ignore the gigantic white elephant here: all attempts to strap an STM onto imperative languages, by some very skilled and determined people, has _not ended well_. I can think of two examples that work: Haskell and Clojure. Their semantics are drastically different from Python's. And Rigo's answer to the very real problem given above? "A large number of implementation details are still open for now." I wish him the best of luck...
check out Pete Norvig's universal sudoku solver. http://norvig.com/sudoku.html Beautiful.
Looks like a good start. Use 4-space indents. PEP 8. "while did_change == True" and "if solved == False" are gross. Just say "while did_change" and "if not solved". Use sets instead of lists for your cells. It seems cleaner to use sets all the time rather than replacing them with a single item when you solve a cell. That would change your type tests to length tests. Some difficult Sudoku puzzles cannot be solved without guessing. Find some, and then add the ability to guess to your code. To adequately test your code without going crazy, it needs the ability to read puzzles from a file.
&gt; IF BEING USED AS MODULE: &gt; Make sure 'sys' and 're' are imported. Modules do not work like that. also.. from collections import * Do not do this. from collections import Counter def Parser(textfile,top_x): Why is it called Parser? What is it parsing? def count_words(filename, limit=10) Other than your "Dr Seuss" variable names(which I hope you realize would make anyone who ever had to work on your code want to kill you), you need to not get into the habit of writing "scripts" as written there is next to no opportunity for code reuse here: * What if you just wanted the top word used, without writing it out to a file? * What if you wanted the output to have a different filename? * What if you already had a list of words that were not stored in a file * What if you wanted to pass a custom ignore list? In general, functions that compute things should not also be the same functions that are opening files and displaying output. You shouldn't have a function that opens a file, counts the words, ignores words, and writes an output file, you should have smaller functions that do each of those things. Try something more like this def extract_words(filename): text = open(filename).read().lower() return re.findall('\w+', text) def filter_words(words, ignore=None): if not ignore: ignore = set(['is','if','a','it','the','an','in','of','to','and','that','be', 'his','he','her','on','not','by','s','ch','are','this','as','for', 'was','with','which','or','for','from','i','you','at','when','have', 'but','may','they','their','be','who','your','says','said','all', 'him','1','2','3','4','5','6','7','8','9','t','o','-','_']) return [word for word in words if word not in ignore] def count_words(words, limit=10, ignore=None): filtered = filter_words(words, ignore) top_words = Counter(filtered).most_common(limit) return top_words def show_top_words(filename, limit=10): words = extract_words(filename) for word, frequency in count_words(words, limit): print("%15s : %s" % (word,frequency)) if __name__ == "__main__": filename = sys.argv[1] limit = int(sys.argv[2]) show_top_words(filename, limit)
It's very impressive. I hope I can understand it someday.
Thanks for the pointers. I will research sets.
I totally thought about "Worse is Better" when writing my comment. I actually do think that Worse really *is* Better, at least here.
Are you sure? When I had to write some Java, you pretty much had to use explicit locks (via "synchronized"), which is pretty much how you would do it in C (i.e. no implicit locking). It doesn't seem to me like the JVM gave any guarantees what would happen if you forgot to properly synchronize, yet it didn't ever segfault on me. I would be thankful for any insight, because I'm not very well informed about Java-related stuff.
It wouldn't. The programmer would, like it is generally the responsibility of the programmer to not fuck things up in C or (I believe) in Java.
Thanks. :) If I'm going to make strong claims I'm not going to be surprised when people tell me I'm wrong, especially if they give me good reasoning or point out critical points I've missed. Speed limits cannot be regularly or uniformly enforced. There are many other examples. Only so much ground can be covered by a given police force. Regular and uniform enforcement of many laws is impractical. If people were easily able to break laws and faced little resistance from law enforcement, breaking that law may very well become widespread--like in the case of music piracy.
Meaning that PySide can be used in non-GPL applications as a library, PyQt can't
Well, clearly you think that. :) You may even be right, in the general case (i don't think you are, but that's a separate conversation). I'm confident you're wrong in the case of python, which has a very strong tradition of giving the programmer simple encapsulations of complex tools.
It guarantees that you can't segfault or in any way maliciously derail the JVM. It doesn't mean that you can't fuck up your own invariants, in your program, if you don't lock the stuff that should be locked. But the ability to say that whatever the program does, there are no leaks in the abstraction of the virtual machine that it can exploit to derail it, not even unintentionally, is worth a lot.
if by 'slow the sort down' you mean 'actually sort correctly', then yes. justin@eee:~$ for x in a a a a a a a a a a a b c c c;do echo $x;done|uniq -c|sort -r 3 c 1 b 11 a justin@eee:~$ for x in a a a a a a a a a a a b c c c;do echo $x;done|uniq -c|sort -rn 11 a 3 c 1 b 
Python 3.0 is deprecated.
modifying existing classes is legal Python - it's not widely used for obvious reasons but works as expected otherwise. Changing the inheritance hierarchy in mid-flight is probably not a good idea, but there is probably framework code out there that does exactly this. It's much harder in Java to poke around in classes mid-flight, and even then (if you implement your own classloader), you'd probably get two different classes instead of breaking things. So, again, why would you make the programmer responsible for perfectly legal Python code that is lurking in the dark corner of some obscure framework?
A good reason for it to change would be that PyPy was many times faster, could use multiple processors, and was releasing new language features before C-python. We aren't there yet, but I hope that someday this will come to pass.
if it works on as many platforms as CPython works, and not just x86 and x86_64? Perl and Python run on a very wide variety of platforms including old (and not-so-old) mainframes, and routers and other boxes that commonly have some non-x86, non-ARM processor. Considering that 64bit support for PyPy is relatively recent, the chances of PyPy (jit) being available on a wide range of platforms is quite small.
Making weaker guarantees means breaking already-written software, and given that PyPy is trying to be "CPython but better" it should maintain the same guarantees as CPython. Sure, we could make PyPy faster by not providing good multiprogramming guarantees. We could also make it faster by not providing safe string operations, or automatic memory management, or introspectable types. We can make this so much faster by offloading all these things onto the programmer, but that's a C solution, and we aren't writing a C compiler. Sure, we can save a little time by making the programmer think about tasks for us, but really, it actually *is* easier to maintain all these things in the language runtime rather than make many application developers solve the same problems over and over again. In fact, I'd say that most applications programmers are *not* equipped to handle all of these things on their own, and even if they were to write reasonably stable solutions for these problems, they would be slower than a language implementation.
STM scares the lesser programmers because even smart programmers cannot make it work. And generally, as a lesser programmer, you don't want your fingers cut off by a machine that a far-too-clever-by-half guy built. So far, I have never seen evidence than the PyPy team is considerably better than the (well-intentioned and rather smart) people who tried to integrate an SVM into an imperative environment and failed miserable. It's close enough to a "I don't understand in detail how it works, so I don't see any possibility of it not working" attitude. Which is perfectly ok for an academic research project, but the PyPy team frequently claims to be, or want to be, more than that.
Anyone have some examples of GUI apps written with PySide?
To be fair, for a novice to Python, programming, and computer science in general, this can initially be confusing. As people commented in the post, the REPL automatically prints all return values, basically making people not see the difference. It's not until you write a real program of some decent length, with at least 2 functions that you really understand what return is doing (stopping, and returning to the line it was called at with some value). Rather silly that he couldn't just Google for it and instead asked Guido, though. That'd be like someone desperately trying to call Bill Gates asking how exactly to open the command prompt on Windows.
It's interesting... I've read the responses on google+ and many of the people seem to have a similar misunderstanding about the difference between information hiding and the enforcement of information hiding by a compiler. I'm happy that my elaboration helped. 
Am I safe if my thread does not do anything that appears to be blocking I/O? I know for sure my thread does not *explicitly* do any blocking, but I guess there could be implicit blocking hidden under the covers that I don't know about. The Stack Overflow answer included a loop around re-raising the exception, until the thread finally answered not isAlive(). If I really had to rely on that provision, I would not feel comfortable with the approach.
I am not sure I follow. How is 3.0 deprecated? I understand they are up to 3.2, but I am not sure that is what you mean?
&gt; was releasing new language features before C-python Which language features were created in PyPy? Speed and the like mean nothing for a reference implementation of a programming language. It's the standard for which others are based on in terms of the what the language means. I think people are conflating "a better implementation" with "reference implementation", where the two are unrelated.
So, that seems like a good compromise... Why wouldn't that work for PyPy?
Colour me crazy but aren't you doing a DFT directly here, not a FFT? It's still a nice example though.
I'm down with Bill Gates, I call him Money for short. I phone him up at home and I make him do my tech support.
I keep seeing all sorts of metrics about PyPy's general awesomeness... how ready is it for prime time? (I'm working on webapps, 90% Django, if that makes any difference.)
com_D always plays rock instead of paper, and com_E always plays scissors despite the doc saying s/he plays rock.
Oops, sorry about that. Thanks for pointing it out.
[Fixed bugs.](http://codepad.org/SOyTHIcJ) Thanks Grumpy. 
Looks good. Keep at it, you are doing well.
 return ['rock', 'paper', 'scissors'][random.randint(0,2)] should make the code shorter for com_A/B
now you can write the game that sheldon created in bbt with ease
Django runs perfectly fine, among most other stuff, on PyPy and from the reports I've seen about twice as fast.
Cool info. Funnily enough, I'm working on a programming puzzle, and part of the goal is to solve it with as little runtime as possible. I figured PyPy would be my ace in the hole, but it actually ran slower by around 15%! Are there any tricks for optimizing for PyPy? edit: That is PyPy versus the standard Windows Python 2.7 (CPython). Cython without any further optimization improved a couple percent over standard.
it doesn't seem to be that way, you're arguing that encapsulation is meant to prevent "Person A from crashing Person's B code by inappropriately accessing Person C's code" *-which is bullshit because you can cause the same by other means even in statically typed languages that support encapsulation-* but anyway my point here is that encapsulation is not the way to prevent that, test are.
Well... the other day I tried it with PIL to do some graphing and with CPython it took under a minute... pypy... 5 minutes. I'm not quite sure what to make of that...
Couple things: 1. Endgame stats, in your initial post, report 100 games played but the breakdown by choice per player only adds up to 70. Your sample output for the revision does not list the endgame stats but I think #2 is the root issue. 2. Your if statements checking the played possibilities only checks for six of the nine possible outcome combinations. I'm assuming that since the other three go unnoticed then these would represent the ones missing from your stats. I'll say I'm going to leave it up to you to solve the issue but the fact is I'm simply too lazy right now. 
MySQL
They are listed as ties. When both players choose the same option, it doesn't add or delete to the score, but it adds to the quantity of games played in total. If you look at the last lines it says "There were X ties." I plan to modify it (tonight) to include better stats about the tied games. 
Can't say I didn't think about it. =)
Thanks for the encouragement.
There are tricks, but it really shouldn't be slower (ever, if possible), if you could file a bug, that'd be great.
Because the interpreter's invariants are more complex. Rather than a field set being a single MOV on your CPU, it could involve calling arbitrary code, manipulating a hash table, or hitting google if you're just completley insane.
He's saying *if* that happened then PyPy would be the reference, I think.
You're welcome.
Here is an updated version with: 1. Stats about the tied games. 2. More opponents. Instead of having 3 choices to choose randomly, the new opponents have 2. [LINK to code.](http://codepad.org/qAtM9vCw)
Music piracy makes a good example, though. Laws should be for the betterment of society, and if the cost to enforce the laws is excessive (both from a resource/money cost and from a societal/freedom cost) the laws probably shouldn't be. The drug war is a good example, as well. Now both of these fall under the "victimless crime" category to some extent. So there are other things to consider. Speeding, it seems, is slightly different. It really should only be illegal when it constitutes reckless endangerment. Furthermore, I think the reliance on government supported and paid standing police forces may be a bad idea. In addition to the issues of corruption, it creates a privileged class with a bit too much power. In other words, the laws should be enforcible by anyone, or at least anyone meeting certain standards of training. It shouldn't matter who pays you. (And to some degree this is true, due to citizen's arrest). So the enforcibility doesn't have to be a function of government resources. I've gone way off topic, though.
&gt; FFT algorithms are so commonly employed to compute DFTs that the term "FFT" is often used to mean "DFT" in colloquial settings.
I can't tell you how much I appreciate this comment. There's lots of people pointing in different directions when you try to learn programming and lots of great people who'll help you fix your code but nobody questioning why you're coding what you're coding. I'll work on the smaller functions and making it more expansive as well as fixing the silly variable names.
I have used pyQt,wxpython,pygtk,pySide in the past 3 years,I found the pyside is best way to create in Gui in python,Hope PySide have a good future!
you're probably exactly right. i'm such a noob at complex operations ;) edit: because i'm out of this game, and FFTs are going to be relevant interests soon, can you explain how i did a DFT vs an FFT? What is a pure python FFT?
man i think it works in only some domains, the FFT (DFT) is one of them. Also PyEvolve loves it, too. Have you tried cython, too? PS: did you mean CPython or Cython? They're both different projects. Cython can be amazing as well. Ask me for more info.
It turns out that this works fine in theory, but works poorly in practice, for actual program contents (classes are a special case) at least in certain kinds of languages (and Python is an extreme example of such a language). **tldr;** [this link](http://mcherm.com/permalinks/1/how-even-immutables-are-hard-with-threads) For an example, consider Java. Java attempts to provide exactly this. Suppose you create some data structure and a function to initialize it. In thread A you create the object, then initialize it, then pass it off to existing threads B and C. Threads B and C simultaneously read stuff from the data structure in ways that WOULD be dangerous except that the data structure is immutable after initialization. The problem is that the guarantees provided in threading are MUCH weaker than you think. It's not just that there are different threads all working at the same time and reading and writing from the same memory locations, the architecture of modern CPUs makes that impossible. You see, it takes hundreds of times longer to read something from memory or write it to memory as it takes to process something in the registers. So to execute "X = Y + 1", the computer COULD spend 100 cycles reading Y, then 1 cycle adding 1 then 100 cycles writing X for a total of 201 cycles to execute. But that would be unbearably slow. Instead, it takes 100 cycles to do a bulk read of the whole memory area around where Y is stored into high-speed caches. It takes another 100 cycles to do a bulk read of the whole memory area aroudn where X is stored. It takes 1 cycle to add, then takes 100 cycles to do a bulk write of the memory area containing X. That's 301 cycles... which sounds even worse. But it's NOT worse if the compiler cheats. Instead, it spends 100 cycles reading Y and 100 cycles reading X. Then it executes the +1 for one cycle. Then, BEFORE writing out X it does some OTHER calculations on the chunks of memory that have been read in. If the program has good cache locality (active objects are near each other in memory) it may get 75 cycles of useful work done before it needs to spend 100 cycles to "flush the cache out" (write X and the other things that were updated. That would be a total of 375 cycles to do 75 bits of work, or just 5 cycles per line -- a LOT better than 201! But in order to do this, the compiler has the "cheat". It has to execute bits of work out of order, although it can take special precautions to make sure that it gets the same answer as if it executed them in the order written. As seen by THIS thread. But as seen by a DIFFERENT thread, the steps may appear to happen in a very different order. The other thread won't see the effects until they get flushed to main memory, and that won't happen after every computation (unless it is running 100x too slow!!!). WHEW!! Big wall of text there, but the story should explain why one thread in a program may see the computations by another thread happen in a different order. So imagine this: "In thread you A create the object, then initialize it, then pass it off to existing threads B and C." But imagine that from thread C's point of view, A created it, then passed it off, and only initializes it LATER. In fact, perhaps C will start using it at the same time that A is initializing it -- so it's not really immutable, and terrible errors result. This is NOT just a theoretical risk: I have written real code that exhibited this behavior when running on a multi-core machine. In order to help protect against this, the Java langage added a special exception to the Java threading model. Despite all other threading rules, if a class is declared "final" (immutable) and then all code executed within the constructor is guaranteed to be occur before the constructor ends EVEN AS SEEN BY OTHER THREADS. In theory, this is a great tool for creating data structures ahead of time and then reading them after initialization from other threads. But in *practice* it isn't so good. Initializing everything within the constructor of an immutable object turns out to be a real pain. Often you really want to use a hashtable (not immutable), or use Spring injection to populate your objects after the constructor, or slew of other choices that make it hard to stuff all your setup code inside of constructors. Some languages support this better: Scala and Closure are examples of languages on the JVM that use this feature well, but in Java it is awkward because there's no little special support for working with immutable objects. Python, as a language, is even worse: there are NO immutable objects in Python! So while it might be possible to do as you suggest (create special locks and use them around every \_\_init\_\_ method, then carefully make sure nothing is modified outside of \_\_init\_\_), the resulting language wouldn't really read like Python. 
What I find interesting, is that because of the way PyPy works (essentially, a giant code transformation toolchain that becomes your interpreter/on-the-fly-compiler) I think it has a shot at succeeding where others have failed. Not a certainty, but a shot. And it's a VERY interesting research project!
PyPy can output to x86 and x86_64. But it can ALSO output to LLVM. And it is reasonable (not easy) for it to be extended to output to other other assembly-language targets. I don't find it all that limited. It certainly isn't as portable as highly-portable C: nothing is!
No, I am saying that IF PyPy were many times faster (today: slightly faster), could use multiple processors (today: has GIL) and was implementing and releasing newly approved PEPs before C-Python (today: a couple of years behind), then it might become the "reference implementation". All of these are *reasonable* expectations for PyPy in the future, but are not yet achieved.
I would be surprised if that PyPy version is even remotely competitive with a C implementation.
Potato, potahto. I use DFT when I'm talking about the sequence and FFT when discussing the algorithm, but whatever works for you. It's usually pretty clear based on context. (However in this case I -- edit -- was expecting to see an actual FFT implementation, which might not be as slow as molasses in pypy.)
Well STM isn't difficult for the *programmer*, it is difficult for the *language implementer* (also a programmer). So **if** the PyPy team got it working, then lesser programmers need not be scared. You may not have seen evidence of the PyPy team's greatness, but I cannot think of any other project ever that implemented a fast JIT in a high-level language, or a fast JIT with selectable features (stackless, sandboxing, transparent proxies, etc). That may not impress *you*, but it sure impresses me. I won't be surprised if they *do* turn out to be "considerably better than the (well-intentioned and rather smart) people who tried [...] and failed miserabl[y]".
What is MORE interesting is that because of the design of PyPy, you can recompile your compiler with different flags to ACTUALLY make it faster by not providing guarantees of memory management, guarantees of thread safety, etc. Recompile your compiler... my brain is beginning to hurt.
Let me reply with an analogy: &gt; C is a good example.. You can say it gives you a very limited set of garbage collection tools which are library dependent. And it works great, you have to know what you are doing and track reference counts or liveness within your data structures. The guarantees are very specific, you as a programmer have to program responsibly and read the documentation to know when memory is freed and when it isn't. In other words, I agree that, given a few threading primitives and the ability to write single-threaded code, it is possible to write multi-threaded code yourself. Threading libraries in C are an existence-proof of this claim. But I don't think that necessarily means that it is "practical" -- it may well be so difficult that adding language-level support would make your language far more powerful.
FWIW, this is not FFT: FFT is a divide and conquer, NlogN algorithm to compute the DFT, whereas you implemented the direct formulation in O(N^2).
... Or they've never done any programming or scripting and are just starting.
No it can't. The LLVM backend is neitehr functional nor maintained.
Which is really the best solution - there are a few cases in which very tightly constrained applications programs (games) need to suspend certain guarantees of safety in order to meet certain guarantees of worst-case performance. But I seriously don't get why some people think every applications programmer can do without $HARD-TO-IMPLEMENT-TIME-SAVING-LANGUAGE-FEATURE.
Isn't PIL a C library? So you had to build it against PyPy right? CPyExt, which supports that usage, is slow. If you're plotting pixels or otherwise making a lot of calls into the C library, it would make sense you're getting bad performance. More than 5x slower seems a bit excessive. If you can produce a nice test case for the performance it would definitely be worth making a bug report (although there may not be a whole lot we can do about it)
Off topic: I think your value for M is off. The non-negative frequencies of the DFT should be a sequence of length `M = N // 2 + 1`. The current value of `M = int((N + 1) / 2)` doesn't include pi rad/sample (\*) for even N -- i.e. such that (M - 1) / N == 0.5. *(\*) pi rad/sample is the highest-frequency for a discrete-time sequence, i.e. A \* cos(pi\*n) == [A, -A, ...].* For example, if N is 8 then M should be 5, for the following frequencies: [0, 2pi/8, 4pi/8, 6pi/8i, pi] rad/sample. If N is 9 then M should be 5, for the following frequencies: [0, 2pi/9, 4pi/9, 6pi/9, 8pi/9] rad/sample. Trivia: Like 0 rad/sample, pi rad/sample gets counted in only one bin instead of split between positive and negative frequency bins -- i.e. + and - pi rad are the same phase. Also, both 0 and pi rad/sample are real-valued in the DFT of a real-valued signal because they only have even symmetry (cosine terms). 
Did anyone find the download link yet? I still only see version 1.0.5 for the binaries available. (I do see the source for 1.0.6).
 &gt; I'm writting this readme in Emacs so expect many formatting, grammatical, and spelling errors. ಠ_ಠ Formatting per-paragraph (default) 80 column wrap: M-q Spelling: M-x ispell Seriously learn how to use emacs well, it will do everything for you. Here's a hint: C-h a Then type in what you want to do. Emacs will come back with a list of commands. And of course there's always /r/emacs ... 
I searched, and I did not find many examples, but [here's](http://developer.qt.nokia.com/wiki/PySide_Based_Apps) one page with a few. But it is a solid library. I've been using PySide for the last month, combined with the Qt Creator IDE to create the UI elements. Then using the pyside-uic.py script, I am able to compile the *.ui files (which are XML-based) into Python code.
Too bad, I really hoped they will port it to Python 3 soon. Now I don't know what to expect anymore.
hes just trolling. 
I *think* the reason why Python can easily get away with not having this kind of polymorphism is due to the inclusion of dynamically typed keyword arguments, but I've never actually looked for documented rationale as to why it's omitted.
Philip J. Eby was pushing to get something like this into the standard library as "generic functions," but it never solidified enough. You can download his earlier versions though.
I believe the jit is the only part that isn't portable at the moment; the translation itself works fine on other platforms (somewhat slowly), and failing that, the untranslated interpreter can definitely be run on a conventional python interpreter (very slowly).
I mean Cython, as opposed to the "default" CPython.
I agree. pySide is the best I've found. pyqt's license is definitely less appealing.
*args, **kwargs .... DOH
Sure. Other than including the code and output, is there anything else I should do? The time was in fact 6.4 seconds for CPython 2.7 and 10.5 seconds for PyPy 1.5, so that is more than the 15% I incorrectly recalled.
code, output, input (if there is any) and anything else you think well need to reproduce the issue. We can always ask follow up questions if there's a problem.
This is great! Thanks!
&gt; So if the PyPy team got it working, then lesser programmers need not be scared. Yes. You see which premise I'm not believing. A half-working, bug-ridden STM is a large pain, also for the users. &gt; I cannot think of any other project ever that implemented a fast JIT in a high-level language You have been living under a rock, then. Most recent Smalltalk implementations implement a (fast) JIT in a dynamic language, and so do all the modern JavaScript JITs (Javascript is about as high-level and powerful as Python, it just suffers from ugly warts in other places). &gt; or a fast JIT with selectable features (stackless, sandboxing, transparent proxies, etc). If you consider very large software systems, having language features that are selectable (i.e., you need to pay an overhead for them in the whole system even if only one module needs them) isn't all that useful. I am definitely impressed by some of the things that the PyPy guys do. It's just, that you don't really want to entrust your future to people who dismiss most existing pragmatic solutions to problems in the Python world (NumPy, C extensions, multiprocessing) as mere "hacks". There are lots of very smart people working on the JVM (or their particular JVM implementation), who manage to do highly innovative stuff and still be compatible with 100% of all Java code out there, including 100% of all JNI code. They also complain about people using class loaders, adding classes later on, and all sorts of nasty things that mean that code has to be re-JIT-ed. But after complaining, they sit down and make it work. PyPy is more driven by research goals (and maybe they even have to, as Unladen Swallow showed much more modest improvements than what PyPy is getting now), and that very fact means that they produce impressive stuff but will not care in the end if someone is left dying between a half-finished STM implementation and a hard place.
I agree. OTOH I'm glad pyqt sticked to its license and thus has a solid finance stream to keep on developing it. We will see what happens now. Maybe Enthoght will pick it up, because AFAIK they hoped to ship pyside on windows.
My answer to that guy: you'll figure it out. Honestly, if he doesn't see the difference between print and return, then he has done very, very little programming. Despite what people may tell you, some questions *are* bad: questions that are based on idiotic premises. One need not directly answer such a question; instead, just ignore the question and teach him what a function is.
&gt;it doesn't seem to be that way, you're arguing that encapsulation is meant to prevent "Person A from crashing Person's B code by inappropriately accessing Person C's code" No I'm not. &gt;but anyway my point here is that encapsulation is not the way to prevent that, test are. Tests don't prevent anything. They show the problems that the tests are designed to show. If you don't have a test designed to show this problem, you won't see it.Tests don't change code, rappers do.
PyGTK seems messy (to me), wxPython is decent but there is something about it that doesn't click with me. I looked into Qt for a while, and finally jumped into pySide the other night - it's great, love working with it so far.
I like a line from a good movie as much as the next guy, but what I like much better are arguments to solidify a point made by said line. You seem to think that overloading functions is counterproductive. Why?
I think you're looking for the R^2 value of the least squares regression: import numpy as np def rsq(y, y_resid): y = y - np.mean(y) ss_total = np.sum(y ** 2) ss_resid = np.sum(y_resid ** 2) rsq = 1 - ss_resid / ss_total return rsq if __name__ == '__main__': #errorbar plot with estimated eqn and rsq in title from matplotlib import pyplot np.random.seed(42) x = np.arange(32) / 32.0 noise = np.random.rand(32) y = 5*x**2 + 3*x + 4 + noise p = np.polyfit(x, y, 2) y_fit = np.polyval(p, x) #&lt;--use this instead of for loop y_resid = y - y_fit rsq_fit = rsq(y, y_resid) ps = ["{0:0.3f}x^{1}".format(a, len(p)-i-1) for i, a in enumerate(p[:-2])] ps.extend(["{0:0.3f}x".format(p[-2]), format(p[-1], "0.3f")]) title = '$y\\/=\\/{0}$ [$R^2=\\/{1:0.3f}$]'.format( "+\\/".join(ps), rsq_fit) pyplot.errorbar(x, y_fit, y_resid) pyplot.title(title) pyplot.show() 
Hey, Thanks for typing all this out! It's a bit beyond my level but I just ran it and figured a few things out. The reason I'm doing this is to quantitatively justify my fit of a quadratic to the data. If I can say that some of the terms in higher order polynomials have an uncertainty larger than their value then they should not be used. Do you know what I mean?
Or maybe Enthoght will do the math and find that just licensing PyQt from Riverbank Computing is cheaper and easier than putting money into PySide. That is kind of the paradox of trying to build a community of developers and contributors around PySide. People who are happy with the GPL are already served by PYQt, and people who are not willing or able to pay for a commercial PyQt license and also unlikely to pay for PySide development. I'm not sure who is left over to continue pushing PySide. I imagine it is a very small group. 
PyPy guys want to use STM as an implementation detail not expose it to other programmers, so people should just chill out. In the same vein JIT is way too hard and complicated for most people, but that means nothing, no fingers being cut off here. STM is complex and for that reason the use of it should be limited to well understood subsystems so that there's no aggregate complexity overload. Using it to do optimistic bytecode execution is perfectly fine IMO. I want to stress the point that the very smart people "who tried to integrate an STM into an imperative environment and failed" tried to expose it. In this case the implementation of STM and the only use of it will be tightly knit in a single system. My experience with STM say that this very likely will work out well.
Its almost certainly less work than you expect, the whole thing is done by a code generator run on a few XML files. Posssibly the only "major" change required is fixing Unicode handling (which would happen in exactly one place, I think, the string&amp;lt;-&amp;gt;QString conversion code) Porting C modules to 3 is actually way simpler than porting Python (the API hardly changed) 
I'm not sure either. By "people" you most likely mean the free software community, which is GPLed anyways. True, there is no motivation for them to invest time into pyside. Hence it must be a company. Beside Enthought, which according to your suggestion might be better off making a license deal to cover their commercial customers, there comes meego and thus Intel to my mind. 
Enthought does already support open source projects like SciPy, and they release some of their own code (like ETS and Chaco) under BSD licenses. So it's not implausible that they'd be interested in supporting PySide, even if it was cheaper to license PyQt. And it's not even clear that it is cheaper for them to license PyQt. They jumped ship to Pyside pretty rapidly (the latest version of EPD already uses it), so it must be worth something to them.
I over simplified the situation regarding the free software community. The Problem: pyqt is dual licensed, which means in order for them to accept your patches you have to give them the rights to use it proprietary code. This is quite a barrier for many free software devs.
That is correct. The software is Free Software, but the development itself is not really run as a FOSS project. So far this hasn't been a big enough problem for anyone in the FOSS community to start a fork or competing project. Maybe having the existing PySide structure in place is enough to make it a viable FOSS project. The simple fact of the matter is that there are not very many people around who are interested in doing this kind of binding related development. Does anyone know what the other MeeGo partners think of Python and Qt? Is it even important to MeeGo? 
The best Python IDE. (Not editor, just IDE. Komodo Edit is a better Python editor.)
Yes you're correct-o in that one, as several have pointed out. Thanks for the heads up.
In some circumstances it can be, but I don't remember what those are. They're limited but growing by the version.
you're right! i changed it quickly for no real reason other than it had a weird if statement in the middle that didn't sit well with me.
When thinking about PyPy it's very easy to misunderstand what level things are happening on. I think you're making that mistake (although It's possible I am). In Python today, if you have a shared object and multiple threads can read and update it, you need to lock it. Some methods of some built in objects are atomic (list.pop for example), and it's not always clear which, and it's not always clear when that's a property of Python the language and CPython the implementation. However, in general, you need to lock your shared objects. This is just like Java. In practice if you don't lock your objects in (C)Python you are less likely to hit a problem than in Java, because the GIL is held for a number of bytecodes, and so the probability of something modifying a value between any two bytecodes is lower. But you don't know when the GIL will be released, so it still can happen at any point in your program. You need to lock your shared objects. The internals of the interpreter are a different level. For example, PyPy has a copying garbage collector. If one thread tries to read a value out of a hashtable while the garbage collector is moving that hashtable, you could get a segmentation fault. That's not something the Python programmer can preempt or prevent. Segmentation faults are always a sign of an interpreter bug. The same is true of the JVM. (*note: gc may be a terrible example for this*). So how should the interpreter ensure *that it's own code, and it's own use of shared data* is correct, and avoid your program crashing in ways you have no control over? There are three approaches: Course grained locking (CPython), fine grained locking (Jython), and STM, which is what Armin Rigo proposes to do. As I understand it, this proposal is essentially about making the interpreter more concurrent, without having to insert locks everywhere *in the interpreter code* by hand. The guarantees that you have as a Python programmer don't change, but they remain what you wanted: you need to lock your shared objects. 
The commented out line is flawed too: M = N//2 + 1 if N%2 else N//2 It's right for even N, but the 'weird if statement' for odd N is wrong. Back to my example, if N is 9, then M has to be 5, but 9 // 2 is 4. It should be `N // 2 + 1` without exception. I think I see the original source for this online. Sturla is wicked smart, but I think this is an example of something that slipped by in a quickly hammered out example.
Given the title, I was expecting a rant, but I got a nicer than expected "state of the Python" article. Thanks. :)
that's what i thought. I didn't think I introduced the bug :) Yeah you're right about the source - i probably should have linked it (normally i would). It wasn't anything official though so I didn't. It was really hard even finding that example. Either way I like it as a pypy example. Maybe next I'll show the difference with PyEvolve. PyEvolve is pure python intentionally, and the speedup can be drastic with pypy. It is interesting to me because typically a programmer does a hybrid python solution for speed - with C mixed in. Most performance applications that would want to take advantage of pypy are not pure python! hah
"Now look here, everything is fine as it is, and by the way, don't you dare criticize things..." #thirdworldproblems
Sure thing, although I should note I'm not the author; Nick Coghlan is, a Python core developer.
Them's fighting words.
'class' is the keyword used to define a new object in Python when you're going to use object-oriented programming. You're not forced to code this way, however a lot of standard library functions (and third-party libraries) will make use of this paradigm so its good to understand it at some point. Get familiar with using functions first, because you'll be using them a lot when making classes in order to provide access methods to your objects; e.g. class Person(object): @property def age(self): return self._age @property def name(self, name): return self._name def __init__(self, name="Sir Robin the Not So Brave", age=32): self._name = name self._age = age p = Person() print "Disparity - {}".format((p.age/2)+7) p2 = Person("Sir Lancelot", 24) print "{} is {} years young".format(p2.name, p2.age) 
Oh sure, one sentence totally softens up the bitchy "i'm entitled" tone. If the conferences depend on volunteers using their own equipment that are likely not doing video as their career, it's amazing that videos appear at all from conferences. (free of charge no less) A brief description of at least one process for capturing and publishing the videos from conferences. http://us.pycon.org/2011/blog/2011/03/02/pycon-2011-interview-carl-karsten/ Doesn't sound trivial to me, it sounds time and cpu intensive. Considering that a: it's live b: on a very tight schedule c: in rooms not designed for shooting episodes of CSI and the like d: on less than ideal equipment for the task e: by likely unpaid inexperienced people f: of people who aren't actors or professional presenters I think that it's amazing that the content is captured and published in a timely manner and the content contained within is consumable at all. Sure it could be better, but let's evaluate while keeping the constraints in mind. 
I have a problem with this line: rate = float(count)/(current_time-start_time) It seems if you're on a computer quick enough to equate current_time and start_time, you end up attempting to divide by zero. Whilst Python may be able to reverse the effects of gravity, dividing by zero may still be a problem (at least in Python's up to &amp; including 3.2.1) I believe when attempting to divide by zero, Python should display a rabbit. With big pointy teeth. 
Dude chill out, it was meant as an ironic reply to your ironic reply, that's all. I do think though, considering these conferences have first class material, why not give it first class treatment too? Dimly lit 480p video doesn't really do the material justice IMHO. Why not just talk to some happy amateur AV club and have them do it for, erm, Youtube streetcred or something?
Yes, exactly that :-)
yeah that would be nice. I'm sure the conference organizers ever thought of that. :)
For a given order, the coefficients you get from polyfit will minimize the norm of the residual vector between y and yfit. If the criteria is just the square error, no other coefficients for a given order will have less error. Then it just comes down to iterating through polynomial orders to find the model with the best R^2 value. Higher orders will naturally give a better fit. In that case, an automated solution can penalize high order results to minimize the automatically chosen 'best fit': def rsq_adj(y, y_resid, d): if d &gt;= len(y) or d &lt; 0: raise ValueError("Valid range for order d is 0 to len(y)") y = y - np.mean(y) ss_total = np.sum(y ** 2) ss_resid = np.sum(y_resid ** 2) penalty = len(y) / (len(y) - d) return 1 - penalty * ss_resid / ss_total 
ahh, yes. it's all coming back to me. thank you.
Hey mate, thanks so much for the post again. Just one problem, when I'm trying to run your code I'm getting a weird error: TypeError: object of type 'function' has no len() If I make the start of the function: def rsq_adj(y, y_resid, d): print y_resid print d print y if d &gt;= len(y) or d &lt; 0: And run: rsq_adj(angle, angleerror, 5) Where angle and angleerror are both lists of length 7, I get: [0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280, 0.0280] 5 &lt;function angle at 0x...&gt; Why isn't it saying it's a function and why is it not printing out the values for angle?
Interested, but no rss feed. Must have taken longer to design the No RSS graphics, then to hand roll a feed.
I made one more change, because I think if...else clauses are ugly. I did a few runs in ipython with %timeit and there was no discernible difference so this is merely cosmetic. def counter(iterable, countevery=100, total=None, logger=logging.getLogger("counter")): start_time = time.time() for count, thing in enumerate(iterable, 1): if not count%countevery: current_time = time.time() rate = float(count)/(current_time-start_time) if total is not None: remitems = total-count remtime = remitems/rate timestr = ", %s remaining" % time.strftime('%H:%M:%S', time.gmtime(remtime)) logger.info("%d items complete (%0.2f items/second%s)"%(count,rate,timestr)) yield thing 
I am using PyDev for class. Not liking it at all. I am already not a big fan of eclipse. PyCharm is better for me and a reasonable cost in my book.
From what I have "heard" about wxPython, it doesn't hide it C++ nature enough.
Your 'y' is the function 'angle' instead of a data array. In the problem description there's a set of points x and corresponding data points y. `polyfit` regresses the polynomial coefficients p. d is the order of the polynomial. y\_fit is the `polyval` evaluation of the polynomial at points x. y\_resid is the error vector `y - y_fit`. I hope that clears things up. 
Larry Hastings has a great voice for this. His lightning talk at PyCon 2011 on Minuteman was awesome, as are all of his PyCon talks.
I hadn't realized that.
Not quite true. The discussion in the article included the idea of exposing STM to the programmers, see the section titled "The state of STM". However, it is the implementation of STM which is difficult, the USE of STM is significantly simpler than using threads. (Okay, that's not saying much, but it's still true.)
At least you're ambitious. Good luck.
You passed my test: exec("print 'Hello unsafe form validation!'") 
I wasn't really trying to make a statement for or against overloads. I was commenting on how python's flexibility gives you the power to do things that may or may not make sense in a traditional way, but you can still do them anyway. But since you asked, languages like c++ have overloading because they are statically typed and functions need an exact number of those statically typed arguments. With python's *args and **kwargs, default parameters, (and even ducktyping I suppose), you can achieve the same thing with less code. Also, if a function is so similar to another and yet requires different parameters, it should probably be named something else that inherently explains that different parameter requirement / behavior.
And I think you underestimate the difficulty. They have actually made an investigation about this, and here is conclusion: "Summarizing, porting shiboken to Python 3 isn’t going to be a trivial task and support both versions will be even harder but not impossible." http://developer.qt.nokia.com/wiki/PySide_Python_3_Issues
I have tried using it before, but didn't know anything about Eclipses environment. Struggled to to setup projects. Is it worth trying to learn eclipses ways to use pydev?
Eclipse isn't that hard to learn. A lot of things use eclipse, java, android, as3/flex, I think it's worth it
I like this a lot because I can easily go to class/method definitions, however I've notice with some source code I'm using with it, it throws a fit since the code does some lazy import magic and compiles fine and runs, but pudev thinks it doesn't compile cause it can't find the imports . Hmm.
Cool cool. I looked it up in 'Beginning Python' by Magnus Lie Hetland. I'm wrapping my head around it now. I don't really have any programming background and Python is my first language since playing with BASIC 25 years ago. But yes, the wheels are turning in my head on how I can use classes to rewrite this program. Thanks for the tip.
That code sucks :(
I like it! You need to initialize timestr if total is None, however. Edit: You also need to use count+1 instead of count with enumerate.
Try the latest version, [psycopg2 works out of the box](https://bitbucket.org/pypy/compatibility/wiki/Home#!db-adaptors). I've also used the [pypy postgresql branch](https://bitbucket.org/alex_gaynor/pypy-postgresql) which has a psycopg2 implemented in rpython. This one seems to be deprecated though. I'm currently running django + postgresql on pypy for one of my websites and it works fine.
In 2011, PyCon US spent nearly $150k on video related expenses. If you know of a happy amateur AV club able to do the same amount of work (5 concurrent recordings several hours a day for three days) at the same or greater quality from recording to post-processing, you should let the organizers know.
how not? that was your reply to "Would it be so terrible for MY code to crash when YOU force it to?" also I'm not expecting the tests to prevent anything but you're supposed to make sure they pass *before* you go to production, crashes on dev are normal.
I would appreciate some remarks from you, if you wish. I'm not Guido of course.
If you have questions; feel free to ask.
Here is the latest version. It has the suggestions made by redditor *roopeshv*. Cut the code down by about 20 lines, and it seems to be faster. [LINK to code](http://codepad.org/E5BQYIbm)
I think you should know by now the answer to your question with your stated experience. In your situation, I would use the one you know best. It will be easier for you and your coworker since you will be able to guide him with ease. If you still have doubts, I would use Flask because it is great for small and simple projects.
I would say Flask or Django, depending on how you feel about Flask-SQLAlchemy vs Django ORM. Wiring functions to URLs doesn't get easier than in Flask, but Django's ORM is really easy to learn. Someone with more Flask experience than I have should chime in as to the relative ease of organizing static content and templates. While Django 1.3 handles that really well, I've noticed the docs on it are still a bit opaque to newcomers.
I kind of think a microframework is a good idea, and because most of my experience in ORM is with SQLAlchemy I was kind of leaning towards Flask but wanted some opinions from the community. Thanks the comment. 
[werkzeug](http://werkzeug.pocoo.org/)
Not a question, but a big THANK YOU. I've just used it for the first time some days ago and thought by myself what a great module that is. It helped me a lot, surely will help me and many many other devs a lot in the future. So, first and foremost I appreciate your achievement very much.
Well, they mention "with atomic" which is not quite exposing STM to the programmer. What makes STM simpler in this case is that it's not generic memory, all they need is tag certain known finite set of structures with last-modified timestamps and convert transactions into inevitable before allowing any of the calls outside another small set. I feel that's a feasible task.
Or if they have only done bourne shell scripting. echo is used to "return" text in shell scripts.
You might want to try math.pow(), simply pow() or whatever Numpy has instead of **. The thing is you should try it and profile it. Check out the profile module. See http://wiki.python.org/moin/PythonSpeed/PerformanceTips
I'm still a django noob - is using class based views version dependent? I'm currently developing a project in 1.3
To square a number is it generally quicker to multiply it by itself than to raise it to the power 2: $ ipython Python 2.7.1+ (r271:86832, Apr 11 2011, 18:13:53) Type "copyright", "credits" or "license" for more information. IPython 0.11 -- An enhanced Interactive Python. ? -&gt; Introduction and overview of IPython's features. %quickref -&gt; Quick reference. help -&gt; Python's own help system. object? -&gt; Details about 'object', use 'object??' for extra details. In [1]: x = 12.345 In [2]: timeit x ** 2 10000000 loops, best of 3: 127 ns per loop In [3]: timeit x * x 10000000 loops, best of 3: 78.5 ns per loop And a few seconds of experimentation show that the math.sqrt function is nearly twice as fast as x ** .5: In [4]: from math import sqrt In [5]: timeit x ** 0.5 1000000 loops, best of 3: 198 ns per loop In [6]: timeit sqrt(x) 10000000 loops, best of 3: 104 ns per loop Also if the path is on a fixed graph network then you could compute all the distances once and put them in a lookup table. 
CherryPy! Solid, mature, straightforward - pick the components you like and come up with a solution that fits both your brains :P I am a library guy ;)
[math.hypot](http://docs.python.org/py3k/library/math.html#math.hypot) will do what you want, it computes the length of a vector from the origin. 
If you are comparing the results of euclidDistance() then there is no need to compute the square root. You can compare the distances squared. It may be faster to do: (point1x, point1y) = point1 and then use the x &amp; y values without indexing in the result expression (assuming that point1/2 are tuples): return (point1x - point2x) ** 2 + ... Instead of '** 2' try: dx = point1x - point2x return dx * dx + ... Of course, you have to time all of the above suggestions to see which actually helps and which doesn't :) And there may be other, more global, speedups possible.
Even simple x**x is quite slower with pypy (1.6) than it is using cpython (2.7). 
I assume this is using very large longs, or something else?
Might want to check the changelog, I'm not sure myself.
any of the ones you've mentioned provided you think it's a good idea. There's no wrong answer for this. :)
A lot of times, you don't even need Euclidean distance. Multiplication and powers and roots are expensive. Adding and subtracting are not.
They are available in 1.3
I'd love to give PyPy a go, but as far as I can tell there's no easy way to test it alongside a normal Python install. On Windows, at least.
Didn't we cover this a month ago? http://www.reddit.com/r/Python/comments/icfu9/global_interpreter_lock_or_how_to_kill_it/ I vaguely agreed that allowing the programmer to take responsibility for the locking might work; others wondered why you would want to allow that. There is a valid point however, in that the GIL is not there to save you, the application programmer, from performing locks, but to ensure the single state that is the interpreter is valid across multiple threads, and that is not something the app programmer can do anything about - it must be enforced at the interpreter level. So if one interpreter is to be able to handle multiple threads then there must be some degree of atomicity guaranteed so that the context switching can take place safely.
I strongly disagree with this quote: "It is important to emphasize that most existing multi-threaded Python programs actually rely on such strong guarantees. [...] With such an example in mind, it should be clear that we do not want a solution to the multi-core issue that involves dropping these strong guarantees." If you're using the GIL to ensure that your program runs correctly, then you're doing entirely the wrong thing. It's there for interpreter integrity, not application integrity. I'd be happy to see a Python interpreter break these false guarantees and force people to write their code properly, using the correct synchronisation systems where necessary.
Also another thank you - I use it frequently in my research studying how languages and cultures evolve. Right now multiprocessing is allowing me to run a few hundred analyses estimating the rates of evolution in language structures.
Sounds very interesting, link would be appreciated! Another multiprocessing-user here. I've learned quite a bit from multiprocessing over the past while. I like it a lot.
It's possible you should look into scalable solutions like hadoop/mapreduce if you're doing this.
Sorry, I'll get my act together soon. I just hate XML. But I'll have an RSS feed set up in, oh, maybe a week. The site is all hand-forged HTML right now, and I want to switch it to being generated, and I'll roll the RSS generation in with that.
Thanks! :) Minuteman is even better now--my lightning talk at EuroPython got thunderous applause!
Yeah, I've played with this, but the calculations are done in another program that's either written in R or C or Java (depending on the program), and I'm using Python and multiprocessing to sequentially run many analyses through those programs on a few Dual Quad-Core Macs. Map/Reduce didn't seem to fit what I needed there - no real reduce phase - as I just want the program logs. Hadoop is on my (long) list of "things to understand and try out" when I get a spare minute though. 
Nothing published on this yet, but I published some earlier results [in a 2010 paper](http://rspb.royalsocietypublishing.org/content/277/1693/2443.full), and these new analyses are a follow-up on a better dataset with a better method. 
This might not be relevant, but have you tried running your game under [PyPy](http://pypy.org/)? I don't know if all of the libraries you're using are compatible, but just the first thing that popped into my mind when I saw the title.
In shell scripting (say, bash), echoing a statement sends output to whatever initiated the the echo (unless there's an explicit redirect). That's most likely the confusion this guy is having. If this guy knows what an exit code is, then he would most likely make the connection that there are different ways to convey different kinds of information. It just so happens that in Python, both status and data information are passed internal to a given program using the return statement, and print is merely to send messages to stdout or some other file descriptor.
(shameless plug for a great small business). At PyCon Australia we had the video done by these folks, they're fantastic: http://www.nextdayvideo.com/
Very interesting stuff! Also RSB is very impressive. Impact factor &gt; 5. I'm a total amateur in linguistics, though I did once manage to supervise a masters student in computational linguistics, specifically evolution of word-order. I love this stuff.
Oh cool - what did the student do? I was on a paper a few months back looking at the evolution of [word order](http://www.nature.com/nature/journal/v473/n7345/abs/nature09923.html).
&gt; Pure Python Linux keylogger FTFY
 &gt; if player == "rock" and opponent == "rock": &gt; rock_tie += 1 If you use a dictionary say ties, like in ties = {'Rock': 0, 'Paper':0, 'Scissors':0} you can simplify the above to if player == opponent: ties[player] += 1
&gt; As to Windows - sorry to hear you are stuck with that mess. I can't help you there. I hear this sentiment all the time. I have used Python 2.5 on Windows for 5 years and it works fine, no mess at all. When I hear about Mac people having the odd Python problem the sentiment is usually more polite and sympathetic. 
Yes, x is int/long and the result is very very large long.
Yeah, it's known that our longs aren't as fast as CPython :/
Needs X? Pah, read it from /dev/input/eventX like a real man ;) does require root, though
Like wilbur-d said: available in 1.3. Addition: 1.3 was the first version in which they appeared, so they're not in 1.2.
Round Robin twitter accounts. 
I thought Turbogears development was going to go into Pyramid? I haven't been looking into it lately so I might have missed the newest developments.
It was, but apparently some people wanted to continue on with TurboGears. I jumped ship to Pyramid though because I got burned by TurboGears changing its requirements too often, and awful documentation too. 
She studied word order in creoles as a function of word order in the parent languages. She had software implementing an iterated learning model and optimality theory. She was citing Kirby, Greenberg and similar. I thought it was pretty good for her first research, but she wasn't interested in trying to publish it, unfortunately.
Not exactly dvorak-proof, is it?
Not really anything you have to do. I installed it using a fresh hg tip virtualenv install, psycopg2 from git. Would you mind sending me the output? Edit: Sorry, I just checked and I actually wasn't using 1.6 but the hg tip, but AFAIK it should work with 1.6 too.
that is coming to an end. it was a subject to bad maintenance. it is not, now. 
a transition to pyramid would probably result in a better looking version number.
I hope for the remaining users that this is the case. :)
 return random.choice(['rock', 'paper', 'scissors']) ..would be an even nicer way to do it
I wrote a slightly over elaborate version of the game, trying to work out an elegant way of defining the rules in Python: http://codepad.org/sFui0dz7
i'm happy with tg2. all my questions are answered, all the help i need is given. no problem. :)
I'd recommend Flask, just for its simplicity, and as others have mentioned, the fact that it's superb for both tiny and large projects.
This program is awesome. Thanks.
OK, now I feel bad.
Helpful - thank you
sleep() is for the weak
[Cython](http://cython.org/)?
That is a simple function you could implement in C. see http://docs.python.org/library/ctypes.html
Which is actually MORE interesting.
I dropped TG as well for the same reason. But given that Pyramid is done - in part - by the same guys (IIRC), do you think that will change?
&gt; I'm trying to imagine what sort of person could have this misconception. I think perhaps if one's first exposure to programming is loading IDLE and working through a tutorial or two, it's an easy mistake to make. IDLE doesn't clearly differentiate between return values and printing. IIRC, they both simply display as blue text in the interpreter window. If one's only experience with a function is a standalone unit, called by the interpreter, then in my mind, it's a fairly understandable mistake. I think the moment (s)he writes a modular (structured) program, (s)he'll figure it out, though.
Pyramid is mostly done by different people, based on a different previous project. It's not a Turbogears rewrite.
Okay. I see. I thought it was the TG guys teaming up with the Pylons guys and working on this together. Must have misread it somewhere then... Thanks :)
Why? PyQt already supports P3.
The scipy.spatial.distance.pdist and others in that module is useful and is very fast if you are calculating many pairwise distances. It uses a C wrapper and I found it faster than a custom cython implementation. 
I love trying to think about how to succinctly but properly answer questions like these. It's been *so* long since I started programming, that I just plain can't remember what kind of mindset I had back then. I wish I'd left some crayon-scrawled notes for myself. It makes it a really interesting mental task to try and identify what preconceptions that person has that I wouldn't even *think of* in a normal day. Then try and think of the shortest path between there and the answer. Like "why is 1+1=2"? Everyone takes it for granted, but you end up having to go some interesting places in order to explain it.
That's what I thought to begin with, but it seems to be more a case of the Turbogears people feeling that the current development was a bit of a dead end and that their efforts would be better employed by helping with Pyramid.
hah, nope :) i need some way to read the keyboard mapping automatically instead of hard coded...
Ah, did not know that. Just tried pypy first time yesterday. x**x is just something I use when I want python to do something for longer time (it's easier to write than a loop).
 My face when I used M-q just now: O.O Thanks for the tips. I always wanted to find an interactive tutorial for emacs (besides the default one). Do you know of any good ones?
I started writing one yesterday ... ;-) Here's the first post: https://gist.github.com/1168462 You can be my test subject!
I always thought something was "pure Python" if it only required/imported modules written in Python. To call this "pure Python" while it's interfacing to libX11.so via ctypes seems like a bit of an overstatement. 
I'm currently re-writing it (about 75% done). Making it more "pythonic", because the original source is just code hacked together. It will: 1. Log all the games played to provide more meaningful stats. 2. Present the data as HTML. Thanks for the code, I enjoy reading other peoples code.
Cool, didn't think about that one. Thanks!
Post your own version. =)
I'm sorry for your experience, the team has recognized the past errors and is now focused on providing a reliable and easy to use framework. An abstraction layer is under development that will permit to change the low level framework under TG without breaking current TG applications and Pyramid is one of the available options that is being looked at. Since version 2.1 the first two rules of the development team have been "never break compatibility" and "improve documentation" and there is a clear public roadmap until version 2.4. Documentation improvements effort also lead to a project called "The TurboGears Book" to fully rewrite the documentation.
Well, I guess you can still use it for non-evil purposes, like hotkeys(assuming you ask the user to press the key they want to use first).
I like it so far! I would suggest you talk about color themes next time so that you can show how to set a color theme at start up using the elisp evaluation
Hahaha, no. That will be the last thing you do. :-) But I'll show you how to install one: 1. Go get a color theme file. I love [zenburn](https://raw.github.com/dbrock/zenburn-el/master/zenburn.el) 2. move it into ~/emacs.d/site-lisp/zenburn.el 3. Add the following code to your ~/emacs.d/init.el (see below) 4. Then restart (or M-x eval-buffer) emacs. It's pretty! Code to add to your config: (normal-top-level-add-subdirs-to-load-path) (add-to-list 'load-path "~/.emacs.d/site-lisp/") (require 'zenburn) (zenburn) 
Oh that's cool - shame she didn't get it out :/ 
I don't know, something about how they wrote it in the first place. But if they say it is difficult, it must be difficult.
Geez man. 80 cols!
If they used Pyramid as the low level layer, what would go on the high level? When I migrated from Turbogears to Pyramid there was hardly any extra work at all - just a few extra lines to specify routes and that was all. Why add another layer on that?
I will look into this, thank you for your response and my apologies for the delayed response on my end.
Yeah, I was thinking about interfacing with HotShot or cProfile, I will look into this (as well as the line_profiler mentioned by jellef). Thank you for your response.
wow I never knew this was in the math module :) Thanks.
I like the double compilation as much as what it is by itself, but thinking longer range, it becomes possible to do all your web coding in Python and just include a standard parser to make it run client side…
For some reason, it's not working on Firefox 7 Beta. execute is not defined @ http://gumbyapp.com/js/app.js:104
Or you could write it in JavaScript. It's not my favorite language, but it's not *that* bad.
Haha, true. But wouldn't it be better to do a slow and convoluted end run around it instead?…
Same with me on Firefox 6.
It doesn't work for me. The function `execute` is not defined. app.js, 104: var pageLoaded = function pageLoaded() { loadPython(); doRun(); // do our initial checks var runButton = document.getElementById('run'); runButton.addEventListener("click", function(event){ execute(); }, false); I'm running Firefox 6.0. 
I'm having trouble making emacs use python-mode.el instead of python.el &gt;.&lt;
Works on Android. There are people still using Firefox?
didn't work on Firefox, had to use chrome.
Now to just run this in a browser coded in python. 
I've seen django just abort without any errors on the first request when trying to run against pypy. *shrug*
Some software need to be "fixed" to be usable/compatible with PyPy. See the [fix for web2py](http://code.google.com/p/web2py/issues/detail?id=288). And there's a [memory problem with Tornado](http://groups.google.com/group/python-tornado/browse_thread/thread/2921b407da0aaf89/1674a836cc260a76).
With that many layers of compilation, it would be faster to add CPython to existing browser engines!
Oh yes it is! But there's CoffeeScript, which is quite nice in my opinion.
It doesn't work: Traceback (most recent call last): File "yourcode.py", line 1, in &lt;module&gt; import this; ImportError: No module named this
many of the "leaks" i see with code running under pypy tend to be people relying on the refcounting GC doing things like forgetting to close sockets and files is a big one that can cause FD leakage and eventually a Out of file handles error if you open too many between a full GC cycle the other mistake people make is not telling pypy to use a limited amount of ram, if you dont set the environment varible PYPY_GC_MAX to something like 100MB then pypy will attempt to use all the ram in your machine instead of garbage collecting you can also tell pypy to collect more offten by setting PYPY_GC_GROWTH to a value such as 1.1, meaning if the heap usage increases by 10% it will cause a garbage collection cycle if you want to play with some of the GC varibles take a look at pypy/rpython/memory/gc/minmark.py the GC is not well documented and looks like it is acting in a strange fashion for people used to cpython and how it handles the lifetime of an object tl;dr * Check you are closing files correctly (ie with .close() or by using "with open()") * make sure you told pypy the maximum amount of ram it is allowed to use with PYPY_GC_MAX there are other things as wll but that should cover 90% of most cases
Can confirm in Firefox 8.0a2.
Same with me on Firefox 8.0a1
Firefox 9 user reporting in to n^th this.
Try this: http://syntensity.com/static/python.html
&gt; if you dont set the environment varible PYPY_GC_MAX to something like 100MB then pypy will attempt to use all the ram in your machine instead of garbage collecting I felt stupid for not reading the documentation regarding this. Then I searched for PYPY_GC_MAX and there's no documentation. Just comments in `minmark.py`. *sigh* 
Are you sure PYPY_GC_MAX is used? I've set it to 100MB and after 1 million requests over 160 MB are used and no exception. After another 1 million requests it's over 270 again. 
Its not very well documented and in fact i think a "tuning the GC page" in the documentation is a good idea as well as a blog post or two keep in mind pypy can go over this limit as IIRC that variable only controls the heap for python objects, there can be some accounting and other objects that cause it to exceed this limit if you need absolute guarantees then i recommend cgroups then rlimit to restrict the mem usage of run away processes
I'm working my way through Learn Python The Hard Way at the moment, which has paid PDF/epub versions or you can read it online for free. I would highly recommend giving it a try: http://learnpythonthehardway.org/ I'm planning on going through the exercises at Project Euler once I've completed it, you may be interested in it too: http://projecteuler.net/
As I don't see anything pypy related in Tornados last issues and fixes, I guess Quora is using your solution. Limiting and maybe automatic restarts with something like supervisord. Doesn't feel right. As for the GC: One of the tests included a call to `gc.collect()` when I request a certain URL. No changes. Seems not to be related to the GC used in PyPy. tl;dr: PyPy isn't a simple drop-in replacement for CPython.
At least it has standard library.
The problem with that plan has never been the speed but the fact that getting Microsoft too add Python to IE might take a bit of convincing. For good reasons too - Python is a lovely language, but it's footprint is at least 20 MB and it has libraries for a bunch of stuff you _really_ don't want to allow random websites to do. Which is usually a strength, but not when embedding an interpreter in a hostile environment.
Looks like premature optimization to me, which can introduce bugs such as del'eting object too early. This kind of work should be done automatically by graph traversing tools, and not by hand.
I was playing around with DFT code (based of [this page](http://madebyevan.com/dft/)), and got even more dramatic speedups https://gist.github.com/1173153 $ time pypy dct.py 13 real 0m9.236s user 0m9.210s sys 0m0.022s $ time python dct.py 13 real 2m4.570s user 2m4.432s sys 0m0.082s
Don't forget that Microsoft already shipped Python in 1996 ;-) (it's true though)
I don't think Microsoft has anything against Python at all, especially since it runs pretty well on their .Net platform in the form of IronPython. What I think Microsoft has something against is allowing Python to be used as a script language in IE, and rightly so. Python's «batteries included» policy, while terrific for a general purpose programming language, is simply terrible for an embedded scripting language. Java tried to do both by defining multiple platforms, but that doesn't seem to have worked out terribly well for them.
I would go with [Flask](http://flask.pocoo.org) or [Bottle](http://bottlepy.org)
As far as I understand the code, this adds a `with thread.atomic: ...`statement that protects a block of code from being interrupted by a thread switch. This is not "Transactionnal Memory" as I understand it. No rollback, for example. Still a very nice feature! It removes the need for locks in many places and may help to reduce the thread jittering problem on multicore CPUs. And the patch is clean and small. If it works, I would really like to see this in CPython.
[http://lmgtfy.com/?q=learn+python](http://lmgtfy.com/?q=learn+python)
Just in case someone doesn't know, *M-/* (pressing alt and forward slash simultaneously) does autocompletion in all modes for variable names and functions.
I shall get around to it, the issue is its very very hard to put it into a blog post so I put it off.
Awesome, thanks!
I think the idea is that you would need this feature to implement STM via PyPy - see this relevant blog post: http://morepypy.blogspot.com/2011/08/we-need-software-transactional-memory.html
It would be working just fine if all you diehards would finally just upgrade to Firefox 107! You need to stop being afraid that high version number means that all your add-ons have broken. Think about this way: how much do you think that they could possibly have changed since Firefox 106 was released three hours ago?
It doesn't even have urllib...
Better than gumby.
@Secretiveslave: You are a douchebag. The OP isn't asking just for links to sites. He is asking for our opinions about the quality of those sites, which Google cannot give you. Way to welcome people to the Python community, you stupid asshole.
Google has a python video lecture http://code.google.com/edu/languages/google-python-class/
What linux are you on? I'm guessing ubuntu. doesn't just sudo apt-get install python-opencv work? And then you'll get all the dependencies?
Try http://www.trypython.org. It's an in browser Python tutorial.
nice! also that's a great DFT reference, thank you.
[Niiiiiiice](https://github.com/kripken/emscripten/wiki)
I had some problem with that a little while ago but I'm not entirely sure how I fixed it. I think the key is that I was using (desktop-save-mode 1) and when emacs started up it was using python.el for those files before loading python-mode. When I moved the desktop-save call to the very end of my .emacs things improved.
There is a email about the patch http://mail.python.org/pipermail/pypy-dev/2011-August/008153.html in there is a link to why this means the basis to STM, well the direct link is http://en.wikipedia.org/wiki/Software_transactional_memory#Proposed_language_support but yes it doesn't have transactions, but maybe you can use the transaction module from ZODB with this.
Altogether frightening.
Glad this is being highlighted. Sadly, we could have had Software Transactional Memory supported in CPython 8 years ago.
This is just a description of memory leaking situations, i.e. the knowledge you would need to write your 'graph traversing tools'. Did you even read the article? This was a very good look at situations that have bitten projects I've worked on in the past.
Only if it is coded in an IDE coded in Python coded in this.
ROFL. 
See http://stackoverflow.com/questions/4750806/how-to-install-pip-on-windows . You did not help yourself much by not searching enough on the internet and the maintainer did not help you much by calling names. A MSI installer is not a bug but a nice to have. I believe you both should work on yourself, your technical and social abilities.
Python _already_ has polymorphic functions. This is not it.
Why is curl or wget needed? and can't pip be installed manually?
Yes, you can.
and you're a big crying child.
Actually, Jannis is a nice guy, incredibly smart, and one of the easiest people to work with in the Python community. I think you'll find that when you're rude, snide, and sarcastic you get the same echoed back to you. Treat people with respect, and you'll get that back, too.
see: http://www.catb.org/~esr/faqs/smart-questions.html#courtesy
You have a few good links on the right in the "Onlikne books" menu.
You're being an asshole and you upset you got shit?
And python's performance would have degraded a huge amount, so less people would have used it.
read the email I linked (http://mail.python.org/pipermail/pypy-dev/2011-August/008153.html ). He is talking about the atomic patch not gil removal I think.
You could use a web browser instead of curl or wget.
Lots. Firefox has many extensions that are better than anything available for Chromium.
Name 1.
AdBlock
You're using Windows, so your opinion is worthless.
do you have any objection to me just posting this on quora? You are more likely to get a response there.
Adblock on chrome has pre-render resource blocking (meaning it doesn't download ads, doesn't just "hide" them like it used to), has for a long time now, and it also has a click-to-select ad blocking that is way better than the adblock+ for firefox. NEXT! If you even begin to mention firebug; don't. That's built into chrome. Right click -&gt; Inspect element.
Our Tornado servers are actually only used for Comet connections, and still run on CPython 2.6. Our main HTTP servers are an entirely separate tier. That said, there are a lot of other third party libraries we use that count on refcounting GC. Our process with dealing with them was basically to patch the libraries ourselves to do the GC manually, or simpler things like enclosing file descriptors in with blocks, instead of relying on it falling out of scope to flush().
Actually, it looks like every extension I have installed on my work machine now has an equivalent for Chromium. I'll check my home machine later, and maybe switch primary browsers for a while if the same is true there.
It takes a while to catch up. A lot of peoples conceptions about Chrome not having extensions etc were from well over a year ago. There are maybe...a handful of extensions that Firefox can do that Chrome can't, but 80-90% of them are on Chrome. (Stuff like IM clients in the browser...but what kind of person does that shit...) Also, with Chrome having its own built-in PDF viewer, Flash, and a sandbox for those plugins, it's so much less prone to crashing than Firefox ever was. Hell, in a recent browser hacking contest, Safari, IE, and Firefox were brought down quickly while basically nobody even attempted Chrome because it was so much harder to get out of said sandbox. Also; in Chrome...get Adblock, not Adblock plus. Then test out the "block an ad on this page" feature. It interfaces with the DOM and allows you to filter ads by GUI (instead of by some weird regex-alike text). It's really quite awesome.
i don't know about linux, but the installer for wndows from these people does everything automatically, that is ..if you want to use opencv on windows from python http://simplecv.org/
I don't think he's the asshole here. Python development on Windows is still noticeably less well supported than elsewhere, I agree. By far the easiest way to get Python going on Windows is to use the [ActivePython](http://www.activestate.com/activepython) distribution from ActiveState. It includes both pip and pypm, their own package manager, by default but also sets the PYTHONPATH and file associations. The only issue I have had with this distribution is that freetype support is not set up in their distribution of PIL, but you can find that [elsewhere](http://www.lfd.uci.edu/~gohlke/pythonlibs/). 
I think I've got an answer: http://www.reddit.com/r/Python/comments/juwja/how_does_quora_deal_with_tornados_memory_leaks/c2fezv7
Thanks. Let's hope "Tested with PyPy!" will be considered good style in the future. I'm not saying it's bad style to ignore PyPy at the moment. Just a nice thing to have. Even if it's just for the cult of "must be fast, regardless if I really need it".
I run Tornado on a very high request rate web service and have not experienced any memory leaks with cpython in the 2.6 tree. It's worth noting I am running 1.1 on that service.That being said I am running 2.0 on other services, just not with the same high level of concurrency and large requests/sec.
This site is the most fun I've had learning the ins and outs of any language. [The Python Challenge](http://www.pythonchallenge.com/)
congrats - you just made an idiot out of yourself in two places now ;-) besides who uses windows with python ;-)
FireGestures (or any mouse gestures extension). The Chrome equivalents are simply not as good.
I actually wrote the simplecv installer for Windows. If you run into any problems, please let us know and we'll be happy to help.
lol, ok :)
Project Euler can get kind of tough after the first few questions, so don't get too frustrated if you're having trouble. It's definitely great practice though.
[old thread with lot of resources](http://www.reddit.com/r/Python/comments/ixaaw/python_a_good_beginner_language/)
I tried that already, running install python-opencv gave me an error. So I tried installing through the downloads on their website, http://opencv.willowgarage.com/wiki/InstallGuide but I'm having trouble following it.
[Full Circle Magazine](http://fullcirclemagazine.org/) has been doing a monthly tutorial for the last couple of years.
what is the error?
is there any performance or speed difference between your simplecv and opencv? I'm asking because I need something that's fast enough to do 100x100pixel scan close to a hundred times per second for image recognition.
OK, seriously, don't just say, "I get an error," and then not tell anyone what the error was. Seriously, if you want us to help you, you've gotta help us in return. Tell us the error!
Sorry, it didn't give me an error. I did it and it installed fine. I was following this walkthrough: http://opencv.willowgarage.com/wiki/InstallGuide%20%3A%20Debian it's from the opencv website. I did apt-get with opencv and most of the required packages. of the two dozen packages, 1 or 2 didn't install correctly. But the problem came when I tried building opencv from source with CMake. I couldn't get that to work and was completely lost there.
I was thinking that too...
I've heard they're OK on Windows, but not on Mac or Linux, which kind of defeats the point.
what do you get from "C-h v major-mode" ? I'm troubleshooting mine 
How so? No expectation that it would perform worse than locks in the places it would be used. This is a high level construct, not at all to be used in low-level implementation - it is a high level alternative to locks which were already available.
So in general the python bindings for openCV are still really fast as they make calls right out to the native C++ openCV api. Anything done in scipy is also really blazingly fast. I've been working on a bag of features recognition system for simpleCV that does 11x11 pixel scans, converts them to vectors, and then does a distance calculation between a 256 code book of learned features. On a ~640x480 image this is a couple of seconds on my MacBook Air. It really depends on what you are doing with the 100x100 image patches. Explain your problem a bit more. 
thank you. and you should be the most voted comment here. being a windows user for years, i have seen my share of ridiculous installations trouble. if you add the 5-words term 64bit to the mix, you are in for a good mexican night. today i just keep quiet about it, when it's not available for windows (even though people do insist on calling these python modules cross-platform), i look on [here](http://www.lfd.uci.edu/~gohlke/pythonlibs/). if it's not on there. i just move on.
Here are the (combined) changes in this fork: [https://bitbucket.org/arigo/cpython-withatomic/compare/2.7..python_mirrors/cpython:2.7](https://bitbucket.org/arigo/cpython-withatomic/compare/2.7..python_mirrors/cpython:2.7)
**2.7**: Hmmmm... **2**: Hmmmm... **2.7**: Hmmmm... **2**: Sorry. That's not Numberwang.
Uh, what?
Glad to see Python's vital influencing by pizza.
That's pretty cool. How'd you write the script to do this?
This is "not really" STM, but the semantics are the same. Since CPython has the GIL, with this "with thread.atomic:" statement you can cause a block of code to be executed as a whole, just as if you used STM. The benefit of this is, that if you add real STM later, you don't have to change application code for it to work with multiple threads running at the same time. Of course, internally, there has to be a lot of stuff to be done to CPython for STM to work at all, but that's what pypy's here for. Its flexible nature and whole-program transformations (have you seen the transformation that turns it into a completely sandboxed interpreter? it's three screen-pages of sourcecode, a lot of which are comments) will supposedly make STM comparatively easy to implement. That part is different from the "with thread.atomic:", though, because the STM that pypy will have will first and foremost be interpreter-level, meaning a replacement for the GIL mechanism of CPython or the fine-grained locks that Jython has. It supposedly will, however, be exposed to the application scripts via "with thread.atomic:" or some similar method. Hope i could clear a bit of stuff up. Also don't forget to read [armins post on the pypy dev blog](http://morepypy.blogspot.com/2011/08/we-need-software-transactional-memory.html) for more enlightenment.
The direction of the arrows seems counter intuitive to me. If Python was influenced by language X, shouldn't there be an arrow from X to Python representing X's influence on Python. Python didn't affect X, X affected Python. Otherwise, pretty cool :D
only 2 hours are up yet. based on the first minutes of the 2nd one, the teacher is not super well prepared, he is typing all the examples and he types slowly with lots of backspacing, he confuses argc with argv, etc. You have to go to fullscreen playback to read the text being displayed, but then of course you couldn't also keep a python window open to try things out. There's no clear structure, no syllabus of what is going to be taught, just kind of a stream of one thing after another. If I was there, I'd'a nodded off.
Why exactly are open fds not closed once they drop out of scope? Does CPython also not do this? I mean, I understand that it's simpler not to, but it doesn't seem very friendly.
Yeah, how'd you define "influence"? Also how come all (most?) of the arrows seem to have their heads on the wrong ends, e.g. COBOL and fortran influenced PL/I and not the other way around?
They are closed, they are just not closed until a GC cycle happens. with cpython collection is predictable, when the last reference to an object dissapears the refcount hits zero and it is cleaned up immediately. with pypy when the last reference to an object disappears nothing happens. the GC will come along at some point in the future and cause the object to be collected. however while you are waiting for a collection many things can happen, eg using more FD's and exhausting them, or even having the program exit, meaning the destructor will never be called (see example below) class Temp(object): def __init__(self): print "exterminate, exterminate" def __del__(self): print "crush kill destroy!" t = Temp() the code above will call both __init__ and __del__ on cpython but on pypy will only call __init__ as the program exits before a GC cycle. this is one of the reasons why it is recommended to explicitly close/finalize your objects when done rather than rely on destructors a ref counting GC is an implementation detail and should not be relied upon if you want to write code that runs on multiple interpreters
I think you're just reading it wrong (as in, 'x influenced by y' is x -&gt; y)
Hm. That doesn't sound web-scale. Seriously though, thanks for the explanation! Edit: aw, downvoted for a silly joke :(
Interesting concept. Any package that inputs sound into numpy array files?
Curious to know how too. Looks like you could merge several nodes too: the Algols, Assembly (language), etc.
This chart implies Java was influenced by C#. It also doesn't seem to grasp that C# was written by the guy who wrote Borland's Object Pascal. Edit: Citation - http://en.wikipedia.org/wiki/Anders_Hejlsberg
Yeah, just came here to say that ;) Edit: Also, algol-w is present as two different languages: 'Algol-W' and 'ALGOL W'
Very cool looking. You have Assembly on there twice, once as Assembly and once as Assembly language. Same with FORTRAN and Fortran
If you go to the wiki entry on any language, there's an "Influenced by" section to the right hand side. As seventheapollo says, the arrows in OP's diagram mean "was influenced by" rather than "influenced". Cool diagram.
That's not a bash utility!
is a 80px by 240px area of a client that will display a name. I have 300-400 png's that are 80px by 240px. I need opencv to take each image and check to see if it's the same as whats in the previously mentioned area of the client. I need this to be done in under 4 seconds
Well, Bash-Utils became "Miscellaneous" over time.
To change *REDDIT* variable I have to edit the script. Not cool.
Looks like he spidered the wikipedia pages for each language that influenced Python, then those that influenced each until he hit assembly. The graph is probably drawn with DOT, or using the archaic GraphViz library.
I think they used graphviz for the rendering 
That's a nice setup, though personally I prefer [pydoc-info](https://bitbucket.org/jonwaltman/pydoc-info/) to pylookup. Instead of viewing the docs in a browser, you view it as a texinfo manual, which tends to be faster and easier to search. 
That's because of Wikipedia's [C page](http://en.wikipedia.org/wiki/C_(programming_language)) where it says Assembly and redirects you to the Assembly language. Same with Fortran 
I won't comment on the math aspect of it because I have no idea what the code actually does. I will say that reading it is difficult. Have a look at [PEP 8: Style Guide for Python](http://www.python.org/dev/peps/pep-0008/). If you can't get through that, here are some comments on the code's style: * *Most* Python developers use **four** spaces instead of tabs * Always put spaces in between operators to aid readability: ``x * 2``, not ``x*2`` * Perhaps not for this example, or others that involve math formulae, but I always try to use more descriptive variable names. For example, ``height`` is more informative than ``h`` -- especially for someone not familiar with math (like me) * You have some extraneous parenthesis... e.g. ``area = (0.5)*h`` ... that can just be ``area = 0.5 * h`` * Finally, if there is only ever going to be one ``f`` function (i.e., f should only ever return x^2), you don't need to pass it as an argument to the the ``trapezium`` function. You could also just define ``f`` locally in ``trapezium`` itself like ``f = lambda x: x**2``. Check [out the PEP8 package](http://pypi.python.org/pypi/pep8). It checks how well your code conforms to PEP 8. This is the result of your snippet: $ pep8 ~/tmp/t.py /home/lhr/tmp/t.py:1:16: E231 missing whitespace after ',' /home/lhr/tmp/t.py:2:1: W191 indentation contains tabs /home/lhr/tmp/t.py:2:3: E225 missing whitespace around operator /home/lhr/tmp/t.py:16:2: E303 too many blank lines (2) /home/lhr/tmp/t.py:18:1: E302 expected 2 blank lines, found 1 
I think it's mostly a matter of convention. You could read the arrow as "influenced by" or "inherits properties of". Reading arrows towards the top of a hierarchy is not that uncommon. At the end of the day, it's still a directed graph which is iso to one with the arrows the other way.
I second/third/whatever the request for the source code. Very interesting concept.
Feel free to change the script to make it hyper cool...
Foreach, varargs, autoboxing and annotations were added to Java 5 after C# had them. [http://en.wikipedia.org/wiki/Java_(programming_language)#cite_note-0](http://en.wikipedia.org/wiki/Java_(programming_language\)#cite_note-0)
I won't do that because I don't intend to use it. I'm simply giving you a feedback. 
i fixed that in a later version, maybe i'll repost one for all languages on programming
Thanks all for your comments. This was the very first graph I successfully created, later versions have several language variants merged, more languages, and fixed the confusing "influences" direction. :)
Another person here who looks forward to following this when there is an RSS feed. 
Licensing. PySide is LGPL.
Well yes, but I'd consider them duplicates and merge them. [Here](http://www.reddit.com/r/Python/comments/jvrgx/i_made_a_script_to_determine_what_influenced/c2fkdi6) he says he fixed it, but sometimes we do miss things.
Well, he said that before we have commented that so there was no way of seeing it
Thanks a lot.
Archaic? do you know of a better, non-archaic system that fulfills the same role? 
Dot is a part of GraphViz, in case anyone is confused.
This is nice. Another possibility: use dot's abilities to do sub-graphs to represent language families and reduce clutter.
The new Boston - This does loads of languages. It does kinda depend on your level though. It's a little on the beginner side - but given I want a nice and easy learning curve I used this for a while. At the end of the day one of the best ways to learn is to pick a project and then implement it. Ask questions as you go. Try asking here, StockOverflow and other forums. It really is the best way to learn. Once you have about 500-1000 hours under your belt you'll be amazed at how much you have accomplished. But you will need projects and goals for that. ie, write something to read stock data. Then make it efficient. Then build on that to pick out buying and selling points. Then make it email alerts. Then build on that or choose another new project.
Or numpy linalg.norm
&gt; Our main HTTP servers are an entirely separate tier. What do you use for your main HTTP servers then?
It is wrong. There are some problems... h=(b-a)/float(n) should be h=float(b-a)/n This sum_y = (f(0)+f(b)) should be sum_y = (f(a)+f(b)) you should not start lopping at "a" but at "a+h". Anyway, you should not use length "i" as looping variable because by doing "i+=h" you cumulate numerical errors. You should instead pre-compute the number of trapezoids and use an integer as looping variables. For example: def trapezium(f, n, a, b): h = float(b-a)/n sum_y = (f(a)+f(b))/2 for i in xrange(1,n): #notice from 1 to n-1 sum_y += f(a+h*i) return sum_y*h 
&gt;Most Python developers use four spaces instead of tabs I'm curious, what's the reasoning behind this?
Also: &gt;&gt;&gt; map(str.upper, ['hello', 'world']) ['HELLO', 'WORLD'] In 3.3 you will also be able to use ``type(None)`` as a constructor for ``None``.
Easy man I was looking at doing something like this myself I might have a fiddle
I have forked and am sending you pull requests! Awesome project!
Alex Gaynor (a lead django contributor as well as pypy developer) is actively attempting to improve pypy's compatibility with web applications. That being said, SQLAlchemy is still not performing as fast as it should on pypy. Basically, keep watching this space.
http://www.google.com/search?client=ubuntu&amp;channel=fs&amp;q=python+tabs+spaces+why&amp;ie=utf-8&amp;oe=utf-8
JUNG is newer and a little more pleasant to use (from Java at least), but suffers the same problems as GraphViz in terms of complexity when trying to render large graphs. I think [igraph](http://igraph.sourceforge.net/) is one of the better libraries for Python as well. I only refer to GraphViz as archaic because of its age and lack of development. I have a bit of a pet hate for it because I use it extensively, unfortunately it's still the best tool of its class despite its age. Don't get me wrong, GraphViz is an impressive project, it just has a few rough edges that tend to expose themselves after you've invested the time in getting it working.
It is really confusing. Somehow C# influenced Java and Java then influenced Python.
Earlier today i was thinking of writing a python script to detect duplicate mp3s, you might want to try that. There are existing programs to do it, but they all seem terrible!
Easiest way to do that is to an absolute difference, then check the mean color on the output -- on my low-end dual-core mac this is a 220 microsecond operation on an image of that size. some example code for SimpleCV: client = Image("path/to/client.png") for pngfile in pngfiles: png = Image(pngfile) if (((png - client) + (client - png)).meanColor() == Color.BLACK): print pngfile + " matches target" 
Check out the [CPython dev guide](http://docs.python.org/devguide/)
Seconded. I want a script that will import all my music into [beets](http://code.google.com/p/beets/), for some reason I can only import 1 song at a time. Beets can auto tag your stuff fairly well. Then once it's all imported I want to find all my flac music and make a vbr copy to another directory to import into google music. I'll totally help, I've just been too busy.
Please port wxPython to Python-v3.x. Thanks!
And Hope, And Clean. Python, developed in hope of clean pizza.
This should be really good way to master your skills. I wish I did the same many years ago and sold myself into a slavery to a Perl guru for a few months. Now I exactly know what to advise young Perl adepts when they ask how to became stronger. Good luck on your way.
This is actually interesting thing to do. PM me OP if you're interested in trying a team project (github). It could use mp3 as an example but it can be extended to any file. I have some ideas on how to make it faster than n^2 too.
&gt; type(None) I was going to say, why don't you use ``types.NoneType``, but I just realized it isn't in ``types`` anymore. It is still mentionned [on the constants page](http://docs.python.org/release/3.2.1/library/constants.html#None) though, [but not in the types page](http://docs.python.org/release/3.2.1/library/types.html).
This is especially useful for string joining. items = [1, 3.14, 'hello', (2, 3)] print ','.join(map(str, items))
http://github.com/marktraceur/stattr I'd love to have you! :)
Why not work on [OpenAnt](http://www.reddit.com/r/openant)?
Help out Tarek Ziade and others with [distuils2](https://bitbucket.org/tarek/distutils2/wiki/Home). Python badly needs a sane packaging tool.
How about find ./ -type f -name *.mp3 -exec sha1sum {} + | sort | uniq -Dw 40 where ./ is your path. Obviously, it requires a unix-like OS...
Something like...? * empty dict * md5sum target file * target_md5sum in dict: report it as repeated; not in dict: add it * next file The underlying dict implementation dictates the complexity. 
That's definitely how I would do it if I really wanted to do it in Python (which seems totally inappropriate for this kind of things, though).
Depending on the volume of duplicates, you might want to pipe to xargs or parallel, as -exec gets odd with huge directories (for instance, some folks have a terrabyte or so of mp3s). 
Why would you use map instead of [ s.upper() for s in ['hello', 'world']] ?
There are a few things that could be done to improve [autojump](https://github.com/joelthelion/autojump), my little python project. PM me if you're interested.
I'd think I'd want something that checks the track sans tags.
Do you know numpy? 
The real idea is that Files are identical by some other means of MD5; because this would not match me having a .mp3 version and a .ogg version of the same song.
IIRC, it's more efficient.
That is what I was going to suggest also. wxPython was meant as a joke, and duplicate mp3s suggestion isn't a pre-existing project. OpenAnt is fun and in need of help !
http://sagemath.org my default answer, because if this projects improves that could really make an impact. 
The pattern works with "key" functions too.
Just posted this yesterday! I'm working on a very basic sound processing library. A fellow redditor has already pushed some pretty awesome changes. I'm in the same boat as you, so I promise you this project won't be out of your reach. http://www.github.com/prezjordan/Melopy Could be a great opportunity to learn some version control (git/GitHub) too. That's what I plan on doing :)
From the login page: Server: PasteWSGIServer/0.5 Python/2.6.2 
It might a be a good idea to merge the functionality of generate\_{major,minor}\_scale(), and in fact, to extend this to support the other modes (Dorian, Phrygian, Lydian, Mixolydian, Locrian) since they all share the same sequence. EDIT: escaping underscores to prevent formatting
They both call another method, iterate(), which accepts a starting note and a pattern of half-steps between notes. Is that what you meant? EDIT: Or I'm sorry, do you mean replace them with generate_scale(type) ?
Yes, it was the latter that I meant. You don't have to necessarily change it to generate_scale(type), but the sequence used by generate_major() and generate_minor() is the essentially the same. Might be fun to extend the functionality of the triads too (e.g. change it to a Chord class with methods such as add_dom_seventh(), add_maj_seventh(), diminish_fifth(), etc). I'd be happy to look at it, but I'm not exactly in a comfortable environment for development right now. 
Great suggestion, I'll keep that in mind. And yeah, no problem, but whenever you feel like contributing - go right ahead!
That looks really interesting. But if I have to start learning a frame, I'd prefer Django.
Unfortunately I suck at math.
nope.
You're helping a dude cheat on his homework. :-(
Like an actual comparison of the content. In such a manner that the same song encoded with different software / settings would be detected? Without relying on 3rd party databases of known songs this would be an interesting and challenging project for an experienced developer. I'm on my phone at the moment so I didn't look to see if something like this has been done. Edit: http://musicmachinery.com/2011/06/25/finding-duplicate-songs-in-your-music-collection-with-echoprint/
Code me something that makes me money, slave.
You are being a bad slave
Blame UML.
/r/techsupport
I took a look at their source code a few days ago, and it's really rough. On the one hand, this could be great for the OP, since there's lots to fix and clean up and add. On the other hand, they should be careful about picking up bad habits.
Something along the lines. * Using your own hashing function based on actual sound data rather than raw byte data. * Sorting an array of N hashes may be faster than checking whole dictionary N times to check if x already exists. 
Your website is down. Some problem with drupal. What exactly is it about? I remember taking a look at it sometime ago but didn't understand what it did.
Yeah, I'm sorry. I've had a lot of web server issues since my computer got dead. The project is basically a web application that keeps track of a league's statistics--or you can just use it to keep track of your friends' progress in some competition.
Well, most frameworks are pretty similar, but OK.
Ah I just saw the readme on the github page. Any live demos online? It looks like a cool project, I'll certainly take a look at using it sometime!
If the Python code is a bottleneck, str.upper() is better. Most of the time, avoid that to keep from getting errors if you get a Unicode string.
Yesyesyes
See section 6. on Loops. http://wiki.python.org/moin/PythonSpeed/PerformanceTips#Loops
A simple program that asks the user to input first and laste names, an ID number, and the current date, and then stores it into a database by reverse chronological order. Then a separate list that lists all the entries in the database that are six months old. 
nice thing about writing mathematical code is that you've got all the time in the world to reason about it, and write test cases to make sure you did it right. i could never remember matrix multiplication until i had an assignment to (short story) implement it in assembly.
I agree about the 'really rough'. I was looking at it hoping I could learn something. When I realized that much of the code would need to be rewritten, I was sorely disappointed.
I need help finishing a script. I created a module that will: 1. say a word in Korean using Google Text-tospeech 2. Save the file as whateveryoutoldittosay.mp3 I need to finish it, but I'm not good enough to do so yet. Basicly. I want to be able to: 1. feed it a (UTF-8) text file, one Korean word on each line. 2. Have it make an MP3 file for each word on each line. I have been looking for someone to do this for me. I can paypal you a few bucks (not too many, I'm a broke student) for the trouble if you'd like. I can't figure out the text processing part.
After looking through the code, this is just an idea for the programmer that is obsessed with final objects (or just tuples/strings). def generate_minor_triad(start,returnType="list"): """Generates a minor triad using the pattern [3,4] (Returns: List)""" minor_triad = [3, 4] output = iterate(start, minor_triad) if returnType == 'list': return output elif returnType == 'tuple': return tuple([i for i in y]) Anyway just an idea...
Very nice idea, thanks for suggesting this on GitHub itself as well.
SymPy (one of the components of Sage) was needing someone to port it to Py3k; that requires very little math knowledge, but would be awesomely helpful.
I'm looking for dev for [new version of the political memory](http://dev.memopol2.lqdn.fr/) of [La Quadrature du Net](http://lqdn.fr) (defending net neutrality and protecting our fundamental rights of the internet). The long term idea behind this is to make a paradigm change: currently when you have to chose for who you will vote at an election you mostly only base you choice on non-concrete fact (promise, feeling, their political communication, historical attachment etc...). We want to give the possibility to people to base their choice on concrete fact like the way their representatives have voted on law propositions that concern subjects that matters to those people. Say in another way: people should be able to vote with information based on fact older than 2 months of political campaigns. [A description of the project](http://www.memopol.org/what-is-political-memory/) [Code](http://gitorious.org/memopol2-0) And it's in Django. Wanna change the world while learning python :) ? Ping Bram on irc.freenode.net#lqdn-memopol (or send me a pm).
1. Port stuff to 3.x. 2. ... 3. Profit!
and multiprocessing
would I be able to find out if the image client is contained inside png? Do they have to be the same image or can one be an image of the entire screen and the other be the visual element I'm looking for inside?
Not too much experience overall with it, but from what I have used, matplotlib is a nice package.
What languages do you believe would be best suited? 
Most language implementations with automatic memory management don't use refcounting for various reasons. Instead, they check with memory is no longer accessible. CPython's use of refcounts in a language with automatic memory management is unusual. Refcounting is more commonly used in languages without garbage collection like C/C++.
its phenomenal
http://stackoverflow.com/questions/2148039/is-there-a-library-for-python-that-will-generate-hierarchy-tree-diagrams seems to have some good suggestions.
I actually saw that when I first googled the issue, but * The answers are a year and a half old. * pydot, as one of the follow-ups mentions, doesn't make very good-looking trees by default and it looks like a bit of an undertaking to get it to produce pretty output. I'm willing to put that time in, but if I can avoid learning a bunch of new tools for a minor feature of a major project, that'd be cool. * Networkx looks good, but it uses matplotlib, and gives deprecation warnings on import. Not reassuring. Also it's a little more 'robust' (read: complex) than I need, not that this is inherently bad. Thanks for the post tho :3
Given a list of male and female first names, and given a list of surnames, give me a function that will return a random name, with a given percentage ratio of male/female. I need a function to do this to populate my database with random data. I can point you to name lists. Let me know if you're interested. We can even put this on github and open source it.
Quick and dirty solution: output dot file syntax and compile it with dot instead of using pydot. For a more serious approach use Networkx, very strong and still constantly maintenaned (see https://github.com/networkx/networkx for last version). Deprecation warnings really aren't a problem.
I don't have anything running, no. It's easy to run, though!
[Nearly same topic (with answer) just right here](http://www.reddit.com/r/Python/comments/jws24/best_and_easiest_way_to_create_tree_diagrams_in/). For short: either [Networkx](http://networkx.lanl.gov/) (part of matplotlib), [pydot](http://code.google.com/p/pydot/) or writting directly [dot files](http://www.graphviz.org/) and compile them with dot.
Ok, now I get it. Anyways, the real meat of the computation is in the hash function; the comparison of hash keys will be a tiny percentage of your CPU time (and adding an item to a dict is already [O(1)](http://wiki.python.org/moin/TimeComplexity)) 
That is why I moved to Ruby :( despite of all beatufullness of Python, I'd preffer to write like this: print items.map(str).join(",")
C/C++ seems to be the best, you don't need "clever" python things here, just fast disk I/O and quick computations, both on large data set.
graphviz might be worth learning. It has come a long way in the past couple years too. But you are right, ugly output is easy, pretty output takes some learning.
doesn't the [futures](http://docs.python.org/dev/library/concurrent.futures.html) do this ?
if you don't like it, or have proof, modify wikipedia ;)
just me. no us here.
didn't know there was a pep8 package... seems like a useful tool.. Thanks!!
it's not homework, I just found it interesting so, I decided to write a porgram in Python, thats all
did you mean "not from 1 to n-1" on that comment? 
Are you french? I am, and I code a bit in python, I might be interested by the project, particularly by developping vote visualizations etc keep me informed if you think about something that might be useful for your app (how many people are working on it, and what it lacks now?) 
He wants to draw something like an histogram, not a graph in the mathematical sense.
What's so much better about this than, say, commands.getoutput? More versatility? 
What the heck is that thing that looks exactly like github?! Woah.
Consider asking questions in r/learnpython. You want to use matplotlib / pylab. I will use numpy to generate sample data import numpy.random import pylab sample = numpy.random.normal(0, 1, 1000) # generate some data pylab.hist(sample) # build an histogram with default settings pylab.show() # open a window with the chart 
My bad. Matplotlib them like it as already been suggested or the python binding for gnuplot.
For some reason, I always seem to lean towards the quick &amp; dirty solution. Must be something about having to pick up an API for a quick solution, for what appears to be a dead simple language.
If you are europoean and know or are willing to learn django we need some help here: https://projets.lqdn.fr/projects/mempol/wiki/Political_Memory
Yes I'm french while I'm leaving in Belgium. We have all the European Parliament vote data (but some of the old one can come with importations problems). We are already doing some visualization on the votes tracked by la Quadrature du Net (I love visualization :p). Just before showing it to you, a concept: on each vote we define the best way to vote for each amendment and we add a weight to each of them. In result each eurodeputies get a score. All our current visualizations turn around representing how each group/country/eurodeputy have scored. [The tracked votes](http://dev.memopol2.lqdn.fr/votes/ACTA_resolution/). There is visualizations on each vote and on each part of the vote (and on each eurodeputy page). If you have idea for better visualizations or want to do some don't hesitate to share. We are currently using [matplotlib](http://matplotlib.sourceforge.net/). I'm currently the maindev, a various number of contributors come and go. We are principally looking for django devs, webdevs and webdesigners. Feel free to drop on irc.
Great tutorial for starting with Matplotlib. Really helped me to get started. thanks!!
du coup on peut se parler en français, à mon avis c'est intéressant, mais du coup la plateforme devient partisanne (eg. la quadrature du net donne le vote qu'elle préfère). Ca pourrait être intéressant d'avoir plusieurs "vues " des votes selon son affiliation politique, qui modifierait les poids, et selon qu'on est plutôt de tel ou tel parti, on peut voir si tel ou tel député a les mêmes idées que nous ou non. Par exemple on se log dans le site, et on s'enregistre sous un certain profil politique (je pourrais dire "j'adhère aux vues de la quadrature du net", et dans ce cas les résultats me sont présentés avec les pondérations de la quadrature du net ,etc. ) 
Well, in a english speaking place I prefer to speak in english so anyone can understand ;) Yes, this result in a partisan platform. But anyone can install it's own platform. In fact, since this will be a totally open platform it will be easy to build a my.memopol.org that allow you to merge all the memopol of citizen groups that represents your interest (even with adding ponderations). I personally don't want to code a central application where you merge everyone, this will introduce so much complications and problems and centralization on the internet is uncool. Also, the political memory is a activist campaign tool from the start, it's hard to share the same instance. To describe you this in more detail the idea is the follow one: - choose for who you vote currently sucks (detailed in first post) - we can identify 2 main reason to that that we can try to patch: to small human memory (you forget or don't even know which representative have done and when) and complexity of the system. - memory is easy to patch from a programming point of view (build an app that store all those data, show them, allow to do queries and allow user input) - complexity is the tricky part So, how to solve the over complexity of the system? Well, we can't fix that with code. But the trick is that there is people that can solve this complexity problem on some specific parts. For example I trush la Quadrature du Net on the internet related topics. So the idea is to ask la Quadrature du Net to provide the best way for elected representatives to votes (with ponderation) on votes related to their subjects. With this information I can see which elected representative have represent the best my interests. So the idea is to have every citizen group that represents my interests to have a political memory with those information. Like that I can have a pretty accurate view for who should I vote for. This is not limited to situations when I have to chose for who I will vote, we can very easily think of other applications. For example it will be very easy to have a RSS of the lasts actions of my deputy on all the subjects that matters to me to see how he is behaving. Hope this clarify it a bit.
the class bool (from which True and False are instance) is a subclass of int. if you look at the doc of bool, you will actually see it: class bool(int) 
Is there some reason why bool isn't a base type?
What would be the advantage? Booleans can be calculated with so it only makes sense for them to be integers. Case in point: &gt;&gt;&gt; True + True 2
`bool` is a subtype of `int` for historical reason. Once upon a time, when Python had no `bool` type, programmers used `1` for `True` and `0` for `False` (like in C), and it’s why `bool` is a subtype of `int` and the special method name `__nonzero__` was not `__bool__` before Python 3.
You might want to make sure the most recent version of matplotlib got installed - I don't see deprecation warnings when I use networkx. I've found pip/easy_install will sometimes install matplotlob 0.91 rather than the current version.
yes. I edited (fixed)
that could also be implemented using `__eq__`. it's for historical reasons/backwards compatibility, really.
Love this. Please can you keep it in one single file "envoy.py" and include the license on top of that file? I can see including it in some of my projects and this would make it easier to keep in sync. EDIT: it a timeout option?
I'd rather suggest using [django-picklefield package](http://pypi.python.org/pypi/django-picklefield). It provides general solution for storing serialized data in db, not only the special case for vectors and 2D-matrices.
Sorry I don't quite understand your qestion
Sorry, but how does that make any sense? You can't add booleans, you can only "and", "or" etc them. Addition on booleans is not something generally defined/common.
GitHub is for Git. Bitbucket is for Mercurial. Mercurial is a distributed version control system, much like Git.
commands module is deprecated since Python 2.6. It's a pity they did not provide something as friendly and easy to use in standard library.
Which is also why Python allows WTFs like &gt;&gt;&gt; print True + False 1 
&gt; You can't add booleans, Yes you can, at least in Python (and C): &gt;&gt;&gt; (1==1) + (1==3) + (1!=4) 2 whether you should be able to is another matter.
Yes, as I've said, it's not generally something defined, so I wouldn't consider it to "make sense" that one can "calculate with booleans". C (prior to C99) doesn't have a boolean type at all, of course.
Thanks, I tried matplotlib and its totally awesome.