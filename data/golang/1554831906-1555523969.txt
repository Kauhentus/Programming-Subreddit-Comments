This looks nice, but more importantly, what theme are you using in your cli? I've been looking for something like this for a while now..
You need to call `wg.Add` before spawning goroutines.
First tried with chi, but then reduced everything to just the basic go lib. No middlewares or something and no different behavior. I am no longer in the office, but will upload the testcode tomorrow, with some screenshots
To make it easier for people to help you (and probably end up finding the solution yourself), mock up your problem in a go playground: https://play.golang.org/ I would also recommend using some whitespace wildcards, that's probably what's not matching.
I think I'll take this to GitHub...
playground is giving me server error when i try to share. Here is the full code i am working on [https://pastebin.com/xidYJjXe](https://pastebin.com/xidYJjXe)
https://eagain.net/articles/go-dynamic-json/
[regex101(https://regex101.com) has been my go-to to develop regex.
Fast version, no error checking: package main import ( "bufio" "bytes" "io/ioutil" "log" "os" ) func main() { data, _ := ioutil.ReadFile(os.Args[1]) words := bytes.Split(data, []byte("\n")) words = words[:len(words)-1] // remove empty last word due to trailing \n w := bufio.NewWriterSize(os.Stdout, 1&lt;&lt;18) for _, a := range words { for _, b := range words { w.Write(a) w.Write(b) w.WriteRune('\n') w.Write(a) w.WriteRune(' ') w.Write(b) w.WriteRune('\n') } } w.Flush() }
Ohhhhhhh. I get it now. The docs are assuming that there would be times when you would need to actually include logic in the get/set methods. Like clean a string value or check for validity.
I highly recommend using hashcat instead. All of which you are trying to accomplish is supported out-of-the-box with hashcat.
[removed]
&gt;Edit: One last edit; if you compile it, it comes down to on par with Perl, even writing to stdout, if you use a bufio.Writer. Perl does output buffering, but go does not. You need to explicitly ask for that in the code. This makes sense, because when you're running the programing via the \`go run\` command, you're really doing a compile to a temp directory, then executing the program. \`go run\` just saves you typing, not time.
Indeed. Time will measure how long it takes in total. Honestly it‚Äôs pretty amazing it only takes 200ms to compile it when it takes about the same time to actually run it!
[removed]
[removed]
Great work, bro üëäüèª
make the switch to go modules - it's real easy: mv $GOPATH/nit ~/somewhere_else/nit cd ~/somewhere_else/nit/ rm -rf vendor/ go mod init . go mod tidy go mod vendor go run -mod=vendor cmd/nit/main.go # ...If that works... rm -f Gopkg.lock Gopkg.toml
TIL backslash is /
Checkout oh-my-zsh + powerlevel9k + hacker nerd font + dracula as well - I use it for my setup: https://github.com/johnwyles/bootstrap-macos
Why support XML in 2019?
nice! recently wrote a blog entry on some tricks with this and git: https://johnwyles.github.io/posts/ssh-tricks-for-port-forwarding-multiple-github-accounts-and-using-alternative-ports/
There is a go.mod
Not surprised. The big cloud providers are probably hiring all the Go developers they can around kubernetes, docker, and other network related services.
This looks pretty cool, but the feedback I'd suggest is to allow the type,const group to be repeated. I feel like a common pattern for me when I write Go is to do something like: type Foo int const ( A Foo = iota B C ) type Bar int const ( X Bar = iota Y Z ) Also, it seems like struct and interface types are forced to be defined at the top of the file. I find that to be a bit problematic especially for private struct types that may only be used internally within the file and would benefit from being defined closer to their use and not fill up an opening stanza that may distract a reader of the code. Similar to the type,const group repeating, I find that when I write/read Go the struct tends to be followed by the initializer/constructor and it's methods. type Foo struct{} func NewFoo() *Foo { return &amp;Foo{} } func (f *Foo) Run() {} type Bar struct{} func NewBar() *Bar { return &amp;Bar{} } func (b *Bar) Run() {} Grouping a type with the code associated with it tends to group better in my head than just grouping all declarations together.
[removed]
I think it depends on the company, and the negotiation skills of the individual. I have 10+ years of relevant work experience, 5+ of which are solely in Go. My salary is very average, so much that some of my friends doing Node.js and Ruby development have significantly higher compensation packages just because they work for bigger companies. That being said, I am grateful for the salary I currently have. I enjoy working with Go a lot, work-life balance is good, I am surrounded by nature, I can go skiing any time I want, I feel safe walking on the street. Go have not only made me a better programmer, it also allowed me to improve my life significantly. I hope I can see Go take 1st place one day üôÇ
Yeah, I've seen this pattern in the standard library as well. I think it makes sense to support both "opinionated" and "standard" grouping; I will work on it. Thanks for the feedback.
Also startups with extra money hiring Go developers to fix their PHP/Ruby/.NET scaling issues.
&gt; Go have not only made me a better programmer, it also allowed me to improve my life significantly This is very good indeed!
lol :D TS
I use [github.com/google/uuid](https://github.com/google/uuid) but then I'm almost completely ignorant of the issues you're talking about (and never likely to achieve the kind of scale where any of them become relevant). I also refactored recently to do most of the UUID generation in the database, using the Postgres function \`gen\_random\_uuid()\`.
If you don't need time-sortable keys then UUID is _good enough_. I mean, generating random bytes would allow you to fit more values into the same number of bits, but you can worry about after your large enough to purchase Google.
I have passing familiarity with DLL injection as my product earns its stay with DLL injection, but I'm not on the part of my team that spends time talking to the Windows kernel and writing C, so I am not coming from an expert perspective. In Windows (the os where this CVE applies) I was under the impression that pretty much any elevated process can use DLL injection (unless you use some of a group of countermeasures which can, in turn, be defeated in a variety of ways when an attacker is determined), so is this CVE discussing DLL injection from a non-elevated process? It appears that this is the case because before it looked at environmental variables whereas now it looks at (without doing much deeper research) calls that require elevated tokens on the process. Is this correct?
Great article. Question: How much refactoring of the Go code was done after the port other than rewriting in idiomatic Go?
Given Golang exists in the same spot in my heart as Lisp and C. My only problem with Go is that i caved and added Object Oriented features which, from my experience, is, of course, slapping people in the face (I can point you to a semi-popular code base right now where you can bounce around to 6 different interfaces before locating any real code - that's called spaghetti) In any case its good people are paying high for Go so it encourages more people to learn and make languages like it. I'm of the opinion that the first language to add Algebraic types and a decent type system to golang's minimalist procedural core will do well.
oops! overlooked it and just saw the `Gopkg.toml`
Oh yeah definitely, I do not have control over it, it's from an application I need to use.
Yep, that worked perfectly, thanks.
Yes, that's what I ended up doing. Thanks!
Not to mention blockchain, blockchain, blockchain...
&gt; (aside from the non-SEO friendly name) Just use *golang* instead of *go* in your searches.
Probably because of its use at Google..
False. DynamicDNS advertising spam.
I do, I just question the naming of a language that to me seems to otherwise have made so many intelligent design choices and thus we need a SEO friendly alias.
I see what you are saying now. I agree that they could have come up with something better like: .dot oh wait. /s
If passing a single `int` argument to `fmt.Println(...interface{})` a Go compiler will be able (sometime in the future) to create specializations of the functions involved which substantially reduces the amount of code the compiler needs to analyze. I am not sure the mainline [golang.org](https://golang.org) compiler, which is optimized for fast compile times, will go this way because it would measurably increase compile times. I agree that it is harder to automatically prove that arguments passed to `fmt.Println` never escape to heap in general.
That funny I got a genuine laugh out of me. Yet, .net is a framework encompassing many languages. So you would usually search for C#, F#, VB.net or something along those lines. Too, even '.net' is far more friendly the 'go'. Gee, almost anything is more friendly than 'go'. Its as if they chose the least search engine friendly thing they could find.
I know you are doing it in Go, but I've been learning C, and love perl, so I thought I'd take a crack making it really fast. Below is some pretty ugly beginner C, but it is about 3x as fast as the perl version on your small input size. And that speedup grows as I increase the input size. It only reads from stdin at the moment. I haven't made it to file IO in K&amp;R yet :) On your input set the average time for C version is .114, perl is .291, both reading from stdin and writing to a file. I have found getting Go to be speedy with string related things is hard, and possibly not worth it if it's just a string munging script. Perl and Python are really really good at strings. Awk is also really good, and most things that you would do in C or Go are done under the hood in a scripting language, so you don't gain much. Only by getting really nasty in C have I gotten a speedup. The 'conventional' C, reading with getline, and writing with printf, was on par with Perl. Compiled as `clang -O2 -o double double.c` #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #define START_SIZE 50000 /* Start size of arrays */ #define PROGRAM_NAME "double" #define MAXLINE 1000 int custom_getline(char *line, size_t max, size_t * size) { char c; size_t count = 0; for(;;) { c = getchar_unlocked(); if (c == EOF) { return -1; } if (c == '\n') { break; } if (count &gt;= max - 1) { fprintf(stderr, PROGRAM_NAME ": running out of room, truncateing string\n"); break; // We don't have enough room in our string } line[count++] = c; } line[count] = '\0'; *size = count; return count; } void c_write_line(char *a, char *b) { char c; for (int i = 0;;i++) { c = a[i]; if (c == '\0') break; putchar_unlocked(a[i]); } for (int i = 0;;i++) { c = b[i]; if (c == '\0') break; putchar_unlocked(b[i]); } putchar_unlocked('\n'); for (int i = 0;;i++) { c = a[i]; if (c == '\0') break; putchar_unlocked(a[i]); } putchar_unlocked(' '); for (int i = 0;;i++) { c = b[i]; if (c == '\0') break; putchar_unlocked(b[i]); } putchar_unlocked('\n'); } int main(int argc, char *argv[]) { char **lines; char *p; char *line = malloc(MAXLINE); size_t size; size_t curr_lines = 0; size_t max_lines = START_SIZE; lines = malloc(START_SIZE * (sizeof(uintptr_t))); if (lines == NULL || line == NULL) { fprintf(stderr, PROGRAM_NAME ": can't allocate memory\n"); exit(1); } while (custom_getline(line, MAXLINE, &amp;size) != -1) { line[strcspn(line, "\n")] = 0; /* Remove newline */ p = malloc(size); if (p == NULL) { fprintf(stderr, PROGRAM_NAME ": can't allocate memory\n"); exit(1); } strcpy(p, line); /* Check to see if we need to add more space to array */ if (curr_lines &gt;= max_lines - 1) { lines = realloc(lines, (sizeof(uintptr_t)* (max_lines *= 2))); if (lines == NULL) { fprintf(stderr, PROGRAM_NAME ": can't allocate memory\n"); exit(1); } } lines[curr_lines++] = p; } /* Double by printing all combos of words as "ab a b" */ for (uintmax_t i = 0; i &lt; curr_lines; i++) { for (uintmax_t j = 0; j &lt; curr_lines; j++) { c_write_line(lines[i], lines[j]); //printf("%s%s %s %s\n", lines[i], lines[j], lines[i], lines[j]); } } return 0; }
Holy shit. This is one of those tools I would never even dream of, until I heard of it, and now I probably won‚Äôt be able to live without it.
I faced this situation a few days ago doing a full text search on top of an open source project I work on: bleve VS the world. Whereas Bleve is quite new compared to say Sqlite, I couldn't find any argument supporting it compared to other players (yes it's go based and? CGO works perfectly fine in the Go world). Maybe someone here knows better and would say why would one pick Bleve compared to Sqlite?
Who signed going to pay for false sharing and pointer dereferencing? Slices are fast, much faster than linked lists because of how modern CPU architectures are Cache dependent with scalar pipelines.
I lead a team of 6 engineers at my startup and we are a Ad serving company with niche data platform. All our ingestion pipeline is in Go or shall move to Go in near future. Why? Because I don't have to sacrifice performance coming from writing low latency distributed system in C to serve Ads. Our data crunching platform is still Java though, it is not going anywhere.
I was looking at a survey like this recently, what I think isn‚Äôt captured well is cost of living. A lot of Go developers are in the Bay Area. How can you evenly compare a language that is heavily employing people at 100k+ because of the region to a language, like Java, that employs people making as little as 40k somewhere in the Midwest. Now I bet if a company in the Midwest used Go, then those developers could be making more than local Java devs, but global is difficult to really compare.
If you have resource, would you rewrite Java code with Go?
I have 9yrs of web dev professional exp. And Have none of GO.
FWIW I merged those a while back, if anyone reads this. Kinsumer is well maintained, I just got bogged down with other tasks and was a bit behind.
Hotdog || !Hotdog
ElonMusk outranks everyone else in becoming the tech influence of the year? LOL. This guy is crazyüòú
Are people really paying conference fees for the privilege of listening to this?
I hear what you‚Äôre saying about the ‚Äúgo‚Äù search keyword, but really, in practice, just use ‚Äúgolang‚Äù. It works.
Obviously it always depends upon your skills and the employer. That‚Äôs universally true. That should go without saying. But for you young peeps out there thinking about what to do next: get good at Go. I say this as a middle-aged person who made a lot of money off of Java, which was my first and true love. Java won the enterprise space; for about two decades now. I caught that wave at exactly the right time. It worked out really well for me. I started on Go about 5 years ago... early, but I could see the wave coming. The wave is here. The rates for experienced Go developers are high. Forget any of that functional language BS. Forget Rust. If you want a salary to pay for your future wife and children, pick a ‚Äúlegacy‚Äù language like Java or C#, but also learn Go. It‚Äôs a fact that it pays like a motherfucker. Source: pays me like a motherfucker. If you think this is one person‚Äôs opinion: do one of those remindme things for 5 years from now.
If it is a data pipeline, then you can't. All big data projects have libraries mostly in Java or Python. Only Beam has Go libraries now.
I'm not sure if I am talking about what works and what doesn't. Clearly using "golang" works.
This is a valid point, it would require a fair amount of time to transition the rest of my workflow into Hashcat. I've been experimenting with AWS GPU instances and hashcat recently, so maybe I should make the change!
Fullstack, Go. No, thank you.
How much are golang devs in Munich Germany doing? I might be interested in jobs in Munich soon
Plot twist: the big wound is actually in the mind...
A nice option to have it less opinionated, is to make this "fixed" from a configuration. And if you use some smart logic, you can also do some bundles like the other person wanted.
What you are saying is only true if you are ranging over the data structure. In this case the memory prefetcher can reliably load needed data into the cache when using a slice. So if you do frequent random access or iterations absolutely use a slice. I imagine that false sharing is limited by Gos size aware memory allocator which will lay out list elements next to each other.
People in 2019 are moving to binary protocols while this guy is living in 1999.
.NET scales worse than Go?
Ok, will try to answer to both of you. Just because i lived in 1999 and still working in the development area i'm sure that there is no place for black and white only. In some cases XML has more advantages over JSON and it still is pretty spread. Also, you may noticed in spec the ability to extend the list of allowed data formats just by adding the new identifier (first byte of the body request). &amp;#x200B; the "moving to compact binary protocols" has been going all the time and i dont think that this will be done someday :)
!remind me 5 years
I like the assurance with UUID's that they won't collide. I mean, I know with enough bits I could just assume that random bits won't collide, but my stupid primate-level back brain will forever be thinking "hey this bug could be in my code, or it *could* be an id collision, right?"
This might help: [https://github.com/grpc/grpc-go/blob/master/Documentation/gomock-example.md](https://github.com/grpc/grpc-go/blob/master/Documentation/gomock-example.md). Shows how to create a mock stub for the client and make assertions about the response from the server.
I meant the '99 part more of as a reverse of the meme, nothing against people in the industry since before my time. My point is that I haven't really heard of any advantages of XML over JSON and certainly while working with JSON or Protobuf it never crossed my mind to use XML. While XML has namespaces, inheritance, schemas I personally haven't encountered problems requiring such features when developing web services for today. There's a good reason XML is least used today, except in legacy systems, which I would like to exclude of any such discussions, simply because they use it due to lack of choice most of the times. Also, the reason more and more organizations are leaning towards binary protocols is mostly to save traffic. Protobuf has a CPU penalty while serializing/deserializing(there are flatbuffers but they have their limitations) but organizations still choose it, simply because the network traffic it saves is tremendous in high traffic systems. It's even more useful for system-to-system communications since the systems don't need human readable text anyways and as we know microservices today talk to each other all the time. This is the scenario where XML fails imho. You have huge overhead to represent data that can easily be represented with JSON for maybe half the bytes in size or even protobuf which would maybe reduce it to 1/4 of the XML size. Then again, traffic isn't everything. The larger payload means more data will be stored in the application's memory. Multiply that by millions of requests per second and you get a system that is slower because it has to: 1. Read larger streams of data 2. Use more memory, which puts a strain on the GC 3. Will respond slower due to the slower GC cycles Multiply that by a scale of thousands of containers and you have maybe 50-100GB of memory more + more traffic to pay amazon and/or google. Not to mention that if your client or server is javascript based you need additional xml parsers on top of the built in JSON.parse/JSON.stringify which seems like an absolute waste.
Nice!
You mean: give the option to enable whatever rules you want to use?
&gt; if you even had a sql manager to build, you will never ever read millions of rows. &gt; &gt; (Hint: pagination) Of course you don't *actually* read millions of log lines. Nooo, no pagination please. You may want to jump to specific time and look around. I don't want to handle pagination, that's why I search for a TUI, which handles virtual rows.
Quite true, but at the same time Go is the language I know that less often makes me feel like I need a search engine.
This is actually more native. They draw directly to the window via OGL. This means they're very low level. GTK, Cocoa, and UWP Eventually all draw either by the native OS Calls or via OGL Calls. This skips all of that. Electron isn't native because it's a web based renderer. There's thousands of more layers of abstraction between Electron and this.
RemindMe! 5 years "Are you rich yet?"
RemindMe! 5 years "Are you rich yet?"
I will be messaging you on [**2024-04-10 13:55:22 UTC**](http://www.wolframalpha.com/input/?i=2024-04-10 13:55:22 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/golang/comments/bbczgv/stack_overflow_developer_survey_2019_go_is_ranked/ekjphbp/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/golang/comments/bbczgv/stack_overflow_developer_survey_2019_go_is_ranked/ekjphbp/]%0A%0ARemindMe! 5 years ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
!remindme 5 years
&gt; .NET scales? ;)
It depends on where your bottlenecks are, and the bottlenecks might not matter for your product, but Go scales a little better than .Net Core in terms of resources demanded and ease of concurrency, and .Net Core scales significantly better than .Net Framework in terms of resources demanded. Also, if you rewrite an application that needs to scale, chances are the rewrite will scale better regardless of the language because it's meant more for the current demands than the demands when the company was a startup. You can argue that Go will scale better in terms of many engineers working together on a project because the language prizes simplicity and avoids things that can cause bad code, but that enters an area where some will argue against you.
&gt;That should ~~go~~Go without saying.
Windows, why do you do that üòÖ
RemindMe 1 year "Have you made any progress?"
 RemindMe! 1 year "Have you made any progress?"
 RemindMe! 1 years "have you made any progress?"
 RemindMe! 1 year "have you made any progress?"
 RemindMe! 5 years "So how did it go? have you mastered golang, K8s, docker? did it paid off?"
We use https://github.com/gorilla/websocket. Pretty well documented!
Why do people use socket.io nowadays? Every browser support websocket.
I didn‚Äòt thought about Not using it, some years ago i Used it at a Node.JS Project, so it came to my mind. But i will try Websockets!
Thank you! I will give it a try!
I wonder why this is not mentioned in the release notes.
No, why would I. I do not think Go is some silver bullet or the best language ever. It is fairly small language missing a lot of higher level constructs needed to abstract out lower details. Also, the compiler is like 10 years behind where modern Optimizing compilers are. Data crunching will remain with Java because it is tried and tested and predates industry unlike Go. Introducing Go, sometimes it lets us down in simple tasks. But I still have high hopes from it, hopefully Go 2 with generics and proper error handling will finally make my Java coworkers to embrace Go and not be pedantic professors of language theory.
They joy of raw websocket is you can build your own solution on top, your own socketio.
I figure Node devs pretty commonly use it still because it still works and used to be necessary. You don't need to fix old stuff that isn't broken, but for new stuff using a fat library when websockets are fully supported by browsers and are included in rxjs, etc is unnecessary.
I always wondered, if people actually use bloom filters in production, considering having 8 hash functions.
It's a resourcing thing. You can trade off a few hash calculations against a disk seek or a even a scan, it can be worth it. Flash changes the tradeoffs again but can still be worth it. They are used extensively in production :)
It's definitely doable. The trick is to use only two 64 bit hash values h1 and h2. These can be computed during a single scan, and then you use linear combinations of h1 and h2 as hash functions. It can be proved that this gives the necessary statistical properties. This particular implementation uses MurmurHash and h1 + n\*h2, where n = 1, 2, 3, ... Here is a link to the relevant code: [https://github.com/yourbasic/bloom/blob/master/filter.go#L95](https://github.com/yourbasic/bloom/blob/master/filter.go#L95)
Then why are you here and what's the point of this comment?
Shameless plug, my [Bloom Filter Calculator](https://hur.st/bloomfilter/), complete with live updates and fancy graphs and code to nick to calculate your own. I also suggest looking up [Golomb {Coded,Compressed} Sets](http://giovanni.bajo.it/post/47119962313/golomb-coded-sets-smaller-than-bloom-filters) if you like Bloom filters. They're a similarly fun, surprisingly-simple structure with some different trade-offs in exchange for higher space efficiency. I used them as my [first Rust project](https://github.com/Freaky/gcstool). Nice mix of language, algorithm and data-structure learning, I'm sure it'd go well with Go too.
It also might generate binaries that are autostereograms when opened in an image viewer. That doesn't have much to do with today...
[removed]
Then you end up with people berating you for using "golang" instead of "Go" in your articles so people can find it.
It's a keyspace issue. Depending on how large the bloom filter, more hash functions can both decrease and increase collisions. Usually have to measure your data set against multiple bitset sizes + number of hash functions to find a sweet spot.
When I saw how it was using waitgroups and goroutines for such a tiny amount of work I cringed so hard. I get it, it‚Äôs illustrative. If I wanted to use goroutines to speed it up and add parallelism, I would have it open several copies of the file, seek to a different place, and each start from there. Or, if you are reading the whole thing together, just have them start at different places in the original data. I probably wouldn‚Äôt read the entire file in either but that only is an issue if the file is too big. There‚Äôs some special cases such as splitting a word between two routines that needs to be handled, but it should be a lot faster.
Comments here are very surprising (and sad). This was one of the best rated talks of the day by actual attendees.
Thanks. oh yeah, it's a shame mangacraft never released the source code after shutting down :/
And the paper that shows it: https://www.eecs.harvard.edu/~michaelm/postscripts/rsa2008.pdf
Glad to see the use of "[errors.is](https://errors.is)" replace type assertions. Seems considerably more readable and intuitive than a type assertion for something as common as err checking.
I would think a single CPU core could count bytes faster than any I/O device should be able to stream data. I'm not sure how opening multiple handles are going to improve I/O throughput unless you are reading from RAM instead of the hard drive.
As one of the co-authors and maintainers of oklog/ulid, I'm not sure I'd call ULIDs it an _upgrade_ from UUIDs, really. They just serve a different purpose. Principally, that's allowing each node in a distributed system to generate its own IDs, and have the resulting aggregate pool of IDs have some rough time (and lexicographical) order. If you don't need those properties, then UUIDs may be sufficient.
A *much* easier way to install Go is to use the installer... https://golang.org/dl/ If using Homebrew, be aware that it changes permissions and the owner of the /usr/local directory, which it shouldn't be doing. Macports is an alternative that doesn't do this.
If you are just getting started I‚Äôd suggest to start with a simple crud app to play around with Vue and Go. Once you got that narrowed down you can think about serving images and then streaming files. Don‚Äôt start with a complicated project where you‚Äôll be discouraged early on. There‚Äôs a lot of simple tutorials on YouTube and the rest of the internet. Just build a todo list to start.
Also check out [gobwas/ws](https://github.com/gobwas/ws). I personally prefer using it over gorilla's (though the better perf is nice too).
Hmm, good to know :)
This is great! Thank you.
I created a proper example using streaming chunks instead of trying to read the whole file into memory. A lot of new developers default to `ioutil.ReadFile()` when it should almost never be used in production. https://github.com/pmorelli92/go-word-counter/pull/1 ``` file, err := os.Open("input.txt") if err != nil { log.Fatal(err) } defer file.Close() words = make(map[string]int) scanner := bufio.NewScanner(file) scanner.Split(bufio.ScanWords) for scanner.Scan() { w := scanner.Text() words[strings.ToLower(w)]++ } if err := scanner.Err(); err != nil { log.Fatal(err) } ```
Why did you pick oklog/ulid over segmentio/ksuid? - 100 years wasn't enough time for the timestamp? - Needed millisecond resolution (without concern about leaking too many details) - 160 bits was too much?
Good job! It's great to see more teams looking at ways to do ML / NLP without leaving Go. There is quite a huge uphill climb to see Go replace python anytime soon, but for many of us simply keeping/ building one more part of the overall system in Go is a win.
Thank you! In retrospect building the infrastructure in Go was a no brainer. The API to the platform is still Python (because we're running TensorFlow and PySpark workloads) but we try to use Go everywhere else.
I don't think tinygo uses threads, but it also lacks some other language features, so it might not be a solution for you.
Oh wow, Gorgonia needs some SEO badly. It doesn't even show up in the search results.
If you mean that you want to stream already existing files, it's actually really easy. In fact if you just bring up a static file server and point it at a directory with media in it, you may be surprised at what the browser already does. Just make sure you use the built-in static file server from a directory. It does a lot more than just "serve files", which matters for the media support.
Yeah, data scientists and machine learning engineers generally gravitate towards the Python stack because of existing momentum. I wonder if that might change someday.
&gt; tinygo OP doesn‚Äôt appear to have mentioned tinygo...
 &gt; It works fine on tens of thousands of servers, but sometimes the admins set a really low per-user nproc limit (which is just the regular kernel nproc rlimit) and then run into this panic: AFAIK [regular limit](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/asm-generic/resource.h) is no limit so all of them are set on "userspace" side. I've seen distros setting per user one to ~100 but anything lower is pretty much admins being stupid &gt; I can blame these admins for shooting themselves in the feet, You should probably allow them to continue feet-shooting until they stop tuning their server limits like it is 1997. Just be sure to clearly specify required minimum ulimits
Thanks for answer. Ye I want to try streaming service in real time. But for first maybe I will try your decision) Thank you
I noticed you are (accidently?) using html/template instead of text/template to generate the config.
Thanks for answer. I did simple todo list, rock,paper game and now I think that work with media will be very necessary experience. So I decided to try. Thank you for advice I really appereciate it)
I've been using xerrors and I'm not a fan of xerrors.Errorf - it's far too easy and subtle for someone to do this with an error and mess up the format string such that you break the wrapping.
Nice article, thanks for sharing
If you like this sort of thing: https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8
&gt; AFAIK regular limit is no limit so all of them are set on "userspace" side Sorry, I meant that CloudLinux's nproc limit = kernel nproc rlimit. &gt; I've seen distros setting per user one to ~100 but anything lower is pretty much admins being stupid Yeah. It's a shared hosting environment, so I expect that their logic is 1 nproc = 1 PHP worker (which is a true but incomplete understanding). I can see how they might end up setting a low value, and then bad things happen when Go is involved. &gt; Just be sure to clearly specify required minimum ulimits I guess so! My take away then is ... Go needs some rlimit overheads and there's not really any knobs in the runtime to control that. (I don't have a problem with that at all).
tinygo is interesting but we use some pretty complex libraries (like gRPC) so I expect that it probably wouldn't work.
How are you going to directly access the GPU with Go? That's what has been holding me back.
Gomaxprocs setting may not be the reason why you see that error. It may be just because of a misconfigured ulimit for the user. Usually default is 1024 which should be enough. Why don't you remove gomaxproc line from the code, it is useless in newer versions of Go .
Hey, I'm one of the project contributors and I worked on GPU support. We don't directly access GPUs with Go. We submit a job to the k8s cluster with the right resource spec ([code](https://github.com/cortexlabs/cortex/blob/a99b7890935db66e08d4d9d180c0fadde23136bf/pkg/operator/workloads/training_job.go#L53)), and then that job would use TensorFlow's GPU enabled Docker image which will run the training with TensorFlow. We leave it to TensorFlow to handle computations on the GPU.
I've built something like this: https://github.com/rodolf0/tox/blob/master/fluxcap/src/time_parser.rs It uses a custom crafted Earley parser (mostly for fun: https://github.com/rodolf0/tox/tree/master/earlgrey) and also a library to evaluate the composed time expressions: https://github.com/rodolf0/tox/tree/master/kronos It has taken quite a bit of time to figure out all the bits and pieces.
At first I thought this is an attempt to directly do the machine learning with Go. But it's more like DevOps? If that is the case then why Go? I know Go is cool but isn't using Python itself would make it easier to conncect the model (which is Python)?
Right, this is not an attempt to run machine learning algorithms in Go. The project is focused on the DevOps around ML pipelines. We use a relatively lightweight Python harness to train models but the bulk of our code is responsible for orchestrating and managing different types of workloads on a Kubernetes cluster (e.g. Spark for data processing, TensorFlow for model training, TensorFlow Serving for model serving).
The thing made me proud of Go "Even though Python is the primary language of ML, my colleagues and I decided to use Go" :)
That was an optimistic comment, but you took it in bad way. Im primarily JS person, trying to learn GO now.
hey Im also from Frontend Background. over 9 years. Trying to learn GO now. I would recommend you to just start learning GO, practice and learn concurrency patterns.
Thanks for the advice! I‚Äôll take a look. My issue has less to do with Go itself, and more so how to structure it. Maybe I‚Äôm uninformed and I don‚Äôt know it as well as I thought I did
Great article. I learned a lot about Go GC.
That should ~~goGo~~Golang without saying.
Backend guy here. Honestly, if you're following along with the code that the other folks at work are doing and you're passing code reviews, you're doing fine. If y'all aren't doing code reviews as a matter of course, find one of the senior folks who has clue and is a decent person, and ask her (or him) to do a code review of something you've been having trouble with. Better yet, ask if you can borrow them for an afternoon and pair-code something together. You'll learn loads and it'll be a good time.
Could be this? https://en.m.wikipedia.org/wiki/Impostor_syndrome
Seconded. Well written piece!
I‚Äôm on year 15 right now of my career, having been at my current job for near 4 years, and there are still days I totally feel like a fraud. Imposter syndrome is a real thing that many developers deal with, and it‚Äôs worth acknowledging you‚Äôre not alone. As for dealing with what you feel are shortcomings in your knowledge, if you don‚Äôt have someone at work to assist with that (which I tend to feel like), self-learning can help. I came from primarily a PHP background into my current job as a Ruby developer, and I adored learning from the likes of the Sandi Metz and Katrina Owen as I felt my skill level needed to improve. A lot of what they cover in many of their talks can be applied to Go, within reason, especially Metz‚Äôs book, ["Practical Object-Oriented Design in Ruby"](https://www.sandimetz.com/products#product-poodr). Having been working on side projects in Go now for over a year, it‚Äôs all been immensely helpful. Also, that example code you are mimicking has been refined and refactored multiple times. Worry about making something work first, and then on to figuring out how best to make it "right". I find refactoring to be a key part of my end design, extracting modules, types, and methods, where necessary. Martin Fowler‚Äôs ["Refactoring - Improving the Design of Existing Code"](https://martinfowler.com/books/refactoring.html) was recently revised and may be worth your consideration, as well. Code examples have been moved to JavaScript from the original Java, so if you aren‚Äôt familiar with Ruby, this may be a better pick for you. I‚Äôm going to be honest though and say that feeling like a fraud won‚Äôt go away 100%, I‚Äôm pretty sure a lot of us are learning as we go and I‚Äôd be lying if I said I still didn‚Äôt have days where I still feel like a fraud.
I'm confused about the part where they mention the memory for the byte array isn't used until it's accessed. Doesn't Go initialize the memory to zero? How does that all work?
Would you expect yourself to be a master painter, chef or a Jiu Jitsu black belt after 1 or 2 years? Probably not, right? It's the same process for mastering any craft. It takes a very long time. Be patient, set goals, learn a little every day. Eventually you'll be really really good and it won't feel like a big deal to you.
I've long struggled with impostor syndrome myself. Even after being in the software world for almost 10 years and using go for about 2.5 years, there's still a lot I am learning about how to best design software. Even recently, I built a new service that went out to production less than 6 months ago and I've learned a lot from building it. Even with such a recent project, I've learned from it and would have designed a few parts of it differently. I went to gophercon in Denver a couple of years ago and one of the more popular talks was talking about go anti-patterns. Hundreds of seasoned engineers were there learning ways to improve their own code. Everyone constantly is looking to improve their own skills so it's easy to look at everything you don't know and forget all the things you do know. Be strong, you know more than you think you do. Everyone else around you has similar thoughts. Don't let those thoughts stand in your way.
That ballast size should probably be a tunable parameter instead of needing a manual reservation allocation. Even better would be a way to register a callback interface object that let's the application dictate a whole bunch of parameters dynamically during runtime... Including possibly an inhibit GC flag (similar to masking of interrupts) during these load spikes.
Simply get more experience. Confidence/skill is earned with time. Do personal projects; refactor the code over and over and evaluate pros and cons. Try writing tests first. Try writing no code first, just comments and then "fill in the code". If your comments were missing pieces or wrong, sit and think about what you missed the first time.
Greatest article üëè
I'm at year 20 and most days I feel like a fraud. Funny pattern happens: everytime I get called for a meeting and an HR or finance rep is pulled along I am sure I'm getting fired, but the purpose of the meeting was a raise. Happened like 7 times in 5 diff jobs. I have no clue how it happens.
How did the ballast not get optimized out by the compiler?
That is great! Going to add this for sure üëèüèº
I'm guessing that Go is calling `mmap` with `MAP_ANONYMOUS` somewhere, which zeroes memory only after the fault is triggered (at least on Linux). [This Stack Overflow answer](https://stackoverflow.com/a/2688522) goes into some good detail on memory allocation and page faults.
Looks awesome. Thank you.
I thought a dilator was what my eye doctor uses.
Per-second resolution was too coarse. I would have preferred even finer time resolution than 1ms and less entropy, to be honest. Information leakage was not a concern.
So, you're in the kitchen, making a meal you found on the internet, or in a book. You start stirring and shuffling and you present in the end a fine cooked meal to your friends. Does this make you feel the same? Because you didn't "invent" the diner you made? Get rid of the feeling. Everything is done before (sort of). We're all copycats. We didn't came this far if it wasn't for our predecessors. Learn from their mistakes by adopting faster as they did, and perhaps one day.... you will be an inventor too. Keep up the good spirit of learning as you GO :-)
Hi. If you want make some twitch analog with streaming in real time, you better looking for some info about RTMP (for transmitting video/audio data from PC to your server by OBS etc) and HLS(for transmitting data from server to browsers and smartphones). Now i making a streaming service on GO too. And i would like to work in a team.
A good practice to trade space for time. Never thought we could do this before.
You might like Gorgonia's [cu library](https://gorgonia.org/cu)
It doesn't take much to trick the compiler into thinking something is used.
Thy still dont use GO for ML...
Disclaimer: I don't have a solution (I wish) and everyone's experience is unique, so take this with a huge grain of salt. I was in a similar situation. I felt I tried everything - I talked to my managers, to my friends and even to therapists. For roughly 2.5 years I felt like a fraud and was told by anyone I asked that "I'm doing fine" and "as long as your manager doesn't complain and your performance scores are okay, everything's good". I still got *incredibly* depressed every six months, when performance evaluation was up and I had to write down what I did and why it mattered. My scores always ended up okay, but I was still super aware that I was performing under my potential and would eventually get fired for not adding value. Then, after 2.5y, my scores suddenly dropped *dramatically*. A horrible year of struggling to improve later, I quit. Now‚Ä¶ I don't want this to sound totally depressing. I don't want to *confirm* your feelings that you're a fraud. But for me, the most frustrating part of my history is that I was told for 2.5y that I'm imagining things and I'm doing fine and *then* it turned out that I was fundamentally correct - I *wasn't* doing fine, it just took a long time for someone to notice. And when the problem finally was acknowledged, it was too late to fix. So I want to be clear that *that's a possibility*, even though your feelings alone aren't proof that it's the case. That's where‚Ä¶ I don't have a solution. Like, obviously it's not your job and it's bad for your mental health to set out to prove that you're a fraud. That's fundamentally dumb. Obviously, there's a statistically good chance that this *is* just impostor syndrome and you *are* objectively doing fine (and due to the statistics, that's a reasonable default assumption). Ultimately I would argue that it's your managers job to take your feelings about this seriously and try to find systematic solutions to it - but I don't think many managers are really equipped to do that. The upside, I guess, is that things have been getting better for me after I quit - slowly. Especially, after I moved and made other things than software a priority in my life - mostly to socialize more. It doesn't change any of my views regarding my competency, but it *does* make them less influential over my feeling of self-worth. So that is probably the best advice I learned from all this: Find meaning outside of your work. Make sure you use your spare time well and that you are happy when you're *not* at work. And eventually, you might get to a point where you can tell yourself that you *might* be an impostor, but who cares, as long as that's still paying for the part of your life it's actually about. Also, consider talking to a therapist. Like, fo' realz. You don't have to be definitionally "mentally ill" for that. In general, whenever you are struggling with your own mind and feel like you can't solve that alone, a therapist is the relevant specialist to talk to. Good luck :)
Well, just because Go is cheaper in terms of resources, does this imply it scales better, or that .NET has scaling issues? I would argue that reactive extensions of .NET are much more superior over CSP. Rx provides ease of concurrency as well &amp;#x200B; &gt;avoids things that can cause bad code reinventing the wheel in every project is not bad code? &amp;#x200B; I would argue that C#/F# are much better languages than Go, in terms of features and clean code
I related to this. 12 years in the industry and I still fight that same feeling
Great article and easy to read.Thanks.
Yes, it is known and acknowledged. https://github.com/golang/go/issues/29934#issuecomment-459824434. I also wrote this - https://agniva.me/errors/2019/03/20/error-values.html
``` {{ range $post := .posts }} &lt;option value="{{ .ID }}" {{ if eq .ID $.post_id }}selected="selected"{{ end }}&gt;{{ .Name }}&lt;/option&gt; {{ end }} ```
This is because you are passing a reference to the variable a to the function.
What is happening here is that you initialised a variable 'a' and passed the reference to that variable to your function add. You are then adding 1 to the reference and returning it through the function. This return value is assigned to x1. &amp;#x200B; Please check out this playground link [https://play.golang.org/p/SSlnoTzPb8M](https://play.golang.org/p/SSlnoTzPb8M)
ok I got it. Its not the return of the function that manipulates the variable x but the actual calculations inside of the function made to \*a which is reference to x. Coming from JS where there are no pointers it was hard to wrap my head around it.
Correct. In fact, you could remove the return entirely from the function and have the change reflect in your original variable. Here's the updated link for your reference &amp;#x200B; [https://play.golang.org/p/Ag\_I2iPvhQD](https://play.golang.org/p/Ag_I2iPvhQD)
I'm still confused on how they did that!
I want to commend you on this piece of work. As a fellow open source maintainer is great to see someone simplifying machine learning and using Go to do that. I have a strong feeling this project is going to be very successful in the long run. Just don't ever stop!
Using pointer variables in golang is somewhat similar to copying objects in javascript.
To me monad is like a sequence of computation that the flow I can control, doesn't have to be an IO, in a way, it's like throwing an exception.
 Because *x is different from x. Really *x is just it's memory address. &amp;x is saying let me use it's memory address. Yes, JavaScript has this but it just looks different. http://www.javascripttutorial.net/javascript-pass-by-value/ https://github.com/miguelmota/golang-for-nodejs-developers/blob/master/README.md#files
In my experience (heh!) trying to combat impostor syndrome with more experience doesn't really work, because you will just dismiss the evidence that you know what you're doing and focus on the failures‚Ä¶
Tangentially, what languages in other domains have such appeal? Scala in the data space, Rust in embedded?
Implement something that requires CS knowledge not just gluing code together. A good project would be to try to implement a programming language in Go.
If you choose to be irrational about it, sure. Normally, gaining more experience and practicing your craft makes one actually feel like they know what they're doing, because they do.
&gt; Who would have thought Google salaries were so high?
Why are/were you planning to switch from xo to GNORM? I'm using GNORM and happy with it, but haven't given xo a serious evaluation.
I think one way to ensure it stays there would be to call [runtime.KeepAlive\(\)](https://golang.org/pkg/runtime/#KeepAlive) on it.
It's because it's allocated in the main function, and is always in-scope. It never gets marked to be garbage collected because it never falls out of scope. It's definitely exploitative of the current GC model, but I doubt the Go garbage collector will ever start treating in-scope main function allocations as "sweepable".
But shouldn't the compiler see that variable as never being used and just simply remove it?
I don't know how Twitch does it, but this is a very simple method: https://godoc.org/runtime#KeepAlive
Cool bro. Can you give me your e-mail or something like that? I'm not very good on English but I will do my best.
Given that how common this particular scenario is (sudden load spikes), makes me wonder how common this sort of performance degradation would be in real-world and what would most of the developers have done to fix it....
I switched to Gnorm a while back and it was the right choice for me. Gorm was built because of some pain points with XO. My biggest issue with XO was it's handling of custom types and nullable types, which is much better in Gnorm. Gnorm also out more flexibility for where the output goes where XO was pretty strict.
I think the reason they use virtual memory space allocations is explicitly so that they don't have to do that for in-scope allocations. That way the compiler doesn't have to worry if a variable remains unused, just if it's in-scope.
If you're raising concerns about your capabilities and knowledge to tour manager, the wrong response is "you're doing just fine". A good manager should be trying to tease these feelings out and offer you opportunities to feel more secure in your position. What exactly is causing these feelings and what areas are you actually having issues in? Are there trainings, books, etc. that can help you feel better? I have unfortunately found that all too often in tech leadership the leaders are more concerned about the process than about the individuals on the team and there is definitely an attitude of "if the process seems like it's working then nothing is wrong". That almost always leads to significant deficiencies on high performing engineering teams and ignorance of real problems that are going on.
Every single major cloud company with truely large infrastructure--Google, Microsoft, AWS, Cloudflare, etc--and many cloud tools have Go in places where scalability and low latency responses matter. My company is almost 100% C#, and I've spent my career in C# quite happily, but our newest product made sense in Go. That doesn't mean I think C# is necessarily flawed at scaling. At significant resource cost, we have individual clients with as many as hundreds of thousands of machines talking constantly to our servers. It means we hire great C# devs, make smart choices, and leverage Azure to load balance and scale. Our choice of C# does not make a material impact on the company bottom line like new features or sales approaches do, but would we save a lot of money if our whole stack was in Go? Yes! Is it worth migrating old products? Probably not, unless something else requires a massive expensive rewrite for other reasons. Even then, going to .Net Core instead of Go would probably make more sense due to where most our skillsets lie, and the edge Go has over Core is much smaller. Reinventing the wheel is usually bad, but I'm not really sure it applies when talking about C# and Go. I assume you're talking about small projects where you are using out of box ASP.Net niceties and other features that you wind up having to customize at great depth once you're looking at something very large. Personally I have heard that Rx.Net exists (and I use RXJS when I work on Angular) and have wanted to try it, but I have never met a developer that has used Rx.Net, so I'm not sure it can be pointed to as a critical C# feature. Perhaps I'll try it out sometime, though I do spend most my hobby time in Go right now. When I talk about the concurrency primitives being better, I am especially fond of the go keyword as compared to the task API in C# that has always struck me as verbose and ugly. The use of channels is great, but if you prefer mutexes, it looks pretty much the same as C#. Certainly the advantage of C# is that generics let you express things like reactive code with relative ease (though you can still pass functions in Go, mind you, and it does ease the need for generics), whereas Go has the advantage of simplicity so that complicated problems can be easy to understand at a glance. Nothing can be hidden via overloading and inheritance. Whether this is good or bad starts to stray from the topic of scaling, but I think it's fine to appreciate both or pick a favorite. It doesn't matter a whole lot when you are talking about two particularly good languages. I don't like F#, but that's even further off topic, and I don't dislike it because I think it's a bad language, but I don't recommend it.
[The Github issue linked in the article has a ton more information.](https://github.com/golang/go/issues/23044)
Brilliant article! Can't wait to see more of that!
Interesting... But I wonder &gt; Ballasts are easier to reason about than configuring a GOGC value is this true?
Brainfuck is an even simpler language than Go, and thus a better language for big teams.
yup, sure
It is similar to passing arrays or objects in JavaScript. A variable holds a reference to the array but never contains the array itself. Hence why we often use the spread operator to create copies of objects/arrays to avoid mutating it. If I said anything wrong feel free to correct me.
The compiler isn't that sophisticated.
There's nothing to measure: the optimal number of hash functions can be expressed in terms of the total size of the bloom filter in bits and the number of elements being inserted. The size of the bloom filter can itself be calculated from the number of elements it will hold and the desired false-positive rate.
Here is an analogy in Javascript: `function add(o) { o['x'] = o['x'] + 1 }` `xvar = {'x': 3 }` `add(xvar)` `console.log(xvar)` `&gt; {x: 4}` In the JavaScript code above, we are passing around a Reference (i.e. Pointer) to an object, not the object itself. In JavaScript, you get no choice: If you pass a simple variable, it gets copied (pass by value), but if you pass an Object or Array, it *doesn't* get copied (pass by reference). But in Go, it has a simpler rule: **Everything is pass by value.** So if you pass a Struct with fields, you function gets a copy of the struct and all it's field data. If you want the behavior above, you must pass a *pointer* to a struct. (The Pointer is actually passed by value, because **all calls are pass by value**. But since only the pointer is copied, the memory of the struct itself is now 'shared' and mutable by everyone who has a copy of the pointer.) Now, imagine a plain var, not just a struct. The same principle applies: If you pass the var, your function gets a copy (that does not change the original). If you pass a pointer to the var, the function gets a copy of the pointer, but the var data is now 'shared' (mutable by the function). &amp;#x200B; See also [The Perils of Java(script)Schools](https://www.joelonsoftware.com/2005/12/29/the-perils-of-javaschools-2/).
The ballast thing sounds pretty hacky to me. I would go for the variables
\&gt; Specifically, I have the hardest time with determining pattens and how to ‚Äústructure‚Äù my code. Having proper design patterns make testing easier, and a lot of other things I‚Äôm sure. I think I really worked that out somewhere around year 10, and even at \~23 I'm still learning things. Question: Why would you expect to have attained mastery of such a complicated subject, where even people with 20+ years of experience rather deeply disagree on even some fundamental matters, in just two years? I'm gonna do the opposite of what a lot of people would do, and rather than puff you up, I would suggest you *greatly* decrease your impressions of the competence of most of the people around you. It's easy to see final products and fail to see the amount of effort that went into them. Go ahead and use mimicry, but use it intelligently and always be analyzing what *actually* happens, rather than what is promised to happen. (I'll give you one example I've posted about a few times here: I've read any number of times on the internet and even been told directly that if my unit tests aren't just testing the public aspects of my code, my unit tests will end up tightly coupled to the code under test and I'll end up either freezing my design progress or throwing away tests as I change things. But in the 10+ years I've been testing the internals of code with unit tests, that's happened *maybe* once, and not even all that clearly then. So I trust my own experience and go with what I've been doing, as I also know I've saved myself from bugs many times by testing the internals. Now, maybe *you* try this and you *do* experience that problem, in which case, go nuts and test only the external interface. Who cares what some jerk on Reddit thinks? Your own direct experience is more relevant to your life. Maybe I'm in a different domain than you or those people, or maybe my own idiosyncratic programming practices leave me immune to that for other reasons but leave your code heavily affected by it. Who knows? Nobody, that's who.)
It's not even trading space, it's not actually in memory. It's just telling the OS it might use it.
I just want to wrap up this discussion with the observation that you're in Perl's wheelhouse here. It's almost literally entirely designed around doing this particular task quickly. It's not surprising that getting Go to go as quickly, while possible, takes some effort. Where Perl is slow is, well, almost everything else. It's when you start writing logic that it gets really slow.
Hi Elliot, try with some events near you: [https://meetup.com/pro/go](https://meetup.com/pro/go)
Just to cut short the answer, read ‚Äúpass by value‚Äù and ‚Äúpass by reference‚Äù
FWIW, I had three managers in my ~3.5y at the company. My first manager was IMO really good at this - but after half a year, our team got reorged and my managers changed. I still wonder how things would've gone if that hadn't happened. :)
I can‚Äôt understand how you guys can be so close minded.
They should not be trying to limit the number of threads. If they want to limit abuse, they should directly limit resources like RAM, CPU, etc. Limiting threads might have been useful before CGroups existed, but now it doesn't seem useful. You will save a lot of time if you just stop working on this trivial problem (caused by someone's bad business model) and find a better hosting provider. Even this host somehow saves you a few bucks a month, how many hours of your time is that worth? (Remember, your leisure time is worth a lot more to you than your hourly pay, because otherwise you would prefer to work more.)
Like the others have said the add1() function is updating the value that is stored at the memory address pointed to by \*a. I personally think that's not the best example to demonstrate how pointers work. For someone new to a language with pointers the return would be confusing (as you discovered). This is, however, a fairly decent example of the concepts of side-effects and idempotence. Side-effects simply stated is when a function (among other things) changes something outside of its local scope. Despite the commonly used definition of side-effects from the medical / pharmaceutical industry, side-effects in programming are not inherently bad and sometimes are useful. For example printing output to the screen is considered a side-effect. Idempotence (if you didn't already know) says that if you call a function multiple times with the same parameters you should get the same result. This is not the case with the add1() function in the example. Again the judgement of whether this is good or bad depends on what your use case is. I'm simply pointing out that -- by definition -- add1 is not idempotent. Anyhow, that's my two-cents. Keep hacking away. :-)
[errors.As](https://errors.As) is replacing type assertions, [errors.Is](https://errors.Is) is replacing \`==\`.
News flash: humans are irrational, mental issues are not a choice ;)
There's a difference between "I'm a beginner and I don't feel like I have a strong handle on what I'm doing yet" and "I have permanent impostor syndrome".
See https://godoc.org/-/about for a couple of other goodies.
https://godoc.org/golang.org/x/xerrors "Most of the functions and types in this package will be incorporated into the standard library's errors package in Go 1.13" It looks like this proposal is already accepted. Is it ?
[removed]
Breathe, structuring and naming code is always hard in my opinion. Just try to see how they do it in stdlib, ask a colleague if you're not sure and discuss things. I bet there are so many developers that have the same issues or are more than willing to share their opinions. Make sure you interact, ask for feedback and get the problem solved. Code can always be improved and refactored üëå
\&gt; Also the number of go routines seems to be fine. Define "fine". Most often I've founding leaking goroutines to be the source of suspected memory leaks. Make sure you always know when and how your goroutines will terminate.
usually hovering around 70-130 at any given time. How would I find leaking goroutines in a concurrent heavy program ?
Hi Elliot, the biggest european conference is [https://dotgo.eu](https://dotgo.eu), you can contact us at ferdinand (at) dotconferences.com for sponsorship opportunities.
Sounds like GOGC approach was discovered after hacking with ‚Äúballast‚Äù. Why just don‚Äôt setup gc percent to the much higher value?
Wuuutttt, super basic and highly effective.
Re: mental health issues being a choice: where you start out may not be a choice but how you deal with it very much is. There are many techniques for changing your ingrained thought patterns. It takes real work similar to improving your body through exercise.
They are my customer - I write and sell a piece of software that's relevant to web hosts. The aim is to be compatible in as broad an environment as possible (even if the constraints are pointless). It's taken 3 years to run into this issue so I'm happy to just live with that edge case. Plus I understand Go's use of OS threads a little better now!
My usual approach is to take a heap profile, then some time later second a second profile and compare them with pprof's `-base` flag. This helps identify where the extra memory was allocated from. This in turn lets you track down why the lifetime of that memory is not what you expect it to be.
You can't achieve the same result with available variables.
IMO yes.
Because it's percentage based your gc threshold can fluctuate wildly depending on load. It would be very difficult to predict what that threshold would be at any given time, or what the maximum would be. For example, if your average memory usage were 500MB and you wanted to GC around 20GB you might set GOGC to 40x. But then a spike of traffic bumps load to 3GB and now you're garbage collection doesn't happen until 120GB. With the 10GB ballast those two scenarios result in GC at 21GB and 26GB, much more stable.
How does [jsoniter](https://godoc.org/github.com/json-iterator/go) compare to [ffjson](https://github.com/pquerna/ffjson)? I'm curious since ffjson is older.
Maybe they can add a vet pass to detect common errors.
MachineBox.IO is go based ML
Sometimes, it's wise to fire your most needy customers.
ffjson is 2-3x faster than the standard lib, according to their github. They are both implement the encoding/json interfaces, and neither require code generation. jsoniter appears to be much faster than ffjson at Unmarshal, and somewhere in the ballpark of ffjson at Marshal.
1. Try to fix a coding problem. 2. Can't fix it. Feel like a fraud. 3. Contemplate new job, new life. 4. Work out exit strategy and try to pinpoint where everything fell apart in your life. 5. oh I was missing a semicolon. 6. Lunch.
So I wrote a program that includes a lot of command line options. I played around with using channels but it never seemed to run any faster. I would love to make my program run faster. Any ideas on how I could do this would be greatly appreciated as well as any other improvements. https://github.com/jftuga/freq
User Interface,
I am happy to help. Why are structs bad now? What have you tried so far? Bolt? Badger? SQL?
Firstly; thanks! I've nothing against using structs, I just want sure that they were the right way of doing this. Regarding the metrics, I only need to store the most recent one for each endpoint as they'll be scraped - there's no need for persistence. I think I'm mostly struggling with how to group the data about all the endpoints together - hence my 'list of objects' in python description.
Glad to hear it's been a good choice.
Thank you for your answer. I agree, but with one clarification. Yes, as you said, when the average memory usage stabilized around 500MB (through several GC cycles from say 10MB) next GC will happen at 20GB; spike of traffic bumps consumption to 3GB and nothing happens ‚Äì we still have ~500MB of live data (needed to handle average load traffic) and ~2.5GB of garbage after that spike. Then, when live+garbage will grow up to 20GB, GC will collect the garbage, leaving the live ~500MB of memory and next GC cycle still at ~20GB. It is less predictable on the corner cases you mentioned: when spike bumps consumption to 3GB for a long time (not a spike actually) and after GC at 20GB you still have 3GB of live data, then you are right and next GC will really happen at 120GB.
What is the kind of data you'll get from each endpoint? What did you do with the data once scraped in Python?
Sorry gee I wonder if we should help with your hw / interview
Its fine to store each endpoint as a struct. What you need is how to glue it all together. If you define similar interfaces for each individual struct. Say a function consume which takes a byte slice( or struct or json or whatever) and a function render which returns one (or json or html) then you can store all of these in a map or list or whatever with the basis of their consistent interfaces. You only add what you get requested to consume from the endpoint api to the slice or map. You are free to then replace it whenever you feel that your endpoint list has been updated.
A good clarification :)
You can most likely store this data in a slice of structs. My crude example is below. ``` type apiData struct { hostname string isAlive bool } func main() { var apiDataSlice []apiData for { var result apiData // Collect data here and store it in the result var result = &lt;DATA&gt; // Add the struct to the slice of structs apiDataSlice = append(apiDataSlice, result) } } ```
Nested messages. It‚Äôs in the protobuf documentation...
Well for one the only duplicate info in other resources are the IDs
I'm on the phone so not feeling for typing a lot. But I had similar go routines "hanging" for a long time. This is the part on SO where I answer my own question. Maybe something here will give you an idea about your situation. https://stackoverflow.com/a/55513832/638488 I tested with 2800 concurrent requests for 30 minutes and I had around 80-100 leaking routines that where locking
Is it a public or private server? If public, an SSL certificate would do the job.
This is the code of server running on my local pc // start TCP and serve TCP server server, err := net.Listen("tcp", ":9000") if err != nil { log.Fatal(err) } log.Println("HTTP Server Listening on port : 9000") defer server.Close() for { conn, err := server.Accept() if err != nil { log.Fatal(err) } go handleConn(conn) This has to be run only on LAN, not gonna deploy anywhere online.
If this server is gonna be only accesible from a local LAN a shared secret should be enough. You could sign every request using some signature algorithm such as HMAC. This way you were not only validating server identity, but also clients.
In the environment that these errors do occur, do they happen consistently on startup? Or is it only after certain operations?
You can use [certstrap](https://github.com/square/certstrap) to create your key and certificates. Use something like [http.ListenAndServeTLS](https://golang.org/pkg/net/http/#ListenAndServeTLS) to start the server. * the client needs to have the generated cert in its trust store (`/usr/local/share/ca-certificates/` on debian, don't forget to run `update-ca-certificates` to update the trusted certs) * the server address needs to match the name in der certificate (IP address or domain)
i can't get you points. it's not depends on real central server. redis(Cluster) and mysql(PXC) both are distribunted system.
You should also check for unclosed channels.
Don't roll your own crypto scheme, use TLS with a custom CA.
Sort of :-) Like: which order and also like the other person asked: to bundle certain combinations. This makes to tool suitable for a lot of people. I personally mind if the linter would check the if the methods are below the functions. So let's say I could configure that, then I would use the tool ;-) But others would like it as it works now, so they can configure it that way. Example Config (for them): ``` # TOML configuration order = [ "import", "type", "const", "var", "func", "method" ] ``` Example Config (for me): ``` # TOML configuration order = [ "import", "type", "const", "var", "func/method" ] ``` (where you would have to write something to check if func's or methods are below the others)
You can paginate without the user knowing ;-) Anyways, nobody looks at million of rows, so try to figure out how others dealt with the same problem. they filter or paginate. To have a person look for information is a weak link because we, humans, are not that fast. Better leave that in a smart way to the computer.
If your API has JSON, use this service to create a struct for it: https://mholt.github.io/json-to-go/ But please be aware that this service is rather static. You need to refine it. But it gives you a nice starting point.
Yeah you are looking for generics. Go of course doesn't want you to mess stuff up or make it slow so you have to be more concise. Personally, If you know the data then make a map of structs. If you don't know the data then you need a map of open interface and then you range over the data to normalize it. It's here https://blog.golang.org/json-and-go If you want a really good example of high level then that's in the go blog context. https://blog.golang.org/context
Doesn't even need a custom CA. A standard SSL Certificate will do the job just fine, because that will prove that the server you are talking to really lives on the domain name you think it should. The major risk here would be if you lose control of the domain somehow.
&gt; A standard SSL Certificate will do the job just fine OP mentioned that the server is running in an isolated network. That'll usually make it easier to use your own CA than to get a SSL certificate validated for whatever internal domain you use.
Still no fix for Android Q? &gt; error: "go": executable's TLS segment is underaligned: alignment is 8, needs to be at least 64 for ARM64 Bionic
For maintaining lists, Gophers use slices where order matters; and maps where order is irrelevant and/or there is a natural key and you want O(1) lookup. Typically you want to store a pointer to your struct in the slice or the map. e.g. `type Endpoint struct {` `// endpoint data here` key string // if this makes sense, then a map is natural `}` `type MaintainerOfEndpoints struct {` `myEndpoints []*Endpoint` `}` `// or` `type MaintainerOfEndpointsVersion2 struct {` `myEndpoints map[string]*Endpoint` `}` &amp;#x200B; `//NB to delete from a slice at position k, you do` `func (m *MaintainerEndEndpoints(k int) {` `s.myEndpoint = append(s.myEndpoint[:k], s.myEndpoint[k+1:]...)` `}`
Hi Elliot, there will be GoLab in Italy too: [https://www.golab.io/](https://www.golab.io/) with more than 500 attendees from all over the world in 2018! &amp;#x200B; contact: [info@golab.io](mailto:info@golab.io)
knowledge is not innate to humans. live and learn. be humble and kind, keep your chin up, avoid self-loathing. You are learning to ride a bike, you are going to lean on the training wheels a bit more than the more experienced riders.
Such binaries do not minimize the program in time or space dimension, do not increase safety. This means autostereogram binaries are just a toy. Please try to avoid posting toys - it is a waste of my and your time.
1.12.5 / 1.11.10 incoming
That's super awesome. I love the "f" key's UI.
I'm not concerned with the content of the endpoint, I'm looking to store things like the response time etc. These then get served up for Prometheus to scrape.
&gt;go-migration Is this issue fixed yet?
It sounds like you should be using a map of structs. The only thing that should be different here between go and python is that go requires all the structs to be of the same type (or all implement the same interface) while python won't complain if your classes are totally different. Is that what is tripping you up here?
Can go be compiled for Android?
yes, since Go 1.5, but it isn't as simple as desktop. See the wiki for a start: https://github.com/golang/go/wiki/Mobile
If you are using Go 1.12 this might be due to `madv_free` call that was added to the allocator. &gt; On Linux, the runtime now uses MADV_FREE to release unused memory. This is more efficient but may result in higher reported RSS. The kernel will reclaim the unused data when it is needed. To revert to the Go 1.11 behavior (MADV_DONTNEED), set the environment variable GODEBUG=madvdontneed=1. https://golang.org/doc/go1.12#runtime
I use React a lot _and_ https://github.com/xeoncross/got (a simple wrapper around `html/template`). React has a hard time with good search engine visibility where plain templates work great (with some progressive javascript for actual users). Admin dashboards are always 100% react because there is zero SEO concern.
I'm very new to Rust so just today I had to look up what that ? was doing at the end of function calls. When I found out, I thought it was very cool. :)
Sidebar, but quietly DO's largest contribution to technology has been their community-written curated guides for everything web development and devops related. They're almost always nearly perfect, they have software version selection for things like steps on different OS versions, and the layout is clean and readable. They really should deploy a Medium-like service for tech bloggers. I've had an ass-full of that site over the years.
Ok, if you were more straight forward from the beginning it would've been easier to give you tips on the implementation. Now I can see why you were having trouble with the details. Your best bet is using a slice of structs to store each endpoint, with each struct having a map\[string\]interface{}. The string will be the type of metric, and the interface will be whatever datatype you intend to store. When you are making the data available to your scraper, you will add the tags for Prometheus then while iterating through the map for each endpoint.
The way they show it in the article every more or less smart compiler will just remove the complete assignment because the variable is not used afterwards. This is extremely easy to detect. They must use some trick to keep the allocation in the compiled binary.
Did agile BS just bit golang team or what? Fix the damn thing instead of releasing a new version every 2 days.
It's all good, just add the script to the template and go with it, it's simple and it works, not every app that uses React needs to be a SPA.
While you could use html templating + react, it might make more sense to just serve a single html file from Go that brings in react dependencies, along with a &lt;div id="root&gt;&lt;div&gt; that ReactDOM can mount. At this point you'll have single page app that can just make ajax calls to the http handlers you expose in your Go code.
I don't quite understand why logging function calls at the DB layer is important when testing the business logic. One of the points of using interfaces to define dependencies of a component is so you can isolate the tests to just that component. That is, when testing your business logic, you should assume that the Storer you're using works correctly, presumably because the concrete implementations of Storer have tests of their own. &gt; This is especially crucial in business logic functions that basically only call the database (like logout - it must delete a session from the database and do almost nothing else). Right, so I'd expect your business logic's Logout method would be trivial, something like func (a *Application) Logout(session string) error { if err := a.storer.DeleteRow(session); err != nil { return errors.Wrap("error terminating session: %v", err) } return nil } Testing that is easy: you just set up a mockStorer with predefined sessions that are logged in, and verify that Logout of one of those logged-in session returns no error, and e.g. Logout of the same session again, or another arbitrary session, returns an error. If you want to test that your real Storer implementation actually does the right e.g. SQL stuff, then that test would be against your DatabaseStorer type, and you might use one of a number of methods to do it: [go-sqlmock](https://github.com/DATA-DOG/go-sqlmock), or an integration test against a local instance of the actual DB, or whatever else.
it will be stored on the heap
Ok, so you nailed the business logic. However, I'm trying to catch a different test case, namely someone breaking the functionality of the function. Suppose some na√Øve dev changed the code from ``` func (a *Application) Logout(s Session) error { if err := c.storer.DeleteSession(s); err != nil { return errors.Wrap("error terminating session: %v", err) } return nil } ``` to ``` func (a *Application) Logout(s Session) error { if err := c.storer.DeleteAllOtherSessions(s); err != nil { return errors.Wrap("error terminating session: %v", err) } return nil } ``` The tests will work fine, the DB code will work fine, but the functionality is completely different. This could of course be caught in an e2e test, but the business logic is faulty and thus I think the unit test should somehow check that the business logic does nothing wrong. Or am I being stupid here?
[removed]
This implies that your Storer interface has DeleteSession and DeleteAllOtherSessions methods. Presumably your mockStorer will implement them differently (i.e. oppositely) and, if that's true, changing that line would cause your business logic Logout tests to fail, right? Like, if you follow the test plan I described before, the first test (log out a logged-in session should succeed) would pass, incorrectly, but the second test (log out the same session again should error) would fail.
I'm imagining that `application_test.go` should have a `TestLogout` test that calls `Logout` and then verifies the intended effect by checking that the mock Storer now has one less row, and that a row for the session doesn't exist. ie) ```golang package application import "testing" func TestLogout(t *testing.T) { store := mocks.MockStorer{} app := New(store) // create a session sess := app.Login() rowLen := len(store.Rows) // remove the session app.Logout(sess) // verify exactly one session was removed if len(store.Rows) != rowLen - 1 { t.Errorf("expected %d rows, got %d\n", rowLen - 1, len(store.Rows)) } // verify the session no longer exists if _, err := store.GetSession(sess); err == nil { t.Error("expected error getting session, got none") } } ```
GopherCon EU is one obvious choice! [gophercon.es](https://www.gophercon.es)
I am actually looking into this TODAY. Are there any good example projects you know of? I basically have all of the REST service written and just need a simple frontend form that's all "AJAXy". Admittedly I have been out of frontend development for over 5+ years so I am looking for a straightforward way to simply place a form on a page, API calls to backend, frontend updates with responses. SUPER simple. In fact it's literally one text field with a button! I saw a few solutions with Vue+Go and one React+Go when searching but they all seem to actually run the node service for the frontend for development - feels like there should be an easier way to just test it without having to run a service.
extensive hello world! good job. however, the final code is modifying the string after the printf statement.
Thank you. Would it be safe from garbage collector removing it? (given it is still being referenced from the map)
Why not just go vanilla JS?
[removed]
&gt;This implies that your Storer interface has DeleteSession and DeleteAllOtherSessions methods. Presumably your mockStorer will implement them differently (i.e. oppositely) and, if that's true, changing that line would cause your Logout tests to fail, right? Unfortunately not. The current implementation only two mock implementations that are shared by all tests: either always return an error or never return an error. But this actually brings me to my next question: Should the mock implementation actually try to behave correctly and implement consistent behaviour? I thought it should not do that since the actual behavior of the function calls in combination with the business logic would be tested in separate integration tests.
I will copy and paste the response I gave to the comment above: &gt;I'm imagining that `application\_test.go` should have a `TestLogout` test that calls `Logout` and then verifies the intended effect by checking that the mock Storer now has one less row, and that a row for the session doesn't exist. Unfortunately not. The current implementation only two mock implementations that are shared by all tests: either always return an error or never return an error. But this actually brings me to my next question: Should the mock implementation actually try to behave correctly and implement consistent behaviour? I thought it should not do that since the actual behavior of the function calls in combination with the business logic would be tested in separate integration tests.
I will probably be extending the UI later
[removed]
&gt; Should the mock implementation actually try to behave correctly and implement consistent behaviour? Well, we're getting into the testing weeds here, and I'm sure lots of testing maniacs will find something to disagree with what I'm about to say, but: yes, mocks should generally behave like miniature, in-memory versions of the real thing, or at least a close-enough approximation that you can fully exercise your business logic. Sometimes that's modeling a database using fixture data in a map. Sometimes that's logging calls and checking the right things were invoked.
So, later, when you have a need for more complex UI, convert your front end to the framework. If it‚Äôs as simple as you say you‚Äôre definitely way over engineering now because you *might* make the UI more complex later.
I tried to join their technical writing team and they take it very seriously. I almost made it, but my writing style wasn't quite good enough! :( Don't they already have something like that for community written articles?
Damn, so we got everything wrong. Thanks a lot, man!
[https://github.com/jmoiron/sqlx](https://github.com/jmoiron/sqlx) [https://golang.org/pkg/database/sql/](https://golang.org/pkg/database/sql/) [http://go-database-sql.org/](http://go-database-sql.org/) [https://flaviocopes.com/golang-sql-database/](https://flaviocopes.com/golang-sql-database/) &amp;#x200B; A quick google search gets you plenty.
When they are still in the map GC won't remove them. When they are deleted from the map, GC is supposed to remove them, but there's actually [a bug](https://github.com/golang/go/issues/20135) that prevent that from happening.
So problem is that the JSON is always empty? First tip is when you write the payload, deserialize a structure. It is poor form to write out that JSON string. Someone should correct me if I'm wrong, but I don't think you can define var x interface{} And expect to get any info. Your interface is empty. Try serializing into a structure instead.
[removed]
I couldn't agree more. Their articles are top-notch.
It's looks like you got your answer above too, and I agree - mocks should try to implement a sort of in-memory observable implementation. For stores this is means actually using append on slices and exposing the slice for the test to check.
If you take a look here: [https://github.com/appbaseio/abc/blob/dev/appbase/user/user.go#L33](https://github.com/appbaseio/abc/blob/dev/appbase/user/user.go#L33), you'll `/user` gets appended to `common.AccAPIURL`, which you're missing in your implementation. It also looks like the response is in JSON, so you'll need decode it similarly to how they're doing it.
Your example code makes a one off request using the default http client, that passes authentication. Then your library call makes another request using a session and no Auth. So this session has never been authenticated: https://github.com/appbaseio/abc/blob/dev/appbase/user/user.go#L39 Your original request would need to authenticate the same session, most likely, so that subsequent requests are authenticated. That is assuming that the server does store a key in the session so you don't have to basic Auth on every request.
Hey, so whilst it's not directly applicable, I have created a small course on building a Go app + a ReactJS frontend: [https://tutorialedge.net/projects/chat-system-in-go-and-react/](https://tutorialedge.net/projects/chat-system-in-go-and-react/) &amp;#x200B; I would recommend decoupling your Frontend from your REST API as this approach allows you to independently scale/serve/monitor both parts independently should you need that control.
Unmarshaling to an `interface{} ` is probably the hardest way to go about it since from that point all you can do is a bunch of reflection. A better approach would be to try and use a type that more closely matched your json message structure. In this case I can see that your response body is always a slice of maps. And that the common key of each map is the "ev" field. So if that is an indicator of how you can handle each map, you have a couple options. 1) simply use `[]map[string]string` as your unmarshal type and then deal with converting the float fields as needed. 2) use a simpler type like `[]struct{Ev string} ` (with the right json field tag) to capture just enough to find out the message type. And then unmarshal it again using a more specific struct type that will handle every expected field in the right type.
TIL. I have used these so many times and always assumed they were DO written guides because of the quality.
Any harm in using your first recommendation, but instead going with \[\]map\[string\]interface{} ? &amp;#x200B; That seems to work, by the way. Only took all day! -=| &amp;#x200B; They should probably update their docs as well -&gt; [https://github.com/Polygon-io/client-examples/blob/master/websockets/golang/main.go](https://github.com/Polygon-io/client-examples/blob/master/websockets/golang/main.go)
Might [this](https://stackoverflow.com/questions/21037241/how-to-determine-a-point-is-inside-or-outside-a-cube) be of any help to you maybe? \[Approach #2 in the answer\] The concept illustrated there of calculating the vector from the cuboid center, to the point in exam and then checking weather such vector projections in the cuboid orthonormal system of reference exceed the cuboid side lengths seems pretty clever to me.
You might want to trim spaces before you print out not after.
I love you.
Rather than storing the 8 points of the post-transformed box (so the box in world-space coordinates), you should instead store the local unit-scale box and the transformation that took it into world-space (a translation, scale and rotation - generally a 4x4 matrix). Then to find if a point is inside it, you apply the inverse transform of the box to the point (which would take it into the same coordinate space as the unit sized box) then do a super simple bounds check.
Sounds expensive. How do you estimate the performance of your procedure? Also considering other basic operations, like rotation the box or moving and scaling it?
Rotation of the box, translation or scaling will be super cheap as you are just applying it to the matrix you have, and when rendering applying that matrix to your set of 8 unit points of the cube. &amp;#x200B; Inverting a matrix is reasonably expensive, depending on what you are doing you could store that with the object also, only updating when necessary. Alternatively if you were going to do an operation like checking the point against a whole \*lot\* of boxes, you'd probably want to have axis aligned bounds for all of them (the max, min set of the 8 transformed points) then cull against that as simple XYZ comparison to find the likely boxes you intersect, then only do the inverse transform on those select ones. If you are talking about a \*huge\* number of , then you'd want to add some sort of spacial indexing (like a octree or kdtree) on top of that, to reduce overall search cost.
&gt;. jsoniter appears to be much faster than ffjson at Unmarshal, and somewhere in the ballpark of ffjson at Marshal. Did you bench them or are you going off their ballpark performance statements? Since ffjson uses code generation and it doesn't look like jsoniter does, I'm kind of expecting ffjson to be faster, but that the expense of having to run \`go generate\`.
Sounds pretty reasonable. Thank you for the answer, I will check it out and write some benchmarks, then decide on what to use. I won't render anything, this is just server logic. Thanks for the keywords octree and kdtree! :)
&gt;Any harm in using your first recommendation, but instead going with \[\]map\[string\]interface{} ? &gt; That works as well. It just means that you need to type assert every value as opposed to letting the json unmarshal into a struct that suits the determined message type. Whatever works best for your use case.
Is that approach less tedious in other languages, and why?
They also fuck up a few buttons on Firefox, but I'll forgive them because of the quality of their tutorials. Particularly the ones on Nginx + Let's Encrypt.
Node for frontend is (mostly) used because you want the goodies of ES6 instead of writing vanilla javascript, there's no hard requirement, the only reason that you will find tutorials using it is because writing react code (or any complex code) in vanilla js is hell... Anyway, the [Vue](https://vuejs.org/v2/guide/installation.html) guide does not assume that you'll set up node for development and go straight with vanilla js. Setting up a simple build environment for modern js isn't that hard, webpack is a monster, but parcel is pretty simple and basically zero conf. Also, if the only thing that you need is a simple AJAXy form, jquery had a new release two days ago, new and shiny frameworks have their reason to exist, but unfortunately they also have a complexity cost that you have to pay.
&gt; This tutorial will walk you through creating this program in Go. However, to make the program more interesting, you'll modify the traditional "Hello, World!" program so that it asks the user for their name. You'll then use the name in the greeting. When you're done with the tutorial, you'll have a program that looks like this when you run it: That's already more knowledge than I have of Go and I've made a Discord bot with dozen of files and access to a database.
There's a spectrum of approaches, from raw `db/sql`, to [sqlx](https://github.com/jmoiron/sqlx) (which is where I land most of the time), to full blown code-generated ORM like [sqlboiler](https://github.com/volatiletech/sqlboiler) (which I use as well).
Man, Ruby on Rails is gonna blow your mind someday...
Are you looking for an ORM?
Check out: https://github.com/xo/xo https://github.com/gnormal/gnorm For a code generating approach. You write templates and it pulls data from a schema to generate all your model code.
I'm a novice on Google Cloud but here are two options I can think of: 1. Use one of the Google preferred ways to send mail like Sendgrid. Create the templates on Sendgrid, supply only the fields in the request from your application. 2. Upload the templates to Google Cloud Storage. Load and cache the templates in your GAE flex application.
[SQLBoiler](https://github.com/volatiletech/sqlboiler) too. Kallax does the reverse, but unfortunately it's essentially unmaintained and doesn't work outside of GOPATH.
I just use [gorp](https://github.com/go-gorp/gorp), it's a super easy ORM that "just works"
I don't think they are, but if so I always recommend [gorp](https://github.com/go-gorp/gorp)
Think you're maybe looking for a generator/scaffolding sort of project. Check this out. https://github.com/wantedly/apig
Thank you so much! Big helped!
You can create your own mocks and track which functions have been called, like [this](https://github.com/marcusolsson/goddd/blob/master/mock/mock.go).
I have a library that generates an API at runtime: https://github.com/royallthefourth/bartlett With this approach, you effectively write nearly all of your queries on the client side. For the few special-purpose queries that don't fit into this model, you can write your own endpoints. It's made to automate 90% of it so you can focus on the interesting parts.
You'll need to use `init` if you ever use the `[flag](https://golang.org/pkg/flag/)` package
Another cool project that could help: https://www.prisma.io
if you use pg, try go-pg with [genna](https://github.com/dizzyfool/genna)
Couldn't you also use the builtin "json" module?
Well, I've been using gorm. Looks like there are some other options I need to look into.
A very nicely written tutorial. Small typo in the last example though, in the code they put the function that trims the white space after the Printf call so the new line will still be in stdout
I like and use xo, but know that it‚Äôs mostly unmaintained
Heres a small app Ive started using GraphQL, GO, and React. &amp;#x200B; [https://github.com/longfellowone/field-services](https://github.com/longfellowone/field-services) [https://github.com/longfellowone/field-services-client-v2](https://github.com/longfellowone/field-services-client-v2)
Where is the source code for this? I just went to have a look and got lost in their [admin.go source code](https://github.com/cockroachdb/cockroach/blob/master/pkg/server/admin.go). 2000 loc in one file, as bad as Reddit source code. We really have no best practice in real world Go apps.
The artcle still hasnt been fixed and shows them trimming after the print :) maybe tomorrow they will update it.
I've spent some time evaluating Prisma but haven't used it for any project. My impression is that it wants you to give up any concern for the database. There's disagreement between programmers about the value of using an ORM or not, but Prisma strikes me as an ORM where you give up even more control than a typical ORM. For example ([from an article about their evolution](https://www.prisma.io/blog/prisma-and-graphql-mfl5y2r7t49c/)): &gt;Today, developers shouldn't care about *how* the Prisma client talks to the underlying database. **What developers should care about is the Prisma client API!** This goes against what (some?) experienced programmers will say, where you will eventually need to understand how the ORM talks to the database, and you sometimes need to break out of that when the ORM itself is not a good fit for a specific task. I worry that using Prisma is giving up too much control. To give a specific example, I used [https://github.com/99designs/gqlgen](https://github.com/99designs/gqlgen) to build a GraphQL API, and implemented pagination. Prisma (at least, an earlier iteration of it before they stopped providing a CRUD GraphQL API -- I haven't checked latest) provides a way to do [GraphQL connection style pagination](https://graphql.org/learn/pagination/), with a catch: you can't filter or order client-side ([\#62](https://github.com/prisma/prisma/issues/62), [\#95](https://github.com/prisma/prisma/issues/95)). This can be very important -- say, for ordering or filtering a list of invoices by client name, when client name is stored in the database in a separate table. In my case, I was able to build a relatively simple implementation of such pagination because I could write custom SQL to support the exact filtering and ordering I wanted to expose via the GraphQL client. This required me to have tooling in place to auto-generate code to interact with the database, which, being in place, eliminated the need for using something like Prisma for even the simple cases. Also, if being able to meaningfully contribute is valuable, Prisma [looks like it may not be simple enough](https://github.com/prisma/prisma/issues/62#issuecomment-438664572): &gt;While I truly appreciate the sentiment by [**@baerrach**](https://github.com/baerrach) to dig in and contribute, I also acknowledge that Prisma is a very complex project to contribute to. &gt; &gt;It certainly is possible for members of the community to contribute to Prisma, but we have found the overhead of getting to know the entire system before being able to contribute is so high that it is difficult to justify as a regular Prisma user. We very much do welcome contributions, but I think the only sustainable solution is to increase our engineering team. For my part, I've been using [gnorm](https://gnorm.org/). I don't know if it's the best option, but I do like it.
gnorm is something I've been happy using to auto-generate basic CRUD functions for postgres.
Ruby is a language. Ruby on Rails is a framework built on Ruby. [Buffalo](https://github.com/gobuffalo/buffalo) is a framework built on Go.
I strongly disagree with the downvotes in this comment. I work with both Rails and Go, and while I love both, Ruby's productivity is way higher than Go, IMHO. ActiveRecord is a pleasure to work with, and we don't have anything close to it for Go yet.
I wrote gnorm templates to generate dB code, protobuf def and crud services for a grpc service. I felt like the work flow was fantastic so that‚Äôs my 2 cents, and adding tables and fields works flawlessly, I just run a make script called rebuild after I migrate the dB and boom new working api endpoints with filtering ordering and pagination.
Of course we know that! But even if we're kind of comparing apples to oranges here, the point is still valid: we don't yet have as good libraries/tools to work with databases for Go as we have for Ruby (ActiveRecord, etc).
Actually if you are just replacing an item in the map, GC should work, e.g.: ``` user1 := &amp;User{...} ug.users["user1"] = user1 user2 := &amp;User{...} ug.users["user2"] = user2 ``` As long as there are no other references to `user1`, it will be claimed by GC. The bug affects when you are deleting from the map.
No, I was just seeing if there was a faster way to develop or a better pattern with database/sql.
I was asking about non-ORM solutions for golang without a framework. Although, I personally love Django and find that the ORM is very nice.
Though I think comparison can be valid. When people say Go vs (insert framework), they typically imply go stdlib.
https://github.com/upper/db Is all you need.
No? I use flag exclusively and I never use init with it anymore. You make a func that takes in a slice of strings and spits out a runnable object. It‚Äôs easy. Nate (the blogger here) had a good post about how to structure CLI apps.
Yeah but the problem is, until you've learned the ruby conventions you're dead in the water. I can't read rails code. I don't know what it's doing. It's completely unclear no matter how long I stare at it and that won't change until I memorize the "rails" way of doing things.
I'm a huge fan of [sqlx](https://github.com/jmoiron/sqlx).
This is just a helper to generate code for use with the built in JSON package.
If that‚Äôs the problem, just make them all implement the same interface.
I completely agree with the down votes. I was asking if the original poster felt this was an easier in other languages from a language perspective. There was no mention of frameworks here. Other languages have framework, and Go has frameworks. It sounded like the complaint was against the capabilities of the language. RoR isn't a language.
5. `error`
IMHO, Go being a very safe, high performance language, I don't choose it for a database application expecting easy. I get that scanning rows one at a time into structs can be a pain, but I've found it reasonably easy to write a few methods around my structs to handle the database operations, similar to what you described, and then stop worrying about it. That being said, if you have a lot of complex operations on a lot of data structures... There's not much getting around the fact that it's work. My other primary language is PHP with Laravel, and yeah, it's way easier. But I can do all the same things in Go without a framework, with a little more work, that absolutely smokes PHP on performance and reliability in my experience. And I've been writing Laravel apps much longer than I've been writing Go. There are tradeoffs for sure and Go's strict typing can make it slower to work in. I don't find myself doing much prototyping in Go, but I'm happy to write a few more methods, queries and scan loops for the stability it gives me.
I think you mean the rails conventions.
yep. the \`Get\` and \`Select\` calls are really all you need
NamedExec is great too
I'm not sure that there's really much established convention here, but two things come to mind: - Context first - Configurations/Tweaks are often at the end, because they tend to grow and they lend themselves to optional parameters (via something like `func foo(a, b, ...Option)`) And, personally, I refactor to either an object, an input struct, or just a different function layout if I hit the 80 character limit, which tends to be 4-5 params.
Java with spring jpa, for example is mostly magic interfaces and annotations, very little boilerplate at all. The transactions are declaratively managed, and sensible. The downside, compared to Go is hundreds of Meg's of heap used, transient objects all over the place, and once you have a non-trivial object graph, it can take tens of seconds to start. And oh, yeah, a dependency tree that will make you weep.
I‚Äôve been searching for something as clean as this for quite a few months now. This is great, thank you so much!
I would love to not use Init and not use underscore imports. Issue is, the Go sql module forces me to do so. I wish it wasn‚Äôt this way... if anyone has any tips for a better way to interact with databases that doesn‚Äôt involve registering a driver with database/sql (I work with PG and Cockroach) I would love to hear it.
One other detail about your code you might not expect: you're not zeroing \`msg\` each time you unmarshal. So the 2nd Unmarshal result is the combination of the first result, with the second reply message overlaid on top. That might not be what you intended. It would be more common to move \`var msg interface{}\` inside the loop, and unmarshal into a fresh, zeroed \`msg\`. PS You can avoid \\" everywhere by using \` to enclose the string. PS You might not want to construct JSON with Sprintf if there is any way APIKEY or CHANNELS might need to be escaped. PS Like everyone else says, try to use a struct, or at least a map.
https://github.com/jackc/pgx if you don‚Äôt need to support access to lots of different sql databases, just one decent one
Are your templates available publicly? I've been working on the same thing. I have the protobufs being generated and the conversions between the Go GRPC structs and database structs. It would be nice to compare/contribute if yours are available. I second this approach.
Thanks for the elaborate explanation. I must admit I was just looking into it, but you pointed me to an interesting discussion that could help me evaluate it faster!
[removed]
You may also consider using [xorm](https://github.com/go-xorm/xorm) that is used in [gitea](https://gitea.io).
Your post was a useful anchor for me to put those thoughts down. They are fresh on my mind because I've also just recently been looking into Prisma. I was comparing it with what I did in fact use for a recent project, considered it in that context as to what advantages and disadvantages there would have been if I'd used Prisma instead. It's been leading me to think about the role of ORM's overall. It seems to me there's a spectrum all the way from what the OP is doing -- custom structs and functions for CRUD for each table -- all the way up to a full-fledged ORM like Prisma that doesn't want you to touch the database. I'm currently thinking the sweet spot is somewhere not far past sqlx -- something to inspect the database (gnorm) and generate structs for tables and their basic CRUD type operations for you (using sqlx or similar), possibly along with a query builder like squirrel. The reason for a query builder is because you end up building one anyway -- something to construct, for example, your 'WHERE' clauses. Plenty of words have been spent justifying or criticising ORM's, and there's certainly no agreement among programmers. I think that eventually the initial similarity between application and database will break down, so better to set yourself up with a solution that's friendly towards eventual customisation because you will almost certainly need it. I'd be very interested to hear from other Go programmers if they think the sweet spot lies somewhere else.
ActiveRecord is the ORM in Rails. It works by using the metaprogramming capabilities of Ruby with naming conventions. Eg the class `Book` is backed by the db table `book` and all columns are mapped to instance variables. The developer just needs to declare relations between classes and annotate the parts that don't rely on naming conventions. ActiveRecord enables the developer to write Ruby code that's transformed into SQL on runtime. You can be very productive with very few lines of code. But there are downsides. To debug your queries you need to know both ActiveRecord and SQL. Sometimes it's better to write explicit SQL than to rely on the magic of Ruby and ActiveRecord.
That's wrong. The metaprogramming possibilities enable the productivity gains of Rails and it's ORM ActiveRecord.
How is it different from an ORM ?
have you tried sqlx?
and structscan() helps a ton too
I stopped when i saw they suggested to use "nano" &amp;#x200B; This message is brought to you by the VIM gang
if you use that tool check your company's policy regarding the use of unauthorized software and the use of network tunnels over the company firewall. Don't get fired for stupidity.
I recently wrote a longer response to this question here: [https://www.reddit.com/r/golang/comments/b9h8xo/the\_state\_of\_orms\_in\_2019/ek5we7k/](https://www.reddit.com/r/golang/comments/b9h8xo/the_state_of_orms_in_2019/ek5we7k/) &amp;#x200B; The tl;dr is that you should use introspection to read the schema from your database and generate type-safe CRUD methods. I've been using this for the last 2 years and it's transformed how I've worked. &amp;#x200B; I'm currently working on making the [https://prisma.io](https://www.prisma.io/) generated Go client awesome. If you have any thoughts or questions about it, please let me know!
Doesn't seem to be.
Dotnet core with entity framework is very smooth with an code first or db first approach. You can separate the mapping logic from classes to db entities in extra files. For high performance queries you can exec plain sql as well. The boilerplate is quite small. You only need the mappings and a context class. Kind regards
I think the number of layers and abstractions is not that relevant. What I mean by native is using the native API and the standard libraries offered by the operating system to provide a native feeling to the end user.
I recommend you to give [https://github.com/volatiletech/sqlboiler](https://github.com/volatiletech/sqlboiler) a try, it's amazing.
It's tedious but it's simple. Then after the first shot, when you have to maintain and optimize this code you're happy to have something really simple. I just use sqlx and few helpers to build simple insert/update from a map and build where clause.
why dont you try [GORM](http://doc.gorm.io/)
Thanks!
Thanks, I will!
Gorm works for me, but another thing that works well is accepting that typing on the keyboard is not a problem, copy and paste is ok for now, etc. My strong preference for LOC compactness (coming from Python/Lisp say) is a very counter-productive mindset in Go.
Yeah, but since you agree that this is the way to do it, I'll follow that scheme. Thanks a lot
[removed]
lgtm
Impressive! I was wondering how this achieved such 3D graphics!
Then even by those standards it still native. For example - GTK Writes Those calls directly to the window manager and even uses OpenGL in some cases to do so. This toolkit does the same - It‚Äôs just primarily done in OpenGL. At the end of the day all those native apis you‚Äôre talking about are just OpenGL, DirectX, or Hardware display calls. This is just cutting past all the tech debt.
It's worth noting that spending the extra time typing everything else out has always taken less development time than the generics-based code for languages like PHP and Node.js. Eventually some developer gets some type or variable wrong and (without any compiler type checking) there is some error/corruption/downtime that requires X man hours to find and fix. We've never, ever had a query problem in Go from the wrong variables being used resulting in a little more time upfront - but no time after that. That said, I'm still hoping for a performant generics implementation + and encourage using code generation from [https://github.com/xo/xo](https://github.com/xo/xo) or [https://github.com/gnormal/gnorm](https://github.com/gnormal/gnorm).
I'm a beginner Go coder, so take this with a grain of salt. What is outline supposed to do? To me it looks like it's doing a depth-first traversal of the HTML body and appending ElementNodes' data to a slice. Do you understand a [depth-first traversal](https://en.wikipedia.org/wiki/Tree_traversal)? Somewhat confusing is that a certain depth's stack is unchanged by future calls of outline (since they either allocate a new array or they only append data after the current slices length). That means the popping of ElementNodes' data from the stack at the end of the function is unnecessary. To understand this you need to understand [everything in this blog post](https://blog.golang.org/go-slices-usage-and-internals).
Panics are not error handling. Why are these being hit in production?
I would not recommend this cause it seems it is no longer supported
Huh? Ive been using upper in prod for few years and the core dev is still active.
Last commit was 4 months ago and considering that there's many issues and pull requests it's not very active. Maybe I am wrong though
That's exactly why this is here! If there is a logical flow in your code or if you have made a mistake that isn't accounted for, the goroutine on your web server is going to panic. This could lead to all sorts of issues in your apis, but will go largely unnoticed if you don't have some sort of monitoring or alerting on the panics. This library just means if a panic you didn't account for does occur, you can take a specified action.
While panics probably should not be used. Panics are [expected possibilities by the `net/http`](https://golang.org/src/net/http/server.go#L78) and [must be accounted for](https://golang.org/src/net/http/server.go#L3168) as a backup to prevent application crashes.
This ^
My only issue was that the structure isn‚Äôt consistent. For example, ‚Äúev‚Äù is there every time, but ‚Äúsym‚Äù isn‚Äôt. Will this be an issue?
I have been using GORM for a side project lately... it's been really nice to use. Highly recommend!!
Thanks, I‚Äôll read the resources linked. Skimmed the Wikipedia article and it seems to be fitting. Outline gets the html tag from a given html document. As far as I understand there is no popping happening at all, just a passed copy of the list.
Nice, would love to watch the progress of this over time. Maybe some tutorials would be nice as well.
I would say this doesn't belong in the application. Panics should be identified by alerting in Kubernetes.
Panicking in a goroutine (which is what happens when someone makes a request to your server) would not cause the pod to crash, the user would just get a 500. You could monitor 500s but they may be caused by other issues beyond panicking and it could be difficult to diagnose. I therefore think there is value in this.
https://github.com/spaceuptech/space-cloud/blob/master/README.md
GitHub Link: https://github.com/spaceuptech/space-cloud
This is correct.
Also you shouldn't just assume everyone is using kubernetes :)
Haven‚Äôt checked your code in detail but the reason your having issues with testing is that you don‚Äôt use interfaces which would allow you to just store the output in a var instead of printing it to stdout. Basically the same issue that occurs often when you want to mock network calls instead of talking to an actual backend while testing.
My first meme, please spare me
Take a look at this. It‚Äôs where I figured out how to mock and test printing to the console. https://github.com/quii/learn-go-with-tests/blob/master/mocking.md
Sorry I don't know what that means?
I think what is catching you up is the "if" statement - they only push to the stack and print if the node is of type Element, but still iterate over and recurse into other types. So the page starts off with a Document node, and skips over some text nodes, maybe some comments. If you can get "delve" running (maybe with visual studio code or something), try stepping through the code, one step at a time. Otherwise, mark the beginning/end of loops and try to keep track of how deep in the (real) stack the loop is at. Try this: [https://play.golang.org/p/6-Llv4Yuj7l](https://play.golang.org/p/6-Llv4Yuj7l).
Looks good to me
I'm still waiting for a fork of Neovim called "Nanovim"
Their webpage could use some proof-reading. Also, I know all the words they're using and still don't know what it does.
It's actually work based on an existing tutorial: [https://lodev.org/cgtutor/raycasting.html](https://lodev.org/cgtutor/raycasting.html)
Hey buddy can you help us out
Is the system time correct?
Username checks out.
Looks like https://github.com/spaceuptech/space-cloud is the next evolution of products like https://github.com/meteor/meteor by adding multilple databases, AWS Lambda / Google Cloud Functions, and distributed messaging/processing with NATS/Kafka. I'd really like to see this with TiDB or AWS DynamoDB support.
It seems to be, nothing was changed with it. I've opened up port 80 to all requests since it seemed some people were posting that could be an issue, and now I'm getting a new error: acme/autocert: unable to authorize "mydomain"; challenge "tls-alpn-01" failed with error: acme: authorization error for mydomain: 403 urn:acme:error:unauthorized: Cannot negotiate ALPN protocol "acme-tls/1" for tls-alpn-01 challenge \`mydomain\` is replaced with my two subdomains (one error for each).
Are the dns entries for your domain resolving to the correct public IP?
Hey. Really appreciate the thought. Maybe you could join our discord server to help us make it better? Here's a link: https://discord.gg/ypXEEBr TiDB is already supported (via the MySQL driver). Cassandra and Dynamo DB are there in the near term roadmap. You think you can create an issue on the same so we can actually accelerate this feature request?
Just checked and yes, the IPs look correct.
I see, that does make sense. Thank you!
Try using another acme client to generate your certs in verbose mode, Tcpdump the connection showing ports open ports to LE and see if it generates your certs. If it fails it might indicate a service issue on LE‚Äôs end or firewall issue on your end. Moving from there on check port 443 is open for your application and try again (once again use tcpdump to verify ports) If it works from the same machine using another acme client, capture a pcap. Then capture a pcap from your application.. look for differences to start isolating where in the stack the issue is.
Not at the moment, it's part of a project we plan to open source, but we want to finalize/refactor a few things and get the documentation up to speed.
Thanks for your responses, it helped me implement a workaround. I tested the certbot-auto program from EFF (following their guide for Debain 8), and it was able to successfully obtain a certificate. Following this, I tested autocert again by restarting my server, and I was still getting the same tls-alpn-01 authorization error. As a workaround for now, I took the certificate that was manually created for both subdomains and hardcoded the `fullchain.pem` and `privkey.pem` files in the `tls.Config` struct for my HTTPS server (using `tls.LoadX509KeyPair`). I also had to change ownership of both the `archive` and `live` folder in the `/etc/letsencrypt` directory to the user of my Go server. It seems all is working now, albeit it's going to be a pain not having the auto-update feature of autocert. Thanks again for your help!
&gt; To me it looks like it's doing a depth-first traversal of the HTML body and appending ElementNodes' data to a slice. This is exactly what it is doing.
Why not just implement a failover.. if autocert fails, run certbot with hard coded pems?
 func main() { c, _, err := websocket.DefaultDialer.Dial("wss://socket.polygon.io/stocks", nil) if err != nil { panic(err) } defer c.Close() f, _, err := websocket.DefaultDialer.Dial("wss://socket.polygon.io/forex", nil) if err != nil { panic(err) } defer f.Close() _ = c.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"auth\",\"params\":\"%s\"}", apikey))) _ = c.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"subscribe\",\"params\":\"%s\"}", schannels))) _ = f.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"auth\",\"params\":\"%s\"}", apikey))) _ = f.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"subscribe\",\"params\":\"%s\"}", fchannels))) var fmsgs []map[string]interface{} var smsgs []map[string]interface{} go func() { for { err := c.ReadJSON(&amp;smsgs) if err != nil { panic(err) } for _, val := range smsgs { for k, v := range val { fmt.Println(k, v) } } } }() go func() { for { err := f.ReadJSON(&amp;fmsgs) if err != nil { panic(err) } for _, val := range fmsgs { for k, v := range val { fmt.Println(k, v) } } } }() } Here's my updated code. Seems to work, but won't know until Monday when the market opens. I'm sure this isn't the most efficient way, but any feedback would be appreciated.
It's going to work in the sense that it will be able to extract the key/value of each map in the slice. But don't forget that value will be an inyerface{} and you need to carefully type assert it into the desired target type for each field.
[removed]
Writing unit testable code: If you want your functions to perform side effects, you should encapsulate each side effect within an interface. The interface could be an actual Go interface, or even a user defined function type (I love higher order functions). However, this alone will not make your code more unit testable. To do that, you will need to follow the principle of Inversion of Control, meaning that your interface should be passed into the function, not instantiated within the function. You could do this in one of two ways: - accept the interface as an argument to the function. Or - attach the function to a struct that has the interface as a property and obtain the struct instance through a receiver. Personally, I like to keep my functions reverentially transparent. Meaning that I push all side effects to the top most level of my program that I can. This is often times the main function.
It‚Äôs so you can cancel it.
It‚Äôs to support cancelling and timeouts. Eg: your proxy or gateway times out after 60 seconds, so you want your app and database to stop working on the request, too. The DB driver will watch for the context cancellation (possibly via timeout) and issue a command to the database to tell it to stop working on the request.
Have a question regarding NamedExec, Suppose I have an insert query, and in the db level the table has default values so that i don't need to include it in the insert query, it returns only the sqlx.Result and I cannot struct scan it. I find myself using NamedQuery instead so that I can struct scan the rows. Or is there a better way of using namedExec that I don't know about?
I‚Äôd suggest making a simple rest API that connects to Grpc backend. And then to a third Grpc service. This will let you run benchmarks at each level. Also, try compiling with the different plugins like Gogo fast to see what the diff is. The Grpc gateway project has a version of this that compiles all from the Proto.
Anything where you need at least two services. For most projects I can think of, it‚Äôs necessary because one far outlives the other or has a different privilege level. Some possibilities: - A basic database with bolt + grpc server and a CLI/client to use it - A basic web server (or maybe a proxy) that collects stats and has rate limits / throttling configurable over grpc - Depending on your OS, a root daemon with a grpc server that lets you do some limited ‚Äúroot level‚Äù tasks from a non root account - a file watcher daemon that receives OS events for when files change and a client application can query it for which files were changed most recently - A keychain-like root daemon and client app that ‚Äúsecurely‚Äù stores passwords and such in files only rw by root. The client can request and update info when authenticated
[removed]
&gt; I recently needed an LFSR... What did you need it for?
The standard \`math/rand\` package is already an LFSR, although I'd like to change that for reasons explained here: [https://github.com/golang/go/issues/21835](https://github.com/golang/go/issues/21835). &amp;#x200B; They are not very good generators. Although popular in the 1980s, the theory has moved on.
Very cool! Lots of opportunity here.
I needed pseudo random number generated that would not repeat, so if I got the number 3 I would not get another 3 again. I did not want to store all the numbers I already got so that I could skip them, so for the range I was interested in Lfsr32 did the trick, but it was not really much effort to provide the others. So basically unique random numbers, at least until the period of the cycle repeats.
I did look at the standard implementation, but rand.Intn for example would give duplicates with in the range. Is there away, without storing and checking each random number to ensure that I had not received it before? I understand it is not state of the art, but the requirement was to get a essentially shuffled list of numbers without needing to store them either beforehand or incrementally.
thanks
[removed]
If you must have more concurrency than the sharding gives you, and the lists are short enough that a singly-linked list can be used, there are lock-free algorithms for manipulating singly-linked lists (basically you turn the bottom bit of the \`next\` pointer into a flag indicating "the next item in the list is deleted; ignore it and carry on down its \`next\` pointer"). However I've found writing lock-free code in Go is inelegant because even though everything you need is there in the \`sync\` package (load &amp; store with barriers, and CAS) the code becomes unreadable because of all the casting to/from \[\*\]unsafe.Pointer. So I don't recommend it unless you're really looking to trade readability for speed.
Sounds like something that would take me a day to figure out how to do badly, but the concept sounds cool
Try remote grpc commands: message CmdRequest { repeated string args =1; repeated string env=2; string dir=3; } message CmdResponse { bytes output=1; } service CmdService { rpc Cmd(CmdRequest) returns(CmdResponse){}; } Compile protobufs and then satisfy the server and client interfaces. Check out the credentials package in the grpc repo for authentication.
I love grpc, and have been experimenting using different languages (the main point) across it. For Go there are two (or maybe more) grpc implementations (or variations), or was it the protobuf stuff? (not sure). My experiments are here (using bazel): [github.com/malkia/rpcing](https://github.com/malkia/rpcing) \- If you have bazel, just do "bazel run :py\_bidi\_go\_server\_go\_clients" this starts one "go" server, and three "go" clients in bidi (streaming both sides) mode. Nothing really practical, just learning experience... grpc itself has really good example with route\_guide - showing all kinds of calls (unary, streaming in, streaming out, streaming in &amp; out (bidi)).
I'm building something in Go using gRPC. I think you might be interested - http://github.com/gravetii/diztl.
Very true. They had another example somewhere with an \_ on the left, but that also doesn't compile. So they have to have some other trick in their code.
maybe use `log` instead of `fmt` ? imho it should to to `stderr` instead of `stdout`
No, that sounds like nonsense.
Commenting, because just an upvote isn't enough: it's absolute bollocks.
CEO blabber. Pair Programming is not a principle. Pair Programming is a practice for sharing knowledge to improve the quality, consistency and maintainability of software. You can Pair Program just as well in LOLCode as Go. http://www.lolcode.org
A message broker, that's what I did to learn gRPC. Bonus, it appeared to be useful and I still use it in my project. I can provide examples if you need. Happy coding :-)
I don't know. Whether they have succeeded or not, the creators of Go have talked at length about trying to design the language to maximize readability so that others will read code written in it. Which seems to be what Schmidt is referring to when another person 'checks it'. If there is such a thing as a language designed for [essentially] pair programming, a language that others can easily comprehend in the moment seems like it would be it. Imagine trying to pair program using Brainfuck. But he might have said the [Go! language](https://en.wikipedia.org/wiki/Go!_\(programming_language\)). It is hard to tell.
I think you're being overly credulous. It's not even a good description of what pair programming is.
yes.
I mean, he literally says "essentially pair programming" so we already know we're not talking about pair programming in the purest sense. In truth, it really reads like he is really talking about code review and fumbled his words. It happens.
See the bottom section on concurrency in this [article](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html). The basic idea is that instead of queuing the thread, you should queue the work. Sharding only works to a small degree due to a hot entry being fetched frequently, but all of those accesses are under the same sharded. The described approach lets you apply other eviction algorithms, such as in the [follow-up article](http://highscalability.com/blog/2019/2/25/design-of-a-modern-cachepart-deux.html).
Sending images from a client, doing some processing in the server(in my case performing camera calibration) and receiving the intrinsic and extrinsic parameters and distortion coefficients. &amp;#x200B; I did this in my mini-job. It was a prototype. I used protocol buffers as the messaging medium. It was a fun project.
Impressive work for a weekend
My current understanding is as follow. The painless readability of the Go language resulting from its simplicity could be considered as a strong advantage for pair programming or any kind of code review.
A context is the overarching concept in a request. So say a client makes a request to a http server. The request on the server side is a context. Now you need to do a bunch of work, pulling in data from various sources. Maybe from redis. Maybe from sql. Maybe another rest api. Now imagine the client closes the connection. You still have 3 more requests to make before you can return something, but there‚Äôs nowhere to return it to. A context, as go implements it, is a convenient and hierarchical way to abstract this. Internally at google, anything that processes a user request needs to have a context as its first argument (except legacy stuff like HTTP requests, that predate context being first-class). You use it as a way to make things a lot easier to deal with. If you want to have something time out after X seconds, you just wrap a context. If you just want to respond to a cancelled request, then just pass the context. Perhaps you have a bunch of SQL queries that run in parallel with goroutines, you just pass them all the same context so you can cancel them all, or wrap the base context with a timeout. Or, if one of them can fail, you wrap the context in a cancelable context and if one comes back, it fires the cancel to halt the rest of them. Lots of reasons to use contexts but it‚Äôs generally a great abstraction for making code a lot more functional and responsive. GRPC uses it as a first class citizen by allowing you to even pass values through it like global request identifiers to aid in tracing.
Your getting started pages are giving 502 errors: https://spaceuptech.com/docs/quick-start/manual
Sorry for the inconvenience. We were fixing a bug and making it live. Try once again.
There is another type of LRU which you might consider. It is not a perfect LRU, but it's a good compromise with speed. Values are stored in an array with a boolean flag. The flag is true when the entry may be discarded. Any time a value is accessed, the flag is cleared. The array has an associated index (or pointer) keeping track of the next entry to drop. Lets call it dropIdx. When a new value must be stored in the cache, you increment the dropIdx (setting it at 0 when the end of the array is reached) and look at the flag. If it is false, the entry may be dropped and replaced by the new value, otherwise, the flag is set to false and the next entry is considered. If it loops twice over all entries, it means that all cache entries have been hit while it did the search for an entry to drop. If must then pick a random entry, usually the first entry it considered.
Who decided to use such uncomfortable tabourets for panel members? :) Maybe it's just me, but it's almost painful to see Go team members sitting on those. Ergonomics of sitting is an important part of programming world, after all, and those tabourets are everything but comfortable sitting device.
Yes, I think `go fmt` helps in pair programming, because it eliminates discussions about formatting. I would also argue that the same goes for when you have to choose between different styles of loops. Go just don't have that many variants to choose from. But I don't see that Go was designed for pair programming.
This is fair feedback, thanks. The log catcher was mostly meant to be used as a guide for how easy it is to make catchers. However, I will make these changes!
My first feedback would be to write better submission titles. Would have guessed it's written in golang, being in /r/golang. The title is long, but the actual important part is covered by a letter *a*. A project. Come on. Sorry for the rant, but I hate meaningless titles and clickbait, whether intentional or not. Also, the project looks interesting and really mature.
There's still something wrong, they take ages to display (not load). Browser is indicating that it is done loading the content and I'm staring at a white screen for maybe 2-3 more seconds. After each click.
From looking at the API, ISTM that it has a consistency problem: What if the result set changes between the user loading a page and clicking "next"? Because you encode the position as an offset into the number of results, items might get skipped or returned twice. [The way you are calculating](https://github.com/cyruzin/tome/blob/1030984835604ffd61ebedc7ac8a9aa6d8d540dc/tome.go#L47) also breaks if the limit changes between invocations - say, I click "next" a couple of times and then change the number of items per page, because I get annoyed. It is a better policy to paginate bast on the last item served. That is, for example, have the "next" link include the sorting key for the last item in the page. That way, even if an item gets added in-between, it gets shown in the next page. And changes to the number of items also get reflected correctly.
Agreed. I think he's on about the enforced code styles built in.
Need to look into it. Why don't you raise a issue suggesting the same? Here's the link. https://github.com/spaceuptech/space-cloud/issues
Man this is pretty awesome!
So far I've been using [https://github.com/go-pg/pg](https://github.com/go-pg/pg). One of the main reasons for choosing it was how easy it is to use postgresql schemas which is a must for enterprise apps. My only complain this project is that it is maintained by a single person who does it great job at implementing features and fixing bugs but the project still lacks great documentation and release management.
Very nice, I'll improve, thanks :)
Looks like a lot of unsafe code, not sure if its a good tradeoff over runtime.ReadMemStats
fix what damn thing?
Hmm do you mean mem leak or data race?
Hm... Just white old men
Nah, just using unsafe and syscalls
I'm gonna fiddle with this code first but it's amazing and the point is to run commands by clicking text. It's basically acme written in go. https://github.com/as/a
&gt; Prior to that, the sequence will be a pseudo random ordering of all the unique numbers in the range. AFAICT, this is the "full cycle" property: https://en.wikipedia.org/wiki/Full_cycle Another, small state, full cycle PRNG based on combinations of prime numbers available here https://godoc.org/modernc.org/mathutil#FC32 (32 bit) and a `big.Int` one here https://godoc.org/modernc.org/mathutil#FCBig
That's what you get if you pick the best for the job and don't pick people based on their gender.
Or it is a function of who was let into the club back in the day..
Before I began the project I had found an earlier Golang based [raycaster in a Gist](https://gist.github.com/peterhellberg/835eccabf95800555120cc8f0c9e16c2) which used another Go 2D engine named [Pixel](https://github.com/faiface/pixel), but it didn't support multiple levels of height which I really wanted and I had already started getting used to Ebiten and didn't want to switch. But I may be using some pieces of it soon as I try to get floor casting going.
I can recommend Kubernetes. It's even bigger than Prometheus, but you can focus on a specific part of the project: CLI (`kubectl`, [`kustomize`](https://github.com/kubernetes-sigs/kustomize), etc), API, networking or something else that you find interesting. They've got great [community docs](https://github.com/kubernetes/community) and it should be relatively easy to find help (in Slack or in a mailing list). About focusing on a specific area of the project: have a look at their SIGs (Special Interest Groups) structure in the community docs. Each SIG has a description, a list subprojects they are responsible for and contact details. They also have their own meetings. I'm trying to be involved in SIG-CLI and I can say that they all very helpful and nice people. SIGs are usually very transparent (for example, in SIG-CLI - anyone can join meetings, see the meeting agenda and notes, meetings are recorded and [available on YouTube](https://www.youtube.com/playlist?list=PL69nYSiGNLP28HaTzSlFe6RJVxpFmbUvF)). You might find that it takes more time to receiving feedback about your changes than in smaller open source projects. I can't say about whole Kubernetes, but some SIGs need more time to review changes than others. More generally: It's probably not a good idea for some people (like me) to start contributing into something that they are not using or have no interest in exploring. "I'll do it just to gain experience" - is not enough for me. Especially because I do it in my own time. Think about it before investing your time into a project and find multiple reasons to start contributing.
Did you meet the standard? If yes it is not too high for you
From the perspective of coding problem it is definitely interesting and challenging, but I doubt that they are going to use that in production. I wonder why would they want you to reinvent [rsync](https://rsync.samba.org/) or [unison](http://www.cis.upenn.edu/~bcpierce/unison/) This sort of things when interns are assigned with some tasks to sweep under rug once the internship is done is so weird nowadays.
That is a nice looking library, I will definitely take a more extensive look at the library.
Well, they haven't given me an offer letter yet, not even a verbal confirmation, so I just hope I meet the standard. But, I loved creating this tool. :)
Indeed, it is interesting! If I get selected, I am going to ask them why reinvent the wheel? :)
Because the only reason to pick women is their gender, it's not like they can be as good or (gasp) better than men at anything
Oh, so this was the take home assignment for a week?! Man, these people are nuts, they don't value your time neither they would appreciate your contribution. I'd stay away from companies like that. Sadly, the bar for the internship is set so high now, I wouldn't be surprised if in a year they would ask people to write Linux from scratch just to be considered for applying for a position in their unicornish company in bay area. Flip side - once you have some projects and experience under your belt - then it will be **your** market and **your** rules. Good luck with that!
this one is cool, all source code is better to read and customized for me.
No you're not the only one, crazy high, I think the standard is a task that takes only a few hours of work and that paid too.
Usually you start contributing to a project because you found a bug or you want a feature. You only learn the parts of the code that are relevant to that bug/feature. As you spend more time contributing to a code-base, you touch more and more parts of the code. Most of the time, only a few developers understand how the whole system works.
Thank you :)
I think for internship this is definitely too much. It is kind of happening now a days, one way to get work done for free. I did it for a company recently. After discussing my solution with them I got to know that they have an outdated old architecture for this problem and are in the process of finding new solution which they got for free from me. They never reached out to me with an offer, though on my face they said I was selected.
In end, I just hope I get this Internship, cause if I don't, I would seriously be a little disheartened.
This is sad. I guess this is why culture is so much more important then it is credited. What happened to you clearly reflect on how much that company respect people. But, I am guessing you are talented, so I hope for the best of you :)
Yes I agree, standard for the company is subjective of course but if it requires more than couple hours of your time it should not be part of the interview/application process. They could just ask you for any piece of code and rather talk to you about it. I feel they don't value your/applicants time. They rather look at the code without consultation. Sad. But you did great job there, I hope you get it. And they better pay well even interns.
Weird, I would not consider these solutions production ready. And they should not be, also because they are your intellectual property at the moment since you were in no legal relationship yet.
Good article, but I wonder If we see random ballasts showing up in repos just because of it. It's a no no.
Get and select both wrap structscan. I haven‚Äôt found a case I‚Äôve needed the scan without her or select.
Hm, I don‚Äôt know if I agree with other posters. This is a pretty good fun problem to finish in a day. If this were to happen in my group, and given your source code, I would vote for a strong yes.
Because the code is on your personal github, I‚Äôm guessing you‚Äôre not working for any of the big tech companies. At those companies, intern projects usually end up being the creation of a tool which solves a problem for thousands of internal engineers. That lands you a permanent spot in the company, if your code quality, operations etiquette, and timeliness is up to snuff with other developers and engineers. I don‚Äôt know how long you had to work on your project, or how long it took you, or what the expectation was that they set with you (and you-them), but it seems reasonable.
&gt; I get that scanning rows one at a time into structs can be a pain I don't even mind that part, it is the NULL handling that is most annoying. Now you either have to accept all of your struct fields being pointers, wrapped types (√† la sql.NullX), or creating a duplicate struct with those fields in order to copy them into the original struct. Sure, you can carefully add NOT NULL constraints to your columns and not worry about the above, but as soon as you want to combine structs with a LEFT JOIN... Boom. Your application blows up whenever the right hand side is not present. For a language that promotes meaningful default values, just give me the default value if the type is not nil-able. Hell, even give me a warning about it if you want, but don't stop executing the Scan because of it. Most of the time I don't care. And when I do care I will explicitly accept the tradeoffs of the above. But maybe I'm missing something obvious.
That is the exact opposite of what I said. My point is that you should hire the best people for the job, if those people are all men then that is fine, if those people are all women then that's fine too.
not sure why this is so heavily downvoted. i give up on reddit.
So who are those gatekeepers exactly? Are you saying that Google is sexist and that they only hire men? That is a bold claim that needs some evidence. &amp;#x200B; Then please explain to me why **THIS** subreddit has a ratio of **\~96%** male and **4%** [female](http://bburky.com/subredditgenderratios/) , who exactly is the gatekeeper here? &amp;#x200B; And what about the Go slack channel? We are almost 40k people on there and have one of the most welcoming communities that I have been a part of, and yet I rarely ever see women on there. I bet that we would have a similar ratio on slack too. And what about university? In my country we don't have to pay ungodly amounts of money for education and can pick freely, and yet 90% of the students in my computer science classes were men. Explain to me who the gatekeepers are for this?
Please explain why we have a 96% [male](http://bburky.com/subredditgenderratios/) ratio in this subreddit? Please explain to me why 90% of the students in my computer science course are male, please explain to me why probably over 90% of go slack users are male? &amp;#x200B; Who exactly is stopping women from joining all these communities?
You could use an Any ([https://godoc.org/github.com/golang/protobuf/ptypes/any](https://godoc.org/github.com/golang/protobuf/ptypes/any)), e.g. message Response { google.protobuf.Any user_or_order_or_product = 1; } Or you could create a message with all three and only populate the one you want: message Response { User user = 1; Order order = 2; Product product = 3; } The latter is less flexible but offers better type safety.
TIL there used to be a spacewar game inside Go's standard library: https://www.youtube.com/watch?v=3yghHvvZQmA&amp;t=500 (this is an interesting part which talks about Go team wanting to make the standard library leaner)
As others have said, I think this is nonsense. But even if the developers specifically had pair programming in mind, I cannot think of how you actually would "design a language around pair programming". I have a really hard time to figure out what "designing a language around some abstract non-technical concept" would look like. If there are examples for this, I would love to learn about it though :-)
I think it depends on the wording of the test and the position the candidate applies to. Maybe they wanted to know if u/ramantehlan knows rsync and he recreated it instead?
I recently learned this is vast right wing conspiracy or some such.
You could also use `oneof` in the latter case: ``` message Response { oneof Resource { User user = 1; Order order = 2; Product product = 3; } } ``` They aren't the most elegant thing to deal with in Go, but it does communicate intent clearly to the caller, and the runtime enforces that you can only set one at a time.
Sorry to disappoint your fantasy but I am European and very left leaning. But my political views don't really have anything to do with the arguments i presented, can you argue the points I made instead of trying to frame me as some right wing conspiracy theorist?
I think the more important part of that quote is "accept interfaces". This makes it possible to use something different than what the author anticipated. I don't think it's that important to return interfaces or structs. It might be easier to read when you see the return type is an interface and there is more than one implementation of that interface, but I generally I don't think there is an important reason to choose one over the other.
My first thought was this--if they are looking to test you, it's a very involved test. If they are looking to steal your work, why not use rsync? Either way, it's not a great look.
thanks a lot for the great article üëç
Those were generally used in all the breakout rooms for Next.
It is a joke. Considering people always like explanation that somebody stopped them from doing things rather than they couldn't or didn't want to do.
Yup this has happened to me. They were using a self hosted php solution for their data scraping and asked me how I would deploy it on aws. At first I answered vaguely but after they pressed I answered in detail and then never heard from them again.
This is one way to validate the designs made by their rather inexperienced teams. In my case I created a design document and implemented it and put it on github as a project. The HR immediately called me and asked to remove the project from github and send it using over file transfer. I got to know that then itself, but I wanted to verify if such companies actually exists. So I went there for the face to face discussion.
Just unset/don't set the password field.
But the empty password field will be visible in the Response, isn't it?
Personally I think it's absurd that this is for an internship, which is yet again some sort of pre-stage for a "real" job. In the end you're probably doing all the regular application/interview stuff, then a demo project, then an internship and then you might probably get a job? When I think back, close to 20 years ago I sent the application, had 2 hours of talking with some people from the company and then you got the job offer (after all, they can still easily kick you out in the first month). With every year it seems the effort for getting even the most boring Codemonkey jobs is increasing (not meaning yours, but I've seen crazy interview procedures for real crap jobs..). Except if you can bypass that, or course..
Not if its unset but it will always be in the proto definition.
 type cx = context.Context func handleIsh(ctx cx, arg int, etc string) {} [https://play.golang.org/p/jniqIrg13b7](https://play.golang.org/p/jniqIrg13b7)
Why not a simple bash script with some help of rsync? Takes about an hour instead of a week and is the better solution?
I thought go was designed to compile fast and work in multi core env very well. And that was my idea going into go and why I preferred it over Rust. I still think go is super good for those reasons but now I let my self explore Rust also. And as long as binary is smaller than 256 mb I can live with it. For me as far as I learned long time ago at uni about compilers most of the time you will want to generate the dumbest code upfront and than have pass over pass adding layers of optimization. This is how we built compilers but uni projects are not often as advanced as let‚Äôs say production scale languages. In general I was always happy with how fast go compiles compared to gcc/g++ . For me the speed up compared to equally feature complex solution was mind blowing. If they can make it fast :) that is just a plus.
IMHO it is not fun thing to ask an intern to do. I wouldn‚Äôt ask person looking for full time job to do this.
I suppose that is quite obnoxious. It would be nice if you could perform some kind of logic to make those decisions if, say, `rows.Scan()` returned an error. Like if the driver would automatically assign to an intermediate struct of NullXs and you could check them before assigning to your struct.
Authors of go? Where is Rob Pike? Where is Ken Thompson? At least Robert is there.
[removed]
[removed]
I don't know, I would think the minimum for an internship position is building a working operating system.
net/http already does this for you. https://golang.org/pkg/net/http/#Server.ErrorLog
No, I don't think there is a way to do exactly what you're asking. If you want to use the same User type, the closest you'll get is an empty password field. I wouldn't worry so much about slightly redundant protobuf definitions though. Ideally a wrapper for the client side will help you keep things clean.
Look at bug which resulted in these releases, maybe?
There are different definitions of "author". Also see https://golang.org/AUTHORS.
If you log the error from http.NewRequest (or return) instead of swallowing it, does it help?
Are you closing your response/request body?
Nope, same. Forgot to handle that as I was using client.Get directly and changed it to help. Thanks for the idea though.
Yes, it's being closed. I am forcing timeouts purposefully and *http.Response is nil in that case so no need to close. But thanks for the idea.
Have you confirmed that the memory increase is due to the client and not your test? Try using the real http server, rather than the test server
&gt; Have you confirmed that the memory increase is due to the client and not your test? &gt; &gt; Yes, I did. I did that minimal-repro in memory, but I was using the same code but on a webserver being served in localhost.
This is nuts. As an offtopic, why ‚Äúmateix‚Äù? Are you from Catalunya? Because if so, I‚Äôm from Mallorca and in Spain they usually tend to abuse people and ask for impossibles...
It is exhausting being such a severe gender minority. If you‚Äôre male, for example, join a community that is 90%+ female and report back in five or six years.
I don't get this argument at all. Please, explain to me how gender plays any part at all in our subreddit? Seriously, look at the top 20 submissions right now and tell me what they have to do with gender? It's just people talking about go. Please explain to me the connection with gender and why women can't participate in this community? If you are a woman then how can you even tell the gender of the people posting here? I certainly can't and I don't care about it either.
You shouldn't be storing passwords. Hash and salt that mofo
I‚Äôm available to give 90 minute talks about my 1-commit contribution to the docs.
Being a women in the programming community is often hard - being a minority in any community is tiring. For example, in your last reply, you are ask for a woman to step forward and explain something. That‚Äôs a common problem - putting the burden of explanation on the minority. If you‚Äôve never been in a gender minority - try it. I‚Äôm unsure how to read the tone in your replies - but we need to have empathy for community members in the minority. It‚Äôs the responsibility of the majority to make the minority welcome. That requires asking ourselves why the status quo is imbalanced without blaming others for that outcome.
What do you mean exactly by "it errors out"? There are many places at which an error may occur. And, you do not close your response bodies if any error occurs reading from one. Typically, unless you're doing something very different, the pattern you should follow with the net/http Responses is, resp, err := client.Do(...) if err != nil { ... } defer resp.Body.Close() ...
So picking people for a panel has to be discriminatory because the industry has historically been discriminatory? The fuck kind of logic is that?
I take it they did not offer payment for this work. I would have. We do simple problems in our interview process for full time software engineers but we design the problem to be doable in about 1 hour. Something like this is over the top unless they are paying you for it on a trial basis. My first job I got through an interview process that included an assignment to complete and turn on the next Monday. He offered me $250.00. this was in 1988. The program was supposed to load a file and convert it to their format. I completed the work and brought in the code. We tried a few different files and had one fail. The test file he gave me didn't have this particular situation in it. So I loaded the code into one of his computers and fixed it in 15 minutes. I got the job and the code I wrote became the core of our ingestion suite.
Do you have any evidence of the "industry" being discriminatory? How do you explain this subreddit being 96% male? Are we all suppressing women on here too? Or how do you explain that? You have a male ratio of 96% males on here and you really wonder why there aren't as many women on the panel? How about that logic?
I know about rsync and unison, I could totally use them, but that won't give the other person any real judgement of my skills, also will make the question way to easy. :)
I have spent time reading about the company, and I don't get the vibe that they will steal it (I have used a GPL V3 License), but yeah I did felt that challenge was way more intense for an internship position.
By erroring out I mean the connection timed out, for instance. By looking at the memory rate, it is clear to me that there is something missing in my implementation while using default http client. Of course this code is coming out of a bigger place where I had the same implementation (with rate limiting, reading all response body &amp; more) but it has the same issues: when there is an error (timeout), it doesn't deallocate memory for some very odd reason. Thanks anyway.
I did think of that, but I know of other people who applied, used rsync and did a pretty good job in less time, but they didn't get selected. So, I had to create it, with rethinking every step, which is why I feel the standard even for an internship is a way to high. :)
Yes, I think an ideal task should be something which can be completed in a few hours, that should enough to judge anyone on their skills. :)
See, the problem is that you already presuppose that there is a problem with a male dominated community, why would it be a problem even if 100% of all users here were men? Or why would it be a problem if 100% were female? I don't see a problem with that because I think gender is irrelevant. Saying that we need to make the minority feel "welcome" sounds completely absurd to me, this is a go subreddit, we just talk about go, this is not a gender subreddit. Again, we are talking about go code, how can that make a gender feel unwelcome? Your argument is completely beyond me, honestly. I've never seen anyone say that women aren't welcome here, ever. Do you have any evidence for your assertion that this community doesn't Welcome minorities? It's actually pretty disgusting to make such a claim that we don't welcome minorities.
So women just naturally don't want to code?
No, I am not from Catalunya. I like to highlight other cultures, and the way I do so is, I name my projects using words from different cultures and languages. Also, I had no idea about the abuse.
Yes women are generally more interested in people and men are more interested in things. Why do you think we also have more men in coal mining jobs? Do you think women just naturally don't want to work in coal mines? Shocking right? We also have more female nurses, teachers and health service providers. Those are very female dominated up to 90%.
&gt;And, you do not close your response bodies if any error occurs reading from one. &amp;#x200B; He does, but its subtle and unidiomatic, the body of the post copy error condition does not return, instead it continues. _, err = io.Copy(ioutil.Discard, res.Body) if err != nil { log.Printf("could not discard body: [%s]\n", err) //NO RETURN HERE } err = res.Body.Close() if err != nil { log.Printf("could not close body: [%s]\n", err) }
Have you tried using the [go tool pprof](https://golang.org/pkg/runtime/pprof/) (the go profiler), and if that fails looking at [/proc/maps](http://man7.org/linux/man-pages/man5/proc.5.html)?
I agree the task should take only a few hours, and if it does take more then that, the person should be compensated (I wasn't). Also, the story of your first job is interesting, I am glad everything worked out. :)
Ugh, sorry about that! Added error handling in that scenario correctly but I did not notice it would be hard to see that. Thanks for the comment anyway.
That's super cool. I keep wanting to do something similar at Mattel, but our APIs are "customer" facing (i.e. people in other teams at Mattel), and the APIs need to be lot more high level than the database schema. &amp;#x200B; If you'd ever like to put up your templates as an example repo under [githiub.com/gnormal](https://githiub.com/gnormal) \- please let me know, I'd love more examples for people to chew on.
yes, but only if you pay like a million dollars. :)
Just a small thing. Why do you want to only expose a single function?
Brad Fitz isn't in that list, he works at Google, so that doesn't apply here.
good luck
My bad. I should be more descriptive and crisp when it comes to titles! Thanks for your feedback though and glad you loved the project!
Solved! See if it's happening again?
Solved.
Are you using transport timeout or context?
youd be surprised the amount of morons who dont know anything
if they arent giving you an already established codebase to work in, its a scam
https://blog.koundinya.xyz/series/gomobile/ Hello, it appears you tried to put a link in a title, since most users cant click these I have placed it here for you ^I ^am ^a ^bot ^if ^you ^have ^any ^suggestions ^dm ^me
A language designed around pair programming is a language that attempts to reduce friction between the pair. This means a language that tries to maximize readability so that the navigator is able to easily comprehend what is being written by the driver, and a language that attempts to minimize the number of ways it can be written to reduce style debates between the navigator and the driver. Pair programming can be done in any language, but there are some languages that have made choices (either explicitly or accidentally) that make it more pleasant.
A language designed around pair programming is a language that attempts to reduce friction between the pair. To free them from mundane concerns so that they can focus on the real issues. A language designed around pair programming might consider things like maximizing readability, so that the navigator can easily comprehend what the driver is writing, and minimizing the number of ways that code can be written so that there is little need for style arguments. Pair programming can be done in any language, but some languages make it more pleasant experience than others. Whether or not Go was designed around pair programming or not, it is easy to imagine how a language could be.
&gt; It is kind of happening now a days, one way to get work done for free. I see this posted on reddit a lot and I can safely say that I don't believe this ever happens. Code you get back from interviews and coding challenges is usually of a pretty poor standard, and often completely unusable in production without significant work. The requirements and scope is too narrow, you would save almost no time in doing this.
Lmao that dude at [7:30](https://youtu.be/3yghHvvZQmA?t=450) who answers a panel question... we're not here to listen to your answers lol.
I do presuppose that 100% of all of this subreddits users being men would be a problem.
Thank you
Ah ok seems relatable, then best of luck with it ;)
\`newMsg.Interface()\` is a pointer to some type. That pointer type satisfies the \`proto.Message\` interface.
Good writeup. Would love to read more, specially about building a whole application using gomobile.
I wouldn't set the password in the gRPC type and offer \`setUserPassword(User, Password)\` and \`checkPassword(User, givenPassword)\`.
When your CPU has `n` cores, and your code spawns `n` goroutines that operate on unrelated data, you should see a good speed-up over the serial version of your code, right? Well, not necessarily. In some regards, CPU's have only a coarse-grained view on data and will sometimes move copies of that data between the cores even if the code would not require this. Luckily, there is an easy solution available. This is a new blog post after a long time of silence. It is not that I lost interest; rather, it is simply lack of time. Besides writing the code and the article, i spend considerable time for creating the animations that help explaining things in almost every of my articles. For this reason, I am thinking about writing more articles without interactive animations, which surely is better than writing nothing at all.
Good read. Thanks
Years ago I worked on a project where we parsed NASDAQ data into our own price database. We started from scratch, with a project lead that wanted ‚Äúas much concurrency as possible, because latency is important‚Äù. Latency _is_ important when parsing NASDAQ data, especially if you‚Äôre an algo trader. Our software was a basic ‚Äú15 minutes delayed‚Äù, so we‚Äôd probably survive a few ms delays. Anyway, We wrote the software in C++. I wanted Java or .NET, but agin, performance reasons from project lead, and we used atomic operations everywhere, along with busy waiting, spinlocks, etc. For messaging we used [IO completion ports](https://docs.microsoft.com/en-us/windows/desktop/FileIO/i-o-completion-ports). All data was kept in ram. Everything was written to run without any slowdowns. Fast forward 9 months and it‚Äôs time to put the thing in production on a 64 core, 512GB ram box. We hit the switch and watched the software crawl itself to death across all cores, with 90% system cpu load. At the time dual core processors were the norm for our developer machines, and our test box had 8 cores, so we‚Äôd never seen the software run at full scale before. What was happening was that the completion ports used for messaging would run in user space while busy, but as soon as it was idle, it would depend on the kernel to wake it up, causing a context switch. Despite having several hundred threads, the software couldn‚Äôt keep 64 cores busy, or even 32 cores, and it would end up drowning in context switches. I wrote a ‚Äúthreadpool‚Äù for the software, scaling the number of threads dynamically, depending on load, and it ran much better. IIRC it mostly runs on 4-6 cores these days, with the occasional spike to 8 cores.
the trick is, that when you are invited to an internship, you're somehow suppose to know something about the task you are going to do. The task they gave you as too much for you and they expected too much from you. If you are working there to gain knowledge, it's okay to give you this, but to made this as kind of a buy-in, I find this really too much. On the other end, you had a purpose in solving particular problems with Go, which I think have brought you some knowledge along the way too. :-)
You're arguing that gender distributions in different fields are purely due to biological traits and somehow "natural"
If I read the words "Lightning fast" and click on a few links on the website, I think about how "fast" the application will be. Perhaps it is, perhaps it isn't. But the website isn't for sure. I wonder what technique is used that makes it so slow and then begin thinking: if they can't make this fast, why would the other part be fast. Please make the front lightning fast so people will believe those marketing terms better.
I'd love to have a microservice example where there is an auth/user service, frontend service and backend service. That's on my todo.
This is about scanning al tags in an html document. it scans and when it finds a tack, it puts it on a stack. Then if looks for children of this tag, which is the recursive part, because those children can has children too, and so on and so on. In my training, I use the example of a chessboard, where the inventor of the game answers the king for which he made it : "Oh emperor, my wish is simple. Give me one grain of rice for the first square of the chessboard, two grains for the next square, four for the next, eight for the next and so on for all 64 squares, with each square having double the number of grains as the square before." This case can easily be done via a recursive function. Perhaps you unerstand this one better: ``` gold := chessBoard(0, 0) println("gold:", gold) // chessBoard is a recursive function demo func chessBoard(square int, gold float64) float64 { square++ if square &gt; 64 { return gold } gold += math.Pow(2, float64(square-1)) return chessBoard(square, gold) } ```
In my case, I don't think it's a scam, nor I feel they will steal my code or something like that. Undoubtedly, I loved making it, just I felt that it was way too much of coding for getting an internship, I mean I had to invest a lot of time in it, and I am still not sure if I will get it, but I am trying to be positive. :)
The interview procedure is definitely getting more tough even for tedious jobs. IMHO, I see two reasons behind that, one is the increase in the number of engineers, other is increased in the ambition of startups. :)
I agree, and It is justified to expect a candidate to know at least something about the tasks the company do. I just think the company should be able to judge that in a much better way and in less time. &amp;#x200B; Yes, I totally enjoyed working on it, It was unlike any other project I have ever created, that is the biggest take away from it.
There was an interesting talk by Rob Pike about writing a compiler (parser) in Go ,and how goroutines are great for that. He basically goes step by step and builds up a parser using goroutines. At the end he says that yeah, but goroutines are slow for this kind of work and one should rewrite it. But goroutines made you think differently about the problem, you'd came up with a clean and interesting solution, and it's easy to get rid of them.
This is awesome. Thanks for the hard work. Will definitely try some of this out
/u/ENx5vP why did you delete the image you posted?
No positive feedback. Thought I missed the point ;)
Why? You still haven't given me an answer. Do you also think it's a problem that almost 100% of coal miners are male?
Not a go expert by any means but as I was reading the first concurrent example I was confused by the use of a shared unprotected array. I know each go routine got a different index but the array as a whole wasn't locked, so in a larger code base issues will happen, and the mutex locked array variant would surely perform worse. I almost stopped reading because the example, though valid, was not correct. After reading the whole thing I realize that you were demonstrating that conccurency is a hammer and when applied incorrectly does more harm then good. I'd either update the intro to better explain the use of an incorrect example or put a note close to the example like you did with CPU cache. Otherwise a good read.
I would do the following: 1. hire you for 1-2 weeks, to work on a real feature (not a theoretical, throw-away one), 2. pay you for your time, even if it doesn't land you a job, 3. add some mentoring during the way, so you're learning too For a free part of the interview process, this is too much. For a "realistic" feature one might be expected to implement, the process, I think, it is better than most. You should have had some mentoring/consultation along the way, not just for the final review. If I'm honest, I'm biased to judging you on my standards, but I haven't explained my standards to you, so it's not even your problem that would penalize you. Trying to keep this bias out of the interview process isn't trivial, because you want to enforce it somewhat. The standard may be high, but the review process, from what you delievered, is very hands-off, and that's a problem.
I didn't say that gender distribution differences are purely based on biological traits, I think that this is one big factor, but I don't think it is the only one. &amp;#x200B; I am however calling out the BS that our community would be "oppressive" or that we just wouldn't let women "in". &amp;#x200B; If you don't think it is biological, then how do you explain the "gender equality paradox" where "countries such as Albania and Algeria have a greater percentage of women amongst their STEM graduates than countries lauded for their high levels of gender equality, such as Finland, Norway and Sweden." &amp;#x200B; Or do you now also claim that Finland, Norway and Sweden are more oppressive than countries such as Albania and Algeria?
&gt; Mateix means 'same' in the Catalan language, which is a language of Catalonia community of Spain. Oh mec, excellent Un Gopher dans le m√™me coin que moi Par curiosit√©, c'√©tait quoi la boite?
At first glance your snippet of code seems to be right. You can check the error from the decode and encode method, there should be an error somewhere. Honestly with the code you posted it's hard to help as you don't provide enough to help you find the error. How do you initialize/declare MyStruct. Does it contain a field not supported by gob?
Note that the language is named "Go", not "Golang", "GOLANG", "GoLang", or any other variant thereof.
maybe the point was not to reinvent but use available libs or tools to achieve the goal :P
I now refuse to do any unpaid projects that take more than an hour, especially if it's before I even had a screening interview. At the very least I would expect to be paid for half the time. It's not like I mind spending *some* effort in "proving" that I can program, but the last company expected me to program an entire database engine, and all I got as feedback was "we reviewed it, and decided to not continue". No idea why, as the code worked fine (as far as I know). It took me about 6 hours in total to get it to work well ‚Äì essentially a full working day ‚Äì and that's all I get? :-/ And how many of these tests are you supposed to do? Apply at 10 companies, spend 60 hours writing bullshit programs? Yeah nah, I'm an adult, I got stuff to do like. Funny enough, I also applied to a different position in the same company, and that team sent me a reasonable 30-minute code test, which is a fair and reasonable amount of time to spend pre-interview (not 6 hours).
Great work and they should hire you. Are they planning on creating a dropbox clone by any chance ;) ?
Thanks for that
I'm not, but you can easily figure that out in less than 1 hour. Actually, less than 5 minutes most of the time.
I have mentioned the format of struct in the edit. It is fixed now. The issues was solved by capitalising the name of the struct and its members.
PostgreSQL, not PostgresSQL.
Perhaps, I was asked once to find some lines matching certain regex in the log files, answered with `grep` and that was good enough.
that would be awesome if you find any problem, fork, fix, make a PR...
You don't actually need to synchronize accesses to the array if multiple threads will never read and write to the same index or if no threads are writing.
Could not have said better. The initial concurrent example is so not idiomatic, I was flabbergasted. I mean, you want to show what to do to improve, but it turns out it's just writing more idiomatic code.
H√© mec! Je ne viens pas de la communaut√© catalane ni de l'Espagne. Je nomme mes projets d‚Äôautres cultures et langues pour les mettre en valeur. De plus, qu'entendez-vous par "quelle est la bo√Æte?" PS: J'ai utilis√© Google Translate.
I am pretty sure they have no plans to create another dropbox! Also, Thank you :)
The go community sets a standard for itself to be welcoming. One way to judge ourselves against that standard is by the resulting diversity. https://golang.org/conduct.
I had an estimated 3h task with 3 day deadline to code a full dns proxy which was easily done with a used of proper libraries.
Ow, okay, sorry, neverming Since the Catalonia territory is bordering France, well you can guess a lot of Catalans lives here in the south-east of France So, this Catalan naming along with your other project named _"Moi"_, the French word for _me_... I was convinced you were a French guy living near the Spanish border Sorry for the misunderstanding The word "bo√Æte" literally means "box" But it's also a familiar way of saying "a company"
Normally you'd expect it to be under an org...like `github.com/smalltime/go` If this is a specialty you want to get traction and want example libs....I'd definitely create a new GitHub org for it.
A lot of Go libraries have a `go-` prefix. Many Go tools (like goimports) also account for this convention, so if you called the repository `go-smalltime` and the package name in that library just `smalltime`, then it'd work fine, and you wouldn't get things like unnecessary import aliases, etc.
Even with an org on GitHub, I'd say you still should use `go-smalltime` as explained by /u/SeerUD
Capitalizing the member names should be enough.. name of the struct is not important for gob or any encoding libraries in golang.
- I would recommend reordering things by importance.. top of the file would be imports, then main function then Pokemon type and it's methods.. then all the helper functions.. this is a good practice in general not just in golang.. - checkAttributes and getAttributeValue functions take a pointer to Pokemon type. A value type would be more appropriate since these functions dont mutate the Pokemon value passed. - avoid mixing usage of fmt.Println and println
You could set up a vanity url. Then, your GitHub repo names could be whatever you want, and the impot names could be as simple as "kstenerud.io/smalltime"
Nice! Just beware there is a typo in the "The cache line" section: "To synchronize cache and main memory in an efficient way, data is synchronized in blocks of typically 64 ~~bit~~ bytes. These blocks are called cache lines." Later on you have it right. :-)
I really like this model. Does anything actually currently work this way?
A couple days ago there was a post called "[Word Counter algorithm on Go with sync and concurrent variation](https://www.reddit.com/r/golang/comments/bbq069/word_counter_algorithm_on_go_with_sync_and/)" There were two examples: using a single-thread, and multiple thread version. I told them [a single CPU core could count bytes faster than any I/O device should be able to stream data](https://www.reddit.com/r/golang/comments/bbq069/word_counter_algorithm_on_go_with_sync_and/ekkv9gy?utm_source=share&amp;utm_medium=web2x) and went ahead and forked the code to demonstrate. My code focused more on [reducing the time between completing the I/O tasks and processing the data](https://github.com/pmorelli92/go-word-counter/pull/1) resulting in a 10x speedup from the concurrent code. Is there a place for concurrency? Maybe. Perhaps you have the file in the disk cache and you can open multiple pointers to different sections of it for faster reads. However, the point was the just adding a bunch of concurrency - without thinking about the actual problems was a waste.
The members of `Items` must start with an uppercase letter (just the variable names, the json/bson tags are fine) otherwise the deserializer can't access them.
Thanks I'll give it a try!
It worked thanks very much! One more thing: the DeleteMany() method returns an error saying "document is nil", what could it mean?
I think removing some.concerns like code style definitely makes the language better for pair programming or just code reviewing in general. In a PHP app at work, an inordinate amount of time is spent enforcing code style guidelines. Go makes that unnecessary.
[removed]
Well, resources must continue to call it "golang" so people can actually search and find these projects. I mean, even the project's _own website_ is "golang.org". The language name, and the way it's _actually written_ are required to be oddly different for Go than most other languages.
What happens if the creation of the same username is at the same time?
So I made a struct, which works fine. My issue now is that when I put the for loop in a goroutine, the program exits after I run it. I'm assuming I need some sort of wait or use of channels? The reason i'm using a goroutine is because i'm eventually going to listen on 2 websockets, (schannels and fchannels in the code). Here's my updated code: &amp;#x200B; &amp;#x200B; &amp;#x200B; package main import ( "encoding/json" "fmt" websocket "github.com/gorilla/websocket" ) // CData as data from c websocket type CData struct { Ev string `json:"ev,omitempty"` // event type X int `json:"x,omitempty"` // exchange id Sym string `json:"sym,omitempty"` // symbol P float64 `json:"p,omitempty"` // price S int `json:"s,omitempty"` // size C []int `json:"c,omitempty"` // trade conditions T int `json:"t,omitempty"` // timestamp (unix) } const apikey = "" const schannels = "T.SPY" const fchannels = "C.USD/JPY" func main() { c, _, err := websocket.DefaultDialer.Dial("wss://socket.polygon.io/stocks", nil) if err != nil { panic(err) } f, _, err := websocket.DefaultDialer.Dial("wss://socket.polygon.io/forex", nil) if err != nil { panic(err) } defer c.Close() defer f.Close() _ = c.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"auth\",\"params\":\"%s\"}", apikey))) _ = c.WriteMessage(websocket.TextMessage, []byte(fmt.Sprintf("{\"action\":\"subscribe\",\"params\":\"%s\"}", schannels))) var smsgs []map[string]interface{} go func() { for { err := c.ReadJSON(&amp;smsgs) if err != nil { panic(err) } for _, val := range smsgs { var d CData data, _ := json.Marshal(val) _ = json.Unmarshal(data, &amp;d) dstring := fmt.Sprintf("%s %d", d.Sym, d.T) fmt.Println(dstring) } } }() }
23: Attributes should be a map with Attributes as a map, getAttributeValue and checkAttributes are redundant and you should use Attributes directly 48: setting the seed should only happen once. move it to an init() function 53: either shorter var name like h, or don't bother with the intermediate variable 54: += 54: you'd probably want a MaxHp attribute to prevent the health from continuously increasing 57: you probably want to make an Attack struct 58: shorter variable name like a or ap 77: if you want to give something a X% chance of happening, a better way to right it is to have a function take in X/100 and then returns if a random float in the range of 0 to 1 is less than it 88: should it be -=, or is a higher defense supposed to take more damage? 91: -= 98: just access Health directly 104: makeAttributes looks kind of convoluted, what are you trying to do here? 126: use more concise variable names. If there's only 150 names, which are all fixed , just get all of them and throw them into an array in another file in the same package so you don't have to do io 138: check to see if the unmarshaling returned an error 145: shorter variable names 157: use Printf 166: have printStatus also take printAttributes so that you can use it for 194 180: give the attribute names as well 223: move i to the for loop 225: battle is redundant since you're breaking in the loop anyways, get rid of it 227: those turn it is should be local to the main method since it's not needed anywhere else 227: break this out into a separate method 253: break this out into a separate method 279: move to loop
Use a oneof in the response proto
You should always have a bound on concurrency or use worker pools/thread pools. I learned that usually it is a good start of fixing bottlenecks and speed issues with lowering concurrency. This is significant with relational databases like Postgres or Mysql. Now i'm not expert on low latency trading but for me as of now you should always focus on bound thread pools/worker pool/goroutine pools and queues. (ofc Destructured data in DB, caching and step computation of end results etc... )
I'm not sure if this n cores to n goroutines is correct. Imho you could have 4 core machine with app having spawned 640 goroutines all the time and working fine. The problem starts if you have unbound concurrency like you spawn new goroutine for each request or TCP connection someone makes and they just open 70000-250000+ connections/spawn goroutins and scheduler spends day and night in swapping mode. In Rust with native threads this n to n kinda makes much more sense to me. I used to write a lot of Erlang and we have still running 2 apps in Erlang. To not lie we have around 500 processes (erlang process \~ goroutine) per erlang beam/node. And do have not one node but around \~120 on many servers. Usually more than 1 per physical server. With each box having 12 cores. &amp;#x200B; Ps: Usually most of your goroutines will wait for something. Ps2: I checked amount of spawned processes with `length(erlang:processes()).` Just to be clear :)
[removed]
These checks should be at the service layer interacting with the database, not at a layer where to perform these checks they have to make http requests.
The purpose of Design by Contract tests is to verify the API meets a precise specification, not to replace the validation done by the application. Similarly, it does not replace software testing (but compliments it). Ordinarily, Design by Contract assertions are disabled in a production environment. This does allow them to be disabled in production then submitted to Sentry instead; for performance reasons, they can be reduced to a randomly select % of requests.
That's something which should be handled by the application layer - Design by Contract isn't there to replace application-level verification. Contracts can be written in such a way that it is not a problem, i.e. the check resource does not exist -&gt; create resource -&gt; check resource exists approach can work regardless.
Technically correct, which is the worst kind of correct. But just because it works doesn't mean you should do it. If the structures are smaller than a CPU cache line, then the data will "ping-pong" back and forth between your CPU cores. As [this article](http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206) shows, this effect can be so bad that a **dual**\-core box crawls to half the speed of a **single**\-core box. &amp;#x200B; If you want to learn *far*\-to-much about memory, read the series "What every programmers should know about memory" on [LWN](https://lwn.net/Articles/250967/). &amp;#x200B; The upshot is: Always benchmark your algorithms, because your mental model isn't always [correct](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/).
A good gotcha that I think that should be mentioned is that if any of your exported functions have unsupported types the function will be skipped without warning
I do not really see how we benefit from the made points. A fast compiler is indeed very helpful if you want to quickly iterate while making subtle changes to a program. Of course, one can argue about what is an acceptable amount of time to compile a single file. From what I know, most compilers are done in seconds. That is what you want, right? Additionally you claim that compilers should run slower to produce faster programs, e.g. in the case of Chromium. As a consequence the compile time increases significantly taking 10s of minutes or hours. To understand most of the problems, you have to view two things separately. 1) Compiling a file. 2) Compiling a project. We already saw that most compilers are very fast and interactive for a single file. But why are they slow for a huge project? There are multiple files involved indeed, but compilers such as the Go compiler or build systems such as bazel made significant improvements. You have a one time compile cost which can be the 10s of minutes or hours. Then, everything is cached and only changed parts are rebuilt. This keeps everything interactive. And, as a side note: Chromium currently is slow because the migration to bazel is a huge task and Google did not have a suitable opensource build system at that time. Expect the compile time to be in the range of seconds once the migration is complete.
Shared private arrays such as that are a common method for splitting async work. As long as it‚Äôs all short lived and local in a function, locking it is sometimes more overhead than is required.
Why are you storing every iteration into each goroutine? ``` start := (limit / n) * i end := start + (limit / n) ``` Please don't do that. You are doubling your memory every run
That was probably the talk he gave about `lex.go`: https://www.youtube.com/watch?v=HxaD_trXwRE It's still based on his code AFAIK.
That's the one I think, thanks.
&gt; Well, resources must continue to call it "golang" so people can actually search and find these projects. Not really. Google is aware of the connection (i.e. if you search "golang" it finds hits for "go" and vice-versa) and for searchability it suffices to *tag* "golang" and still use "go" in prose (it's what I do on my blog).
To mention: This isn't about a roadmap of the developers of Go, but a roadmap to become a developer using Go.
Literally first thing I saw was the lack of capitalization. Go‚Äôs reflect package only sees public variables and not private ones, so you need to make the names public by capitalization.
The application can't handle that in most cases. They'd have to lock the db. In a typical RDBMS like postgres/mysql/etc it's something that should be handled by the DB itself with the application appropriately handling errors from the DB on uniqueness constraints.
A pure golang project by the gophers for the gophers!‚ô•Ô∏èüôà
If you're repeating that same pattern for a bunch of different models, just make a Getter interface that your models comply with. Then have a function which takes in a Getter and returns a handler that contains those steps using the supplied Getter. Now all your steps are only in one place. Also all header setting and outputting in the even steps can probably be a function to make things a _little_ simpler within the handler.
You're right, an interface can definitely fit here, but this pattern would definitely be better for multi step operations that include data validation, authentication etc.
Sure, great. I would just appreciate if I don't see 1-2 posts from you about this every day in this sub.
Thanks for creating Gnorm! And thanks to Mattel for allowing you to open source it - now it isn't just my kids that are playing with Mattel products. I am also using gnorm to do the same thing (SQL -&gt; gRPC -&gt; API). I haven't found a way to reuse the gRPC structs all the way through, so I'm stuck creating redundant structs for SQL binding. Anyone know a solution? I know that there are tools to modify the struct tags in the gRPC output code, but it seems really fragile.
It doesn't really make sense to unmarshal into the `[]map[string]interface{}`, loop over them, marshal them back to json, and then back to a specific type. In your example you already know the types that are going to come back so you might as well use ``` var smsgs []map[string]CData var fmsgs []map[string]FData ``` Then you unmarshal straight into your desired types in the first place. My previous suggestion was to handle not knowing the type and using a "test" struct with only an Ev field to check the type after you unmarshal. Then you can do a full unmarshal again with the correct value type to capture all fields. As for the channel at the end, you are just using that as a way to block forever.
Typical German humour.
Interesting video - I find it useful to get in the minds of the creators to better inform whether I'm in the center of the lane or off on the shoulder. But my biggest question of this entire thing relates to this...WHERE CAN I GET A COUPLE OF THOSE [PLUSHIE GOPHERS](https://imgur.com/a/cg0x7Md)?!!!!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/BRYgVUY.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ekyt8og)
As I said, I'm still learning, so any feedback is greatly appreciated!
An alternative is to define expected input, then let the system decode/validate and/or respond for you. See https://github.com/mustafaakin/gongular for an example of this. (and also write a shared auth middleware). That only leaves you with: 5. make db/cache request
What is the output of the following commands? ls -ltar /Users/ashleyelder/Library/Caches ls -ltar /Users/ashleyelder/Library/Caches/go-build
`n`goroutines are certainly not a strict upper limit for `n` cores, but if each goroutine uses 100% of the core, adding more goroutines yields no further speedup. If your goroutines frequently wait for something (like I/O, scarce resources, etc), spawning hundreds or thousands of goroutines does make sense, as for every goroutine that goes into wait state, another goroutine can start or continue running.
Goroutines for better code design! Makes sense, considering that goroutines provide a much higher level of abstraction compared to locks, mutexes, or semaphores. Write clear code now, optimize later.
Do you still need contribution for this? I
I forgot to mention this is built to work with procfs on linux (for now), so it does some getdents without fstat for performance reasons (similar to tools/internal/fastwalk or godirwalk)
It should be no coincidence that the better code is the more idiomatic one, and vice versa. That's why language idioms emerge.
&gt; so in a larger code base issues will happen To avoid that, you can make the slice visible to the package only. (And keep the package small and focused of course.)
Ouch! And I cannot blame the spell checker. :D Thanks for spotting the typo, I have fixed it.
[removed]
I am afraid I cannot follow your reasoning. This code snippet calculates the chunk of data that the goroutine is going to sum up in the loop that follows. Nothing more. As a result, each of the `n` gorutines sums up a different part of the range of input values.
My pleasure sir :)
Good and important article. I don't understand the downvotes.
Yes, I took those ideas from [go-kit](https://github.com/go-kit/kit) that follows a clean/hex/onion model.
From: https://tools.ietf.org/html/rfc7231#section-6.3.1 &gt; Aside from responses to CONNECT, a 200 response always has a payload, though an origin server MAY generate a payload body of zero length. If no payload is desired, an origin server ought to send 204 (No Content) instead. For CONNECT, no payload is allowed because the successful result is a tunnel, which begins immediately after the 200 response header section. If you're not going to follow the specs for the method then you need to account for that and adjust your front end accordingly.
Are you selling any accessories to add on to the gophers? Love it!
Understood. I was using ‚Äúapplication‚Äù to generally mean the downstream API service (and its database) collectively. My point was that it‚Äôs not the purpose of middleware or a reverse proxy to handle database locks.
We have made a few accessories, but want to find out what people may want to add to their gophers. Ideas are always welcome. Glad you like them.
Yes!
Very nice. Thx
I downloaded directly from [the site](https://golang.org/doc/install?download=go1.12.4.darwin-amd64.pkg). ls -ltar /Users/ashleyelder/Library/Caches total 336 drwxr-xr-x@ 3 ashleyelder staff 96 Sep 14 2018 com.apple.nsurlsessiond drwxr-xr-x 3 ashleyelder staff 96 Sep 14 2018 com.apple.appstore drwxr-xr-x 6 ashleyelder staff 192 Sep 14 2018 com.apple.akd drwxr-xr-x 6 ashleyelder staff 192 Sep 14 2018 com.apple.passd drwxr-xr-x 6 ashleyelder staff 192 Sep 14 2018 com.apple.imfoundation.IMRemoteURLConnectionAgent drwxr-xr-x 5 ashleyelder staff 160 Sep 14 2018 ckkeyrolld drwxr-xr-x@ 3 ashleyelder staff 96 Sep 14 2018 com.apple.Safari.SafeBrowsing drwxr-xr-x 6 ashleyelder staff 192 Sep 14 2018 com.apple.touristd drwxr-xr-x 5 ashleyelder staff 160 Sep 14 2018 storeassetd drwxr-xr-x 5 ashleyelder staff 160 Sep 17 2018 com.apple.QuickLookDaemon drwxr-xr-x 5 ashleyelder staff 160 Sep 17 2018 storedownloadd drwxr-xr-x 6 ashleyelder staff 192 Sep 17 2018 com.apple.nbagent drwxr-xr-x 5 ashleyelder staff 160 Sep 17 2018 com.apple.keyboardservicesd drwxr-xr-x 3 ashleyelder staff 96 Sep 17 2018 com.crashlytics.data drwx------@ 3 ashleyelder staff 96 Sep 18 2018 Google drwxr-xr-x 5 ashleyelder staff 160 Sep 18 2018 com.google.Keystone drwx------ 3 ashleyelder staff 96 Sep 18 2018 com.google.SoftwareUpdate drwxr-xr-x 5 ashleyelder staff 160 Sep 18 2018 com.google.Keystone.Agent drwxr-xr-x 3 ashleyelder staff 96 Sep 19 2018 com.evernote.edam.usage drwxr-xr-x 2 ashleyelder staff 64 Sep 19 2018 com.apple.AssitantServices drwxr-xr-x 4 ashleyelder staff 128 Sep 19 2018 com.apple.DictionaryServices drwxr-xr-x 6 ashleyelder staff 192 Sep 19 2018 com.apple.CommerceKit.TransactionService drwxr-xr-x 5 ashleyelder staff 160 Sep 19 2018 com.apple.helpviewer drwxr-xr-x@ 10 ashleyelder staff 320 Sep 22 2018 com.apple.Safari drwxr-xr-x 8 ashleyelder staff 256 Sep 27 2018 com.apple.Spotlight drwxr-xr-x 7 ashleyelder staff 224 Oct 1 2018 com.apple.finder drwx------@ 3 ashleyelder staff 96 Oct 2 2018 Firefox drwxr-xr-x@ 3 ashleyelder staff 96 Oct 2 2018 Mozilla drwxr-xr-x 5 ashleyelder staff 160 Oct 2 2018 com.tinyspeck.slackmacgap drwxr-xr-x 3 ashleyelder staff 96 Oct 5 2018 XamarinInstaller drwxr-xr-x 5 ashleyelder staff 160 Oct 5 2018 com.xamarin.universalinstaller drwxr-xr-x 9 ashleyelder staff 288 Oct 8 2018 com.evernote.Evernote drwxr-xr-x 5 ashleyelder staff 160 Oct 10 2018 com.evernote.EvernoteHelper drwxr-xr-x 3 ashleyelder staff 96 Oct 13 2018 VisualStudio drwxr-xr-x 5 ashleyelder staff 160 Oct 13 2018 com.microsoft.visual-studio drwxr-xr-x 6 ashleyelder staff 192 Oct 17 14:31 com.oracle.java.Java-Updater drwxr-xr-x 5 ashleyelder staff 160 Oct 19 11:10 com.postmanlabs.mac drwxr-xr-x 8 ashleyelder staff 256 Oct 23 15:34 com.github.atom drwxr-xr-x@ 7 ashleyelder staff 224 Nov 5 14:57 com.google.Chrome drwxr-xr-x 5 ashleyelder staff 160 Nov 24 15:21 com.mongodb.compass drwxr-xr-x 5 ashleyelder staff 160 Nov 27 16:15 com.sqlopsstudio.oss drwxr-xr-x 8 ashleyelder staff 256 Dec 3 09:14 com.googlecode.iterm2 drwxr-xr-x 5 ashleyelder staff 160 Dec 10 20:02 com.apple.SystemProfiler drwxr-xr-x 2 ashleyelder staff 64 Dec 10 21:50 com.mongodb.compass.ShipIt drwxr-xr-x 6 ashleyelder staff 192 Dec 13 08:40 com.apple.installer.osinstallersetupd drwxr-xr-x 5 ashleyelder staff 160 Dec 13 09:07 com.apple.iCloudHelper drwxr-xr-x 4 ashleyelder staff 128 Dec 13 09:07 PassKit drwxr-xr-x 5 ashleyelder staff 160 Dec 13 09:08 com.apple.ap.adprivacyd drwxr-xr-x@ 22 ashleyelder staff 704 Dec 13 09:08 CloudKit drwxr-xr-x 2 ashleyelder staff 64 Dec 14 07:56 MobileActivation drwxr-xr-x 5 root staff 160 Dec 17 08:47 com.xamarin.fontconfig drwxr-xr-x 6 ashleyelder staff 192 Dec 17 11:58 com.torusknot.SourceTreeNotMAS drwxr-xr-x 2 ashleyelder staff 64 Dec 17 13:01 com.apple.AssistantServices drwxr-xr-x 3 ashleyelder staff 96 Dec 18 11:36 Yarn drwxr-xr-x 2 ashleyelder staff 64 Dec 18 11:40 com.microsoft.VSCode.ShipIt drwxr-xr-x 4 ashleyelder staff 128 Dec 18 11:42 typescript drwxr-xr-x 3 ashleyelder staff 96 Dec 26 07:53 com.apple.iBooksX drwxr-xr-x 3 ashleyelder staff 96 Jan 29 09:18 com.apple.XprotectFramework.AnalysisService drwxr-xr-x 5 ashleyelder staff 160 Jan 29 09:18 com.apple.installer drwxr-xr-x 7 ashleyelder staff 224 Jan 29 09:23 de.appsolute.mamppro drwxr-xr-x 5 ashleyelder staff 160 Jan 29 10:15 com.plausiblelabs.crashreporter.data drwxr-xr-x 7 ashleyelder staff 224 Jan 29 10:15 de.appsolute.MAMP drwxr-xr-x 7 ashleyelder staff 224 Jan 29 13:45 com.oracle.workbench.MySQLWorkbench drwxr-xr-x 3 ashleyelder staff 96 Jan 29 14:47 io.fabric.sdk.mac.data drwxr-xr-x 2 ashleyelder staff 64 Feb 16 20:47 com.azuredatastudio.oss.ShipIt drwxr-xr-x 5 ashleyelder staff 160 Feb 16 20:47 com.azuredatastudio.oss drwxr-xr-x 5 ashleyelder staff 160 Feb 16 21:07 com.docker.docker drwxr-xr-x 4 ashleyelder staff 128 Feb 16 22:12 com.apple.iTunes drwxr-xr-x 5 ashleyelder staff 160 Feb 22 12:00 com.sqlopsstudio.oss.ShipIt drwxr-xr-x 5 ashleyelder staff 160 Feb 26 13:35 com.postmanlabs.mac.ShipIt drwxr-xr-x 5 ashleyelder staff 160 Mar 8 13:34 com.tinyspeck.slackmacgap.ShipIt -rw-r--r-- 1 ashleyelder staff 10799 Mar 11 16:39 com.apple.siri.bundleservicecache.plist -rw-r--r-- 1 ashleyelder staff 137216 Mar 13 07:12 com.apple.preferencepanes.searchindexcache -rw-r--r-- 1 ashleyelder staff 1678 Mar 13 07:12 com.apple.preferencepanes.usercache drwxr-xr-x 10 ashleyelder staff 320 Mar 14 08:17 com.apple.appstoreagent drwxr-xr-x 15 ashleyelder staff 480 Mar 21 14:02 Homebrew drwx------@ 66 ashleyelder staff 2112 Mar 21 14:04 .. drwx------ 5 ashleyelder staff 160 Mar 21 14:04 pip drwxr-xr-x 5 ashleyelder staff 160 Mar 29 14:22 com.apple.systempreferences drwxr-xr-x 5 ashleyelder staff 160 Mar 30 18:41 com.spotify.installer drwx------ 8 ashleyelder staff 256 Mar 30 18:42 com.spotify.client drwxr-xr-x 5 ashleyelder staff 160 Apr 12 11:38 com.github.atom.ShipIt drwxr-xr-x 7 ashleyelder staff 224 Apr 14 12:14 com.microsoft.VSCode drwxrwxrwx@ 9 ashleyelder staff 288 Apr 15 11:30 GeoServices drwxr-xr-x 261 root staff 8352 Apr 15 13:53 go-build drwx------ 3 ashleyelder staff 96 Apr 15 15:09 com.apple.cache_delete drwxr-xr-x 67 ashleyelder staff 2144 Apr 15 16:36 com.apple.commerce drwxr-xr-x 12 ashleyelder staff 384 Apr 15 18:24 com.apple.helpd -rw-r--r-- 1 ashleyelder staff 14201 Apr 15 18:34 com.apple.nsservicescache.plist drwx------+ 91 ashleyelder staff 2912 Apr 15 18:34 . drwxr-xr-x@ 26 ashleyelder staff 832 Apr 15 18:34 com.apple.parsecd
&gt;ls -ltar /Users/ashleyelder/Library/Caches/go-build this output is very long...i pasted it [in codepen as a comment,](https://codepen.io/detstammer/pen/ROjQze?editors=1010)
Was surprised go doc -http wasn‚Äôt mentioned. I‚Äôve found it much more useful than using the doc command to print out individual tidbits.
This is the last one this week. Promise!
Thanks buddy!
There are a ton of features yet to be implemented! For starters: - elasticsearch support - redis and a new caching module - integration with other FaaS - the most important one! Sample apps to show what all can we do with it!
&gt; even the project's own website is "golang.org" That's just the domain name; which is clearly not the same as the name of the thing.
[removed]
Why aren't you using 204 No Content?
``` $ go doc -http flag provided but not defined: -http ``` do you mean "godoc -http"?
Does anyone do external testing to get coverage profile? e.g. create a test binary, call APIs using external tool. Stop, and then use the coverage? I did based on various blog posts, stack overflow answers etc. but it only shows me "main" functions coverage, nothing else
The only thing you should see as an error are things that return an error, where you can just return in your handler after writing your response as normal. The only exception to that is if the routine panics. You can recover those with the recover() function, but if you have something panic, you‚Äôve got some more serious issues to deal with first.
&gt; Current openings will be paid through stakes in current projects that are developed with private partners. For instance, project members will get coins according to their role in new projects and rewards once milestone have been achieved So basically you don't actually get paid?
Bah, truthy but non-200 codes make it waaay more complex just to figure out whether a request in fact succeeded or not.
Nah. You get paid in shitcoins. They‚Äôll be HUGE one day, we swear!
Ah probably. Not offline that often so I don‚Äôt use it much but the couple of times I‚Äôve needed it it was a lifesaver.
Thank you :)
A whole application including the UI? I think you might find this interesting - https://github.com/gomatcha/matcha . I haven't tried it though and the last commit seems to be a year back, so I am not sure if it is maintained.
I am trying to learn about right techniques and optimal code. Could you explain why this approach is better? I would assume single sequential read of file to memory and then accessing data from memory should be faster than accessing chunks constantly from slower I/o device. Why is multiple scans from hard drive faster/more optimal than accessing it from ram? Thanks
``` type Resource string func Poller(in, out chan *Resource) { for r := range in { // send the processed Resource to out out &lt;- r } } ``` https://blog.golang.org/share-memory-by-communicating This explains it best. http://www.minaandrawos.com/2015/12/06/concurrency-in-golang/ Basically when you spawn a goroutine you really shouldn't declare things inside of it. Because each goroutine will have to carry this in memory. "Unless" you have a system setup to send data with channels and they are for looped with an iteration, or/and there is a switch statement to determine when the goroutine should be run. Basically anything to minimize running them and minimize storing inside them and please use channels to do communication. https://gobyexample.com/stateful-goroutines
&gt; Basically when you spawn a goroutine you really shouldn't declare things inside of it. Because each goroutine will have to carry this in memory. I see your point. And you are right, this pattern should not be used for the general case. However, you only get a problem when spawning large amounts of goroutines that store a significant amount of data inside. In the given code, the number of goroutines is strictly limited to the the number of available CPU cores, and the additional storage inside sums up to the size of two `int`s per goroutine, which is negligible compared to the 2K to 8K of storage that every goroutine needs anyway.
@8fingerlouie do you think that it would have been easier to write that today if you used Golang along with simpler concurrency primitives such as goroutines?
It's important to think about the larger picture when thinking about concurrency, and how you can bake that into your architecture, so you do not take too narrow of a view, which could result in wasted effort, or worse degraded performance.
I really appreciate the effort.
I don't think the tone of this submission is acceptable. That being said, it would be interesting to know more about the server and the client side here. Because an empty response should be fine. In particular, waiting for a timeout only makes sense if the server doesn't send a correct Content-Length of 0 and/or doesn't close the connection. In particular, with the minimal example below, the behavior isn't what you describe (instead, a parse error is returned). I think while the parse error might not be what you want, it is still correct behavior in this case (as I specifically request the response to be parsed as JSON and an empty string isn't correct JSON) and if I'd want to catch that, I'd not use `getJSON`. Either way, ISTM that your problem is solvable even without serving `{}`. Whether two bytes actually qualifies as "waste" is a different question (I'd decidedly say "it never matters"). package main import ( "io" "net/http" ) func main() { http.HandleFunc("/json", ServeJSON) http.HandleFunc("/", ServeIndex) http.ListenAndServe(":9000", nil) } func ServeJSON(w http.ResponseWriter, r *http.Request) { } func ServeIndex(w http.ResponseWriter, r *http.Request) { io.WriteString(w, idx) } var idx = `&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;script src="https://code.jquery.com/jquery-3.4.0.min.js" integrity="sha256-BJeo0qm959uMBGb65z40ejJYGSgR7REI4+CW1fNKwOg=" crossorigin="anonymous"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;script&gt; $.getJSON('/json') .done(function(data) { console.log('success:', data); }) .fail(function(_, e) { console.log('error:', e) }) .always(function() { console.log('complete') }); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;`
It‚Äôs hard to tell. The system processes 600k events/s at peak load, and scales up to 1.5m events/s on 4-6 cores. The events are linear up to a certain point after which they‚Äôre parallel. The linear part is the choke point. Events come in ‚Äúin order‚Äù and have to be processed in that order. Once you‚Äôve separated the events by their order book id, you can then process those events in a thread by themselves, but still in order. To avoid events piling up anywhere in the system we specifically avoided garbage collection and regular queue systems, and instead went for a manual reference counting protocol. As OP shows, you can just as easily shoot yourself in the foot with Go and concurrency. Go allows you to set a max concurrency level, so we could have controlled it like that, but IIRC you cannot set that runtime, which would make it harder to scale dynamically. Back then I wanted to go for Java or .NET, but as things turned out I‚Äôm not sure we would have been able to fine tune it as much. Go comes in at a place between C/C++ and Java/.NET, so maybe it would have been the perfect match.
&gt; If you're not going to follow the specs for the method FTR, serving a 0-length payload is explicitly allowed. The "ought to" isn't authoritative - otherwise it would've been `MUST` or even just `SHOULD`. As it stands, wile I agree that using 204 is better, it's definitely conforming (and honestly super normal) to use 200.
R/choosingBeggars
Not sure what you are trying to do, but how exactly are you getting guarantee that you are getting the packets with the headers first? I mean the server may be sending those parts first, but you know that packets could get lost or get corrupted and there could be retries based on that which could be after some subsequent body packets have arrived. &amp;#x200B; Further more, I don't see this as being good for the server. If I don't want body, I will simply use a HEAD request to get only the response headers.
Does it, though? It‚Äôs not like there are many of those, and they are all between 200 and 299.
nice
I know of a few people who applied, they indeed used the available libs, and I think did a good job but were not selected. So, which is why I had to put in so many efforts. :)
That makes perfect sense to me. If it is taking a day, or more then that, and you have to do the same for multiple companies, it is obviously a lot of investment.
Now it makes sense to me. ah, "bo√Æte" is a nice sounding word, maybe I can use it next time. ;) So, the company helps organizations to make data-driven decisions using AI. you were right at your place, nothing to be sorry about. :)
your approach looks good to me. I guess that is how it should have happened. :)
 drwxrwxrwx@ 9 ashleyelder staff 288 Apr 15 11:30 GeoServices drwxr-xr-x 261 root staff 8352 Apr 15 13:53 go-build drwx------ 3 ashleyelder staff 96 Apr 15 15:09 com.apple.cache_delete Your `go-build` directory is owned by `root`, which means your user `ashleyelder` can't write into it. I'm not sure how this happened, but one way to fix it is probably: sudo rm -rf /Users/ashleyelder/Library/Caches/go-build # i.e. as root mkdir /Users/ashleyelder/Library/Caches/go-build # i.e. as your user And then re-running your original `go` command which failed.
I don't have previous experience using it but it sounds like something OpenCV can handle. https://gocv.io
We've been using https://blog.cloudflare.com/go-coverage-with-external-tests/ with this wrapper script https://github.com/FuzzyMonkeyCo/monkey/blob/master/ape.sh and we're able to see more than just the coverage of `main`. Hope this helps.
This is not a full time gig obviously. While we cannot provide an hourly wage at the moment , people who are interested to help will get a stake in the project once milestones are achieved. You get paid then! So far we are all volunteers but we want to pay people who are willing to join the team.
It looks like a cool project, I like it. I wish it was more like terraform though, without hcl as the scripting language. A plugin arch and no need to compile my configuration to a exec would be great
Maybe it‚Äôs just me, but reading this article made me conclude that the `go` CLI is starting to show signs of becoming a sprawling mess of new features and their related operations spread out amongst multiple top-level subcommands with different grammars and inconsistent flags- just like `git` CLI. For example, I must remember to use `go list -m all` to list all module dependencies including those not in the `go.mod`, instead of something like `go mod list` or `go mod list -a`. I realize that Go modules are still experimental and that the `go list` subcommand predates modules and existing package ‚Äúlisting‚Äù functionality is here, but it is less risk and more acceptable to introduce breaking changes to `go mod` than to modify an existing subcommand. I can come up with other reasonable arguments for exposing this functionality here, as well as counter arguments- so ultimately it probably comes down to personal preference. I completely understand that there is only so much that can be done once a CLI is released and has a significant user base with software/automation/CI/CD built around it without breaking and frustrating existing users. I just know that I currently tolerate Go‚Äôs command line just like I tolerate `git`.
Thank for your reply. OpenCV is build for video, can i make it work with image ? I will try it.
https://github.com/spaceuptech/space-cloud For mobile users
I (my opinion) wouldn't recommend nano as an editor. It really has no support at all for go. There is a lot of GUI editors available with Visual Studio Code and it's go plugin being one of the most popular. If you want to have a textbased editor I would recommend micro https://micro-editor.github.io/ which has syntax coloring support for go and most shortcut same as other editors like cntrl-s for save.
Also learning go - will check the repository soon. But also, how _cool_ would it be to have online Pokemon battles through the CLI? Lmao!
Please just create a function that does the sum += i and then call it in the for loop and have it return the value in a channel. This helps for many reasons. 1.One being sum won't be as much stored in the goroutine. Cause it's not just 1,2,3 it's each goroutine up to 10000000000. 2. Usually you want to have an operation defined outside of a go routine to make sure your problem isn't the operation. 3. A channel allows for data to be sent right from the operation but also the goroutine. This way you don't have to deal with all the issues of concurrency. 4. This also ensures others won't touch your code as much or rather not add operations to the goroutine that you expect behavior from. 5. This is what makes the gophers work in unison. Remember that diagram? https://images.app.goo.gl/4BjrgthtMi8KnvRw9 What you are doing is stacking the books one heavier each gopher, when it you have channels (the single gopher in the middle) you lighten their load drastically. A really really good resource on this. https://www.oreilly.com/library/view/concurrency-in-go/9781491941294/ch04.html
I think the main issue would be what to name it... Like if I wrote it in js I would call it Pok√©mon js.. but I can't call it Pok√©mon go
Also, chances are you are running these inside a request, which is another goroutine. Which is gonna double all these into memory. Look at what Google did here. https://blog.golang.org/context/google/google.go At the very bottom you have this. ``` func(*http.Response, error) error) error { // Run the HTTP request in a goroutine and pass the response to f. c := make(chan error, 1) req = req.WithContext(ctx) go func() { c &lt;- f(http.DefaultClient.Do(req)) }() select { case &lt;-ctx.Done(): &lt;-c // Wait for f to return. return ctx.Err() case err := &lt;-c: return err } } ``` That "c &lt;- f(http.DefaultClient.Do(req))" is the most important part. Your request is a goroutine and you run operations inside each request. But what Google is doing is having generic operation that spawns a goroutine in each request to do a certain operation but that goroutine is returning automatically to a channel to share data each goroutine. This is amazing but also really important to ensure nothing gets stuck and nothing besides whatever http.DefaultClient.Do(req) is doing is gonna slow this down. Think of it as a beehive. Each request is a bee that gets sent out. You then have this model of communication that really only splits off from the bees route, just to carry the honey back per flower but it's as if the smaller bee was already at the flower ready to fly back. This is where the magic is. You didn't have to set this up.
Many servers reject HTTP methods outside of GET and POST. With Winsock the headers always come first, but I am looking for a Golang equivalent/
In case the author is here, have you had a look at [hypothesis](https://hypothesis.works/articles/how-hypothesis-works/), since they explain how to improve your own quite a bit!
Can someone explain all the downvotes?
It might be controversial but i still think after 10 years that CSV is best logging format :D. Not Json, Bson etc... plain old CSV. I was trying out to make Json a thing and i see many companies pushing for it but amount of extra tokens i see plus removal of plain tools like grep from easy use is just not worth it for me. Ofc i might be in minority.
This feels outdated in 2019 when go mod is the shit to use.
Maybe call winsock api directly if you don‚Äôt need portability.
204 should work for you.
Can't you just use `net` package directly?
It depends on the situation I suppose. I don‚Äôt mind either format but packing a struct from JSON is easier than csv so for putting your data back into your program it JSON has its advantages.
How do you handle versioning? CSV get borked if you add/remove columns?
Take a look at https://github.com/kisielk/godepgraph
Though it gives you the opportunity to use different command line utilities, like [jq](https://stedolan.github.io/jq/).
We do all those things today, but back then the majority of our software was running on the mainframe, and windows was mainly used for running our web apps. We didn‚Äôt test in production as such, we did have a somewhat beefy test server (for the time), and the software ran fine there. These days we have hundreds of test servers and thousands of production servers, and the huge monoliths are mostly gone. The mainframe is still there, but it‚Äôs more of an advanced batch machine/database now.
What the hell is that link?
In the first example, the entire file is read into memory (the CPU waits for _ages_) then the data is processed. In my example, as the data is read from the file (in chunks) the CPU can process some of it. So by the time the full file is read - it's already been 99% processed as well.
How do you handle versioning ? Short response is: "You don't". Long response is: What i do is format of &lt;timestamp&gt;,&lt;event\_type\_token&gt;,&lt;param1&gt;,&lt;param2&gt;... so if you have something like &lt;timestamp&gt;,http\_request\_received,... you would simply add new event like &lt;timestamp&gt;,http\_request\_received\_v2, We have a toml file with "schema" or "mapping" defined Yes.
I'll be honest... We log and we do structured logging... But when it comes to observability logging doesn't come anywhere near the benefits that we feel proper metrics and tracing setups provide.
if it is for development can't you just run the container hanging main thread and attach to it on second shell. This way you would keep the build cache after 1 build in the image.
Why would you emulate one of the worst aspects of NOSQL databases?
The only reason to use go in interview is when you are looking for go developer.
Anyone want to share reviews on self-hosted structured logging tools?
One of my dependencies is `k8s.io/client-go` so... the build step has 185 packages to compile.
Since when safety is "worst aspect" ? Is this trolling or legit question ? If your system generates 30+ gigs of logs per day and it is deployed on more than 10 servers even doing deploy same time becomes a problem. So to avoid issues with old formatted data or left in queue old strucutred data being tried by new code and exploding safest best is to not fucking do it. Add new type or new type version which in this case is synonymous and handle old case and new case. If you never update old but only add new you never take the risk. Why take an avoidable risk ?
But now you introduce whole bunch of issues around schema syncing which is non trivial. But I see your approach.
We eliminated this by deploying our app from back-to-front. You can do similar by using queues like RabbitMQ etc and handling unknown events by requeueing them etc.
So I am assuming you want to write a client that connects to a server, and close the connection right after the headers are sent. With that assumption in mind ``` resp, err := http.Get("http://google.com/") ``` The response after that command does not have the body read. So closing resp.Body is enough. For more on the response handling look at https://golang.org/src/net/http/response.go?s=5440:5507#L143. If you want to code it yourself, it might look something like this ``` package main import ( "bufio" "fmt" "net" "net/http" "net/textproto" ) func main() { req, err := http.NewRequest("GET", "http://google.com/", nil) if err != nil { panic(err) } c, err := net.Dial("tcp", "google.com:80") if err != nil { panic(err) } req.Write(c) tp := textproto.NewReader(bufio.NewReader(c)) tp.ReadLine() // HTTP/1.1 301 Moved Permanently header, err := tp.ReadMIMEHeader() if err != nil { panic(err) } c.Close() fmt.Println(header) } ``` Not that similar behavior exists for http servers, where the handler is called before the body is finished streaming over the wire. I would suggest writing a prototype, and seeing where it falls short. golang has nice profiling to help with that https://golang.org/pkg/net/http/pprof/.
I see thanks. Did you measure what is the latency if reading chunks from drive too? For comparison?
This ^. When you have a proper metrics platform like Prometheus or Datadog, you don't need structured logging (as much) because logs are meant for humans to use while debugging, and metrics are meant for machines (dashboards, aggregation, alerting).
No, that's called "stuttering". https://github.com/golang/go/wiki/CodeReviewComments#package-names
I saw that before when doing some research into it and just tried it with a project I have. Unfortunately, it seems as though it's going to parse my source files instead of reading dependency management tools like `Gopkg.lock` and `go.mod`. Because of that, it seems tricky to get it to work (which I was unable to do) for projects that don't have source in the root directory (e.g. I have a couple `package main` files under cmd/). Additionally, it looks like that repository hasn't been updated in some time and has an open issue stating that it does not support Go Modules yet: [https://github.com/kisielk/godepgraph/issues/35](https://github.com/kisielk/godepgraph/issues/35)
Ah okay.
I think I misunderstood your previous post. But I‚Äôm against using CSV because including the keys is negligible because that type of repetition compresses very well.
[removed]
[removed]
CSV also compresses well :) That is not an argument against even pro. "zgrep" makes you not need to uncompress them when searching. I respect your POV ofc.
I apologise for this. I realized the go engineers made a trade off for making simplicity #1 and not brevity. I'm guessing because it's a language for giant code basses and well concurrency is just hard. So yes, just typing "go" is easy but at the cost of beginners having a really hard time. This is something that gophers shouldn't complain about because this is what we bought into. This is go's philosophy and therefore we should wear it as a badge. I'm sorry if I sounded like I was correcting you. I should of explained it in terms of trade offs first, not right or wrong. I apologise.
[removed]
[removed]
[removed]
Not much can be done to speed up such choke points other than passing it off to C/C++ or maybe even Rust today. As for the dynamic concurrency in Golang I believe you can do that by calling runtime.GOMAXPROCS: https://golang.org/pkg/runtime/#GOMAXPROCS
Mount your src as a volume.
Tracing &gt; Logging &gt; Metrics generally, you can derive everything on the right from the left, barring tons of bandwidth and storage overhead I think we would only trace
&gt;The response after that command does not have the body read. Thats what I said/meant to say. And even in the DIY part all the http code is from http.NewRequest and textproto.NewReader so there is not http logic reimplemented.
People still deserve to learn why GOPATH was great and how modules are a step backwards.
 Because if you don't rate it highly, you are bigoted. I have attented 3 Gophercons and other various GO events. Their side purpose seems to be hard-left polemics. If you give any dissenting or contrary voice, you are outcasted. No one dares speak against the God of Inclusivity and Diversity. If you are not the "right" kind of diverse in the go community, you are not included. &amp;#x200B; Reminds me of the famous line from Animal Farm: "All animals are equal, but some animals are more equal than others."
True, and I'd say that cost-wise the inequality reverses.
Maybe I just haven‚Äôt delved into the features of jq deep enough, but the one thing I need most that jq isn‚Äôt the most helpful about is with regards to nesting, and know at which level different things are at. It is incredibly beneficial to be able to collapse different nodes in order to get what exactly everything is nested from
What do I call instead so that I don't abstract myself out of that decision? I'm guessing the solution has to do with using a custom `Transport` or `RoundTripper` and perhaps using `apiclient.New(...)`. Assuming that's the correct method, I've gone on to create and use a custom `RoundTripper`, calling the original when appropriate but am still unsure how to perform either of my requirements in the custom handler.
Totally, I‚Äôm kind of surprised we don‚Äôt see more systems that start with one and product the others, but maybe it‚Äôs still a matter of wasted bandwidth if some traces are only actually used for metrics, I‚Äôm not sure.
I think you misunderstood. In both cases, the whole file is read from the hard drive into memory so the process can use it. The only difference is how long the CPU has to wait to start working.
PokeTerminal, TerminalMon, GoMon, GoPoke. All pretty lackluster, but continue to brainstorm and you'll get one.
And JQuery is conforming by timing it after it doesn't get any content.
gopter looks like the missing ingredient I was searching for in order to do the "shrink" which isn't in testing/quick. The integration to goConvey is a great bonus. I will look into it and perhaps post a follow up article once I get some working knowledge of it. Thanks for the links.
Exactly, I misunderstood the implementation. Re-read the code now it makes sense. I got it. Thanks for clarification. My bad.
I'm now able to get at the response body by implementing a custom `RoundTripper` and placing it inside a custom `Transport`. The custom `Transport` calls the original `RoundTripper`, reads the response body, and stores it for later retrieval. It then replaces the original (and used) response body with a new reader containing the original data and returns. This seems to be an overly engineered way of getting at this response body, so I'm still not sure if it's the ordained method. The new `main.go` with new functions are on GitHub at [https://github.com/arcanericky/petstoreclient/blob/master/main.go](https://github.com/arcanericky/petstoreclient/blob/master/main.go). It'd be great if someone could take a look...
For me it is: 1. Context 2. Values 3. Other, usually config options
&gt;file, err := [os.Open](https://os.Open)("input.txt") // file is only a pointer to location on hard drive, at this moment no data is read to the memory if err != nil { log.Fatal(err) } defer file.Close() words = make(map\[string\]int) scanner := bufio.NewScanner(file) // scanner uses only pointer at the moment scanner.Split(bufio.ScanWords) for scanner.Scan() { // every Scan() accesses the pointer which does I/O read to hard drive, I assume there is latency for every single call here, since the file is not read as entire chunk w := scanner.Text() words\[strings.ToLower(w)\]++ } if err := scanner.Err(); err != nil { log.Fatal(err) } hmm actually I am sorry to bother you but I want to understand it right. I tried to comment on your code example, could you please update where I make a mistake ?
arent the ratings anonymous? will the organizers know your rating for the particular presentation? imo this talk would be better if the presenter used Go to model a gopher mascot instead.
Take a look at tools for `go generate`. You can include any kind of data in your binary, constant or changeable.
It is possible but it requires writing code that slices and edits binary objects, which will be different per platform, but in theory there is no reason why it can't copy itself with modified data bss or whatever they are called, then remove the previous version and move the new version into place of the old. It would be more complicated on windows, you would have to register a script with the system to run at exit as in windows you can't change an exe while it is running as the handle is open and that blocks other processes opening it (at all, btw, not just read). I've long been an advocate of a filesystem that has a tiered structure with the main layer all as folders, and inside these labels are child filesystems, like how xml and DOM trees are structured. A shell could be written to encapsulate a binary with its data transparently also. With such a thing you could also encapsulate many more things than just settings, it could keep inside there a whole set of Go build intermediate objects and the source code with its CVS change log, but i'm rambling on quite a bit there. I am not that familiar with the MacOS bundle format, but I suspect it already holds settings for you, and there is an API to access it.
Great read, thank you for the share!
I read the blogs a 100 times, and missed the most important part "sub packages are covered but not imported packages". And I have my main package inside "cmd", so it didn't work. Now I'm restructuring a bit to bring in everything as sub packages when running tests
I'm glad it helped! Have a great day :)
The important tidbit missing from OPs post (I assume) is that the server isn't sending any Content-Length. Because otherwise, timing out would be wrong - and observably, that's not how jQuery behaves. Even then, I'd think that something else would have to go wrong with Keep-Alive or somesuch, otherwise the server would close the connection after sending no content and a read would return an error immediately. i.e. I'm pretty sure everyone in the chain behaves correctly (as far as we have info) according to the specification (though as it turns out, the [DELETE method section](https://tools.ietf.org/html/rfc7231#section-4.3.5) is more specific than your quote, in that it specifically says a 204 `SHOULD` be returned when the body is empty - but it's still not a `MUST`) and jQuery is behaving in the best way it can given what it gets. It's still an undesirable outcome. But it's very hard to point any fingers without access to the server and client code. See also [my other response](https://www.reddit.com/r/golang/comments/bdlyn9/psa_microservices_are_expected_to_always_provide/el04pg5/). I would be willing to bet that the problem in this case is a combination of bad server code and bad jQuery usage, but it's hard to tell. Either way, I'm pretty sure that the derogatory tone of OP is‚Ä¶ a little bit embarrassing given the circumstances :)
We‚Äôre looking at K6 to do e2e testing here, the in-language way to test http handlers isn‚Äôt exactly what we need, and we‚Äôre also generating JS api clients based on our api schema. I guess when you‚Äôre going that far, it‚Äôs a bit of a trial and error to see what works best for your use case. Thanks for your comment :)
Hope you land the job tho :)
Thanks for the suggestion. Sounds like that would work for a simple case, but it would fall down when using something like skaffold for more complex configurations.
\&gt; arent the ratings anonymous? will the organizers know your rating for the particular presentation? &amp;#x200B; Who knows? The risk is not worth it. And I 100% agree. Also this dotGo presentation firmly breaks the "family friendly" content rule of the Go CoC, but I digress there, less I be also labeled some "bigot"
Check out the sync configuration `sync: {'***': .}` in Skaffold. If you use multi-stage builds you'll need to patch the artifact configuration with `{target: 'my-build-stage'}` but on your build stage you can use filesystem events depending on your OS to rebuild &amp; restart your project inside the running container after sync as a work around that takes advantage of the build cache.
Yes, I'm asking if anybody knows the "trick." :) - `go build all` didn't work; it said all didn't match anything. Which I suppose makes sense, since the working directory won't have the vendor directory. - `go mod vendor` didn't work, since that tries to read the source files in the working directory to determine which packages should be pulled from the mod cache into vendor. There are no source files at this stage in the Docker build. - `go build ...` almost worked. It built a bunch of things for a long time. I think it was compiling everything in the mod cache, not just the packages that my code depends on. And it didn't respect the version pinning that's specified in my `go.mod`, so it tried to pull down the latest versions of lots of packages, and it ended up with incompatibilities, so it failed. - `go build k8s.io/client-go` similarly failed, because it tried to pull down the latest (v11) whereas I'm using v10.
No sane OS should allow a non root user to do this.
Generally, when dealing with files it's best to assume a design around streaming chunks to save memory unless you are sure you'll only ever be dealing with small files. That said, it probably is slower (overall) to call \`[io.Reader.Read](https://io.Reader.Read)()\` every X ns/ms vs \`ioutil.Readall()\`. Then again, if you're loading a file you probably want to do something with it (process or forward it) so again, there isn't much reason to load the whole thing first. If this was an HTTP server you could \`io.Copy()\` right to the client as soon as the first chunk was read from the disk.
TCP guarantees in-order arrival, bro.
https://peter.bourgon.org/go-for-industrial-programming/#observability
Good insight, I am at the first few months if journey with Go and I am trying to understand best practices. Thank you again
If the vendor directory contained the compiled go libs, this would be simplest and best solution. But vendor only has source code. Mounting the src directory into Docker wouldn't speed up the build, because it would be caching the source, not the compiled libs.
One the plus side you can merge cvs files very easily :)
[removed]
Oh. I did not read your post correctly. My approach would be to mount /go of the image (in the case of go alpine) as a volume.
They‚Äôre not using context background. They‚Äôre using a context. It‚Äôs an easy way to deal with per-request cancellation and have it propagate to every sub request. I don‚Äôt know where the first snippet comes from but I suspect it‚Äôs a test block, or documentation, or perhaps just an error. Say you have a request come in via HTTP or gRPC (gRPC makes it the first argument which is the current norm; http is an attribute on the request as it predates context). Imagine now that it takes you 4-5 sub requests to various other services to handle the requests. Maybe a call to redis, mongo, Postgres, and an HTTP endpoint or two. If you get done the first sub request but the user cancels the request, you don‚Äôt want to waste resources making further requests. So the server cancels the context. Each of those further requests then cancels as well. Since it uses an underlying channel that closes, it‚Äôs also a very convenient way to deal with stopping parallel goroutines. It‚Äôs also convenient to provide a global timeout. For example, if you have an SLA that you need to respond in 500ms, you could create a subcontext on the request context that times out after 500ms, which makes it really easy to deal with. If you have code that takes a while to run, you should also check the context for closure. I typically use a select to make it non-blocking, but your use case may be different.
No its not only for video. It's for any kind of image vision processing. It could even be used as a simple image conversion lib.
The CLI. Learn to use it instead of relying on a crutch GUI tool. Those GUI tools have historically been one of the worst security issues plaguing multi-hosting. They often have vulnerabilities and rarely get patched. Even worse, people often configure them to listen to any connection, providing a nice easy way for an attacker to brute force your passwords, or just exploit a weakness in them. They‚Äôre really not good. Alternatively, you can (but shouldn‚Äôt) open the port so you can remote connect from your dev workstation. As a last resort, you could also use docker to install the tool. It‚Äôs stupid easy to set up and if there‚Äôs an image, it‚Äôs extremely easy to configure to run all kinds of apps as it just grabs an image with everything pre-made and ready that you can easily uninstall in seconds. Remember to limit access to it from only your own addresses tho.
Go generate lets you use these things at build time. I want to edit these things at run time.
I mean, if the files in /usr/bin are user writeable, that‚Äôs your fault. But I can have a binary in ~/bin that edits itself. Hell thats even a thing scripting languages like python or even bash sometimes do. Though weird.
Really like the declarative nature of this library and how succinct the use of reflection makes the resulting code and the autogenerated help is really nice.
For the last two, it sounds like you weren't running inside the directory with `go.mod`, or in module mode (inside GOPATH, etc). Worst case scenario, to get a list of dependencies you can parse `go.mod`. https://godoc.org/github.com/rogpeppe/go-internal/modfile#Parse would go a long way, I bet.
[removed]
usql is a nice tool that will connect you to postgresql: [https://github.com/xo/usql](https://github.com/xo/usql)
It may be operating-system specific. Linux doesn't take kindly to currently-executing programs getting opened for write access; for instance, try scp'ing a file over a running program. (I encounter this a lot.) Windows may not object so much, but it should. Even if it works today, you're swimming against the security stream and I wouldn't care to guarantee it'll work tomorrow. The raw byte manipulation would be no big deal. Executables have a documented format. I know ELF can have arbitrary "text" sections and I'm sure windows executables can too. The problem is that you probably can't obtain write access to them while executing.
I don't know why you were downvoted. so far you were the only comment to actually consider how this could be done. And extra thanks for considering multiple platforms.
hmm very good point. I wonder if it helps if the file was only modified at/after exit of the program. Since while it's still running, you can just hold things in ram. Maybe some way to spawn a child, detach/disown it so it becomes a parent of the OS/PID 1, and have that child modify the file? But I guess the OS would still probably tread that as the original file still executing, perhaps.
As someone new to go, I'm trying to decide which to use.
Run all your database admin tooling locally, then use port-forwarding to connect to the server on the other side. This prevents running hack-able admin tools directly on your server.
Been using this for a few years now, it's a really nice library. Lightweight and a huge feature set.
Which simply means that byte stream is delivered to you in order its sent. It doesn't mean that the raw packets are delivered in order. The packets are numbered on the sender side and reordered on the receiver side regardless of the order they are received. &amp;#x200B; For example, lets say you have a 50 packet transmission and you are only interested in the data from first 10 packets. If packet 5 is corrupted/lost etc. and sent after say packet 40, the receiving end will give you the data till packet 4 and wait till packet 5 is received even if packet 6 to 40 are received before that happens. &amp;#x200B; So, if the goal is get only data pertaining to the HTTP headers and stop before the body is transmitted, that is not going to be guaranteed. All the data is chopped up into packets and sent on the wire and they could arrive in any order and reordered. You may think that you are only reading till headers are received and closing the socket, but that's not guaranteed and some body packets could have been received by then.
There is `go mod graph`, although it's pretty limited; it doesn't show all detail for non-module dependencies (although they all get listed once, it effectively flattens them beneath each leaf module).
Use modules unless you can‚Äôt. Then revert to the old way of doing things.
Point to note is that this does not guarantee that the the body packets are not received if that is what the OP is looking for. It simply means that you can stop as soon as all TCP packets pertaining to the header part are received at application level, but in between some of the body data could also have been received. &amp;#x200B; Also, this whole approach will go for a toss if the target server is using HTTP/2
Adding ‚ÄúDigitalOcean‚Äù to the title here is misleading. This is not by the DO team or their blog, which is generally a very high quality source of Go discussion. This is a community post that hosts content authors get paid for but aren‚Äôt as heavily scrutinized. These are mostly tutorials and in this case, fairly low quality. Not that it‚Äôs wrong and poorly written, it‚Äôs just outdated and shallow.
Don't install pgadmin directly on the server. Instead, use a local client on your desktop (windows/mac ?) and then ssh connect to the database host. That is the best way to do it. This way, you can access the database directly from your computer. I am not sure about Postgresql but for mysql, there is a tool called Sequel Pro that does it well. I am sure there must be a tool for Postgresql as well.
Postico if you are on macOS
Awesome! Thanks for sharing.
It works fine with a cmd directory if you change coverpkg to a string that includes your other packages. E.g create the binary from the root of your repo and use -coverpkg=./...
For simple querying and data exploration, I'm using [pgweb](https://github.com/sosedoff/pgweb). It's installed on my dev machine and I can connect through SSH to the production DB (although I rarely need that, I mostly interact with my local DB). For anything else, I just use `psql`. If you've an older Postgres installation (up to and including 9.6, I believe) you can still use [pgadmin3](https://www.pgadmin.org/download/) (the Qt application) - but that's also a local installation which requires an SSH tunnel to your DB server.
I would say if you‚Äôre goal is to hide the db, you could extract it out to an invisible folder elsewhere and update it there going forward, without needing to update your binary.
Would the remote server still calculate and send the body "in chunks" though? Many HTTP wrappers wait for the entire body to be served. I only need a paragraph's worth of the body and want to free up the socket before the remote server is done on its end.
&gt; It simply means that you can stop as soon as all TCP packets pertaining to the header part are received at application level, but in between some of the body data could also have been received. This is exactly what I'm looking for. I want a little bit of response body, just not the whole thing. Whether the remote server bloodies its knuckles rendering it with no one to receive the whole response is of no consequence to me. Pulling the size of a single paragraph from body and closing the connection is ideal. I *think?* /u/NeoinKarasuYuu knows what I'm talking about?
&gt; 90% system cpu load Man, you'd think people writing many-core software would've seen this at least once.
[https://github.com/jzelinskie/faq](https://github.com/jzelinskie/faq) is a format agnostic jq that supports a couple of formats. We've been thinking about adding CSV support to it as well.
Oh interesting, thanks!
Not so much hide it as, make it portable to the binary. So it moves, the db moves with it (within the same architecture and type of OS). But have the saved (configs, data, whatever) move with it
[removed]
It would be nice to use the GRPC structs for everything, but I haven't found a way either. I'm not sure how it would interplay with the auto generation of the structs using protoc or the types being used. I'm using [github.com/jackc/pgx/pgtype](https://github.com/jackc/pgx/pgtype) for database specific and nullable types for SQL and some protobuf specific items in my protobuf definitions (e.g. protobuf.Timestamp). Using only go specific types The nice thing is that there are pretty straightforward mappings between the two. My solution was to generate `RowToPb` and `PbToRow` functions in my database table output files to do the conversions. The conversions use functions who's name is the combination of the from/to types so that I can auto generate the calls to them from converting from one to the other. Here is an example of one function name: `WrappersInt64ValueToPgtypeInt8` The whole process isn't as clunky as it may sound. All you have to do is manually write the code for each type conversion, and the templates don't even call the functions where the grpc/go type and the sql/go type are the same. My main complaint is that I have to maintain a list of types in my gnorm.toml file but then I have to manually create an additional mapping inside the template, which looks like this: [https://gist.github.com/freb/4500cb5258abe2e4740511743160e857](https://gist.github.com/freb/4500cb5258abe2e4740511743160e857). It would be nice if they could both be maintained in the toml file. I tried creating arbitrary tables in the toml config for access in the template, but that doesn't seem supported. I haven't looked at the code, but maybe adding support for an `Other map[string]interface` section in the config struct would let us do things like that. I'm happy to contribute what I have now if we want to iterate on it. I'd feel a lot better about doing that if we could work toward a solution for the type map issue inside of Gnorm itself.
[removed]
There is no decision to be made. Use go modules
&gt; For example, lets say you have a 50 packet transmission and you are only interested in the data from first 10 packets. If packet 5 is corrupted/lost etc. and sent after say packet 40, the receiving end will give you the data till packet 4 and wait till packet 5 is received even if packet 6 to 40 are received before that happens. That's good enough for what OP wants -- either he gets the headers and the \r\r, or he does not.
`jq` is wickedly flexible. You can do substring matching on keys and generate intermediate objects if that's what you're lacking. That'll let you find things and make new objects/collections out of them.
[removed]
It's pretty much reversed to what you wrote. https://stackoverflow.com/questions/196897/locking-executing-files-windows-does-linux-doesnt-why It might be a security mitigation in SCP or some Linux setting/extension you have enabled that prevents you from replacing a running binary. I used to replace running binaries on Solaris and then restart the program, that worked (in most cases, there was a bug that could cause the process to turn into a fork bomb).
Ignore the package name. Consider this alternative instead: ``` package hospitals import ( "github.com/hello/is-it-me-you-looking-for/hospitals" ) type Patient struct { hospitals.Patient } ``` I don't like the fact that in my `hospitals` package I have a struct named `Patient` that is composed with another `Patient` package from an external dependency's `hospitals` package. But I was wondering if this is some sort of common practice and there's a good reason behind it?
I‚Äôm genuinely curious as to what system generates 30GB a day that‚Äôs best stored as CSV. What type of data?
After some googling and what not I arrived at this solution: ``` package main import ( "log" "net/http" ) func main() { http.HandleFunc("foo", BodyCloser(func(writer http.ResponseWriter, request *http.Request) { })) } func BodyCloser(h http.HandlerFunc) http.HandlerFunc { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { defer func() { if err := r.Body.Close(); err != nil { log.Println("...") } }() h.ServeHTTP(w, r) }) } ```
We use Uber's zap in all our go code. It is a little clunky, so we usually use it for critical error cases only. But metrics are awesome, we use a custom nested struct with named fields for each error category. This is exposed via UDP to statsd every 5 secs. Almost nil overhead compared to logging everything verbosely.
That is just really not a ton of logs. Where are you storing them? I'm thinking you don't store them in elasticsearch although I think if you did you'd not have to worry about json and storage size.
You need to get tricky: 1. Checkout a specific commit of src into say \`src-static\`. 1. Checkout a specific commit before you run docker build
Then, you should just use the HTTP request primitives as /u/NeoinKarasuYuu pointed out. Get the response and read the headers. Ignore the body.
I draw the limit at 2-3 parameters. As soon as I need more, I wrap them in a struct.
We store them in gzipped files tagged by name and date. There is a simple tool that can find Ing you need fast. If I‚Äôm looking for something it usually will be by a uuid tag or timestamp so script doesn‚Äôt need to go via a lot of data. Finance reporting tool is also feeding of this files so if you want to do any sort of extra new thing like pattern analyze you just pipe in. Doesn‚Äôt need much cpu power just space really. I tried before logstash and elastic search but it was grinding mega load on server and constantly being 2-3 days behind.
This is pretty cool. How does it compare to something like frameworks + selenium?
No probs! It was an interesting discussion.
This was 10-15 years ago. Multi-cores weren‚Äôt exactly new, but they were new enough that it was the first system we wrote with that much parallelism. We learned something along the way, and managed to get it running without any issues.
Spent some time thinking about this one... It's probably because of how expansive they each are. E.g, keeping all of your traces can be infeasible, so by the time you're ready to infer your logs from tracing data you've lost quite a bit of information. Similarly, it's pretty cheap to just aggregate metrics at the service layer (basically the Prometheus model) which is why that's a popular approach. Just my $0.02.
Hi, interesting blog post. Please note that it's also possible to implement receiver functions on `int` types. type Color int const ( ColorRed Color = iota ColorBlue ) func (c Color) String() string { switch c { case ColorRed: return "Red" case ColorBlue: return "Blue" } return "Unknown" }
Nice. That's essentially what the generator is doing. I think I'll add a comment about this in the post.
I was just about to write one myself! Thanks so much!
No worries; I've got a lot of improvements planed do keep an eye out; hopefully we can get a stable version to submit to the Chrome Store.
Poor terminology. &gt; In this post we look at generating a powerful enumerated type using go generate ... Go has no enum types. No generator can generate a Go 'enumerated type'. &gt; One way to define an enumerated type is to construct a set of related constants aided by a type alias. &gt; ... &gt; type Color int That's not a type alias. That's a distinct type. `type Color = int` is a type alias.
Thanks for the comments, will update
Please try again; I pushed a fix that removes the use of the two struct fields.
.jar installer that's needed to be executed with sudo, is this a joke?
Thanks for the kind words :)
There's a couple of useful links in this thread related to collection type handling [https://www.reddit.com/r/golang/comments/ar6yua/functionalstyle\_operations\_on\_elements\_slices/](https://www.reddit.com/r/golang/comments/ar6yua/functionalstyle_operations_on_elements_slices/)
Go already has higher order functions in that you can define functions which accept functions. &amp;#x200B; The issue is \_really\_ that it doesn't have generics as such, so it's hard to define a generic map function that works across all types of slices &amp;#x200B; [https://play.golang.org/p/EzeNh01FL8X](https://play.golang.org/p/EzeNh01FL8X)
better! now it builds, and *then* falls over: ~$ GO111MODULE=on go build -o /var/tmp/hello gioui.org/apps/hello go: finding gioui.org/apps/hello latest go: finding gioui.org/apps latest go: downloading gioui.org/apps v0.0.0-20190417104637-47fbe56766ed go: extracting gioui.org/apps v0.0.0-20190417104637-47fbe56766ed go: finding gioui.org/ui v0.0.0-20190417104218-8b47e2b4bfe5 go: downloading gioui.org/ui v0.0.0-20190417104218-8b47e2b4bfe5 go: extracting gioui.org/ui v0.0.0-20190417104218-8b47e2b4bfe5 ~$ /var/tmp/hello 2019/04/17 13:15:25 wayland: wl_display_connect failed
If you define collection types (eg, a type that's a slice of another type), you can use [https://github.com/jmatosp/collections](https://github.com/jmatosp/collections) to generate map, filter, etc.
Every observability signal emitted from a running process, from structured log event to metric counter increment to distributed trace annotation, can be understood as a measurement, or observation, about the state of a running system. From a certain perspective, they‚Äôre fundamentally alike: each of them is some number, captured at some point in time, with an associated set of metadata to identify what the number is or means. Even log events can be understood through this lens: the set of key/value pairs encoded in the event constitute the metadata, and the number is ‚Äú1‚Äù to indicate the event occurred. If we had a perfect collection system, where all interpretation could be done at read-time with zero cost, we could emit these sorts of raw observations to it, and stop there. But what makes the field interesting, or challenging, is of course that no such data system exists. So we‚Äôve had to make engineering decisions, compromises, imbuing certain types of observations with semantic meaning, and, critically, performing different classes of write-time optimizations, to enable specific observability workflows. Metrics, tracing, and logging are actually emergent *patterns of consumption* of observability data, which inform the way we produce, ship, and store the corresponding signals. They‚Äôre the product of an optimization function, between how operators want to be able to introspect over their systems, and the capability of technology to meet those demands at scale. There may be other, yet-undiscovered patterns of consumption, likely driven by advances in technology, which will usher in a new era and taxonomy of observability. But this is where we‚Äôre at today, and for the forseeable future. https://peter.bourgon.org/blog/2018/08/22/observability-signals.html
I meant higher order functions for slice processing. I am sorry if my meaning was not clear. This approach provides a semblant of generics without necessarily having generics itself. While this can be achieved easily once we have generics, it does not directly depend on generics or the syntax that comes along with generics.
Gopter's great but I decided that it encouraged an unidiomatic style of go... I did an experiment to see if that could be remedied: https://github.com/au-phiware/gopter/tree/propt I think it worked out well but would like to refactor more instead of the renaming that I did.
That is the idea of what I want achieved, but not necessarily the implementation. To begin with, it would be appreciated if code-generation were avoided - my proposal provides a touch of generics without having to go through the intricacies of the syntax. Then it would be nice to have this paradigm recognized and implemented as part of the language built-ins. With regards to your implementation: **Generation** `//go:generate collections -file $GOFILE` would be better as `//go:generate collections -file $GOFILE -type MyType -methods apply,filter` where `-type` allows me to specify which types I want, and `-methods` allow me to choose which methods I want. Both being optional. **Optimization** Certain functions can be optimized by using go routines, e.g map, instead of running a for loop for each operation, have them run concurrently in a separate go routine, then join the results. Given the above 2, I would use your library. However as much as your approach is laudable, I believe a built-in implementation can make it even better; simply by eliminating the need for generation all over the place.
‚ÄúNo Coding Required‚Äù learn to write code :) this is not pure go project when includes jar files. Capybara for functional test great tool but requires coding :/
The Rob Pike link is just a proof of concept of what can be achieved using existing tools, rather than the proposed language built-ins. He himself says, on the repo itself: &gt; I wanted to see how hard it was to implement this sort of thing in Go, with as nice an API as I could manage. It wasn't hard. &gt; Having written it a couple of years ago, I haven't had occasion to use it once. Instead, I just use "for" loops. &gt; You shouldn't use it either. The code uses reflection and would have a terrible penalty on performance. With regards to Koazee https://github.com/wesovilabs/koazee The api seems okay and it does say it cares about performance - although a quick look at the source code shows reflection at work. I have not tried it, but the fact that it exists, along with others that may exist in the wild means that this approach has merits for consideration.
Huh, that's awesome! I know that klauspost is also working on something, see https://mobile.twitter.com/sh0dan/status/1097204985811603456 (Thanks for using restic!)
Well that is unfortunate, I did some search for other work in this direction before I started but I guess I missed this. Would have loved to contribute to another project rather than duplicate work. &amp;#x200B; (I love restic right now for pushing my personal backups to a remote server :))
All that work for an internship? Sounds like you got screwed.
Haven't really used REVEL so I'm not sure about this; but it seems you're trying to get a form GET parameter rather than a POST Checkout this answer on stack overflow: &amp;#x200B; [https://stackoverflow.com/questions/46619860/post-form-with-revel](https://stackoverflow.com/questions/46619860/post-form-with-revel) &amp;#x200B; The solution appears to be using \`c.Params\` instead of \`c.Params.Form\` \`\`\` func (c App) Index() revel.Result { dat := c.Params.Get(sendIt); return c.Render(dat) } \`\`\`
If your structured logs have a single unified schema, sure. If not, having a bunch of CSV files for each log entry type is going to be a nightmare.
I use IntelliJ IDEA's database tools.
I am okay with being screwed, just the fact that I don't know why I wasn't selected is bothering me. :(
Maybe you can join forces :) I know he also uses restic btw
I also wanted to note that the different systems are imbued with different expectations and SLAs. Your BI (and other) events are non-negotiable, as your overall system will not operate correctly if you stop sending them, they are also an API. Your metrics too - while there is a certain level of approximation in various measures (p99 etc), it's assumed they aren't really sampled and need to be pretty real-time (based on the assumption you use them for alerting and actions). Your logs can potentially be leveled (we have a setup where we can trigger different logging levels on a per-request basis) and could be delayed a little. Your system should not break if logs stop. I assume tracing also has different base expectations, and would not benefit from being required to follow the most demanding SLA of all observability systems.
Why do you think so? If you are interested in events for a given uuid you grep by uuid and you get events. I don‚Äôt get why people keep repeating word ‚Äúscheme‚Äù here if schematic doesn‚Äôt matter. You could say it is ‚Äúnew line separated event logs‚Äù of you have a problem with getting it generate a file like this and play with it. It is much better imho than any json representation. Unless unless you hate console Unix tools.
I will definitly look into his code and see if I can contribute. On a first glance his approach looks a bit different than mine, maybe I can at least take away something to improve my Design :)
I guess 'the same for every project' was part of the reason. If a project needs code generation i know i need go generate. No need to look at the readme or search the scripts
You don't *have* to run \`go generate\`. You *can* run it, and it will run all of the generate directives in all of the source files that Go operates on. So instead of needing an out-of-band process to look for Go files, you can use the in-band Go tooling for a more unified experience.
But... but.... but... it was an internship, so it was to be expected for them that you had not that much experience. (but perhaps you showed them some unexpected skills that they didn't understand , so they feel threatened) Anyways, chin up! The fact that they never bothered you to tell you the feedback shows me , that is wasn't that great company to begin with.
I was discussing this with one of my colleagues. I have done it in the past in PHP, so it must be doable in go. Perhaps one day, when I have more time....
perhaps you find some in this package: [https://github.com/arthurkushman/pgo](https://github.com/arthurkushman/pgo)
I can understand they might be having many applicants they have to reply to, but first, they didn't bother to even tell me if my task was selected or not, I had to write them two emails and wait for 3 weeks, second, I give in so much of my time and energy to it, I expected that they will tell me exactly why I was not selected but nothing, I could at least improve myself. But, I let's learn something from it and keep moving, thank you :)
An unauthorized access attempt, a set of disk usage statistics, and a database connection failure are each going to have totally different sets of data fields, and so on for every other type of log entry. When you bear in mind that you might want to write multiple objects in association with a single log event, you end up with a combinatorial explosion in the number of log files, and it's a pain having to cross-reference them.
wrong link: [https://stein.wtf/posts/2019-04-16/enums/](https://stein.wtf/posts/2019-04-16/enums/)
PS: Also would like to point out, this is a small startup and near my place.
Thanks corrected
[removed]
&gt;entation can make it even better; simply by eliminating th I'm the author of that collections package, very similar to Robs trial, I got myself writing a lot of for loops in a project and I wondered if it would or not simplify redability or reduce some boilerplace code. Using in some place and I have mixed results. Not bad not good, only really worth if that package is extended like you mentioned and also to have custom map output types as: \`//go:generate collections -file $GOFILE -type MyType -methods apply,filter -map Int,AnotherType\` but then again not sure it's worth the effort.
[removed]
Why ? if you assume time is in sync with each server which kinda should be a thing. You have a very easy way to select them. By time bracket. But let me showcase an example as i think you didn't even try. example: \`\`\` 2019-04-17 14:57:06.450466,processing\_in,d65e5246-1f90-4908-b777-08629ccd547e-1-1,A,B,C 2019-04-17 14:57:06.470299,processing\_out,ad2bb50f-db06-4626-882e-7379b1cf0144-1-1,A,B,C,D 2019-04-17 14:57:06.470546,processing\_out,234d03e1-d2e8-4073-a862-a83d6a122705-1-1,A,B,C,D 2019-04-17 14:57:06.494928,in\_stored,d65e5246-1f90-4908-b777-08629ccd547e-1-1 2019-04-17 14:57:06.495608,out\_routed,ad2bb50f-db06-4626-882e-7379b1cf0144-1-1,A,B,C,D 2019-04-17 14:57:06.497445,out\_stored,234d03e1-d2e8-4073-a862-a83d6a122705-1-1,A,B,C,D \`\`\` If you need events between 14:00-15:00 you can grep by time gap and if you need special event type you grep for event type and if you need special instance uuid you grep by uuid. This is an example and dumbed down version. Currently we have 223 types of events that can happen in logs. Also if you have a analyzer that is interested only in in\_stored, out\_stored event he can filter them out easily. It can be stream processed or post processed. Copy paste that to a file and play with grep, cut etc.. it is really useful for close to 0 effort setup.
[removed]
[removed]
Your JSON looks wrong to me. Are you sure that's what it really looks like? I would expect something more like: { "animals": { "dogs": [ { "name": "hugo", "breed": "chihuahua" } ] } } In which case you might do something like: type Dog struct { Name string `json:"name"` Breed string `json:"breed"` } type animals struct { dogs []Dog `json:"dogs"` } type wrapper struct { animals animals `json:"animals"` } func decode(input []byte) []Dog { res := &amp;wrapper{} json.Unmarshal(input, res) return res.animals.dogs } But if your JSON really is how you've listed it, then it can't really represent a set of dogs usefully, and you'll need to describe your problem better.
[removed]
Of course you *don't* know that it needs code generation unless you look at the readme or something, because it's entirely not integrated into anything else.
&gt;Your JSON looks wrong to me You're right, I provided a bad example, that should've been an array/list, whatever it's called in Go. What is the ``json:"name"`` part doing? Thanks for the help.
Go has higher-order functions, it just doesn't have the type system necessary to express *useful* ones.
The JSON struct tag specifies what key in the JSON corresponds to that field. Go actually does some automatic matching based on struct field name if you don't specify, but it's best to just have them for clarity.
Take a look at this site -&gt; https://mholt.github.io/json-to-go/
Thanks.
It's probably for chaining action together
The `json:"name"` struct tag maps the key `name` in a JSON object to the struct field that the tag is declared on. Bottom line is, you need to declare the full nested structure (or use `map[string]interface{}` and cast every nested level manually) to decode the nested data you're looking for - you might also write the above `wrapper` struct like this, in case that might help you visualize it: ``` type Dog struct { Name string `json:"name"` Breed string `json:"breed"` } type wrapper struct { Animals struct { Dogs []Dog `json:"dogs"` } `json:"animals" } ```
Corrected the JSON. From what I was reading I thought that when you're just trying to snag certain parts of JSON that would be 'partial unmarshalling', guess I misinterpreted.
Also see these related map/reduce examples using interfaces{} and type assertions: https://www.reddit.com/r/golang/comments/1m25a1/map_reduce_and_filter_in_go/
Okay, thanks.
Should definitely mention on the site what go version and GOARCH this is. See [https://dave.cheney.net/2015/10/09/padding-is-hard](https://dave.cheney.net/2015/10/09/padding-is-hard) for reasons.
Here is the follow up https://itnext.io/gopter-property-based-testing-in-golang-b36728c7c6d7
Yeah, I noticed that it wasn't idiomatic even in the examples. Fortunately this isn't python and I personally think there is room for a tiny bit of wiggle in form if the function is there.
This really helps, thanks. Got it figured out.
informative, funny, with the bonus of bringing a bit of light to parts of humanity a lot would rather have swept under the rug for their own convenience. thanks for doing this, ellen.
Welcome to real world my Man. Don't give up and keep grinding. There is no substitute for hard work and perseverance. Good luck and Godspeed.
I saw this while searching before I started. Looks pretty cool if it actually worked. The general concept of wuff seems practical in general.
you need to iterate over the slice and initialise each slice individually. make([][]int, 10) creates a slice of 10 nil slices of integers, memory will have been allocated to hold the 10 nil slices Note also that s[3] is the 4th slice as we count from 0.
Awesome and quick follow up, thank you :)
[removed]
Clearly you and I have very different ideas of what is convenient and readable in a set of logs. I simply do not want 223 different log files cross-referenced by timestamp and UUID.
Normally you'd pass in a context that you got from a "higher level". (Ie. for an HTTP request, you want to cancel mongo requests if the user drops the http connection). However, in some rare cases you might wind up in part of the code where you dont have a context on the call stack to use. Hence, background context is used. &amp;#x200B; This is a general answer, I don't know the specific code in question.
great work. .. and dont give up.. nothing can beat experience
You have to be careful; what Linux is doing is subtle. You can not open an executing file for write. You *can* delete it, and you *can* move it out of the way, and then write a new file where the old one used to be. This is because what Linux is executing is not a "file name"; it's an inode. At execution time, the file path is resolved to an inode, which is used for everything after that. However, you *can not* open that inode for writing while it is executing. You can delete the file because inodes are reference counted and the executable will have a reference held open. Hypothetically, if you ran out of disk space because you have a multi-gigabyte exe, and you tried to free disk space while it was running, you'd find the disk space still wasn't freed, even though you could no longer see it on the disk. You'd have to stop the process first. (You're much more likely to see this with something like a log file deleted while being looked at by someone in "less" or something. The disk space won't be freed until less closes.) Windows... is much more difficult to characterize quickly. I don't know all the details, but I know there are a lot of them. It acts a lot more like the file path is the real identifier of the file, but it's complicated. So because the original poster is asking about modifying the *running executable*, what I wrote is correct. Linux won't let you do that. You have to be careful while testing my claim because there's a world of difference between doing something that will delete the file and recreate it for you behind the scenes, and one that modifies the *existing* file. The running executable could move itself, and write a new copy of itself with the new data, but at that point you might as well just have a database file.
I see my dad was right, he always told me that "life is not fair, get used to it, be patient, and life will eventually give you what you work for". &amp;#x200B; Yes man, thank you for your words. :)
I won't give up :)
I wouldn't expect you to be able to "detach" from the on-disk executable in Linux, because it doesn't "copy it into memory", it memory-maps the disk file. Under the hood, in the hardware, there are of course copies being made. But as far as the OS is concerned, you really are executing that file specifically. (Or, well, inode; see my other comment.) To try to "detach" from the program would then leave you with no executable code.
I'm yet to hear a practical reason. As near as I can tell, `go generate` is basically useless; anything that could run `go generate` could equally well run anything else. I use it in a couple of places even so, just to keep things "go standard". But if there was some other reason to drop it, I would with very little provocation.
Replying because I want to build on this: It may help /u/thomac to look at [the slice header](https://golang.org/pkg/reflect/#SliceHeader). The language hides the nature of the slice behind syntax for you, but under the hood, that's what a slice is, almost as if it was a struct. The "slice" you get from `make([][]int,10)` is under the hood a pointer to a `Slice{*([10]*Slice(of int), 10, 10}`, where the array will be full of nil values just like any other freshly created array. You'll need to initialize that array with whatever it is you want it to have. I typed `Slice(of int)` the way I did because there is no proper way to express this type in Go syntax; the runtime is doing some generic (in the generic type sense) stuff under the hood that we can't do at the Go language level.
it would be something like this: ```go foo := make([][]int, 10) for _, x := range foo { x = make([]int, 100) } ``` I wasn't aware of being able to use `make` for anything except new maps and presized slices. You actually could do this also, because slices and arrays are relateable: ```go var foo [10][100]int fooslice := foo[:] ``` The elements would still be `[100]int` in this case but you can use `[:]` again the same way to make it dynamic, in a for-range loop.
Can do something that logs an app. This is just how you get the headers from the core lib. https://gitlab.com/zendrulat123/prettymiddleware/wikis/home You can do a lot with storing this and searching it.
Does all this phallic talk not create a hostile environment for women? I'm seriously asking. I don't understand all the rules. I guess if your trans, you get a free pass to talk about whatever you want?
Thanks!