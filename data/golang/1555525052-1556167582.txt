One good thing is to try to tackle one of the problems your are facing in your job or personally. For example if you need to build some command line tool to process some files or to automate some stuff use Go instead of resorting to other tools and languages
I don't get it either. It would make more sense to let some functions generate either Go source tree objects or strings containing the code of functions, then have a similar naming scheme to Tests and Examples. Like: ``` func GenerateAdd(T string) string { return "func Add" + T + "(a, b " + T + ") " + T + " { return a + b }" } ``` Which would be run multiple times at `go generate` time, one for each type that was passed to `Add` in the actual program.
I still don’t get why go build does not run go generate before...
One of Go’s goals is to allow you to `go get` and `go build` untrusted code safely. `go generate` is inherently unsafe as it allows arbitrary code execution. So making it happen during `go build` would violate Go’s security model.
Big projects. If you have multiple pieces of codegen inside a large project, `go generate`'s directory traversal allows you to decouple those different pieces. Rather than having a big script with every bit of codegen in it, you run `go generate ./...`. As a bonus, each `//go:generate` directive is run inside the directory containing the source file, which is probably the behavior you want anyway.
That’s a very nice write up, especially for a kid. You have better written communication skills than almost everyone I’ve worked with.
I do focus on writing clean and easy docs, I guess it is usually not valued enough, but it is the front page of your work. Thank you, your comment made me happy. :)
&gt;Update: I didn't get this internship and they didn't even give me feedback. :( Best guess? There was no internship, they just wanted free code.
When you're introducing it (0:40), there's no context for starting with n==0. It's a bit confusing as you're jumping back and forth between the definition of the function, n, and recursion principles. I suggest starting with a problem to solve (factorial numbers), and then show how to do it recursively. Mixing the function code and the call tree adds to the confusion.
We generate queries and data access methods for some structs that go to our DB. So we don't have to hand write an insert statement, an update statement a simple select by ID etc... for each and all of our models. Using go:generate, we can do: //go:generate genmodel MyEntity --table my\_entities --timestamped type MyEntity struct { Entity // &lt;= base struct that has id, created\_at, updated\_at FieldA string `db:"field\_a"` FieldB string `db:"field\_b"` }
does go support tail recursion?
I'm not sure I understand this. Wouldn't it be exactly the same? The source code could allow for arbitrary execution. I think at least?!
I know exactly what you mean. I was think of redoing that part of the video because it was confusing like you mentioned. A lot of that came from me being so nervous still and my lack of structure when doing this video. For the following videos I will have a script, because this video would be so much better and to a standard I would like with a script. But thank you so much for this feedback! I will keep trying to improve as much as I can.
Have you heard about [https://github.com/jmoiron/sqlx](https://github.com/jmoiron/sqlx) ? It seems to do the exact same thing. It's a very popular alternative to full ORM like you mentioned.
https://stackoverflow.com/questions/12102675/tail-call-optimization-in-go The short answer is no. It looks like Golang will not have any plans to support tail recursion.
The source code can’t do any arbitrary execution until you actually run the program or use the library. So you can `go get` code safely (which also builds it), inspect it, then decide whether or not to run it.
Ah that does make sense. Thanks!
Sounds good! I'm sure a video series will be valuable for learning.
\`go build\` has no side effects on your system. It creates a binary file. It doesn't even have to be a binary that executes on your architecture, since go supports cross-compiling. \`go generate\` runs a shell command so it could do literally anything. It might delete files. It might start a keylogger in the background.
It's not just for you. It's for the people you share your code with. It's for your CI system. Tell them to run `go generate` whenever they get latest from source control. It's easier than giving them some long argument string that changes every time you add a new `.proto` file to your project.
Thanks, sqlx.Get()/.Select() is basically exactly [what I was picturing](http://jmoiron.github.io/sqlx/#getAndSelect).
Thanks for that, seems I had a big miss understanding of what go generate does. I was under the impression it was just a templating language
Don't worry there are lots of internship opportunities! My opinion is that take home assignments for jobs are not friendly to the candidates' time. You could keep doing them, but perhaps next time consider exclusively pursuing internships that don't have them instead :)
I see they didn't give you any feedback. If you haven't already, you should definitely _explicitely_ ask them for feedback. Feedback is literally the most important about interviews, no way to learn from mistakes without it!
That's fantastic news - as soon as I read the title I was immediately thinking this would also be good news for restic. As an aside, I'm not sure why restic is hellbent on zstd and won't settle for something that is good enough like gzip/bzip2 (which I suspect tarsnap uses). Not a big deal but I think it would be silly if a pure go implementation of zstd is the blocker that is preventing progress on implementing compression.
I also have the same opinion about tasks now. I will remember that. :)
I have explicitly asked them for feedback, but no response to that yet and as time passes, I don't think I am gonna get one. Indeed it is the most important part of not just an interview, but anything we do.
Definitely try to follow up! Or even better call them if you have the recruiters phone number!
I don't have the recruiters phone number, I just have his email, I have already dropped him an email, and told them I would be happy to visit them just for the feedback but no response. I have a friend of a friend working there, I am trying to get in touch with him, and see if I can get feedback.
Is this a company based out of Munich? We should write feedback about such companies on Glassdoor/Google/Quora so that the chances of getting scammed is decreased.
What was the company? So we can avoid it...
It might log for you, but it wouldn't allow you to send to slack or prometheus etc.
It is based out of Delhi and from what I have read about this company, I don't get a vibe that it is going to use my source code. But, I strongly feel that their whole process should be more transparent and self-reflective, the recruiting culture is definitely not healthy. I am indeed going to share my experience/feedback on Glassdoor, it might not be a scam, but it is not ideal either and should be avoided.
It's not a big company, it's a small startup based out of Delhi.
They gave me a coding assignment to be done in two weeks. It would be foolishness to take code from a coding round for sure. It was more than coding. It was about picking the technology stack, system design associated with it, which comes from years of experience. Coding is simple, when a solution is well thought. Same with the guy who posted this(he got one week, good time for making a PR in an open source project). Believe me this happens. I have seen people doing it, never thought that would happen to me at one point. The interviewer gets the idea from candidate, brainstorms with the team, makes a proposal to CTO(once he finds similarity between multiple solutions provided by the candidates).
[removed]
Many get into tech with the poser-syndrome and fail to prove themselves in front of their bosses, teams. This is one way to get solutions and present it to them.
[removed]
I didn't know that, I guess yes some people do such practice. I actually have a friend of a friend working in that company, I am trying to get in touch with him, and understand this matter more closely, and at least get feedback. If nothing works out, as I said, I will definitely drop my feedback/review on Glassdoor. You can say I am still giving this company a chance.
Good idea to keep things local. Kinda offtopic: Is there a difference between this solution and ORM?
Yes, there are a couple differences. Most ORMs use a query builder and will create a DSL for you to build queries in code. We wanted to stick to standard SQL use. On a separate project where we're using an ORM, there was a PR I reviewed recently that will change your query in some way to add some conditions on a JOIN. This code is so far remote from the SQL it both works on and generates that you have no clue what it does. I'd rather duplicate the SQL and have a clear statement of what the code is doing. &amp;#x200B; Most ORMs I've seen in Go don't do error handling the go way. We deal with simple queries and return the errors directly. &amp;#x200B; With our approach, if you did a JOIN query, you'd have to have a custom struct to deserialize the results. Most ORMs are more advanced than that. &amp;#x200B; We always work from the standard SQL. We can easily add standard sql methods. Mostly we just generate the INSERT and UPDATE statements since there are a lot of repeated fields in those. For SELECT statements, we're fine with just sqlx for the struct mapping and writing the sql by hand.
I see you lot around Sydney a bunch. Good work guys, I can see you put a lot of effort into tooling
You don’t need to do that at all. Read this from the official docs [1]. [1] https://golang.org/src/net/http/request.go?s=5434:5875#L165
You don’t need to do that at all; read this from the official documentation: &gt; The Server will close the request body. The ServeHTTP Handler does not need to. Source: https://golang.org/src/net/http/request.go?s=5434:5875#L165
Interesting. I wonder when that changed. The rule was to always close it.
I need some of the top area of some body responses but don't want to wait for the entire body to load even as the remote server does load it on their end.
I need some of the top area of some body responses but don't want to wait for the entire body to load even as the remote server does load it on their end.
Sux you got screwed, but I'll give you a star for your efforts, that's pretty good.
I stand corrected, there is that case of not being able to open a running binary for writing. Reminded me of "Text file is busy" error that I have seen on FreeBSD, thanks to this now I know that it was because a program tried to open the running binary for writing. OP should trivially be able to achieve his goal on Unix at least though with the following sequence open binary for read read all data into a buffer close binary unlink the binary create new file with identical name. write buffer to new file. I wrote a small c program to test it and I sucessfully updated a static string in it. There might of course be other issues, it's a fairly unusual use case after all.
1:40 for the start time
Let me start by saying that this sounds pretty insane, and I’m having a hard time imagining a real use case for this that isn’t better solved through other methods. That said, I think you could adapt the concept of self-extracting binaries to accomplish something along these lines, though it wouldn’t technically edit the binary itself. The idea behind a self extracting binary is basically a small wrapper with a chunk of binary data appended. The wrapper can some simple bash bits or something more complicated, and this wrapper just extracts and runs the binary data (really, the compressed part is not necessary for you use case). So anyway, I imagine you could take this concept and expand it to have two bits of data: One is the binary, the other the config file. On startup, the wrapper unpacks the two files, runs the binary which opens the config file and edits it as things are changed. On shutdown, have the binary rebuild this package to be moved around, as you describe. You can google self extracting installers/archives to get practical examples. The end result would end up being different, but I think you could run with some of those concepts to make this idea work somehow. Let me close by reiterating that this idea is insane :)
unfortunately getting no feedback is common , it occurred to me as well some times and is something in the industry that pisses me off. Actually I was surprised this week that I got a feedback from a company lol..
It tends to be a little cleaner to write tests for functions which return structs. Your in-package tests can open up the struct without messy type assertions. Otherwise, I agree, returning a struct pointer doesn't offer significant advantages over returning an interface.
To be honest i didnt read through all 180+ comments on the issue. Something about maybe matching their blobs onto zstd blocks (which sounds really weird to me after reading the whole documentation)
[removed]
&gt; Interesting. I wonder when that changed. The rule was to always close it. You are confusing outgoing requests with incoming requests. The body of an incoming request is automatically closed by the server. The body of an outgoing request needs to be manually closed.
&gt; Interesting. I wonder when that changed. The rule was to always close it. You are confusing incoming requests with outgoing requests. The body of an **incoming** request is automatically closed by the server. The body of an **outgoing** request needs to be closed manually.
I understand it is common, but I have multiple reasons that I expected to get feedback. That being said, I am happy that you got it. :)
No no, the rule was always to close it both directions. Which is why I always found it weird that you would find boilerplate without a close. It’s good that it does tho. Makes things cleaner.
Would recommend checking out zerolog in the future (if you have that option). I was not very happy with zap when I tried it. Zerolog is still clunkier than unstructured logging, but I find it worth it in most cases.
I had to use that piped into grep today to figure out why `go get -u` would keep falling over. I don't think I'm a big fan of the module upgrade logic, since it fell over the second time I used it, and the fix was to manually walk through the graph and upgrade parent dependencies until the global upgrade would work. Just giving me the newest code for everything, starting from the root, would have made everything work, and is what I want an update statement to do 99% of the time.
You're the Go 2 MVP
is there a tldr for this hour long clip?
Thanks
Thank you.
Generics bad. `if err != nil {}` good.
sounds like regular Go?
Tip: Scroll down to the transcript and look for the questions that seem interesting to you. (Sort of a DIY TL;DR)
nice thanks
Lots of koolaid drinkers don't see any reason for Go 2.
Going for the followers instead of the philosophy is a bit shitty, don't you think?
We also did this XD &amp;#x200B; github.com/ezbuy/redis-orm
To actually discuss some of the content. They mentioned that if err != nil is fine. I agree but errors themselves aren't fine without additional tools. It's hard to find out the errors that a function can/will return and it's even harder to find out how you're supposed to distinguish them for each library. I think the tools included with Go should enforce a doc comment for each error type that can be returned by a func. It would be nice if errors by default included a stack trace or a trace of each function the error passed through that could be printed if you wish to. A bit more controversial, I don't think you should be able to create an error without creating your own type for it first. Returning a generic error type defined by the language is rarely useful. errors.New("eg: I think this is bad and detrimental to the language")
Can you explain further why you believe that? Is it because interviewers might not be familar with it? Practically speaking I think its a better option than c/c++ since you don't have to deal with pointers/memory management (although c++ does most the work its still something to be mindful of).
Real question is "what do you want to check during interview" skill in writing code or problem solving skills. If it is the latter you should focus on pseudo code and if it is the first than you should book more time with candidate.
What do you have against Koolaid? It’s great.
can't up-vote enough! after two straight days trying to find an up-to-date sensible way of playing with gomobile and react native I got it working! awesome work! thanks a tone.
I agree. Errors without context (which function caused it, when was the error, or meta data) is difficult to use. The creators of Go didn't aprove of the exception catching in Java, but at least that has context e.g. mysql.ConnectionError. An upgrade for errors would be really nice. Maybe there is a package already that gives the errors interface more context? There are some articles about wrappers: https://dev.to/chuck_ha/an-error-wrapping-strategy-for-go-14i1 Havn't read those yet.
Seems like the new Go 2 error values proposal addresses some of those issues: https://github.com/golang/go/issues/29934
I only skimmed it so far but it looks like it adds a lot of context but if people are still allowed to go errors.New then I don't think much will change. Thanks for the link.
How do you use it? The readme states nothing but a reference to godoc which is not the best way of learning how to use a library as it is.
&gt; if err != nil I don't fully agree with that, mostly because of the fact that you have to modify loggers/stack traces to look up the call stack for anything situation where you for example have an anonymous error handling function to get a good context for the error. If the standard library log package is used you have to use the Output function instead of Print... to even set the call depth which adds even more cruft around error handling and line numbers..
if they don't allow \`errors.New\` then they'll create a Pythonesque split in the community because there'll be too much code to change (with no business benefit) involved in switching to Go2. I shudder at the thought of going through all my \`errors.New\` and replacing them with a typed error instead. It would take years. And I wouldn't gain anything - the error would still be handled (or not) in exactly the same way.
Oh, that's not the case, we haven't really settled onto anything yet. The issue on GitHub is rather old and had its fair share of bikeshedding around which algorithm to use. The only requirement is that there must be a Go implementation of it, because we're not sacrificing easy cross-compilation and memory safety. &amp;#x200B; Adding compression involves changing the repo format, which takes great care and time (which we don't have right now, there are other, more pressing issues to solve). In my opinion, for a backup program, repository format stability and compatibility are much more important than adding features :)
There is also this if you want to experiment https://godoc.org/golang.org/x/exp/errors
[removed]
/u/evil-teddy is referring to the fact that you **can** do errors with context. Since an `Error` is just an Interface with one function (`func Error() string`). Instead of using `errors.New()` or `fmt.Errorf()`, one can define a custom type that has the `Error()` function which returns a string and the receiver can check for the actual type. Or create a var in your package with `errors.New()` so the receiver can see if they got that var. Much like how doing a `Read(buf)` from an `io.Reader` may return `io.EOF`. That way you can tell the difference between `io.EOF` and `io.ErrUnexpectedEOF` or any other error it my return.
[removed]
I understand it's not going to happen and I guess I agree but errors.New is terrible. You absolutely gain by replacing it with typed errors.
Yeah, I agree. I just mean checking if an error happened with if err != nil is fine. Just about everything else is a problem.
&gt; Instead of using `errors.New()` or `fmt.Errorf()`, one can define a custom type that has the `Error()` function which returns a string and the receiver can check for the actual type.
[http://rosettacode.org/wiki/Category:Programming\_Tasks](http://rosettacode.org/wiki/Category:Programming_Tasks)
And by Koolaid drinker, you mean devs that didn't formerly work with Java.
Personally, I really don't like this proposal, I think it just isn't very intuitive at all. Why is wrapping an addition to `fmt.Errorf`? This could all be organised under `errors`, and be explicitly handled.
A little tangential, but there was a discussion of various CRUD approaches recently: https://www.reddit.com/r/golang/comments/bcj0tm/database_access_in_golang_seems_a_bit_tedious
Yeah I just read up on it. I appreciate the effort you take towards the compatibility of changes to the repo format! I agree, if zstd is not ready at the time you implement compression there would be a lot of other good choices. I hope you get to make the algorithms exchangable so at a later point other compressions can be added though :)
Actually, disregard that first case. I think you'd even have to resort to icky reflection for that one. But in the second case of defining a var in a package to set apart the different error 'types', that is pretty idiomatic Go (like in https://golang.org/pkg/io/#pkg-variables).
Most of the time the message is purely for logging so I can work out what went wrong later. I almost never actually handle the errors or do anything except report them. Most of the time I have no idea how I could even handle the error - wrapping every sql.Db call in some logic to work out if the call can be retried or not is way more hassle than "something went wrong, roll back the transaction and do it again, see if it works this time..." or even just "tell the user something went wrong, let them push the button again". I'm curious whether you actually handle every error? what do you do with all these error types?
It is, but they do prevent you from carrying over any additional context with the error, and maintaining that type if you use the stdlib's error handling functionality (e.g. `fmt.Errorf` to add context). I'm not sure you can avoid things like type assertions right now if you want useful error handling.
Not the original commentor, but where I work I developed a similar approach to using typed errors so that they can be handled differently. It's pretty useful because there are often different errors returned from deeper parts of an application that can be caused by things like; bad input, unexpected issues (e.g. networking failures, host errors, etc.), or semi-expected errors that can occur because some data is not found and you want to inform the user of that too. This kind of thing can be important, e.g. when making an HTTP web service you will want to try return the right HTTP status code so that consumers of your API can respond appropriately. That might be something like showing a useful message, or redirecting to somewhere, or even highlighting specific things that went wrong if it's possible, or just let letting the user know something blew up unexpectedly. In some cases, this can even help _inside_ the application. Maybe you've hit an error that you don't care about, so you want to handle it there and then and not have it bubble up, maybe you log it there and then, maybe you ignore it, or maybe you try an alternative code path.
I see what you did there.
Counterpoint: I prefer not do define error types or values unless I have to. Making promises about error types is, at the end of the day, API surface that I have to commit to. In the case of errors, it's doubly problematic, because errors are inherently tied to the implementation of a function - if I promise, say, to return an HTTPError, I can't in a later version switch to gRPC as the underlying protocol. It's ultimately why checked exceptions in Java are annoying and ultimately converge to people inheriting from RuntimeException (which doesn't have to be checked). I tend to return opaque errors. And if you need to distinguish them, that's a change of API surface and just like for every other API change, I expect you to file a feature request that I will then consider on a case-by-case basis. But I won't pre-commit to a particular implementation by making promises about error types.
I agree, I think, but I the interpretation of the "unless I have to" part is where we may differ. I often write functions that return both plain errors (e.g. from errors.New) or explicit error types declared in the package interface. A typical case is if the function is to return a property of some object, but the object is not found, then the function documentation will state the the function returns an ErrorNotFound error, which is part of the API surface as you say. Other errors like logic errors or other internal unexpected errors will return plain errors and are not documented as part of the API.
I'd like to see your project! Is it open-source or at least, in Github?
&gt; the interpretation of the "unless I have to" part is where we may differ. No, not really. I do the same thing you describe. But as a corollary, I don't want my errors to *automatically* get wrapped. It sometimes make sense to wrap errors. It sometimes make sense to return opaque errors. It sometimes makes sense to return sentinels or custom error types. Which of these should be a deliberate decision - and I personally tend to err on the conservative side, especially in the beginning, when I don't know anything about how the package will be used. Of course, the specific case you mention is sufficiently straight forward that I would directly add it (I mean - that's literally a case I have in my current (unpublished) pet project :) ). But a lot of these discussions make it sound like you should never use `errors.New` or `fmt.Errorf` and I couldn't disagree more - IMO you should pretty much *always* start of with `errors.New` or `fmt.Errorf` and only move on when you have a specific enough reason to do so. I don't think we really disagree at all, in substance. Maybe in phrasing. :)
I wrap every error I create in a type. I don't always handle the error beyond printing it. A lot of errors don't need handling but they still need to be distinguished from errors that do need handling and whether or not they need handling depends on the use case and caller. If you receive an error and don't know what it is then you don't know how to respond to it or necessarily where it came from. What I don't do but should start doing is add documentation about the possible errors returned from functions.
I would argue that if an externally visible error changes then that is an API change. It doesn't matter if it's typed or not. You can still return opaque errors even if they're typed. Just use them to indicate the source. Like RequestError could be a HTTPError or a gRPCError. You're not making a promise of the underlying reason, just that there was something that went wrong when making a request. Then people using your API or using something that wraps your API would know more accurately where the error came from and what it is. They might still be frustrated by not knowing why there was a request error but at least they know it wasn't a parse error without having to parse a string.
What exactly are you saying? I can't figure out if you're in favour of, or if you're against the Go Generics proposal.
Followers of who? Rob Pike?
https://github.com/pkg/errors You might be able to use that to get stack traces with errors. It was written by one of the authors of Go and I try to use it in all of my projects to get those very useful stack traces.
Copying my reply to the parent comment: https://github.com/pkg/errors You might be able to use that to get stack traces with errors. It was written by one of the authors of Go and I try to use it in all of my projects to get those very useful stack traces.
Where do the `Starter` objects come from? The interface could be defined there.
It's complicated. Try these articles: [https://rakyll.org/scheduler/](https://rakyll.org/scheduler/) [https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html)
But I'd then be referencing the package in both \`Plane\` and \`Car\` packages, introducing a hard coupling between those two - no?
I've been working on dependency analysis tooling and it does have option to output dot graphs. https://github.com/loov/goda But, I have few things to do before I make it `v1`.
The ardanlabs article is really enriching and great.
Seems like this have to be the one package. Dave Cheney had a good presentation for packages https://dave.cheney.net/practical-go/presentations/qcon-china.html#_package_design
ok, I get that, I think... so do you type them by cause, or by handling method? i.e. do you have `type RetryError` and `type IgnoreError`, or do you have `type BadInputError` and `type DatabaseError`?
same question then... do you type them by cause, or by handling method?
Go in a nutshell
It really bothers me how such 'problems' are set as exams. It either means they want the candidate with the solution most like theirs, or they want free code to steal.
name and shame.
&gt; type BadInputError and type DatabaseError This way but I think either are fine. It's just hard to tell when creating an error if the caller should retry or not so I'd rather leave that up to the caller to decide.
By cause, if I'm interpreting your question correctly. Similar to how things like `sql.ErrNoRows` would work, the error kind informs the caller about what went wrong, sometimes it's a bit more vague than that (e.g. some kind of generic "thing not found" error). In practice, that ends up looking something like this: return errors.Wrap(err, ErrNotFound, "couldn't find foo") Where somewhere further up you'd have a const defining that: const ErrNotFound errors.Kind = "foo: not found" The contents of that string aren't super important, it's useful if the string is unique, but it's just something to compare against later like this: foo, err := fooRepo.FindFoo(ctx, fooID) switch { case errors.Is(err, ErrNotFound): // Handling for not found, specifically. logger.Warnw("foo not found, continuing with bar", "stack", errors.Stack(err), ) case err != nil: // Handling for some kind of unexpected error, etc. return errors.Wrap(err) } That's the kind of approach we take currently.
&gt; I would argue that if an externally visible error changes then that is an API change. It doesn't matter if it's typed or not. Yes. But FTR: It's never typed. You always use `error`. There is no static type-checking. Which is part of the problem - someone might type-switch on your error and if you need to change it at some point, you can't even check your callers and their invariants might just break. Opaque errors specifically disallow breaking, to prevent people from depending on implementation details you don't specifically want to commit to. Like, the `os` package returns `*os.PathError`s, but that's not actually part of any type-signatures, it's just part of the comments. Which means it's still part of the API, yes. And that can be fine (in this case, it definitely is). It means you still commit to that API. If you genuinely *don't* make any guarantees about what errors you can return, there is no commitment, though. If there is no way for a caller to distinguish errors, they can't depend on them. &gt; You can still return opaque errors even if they're typed. Just use them to indicate the source. Like RequestError could be a HTTPError or a gRPCError. FTR, that's not an opaque error, in my book. But: What if you at some point decide to no longer use the network at all? Yes, you *might* be willing to have the notion of a "request" part of your API. You might also be willing to have using HTTP part of your API. But it should be a deliberate choice, not the default. In general, you won't know in advance what sort of errors your users will want to disambiguate and you won't know in advance what sort of implementation changes you'll want to do in the future. Going from "I make absolutely no promises about the errors I return" to "I return an HTTPError, if an HTTP request fails" is a backwards compatible change - but not the other way around. That's all I'm saying, it makes sense to keep your options open in the beginning and not overcommit to implementation details, until you have at least an idea what errors are interesting and what aren't. You can always add more details later, *if they are needed*, but you can't go back on them. &gt; They might still be frustrated by not knowing why there was a request error but at least they know it wasn't a parse error without having to parse a string. They might be frustrated by my API not giving them a way to pass in channels. Or not having cancellation. Or any other API decision. The solution to any of these frustrations isn't to "parse a string" (you should never inspect error strings, they should be opaque), the solution is to file an issue so I can decide whether I want to expose more details as part of the API or not. Error details are not different from any other API decision - and API design is always going to be a negotiation and tradeoff between clean abstraction and detailed power.
I'd probably just declare it twice and reduce your logical coupling of two different models at the cost of a little bit of duplication. Such a move also lets your notion of starters evolve separately, as two very disparate things will naturally want to do.
Good points, thanks!
Sqlite sounds like it would fit.
Thank you for the detailed response.
actually, the tests gives you one implementation, but you re right, the readme is a bit rough and should point that out. Even better with an example
You could use a simple key/value store by the sounds of it for this. Something like [bbolt](https://github.com/etcd-io/bbolt). I've used it before, it's pretty great. Bear in mind, using a file-based database will be more difficult to scale in the future. But you could easily abstract the use of this K/V store and swap the backend to use something like Redis in the future.
Im not saying its the best as I have limited experience, but my first and only experience with databases (and go) is with mongodb. I know go/mongo have official libraries, too.
The “One True Go Way”. My and your opinion doesn’t matter because Google is webscale and if we don’t agree it’s just because we don’t have enough users to have discovered their infinite wisdom.
https://gobyexample.com/collection-functions
I don't need any scaling. This is just for a personal project.
[removed]
What is a philosophy without its followers?
[removed]
Your opinions do matter. I am already playing world's smallest violin just for you.
Thank you.
Being dismissive of people's experiences by calling them kool aid drinkers unfortunately doesn't invalidate those experiences so the drivel goes on. I'm not part of a cult, there's no one true way, Go just happened to get a lot of things right that made me very productive in a lot of my endeavours and some of these changes can have drastic effect so I, and others, worry.
Google is going to do what they want, so why worry.
A loop and a map. Or without the map in exchange for the additional time to sort the arrays. (Did you mean slices?)
Well, _what does he know anyway_...
One way is to get rid of the declaration whatsoever. func Foo(starter func()) Another is to _not_ make it a named type. func Foo(starter interface{ Start() })
You didn't waste your time. Every project is an opportunity to learn more. You gained new skills and more experience in your development of this project. You might never go back to it but your future work will be that much better because of it.
PM me when you get started and I may help you a little bit!
What I mean is that constructive criticism is better than ad hominem.
The constructive criticism has been hashed and rehashed both on reddit and elsewhere. You seem to be caught up on the use of an adjective and are missing the larger point that there are a number of people that don't believe there is a need for Go 2.
This is noobish but what is an error? Is it like something that gets flagged based on conditions inside the core library? I was thinking of this yesterday when I seen someone in php put together like 50 arrays in one page and I was like "you are not gonna check anything?" And they got really mad at me and said like what? Which made me think, what are errors? Or even in go what are they?
I'm well aware of the debate as I've been involved myself. However, dropping it in favour of shit-slinging is only likely to entrench both sides further and serves no purpose other than pissing people off.
I think you're taking my 5 second, low effort reddit comments too seriously.
It's what I do.
If you have a package called transport, it could be defined there. Otherwise, define it where it is used not where it is produced. When possible, take interferences as inputs and concrete types as outputs. Everything that is publicly exported is hard coupling in the sense that you promise they will never change.
I will, but first I'm quite busy in my spare time to make a book-writing-application (with GUI) in Go :-)
&gt; The key point here is our programmers are Googlers, they’re not researchers. They’re typically, fairly young, fresh out of school, probably learned Java, maybe learned C or C++, probably learned Python. They’re not capable of understanding a brilliant language but we want to use them to build good software. So, the language that we give them has to be easy for them to understand and easy to adopt. – Rob Pike
Yes, I get it, you're helpless.
Realizing that Google is going to do what they want regardless of what the community wants or needs isn't helplessness, it's enlightenment.
[Relevant XKCD](https://xkcd.com/927/)
**To add a little more detail:** This company are building blockchain applications for clients around the wolrd! The VP of Engineering would be their most crucial hire to date and will be vital in helping them grow, in definging their technical architecture and in leading/growing the current engineering team of 10+. You would ideally have some experience coding in Golang or Scala before, Python is okay too and have some strong experience in building high-performing systems. Of course, knowledge of blockchain is very much needed here!
Focusing on what others can and what you can't isn't enlightened.
GCC includes an open source Go compiler to avoid the "one true compiler" problem.
But who defines the language specification?
Wonder how this compares to golangci-lint
The essence of an error is reading a file, but then the file is gone. Or allocating memory, but then there is no free memory to allocate. Every time external factors may cause a function call that normally works fine, go wrong. It may not be the definition of an error in the context of Go, but I believe it is close.
I’m always worried about sync.Pool for security wise operations. If you don’t properly clear it out, you’re liable to have data left over in it for the next run, which can lead to all kinds of security issues. Often, the performance benefit just isn’t worth the potential security issues.
It would be great if there was an interface that defined a Cleanup method for performing cleanup on objects when they are returned to the pool. Maybe the interface could be optional?
I think it's too hard for abuse it. For getting allocated and not cleared memory in other app you must have root and know which app you are attack.
“The nice thing about standards is that you have so many to choose from.” - Andrew Tanenbaum Computer Networks, 2nd ed., p. 254.
Why so? It's not a new standard right? The rules are same, and it claims to be faster 😕
I barely escaped the _Javascript Lint Standards War_ with my life. Get out. We don't need this war-mongering here.
Cool, well bbolt on it's own would be a great fit then.
In the go memory model there aren't really any memory clearing guarantees at all, so doesn't matter if it's a sync pool or something else, Go just isn't great for operations that are sensitive about what's in its memory. It makes copies of items and shuffles them around in memory transparently, which is awesome in terms of "managed memory" convenience, but isn't great for knowing exactly what is where
Really glad to see ownCloud embracing Go. They have an important product that can really benefit from the easier setup (Embedded databases, built-in webserver, single binary, etc...) and lower hardware requirements of a Go port.
Eh, a new struct should be initialized to its zero value. So there’s at least some guarantees.
This, rsync over ssh and a simple fsnotify wrapper. If I had written this as an interview exam, that's what I would've been shooting for: moderate effort, displays knowledge of tools, don't reinvent wheels. Still, would be polite to provide feedback.
Let's say it's a large set. Which would use more memory and which would be more CPU intensive between your suggestion and double looping over a pair of slices?
golangci-lint is similar to gometalinter - it's a tool which invokes a bunch of binaries with third-party static checks. revive is a static analysis framework with over 50 built-in rules (including the ones from golint). It does structural AST sharing between rules, which makes it run fast. It also runs the individual rules in separate goroutines which speeds everything up additionally. &amp;#x200B; Taking this further, golangci-lint could run revive internally as one of its third-party linters. See this issue [https://github.com/golangci/golangci-lint/issues/238](https://github.com/golangci/golangci-lint/issues/238).
Reading through the README it looks like a configuration file is required, even for what you list as the default config. Why not have the default configuration be what is the default without a configuration file?
It really would be. This is something often not thought about by api authors that often has to be shoehorned back into a project at a future date.
&gt; Revive provides more rules compared to golint. And more to the point, future changes to golint rules are not guaranteed to be reflected in this project.
I'm in exactly the same boat. Working in large(ish) projects/files using go modules in VSCode completely wrecked my programming ux, so I took the nuclear option: went to vim.
A slice and a map use roughly comparable amounts of memory, for most purposes. You need a map on the side where you want to look up "hey do you contain X", slices can't do that cheaply. Alternatively for a union, you can merge sort two slices. But if this seems tricky, that's overly complex for no real reason. tl;dr Sets are spelled `map[T]struct{}` in Go.
Not enough to get it right the first time?-)
Vim is not something I could switch to, I admire your bravery! The go plugin is made by fatih so I'm sure it's top notch. I wouldn't mind not having documentation in code, I always have godoc open as well, but lack of things like autocomplete and peek definition would really kill my productivity.
You can pass values to your build. `var major, minor, patch string` `$ go build -ldflags="-X main.major=3 -X main.minor=7 -X main.patch=2398" -o exe &amp;&amp; ./exe` `version: 3-7_2398`
To use the default configuration (which matches golint), you don't need a config file.
Yeah, I understand that I can do it that way. That works fine for when I build the program locally. My goal is for others to be able to go get the program and have the version built in already. Is this possible?
So far the project has been introducing upstream fixes, but yes, there's no 100% guarantee. Here's a blog post which discusses the philosophy of the project [https://blog.mgechev.com/2018/05/28/revive-golang-golint-linter/](https://blog.mgechev.com/2018/05/28/revive-golang-golint-linter/)
AFAIK, not without hard-coding the version into the source. The common alternative is generally to make use of different package managers for distributed your built binary (e.g. the various Linux distro managers, homebew/macports for macOS, chocolatey/scoop for Windows). GoReleaser is a great tool to help streamline the overall build/release process and has degrees of integration with some of these package managers (among many other features and useful service integrations): https://goreleaser.com/customization/
Of course. &amp;#x200B; You have re-discovered why so many CLI tools support a -v flag, that outputs the version information. &amp;#x200B; Add a command to your CLI tool that outputs the version information. You inject that information into your program at build time with the -ldflags option, as /u/AllThingsWiseWndrful above pointed out. &amp;#x200B; For an example, [check this tutorial](https://blog.alexellis.io/inject-build-time-vars-golang/).
yeah I seem to have been unclear. I understand that I can build my binary using flags and give it the version. &amp;#x200B; What I want is for people to be able to use "go get" and have the version installed automatically.
looks pretty nice!
Heh, I hear you, though I feel the lag in VSCode was killing my productivity more than not having autocomplete. The biggest time sink in switching to vim was getting it configured _juuuuuuuust_ the right way, but I feel reasonably fast on it now.
This isn't possible unless the source file has its version string updated. You might be able to automate this by having some kind of github action or CI action that will rewrite a version source file every time you push a semver tag.
SQL will take you about a week to learn what you need. Just do it!
Hard code it in your source. That's the only way for it to work with go get, and it's not really a big deal, is it? makes it easy to see in the code what version you're at. You do have to remember to update it, though.
Very good! I was looking for such tool for some fast manual dirty tests :)
Impressive
I'd call it introducing a soft coupling. Because you don't have to "declare" conformance to an interface when creating a struct, putting an interface somewhere in your app doesn't preclude more structs conforming to that interface somewhere else, or another package also declaring something that happens to be the same (and the two things still "just work" together), etc. Declaring it in some package does nothing to prevent other packages for doing whatever they want, be it importing that interface or declaring their own copy of it, etc. So while you may have some local concerns around readability or documentation, it's not worth stressing out *too* much about, because it can't mess you up that much.
Wow that’s an insane bug. Kudos to the guys for tracking that one down. One curiosity tho, their concurrency algorithm. I’ve seen the semaphore method before, and it’s kinda clever, but I always considered it to be really slow as it would start a new goroutine. It obviously bit them here. My question is about their fix. They have an array of goroutine channels, and send the “work” to one channel selected by taking modulo of time and the number of workers. Is that actually faster than using one single channel and having multiple goroutines reading from it, and allowing the runtime to pick the first available goroutine? I would imagine selecting a specific worker with a modulo on time could end up leaving goroutines sitting idle while the other one waits to finish if the feeder is a single (or small pool) of threads. Am I doing something bad by having multiple goroutines listen to the same channel? I thought they were designed for that.
I think it's a bit of an error in the proposal to use fmt.Errorf. It's not actually part of the proposal to require you to do what is known to be bad practice. I'm in favor of the proposal because it makes good practice easier than it is now.
thank you i am going to look into this
Don't use Go if you care about keeping things out of RAM.
That is what `Reset()` is for. Eg for a buffer implementation, in Reset I would reset the index and overwrite the buffer's bytes (at least before I free the memory), at least if it’s a security issue
Overwrite it? Freeing it doesn’t zero it in any language
Take one of the examples in the post. You respond with a JSON object. Imagine the first request has a attribute like “email”. It responds normally and resets it back to the pool. But, perhaps email was added later and they forget to clear “email”. On the next request, the response doesn’t set email, expecting it to be omitted because it’s empty. Except now it includes the original email. It’s very easy to make a mistake like that, or far worse. Sync pool is a great tool but needs to be used with caution.
&gt;golangci-lint is similar to gometalinter - it's a tool which invokes a bunch of binaries with third-party static checks. Not really. golangci-lint is doing more or less the same thing as revive: [https://github.com/golangci/golangci-lint#internals](https://github.com/golangci/golangci-lint#internals)
In a system library I would assume they’ve done it right. But I wouldn’t assume a third party library has correctly reset the structure. I’m not saying it’s an issue always, only that it can introduce issues much more easily. Allocating an object often isn’t the biggest cause of performance slowdown in an application.
I now understand what you mean. There are a few ways to do this, and probably others I don't know about. &amp;#x200B; * One is to just set a version value in your source. This has the advantage of being based on a versioning scheme of your choosing and not just a commit hash. * Alternatively you could include a version file that is populated via a git commit hook and read by your "-v" command (or injected directly into source). * Another option is to [provide a makefile](https://sahilm.com/makefiles-for-golang/) for your project and have people use "make" instead of "go build" for your tool. Philosophically speaking, this is exactly what make is for; to ensure a pile of code is built the same way and with all desired 'extra steps' on different machines. * If you're feeling modern, go 1.11 has shiny new module support. Modules [include versioning](https://github.com/golang/go/wiki/Modules), which you could then add a CLI option to just cat the file (basically).
Then maybe you shouldn’t use such structs with a pool. I mean, it’s not like: Use a pool, get better performance
Start with node if you know JavaScript, it is kind of the same thing, same syntax and all. At least you won’t have to deal with new concepts like strict typing and the lack of coercion
Exactly my point. It can easily introduce security issues. You get better performance but at the cost of potential security issues.
Great. Thanks.
How does this play with recent changes to `go vet`? Starting with version 1.12, it looks like the team is trying to position `go vet` as a platform for analysis. &amp;#x200B; [https://golang.org/doc/go1.12#vet](https://golang.org/doc/go1.12#vet)
But isn't that a good thing? I remember learning typescript for the first time (after having worked with JS). I found handling errors much easier in TS.
Go is such a nice language. You will like it :) To practice, try to do the tipical rest endpoint: ```http POST /messages GET /messages GET /messages/{id} PATCH /messages/{id} DELETE /messages/{id} ``` Just a normal back-end service to work with messages (a message has an auto-generated ID and text content). Maybe store the data in a Postgres database. Postgres is a SQL database; you will need to know at least that for back-end development. Good luck.
Arw how cool. I spent my internship (last week) writing a Wireshark plugin in C. I don't know C. At all. Wasn't fun.
Go routines are a smarty pants name for green thread implementation. [https://en.wikipedia.org/wiki/Green\_threads](https://en.wikipedia.org/wiki/Green_threads) Before Go, Erlang as well known for this kind of thing and still is. Once you get the basic concept you can dive into some Go specifics. Depending on requirements pure OS threads can be more efficient than green threads. That's the reason why languages like C++ do not come with green thread baked in and introduce them as library.
I have two comments. First I think it's a bad idea for a library to panic, which your constructor does if you pass a max concurrency value less than 1. There must be a reasonable default you can use or return an error. Panic seems excessive. Then, I think you should document and emphasize the importance of calling the Wait() method since that is was actually cleans up the runner goroutines. If someone were to use your library to simply run bounded concurrent function and not call Wait(), they would leak the goroutines. In general when I have seen these wrappers for bounded parallel runners I always wonder if they provide enough value over a simple semaphore pattern and a Waitgroup. With those two tools you end up with the same bounded parallelism but without hidden goroutine management.
Thanks a lot i'll try this out. I was planning to learn mongoDB or MySQL. Is PostgreSQL a better/easier option as a database?
Go Is getting to be super popular. The original creator of Node actually stopped working on Node and walked away from the community to develop in Go instead (he’s now back working on Deno that promises to solve a lot of the Node issues). It gives you a lot more control and a lot more performance. I chose go over Node for new projects because it’s a lot more versatile. You can even use go on the browser via web assembly via a compiler. It’s great for devops tooling, as docker/kubernetes is written in it. The only thing that has brought me away from go is machine learning, so I’ve had to start writing python for that. Yet, I come back to go for everything else. So, I would learn it again if I went back in time. It’s well worth it. There’s a bunch of strange gotchas, like the way slices work with append. But generally it’s pretty robust and well designed.
Go is great for backend APIs. Plenty of tutorials exist. Another option is C#/dotnet core. It's more used in industry, and is more mature. It is though sprinkled with 'magic-dust' that hides a lot of the mechanics of what is going on. Especially if hooking up to DBs. Go is more readable and straight-forward. And easier to learn. Also easier to handle, as in the end you'll end up with a single static binary that don't rely on a lot of various 'must have' dependencies of a certain version that you need keep in sync between dev, test and prod environments. In my projects - which count a few backend APIs by now - the dev-cycle using Go is pretty straight forward. On startup it sucks in a Config json-file containing stuff like public-facing IP, API credentials for client, 3rd party API credentials and then spin up on the local machine with a simple 'go run appName.go' - From there, just point browser or app-client to [127.0.0.1](https://127.0.0.1) and a given port (I tend to use port 5000) and you're good for testing the endpoints. When it all works as intended, build the binary, upload to server, close old version, spin up new version and done. No magic. Server-side got it's own config file in the same relative location, and of course TLS certs from Certbot.
I haven’t used either one of those drivers, but are you sure that’s the right connection string syntax for the mgo driver? A quick google shows a different syntax. Remove the “mongodb://“ from it and it may work. session, err := mgo.Dial("server1.example.com,server2.example.com")
If you are familiar with TS, then you don’t have as much of a learning curve, but there are many other things that work differently on Go than. I think it is great for anyone to know both languages, If what you want to learn backend concept first, then start with a familiar language, then switch to go once you get that part. It would save you a lot of hair pulling while figuring out why you functions aren’t behaving as you think they should.
I think it's misleading to say they're green threads. They're similar to green threads in some ways but the implementation in Go is "greener than green" (ie. more lightweight) and the stack is managed completely differently from any green threads I've seen.
It's coming. See https://github.com/Microsoft/vscode-go#settings-to-control-the-use-of-the-go-language-server
&gt; I think it is great for anyone to know both languages, If what you want to learn back-end concept first, then start with a familiar language, then switch to go once you get that part. Yeah i agree and where I live, Golang job opportunities are pretty limited as compared to languages like C#, nodejs, java etc. &gt; then start with a familiar language This might sound like a dumb question. A lot of time I see nodejs mentioned as alternative to Golang. But isn't nodejs a framework and Javascript the language here? If that is so, whats the corresponding framework when using Go as a language? Sorry but i'm not really clear about some basic stuff here.
Thanks. &gt; There’s a bunch of strange gotchas, like the way slices work with append. I'll keep an eye out for that :)
Mongo stands for other needs than mysql or postgres Postgres is very popular in Go, the driver lib/pq is just great you could try it but at the end of the day mysql can do the job if you stick to standard sql If it's mysql, I would recommend version 5.7 minimum with sql_mode set to "strict_all_tables", so that you learn proper sql (or else very strange requests would be accepted)
[removed]
&gt;In my projects - which count a few backend APIs by now - the dev-cycle using Go is pretty straight forward. On startup it sucks in a Config json-file containing stuff like public-facing IP, API credentials for client, 3rd party API credentials and then spin up on the local machine with a simple 'go run appName.go' - From there, just point browser or app-client to [127.0.0.1](https://127.0.0.1/) and a given port (I tend to use port 5000) and you're good for testing the endpoints. &gt; &gt;When it all works as intended, build the binary, upload to server, close old version, spin up new version and done. No magic. Server-side got it's own config file in the same relative location, and of course TLS certs from Certbot. All this stuff was too advanced to understand for me but I got the crux :)
I just suggested that by popularity. Both MySQL and Postgres are SQL databases; they should work almost the same. MongoDB is a different kind of database. If you are interested in that, go for it ;)
&gt; MongoDB is a different kind of database. If you are interested in that, go for it I have no idea about any of three :D Just going with what I hear through my colleagues in day to day life. But i guess i'll go with MySQL most probably.
Thanks for the explanation. Which one (MySQL or postgresSQL) do you think has a better/easier learning curve? I'm looking to learn this as quickly as possible so that I can start practicing on a personal project.
I would say postgresql Commands to get you started are cleaner, MySql is not always intuitive You could also go deeper with postgres if needed As for actual requests, postgres has a stricter compliance with sql standards, mysql can teach you bad habits
Something you will love about Go, is its nice stardard library and the fact that Go is writen in Go itself. If you are using VSCode with the Go extension, just `Ctrl + Click` over a function and you can look how things work :)
It also struck me as a bit over-engineered. Though they admit it later ("only the paranoid survive").
Go does guarantee that newly allocated memory will be cleared, which is similar to OS guarantees that allocated memory will be lazily zeroed on first use. If you really care about clearing memory prior to giving back up to the runtime/OS, you can use a [finalizer](https://golang.org/pkg/runtime/#SetFinalizer).
We use go.uber.org/zap with a json encoder. We have the ELK stack but I just like using `docker logs | jq`... it's awesome! We're using protobuf and we wrote some code gen to write the zap ObjectEncoders, it works really well.
I re-read the fix and it looks like they do have several goroutines reading. But why not have one big pool. I’m worried that I’m missing something.
Now this is cool :D
To be fair, I've seen the Go community take down Rob Pike for a very reasonable suggestion in the past for how to deal with a lot of the Go boilerplate around errors. Definitely gave me a "this is starting to feel cult-like" vibe.
I am not an expert, but here is what I know JavaScript, technically ecmascript is a language specification that is interpreted by something else. Meaning you can write a piece of code send it to chrome or firefox or safari. And each of these browsers would execute that code. It should mean the same thing everywhere, but that is not always the case. That is why the same price of JavaScript code can execute differently in each browser, usually the weird browser is internet explorer. But at the end of the day, you write the instructions and the execution is handled by the browser Node.js is basically a combination of an execution engine, written in C, that allows JS code to be executed on the sever outside the browser environment, as well as a bunch of additional JavaScript libraries and modules that has functions and such that would allow your code to do things it wouldn’t do in browser such as writing files to the hard drive, etc So when you are building a server to run a node application, you install node on it. Add your own JS files and have that node program execute your code at that time. Go on the other hand is a compiled language, meaning that your write your code, then compile it with the “go compiler” into an executable binary file that contains machine code with all the required machine code to run independently. So you can build a go application on your machine, generate the executable file. Take that file itself to a machine with nothing but an operating system on, and run it directly. So you really don’t need any node equivalent for go, because go programs run themselves while node programs need an execution engine that can read the code and act on it Independent of that, there is a difference in culture between node developers and go developers, where node developers prefer to add in libraries to help them build their apps, go developers prefer to write their apps from scratch. That difference is likely due to the fact that Go was designed from the start for writing efficient backend apps that are run on servers. So the go standard library can act as highly capable webserver while node would need to have a webserver imported into the project such as express. But in the end, you will notice that you tend to write a lot more of your own code in go rather than node.
Node.js is not a framework. It's a tool that allows you to execute JavaScript outside of the browser. Normally, JavaScript runs in a `.js` file you load with `&lt;script src=file.js&gt;&lt;/script&gt;` from your HTML. But with Node.js you can execute that file from the terminal. ````bash node file.js ``` Now, Node.js includes some things that are not present in the browser, like `require('./otherfile.js')` to import other JavaScript files. And a standard library to do native things like working with files, or http connections, etc. The things you need to make a back-end service. In Go, the standard library is so good, you don't need a framework at all.
&gt; Node.js is not a framework. My bad. I guess express js is the framework here? &gt;It's a tool that allows you to execute JavaScript outside of the browser. So this means it provides something like a clone of browser engine (like chrome v8 engine) to run javascript anywhere other than the browser?
Express is not that big as a framework. It doesn't force you to follow a project structure or anything. If I have to say, its just a router. And yes, Node.js uses exactly the v8 engine from Chromium :)
Just coming back to say thank you for your input, it was very helpful and got me going in the right direction.
Just coming back to say thank you for your input, it was very helpful and got me going in the right direction. I ended up going with a map of structs and it worked perfectly (uniqueness was very handy).
Yeah that's fair for new memory. I had gone down a bit of a rabbit hole in the past in terms of trying to keep something like a cryptographic key in memory "safely" with Go, and the tldr of that was that you basically can't, even if you ensure its stack-allocated; if the Go runtime touches it, there's no guarantee.
Just wanted t say thanks for your input here, it was very helpful. I think I just had a hard time getting my head around the concept of pointers etc - python is essentially doing the same thing but hiding the lower level implementation from you so I need that spelling out in slightly more simple terms.
Have a look at this: [https://github.com/TheWildBlue/validator-benchmarks](https://github.com/TheWildBlue/validator-benchmarks)
I think the dep ⇔ go modules debacle has informed the community that their contributions are unpaid and will not be recognised nor compensated. Talented developers should not be working for free for a multi-billion dollar behemoth that pays its California-based employees between 3-20 times what equally talented developers elsewhere in the world earn. It's a good job and a great salary if you can get it, but if you can't then don't be a dumbarse and spend a single second trying to contribute code to that project.
Well green thread remains to broad thing, but the defining trait is that it's a thread, which is spawned by virtual machine OR runtime library (Go). How it's managed is not really the defining thing here. It's hits the same boundaries. Requires a run-time and therefor cannot run in parallel. I think it's important distinction to make, because I saw quite a few people in this subreddit mistaking Go for language which is good for multi core processing, when something like C# is better equipped for that to be frank. However that's besides the point. Yes Go routines have their special kind of magic. I've never denied that. My intention was to point the person who asked the question to the broad term, how in principal it works and when suggested to read on the sources other people provided here for implementation specifics. Thus allowing to form more detailed picture.
Thanks man, this explains a lot. &gt; JavaScript, technically ecmascript is a language specification that is interpreted by something else I always wonder what's the use of keeping a language as interpreted language? Why not make compilers for every language so that they can be used as binaries directly by the hardware? Is it difficult to create a compiler for javascript?
Yea go will perform an escape analysis to see if it should be readable outside of the function and allocate it to the heap. It’s pretty smart. I assume you looked into mlock (Golang syscall.Mlock()) for preventing swapping? It’s OK to be on the heap as long as it’s not swapped, generally speaking. Usually for crypto I leave that up to the crypto packages as they implement things correctly.
I have little concern about this kind of libraries that better than the official libraries. In our startup we are using lots of struct encoding to JSON. If we use Jingo instead of encoding/json, we can improve performance. But in the future Jingo can have a security flaw or critical bug and the company(community) that wrote this library may not fix this issue. In this situation, we can turn back to encoding/json but that means lots of work and losing time instead of working for the project or we can try to solve this issue and again losing time. So, for a little (or more) performance improvement can costs lots of time in the future. But if we use official library, there will be no performance improvement and everything will be same after 10 years. Also we don't have to worry about issues about encoding/json.
...I never mentioned OS threads at all.
The biggest difference that you will find between Go and the Node/Javascript frontend ecosystem isn't so much the language as the toolchain. Go's tooling ecosystem is much less fragment and generally quite idiomatic/opinionated. Usually there isn't fifteen different libraries to do the same thing, though as time passes more pop up. Reinventing the wheel is kinda discouraged by the community. This is a good thing in most cases IMO as it makes life much easier when starting a new project. The tooling itself is honestly really good. There are gotchas but on balance, it's a pleasure to work with. Every time I have to work with the node/js ecosystem (we have two react apps for FE at work) I pretty much just want to die and long for the simplicity and predictability of Go codebases.
On the contrary, sync.Pool seems like an easy api to drop for Go2. No matter what your problem domain, you can almost always find a vastly superior approach. sync.Map likewise seems like a no-brainer to drop for Go2 and I often wonder who (and why) these weak apis were ever introduced into Go
Unmatched quotes in the command. There’s a single quote at the end instead of an escaped double quote.
Oh. Sorry, was not ment to mean you as recipient of the message, but 'you' as some person. Not a native speaker. Mannerism of native language sometimes dictates the expressions over english when I'm not thinking about my grammar. My bad.
This. Use the native http to make http requests.
I'm not seeing that. The single quote at the end matches to the single quote beginning the Content-Type argument, unless I'm missing something
The Go community is exactly at the right age and level of adoption for Stockholm-syndrome attitudes like this. People feel like they can't critique the status quo or they will derail progress and we'll all be stuck with Java. You want generics. You do. You really do. Because once the tide of hype has receded, it will become clear that generics are the table stakes for very large projects. Go becomes incredibly inelegant and unwieldy even at the 20k or so LOC count...I can't even imagine anyone building 100k LOC projects with the language as-is. If Go can't keep up with expectations, people will outgrow it and move on to a tool that grows. I actually think C# could end up eating some of Go's market if Msft can do more to bring it to non-Windows platforms.
\&gt; *Instead of using* [*errors.New*](https://errors.New)*()* *or* *fmt.Errorf()**, one can define a custom type* In theory, but in practice no one likes introspecting to determine if the Error they got back is the SDK version or your variant. This is why you don't see much use of \`pkg/errors\` in the wild.
Many goroutines reading from a single channel can cause lock contention in very high throughput systems as selecting on a channel involves taking a lock. Having a pool of channels and goroutines reduces this contention. I only skimmed through it and not sure if that was the case they faced though.
There's a lot wrong with this that makes it painful to read, and hence difficult to answer: * All the ugly string concatenation - you can write multiline strings as a string literal using backticks, this will also let you avoid all the quote escaping. * Creating the command as a string, then running it as a command argument to a shell invocation, instead of just executing the command directly, passing its arguments as parameters * Using curl instead of the native HTTP client in the stdlib
Yes, what I learned along the way was that Go will also move the stacks themselves, which is kinda interesting. I indirectly looked at mlock because I was looking at GitHub.com/awnumar/memguard which uses that. That library is well written imo, but to the best of my knowledge, it only delivers the guarantee about keeping the key safe under the condition that the user uses the `NewImmutableRandom` function. That is, if the random bytes (that will be used as the key) are generated in the memory page directly, and not "imported" from a go variable/parameter/etc. None of the go stdlib crypto packages will provide guarantees about key safety either, again to the best of my knowledge.
You’re right, the formatting is off. Either way, http is a better option.
Ah ok. That makes sense then. Thank you.
&gt; None of the go stdlib crypto packages will provide guarantees about key safety either, again to the best of my knowledge. If that’s true, that’s quite concerning. I’m going to need to look into that.
We log to standard out then use Docker to ship to instance syslog, then syslog to final destination.
"Requires a run-time and therefor cannot run in parallel," The consequent doesn't follow. Go's goroutine implementation is a counter-example. The term M:N refers to the way goroutines are multiplexed onto (multiple) is threads.
Use an HTTP client library. Calling out to shell commands is error-prone—as you’ve discovered—and almost impossible to do securely if you’re using external data. The “title” parameter you’re adding into your command string is an exploit just waiting to happen.
I think this was made for you [https://mholt.github.io/curl-to-go/](https://mholt.github.io/curl-to-go/)
There are some comments here just saying **use the http client in the standard library**, and this advice is almost certainly correct. I'm just wondering if you understand why this would be a better idea because no one has really addressed that? Before I get to that it may be that there is actually some specific reason you want to shell out to curl instead of using the standard library, so if that is the case please ignore the following but here's some answers from the top of my head: * the standard library has a very good, and powerful http client, although I will admit the API isn't perhaps the most user friendly * shelling out to curl means a lot of awkward and messy string concatenation - as you've found it's super easy to make a very hard to find mistake. * the curl binary will not exist on every machine meaning your code is really only sure to work locally for you. * you also look like you are relying on the presence of bash in \`/bin/bash\` which again is very machine dependent - trying to run any of this on a Windows machine would be challenging for example. * the summary of the above points is that by writing your command like this, you are losing one of the joys of Go in that it has such good support for writing your code once, and having it compile for a bunch of different OSs and architectures. There are many more reasons than that, but I'm not interested in piling on just trying to give some suggestions. Finally I didn't see any examples of how you might do this in Go using the standard library. I created this little example which should give you a pointer: [https://play.golang.com/p/r49iGtKfA8s](https://play.golang.com/p/r49iGtKfA8s) You'll note I'm using a funky \`map\[string\]interface{}\` type to build the request. This is fine for something quick and dirty but if you were doing this a lot you would probably want to create some some concrete types for your request objects, which would give you even more type safety. It is a bit more verbose than the simple curl approach but here we don't have to worry about missing quotes - in fact the compiler will do a bunch of work to help you figure out where any mistakes are. Note the above link will not run on the playground, partly because I think POST requests are disabled, but also because it is trying to POST to a localhost address, and I think the code shown will compile, but I make no guarantees beyond that. good luck
Not true, golangci-lint's linters are built in. What linters aren't native are pulled in as libraries. The linters load the AST once and share it. [Here](https://github.com/golangci/golangci-lint/tree/master/pkg/golinters) is the relevant code.
Thanks for the clarification :)
In fact if I remember correctly, there was a commit recently that updated inaccurate documentation in one of the cipher implementations about that exact issue. The docs used to say that it "wiped the key" or something along those lines, it was from years ago, and it was recently updated
Was anyone complaining about the speed of golint? Just curious
When you compile a program, you have to include all the mechanisms that makes it work inside its body. Which results in a large sized program. So a browser can be 100-200 MBs. Interpreted language allows you to use relatively small scripts that can be executed by the compiled browser which the user have to download only once. If you try to compile every webpage down to machine code independent of anything, you will end up with each webpage being 100-200 MBs. Instead you send a small script over the internet and the browser will compile it just in time. When we are not talking about browsers, this advantage holds up when you have one execution engine, like node or python, and then you can have each specific program as a small easily editable script, the interpreter uses what is called JIT (just in time compilation) to compile only the part of the program it needs, this takes much less ram and resources to run than app especially if you are running many different apps on the same server. If you are building a server that only does one or few functions thousands or millions of times. You would be better off with a compiled app. The app will be bigger, takes longer to start but once it starts it is much faster. But if you have a server that needs to perform many different tasks, such as serving several different websites, then you would be better off with one node core running and executing whichever script is needed at the time. One of the great things about Go is that it compiles much faster than the older languages such as C++ or java. So a programmer can go through several iteration of his app quickly, instead of waiting minutes to hours for each try. That is another important feature of interpreted languages, you can change your script any time by editing a few lines and run it. Another big difference is typing,compiled programs need to no in advance what type of data will be given to them, if they get the wrong thing they will panic and crash. So the compiled languages are typically statically typed. But with interpreted language, the interpreter core can compile differently depending on what type of data it has at hand as it doing JIT
This is a great analysis. One quibble, although irrelevant to the analysis: The article claims the runtime never shrinks the stack, but it sometimes does. The goroutine does not need to exit to have its stack shrunk if it has grown large but not continued to need the deep stack.
I'm unclear on why you assume it has fewer built-in rules that golangci-lint when golangci-lint has 31 _types_ of linters built into it. I'm sure the average number of "rules" per linter is well over 2. I'm sorry you put all this work in thinking that you're the first and then you only find out at the end that there's actually two or three other existing, mature products in that space. Been there. Sucks. I'm being totally serious. It's a bummer. But it's not going to be fixed by trying to downplay those projects.
Yup, the new Analyzer API seems to be the first-party solution to this common community solution: [https://godoc.org/golang.org/x/tools/go/analysis#hdr-Analyzer](https://godoc.org/golang.org/x/tools/go/analysis#hdr-Analyzer)
that makes sense, thanks
I would suggest playing around with MongoDB and PostgreSQL. They have different data models. It will be nice for you to be able to store your data well in the future whether it's a document or relational model.
Oh, it is really sad that you'd say or even think something like this :) Maybe you've noticed that I've been doing open source quite actively over the past a couple of years. I completely respect all the effort that people have put into building open source tools for other developers, making them freely available for everyone. Such people inspire with the ideas built into their projects, learn, and teach, and motivate others to do the same. It is really upsetting to read comments which ignore all this and look into open source as an opportunity to divide people. Maybe I didn't express myself well enough. I was not referring to composition of existing, third-party linters. Depending on the level of abstraction you're looking at it might be irrelevant, but there's a different approach in both tools. Revive implements the rules natively, as part of the linter without pulling third-party libraries. Looks like the architectures of both tools are different and that's completely fine :). Regarding the comment "only find out at the end that there's actually two or three other existing, mature products in that space", please, check when the projects have been pushed to GitHub for the first time: * revive (July 2017) * golangci-lint's repo (May 2018) Again, if people find that golint, golangci-lint, gometalinter, or any other tool is more appropriate for their use case, they should definitely go with it! :)
Ill give it a try. I've been using this one for a while and its also pretty good and also an electron application. https://github.com/uw-labs/bloomrpc
here's the tl;dr to save you a click: lots of tooling doesn't really support modules very well, but this workaround seems to work pretty well for me: 1. Put your project in `GOPATH` in the correct location defined by the `go.mod` file. If your `module` line specifies `foo.com/bar/baz`, you need to put your project in `$GOPATH/src/foo.com/bar/baz`. It's the `module` line that matters, not the URL to the actual project. 1. Use `GOMODULE111=on` when you're dealing with dependencies. By default, when you're inside of `GOPATH`, modules will be disabled. You need to manually enable them in order to add and update dependencies with `go mod ...` subcommands, or `go get`. These dependencies are what our builds use, and will be deterministic. 1. One catch: when you alter dependencies with modules, you must run `go mod vendor`, still with `GOMODULE111=on`. This will save your dependencies to `./vendor`, which will allow non-module-aware tooling to function.
I have two recommendations from someone who started with go about 2 years ago. Get The Go Programming langauge book and work through the whole thing, then work with go for a little on something, then go find The Ultimate Go Programming course by Bill Kennedy and go through the WHOLE thing. Also while you're doing that listen to the Go Time podcast, check out @francesc JustForFunc youtube channel, and do some gophercises from @joncalhoun These were all the best things I found to successfully animorph into a gopher.
Go is simple the same way C is simple, there isn't that much to the language and very little if any syntactic sugar. So language can be learnt very quickly, but you might need to write some more code instead of relying on magic methods and syntactic sugar. For example, in today's popular scripting languages you can reverse a list/array by simply calling a reverse method on it. In go there is no generic reverse (unless something just got added that I missed) so you would have to write the code to reverse the list yourself. You do get more exposed what exactly happens when you reverse the list and the cpu/memory cost to do so.
The specifically say that it's not technically true, but that it's very rare.
I got really excited for a half second there about the go.mod option. Unfortunately the go.mod file doesn't explicity contain the version of the module. Only the versions of the dependencies. The makefile idea is pretty good but ideally I would want the user to not have any other step other than "go get". Perhaps I will try the git commit hook. Thanks for your answer.
The question is, why not improve the code in the code standard library? I find that when alternatives pop up left and right things get splintered and looses focus. Simple seizes to be.
Does it support using the ServerReflection so you don't have to upload the .proto file? I think that's one of the most useful aspects of grpc_cli for me.
Or they can also use limiter package instead of using a channel, wouldn't that work?
If you're worried about that, your options are very limited. It isn't just Go with that problem. Pretty much every high-level language makes it extremely easy for library authors to accidentally copy something. Even languages that may nominally have that support like Rust still have to be used 100% correctly. And C++, for example, also nominally can do that but is even easier for an accidental copy to come in, possibly even depending on optimization level. A lot of times it just isn't practical to be worried about that, and you're better off spending your time securing your system in general. In most cases, "the attacker is able to read arbitrary RAM' is an "you've already lost" situation anyhow.
Excellent article!
perhaps *THE* most relevant xkcd in open source community.
I am not surprised, judging from how they do versioning and releases. Its a mess.
Need to elaborate a little more on what you're doing. &amp;#x200B; It all comes down on what the code is doing.
&gt; Adding compression also involves changing the repo format, which takes great care and time (which we don't have right now, there are other, more pressing issues to solve). In my opinion, for a backup program, repository format stability and compatibility are much more important than adding features :) Fair enough and I really do respect what you're saying. I guess I just came out of the issue thread thinking that a go implementation of zstd is what was holding the implementation back.
In this case, it seems difficult or impossible to use the techniques they've used to speed up encoding while maintaining (exactly) the standard lib API. It also looks rather complicated, which might be a blocker as well. Most of the stdlib code is pretty straightforward.
On the panic, maybe. This is a precondition violation, which is perhaps one place where a panic is acceptable. The runtime and standard library panic on, for example, index out of bounds. That said, there might be a way to limit the interface to make the check unnecessary. You're right that the Wait is absolutely required, and the documentation could be improved. There is a good example of limited concurrency in Go by example. It's not that much code. However, when you add a context with cancellation or a deadline, it's probably too much for copy paste. It's true about hidden goroutines, but another word might be encapsulation. In the end, it will be context dependent whether the dependency is worthwhile. Thanks for the input.
Not yet, but it's definitely on our list of to-dos. it's a tough one.
By parallel I mean multi core processing here. Although perhaps not a clear expression. The issue with runtime requirement is that the runtime it self runs on one core. That still doesn't change the fact that Go doesn't do work in parallel. It's concurrent. Meaning the program doesn't idle. In essence it solves same problem as Node's event loop, but in a different way and arguably more efficiently.
I just switched on module support and the only thing that I'm not getting is outlining. I'm using gopls, with language server support, with the default things enabled and completion enabled. I'm using goimports for autoformat and import resolution.
I'll give this a go, thanks
This is correct - the library has been designed solely for performance reasons and there are a couple of trade offs to go with that. You just need to be able to justify those trade offs fit with your scenario. For general use the stdlib will always be a good option. If you need to increase your throughput or reduce your compute time then it may be worth a look.
What the fuck is this atrocious clusterfuck of a website on mobile? I see nothing relevant to what I am actually clicking.
Interesting, so you're using the standard Docker log driver? So basically write to stdout
While I agree in general, the library writers should care about that. There are absolutely circumstances where you don’t want the system swapping out keys to disk, or any cryptographic material. Usually it’s a single call on the memory chunk, and it’s safe. If you’re using all of their primitives, it should never be copied to an unsafe place. A consumer may do something bad, but inside the libraries they shouldn’t, which is how most people will use it. Generally I don’t worry about it as in my cases, it’s really a non issue. But there are cases where it is, and they should be handled at the library level.
Is the json encoding performed by the monitored app ? I noticed that json encoding is time consuming. This is a task I would offload from the app, unless its overhead is insignificant.
But isn't this the purpose of free software licenses? That you have the right to modify and redistribute it, so you don't rely on others to fix it. Just do it yourself.
Zap has its own encoder (so yes, encoding is done by the app), however, zap is built for performance... It's certainly much faster than `fmt.Printf`!
Thanks I'll try both
Thanks for the list of resources ... I've never been studying from book kinda guy but I'll try
For sure :) Here: https://github.com/monkeydioude/heyo/tree/feat/add-listen-refacto-needed I made a quick branch to clean some of the mess... Need to work on it again asap. /bin/heyo contains the message broker binary (Heyo), it runs on itself on port 9393 / contains the source of the client "lib" to Send and Listen to Heyo. any .pb.go file is generated by the google/grpc generator, but it is easily readable. Not sure it compiles at the moment. Please tell me if there's any problem, i'd find a real computer to fix things up.
More code is not a problem I guess. My main concern right now is understanding concepts like structures, concurrency, channels, oop(though Go has much more simplified oop concepts I guess) which are totally new to me as I've never worked on any backend language
Sounds like they can't connect at all. Can you check if your services are running? If so please try restarting them.
You are right. I forgot that encoding is simpler and thus lighter than decoding.
Actually Node.js is a runtime environment for javascript. libuv and v8 engine are the core components of Node.js. While v8 engine is used to interpret ecmascript code, libuv maintains the event loop mechanisms to make sure that nodejs works in a non blocking event driven way. So the real magic of nodejs lies with the libuv and event loop. So you can write any ecmascript implementation and it can be run by Node.js. &amp;#x200B; ExpressJS / KoaJS / HapiJS are frameworks so that devs don't have to write so much of boilerplate. If we draw an analogy in comparison to golang, golang have stdlib so that dev don't have to write all the complicated logics of server handling.
I find this idea more liberating that project structure should not be forced. Also its much more than a router. Its a MINIMAL server side framework offering middle-wares and template engines. It would be a request if people learn to understand the meaning of MINIMAL
I don't know about golint, but gometalinter was very slow (well, running the individual linters was). But the golangci lint is quite fast, so I don't feel the need to move again.
Another vote for Postgres. There's actually a commercial version called EnterpriseDB that contributes a fair amount to the project. And has been pointed out, it has a query optimizer; I've seen MySQL/InnoDB do some idiotic things, and it's hard for me to believe that it even considers multiple options. Postgres is also far more compliant with SQL, though nothing implements SQL fully. And on this topic, I'd go with MariaDB over MySQL because Oracle is evil.
TLDR; do your benchmarks on target system, some regressions cannot be avoided
log to a linux named pipe, then a collector reads from that pipe and ships to remote storage. 1. [https://www.komu.engineer/blogs/timescaledb/timescaledb-for-logs](https://www.komu.engineer/blogs/timescaledb/timescaledb-for-logs) 2. [https://apenwarr.ca/log/20190216](https://apenwarr.ca/log/20190216)
I updated the repo with more explanations and an example, let me know if it is clearer and thanks again for your feedback :)
A couple of thoughts. &amp;#x200B; First, every value in Go will conform to \`interface{}\` so defining \`type Job interface{}\` doesn't really achieve anything. A not uncommon pattern that solves this is to define an interface with a no-op private method that your job types implement. eg. &amp;#x200B; \`\`\` type Job interface { job() } &amp;#x200B; type JobTypeA struct { Param int } &amp;#x200B; func (\*JobTypeA) job() {} \`\`\` &amp;#x200B; Second, I would generally avoid spawning a new goroutine for every single request. Instead I would use a pre-spawned, bounded pool of workers and send the jobs to them on a channel. The reason for this is that spawning a goroutine per request introduces a potential for an unbounded amount of work, both CPU and potentially memory. You can very quickly kill your server. &amp;#x200B; eg. \`\`\` func run() { jobs := startWorkers() jobs &lt;- &amp;JobTypeA{Param: 1} close(jobs) } &amp;#x200B; func startWorkers() chan Job { jobs := make(chan Job, MaxWorkers) for i := 0; i &lt; MaxWorkers; i++ { go runWorker(jobs) } } &amp;#x200B; func runWorker(jobs chan Job) { for job := range jobs { switch job := job.(type) { case \*JobTypeA: ... } } } \`\`\` &amp;#x200B; You might also want to introduce a WaitGroup to allow for cleaner shutdown, and a struct to store that state in, but you get the idea.
Well, there's a good chance any limiter's going to be using channels under the hood anyhow, and it'd be a pointless dependency to add considering how simple their solution was (pool of channels + workers)
TLDR; Go 1.11 and lower weren't compiling/linking binaries as it should on Darwin (MacOS) and resulted in a performance improvement Go 1.12 fixed that resulting in a performance loss on Darwin/MacOS only
That is good, I am happy that things are working out for you. :)
I also had the same thing in mind when I started, to use rsync or unison, but I had two reasons to not do so. &amp;#x200B; 1. I know of a few people, who used rsync or unison, but they didn't get selected with this company. 2 I once created a solution using the present library, and the recruiter rejected me saying they wanted to see If I know the underline technology and can create it. &amp;#x200B; I agree with you on this the fact that companies/startup have a to put a lot of efforts in hiring and operating itself, which is why I haven't named the startup yet, I have no intentions to dismiss it. But, I feel the company should tell their candidates exactly what they are looking for, otherwise, why would I reinvent the wheel. :)
Interesting, thanks for the infos!
Deno is now running on Rust
What's GoLang? Do you mean Go?
The no-op is definitely something that I'll implement, thanks! I see gRPC also does this with their generated go code for `oneof` fields. I'm not 100% sure I'll need the worker pool part though, it's not a public web server and the number of requests will be veeeeery low (5 a minute would be an exaggeration). I may implement it in the future but for now, I think I'm going to keep it simple. &amp;#x200B; Thanks!
A) the server should detect the unreasonably high number and reject your request after just a few bytes B) the server should have timeouts in-case you present a high, but reasonable number C) the server should immediately close the connection if you send a stupidly small chunk Chunked encoding is an abomination and waste of resources to do something you should be using another protocol to do.
Considering we're all at least programmers, a go routine is a special kind of userland application thread. There are real OS threads AFAIK, but by maintaining a pool of real thread(s) which just execute the application can bypass the overhead and cost. Go-routines are not free though and there was an excellent post just a few days ago about how using them without thought, you can actually slow down a program. TLDR your CPU has a very specific design and your goroutines should communicate using channels rather than a global mutable state object to ensure each go-routine is not invalidating cache for it's siblings.
That makes sense. &gt; If you are building a server that only does one or few functions thousands or millions of times. You would be better off with a compiled app. The app will be bigger, takes longer to start but once it starts it is much faster. But if you have a server that needs to perform many different tasks, such as serving several different websites, then you would be better off with one node core running and executing whichever script is needed at the time. Can you just give an example of first case (i.e. a server that only does one or few functions thousands or millions of times)
Databases are a good example. They store and retrieve data and they need to do it fast. Another are static webservers who serve pure html or just route traffic to other apps. Like nginx
We're using zap with the json encoder and support both writing to disk and stdout, mainly for compatibility with existing services. As an extra nice property, we have ways that we can trigger debug logging within a context logger dynamically, even in production. If there is a particular user who we know is able to reproduce a problem, we can turn on debug logging just for them rather than over the whole stack.
Oh excellent. Now I have a reason to learn rust.
I would do as the commenter mentions, and use the pool as it’s a nice primitive. But I also wouldn’t have the spawner care what the job is. Instead, use an interface to define what the entry point is for the job, like Job(). Then you can easily add more jobs, and all they need to do is implement the Job() method and it’s ready.
I like where this article is going. If you’re up for some constructive criticism, please read below. The message was often times hard to follow because of the grammatical errors. If you have someone who can proofread and clean it up, please do. If not, find someone. I only comment on this because of how extreme the situation is. In most cases, a few typos don’t take away much from the overall message. This is not one of those situations. Also, I’d like to see more data across more types of JSON. Large, small, nested dicts and lists, different data types within, etc. Also, more random data generation with distribution charts of that data would be great. We all know protobuf is fast, and faster than most serializers, but when and how much? That would be a great insight.
&gt; People feel like they can't critique the status quo or they will derail progress and we'll all be stuck with Java. I critique Go all the time. I did it to one of the core maintainer's face at Gophercon last year, as well as a table of other people as well. Ian Lance Taylor welcomes it. &gt; You want generics. You do. You really do. I use/have used languages with generics for years before Go. What I specifically don't want is other language's generics. I want whatever generics will look like in Go, eventually, but not at the cost of shoehorning them in there to help qualm the mobs. &gt; I can't even imagine anyone building 100k LOC projects with the language as-is. Kubernetes, Docker (moby), etcd, upspin, etc all are over 100k and seem to be doing quite fine. I manage several 20k+ LOC go projects that still feel just fine and "elegant" (I don't give a rats ass about elegance, I care about readability and maintainability, which Go gives me more than any other language I have ever used) &gt; If Go can't keep up with expectations, people will outgrow it and move on to a tool that grows I am so glad Go isn't trying to keep up with the Joneses of people's expectation. That is a battle, up until this point of human history, no one has ever won.
Don't necessarily need a Job type. Could use a chan of func().
Ok. Thank you! I will try to find someone to help me edit future posts.
I'm unclear on why a Job shouldn't be able to execute itself, and why a worker should know about all the things it can run.
[removed]
Have a look at Dgraph. It seems to fit your requirements.
[removed]
Nice article. As a bit of an ES noobie, what are the two ES containers doing for us here? Why a cluster and how do they work together? Is it mostly doing some data partitioning? Which container has the records that were written?
Tarantool.io
The multiple nodes allow the data to be sharded across the different nodes and to provide replication. To answer the question about where the records are written, it really depends on how many shards and replicas you have. So it could be one of them or both (if replication is enabled). &amp;#x200B; If you are interested in elasticsearch, here's a good place to get up to speed with all the terminology that's thrown around: [https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-concepts.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-concepts.html) &amp;#x200B; Replication in more detail is explained here: [https://www.elastic.co/guide/en/elasticsearch/guide/current/replica-shards.html](https://www.elastic.co/guide/en/elasticsearch/guide/current/replica-shards.html)
Thanks!
✅
You could also use ElasticSearch for this.
the common pattern would be to delegate the execution on each type instead og handling it in the worker. &amp;#x200B; `type Job interface {` `Execute()` `}` `type JobTypeA struct {` `Param int` `}` `func (*JobTypeA) Execute() {` `//IMPLEMENT TYPEA SPECIFIC execution` `}` &amp;#x200B; `type Worker struct {` `// worker info if needed` `}` `func (w *Worker) execute (job Job){` `//pre execute tasks` `job.Execute()` `//post execute tasks` `}` &amp;#x200B; `job1 := JobTypeA{Param1: "hello world"}` `worker := &amp;Worker{}` `worker.execute(job1)`
If you want to eventually pipe to a log manager, JSON is likely your best option. That suggests zap or zerolog.
Postgres supports JSONB too and supports indexing on them too.
Seems like a very roundabout way of calling methods
This is a good option as there's built in data sharding and a huge feature set to support different needs as the application scales up.
DNS resolution problems can cause 10+ second delays in communication as timeouts take a while to actually time out. Double/triple check your DNS settings and run test queries using test containers.
&gt; Postgres supports JSONB Neat, I wasn't aware [PostgreSQL supported JSONB searching](https://www.postgresql.org/docs/9.3/functions-json.html). So if I had the following document (stored in the "data" column): {"column1":{"subcolumn1": 'foobarbaz'}} It looks like I could search like this: SELECT * FROM objectstable WHERE json_extract_path_text(object.data,'column1', 'subcolumn1') LIKE 'foobar%'
I was under the impression that ElasicSearch is not designed for use as a database - just as an index. How durable is the storage? (or how is the elasicsearch storage designed?)
If you like mongo as an option, AWS has a new thing called DocumentDB you might want to look into. Basically their version of mongo. Its pretty new though, and I've not used it myself.
this looks awesome! Does it work with both grpc-web implementations?
You can get around modules breaking your tooling by using a vendor folder for local development until those tools are fixed. Try this: * keep your project in the GOPATH * keep modules on auto: \`export GO111MODULE=auto\` * run the following every time deps change: \`GO111MODULE=on go mod vendor\` Also make sure VSCode has \`GO111MODULE=auto\` so that the vendor folder is used.
Well, maybe that's a good point. Here's a relevant article: https://www.elastic.co/guide/en/elasticsearch/guide/current/concurrency-solutions.html As far as the persistent storage, I don't think you have any concerns there. Assuming you can work around concurrency issues or are working only with single documents for write operations, once a change is committed, it's replicated as per however you've configured sharding. If you've got 7 data nodes and have configured it to store every piece of data on at least 3 of them, it will do so. If a node dies, it will redistribute shards so that there are still 3 copies of everything on disk. It is also rack and data center aware if you configure it as such.
I've left out many non-important details. The workers need to execute only those specific jobs and they act very differently based on the job type. Also they need to run in parallel and acquire/release tokens among other things. It's not just method calls
This is pretty cool, but does it update your own .mod and .sum files too? You might not want that, but only see what's new.
You can use dependabot for this.
\`\`\` I've looked at AWS DynamoDB, but the [cost of doing constant scans](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html) (or large queries) for reads and map-reduce jobs seems like it would be prohibitively expensive over a dedicated cluster with this many records. I also can't setup any GIS (indexes) because of the unknown data columns. I was originally considering Cassandra or TiDB, but I don't know the schema of the objects being stored ahead of time and want to remain flexible in what I can consume. \`\`\` Do you think rolling our own database cluster/implementation will be cheaper with labor costs factored in? FWIW - If you're truly just trying to get performance up/cost down, use PostgreSQL. Document stores are rarely worth their effort, which is why DynamoDB is usually an exception.
Although you said you were in a hurry, you might want to check out the bare basics of both mysql and pistgres. The differences between them can work as a key to understanding how rdbms work. As a bad analogy: you are trying to learn how to fix a car, and you got instructions in both swahili and japanese. So in order to be able to do the repairs, you need to learn at least some of either, but you definately should at least check out the images and keep in mind that in the end this is about getting the car to move
In addition to the answer provided, ES can be confirmed to run in a single-node cluster.
Log to stdout (\`log.Println\`), it's the simplest thing that could possibly work. In dev it's perfect, you see the logs. In prod our output goes into systemd's journal, where rsyslog picks it up and ships to a central server. &amp;#x200B; In my experience a lot of logging can be replaced by metrics (Prometheus) or alerts (email/pager/etc). Having to think too hard about logging might be a symptom of overloading the logs with things that aren't really logs.
Regarding Go and Machine Learning: [https://awesome-go.com/#machine-learning](https://awesome-go.com/#machine-learning) [https://github.com/gorgonia/gorgonia](https://github.com/gorgonia/gorgonia) is quite active and interesting project.
[removed]
Just tried it out and it looks promising.
Someone... please... _please_... help me understand. The podcast starts off with everyone praising Go for being small. They then proceed to bag on generics. "We don't need them. It only causes a tiny bit more typing. I don't miss them. They just complicate things." Then they ask what they would add to the language... **and the very first thing** is an addition of a type-safe data structure (`smap`) because casting to and from `interface{}` is a pain. What am I missing? Generics would solve this very problem quite elegantly. Generics would enable programmers to almost never have to wait for the language creators to provide any such data structures. Generics _reduce_ complexity. Change my mind.
Can you explain when and why I would want to do this? Thanks
Indeed. It’s just not quite as mature yet. I’m more interested in Go for running models than training them, so it’s a lower bar.
This was made for the official implementation; I recon it wouldn't be hard to add the lmprobable version if there is demand.
Much of gRPC and HTTP uses this pattern. Very clean and functional pattern using safe high level functions.
Constructors shouldn’t return errors.
Yes. Never ever trust elasticsearch with your primary data. BUT. If it’s a secondary copy that’s easy to search, it can make life a lot easier. I typically use Cassandra for large loads. It has a schema sort of, but you can dynamically change it. You can also do some column tricks to make it more schemaless. Then I have it store updates into elasticsearch for complex search queries that are less frequent, and denormalize into Cassandra for frequent searches. Seems to work pretty well.
I'm elated to find someone filling this gap. Golang makes it easy to update all dependencies, but not to check if updates are needed. I can't wait to replace my hunk of jq and bash with this utility.
I don't know if it's simpler or faster to develop (compared to casbin), but you could implement a lot of auth/access models using open policy agent (https://www.openpolicyagent.org/). It's part of cncf.io. You could create an RBAC system with it (https://www.openpolicyagent.org/docs/v0.10.7/comparison-to-other-systems/), defining roles via web interface and storing role types and roles of users in DB.
We use the "wire" at work. Is there any comparison of this new guy?
I didn't even know there was a semaphore in the /x/ repo. I've just been using a simple channel with contexts for timeout and cancelation and it always works fine for me. For your example, shouldn't you be canceling the timeout context so the resources don't hang around for up to 5 minutes each time?
Why not?
&gt; I'm trying to load balance it Load balancing implies that you're distributing the load across multiple servers. Here you're using a single client, which implies a single server. Could you be a little more specific about what it is you're trying to achieve? There's quite a bit missing from your problem statement. Will you be calling your request handler from multiple goroutines, or serializing your calls to the handler through a single routine? What do you want the calls to look like at the server end? Only a single server? For future reference: http://catb.org/esr/faqs/smart-questions.html
I found it today when re-writing my db sync tool. Yea, i should probably should clean that context up, I usually just use the with cancel function so I didn't think about canceling the timeout.
He expressed it as an absolute, it must be true.
Curious as to why you added the extra setter stage.
look it again
Great collection! Very useful. Some references or implementation examples on each pattern would be nice.
If you click on a type or function on godoc it'll take you to the line in the go source where it's defined. In this case it's [this line](https://golang.org/src/time/tick.go?s=269:371#L1)
https://golang.org/pkg/time/#Ticker It’s not a function. It’s a struct variable that’s a channel. It gets a value emitted on it on a regular interval.
Thank you both. :-) I would've never (or at least not within this month) found it on my own.
Wire and Dihedral both do similar things, but they have very different syntax. Dihedral's syntax is more powerful, especially once subcomponents are ready. I maybe be biased, but I find Dihedral's syntax more pleasurable to use. The other main differences are: &amp;#x200B; Wire requires declarations for every single dependency inside of a set. With Dihedral, it's enough to have a private member of type \`embeds.Inject\` on a struct to have it be provided automatically. Typically, most components can be automatically injected, so this reduces the amount of boiler plate and is better for large code-bases with multiple sets / components / modules. &amp;#x200B; Dihedral's module concept feels nicer than Wire sets. The reason is that modules in Dihedral can declare explicit dependencies on each other, whereas merging of Wire sets needs to done outside of the sets themselves. &amp;#x200B; Each Wire type needs to have its own injector function. Dihedral takes a single interfaces and figures out the types to inject based on the signature of the interface functions. &amp;#x200B; The one thing I like better about Wire is that the provider functions can return errors. In Dihedral, the only option is to panic, which was done to simplify the implementation. I plan on fixing this soon.
The main reason to do this is to clean up the main file and make code more modular. Most Go code have huge main files that can reach thousands of lines long. Usually the main files contain a lot of boiler plate with the ultimate result being the construction of a single struct. With DI, it's much easier to modularize code. When writing a new piece of functionality, one simply declares a module that provides the new dependencies and adds the new type as a field of the final struct. The construction of the new field happens automatically without any boilerplate. &amp;#x200B; For large code bases, DI is extremely helpful. Imaging a large code base that has something like a "UserDAO". The construction of a "UserDAO" is probably complicated, but with DI, any new code can use the DAO just by adding a field of type "UserDAO." It feels magical to be able to pull in additional functionality so easily. &amp;#x200B; The other case where DI is helpful is in stateful applications with many nested scopes. To give an example, a typical Android app looks like this: Application &gt; User Scope &gt; Activity &gt; Fragment &gt; View. DI lets you separate these components so that things in the user scope can access the application components, but cannot access the activity components. This prevents data and memory leaks, and also reduces the number of factories required because you don't need to pass data through every layer.
TiDB is a 3 part architecture: PD the placement driver, TiKV the storage layer, and TiDB the stateless MySQL compatibility layer. You should be able to use TiKV (along with PD) alone for your purposes.
I streamed a Go-programming crash course for everyone interested in learning the language this afternoon, and a bunch of people joined me over on Twitch. &amp;#x200B; It was tons of fun, but certainly also a lot more exhausting than I anticipated... People kept asking great questions though, and all the positive feedback makes me think I'll do a few more streams. This is certainly geared towards Go beginners with prior programming knowledge in other languages. Maybe it helps others here getting started with Go as well. Let me know what you think, any feedback is appreciated!
If latency is a strong concern, you are best off with a dedicated rack server (or an AWS dedicated instance), as otherwise other clients' workloads on the same physical machine can impact your latency. You will also need to make sure your host supports NTP. Needless to say, the storage for your PostgreSQL instance should be on SSD.
I like this a lot! I'm kind of mad I didn't find this earlier
Exactly the kind of advice I was looking for. Thank you. And I really appreciate your caching the NTP part.
Thank you, we are on the path of building Go microservices first time as we started just couple months back. Hence we are still evaluating cool technologies at project basis and I will definitely bring it up for internal review if it would fit better for our scenario. Thanks again, great job!
In general, boolean arguments are poor style. Having named types is just one solution, but there are others: https://8thlight.com/blog/dariusz-pasciak/2015/05/28/alternatives-to-boolean-parameters.html
Is this for consumption by other packages or is it self contained? Generally, I'd prefer to have an `Application` interface with a `GetAccessToken() (string, error)` method. Then, you can have two structs that implement that method: `ProductionApplication` and `SandboxApplication`. Each implementation would handle `GetAccessToken` in their respective way. This way you only have one conditional (which struct to use) and then use it the same way from there on out. If that doesn't match your use case for some reason, a middleground between your other suggestions exists. That's to create a boolean constant that is named in a way that suggests the intent. const IsProduction bool = true const IsSandbox bool = false This allows your invocaions to look like `GetApplicationAccessToken(IsProduction)`, but still switch on them with normal conditionals. Without knowing your code though, I'd highly recommend the interface method. Only way I would go with a non-interface option is if this is literally the _only_ function where behavior differs and I know for sure that that will always be the case.
Very useful, thanks. I was fumbling toward an approach like that but didn't know it.
Very clarifying, thank you. I believe there will in fact be this single entry point (it's for using the eBay API). And while I view it as highly unlikely that there will ever be a third way their API is used, I love the interfacey approach. That absolutely does feel like the Go way to do it. Really appreciate your brain here!
I'm assuming you are doing something to do with sniping, so it's not so much how long it takes things to do than making a request at exactly the right time. NTP will help with that (PTP is even better, but it's pointless if eBay itself doesn't have its clocks aligned to that precision). You should also compensate for the network round-trip time. If the ping time from your server is X milliseconds, you will need to make the call 4*X earlier since a SSL handshake takes a minimum of 4 round trip times, unless you use TLS 1.3 0-RTT: https://blog.cloudflare.com/introducing-0-rtt/ Finally, if one of those handshake packets are lost, your session will time out for at least 3 seconds, so you will need to make sure the packet loss rate between your host and eBay is as close to zero as possible
Only a sith deals in absolutes...
JSONB supports field indexes as well so no it won’t be slow. Read the docs you won’t regret it; it’s really powerful. You can use Redis on with HASH and SET commands to keep an inverted index as well.
Damn, you're good! Yes, we do sniping and I've taken over the codebase. I actually have a patent on dynamically adjusted bid times based on server load--but having that patent is completely useless when set against actual real life back end experience, of which I have precious little. Thanks for the NTP inside info--how do you know so much about it?
P.S. I am trying desperately to avoid an actual physical machine, or at least one that can't be replaced remotely. I know that my competitors are cloud-based. We've been colocated for 20 years and it's just too much pain.
Elasticsearch can be ran as single node. For development that is perfectly fine. For production I would not recommend. In this blog I used 2 for the sake of showcasing a nice example. In proud I would at least run three for HA. Depending on size you can even assign nodes a dedicated role. E.g. Ingest: exposes the rest api. Master: coordinates the cluster. Data: stores data shards. On top of Ingest you could put a load balancer. And give these nodes more CPU for handling requests. Data nodes you could give SSD storage. Etc. Extremely scalable.
You could define the closure before the two invocations and pass it instead of using a literal, so both calls use the same function. I'm just not quite clear on what you mean by 'combine' exactly. Deduplicate or run in parallel?
Amazon has i3.metal instances where you can reserve a whole machine. That way you don't have any noisy neighbors to worry about. ALternatively there are a number of cloud providers that specialize in bare-metal machines on demand, like Packet.com or Vultr.com.
I've experienced enough bugs caused by out of sync clocks that I have a Stratum 1 GPS-disciplined NTP server on my home network (and until recently two, but I am moving). You know what they say about a man who has two clocks...
Reflection is actually really fast in Go.
They basically went beyond marketing and got to the point of spam
A little copying is better than little dependency - https://www.youtube.com/watch?v=PAAkCSZUG1c&amp;t=9m28s
`make` returns a slice, with it's underlying storage (an array) initialised and ready for use. `new` just creates a pointer to a slice, pointing to nothing (as the docs say `*p == nil`).
You are confusing make with new. make just returns the empty zeroed slice \*value\*. new makes a new slice (or map) but gives you the pointer instead.
- what `make` returns? Value of type T, first argument. It could be slice, map, channel or something else. - particular example of `arr := make([]int,10,100)` is doing same as `var _storage [100] ; arr := _storage[:10]` - in all cases you don't really know whether it was allocated on heap or stack or in registers - personally I find it useful to view both new/make as aliases to some special internal code that creates new values, sometimes they happen to be pointers/slices/whatelse. Semantically make is strict superset of new, but doesn't accept all types. It's wierd but it works and we have better things to worry about, right?
Yes it's the good way : [https://yourbasic.org/golang/clear-slice/](https://yourbasic.org/golang/clear-slice/)
Great question! I did some experimentation with slices to explain the difference between \`new\` and \`make\`. I think you will find it very informative! &amp;#x200B; [https://play.golang.org/p/M5TpVlIAWFD](https://play.golang.org/p/M5TpVlIAWFD)
So do I need to set the values to nil in order for the structs they're pointing to, to become collected? &amp;#x200B; Say... `s[0] = &amp;somePerson{Name: "Bob"}` `s = s[:0]` &amp;#x200B; Would bob be removed by garbage collection? Or would being referenced in the underlying array stop him from being collected?
Never had this case, you should have to try I guess. &amp;#x200B; I think (but I am not sure), you have to set nil all structs before reset the slice.
&gt;preallocate by this you mean "make" with sizes?
I came across that, but that's talking about setting the slice itself to nil, the underlying array gets collected. For now I'll just set the values to nil before resetting, but at some point I'll have to do a test.
Oh wow. I didn't see that coming. So basically there is no equivalent to \`alloc\` is Golang? &amp;#x200B; It seems weird for a "low-level" language to handle these stuff. &amp;#x200B; I mean like if I have math calculations with many structs, I rather use value type in the stack, so we won't mess up the heap with fragmentations. So Golang will figure it out by itself? &amp;#x200B; I guess I need to read more about the compilation &amp; runtime process
Oh wow! That read was amazing. Super informative. Maximum knowledge in minimum time. I didn't fully understand the implementation of your functions, but I'm gonna trust you here. &amp;#x200B; You have more of these?
Awesome, I glad you got value out of it! I actually did a talk about this a little while back. You can find the slides, source code and recording here: https://github.com/jasonkeene/monkeying-with-memory/ https://www.youtube.com/watch?v=Cv-F3cHsXiI There are several more examples like what the difference between a nil interface value and a typed nil interface value is. And yea, those helper functions take a bit of work to grok. I think dumping memory like this is super useful though. You can answer questions about the language in a way that you have no uncertainty about what is going on.
Me too
The usual Go way to do that is to make a slice of struct values and then just take pointers to them. It’s a garbage collected language, so it doesn’t require manual memory management but you can fake manual management when you need it.
Just to complete my question, why does "decode" gives me a struct of key-value? My code is as follows: `func GetMonthStatusByID(ctx context.Context, id string) (interface{}, error) {` `var monthStatus interface{}` `filter := bson.M{"_id": id}` `err := db.Collection("Months").FindOne(ctx, filter).Decode(&amp;monthStatus)` `//fmt.Println(count)` `fmt.Println(monthStatus)` `return monthStatus, err` `}`
All I want any of these services to do is just run a damn Docker compose file. I don’t want to learn your custom YAML config. I already know Docker compose and you don’t offer any advantages over it. Just build my test image.
https://github.com/kubernetes/kubernetes has very accessible good first issues so you can start getting your feet wet
Voted! Todd is the reason I can even read golang let alone write it.
Awesome. `go get`ting this is a no-brainer. A tiny wishlist: * Have `--help` output the usage. I am sure tomorrow I'll have forgotten all the required args to pass to `go list`. * Or maybe have go-mod-outdated call `go list -u -m -json all` under the hood, thus saving the users some typing.
Thanks for your reply !
An excellent teacher. He deserves to have some success with Greater Commons.
Oh it doesn't put everything into heap either. Consider it "wants to use stack by default" and fallback to heap when it can't prove stack is safe. But super fast compilation has it's toll on optimizations and occasionally I wouldn't mind to provide some hints to compiler about values lifetime. The bad side is that you don't control it directly in source, can get limited information from `go build -gcflags='-m -m'` and factual proof from `testing.B.ReportAllocs` and/or `go tool pprof` The good side, it "does the right thing" most of the time. I have huge premature optimization issue and pollute code with indirect stack forcing such as copying structs instead of semantically appropriate pointers and "storage array" tricks. Just read whole spec, it's super compact (though cognitively dense) and helpful.
If the slice is still accessible, you can do `s = s[:cap(s)]` to expand it back to its capacity and read the values you sliced out earlier. That's why none of the values in the array can be GC'd.
Seems a bit dead, desn't it? There are PR's and issues left ignored for a couple of months.
Well sure, if you don't synchronize ANY concurrent construct in ANY language, you basically have a race condition (parent context completes before child). The author ends up with a solution that is very heavyweight, loaded up with a bunch of concurrency stuff that will probably result in net performance loss or at least a lot of degradation. He could have probably solved for the 95% case by just putting the dumbest version of the tracker earlier in this function. Using Context with timeouts for something like event logging is overkill and will result in the expected performance to degrade substantially. But in reality, almost all tracking/logging systems work by sampling and it is understood records will be dropped, it usually doesn't matter. If you truly need 100% coverage of events, you write them to a transactional store and you care about the commit. But in most logging/tracking situations, dropping records here and there is fine. At scale, you probably sample like 10% of your events.
Yeah that's why I was initially unsure. Go is too memory safe to let that happen.
Your database conns are timing out...you have a database problem not a Go problem.
Todd’s a great educator, but more importantly he seems like a really great, genuine person. I took his Go course on Udemy awhile back, and some of his non-tech related comments and insights I found very genuine and profound. Would love to grab a beer with the guy one day.
But this doesnt provide any mechanism to detect contradicting transactions right? Replication is really hard to do the right way in my experience
I appreciate the videos. Keep going.
Couldn’t agree more.
Try decoding it into a struct like this: type MongoDoc struct { ID string ‘bson:”_id”’ Status string ‘bson:”status”’ }
&gt;{ ID string ‘bson:”\_id”’ Status string ‘bson:”status”’ } Thanks!! I've tried something like that, but it doesn't work if I name my string values in lowercase... Any idea why?
&gt;Just read whole spec, it's super compact (though cognitively dense) and helpful. You mean [https://golang.org/doc/](https://golang.org/doc/) yeah? &amp;#x200B; Yeah I guess this language really requires some research
Yes , maybe some forks are most updated.
Great videos. Just one suggestion. It would be pretty much more intuitive if you would run the commands using built-in terminal as you won't have to alt+tab each time as it could be distracting. Keep on with the good work!
It does not have a cache built in. Try running a benchmark and you'll see the results of cached vs non-cached are pretty dramatic, although I'm not sure how much it would affect real-world throughput in a larger application that depends on a database.
Yea if you were to use iD rather than ID or status rather than Status I would venture to guess that it wouldn’t work just due to the nature of Go. Lower case var names indicate private variables.
Todd is the man. Absolutely agree that his approach and attitude makes learning anything - not just golang - awesome.
Heres my code I've got so far, the prevFunc gives me the previously declared function which was set using SetInputCapture, and then SetInputCapture called again includes the previously set input capture: `for _, prim := range composite.componentArray {` `prevFunc := prim.GetInputCapture()` `prim.SetInputCapture(func(event *tcell.EventKey) *tcell.EventKey {` `if event.Key() == tcell.KeyDown {` `if composite.selected+1 &lt; len(composite.componentArray) {` `app.SetFocus(composite.componentArray[composite.selected+1])` `composite.selected = composite.selected + 1` `}` `}` `if event.Key() == tcell.KeyUp {` `if composite.selected-1 &gt;= 0 {` `app.SetFocus(composite.componentArray[composite.selected-1])` `composite.selected = composite.selected - 1` `}` `}` `prevFunct := prevFunc` `prevFunct(event)` `return event` `})` `}`
This is cool, I'll have to take a look. How fast are searches? I've used [https://akhenakh.github.io/gozim/](https://akhenakh.github.io/gozim/), but I had to fork it and clean a few things up to get it to do what I wanted.
Totally agree. Still little unclear on certain topics from his course but need to put time/efforts as he says, keep going at it. It's refreshing to see a guy with beer teaching Go.
Lots of good information in the articles you referenced. I like the concept of domain types and generally agree with your interpretation and conclusions. For any project, you're obviously constrained not to have circular package dependencies. Beyond that, I think this is one of those topics which can be very subjective. For example, depending on the project, I wouldn't necessarily make it an over-arching goal to "Code like the Go Team", but I do think it's worth giving strong consideration to the project's development process and the types of developers, QA folks, etc. who will be be working on the project. I think this is also one of those areas which could benefit from thinking of some anti-patterns. For example, I personally find it challenging to work on projects where to fix a single bug or make an enhancement, you're having to make code changes in lots of different packages. An article I wrote about a project I've worked on includes a section on package design, which might be of further interest: [https://medium.com/resultra-bts/software-engineering-for-full-stack-golang-web-applications-3b0ebe4c5e04](https://medium.com/resultra-bts/software-engineering-for-full-stack-golang-web-applications-3b0ebe4c5e04)
Do you mean searches in the full text of the blob contents, or in the "fulltext index" or only URL/title searches?
The "make two versions and name them properly" is my go-to advice for this. The crazy pluggable version seems pretty overkill for a discussion of Boolean parameters lol.
Because I was curious about this for a pet project of my own as well, I wrote a benchmark based on the template documentation example which can be found at https://play.golang.org/p/7Mvav2WXVM_V. It should be run on the local machine because playground will kill it as it runs too long. Results: template 200000 16485 ns/op variable 2000000000 0.00 ns/op Quite a difference. Now I do have to say that I don't know, and am not able to check, if the code actually does the same. Writing to StdOut appears to have no buffer, while the variable one obviously does.
Up to the user to implement.
His attitude kept me from going insane while leaning to code. His constant reassuring that it isn't all supposed to make perfect sense immediately really helped steer away from the discouraging idea that I had no clue how anything worked.
Basically a fulltext search for a given term, what is the "fulltext index"?
This is currently not supported, only prefix searches on the URL and title of a Directory Entry. The fulltext index is a xapian file often added inside the ZIM file in Namespace 'X'. I think there are no pure Go bindings for xapian so I skipped this for now. Also Go has no crossplattform dll loader... Maybe I will implement a fulltext search in future. The prefix search is not bad though and adds a suggestion list with similiar prefixes.
Hi guys i was just browsing though the negative comments on his course deciding if was good or not and they scared me, so can i have your honest opinion, like pros and cons please? Thanks!
What particular negative comments? I'll take a look at them and give you my view on them if you let me know where they are located. Spoiler - I'm a big fan of Todd's courses :)
I’m not sure what this benchmark demonstrates. If you’re talking about just caching the template after execution, then I guess it demonstrates that printing a variable over and over is faster than parsing, but that’s no surprise. A more useful benchmark would be to test parsing the template and executing on each iteration, versus parsing once and only executing on each iteration. That would obviously be better to parse only once, especially since template is thread safe. As for buffering, go doesn’t buffer unless asked. You can simply set up a bufio.Writer and it will add that buffering, which dramatically speeds up writes. Earlier testing showed somewhere around a 10x increase in speed.
Okay I got it figured out, I have to define that function in an array first, I'm guessing so it doesnt get deleted when the function ends: &amp;#x200B; // Up Down Key for Primitives var prevFuncs \[\]func(event \*tcell.EventKey) \*tcell.EventKey for n, prim := range composite.componentArray { prevFuncs = append(prevFuncs, prim.GetInputCapture()) prim.SetInputCapture(func(event \*tcell.EventKey) \*tcell.EventKey { prevFuncs\[n\](event) if event.Key() == tcell.KeyDown { if composite.selected+1 &lt; len(composite.componentArray) { app.SetFocus(composite.componentArray\[composite.selected+1\]) composite.selected = composite.selected + 1 } } if event.Key() == tcell.KeyUp { if composite.selected-1 &gt;= 0 { app.SetFocus(composite.componentArray\[composite.selected-1\]) composite.selected = composite.selected - 1 } } return event }) }
Todd is awesome. I am eternally thankful for him for coming up with his course on GoLang on Udemy. That course helped me reshape my career and is responsible for making me what I am today. Voted for him :) Thank you Todd!
Up to the user, but "template cache" makes it sound more complicated than it is. You call Parse once and call Execute many times against the same template variable.
Sorry I'm late to reply but wanted to thank you for explaining this and giving me a solution! This worked for me :)
I meant it was probably slow without indexes since I don't have the luxury of creating them (unstructured data). Well, I guess I could just index every column I see - but that would have it's own drawbacks (insert penalties + extra space).
Building a custom store by using the strengths of different databases + application logic is a neat, and much repeated idea with caches like memcached and redis. Not sure what you mean by column tricks though, the only thing I can think of is creating X columns (equal to the number of all known attributes) all set to string and reusing those columns for different data types?
You can write Java in every language. &amp;#x200B; Seriously there are probably way more idiomatic variants of these patterns around.
As mentioned above, the only way I can use PostgreSQL for unstructured documents would be as either a key/value store (see reddit) or using JSONB. In addition, my understanding is that PostgreSQL has no support for sharding - just clustering with lots of read slaves.
Yea it’s certainly not pretty to use Cassandra fully schemaless. Usually you just add the new columns to your schema based on what your code is going to be. Usually you have some idea, and altering the schema in Cassandra doesn’t incur any real penalties. Obviously sometimes that’s not the case tho.
You're right TiKV supports [prefix scanning](https://tikv.org/docs/concepts/apis/#raw) and has a [good number of companies using it](https://tikv.org/adopters/).
Agreed! I've come across a few threads in this subreddit where people were stating he was incompetent and not a good instructor! I was in disbelief, and I thought he was superb! They're entitled to his own opinions; however, I think he makes beginning Go developers feel at-ease and comfortable, and does a very good job and explaining details that most others fly over without second thought. He's very thorough and gives reasoning for everything. Perhaps it's his teaching style they don't agree with? Not sure, but it's not due to not explaining things correctly or in enough detail.
* try -h or -help. All Go commands using the flag package have this functionality out of the box. * an alias in your .bashrc could do the trick. Something like alias gmo="go list -u -m -json all | go-mod-outdated" alias gmod="go list -u -m -json all | go-mod-outdated -direct" alias gmou="go list -u -m -json all | go-mod-outdated -update" alias gmodu="go list -u -m -json all | go-mod-outdated -direct -update"
Thank you to everyone for your responses! I ended up creating a struct that loads in the templates I need and exposes a render function to write my templates to a ResponseWriter.
Today is definitely not April 1st
https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/
Several times a second is not very often. Consider coding it the obvious way, rather than optimizing too soon.
Several times a second is testing it for 1 connected client. It increases O(n^2). On stress tests, the GC overhead became terrible.
One really minor nitpick because it's just my pet peeve: &gt; Setting the sliding window size to 1directly in the codebase (**could of** existed in the database or config, but there was no initial need) meant we could enforce 1 active session per user. That "could of" hurt my soul a tiny bit :)
Classic medium style article... Not a fan
Personally, I think people spend way too much time worrying about this. Tests have characteristics involving what they test, how much context they require, how long they take to run, how well they cover the code, how well they cover corner cases, etc., and the degree of difference in these various characteristics maps poorly to trying to slot them into being either "unit" or "integration". I think the tradition "integration vs. unit test" is a sloppy attempt to capture a deeper truth, which is that you generally end up needing two types of tests. The most important difference is run time. The one type is the one you run quickly, and can afford to run every commit. The result of this requirement is that you'll have to remove as many external dependencies as possible, just because they're too slow. But you won't be able to cover everything in these tests. The other type is the one that prioritizes coverage, but will consequently take longer to run due to needing to set up various bits of context, like a database or a browser execution (via Selenium or whatever) or something. You really need both fdr any non-trivial project. The traditional "unit test" is the former, the traditional "integration test" is the latter. However, I unashamedly will run "integration tests" on every commit if they're fast enough, and certain "unit tests" may require a certain degree of setup (yeah, you're not supposed to have your "unit tests" set up _too_ much context but when unit testing "the code that manipulates the database" you really can't avoid having to set up a database) and consequently end up in the slow, only-run-on-the-CI-system tests.
So a Unit test will test your codes logic. An integration test will test how your code integrates with other code. Eg, getting the number of nodes by asking their API is a integration test because you’re testing how your code and theirs interacts. You test that the responses from the server look sane and are as expected. Once you have the number of nodes, what your code does with that is what you test with a unit test. For example, if you spawn off one goroutine per node, for whatever reason, you would want to ensure that it actually does. Since you can’t necessarily control what the server responds with, you often create mocks that fake responses. Eg, in one test it would respond with 1 node, so you would spawn one routine. In the next test, your mock returns 8, so you want to respond with 8. That way you don’t need to go and create several different infrastructures just to test your codes logic. If the other server is acting normally, you often can’t test an error case either. A mock makes this easy, as you can fake errors at various points. Eg, one test can fail to connect to the server. You would want to test that your code attempts to connect to the next server in the pool and doesn’t just bail. Or you can fake that a query fails with a defer message because the server is out of resources; you would want to ensure your code waits and tries again, or perhaps faults in a clean way instead of corrupting data. You can’t usually do this if you have an actual server that you’re relying on its responses. Both tests are important. You may not be able to run all, or possibly even any, of your integration tests at each commit or each call of the CI pipeline, so you want to test those infrequently and have fewer changes to that code. But your own logic, you want to test that continuously, and before every commit to ensure you don’t have any strange bugs appear or re-appear. Usually you’re updating your own logic anyways, and not the connections to outside services.
The Go Wiki on slices describes this issue. "Re"-slicing from the underlying array does not release the values of any contained pointers and can lead to memory leaks. They have some alternatives to make sure the re-slice stays memory safe. &gt; NOTE If the type of the element is a pointer or a struct with pointer fields, which need to be garbage collected, the above implementations of Cut and Delete have a potential memory leak problem: some elements with values are still referenced by slice a and thus can not be collected. The following code can fix this problem: [https://github.com/golang/go/wiki/SliceTricks#delete-without-preserving-order](https://github.com/golang/go/wiki/SliceTricks#delete-without-preserving-order)
[https://www.codetriage.com/?language=Go](https://www.codetriage.com/?language=Go) many to choose from
You got me wrong you can create index on fields of instruction data. Yes Postgres can create index on arbitrary fields of JSON. Every store that you might use (MongoDB, CouchBase etc) will create index as well and would cost you same at time of insertion. Here is an example indexing I just quickly Googled CREATE TABLE publishers(id INT, info JSON); CREATE INDEX ON publishers((info-&gt;&gt;'name')); Here is a rails specific tutorial but you can just look at SQL part https://blog.codeship.com/unleash-the-power-of-storing-json-in-postgres/ Again I would highly recommend that you take a look at docs. It would open up your imagination.
https://godoc.org/google.golang.org/grpc/test/bufconn You start the “server” on a bufconn instead of a true socket. Then you connect to it. Use your client code to make the calls to it. Alternatively, manually craft the requests and just call the server code directly. Both are valid testing methodologies.
[removed]
[removed]
&gt; Oh, yes as mxz3000 says, new doesn't allocate. Yes it does.
[removed]
[removed]
[removed]
[removed]
If your PHP background ever had you working with WordPress you might find Ponzu interesting! A CMS project I started which is always happy to see new contributors: https://ponzu-cms.org
I meant this document https://golang.org/ref/spec which specifies set of rules a program must obey to be called go compiler :-) it defines every aspect of language from syntax to compilation to execution. Effective Go, I'd say is collection of advices on solving particular problems. Very worth the effort.
This is super cool. Well done.
[removed]
[removed]
Build the protos and call the RPC function and get back the return proto and error. That is, instantiate your server without setting up the listener, and directly call the methods without dialing. Full disclosure: I haven't actually tried it out myself, but it seems to me that it should work
I mostly agree, but it's not just about speed, it's also about isolation. A test with tighter isolation (a unit test) gives a very precise indicator when it fails. A test with less isolation (an integration test) does not; the failure could be related to the local or remote side of the integration, or the wiring of the integration itself. There can be slow isolated tests and fast integrated tests. The results of the tests reveal different information.
Good resources to read later: - https://martinfowler.com/bliki/UnitTest.html - https://martinfowler.com/articles/practical-test-pyramid.html
It sounds like you want a [sync.Pool](https://golang.org/pkg/sync/#Pool).
Your URLs aren’t complete and I think you probably saved the MP3 in the wrong folder. Try using the absURL filter or just set the parameter to the full URL in your file. The MP3 should be in he static folder because it isn’t processed as content.
I can highly recommend a bare metal server from https://www.packet.com experience has been great for me so far.
how is it different from "github.com/dustinkirkland/golang-petname" ?
Thank you so much. I'll look up the absURL
I need to learn fan-out / fan-in parallelism for calling remote services, and also one that caters for exception handling.
Awesome. I have been looking for this quite a while. Will study this. Thanks
What does this have to do with golang and why was it forked?
great series. im glad someone took this challenge :)
Change it to a private internal variable and create getter and setter for it.
Well done. How about the link to your Twitch channel? :)
Creating a pure css framework and need some help! https://github.com/ainsleyclark/mesh
Thing is, the slice "belongs" to a client, and for as long as the client is valid the main thread will be continuously remaking this slice and populating it. It's a GC nightmare.
[Busy waiting](https://gitlab.com/jmackenzie91/golang-object-pool-example/blob/master/client/pool.go#L78) consumes CPU. Instead of discussing how to fix that, I'd like to mention a different approach to solving the higher-level problem. This approach is to start N worker goroutines where N is the maximum level of concurrency allowed. Each of the goroutine gets work by ranging over a channel. The goroutine exits when the channel is closed. The main goroutine feeds work to the channel and then closes the channel. func main() { ids := make(chan int) var wg sync.WaitGroup for i := 0; i &lt; 4; i++ { wg.Add(1) go func() { defer wg.Done() for id := range ids { // do something with id fmt.Println(id) } }() } for id := 0; id &lt; 1000; id++ { ids &lt;- id } close(ids) wg.Wait() }
In the back of my mind i was thinking that I should be using channels. Thanks for the feedback! \+1 for the golang -&gt; Go
Yes my bad. I didn't realise `var p *T` doesn't actually create a pointer
Yeah I love it. Deserves a clap. I love it because goroutines are just hard to wrap your mind around. Let alone learn from as a beginner. I still don't totally have a mental model of it. I think though in the future get into switch statements and how you can use them in a server and then after that context package. Because context is even harder.
Of course: [https://www.twitch.tv/mueslix](https://www.twitch.tv/mueslix)
Goroutines: For shit that can be done whenever, so your current thread isn't having to wait. &amp;#x200B; Channels: How's that shit going, anyway? &amp;#x200B; vOv
Not sure what do you mean by bid ask algorithm. Bids and asks are mostly kept in two lists. Try searching for quant cup order matching algorithm. It has a simple implementation to start with.
Its quite possible those interfaces are using the context, and more specifically its cancellation function. Since they require a context you are creating, then you can control their execution using said context via either time and/or manual cancellation through a cancel function.
so basicly im limiting the time of API call for performance and security reasons?
This approach works only if you have only one instance of your app(one app and one process), it's not horizontally scalable at all.
[removed]
That seems right, but I do think it's a little odd/surprising. I've opened [GODRIVER-988](https://jira.mongodb.org/browse/GODRIVER-988) about it.
I agree with /u/matthold on the high-level approach. In no particular order, and not knowing at what level you want the review: * No documentation or tests. * You are creating interfaces for every struct. Go interfaces are usually defined where they are being used, not where the struct is defined. * Structs that contain a single field could just be aliases for that type, where it makes sense. E.g. `SomeClient` doesn't have to be a struct. * The `Create` function in `ClientFactory` should be called `NewClient` to fit with normal Go style. * Why do you even need `ClientFactory`? Why not just pass around a function if you need it to be dynamic? See https://golang.org/pkg/sync/#Pool * There is no way to change the factory field, so the `ClientFactory` seems even more pointless. * You return structs by value. That's highly unusual for structs this large. * `FixedPool.mu` doesn't have to be a pointer. * Use `defer p.mu.Unlock()` to be more defensive. Less risk of someone screwing up the code in later revisions. That will also make `FixedPool.Get` look nicer since you can return early. * Same with `defer wg.Done()`. * `WaitToGet` never returns with an error. Why does it have an error return? * Your `Get` is assuming the create function is cheap. That's fine until you let callers supply the factory. Avoid holding the mutex while calling it. * `Return` sets `err`, but then also returns it. That's weird.
Yes. But you have to manually respect it. It does not cancel your code automatically.
thanks man !
`/go.aum`? Boom. Roasted.
💯
Use the x/limiter package instead? :)
Ok. Thank you
Keep adding to Playlist, already subscribed to it. Going to start watching! Thanks for taking your time to share.
Awesome write up. Getting around boxed values is probably my biggest frustration with go at the moment. Biggest issue with this approach is the user needs to own the slice and the container it references separately
[removed]
[removed]
golang-petname uses a small dictionary which consists of only \~1200 words, which wasn't random enough for me. &amp;#x200B; This package embeds the medium and large dictionaries from the Python equivalent. You only import the dictionary you use (so smaller binary size), and the dictionaries are stored in a memory efficient way (see \[dict.go\]([https://github.com/zippoxer/petname/blob/master/dict/dict.go](https://github.com/zippoxer/petname/blob/master/dict/dict.go))).
It's not the Go way. First try to use a concrete type instead of a map. Second, is shorter `json.NewDecoder(r.Body).Decode` than both `ioutil.ReadAll` and `json.Unmarshall`. Third, instead of receiving an `*http.Request`, a `io.Reader` is more generic. Anyway, `json.NewDecoder(r.Body).Decode` is just a line, so you don't need a wrapper function. Even if you are parsing to a map. ``` var in map[string]interface{} json.NewDecoder(r.Body).Decode(&amp;in) ```
If possible, it’s recommended to decode the data into a struct. Decoding to a map is really only useful when you don’t know what data you’re receiving. You can find more information [here](https://blog.golang.org/json-and-go). That said, the most Javascript thing about this is that the library implements one function that isn’t too complex or out of the norm for what someone may do in their own code. That’s a popular pattern with things like NPM packages, but Go libraries traditionally are a bit more robust.
I think it's mostly for cancelation and share request scoped data. &amp;#x200B; Most database drivers will accept a context argument precisely for that, when the request ends, so the database call. Database quesries can be slow, so you can end them along with your requests if that's what you want. &amp;#x200B; The other one, sharing scoped data... Maybe you want to put the current authenticated user in the request scope with a middleware. You can get it then with \`ctx.Value(AuthUserKey).(User)\`.
Your link is bad or this repo is private, so we can't see it.
"Lock tables or sync.Mutex" is not a choice you should ever have to make inside you program. Unless maybe you are writing a new database. All modern DBs (excluding mongo) should handle atomicity for you, doing it yourself is almost always a bad idea. My guess is that your race condition is not with the datastore but in your worker code. It might be time to press reset and think about your code and its access patterns for storing this data.
Have you looked at Scylla? They support custom types, json columns and have also hosted option - https://www.scylladb.com/2018/12/13/scylla-vs-amazon-dynamodb/.
Oh sorry. Fixed it now!
It's worth noting that you could make an interface that is trivially implementable on a slice type to implement compaction. I think all you would need is swap and set-length (which would require the receiver to have access to a pointer to the slice). The skip list could either be given this when constructed to do the compaction itself or it could be passed only when compaction is requested to make the usage simpler for cases which don't require it.
https://www.reddit.com/r/golang/comments/bdxgqz/can_anyone_explain_this/
I literally have no idea what the file is or how it got there :/
Thank you for taking this time to reply with the great feedback!
Not only one-line `json.NewDecoder().Decode()` is shorter, it's more memory efficient. Even though Go's json library is not very streaming friendly (yet), using `ioutil.ReadAll` when you only need to read the data in `io.Reader` once is [a bad pattern for memory](https://wang.yuxuan.org/blog/item/2017/02/some-go-memory-notes).
This article [https://blog.golang.org/pipelines](https://blog.golang.org/pipelines) might be the best place to get started. The examples are small but straight to be point. My video series builds a higher level abstraction on top of it. I will focus more on data concurrency and a lot of focus on error handling. After I finished Map, I will talk about Reduce and Fold, which might even be a closer analogy to fan-in / fan-out.
Pro tip for anyone struggling with concurrency in Golang... If you're trying to debug a complicated thing and can't figure out why it's a) leaking threads b) dead locking c) race panicking, you have a few options 1) Try to solve it by reading the code 2) Wrap it in tests and run go test -race 3) Throw it away and rebuild it the right way from scratch You'll be surprised how often #3 is the quickest solution.
As I just started to do English videos (I used to focus on teaching Chinese developers), your support means a lot. Please share it with your Gopher friends.
Your support means a lot. Please share it with your Gopher friends.
I will do at least 2 videos a week for the next 1 - 2 months. Please share it with your Gopher friends!
Not an expert on micro-services, but your master thesis project looks great. Keep it going.
(Author here.) That's a good point. I'm still sort of undecided on how much of that should be part of the skip list library. The compaction part could definitely be a separate lib. (You implement this trivial interface and then I'll compact your slice for you.) That leaves the question of where to put the logic for deciding when to compact. That could potentially be a pretty simple test: just compare the length of the skiplist to the size of the slice and choose a threshold. So then I wonder if it's worth putting that logic in the library, rather than just leaving it to users.
Nice work. I think you should be able to use the built-in autocert for TLS: https://godoc.org/golang.org/x/crypto/acme/autocert
In this case the complication is that I'd need to setup a wildcard certificate to handle the forwarded names - so `*.tunneller.example.com`. I've used autocert for simple HTTP-&gt;HTTPS stuff, but the last time I checked to handle a wildcard would involve using the DNS challenge. A job for another day ..
If they already require a bit of logic for storing and loading from the list, it's likely most users will have a small wrapper struct, so adding an extra `lib.MaybeCompact(...)` call probably isn't the end of the world, and would let discerning clients build their own compaction. Having it in a separate library ensures that you've exposed enough from the main library to make that possible, so it's definitely a defensible approach.
[https://present.euggo.localhost/talks/objectively\_harmful/objectively\_harmful.slide#47](https://present.euggo.localhost/talks/objectively_harmful/objectively_harmful.slide#47) through slide #50 (press "n" for presenter notes).
Share in the slack Gopher community: there are plenty of chat rooms from beginner to specific areas: https://gophers.slack.com/messages/general/
You are absolutely right about that the library implements only one function . That's totally NPM .I will put that in mind . As a matter of fact , I am familiar with decoding into a struct . I only thought of this because I am trying to decode a request body that I mostly don't know the fields of . Thanks a lot for the advice.
So in my systems i have some http middleware that does several things. Firstly i add a request id (its a uuid) to the request context, and i also log it to the span. It is key when you want to correlate your log entries and traces. Secondly you can create a fake response writer that embeds the real one but captures the response status so that you can add that as some metadata to the trace. Thirdly, what more specifically were your issues with the registry?
I don't understand the question.
wow . That was really useful. Thanks a lot
The best example I can think of is the sort package. https://golang.org/pkg/sort/ You add a couple methods to your struct/array/slice/map, and then you can pass it to pre made sort functions. It means you don’t need to write anything to sort your own data except very small functions.
 &gt;I have cobbled up implementation where I have 'service' where code serving HTTP request registers itself with 'feedback' channel and func handling HTTP request is selecting over "timetime.After(n \* time.Second)" + this channel to achieve timeout functionality. &gt; &gt;When response arrives from server (B) goroutine handling this part calls service to report response. That call will look if there is waiting request for that response and if there is then sends payload to their 'feedback' channel. I'm not really clear on which part is pulling from which channel with a timeout. Maybe you could diagram it or mock it with pseudo code. But I might approach this using a Request struct that contains a response channel field on it. Then when the http handler receives a client request, it creates one of these async request object, puts the request into the async request channel and then waits for a reply (with timeout/cancelation on the response field). The async mqtt component is receiving these requests and then putting the response onto the response field. If my suggestion doesn't make sense to this problem it's because I wasn't able to visualize your current design from your description.
It's very un-Go style to release libraries that effectively do 1 to 3 lines of code. Especially when it also locks you into a single and inefficient way of doing things.
You might want to steal ideas or code from my attempt to do this: https://github.com/Xe/x/tree/master/tun2
Looks interesting. Where did you learn all about cloud native tool and their integration? Would be cool to share the resources.
Indeed yea . Everyone pointed that it's a very tiny thing for a library . But I am curious ( in order to learn) why does it get you locked into single inefficient way of doing things ? what's more efficient ? Decoding with `json.NewDecoder().Decode()` or just decoding into a struct ? or maybe not only using json as the body type by the client ?? . It's a bit confusing for me since I am used to the expressJs way of abstracting the decoding step .
It’s inefficient because you are allocating a map object which can dynamically store data bs a struct object which will have a set size. The struct will also be much faster to access than the map.
I would love for you to please try CertMagic instead: [https://github.com/mholt/certmagic](https://github.com/mholt/certmagic)
Yes, this. My comment was specifically referring to your implementation of accepting an http request (limited type), reading the whole json body into memory, deciding to a map and returning it. If you had some non-trivial implementation then it might make sense as a library but it should also work for a wider use case. I'm not too familiar with the JavaScript way of releasing libraries but I have heard that it involves many many trivial snippets amounting to massive dependencies.
As most "web frameworks" just wrap it it's definitely a thing for production as well. As usual it strongly depends on your current interpretation of "production" as well as the size of the service/API you gonna build. I would start with one of the common libs like gorilla,gin,echo and others (just examples no ordering).
Use it for production, but configure it right to avoid DoS issues. https://blog.cloudflare.com/exposing-go-on-the-internet/
It can be used in production to serve HTTP, but I suggest hiding it behind a reverse proxy, like Varnish or Nginx.
Interesting reading: https://blog.cloudflare.com/exposing-go-on-the-internet/
Very interesting, thanks!
Hey thanks for the explanation on DI. I think my question should have been better worded as I already use standard DI in my go code, but not a compile time framework; Can you explain when and why I would use a compile time DI framework like dihedral? Thanks
HTTP is entirely performant and ready for production. It’s certainly safe. However, I always put it behind a balancer. The http package works great for serving but when I’m serving high volumes of traffic, I would rather the balancer be terminating TLS traffic so I can leave the app servers to doing their thing.
Agreed. I'm happy to hear go can serve http in production and directly to the internet if needed. I would also use something like haproxy or a netscaler etc for the reasons you mentioned.
OK, that sounds like a misjudgement of effort required for proving skills, and yes, they should at least give feedback if they're being that opinionated. :)
Check this out: https://youtu.be/LSzR0VEraWw
Another go alternative: https://github.com/LiljebergXYZ/tnnlink From my experience those utilities ends up by crashing after 2 or 3 hours of use. Just like you I wanted a self-hosted alternative, and I ended up building my own. I only used existing software (ngnix, docker, sshd) that are well used and tested to craft an ngrok alternative. From what I was able to make, writing custom software isn't needed, only configuration is.
Adding an UUID to the context makes sense! Spans can automatically consume it and the logger middleware could pick it up too. I'll try this out. With the Prometheus registry I'm getting panics when starting both services since they seem to try registering the same default metrics / collectors with the registry. I basically can't use the plain default collector with more than one service but have to create a new one for each service.
Mostly godoc, which is quite good for Prometheus and Jaeger, combined with trial &amp; error. For jaeger there is an awesome tutorial on github: https://github.com/yurishkuro/opentracing-tutorial
As mentioned in other comments, parsing into a `map[string]interface{}` is inefficient and error prone when compared to parsing into a struct with known types and fields. This lib also blindly parses the body without checking the `Content-Type` header to ensure that an `application/json` variant is being sent. Which could lead to confusing errors for both the client and servers. I wouldn't worry so much about "the Go way" of doing this as much as making sure you use the type safety of the language you are writing. The official docs do not help much here as they use the `map[string]interface{}` anti-pattern as well, last I checked. `json.Unmarshal` or `Decode` give you all the dynamism that you should need. https://play.golang.org/p/8gTrkGUFIn3
For the registry, you can instantiate your own copies of the registries and use them instead of the global singleton registry. I think i ran into the same issue earlier this year.
Not smart but you can always create a map to store channel for each goroutine, while groutine blocks on this channel. Key would be some request id for each backend request, add it to each request. When you get reply from backend, lookup in map to find the channel and send reply on it. To handle panics due to writing to a closed channels, just make sure that goroutine removes it's channel from map before terminating.
For curl commands https://mholt.github.io/curl-to-go/ Encoding and decoding https://blog.gopheracademy.com/advent-2016/advanced-encoding-decoding/ To create mock http://www.json-gen.com/ https://onlinerandomtools.com/generate-random-xml?&amp;depth=1&amp;max-elements=10&amp;always-max-elements=true https://www.programming-books.io/essential/go/6744c6d0d620448dbe66e224f64b6f8b-xml To consume a json resource in go https://tutorialedge.net/golang/consuming-restful-api-with-go/ To consume a xml response in go https://larry-price.com/blog/2015/12/04/xml-parsing-in-go/ To create structs from mock https://www.onlinetool.io/xmltogo/ An example put together https://play.golang.org/p/hvlcsPAbUmL
Thank you!!!!
This helped alot!! great channel!! if you have any simmilar sources i would love if you could paste them here!! really thankful!!
thanks!
If I do every db call one after the other it takes 15 minutes. If I have 5 go-routines doing the calls it takes 5 minutes but some fail, so i added retries. There is one main table lookup that then dictates lookups in other tables so I'd want to serialize the other tables. This over the internet so it seems like most of the waiting is in the network.
I have a database syncing taking too long problem.
the idea is to have the one requesthandler doing thousands of requests, so each call of Do requires its own timeout/context. My thinking was like mutexes, release the inner mutex (in this case semaphore release) before the outer mutex (the timeout)
Concurrency-wise, your program seems correct. Two things might be happening: 1. `icanhazdadjoke.com` is rate-limiting you. If I ran that server, I would be. Also I'd be hating you a little bit. 2. you're reusing a single client and the default transport, which might be limiting how much network you're actually using. Try creating a transport and a client in each goroutine, instead of reusing the default transport. I.e. instead of passing in the client, do tr := &amp;http.Transport{} client := &amp;http.Client{Transport: tr} inside the goroutine, and see if that makes a difference.
Use semaphores to control how many requests to send per batch [1][2]. Your code is sending N requests immediately after execution. In your first example, you sent 100 and then 1,000 after your update. You should use semaphores so that if you change N=1,000,000 it doesn’t send 1M requests but instead a small amount of 100-or-so per batch. Check your computer’s limits with the command `ulimit -a` so you can have a better understanding of what are the bottlenecks your program is running against. [1] https://godoc.org/golang.org/x/sync/semaphore [2] http://www.golangpatterns.info/concurrency/semaphores
[https://github.com/autom8ter/goproxyrpc](https://github.com/autom8ter/goproxyrpc)
I wouldn't do it like this. Return and ID of some kind and tell the client to check back to a different endpoint/API or even better return the full link. The client can check back as many times as it wants to find the response.
As /u/robbert229 stated, you want middleware. By passing your R/W through various middleware funcs you can aggregate/act on whatever you'd like. For instance, I have a micro-service setup on NATS that wraps Prometheus related metrics, logging, and routing around each call to the middleware pkg I have set up. It's literally a pass through, but it allows me to call specific middleware wrappers when I need, based on the request. I'd use a counter, and capture HTTP response codes via `response.StatusCode` `prometheus.HTTPResponses.WithLabelValues(response.StatusCode).Inc()` Then call rate later on `sum by(status_code) (rate(my_http_response_codes[5m]))`
But ngrok is oh so comfy.
[removed]
I have commented , liked on your videos. I go by name "Javascript Evangelist" :)
Also posted here: https://stackoverflow.com/questions/55788194/what-causes-pattern-matched-no-module-dependencies-when-using-go-mod-download
HTTP requests are already handled in goroutines for you. You don’t need to do anything special. Just set a timeout on the request context and pass that along to your queue client, so it will die if needed.
&gt; I was thinking to add a unique ID to each request that can be displayed in the logs as well as in the traces. Have you guys implemented something like this before? Does it make sense in production? Yes to both. Tracing helps establish where something interesting happened in the life of the request and transaction IDs let you dig further into it. With a sufficiently complex web of interconnectivity between services you start getting questions around "what happened in request X at downstream service Y?". For this kind of investigation the main thing is being able to relate a particular trace to the relevant logs in a particular service. That could be using structured logs with a field for the trace id, or it could be something else. Using the same identifier throughout the request lifetime in all services makes things easier.
While it wouldn't be impossible it seems cleaner to handle the SSL at the proxy-layer. &amp;#x200B; (i.e. Hide the server behind apache/nginx, and do it there.) &amp;#x200B; Because we need wildcard support we need to use a DNS challenge. I don't think the tunnel-code should have to be configured with your DNS provider details, as it just shouldn't care about such things.
Yea I get it now .. Thanks a lot
Yea , you are probably right
Using ints feels very much like idiomatic Go. I've met people who have been put out by the standard [sort](https://golang.org/pkg/sort/) but to me it is appropriate that sort algorithms should be concern with indices of lists and not the elements of a list, per se. I wonder if you could distill an interface in the same spirit as [sort.Interface](https://golang.org/pkg/sort/#Interface)? An interface that the algorithms can operate on instead of the algorithms encapsulated in a struct with methods...(?)
Thanks. I have uploaded the 3rd video: [https://www.youtube.com/watch?v=GXk\_nEwsnMo](https://www.youtube.com/watch?v=GXk_nEwsnMo) Hope the audio sounds good to you this time.
its much better now :)
There is more information in the stack overflow, but still not enough. Where is the project with this go.mod located ? What is the value of GO111MODULE ? What operation were you performing (instruction) ?
I think I wrote that :) [https://github.com/quii/learn-go-with-tests/blob/master/structs-methods-and-interfaces.md](https://github.com/quii/learn-go-with-tests/blob/master/structs-methods-and-interfaces.md) &amp;#x200B; Ever worked in a code base where changing one part seems to cause other parts of the system to break or need lots of changes too? That points to code that is highly coupled. &amp;#x200B; Decoupled code generally means that changes in one area wont effect changes in another. &amp;#x200B; With respect to interfaces, what it broadly means is that I as a consumer can say something like "give me a thing that has this method on it and i'll do something useful". &amp;#x200B; \_How\_ people implement those interfaces is completely decoupled which means implementer of the interfaces can enjoy the freedom to refactor, changes databases, whatever without having to change \_my\_ code. &amp;#x200B; A great example is http.Handler. You can read this reddit and see dozens of ways to make HTTP servers with different libraries etc but quite often in the end they get plugged into http.ListenAndServe
This is my first Go project. I'm one of a big fan of GraphQL. I've been built a GraphQL API boilerplate with Python, JS(typescript), Rust, Go recently. I think that Go has a great potential in the field of backend. I've been working with `graphql-go` (https://github.com/graph-gophers/graphql-go) for this project. I'd like to hear other experiences with other GraphQL packages. And I'm wondering there are any package to generate a GraphQL schema with merging all *.graphql files in specific directories, something like `gql-merge`(https://www.npmjs.com/package/gql-merge) in Go ecosystem.
Don't ask me, ask the OP /u/Aoradon – I just noticed it was posted twice.
 &gt;3. you're reusing a single client and the default transport, which might be limiting how much network you're actually using. Try creating a transport and a client in each goroutine, instead of reusing the default transport. I.e. instead of passing in the client, do &gt; &gt; tr := &amp;http.Transport{} &gt; client := &amp;http.Client{Transport: tr} &gt; &gt; inside the goroutine, and see if that makes a difference. Do you have information on where you got this point? I was only aware of the default client not having a timeout value. Where did you find a default rate limiter?
&gt; Do you have information on where you got this point? I was only aware of the default client not having a timeout value. Where did you find a default rate limiter? I didn't say there was a default rate limiter. If you have a shared *whatever*, trying to use it from a bunch of goroutines at the same time, you might be running into some kind of contention. You should stop sharing the whatever, and see if it improves. If it does, see if there is a middle ground where you share a pool of them, or something like that (because presumably you were sharing it because it was expensive to create---if that's not the case, just don't share it in the first place; keeps the code simpler).
Matt Damon is a Gopher.
Very readable code :-) &amp;#x200B; When a user signs-up the intended client flow would be to prompt the user to sign in and then active for 30 days?
What I did in [one of my projects](https://github.com/AndreasBackx/remote-and-chill/blob/master/schema.go) that used graphql-go as well, I used packr to pack all of the GraphQL files into 1 string that is saved in the binary. Graphql-go supports the input of a large string so I used that and it works splendidly.
Thanks! I will look into it.
Oh! good point. I missed that part to check the exp date in validating. I'm not sure jwt-go Parse() is checking the exp date during the process of validating. I think I need to check and fix it. Thanks for your review!
I think JWT checks the date, so I didn´t see any bugs when reading through. I was just curious about the intended user flow.
Oh! Thanks for sharing. Yes I intended a 30 days activation of token.
There is more information in the stack overflow, but still not enough. - Where is the project with this go.mod located (inside or outside of GOPATH) ? - What is the value of GO111MODULE ? - What operation were you performing (instruction) ?
As long as you trace each request you can use the traceId as a uuid. And regarding logs. They can be added to the trace so you don’t need a separate id for them.
This release was long overdue, but we've been hard at work for the past couple of months. Not many features were added in this release, as we focused on redesigning the API and fixing the internal flaws. We'll shortly begin work on v0.3, which will include more useful features like custom event listeners and better iframe support. Stay tuned!
Thank you for welcoming me as a Gopher. haha
Looks interesting. I may give this a try for work. Something that may be interesting would be to allow an SSH list to be generated by an SQL query.
+1 to grammar comment above, but also you really should mention the backwards compatibility idiom as well as all the plugins. No mention of the code gen. There’s just not much meat to the article.
Thanks. I'm just seeing this, so I'll start digging through. I've never used OPA, so I'll do their little tutorial.
I found jwt-go.Parse() is not checking `exp` claims in process of validating JWT. I tried to run some tests, then it turns out that it doesn't check the none of Claims["exp"] or Claims["ExpiredAt"], in case of using `MapClaims{}` at generating a token with calling jwt.NewWithClaims(). So, I add some logic to check `Claims["exp"]` agains time.Now(). https://github.com/mattdamon108/go-graphql-api-boilerplate/blob/ff99f0237919064e69b28fb91637671b38a268db/utils/validate_JWT.go#L31
Sound like my boss!! He picks packages by the number of commits and users. ;) I'll probably end up looking harder at Casbin. I just wanted another viable option.
GOPATH is /home/myname/go GO111MODULE is unset (I thought it only mattered go &lt;= v11 and I'm on go 12) my go.mod is in /home/myname/go/src/github.com/j4ng5y/scraper-api I'm running go mod download from the same folder as my go.mod file.
&gt; it's also about isolation. In my original text, that's what I meant by "how much context they require". An isolated test requires no context. A round-trip application test testing at a user's UI level requires the entire app to be set up as context.
I'd be interested to hear how you end up solving this need.
That is certainly up to you. You'll find there's no better integration for the dna challenge than in CertMagic though, Apache and others so not have such good support.
super! I use chromedp in production and am very happy that the library is developing. Thanks
I've actually used this pattern before (but at the time, I was just trying to write a multi-index data structure) and It's been a pleasure to use it. The only challenge lies in defining the client interface for the data types. It's usually trivial to define a wrapper type that can hold both the slice and the container value. awesome writeup btw :thumbsup:
Thanks for the recommendation. Dgraph looks like a under-represented database for it's power (like TiDB). For highly relational data this looks like a great project. Dgraph is designed to [reduce network requests when fetching complex nested/join result sets](https://docs.dgraph.io/design-concepts/#minimizing-network-calls-explained) which is very good for a distributed system. Like most other databases, Dgraph [requires indexes on all fields/columns you wish to use functions on](https://docs.dgraph.io/query-language/#functions). However, there doesn't seem to be any way around this requirement as no databases offer "smart" indexes that are created based on load.
pardon my ignorance but can we use this headlessly?
If you're inside $GOPATH/src with GO111MODULE unset, you're not even using modules.
Summary: &gt; **calling value receiver methods through interfaces always creates extra copies of your values** So maybe use pointer receivers for types used via interfaces.
Why not? ```c, err := cdp.New(ctxt, cdp.WithRunnerOptions( runner.Flag("headless", true), runner.Flag("disable-gpu", true))) if err != nil { log.Fatal(err) }```
Excellent, this is one of my favorite Go libraries!
Yes, you can. This library is fantastic.
Great library! There is also https://github.com/mafredri/cdp which seems to be more complete (but harder to use?).
You wrote "the most important difference is run time". The value of tests is in the information they deliver; isolation affects that, runtime does not. Runtime is a convenience factor and affects how often you can/will run tests, but the distinguishing factor between unit and integration tests is isolation.
1.12 is fairly recent. if you need the most recent version, you can download it from https://golang.org/dl/ and make a local install in your home directory.
Related, here is an [article on using `context.Context` to store the spans](https://github.com/yurishkuro/opentracing-tutorial/tree/master/go/lesson02#propagate-the-in-process-context) from a tutorial series on opentracing.
I followed this, https://golangcode.com/updating-go-on-ubuntu/ but 'go version' still gives me 1.12 instead of 1.12.4
 whereis go go: /usr/lib/x86_64-linux-gnu/go /usr/local/go /home/a/.gvm/gos/go1.12/bin/go
I tried using this but has too many inconsistent results. Had to use pyppeteer instead, unfortunately. I will try again with this release, hope its more stable now
You have to explicitly get version 1.12.4. If you wget the 1.12 tar you get 1.12, not 1.12.4. Replace 1.12 by 1.12.4.
I am trying to think of how you would use this technique for a deque, but it seems tricky because you need to be able to grow the slice.
Related https://github.com/fatedier/frp: A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.
Ouch. You have gvm. You should remove it. Otherwise it will interfere with the manual install. Remove the .gvm dir and the instruction in your .bashrc calling it. Gvm is old, overrides your GOPATH environment variable and doesn't know about modules. I had gvm too and is was ok before 1.11. Now I only use the manual install. I wished there was a ˋgo.latest.xxx.gz` we could use to update with a simple script.
I did that. &amp;#x200B; wget [https://dl.google.com/go/go1.12.4.linux-amd64.tar.gz](https://dl.google.com/go/go1.12.4.linux-amd64.tar.gz)
Can you provide some additional help? How do I do that? Thanks.
i run govendor on my laptop , it execute properly, then i run govendor init command produce error that i share you fork/exec
I must have missed something or read too fast :-) That´s so strange, when I read the JWT code for parse.go and claims.go it seems it should. The map should be decoded and filled during the Parse() -&gt; ParseWithClaims -&gt; ParseUnverified in the dec.Decode(&amp;claims) call.
It’s pretty easy. You need to set up a CA. There are a bunch of tutorials online on setting up a CA. I use OpenSSL for the CA since it’s free and readily available. Once you have the CA configured, you use it to sign the Cert for the server. Then you distribute the CA public key to every client. The public/private key for the server only need to be in the server. Now you can revoke and replace the server keys without much hassle. Just ensure you keep the CA private key super safe, often offline. It’s important to note that go does NOT do ANY revocation checking at all. No CRL. No OCSP. Nothing. You need to implement that yourself.
Thanks! Actually the mistake that I was doing was not using the full chain. It is resolved now :)
If you haven't already seen it, the tool \[decodecorpus\]([https://github.com/facebook/zstd/blob/dev/tests/decodecorpus.c](https://github.com/facebook/zstd/blob/dev/tests/decodecorpus.c)) will generate compressed zstd frames. It tries to generate edge cases in the format, and other tricky cases. It isn't perfect, but if you're passing that, in addition to decompressing all "normal" files, you're well on your way. &amp;#x200B; I'd also recommend using a coverage guided fuzzer, like libFuzzer or AFL. Two interesting test cases would be: 1. Compress the input with the zstd compressor using a variety of options, like the \[simple\_round\_trip\]([https://github.com/facebook/zstd/blob/dev/tests/fuzz/simple\_round\_trip.c](https://github.com/facebook/zstd/blob/dev/tests/fuzz/simple_round_trip.c)) fuzzer, and make sure that you can successfully decompress it. 2. Decompress the input with both the zstd reference decoder, and your decoder. Make sure that your decoder never crashes, and errors only if the zstd decoder also errors. You can use the inputs generated from \`decodecorpus\` as a seed corpus for the decompression fuzzer.
Also, feel free to open up Issues on the GitHub page if you have questions or find bugs or inconsistencies between the format and the decoder. We're happy to help answer questions about the format.
In a post last month, [kjk said](https://www.reddit.com/r/golang/comments/b2tx5r/chromedp_vs_puppeteer_to_fully_load_html/eiv6bvs) &gt; The difference is that Puppeteer is up-to-date library actively maintained by Google and chromedp has pretty basic bugs that haven't been fixed in a year (https://github.com/chromedp/chromedp/issues/168). Since chromedp is obviously not dead, where does this leave developers looking for the pros/cons of these two libraries?
The output of &gt; whereis go &gt; go: /usr/lib/x86_64-linux-gnu/go /usr/local/go /home/a/.gvm/gos/go1.12/bin/go You have to use the instruction $ which go To determine which of the go installation is used. You have 3. To remove the ubuntu one you have to use the instruction (if I remember well). $ sudo apt-get remove go This should remove the path /usr/lib/x86_64-linux-gnu/go you get with wheris go. For gvm, a script is called from your .bashrc or a script executed at startup. To locate it use this command from your home directory $ grep ".gvm" .* Comment the line calling the script in the .gvm subdirectory. You then also need to add the path to the go binary in /usr/local/go. This is also missing in your current configuration. I have the following in my .bash_aliases which is called by .bashrc. If you don't have a .bash_aliases file, you can create one. # -- GOLANG -- export GOROOT="/usr/local/go" export GOPATH="$HOME/go" mkdir -p "$GOPATH/src" "$GOPATH/pkg" "$GOPATH/bin" export PATH=$PATH:$GOROOT/bin:$GOPATH/bin The defines the GOROOT and GOPATH variable, make sure the required directories needed for go are present, and finally, it adds the directories to PATH. The mkdir will do nothing if the directories already exist because of the -p option. Start a new shell and execute "whereis go". You should see /usr/local/go and /usr/local/go/bin/go and nothing else. Then execute "which go". You should see /usr/local/go/bin/go. Finally, type "go version", this will give you the version of go that is installed in /usr/local/go and that will be used.
awesome thx. btw. wrote an opencv addon for chromedp https://github.com/rand99/chromedpcv
Why not use the webdriver or is it an issue in Go?
But http clients already use a connection pool and are meant to be shared concurrently. So I thought you were suggesting some actual attribute of http clients. But I see you were just making a random suggestion to test the difference.
That is the entire error? It just prints "fork/exec" and returns? Doesn't seem like alot of information to go on. Maybe try getting support from the project maintainer? They seem to have alot of open issues: https://github.com/kardianos/govendor
If you want to 404 when \`f\` is not found, then do so. After the loop, check \`if f == nil\` and use [https://golang.org/pkg/net/http/#Error](https://golang.org/pkg/net/http/#Error) if needed. You can also exit the loop early (rather than iterating through every value in \`vars\`) by calling \`break\` after \`f\` is set.
When GO111MODULE is not set, the default value "auto" is assumed. This is still valid for go1.12. This means that go tools don't use the module system when in GOPATH. You could either set GO111MODULE to "on" when invoking your instruction that fails (go get, go build?), or move your scraper-api directory out of the GOPATH directory tree. The former solution would be like this $ GO111MODULE=on go get . I assume that this is the cause of the problem. I don't have much experience with modules. I can't thus guarantee that my analysis is correct and that my suggested solution will solve it.
Safe thanks bro, thought I might have been overthinking it
expert. thanks!
The bug linked in the quote is fixed, now. It isn't clear to me if you're aware of that. To answer your question, probably best to simply try both yourself and see if one or both meet your needs.
Well, I did measure the difference before suggesting it, not wanting to send people new to the language down pointless alleys.
I suppose that wasn't clear, I included the bug link as part of the whole quote since it was an example. &gt; try both yourself and see if one or both meet your needs This is the issue I'm raising, rather than spending the next 10 hours building a prototype only to find X feature missing/awkward - can anyone share experience using either of these?
If you're trying to reduce risk, then reduce risk: write typescript and use the Google maintained official library.
New follower incoming :)
[removed]
I'm using glib.IdleAdd but the problem is that it seems to process things rather slow and it's very hard to cancel GUI actions once they've been added to the queue.
[removed]
Nice tool! Easy to use. Wasn't familiar with these metrics. I ran it on a sample service of mine: ``` my-service 1.0 my-service/cmd 0.8 my-service/internal/app 0.8 my-service/internal/context 0.4 my-service/internal/cache 0.2 my-service/internal/database 0.3 my-service/internal/jobs 0.5 my-service/internal/services 0.6 my-service/pkg/model 0.0 my-service/test/fixtures 1.0 my-service/test/mocks 0.5 my-service/test/testutils 1.0 ``` It was good to see that the model package (a public pkg) has 0.0 and doesn't depend on anything from the internal pkgs. Little feedback I can give for now. First thoughts: - more metrics would be cool :-) (I noticed more are planned) - having some way to list, drill down or visualize the dependency graph would be a nice addition to troubleshoot problematic packages
I'm a Qt developer (no gtk experience) but I thought I might add my 2cents since the problem is exactly the same for Qt apps. These UI frameworks have an event loop in the main thread, and because of how the underlying drawing calls work, it always needs to happen on the main thread. In Qt, the idiom is to do any blocking non-gui work in a thread and then use signals or events to trigger gui updates with the data in the main thread. Your updates can't go any faster than the main loop can iterate and apply them. But if your slow down includes data processing, then you should separate them. Do the processing in the goroutine and then issue fast gui updates in the main thread.
That sounds like you are trying to bulk very large multiple gui updates with data processing, into a single call in the main thread. Typically for a situation where you need to cancel the operation, you would be looping and pushing updates, while checking for cancellation. Break up your calls?
You might find [this](https://github.com/lpar/goup) useful.
Also, https://mholt.github.io/json-to-go/ so you don't even need to write the struct code yourself.
Thank you! Likewise, I would love to connect in person some day!
Thank you!
Thank you! Thank you!
Forgive my ignorance, I am new to Go, what might the benefits of doing this be? Is this just Go's version of Pythons BeautifulSoup?
Awesome. LOL. I'm glad you didn't go insane! Wonderful news!
Thank you!
&gt;my Yes, yes yes! Together we all succeed!
Holy crap, now I feel famous.
Message me and I will give you free access. Then, if you decide it's not for you, nothing lost. And if you like it, you can paypal me the money later. :)
Even though Go and C/C++ are quite orthogonal in terms of perf comparison, but I see no reasons for Go implementation to perform slower on these "linear parts", if memory allocations are carefully managed and data structures laid out efficiently. Of course highly depends on scenario.
[hmmm...](https://i.imgur.com/EW7FgDp.png)
Calling any value receiver does a copy. That’s how value receivers work.
People like me that are wondering Gitea - Git with a cup of tea. A painless self-hosted Git service. Gitea is a community managed fork of Gogs, lightweight code hosting solution written in Go .
Thank you. Can't believe it was the environment variable all along.
BeautifulSoup does http requests directly doesnt use a browser. This is for actually driving the chrome application, so if you wanted to do automated frontend tests of your webapp or something similar, you'd use this. Like selinium.
[removed]
I see, when I tried it out I expected to see a browser pop up based on what the code / names indicated, so I was a bit thrown off when that didn't happen, but I suppose that makes good sense. Though, it feels like running headless might not end up being the most accurate representation of front end testing, but I am probably wrong.
iOS / android developer here, it’s pretty much the same on all graphical frameworks. The last thing you want is your background code updating something while your interface code is working on drawing it. The reality is, it’s no slowdown to do interface updates in a single thread and just use your background threads for other things. Your machine can only draw so fast. If you’re bottlenecking your graphics thread, then there’s likely something else wrong.
Thanks for the reply. Where did you find the reference to arbitrary maps in Gnorm params? I looked, but didn't see it. Is it default TOML?
It's useful for testing but also great for writing screen scrapers with Javascript-heavy sites i.e. sites that generate cookies, update query strings, add headers, etc, etc in Javascript. It's very difficult to emulate a lot of that activity without an actual browser present, especially if you're writing a scraper in a language other than node/Javascript.
Hello! If I understand correctly, I think you almost certainly will want to remedy the blank array bug in order to get a good grasp on GraphQL's capabilities. One of my favorite features of GraphQL is receiving all the data I need for one view in a nicely organized tree. To use your schema as an example, I find it extremely convenient to have a user's posts nested within a user object as opposed to (what you might see under a REST-like model) in the results from two separate endpoints. &amp;#x200B; I will note that you should not think of getting actual results instead of the blank array and adding a \`posts(...)\` query to your schema as in any way mutually exclusive. Many GraphQL APIs you find in the wild will have a "root query" (that's the \`Query\` type in your schema) field for the same types found nested within other types. More concretely, it is common practice to expose a direct query of, for instance, posts while simultaneously having a \`posts\` field inside the \`user\` type. API consumers use this root field to get posts when they do not necessarily want to get posts from a single user. So that is all to say: consider adding the root \`posts(user\_id: ID!)\` query if your application ever needs to access some wider swath of posts. &amp;#x200B; I can't say definitively what is causing the empty array bug without seeing your User model, but I would happily put $10 on a bet that your \`returnValue\` variable contains the user's \`id\` and \`email\`, but not an array of posts. This would cause the empty array bug because your GraphQL server of choice likely evaluates \`returnValue\` and tries to map its properties onto your schema (or vice versa). When it doesn't find a \`posts\` property in \`returnValue\` (or \`models.User\` for that matter -- one and the same with \`returnValue\`, I suppose) it substitutes the blank array to say, "I couldn't find any of the user's posts!" &amp;#x200B; My best bet is that you may need to change the model to look up posts, whether that goes in to the \`GetUserById\` call or gets added to \`returnValue\` in a subsequent step before the resolver returns. Let me know if you get stuck again!
It should indeed be more stable. If it isn't, please file bugs.
While chromedp tries to emulate Puppeteer features at times, and the purpose is very similar, I'd say they serve different purposes. For example, we use chromedp in production because we strongly prefer maintaining and deploying Go code. On the other hand, if you're new to this technology or just want to write a quick script, Puppeteer will likely be easier for you, as it's an older project with more users on the internet.
Thanks allot for the quick response. I've added posts(ID!) in the root and of course it worls And I also added in the GetUserById function to also call the GetPostsByUser id function to load the posts data. The missing peace of the puzzle in my head is that if I only ask for a the following: `query {` `user(id: "1") {` `id, email` `}` `}` instead of `query {` `user(id: "1") {` `id, email, posts { id, title, body }` `}` `}` there should be a way to not have to execute the query and load data I'm not actually using at all. If the it's another type of structure and the user has thousands of data to load that's not going to be uses then I'm just wasting Database processing time. I added my test project to github in case you or someone else can take a look at it and maybe find what's I'm missing in the puzzle. [https://github.com/yuval08/graphql\_test5/blob/master/postgres/users.go](https://github.com/yuval08/graphql_test5/blob/master/postgres/users.go) I'm about to start my first Golang real project and it's one of the last issues I'm trying to clarify in my head ;-)
So pkill.
Once you dove to the goroutine, you should always update GTK3 interface from code wrapped to glib.IdleAdd. You can take a look to the big enough application written with GOTK3 fork (I'm a developer of this application): [https://github.com/d2r2/go-rsync](https://github.com/d2r2/go-rsync). It this application any events occurred asynchronously in RSYNC external process finally reflected to the GUI interface with synchronization via glib.IdleAdd.
I haven't used gqlgen yet, but you need a posts resolver under the user resolver. When you configure your models in `gqlgen.yml` you can tell gqlgen to generate a resolver for a specific field. ```go models: User: model: github.com/my/app/models.User fields: posts: resolver: true ```
Don't worry about the negative comments. For me the courses were awesome. I am confident that many others here would agree with me. Just take the course and enjoy!!
I've used gqlgen a fair bit now, and I really like it. You can definitely set up your project so that if you do the smaller query, you don't actually pull the posts out of the database. To do this, you'll want to override the model for User that is auto-generated, providing your own. This new model will NOT have a 'Posts' value at all. Just something like: type User struct { ID *string `json:"id"` Email string `json:"email"` } As an example. Then, run the gqlgen generate command, and it will no longer build because you now need to provide a custom user resolver. What happens is that a new Posts resolver is being created, and this function will only be called when someone requests a user's posts. It's there that you then do the DB call to fetch the posts, not when you load the user. An example that covers exactly this can be found in their getting started guide: [https://gqlgen.com/getting-started/](https://gqlgen.com/getting-started/) &amp;#x200B; Note that they provide a custom Todo struct that stores a UserID value rather than the User itself. User is only fetched if it's asked for. I'd definitely recommend working through the gqlgen example, and reading the rest of their documentation, as there's not much there to read.
Apache benchmark (ab)
Hey! Thanks for trying out my project! And thank you very much for the feedback! :) Much appreciate it. I definitely am planning on moar metrix! :) I have the other two in the works it's just more tedious as I'm using an AST to analyse the code. It's going well though. I think I'll have something in the coming week. For the drill down, yes. I'm planning an ncurses front-end where you can select the metric and see outgoing and incoming dependencies individually. Hopefully I'll be able to finish that some time soon at least some rudimentary version of it that's already useful. :) Thanks again!
Thank you for the platinum. :)
I like hey a lot (https://github.com/rakyll/hey)
ps, pkill? 🤨
I'm using Golang + sqlboiler + gqlgen and am pretty happy with the results so far: [https://github.com/emwalker/digraph](https://github.com/emwalker/digraph) &amp;#x200B; It might take a little effort to use this project as a boilerplate, but maybe not too much effort.
Try [Vegeta](https://github.com/tsenart/vegeta). It can generate a surprising amount of load from just one modest workstation. It can generate charts too.
Lightweight. It has boundries.
Looks cool, but "inspired by" is an interesting choice of words, as it is wrapping the terminal version of wireshark.
I love this product. Easy to use.
this repo do not actually automatically subscribu Pewdiepie ,it just a slogan
i read your repo, how doest it work with OpenCV?
You can install the newest version of go by using https://distro.tools. That's the way I do it.
Thanks * Felix can be used for managing a lot of ssh connection * use a SQL database connection to generate Golang code project for a RESTful APIs app
Thanks for your support :) I might have some questions when i try to rewrite the parts i ported more or less directly from the your C implementation. Are you interested in issues about the educational implementation too? I am not exactly sure why but it detected corruption in a valid compressed file.
That sounds like a good thing to setup once I pass all 'normal' files I have right now. With those I could confidently call it working and correct. Thanks :)
Hi everyone. Here is the full changelog: https://github.com/fatih/vim-go/blob/master/CHANGELOG.md#120---april-22-2019 This includes `gopls` , golangci-lint and many other improvements and bugfixes. Also Billie (core maintainer of vim-go) has created a new Patreon. I know that many people wanted to support vim-go, so here you chance to do it: https://www.patreon.com/bhcleek
I get the ability to have more dynamic ways to collect process information (inspect), but I'm not clear about the need for start, down, signal, etc when those are all well exposed in both library and command line form. What was the motivation of having all these extras? What benefit is there to starting a process through this cli?
I've been wanting something like this since forever. OP: if you're the author, thank you so much.
Blurb from (@apuchitnis): In this talk, Apu will demystify some of the magic that surrounds networking by creating a HTTP server from scratch in Go. You'll learn some networking fundamentals, visit the highlights of the HTTP spec, and leave with an appreciation of Go's excellent net/http package.
I've tried `packr`, it is awesome for the purpose to merge files in specifi directories. My study and codes are as linked. https://github.com/mattdamon108/go-graphql-api-boilerplate/blob/master/schema/schema.go But, in case of npm modules `gql-merge`, it is actually stitching all the graphql types, query and mutation with merging any duplicated types in *.graphql. Anyhow isn't there any packages in go ecosystem? If not, I think I'm gonna build a cli-tool or package to do it as my second go project.
I have been meaning to setup it up using docker but I can't find my way around it. Do you have an idea of how to go about that ?
The UI is inspired by Wireshark's UI, isn't it?
Why are you working with npm modules? I'm not really following what you're trying to achieve.
Just use something like (after you turn `f` into a pointer or something): ``` if f == nil { http.NotFound(w, r) return } ``` I'm writing this comment, because daveddev suggested using `Error`, but there's a better suited function for this.
Great, thanks. Hooked it up to the database now so no need for the pointer but yea this seems way slicker.
Sorry for my english. haha. I didn't mean working with npm module at all. What I meant is that I've been searching for any go package which merges *.graphql files and stitches it as a full schema. e.g. ``` // query.graphql type Query { getDogs: DogsResonse! } type DogsResponse { ok error dogs: [Dog] } type Dog { name age } ``` ``` // mutation.graphql type Mutation { saveDog(name: String!, age: String!) { ok error dog: Dog! } } type Dog { name age } ``` If I divide graphql schema as above seperated files, there are duplicated type Dog. I want a go package which merge seperared *.graphql files to one full schema after removing duplicated types. gql-merge npm module is just a example.
Why do you have the type Dog in multiple files? It should be in a separate file, then you also don't have duplicates.
Some implementation notes I finally can get off my chest. The implementation is pretty much done from scratch, by referencing the spec. Of course I 'peeked' at the C implementation so see if there was any tricks I was missing. As it often ends up, the biggest challenge was in regard to concurrency. I wanted to try out a concurrent stream decoder from the beginning. This is only possible to a certain extent, since each "blocks" to some extent depend on the previous block. It is not as concurrent as technically possible yet, but in the end I think it was a good approach since it will allow adding more in the future. The main challenge in this was, by far the error signalling and properly handling error conditions and cancellations. So when concurrently decoding a stream, making sure that an error is properly propagated and all running code is cancelled was quite a challenge. In terms of speed, on typical input `runtime.memmove` is taking about 30% of the CPU time. This is pretty much unavoidable. The C version has some tricks that would require using 'unsafe', so I am keeping away from that at the moment. The main speed difference is due to unavoidable bounds checks. It is not in all cases possible to "prove" to the compiler that bounds/nil checks isn't needed - but considering all the benefits we get from that, I think we can live with that.
This is so cool. Thank you for writing this. &amp;#x200B; I love pure go implementations, which free me from cgo, because I write a ton of code that I have to use across platforms and architectures. When code includes cgo, I need to compile with docker (or SSH into an appropriate architecture). &amp;#x200B; A 50% slowdown for the vast majority of my code is actually well worth the simplification of the build and deploy process.
&gt; You wrote "the most important difference is run time". I did, but I also wrote &gt;&gt;&gt; Tests have characteristics involving what they test, how much context they require, how long they take to run, how well they cover the code, how well they cover corner cases, etc., &gt; Runtime is a convenience factor and affects how often you can/will run tests, but the distinguishing factor between unit and integration tests is isolation. This is precisely what I'm saying I don't care about. I don't care about the distinction between unit tests and integration tests. I consider it a useless and failed attempt to partition a multidimensional space into two parts, along a not especially interesting dimension. So trying to correct my own text to bring it in line with the "correct" definition of unit and integration tests is going to be a waste of time, as I am explicitly rejecting that as a useful measure, and consider it something that has wrecked people's thinking about tests a great deal more than it has helped.
&gt; You do get more exposed what exactly happens when you reverse the list and the cpu/memory cost to do so. ...
Thanks! It should also make it useful for webassembly for instance. Do note that it is "up to 50%", usually it is closer to cgo, but I didn't want to set up false expectations. And of course, I'm not completely done optimizing it :)
There any reason in particular the standard library chose not to do sentinel errors this way?
My friend, you rule! All I needed is just to specify that that specific field required a resolver &gt;models: User: fields: posts: resolver: true After that I just had to run the gqlgen sccript and it added a User resolver in the resolver.go file Thank you and everyone else for the help! Go Go Go!!!
I think some of his suggestions are fine - but his arguments about sentinel errors in particular, I strongly disagree with. First of all, the errors returned by `errors.New` aren't fungible for a reason - Dave paints this as weird behavior or an undesirable side-effect of how `errors.New` works, but it is so much an intended effect [that there's a test for it](https://github.com/golang/go/blob/f0fdbb1e8ba44da536050329050424b7b555aecb/src/errors/errors_test.go#L14). It would be extremely weird if two packages defined different sentinels with different semantics and those would then compare as equal - just because they have the same text. i.e. the semantics and meaning of a sentinel error is defined by it's declaration, so it makes *total* sense that the identity of that sentinel is bound to that declaration too. What's more, this could even lead to subtle and confusing bugs, if one function call would be able to return two errors from different sources, that still compare as equal. In a way, this is ironic, because Dave is a vocal opponent of global variables and the like, but with this change he'd actually introduce global state *beyond* what a global variable does (it would be shared between different packages even). Of course, that change would also mean that error messages are now part of your API (specifically when you use the `errors` package), as comparing them would be the canonical way to check them. Currently, if the `io` package would change the error to `var EOF = errors.New("End of file")`, that'd be a backwards-compatible change, because comparisons continue to work as before. It wouldn't, if you make the error message the identity of the error (I mean, yeah, this would only break if you actually write out the error-message or do a local definition of `EOF` - but allowing that is the explicit point of his suggestion). Lastly (and least importantly, but still) his change requires a full string comparison to check for sentinels. Currently, `errors.New` returns a pointer and those only compare as equal if they point to the same memory. So comparing against `io.EOF` requires a simple two-word comparison (without extra indirection). Strings, however, compare as equal by content. So, under his scheme, comparing a non-nil error against `io.EOF` would require chasing both pointers to the string-headers, then comparing the pointers+length in both and if there's a mismatch, chase the value-pointers and actually compare the bytes. It's probably not a *major* issue, but it does have worse performance. So, in total you get super weird, dangerous and slower semantics, for the sole reason of preventing some obviously buggy code. IMO, if you are concerned about someone assigning to `io.EOF` or other sentinels that much, write a static check. Don't use stringly typed values.
BTW, another way to prevent this problem (if you perceive it that way) is to make sentinels *types* of zero-length, i.e. `type EOF struct{}`. The comparisons will look differently (`if err == (io.EOF{})`, instead of `if err == io.EOF`), but you still prevent re-declarations *and* preserve the desirable property of identity being bound to declaration (and also get the same performance characteristics, at least with gc).
Have you read the [blog post](https://link.medium.com/MjJbmgZL7V) they did when releasing the driver? In their example app they use Docker
That's only a useless distinction if you can't manage to find the ample value in it. It has a significant impact on the information delivered by the test results, and the information is the entire point of the tests. Different levels of isolation and different types of test examine the system from different angles and provide different information. A unit test tells you if a particular piece of business logic is correct. An integration test tells you if two parts work together correctly. A black box test tells you if a system works as expected by a client. A fuzz test tells you if the system behaves appropriately across a diverse set of inputs. Categorizing tests helps people to remember they need to examine from all angles, and that the results need to be interpreted according to what is being tested and how. That's far from useless and far from wrecking people's thinking, it improves and focuses it.
Actually, it is my code habbit. As you can see the files and structures as linked. https://github.com/mattdamon108/RateLink-Backend_v2/tree/master/src/resolvers/Client/Mutation I'd like to make a graphql schema (*.graphql) for each resolvers. Hence I need a tool to merge it. This structure helps my code more readable and easy to manage.
See [my adjacent comment](https://www.reddit.com/r/golang/comments/bgg4zs/dotgo_2019_dave_cheney_constant_time/elkuppg/). The Go team didn't *want* errors to behave that way (i.e. be defined by their text) - the semantics of an error are set by whoever declares them, so their identity should be bound to the declaration too.
ab can only use a single CPU core. Vegeta, hey, and others are recommended instead.
😉
I tried Vim as an IDE a while ago and finally gave up, thanks for sharing your setup. I might try again.
`Context` isn't transfered between client and server. Probably you want to use [metadata](https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md) instead.
Does anyone have experience using this plugin? I'm a goland fanboy at work, but I don't have a personal license to use at home, is this good?
It's here: [https://github.com/gnormal/gnorm/blob/23a95a3adc933f4af5585b1f87f5e6321e26ef4f/run/config.go#L54](https://github.com/gnormal/gnorm/blob/23a95a3adc933f4af5585b1f87f5e6321e26ef4f/run/config.go#L54) So you can create whatever keys or nested maps you want as long as you put them under `[Params]`. Here is an example of what I have: [Params] RootPkg = "gendb" [[Params.TypeMap]] Nullable = false Db = "bigint" DbGo = "int64" Pb = "int64" PbGo = "int64" [[Params.TypeMap]] Nullable = false Db = "boolean" DbGo = "bool" Pb = "bool" PbGo = "bool" #...
`pgrep` and `pkill` took my Linux Fu to new levels
Yes absolutely! The educational decoder hasn't been given as much attention as the real one, so some bugs likely lurk.
Copy-pasting a comment I made on u/Killing_Spark's post, since its likely relevant to your implementation as well. It looks like you already are running the decompression fuzzer! &gt; If you haven't already seen it, the tool [decodecorpus](https://github.com/facebook/zstd/blob/dev/tests/decodecorpus.c) will generate compressed zstd frames. It tries to generate edge cases in the format, and other tricky cases. It isn't perfect, but if you're passing that, in addition to decompressing all "normal" files, you're well on your way. &gt; &gt; I'd also recommend using a coverage guided fuzzer, like libFuzzer or AFL. Two interesting test cases would be: &gt; &gt; 1. Compress the input with the zstd compressor using a variety of options, like the [simple\_round\_trip](https://github.com/facebook/zstd/blob/dev/tests/fuzz/simple_round_trip.c) fuzzer, and make sure that you can successfully decompress it. &gt; 2. Decompress the input with both the zstd reference decoder, and your decoder. Make sure that your decoder never crashes, and errors only if the zstd decoder also errors. You can use the inputs generated from `decodecorpus` as a seed corpus for the decompression fuzzer.
Try SpaceVim [https://spacevim.org/](https://spacevim.org/)
I use it. My only complaints are that it opens a second window (pane?) in vim when you \`wq\` that stops it from exiting cleanly and it always chmods the file so it's difficult to [use go as a scripting language](https://blog.cloudflare.com/using-go-as-a-scripting-language-in-linux/)
&gt; decodecorpus Yes, it was very important to get things done. There are 100 samples included for CI and I have a local example with 10k entries (link in the source). I also have a few larger single files for stream performance testing. &gt; use the inputs generated from decodecorpus Indeed. Seeded with 100 files it was rather successful. Found some crazy edge cases. I will try a run fuzz against the reference decoder. Could be interesting.
These are the settings I use. I think the auto-import function is the go.formatTool item ``` "go.lintTool":"golangci-lint", "go.lintFlags": [ "--fast", "-E", "goimports", "-E", "gocritic", "-E", "gocyclo", "-E", "gosec", "-E", "maligned", "-E", "scopelint", "-E", "interfacer", "-E", "goconst", "-E", "unconvert", "-E", "unparam", "-E", "prealloc", "-E", "varcheck", ], "go.formatTool": "goimports", ``` Some of those linters are a bit noisy but the advice is good. It can be hit and miss sometimes but usually once you have shown it a valid import it works. The only thing is I think you can't use modules and have autoimports working. In my opinion modules are still beta grade as not enough of the other tools have adopted it.
&gt; decodecorpus Yes, it was very important to get things done. There are 100 samples included for CI and I have a local example with 10k entries (link in the source). I also have a few larger single files for stream performance testing. &gt; use the inputs generated from decodecorpus Indeed. Seeded with 100 files it was rather successful. Found some crazy edge cases. I will try a run fuzz against the reference decoder. Could be interesting.
I'd like to get some feedback, do you find it helpful when I post release announcements here? Or is it maybe not interesting at all (and I can save some time)? :)
yes, new only creates the struct that stores a slice, a so-called 'empty slice'. Make also only creates this if you give it no parameters, and you can secondarily specify the pre-allocation in the third 'capacity' parameter, these elements will be used to store appended elements automatically. I'm not sure if specifying capacity with zero size in make is a thing but it seems to me it would give you preallocation without you having to change append-based code.
&gt;"go.lintTool":"golangci-lint", "go.lintFlags": \[ "--fast", "-E", "goimports", "-E", "gocritic", "-E", "gocyclo", "-E", "gosec", "-E", "maligned", "-E", "scopelint", "-E", "interfacer", "-E", "goconst", "-E", "unconvert", "-E", "unparam", "-E", "prealloc", "-E", "varcheck", \], "go.formatTool": "goimports", Thank you!!
I use it and I find it helpful
Super interesting! Thank you for sharing your implementation notes. :)
What a great plugin. And absolutely free. I don't know why anyone would want to PAY for an ide for golang when you have this.
&gt; Indeed. Seeded with 100 files it was rather successful. Found some crazy edge cases. Awesome! Zstd has some seed corpora [here](https://github.com/facebook/zstd/releases/tag/fuzz-corpora). The first 4 bytes of each input is a seed for a PRNG, but if you strip it off, the files in `*_decompress_seed_corpus.zip` should get you a bunch of extra coverage for your fuzzers.
Awesome - adding those!
I might try spacevim or neovim, thanks for the suggestions! What are you people using? VSCode? Some JetBrains IDE? Any other favorite setups of yours?
It might be annoying at times, but once you configure everything, it should be as stable as any other IDE. If it's worth your time and you have some time to spare go for it! Let me know if I can help with anything.
makes screenshot, searches image within screenshot, gives you coordinates in website, dom element at position or clicks
makes screenshot, searches image within screenshot, gives you coordinates in website, dom element at position or clicks
https://blog.golang.org/survey2018-results VSCode is the clear winner: https://blog.golang.org/survey2018/fig17.svg
Well, it seems as if you've done a great job at creating the edge cases with decodecorpus. I currently pass 2 out of 10 of the files. While that is a sad bilance it is at least a clear path to look for bugs. After having to dig through big files over and over again this should be faster and easier to debug :)
I'll see if I can narrow the bug down somewhat before I create the issue
The speed is actually very impressive! My implementation does not come close to that right now (for big files zstd is awesomely fast)
Ran it for about an hour. No decompression differences, but got a bug and an error. For the error I will have to investigate if I should remove a check for left-over bits on the sequence stream.
Borg doesn't works with object storage but has compression and Restic works with object storage but has no compression. I hope Restic will fill the gap !
Coming from someone who uses a Jetbrains IDE professionally and Neovim personally, there are a lot of reasons why someone might opt to pay for Goland even though you and I personally do not. Goland has a massive feature list inherited from its sister products: Docker integration, transparent remote filesystem management, advanced build configurations, and so on. Maybe you and I don't care about these features, but some people do and it's what they're paying for.
&gt;It also seems like the reference ignores very very small blocks and just returns no error. I don't know what the cgo implementation is doing, but if it is just forwarding to the zstd decompressor then their may be a bug either in the reference decoder or the spec. In any case, if you can open an Issue on GitHub and provide the input, I'd be happy to help debug it.
It’s helpful. Thank you for all the hard work. I love restic and it is one of the tools that I get really happy when using since it simplifies doing secure backups a lot.
Hi Jean thanks for the great talk. I have one question, why would you use multiple modules in that scenario?
Thanks!
Yes, when I remove the checks for extra bits everything is fine. Added an issue: https://github.com/facebook/zstd/issues/1597 Some small differences, but nothing major. Thanks for pointing me to this!
Testing support, debugging, build integration.... etc etc etc. I like vim for when I need to do certain things sure and have this plugin installed, but I absolutely use a different tool most of the time.
The solution depends on how much accuracy you want. If you want an smart algorithm, take a look here [1]. If you want a brute approach, simply check every character in the first string with the characters in the second string, done. You can even translate a solution from a language like Java into Go, like this one [2]. What do you need this algorithm for? [1] https://github.com/sergi/go-diff [2] https://stackoverflow.com/a/12089970
There's something about Dave Cheney in the way he talks that just anger's me. I don't know exactly what it is but I can't stand the guy. I try and push through his talks because there is sometimes wisdom in there. But the delivery of each is like listening to a sermon given by a condescending, holier than thou priest. It's almost if Rob Pike is the pope and this guy is a cardinal, spouting platitudes hoping to be next in line for the big job. I dunno, I'll plough on...
Despite an [issue](https://github.com/faith/vim-go/issues/2234) being recorded and consequently closed, I still have a problem with this plugin that remains unresolved. Have a similar setup to [syscll](https://github.com/fatih/vim-go/issues/2234#issuecomment-484547115) but cleaning the cache didn’t work.
This looks like a fantastic project, thanks for sharing. _Syncthing + Restic it is!_
That's what I was doing. I broke it up into smaller pieces and it worked better. Still hard to cancel, though. Thanks a lot!
Would [strings.Index()](https://golang.org/pkg/strings/#Index) be what you are looking for?
I'd just like to personally thank you for writing the new standard in backup programs. It works so smoothly, is incredibly easy to use and has great integrations. Honestly, backing up was a pain before and now it's a joy. Thank you!
Yeah I also quit the one or the other interview process. Especially considering I was in another job when that happened - shall I take a week off for that crap? In the end I always took the jobs where they were rather eager to have me... and they all turned out to be good jobs.
Right. And even the... Average at best companies think they only deserve top talent. Offer nothing, demand much and then whine about the shortage. Also considering that most startups are gone soon anyway... Good luck anyway To us all ;)
already on the arch linux AUR as well. aur/termshark-bin 1.0.0-2 (+1 0.99%) A terminal UI for tshark, inspired by Wireshark
You may find this package of mine useful: https://github.com/faiface/mainthread It does basically exactly what you're asking for. Implements a queue on the main thread and lets you send functions to be executed there. I use it in the Pixel game library and I think some other people use it as well.
This bit me pretty hard the first time I used gRPC. One of the things I was looking forward to was passing along context over the network like I was doing in memory. Feels janky, and I wished it were called out better.
I tried it. Couldn't wrap my head around it after a few hours of use... and I've been a Vim user for close to 15 years.
I'm checking on my ubuntu box, here is the output of whereis: whereis go go: /usr/local/go /usr/local/go/bin/go So make sure that you have same one. Home it helps.
Two problems: 1) find a PDF library that can correctly parse the PDF you need to read 2) if your PDF file is OCRed, chances are there are no indications that a particular word was in bold (most of the times a PDF with OCRed text will show the page image and embed the text in a way that is searchable, but with no styles). Are you sure you document does contain the style information ? If it does, you can try https://github.com/raff/pdfreader and create an issue (with a testable example) if you find any problem.
You can just set it's value
I’ve been using it for 3.5 years now and love how it has evolved. Thanks for the hard work, it’s truly appreciated.
Can we stop linking directly to the release page, please? I can find it if you just link the repo, I promise.
I don't say this lightly. Vim-go is the single best language plugin I've ever used for any editor in any language. (Limited sample size, of course). But it is phenomenally well thought out. Total and full recommendation from me.
You can use a custom context and then cast back in your handler like so: &amp;#x200B; `type MyCtx struct {` `context.Context` `Token string` `}` &amp;#x200B; `func (s *Server) MyHandler(ctx context.Context, req *Request) (*Response, error) {` `// context was replaced with MyCtx by gRPC intereceptor prior to request reaching this point` `myCtx, _ := ctx.(MyCtx)` `// omitted for brevity` `}`
Yeah, he even has prior writings that discourage the use of sentinel values. https://dave.cheney.net/2016/04/27/dont-just-check-errors-handle-them-gracefully
Wait, what? Do you have errors in your file? I haven't ever seen that and I use it dozens of times per day.
*In some cases the real-time protection of antivirus software can interfere with restic’s operations. If you are experiencing bad performance you can try to temporarily disable your antivirus software to find out if it is the cause for your performance problems.* Have you considered taking a VSS snap and backing up from that in order to work around this issue?
Is there any plan/roadmap for compression?
The main benefits of all compile-time frameworks is that you're guaranteed to have a properly injected setup if your code compiles, whereas run time DI frameworks don't have that guarantee. Also, compile-time DI frameworks are faster because they don't have to any work at run time. &amp;#x200B; The reason why Dihedral is the best choice, specifically, is that it has much better code organization than something like Wire. There are also some API choices that allow for much cleaner integration of things like lazy initialization, scoped components, and subcomponents that should really push it over the edge in terms of flexibility and power. Thanks again for reading!
wouldn't the difference be " world"?
yeah, actually. sometimes, i just wanna save and close the file, errors and all, and the plugin gets in the way. usually it's just fine though.
Here’s your answer — https://play.golang.org/p/CNf4BvrI90S
https://github.com/restic/restic/issues/21 Supposed to be working on it , but I am not sure of the status. According to that issue it will be worked on after this issue (https://github.com/restic/restic/pull/1494) which appears to have just been completed. So answer seems to be it is next on the agenda - I hope so :)
Yep, find it very useful. Use it all the time.
``` if runtime.GOOS == "windows" { fmt.Println("Hello from Windows") } ``` Source: https://stackoverflow.com/questions/19847594/how-to-reliably-detect-os-platform-in-go
If you can develop software, you should be able to develop this too? What answers are you looking for exactly?
Don’t use global variables.
**Short answer:** you cannot. Once they have the program, if they really want to reverse engineer it, they will. **Long Answer:** as with every software out there with a sane licensing model, you just need to make it difficult enough for the ~hacker~ customer to break whatever “security” you have in place _(between quotes, because this is not really something I would call secure)_. An example of this would be to check the device unique ID [1], maybe the MAC address, or pinging a remote web API every few minutes to verify only one computer _(aka. IP address)_ is running a program compiled with a unique ID _(which you can embed into the binary during the compilation process)_ [3]. There are many _—MANY—_ more ways to add complexity to the license validation process, the more steps you add, the more difficult it’s for the malicious user to steal your software, however, keep in mind what I said before: if someone really wants to crack your software, they will, sooner or later. Good luck [1] Git Tower, for example, takes the hash of the username and the hash of the device ID that macOS provides to check if a trial license has already been granted to certain user. However, you can easily reset these values when you are booting the operating system. [2] https://en.wikipedia.org/wiki/MAC_address [3] Allow for several hours of functionality in case that the web API is unavailable. Paw, the popular HTTP client for macOS, frequently checks if the license is valid with a remote web API, if the server is down the program keeps running for up to 15 days. But be careful, because a tech savvy person can put a fake server in the middle and validate the license forever _(which is what I do with several apps I have installed)_.
**Short answer:** you cannot. Once they have the program, if they really want to reverse engineer it, they will. **Long Answer:** as with every software out there with a sane licensing model, you just need to make it difficult enough for the ~~hacker~~ customer to break whatever “security” you have in place _(between quotes, because this is not really something I would call secure)_. An example of this would be to check the device unique ID [1], maybe the MAC address, or pinging a remote web API every few minutes to verify only one computer _(aka. IP address)_ is running a program compiled with a unique ID _(which you can embed into the binary during the compilation process)_ [3]. There are many _—MANY—_ more ways to add complexity to the license validation process, the more steps you add, the more difficult it’s for the malicious user to steal your software, however, keep in mind what I said before: if someone really wants to crack your software, they will, sooner or later. Good luck --- [1] Git Tower, for example, takes the hash of the username and the hash of the device ID that macOS provides to check if a trial license has already been granted to certain user. However, you can easily reset these values when you are booting the operating system. [2] https://en.wikipedia.org/wiki/MAC_address [3] Allow for several hours of functionality in case that the web API is unavailable. Paw, the popular HTTP client for macOS, frequently checks if the license is valid with a remote web API, if the server is down the program keeps running for up to 15 days. But be careful, because a tech savvy person can put a fake server in the middle and validate the license forever _(which is what I do with several apps I have installed)_.
**Short answer:** you cannot. Once they have the program, if they really want to reverse engineer it, they will. **Long Answer:** as with every software out there with a sane licensing model, you just need to make it difficult enough for the ~~hacker~~ customer to break whatever “security” you have in place _(between quotes, because this is not really something I would call secure)_. An example of this would be to check the device unique ID [1], maybe the MAC address [2], or pinging a remote web API every few minutes to verify only one computer _(aka. IP address)_ is running a program compiled with a unique ID _(which you can embed into the binary during the compilation process)_ [3]. There are many _—MANY—_ more ways to add complexity to the license validation process, the more steps you add, the more difficult it’s for the malicious user to steal your software, however, keep in mind what I said before: if someone really wants to crack your software, they will, sooner or later. Good luck --- [1] Git Tower, for example, takes the hash of the username and the hash of the device ID that macOS provides to check if a trial license has already been granted to certain user. However, you can easily reset these values when you are booting the operating system. [2] https://en.wikipedia.org/wiki/MAC_address [3] Allow for several hours of functionality in case that the web API is unavailable. Paw, the popular HTTP client for macOS, frequently checks if the license is valid with a remote web API, if the server is down the program keeps running for up to 15 days. But be careful, because a tech savvy person can put a fake server in the middle and validate the license forever _(which is what I do with several apps I have installed)_.
You could do it with a two step process: The customer buys only a download software to your server. When he executes it, you compile the real program based on the users licence and include the specific information into the application. But I think this question is not Go related.
It's similarly easy to find the repo from the release page
I think it's because your code is not formatted correctly, hence `gofmt` outputs an error and that's showed to you. You can disable showing formatting errors with the following setting (add it to your vimrc): ``` let g:go_fmt_fail_silently = 1 ```
I assume you’ve looked through the documentation and understand that a Goroutine is scheduled using cooperative multi tasking. Adding runtime.Gosched() should let it yield. Or certain other things like a select on a channel will yield.
I believe it might be helpful for anyone consuming/migrating to golang modules might find it as an interesting read. &amp;#x200B; I certainly didn't had much practical use of **replace** before this situation.
Maybe check out [https://keygen.sh/](https://keygen.sh/). Haven't used them but looks interesting. They have a Go example on [Github](https://github.com/keygen-sh/example-go-program).
Agreed, but Docker "integration" is one of the worst implemented parts in JetBrains IDEs, IMO.
&gt;most of the times a PDF with OCRed text will show the page image and embed the text in a way that is searchable, but with no styles This is what occurred to me when I hit submit and went into bed. I'll see what I can do. Ty.
Thank you very much for your feedback! I'll continue posting announcements here :)
Unfortunately restic is a spare-time for-fun project (at least for me), so features will take time. We'll get there. &amp;#x200B; In practice, many people reported that they don't miss compression so much because of the deduplication restic uses.
Yes, but that would mean having even more platform-specific code in restic, and we don't have a developer who actually uses Windows. We're unsure whether or not taking a VSS snapshot is functionality which should be included in restic or not, here's the issue for it: [https://github.com/restic/restic/issues/340](https://github.com/restic/restic/issues/340)
Go also sometimes yields during function calls. What I'm looking for is the right profiling tool/tools to use to help find problems like this. I've found a case where it is not at all obvious from looking at the code that this is the issue, and the only way I could find to test it is isolating parts of a suspicious func and calling them in a loop while putting runtime.Gosched() at different spots to try and find the problem areas. If nobody knows of docs or a tool for this, I might open an issue. I just wanted to make sure I wasn't missing anything.
Fatih is one of those rare highly productive people, and somehow we've tricked him into making a free open-source vim plugin when he could probably be launching an amazing startup or something.
[removed]
I think you're looking for \`go tool trace\` &amp;#x200B; [https://making.pusher.com/go-tool-trace/](https://making.pusher.com/go-tool-trace/) [https://about.sourcegraph.com/go/an-introduction-to-go-tool-trace-rhys-hiltner](https://about.sourcegraph.com/go/an-introduction-to-go-tool-trace-rhys-hiltner) [https://golang.org/pkg/runtime/trace/](https://golang.org/pkg/runtime/trace/)
Indeed. There’s a handful of things. I think most OS calls will cause a yield. I don’t know of any tool unfortunately.
could also be a case of "more new gopher (youger) --&gt; less familiar with "old" tools (vim) --&gt; %vim decrease %others increase"
Gitlab CI * Easy to use gitlab-ci.yml for jobs * Directly integrated with your source management * Different runners (Docker, Kubernetes, Raw, ... whatever you want) for the jobs configureable &amp;#x200B; We bascily automated our deployment for reviews to staging, to finally production (first qa with smoke tests and finally real production) deployment.
Develop yourself the protection based on MAC of network NIC, as usual commercial software.
Did you write write that subheading in Go as well?
Well, kind of. I use Hugo, which is written in Go, to generate the web site. :)
Drone.io but investigating concourse-ci and maybe Argo (https://argoproj.github.io/). I have no recommendations at this stage. Drone has worked well, but I would like to avoid dual open source and commercial licence if I can, so seeing what else is around.
Jenkins because time immemorial. It's not bad tho, the go plugin is ok, can build different projects with different go versions easy to self-host &amp; maintain, my personal one lives on a 5$ droplet from digitalocean and our company one lives 10$ droplet.
You go, Hugo! It's better than a Yugo!
Jenkins at the moment, but it's not ideal. I'd prefer something more like Drone or Concourse really, though both have their own issues. Jenkins at least is ultimately flexible.
I was getting snarky so maybe you didn't understand me correctly. Sorry about that :P I was trying to point out that you had a typo where you tried saying that go let's you *write* correct software
Thanks for pointing that out! Typos in headings in headings are the worst and most difficult to spot.
Not sure what ... is supposed to mean but the same point and even example is referenced here: &gt; The Python code snippet del a[i] deletes the element at index i from a list a. This code certainly is quite readable, but not so transparent: it’s not clear if the operation is order-preserving and if the time complexity is constant or linear. &gt;Go doesn’t have a similar utility function. This is definitely less convenient, but also more transparent. Your code needs to explicitly state if it copies a section of the list https://yourbasic.org/golang/advantages-over-java-python/
I am facing the same issue with Drone. I will take a look at those 2 projects a well.
A very nice area to learn about Go. But some of your links redirect to the localhost. You might want to update them to your custom domain.
&gt;The Python code snippet del a\[i\] deletes the element at index ifrom a list a. This code certainly is quite **readable**, but not so **transparent**: it’s not clear if the operation is order-preserving and if the [time complexity](https://yourbasic.org/algorithms/time-complexity-explained/) is constant or linear. &gt; &gt;Go doesn’t have a similar utility function. This is definitely **less convenient**, but also **more transparent**. Your code needs to explicitly state if it copies a section of the list. See [2 ways to delete an element from a slice](https://yourbasic.org/golang/delete-element-slice/) for a Go code example. So you are against the idea of functions?
Thanks. Fixed.
No I'm not against functions, not if they are well designed and well documented.
Could you please write me a well designed function that removes an element at a given index from a given list in Go.
I think you need to provide more info (like: more examples) What do you expect with the following example (1): ``` x := "Helo world" y := "Hello" ``` What do you expect with the following example (2): ``` x := "lo world" y := "Hello" ``` What do you expect with the following example (3): ``` x := "Hello world Hello" y := "Hello" ``` If you want all letters that didn't occur in the second string, you could fill an slice with those whilst comparing the first as byte-array against the second as byte-array. (if the compare fails, just move the compare pointer in the second string) But this could note be what you want ;-) Result of example (1):lworld example (2):helworld example (3):world hello You could even add a single space when both have a space (and perhaps later get rid of more than 1 space) But as long as you don't tell us enough, we'll just guessing :-)
My advice is to find a different business model that doesn't rely on you having to so this, because it's generally painful for both you and customers, and it can always be circumvented fairly easily. &amp;#x200B; Having said that, one way would be to require a so called "licence key" file to be present and valid. That file (generated by you, once per customer) can contain the customer name etc and a signed (by you) hash of their name etc. You can validate the hash when the application starts, and refuse if it is invalid. This approach will not prevent someone sharing their "licence key" to others, but it will contain their name so they might be less inclined to do that. You can implement this using the crypto primitives from the standard library. &amp;#x200B; Using MAC address is not a good idea these days for a number of reasons, not least that it's easily changeable.
https://github.com/golang/go/wiki/SliceTricks
My argument is that such a function probably shouldn't be part of a standard library, and certainly not have it own special syntax. If you feel the need to remove an element from the middle of an array, you should probably stop and think. Perhaps your approach is sub-optimal, or perhaps you should a different data structure. I suspect that the Go designers made some things difficult on purpose. You need to jump through hoops to catch an exception (panic) and you are forced to sprinkle your code with the word "unsafe" to bypass type safety. Also, if you want to copy a large hunk of memory, you need to use the word "copy".
&gt; The Python code snippet `del a[i]` deletes the element at index i from a list a. This code certainly is quite readable, but not so transparent: it’s not clear if the operation is order-preserving and if the time complexity is constant or linear. It's lovely how the Go people will turn any and every absence of some feature/convenience/abstraction in Go into an 'advantage' and immediately go into a riposte, claiming how having the said feature/convenience/abstraction is in fact bad, because of reasons. Why don't just be honest and say that go slices lack the delete/remove function because no one bothered to implement it. That sort of thing happens all the time in all the languages. And anyway, it's pretty obvious the Python thing preserves order (I'm not an expert in Python, but I would bet it works just as I expect). The whole "transparent / non-magic code" argument falls apart anyway once you start pondering what go code like `go some_function();` or similar does internally, or how panicking works, how maps work etc.
I always wondered why the channel operators aren't just member functions. There doesn't seem to be any reason for them to be operators.
My argument isn't that the absence of this utility function is universally better. It's a different trade-off: "This is definitely \*\*less convenient\*\*, but also \*\*more transparent\*\*." However, it's a trade-off that I prefer, since it can make it easier to write clear and efficient code. I can assure you that there are no sour grapes involved.
[removed]
Those aren't functions. The whole point of a function is to not have to repeat the same code over and over again. And you can't tell me to simply put it in a function, because that's exactly what OP is arguing against.
I'm sorry, but I find that pretty hard to believe. Code like this: ``` copy(a[i:], a[i+1:]) a[len(a)-1] = "" a = a[:len(a)-1] ``` Really isn't particularly easy to read, understand and verify correctness (an off-by-one error could hide in there pretty easily). Calling this 'more transparent' compared to a well-documented well-known standard language function seems like a bit of a stretch to say the least.
Yes, it's butt ugly. I certainly would think twice before committing code like that. :) However, it's very clear that the code performs a potentially expensive copy operation.
I agree about the special syntax for del: I'm not fond of it myself. But having a operations (regardless of syntax) that exist for many data structures also has its advantages: you can write algorithms that can operate on a wide range of different inputs. Perhaps it's less efficient, but that's not always a priority. What you call a "key advantage" others would call a sacrifice. Each language has different goals. Python has never pretended to be efficient, but it does endeavor to be readable and to let you write code quickly and succinctly. In other words: Python does exactly what it promises. You can't hold that against it. If you want to prove that Go is somehow superior, at least compare it to languages that have the same intended use. Otherwise I could claim that JavaScript is so much better than Go because it runs directly in the browser.
Go has no global scope. The closest is universe scope, but user code cannot declare anything in universe scope. Only predeclared stuff lives there. What you probably mean is package scope. Note that it's substantially different wrt global scope.
[Sprig](https://masterminds.github.io/sprig/) might have what you need... I find that it usually does!
Doesn't look like 'license protect' to me. That'd be easy. Sounds more like DRM. DRM is bad. Do not do it. It usually hurts the legitimate customers and never does it stop the pirates.
I like Go because it's a better version of C. C++ was supposed to be that, but it turned into some other monstrosity. Java and C# are better versions of C++.
Why?
I very much agree that Python and Go were designed with different goals in mind. Python can be an excellent tool for scripting, and I've frequently used Python to teach beginning programming classes for non-CS student. But there is also a lot of server-side Python code, much of it written by coders who don't have a full grasp of the underlying mechanics. Go might well have been a more sensible choice in many cases. I believer Go is particularly well suited for CS majors. It has the crucial concepts of modern languages, and at the same time gives a clear view of the underlying architecture.
I don't know, but as Go has static types and does not have parametrized polymorphism, the return type of the member function should be interface{}, which Rob Pike has said that is not a good practice, and honestly I agree with him in that part. Of course I'm talking as a newcomer in the Go programming language, maybe there exist a way to implement generics that I don't know yet.
Ah, yes, that's a good point. Then again it could probably be done the same way as the built-in functions, such as `new()` or `make()` which also return different types based on context...
&gt; Why don't just be honest and say that go slices lack the delete/remove function because no one bothered to implement it. Because that's not the reason. Do you honestly think the original authors were lazy or forgetful? That doesn't scan. Delete was deliberately left out, because slices aren't meant to be general-purpose e.g. lists or vectors, due to precisely this reason. You may not like the rationale, and that's fine, but it exists and it's internally consistent with the rest of the language.
If you combine faiface's suggestion down below, you can further wrap the functions you need to cancel with something that will cancel them if you can, something like: func CallCancellable(f func()) func() bool { var m sync.Mutex state := 0 mainthread.Call(func() { m.Lock() if state == 0 { state = 2 m.Unlock() return } m.Unlock() f() }) return func() bool { cancelled := false m.Lock() if m.state == 0 || m.state == 1 { m.state == 1 cancelled = true } m.Unlock() return cancelled } }
&gt; Delete was deliberately left out, because slices aren't meant to be general-purpose e.g. lists or vectors, due to precisely this reason. [This blog](https://blog.golang.org/go-slices-usage-and-internals) says: &gt; Go's slice type provides a convenient and efficient means of working with sequences of typed data. ... seems just like a vector / dynamic array to me. Besides, it already has the `append()` function which has "hidden" cost when the slice is out of capacity. If you use `append()` in a loop and don't set capacity upfront, you might run into a similar performance problem (although less severe). So really what's the argument against delete ... and anyway why would someone think it's O(1) in the first place, that makes very little sense...
Not on mobile devices.
Your counter-narrative requires that [Ken Thompson](https://en.wikipedia.org/wiki/Ken_Thompson), primary author of slices, either _forgot_ about the delete operation, or _was too lazy_ to implement it. And that the entire core language team ignored or overlooked the oversight, not only until 1.0 shipped, but continuously, until the present day. Sorry, it doesn't even begin to pass the smell test. Again, it's fine if you don't like the decision. But stop claiming it's due to incompetence. It was inarguably deliberate and considered.
&gt; It's lovely how the Go people will turn any and every absence of some feature/convenience/abstraction in Go into an 'advantage' I think this is fairly universal and not at all restricted to Go(phers). I do love the “not clear if the operation is order-preserving” line. No, mate. Obviously, `del` does the sensible thing and shuffles the list, too.
But what if you don't have time in your application for garbage collection?
&gt; and anyway why would someone think it's O(1) in the first place, that makes very little sense. Because the same operation (that looks identical in code) on a map is O(1).
&gt; But stop claiming it's due to incompetence. Oh, I don't think it was due to incompetence or overlook! It seems to be more like a _"not worth it, we've got more important things to do"_ type of thing to me. Or perhaps the reason is historical. And perhaps it even is deliberate like you said, but in such a case the reasoning for that would need to be ideological. But whatever the reason, I'm not buying the claim that it's some technical benefit trade-off thing.
Absolutely agree with all the positive notes on Scott's teaching and upvoted on [bigthink.com](https://bigthink.com). I am always happy to find worthwhile substance on subject specific learning, and Scott's courses are worth more than what Udemy charges.
Then use Rust?
You're missing the point in Python though - it is a design decision to hide those implementation details. Python is not intended to be used for time or memory critical applications and I think it's disingenuous to act like Golang and Python meaningfully compete with each other. Part of engineering is choosing the right tools for the job, and neither Golang nor Python is better in general, only better for specific applications.
Your claim makes no sense whatsoever. I don't like Haskell, but I would never claim they don't allow impurity because they didn't get around to implementing it. Not implementing impurity was kinda the whole point of the thing. The Go authors similarly didn't forget to implement an O(n) delete from slice operator. Not implementing things that encourage inefficiency was kinda the whole point.
Because a) the only types with methods are declared types and b) only types declared by the user have methods (well, `error` is the only exception). So this violates two design choices of Go at the same time.
&gt; I think this is fairly universal and not at all restricted to Go(phers). True... I used to hold similar views many years ago and Go wasn't around back then...
I'm ok with [sub millisecond pauses](https://groups.google.com/forum/?fromgroups#!topic/golang-dev/Ab1sFeoZg_8)
Two more reasons: a) `select` uses cases with communication operators. It would be a bit awkward having to specify what kind of call-expressions are allowed and what aren't. b) There is a one-value and a two-value form for channel operations. There currently is, off the top of my head, no precedent for functions or methods in Go having that distinction, it's all specific forms of assignment-statements.
This is something built into and fundamental to working in the language. If you don't know what foo.append() does and how in Python then you simply do not know Python. You can extend this argument to literally everything. Why does Go have garbage collection? If you manage it manually you get to see how every new value requires a malloc. Why does Go have channels? If you build them yourself you can truly understand the resource cost. Etc. It's missing the point. Just write assembly if remembering how *standard library functions* behave is too much.
You seem to have the basics down. Tagging your releases with version tags of the form \`vX.Y.Z\`, where X, Y, and Z are numbers that should probably more-or-less follow semantic versioning is also a good idea. I'm not sure what you're finding awkward about it. You may have to be more specific. That's pretty much how you're supposed to develop things with source control.
Go's GC is not talked about enough. It's definitely one of the best garbage collectors out there. Have you read about the "Biscut" OS -- an OS kernel some MIT nerds wrote in Go? On average, they found the GC pauses not that significant. Some critical sections of code C was a clear winner, but of course, when you are in the situation where performance is an absolute must, you should be dropping down into assembly anyways, it's unavoidable in kernel development since that is often times you have access to some of the features offered by the hardware to truly make things go fast. &amp;#x200B; Which brings me to the other point. The fraction of developers who *truly* care about that level of extreme performance, where it is an absolute must, would be better served by writing assembly anyways, and in that case, the point is moot.
I am trying to learn Rust right now, writing an OS kernel. I really want to like it. I want to understand all the hype. I am not getting it yet. And I really hate the basically parallel DSL hidden within the \`#\[\]\` attribute syntax. If you think build tags are bad enough, imagine having those everywhere, fundamentally altering the way two seemingly similar functions work. It gets even worse when you have to drop down and write unsafe code using these magical \`repr\` attributes. The other problem I have with Rust is all of these safety guarantees are guarantees in the sense of the guy who sells you flex seal on TV guarantees it won't leak. They are just really strong assertions that "this really shouldn't happen if you follow these rules", but it isn't foolproof. &amp;#x200B; Another final point, I recently read a snarky, jovial tweet about Rust's claim about how safe it is and avoids undefined behavior, \*technically speaking\* since Rust doesn't have a specification, \*all behavior\* in Rust is undefined, :P I am still trying to like it however. I want to write low level systems code and I don't trust myself enough to manage memory correctly or avoid writing insecure code (and don't know all the fancy tools and static analyzers people use these days or where to find out about them) and I sure as hell don't want to spend a decade catching up with the 200,000 idioms of the C++ language, so until it becomes a little more feasible to write real low level systems code in Go, I'll still to giving a rust the old college try. Those #\[\] things
Haskell's reasons to not allow side-effects is based on rigorous theory and is well set out in multiple places. It is also applied throughout Haskell with consistency that is probably unparalled by any concept in Go. (Disclaimer: I am not a fan of Haskell and I've used Go more than Haskell.) The reasoning for not having a delete/remove operation on a slice is set out where exactly? In any case, looking at it again, I think the actual reason is that slices can't have member functions in Go and the global builtin function namespace would get too polluted if too many convenience functions were added.
&gt; only types declared by the user have methods Do you know the reasoning behind this? Why can't the builtin types have methods?
I totally understand. Even though it would be the one feature that would propel restic as the "go to" backup tool, right now it is one heck of a tool! THANK YOU for creating such a great backup program in your "spare-time for-fun" :) Truly appreciated!
&gt; It gets even worse when you have to drop down and write unsafe code using these magical `repr` attributes. It's basically the equivalent of those `__attribute__((__packed__))` you see in GNU-style C/C++. I agree it's not very pretty, but to me it's sort of meh. Macros are worse imo. &gt; I am still trying to like it however. I want to write low level systems code and I don't trust myself enough to manage memory correctly or avoid writing insecure code If you're going to write very low-level code like a kernel, drivers or embedded things, there's probably really no way around unsafe code even with Rust. The best Rust can do for such usecases is mark unsafe code clearly as such and minimize it / isolate it.
Good point. While Go's CC is really good, it will not replace C for operations that interface with hardware. Go and C are designed for different things. A more apt statement is that Go's syntax is like a better version of C.
What are you talking about? Go has a delete builtin already. It works on maps (where it’s O(1)) and not slices (which would be O(N)). That’s a rigorous mathematical theory! It’s not set out because the designers assume you took CS101.
\&gt; It's basically the equivalent of those \_\_attribute\_\_((\_\_packed\_\_)) you see in GNU-style C/C++. Yes and it bugs me that those are necessary. It seemed like Rust really zeroed in 100% on the "memory safety without GC", putting blinders on to some other really quick wins that would have truly converted some other systems developers, but like many point out: Just like Go was written to solve a distributed engineer challenge at Google, as long build times, it really seems like Rust was written to help Mozilla write better C++ the way they like to write C++. The only problem is more people seem to relate to the Google "distributed engineer, easy to build, write simple code that is readable" philosophy. Rust should have made data alignment a first class feature of the language rather than hidden behind some special attribute syntax you have to look up and trust the compiler will do the right thing with.
It’s awkward in a sense that I’m pushing to develop. And then when the release version is ready, then I’m pushing develop to master. It’s like a multi step release. Im wondering if there’s a better way to do this.
&gt; Go is strongly and statically typed with no implicit conversions This is at best misleading: var foo net.IP var bar []byte = []byte{1, 2, 3, 4} foo = bar https://dominik.honnef.co/posts/2012/12/go__on_implicit_type_conversions__type_identity_and_a_little_gotcha/
Using Jenkins currently and moving to Circle CI. &amp;#x200B; Why? That's what our DevOps team chose, and I really don't care anymore I just want something that works and builds fast. It feels like there are way too many these days.
&gt; The only problem is more people seem to relate to the Google "distributed engineer, easy to build, write simple code that is readable" philosophy. Eh, I think this is more about people generally not being very interested in systems programming. Go is simply good enough for most things like various tools and, most frequently, web/network services.
I think a great deal of systems programming still happens in Go. Rob Pike makes reference to it occasionally in his talks that it is replacing more and more C and C++ code at Google, but obviously none of us can see for sure. There have been quite a few attempts to build full on OS kernels in Go, most prominently the "Biscut" OS https://pdos.csail.mit.edu/papers/biscuit.pdf and the research group there at MIT made mention that Go would be a great choice to write some kernel features in Go after you have bootstrapped memory management.
&gt; What are you talking about? Go has a delete builtin already. What I meant was if you wanted the O(N) nature of the remove operation to be explicit, you could use a name like `shift()` or similar, but then you would need _another_ builtin function, which I assume is undesirable. &gt; It’s not set out because the designers assume you took CS101. No, they don't. If they did, they could've just trusted me not to mess up with the `delete()` function in the first place...
Because history has shown global variables to be witch’s trio of tight coupling, performance bottleneck and, most importantly, bug magnet.
https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow
They *could*, but they don't. Don't think there is a more specific reason than "they didn't want them". But note that there's a third class except predeclared (That's the Go parlance, not "builtin", but that's just a nit :) ) and user-declared, which is type-literals and most channel-types (well, technically most types in general, but oh well) fall into that class. IMO it makes a lot more sense for those not to have methods, as they are anonymous and throw-away, in general. But either way, there's no real reason why you *couldn't* make channel-operations methods. Of course it's possible to build a language in which they are. It's just a design decision to not do it and ultimately comes down to preference.
When I explain Go to someone in one sentence I say it is "simplicity of C with productivity of Java".
That awkward step is basically for your protection, so, basically, no. Anything that was easier for lacking that step would basically be too scary easy. Otherwise, there is the "just do everything on master" option. Which can kinda work, for sufficiently small projects.
There was some crazy old bat on TV the other week explaining how the food shortages the supermarkets are claiming Brexit will cause are a fine thing…
I know this isn't correct but when I tell people about go I call it C+
That's not a type conversion. net.IP is a type alias to byte[]
We're using kubernetes/prow for our build pipelines. For personal projects and in a former team at my company Google Cloud Build is straight forward and easy to use CI tool
IMHO it has some very sane settings out of the box and its documentation is pretty good.
https://github.com/qmuntal/go3mf Hello, it appears you tried to put a link in a title, since most users cant click these I have placed it here for you ^I ^am ^a ^bot ^if ^you ^have ^any ^suggestions ^dm ^me
If any gopher out there is working with 3mf files and is tired of regex parsing, string lookups or using C++ bindings should definietly take a look at this package.
In the work we use gitlab-ci, but with my open source projects normally I use Circle-CI
In the work we use gitlab-ci, but with my open source projects normally I use Circle-CI
By the time code gets out of the trivial layer it enters the fog of obfuscation. For all problems there are dozens of viable solutions and the poor sap that implemented the code you are using had to choose one of them. Likely that solution is sub-optimal in some critical way for your use case.
I created this new library for Mocking server with a easy and understable way, using jsons files for the imposters. The library at the moment is very basic but, it have a future roadmap with an interesting features. I would some constructive feedback about it and if anyone want to collaborate to improve the library is more than welcome :)
Since this is /r/golang, I would encourage you to lookup https://github.com/apex/up and join their slack channel https://chat.apex.sh/ to search past conversations about using Go on AWS Lambda for real applications.
Drone for now.
ok, thanks
Thanks! Wish I had found this earlier!
Thank you! I will definitely check this out.
I was using travis, but since they have been acquired I moved to [sourcehut](https://sr.ht).
I don't think that's true. IP is defined like this: type IP []byte type aliases are a specific thing, and this is not one as it requires an "=" in the declaration: https://golang.org/ref/spec#Type_declarations This is a type definition that creates a defined type that is different from the underlying type. Notice how in the spec on Type identity, B0 and A0 are NOT identical, and B1 is not listed as identical to []string. However, I believe they are assignable because of the rule: &gt; x's type V and T have identical underlying types and at least one of V or T is not a defined type. I believe that this is why the following fails on lines 9 and 10: https://play.golang.org/p/cjSA8o9kl7h
Regardig Docker, I believe Rust would've been a much better fit than Go for that type of software. But at the time Rust wasn't ready enough for production and it made sense to use Go. But generally, yes, I suppose Go is good enough for that sort of thing too, even though I don't believe it's particularly great at systems programming.
&gt; If you use `append()` in a loop and don't set capacity upfront, you might run into a similar performance append used in a loop is amortized constant time though, unlike del which would be linear. The only builtin with super-constant complexity is copy which makes sense, if you extended the delete builtin to handle slices as well as maps you'd have the first builtin that either runs in constant or linear time depending on the type. I think it makes sense as it is and `a = append(a[:i], a[i+1]...)` is not that bad.
Using Azure DevOps for [this](https://github.com/IxMilia/dxf-go) project via .azure-ci.yaml.
Not a comment strictly about Go, but good read. Specifically found the bit on Oracle v. Google to be interesting and enlightening.
Lots of exciting news here. I might be most excited to try out the new dyndns directive on a home computer. The licensing and new venture stuff seems promising. I'm sure the free tier will remain my favorite server (and be easier to use now that the binaries are not under a eula), while the enterprise services such as the build server will help ensure ongoing viability of the team and project. I hope to contribute to and recommend caddy long into the future. :)
I’ve used Go lambdas extensively for work at a fairly high traffic e-commerce company. My personal thoughts: - If you need data storage, consider not using a lambda. If you must use a lambda, then DynamoDB is your only option. Do not use RDS unless you enjoy random spikes of execution time. - If you want concurrency, you will need to bump up the memory your function has. This is because AWS scales your access to CPU cores alongside your memory. By default you get 128mb and one core. Also, you probably want to profile a lot to find the sweet spot for performance. - You’ll want to target your compiler at the specific flavour of Amazon Linux that runs in the lambda container (or build your code inside one of those handy lambci containers). In general I think lambdas are great for async workloads. The real advantage is how little code you need to write. However, there is a performance and startup overhead, which is worth keeping in mind.
[removed]
Looks good, although I don't see the fluent style of programming in go much... I think part of the reason for not using the fluent style is because it makes it easy to ignore errors. You seem to ignore errors from fetch, prune and the list branches operations...
Someone posted a very good question here a moment ago that asks if the Caddy 2 code base improves over the weaknesses in Caddy 1's design, I think it was related to plugins specifically (but I'm not sure the whole comment because by the time I tapped on the notification it disappeared, unfortunately). To answer: Yes, it's much improved. It's the whole point we're writing Caddy 2.
&gt;Put your project in GOPATH Or in other words: please do exactly what go modules are supposed to allow you \*NOT\* to do. Just great.
I use Caddy for alot of my personal / side project work. Congratulations on the release!
Thank you! It was a community effort.
Oh, I think it's probably fine if you go from 0 to SpaceVIM, but if you've been using Vim for a while, the whole concept of "layers" becomes really bizarre. It also override too much default behavior. Mind you, when I tried SpaceMacs I had the exact same issue.
Bazel is a build tool, not a CI tool. You call Bazel as a step in your CI job.
&gt; Beginning today, commercial licenses are no longer required for commercial use of any Caddy binaries. Going forward, only commercial use of our build server (including the download page and getcaddy.com) requires a subscription: all binaries are licensed the same. I know it's a _technical_ point, but I've avoided Caddy partly because of this, glad to hear all binaries are licensed the same now.
Thanks for that, but it doesn't solve the flow I have with two different branches.
That makes sense! Thanks! Maybe later releases of go will have a feature that solves that problem
What do you think of sourcehut so far?
Well the `append()` builtin also performs in O(n) if you use a slice with the `...` syntax as the second argument... In which case it basically does the same thing as `copy()`... But I guess it's still somewhat less surprising than the `delete()` thing... TL;DR global functions suck... I didn't like them in Python or PHP and I don't like them in Go either...
First off: amazing work! 💪 How difficult would it be to implement the compression part would you say? (cgo-less mirage)
Concourse CI 🤘
[removed]
I was going to say that my attempts with systray under Linux all failed (started fine but no icon), but after adding in some base code a few minutes ago and running it under KDE Plasma, it worked. Originally it never worked on i3wm nor with stalonetray and I can confirm that stalonetray still does not work. This will likely not be of much help but the repository of the application I'm using systray in is at https://github.com/kettek/arbitray/ I don't recall exactly how system tray icons work, but as I recall there are two(if not more) methods (or X11 meta hints?) for notifying the WM/DE that it is supposed to be a tray style window. Perhaps systray in particular is only implementing one method so that it works in some WMs/DEs and not others.
Have you considered renaming it to "Umbridge" or maybe "Ratched"?
Did you consider Rust for Caddy 2?
Thank you for reconsidering and reunifying the license. I use Caddy for personal reasons and wanted to advocate for it at work over things like NGINX, but the licensing was a non-starter. Congrats on 1.0!
Yes, we use Jenkins mainly because nobody wanted to spend time evaluating all the other alternatives. (We had tried Go.CD &amp; Bamboo. Yuck.) Nobody could agree on a set of build steps, so every app had it's own Jenkinsfile. We used the Docker version, passing in the plug-ins to install at boot, and a data dir. There was one Jenkins job that created all the others (pass in repo name, and it sets up a new Pipeline that builds using webhooks on commit.)
Gitlab is very nice in all honesty. Such a simple integration
Nice set of articles on the site.
Im also looking for one
I have to disagree with a part of what you said. Error values won't be equal solely based off their text, they need to have the same underlying dynamic type. [https://play.golang.org/p/8H2p\_0z1fb8](https://www.youtube.com/redirect?stzid=UgzsIjAZNnQuq5RdA594AaABAg.8u4dmCA74Oj8u82rU_nk4_&amp;q=https%3A%2F%2Fplay.golang.org%2Fp%2F8H2p_0z1fb8&amp;event=comments&amp;redir_token=c_3HxBqo-lE8zH77EdAAqmptXyV8MTU1NjI1MjY0MkAxNTU2MTY2MjQy). This would only occur in the case of the two packages utilizing the same underlying type to implement the error interface. If the underlying type of the errors is different, even if they have the same text, they will not be equal. 
It’s been years, but I used to get/set joystick/gamepad values with the Windows API. Calls to these to GetWindow names, etc. are easily doable in Go (I’ve used them in fishing bots I’ve made for World of Warcraft, when I played it... all written in Go). Again, been years... but I’d start there. Google “Windows API Joystick” and find examples in Go to call those libraries for the Windows API directly. After that, it should fall together pretty quickly. You shouldn’t be looking at more than a couple dozen lines of code to get/set joystick/gamepad input.
Errors would only be defined by their text within the same package though. If each package declared a different error type, they would not be evaluated as equal when comparing two values of type error, since they have different dynamic types.
Now you've got me wondering about a tangential topic, about the in practice (somewhat) common use of interface{} negating the advertised strong static typing.