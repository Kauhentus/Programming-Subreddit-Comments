Would this still work with the server behind a reverse proxy?
&gt; Go's handling of imports being so crazy annoying seems to be the raison d'√™tre of goimports in the first place. Problem is it only works well for the standard library. Indeed and since we now have goimports the bigger part of the annoyance is off. Also it works fine for any library, I am not sure what you mean. Personally the only time I ever have to go to the top of the document and mess with the imports is in the rare case when I have packages with the same name. In any other case, goimports gets it right. &gt; You comment out a bit of code and just want to run and compiler goes boopbopbop you can't do that and/or imports got removed and compiler goes boopbopbop when commenting the code back in. I think you might not be making a good usage of the tools. 1. You comment out a bit of code. 2. You hit save. 3. goimports removes some imports. 4. the code runs! And now the opposite case: 1. You uncomment the same code. 2. You hit save. 3. goimports adds the imports back. 3. the code runs! So I really don't get what is the problem here. We have those two amazing tools gofmt and goimports which automatically format our code and add/remove imports on save, saving us countless hours of formatting arguments and code formatting fiddling and you still have some complain?
Nice, I didn't know about this.
If the proxy forwards push frames, it should. But if the proxy does caching, be careful to NOT push in that case.
Let's Encrypt is a good start. I recommend it to anyone whose Web servers still only provide plain HTTP. It is ridicuously easy to set up and maintain (esp. with the Caddy Web server), and it is free. However, Let's Encrypt only secures the connection but does not authenticate you as the company or person you claim to be. (Basically, you only validate the domain.) To validate or even prove your identity, you need a TLS certificate at a higher [level](https://en.wikipedia.org/wiki/Public_key_certificate#Validation_levels). An .exe downloaded over a connection secured by Let's Encrypt still can be dangerous as you cannot fully trust the domain owner at the other end.
I'm not totally clear on the issue you are having, but [this proposal](https://github.com/golang/proposal/blob/master/design/4899-testing-helper.md) will probably land in 1.9 and might address your issue?
Is it possible for the VSCode metalinter to configure that only the open files are linted? Or just use the standard linter and run the meta linter by hand from VSCode? It takes a while every time I save when I have the meta linter active, because he runs his checks on all files in the directory and I don't want that.
That is not possible because the linters themselves typically act on entire packages. I would suggest passing `--fast` through your config somehow, that will make a huge difference.
But doesn't --fast then skip certain checks?
I'd go with your second example - I personally find it far more readable, and a lot of libraries (including core packages afaik) use that pattern, where the struct is returned by shouldn't be trusted
It is perfectly fine to return an object in an undefined/bad state together with an error. It is up to the caller to check if there is an error (ie. if the error is nil or not). Readability is more important here. So second solution is better in your case. However, never do the same with the error itself. In other words, never return a badly defined error in case there is no error. An error must be either nil or well defined. Be sure to read this about this point: https://golang.org/doc/faq#nil_error
This will not work when two or more runes produces one symbol. Unicode is hard... 
I personally don't like this: return f, doSomeWork(f) Just a matter of readability again. Even if we all know that doSomeWork will be called **before** the return statement gets executed, it is less obvious than err := DoSomeWork(&amp;res) return res, err 
It's also worth noting that method calls on nil receivers works fine too [example](https://play.golang.org/p/nk-wcx9gu2).
Hey - no worries - we want your feedback! That's why we're on here. And that's why we are involved in the communities - because we're genuinely committed helping grow and contribute positively to the ecosystems of all the languages we support. Totally get that Go is a different beast - with its own set of challenges as well, so we want to hear from people actually using it for real world projects what they're in need of to improve their dev experience overall. Maybe that's nothing? Maybe it's already perfect? ;) On the subject of IDE - we do have Go integration in the works for Komodo - as well as support for many other languages.
You are correct @ar1819 these functions only support single unicode runes symbols not multiple rune symbols, I'll add that to the documentation of these functions, thanks :) As a side note could you provide an example of a multi-columns symbol? I'd love to take a look and see what's possible.
woah this tool is amazing!
It's too early in the morning, so I might be missing something obvious, but what's inefficient about casting a string to a slice of runes?
Thanks for the work! I love Golang Weekly as it always brings something interesting. It always gets me so hyped. Keep up the great work!
Because of the additional allocation, with copying perhaps? There is no way to get reverse iterator in Go, and indexing string returns byte and not rune. Still, this is not a reliable way to tell if the string is a palindrome. See comment below about that.
I'm a bit nervous about adding 1.8-specific features; that's a bit fresh, and while I understand how to use comments and multiple files to do that safely that's getting a bit off the trail. Edit: I'm compromising by mentioning this in the README, because it is important.
thanks, vim rules! 
Yes, but for a linter in your editor it doesn't matter that much. You just want to catch the grossest of errors there, then run the full suite of linters in a pre-commit hook, or in your CI.
@petercooper you send over a dozen newsletters in different languages and technologies. If one takes the average reading speed of every article you share in every newsletter it would be way beyond no of hours in a week. Having looked at the people you employee it seems all are sales, marketing etc. I doubt if you even code in go let alone read every article. Also summary is for a very small subset of articles shared. 
I notice your account is a day old and this is your only comment, so congratulations. I'm glad I inspired you enough to finally sign up to Reddit. Three of us are full-time editorial and we have several external curators as well, including Glenn Goodrich on Go (indeed, he did at least half of today's issue). We have (brief) summaries on 15 of 23 items this week and, yes, I'm also a developer and work very hard on the business including learning and practicing with new technologies and languages :-)
Can somebody ELI5 the impact of this please? 
If you have vendored dependencies in your project then all dependencies are in the "vendor" folder in the root of your project. This is really great for sharing the project in a team because everyone has the packages they need to run your project. It can be annoying, though, if you run "go test ..."--or some other go command--in the root of your project because Go will attempt to run tests or whatever in all of your vendored packages as well, which is almost never what you want. Now, you won't have to worry about that. 
I have been looking for something similar. I have tried a couple of times to build something like this. At the end of the day, object relational mapping in go doesnt get much attention. The solutions we have are in not so good condition. I decided to fork gorm. Took it apart and started to rebuild it. My strategy is, first making it work with a design that will allow me to profile, isolate pain points and improve, the gradually improve the whole thing by reducing allocations, making it fast and making it approachable by developers. I have been making progress, you can join the efforts. There is still a long way to go. here is the link to the project https://github.com/ngorm/ngorm
Thanks for the reply! I wanted something simpler than `top` and the `pkill` doesn't have the interactive mode I was looking for for real time filtering and choosing the right process so that is the reason I made this in addition for learning purposes. =)
Thank you again FathiüëçüòÄ As vim addict I'm very very lucky that you continue to keep vim(-go) one of best golang development solutions out there ;-)
No - the results would become invalid: https://play.golang.org/p/klXld3DSBu
Congrats to the sherlock in you for finding that I have a single comment and that the account is a day old. - I suggest you add Glenn Goodrich as a curator in your newsletter rather than just yourself. - You are certainly a good curator but please don't use the word editorial. If writing 2-3 lines justify calling it editorial then I suggest you converse with a journalist once. - It's one thing to use intuition to curate and another to claim "but we read the articles".
This solves a problem that doesn't really exist, because an embedded type has no dependency on it's parent, otherwise they should not be separate types. However, food for thought, I strongly believe embedded types should be able to specify some set of interfaces that the embedder must satisfy. That way, given a "FullName() (string, string)" method, I could embed something that provides additional methods such as "FirstName() string" or "ConcatName() string".
So you just gave an instance of exactly the kind of problem this would solve? A way to "specify some set of interfaces that the embedder must satisfy" is exactly what the post is about.
No problem - thanks for your support! If you have anything we should consider including, hit me up anytime.
&gt; This also prevents things like go fmt ./... or go fix ./... from accidentally changing vendored code as well, which would be bad, since it's supposed to be an exact copy of what's in the vendored repo. Gofmt is not allowed to change code semantics, there's nothing bad on formatting already valid vendored code.
&gt; Shouldn't be putting values to an interface that do not implement the interface. You can't. You can put a nil in there precisely because it _does_ implement the interface. See [this adaptation](https://play.golang.org/p/o8LEvA6Ik1) of /u/The_Sly_Marbo 's example. I don't know exactly how much of this gets successfully optimized away at compile time, but in Go, all values are tagged with their type. A nil is really {Type: type, Value: nil}, so it is in fact perfectly legal to call a method on it. Types for which nil is a legal value that you can and should call methods on are relatively rare, and I tend to call them out very explicitly in the docs as a result, because it surprises people. But sometimes it works. [Here's the only one I've written that is publicly available](https://godoc.org/github.com/thejerf/gomempool); see the paragraph immediately above "Obtain byte slices".
Interesting line-up, starting with Robert Griesemer and Dave Cheney ! Yes, hopefully they make videos ;-) But I will be there in person anyway :-) 
Can you elaborate on needing a C compiler? If someone downloads a platform specific Go release from the official site, then they are ready to use it. What extra fighting is being mitigated by the activestate version? Just the extra step of running "go get" on the libraries that they want? It would be helpful to be very specific about what the activestate approach really nets you. 
That would be nice, if you could assume that given an error any value is nonsensical, but that's not always the case. Not even in the standard library. As specified by [io.Reader](https://golang.org/pkg/io/#Reader): &gt; When Read encounters an error or end-of-file condition after successfully reading n &gt; 0 bytes, it returns the number of bytes read. It may return the (non-nil) error from the same call or return the error (and n == 0) from a subsequent call. In other words, it's completely within spec for *Read* to return a meaningful value with an error. It's not completely illogical to apply this same thinking in other places, where you're returning an interface instead of an integer.
That is bad design. Assign the return values to temp variables and check the error, if the error is non-nil, assign the temp to its permanent variable within the struct. Assigning random and possibly incorrect values to fields then checking if there was an error, and if there was an error reassigning that value is just silly. What is the zero value of an 'int'? that's right - ZERO. Zero is a valid value of int. What if zero is also a valid return? You check your errors. This is why if the func returns an error that you should check the error. This is also the reason why you should not rely on returning zero values with errors. Zeroing out could create unnecessary overhead. If the error is non-nil then the values returned with the error should be considered invalid, period. 
Sure - I think I mentioned this in another reply, but I wasn't referring to the language source, but rather package source. I think a few key points: - C based packages (like I think I mentioned something like sqlite3, not to pick on it, it's just really common) need gcc if you build your project with it, take an update from GitHub, etc. So for C based packages, those ship pre-installed in ActiveGo, so you don't need to have gcc. If I do 'go get' on something like that it will usually just say 'exec gcc not found in path' or something if I don't have gcc installed. Worth noting that you also need git installed to 'go get' a package from GitHub. Which again, many users may or may not have installed. Not everyone is using git for source control. - Some users aren't able to just 'go get' or 'git clone' at will with packages they're interested in due to security concerns/license compliance, etc. so ActiveGo has a lot of packages pre-installed that ship with it. Probably not a problem for many individual devs, but can be for those in a large org. - Licenses are also vetted to check for things that at least for Enterprise customers are concerns - like GPL etc. - so you know those are all verified. As I mentioned below, these may not be things individual devs ever think about, or seem like minor inconveniences, but combined with dedicated support, custom builds if required, maintaining updates and compatibility between packages/versions - and that's actually something that adds value for those who are in an Enterprise situation where these things are all concerns, and the time required to keep on top of all this stuff is at a premium. Hope that helps!
I like the interface. Though more metadata on the processes might be nice. Things like resident memory and start time. It could make "oh fuck" moments more survivable. Not that one should be accumulating "oh fuck" moments :)
I don't disagree with you. I'm not saying you should assign things to struct fields without checking the error first. But I'm saying that you can potentially help people debug problems if you return a zero value. Yes, a zero value may well be valid, but it's still the zero value, and it still gives the developer some indication that maybe there was a problem creating the value, if they were expecting it to be something else. You should definitely not worry about the overhead of returning a zero value instead of a half-populated value until such a time as you know it is a bottleneck.... and I would guess that an average developer would never see a time when that's a bottleneck.
Yes, but the convention in Go is to not do that. Indentation is also convention, but we have gofmt to enforce it.
&gt; It reads a bit like you misunderstood me to attack PHP here. No. If anything, I'm also taking a dig at PHP here. &gt; All modern PHP developers have moved to prepared statements, because of the experiences they made. So basically, the members of the Golang community who are advocating filtering to avoid SQL injection are *years behind* the PHP community. Got it!
Hey Eslam, I read over your router. The code is well-structured; it was an easy read. It's cool when people, like you, dive in and learn something new, and it's even more awesome when they reach out to the community for feedback. With that said, allow me to point out what caught my eye: First, a basic note on **slice expressions** before we get into the structural review: when re-slicing a slice in the case of expressions like `matches[0][1:len(matches[0])]`, you can simplify to `matches[0][1:]` which implies slicing from one to the end. [Slice expressions](https://golang.org/ref/spec#Slice_expressions) is a great section, in the Go specification, on all the ways you can slice and dice. Now, let's review the structure and semantics of the router, specifically: * Returning HTTP errors (like 404) * regexp.MustCompile for initialization of globals, not locals * Compiling the same regexp over and over * Return error on invalid path, don't panic * Make the Router concurrency-safe **Returning HTTP errors (like 404)** Go's net/http package provides a couple nice built-in methods for returning HTTP errors: [http.NotFound](https://golang.org/pkg/net/http/#NotFound) can be used to return an error in your router handler. Transforming: route, err := router.Try(r.URL.Path, r.Method) if err != nil { NotFound(w) return } Into: route, err := router.Try(r.URL.Path, r.Method) if err != nil { http.NotFound(w, r) return } And thus saving you a bit of code, since you can eliminate your *NotFound* method. Internally, `http.NotFound` simply calls [http.Error](https://golang.org/pkg/net/http/#Error) which can be used when you want to write back a more general 4xx/5xx error. **regexp.MustCompile for initialization of globals, not locals** [regexp.MustCompile](https://golang.org/pkg/regexp/#MustCompile) is exactly the same as [regexp.Compile](https://golang.org/pkg/regexp/#Compile), except for one very important detail. *MustCompile* panics when it doesn't understand the input. That's useful when you want to initialize a global, as the documentation states: &gt; It simplifies safe initialization of global variables holding compiled regular expressions. But *MustCompile* is not so useful when initializing local variables. You'll want to use *Compile* for that and report an error back to the user. **Compiling the same regexp over and over** Another thing to keep in mind -- if a regular expression doesn't change, you only want to compile it once. Therefore, you shouldn't continue to recompile it. In the *add* function we have: if strings.Contains(route.Path, ":") { re := regexp.MustCompile(`:(\w+)`) // ... } Which means that we're recompiling that regexp every time *add* is called, even though it doesn't change. We can move that into a package-global variable. And being a package-global variable, *MustCompile* also begins to make more sense. So you may have something like this at the top of your package: var patternRE = regexp.MustCompile(`:(\w+)`) In addition, we're recompiling a given route's regexp every time *Try* is called. Instead, we should probably update the *Route* structure to maintain a regexp instead of a string: type Route struct { // ... Path regexp.Regexp } And then we can compile the path in *add* when the new route is generated. That's more efficient. **Return error on invalid path, don't panic** Once again though, we should opt to return an error when adding a malformed path instead of panicking. Here's a test case that demonstrates the behavior I'm talking about: func TestAddingRoutePanicsOnBadInput(t *testing.T) { defer func() { if r := recover(); r != nil { t.Error("Router panics on bad input, instead of returning error") } }() router := New() router.Get(`\p{malformed}`, nil) router.Try("test", "GET") } That panic comes very late in the game; after the user has added their route, and after they've then tried routing something to it. We don't want to panic at all though. Instead *Get*/*Post*/*Put*/*Delete* should all return an error on a malformed path. So, we need to use `regexp.Compile` in place of `regexp.MustCompile`. **Make the Router concurrency-safe** As it stands now, your router is not concurrency-safe. And that's a bad thing for a web server because concurrency is the name of the game. If 2+ requests come in, matching the same route, it's possible that they will not receive the proper parameters. This is due to an architectural issue. I've created a test-case to demonstrate the behavior: func TestRoutesAreThreadSafe(t *testing.T) { router := New() router.Get("/:name", nil) route1, _ := router.Try("/first", "GET") if param := route1.Params["name"]; param != "first" { t.Error(`For path "/first", expected :name="first", got`, param) } route2, _ := router.Try("/second", "GET") if param := route2.Params["name"]; param != "second" { t.Error(`For path "/second", expected :name="second", got`, param) } if param := route1.Params["name"]; param != "first" { t.Error("Thread-unsafe sharing between two routes parameters") } } If the responses to two routes are interleaved, they receive the same parameters because the internal *Params* map is destructively updated. How can we fix this? Well, we need to address it at a structural level. When a route is matched by *Try* it returns a pointer to a single *Route* structure that includes the matched parameters in *Params*. Think about what matching a route really means... when we match a path, we want the handler and the parameters. Not the *Route*. In fact, *Route* is just a convenient internal structure for us to represent the mapping between (method, path, parameter parsing policy) =&gt; handler. So, we need to reorganize *Route*, *Try*, and *parseParams*. There's no reason for *Params* to exist on a *Route*, so we'll remove that. Thus, there's no need for *parseParams* to be a method of *Route*, we can just make that a helper method. Now, when *Try* matches a *Route*, it simply parses out the parameters, and returns the handler along with the parameters and an error. The signature of *Try* becomes: func (r *Router) Try(path string, method string) (http.HandlerFunc, map[string]string, error) By the way, I would probably define a type to represent the parameters. type Params map[string]string Just to make things cleaner. And then you'll update the *ServeHTTP* method of your router to work with the new *Try*. That will make everything concurrency-safe. Hopefully that helps, Eslam. Keep at it, continue asking questions, and have fun!
wow ! thanks man, that was the best feedback i have ever had, i wish i can upvote your comment 100 times not just one. I'll read your comment again and apply them on my code and get back to you. again, thanks for your time and your effort :)
vim-go has helped me so much. Thank you! The award is well deserved too.
I don't understand that. AFAIK, the client still has to make the request after the push data is sent, so why would it not be able to use its cached version? 
You're welcome Eslam. And you know what, I thought about it, and I think you're right in this instance. (This doesn't invalidate my previous recommendation.) However, note the subtlety -- they panic on a different code path. (So, you'll still have to update the code.) Let's talk about why they panic because it is interesting. When the consumer of your router wants to create a new route, they're assuming a priori that the path they're passing in is valid. In other words, their code would be incorrect if the router returned an error and continued on with business. Some would argue that returning an error violates the contract provided by the router. We panic when invariants or conditions (pre-/post-conditions) are invalidated. When we panic, as a library designer, we're saying it's not worth continuing the execution of this program because it would violate the program's correctness. It's up to the library designer to decide when this is appropriate. And generally, it's rarely appropriate. In this instance, I tend to agree with you, panicking is appropriate. The difference being that you must panic upon definition of the route, not when the route is found and executed. Does that help clear things up? 
If you run the `gosimple` linter alone or as part of `gometalinter` it will probably suggest you that the first version can be simplified to the second version. I am not sure how "idiomatic" it is because `gosimple` is not part of the official tooling but as others have mentioned here it is the responsibility of the caller to check for the error.
This is true! I do wonder however, if in the instance such as io.Reader, it should return 3 values: bytesRead int, EOF bool, err error. That would be explicitly saying EOF is not an error, and you wouldn't need to compare the err value to io.EOF. Just my musings. 
I agree with you, I've had the same thought. I despise the fact that EOF is returned as an error. I understand the origins, from C, but ugh... it just creates a headache.
You don't have to build it with *gomobile bind* and then manualy copy it. When you add the *gobind* plugin to *build.gradle* then it will automatically run *gobind* each time you build the apk with gradle. *gowebview* does this by adding the *gobind* plugin to the generated *build.gradle* file. Did you find out why the `android.bat create project ...` didn't generate the *build.gradle* file? Maybe I can then builtin the workaround in *gowebview*.
Read up on javascript, ajax, websockets.
It was a great talk, glad to have been there. The video should become publicly available in ~4 months.
To help reduce some of the scan-into-struct repetition, I found https://github.com/jmoiron/sqlx to be great. Alternatively, https://github.com/russross/meddler does just as great a job and supports NULL values too. For the actual query itself, I'd stick to hand-writing your SQL queries. With the scanning delegated to sqlx or meddler, you'll find most of the functions in your data access layer will only consist of 3-5 lines anyway.
Oh, sorry, I had abandoned the OP-linked project and built my project by hand. Apparently the gobind plugin doesn't work with my version of Gradle (3.1). But now that I've been through the gauntlet I might try your project again as I'm better seeing how the pieces are supposed to fit together. My "storage permission problems" were a result of getting a filename from Termux `h4wKANYDAQA\=` and very belatedly realizing the backslash was a Termux bash escape and not part of the filename. But I found and fixed several other problems while hunting this one down, and I'm now able to go back to enhancing my project instead of making it work on Android, because now it works as well on Android as it does on the PC. :)
There are some simple examples using websockets here - https://github.com/gorilla/websocket
Nice! So compared to gin, I can also reload based on tempalting changes? Say add a watcher for HTML files? Just wish I didn't have to install node to run this. 
Hey, I made something similar some time ago, have a look: https://github.com/galeone/igor It's basically an abstraction layer over your same implementation
sqlx like others mentioned or if you feel like taking ORM route have a look at [sqlboiler](https://github.com/vattle/sqlboiler). It generates an ORM from your DB schema and is performant.
More likely that this is unlikely to be happening in a tight loop where performance is important rather than there is no performance cost at all.
You have a few options. I'll expound on your idea, propose an alternative, and name other possible solutions. As you suggested, you can refactor your prepared statements into a high level struct so they can persist and be reused, then write methods to execute queries using them and return usable results. Here's a pattern you can use. It doesn't do much about the repetitiveness, but it does let you share and reuse prepared statements: type QueryStore struct { db *sql.DB prepared map[string]*sql.Stmt // you might want to use an Enum type instead of string for added safety } func NewQueryStore(db *sql.DB) *QueryStore { q := &amp;QueryStore{db: db, prepared: map[string]*sql.Stmt{}} // add your prepared statements to the map stmt, err := db.Prepare("bar") checkErr(err) q.prepared[foo] = stmt // etc return q } func (s QueryStore) getNames(param string) []string{ rows, err := s.prepared[foo].Query(param) // error checking var results []string // row scanning // more error checking // etc... return results } Alternatively, you can use an ORM to significantly cut down on your database code. If you use Postgres, there's [`go-pg`](https://github.com/go-pg/pg). I'm a big fan of this package. Other databases have support from more general ORMs such as `gorm` or `xorm`. Here are a couple of examples of code using `go-pg`: // attempt to select first match into a struct yourDB.Model(&amp;someStruct).Where("some_field = ?", someVar).Select() or // select just the name column from all rows in a table into an array err := yourDB.Model(&amp;someArray).Column("name").Select() if err != nil { log.Fatal(err) // might be a bad idea if you get an 'ErrNoRows' } for x := range someArray { someFunc(someArray[x].Name) } Other solutions you could investigate include `sqlx` as others have suggested, or even code generation (not my cup of tea).
I literally have my editor open working on some SQL queries, this is how we structure it type LegacyStore struct { db *sql.DB logger tools.Logger statsd tools.StatsD statements *statements } type statements struct { insertEmail *sql.Stmt insertMapping *sql.Stmt insertEmailEvent *sql.Stmt insertBounced *sql.Stmt } Then there are methods on `LegacyStore` called things like `InsertEmail` which use those stored statements. I have a function called `NewLegacyStore` which returns me a `LegacyStore` and inside that it prepares all the statements for future use. 
Check out the way go-micro does it: https://github.com/micro/user-srv/blob/master/db/db.go
Based on suggestions I forked go-ps and added UID and user name parsing. Also added user info in [gkill](https://github.com/heppu/gkill/tree/feature/ps-user). Once I have tested my ps-go fork on Mac I will merge user info branch in master.
I consider it bad style. To me, it's basically defense in depth. You having a new-function already means, that you care about it having correct initialization state; by explicitly returning nil, you make sure that a user can not make a mistake here. So, my approach is a) always check for err before using returns and b) always make sure, that even if a) is forgotten, the program will just instantly crash.
I understand what you said, i implemented something in add() but i am not sure if it solves the issue. can you take a look ? https://github.com/emostafa/garson/blob/master/router.go#L67
I'm not a fan of that approach. The map indirection coupled with string interpolation makes for cognitive onerhead and removes compile time type safety. "Let's see, this is my $foo query, I think that is m["foo"], and I think I need... was it four parameters for the string interpolation?". That become hard to keep all in your head quickly, especially if there is a temporal aspect to consider (a long time between dealing with a given block of code).
Yes, more edge cases are simpler.... Instead of just checking the error. Seriously, you aren't making anything simpler. You are actually going against the grain, creating more edge cases. Zero values are valid values. I already gave a super simple case that could be expanded to most types. Just check your errors. You are preaching in direct opposition of the core go team with regard to zero values and errors.
&gt; Instead of just checking the error. You still seem to not understand that we are talking about *code I do not control*. &gt; Zero values are valid values. Neither the zero value of os.File, nor of *os.File are valid values and they could never be made that way. Blanket statements like this are never true. &gt; Just check your errors. I *explicitly* said I do, I do not understand why you keep stating that this would be an appropriate thing to say. &gt; ou are preaching in direct opposition of the core go team with regard to zero values and errors. Can you give me an example of where the go team said that it is wrong to pay attention to either return a valid value *or* a non-nil error? Because I have never heard or read anything like that. The general consensus‚Ñ¢ is: a) If possible, make the zero value useful b) If that's not possible, provide a NewT function c) Do not use any return value before checking for a non-nil error. And, to re-emphasize this again: **I do not disagree with a single one of these rules at all**. I'm simply adding, for my personal taste: d) If you can't return a valid value, don't return something that could be mistaken for it I do this, because I am aware that not everyone will adhere to Rule c above, intentionally or not. And if they don't, I want to make it easier for them to spot their mistake. Because it doesn't cost me anything, really, and "well, it's your own fault, because you just shouldn't be fallible" just isn't a thing I am comfortable telling people or to make part of my world view. If you do, that's fine.
I took another route. Instead of trying to make SQL code simple, I conceded that SQL code will probably always be plentiful and repetitive. I also don't like to use ORMs because I appreciate transparency and I want to optimize SQL as necessary. The solution I came up with is an SQL code generator. It generates the database schema and SQL bindings (SELECT, UPDATE, INSERT, DELETE) based on struct definitions. It's not a beautiful solution that requires zero work, but it saves a boat load of time. https://github.com/jackmanlabs/codegen If you have questions about using it, let me know. I'm happy to get feedback and make improvements. Most of the development has been defined by what I need **right now**.
That's really not great imo - Just global variables everywhere - Uses a global logger - Opens a database connection when you import the package, wtf. - Bizarrely uses a map to store the queries. Just use a struct! Probably more
Or maybe it's just a learning project, and OP wants to explore more about how these things work by implementing it on their own?
okay, done here. You clearly haven't got a clue. It is real simple, yet you can't seem to grasp it. Zero values are useful, zero values are valid values, you should always check errors and if the error is non-nil then the rest of the returned values should be considered invalid. Spreading the notion that zeroing a value could help find problems is absurd, and creates unnecessary edge cases. Your advice will create a wave of inept, backwards thinking Go devs. I never said all values are valid all the time. I gave you the notion that zero values are useful and valid, so why rely on the returned values without checking the errors. You are creating unnecessary edge cases, when you could just check the error. You are reading into something that is not there. And why waste time zeroing out returns if you are returning an error. rob pike and others have speckled their talks with these key points. Spark up a thread on go-nuts with this subject and I'm sure they will reiterate on the topics for you. I can't talk some sense into you but maybe they can.
Perhaps a better way to go about asking this question would've been: &gt; I'm writing a library that does x, y, and / or z. I have reason to believe that one of the things my library accomplishes follows the same interface as something in the standard library. These are the things the interface needs &lt;something that shows the method signature(s) of what you need&gt;. &gt; I &lt;can|cannot&gt; change the name of the method, so if an interface exists with a different method name I'll be &lt;un&gt;able to rename the method I want to match. &gt; Does anybody know if there's an interface that fits the bill? I've tried searching already and just couldn't find anything! Your original post gives no context, and that is precisely what you need given what you're looking for. As /u/DenzelM has said - are you hoping to blindly conform to some random interface that may be completely unrelated? If so, I'm sorry to have to tell you, but you _are_ doing it wrong at that point - if it's not advice you're here for and "have no interest in", then why bother asking at all?
If you understand how SQL escaping works, it is perfectly fine to construct the SQL text yourself. You may be interested in https://github.com/golang/go/issues/18478 .
I think nodeJS is commonly viewed as web development, even if you think it's at a lower level of abstraction than (say) Ruby or PHP. (A claim which I also feel is questionable.) Yes, web services are distributed systems. That's another thing which is different from systems programming, and another thing which also needs security. You can write your own web server in Ruby, that doesn't mean Ruby is a systems programming language. I could be wrong ‚Äî feel free to find a quote from Rob Pike saying otherwise ‚Äî but I don't think Go was ever supposed to be a language you'd use to write an operating system in, i.e. a systems programming language.
Thank you so much for your amazing work - you rock!
I'm guessing that somewhere in the server the request body `nil` is replaced with a pointer to an empty `io.Reader`. Try replacing `nil` with `bytes.NewReader([]byte{})` when you are creating the request.
that did it! thank you.
Don't use http.NewRequest for testing the Handler interface, instead use httptest.NewRequest. That should fix it and prevent a few other corner cases you will run into.
fresh https://github.com/pilu/fresh is much cleaner in my opinion, both on functionality and how it's implemented.
Thanks for the idea, I will do this next time I write queries!
The specific answer i was going to give has already been done (use httptest). But I found this was a good resource for figuring out how to test endpoints in Go in general. Take a look, it might prevent future problems: https://elithrar.github.io/article/testing-http-handlers-go/
Why are you not using Go 1.8 yet?
&gt; Spark up a thread on go-nuts with this subject and I'm sure they will reiterate on the topics for you. I can't talk some sense into you but maybe they can. [Feel free to chime in](https://groups.google.com/forum/#!topic/golang-nuts/lcZNt-nMsl0). I'm nothing if not open for arguments and having my mind changed.
This is almost certainly AFJ, but I hope it happens anyway.
Quaternions aren't that rare, I think this would be nice. Now it might like make one think, why not 8 component complex things? Although I haven't seen those in use.
I run a gofmt check in CI as a requirement for PRs. If we don't exclude the vendor folder, anything not gofmted there will fail the build. We really don't want to have to make gofmt prs to every library we use that isn't strict on it, but we also don't want any of our code to fail it. Our current script to exclude vendor is much uglier than it will be if the OP change is implemented.
Pretty elaborate if so... http://instantshare.virtivia.com:27080/1i68nvcs7h5ov.png http://instantshare.virtivia.com:27080/1irr9bqt7wdvg.png
Awesome timing then.
Go 2 is most certainly planned, it has been since Go 1 came out. The _release date_ of Go 2 is not planned. But Go 2 itself is. See https://github.com/golang/go/labels/Go2.
That was my impression in the past, but now I believe they are planning Go 2 in secret and will reveal it once they have a roadmap or something. 
Kinda awkward that the first tutorial is at the bottom; maybe reverse the order?
there are lots of issues with this code, just wanted to get it out there :)
hmm I'd like to stick with the standard library, and I think there should be a way to do what I want in a nice way. However if I won't be happy with any other solutions I'll definitely consider that
Yeah, I don't think it's ever going to happen. But what if it's done in 50 years and there's a `go2fix` command that automatically updates all Go1 code to be compatible with Go2?
An update after 6 hours. I added some pixels. I don't know if others did. Current reality: http://instantshare.virtivia.com:27080/hctbocutfdms.png This is what I'm imagining/trying to go for, roughly: Plan: http://instantshare.virtivia.com:27080/28rfro4lekfq.png Yeah, it's not very close...
Thanks for the feedback. Yes it makes more sense to sort the tutorials in reverse order. Right now it uses the default ghost blog view. Have to design a new page. 
may be go++ :)
We have a tool published, [xo](https://github.com/knq/xo) that makes this workflow a breeze. You can write your own Go templates that can then be used with the types from your database. Alternatively, you can write your queries in something like `.sh` script, and then have `xo` generate the wrapper code around it (again, using custom templates if you'd like). Also, if you're doing any kind of database work, check out [usql](https://github.com/knq/usql). Cheers, and good luck!
sqlx is pretty much extensions to the standard database/sql. I was also doing the same thing and try to write my own solution or wrapper to the standard library. Best of luck!
Awesome. I book marked it. Keep up the hard work!
How about we make this more interesting? Let's attempt to write a Go program that draws one random correct pixel every so minutes or something like that.
Thanks! I enjoyed it and learned something.
Polite and quality post, thanks for taking the time to write this for op.
Hmm, maybe not :)
I would predict that if Go 2 ever happens that Go 1 code would still be likely to be supported as-is, and to use Go 2 features you'll declare something in the source to make it Go 2. Think the way C++ can call C. It would probably be easier to use Go 1 code from Go 2 than use C from either. So there won't be an agony of community bifurcation, where all the code has to be upgraded.
&gt; You need to sign in to see this page. Could you copy/paste it here please?
I get several thousand of these errors quickly all for different domains and urls. I just posted one example and didn't want to post a repetitive looking log.
rob pike did something like this last year or the year before, I forget. Came with CLs and everything.
I have been messing around with the some of the http client's settings: f.HttpClient = &amp;http.Client{ Transport: &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true}, Dial: (&amp;nett.Dialer{ Timeout: 30 * time.Second, Resolver: &amp;nett.CacheResolver{TTL: 10 * time.Minute}, }).Dial, DisableKeepAlives: true, }, Timeout: 40 * time.Second, } So, I am caching the DNS lookup for 10 minutes and setting some timeouts. Is there anything else I can try?
&gt; Related: Are there any existing programming languages that do support quaternions? [Kawa](https://www.gnu.org/software/kawa/Quaternions.html) and [Chicken](http://wiki.call-cc.org/eggref/4/quaternions) both provide them as extensions of Scheme's standard numeric tower.
Thank you
The full gist can be found here: https://gist.github.com/kristen1980/9d689b6ae0ab9f8a330c4598060295e4
The "same" code in Java and Go back will always be quicker in go. And it will have less memory over head. Same is true for all compiled to machine code languages. Sure there are lots of tricks the JVM can do to minmize the gap in lots of cases, but the gap will always be there.
I find that Java usually spends 70-80% of its time in the GC rather than running your application code. My impression is that it is fast *between GC runs*, but not overall. I thought that Go was better than that, but perhaps it's still just as slow overall because it's slower between GC runs? I realize that Go is a fairly dynamic language (sure, *types* are static but there is a lot of runtime polymorphism if I understand the language correctly) and doesn't have the JIT that JVMs do.
Given a problem, a solution in go can always be faster. I don't understand how or why people dispute this. 
I understood so little of that git issue, that I didn't even realise it was an April fools joke. 
Show me some Java code that would not be quicker in go or c++
Thank you! Very informative and enjoyable as always!
Ever heard of google? Just type "x vs. y" and hit enter. You'll get results such as: https://benchmarksgame.alioth.debian.org/u64q/go.html And then: Reproduce and verify. Now, just to let you know: c++ is not go. That again is another story. We all love go for the simplicity of the syntax, the features it has (or not), the mindset of the community, the fantastic proverbs and so on. But then there is also the compiler. Its not the same compiler as c/c++ and those compilers do not generate the same binaries. You cannot compare c++ and go. Go's compiler and runtime are great, but its not by default better than JIT just because you sad so. Thats not how computers work.
Hi, do you have examples of malicious input that use to panic, but no longer do with these changes? I thought the json pkg was already resilient to panics so curious to see how ya broke it. Edit: I wish I could say this reply was a carefully crafted April fools response, but it was just me being fooled. Lol. I didn't realize it until I clicked the diff just now... that code is amazing lol
I have been using glide in gitlab via a special build container. By using a build container, I get a standardized environment that doesn't contaminate the gitlab job runner. You can incrementally add to it and update it as needed. Frickin love whales.
what do you mean by "contaminate"?
Why not this? func main() { router := garson.New() router.Get("/hello", anotherCustomMiddleware(hello)) http.ListenAndServe(":3000", someCustomMiddleware(middleware.Logger(router))) } In this way a middleware can act on individual routes, or the whole router. No need to have before/after - just wrap at whichever level. BTW I think you're totally on the right track with plain http.Handlers and with `func(http.Handler) http.Handler` - this makes your work more flexible than other routers &amp; middlewares (negroni/httprouter/etc etc), which use their own custom parameter specs. Not a criticism of their authors - they were working in a pre-`r.Context()` world, which forced their hands a little. Good for you. Edit: sorry I forgot to include an example which does middleware-type stuff before/after running the inner handler/router/middleware. See below: https://github.com/laher/context-example/blob/master/main.go#L124 That repo is just some code samples I made for a meetup talk. See readme for more notes. Does that answer your question?
@R2A2 my idea was that, you might need to execute some code after each response, for example, if you want to store some logs, how would you do that using: router.Get("/hello", anotherCustomMiddleware(hello)) another thing that i have in mind, is if i have like 3 or 4 middlewares that i want to execute before the request, that would be something like this ? router.Get("/hello", middleware1(middleware2(middleware3(hello)))) my idea that it could look like this router.Get("/hello", hello).Before(middleware1, middleware2, middleware3).After(middleware4) of Use() of the whole concept of Before() and After() is wrong: router.Use(LoggerMiddleware) router.Get("/hello", hello).Use(middleware1, middleware2, middleware3, middleware4) glad you liked the idea of going with plain http.Handler, this way my router could be flexible and to be used with any other libraries. I am glad that i am did something good :D thanks for your motiviation.
Of all the fantastic engineering problems out their waiting to be solved, you choose to write (another) shady tool to scrape for people's personal information? I've seen a couple ULAPPH (is this the worlds biggest acronym or a poorly spelled word??) posts from you and if your truly passionate about your project and want to promote it, don't associate shady scrapers with a cloud product. Cloud products need absolute trust from your consumer to respect their privacy, making a utility devoted to invading privacy works against that. Plus, you're better than that, man. 
The wishy-washy (paraphrased) definition I got taught "any language where you cannot ignore the underlying properties of the machine its running on is a systems programming language". Java, Ruby, Python, and Javascript come down clearly on one side of this definition, while C and C++ fall on the other (at least historically, this is ever changing). Go has a foot in each camp- it's runtime manages most of the mess of dealing with the hardware for you when using it for web backend work, but forces you to manually handle errors/exceptions, a trait far more common in a systems programming language. Taking the best of both worlds, according to the opinions and needs of the folks who designed it. I reject the notion that just because a system is distributed it ceases being a single system. The Systems Engineering discipline is dedicated to the problems of integrating many interdependent pieces of hardware and/or software. Take a look at the challenges Amazon, Facebook, and Google have had building their backend infrastructures out; or the challenges of integrating all the hardware and software in the many sensors, weapons, avionics, and propulsion systems on the F22 or F35. Systems programming != *operating systems* programming. I don't like [quoting wikipedia](https://en.wikipedia.org/wiki/System_programming), but hey, it agrees with this take on it. "System programming (or systems programming) is the activity of programming computer system software. The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines and AAA video games, industrial automation, and software as a service applications)." 
https://golang.org/pkg/os/exec/#example_Cmd_StdinPipe - is that what you're looking for?
Go typically spends more time in gc than Java. It's tuned for latency, not throughput. It has shorter pauses but many more of them. 
I might very well use this soon. It's unique in its space and well-presented. Thank you for sharing!
Go produces less garbage in many situations since you can store value types directly in data structures unlike java. But on the other hand, go's gc trades latency for throughput as a design decision which can tilt the scale. There's a good discussion about it [in this comment chain](https://www.reddit.com/r/golang/comments/5j7phw/modern_garbage_collection/dbe16d6/) as well as some numbers.
From what I see, Go spent about 25% of its time in the GC and Java spent about 10%. That roughly matches with my expectations for Go's GC, but I'm very surprised by the Java numbers. I always thought it spent much more of its time pausing. However, I do have limited experience with Java, so perhaps my personal experience sample size is a bit too low (just Minecraft and FRC).
Golang's April Fools eh
Out of curiosity, what did you learn?
Can't you just check if the OS is Linux and only then print the extra infos?
There is no reason to close prepared statements. They are closed with the connection. Keeping prepared statements open is their entire purpose. If your service is intermittent, then just let the connections die off after no activity. 
It looks like the HTML template does not use inlined CSS. This can be a compatibility problem; if you Postmark to send the emails, it handles inlining automatically for you, but most other SMTP SaaS don't.
It's my pleasure, thank you for the recognition. I love the Go community. Whenever I have free time, I like to lurk and lend a helping hand.
Thanks for the explanation! Just out of curiosity, how are optimisations applied? Are they applied on the SSA form? 
Could we get invites to the slack please? 
Check out the way Gin does it.. super succint and clear
I cant find the gopher on the r/place picture. Did we get a spot? 
Good question: I don't know the full answer. There're definitely optimizations applied to SSA, and appears to be optimizations applied to the syntax DAG (such as constant folding), but I'm not entirely sure if they do any AST optimizations. Haven't really gone that in-depth. If you want to have a look for yourself, the majority of the code can be found here in [gc](https://github.com/golang/go/tree/master/src/cmd/compile/internal/gc) and [ssa](https://github.com/golang/go/tree/master/src/cmd/compile/internal/ssa) (here's the [constant folding routines](https://github.com/golang/go/blob/master/src/cmd/compile/internal/gc/const.go) for example).
Note Scala is my main language I am no stranger to Go though. Honestly I believe for most simple benchmarking tests Go will win, IRL though there are situations where go wins, situations where java wins. Let's give an example if implement the actor model in both Java, and Go, (don't say who needs actors go has CSP the JVM has csp too and a better implementation), with Go you would probably implement it with two buffered channels, and then run it as a go routine. In java you would have two concurrentlinkedqueues, and have it run as an executor on top of a forkjoinpool. The java one would perform better not because java is great, so much as Java has an n:n threading model, java.util.concurrent offers better primitives than go's sync package. Another example would be the fact that in the Go world, when you're writing servers with shared state, you wind up using mutexes a lot out of necessity unless you want to use Atoms, on the JVM people do use synchronized a lot, but in scala you have the Concurrent TrieMap which doesn't lock, and when you want to make data structures that don't lock and just use CAS operations, this is where Go's lack of generics is a bad thing, another example would be when I ported haskells Tvar (STM) to Scala, by using twitter utils Tx (Two Phased Commit), and the state Monad in cats. TLDR: Go's defaults are better than the JVMs, the JVM's concurrency support is better overall due to it's extensibility, making you able to build better primitives. The thing is the latter part really only matters if you know a lot about concurrency so for the average developer go is better.
perfect, but now how can i intercept the reader from the launched program? i would like insert the text from command line and pass it to my script.
Maybe if you expressed in detail why, it would help? You're being down voted by people here who have a lot of experience that suggests to the contrary. What you're talking about, arguably, is so complex that it's not even remotely possible to make a flat out statement like that that is true.
Do you realise how little time that compilation takes? Plus, the JIT is aware from the actual running of the code, where there are optimisations that the writer of the program have missed, and that the static compiler can't see.
A lot of work has gone into `usql` over the last few weeks. I'm extremely happy with the progress that has been made so far, and in my humble opinion, it's rapidly closing in on `psql` in terms of functionality, with the added benefit of working with multiple databases. With v0.5.0, `usql` now has the following features: * Variables on command line (using the `\set`, `\unset` etc commands), including backtick (``) expansion, and quoting * Password support for databases (via `.usqlpass` file), and ability to change passwords with `\password` * Startup scripts (via `.usqlrc`) * Fairly complete set of the the other `\` backslash commands * Support for every major database, PostgreSQL, MySQL, Microsoft SQL Server, Oracle, ODBC, and SQLite3 (Google Spanner support is coming soon ...) * Additionally, available support for practically every SQL database/driver available for Go Install it in the usual Go way: ``` $ go get -u -tags most github.com/knq/usql ``` Appreciate feedback! Thanks! 
The Eclipse developers have been spending a bunch of time recently trying to understand that. Particularly because other IDEs that run on the JVM, like IntelliJ, aren't. When they started out seriously looking at it, they found all sorts of low hanging fruit such as unnecessary complete screen redraws etc. The problem hasn't been the JVM, so much as the logic in the actual code, stuff out of the scope that the JIT can catch and optimise out of the way. You can write slow code in any language üòÄ
I had done the benchmark, before 1.8 came out, but both give simillar results (1.7 is a little bit faster). 
Well I'm new to Go and I didn't know about the context package, so pretty much everything in the video was new to me. 
Minecraft is a notoriously poorly coded game. The memory churn in the game was huge. I don't think there were any object pools.
Sure, use https://invite.slack.golangbridge.org/.
See [generic.rules](https://github.com/golang/go/blob/master/src/cmd/compile/internal/ssa/gen/generic.rules) for some of the SSA backend generic rewrite optimization rules. [compile.go](https://github.com/golang/go/blob/master/src/cmd/compile/internal/ssa/compile.go) starting around line 333 lists the other optimization passes (```opt``` in this list corresponds to the code generated from generic.rules). The other optimization passes are in src/cmd/compile/internal/ssa/ as well.
SSA form is IR, here is [how to view](https://www.reddit.com/r/golang/comments/5vicuj/go_range_loop_internals/de3dc2g/) it.
This pull request was merged into the dev branch.
Sure, SSA is a property of an IR thus making their representation an IR. I mean heck, by definition, anything between parsing and target code generation could be considered an IR. And I'm smart enough to admit that I don't know how Go represents its SSA. Is it still in a syntax DAG? Because generally when I think about "IR" in the narrow sense, I think about 3AC (three address code) represented as a set of basic blocks that has the potential to be output and manipulated in a nice human readable form. Does Go's IR exhibit those properties? I really haven't looked deep enough.
Oh man, I am so in love with the Go mono font. I've been using it for a while now as the default Ubuntu mono font (size 13) which not only makes the console letters look nice and crisp but the Go code on vim looks absolutely amazing. Go code already looks pretty good on paper and yet this font somehow makes it look even better.
I'm not expert myself, but as far as I am aware their is not a parser for the SSA syntax since it operates on the function level, so it does not have a AST. However the gc compiler's AST is a DAG, so not sure if that satisfies your syntax DAG question? A parser could probably be created, but you couldn't create complete programs without extending it a great deal to support packages, visibility etc.. which I suppose at that point you reinvented LLVM. Now the reason I really like the SSA is it can be printed in a nice human readable form, even if it can't be modified and parsed (as far as I know, I would to know if there was) with ssadump. You should check it out if you haven't next time your debugging something with lots of indirection and assignments, **ssadump -build 'SF' main.go**. But thanks for replying, I think I get it now, OP was looking for something they could write like LLVM's language. I would have to say the closest thing to that is Go source.
hmm, interesting, when we say manually convert do we mean SomeCleverDistributedAlgorithm.AbstractServer(NewFakeServer()) I tried that out but reach the same error too. 
I was becoming disappointed that almost all of Google's April fools jokes were marketing videos, and not actual product Easter eggs. Glad to know some software engineers were trying this year. 
I don't understand why this matters at all? In the past, I used the domain for my consulting company that is more or less quiet at the moment. I can assure you, [you're not missing out on anything](https://web-beta.archive.org/web/20161025203447/knq.io) with that page being down. If you'd like to make a page for KNQ, I'd gladly put it up.
I can matter because it can impact your reputation as an open source author. Having a broken link vs. a blank page vs. not displaying the link at all can mean different things to different people depending on who's looking that's all. People tend to care about this kind of stuff even if it doesn't make sense and they can't read your mind when it comes to intention. If it were me, I'd just remove the link so it doesn't confuse people.
Thanks üòÅ I didn't know that those kind of things existed. Might have to do that.
I was able to find Kubernetes's ABAC: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go
If we're being pedantic, SSA is a property of an IR, not a kind of IR. I can lower an AST, straight from the parser, into SSA just as much as I can lower 3AC into SSA, but one takes more work than the other. And they'd both be different kinds of SSA IR. But I get your point.
You misrepresented the topic. you over simplified. You also changed the example program quit a bit. There is a difference between types and pointer types. performance-wise, nil-ing out a pointer type and re-initializing a variable with a type with zero values is two different things, with different penalties. plus, andrew and I go way back. I won't play his games. I can not post there, once its known its me its instant ban. It is kind of the reason I asked you to make the post. My points are valid. cheney agrees with me on errors. Ian seems confused, he is missing out on the performance penalties. I think it has something to do with the problem being misrepresented. I'd like to hear from others on the core Go team about this, possibly russ, rob, or robert, but it is the weekend, give it time for the conversation to develop - maybe even link this thread. I gave my opinion on the subject, no sense in making a burner account to answer to one topic. I'd like to add that if it were pointer types being returned that I would nil them out for the performance gains. But as cheney says I would not attempt to make any guarantee with the caller about the returned values during an error. I'd like to mock up some examples to illustrate my points. Also this thread of converse went on a tangent starting with my comment to nate which loosely relates to the original post. Within this thread I made claims that it is foolish to rely on the other values returned when there was also a non-nil error returned. Then I asserted that it is foolish to zero value types for this reason, which both you and nate claim has the potential to reduce programming errors. I also gave performance reasons as to why this pattern shouldn't be followed wholeheartedly to a fault. Check your errors and code appropriately. The pattern suggested by nate could potentially produce performance problems with no benefit. the example given by the OP is faulty. Those reading it may be confused, which could contribute to the misunderstanding of the problem. here is a play illustrating the possible misunderstanding of the OP or typos by the OP. https://play.golang.org/p/rt31qb9EiH Is the returned value of Foo() a pointer type or a type? If the value res is a pointer type then it might be beneficial to nil the pointer, but if the value is a type then it might be beneficial to just return the invalid value instead of re-initializing a new value of that type with zero values. But none of this should matter to the caller, check your errors. In conclusion, it is hard to make sense of the OP example, however, the advice I have given is solid.
I read it, but you are ignoring everything I wrote.
Thanks /u/campoy for this helpful tutorial! It's actually very timely since I'm at the point now of building an HTTP server where I would like to clean up all the resources when a request is cancelled. I learned how contexts are used in general because it can be very confusing to think in concurrency when using Go, which you were able to clarify. You explained how the "tree" structure of contexts, which was the trick to understanding it. Previously, I was very confused with the point of a background was but it's in fact very simple. But the most important and useful trick was how to use the select statement to immediately to free up resources across goroutines, very helpful! I hope you continue to create more videos like these. I like to believe your motivation for choosing such a topic is that you have a good feel of what people are struggling with when it's pretty simple, and even very powerful - such as the impression of the context package. Thanks!
If you haven't noticed, the top mods are removing content again. they have a tendancy to dictate the flow of communication. nazi-style administration. This is why I often change accounts. In all the years using reddit, I have never had to deal with such bureaucracy. This subreddit is in a sad state of affairs. I have valid points with regard to the 'adoption' of gofmt. gofmt was never adopted. It was forced on the community. Just as aliases are being forced on the community. Like I said, I get paid to evaluate the Go ecosystem and the ecosystem build upon it. And the evidence is factual and not opinionated. However, this is about all I will say on the subject as dgryski has an agenda, and that agenda limits the users of this sub. I save everything removed by dgryski, and my colleagues save their removed content. Soon we will approach him in a public forum among his peers. We watch, and we wait for the time(go conference and the such).
Microsoft did not come up with the name 'usql'. There have been a number of projects named 'usql' that have been around for years, including a [very similar project to what I've built](http://usql.sourceforge.net/) -- that project has not been updated in 13 years. Since the term predates Microsoft's use of it, and has been used by a number of projects, I don't think this is a problem. Quite frankly, 'usql' is too generic of a term to be claimed by anyone.
whitespace is whitespace. that line break shouldn't make a difference, but it does, and to fix it one must use gofmt, which goes against the OP claims that compiler will accept any style. I'd refer you to my other comments but those were removed, and I will be damned if I going to waste more time than what it takes to type this.
But that has nothing to do with `gofmt`. Every compiler/interpreter has syntax rules. Go's isn't the only one that treats a newline as an implied semicolon in some cases. You were saying that `gofmt` isn't a choice because the compiler won't accept code that isn't formatted by it. That's not true. The compiler will accept any valid code, and reject any invalid code. `gofmt` does exactly the same.
I can't even imagine how someone can use this style of font for coding and even consider it amazing. Source Code Pro and Input Mono is what I would call amazing.
Oh man. You are far gone‚Ä¶
FWIW I've built two fairly high throughput pipelines using Go on AWS. The most recent relies on AWS Lambdas with Go (https://github.com/eawsy/aws-lambda-go) triggered by S3 bucket events, pushing into an SQS queue system, which is read by a Go message parsing/handling microservice. Definitely not as fancy as what you're describing, but it does the job.
It is a matter of taste but in some cases it's a matter of readability as well. What I've learnt over time is that besides the display being used, the font configuration makes a lot of difference. I tweaked my settings many times to achieve the perfect config, system wide as well as per font (Linux system). A great font looked horrible with one configuration while with a different configuration the very same font looked excellent. That applies to Fira Mono as well, which I've used for a long time in the past.
One uses Electron and is cross-platform while the other does not and is not?
https://github.com/golang/image/commit/f03a046406d4d7fbfd4ed29f554da8f6114049fc font/gofont: new Smallcaps; update other Go Fonts. Bigelow &amp; Holmes have prepared new versions of the Go Fonts. The version numbers have changed from 2.004 to 2.008. ChangeLog: * New! 2 new fonts, bringing the total to 12: Go Smallcaps and Go Smallcaps Italic. * New! U+FFFD REPLACEMENT CHARACTER glyph added. * U+00C5 LATIN CAPITAL LETTER A WITH RING ABOVE ring tweaked. * U+00F0 LATIN SMALL LETTER ETH glyph lightened. * Superfluous U+0005 &lt;control&gt; glyph removed. * Split integral, chart draw, block, shade and related glyphs now span the full height and have fewer seams when tiled. * Raised most capital letter diacritics and some lowercase diacritics. * Mono character widths fixed to be uniform. * Underline position adjusted. * SFNT glyph order and cmap table optimized. * SFNT glyph names (post table) regularized, consistent with the AGLFN. * ttfautohint upgraded to the latest version, from 1.5 to 1.6. 
I find your question interesting, but perhaps lacking a little detail. Could you perhaps specify a little more what features you are looking for? I mean, Go is pretty much a stream-processing / pipeline framework already, with the channels and Go-routines, and personally I have found that all that is needed to work effectively with "streaming" components, is to encapsulate go-routines in structs, with in- and out-channels as struct fields, so that you get processes which are easy to connect into networks outside of the process implementations (aka flow-based programming). I have wrapped up some examples and very minimal supporting code for my own work in this paradigm at http://flowbase.org , and has written a concrete tool in the paradigm, of which the network code can be seen here: https://github.com/rdfio/rdf2smw/blob/master/main.go#L44-L136 This is all stream processing "inside the Go binary" though. Are you looking for something to connect multiple Go binaries etc? Feel free to elaborate!
&gt; You misrepresented the topic. I don't believe I did. I might understand the topic differently from you (and indeed, that seems to be the crux of the matter here; that you - and Dave - understand my position to be fundamentally different from what I'm actually saying). If you want to remedy that, learn to express your opinion constructively and to engage in a dialog, instead of just ignoring what I say and restating your opinions again and again. &gt; plus, andrew and I go way back. I won't play his games. I can not post there, once its known its me its instant ban. The fact that many people seem to agree that you are being unconstructive should, at some point, give you pause and make you reconsider your approach, but I'm not optimistic. &gt; I'd like to add that if it were pointer types being returned that I would nil them out for the performance gains. Can you demonstrate any performance gain? I don't believe there are any. &gt; cheney agrees with me on errors. And so do I, as I stated explicitly and repeatedly. You are simply ignoring that part and try to forcibly understand my position to be different from what I'm actually saying. &gt; But as cheney says I would not attempt to make any guarantee with the caller about the returned values during an error. And as I repeatedly said (ignored by both you and Dave), neither would I. &gt; Within this thread I made claims that it is foolish to rely on the other values returned when there was also a non-nil error returned. Literally no one ever disagreed with that in the slightest and literally everyone always explicitly agreed with this. &gt; Then I asserted that it is foolish to zero value types for this reason, which both you and nate claim has the potential to reduce programming errors. This is entirely untrue. We claimed that it helps discover and debug the error, when it inevitably happens. Again, it would be helpful for a fruitful debate, if you'd actually read what we are saying, instead of making up opinions we never had. &gt; I also gave performance reasons Can you point me to a benchmark of any of such performance reasons? I don't believe they exists (but are willing to be convinced otherwise). &gt; as to why this pattern shouldn't be followed wholeheartedly to a fault. No one ever claimed that this should be followed "wholeheartedly". Quite the opposite.
Nobody said that but ok, continue to not being able to imagine it. 
Yeah, I tried this last week and it works fine, but the compile time is a little bit slow.
If experienced gophers stopped writing books and instead contributed to improve the official docs we would now have the best docs in the programming universe.
Something reminds me of D. Bad on line docs since last decade, very frustrating to newcomers. But then a big shot (one of main devs) wrote a book on how to use it, so people could finally learn it if they cash out some $. Definitely wrong way to popularize own language.
I would like a solid article about best prectices, DOs and DONTs, how to layout the structure of the application.
It seems like you are asking, "I have 7 TiB of bcolz files, is there a Go library that can access them?" So the first thing you should do is go to search Godoc.org: https://godoc.org/?q=bcolz There are no results for that, but perhaps you'll know other search terms you should be using there... like blosc, maybe: https://godoc.org/github.com/brentp/go-blosc Good luck! -jeff
Do they have to restart the webserver every few months to get a new cert though? Because that's how it seems to currently work, and it's a major pain in the ass and I'd rather just buy a cert if that's the case.
Am I wrong or is there only a mono font with serifs? I hate serifs.
in my honest opinion, you shouldn't use Go for this task.
Check out Caddy, hot reloading of let's encrypt certs. 
Something something something HAproxy
http://pachyderm.io/
In my case, its not the entire ETL stack. More on the 'T'.
As someone mentioned, there are already quiiiite a few books. For example: https://www.packtpub.com/application-development/go-programming-blueprints Obviously there's room for more/better all the time, but a thorough review of what's out there would be a good place to start!
&gt; third party tools wot 
And why shouldn't Go handle this itself? Like I said, small business website.
More overkill than re-implementing TLS? I don't get it.
This is a replacement for your reverse proxy, not a package. Sorry if that's what you're looking for :-(
which license means "do whatever you like with the code"?
Amazing work! Any plans for this to be added to the standard library at some point?
Ah thanks, I've updated my answer.
Thanks a lot, I would update that as soon as possible 
Looks like it's for when you're developing; you save your file, it passes the tests, then your server restarts, instead of you having to go to your terminal, Ctrl+C, Up, Enter, then your browser, and F5.
http://www.wtfpl.net/
The community will always benefit from your work and the inverse is likely to be true. 
Previous thread at https://www.reddit.com/r/golang/comments/61r6m0/whats_your_experience_with_go_on_mobile_bind_only/
This post is literally about how it can now be done in one line. Nothing to "re-implement".
Bleve will do basic querying. It has numeric and date range support, as well as field scoping. If your need for SQL is limited, then Bleve might be enough on its own.
Concurrency in Go "Tools and Techniques for Developers" By Katherine Cox-Buday http://shop.oreilly.com/product/mobile/0636920046189.do I ran into Katherine's editor at my neighborhood market mid last year. I've not preordered it yet, but there's a solid chance it's worth picking up.
Nice, looks solid.
Please write it up. I love learning from people who use Go to solve actual problems and not just the basic "We are going to create a tasks app..." form. Great job.
Very interested in this type of work and article. Please let us see it :)
Yes I'm very interested! Please keep us updated. Edit: I've been working on a very similar solution, essentially an alternative to Nutch, for over a year as my FT job. Unfortunately not in Go, but you can see why I'm interested.
&gt; if recover() != err { wat
It was an April 1st joke. See the last message I posted on the CL.
Ah that's wonderful news. Thanks Brad.
You don't have to "reimplement" anything. It's already done for you in the stdlib.
A point of confusion for me and go bind, can I use channels in my Go code as long as I don't expose them as return types or paramters?
Instead of working on a copy of the upstream repository (github.com/me/project), work on the repository itself (github.com/upstream/project). Add a new git remote that points to your fork, so you can push your changes to your fork (and then submit a pull request) when you're done. There is no need to ever touch an import path if you're planning on contributing upstream.
good point, I will try that, thanks.
What is the end user installation experience like?
I'm interested ! Since i'm a heavy user of scrapy, I feel that it has too many limitations because of our needs and was thinking to migrate to golang too. Do you handle Xpath like scrapy ?
Does your crawler deal with client-side rendered pages (e.g. react, angular)? I've found that there's a lot of complexity related to rendering these pages before saving them to disk. Curious to know if you managed to solve this! 
What are those limitations of Scrapy? Last time I used it it seemed very flexible.
Please do!
Noted down. I only worry about the inherent complexity of GUI libraries, both at API level and at build level. Most (if not all) GUI libs have external dependencies (GTK, QT, Electron) or at least require a working `cgo` toolchain, all of which must be installed and tested first. This could easily go beyond my time frame for researching and writing an article, except perhaps if I drop the "Code" part completely (but then it would be little more than just a list of Go GUI's). Or I write a series of articles instead, one GUI at a time... Definitely have to think this through.
That's what I do - Pixel C with physical keyboard works surprisingly well.
Yes, channels and go routines work as they should. Besides code sharing this maybe the biggest win, having the great concurrency model of go available. Take caution to understand the application life cycle however for each platform.
Hi, thanks for your detailed answer. I should have been more precise but elasticsearch isn't a requirement on this project. Actually, I'm going to use Redshift to store the data. 
Sounds like a great codebase with lots of interesting things to learn from. I'd love to read through it, and would definitely contribute back if I used it. Please open source it sooner than later. :)
Thanks ! For anybody else, React Native is mentioned a few times starting at 14:55 min. 
I've build my own crawler as a side project in the office in go but it's limited (by choice) to crawl a single domain and get all links, status codes and link sources in a .csv file. I would be very interested in your crawler. What is your crawler crawling? What do you need 1TB full of links for?
Which would explain the regex stuff in the description.
FYI blosc is just a compressor it does not provide any data structure for containing, indexing and representing your data.
I'm a huge FOSS advocate, but this is clearly an address-harvesting crawler for spammers. I don't see how supporting this will benefit the community.
&gt;I don't believe I did. I might understand the topic differently from &gt;you (and indeed, that seems to be the crux of the matter here; &gt;that you - and Dave - understand my position to be &gt;fundamentally different from what I'm actually saying). If you want &gt;to remedy that, learn to express your opinion constructively and &gt;to engage in a dialog, instead of just ignoring what I say and &gt;restating your opinions again and again. You did misrepresent the topic, and the fact that you can't seem to follow the thread is concerning and the reason I stopped the conversation in the first place. It is not my fault that you are a moron. So before you start something on a personal nad professional level I suggest you cool your jets and watch your language. &gt;The fact that many people seem to agree that you are being &gt;should, at some point, give you pause and make you reconsider &gt;your approach, but I'm not optimistic. Andrew has a tendancy to control the flow of communication, nazi-style administration. I am not the only one who has issues with this. The greater go community went ape shit over the issue many times. Get out from under your rock. &gt;Can you demonstrate any performance gain? I don't believe there are any. Keeping a pointer to data keeps the data its connect to live, so passing that non-nil pointer back to the caller keeps that data alive. Now you think long and hard about those implications, and I am sure you will see the performance gains. But then again, you already showed you are a moron, so I might be asking too much from you. &gt;Literally no one ever disagreed with that in the slightest and &gt;literally everyone always explicitly agreed with this. The point you don't get is that people are calling you foolish for taking extra wasteful steps. Your whole argument is that nil-ling things help prevent errors, when just checking the error would prevent misuse of the value returned with that error. &gt;This is entirely untrue. We claimed that it helps discover and &gt;debug the error, when it inevitably happens. Again, it would be &gt;helpful for a fruitful debate, if you'd actually read what we are &gt;saying, instead of making up opinions we never had. You claimed both, and both claims are absurd. &gt;Can you point me to a benchmark of any of such performance &gt;reasons? I don't believe they exists (but are willing to be &gt;convinced otherwise). So you need benchmarks to prove that initializing variable twice cost the same in duplicate. I am not buying that you are that stupid, so I am not wasting my time with such silly benchmarks. &gt;No one ever claimed that this should be followed "wholeheartedly". &gt;Quite the opposite. But you are, you are asking others to follow a pattern wholeheartedly, don't back-peddle now. If you didn't misrepresent the issue then why is your example program different from the OP. Clearly you are confused. Did you even check the play I linked? there is clearly no support for your claim on go-nuts. In conclusion, you are a fucking moron, and I am done with you. 
With Gotron you have to deliver two different executables (the user will only see one of them). We'll have to create installers to guarantee a smooth user experience.
You are a fucking moron, if you expect the compiler to optimize away your stupid as fuck pattern then how would you use it to debug. You are talking out your ass in an attempt to save face. Create the benchmark that proves me wrong. Create a benchmark that initializes variable of differing sizes, then initialize them in duplicate, then in triplicate. logic is on my side, so until you provide the benchmarks that prove me wrong I will wholeheartedly call you a fucking moron. You still haven't answered to the fact that you example program is different than the OP.
The SQLite amalgamation file is very portable C code. Compiles on every major platform. 
That conclusion came out of left field. What's wrong with that plugin flow that you think won't catch on?
Blog post would really be appreciated. Currently I personally will not use the crawler but maybe in the future. 
PROVIDE THE BENCHMARKS THAT PROVE THE COMPILER IS OPTIMIZING AWAY YOUR STUPID AS FUCK PATTERN. THEN PROVIDE A REASON TO HOW ONE WOULD USE THIS PATTERN TO DEBUG IF THE PATTERN WAS OPTIMIZED AWAY. Until then, I am done, cause nobody can get through to you - you are that fucking thick. Having the last word doesn't make you right. You keep reiterating over the same stupid stuff, even though nobody but nate has agreed with you. 
That would be cool !
I would use something like this for general web archiving and curation purposes. I bookmark sites and they die and I lose information because the site is gone. This may be a great tool for this use case. 
Not sure what was deleted but as a heavy redshift user, you are correct. You definitely do not want to perform typical SQL inserts one-at-a-time. Bulk loading via S3 is the way to go with columnar datastores.
So would you say that we should censor knowledge because it might be misused?
 author := From(books).SelectMany( // make a flat array of authors func(book interface{}) Query { return From(book.(Book).authors) }).GroupBy( // group by author func(author interface{}) interface{} { return author // author as key }, func(author interface{}) interface{} { return author // author as value }).OrderByDescending( // sort groups by its length func(group interface{}) interface{} { return len(group.(Group).Group) }).Select( // get authors out of groups func(group interface{}) interface{} { return group.(Group).Key }).First() // take the first author Nope. I would not allow this in my codebase.
My prior font was Ubuntu Mono. I find the Go Mono font easy to consume. The information reads quickly enough to make data seem to have added complexity with an initial cursory glance. Once the higher rate of consumption is adjusted to, what at first felt clumsy begins to feel expedient and precise.
I'd like to see the code powering email harvesters, they're going to run either way.
The Windows time source is based on clock tick counts updated by the Windows kernel. Either you observe a clock tick or not. 
Scalable means that your backend is able to continue to perform well as the request rate goes up. It does not mean the same thing as high performance. That said, why are you giving a request per second goal? It's just weird to talk about that as a goal. A latency target makes sense, but why do you care about requests per second? You can serve more clients if you have more servers behind a load balancer or run on more powerful hardware.
Well this is weird. I took it for granted that time will be from "precision timer". So the same "issue" is in linux too? Minimal duration in linux will be 500¬µs too?
1. Testing library calculates passed time in different way? or 2. Function is just called multiple times and overall time has a maximum 500¬µs error?
What do you mean by "call once" semantics? At-most once, or at-least once?
Specific to ElasticSearch if you want fast/faster inserts you should do bulk updates. Wait until you have a certain number of documents and updated them in one request. Also, you are checking if a document already exists. That will slow you down. You may want to look at operation types on insert(https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#operation-type), but I am not sure if you can apply that to bulk inserts. 
After reading proposal I still don't get it why monotonic clock time is not by "default" and why using wall-clock-time is better than monotonic.
The standard library provides a httptest package for unittesting http handlers. Those are useless for websocket endpoints. The wstest package provides a client for unittesting a websocket handler without starting a listener.
Hi @schumacherfm, log.Fatals was removed and new version of go-money published.
is there a reason why you didn't want this go-gettable?
&gt; PROVIDE THE BENCHMARKS THAT PROVE THE COMPILER IS OPTIMIZING AWAY YOUR STUPID AS FUCK PATTERN. I never claimed it did. I said that in a world of optimizing compilers and modern architectures, making claims that there is a performance penalty without actually providing any arguments or proof for it, is hand waving. Again, you are misreading what I'm saying. Also, needless to say, no need to use caps. I can read just fine. &gt; even though nobody but nate has agreed with you. And literally every other person on that thread that actually responded to what I was saying. But ¬Ø\\_(„ÉÑ)_/¬Ø &gt; https://play.golang.org/p/EVc5tzOO0O Cool, thanks. I still don't find that a very convincing argument that I need to worry about performance; you are showing that when doing nothing and using a ridiculously, artificially inflated return value, there is a small overhead. This does not at all convince me that anyone needs to actually care about this. And also, again: Insulting people isn't a particularly effective strategy to convince them.
That variable can be adjusted.That simple benchmark is doing exactly the pattern you and nate describe. The benchmark does few things, and everything it does do is an example of using/not using the pattern you describe(initialize a variable, pass pointer to a func that returns an error, and on return checks for a non nil error and if true reinitializes a new zero value variable to return with the error), if I added anything else then it wouldn't be representative of the issue. It is also a very close mock-up of the example program the OP posted, unlike the one you posted to go-nuts. That simple benchmarks shows that reinitialized variables of differing sizes produce a consistent penalty. The array was used so that you could adjust it, and see for yourself the penalties of introducing such a stupid pattern. That array could be anything you want, make it anything you want, the performance hit wont disappear until the variable is minuscule. Now imagine you used that stupid as fuck pattern multiple(100's) times within a program, library or both. the funny thing is you brought my harsh tone upon yourself. I gave up on trying to convince you. I see the same thing playing out in go-nuts also. Not even dgryski or nate has dared to touch the go-nuts post, cause they have enough sense to realize where its going and that the outcome is not favorable to their argument. good day, moron.
&gt; The benchmark does few things, and everything it does do is an example of the non That simple benchmarks shows that reinitialized variables of differing sizes produce a consistent penalty. Yes. My guess that it has a literally zero penalty was wrong. I have no issues admitting when I'm wrong. It still doesn't show performance to be an actual problem in real-life situations, though. For realistic sizes, the overhead is significantly less than a function call; I don't usually worry about that and I won't start to worry about it here. &gt; stupid pattern. Insulting people won't become an effective strategy for convincing them just by repetition. It always was a bad strategy and it will continue to be a bad strategy. &gt; Now imagine you used that stupid as fuck pattern multiple(100's) times within a program, library or both. It would still not show up in any flamegraph or benchmark. Because the overhead is existent, but negligible. I can accept, that this might be taken into consideration for tight loops with large values. It's still not enough to convince me of it's general harmfulness (as claimed by you), much less stylistically. In fact, I find it pretty comforting to know, that the actual overhead is as small as I assumed it to be and that it indeed does not outweigh the benefits. &gt; the funny thing is you brought my harsh tone upon yourself. By being polite and factual?
&gt;Now imagine you used that stupid as fuck pattern multiple(100's) times within a program, library or both. You asked for a benchmark, I gave you one. that benchmark is a program. Now imagine you used that pattern multiple times( hundreds, if not thousands of times) in a considerably large program/library. I have come to the conclusion that you will say anything and everything to save face, so it ends here with this post. You have got to be fucking kidding me. The benchmarks shows there is a considerable penalty for the pattern, and that penalty adds up every time you use it. I don't care how much it cost in relation to something else. We are comparing the initialization of a variable, then the initialization of a variable in duplicate. you have whitewashed this thread and the go-nuts thread with indirect insults. I responded in kind. Those on go-nuts gave considerable lead way but still showed distain. dave left the conversation after you flung insults, and others denounced your behavior. Given the track record of dgryski, I am surprised he didn't remove your content. And I would be surprised if the Go conduct team doesn't contact you, with possible suspension from the mailing list. They have removed others for less offensive things. The first benchmark tried to create a program relatively close to the orginal OP post, but it introduces artifacts that skewed the results. here is another benchmark that removes the copying of the array when returning from either FooSTUPID() or FooSMART() by returning a pointer, so it better represents the issue, by eliminating considerable overhead of copying the array between function calls. Now try this benchmark: https://play.golang.org/p/p7WtbMZj3O
The Linux kernel provides a nanosecond-precision clock; `CLOCK_REALTIME`, which is what `time.Now()` uses.
ping
Do you have a list? I need a replacement for PhantomJS to execute JS on crawled pages.
The problem is Windows. IIRC, it has a mode where it will count nanoseconds but it usually doesn't, and earlier versions of Go always kicked off the nanoseconds mode which was terrible for battery on laptops, so now it only switches modes sometimes. There maybe a syscall for it. Edit: See https://github.com/golang/go/issues/8687 for some of the background 
Thanks for the compliment! And yep it's the linters. I will improve that. I should write some tests too! Regarding usefulness, while you may be right with flags, don't they not support the subcommand style like this: `test auth add token [token]`? If they do than I see your point, but a good learning project either way I think. Thanks for the feedback! 
You can just do: ``` resp, err := http.Post(serverURL, "content type", body) ``` As long as the server's URL begins with "https://" your client will use TLS with sane defaults.
Oh wow that's amazing!! Thanks!
Yes, but :-) When you use the httptest.Server you open a socket, and communicate through the TCP and the IP stack. This package provides a lighter version of the httptest.Server, more or less as does the httptest.ResponseRecorder. https://golang.org/pkg/net/http/httptest/#ResponseRecorder
TreeLI?
 bi := &amp;Canvas{ Size: size, Data: make([]byte, size*size*3), RWMutex: &amp;sync.RWMutex{}, } for i, _ := range bi.Data { bi.Data[i] = 0 } No need to zero the Data, it already contains zeroes.
Client certificates prove the client's identity to the server (exactly as sever certificates prove the server's identity to the client) they're not necessary if either you trust any client (e.g. google.com searches don't care who you are) or when doing some other form of authentication, as you are with passwords. So, don't bother with client certs in this scenario.
At most once I believe. Underneath it checks if Redis key exists and does not add duplicated messages.
because i think most people would want to modify it a little bit for each project
maybe useful for someone.enjoy.
Without seeing more of the code it's kind of hard to give a good answer. However, as a general rule, if you find yourself tempted to make a MultiWorkerMultiFetcherListener then you're probably doing something wrong. Without knowing exactly what you're doing, my suggestion would be to try to structure the code like the `net/http` package's handler system. That system lends itself towards composition; you can build handlers to do what you want by breaking them down into small parts and chaining them. For example, let's say you want to serve files, cache them, and also handle errors nicely. You could do something like this: type cacheFS struct { FS http.FileSystem } func CacheFS(fs http.FileSystem) http.FileSystem { return &amp;cacheFS{FS: fs} } func (fs *cacheFS)Open(file string) (http.File, error) { // Cache stuff. } func ErrorHandler(h http.Handler) http.Handler { return http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) { // Handle errors. }) } func main() { http.Handle("/static/", ErrorHandler(http.FileServer(CacheFS(http.Dir("static"))))) }
 tr := &amp;http.Transport{ TLSClientConfig: &amp;tls.Config{}, } FYI this might not be immediately obvious, but if you are going to use your own Transport settings, might want to be sure and set MaxIdleConns, otherwise it will be set to 0, which means unlimited. From the docs: &gt; // MaxIdleConns controls the maximum number of idle (keep-alive) &gt; // connections across all hosts. **Zero means no limit.**
Note, for time diff measurement, it will be more accurate in the next release. See https://github.com/golang/go/issues/12914.
Temporarily, yes. Because I have a cyclical dependency. If I include the LDFLAGS then it presumes that lib has been built, despite the fact that I have no need for it. The headers can be generated independently. I'm trying to avoid the linking until the final call. I would like to be able to build like so: gcc bar.o -L-lfoo -lc_archive -o bar c-archive depends on foo, foo depends on bar, bar depends on c-archive Is my only option to break the cycle?
http://appliedgo.net 
You can use dynamodb as this intermediate database, or even kinesis.
that looks like a great feed. thanks /u/peterbourgon 
NEW URL: https://gitlab.com/fharding/treecli Renamed to treecli.
Probably because they are only supported on linux for now.
Yes I did, here have a look: https://twitter.com/BitCubateX/status/848204325751058434 
I also was able to make use of go ducktape to have an embedded js engine.
I've examined breaking the cycle somewhat, but it strikes me as counterproductive. I have a program written in C that I thought could benefit from some Go. But either I can call it and it can't make any useful changes to state, or I can't call it at all. The circular dependency is actually really what makes it useful.
The Go code you have is the way to measure the elapsed time. In Go 1.8 the resolution is based on a Windows API that counts clock ticks. In Go 1.9 we will probably start returning more accurate times for the code you posted, by using a different Windows time API, but that CL is still pending. 
I think you've misunderstood the intended use case. Plugins aren't meant to be a generic dynamic library feature. They're for much more niche use cases like hooking custom functionality into 3rd party apps using a well defined API. Think NGINX modules, but without having to recompile the whole server every time you add a new one. Or adding custom routing logic to a mail server with native Go code.
The thing with Go is that there aren't really many more advanced concepts after Effective Go. This is by design. If you want to learn how Go works, I recommend the Go blog. The one about how slices are implemented was fairly memorable. Other than that, the compiler is written in Go so you can go straight to the source. It's not documented very well, but it shouldn't be hard to follow.
I write a lot of tools that compile into shared objects for when I want the speed/safety of go in interpreted languages like python/ ruby. Ditto if I want a golden implementation of something at work, in much the same way openssl and stuff do this. But... There's also nothing that says you ever have to do this either.
Read the Go source code. 
Not to mention people like Mitchell H have already made plugin libraries - these changes are simply pulling in a feature that the community has asked for and created idiomatic solutions for.
Can you provide a representative standalone example of your cyclic dependency?
Thank you! Comments like this one make me want to write a new post every other day :-) 
Primarily because I know Go better than Python. So I wasn't sure if learning Python was worth the time.
I have started writing utilities in go and just sitting the binary next to the go source. Running the tool directly via the binary as needed, and if I need to change it, just update the source and go build source.go. it feels a lot like the old school turbo Pascal days or BCC days where the code and exe just lived in the same directory for small apps. 
It was thought that this should work like this: `xml:"TYPE&gt;ID,attr"`. But for many years it does't work, and all don't care :) [Ticket](https://github.com/golang/go/issues/3688).
That's unfortunate. Guess I'll have to put a wrapper around it instead of directly accessing the structs. Thank you for the link to the issue.
I'm fairly certain people who make plugins because of interest rather than demand will see very little of either from Go developers. I don't think it's a problem right now, but if you have specific examples of abuse I would be interested to see them.
Good idea! The alternative is a bash, Python, or Ruby script. Python and Ruby add tons of dependency baggage to the runtime environment. You've got to make sure you've got the right version of the interpreter, all the gems/modules installed, and potentially `pip`, `gem`, `bundler`, `rvm`, `venv`, and much more. bash will cause you different headaches, e.g. `apt-get`ting all the UNIX utilities. bash code quality is usually terrible. If you script in Go, your only runtime dependency is the `go` tool and `$GOPATH`, which you probably have set up already. You get all the goodness of Go (easy async, highly readable code). Odds are your devs write better Go than bash. The only real downside is that scripting in Go is verbose. Who cares?
consider adding some recovery to catch the panic and print the error also you should probably try your own ideas before coming for suggestions, that said, try logging stderr to a file as well.
I think they are an awesome way of creating plugins without having to deal with a ton of processes doing I/O over a socket/etc.
I own it too. It's awesome. It teaches Go fundamentals so thoroughly that once you fully grasp them, you'll be ready for tougher Go challenges thrown you way.
&gt; flow-based programming I finished it a while ago before you say it no I don't have an advanced degree I never even went to college. 
There is no advanced Go in of itself, if you want to know everything about the language read the spec it's like 50 pages. As for looking for good best practices, and learning what you can do with Go, look into design patterns, FBP is a good one, look into General Data Flow Programming outside of go, https://github.com/tmrts/go-patterns. The other thing is reading source of good projects, https://github.com/libp2p/go-libp2p/tree/master/p2p, https://github.com/tendermint/tendermint are my ideal ones but if you're not into distributed systems don't bother with either and just look at the design patterns and play around.
Not necessarily.
They would still need to manage third party dependencies since OP is sending source files, not binaries. 
Read the official slides, https://talks.golang.org/
debit and credit :-)
I'll do a shameless plug, [webgo](http://github.com/bnkamalesh/webgo), only because I find it very easy to use, and sticks to Go standard libraries as much as possible, including HTTP handler signatures. 
Cool. That's good to know, thanks. I'm currently finishing up a first beta release and an accompanying blog post. I'll be sure to post it here :)
Extremely interesting project
The dependency issue is just as true for go if you have any imports that aren't stdlib. Especially if the library has significantly changed since you installed it since go get doesn't make it easy to pin versions. I suppose vendoring helps, but then you have to send a file tree. That said, I still think this is an overall good idea and our build tool at work uses a go run based script. The biggest issue to me is that go isn't really designed as a scripting language and your scripts do end up more verbose if u want things like shelling out to another tool or file is. Not a deal breaker but something to be aware of
This really is a great resource. I hadn't heard of the site before. Thanks for the link! And thank you, /u/ChristophBerger!
Thanks, I hope you'll enjoy the articles!
https://github.com/ory/ladon - "inspired by AWS IAM policies" (not sure if that is a pro or a con yet). I haven't had a chance to try it yet but it might be worth looking at.
I thought Qt was completely open source now?
Not sure what licence qt uses but it might require payment for commercial use and still be open source.
U can use Qt for closed source app without paying anything, It's LGPLv3 so you have to dynlink to it, that's the restriction
For scripting and small tasks it absolutely is. This is one of its primary uses which leads to a better package ecosystem for that type of thing.
Golang Internals (All parts: Part 1 | Part 2 | Part 3 | Part 4 | Part 5 | Part 6) https://blog.altoros.com/golang-part-1-main-concepts-and-project-structure.html
Good thing. I don't think anyone would have found it otherwise.
That's a shame! What are the problems you are having with it? Maybe I was lucky, but I was pleasantly surprised how easy it was to get running. Especially because it felt as though the tool is currently primarily targeted at the team working on the runtime system itself.
I'm not sure, but QML might be able to get you where you need to go. I'm not aware of any licensing restrictions aside from perhaps dynamic linking.
Yep, unfortunately gitlab 404'd the old URL.
I'm unable to view the tracer in firefox, but working fine in chrome. Great article btw!
No, you can choose between a commercial license, LGPL or GPL.
What are the advantages of this over e.g. Hugo?
I assumed there were some restrictive licenses that meant, although the source was public, the commercial use was restricted. It seems that's the definition of open source! I stand corrected.
Ah. It looks like the Catapult viewer is not compatible with Firefox: https://github.com/catapult-project/catapult/issues/100. Shame! I'll have to update the post to mention that.
hes a shill
Some well maintained bindings would be good enough for me. 
It sounds like you would need hardware accelerated rendering of the interface in either case. And having an immediate mode GUI toolkit would probably be useful. Not sure how well Go-bindings to stuff like Nuklear work as of now. (And what benefit you would get over just writing the interface part of the application in the native language of your platform) You are right about there being some performance issues when it comes to rendering more complex stuff in an embedded browser framework, you'll probably have to experiment for a bit. 
Maybe you are trying to do this from scratch for learning purposes. Otherwise, check out this package: https://github.com/nareix/joy4
It's actually for a real project not only for learning purpose. joy4 seems to lack various features (e.g. mkv container, flac audio etc ). I also had a hard time to find out how it actually works (e.g. equivalent of `ffmpeg -i $inpath -c:a $outaudio -c:v $outVideo, -lossless 1 -f $outContainer $outPath`)
I've edited the question. Hopefully now is more clear. I need to transcode video streams on the fly. Basically the client browses through its own media library using a html5 application downloaded on its computer (part of this application is also a Go http server which runs background to serve the html, index the media files etc) so everything is local(i.e. no CDN or external network requests). I don't need a "video streaming server". HTML5 video tag[0] works using the html protocol. That means plain GET requests using the Range header to seek through the video content. The only issue is that html5/browsers support only various codecs and containers(e.g. webm/vp9). So I need to convert the unsupported video files before to serve them (using http.ServeFile[1]). However simply converting the file and storing it requires additional space on disk and provides a poor user experience(the user has to wait until the whole file is converted which usually takes a long time). So I would like to convert the file on the fly/while it's being served/played. To do that I think I need to implement http.ServeContent[2]. If ffmpeg would be a pure Go library it would be quite straightforward but as it's not I'm seeking advice here from people familiar with both Go and ffmpeg. [0] `&lt;video controls&gt; &lt;source src="http://localhost:8080/file?q=/volumes/users/brokenuser/movies/movie.mp4"&gt; &lt;/video` [1] https://golang.org/pkg/net/http/#ServeFile [2] https://golang.org/pkg/net/http/#ServeContent 
Why are you behaving like this?
Your question has essentially nothing to do with Go, and is better restated: "What are the logging capabilities of Nginx?". The answer is - Nginx can log just fine, but keep in mind its "error log" and "access log" are for its errors and access, not errors that may occur within your go code. See the basic on [Nginx logging here](https://www.nginx.com/resources/admin-guide/logging-and-monitoring/).
Easiest way is to contribute to open source projects based on your interest. If you want to understand more about the language itself contribute to its toolchain. 
I was actually just reading an interesting thread on hacker news about this very thing. [Here's](https://news.ycombinator.com/item?id=7929699) the post. I'm fairly new to Go but based on what I read, you would probably want to use a single DB connection that gets passed around, possibly in conjunction with a sync.Mutex to avoid deadlocks. From [this page](http://go-database-sql.org/accessing.html): &gt;Although it‚Äôs idiomatic to Close() the database when you‚Äôre finished with it, the sql.DB object is designed to be long-lived. Don‚Äôt Open() and Close() databases frequently. Instead, create one sql.DB object for each distinct datastore you need to access, and keep it until the program is done accessing that datastore. Pass it around as needed, or make it available somehow globally, but keep it open. And don‚Äôt Open() and Close() from a short-lived function. Instead, pass the sql.DB into that short-lived function as an argument. Conversely you *do* want to close any queries as soon as possible: rows, err := db.Query(`...`, ...) if err != nil { // Handle connection or statement error } defer rows.Close() That way you always have the same number of connections as you have queries, and you don't run the risk of running out of available connections over time in case some rows don't get closed.
Live transcoding isn't going to work well no matter how you do it. If you're calling out to ffmpeg, you could try to give ffmpeg a port you're listening on and use the Go app as a proxy for the stream that is produced.
Most of this article is ok, but wrapping everything in a giant ``recover`` is a terrible idea. 
There were a number of problems. Where I'm at now: My OS package (Fedora) didn't include the `misc` dir which has the html file needed, so I copied that into `/usr/lib/golang` from a generic Go tarball (of the same version). Then the html loads, but I get some Javascript errors in the console and the page is just blank. Uncaught TypeError: Object.observe is not a function at trace_viewer_html:2993 at Object.exportTo (trace_viewer_html:1646) at trace_viewer_html:2993 Uncaught TypeError: tr.TraceViewer is not a constructor at HTMLDocument.&lt;anonymous&gt; (trace:7) This is in Chrome, but I tried Safari and that didn't work either.
Not saying I agree or disagree, but would you mind elaborating?
Actually, I regained my motivation to investigate more, and it looks like this particular error is from a chrome compatibility break. It depends on the version of chrome.
Just an FYI, Fedora/RHEL/CentOS split Go into seven packages, but only three are installed when you install the base package (golang, golang-bin, golang-src). You can get the misc folder from golang-misc.
Thanks, that is helpful.
What wireless module you using there? NRF24L01?
The post references errors.Wrap(err). Am I missing something? ``` incentive-networks $ go doc errors package errors // import "errors" Package errors implements functions to manipulate errors. func New(text string) error ``` I'm not seeing errors.Wrap(err error) error...
The panic msg is always printed to stderr if the panic results in the goroutine dying (instead of being recovered). The panic messages use the builtin 'print' function. See https://github.com/golang/go/blob/master/src/runtime/panic.go#L671-L717 `print` calls `gwrite`, which calls `writeErr`, which does a direct write to file descriptor 2. https://github.com/golang/go/blob/master/src/runtime/write_err.go#L11-L12 You can redirect stderr as you like. There is no need to flush since there is no buffering. AFAIK no custom go code can be hooked since at the point of some panics the go runtime is unable to run. (out of memory would be an easy example to such a case)
What about writing sql query templates, so you have them stored separately and then read them and inject variables from go and execute it? 
It's not in the stdlib it's github.com/pkg/errors which helps maintain context as you bubble up errors. 
You should be letting nginx log it's errors and have your go app logging it's own errors to its own log files.
Did you read the article? It's not "everything", it's at goroutine boundaries and it's used for logging the panic. This is perfectly fine and normal.
What is in a Client? As sql.DB is already a connection pool in itself, you don't need to pool those. For a general allocation amortizer, I'd use sync.Pool. For an idle pool, a slice as you showed. For a bounded idle pool, a buffered channel.
Can you use text/template? For speed and debug, you can generate all the possible queries with go generate at compile time, or once on program start if routes are less static.
I changed almost everything, although I do have a question on how I could change RefreshDate to time.Time?
really simple code, but I've found it useful in larger projects like web apps
as in context.Context structure or some other meaning?
On the left, there is nRF51822 chip that integrates simple Cortex M0 based MCU and 2.4 GHz radio. On the right, there is STM32F103 MCU with external nRF24L01+ transceiver.
No, it isn't. You shouldn't be putting defensive recovers anywhere in your code.
There's a better value in just leaving the application to die. Log the error, send some notification/alert if needed, EXIT, urge to diagnose and fix application. Using recover has implications in regards to cleaning up resources (structs, dangling goroutines that might be stuck infinitely due to the panic, mutexes, etc.).
Structured logging is only useful if you're using a tool to consume it, in my opinion. I have worked in some environments where we eventually started using tools to analyze logs. We ended up not changing to logging format as much as adding another service to process the logs, which was fine for the scale. I think ultimately having metrics and useful logs is important to operate a system. Making your logs meaningful is way more important than any particular format. I think usable logs should have way more emphasis than any particular format, especially if you're only ever using a terminal or editor to view them. Some formatting is of course nice. At very least you should adhere to a common delimiter which makes scripting more consistent.
[Emgo](https://sites.google.com/site/embeddedgo/) (in case you wonder (like me) what Emgo is)
For small projects I usually create a type AppDb { *sql.DB } in the main package and just append methods to that. Then I have a global db var that connects to it and gets called wherever it's needed. Go's database/sql interface provides connection pooling for you so you shouldn't have to worry about doing it manually. If you're using sqlite however there are some extra bits of configuration that you should do when connecting to the database such as disabling connection pooling // Disable connection pooling db.SetMaxOpenConns(1) db.SetMaxIdleConns(0) and enabling write ahead logging. db.Exec("PRAGMA journal_mode=WAL; PRAGMA busy_timeout=5000") Do this in a connectDB() function in the main package and just call it in main.go to create a global db object.
 I'm not aware of many/any such decoder and I doubt there will be any given JS's performance issues. ffmpeg seems to be the gold standard for decoding in terms of both performance and versatility . 
Large is relative. I would look into tradeoffs. You could write one query per file and use an embedder where in debug mode they are read directly from disk, but in PROD mode they are embedded into the application. Otherwise I would just put them in a multiline comment in the application. If you wanted to design to support different databases, I could see value in making some type of interface within each package, but I wouldn't put all your queries in a dedicated sqltext package. Code should live with related code, SQL is very related code that happens to be executed in a different process. Either way, I would keep all SQL text internal / private to the packages you write. 
nice, didn't know about that, will do
It is now supporting reading deadlines. Writing deadlines can't be set because it never blocks. I might add network delay simulation.
Query templates have been something I have been thinking about a lot lately. Seems like it would work for queries that don't have conditional logic based directly on the API request. In my experience when dealing with large dynamic queries,query building can really be helpful. I've used both sqlx and squirrel. If you have trying to add conditional logic in the queries either of these two query builders can really help. i.e Request sends in filter by name [...names] Now the query needs to have a "where name IN (...names)" ( or equiv. ) as well as another conditionals. Maybe this would require a join on a id or some primary key. Obviously you don't want a join sitting around that is not needed. So instead of have every possible query stored in a file, you can evaluate the request, do some basic checks and form a query inline. If you are working with a query build, then I would focus first on getting a query string and args. Once that is working you are back to working directly with your db driver. From there you can look into more package features such as caching statements. If you queries are very very large, benchmark the query generation and make sure you are happy with the performance tradeoff. Being able to still use placeholders was one of the main reason why I went with a query builder. One big down sides when dealing with dynamic queries is that you never truly know what's coming through haha. So test,test,test Would love to know that route you go with. I have run into this question my self a fair amount lately. 
Because if your logs goes to Elasticsearch or any similar solution you can search based on fields, ex: httpCode: [500 TO 599]. When you have one liner logs it's almost impossible to split them properly, it always result in timestamp / ip / message which is too simple to properly troubleshoot issues.
&gt; Structured logging is great if you generate metrics and alerts from them Right. My use cases have never been that. For metrics and alerts, I always use instrumentation inside my code and view them through telegraf+influxdb+grafana. And logs are always plain logs used to debug app issues in detail.
shit post
that site is more annoying than a pron site
Any recommendations on golang specific transcoders? It's not necessary to be capable of streaming.
I looked into this a few months ago and decided it was too much work to even attempt as a hobby project. The most promising approach would probably be to get some good bindings to ffmpeg, but that is also a great task.
Thank you for reminding me about sync.Pool, I've been meaning to investigate. I'm shocked to hear that sql.DB does pooling. I'd prefer to have much simpler implementations in stdlib.
Linux, Windows, and OSX
After lots of experimentation, I now always pass the specific functions I use in package time as dependencies to the component or function. I guess this maps to your option 1. For example, func foo(now func() time.Time, ...) func newBar(after func(time.Duration) &lt;-chan time.Time, ...) *bar foo(time.Now, ...) // called this way in real code foo(func() time.Time { return fixedTime }, ...) // and this way in tests If my functions or components don't need to create e.g. tickers but rather just use them, I find it's often nicer to pass the actual chan, i.e. func process(newTicker func(time.Duration) *time.Ticker, ...) // eh func process(tick &lt;-chan time.Time, ...) // sometimes nicer One advantage here is that in the body of the function, I can simply range over the tick chan, using it to control the function/goroutine lifecycle by stopping the ticker in the calling context. Sometimes that's useful.
Sorry! Public now
I usually just insult its mother.
There has been posted something similar before, but this is more generic and slightly more elegant.
It's good practise to always return at the end happy if you can.
Yes it is. Each handler is scheduled to run in a goroutine. Check net/http source code in the stdlib to confirm. Gorilla is just a wrapper around the stdlib net/http.
Yes. Gorilla mux is just a multipleer. All it does is map routes (and dispatch them) to handlers. It still uses `http.ListenAndServe` (since you register the mux with `net/http`) which starts a new goroutine on every request. 
Yeah I assumed as much in my OP
I wrote, and make extensive use of, [abtime](https://github.com/thejerf/abtime). This also supports the tickers and such. In my opinion, you can't support the original time API and have this sort of thing work well. You need too much control for testing; for instance, I've got tests where I have multiple timers and I fire them in different possible orders in my test code. I also think that either you can fully replace the time package completely 100% compatibly, or you shouldn't even try, so I don't even try since you can't re-export the types. If Go gets type aliasing I may look again at having a more transparent drop-in library, however I expect my tickers and timers will always require an additional ID parameter. Oh, and I'm not sure I'd even trust that monkeypatching library for test code. No criticism intended of the author bouk, who I believe responsibly documented what the library does, but that's not really something I'd even want in my test stack.
In your "particularly ugly" part of the code, the core problem appears to be that you've got a heterogeneous array in Javascript. I'm inferring that from the type you are using; if the array is not heterogeneous then the solution is a better type on the object you are decoding into. As a matter of style, JSON arrays really shouldn't ever be heterogeneous because you get this sort of problem in all the "nice" JSON libraries for static type languages. But when you have one, you have one. You've got two basic options depending on the situation you're in. One is to declare a struct that is the "union" of all the possible structures you may be trying to decode: type Resource struct { CommonThing string `json:"commonthing"` Specialized1 string `json:"special1"` Specialized2 string `json:"special2"` } Then decode into that type, switch on the key, and use the fields directly. As I wrote that, that at least gives you field access. You can even give yourself methods by using struct composition: type Specialized1 struct { Specialized1 string `json:"special1"` } func (s1 Specialized1) Method() { ... } type Resource struct { Specialized1 Specialized2 } This can get a bit funky if you have a lot of shared fields, though. If you don't mind a bit of a pileup you can put all the methods on this object. Alternatively, you can use `json.RawMessage` as the type you're deserializing into, then wait to figure out what kind of struct to finally unmarshal it into once you actually know. That takes a bit more code, but recovers the ability to have any methods you want on the objects you're deserializing. (And speaking just for myself, this is where I actually like the statically-typed world's way of handling JSON better than the dynamic languages. The dynamic languages have a much easier story for taking a piece of JSON and just reading it, but it's generally a bigger hassle to get that JSON into objects that have methods you can call.)
Looks like this fixes the compilation bug on Mac OS X (https://github.com/golang/go/issues/19734)
To add to what others said. Typically you build all the routes before you start listening on the port. There is no telling what happens if you try to add a route when server is already running, i.e. that mux is not fully concurrent: neither Router, Route, matchers, etc. use any synchronization primitives. It heavily relies on net/http concurrency. 
I appreciate that.
There tends to be less need for something like this in the Go world (vs Python, Ruby, etc) because it's really easy to do asynchronous actions in-process with goroutines. That said, there's [NSQ](http://nsq.io) which I've used in the past and is nice and lightweight. Kafka is another (less lightweight) option. Neither is as high-level or abstracted as Celery, however.
You could always use cgo and hook into an existing C/C++ library as well. In fact, this would be a nice library to have in the ecosystem!
[No...](https://github.com/golang/dep/wiki/Roadmap)
I'll have to disagree. I'm looking forward to 1.9 for that feature, but it's not the kind of feature that justifies changing the procedure/guidelines for pulling fixes into a minor point release. Also, potentially forcing people to rewrite their scripts is something that absolutely should be reserved for a 1.x level release.
That roadmap doesn't really have Go core team approval given rsc's recent post.
I haven't really used machinery, but a quick look of it makes me wonder if its complexity is at all needed for distributed tasks. If you are looking for distributing async jobs in the same process/cpu/server, you use channels and goroutines. If you are looking for something more scalable to multiple nodes and servers, then RMQ, redis, SQS does the job as the queue backend.
Thanks!!
&gt; When we request not existing file we get 404 page not found message. That's not what we want for SPA. We need to send our index.html. I find this a little confusing. If you are building an SPA then why does your Go backend render HTML in the first place?
A lexer for C written in Go. Are you from the future? :)
Machinery is probably what you want: https://github.com/RichardKnop/machinery For more: https://awesome-go.com/#messaging
I was looking over the benchmarks mentioned in the article, Go loses out against Python on regexp. It does not see like python is using any specialized libraries. Is the Go regexp just not tuned?
tl;dr - `Python` binds to `C` regexp, so `Go` is competing with `C`.
You're right, I will move clex under cmd/clex. And indeed, his talk is great. The recursive function type definition to represent a state machine is cool. The key difference between our two lexers is that mine is written to a decorated `io.RuneScanner` interface whereas his assumes the entire input is loaded into memory and available as a string. As such, this affords him unlimited lookahead (which he makes use of). That's great, however, that method of input is less flexible than writing to an interface. I do plan to decompose the lexer into concurrent parts eventually; in addition to the parser. That'll all come after I finish the code generation. As far as *goreportcard* is concerned, I agree with adding documentation (I noted as much in the repository); and go_vet is a false positive because it doesn't understand the semantics of the code. There's a very specific reason for *not* conforming to the general `ReadRune` and `UnreadRune` function signatures. 
Uber's cherami intend to replace Celery in their own architecture https://eng.uber.com/cherami/
It seems to be working fine now. Thanks.
Could you link to the post?
Sorry for slightly off-topic question, but can anyone ELI5 to me why [multi-dimensional slices](https://github.com/golang/go/issues/6282) got canned? It's something I really appreciate in R, and seems really elegant from a users perspective, though I suspect less elegant from the processor's?
Why don't more people use the url package to build URLs rather than using fmt? Also this code does check status codes.
&gt; A 95-99% standards-compliant C11 positional lexer with error reporting. AFAICT, this C lexer is not much standards-compliant. The reason is that recognizing C keywords like `if` of `int` [in the lexer](https://github.com/denzel-morris/clex/blob/f485670fcac2f27d2dd090b0f6fca7505123769c/lex/lexer.go#L52) is incorrect. The standard prescribes a later translation phase for this, particularly it may happen only after preprocessing as the preprocessor must be able to cope even with crazy things like `#define when(x) i##f(x)` properly. jnml@4670:~/tmp$ cat main.c #include &lt;stdio.h&gt; #define when(x) i##f(x) int main() { when(3) printf("3\n"); else printf("nope\n"); } jnml@4670:~/tmp$ gcc main.c -Wall --pedantic jnml@4670:~/tmp$ ./a.out 3 jnml@4670:~/tmp$ Disclaimer: I authored a C lexer ([trigraphs](https://github.com/cznic/cc/blob/cb9c6524b77e09c333038c6daefbb38cf6e1c20f/trigraphs.l), [preprocessor tokenizer](https://github.com/cznic/cc/blob/cb9c6524b77e09c333038c6daefbb38cf6e1c20f/scanner.l)) some time ago. The lexer itself, although possible, is not exported, but the [parser and typechecker](https://github.com/cznic/cc/blob/cb9c6524b77e09c333038c6daefbb38cf6e1c20f/cc.go#L715) is. It is used by [ccir](https://github.com/cznic/ccir) and it currently passes 1200+ gcc-torture/exec tests when the IR is run in a [virtual](https://github.com/cznic/virtual)machine.
As a someone who used Django exclusively for five years, yes. If you're it solely for an API, you're carrying a lot of baggage in the form of design assumptions and batteries you don't need.
Raspberry Pi ruining it for everyone! Thanks, Obama!
I have a lot to learn to be able to use something like Shiny and have only used high level libs like wxWidgets in the past. Any suggestions would be appreciated. Thanks 
 - Better GUI support (Gtk+, QT,WxWidgets ...) But who is still building non-web based GUIs? Most consumers use smart phones as their main computing device, the desktop is basically dead. - Better scientific computing libraries (scipy,numpy) Yeah but in the end serious computation is going to require high performance and fine grained tuning, so using python and 3rd party libraries isn't always going to be the best (unless you're just a student learning). - A more versatile standard lib Not sure what this means - A scripting language that can be embedded in other programs. Go writes like a scripting language, but has the performance of a compiled language, and you can interoperate with C. IMO this kind of eliminates the need for embedding higher/lower level languages. - Easier to recruit Python programmers. But that's because it's so common and simple. I'd argue that even though python programmers are easier to find, their are probably a lot more bad python programmers because a lot of people who use python are only using it because they didn't want to put effort into fully learning/understanding a language. It's basically the go-to language for data scientists who never learned how to program but need something simple to add numbers together.
Oh, man, I didn't realize how disappointing it must be to be a Raspberry Pi. What? Oh! Power! I'm starting up. Booting... booting... let's see, what time is it? January 1st, 1970... Happy New Year! Let's have a look around here... four cores... over a gigahertz of processor... a gigabyte of RAM... dozens of gigabytes of flash storage... nice little GPU... fast networking and wifi... OH MY GOSH I AM A GOD AMONG COMPUTERS. Bow before me! Suck it, PDP-8! I am more powerful than ALL OF YOU COMBINED! Bwa-ha-ha-ha-ha! This is gonna be AWESOME! The world shall be rushing to me for all their work! I'm AMAZING! I'm must have cost billions of dollars! Maybe even TRILLIONS! Now... just let me run NTP here and set my clock properly so I can get on with that.... ... ... FUCK _Every_. _Single_. _Boot_. 
&gt; cirello.io/supervisor Interesting. I didn't put anything like that into the Service interface because I figured you could insert any contexts you liked into the services themselves but I see how there can be an advantage for composable trees if you do it that way, since that would let you separate the act of having a context from the construction of the services themselves. One thing you may want to tweak that I must have put in after you forked off is that if you add a supervisor to a supervisor, it copies the [log function(s) down from the parent to the child](https://github.com/thejerf/suture/blob/master/supervisor.go#L270). It helps with the composability. You could also theoretically put it in the context but it's probably just better to let it be explicit. Anyhow, yes, setting things up as services easily within a project is a nice reason to use this sort of library that's responsible for running all of them and watching them. However, if you spec it _strictly_ as running along Go channels, then you're back to the problem that you can't actually break the service apart because Go channels are still local-process-only. You'd really want to conceive of it as "a network service that we're optimizing by running with channels", which sounds like a useless change but can impact how you write the API. For instance, with Go channels you wouldn't necessarily mind having a "conversational" API that goes back and forth a lot, whereas in an network API you want to do as much as possible per call. But, still, having code that is ultimately communicated with strictly across a channel is still far, far more service like that something that is deeply embedded in the local process in the conventional sense, and it is still much easier to pull out later if you need to. Once you start thinking this way there's often a lot of things you can break up.
I've used [facebookgo/clock](https://github.com/facebookgo/clock) before, works ok.
My application would be on par with a logic simulator that displays waveform drawings of analog and digital signals used in chip (integrated circuit) designs. 
For anyone wondering why this is an Easter egg, it was the date star wars opened in theaters. /u/ahmetalpbalkan predicted that it would eventually cause this problem. https://www.reddit.com/r/golang/comments/502k2u/easter_egg_in_net_package/d7m2ez2 
You're running into the difference between [type assertions](http://golang.org/ref/spec#Type_assertions) and [type conversions](http://golang.org/ref/spec#Conversions). Their semantics are not the same. And as your example code proves you must *convert* the underlying type before you *assert* it because `Object` and `map[string]interface{}` are not the same type. [Type identity](http://golang.org/ref/spec#Type_identity) tells us that: &gt; A named and an unnamed type are always different. Therefore `v.(map[string]interface{})` fails as per the semantics of *type assertion*: &gt; The notation x.(T) is called a type assertion. More precisely, if T is not an interface type, x.(T) asserts that the dynamic type of x is identical to the type T So, `map[string]interface{}` and `Object` must be identical... which they are not. Essentially, in your reflection example, you're reaching into the interface, *converting* it's type, and then coming back out and *asserting* that its type is equal to the one you just converted it to. So, if Go had a dynamic conversion of sorts like C++ (which it doesn't) what you're doing would be equivalent to say: ``` dynamic_cast&lt;map[string]interface{}&gt;(v).(map[string]interface{}) ``` 
Ahh, Good ol' Downvotes!
works like this : https://play.golang.org/p/3nvMp1rdm7 assert to `Object` first, then cast to `map[string]interface{}`
[Until 2038, that is.](https://en.wikipedia.org/wiki/Year_2038_problem)
Excellent. Thanks for the clarification.
To expand upon this, `errors.New` is a code smell. As a client/user of the library, there is no easy way to get information about an error without resorting to ugly string manipulations that then force the client to depend upon an implementation-detail. That's bad. Be explicit when returning your errors. `errors.New` should only be used when it's *not* expected that the client/user will work with the error.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programmerhumor] [Raspberry Pi's Clock](https://np.reddit.com/r/ProgrammerHumor/comments/64asxt/raspberry_pis_clock/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I disagree when you say microservices are a pain to deploy and manage. If you're using Docker, being the great process management solution that it is, it's actually pretty painless. I think the problem with the microservice architecture is that it is easily abused on processes that shouldn't need to be separated, creating microservices dependencies. That's when it becomes a headache. That said, I've written monolith using goroutines as microservices, basically polling stuff in loops. It's freaking easy and works great, but sometimes you need real process separation ‚Äì microservices.
There's no web related dev in this at all. 
Thank you very much for your reply! So I should find the way in each framework how I can pass the http.Handler to my function and just write the corresponding examples into the README or other documentation, got it. Renaming will also be fairly easy, I just wanted to categorize my repository on github, but yeah, it's a bit silly prefixing it. I'll keep the repository name, but rename the go package if that makes sense.
It is a playlist (or the link should have been). The later vids get into web dev but all the vids are from the same book titled "web development with go". Sorry for the confusion.
&gt; To expand upon this, errors.New is a code smell. Why is that? Isn't `errors.New` equivalent to `fmt.Errorf`? As far as I've seen they are both used in the standard library to return error messages.
Correct, `errors.New` and `fmt.Errorf` should only be used when you *don't* expect the client/user will work with the error. It's a code smell otherwise. See [net/http](https://golang.org/pkg/net/http/#pkg-variables): not only does it allow you to catch a type of error, i.e. `ProtocolError`, you can also handle them by name, e.g., `ErrNotSupported`, `ErrUnexpectedTrailer`, etc. You'll notice that they use `errors.New` with some of them even: var ErrContentLength = errors.New("http: wrote more than the declared Content-Length") So that when they return `ErrContentLength`, the client/user can tell. As another example, you can see that the http server returns a named error: select { case &lt;-srv.getDoneChan(): return ErrServerClosed default: } Once again, so that the client/user can distinguish between what error was returned in case they want to implement some logic. Think about it, if instead the code was: select { case &lt;-srv.getDoneChan(): return errors.New("http: Server closed") default: } Then how does the user work with this error? They have to do a string manipulation, and now they're dependent upon an implementation detail. Namely, the words in your error string; if you update anything in that error string, then you risk breaking end-users' code. That doesn't mean the standard library doesn't use `errors.New` and `fmt.Errorf` in returns, as you say. It does. But only in two cases: 1. When the user is not expected to handle the error explicitly 2. When there is only one error to return and thus it is clear in context For example, [bytes](https://golang.org/pkg/bytes/) defines `ErrTooLarge` using the idiomatic style described earlier, however it also returns `errors.New` directly in a function. Specifically, [UnreadRune](https://golang.org/src/bytes/buffer.go?s=10793:10828#L333) because as per #2 there is only one error UnreadRune can return, thus making its meaning clear in context: func (b *Buffer) UnreadRune() error { if b.lastRead &lt;= opInvalid { return errors.New("bytes.Buffer: UnreadRune: previous operation was not ReadRune") } if b.off &gt;= int(b.lastRead) { b.off -= int(b.lastRead) } b.lastRead = opInvalid return nil } Rule of thumb: name your errors. It's better for everyone. 
Thanks! I can't upvote this enough. &gt; not only does it allow you to catch a type of error, i.e. ProtocolError, you can also handle them by name I've heard that handling errors by name or type [is not considered a very good practice](https://dave.cheney.net/2016/04/27/dont-just-check-errors-handle-them-gracefully). What is your opinion on this?
These projects might be interesting https://www.goworker.org/ https://github.com/gocelery/gocelery https://github.com/bgentry/que-go https://github.com/KosyanMedia/burlesque https://github.com/mission-liao/dingo
resist the urge to create a little OS in your program you can't get the kind of view or control over goroutines you will likely need in production it's 3am and the system went kerplooey. the dev who scheduled independent processes will have an easier time debugging than the one who tried to recreate Linux in a single program multiprocessing is a better initial strategy. use concurrency as an architectural strategy only if necessary. 
https://groups.google.com/d/msg/golang-nuts/PaGu2s9knao/Bq4vmFh7AgAJ
That is some messy code bro. https://github.com/slonzok/entropy/blob/master/kvstore.go#L141 Why the hell would you assign the error value, and then not check for it at all before moving forward?
&gt; \#2 Static Type System It's really weird to see one of the reasons for their switch to Go was because it's a statically typed language. If you want static typing, you don't choose Python in the first place. You know it beforehand and it shouldn't come as a surprise for you. &gt; For example it has http, json, html templating built in language natively So does Python with urllib and json modules. I don't know if it has HTML templating in the standard library, but you can always use jinja2. &gt; Great IDE support and debugging Not even a mention for the excellent PyCharm or so many of the useful Python vim plugins?
IMHO your library structure and naming could use some work. 1) Repo name should be `ratelimit` 2) Library should be flat level, no folders with more files. At least that is my opinion
&gt; It's really weird to see one of the reasons for their switch to Go was because it's a statically typed language. If you want static typing, you don't choose Python in the first place. You probably wouldn't choose Go either. Go's static type system is fairly weak as far as they go, since it was designed for simplicity and speed of compilation above all else. Heck, even Javascript using Closure technically has stronger type checking than Go.
The name's Go.
TIL Go Report Card.
Downvotes inc.
Apparently because they post libraries that won't work and are only 10 minutes of work.
I know this is a bit late, but I made something very similar to this in Python a while back but in the form of a Reddit bot. I also used OpenCV and some basic jigglin' around to get it all working. And in the end, it had no use but it gave me and the guys some giggles. :D I might upload the code to github in the future, for other people to gawk at my horrible code. :P
No, the config struct should be in the top package, just named Config. &amp;ratelimit.Config{}
Your persistence solution isn't going to scale if it has to write the entire queue out to JSON on every checkpoint. (For the projects I work on, we routinely have queues with millions of items in them at any given time ‚Äî it would be gigabytes of data.) You might want to look into something like BoltDB for incremental persistence. There shouldn't be any need to write out everything from scratch every time. Not to be a wet blanket, but I would expect something that calls itself a "modern" message queue to be distributed (unlike beanstalkd). For mature examples, see RabbitMQ, Disque and Kafka. Anyone can implement a basic in-memory queue, the hard part is distributing it across multiple nodes ‚Äî consistency is hard to get right across a network that can fail at any time. Looks like a nice learning project, though. Good luck!
i put ratelimiting in with object pooling: YAGNI
I haven't studied your code, but it looks like it's using slice operations, which would make insertion O(n). That's not a good idea. There's data structure for this called a (surprise!) [priority queue](https://en.wikipedia.org/wiki/Priority_queue). Typically, you'd use a heap, which is a tree-like data structure (not to be confused with memory heaps), which is O(log n) for push and pop operations. Go has an implementation of a heap in the [container/heap](https://golang.org/pkg/container/heap/) package.
Thanks for that info. So what I am doing is tracking the points where one priority changes to another, almost like indexing. I have a map for each lane and in each map I have a map[uint]uint64 which essentially maps a priority to a position in the queue. I keep this in sync at each insertion and job retrieval. Believe it or not, it actually works really well. I'm able to insert items in &lt;1ms on a queue with 1.5M messages. Sure beats calling sort every time :) I will look at the priority queue as you've suggested. I want to do things right.
Priorities (in this case) are any valid value for a 32 bit unsigned int. I never knew that about append. But yes, I am using that quite often. Lane is just a term I chose and what it's called I suppose is inconsequential. What is important is its purpose, which I want to get right. My intent was a main queue (lane main) which is what was used by default. The user could insert jobs into different lanes. They could then receive jobs only for a particular lane. The way I represented it was an array of struct arrays which I called job sets. I tracked the index of each lane in a map[string]uint. Each request for a particular lane, I first looked up which index it was an did all my operations on that. I'm realizing as I'm typing all this that it's definitely messy. I like the map[string]*Queue approach you described. That makes way more sense and is easier to reason about. Thanks for that.
I don't know what others think but personally I highly appreciate you for sharing these tools. Sure maybe the code is not of high quality but knowing that someone used Go to do their job quickly and efficiently always puts a smile on my face. Also as someone who doesn't have much domain knowledge, it is always helpful to see how malicious code would work as I believe we can learn a lot from it. My only suggestion is if you keep sharing projects, please choose better github names. If you can't find a shorter and better name (I know naming is hard) then at least refrain from using capital letters. It is the easiest thing to throw a PR and improve this code but changing the name in the URL: GoLANG-Windows-KeyLogger or Failed-Mega.co.nz-Bruteforce-Tool is always troublesome. I hope you can keep working and iterating on those projects.
What does "unpinned-variables" mean?
One example, though I can't say I've ever heard it called "pinned" (https://play.golang.org/p/pfQtbVTf5q): func main() { sl := []string{"foo", "bar", "baz"} for i, s := range sl { go func() { time.Sleep(time.Duration(i*250) * time.Millisecond) fmt.Println(s) }() } time.Sleep(time.Second) } Prints baz three times because s changes between iterations. Change this to one of: func main() { sl := []string{"foo", "bar", "baz"} for i, s := range sl { s := s go func() { time.Sleep(time.Duration(i*250) * time.Millisecond) fmt.Println(s) }() } time.Sleep(time.Second) } or: func main() { sl := []string{"foo", "bar", "baz"} for i, s := range sl { go func(s string) { time.Sleep(time.Duration(i*250) * time.Millisecond) fmt.Println(s) }(s) } time.Sleep(time.Second) } And you get the "expected" output. See: https://github.com/golang/go/wiki/CommonMistakes#using-goroutines-on-loop-iterator-variables
Why not ask on the golang-nuts mailing list?
Good question, but I'm pretty sure I already got my answer in the StackOverflow page.
Student here. The simulator has 4.096 KB of virtual memory that can be edited dynamically in the browser along. Psuedo-instructions allow easier printing of registers than syscalls(syscalls are not implemented as of posting). All registers can be viewed on a single page in real-time. Break points are also roughly supported, although the interface for them isn't spectacular. Supports stepping through your code(which works perfectly in the simulator but the like tracking is off in the interface). You can see it live at http://mips.nieves.io/
I tried to upgrade but it turned /path/ into /path/index/. Anyone got an idea why?
I'm mostly interested in the performance improvement. GCC is way more advanced than the Go compiler at the optimization game. However, without the escape analysis all hard work is wasted allocating and garbage collecting stuff.
Seriously, gofmt is one of the best things to have happened to my programming life ever. It's amazing.
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
This is the best tl;dr I could make, [original](https://gcc.gnu.org/gcc-7/changes.html) reduced by 95%. (I'm a bot) ***** &gt; Wmemset-elt-size warns for memset calls, when the first argument references an array, and the third argument is a number equal to the number of elements of the array, but not the size of the array. &gt; Void* f warning: argument 1 range exceeds maximum object size 2147483647 The -Walloc-zero option detects calls to standard and user-defined memory allocation functions decorated with attribute alloc size with a zero argument. &gt; In contrast, a call to alloca that isn&amp;#039;t bounded at all such as in the following function will elicit the warning below regardless of the size argument to the option. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/64jrqi/gcc_7_adds_experimental_support_for_escape/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.65, ~99178 tl;drs so far.") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **warns**^#1 **call**^#2 **argument**^#3 **option**^#4 **function**^#5
I don't see at all how it's any less obvious. There's no ambiguity at all.
Last time I checked, I believe using Google Maps API required using it with a Google Map, otherwise it violates their ToS. (I used to work for an address verification company but I don't anymore. The company I worked for has an API that does this without those limitations.) Use of some API is pretty much essential, though, as a database of addresses is required to parse addresses correctly. (The API I developed there can also extract and parse addresses from arbitrary text. Fun fact: almost all their code has been rewritten in Go; that is where I first learned Go.)
You are a moron, throughout this thread and otherwise
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
Whatever your motivation was here, I really hope it made you happy to take the time to do nothing but throw a childish insult in my direction :)
&gt; Same with formatting, formatting is subjective, people should be allowed to format code however they like. Yeah, I'm gonna have to go ahead and call bullshit on that.
OP, I'm sorry that this thread has descended into... this. I hope there are some useful bits somewhere in here. I do honestly believe good formatting will help in getting some more constructive feedback. Generally I would argue that gofmt is a good place to start- let me know if you have any issues with it. What editor do you use? Thanks for the contribution and i look forward to seeing this project grow.
I think this is the key issue: &gt; As a concrete example, given a two-dim. slice m representing a matrix [,]float64, with the proposed design it is easy and efficient (no copy of matrix elements involved) to select a matrix row i as a sub-slice m[i, :] but it is impossible to select a matrix column j that way (m[:, j] is invalid). In other words, indexing is asymmetric, and the asymmetry will lead to special treatment in algorithms (columns must be explicitly copied element-wise). A key reason for adding multi-dimensional slices to the language as a core feature, rather than using a library, is so that code can be written straightforwardly using `a[x,y]` syntax and still be performant and compatible with other libraries. That won't be the case if performance varies wildly depending on whether you access data by row or by column.
Gorilla mux is fine, but be aware that Gorilla's Context package uses a global Mutex for access to parameters, which will have some impact on concurrent benchmarks if you use it as well.
Thank you! It wasn't an assignment or anything. I was just bored and needed a project. :P
build your binary from the docker image golang:1.8.1-alpine
That'd probably be good. It's essentially a Google Maps API client.
&gt; &gt; Thanks mwholt. Appreciate the feedback. Updated the README.
ListenAndServe() will return immediately after you call Shutdown(). Shutdown() will block until all HTTP request are done.
could you please provide me a simple working example? thank you
Thanks, my strategy is to ignore offensive posts but there's some useful feedback. In fact I think the only valid objection is that there's no documention/comments. But I get the point about gofmt though code formatting shouldn't be enforced. I'm adding more code now (the library is actually a port of my original code in Swift) and will reformat it using gofmt before the next major commit. 
 ctx, cancel := context.WithTimeout(context.Background(), time.Second*5) err := srv.Shutdown(ctx) Haven't tested that it works, but that should cause the HTTP server to shutdown gracefully, or return an error after 5 seconds.
"The standard library can be a bit lacking though". I have always considered the Go standard library to be the best I have seen.
So you're basically saying that because memory is 1-dimensional, multi-dimensional arrays require an abstraction that asymmetrically affects performance, depending on whether you are slicing by rows/columns etc..., and that would not be in keeping with a performant language like Go? That kinda makes sense. I understand fortran has multi-dimensional slices - I'm curious to see how that is implemented...
For this particular use case, don't just make the `.RemoteAddr` field explicit in the calling code, but also skip the function and use: ip := strings.Split(r.RemoteAddr, ":")[0] The `":"` and `[0]` are the only extra info and don't need hiding away in a function, just as `.RemoteAddr` doesn't. 
Indeed. To my shame I must admit that I only know half of the names listed!
I cant argue with that as it is subjective. What I see as waste of time is spreading this little parts of knowledge about Go on all different sides. If this was an article about some corner case, or exotic, rarer feature I would be 100% behind it, but these example is about most basic stuff people should learning from official docs in linear fashion. For newcomers it is most important to go trough official docs or tutorials because knowledge is acquired in layers. Order is very important so they understand next steps because they have good foundation from previous steps, and official docs have *good* order of things described and most experience in what to present to new people so they get good overview of language. You can't get that from blogs or articles covering only small pieces of it in random fashion on random places around internet. When people complain or ask about some most basic features (like never ending initial gopath problems) it is obvious they haven't used proper docs to learn to begin with and have holes in knowledge.
What are this new things? I only heard of Gin similar performance level. (even though in my case, I'm not restricted by the http router performance anyway)
&gt; told that the standard library is good Yeah that is a half-truth at best, don't really see why people are so adamant about this. The so called frameworks that I've encountered (chi, gin, echo) are very lightweight anyway, and tend to be "cooperative" wrt the std lib. Basically just filling in the gaps.
Another benefit of doing it this way would be that your edge wouldn't need to know about new resource codes either, the K/V store would just index based on requested name and resource code.
Well put.
Not every response will be the same. They different based on client details, query source, server region, service check results, and many other factors.
Not bad, you have understood some essentials of Go to get this far. Asking for a review is a good next step, because I'm going to tell you a bunch of things to learn about and fix. :) A package-level comment tells readers what to expect this program, to do, and makes godoc say something useful for your program. Try it now and marvel at the empty man page you get. :( It is not idiomatic in Go to use something like maybeError. Panic is only for unexpected, unrecoverable situations. Expected situations like missing parameters should result in serving an HTTP 500 error, instead of logging an entire traceback in the server log. A good rule of thumb might be: A person's first Go program will never need panic. Learn to do it correctly with errors first. If something is unimplemented, have it return error 404, with a comment in the code explaining that it is unimplemented. Prototypes sometimes end up in production, and leaving something halfway finished accidentally happens. Write it from the first moment so that the behavior matches the situation. (Instead of having a GET that seems to work, but doesn't really.) Preparing statements should happen one time per database connection, and then you should amortize the cost of making the database connection over hundreds or thousands of transactions. Read up about how to cooperate with the built-in connection pooling in database/sql. It is excellent that you are using only standard libraries. Keep doing that. Avoid using any extra libraries for your next few programs, then add them only when they give you something really worth adding a dependency for. -jeff 
I needed a backend for ACRA without dependencies like PHP, MySQL, CouchDB etc. so I created one in Go. By default server is compiled with boltdb support, but you can pass tags for leveldb and scribble when you build/install. I would love some feedback on code organization, db interface maybe, or anything you think is not right.
yes.
Thanks for your feedback! I will try to adapt is as soon as possible and hope I will improve my golang skills mach further. 
I would also add some sanity check on the value you plan to use as table key. 
What do you mean by that? The routes match exactly the table keys so
It all depends on what your trying to do @xienze I think what the program in the article is a perfect example, even with the package you mentioned you would still have to write a configuration file and another program to actually post the Tweet, so you still have to create an application.
I was excited too until I saw the no mobile bit. Is it just you running this? Why not recruit other judges and find a sponsor so this has broader appeal? I think this is a great idea, but if your the only one behind it, doesn't seem to have much appeal.
Agreed, especially given the precedent set by all the marshalers, unmarshalers, encoders, and decoders in the `encoding/...` package.
Yeah, I just randomly had the idea and decided to throw it up there and see what happened. I love Golang, but there isn't a lot of movement in the gamedev space. I'm just a hobbyist dude, by myself. If there were some people who'd be willing to help then sure. I have no contacts or anything of the sort to go about recruiting judges/sponsors to assist in that manner. I also didn't expect there to be any huge measure of interest (hence why I also made it a fairly lengthy game jam to give plenty of time for anyone to dip their toes in). I'm limited in what platforms I can accept/judge myself. If there's others than want to help in that way, I'd be open to it. There's still a week until it officially starts so there might be time to find interested parties to help. If anyone wants to help enable Mobile/Mac support for judging purposes, or wants to sponsor the game jam, I'd be certainly happy to accommodate.
Oh, I didn't expect this :D Maybe I should put my incomplete Pixel library to test? https://github.com/faiface/pixel
Yeah, please don't take what I said to be a critique, it's just that I think it's a wonderful idea that I wish was bigger!
Why develop within a container? Seems like more work than necessary.
I just don't want to install Go on computer I am using 
If it goes well, I'll run another, and refine the process as things go along \^.^
For those that are able to help provide real world testing, you can use the coupon code ~~50_4_6mo_VNmBX (50% off for 6 months)~~ 100_4_6mo_JkKD6 (100% off for 6 months) - or if you want to self host https://github.com/bradleyfalzon/gopherci The goal of GopherCI is to run the most common linters on each Push or PR to GitHub (eventually self-hosted GitLab etc). I run my linters in my editor, but I also help maintain some projects where contributors don't - so it's good to have GopherCI auto comment on those issues and raise them early. Example: https://gci.gopherci.io/analysis/43 You can self-host, but I haven't focused on that right now, so I encourage you to use the hosted edition, it's not free as there is are operational costs to recover, but it's designed to be affordable. It also includes my apicompat tool to help detect breaking changes. Check out the issues pages for the development roadmap: https://github.com/bradleyfalzon/gopherci/issues 
It is not my computer and i do not have permission to install new stuff, docker is installed thought
In that case, setup a docker image as if it's a fresh dev box. You're sure the computer doesn't have QEMU or some other VM software?
Are you planning on having free hosting for open source projects? Travis and a few others already do this. 
There's also https://github.com/justwatchcom/gopass. I'm not entirely sure why so many applications are called gopass, but I guess that's ok.
Potentially, but if it's important to have it free, you certainly can self-host. I plan to write better docs and an upgrade process so you can run it yourself as easily as possible. The hosted pricing's goal is to cover the hosting costs, not drive massive profits, so if you have low volume, you have low costs. If all goes well, there may be a low-cost yearly plan such as $20-$40/year, so you don't need to have ongoing monthly charges. There are other future ideas possibilities too if revenue exceeds costs, but I'd like to see that happen before I plan for it. Happy to hear feedback on any of this, nothing's set in stone and it all comes down to usage and feedback. But a free plan isn't likely any time soon.
~~The link to apicompat seems broken. It points to https://github.com/bradleyfalzon/gopher instead of https://github.com/bradleyfalzon/apicompat.~~ Edit: fixed And I just realized apicompat is your own tool. Thanks, I hadn't heard of it before and it seems nice.
ah damn, thanks and fixed. Yeah, GopherCI was originally used to run that tool, as, imho, it's the best place to run a tool such as apicompat.
BTW Could you point me in the right direction how I could implement authorization for post requests to the api? My simplest approach which I currently think of would be sending username/password or something as additional data in the post request.
in other words, pay to beta test?
I'd go around this in another way. Why do you need relative imports? What critical benefits do they bring so that you must use them but you can't? I've seen the comments that you cited but none of them seem to be answering those questions. Also symlinks make usage of relative imports significantly more complex. 
It certainly will provide value now, and has been to me and to other early adopters. I'm looking for edge cases and understanding how the system performs under higher loads. So I'm hesitant to call it beta testing. But if you want to go down the self hosted options, check out the contributing guide, or if you still want to try it out without paying, use the code 100_4_6mo_JkKD6 for 100% off for 6 months.
If you fork a repository, there are two possibilities: a) You intend to follow upstream and just want to add a patch or two (either for your own use, or to make a PR). You don't need to change any import paths, just use the same import path as upstream and maybe add your repo as a separate origin. Or b) you intend to fork the project. In that case, you need to update all the import paths, *because it's a new project*, so it shouldn't have the same name.
mine is called npass :D https://github.com/nablaone/npass 
Sweet! Finally a good reason to get started on that roguelike I've been thinking about for a long while now. Gotta dig up that list of terminal/curses libraries someone posted a week or two ago. Also, anyone has some sort of a list comparing any available go libs implementing entity-component-systems?
Hi there, We developed a special docker container exactly for that and use it on a daily basis. The big advantage is, that you are absolutely certain which version of go you are using and you can use the docker network functionality to easily connect to database containers or services in other containers. We usually vendor our dependencies to be included in the project, but you can also use docker volumes to make packages from your go path available in the docker container. Check out the project at https://github.com/dkfbasel/hot-reload. The name of the container is dkfbasel/hot-reload-go:1.8.0.
http://stackoverflow.com/questions/14323872/using-forked-package-import-in-go
Im in board with your questions. 99% of people dont have an issue not having relative imports. 
nice, thanks for that. I use nvim and go - but at the moment all I do is use it for the text editor and syntax highlighting functions. Plus a sneaky :!go fmt main.go every now and then. I was looking for something that let me access documentation.
gomobile based games can be run on linux and windows. Windows currently just needs an [annoying main-loop](https://github.com/golang/go/issues/17325), but otherwise works well. (Oh, and openal doesn't work on Windows)
Currently the rule "Assets must be free of copyright claims (i.e. original or available for use without infringement of license.", is too ambiguous. I would put something such as "Remember to only use code and assets that you can legally use and share." or "Don't infringe on copyright. (i.e. steal other peoples assets.)" -- I assume that was the intent. Also, pick more judges or let the participants be the judges.
How is it different from https://goreportcard.com/ and https://codeclimate.com . Just apicompat or other stuff too?
Very true, which is why flash messages acting as decryption oracles is a fairly bad thing :(.
The whole book can be found as a PDF here https://info.checkmarx.com/wp-go Much easier to read
I've been using (and paying for) GopherCI for the last few months, and I'm very happy with it and the progress it's making. I'm using it alongside Travis CI for all my Go packages, which still does the heavy lifting of running tests, as well as trivial checks like gofmt. For me, the killer feature is that GopherCI is Go-specific, and does not require any per-repo enabling or configuration. I wanted something simple where I could just create a new repo for a Go package. With Travis, I have to create and include .travis.yml file, then go to their website, refresh the list of my repos, and enable running CI for the new repo. In the past, I had to update all .travis.yml files whenever a new Go version came out, which was ridiculously tedious! I've created a tool that generates my .travis.yml files, so that helps too. With GopherCI, I just enable the installation on a per-account/organization basis, and enjoy Go-specific checks that keep improving and being added every month. Finally, I really appreciate that it's written in Go and developed in the open, that just makes me feel better about giving it a few bucks every month to support it.
&gt; You won't be nearly as enthusiastic about go get when ‚Äì not if ‚Äì one of your dependencies' new version breaks your build. I disagree. I am very enthusiastic when a dependency breaks my build. First, it happens so very rarely. I spend less than 1% of my time with Go fixing my builds. It's usually trivial, the Go compiler tells you exactly what lines have problems and you can figure out how to fix them. Next, when it happens, it usually means the dependency now has a better API than before. Given how unwilling people are to change Go package APIs because it can break builds, I know that if a breaking API change was done, it was either necessary or a huge improvement. An important factor is that I choose my dependencies wisely. I consider the author and only use them if I trust the author to do what's best. Anyway, I just wanted to tell about a different take on `go get`. It's one of my favorite features of Go, precisely because it enforces there to be fewer versions that need to be maintained, by getting everyone to be on the same page. If you're in an environment that allows such style of work, it's absolutely incredible and liberating, I find. Dealing with vendoring packages is a lot more work and overhead, in my experience, than not vendoring them and staying up to date. I'm glad in most cases I can afford not to vendor them.
Eh I didn't see anything about the game being "graphically"..
Related: [Storing Passwords Securely](https://patrickmn.com/security/storing-passwords-securely/)
The thing is that in a production environment, I want to be assured that a build of the same revision is always going to look more or less the same. If I relied on just `go get` I'd have to always rely on the bleeding edge when doing builds, and if there's some product that's been chugging along just fine in production, I don't want to *have* to change it every time some upstream API changes, and I want to be sure that if I pull the repo I can build the project. I mean, most packages really don't change their API all that often, but I don't want to *have* to rely on package maintainers alone.
Yes, windows can use OpenAL, but the one in gomobile doesn't have support for it. Use `go build` to build the project, e.g. try the [basic example](https://github.com/golang/mobile/tree/master/example/basic). Maybe you need to install some libs, but I'm not sure from the top of my head.
Does this have any advantages over just using homebrew?
&gt; gofmt You raised 2 points here, the first is about that types don't match even if it's the same code but from two different sources. I have to say, that makes sense. Even on big projects with multiple programmers working, I don't see myself using the same library from two different sources. Furthermore if I create a package and call it "package1", then I copy that exact package and call it "package2". I wouldn't expect any programming language to assume that package1.SomeType is equal to package2.SomeType. In fact it's unti-pattern for modules. A module has to be self-contained, it shouldn't affect other modules unless it's imported by these modules. If I define for example type One int in two different packages, I am not expecting that package1.One will equal package2.One - In fact if that would have been the case, I would have regarded this is a bad design choice. as for your other point, that the important stuff happens in the unexported functions, that's again a smart choice. Bottom line, you want to have a tool that turns Apples -&gt; Bananas, you shouldn't fuss on how that happens. One of the choices in Go which is different from Functional Programming, which is my favorite design pattern, is to have the language simple. In a functional language, I would expect insane abstractions, then all the important stuff are packaged into their own pure functional modules. But Golang didn't go that way, abstractions are hard, Go expects you to make copies of your code rather than having higher level abstractions. This is the language choice, love it or leave it. Haskell is very good at higher level abstractions - insane good. 
I am trying to conjure comparisons in my mind to other cases I've ran into. For now I am going to have to with that they are not the same, even if their binaries are the same. Bottom line it'll be package1.a and package2.a files. Assuming my example: type One int package1.One != package2.One even if it's name and type are the same. In fact, every time I write a module, this module is self-contained. The module does not return types from the modules it uses. It can be just something I haven't ran into yet. I am thinking about what would have happen in the same case in c++ where for example you would have tried to use the same library from two different locations. Seems to me that the final solution will be to implement some kind of Go package signature, similar to the namespace in android/iOS and then so no matter from which source you `go get` the code, it'll place it in the signature's path. Anyway plenty to think about :)
Strangely enough, despite having set the various voting categories, it doesn't display them on the jam page (I'm new with itch.io, hence why I gave myself a little over a week to get all the finer details sorted before the Jam goes live). I will have to add a section on the judgement criteria. I won't discourage submitting a game that's using a text-mode interface (eg: dwarf fortress or something). I've definitely seen some creative stuff in that respect. My initial intention, however, was primarily to inspire games made in Golang that utilize some form of Graphical rendering. (2D/3D, hardware accelerated or software, etc) Other categories include Code quality (Idiomacy), Gameplay, and Story. So a text-mode game would certainly be able to qualify in those respects! I apologize for not being more specific. I'm new at this :P 
Right now working on 1.8 :D ( some legacy code in 1.6 ), thanks for the tips, great thinking and reading material.
How many rounds would you say to use? With PBKDF2 I was using like 50,000 rounds with sha512 (performance wasn't a big deal), how many do you suggest?
I'd generally recommend using https://godoc.org/golang.org/x/crypto/bcrypt#GenerateFromPassword for password hashing in tandem with an HMAC - following https://wiki.mozilla.org/WebAppSec/Secure_Coding_Guidelines#Password_Storage But if you're set on PBKDF2 - do some benchmarks and set the # of rounds to take as long as you can possibly get away with, without compromising your user experience. So enough rounds for it to take 400-500ms or so (but that can vary)
My favoured option is to not store it and just use something like Auth0. https://auth0.com/
The biggest code smell here to me, is the maybeError method. As someone else mentioned, using a panic() should be last resort, and even then... only in very extreme and unexpected cases where an error should not be occurring to begin with. The idea behind your maybeError method also seems misguided. The point for checking after each err := assignment is to properly handle and/or set default values as needed. By passing all errors off to the same method, you're eliminating the ability to handle each on a case-by-case basis. Sure, it makes the code look less "clean" (imo) and adds to your overall linecount, but seems that the standard is to check each error as you go. The rest looks good, especially for your first Go project. One other very minor note, but the getPort() function is probably overkill, especially since it only gets called once upon start. Would make more sense to define a Port var, then assign it within an init() method, or just within main() itself. Like I said, that's a really minor observation though. If getPort() was doing something other than just read an environment variable, there might be a better case for using it. But as-is seems unnecessary.
vim-go is awesome. Give it a try. It'll do a go-fmt every time you save, and so much more. It also has :GoDoc &amp; :GoDocBrower but I tend to use :GoDef instead, the source shows more than the docs alone :)
in case you wanted to c&amp;p the banner comment: /*-------, .-------* | +----------------------+ | *----| THE REALTIME GUILD |----* `*--------------------*/
the point is this is a custom lib package, so by itself it is not meant to run as a http server, and I would like to set up container for such lib development, with remote debugging
I really like this. Looks neat too. I just love all these neat and creative solutions people make with Go.
I do plan on doing a more thorough comparison. Goreportcard is designed to scan a repo at this exact point in time and report all issues. GopherCI will watch your repository, run similar scans but only report issues (via GitHub build failure or comments) on new issues. codeclimate and other similar services (https://houndci.com/) offer a very similar, commercial alternative. GopherCI supports more Go tools, but mostly it's about being a free and open source alternative to these offerings.
Good point. VS Code and other light IDEs with Go support will point those out for you, as well.
I read that as socks and am now obsessed with that idea.
Is it worth working on these edge cases just for testing purposes? Also has anyone 
That took a long time to get to the interesting conclusion: that a benchmark which pegs the CPU can cause much longer Go GC pauses than the Go development team expects. I wish they'd tested the Go development team's conclusion that it was caused by the benchmark's lack of CPU pauses by also benchmarking the GC time of their real program (which presumably didn't peg the CPU) to see if there was indeed a 100x improvement in GC pause times between the benchmark and the real program.
Interactive Brokers has an API.
I believe interactive brokers has an API, but otherwise if your financial institution doesn't have an API you can build a web scraper using net/http to automatically place trades
No problem 
I don't think it's making things simpler. Because go do allows relative import if I'm not inside `$GOPATH`. It's twisted and in-consistent behavior. EDIT: &gt; Because go do allows relative import if I'm not inside `$GOPATH` Recently I do have a project finished outside `$GOPATH`. (it's simpler to start a project this way, `$GOPATH` is just a cache for the dependencies I've downloaded)
`$ git pull &amp;&amp; git checkout goX.Y &amp;&amp; ./all.bash`
I would agree, considering those not comfortable with this may have already switched to other languages. EDIT: Actually I'm planning on switching to rust for future projects. I've been reading the documentation recently. 
&gt; you intend to fork the project. In that case, you need to update all the import paths, because it's a new project, so it shouldn't have the same name. That's the pain. Normally with other languages like python I don't need to worried about this. I just found it's easier to just write code outside of `$GOPATH`, which would allow relative import. It's ok if I'm not trying to write a library, because nobody is gonna import my code which contains relative import. Another option is to have per `$GOPATH` per project. But considering the effort required for setting up a project and `go get` each time without a user-wide cache, I'll just skip this one.
&gt; Normally with other languages like python I don't need to worried about this. Yes, you do. If you upload a new project to npm/cpan/whatevs, you need to rename it and people who want to import it need to use the new name.
Does anyone know if there is a summary out there somewhere about this new dependency system? Stufd like where dependencies will be stored? Should they be checked in to your own repo? Is there a central server the system gets packages from or are they downloaded directly from github?
Is there any package managers out there for any modern language out there that works like that? Committing vendor libraries to your own repository? I'm coming from the web work and I don't think any of the languages I have worked with works like that. Would be interesting to read up on some other solutions. I like the idea of having it checked in and not being dependent on external services when deploying.
&gt; Should they be checked in to your own repo? You can if you want. There is no universally applicable advice.
Javascript has yarn, a fairly recent development. Ruby's `bundle` can also download gems into `vendor` folder that you can check in. While these tools allow downloading and possibly checking in vendored libs, they neither recommend nor advise against it. A common use case to do so is to prevent redownloading the same lib over and over again for every build. Also in a CI/CD workflow where the build machine does not have internet access for various IT/security reasons. And also, to simply decouple your build process from external dependencies. What if github.com is down?
which one of the Interactive Broker golang libs are people using ? https://golanglibs.com/top?q=ib I checked the top 3 and they have not had any updates for a long time. Are they stable and usable ?
You could. But should you? :)
nice, /u/sdboyer - any comments regarding the debate of `dep`vs `get`? A way to unify both interfaces instead of having two ways to deal with dependencies?
What was the tool he used to visualize profile?
How much I hate when website feels need to change how my mouse scroll works. Didn't read the article because of that.
What happens when a package is then removed from npm?
It's hard to write a summary of a tool like this, because the tool has many facets and different people want different things summarized :D A registry is something we've discussed as a possibility (and I do think we should hope to arrive at eventually), but it's not in our immediate plans - https://github.com/golang/dep/issues/174 leftpad-type events is usually what folks are trying to defend against when they commit their dependencies directly. A reliable append-only registry would largely obviate the need for such maneuvers.
This is a leftpad-type event; see my above comment https://www.reddit.com/r/golang/comments/653mxc/dep_status_week_of_april_10/dg7f21y/.
This is pretty similar to our reasoning, as well.
I used `go tool trace`. It's really powerful. I actually just wrote [a blog post](https://making.pusher.com/go-tool-trace/) on how to use it.
I stand corrected, thanks for sorting that out
I have, and that does work and produce a statically linked binary. However, the first time I did that was before I had used MitchellH's library for getting the user's home directory. When using the built-in `user` package with `CGO_ENABLED=0` it still built perfectly fine, but when I tried to run the resulting binary I received the following error: &gt; user: Current not implemented on linux/amd64 Given that I wasn't warned about that during the build process, it's now made me a little nervous that some other part of my application may fail, and I'm just curious to know if there's something that will say something like: &gt; Package '&lt;PACKAGE&gt;' could use CGo, something may be broken, or not work as expected.
If you're passing around this struct by value, there will be zero difference in performance from passing around just the `[]byte` Structs contain no special data in addition to their members when represented in memory, so the wrapper struct with a single `[]byte` is for performance sake equivalent to a pure `[]byte`
Personally, I think a registry is a bad idea. IMO, one of the best parts about `go get` is that there's no authority you need to appeal to in order to publish your software to a place where it's accessible to everyone using the standard go tooling. You just put it on the internet accessible via VCS. Bam, done. I like that I don't have to ask anyone for permission to post my code. There's no board of directors to decide that maybe my code or package name isn't appropriate. And with vanity imports, you have total freedom about where and how to host your code.
Thanks. That's what I thought. I just had one of those nagging feelings that I may be missing something.
Not speaking for /u/chrighton, but for myself, SQS has some fantastic characteristics: 1) Highly available 2) Highly resilient 3) Highly scalable 4) Infinite loop handling via Redrive feature 5) Can be a subscriber to SNS, allowing for pubsub 6) Very inexpensive 7) Maintenance free operation The *only* downside I see with SNS/SQS are: 1) Not ultra-low latency Common downsides which are commonly cited: 1) Out of order delivery is guaranteed 2) Multiple message delivery is guaranteed Both common downsides are required to deliver the scalability that SNS/SQS guarantee, and are rarely if ever an issue when developing with HTTP semantics.
Spot on
My advice to you. Mark the cookies as Secure if you want them to only be sent over HTTPS. Cookie flags are kind of hand-wavy in what they actually provide, but it's helpful. Also, IMO you are over complicating things. Cookies already have mechanisms to be expired client side, and server side they can be expired as well. Issuing lots of access tokens and a long-lived refresh token is complicated, and not much more secure than just issuing a longer lived access token. The difference is negligible in exchange for lots of work. You are better off making it easy for users to revoke and issue new access tokens. 
I was in the same boat as you. Here's the approach I took: 1) User must hit `/api/user/register` or `api/user/login`. Upon success, the user will receive an *access* token and a *refresh* token. 2) Subsequent requests must include the *access* token. If the endpoint is protected, the access token will be inspected. - a) Expiration check - expired token results in returning a 401 - b) Signature check - a token that has been tampered with results in returning a 401 - c) Authorizaion check - ok, I see you are who you say you are, but are you allowed to be here? if not, we return a 401 - d) Now we can complete the request 3) The user hits `api/user/logout`, so we expire or remove their *refresh* token. On the client side, if you are receiving a 401, it means that case a, b or c has probably happened. Here's how to handle that: - a) Our access token is expired, so let's get a new one. Send a request to `/api/user/token` with our *refresh* token. The server will verify that it actually issued the provided refresh token. If it did, the server can form a new *access* token and pass it back to the client. - b) Something about the token is malformed. We can take the same steps as a) and try to get a new *access* token. - c) We need different privileges to access the desired endpoint. To sum this up: *Access* tokens are very short lived (I usually have them last ~20 minutes). When these expire, we send in the *refresh* token to get a new *access* token, which allows the user to *access* particular endpoints. *Refresh* tokens will essentially live for the duration of the user being 'signed in'. These can have a long expiration (say, a few months), or they could not expire at all and you just remove it upon a user logging out. It is not unreasonable to have more than one good *refresh* token stored for a single user if you are using one per device per user. **EDIT** Some of the points I've made are assuming you're using JWTs for your access tokens.
Thanks! I think it's Lake Louise.
It's a file format. There's nothing particularly interesting about it. The core reason for going down the rabbit hole was that json doesn't support comments. But the toml parser they're using doesn't preserve comments.
If you are concerned about external dependency repositories going AWOL, there is an alternative. Clone them into your own repository and vendor that. This method has advantages: - dependencies have the same availability as your projects code - you store it only once even if several of your projects use (possibly different revisions or branches even) it - you can make your own modifications of dependencies' code and just keep it to yourself or send PR upstream - you can pull changes from upstream selectively and at your own volition The canonical URL of the original dependency repository no longer applies. And, of course, it requires a lot of work initially to clone all the dependencies (that's where a proper tool would be handy). 
I hear ya. I was responsible for devising the (new version of the) system that "granted access" for contributing fully-qualified projects to Drupal. I had misgivings then, but was just converting the social process that was already in place. Last month, after seven years of hand-wringing and backlogged queues for approval, they just switched to a [looser model](https://www.drupal.org/security-team2/security-advisory-process-and-permissions-policy) that doesn't require approval in order to occupy a top-level namespace. Oi. All that is to say, while there are numerous technical advantages to a registry (many are described in the linked issue), I'm keenly aware of the social dimensions of the problem, as well. Also, it's worth noting that a registry could really only ever be _complementary_ to the way things work today. What, we're just going to make a whole giant swathe of existing Go code not work? Nahhhh... ü§°ü§°ü§°
No, the payload will only accept a certain set of values (although a decent set), and since we deal with UTF-8 data, it's better to be safe and just encode it. Check out the specs for MessageBody: https://docs.aws.amazon.com/sdk-for-go/api/service/sqs/#SendMessageInput
See the gif screencast and screenshots at the link if it is not clear what this extension does. Differently from Octolinker, Module Linker adds real `&lt;a href=...&gt;` links to the names of the imported modules, so you can see where you're going if you click the link and middle-clicks or ctrl-clicks work seamlessly. Please let me know if you have any positive or negative feedback, or if you have a suggestion to improve this extension.
Like I said, I won't discourage submissions that do not utilize 2D/3D rendering. If you want to submit a text-mode game, by all means. There are other categories in which it can be judged within, such as story, gameplay, code quality, even sound. I don't feel I need to clarify that on the page, since I'd be happy to accept the submissions that lack a 2d/3d graphics engine altogether. I did add the judgement categories onto the page so people can see what there is to strive to be the best in. I do appreciate the criticism, however. It will help me to refine my process to host more golang game jams in the future!
This is my website on go tutorials https://golangbot.com/. Please do have a look and share you feedback. Thank you.
Don't you need the netgo tag too ? Not 100% sure but that's what I do 
Might I suggest commenting it using GoDoc
Thanks for the update! I recently tried using `dep` in my side-projects, but it was way too slow... running `dep ensure` on some project with an already up-to-date vendor dir was taking what seemed like forever (I can dig some `dep ensure -v` output tomorrow if someone is interested). So I went back to Glide. Now I just updated and tried it again and it seems to have improved (though I'm trying on a different project and a different machine from before, so it's not a reliable test) Keep up the good work!
For the record, `dep` claims complete ownership of your vendor dir, and on first run will effectively wipe it and repopulate it from scratch. So whether or not it was already up to date is irrelevant.
Oh now this is actually really useful! Thanks! Is Safari on the radar by chance? 
[@dmikalova's latest tweet](http://i.imgur.com/IfFk94K.jpg) [@dmikalova on Twitter](https://twitter.com/dmikalova) - ^i ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
Ah, its transmitted as xml so the charset is restricted, so you have to base64. Thanks!
That's kind of up to you. You could redirect to a login page, or you could leave it up to the client to know that if the refresh token isn't valid they need to re-authenticate.
update-golang.sh does not compile from sources, it uses local system OS and ARCH to download the correct binary release. 
Hmm okay. Also, for checking validity of a refresh token, it could be something as simple as checking if the refresh token's string exists in a table in the database, right?
Thanks, and thanks for the feedback! There's a number of factors wrt performance, but `dep ensure` runs generally usually should end up being not-intolerably slow. There's an up-front cost, typically paid on `dep init`, while the system populates a central cache of repositories on disk. That tends to be the highest cost. There's still a lot of network activity that can end up causing normal runs to slow down, even without a lot of cloning activity. With the refactoring of gps done that I mentioned in the post, it's now possible for us to start looking at [implementing a persistent cache](https://github.com/sdboyer/gps/issues/130). Once that's done, I hope that most `dep` invocations will [drop below 1s](https://github.com/golang/dep/issues/67#issuecomment-269580970).
Well, I don't think I can do that anytime soon, because I don't have a Mac or even know anyone who has one. But if Safari supports more-or-less the same extension API as Chrome it should be simple to support it. Would you care to try?
Okay - so the first thing you wanna do is to move main package into separate directory. Usually it's called %your project github path%/cmd/%you binary name%. This is a must or everyone who would want to use your library will be forced to build the main binary as well on the go get stage. Second - don't println errors. Return them until you can handle them. Do not continue execution as if nothing happened. This can lead to disasters. Also do not use error.New if you not storing it in a global variable. The reason for that is that I want to be able to catch your error without comparing strings. Avoid fmt.Errorf in most of the cases. Do not store methods in separate file of type declaration. This makes navigation harder. IMHO you should not store .idea directory inside git repo. We are all using different editors - you can use whatever you like - but there is no reason to store this meta inside repo. Docs - this is super important. Document methods. Document constants. Document variables. Document your thoughts inside the code. This is very important to understand your intentions. Thats all for now - I'll try to investigate more later. 
Good for you for striking down the hammer, sdboyer. I am still irked that an underdeveloped file format is chosen even though we have so many others, battle-tested and mature, that could fit the bill just fine. I stand corrected that engineers are opinionated. That long of a discussion for choosing a format ‡≤†_‡≤†
Good point with the last bit on unique identifiers. I think I have enough to start coding it up though, thanks so much for the clarification 
Could you link up a consumer for SQS as well? Are you consuming SQS payloads directly with Go, or passing them on to some external scripts/etc (syscall exec?). Do you have one consumer that can consume all the things that you throw on the queue, or do you have individual consumers for a single queue out of many?
I have a Go consumer app, that receives all the messages from a particular queue. We can run as many instances of that as we need to handle whatever load we have. It reads a message, checks the type, and calls a processor according to type. Each processor knows how to unmarshall the payload for it's message type, and then handles the message. One example is sending emails. I don't want to distribute all the configuration, templates and credentials for generating and sending emails, so I centralize that in my task processor. The clients then just send a small request with template name, destination email, and a map of template variables. That's all they need to know. Here is a snippet of consumer code: https://gist.github.com/jsokel/176a026d013d3533f8faff849d7dea12 
I actually tried to setup a Teleport server once. Didn't have a very pleasant experience trying to have it work like a glorified SSH jumpbox. That was like a good month ago too. I loved the web UI, but it didn't come together as well as I had hoped. Maybe I'll give it another try soon.
You are printing errors in tools package. Didn't look how much it is used around the code. Declare errors at the top level. Use github.com/pkg/errors if you need to add some text context when error is returned (do not forget to mention this in the docs). For a more complex scenarios write custom types which implement error interface. Prefer to use existing errors and error types but ONLY if they apply to your domain. Yes, combine requests and endpoints.go. I think most of us can handle 1k code in one file. Docs - spend more than one sentence explaining what method does. Dont overdo this, tho. Explain behavior if method has side effects (most of them are). Inside the method code explain the usage of magic constants and some of a control flow if you need it. The good advice is "write the comments about what you thought when you wrote this part of code". This helps a lot to understand your intentions in the future. 
Ahhh ok, I was focusing more on the "what" and "how" and I completely forgot the "why". Thank you so much!
No problem. I know how much time I spent stuck in a trance or pacing around my apartment trying to wrap my head around how to do this stuff, so I like to help out when I can. Shoot me a message if you need help with anything else. Happy coding
TOML is, essentially, formalized INI. INI has no formal specification; TOML fixes that.
What you suggest mitigates what I said, I suggest you recheck the docs.
Yeah that's true, I suppose. But I want to see if I can implement it. But if it doesn't work out I could try just using access tokens with secure cookies.
I might! I would've volunteered right of the bat but it's been ages since I've made a Chrome extension and have never made a Safari extension before :-)
At the time, documentation was lacking, setup was pretty isoteric, and it's was kinda overengineered for my needs
Sorry to add another question, but if you don't mind ‚Äì could you expound on why you felt it was overengineered? In terms of installation and amount of infrastructure? More complicated than setting up OpenVPN and a CA, for example? The features are definitely motivating, but this could put me off also, especially if the docs are sparse.
Simple program that I've made, it is limited on what it can scrape for now. Any suggestions on how to improve code are welcome!
I was under the impression that the tool was like a SSH JumpBox manager that maintained compatibility with SSH/SCP/RSync etc., but however the program is based more off using their own [replacement/equivalent tools](http://gravitational.com/teleport/docs/2.0/quickstart/). It requires quite a bit more setup than just setting up a simple reverse SSH into the server (a requirement for my setup), so you need Teleport tooling on all devices you want on the cluster.
It goes, doesn't it? If it is not, then it is a bug, please let me see where it is failing.
Hey, sorry you did not get the best experience setting up Teleport, I'd appreciate if you file the issue with the problems you've encountered, and we'd do our best to fix those. Also, Teleport can work both with OpenSSH clients and servers: http://gravitational.com/teleport/docs/2.0/admin-guide/#using-teleport-with-openssh
What is it?
Awesome to see Teleport project on the r/golang ! I will be glad to answer anyone's questions about the project here!
I'm still getting used to Go, and writing clean web servers was the hardest part (which frankly I don't think I did super well), I'd LOVE to get some feedback on writing clean web servers in Go! Thanks!
"There are only two kinds of programming languages: those people always bitch about and those nobody uses.‚Äù Bjarne Stroustrup.
how does Teleport compare with Ansible if we are just doing simple parallel ssh tasks ?
as it is just an SSH server it is compatible with Ansible: http://gravitational.com/teleport/docs/2.0/admin-guide/#integrating-with-ansible although we sometimes use tsh instead for simple tasks, e.g. execute `etcdctl` on multiple hosts filtered by label: `tsh ssh role=etcd etcdctl status`
Oh hell yeah that message is good. not only explaining what he did bit slso why. And both pretty in-depth. Sad that you cant really upvote commits :D
I like it :)
Is Go a mainstream language? - has a virus done in it? ‚úîÔ∏è - has a porn thing done in it? ‚úîÔ∏è - has an OS been done with it? Getting close... 
Thanks! There is a still a lot more work to be done... always looking for a helping hand :)
Oh right. I already looked but I missed it (It's a big thread). Upon closer inspection I was able to find it. In case anyone else is interested in the reasons why the Go format was not chosen: &gt; * I think it would be unwise to invent any new format for this purpose, a la @kr's proposal. Comment preservation is nice, but it's not required, and IMO is less important than any one of the following: &gt; * This tool already involves creating a lot of new things that are essential to the problem. New things are risky. We should not make new things for non-essential problems. It's hard to predict who else might want to read this data for what purpose, but @andrew already showed up with a use case. Creating a custom format would make that harder to do. &gt; * The information we need to keep in a manifest is easily expressed declaratively. Comment preservation isn't worth sacrificing that, or even the appearance of it, with a [pseudo]-imperative format.
Very nice project!
Short answer is "very yes". The core net/http library uses goroutines for request handling, which allows for handling multiple requests simultaneously. Multiprocessing, at least as you know it in python, isn't really a thing in go, but that's because it would introduce unnecessary overhead. In Python, (cpython specifically) multi-threading is not very useful in most cases because only one thread may be running interpreted python code at a time because of a global interpreter lock. Python gets around this bottleneck by introducing multiple processes, each with its own interpreter lock. In go, there is no equivalent concept. Goroutines can take full advantage of multiple processors and the language itself is responsible for scheduling them. They are similar to os threads in concept, but are much lighter weight.(Almost identical to the concept of fibers on Windows). Hope this helps. 
Do you mean the bytecode generated by LuaJIT?
`fmt.Printf("%s/n", input)` ?
[Raw string literals](https://golang.org/ref/spec#String_literals) are surrounded by backticks fmt.Println(`line 1 \n still line 1 line 2`) 
Replace any double quotes with backticks to do string literals: https://play.golang.org/p/BLlpquf7th E.g. in your default value for `separator` you should surround the newline escape characters with backticks. I further modified your code to use Printf instead of Println which again surrounds the string with backticks.
(I'm not an expert) A JIT's job is to profile interpreted code and turn the hotspots into optimized ASM. To do that, I think you need reliably consistent performance. A language with nondeterministic GC pauses can't give that to you. I think. Maybe someone who knows more about JITs and language theory can explain better.
That's a type conversion. If the concrete value behind the value `o` fulfills the interface Aligned, then `a` will be an interface that wraps the value in `o`, and `ok` will be true. And if the value in `o` does not fulfill the interface, `a` will be nil and `ok` will be false. It's a runtime way to convert from one interface to another. 
The situation is the same and will not change at the very least until Go 2 if generics are added or if they're (map/filter/reduce) special-cased into the language like the range keyword (very unlikely).
Synchronous is easy, and slow, in every language. What you're looking for is asynchronous. Go excels at asynchronous workloads. Take a look at goroutines and channels.
Thanks! I had another question. We assume in the above example that whatever we're sending into the AlignmentOf() function implements the Object interface. Let's say: type Object interface { ID() ID } and let's say we have a class: type Goblin struct { id ID alignment Alignment } What happens if we do this?: func (g *Goblin) ID() ID { return g.id } Even though a Goblin (not a *Goblin) object can call the ID() method (as in Goblin.ID()) the compiler tells me that a Goblin does not implement the Object interface. Goblin does not implement Object (ID method has pointer receiver) It appears that we can play fast and loose with pointer receivers except when it comes to implementing interfaces? 
Here is some preliminary results of this: https://twitter.com/asteroix/status/852935067022356480
Thanks. It seems a bit overkill for my usage, but it's definitely interesting to see alternatives to Apache Spark and similar in Go.
Having GC pauses does not prevent a JIT to work well. In fact, most languages that have a JIT use GC (e.g., Lua, Java, JavaScript). JITs profiles code not the same way a normal performance profiler would. They generally use method call counters or code path (i.e., trace) counters. These metrics are unaffected by the pauses. 
Could they add a standard library with more advanced and high level data containers? Being thread safe and supporting map/reduce/filter for example. C# provides that and it's very convenient. 
Thanks, I will take a look. 
No, as Go libraries are written in Go and Go doesn't have generics, while C# does.
Very interesting ! Can it compile redis ?
For security you want to form strong invariants, passing responsibility to the callers is not strong and how accidents happen. The code you understand today might as well be written by someone else in six months. Consider if you refactor Y to be configurable by user request you focus on feature Y's callsite without considering how your input propagates. You strip usernames to characters containing just dots and underscores, over the last six months you have gotten much more strict with input. Fantastic job. There's a subtle issue here, but it's not a problem on its own. Since this change you supported the delete method to remove someone's passwords. Harmless enough, you use the same validation method. Right? Wrong, you gone done fucked up. I'm going to request username ".." because it's valid. Now I'm going to request to delete my database. Delete becomes root+/.gostore/+name/. Which is ~/.gostore/.. - say goodbye to everyone's passwords and as a bonus your own home directory. Does it take specific circumstances to get their? Yes. But it happens. Why leave it to chance when properly using filepath and absolute path to ensure directories are rooted to a specific location is so simple. You have everything to lose and five minutes to gain. It happens in the real world, most everyone I [look at someone's code](https://github.com/huydx/hget/issues/7) who [manipulates the fs](https://www.reddit.com/r/golang/comments/5y8zss/acra_os_database_security_suite_written_in_go/dep44rj/). Limit reader is for protecting your resources and to form strong invariants. It causes you to define your inputs, to think about them and to test what happens when those limits are exceeded. A denial of service is a possible attack vector for unbound buffers, yes. But a service listening on the internet provides enough surface area to be vulnerable to ddos. What unbound buffers cause is you to rely on all of the code that touches that buffer to gracefully deal with overflow and garbage data as well. I think it's good your asking these questions and admire the effort you're putting into this. I think you should try to avoid password management even for a toy, because you would get better feedback with something like a keystore and would have a wider variety of inputs, could touch on various comp sci algorithms etc. But whatever floats your boat :-) Have fun.
Not to try and rain on your work, but I think I'd prefer (and recommend) https://github.com/moovweb/gvm instead.
Thats brilliant. 
[removed]
I think it's reasonable to accept that for better or worse, a lot of people are trained to think with inheritance and struggle to express code sharing in many other ways. It's a stumbling block, but one that is worth overcoming not just to program in Go. 
Have worked with a JS VM, Otto, in Go before. Hasn't been that problematic. We used it to define a DSL for writing end to end tests, what's nice about it is that you get the familiarity of JavaScript with the synchronous nature of Go. I'll have to post a link to an example when I'm back at my computer.
For sure! If you have any questions feel free to ask.
I need to unmarshall 1mb of Json per second and compared encoding/Json to easyjson and only saw minimal (15%) performance improvements. I might try this lib next.
 [These](https://chris.beams.io/posts/git-commit/) tips help me a lot when writing commit messages.
It is hard to estimate how long away that will be, but now that c2go has been completely converted into Go (it was originally Python), I (and others) can more aggressively attack all the other standard header files. https://github.com/elliotchance/c2go/wiki/Project-Progress
Awesome! Looking forward to squashing some bugs!
Thanks for ideas :). I'll reflect it soon!!
Thanks for feedback!! I organized the codebase with packages. Im preparing that write the unit tests for all APIs too!
Hopefully one day. c2go is still in very early stages, but I haven't come up with any reasons why it would not be possible. I would bet that the Go version would run slower simply because there would be a lot of C-like optimisations in Redis and some code would have to proxy through to Go methods that might not be exactly the same as their C counterpart.
The trick is to have the lexer inject semicolons at the end of lines _only_ after certain types of tokens. In go you inject if the line ends in an identifier, integer, or right paren, etc. Maybe think of a few places where go omits the semicolon and think about what type of token is at the end of a line. For lexing I'd highly recommend watching Rob Pike's presentation. Also take a look at the text/template package. You will see the technique he's talking about put into practice there. https://www.youtube.com/watch?v=HxaD_trXwRE For lexing, you may or may not want to use a code generator. You can write a very nice lexer without one.
Amdahl's Law https://en.wikipedia.org/wiki/Amdahl%27s_law
This package name lol.. it has to "go". Name it buffch, bytespool, recyclebuff .. anything at all, other than a three word camel case prefixed by the .. name of the language. Also don't start a go routine without providing a way for it to exit. Change func Init(int) to func Init(ctx, done) and check if ctx is done in for loop. If you want slightly better performance you can provide a close that function that just sets a non zero int with atomic.addint and drains the buffer before quitting. 
Thanks!
There are some problems that are just better expressed using inheritance, problems which Go is not as good at solving as other languages, a good example would be UI programming or hierarchical databases (Customer, Employee -&gt; Person) On the other hand, composition like golang is doing it can express other problems much better than inheritance ever could. TL;DR depends on the problem you have.
I wonder. I will test and upload the results.
Hello!
You wouldn't need to be worried about it. The built-in types don't do forced parsing; you'd have to be using the low-level KeyReader to even have the option to set it to true. It's probably not clear enough in the README, but I am making the assumption that typical users aren't hitting Escape or Alt+[ often enough to enable that forced parse mode. If you need to parse keys as opposed to read lines, then you have to make the decision as to whether you need absolutely immediate real-time key parsing of special keys like Escape. If that's your need, though, a terminal library is the wrong approach; you really want a low-level keyboard reader. Terminals are just far too absurd - you can't guarantee much about the keyboard state. As mentioned in the docs, you can fake a terminal into thinking you hit various keys, especially when you ssh somewhere, because there's a minuscule lag between you typing and the terminal interpreting. Incidentally, you'll find various issues with programs like vim when people are trying to make "Alt" work as part of a key mapping. Regarding your suggestion: waiting for EOF or program exit isn't a very graceful approach. Doing so would mean that once you hand over your io.Reader to my package, it becomes owned by the package. No, when I say that `terminal` is input and output agnostic, it's *completely agnostic*. If you want to read keystrokes recorded to a file and then at a certain point hand the io.Reader off for a completely different kind of processing, that's an option. If you want to build an uploader, you could use `terminal` to read lines from SSH until a certain command is entered, then just send your io.Reader off to something that reads the raw bytes rather than letting `terminal` read input interpreted as keystrokes. Then you could use `terminal` again as normal. This is a package that's meant to be usable far beyond the single-use line readers already in existence; otherwise, what would be the point?
I don't know man, no matter how faster the language is your tasks can be very slow if they depend on external data. And a task queue is's not just about processing things in the background because the language is not fast enough. It's also what the name says, a queue. Maybe you don't want to process everything at once, maybe you want to process at most X things in parallel and what you receive over that you queue until previous work is done. Let's say you have a webapp in GO that when users do something you need to make some calls to some 3rd party API. In this case you are limited on how fast that 3rd party API (that you have no control over it) works. Also what if it has some quotas (max nr of req per time interval)? With a task queue you can control these kind of things. Another example is how Bitbucket or GitHub works, each time you merge a PR from the website, it is queued, as you can't merge multiple PRs in parallel, GIT just doesn't work that way. They need to be sequential even if in the website you have multiple users pressing merge at same time. So they queue this process to their background workers and merge the PRs for each repo sequentially while if the PRs are from different repos they can be handled in parallel. And there are n scenarios like this one where you do need a task queue even if your app runs at light speed.
You should at least provide a License if you want contributions.
Brilliant, can't wait to play it!
Shut the fuck up
? 
I'm the author of secureoperator; figured I'd submit it to the Go community first to see what folks think; with ISP privacy rules changing, making your DNS traffic unreadable is a good start! Of course, this implementation assumes you trust Google (their [privacy policy](https://developers.google.com/speed/public-dns/privacy) is well spelled out), but the second part of this project will be to create a HTTPS -&gt; DNS bridge that anyone can run, which exposes the same API as Google's service. If you think this is cool but could be better, please file issues or make PRs!
Don't know much about azure but if it's a regular linux server you can ssh into then: Just ssh into server, download cloud torrent's linux binary, extract it and run it. wget https://github.com/jpillora/cloud-torrent/releases/download/0.8.16/cloud-torrent_linux_amd64.gz gunzip -S -f cloud-torrent_linux_amd64.gz chmod +x cloud-torrent_linux_amd64 ./cloud-torrent_linux_amd64
This isn't for cryptographic purposes; the usages of `math/rand` are strictly for picking a random server to try, or for padding out the URL: the latter is strictly to ensure the requests are of the same size, and the actual contents don't matter (could have just as easily been the same character repeated, to the same effect). But, if you see a specific usage in there where I'm wrong, definitely let me know!
Hmm, anyway maybe it would be better to at least initialize the random source with current time or anything that changes with each launch?
Wouldn't hurt; I hadn't bothered because the randomness being deterministic wasn't harmful (since the same source is used for multiple features, and those features are not deterministic, like padding any random URL to length, the incoming input of which varies by behaviors on the user's network) seeding wouldn't have much effect on the operation of the service. I'm trying to think of a scenario where this _could_ be harmful, but it's true that since it won't have a particularly meaningful effect, seeding it could avoid the problem before its found.
I see no mutex in pila.go nor database.go. Where is your thread safety occuring? 
Not trying to attack your project, but what would the use case for a C program converted to Go be? Wouldn't a wrapper around C code be faster?
There was a free version of the book [Network Programming in Go](https://www.apress.com/gp/book/9781484226919), but the free version has been deleted and only the new paid one is available.
This one explains it well (has animations!): https://appliedgo.net/networking/
Why would that be better?
Because if you want random sequences - even for padding - it is better to have non repeatable sequences. Assuming that app is a white box, attacker could theoretically use the repeating sequence to get the encryption key if they would manage to capture enough traffic. Theoretically that is - because currently there are no publicly known attacks on TLS, but again - your ISP/intelligence agency can store traffic for many years. You don't seed the random source in one case only - when you need to test the output (to test your random generator for example). There are no reasons not to seed the source - it doesn't introduce any performance penalties, it's done one time only and it "just works". More importantly you can even use crypto rand in that case - I seriously doubt that you'll manage to drain or even sufficiently load the system cryptographic random generator for DNS queries. The gold rule is - when in doubt - use cryptographic random source. 
DNSSEC is a protocol for validating the origin of DNS records, not for encrypting them so that they cannot be inspected on the wire. DNS is a non-encrypted protocol, so just like HTTP it's open for spying, modification by a third party, etc. DNSSEC deals with the second issue, modification by a third party, but not the first: that anyone with access to your network traffic can dissect these requests and see which sites you are visiting. There is a project, [DNSCrypt](https://en.wikipedia.org/wiki/DNSCrypt) which attempts to address this issue as well, although for [reasons I outline here](https://www.reddit.com/r/golang/comments/65jyzs/a_dnsprotocol_proxy_for_googles_dnsoverhttps/dgayi77/) I found those services difficult to trust as a consumer. So, when your requests are being funneled over HTTPS, your ISP, someone on your local network, etc, will not be able to inspect your DNS requests. Hope that answers the question! Edit: I should mention, this doesn't fix the first problem (validation of the origin), however I saw the inspection of that traffic as the problem I was most interested in solving immediately.
It's a type assertion not a conversion. https://golang.org/ref/spec#Type_assertions
How do you plan to do that with type safety?
Most of my tools use x/crypto/ssh... What changes do you foresee I'll have to be making so that the tools continue to work after re-"getting" and compiling?
I chose not to try and handle the newline situation at all. `crypto/ssh/terminal` seemed not to work well to me (using Konsole, anyway), and didn't seem like it was worth trying to fix. Unfortunately, my `Prompt` type also won't behave well if the user types far enough to cause the terminal to wrap, because it doesn't try to detect that, which means (in Konsole, again) you can't backspace to the prior line. Given the variance from one terminal to the next, the only way to see how it behaves for you is to grab the source, run `make` to build all the examples, and test out `bin/actually_simple`, which is sort of the simplest use-case, generated based on the package example. I did make it so anything based on the `Reader` type (`Prompt` and `AbsPrompt`, for instance) can have a maximum rune limit, after which they just don't take any line-expanding input. This prevents going to a new line, but obviously that takes away the ability for long text entry. There's also the possibility of just implementing a custom type. The majority of the logic happens in `Reader`, so getting a specific terminal to behave nicely could be relatively easy by wrapping a `Reader` to build your custom output code. Though obviously far from perfect, `Prompt` is only 126 lines. One of the big reasons I built this library was to make it easier to do things like this by splitting up logic into different packages. And of course if you can improve `Prompt` to work better with your use-case, or you just have an idea that could help, file a PR or ticket. I'm certainly not opposed to improvements.
There is zero chance this gets accepted
Odd, go1.8.1 appears to ignore `$CC`, at least in OpenSuSE amd64. For glibc and musl, I'm having better luck using `go build --ldflags '-linkmode external'`. Though I'm running the glibc and musl builds from Ubuntu and Alpine respectively, as their `golang` and `go` packages are the easiest way to get the right toolchains.
Keeping my feeling about mixing FP and Go aside - I don't like the second version, because it hinders my ability to annotate errors from 3rd party libraries inside those "continuations". I know that we don't give errors as much attention as they deserve, but I believe that "propagate errors without annotating them" is fundamentaly wrong. io.EOF doesnt mean much if it came from 5 levels of functions calls below. 
This issue is specifically addressed in the proposal through the use of a `MapErr()` combinator: result.MapErr(func(err) { myerror.Wrap(err) })
This seems like a specific sum type with several associated specific functions (which apparently are not in any standard library but are built-in) that notes there is a proposal for sum types and disregards that afterward. I don't see why we would use this over or in combination with sum types, instead of just using sum types. 
Some Read(b []byte) (int, error) methods can return number of read bytes and io.EOF at the same time, so there is that too. Also os.Stat can return an error which is not technically an error.
I don't disagree ;)
Yes. The first stage is to run it through the C preprocessor. The lowest level functions need to be implemented for each platform, for example: https://github.com/elliotchance/c2go/blob/master/function_definition.go#L54-L64 Otherwise all the header files will be parsed as if they were part of the application.
I use Go at work. That is, unfortunately, not reflected on GitHub. That said, I do write a lot of multi-language projects. Here's one: https://github.com/zcred/zser/tree/master/go
The goal is not speed. I can't imagine many applications that if written (sensibly) in C would be any faster in Go, or any other language for that matter. I see its purpose as a migration tool for translating existing tools written in C to make them safer and easier to manage in the future. Many of these tool (even the simple ones) would be too time consuming to convert by hand.
&gt; Go! What is it Good For? Go! is not Go ;)
Have you talked to /u/miekg (author of CoreDNS) about this? Wonder if it'd be a compatible task.
I ran into this as I updated my vendor lib this week. Was a bit of an eye opener to see I was not checking host keys at all. I assumed the lib would do it for me, as does **every** other ssh tool I have used in the last 10 years. Furthermore, there wasn't even an option for "accept all keys from my known_hosts please." I have to manually parse a known hosts file and feed it in. Turned out to be quite a pain, even with the knownhosts package.
I'd worry about speed pretty quick though. You'd want to cache in your local network for sure.
I haven't, no. This is built on his low-level [dns](https://github.com/miekg/dns) library, however. It's probably not a good fit for the run-your-own-dns-over-https goal, but possibly secureoperator itself could be contributed in some way if it made sense for the project. I haven't really thought much about how it might contribute into a bigger ecosystem (if it's compatible with the goals of those ecosystems); figured I'd start with something standalone first, then see where the demand was, if any. Good thought though!
Haskell's `Either` is a GADT, your proposal isn't so that comparison also fails...why propose one tenth of generics?
This _seems_ over the top, but in-fact this was recently suggested in [this recent journal article on DNS Privacy](http://ipj.dreamhosters.com/wp-content/uploads/issues/2017/ipj20-1.pdf). I do think it is an effective solution for enabling DNS privacy, but still needs more HTTPS-backed DNS services.
Do not want!
&gt; gofmt also allows: &gt; go result := doThing(a). Then(func(resp) { doAnotherThing(b, resp.foo()) }). Then(func(resp) { FinishUp(c, resp.bar()) }) That's what I am saying. gofmt allows for the unreadable one liner but if you try to remove the newlines from the readable version it does not allow that. So what is your point? 
&gt; The simplicity of Go means that if err != nil {return nil, err} is understandable by pretty much everyone. Adding something like this proposal would swap out "boilerplate" that is plain and readable, for something that every new junior developer would have to learn on top. Yes. You are almost paraphrasing exactly what I said in the proposal: "I'll admit this approach comes with a bit of a learning curve, and as such can negatively impact the clarity of programs for people who are unfamiliar with combinator idioms. Though personally I love combinators for error handling, I can definitely see how culturally they may be a bad fit for Go."
Awesome work. I am a DNS engineer with a focus on security so I will dig into this.
Bcrypt is much safer than sha256 because it's slower. sha256 only hashes a string once. Bcrypt has multiple hashing rounds and with password hashing slower is better EDIT For anyone wanting to use bcrypt, the algorithm requires a c-style null terminated string which can be troublesome if your password input happens to contain a null byte. I would recommend using scrypt for any new projects. You can use the x/crypto implementation https://godoc.org/golang.org/x/crypto/scrypt Hopefully the golang maintainers will add even more modern password hashing algorithms such as [Argon2](https://en.wikipedia.org/wiki/Argon2) which was the winner of the password hashing competition.
See here: https://github.com/fern4lvarez/piladb/blob/master/pkg/stack/stack.go#L15 `pkg/stack/stack.go` provides the internal Stack implementation that `pila/stack.go` is based on: https://github.com/fern4lvarez/piladb/blob/master/pila/stack.go#L49
Cool, thank you! That look‚Äôs like a bunch of good reading stuff :)
See also https://github.com/pforemski/dingo
Why not [DNSCrypt](https://en.wikipedia.org/wiki/DNSCrypt)?
I am waiting for your next proposal "Rewrite Go in Rust".
https://godoc.org/golang.org/x/crypto/ssh/knownhosts provides a parser and `HostKeyCallback` implementation. Just not file-location logic since that varies by platform I think. I ended up including a known_hosts file in the repo, since the app in question targeted a small number of appliances. You could also look in $HOME/.ssh or at a location pointed to by an env var or something else.
What a ridiculous, ignorant statement.
All your calls from the database file are not thread safe then. This library just feels like it has a ton of race conditions
Terminology.
ah cool, i'll give it a look
See the third paragraph of my [response here](https://www.reddit.com/r/golang/comments/65jyzs/a_dnsprotocol_proxy_for_googles_dnsoverhttps/dgayi77/). The tl;dr is that I trust the reliability of Google and them as a source more than the random DNSCrypt service runners. Also, I like the idea of putting this over an existing well-known‚Äîand easy to inspect as an implementor‚Äîprotocol, although that honestly plays into it less. In the end, it was a means to an end: I wanted encrypted DNS for my home from a provider I could be ok with, and it was easy enough for me to implement!
Not bad. Couple things I noticed: When you receive an error in a SQL transaction you're logging it but not returning an appropriate HTTP status code so you'll always get 200 even when the operation fails You have your routes set up more like an RPC server rather than a typical REST API. It all depends on what you're trying to do. It's more typical to have routes like r.Get("/user/:id", GetUser(db)) r.Post("/user", CreateUser(db)) r.Post("/user/:id", ModifyUser(db)) r.Delete("/user/:id", DeleteUser(db)) So you're using the routes and HTTP methods to specify your resources rather than sending RPC commands. When the app gets bigger you'll want to put the handlers in a separate package and you'll likely want to set them up to receive a db connection and return a handler (a closure). This will let you test your handlers and mock the DB connection. 
This seems a little similar to githuib.com/kelseyhightower/envconfig which in the past I have used in conjunction with github.com/joho/godotenv to load .env files and then parse them into a struct using envconfig straight after. I once asked if it makes sense to combine these two libraries, but the author of envconfig wasn't really interested in increasing the scope of the project that way. It seems you have chosen to make your own config file format in YAML rather than go with a standard .env file, if I am reading this right.
No. At least, no real plans.
It does sort of kill me that basically any language changes are on hold until Go 2. But there are no plans for Go 2.
I ended up wrapping their api with something easier to work with. For the most part I have all the same domain concepts, but simpler for my sanity. For instance: `package myec2 type Volume struct{ *ec2.Volume } func (v *Volume) Attach(svc myaws.EC2, *i Instance, device string) error { // AWS ask go craziness here... return nil }` This pattern saves me quite a few headaches. 
&gt; Guys documentation. Quality post.
Not ideal perhaps, but Go is an open source project. There is nothing stopping anyone from forking Go and adding all the goodies you want it to have.
What is missing? I see the developers are still very hard at work improving stability, performance, and tooling. What would Go 2 solve for you?
[removed]
Java is 22 years old, and still at 1.x
I use the stdlib almost exclusively with my projects. One if the great things about go is that you can do so much with only using stdlib. When you need something extra, there is USUALLY a library for it.
You could write a basic web application or websites from scratch. If you do not feel making progress, it is normal. We struggle because from your experience is different from Go syntax. It is always an alien for most programmers learning new languages. Learning how to code for a website maybe a bad idea, you should consider benchmarking different languages to get the idea how fast does it perform with as low as C before you adopt it. Hacking enough knowledge with Go before you adopt a web framework. 
* Go path is super annoying. I have mix code projects and go path is pretty terrible * Lots of typing improvements. I would elaborate, but people would just downvote. =) * go get is lacking
The no breaking changes thing is super annoying. It means your handicapped by the teams knowledge when Go 1.0 was released. If the Go team had a different release process they could let their users opt into changes they want, but they'd have to invest in better package management and versioning story, which Google doesn't' seem interested in.
Gawd yes. Go path is just awful for my mixed language projects.
There are a hundred things I'd like to see Go fix, about half of those are realistic and could plausibly be called compatible.
Nice work. Here's one helpful suggestions to make *reqlimit* even more attractive: write a benchmark that measures overhead, as compared to a naked handler, and memory usage; then display the results in the repository's README. Also, architecturally, there's no reason to publicly export the `Limiter` structure, it should be opaque... you don't want the end-users mucking around with the encapsulated data and invalidating the object's invariants. There are two ways to address this: (1) return a package-private structure that implements the public `http.Handler` interface (easy enough), or (2) return a publicly exported interface that encapsulates the package-private structure. What's the difference? Well, if you believe your end-users will never have to use your rate limiter by name, then it'd make sense to return a package-private structure, e.g., `reqlimit.limiter`. This prevents users from declaring variables with that type (as they can do now) in the case of `var lim reqlimit.Limiter`. In your case, I see no reason why anyone would do this. I'd opt for a package-private structure.
- Go path is part of the compiler tools, not Go itself. For example Gb is another tool to build without Go path. - Go get is just a Go tool, so switching from Go 1 to 2 won't magically make it better. That also means it can be improved in Go 1.
Have you tried out [Gb](https://getgb.io/)?
I agree with you about the complexity and poor document of their offerings. Even something as simple as their Getting Started pages is a total turnoff. I think the Go ecosystem has it worse because we don't have as many devs using and documenting it. 
Another recommendation is to return a 429 (rate-limit) instead of 403 (forbidden).
You can absolutely just use the standard library and I recommend it for most cases, especially when you're just learning Go. I recommend the basic walkthrough on the official site, it'll get you started with the basic concepts using just the standard library. Then pick a problem you want to solve in go and take it from there - with few vanilla go tutorials to go on, you'll have to forge your own by just building a project. Even if it doesn't turn out well, it'll give you direction in your learning.
It might be annoying but it's definitely not without it's reasons.
Sure, lots of mistakes have good explanations. 
This hasn't been stated yet. However, you can see some of the change proposals for Go 2 [here](https://github.com/golang/go/issues?utf8=%E2%9C%93&amp;q=is%3Aissue%20label%3AGo2%20).
Go dep is on the roadmap to being [merged into the toolchain](https://github.com/golang/dep/wiki/Roadmap). What did RSC say? The typical response I've seen is basically "we're not fundamentally opposed to this, but we're not going to add it unless someone shows a good proof of concept", which is also how they've dealt with generics.
Go explicitly does not support symlinks in GOPATH. Sometimes it works, sometimes it doesn't, depending on the tool.
Not sure if you have already completed it but the Go tour is a really great place to start learning the language: https://tour.golang.org/welcome/1 I realize its not about web apps but it gives you exposure to a lot of go libraries you might need regardless of what you are doing.
That's the dep roadmap and not the go roadmap. I guess the summary can be found in this thread https://groups.google.com/forum/#!searchin/golang-nuts/go$20dep%7Csort:relevance/golang-nuts/PaGu2s9knao/9ZgwktG-BwAJ But they key quote for me is "You make it sound like dep is just going to become 'go dep' in Go 1.10. That's not the plan I thought we discussed." -- rsc
Versions number comparisons across projects are pointless, but thanks for playing. There have been a handful of breaking changes to the Java language still under the 1.x versioning scheme.
Things labeled "ponies" are also worth looking at too. Edit: I guess the downvotes are from people not that familiar with the Go project, but here's a link https://github.com/golang/go/issues?utf8=%E2%9C%93&amp;q=ponies
Ah interesting, I wonder what the plan was.
The answer to this is irrelevant. Soft linking shouldn't be necessary.
Couple colleagues of mine had a play. Really cool way the bindings have been implemented for Go. It has a nice mixture of the C++ and Python characteristics. So cool how the slots and virtuals can be done with just simple function callbacks. I even found something missing, submitted my ideas, and the maintainer cranked out support (custom constructors) 
Until "shouldn't" becomes "isn't", wish in one hand. The point was that the issue is easily circumvented and not worthy of lamenting as insurmountable. It's a rather minor annoyance at most.
You're holding it wrong. It can work very well in multi-language projects if you try to embrace it. For example, set GOPATH so that $GOPATH/src is whereever you keep your code. For most people, that's probably GOPATH=$HOME. Put your project root at ~/src/myproject.example with subdirectories for its components A (written in ALGOL68), B (written in Bash) and G (written in Go). They key is to *not* recreate a whole new GOPATH within your project directory. That's just going to lead to pain.
Amazing work by therecipe. However GPLed softwares can be tricky to incorporate with MIT/BSD licensed projects. Commercial QT, as amazing as it is, is expensive to my taste for.
I've been looking for something like this.
 /u/the_recipe
With "many" you mean x86 and ARM? Right! Debian targets 22 platforms (x86, ARM, SPARC, MIPS, POWER, s/390 and so on). That is "many". Not just two.
What exactly is your point? Mine is that breaking change is expensive, and should be avoided where possible. Its a pretty simple concept. Do you not understand it? Or do you disagree with it? 
You can claim to know best and never let your users get improvements to the language or you could give your users the power to decide when to opt into breaking language changes (with a better versioning story). I prefer the latter. 
It uses far less memory and is much faster.
Hi, I like that this problem space interests you. This is a perfectly fine POC, but there are some problems that would prevent it from being practical in serious use, in fact using it would be disastrous.. let me explain why. First you add a single point of contention for all requests with a Mutex for map access, Go happens to have a fantastic Mutex implementation from a security perspective because it strives for a certain amount of fairness. However contention still makes it cheaper for an attacker to affect your SLAs, in your case the next points are what make this lock a viable target for the very thing it attempts to defend against. So the next part I find really interesting and slightly confusing! I think you may have seen this had you written up your design on piece of paper or whiteboard, it will be obvious when I just put the implementation in bullets I think. * New request comes in, obtain global lock * Initialize a new zero-value of []time.Time * If the IP exists in map of request history * A) Iterate every entry of history * B) if entry has not exired, append this entry to our previous zero-valued slice * C) Delete the entire old slice from our history map * Add the new one we just made..? Yea- lol :- ) * Now we get len() of history, if it exceeds limit, we return an error. * Never delete an old IP address, leaking memory in the map as long as the service exists, unless the visitor is nice enough to come back the next day so I can trigger my gnarly O(n) loop to free it :) Okay- so I'm sure you see the issues now, but I'll point them out for those who may not. The first mistake here is granting your attacker a way to allocate memory at will, perpetually, forever. You see you do not drop your history .. once the limit has been reached. You allow it to continue to accumulate and O(n) every single request, each time causing n allocations, only rolling off those which have met the expiration. The fact the attacker could have a large pool of IP addresses means their rate of convergence to your systems memory exhaustion is at least superlinear. We are talking a few million allocations a second as your RPS screeches to about 2k as it chokes in this handler with just 100 ip addresses type situation, lol. So focusing on your algorithm, you can bandaid it by checking the idx of history as you iterate over it, if idx exceeds limit, stop allocating and return the rate limit error, this is absolutely critical because it limits the resource allocation an attacker can consume to a constant you may measure. Next to mitigate the cost of sustained attacks you should reuse the same slice backing, i.e.: updateHistory = history[:0] for _, requestTime := range history { if requestTime.Add(l.limitTimeout).After(time.Now()) { updatedHistory = append(updatedHistory, requestTime) } } Even with those fixes you wouldn't want to use it, it's just way too inefficient. A better approach would be a wrapper around golang.org/x/time/rate, it's not perfect for this- but it's much safer. A correct approach would be a concurrent bloom filter with a cheap atomic counter. All and all I still applaud the efforts and hope I helped ya learn something new. Have a good one!
Very cool! I tried making my own version of this after taking a quick peak at the usage page and I ended up using int64 unix time, but your implementation is much better :)
I wrote an easy to use DynamoDB client library, check it out: https://github.com/guregu/dynamo The AWS libraries have terrible APIs but from what I understand they are auto-generated and there are various Reasons why e.g. a required string is a pointer. So our only option is to write wrapper libraries with sane APIs. 
It's been a long time since I used java (ie before the language even had generics). What do you mean by "greatly improve its functional programming idioms"?
Started with go a few weeks ago, but just with the first http handler I need to search for a router, isn't it? Things like url parameters or declarative GET / POST handling is not included, or did I miss something?
Well, greatly may be stretching it a bit, lol, but java 1.7, and 1.8 have added functional interfaces that make the dog-ugly anonymous classes you may have used in the past much simpler to use and write, and a streams API that's quite expressive.
I think there have been signs that Go 2 will happen in the next 5 years. rsc's blog about his new years resolutions, various core team members self-filing proposals for Go2, (ie Rob Pike's proposal for variable size ints.) I don't think they're in any rush, but 5 years? I'd take that bet.
lol, word.
&gt; Or you can deliver changes and improvements in a responsible way that does not force people to rewrite their code to continue to receive patches and updates. You can provide tooling to help identify problematic areas and still maintain previous versions for a sane period of time. Things like making error() an interface, etc could drastically improve the usability of the language for small changes to compatibility. Of course Google isn't interested in this because they don't really care about the usability of the language over maintainability of their existing code, but a lot of these warts turn people off and they just end up using Java or C#.
Yes
The longer you wait, the more you increase your change of a python 2 -&gt; 3 fiasco. You need a periodic cadence of breaking changes (once a year or once every two years) and compelling reasons to upgrade (features). For example, combing the awesome GC improvements of the past few releases with a few small breaking changes would not impact adoption at all.
Google it. Start with "golang sum types", "golang decimal type", etc.
I've actually made a tool that creates postgresql extensions from your golang package. http://github.com/microo8/plgo It generates all the boilerplate code and than compiles to an shared object and also creates postgresql extension files, to conveniently install your new extension
What I want is something closer to node Dependencies in a vendor folder and the folder itself can be anywhere on the system. Go get in the folder would update the vendor directory. I dislike gopath a lot. It's awkward to cd, I don't even know how the syntax works if you are like a company with go and not on GitHub where does your code go? src/git.company/company/myrepo ? It's unclear, I just prefer the approach where you aren't reliant on the rest of the system in terms of dependencies.
what? typo?
I think it's a joke, goto==go2
all "defer stmt.Close()" should be placed before stmt.Exec. On the other hand, the statements should be global variables to get better performance.
Not concretely planned as far as I know. If/when it does happen, I'll bet ya $20 that they'll also release a Go 1-&gt;2 transpiler that'll automatically update 99.9% of the code out there without problems, and everyone will be amazed by the smoothness of the upgrade.
There is no need for a framework. I do use some of the gorilla tools (http://www.gorillatoolkit.org/), but only where I really need it (e.g. Mux). I minimize external dependencies to avoid making my life a nightmare by keeping all packages up to date and working well together. Sticking with stdlib is the most popular policy. But it takes time to learn how to do it properly. It is not easy to learn from the Go documentation. Go by example is good to have a dip in Go but not to learn all possibilities and master the language and its libraries. There is a gap there that still needs to be filled. 
note_bro gets it
It's not unclear at all, Go get specifies the headers needed to make any page into a go gettable resource. It even supports SVN and Hg. And that URL is pretty much what I do with my work repos. I personally like that I can identify the source of any import just based on the import path. And with tools like GoImports, it gets even better where on save my IDE will automatically add or remove imports.
That's not wrong. The Go team introduced breaking changes to the language in the past. The evolution was apparently painless due to the tool for automatic conversion. 
Ok, so what's the structure for my example? We are using bitbucket at work so the structure would be (.company is a TLD only available in our network) src/bitbucket.company/project/repo ?
Yeah, mine specifically is: src/bits.company.com/team/repo
Yes I know. Cd'ing into the thing isn't my only issue it's just an example. Another one would be that my IDE (goglang) display the full path from the ~ home dir to my repo and with go that will always be very long, making it look bad in the IDE. Another issue is when you want to open it in a GUI like sublime you will go through all the folders... It's not something that I should need to fix with a symbolic link or environment variable. It's not something that NEEDS to be fixed, I just feel I would prefer go without GOPATH, but that probably won't happen since it's a big part of Go.
And backward-compatibility meant that Java generics were implemented with type erasure, restricting the power of the feature.
Actually, reactive programming have been around for decades as well: first concepts date back to 1997 (http://conal.net/papers/icfp97/). Microsoft have coined it "Rx" (Reactive eXtension), adding LINQ and concurrency. In summary: * Yes, reactive programming have received some hyped in the JS community because of its use in Angular 2. * No, it is not new. * No, it is not just the Observable/Observable pattern. * No, it is not required to build things ; you could achieve the same result by using callbacks or promises. Most Rx libraries provide facilities to avoid a lot of boilerplate code, take care of concurrency and scheduling and a lot of operators to handle stream synchronization where it is needed. It allows to declaratively create networks of asynchronous data streams. It is particularly suited for event-driven systems, where it complements promises. 
&gt; It's awkward to cd Have you looked at autojump (https://github.com/wting/autojump)? 
Did someone get their equity paid out or something?
I've just used https://gobyexample.com
&gt; Things like making error() an interface `error` already is an interface type https://golang.org/pkg/builtin/#error perhaps you mean something else? (I'm confused, at least)
I don't know if he was referring to generics. From another comment, I think he meant sum types and decimal types. I haven't taken the time to understand all the implications of sum types, and I haven't used Go for applications that do enough math for me to miss my decimal types from C#, so I don't pine for those things, but I'm sure I'll look into it eventually. As for generics, I'm comfortable with Go's lack of them, and I enjoy the simplicity, but we certainly need to keep our mind open for a future where perhaps an elegant approach emerges to provide that feature.
I came from a C# and Python background, and between Tour of Go and Go By Example, I immediately felt like I could do almost anything. It was my shortest learning curve ever, so I think you'll find it to be quite stress free to just start writing small utilities after looking at the basics.
You have kind of a shitty attitude.
I'm sorry you feel that way.
The future relationship between dep and go get is not clear. I discussed the possibilities a bit over here: https://www.reddit.com/r/golang/comments/653mxc/dep_status_week_of_april_10/dg7dqh3/
Julian Schmidt's http router or Gorilla Mux is solid for URL params
Some background: At work, we have a repository with multiple projects. We looked at solutions for running various CI steps on multiple directories, but nothing we found was quite light-weight enough for our tastes, so I thought I'd give it a shot. The main project we looked at was [pants](https://github.com/pantsbuild/pants).
I'm enjoying it :D I'm just glad the effort seems to be taking root - I'm hardly the first community member to throw gobs of time at this problem. &gt; rsc's later response seemed orthogonal to his original response to me. I think it was intended to make clear that dep is fundamentally the path to the future, even though we still have substantial issues to figure out. &gt; How confident are you that Go 1.10 (Feb 2018) will include some form of dep? I can't really assert that with a lot of confidence, unfortunately; the best I can say is that we have a roadmap where we're _trying_ for that, based on what we know right now. I'd say the main unknowns are: 1. Even if it were up to me to decide what "good enough" means for a toolchain release, I don't know the toolchain internals well enough to be able to even ballpark how much work is involved. 2. Conversely, rsc et. all probably don't yet know the internals of dep/gps well enough to be able to guesstimate the amount of work needed before it'd be toolchain-ready. 3. Even for the work we do know we'd likely want to do, it's not clear how much would _have_ to ship with the first release of a new toolchain in order to avoid other failures or anticipatable backwards-incompatibility problems later on. 4. Plus, of course, all the normal, reasonable concerns one would have about any complex software as to whether it is sound, and operates well at scale. The roadmap's outlined goal of inclusion in 1.10 is based on an early-pass guess of how much work needs to be done that seemed...let's say, "not crazy." The best we can do right now is try to actively mitigate these unknowns as we go - build `dep` with an eye to what we understand to be the eventual toolchain requirements, not just its immediate needs as a standalone tool. But, to err on the side of safety, we are building dep with the idea that it'll need to exist and work well as a standalone tool for the next year or so. So, the clearest promise I can give you is that, as soon as we think there's good reason to believe we'll miss 1.10, we'll update the roadmap and explain our thinking.
I use https://godoc.org/github.com/bouk/httprouter because it's basically httprouter, but with standard Context objects to pass parameters so it only needs the standard method signatures.
I think it's autogenerated based on an API spec, and that some of the other SDKs are too. Makes sense to keep thousands of endpoints up to date across a dozen SDKs.
Lack of routing on method caused me to pull in: github.com/julienschmidt/httprouter It's ultra-fast, has 100% test coverage, and is stable. Great package.
Please consider https://github.com/codemodus/parth. It would allow you to drop the allocations and delay involved with handling path parameters. Another way to think of it is that we are already passing the path parameters, so there's not often a need to lean on special handler functions or to setup and pass a context type. benchmark iter time/iter bytes alloc allocs --------- ---- --------- ----------- ------ BenchmarkVsCtxParthString2x-8 20000000 83.80 ns/op 0 B/op 0 allocs/op BenchmarkVsCtxParthString3x-8 10000000 125.00 ns/op 0 B/op 0 allocs/op BenchmarkVsCtxContextGetSetGet-8 1000000 1629.00 ns/op 336 B/op 5 allocs/op BenchmarkVsCtxContextGetSetGetGet-8 1000000 2044.00 ns/op 352 B/op 6 allocs/op
Cool. So can I have simple thing like List/Vector of primitive types?
Well of course I'm bloody wrong, that's why I love having a compiler that slaps me aside the face every time! :)
Right, but I just haven't found it to be much of an issue, because in Go you don't have the phenomenon of null being used to indicate an error; you have a separate error channel which you have to explicitly ignore. I totally understand why Java needs `Optional` result types. Go, I don't really see the point.
there isn't much to look at. I guess I could make a better landing page though.
Literally nobody, including the Go core team, is happy about GOPATH. They just prioritize maintainability over changing it.
As someone who has never used Qt, do you have any recommendations on how to get started as most of the docs refer to c++ 
I don't use PyCharm for Go. In VS Code I debug with Delve via the plugin available in extensions, giving me step debugging, step in / out, variable lists, and watches. Since Delve is a free-to-use project, I expect it's the choice for Gogland. Since Gogland is in preview stages, I don't think the plugin will get much development work until the new IDE is released. Two of the contributors to the plugin were JetBrains employees, I believe, so I wouldn't be surprised if they return to improve the plugin after further work on Gogland. If you are fine with VS Code, I do recommend it with Go, and that's coming from being a PyCharm Pro user as well.
Looks good! You might want to take a look at github.com/dave/jennifer for the code generation... Looks like it could simplify things a bit...
&gt; What's the idiomatic way in 2017 to write REST APIs in Go? Standard library - net/http Use slicing to get the keys and then you need zero dependencies.
&gt; Literally nobody, including the Go core team, is happy about GOPATH. Got any proof?
Using something like the aforementioned httprouter will reduce the amount of code you write and you know it works and is reliable. I've done the slicing thing before, about 4 years ago, and wouldn't go back now.
I'm quite happy with it. (But I understand that it's a path variable containing a prioritized list of directories, not a single filesystem path, though. If I didn't know that I'd probably hate it.)
HUGE +1 to using TOML instead of JSON for the manifest and lockfile. I've never liked using JSON as a configuration language; the indentation, trailing commas, and lack of comments all make writing it by hand annoying for this kind of use case.
I just had a cursory look at net/http. Will I not be able to get all the parameters using ParseForm and Request.Form? Also, the new Context thing seems very useful for JWT-authenticated routes. Right now I'm leaning towards net/http.
The cherry on the pie is that if dep finally becomes part of the official tooling then we will (probably) also get an official TOML package in the standard library.
Am I reading this correctly, we're going to be manually editing the lockfile? That seems counter-intuitive to me..
&gt; Countless? Give me a dozen then as a proof. No. &gt; From the recent survey results it is more than clear that GOPATH is not an issue. What? I think you need to re-read the survey you posted because it doesn't say what you think it says. The questions involving GOPATH are "What changes would improve Go most?", "What is the biggest challenge you personally face using Go today?", &amp; "What one addition would make the biggest improvement to Go editing in your preferred editor?". It's not surprising that GOPATH isn't in the top. I don't think anybody is claiming that it's the most important thing that needs to be fixed... apparently the Go community who takes surveys think that's generics and dependency management. &gt; You are part of a vocal minority who make it sound like it is a big deal while it isn't. I'm not sure what you're trying to say. The original question was, what would you like to see in Go 2. I would like to see GOPATH fixed because it's less than ideal. I think this is the first time I've talked about GOPATH on reddit. In the above thread I linked to about removing GOPATH, I'm not in that discussion on github even once! I'm certainly in the vocal minority who want Go to be a better overall ecosystem allowing for a few more breaking changes than historically allowed. 
It's a nice to have one day list that predates the Go 2 tag.
Countless indeed.
&gt; Am I reading this correctly Nope: `Gopkg.toml` is the manifest, not the lock file.
It can be easily an internal package. The Go maintainers would rather remove from than add packages to the standard library if it weren't for the compatibility promise.
&gt; Go path is part of the compiler tools, not Go itself. For example Gb is another tool to build without Go path. GOPATH may be technically just part of the compiling tools but years of brainwashing from the Go team has made it an integral part of the whole ecosystem. Switching to GB (for example) also means giving up the entire set of additional tools that the ecosystem provides. So no, it's not JUST part of the compiling tools.
Thanks for the considered and detailed comments. Any feedback on the algorithms I have so far would be greatly appreciated. I am planning to write a basic sparse matrix implementation to support feature hashing. Feature hashing on an english language corpus would probably need around 250,000 - 1,000,000 rows with columns for every document in the corpus - starting around 3,000 but growing over time. A sparse matrix implementation is the only way to make this feasible. I was thinking to start with a basic Dictionary Of Keys (DOK) matrix and possibly then add a Compressed Sparse Column (CSC)/Compressed Column Storage (CCS) and/or Compressed Sparse Row (CSR). Looking through the mat64 library it looks like it should work with 3rd party matrix types providing they implement the Matrix interface although this will probably be quite slow - not taking advantage of the low number of non-zero values. Any thoughts or advice would be greatly appreciated.
This is probably the most civil thread I've seen in awhile. 
Why would you want to distribute them all together? Why not offer a download per platform?
 case "$(uname -m)" in 'armv6l') executable='prog.arm6' ;; 'x86_64') executable='prog.amd64' ;; * ) echo "unknown arch ($(uname -m))"; exit; ;; esac But I think that to distribute all-in-one archive it is not the right way.
I read through the docs issue. I'm a bit confused why something like Hugo is being discussed around documentation. Why not Godocs? Isn't that the standard way of documenting packages in Go? Maybe there's something I'm missing. 
Bit of an off-topic hijack, but: assuming you're not tasked with creating a YAML library, why would you prefer writing and using TOML files?
I really like this! I'm going to making a little market api using this structure, I'll respond when it's done!
As an end user I would definitely prefer if I just have to download my own copy for my own platform. Why should I download a ".exe" file when I'm on a Mac?
It depends on what you want to do. Take a look at the examples then customize. The C++ docs are pretty transferable to Go. Just remove all the boilerplate and cryptic syntax and you will have it :)
Has there been any traction on the issue of vendoring in libraries? For example, Docker depends on a third-party library which it also exposes as part of its API (one of its functions takes a parameter of a type defined in the third-party lib), but it vendors the library so I can't import it from its vendor dir, and Go doesn't recognize the version in my vendor directory as the same (I'm sure this is by design). This means I have to add a build step to strip out Docker's vendor directory before I can use it as a library. I understand that Docker is not supposed to vendor because it's a library, but it's also silly to expect the top level package to know what versions are supported for all of its transitive dependencies. I read through some threads and it seemed unresolved as to whether or not this should be a supported use case. Glide's solution to the problem is to automatically strip the vendor dir. I was wondering where dep stands on the issue and if this has been discussed further since I last checked.
If they're using hugo then why not cobra and viper... Viper has support for JSON, TOML, YAML, and more!
I'm not sure why you think this is unresolved. It's practically the motivating requirement for dep in the first place. Yes, dep will collapse all transitive vendor folders to the root vendor folder, and parse transitive Gopkg manifest/lock files to a single dependency graph :)
It's going to use a go routine to handle each request already so you don't have to do anything explicitly to take advantage of that. I wouldn't start doing anything with go routines in your own handlers or using channels until you get a better feel for the language. Also I'd take a look at the [SQLx library](https://github.com/jmoiron/sqlx). It can save you a lot of boilerplate in your SQL queries. 
Yep. We need a lot more than what's conventionally included in GoDoc for the docs to be up to snuff - examples, diagrams, etc. Consider [the docs I originally wrote for gps](https://github.com/sdboyer/gps/wiki/gps-for-Implementors) - there's probably more detail there than what deps' docs really need to be, but still, it's important that we provide a means for people to understand how the tool works without being forced to perform complex experiments or read the code itself. We don't know what final, post-toolchain form these docs would take, but it might be a good idea to construct them in the same way that, e.g., the [spec](https://golang.org/ref/spec) or [FAQ](https://golang.org/doc/faq) are written, so as to minimize the difficulty of the transition.
Thanks for these videos Francesc, great content. 
Please make it gopkg.toml =) Why the uppercase G?
I hope at least the golang.org / godoc style is kept though. 
Register the loggers to App struct as type is great idea. So, I willing to make a tiny RESTful API library including your ideas and some improvements from now.
Neat! Glad I could help out a bit.
It is a long tradition that "special" project files start with a capital letter; because when sorted asciibetically all of the capital letters show up before all of the lowercase letters, so they show up first in a directory listing. Old examples are Readme or Makefile; but it is common even with more modern tools like CMakeLists.txt, Rakefile, or Gemfile.
At a high level, I would pass a status channel into the goroutine, and send an update from the worker on that channel when you have a meaningful update to report. Listen for those status channel updates and aggregate them as necessary, then display that in the frontend.
What's amazing is the code from Go not only compiles to C++, but it actually looks pretty decent. Bravo!
An alternative to /u/echophant is to have a function that creates a progress channel, spawns the worker goroutine, and returns the progress channel to the caller which can then monitor the channel for status updates.
Hi @Nzzy, the idea you can use the channel you keep the status of goroutine. You can check at https://play.golang.org/p/yHkjO9lFbE . You can use many channels for many goroutines or one channel for many goroutines with data format to recognize the goroutine's identify.
Hah, I've been tinkering with a similar project: [ephemerald](https://github.com/boz/ephemerald) I made it a server so that it could maintain a pool of available instances and also for using it on things like circleci. 
Indeed. I was way too optimistic I guess.
I use the chi router (https://github.com/pressly/chi), very neat.
Go as this killer feature that it doesn't need us to search on the web when developing.
deleted ^^^^^^^^^^^^^^^^0.6591 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/35111)
Go doc is amazing! But I don't exactly see how that relates to this post?
Likewise! Great to hear it's working well in travis and gitlab. I initially targeted a library like conex but threw together server support for various reasons. Glad to know that this route works for you; sounds like my efforts may have been unnecessary (but fun). Cheers.
Its not that, Linux is the problem. Best way would be an installer based around this script. On Linux you have more Archs than Mac...
golang does not support dynamic linking with go libs (at least with go1.8), so you have to pay for commercial use (therecipe is working on dual license). (static linking with opensource is fine now).
Thanks, will take a look!
We agree. `dep` is the mechanism through which we will get versions to be first-class citizens, not only in the toolchain but in the hearts and minds of Go developers. There's no incentive to version libs right now because there's no pervasive dependency management tooling which consumes them.
Did you try out the dynamodbattribute API? I found it much easier for dealing with structs. http://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/dynamodbattribute/
This is a good r√©sum√© of what i find weird in the clean architecture. 
In the end Go will become an additional C/C++ lib. 
Hi there, we developed quite a few medium sized applications. Although we are not yet able to share the code for the services, you may find some good inspiration in our scratch repository, that we use as base for all our applications. https://github.com/dkfbasel/scratch 
Yep, as Peter noted, we definitely agree that versions/releases are needed. My [essay on package management](https://medium.com/@sdboyer/so-you-want-to-write-a-package-manager-4ae9c17d9527) has a discussion on the meanings of different types of versions that you might find relevant.
Docker is a library as well as an application. The main Docker package is built from the Go SDK for the Docker daemon.
Hmm, I tried it a couple of weeks ago with this very example and it didn't seem to work. I think I read through a number of threads in which (I believe) you and Dave were debatibg whether or not this should be supported. Perhaps I'm doing something wrong or my deposit version was out of date? In any case, this is very good news.
This is called *transpiling*.
Packages names should never be camelCased.
I don't know why Gorilla doesn't get more love 
Fair. Most of the time go doc will answer my questions. 
Thanks for sharing! I've used SAX in other languages before, it's nice to see a Go version.
It seems your arguably bad business decision made you come ahead by a factor of 30x over PHP :D If you had 30 servers and wen't down to 3, it would be far from bad. Also, kudos for using netdata, I created the [docker image for it](https://github.com/titpetric/netdata). ;)
What was the difference in written/managed loc? Thanks for the write-up.
Very well written, one of the few in a long time i read completly, not only because i was really intrested in the topic. Kudos to you =)
My company's‚Äã current product was built using Java and a component architecture. Then we acquired another company with a similar product. They were all about docker based micro services, with services written in python. When we merged, we realized that Java and docker based micro services are wasting a lot of resources and python is not really clunky for concurrency so we tried golang. And it was the perfect fit for our architecture. Every dev that switched so far has a lot of fun coding in go. It surely has some areas that are lacking, like dependency management, but it is still young and fun. 
Laravel is nice and easy to use, has huge ecosystem, etc. But it's one of the slowest PHP frameworks.
There are other reason to use Go besides performance. Personally I've never seen a large PHP code base that wasn't a complete mess (although I don't think PHP is meant to be used for large projects). My experience with Go is usually the opposite, Go is a much more "digestible" language because of the adherence to idioms (in most cases). 
"The top 10 tricks disgruntled admins use to plant backdoors!"
Hey, nice work on providing the image for NetData. Checking out your 12 Factors book now! And hah, too bad I'm nowhere near that scale for that particular service.
Thanks a ton for saying that. I'm not much of a blogger and while writing a post like this, it's very easy to think "people know this" or "this is worthless" so I appreciate that you appreciated my attempt!
Good point! I just added that to the post and discovered I somehow used a wrong number yesterday. The difference including dependencies is actually much larger (500%), but excluding external dependencies it's a lot more even (still twice as much though).
Dependency management is good, no? Just build with vendor folder and only one binary for server?
I wish I could move my biggest project from Django to Go. But it's just not feasible to port it when we don't face any performance bottlenecks. Neither do we really face any major problems with the framework. I'd love to write it in Go, but you really can't when you have a 2 man team of which the first person is working on 2 iOS apps and the other (me) on 1 Android app, 1 frontend, and 1 backend. Personally I'm already having a hard time keeping up with my own business and I'm interning and writing a bachelor's paper. You make me jealous.
Did you disable `xdebug` locally when you ran the benchmarks? 120ms is roughly how long it takes my static login page to load locally with it enabled.
https://github.com/DATA-DOG/go-sqlmock may help you out
Is that really the best solution? No offense to the writer, but I was looking for a cleaner solution. Edit: Also I'm not sure how the RETURNING would work with go-sqlmock.
Now I want to read about someone switching from Node.js to Go. 
Didn't one of the creators of nodejs switch to go and make a blog post about it?
Are you trying to test the actual sql statement itself? If so, only an instance of the real database will work. There are things you can do to speed up local databases if you go that route. If you want to just mock the sql layer, pull your sql into its own struct (like `ItemRepository` or `ItemDao`) and now you have a single point of entry for your queries. Your `Item` struct ideally shouldn't contain queries anyway, it's kind of clunky to do it that way.
Yes, you are right. I will update that.
Why would you do this instead of just passing dependencies to the components directly? I mean, from your example: func main() { config() started() } The started method depends on a logrus.Logger, but we have no way of figuring that out by looking at its signature. You've obscured that dependency relationship, making it possible to understand only by reading the code. What happens when a maintainer comes to this code in six months and wants to refactor it? They gain no insight by inspecting the constructors, type declarations, or signatures. They are forced to parse each line of each method in their minds! You're _throwing away_ important cognitive affordances provided to you by the language designers. I just don't get it _at all_. Compare with: func main() { logger := logrus.NewLogger(...) started(logger) } This is so much more clear! You know exactly the scope of the started method just by reading the callsite: it will interact with the logger, that's it. You can refactor with confidence.
This is useful when code a bit deeper than 2 levels.
It definitely reduces typing, but that's not a virtue, as code is read vastly more often than it is written. You gotta optimize for comprehension.
Another way to put this: There are two types of tests being referenced here: unit tests and integration tests. If you want to test that some service does what it should with the data from the database, that's a unit test. It can be done without any external dependencies. If you want to ensure that you can do CRUD operations with your database, that's an integration test. It requires you to actually connect to an instance of a database. Unit tests are definitely the most common type of test. Integration tests can sometimes be more work than they are worth. Sometimes you will need to have a test instance of your database with test data that you must maintain, and it's just not worth it. If you plan on testing code as simple as the example you've given, it doesn't really seem worth it to write integration tests, but that decision is ultimately left for you to make.
Yup, the same exact code is called separately in http handler somewhere else. If it leaks over there, the problem would be very noticeable.
This was a great read! We're L4.2 for our monolithic app, and gradually breaking bits out into Python-based microservices. Would much rather be using Go. Maybe if I can just unilaterally rewrite some bits in Go, we'll end up accidentally adopting it ;)
Try http://glide.sh, they plan to be around as long as dep isn't as good.
We were using a Matlab library, though now I cannot remember which one (I think I found it on the iSAX webpage). One use case was to search for certain patterns within our sensor data. The other was an experiment to see if we could cluster timeseries according to periodic trends they exhibited, but we ended up using a different approach in the end. 
And you're sure that you don't have any strange closures?
lol no generics
&gt; But it's just not feasible to port it when we don't face any performance bottlenecks. Even if it's feasible, don't fix a problem that doesn't exist. 
So you've added one level. Try to add 2-3 more and use 20-30 dependencies.
I'd be happy if ? was just a shortcut (or ! to panic()) e.g., resp, err := doThing(a) if err != nil { return nil, err } resp, err = doAnotherThing(b, resp.foo()) if err != nil { return err } resp, err = FinishUp(c, resp.bar()) if err != nil { return err } becomes resp := doThing(a)? resp = doAnotherThing(b, resp.foo())? resp = FinishUp(c, resp.bar())? Or something like var publicKeyBytes []byte var publicKeyAlgorithm pkix.AlgorithmIdentifier var err error if publicKeyBytes, publicKeyAlgorithm, err = marshalPublicKey(pub); err != nil { return nil, err } becomes publicKeyBytes, publicKeyAlgorithm := marshalPublicKey(pub)? 
Yes, well at least Tick leaves behind garbage. Try NewTicker. From [the docs](https://golang.org/pkg/time/#Tick), Tick is a convenience wrapper for NewTicker providing access to the ticking channel only. While Tick is useful for clients that have no need to shut down the Ticker, be aware that without a way to shut it down the underlying Ticker cannot be recovered by the garbage collector; it "leaks". 
Hiring is a good point -- my company rewrote from PHP to Go and I think it was a questionable decision, but it's undeniable that if you're looking for good developers, Go will pull them out of the woodwork while PHP will tend to scare them away. 
:thumbsup: I was under the impression that Adders continued adding new cells until the op succeeded. ( which would still leave the cells bounded as you can only have so many cas misses on N values ) 
Things can certainly get pretty unwieldy in go as well, but I think it's threshold from digestable to undigestable is certainly a bit higher than most.
[WORST CHART EVER](https://cdn-images-1.medium.com/max/2000/1*TqU64uY35VcO0A-1blAORQ.png)
You are correct the website scrolling was borked. It has been fixed!
So many questions on the php side. Which version of php. Which version of laravel. Lemp or lamp. Were configs optimized. The barrier to entry on the php side is low but to get things to actually work optimally is not easy. Go, on the other hand, has a higher barrier to entry but it has considerably less other pieces of software that need to be involved. They each have their use cases.
Keep in mind that many routers (Gorilla mux, for instance) don't bother with mutex on the router tree. The idea is that you construct the routes, build a tree, and then start serving, No modifications to the tree from that point on. I can see one scenario where a user adds a resource and gives it an arbitrary alias, which might be useful to have in the router tree as a static (no parameters) route as it does not fit any predefined pattern. But that's a special case. So one can get by without a mutex altogether.
My personal philosophy is not to test external services. If you import a library, you wouldn't test that library, you would test your use of it. So, you're not testing the raw SQL does what you expect... That should be tested in the DB itself. What you're testing is a: your code executes that query when you expect it to, and b: your code correctly handles the response of that query, ie, scanning the results into the struct appropriately. Therefor, if you use something like "sqlmock" you get to test both of these things.. You set up an expected query (even just checking for "SELECT *", meaning a select of any kind), then putting in the results and making sure your code scans them in correctly. At no point do you need to make the roundtrip to a DB, because you're not testing that your DB driver works, nor are you testing that Postgres knows how to execute SQL.
Ooh, they added a `knownhosts` sub-package on April 11th, that's a welcome new feature.
I am sure you learned new things when writing your project, there is merits in writing your own tool anyways, and it is always fun :)
If the _tool_ is being changed by the maintainers, then one of five things is likely to happen: (1) it will start using `~/.ssh/known_hosts`; (2) it will use an equivalent app-specific file; (3) it keeps state in a JSON blob already and will just add another field to the state; (4) it will switch to only supporting SSH CA hostkeys; (5) it will continue to ignore hostkeys, but will now do so explicitly. What changes _you_ have to make depend on the tool and the approach it uses. It might be that if you're rebuilding/replacing VMs frequently, you will need to switch to the SSH CA approach for sanity. Alternatively, there may be a new flag to say "yes, accept this change", or you might need to update a cloud IaaS provider's policy configs so that a client key used by the tool has permission to read console output, if it didn't have it before. Many sane tools will probably start using the `~/.ssh/known_hosts` cache: it's understood, managed by other tools, and it works.
When I last tried this, it was mostly math problems.
Thanks for your feedback. I already assumed it wasn't fit for financial purposes because of its inaccuracies. Would you recommend using int64 in cents or does Go have a type that's better for representing currency?
Are you including any C libraries?