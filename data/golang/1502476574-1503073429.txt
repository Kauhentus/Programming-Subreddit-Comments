Why? But sure, if you want/need to.
Yeah, good analogy.
I can give it a shot, but I don't know if any handwaving I do will be helpful. I suppose the first thing I need to do is make a distinction. The words "pointer" and "reference" are often used interchangeably. In a garbage collected language, this is fine, because all pointers can keep memory alive. The garbage collector sees a pointer into memory and says "oh, you're still using that? I'll leave it alone". Rust doesn't have a garbage collector, so it's important to distinguish between a pointer, which can keep memory alive, and a reference, which cannot. The most common kind of pointer in Rust is a box. If memory is pointed to by a box, we know that no other pointers to that memory exist. For this reason, they're also referred to as "unique" pointers, and we say that the box *owns* the memory. This also means you can't have two boxes pointing to the same memory; if you make a copy of a box, you also have to make a copy of the memory. When a box value goes out of scope, the compiler knows no other pointers exist to the memory it points to, so it can free that memory. __Example 1__ Let's say you have a box pointing to some data. You create a reference to that (box) pointer (double indirection!) and pass it to the initializer for an object we'll call A. Now, object A doesn't actually need a reference to a pointer, so it instead stores a reference directly into the data. Now, you're very forgetful, and decide you don't need the pointer anymore, so you replace it with a different pointer value. The original pointer value drops out, and the compiler sees that, since it's a box, there are no other pointers to that memory, so it can free the memory immediately. But now object A has a reference to freed memory! Now anything could happen. Perhaps object A thinks it has a reference to the body of an http response, but the allocator will give the memory to someone who wants to store some passwords. Eventually, the http response goes out, and includes a bunch of passwords. Oops. Rust says that, as long as object A exists, you can't replace the pointer. It considers the pointer to be "borrowed" for as long as any references to it might exist, and you can't modify something you've lent out until you get it back. This also means you can't return object A from the function containing the pointer variable, since the pointer would go out of scope and free the memory at the end of the function, while A would still exist holding a reference to the memory. __Example 2__ Now let's say, after you create object A, you decide to make another reference to the pointer, and pass it to the initializer for object B. Now, B actually *wants* a reference to a pointer, so it stores that. You call a method on B, and in that method, it decides to update the pointer. So it assigns a new value to the pointer through the reference it holds, and the original value of the pointer gets dropped, and the memory it points to is freed. Now object A holds a reference to freed memory again! Anything can happen. Maybe object A thinks it holds a reference to something you want to send to your boss, but the allocator gives the memory to code that wants to store YouTube comments. Your message to your boss goes out, and your boss is hurt and confused. Oops. Rust says that, if you lend out multiple references to the pointer (or any other value), none of them are allowed replace the pointer, just like you can't delete or replace the pointer as long as any references to it might still exist. It's possible to lend out mutable access to the pointer using a mutable reference, but it has to be the only active reference to the pointer, and you can't access the pointer directly while the reference exists.
If we didn't have a dedicated server for it. So we could run other projects through Apache.
There is some recover magic going on already, but there does seem to be some errors that it can't recover from fully sometimes. Also I must not be using net/http right in go cause I think anytime it panicked it crashed the server, but I think that may of been from using log.Fatal before. Thanks for your advise though! 
log.Fatal doesn't panic, it calls os.Exit, which is not recoverable.
There are always subdomains...
OP wanted to see what was up, and this person was like "its exactly like everything else" It literally could appear in any programming context, and adds little to nothing about the particulars of programming in go.
8 ppl but many many years of full stack between us. It does impact throughput, but so would losing a team member, and having every party involved hedges the risk that someone moves on to greener pastures and suddenly there is a support or mission critical failure.
Your definition of "a while" is intriguing... 
I'm not sure I agree with your choices of words. But I share you sentiment. As I said in cpp subreddit - the /r/programming is our own /b. It's funny sometimes, but it was never good. Also it's full of Magpie Driven Developers and marketing articles. Long story short - stay away from /r/programming. 
8 developers is a good-sized team! and your choice makes sense if there's a high turnout, it's all about trade-offs, as everything with life. I think you'll like Go - it was engineered to fit well with Google, where there is a large turnout of engineers, engineers move to other teams frequently, leave Google, etc - so one of the design choices in Go is to standardize as much as possible - there's only one way to write tests, only one way to write loops, etc, which makes long-term maintenance much easier because you can pick someone else's code and it will look a lot like code you could've written yourself (enforced by linters and the automatic `go fmt` formatting).
I actually don't like go. I prefer it to other compiled languages, but I tend to choose dynamic typing and interpreted languages. Our environment features js, rails, python, c, go, scala, gallons of sql, c#, r, and a bit of matlab. si fueris Rōmae, Rōmānō vīvitō mōre; si fueris alibī, vīvitō sīcut ibī “if you should be in Rome, live in the Roman manner; if you should be elsewhere, live as they do there”
Define "moving forward". If you talking about the adoption - you are wrong and big companies like Dropbox, Twitch, Palantir, Netflix and even MS will prove it to you. If you talking about language evolution - I agree and disagree at the same time. I certainly would like some things changed and fixed. We need generics problem solved - the Go adoption and problems "at scale" definitely show a need for this. Some parts of the standard library need refactoring. The context problem... But I would also like not to rewrite my code each two months because "new shiny thing were added to the language in out latest version". And if you don't rewrite - your code is getting inconsistent. And inconsistent code is hard to read, and to support. If you worked in any codebase that survived several language versions - you know what I'm talking about. 
That's not the impression I get reading the comment. Sure some of it could apply to other languages but Go wasn't exactly hand down to us by aliens, it builds upon the past few decades of computer science, so there's unsurprisingly overlap with other languages. That doesn't make the comment meaningless or useless.
Wouldn't hurt to actually mention the name of the place.
Never said it was meaningless or useless, just not that it gave any indication that working in go was different than working in any other language. I'm sorry about your feelings and impressions( I'm not ). Don't get me wrong, I use go at least once a week. But it's hardly the first tool I reach for. Few of these points would be any different if it was implemented in ArnoldC. Not sure what all the downvotes are for, I'm trying to add a little context for OP saying that if they chose say, Visual Basic, they might encounter similar problems. 
Hi, it looks like we crossed paths (I just updated with more details). I was hoping to have a link to our job board by now, but in lieu of that I added more details. Let me know if you have any other specific questions I can answer.
Sounds cool, too bad I'm from europe. Good luck =)
Out of curiosity, why Thrift and gRPC and not Protobuf and gRPC?
Very true! At first I didn't like Go very much because I typed more compared to PHP/Python (our legacy languages), but once I got proficient with it (able to crunch code nonstop without having to consult documentation all the time) things normalized and I got fairly productive (let's say, 10-15% less productive than Python/PHP in comparison, as far as feature delivery time is concerned) but application speed got orders of magnitude faster in some cases, latency went down dramatically, number of bugs went down dramatically (unless you're abusing `interface{}`, if your code compiles, there's a good chance it will work as intended) and system stability and robustness went up too (because Go forces you to think carefully about error handling). Overall, I think it was a good trade-off and I would do it again.
By no means a disqualification. Feel free to contact me if still interested.
We have long used Thrift on our transport APIs and for general RPC. gRPC and Protobufs have been used for some other Go testing tools (specifically for master/slave coordination), and I personally prefer that combo. We didn't see enough of a performance difference to move all RPC to Protobuf and maintain both protocols in a large part of our code base.
EDIT: see below. I now argue for three instead of two marks. --- I very strongly oppose the syntax you have chosen: c := misc.Min[float32](a, b) My reasoning is that [] has certain meanings that are not in any way related to type in any of the cases. In the [Go Specification](https://golang.org/ref/spec) there are 301 occurrences of the opening bracket of which 58 are marked with the class ebnf in the HTML, i.e. are not actual Go code. After removing those (I used a simple regex to exclude the ebnf class the HTML is marked up with), we then see that all 252 remaining occurrences are in the syntax of maps, slices, and arrays. I looked through all of them. There are no other occurrences of the opening bracket in the grammar. When the same notation is used in different constructs, it becomes more difficult for both humans and computers to parse correctly. Your proposed syntax of: c := misc.Min[float32](a, b) Is therefore Very Bad. What is a better syntax for types? You propose "There will be less intention to overuse templates due to more verbose templated code." In this case a more verbose syntax is one that does not occur in the grammar, but is used in other languages. c := misc.Min&gt;&gt;float32&lt;&lt;(a, b) This is extremely verbose. It is also "foreign", because it is a quotation convention not used in English, but [common in French](https://www.google.com/search?q=french+quotation+marks&amp;tbm=isch), for example. Rather than being a proper single UTF character, it should be double greater-than characters. (In much the same way as the ellipses are not an ellipsis code point but three dots, ... ) This is not used in Golang: https://www.google.com/search?q=double+greater+than+in+golang The double &gt;&gt; or &lt;&lt; does not occur a single time in the specifications. Note that I have used the inverted guillemets (which are [the German type](https://german.stackexchange.com/questions/3075/why-are-the-german-guillemets-inverted) ) for three reasons: 1) it calls greater attention to the type inside - they are like arrows pointing to it. it is easier to place the cursor on it and edit it because there is just 1 pixel at the the narrowest point. 2) it is weirder and Go's syntax is orthogonal and kind of weird. It's not something you "guess" - it's something you learn and remember. 3) it does not recall the horror and pain of C++ templates. Further, it is very clear that in no case would any Go compiler that did not support it, falsely parse it as something different. I urge this syntax to be adopted.
Good bot
Thank you RandNho for voting on \_YOU\_DROPPED\_THIS\_. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). ***^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I was talking about language evolution, speculating about why other languages are received better in general subs like /r/programming as opposed to giving a bad faith analysis of the people there like the root comment. It wasn't a judgement on Go or what is the correct pace of language development. I do agree with you about Go, 100%. I don't want a future where the big-name libraries require some bleeding edge snapshot compiled straight from HEAD.
Programming sub is... well it tend to have interesting articles from time to time. And even interesting discussions. Rarely. But that about it for the good points. On the other hand, most of the discussions which are happening there gave me the impression that people who comment submissions tend to be either devs who never really worked on anything other than glorified CRUD for some random functionality nobody asked for, or devs who are working with 10 year old legacy software with no ability to move forward. Also the amount of arrogance there is simply astonishing. That doesn't mean programming sub do not have smart people. They do. But their practical experience is rather limited, but at the same time they are greatly extrapolating it to the "webscale" scale. Which is disturbing. Go is received well when you need to solve a practical problem. Mainly for networking and tooling. Web apps with high demands to speed or robustness. The language is not without issues, but which one isn't? Pick the right tool and do your job. This is what you payed for. But neither HN nor /r/programming is about solving practical issues. Most of the time it's about searching for "holy grail" which will save us all. 
I would have applied if I wasn't starting a new job in a few weeks. Damn timing! :)
Past the physical age of the universe, I reckon, without some sort of winnowing technique
I think every go programmer coming from python has written this. 3 months from now you will wonder why you bothered...
Serverless doesn't mean no server. It means the service providing the serverless service doesn't require management of resources like compute, memory, and scaling. Aws lambda being a prime example. 
I know its in /x/ but they should really just [drop their WebSocket library](https://godoc.org/golang.org/x/net/websocket) and adopt the Gorilla package in its place. The doc page already suggests you use Gorilla instead. 
Hopefully Michal's well-written article about Context will motivate it being replaced. I'm crossing my fingers on that one. For reference, https://faiface.github.io/post/context-should-go-away-go2/
Wonder if there will be a webextension of this? From what I understand they are similar.
I hope they rethink variable declarations, new and make. There's far too many ways to do the same thing. I'd be fine getting rid of := since it really doesn't add anything and is easy to miss. I also hope they revisit nil vs interface{}(nil) (they should be equivalent IMO).
Not a Go developer but just wanted to say the product looks fantastic!
I love :=. Please don't take it away.
Fair enough. Figured it had something to do with a legacy.
Something's gotta give. There's way to many ways to declare/instantiate variables, such as: var x T var x = T(...) var x T = ... var x T = T(...) x := T(...) And if T is a struct: x := T{...} x := T{&lt;name&gt;: ...} But why doesn't this work? var x T = {...} And are these both equivalent? x := new(T) x := &amp;T{} In fact, the whole existence of new is terrible, and I think the only reason they have it is because otherwise there's no way to create a new primitive pointer without a temporary variable, but it causes all kinds of confusion since it's only rarely useful and make it not often what you want. There should be only one obvious way to do something, and there are currently multiple with variable declaration and instantiation. I pick on := because it literally adds nothing besides syntax sugar over `var x = ...`.
If using systemd you could configure it to restart on failure. Docker doesn't make much sense to me unless that's already a standard deployment tool for you. 
I think there's been discussion about removing `new` before, but it wasn't because of backwards compatibility. `:=` is a lot more terse than using `var`, and I more or less only use `var` in bottom-level blocks or when necessary.
This is cool. I'll check it out!
or the new gobwas/ws package.
Download and install `up` on linux/mac with: ``` $ curl https://i.jpillora.com/apex/up! | bash ```
&gt; Also useful, if you use go build a lot without doing go install much, is to use go build -i so that your dependencies get installed so you are not re-building them needlessly. Does this apply even though the dependencies were earlier "installed" with **go get**
Eh, I don't care too much what the solution is, I just think it's just unnecessary how many subtle variations there are.
How is this different from ssh-keyscan that comes default with openssh-client?
Is there a way to declare multiple variables at a time and assign them to the results of a multi-return function call without := ?
This works: func do() (int, int) { return 1, 2 } var a, b = do() However, this doesn't seem to work in an if statement, but := does. The easy solution is to allow `if var a, b = do(); a != b { }`. One case where it breaks down is if one of the variables is already defined in the same block. := will not redeclare the variable, though it will shadow it if it's in a different block, which I think is a bit dangerous, especially since it could bite you in a refactor.
Damn, that's pretty compelling. 
Interesting. I might be interested, but I don’t know. I’ll have to learn more about the company, and think about it. I’m in Livermore, currently commuting to Sunnyvale for one of the big tech companies. I’m not so sure I would want to increase my commute even more, though. 
Download the Go extension
Yes, the go compiler is written in Go. What problem are you trying to solve? Why is go build via os/exec not an option?
Can I just say that I've been using golang EAP and setting up the debugger was a breeze. It incorporates dlv (delve) into the GUI really nicely. I think it's free for now but will cost money once version 1.0 is out.
Ok, that means i have to copy over the go build sourcecode? I am looking for a library or api to do that. My problem is like you want to deliver to your clients an office package bundled with the documents to edit. So that the client needs no separate install of the office package. The clients themselves have no go sdk installed generally.
So you want to attach the documents as data attached to the binary? You don't need to recompile to do that. You can just concatenate your data to the executable and read it like any file.
With vscode I have done nothing more than downloading the go extension. The only complaint I have had is it is slow to find implementations of an interface or perform a refactor. Gogland offers improvements in this area, but it is much more resource intensive.
Can you not use a separate file to store these things, like a sqlite database? Or any of the other embeddable dbs written in Go? 
Binary A produces Binary B. 'A' bundles documents with the office program to produce B which lets clients edit the documents directly by calling B. The problem is here, a client , who in generally does not have golang sdk installed, should also produce B's from an A what ist delivered to him. That is why i need to produce a binary A, which does not use go build. Binary B does not produce a binary, only 'A'. 
Documentation seems limited.
Hey I dont know if this will help you but I remember campoy have this go tooling video that include go debugging with delve https://youtube.com/watch?v=uBjoTxosSys anyway it shows other cool go tools that is definitely worth to use in your go environment :) 
XY problem, but from how you describe this makes little sense. It's akin to PDF files containing a C++ compiler to "compile" changes to itself each time you save, including the old compiler each time. No one would ever do this, you shouldn't either. Security alone is reason enough, if you ever need to email one of these things and get a new one.. you are asking your users to compile software they got from smtp. Create a client (PDF reader for your app) .. and settle on a format for documents if they have to contain metadata so be it- better a homegrown binary or haggard text format for documents than people in your office passing around documents containing a compiler... lol. A easy solution would be to use the tar or a zip pkg to have the client use the function that gets he path of the executable they added in 1.8 to put itself and derivative documents in a archive. Most people can open a tar or zip and when they use your binary to make changes it will pass itself along. I really don't suggest this though if it will ever be received from a untrusted source. You should just have a secure canonical download location for your client.
I have been a game server engineer for many years and built many game services in Python. While enjoying the productivity of python, I have always been concern about performance issues, especially when heavy calculations are carried out in Python scripts. Golang has been recognized to be an outstanding programming language for server-side programming. But since it's static compiled, we can not update server-side logic as we do with Python or any other scripting language to fix some bug in servers. GoWorld tries to verify whether or not Golang can be a good fit for game server programming, and update server logic by hot-swapping game processes. I started this project also to enhance my coding skill and gain a deep understanding of game server engine. If you are interested in game server engines, maybe you can share some ideas and give some opinions about GoWorld. 
I found that article a bit confused and unhelpful because it didn't propose anything better that solved the cancellation problem. Not wanting to see context everywhere doesn't make any more sense than not wanting to see errors everywhere (unchecked exceptions anyone?). In such a highly concurrent language, you should expect to see cancellation constructs often. Its not a problem it's a solution.
i want to deliver one binary 'A', nothing more.
I think this needs to be looked at a case at a time. The cost of breaking compatibility is often underestimated but it's still sometimes worth it if the value is really high enough. A blanket decision to break compatibility of any API even for slight improvements would be unhelpful.
The problem with Context is orthogonal to monads in Haskell. Monads are possibly the worst aspect of Haskell, and they also exist because they tried to solve what should have been solved in the language through passed-in variables. As Michal points out, it's hard to avoid having Contexts infect code that doesn't need it. It's a trap, and a particularly bad one at that. Michael says, clearly, he doesn't have a solution. Once we're in 2.0, though, if Contexts are there we'll be stuck with them indefinitely. I, for one, hope we can avoid that.
Remove the keywords: new &amp; fallthrough Drop packages: container , encoding/csv, database/sql. 
Everything. Even installing is badly documented: it's just a link and it's anybody's guess what you have to download. Change logs seem to be have been left untranslated. There is no documentation about the editor, just some technical things about environment (and at least one image is missing) in bad English. This is the first sentence &gt; LiteIDE build config default use -i flag, if use Go1.1 or Go1.2 not support What does that mean? It didn't recognize my $GOPATH, and there's no way to find out where that's set. It's not in the preferences. Turns out it's a button in the tool bar, but I had to find out myself. What the other buttons do? No idea. How to build or run a project? How to run it with different arguments? Etc. These things matter. If someone looks up liteide and there is not enough information, they'll stop using it.
A basic UI library would be nice. Gxui came really close. A cross platform UI library for simple applications would make Go accessible to those who think in terms of point and click. Not sure how far along shiny is, but being able to "go get" something that allowed you to create a cross platform UI would be really cool.
Whats wrong with fallthrough?
Errors and Context are infectious in a very different way. Errors infect the code that uses them. Context infects the code that is used by another Context-equipped code. The latter kind of spreading is much more evil.
How can any code infect the code it calls? I don't get it.
The problem with these is that they're not type safe. So as a consequence, to be able to use a general structure like this, your code will have to be riddled with type assertions or risk panics at runtime. There's a few code generation based solutions to this that will generate this kind of container for you for the different types that you need. I generally prefer those b/c of the safety they provide and b/c it's just a `go generate` away, which makes it easy to have it automatically happen during a `go build` etc.
I'm trying to switch from LiteIDE to VSCode because thats what the cool kids are using and as much as I like the VSCode editor better than LiteIDE the whole simplicity and features of LiteIDE are hard to beat. Everyone who is starting out with Go should start with LiteIDE, takes so much pain away.
It infects the developers who create that code. Context needs to be propagated to all blocking operations for cancelation (= code called from Context-equipped code). Thus, developers attach it to functions which will be called from server code.
I'm all for removing `new` but why drop `encoding/csv` and `database/sql`? Is there something fundamentally wrong with the implementations or do you think they should just not live in the standard library. And if so, why?
Remove context.Context Combining functionality of cancellation and data storage in one place looks too much like project specific. Go runtime and execution is full of tricks anyway, so why not to add a goroutine cancelation handler?
Doesn't seem like you would have to though, unless you don't want to be remote: &gt; We have an office in San Francisco, as well as many remote employees. Fully remote is OK.
I use both `x/net/websocket` and `gorilla/websocket`; I have to admit that i like gorilla's upgrade better, and also it has streaming support that is missing from the `x` package, ie. `x`'s frame writer forces fin=true on every Write(), instead of only on Close(), which only seems like a bug when I think about it.
By pretty much all bench marking standards VSCode is an inferior editor. The reason you hear about it so much is because Microsoft pays a marketing team (shills) to market the editor in programming communities. 
Yes, that's what I got. &gt; it's your choice of course I've tried it a bit, and it's a fast editor. Everyone who works on an underpowered laptop, should give it a try. A lot of work has been put in this tool, and it would be a shame if people discarded it because the documentation is lacking.
You're completely wrong. It's legacy c# and c++ codebases 
This seems unlikely. Could you point to an actual example?
Look into using traefik instead of apache then.
In my opinion, if 'go fix' can fix it, it isn't broken. 
It doesn't?
https://peter.bourgon.org/go-best-practices-2016/#repository-structure
:= is ok if it will not shadow outer scope variables. Now var = and := are too overlapped. 
If it's just a simple binary, systemd or supervisord would be the way I'd approach this - probably proxied via nginx/apache so I could still have some control over ingress.
I was using sublime before vs code. I didn't care for atom. I moved to vs code after about 30 seconds of use programming Go with it. Functionality and features won me over. 
vs code will handle auto imports for you. If you comment out something and the import is no longer needed, it auto goes away. As for variable usage, you can get rid of unused variable errors by just adding '_ = theVariable' on the line after it. You can remove that line later. I tried wiring up the debugger a long time ago and failed. It is supposed to be easy now. I get pretty far with log statements and a verbose flag. I should look back into the debugger.
Why not have binary A and binary B be the same binary with different options? A copies itself and puts the documents in sqlite and writes a new start up script with the right options (start mode, sqlite db location, etc). Another option would be to have your binary A trigger a job on your remote server that compiles the new binary B and returns it over the network to A to be written to disk.
Can you not use golang plugins to update server-side logic? (Though linux only atm) I don't know if you can, but seems a way to get new logic inserted into a running process.
What is Magpie Driven Development?
So I've written a small Tensor library for purposes of various computations inside a hardware product that my startup is working on. I've implemented in the library, the functionality to slice a matrix similar to matlab using a proxy. The implementation, I have works just fine and does exactly what I want, albeit with one hiccup, every time I slice the matrix, it allocates for the proxy. This caused a problem inside hot loops. Hence, I implemented the same functionality by allowing the user to pre-allocate the proxy and slice the matrix into it, thus eliminating the problem. However, I'd like to know if the size of the proxy can be reduced any further or a more efficient way to achieve the same functionality? The relevant code is linked to in the title.
I don't think plugins can be updated or unloaded. Plus, you still have the problem of gracefully handling network connections.
Indeed I need to evaluate this library as well. It's stated goals are around higher performance than other packages but they have some nice utility functions that could remove some of the boilerplate as well.
I am not using plugins currently, though I have been thinking about it. Gracefully handling network connections is the easy part of hot-swapping. This can be achieved by only accepting connections in gate processes and running all game logic on other processes called game processes. The major problem of hot-swapping is keeping game server states consistent. GoWorld game servers run game logic on entities. All game objects including players, NPCs, monsters are implemented as entities. So hot-swapping is all about saving and restoring entity states. Also, to make hot-swapping possible, developers should only store server information using entity mechanisms provided by GoWorld. This imposes some restrictions on server-side game development using GoWorld.
Ah yes, unloading seems to be the problem. Update is doable judging from this repo https://github.com/campoy/golang-plugins 
This, I think: [Like Magpies, software developers are unusually smart and curious creatures, almost by definition. But we are too easily distracted by shiny new toys and playthings.](https://blog.codinghorror.com/the-magpie-developer/)
That isn't what your post says, try explaining your problem or I'm not sure anyone can help.
Sorry, but it doesn't seem like you're living up to your standards: &gt; Stick to mailing lists if you want to talk to good programmers. Even hacker news is complete shit because fanboys use it as a platform to push their dumb agendas. or.... you're using r/golang to push your dumb agenda that useful features are not needed if you can copy-paste code?
Context is needed whenever anything down the call stack can block. I don't see how that's much different from errors. Errors (unless they can he fully handled there need propogating up the stack even if one of the things in there doesn't care about it.
Thanks for the clarification!
You say it yourself. Context is needed whenever anything *down* the stack can block. But it's needed *down* the stack. It propagates down the stack. Errors only propagate up the stack. EDIT: I messed it up a bit
Thanks for the rundown of your architecture! Regarding network connections, your setup and use case makes total sense. I just want to point out that there is another way: transferring the listening socket to a child process of yourself and gracefully shutdown (hot reload). But this would likely make more sense do it that way for stateless processes like web servers.
Yes, it was tracked in issue https://github.com/golang/gddo/issues/502.
please make struct tags part of the language and not just some DSL encoded as a string
yes `new` could be eliminated entirely
exactly! provide a migrate script or something...otherwise Go will become C++
a DB abstraction is necessary....but the one in the SDK isn't really adequate
It might be theoretically possible if you put enough work into it, but if you look at all the resources the Go compiler touches in the process of building a non-trivial program, you have to include all of them and set it up somehow so the compiler isn't looking on the file system for them. So you're going to be embedding the full source code of the standard library, the full source code of any other libraries you may want to build, and the full source code of everything that goes into your secondary binary. (You could try to ship the .a files alone out of the $GOPATH/pkg dir, but I would consider that much more fragile.) It's going to be a crapton of work (technical term) and it is way easier to simply ship a full copy of the Go compiler and use it normally than to undertake all that work. I can imagine ways to do this but none of them are even _remotely_ worth it.
You have a data race, and so the program has undefined behavior. Specifically, you can't both read and write the done variable from multiple goroutines without some form of synchronization. 
That I understand. What I did not understand was the different behavior always.
I think that at some point should give the same result of the playground. Or no?
I think the API of `x/net/websocket` is much nicer and friendlier. I know its implementation is not as advanced.
I think it's because play.golang.org wouldn't let your code run for that long time. go setup() will create a new goroutine but it's not executed immediately. The for loop takes 100% CPU, maybe play.golang.org detects this abnormal behavior and just shut your code down.
And this? Why not work? Link: https://play.golang.org/p/kZwGEY1Nz4
Thanks! I think that you are correct, but I not found anything document on this.
For Go, it's more important for types and values to parse compatibly than it is for them to be distinct. `F(v)` could be a function application or a type conversion. `(*F)` could be a dereference expression or a pointer type. The important thing is that it parses the same without caring what `F` is. Analogously, `F[T]` potentially being an index expression or a type parameterization is fine as long as parsing doesn't depend on resolving the identifiers involved first. Your suggested syntax `F&gt;&gt;T&lt;&lt;` doesn't parse consistently. Maybe you aren't aware, but `&gt;&gt;` and `&lt;&lt;` are bit shift operators. That means they can't be parsed as matching delimiters. Go would need to resolve the identifiers involved before it could determine the correct way to parse `f(a &gt;&gt; b, c &lt;&lt; (d))`. Is that two bit shifted arguments to `f` or one type parameterized function call as an argument? This is important because syntax is parsed into a tree: (_(..) (_(..) f f (&gt;&gt; (_(..) a vs (_&gt;&gt;..&lt;&lt; b) a (&lt;&lt; b c c) ((_) d)) d))) The challenge with `F[T]` type parameterization is it doesn't quite work for type declarations. Is `type T [N] U` a parameterized type or an array type? You'd need to invert declaration syntax, like `type[N] T U`, to remove the ambiguity. Also, making a language feature ugly doesn't discourage its use, it just makes your language ugly.
Excuse me. (My) Conceptual error. I understand now. Thank you all. Link: https://play.golang.org/p/R5a2u7Xxc2
Best lesson: https://play.golang.org/p/WfWkzd60qR
I'm still a student but I'm learning Go on my own pretty quickly, building my own project. I don't know what your hiring timeline looks like though, I graduate in May.
It's pretty adequate at our company and we got over a million users. To me it works out just fine.
The issue is to avoid opening a fault line in the Go community. Just abandoning backwards compatibility wholesale isn't going to go well unless the majority of the userbase is quickly convinced that moving to the new version is a great idea. Python 3, Perl 6 being fine examples of when this goes wrong. Despite that, I think the right sort of idea was Python's `__future__` module, or Perl's `use experimental`. I think that it's important to be able to move code across a backwards-incompatible boundary *piecemeal*, not having to change all of your code at once, and not having to adapt to backwards-incompatible changes at once. Ideally, you'd *never* need to port code which wouldn't benefit in some way from being ported. OTOH, there's a more subtle backwards compatibility issue. Suppose that in a hypothetical future Go has opt-in-able generics, the community adopts them widely, and many popular libraries are converted to export generic types and functions: the people who don't want to use generics will increasingly find that they can't find libraries they can use. The Go ecosystem may fracture into generic and non-generic camps, which is unlikely to be good for anyone. For language features that don't leak through package boundaries, this is probably not an issue. For example, if one can set for a file whether typed interface values will equal nil, nothing outside that file needs to adjust its behaviour accordingly. Even if you disagree with doing that in your code, you can use a library which does that, because importing code which uses that feature doesn't mean you have to use that feature. The conclusion I'd draw here is that where possible one should prefer backwards incompatible changes that can be isolated to a file or package, and are opt-in. If a backwards incompatible change that inherently effects the consumers of a package must be made, it cannot be opt-in. This also implies that such changes should be based on a reasonable consensus so as to minimize the trauma to those that don't really want the change: i.e. a hypothetical generics solution shouldn't be as extreme as Rust.
Let me rephrase as: "Which external packages you believe are so well-written, clean, battle-tested or otherwise awesome enough that you would like them to have their APIs irrevocably frozen and constrain their future development?" None. I want fewer packages in std lib, more should be moved out to golang.org/x repos or so. Basically, being in std lib benefits the discoverability and ubiquity of a package, at the extremely expensive cost of significantly constraining it. Being in std lib shouldn't be a desirable goal, it's just an unfortunate necessity sometimes. Std lib is a great place to define base types that external packages can use to have a common interconnect. For example, things like `io.Writer`, `net.Conn`, `time.Time`, etc.
[pkg/errors](https://github.com/pkg/errors) is one package I use on nearly every project I work on--no more mysterious trace-less errors. Getting it into the standard library and wider use would be fantastic. That said it does come up with a runtime overhead from capturing those stacktraces, but in most applications I work on the ease of debugging far outweighs the extra few CPU cycles.
You are completely correct, I forgot that &gt;&gt; is a bit shift operator. I like your analysis a lot, thank you for producing it. We need to introduce something for the delimiters. I have argued that [] is the wrong notation. Your analysis and example of (a &gt;&gt; b, c &lt;&lt; (d)) is very well-done. So, what about three &gt;'s? a &gt;&gt;&gt; b, c &lt;&lt;&lt; (d) which of course would be better written as a&gt;&gt;&gt;b, c&lt;&lt;&lt;(d) In this case the bitshift could not be followed immediately by a greater-than, because that is not an allowed token that follows it. It would also call even greater attention to the template. I would like to also point out that three quotation marks are used very frequently by Python as a special quotation operator, so that three &gt;'s is not a burden at all. I disagree that making a language feature ugly does not discourage its use: making a feature ugly enough means nobody will use it, while making it beautiful and simple enough may mean everybody does.
&gt; That I understand Not trying to be rude, but I don't think you do. [The behaviour of a program containing a race is **undefined**.](https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong) That means different compiler implementations, different architectures, and even different runs on the same hardware without changing anything are allowed to do whatever and the behaviour can change each time.
If you don't want people to use something, don't put it in the language. If you put it in the language, it should be with the intent that it be used. Putting a poison pill in a feature because you don't personally like it as a way to sabotage an attempt to satisfy the people asking for the feature is petty and pointless. Someone will just create a dialect that's less ugly, which will be fairly straightforward to do once the feature is supported in the compiler.
The Go authors are doing the reverse, keeping new stuff out of the standard library unless there is a *very* good reason for it to be there. They put most new stuff in the [sub-repositories](https://golang.org/pkg/#subrepo) (i.e. under `golang.org/x/...`).
Thank you! Triggering a regular remote golang compiler sounds heavy but maybe feasible. A and B cannot be the same, because B contains documents, A not. Delivering documents separately from B is not allowed in this use case.
I won't suggest any of them. One of my favorite parts of Go is that it only provides essential tools that enable developers to build more customized and useful tools on top of them. Adding more packages to standard Go will only bloat it up and produces monstrosities. Let me elaborate it a bit more here. I believe there are three kinds of packages in software framework: essentials, utilities, and preferences. Packages like fmt and os are essentials and is required by almost all development. Utilities like clients of paypal, Apple pay, twillio are useful only when you develop applications that need them. Preferences such as Gorilla, Gin, beego serve the same purpose in different ways. In my opinion, only essentials should be in Go's standard packages, while utilities and preferences should be developed and maintained individually.
You go too far. :) I'm going to take this to PM as this is controversial.
Nothing fundamental. I just think these things would flourish elsewhere, partly due to being able to have a very different release cycle from the standard library.
Imo, I don't see why they should document any steps they take to prevent denial of service. An infinite loop would obviously be unwanted on that kind of service.
Are you asking for explanation? Line 21 is waiting for the lock before continuing with the execution of G(). However, it will never get the lock as G() itself needs to complete before F() will unlock the mutex.
Does it not work when you insert the NullInt64 variable? Your column type would be BIGINT. You just create a new NullInt64 in go and set the Int64 field to your value and set valid to true.
That's not part of the language.
Could you show me what you mean? Sorry first time using MySQL and Go. Thanks for the help!
Oh, it's worse, struct tags are a language inside a language inside a language. The struct definition is the outermost language, the `` is the middle, and the someidentifier:"..." is the innermost language.
Sure, look at this: https://stackoverflow.com/questions/40266633/golang-insert-null-into-sql-instead-of-empty-string Go doesn't really have the concept of nullable unlike say, C# (``int? value = null;``) However, in the ``database/sql`` package we have a few types to help around. https://golang.org/pkg/database/sql/#NullInt64 They are just wrappers to help around. They contain the variable of the type that you want and a bool to tell whether or not it's a null. When you read from the database from a nullable BIGINT column that have "42" as the value it will decode into a NullInt64 in Go. The Int64 field will be set to 42 and Valid will be true. If the same column is a null value, Int64 will be 0 (default of the int64 type) with Valid to false. So to know if you got a null, check the Valid field first. If you want to write a null, just set the Valid field to false.
Build mircoservice. Have covfefe. You should really check your spelling before posting something on Reddit, especially typos in the title. A few points and I only had a quick look at the code: * It doesn't look like you are running your code through gofmt as you are using a different indentation (i.e. not tabs). Please do so, gofmt is extremely important in the Go community, even more so than pep8 is in Python. * Your package layout needs fixing, I am pretty sure in main.go, import "./app" is invalid. You may want to have a look at the standard package layout for Go. I initially started off my project layout similar to how you have it there (but without the relative importing as that is invalid) But I have changed how I do this now to closer match this standard package layout: * top level package matches the project name, e.g. "package foo" not "package main" * a cmd folder, in this another folder for each command, e.g. cmd/foo and inside there you can use "package main". * Each command can import from package foo and it's sub packages, e.g. I have foo/web as a subpackage. Never use relative imports like "./app" as it isn't official I think.
+1
If we accept it's necessary to have `var x T`, and that it's desirable to have `var x = ...` so you don't have to type a type name when the compiler must know the type itself, and that `T(...)` should exist as an expression, then we must allow `var x = T()` or complicate the compiler by putting in code to prohibit it. I prefer `x := ...` simply because it's shorter than `var x = ...`. Also, I think `if var x = ...` looks ugly. I agree about `new`. There's no good reason `&amp;int(42)` *shouldn't* work. I'm not sure about `make`. You could replace its use with "calling" a type: `make([]int, 10, 100)` to `[]int(10, 100)`. But, there is an established use of `T(...)` as a conversion, and it doesn't seem very "Go-like" to overload certain types with "conversions" that aren't. *If* we had generics, we could have, say, `arrays.make{[]int}(10, 100)`, but without them I don't think there's a type safe way to do what make does without dedicated syntax or punning on conversions.
netchan Plz don't hate me Rob
I think Erlang has a better solution for cancellation: although that is not all Context is used for. In Erlang, one can link two processes - roughly equivalent to goroutines - so that if one panics, so does the other one. One can also check/wait for a process to panic, or end normally. So a job can start up a tree of processes which are linked together, and then one of them can panic when it learns that the job needs to be cancelled, and then all the others panic. Another process not linked to them will be waiting for the root process to stop and then respond accordingly. e.g. If they produced a result, pass that on to whoever wants it; if they panicked because of cancellation, just carry on; if they panicked because one of the processes fell over, restart the job tree.
Thanks for the comment Rob! I will take your feedback and fix it.
Without context, this comment might sound dismissive or even snarky. While I actually agree with the sentiment, I thought it'd helpful to provide an explanation for any new programmers that come across this and want to know where the perspective is coming from. Slices are related-to but subtly different-from arrays in most other languages. A slice is actually a pointer to a slice header, a special kind of struct that contains information on how many elements are present and how many it can hold before new space has to be allocated. These values can be retrieved from any slice with the special builtin functions len() and cap(). Your List struct contains size and capacity variables that precisely duplicate the values already present in the slice header. And most of the functions present already have equivalent expressions in terms of slice logic. For a given slice "s", the following equivalences can be given for your list API: * New(): make() * Add(v): s = append(s, v) * Clear(): s = nil * Count(): len(s) * Delete(i): s = append(s[:i], s[i+1:]...) * Empty(): len(s) == 0 * First(): s[0] * Get(i): s[i] * Last(): s[len(s)-1] * Put(i, v): if i &lt; len(s) { s[i] = v } * Swap(a,b): s[a], s[b] = s[b], s[a] All of these are familiar and comfortable expressions for experienced Go programmers. Much of the benefit of Go is that we can reply on powerful expressions rather than opaque function calls to do routine operations like these. When we see these expressions, we feel confident that we understand what's going on. By contrast, the function calls of this API are opaque. While the compiler can generally be expected to inline simplistic function calls, it's actually more important that the programmer see them inline to know that there won't be any unexpected bugs or side-effects. ~~All~~ (Edit: Most) of the above expressions are constant-time. More importantly, they _look_ like they're constant-time. That's the power of slices. Insert() and Remove() probably deserve special mention. I'll discuss them in a reply to this post. SetCapacity() also deserves special attention.
The Insert() function of your API is bugged and doesn't actually perform any insertion. The function body doesn't make use of the 'v' parameter. A pair of nested append calls can do it in one line, at the cost of some unnecessary memory allocation. The best way to perform an insertion is probably as follows: s = append(s, 0) // use the appropriate 0 value for the type copy(s[i+1:], s[i:]) s[i] = v Remove() is simpler, since you don't have to worry about having sufficient space. It can be implemented in the following fashion: for i, sv := range s { if v == sv { append(s[:i], s[i+1:]...) } As you can see, Remove() is just a linear search followed by a Delete()
Inferior to what exactly? I thought its better then atom at least 
Even in this light, I would still like to see pkg/errors added to the std lib. Go's Error interface is a bit on the anemic side. Often, there are 3 different things we care about when dealing with an error. There is the actual message itself that could bubble up to the end user "could not find file foo.txt" but that's kinda hard to work with inside the program. So we also want an easy method way to compare/identify errors in the program ie `err == FileNotFound`. And sometimes, we want to have extra info for the programmer such as stack traces. Go's std lib currently requires you to choose one between the first 2. There are ways of working around that but they're less than idea. `pkg/errors` seems like a tolerable way to solve that issue without making new code completely incompatible with the way current Go code works.
9 days ago you were asking for help just figuring out what it would take to implement reddit-style comments. Why are you speaking like an expert?
SetCapacity() is the most interesting of these API functions, since its nearest equivalent didn't even exist until Go1.2. In your library, SetCapacity() is a no-op if the existing capacity is greater than the requested capacity. The Go1.2 functionality actually has the opposite behavior: it can only shrink the slice, not grow it -- this behavior can be valuable when the programmer wants append() to fetch new memory to avoid overwriting values beyond the slice's current length. In this regard, it's actually somewhat interesting that the Go language developers chose not to extend this function to the slice API. The intent was very much that programmers would use make() to build slices of the appropriate size and append() to grow them where appropriate. If I absolutely had to suggest a replacement, SetCapacity(n) would look like this: if n &gt; cap(s) { s = append(s, make([], n-cap(s))) } Note that this does have the disadvantage of an unnecessary slice allocation of n-cap(s) size.
&gt; Also, I think if var x = ... looks ugly. Eh, eye of the beholder I suppose. Rust has `if let` (different semantics though) so it's not entirely out of left field. While I like the convenience of `:=`, I just don't think it's justified since `var` is still necessary (though I suppose I'd be open with eliminating `var` in favor of `:=`). I just don't like having several right ways of doing something.
&gt; Let me rephrase as: &gt; "Which external packages you believe are so well-written, clean, battle-tested or otherwise awesome enough that you would like them to have their APIs irrevocably frozen and constrain their future development?" I understand what you mean. But I was thinking about packages that are so well-designed or have reached a certain level of maturity that are basically "done".
You clearly put a good bit of effort into this, I dig the full test coverage and such. But the API I feel has some pretty glaring design issues, feels a bit over engineered for what you're actually wanting to achieve. [State](https://godoc.org/gopkg.in/workanator/go-floc.v1#State) It states this interface is for shared state in jobs- the interface exposes synchronization but does little to ensure they are properly used. For example nothing stops someone from calling the Data method causing a race condition. Your state interface should clean up just fine if you replace everything with a Get and Set method and enforce concurrency constraints at your call to New, i.e NewState() returns a state wrapped in a RWMutex and if the user wants to provide their own State that is not safe for concurrent use they can, but for a concurrency library I'm not sure it needs to even exist. [flow_impl.go](https://github.com/workanator/go-floc/blob/master/flow_impl.go#L61) does not need unsafe usage here, actually this entire usage of atomic is incorrect. Unsafe and atomic packages both require slightly more care to use- your usage here is pretty unusual... not sure what your going for. I imagine you didn't run tests with -race flag and do not run concurrent unit tests because you would have seen failures. Reasons: // You castes to int32 because you had trouble // converting a Result (underlying int32) to a // pointer to int32. Google misguided you here. return Result(atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;flow.result)))).IsFinished() // this should just be below, instead of storing // a Result store a int32 and convert it as needed. // you can convert safely without unsafe usage but // there is no need, it makes the code less clear for // no gains. r := atomic.LoadInt32(&amp;flow.result) return Result(r) // this incorrect usage then proves to be // annoying and verbose and leads you to make // a helper function to access the value. But this // just leads to more incorrect usage of atomic: func (flow *flowControl) resultAsInt32Ptr() *int32 { return (*int32)(unsafe.Pointer(&amp;flow.result)) } // which you then use as: atomic.CompareAndSwapInt32(flow.resultAsInt32Ptr(), None.Int32(), Canceled.Int32()) That is invalid because because you have already read the memory without atomic access. You safely access the field in cas operation- but since the prior operation obtaining the address was unsafe it doesn't matter. A concurrency library that doesn't accept a context to signal cancelation is essentially a non-starter for me and many other Go developers. It seems you provide part of Context in your main Flow interface. You should probably have Flow simply be Context and add your Complete methods to State. You could remove the Data() aspect from state if you feel it's too bloated and use Context.Value instead, but job state and job data is probably fine. Their are more things I was going to mention but you should probably start with this stuff. Good job overall with documenting and actually placing a good bit of effort into it, I recognize that so please don't be discouraged by my comments you did a good job. Have fun.
Hi, we're in the same space and the way I solved it was to do a lazy materialization of the tensor and make laziness part of the process (https://github.com/chewxy/gorgonia/blob/master/tensor/dense_views.go). What I mean by this is can be found in [this example](https://github.com/chewxy/gorgonia/blob/sparse/tensor/example_dense_arith_test.go) (sorry you caught this lib under heavy refactoring... it's in a separate branch which will be merged back - the majority of the API will be untouched). So by default a `Slice` op is a view on the original tensor. So to solve your problem using gorgonia/tensor for example, all I'd have to do is create a new type: type proxyslice struct { *tensor.Dense // or tensor.DenseTensor } // implement Slice interface func (p proxyslice) start() int {...} func (p proxyslice) end() int { ... } func (p proxyslice) skip() int { ...} // logics! - implement your matlab colon slicing logic here func (p proxyslice) Slices() []tensor.Slice {...} Then to slice you can do something like that: func main() { T := tensor.New(tensor.WithShape(3, 4), tensor.Of(tensor.Float64)) S, _ := T.Slice(ss(2,4)) // ss is another impleentation of tensor.Slice q, _ := T.Slice(proxyslice{S}.Slices()...) fmt.Printf("%v" , q) } should give you the same result with no copying or allocation of tensors. It does allocate the memory for metadata tho
The problem with struct tags runs deeper than that. The problem is that they offer a way to attempt to map fields to various other things (db cols, json etc), when in fact go code can be used to do that already. So they could just remove them instead and it'd be far cleaner. You can do everything struct tags do with go code instead - simply assign your fields directly to whatever thing aligns with them, with the appropriate transformations and checks (I do a lot of work in go involving dbs and json and have successfully avoided using them so far). They're not actually necessary and like magic comments I think cause more problems than they solve. These are the two things (magic comments, struct tags) I'd remove in a putative Go 2, but the struct tags one would be too painful to force on current users so it'll never happen. 
I wonder if the author has made any attempt to bring it in the standard library.
This is very cool. 2 years ago I considered building a multiplayer server side engine in go.
I understand what you mean. I think there are trade offs. Having a package in the standard library gives it guaranteed stability which is a huge boon. An external package on the other hand has more flexibility and it can grow much easier. Unfortunately unless there is a common interface in the standard library for people to build on, like you described, the community will keep building and building and keep the ecosystem fragmented (See loggers). The line about what is essential and what not can be gray sometimes. Why exactly is a production level web server essential and why is it in the standard library? I also remember a thread where Brad was discussing bringing the Let's encrypt package in the standard library. Maybe those things are not essential but especially the web server and the rest of the strong standard library has played a major role into making Go a success. In my opinion, if a package is so well-designed and mature that is pretty much "done" then it would benefit the community a lot by having it in the standard library. I understand of course that the more we add in the standard library the more burden we put in the shoulders of the Go authors. But still I was looking for packages that are so tremendously good that they wouldn't add much burden for the Go authors and would enhance the standard library. Or maybe one of those packages get get in the standard library and some packages can get out in Go2. Or if nothing can be done then at least hypothetically speaking!
I am very well aware of that but hey... what if? :)
I hope we get it at least in Go 2! :)
&gt; I'm curious if anyone compiles their own version of the Go compiler Yes (I don't because I don't like wasting time to do something that's already done) &gt; what the process is like https://golang.org/doc/install/source
How much time are we talking about to follow that document? (Glancing over it, I can guess anywhere from "90 seconds" to "never - you will probably not succeed if you haven't done this before." I'm just a beginner.) In addition, how long does the build process itself take?
Use the official "contributing to go" document and start at the git clone part. Pretty easy actually.
It is neither that difficult, nor that time-consuming (especially after the first time). I compile from source whenever I upgrade Go. You do have to run make once for each architecture you intend to be able to compile on that machine, so if you want to cross-compile to all of them it's quite a bit slower than just doing amd64 or whatever. I think I remember each execution of make taking 5-ish minutes, but I'm usually tabbed off doing something else so I don't keep careful track.
Building Go from source is both easy and fast. You can find various 'how to do it' instructions on the web; [here is mine](https://utcc.utoronto.ca/~cks/space/blog/programming/GoBuildFromSource). Just building Go itself is pretty fast (about 30 seconds on a vintage-2011 desktop for me), but you can and probably should also run the self-tests, which will add a minute or two. It's certainly fast enough that you can rebuild casually. Once built, you can use your new Go simply by making sure that its version of the various binaries are first in your $PATH. How many useful comments there are in the compiler, the runtime, and the standard packages vary somewhat. I find it pretty easy to read more straightforward sections of the standard packages (and it can be both interesting and useful), but getting down in the depths of the runtime and the compiler can require some work to understand the code.
Nice reply!
&gt; You do have to run make once for each architecture you intend to be able to compile on I haven't had to do that for ages. I don't recall what Go version changed that but now I just run `bash all.bash` once and then run `env GOOS=xxx GOARCH=yyy go install std` for each combination of GOOS and GOARCH I care about (actually, for kicks I run `go tool dist list | awk -F / '{print "env GOOS="$1 " GOARCH=" $2 " go install std"}' | sh -x` to build them all).
Oh, that's nice
If you're only going to cross-compile things occasionally, I've had good experiences with not doing the `go install std` before hand and just doing `GOARCH=... go build ...` when I want to cross-compile something in specific. I assume that this is less efficient than pre-building the standard library, but since I only cross-compile occasionally and I rebuild Go frequently, it saves me time overall.
I think you mean gogland and I completely agree. Replaced Neovim for me for go programming.
Personally I prefer go-gin/* libraries for my RESTful APIs and my dockerfiles either based on scratch, alpine or if you so must insist on building the app inside a container, use multistage dockerfiles introduced in recent versions.
You want `x % 2 != 0`. If the number is even, you _don't_ add it up. Also: you can simplify `y = y &lt;&lt; 1` to `y &lt;&lt;= 1` and `z = z + y` to `z += y`. Go is good for lots of things. I use it anywhere I'd use PHP or C or Python (for various reasons) as long as I can find a good package for what I'm working with.
Hi chewxy, I'm sorry if I'm not following what you're saying. When you mean you solved it, what is the 'it' you are referring to? If you mean the allocation of a proxy, when I do m.Slice(...), I have solved this by preallocating the proxy of the required size. I would like to know if there is a more efficient way to implement the proxy, without maintaining the list of linear indices of the elements in the slice as I do currently. If this is possible, I'd be able to save considerable amount of memory. Do you see any inefficiency in my current implementation of Slice and SlicefromView methods where I can do better?
&gt; You want `x % 2 != 0`. If the number is even, you don't add it up. Yep, holy crap. What a mistake!! I am in the middle of building a repo of alogrithms in a few different languages and I just threw Go in the mix and I guess I decided to mess up that line.. It figures that it would be such a small mistake. Thank you for the shorthand too!
Well, people are welcome to write their own handlers (and have) on top of or without the standard library's implementation, but they're very useful to have in the standard library.
Something down the stack can block is the same as something down the stack can error. That's propagating up the stack *from* down the stack. What's propagating *down* the stack is the need to be able to stop what you're doing. If you need to be able to stop what you're doing, then you need any long running (blocking or non-blocking) code you use to be able to stop what *it's* doing. The need to be able to be interrupted from outside is what propagates down the stack.
From my reading, you have solved your problem of using proxy slices. The problem you have with proxy slices, from my reading, is in line 67 of your playground code, where it allocates additional slices. If this is correct, that means your question in this thread is the additional allocation in hot loops. My solution is: since you're only going to read(and not modify) a slice of `m` as your proxy slices, there is no need to create a copy (necessitating a need to alloc a new `[]int`) I then used an example from my own library where the slices are views-by-default (meaning they share the same underlying memory as the parent tensor). Then read from those views to form slices. So my suggestion to you is to make your `pmatrix` a view (that is to say the data is shared) p/s: my email is my username at gmail. feel free to send questions
Come along to the Sydney Golang meetup later this week and make friends there to recruit!
&gt; (and a gist if that's easier to read) Why didn't you just linked just the gist to being with? Now, to the code. Remove TripIterator and TripStore interfaces. If you just started why are you already abstracting these? Use the concrete types until you notice there's an action that can be expressed as a common action among a group of types then place it in a interface if you really must do. &gt; (*sql.DB)(s) Don't do this. Just have the DB on the struct so you can use d.DB. You can then add read-only connections as well. Right now you just back yourself into a corner. Do those and your code will dramatically simplify.
I can't see how removing the interfaces would simplify it, as all I'm doing is satisfying those interfaces by creating the functions I needed anyway. The reason I created the interfaces is so that I can change backends easily. I understand swapping out the typecasts, which I will probably end up doing.
Will you actually swap the backends? As in move from a database type to another? Or you just think you might, some time in the future, maybe? You can also use build tags to have different SQL backends while not changing much of the code. Say for example you need to switch from mysql to postgres and the placeholders are different, as well as the init part. Just split the code then into core, _mysql.go and _postgres.go files, use build tags to tell the compiler which code you want built and that's it. You can also extract the queries into a const block so you can reuse them easier.
Monads aren't that bad. Everybody trying to understand what they *really* mean creates a bunch of unnecessary hysteria. All they *really* are is a way to chain operations where the outputs don't quite match up with the inputs. As long as there's a consistent relationship between inputs and outputs (embodied by the `return` function), and a systematic way to use the outputs as inputs (encapsulated by the `bind` function) which composes in a sane way, you have a monad. For any collection you can iterate over, `bind` is just `flat_map`, applying a function to each element to produce another collection for each, and then concatenating the results. You could technically get context to work as a monad. Functions would just have to be generic over the type of monad so you can opt to use a context or not, depending on need. The problem is it requires fairly powerful generics (Rust can't handle it yet) and a fair bit of compiler optimization to rewrite the two steps—chaining operations and *then* executing them—into something more immediate and sane.
&gt;Will you actually swap the backends? As in move from a database type to another? Or you just think you might, some time in the future, maybe? It's always nice to have the option, y'know? &gt;You can also use build tags to have different SQL backends while not changing much of the code. Say for example you need to switch from mysql to postgres and the placeholders are different, as well as the init part. Just split the code then into core, _mysql.go and _postgres.go files, use build tags to tell the compiler which code you want built and that's it. That sounds cool, not really looked into build tags but may end up using them. &gt;You can also extract the queries into a const block so you can reuse them easier. I don't think I'd end up re-using any of the queries, but it's a good idea nonetheless.
They wanted to replace `new(T)` with `make(*T)`, but the community had gotten so used to defending the dichotomy that the autoimmune response killed the proposal. The key issue with `:=` is the implicit shadowing. That's why there have been proposals to make variable reuse explicit (like `v, ^err := ...`) so that you don't accidentally shadow in nested scopes, and making it illegal to shadow a variable with a short variable declaration. You missed var (x int = 1; y int = 2)
&gt; It's always nice to have the option, y'know? I know, but, as I've said after that, you don't always need to do that in order to achieve what you need. The problem with needless abstractions early in the development cycle, like in your case, is that you will be forced to build your code in a certain way. That way will favor cases which you may never end up with, complicate the code and provide nothing from the value you expected at this stage, (please read the following in a very sarcastic voice:) y'know? (end reading in a very sarcastic voice).
&gt; make variable reuse explicit I support this. Though I'd prefer to only have one way to declare variables, so `:=` **or** `var`. And yes, `new` should die. I'd prefer to replace `make` and any other builtin that does something with generic types with something that actually uses generics. It's weird that `make` can take 1, 2 or 3 parameters depending on the type of the first.
`var` is useful if you need to predeclare something, if you actually want shadowing, or if you need to specify a type such as with a package variable. I don't see the need to radically pare down the ways to declare variables, since they each have their use cases and there's not much overlap.
It was due to the allocation on Line 67 that I implemented the SlicefromView method. For example, if I have a tensor of dims 3x3x1 and wanted a 2x3x1 slice of this tensor, I'd create a proxy of size 2x3x1 and give it to SlicefromView, which would fill the 'ind' member of the proxy with the linear indices of the elements corresponding to the 2x3x1 slice in the tensor. By doing this, I avoid any allocations inside the SlicefromView method. In this example, the tensor would take 96 bytes and the proxy would take 80 bytes of which 48 bytes corresponds to the linear indices. I'd like to know if there is any way to reduce the size of the proxy. The 2x3x1 slice is also used for modifying elements in the original tensor, hence, the proxy is not meant for read-only purposes.
&gt; there's + not much overlap There's quite a bit of overlap, enough that a couple minor tweaks could put them on feature parity. I just don't see the need to keep both, and I don't think that removing one is "radical", since most languages don't have this distinction. In fact, I think we should add tuples and use them for multiple return since they're useful in many other contexts as well. This would make an even stronger case for getting rid of `:=` since `(x, y) := ...` looks weird.
I meant overlap in usage rather than in functionality. People don't use `var` unless they have to.
But that's only because `:=` is slightly less typing. The problem is that `:=` can't solve all your problems, so it needs what `var` provides, whereas you can accomplish anything you need with `var`. Thus `var` is strictly more powerful than `:=`, while `:=` is more convenient. If we make `var` more convenient, then we can eliminate `:=`. Let's do something about shadowed variables (e.g. `var x, ^y = ...`) and allow `var` in contexts where `:=` or `=` are the only options (`for`, `if`, etc). Go is all about orthoganal features, which is why I think `:=` and `var` having so much overlap is weird.
&gt; What could you hypothetically see replaced/redesigned/removed from the language or **the standard library**? 
Ok I think I have it correct but I am now getting an error that says " Composite literal uses unkeyed fields" do you have idea what it means ? 
&gt; Composite literal uses unkeyed field I hadn't seen that error myself, but [the first Google result](https://stackoverflow.com/questions/36273920/disable-go-vet-checks-for-composite-literal-uses-unkeyed-fields) points out that if you use `go vet`, you get a lint warning for structs you declare a literal with the values and no field names. It isn't an error, so you can disable the rule in go vet, not use go vet, or probably ignore it. According to some, you should instead specify the fields when you make a struct literal. Personally, I don't care much if there are only two fields in the struct.
I don't know that the packages I use are perfectly battle tested, but I can't help but feel like NTLM auth (easier in 1.9 with the custom dialer), LDAP queries, and UUID types are things that should be in the standard library due to how common and critical they are to many codebases.
Yes, I plan on doing that too. Thanks. 
I'm on the Windows 10 platform and I install the Go compiler from source. It is a very easy process. These are the steps: * Download and install the bootstrap compiler (you only do this once) * Set the **GOROOT_BOOTSTRAP** environment variable to that bootstrap install folder (you only do this once) * Download the Go source code * Run the build using the `C:\Go\src\all.bat` So basically once you have done it once it is just a two step process. 
*SIGH* the pains people have to go through when you don't have generics (code generation). Hopefully 2.0 will fix that.
I wouldn't consider a program with undefined behavior correct...
I didnt use library but i find this code review quite useful. Tnx
That is kinda nice, but if you want the same effect in a more "standard" way, you can use something like [Mongrel2](http://mongrel2.org/) or [NATS](http://nats.io/). Clients connect in to a long-running (but dumb) server that just forwards all requests on to a message bus (with a connection ID representing the incoming TCP socket). Your game processes picks up messages off the bus and sends responses to the client (carefully copying the connection ID, so the server knows where to send the response). If you are OK with a tiny bit of game downtime (~100ms), you can just kill your game server and restart it (serializing the data temporarily). The front-end will queue up messages while the game is down. If you need 100% uptime, you can store your game state in a database like Redis (or just use Shared memory or other IPC.) This lets you do a rolling restart on the 'middle tier' of game servers, as long as old+new process messages similarly. (i.e. during the cut-over, either process will handle the message.) When adding new features, it's best to default the feature off (via feature flags), then turn the feature on when the cut-over is done. If you want to scale better, you can actually run many game processes, each getting a fraction of the messages and updating (the shared) state in parallel. 
I thought Dave Cheney wrote that.
"Without generics!" Proceeds to explain code generation Ugh.
Adding a couple of comments to the code to do the code generation doesn't seem like a really big pain to me.
That's right. Go should start removing features. I'm dead sure it will increase adoption.
Relying on a 3rd party library for basic collection functions is a pain.
For your future investigation, check out Dep to manage your dependencies. It's pretty easy to use and to migrate to. I've had no problem with it so far and the alternatives, like glide, are really buggy.
&gt; Stick to mailing lists if you want to talk to good programmers "People elsewhere tend to disagree with me at times, therefore they are idiots."
None. Pull stuff out of the stdlib into their own repos with their own release schedule and stability policy. Putting something in the standard library is making any kind of maintenance on it incredibly hard. My favorite example is [plugin](http://godoc.org/plugin) -- buggy as hell, mostly pointless, only supported on a tiny subset of architectures, but it's in the stdlib now, so we're stuck with it *forever*.
I really dislike that package. It's born out of an unreasonable rejection of debug logging, it encourages useless error messages (It equates "helps telling the user what went wrong" with "barfing out a stack trace") and with the language as it is, it just fails if not *everyone* is using it (and thus violates the whole "errors are just values" idea). I think it's a perfect example of something that seems reasonable and helpful at first glance, but just encourages bad design and bad code when looked at with some distance.
Keeping another round of generics bikeshedding aside, I like the minimalism and clever json usage. Tho I would like to know the complexity of Uniqfor example. 
This is apparently a pretty divisive topic, because someone (I suppose a java programmer) was telling me just last week that Go was garbage because it didn't bake in stack traces to all its errors but required a separate package to do so
Great ideas! I am currently using the "kill your game server and restart it" method. 
thanks - just what I wanted to know.
Thank you all very much
There is a large number of web routers with many claiming to be fast, etc. I wished this router could get more attention. https://github.com/gowww/router. It's API is intuitive and straightforward. It is apparently faster than all other equivalent packages. 
I'm wondering this also...?
SIGH another idiot in support of generics SIGH
There is no UI package in the standard library. So none can be removed from it.
&gt; More importantly, they _look_ like they're constant-time Delete(i): s = append(s[:i], s[i+1:]...) This doesn't look constant time to me? Am I missing something?
No idea why people down voted you... This sub is unbelievable sometimes..
I'm sick of all those this is the fastest router claims. Most of the time it doesn't even matter because you are waiting on dB queries or whatever in your handlers. 
I don't have any issues with Glide, yes I want to use dep in the long run, but I don't yet because it's still "under development". I will switch over as soon as it's considered "done".
There is no project in Go, there are only packages. Sub-packages don't exist. So it makes complete sense that go test runs the tests of the current package.
Been using Go for years in high load production. Never had need for stack traces. Message is enough to pin point where problem originated, and possible causes were never obscure enough to require stack dumps. It was always either nil pointer or function returning error that should be handled correctly. Maybe stack traces are invaluable when debugging shitty third party libs I always stay far away from. Only "gremlins" I ever encountered were with CGO and using C libs that are not thread safe so you need to lock goroutine. But in that case stack traces are just random useless garbage as crashes happen randomly.
&gt; 4.2. Support `string(int)` and `string(floatXX)` conversions that assume base-10. Unfortunately, `string(int)` are supported by Go now, but the input integer value is viewed as a rune value. [btw, I'm not the article author]
This makes no sense. If there is a new Go user on let's say Windows (non Unix), how do you tell him to test his code changes on a bigger project? When the answer is, go and google or test every package by hand, Go IMHO fails. Example2: I write &amp; test all my code on Linux. Right now, if I would like to check if one of my programs run on Windows too, I would have to google, to figure out how to tell the Windows shell (what ever the name of the default shell is), how to run all tests of my application. It is strange to have a programming language which ships – code wise – with good testing support, but than "forgets" a command to run all tests of your application.
Yes, on mac using https://github.com/moovweb/gvm
This was a really interesting post. I think there is a lot of value in it. Thanks for posting it
For 4 you'd want to introduce students to the flag package, instead of wanting to change the language. The thing you're looking for is flag.Int. For 5 you can add the edu package yourself. I read the whole list and I have a hard time finding a real problem that can be addressed in Go.
They obviously put a lot of thought into this, born of their day-to-day experience at CMU. It makes sense that Go is useful for teaching. Go was built to be easy to understand and pickup for newly minted graduates at Google. With that said, I find value in recommendations 1, 7, and 8. I don't think #5 (built-in precondition, postcondition, assertion and loop_invariant mechanisms) will happen. And I'd be interested to hear more details on why #3 (int vs. int32 vs. int64) and #6 (Assignment to fields in a map of structs) exist as they do today. As far as renaming *slices* to *lists*, I don't feel that will happen either, rightfully so. A *list* represents a fundamental misunderstanding of what slices are actually meant for: memory efficiency. In other words, you can hold 2 GB of data in memory and pass around thousands of references to different portions of that data all backed by the same data. That was the original primary use-case and hence the word *slice*. I admit, however, the naming is counterintuitive to new learners since they mostly treat it as an *ArrayList* like equivalent. But slices, named as such, were pretty fundamental in Google's software before Go. They used them in C++ from what I remember... can't remember if they had an equivalent implementation in Java.
Though the behaviour makes sense when you're aware of the "there's only ever packages" it is rather confusing if you come from any other language. There's a reasonable expectation that when you run `invocation of test runner` it will run all tests from the current root and recurse down.
I think you're creating artificial problems. `go test ./...` or `go test github.com/you/repo/...` works the same on Windows as on Linux.
&gt; It makes sense that Go is useful for teaching. Go was built to be easy to understand and pickup for newly minted graduates at Google I agree totally. This simplicity is the trade -off that google chose to do and therefore making a lot of experienced people not wanting to adopt Go. Coming from Java, C++, C#, Scala, Kotlin to Go is not a pleasant surprise for most because of lacking features. If Go adds those features then more people from those languages might switch but at that point Go is not a simple language anymore. Go found its now niche place and can attract some developers and as soon as they leave that niche they go head to head with the bigger dragons in programming languages and its a hard fight.
Yeah... so has Dave Cheney made any attempt to bring it in the standard library?
For good or worse, it seems that "[everyone](http://godoc.org/github.com/pkg/errors?importers)" is using it. This is the first time I hear anyone say that it is not a good package. I was under the impression that everyone thought it's great and that it might even become part of the standard library. In fact pkg/errors it is so popular that some other popular 3rd party packages are using it as a dependency despite them being libraries.
Again I am not talking about garbage packages. Everyone has been saying that plugin was a big mistake that it ended up in the standard library. No. I am talking about external 3rd party packages that have reached a certain level of maturity and have considered everything there is about design so now they are pretty much "done".
I kinda wish the standard library router would become more powerful so that all the router fragmentation of the ecosystem would stop.
I agree! I was pretty surprised the standard library didn't offer anything about these!
I tried to point out that relying on a thirdparty tool (the shell) seems like a bad idea for such a general task. Also there exists more than one shell. From your answer I would still not know if it works with cmd.exe or only with PowerShell. Also why not write a Makefile – like we used to do – since thirdparty tools are needed, why not stick with one that exists since a long time? I love a lot of things about Go, but some decisions are – from my point of view – just wrong.
&gt; I tried to point out that relying on a thirdparty tool (the shell) seems like a bad idea for such a general task. Are you trolling? `...` is expanded by the Go toolchain, not the shell. Show me a shell that expands `...`.
&gt; Are you trolling? No If no shell expansion is used here, why can't we have a flag for it? `./...`is quite cryptic. This is a serious question.
All depends if you've done any coding in the past. If you've done a fair bit, it shouldn't take you too long to get to learn its excentricities. But if you've never done anyway at all, then you may find it a while as you have to learn some core programming fundamentals.
I've done C and C++ stuff in the past. As in I can make a program work, but nothing major.
I think people should take a note from the standard toolchain and golang/x/tools. The tools produce helpful, direct error messages that do not rely on any wrapping or stack traces. Because the people who wrote them sat down and *handled* the errors they could handle and explain the ones they couldn't. pkg/errors is certainly popular (though I haven't yet heard of anyone on the go team who gave any indication that it might end up in the stdlib. So far, I've only heard people express the opinion that it should). But I claim that this is because it makes the job of writing code easier - at the expense of *using* that code. As a user, whether I'm using a software or whether I'm depending on a library, I don't ever want to have to read a stack trace to figure out what's wrong. I want an error message that tells me.
Shouldn't take too long then really.
Very difficult to answer. But given you said you had some previous experience with c and c++, probably less than if you were a complete beginner. If you understand general programming paradigms, it won’t be long before you can learn to apply those to Go. But be wary that sometimes this will not always be the best way, Go has a specific way of doing things which sometimes goes against the grain. Understanding those gotchas is what will take you from being a programmer who uses Go to being a Go programmer, at least in my opinion 
It will take you 2 hours 7 minutes and 56 seconds to learn it, and to become proficient it will take an additional 16 hours, 40 minutes and 18 seconds.
1) and 2 I can agree, but i don't think that's absolutely necessary. If we can tolerate "public static void main()"... 3) I don't think that, for a beginner course, is that relevant to know if a int is 32 or 64 bits. 4) and 6 I agree. 5) I agree about the usage of defer, it's not literate programming. Kudos for the edu package btw. 7) I can agree, but i don't think that a non-safe function must be implemented. 8) I always hated the "for" keyword, even if in Italian it makes **a lot of** sense, and i'm Italian. Even a "loop" keyword imho can be better, with the same syntax of the "for". 
Thanks bud.
Is it a prerequisite to develop one of these to call yourself a go developer? I am pretty sure I have seen around 10 of these posted on this reddit... Can't you just use an existing one with code generation?
I agree, I don't see why these have to be implemented in the standard library. I'm also personally not a fan of the proposals for conversions via `intXXX(string)` and `floatXXX(string)`, for the following reasons: * Packages are going to be introduced extremely early on, as mentioned in the post, so why gloss over `strconv` when `fmt` is surely known by that point? * Panicking upon failure is completely against the whole idea of Go's error handling. This will lead to having to teach how to recover from a panic and form bad habits that will have to be taught out of later on. Implicitly importing the `fmt` and `edu` packages and setting files with a `func main()` to `package main` just adds mystery. If the lecturers absolutely must ignore packages to begin with, they can use the built-in `print()` and `println()` functions; despite being discouraged to do so. Slices are not lists, so I don't know why they should be renamed as such. I enjoyed the beginning of the post, however. 
Based my experience from the company I work at. If you already have experience in other languages and learn it mostly through coding something full time. It normally takes about a week to get comfortable with syntax and basic design. It takes about a month to design stuff in an idiomatic way. And after about three months you know most of the intricacies of the language. 
&gt; because your "done" is a boolean, either true or false Are you claiming that having a race on a boolean variable is okay? That, for example, having one go routine do `for !done { /*something*/ }` and another do `done = true` is okay?? If not, please clarify what you do mean. If so, you are very very wrong. Please read [Benign data races: what could possibly go wrong?](https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong). &gt; which part is undefined again? The entire behaviour of a program from the moment it encounters a race. In your example: func setup() { a = "hello, world" done = true } The compiler is free to re-order these lines or stick any junk it wants into those variables as long as by the time the function exits it has written the values given to the variables. The most obvious possible outcome of this is that a valid Go compiler could first set `done` to true (which `main` sees and accesses either uninitialised or garbage `a` value) and then later `setup` writes to `a`. The linked article above gives other less obvious (and less likely for such simple examples) possibilities. To be perfectly clear, the code you linked to is wrong, it is not a valid Go program according to the Go language specification and memory model. Because it happens to not get rejected by the compiler and because it happens to produce desired output with a specific version of a specific implementation of a specific Go compiler is completely irrelevant. 
Zsh can do, at least in my Prezto configuration, as I type `...` it expands in line to `../..`
&gt; What won't be in Go 2.0?
In particular: are you saying that falling through should be the default and `break` should be required to prevent it - as in C - or that falling through should never be allowed?
&gt; you do understand that you can either read either true or false and not a third state, at any point, right? You have an all too common misconception of why races are bad. *Please* read the above linked to article. You do understand that the compiler is free to re-order code, right? You do understand a valid and correct implementation of a Go compiler is allowed to take: a = "hello, world" done = true and generate code equivalent to: done = true runtime.Gosched() a = "hello, world" or (as an extreme example) to generate code equivalent to: a = "some random junk, see previous linked article about when/why it might want to do so" var tmp int8 = 42 *((*int8)(unsafe.Pointer(&amp;done))) = tmp runtime.Gosched() done = true runtime.Gosched() a = "hello, world" The above code is convoluted but, again, a perfectly valid thing for a correct Go compiler to do as it will not change the behaviour of any valid Go program.
Depending on whether you're already a programmer, I estimate around a few weeks, maybe even days. See this explanation and list of resources: https://news.ycombinator.com/item?id=5365401 (Note: it's from 4.5 years ago but still completely relevant.)
&gt; As a user, whether I'm using a software or whether I'm depending on a library, I don't ever want to have to read a stack trace to figure out what's wrong. I want an error message that tells me. It seems pkg/errors is so popular and convenient that can be used [by libraries](https://github.com/gorilla/csrf/issues/69) regardless if they provide help to the end user or not. I think it is an unfortunate result for having a lack of guidance, opinions and articles about good error handling. Yeah sure we have the classic articles 'Errors are values' and 'error handling in Go' (which is now 6 years old!) and the occasional random error handling article but those can only get you so far. The look-at-the-standard-library argument doesn't help either as it is not helpful at all for beginners. Right now if I do a Google search on my machine for "golang error" Cheney's package appears on top and surpasses the few good articles that teach proper error handling. Personally I've never used the package and I always try to good error handling myself. But I have to admit even after quite a few Go projects and me actively researching and trying to do proper error handling, I still have no idea what I am doing. I can't imagine how a beginner would feel. Where are the best practices? Where is the guidance? We have none. Just abstract advice hinting to an elegant solution that supposedly exists somewhere if we search hard enough. So no wonder Cheney's package is so popular.
Code generation doesn't work with `go get`, `go test`, `go build`, etc. You have to run `go generate` manually, every time you change the code.
How would you rewrite the following to use your flag instead of ...? go test pkg1 pkg2 project1/... project2/... pkg3
Still ../.. is not how Go expansion of ... works. I should be more precise with my question.
Yes, I maintain my own version of OpenBSD port. Background: official one lags a bit behind.
Pretty much the whole program. You have no guarantees about what it will do and any behavior you happen to see from it now could change at any point since it's undefined. In addition to the data race article I think the [go memory model](https://golang.org/ref/mem) article is a good read
I don't agree with (1). I feel like doing this would lead to more confusion long term and you don't have to introduce the concept of packages here. Instead, I've always taught this as, "package main is our way of telling our program that the code in this directory will have a main function and that is where we want our program to start from. While we might only have one source file now, when we have many different source files spread across many different directories we need to tell the compiler where to start when we run our program. This does that." Sure, it doesn't exactly cover why we say "package" instead of something else but would having a main package implied truly help? We would still have to explain that a main function implies we want out program to be where our code execution starts, and it opens the door for tons of confusion. What happens when we have 3 source files in our main package? Do the ones with no main func need a package declaration? Or can that be assumed because one file has a main func? What if every source file in a direct has `package abc` but one has no package and a main func - is this an error? Do we have two packages in one directory now? Or what happens later when a student adds a main function to a non-main package and is confused about why it isn't run? And later when we teach about packages we now have to teach about the one exception to declaring your package that occurs when you have a main function. To me this would feel like a very minor short term gain in exchange for some potentially serious confusion down the road.
I think you are proposing they implement their own edu package and provide a lot of this functionality in that. Is that what you mean? If so, I agree. By the time they have their class using packages it wouldn't be bad to just tell them, "here is our edu package that simplifies a few things until you start to get the hang of what you are doing. After that a lot of this will make more sense."
Solid points, definitely something to consider for teaching programming for the first time. Go could probably simplify some of this logic, such as implicitly assuming "main" package when unspecified. Though Go represents a much shallower learning curve than say C, Rust, C++, or Java. 
&gt; It seems pkg/errors is so popular and convenient that can be used by libraries regardless if they provide help to the end user or not. I don't understand why you are writing this. I already acknowledged that the package is popular. But programming is not a popularity contest. &gt; The look-at-the-standard-library argument doesn't help either as it is not helpful at all for beginners. Why not? The level of expertise doesn't seem to matter to me here. In general, the code of the stdlib and the tools by the go project itself seems, if anything, *better* suited for beginners, as it's of an above average quality (not saying it's perfect, though). &gt; Where are the best practices? Where is the guidance? We have none. The best practice is, to handle errors and to think about error messages. Don't write `return whatevs, err`, but think about what it means to have an error at this point and translate that into a meaningful message to the user. `fmt.Errorf("read failed: %v", err)` is not a meaningful error message, `fmt.Errorf("configuration file %v does not exist", cfg)` is. Yes, that will require writing some code to handle different error-conditions, which is the essence of the "errors are values" adage - error handling is code and to give useful errors, you are going to have to write some code. Yes, that requires doing things like if os.IsNotExist(err) { return errors.New("config file does not exist") } if os.IsPermission(err) { return errors.New("config file is not readable for current user") } and it requires doing that on several layers of your program (it also requires adding generous amounts of debug logging, so that if there is an actual bug in your program, you can get at the additional context easily). But that is, what error handling *is*. Error handling isn't the technical implementation of how to bubble them up the stack, it is looking at what went wrong and reacting to those conditions appropriately. &gt; Just abstract advice hinting to an elegant solution that supposedly exists somewhere if we search hard enough. The solution is not "elegant", but it is simple: Write code. The basic issue here is, that people want a magical system that absolves them of the responsibility to care about error handling, be it exceptions or Dave Chaney's error package. Something that they can just lazily and mechanically call into and have good error messages fall out the other end. But that simply does not exist.
I like to think of Go as action oriented rather than object oriented, with the way interfaces and methods work. Edit: spelling
Most programming languages only allow to test one project at a time, and I think that is what Go should do, too. What's the use-case for multi-project testing? That's how I think it should be: go test # test the project go test -package pkg1 # test package(s) go test -all # test project and all of its dependencies This is maybe more compatible with the current logic: go test -project # test the project go test pkg1 # test package(s) go test -all # test project and all of its dependencies The `./...` notion should be removed, because it is not intuitive.
So you want to cripple flexibility just to force your argument? Running multiple go commands has an overhead that single one does not have - if the packages being tested have the same deps they need to be parsed multiple times, instead of just once with single command. Sorry, but I do not treat the "most languages" argument seriously.
Exactly, that's why the above code is terrible!
I think I've figured most of it out in two weeks. I have a bunch of files, unit tests, and documentation now in my new project. The syntax, philosophy, and toolchains are surprising straightforward. It's very different from C-based languages at first, but it makes a lot of sense a few days in. There's good documentation online and there's a Go By Example site that I often check out.
Hi guys! I've created a simple package for managing templates for web apps. Please criticize :)
Things getting out of sync can be a pain too. It is easy to regenerate like 9/10 things and miss one when you fix a bug without realizing it.
Dep is safe for [production use](https://github.com/golang/dep#current-status), just not for scripting/CI but I understand that you want it to mature.
Please calm a bit down. My point-of-view is that in the end, the programmer is writing an application and creating for it a project/folder. Tests are there to support him to do the job right. As such, it should be easy to run tests. I think that the current solution is usability whise not good, and missleadingly thought the current solution would also depend on shell expansion (which it does not) -&gt; the usability argument still stands. I don't see why argumenting that most other languages see the world from a project point-of-view would be a non-argument, since it is probably what someone comming to Go would expect. Since Go is the only language I know that choose to view the world from a package point-of-view, it would be intresting to know why.
Took me a year to be comfortable with most of the language, learning it on weekends by developing some programs and reading here. My background is with OOP so moving to a ~~functional~~ procedural language was the most challenging part. **Just tag me as the guy who always mean procedural when writing functional. It's not the first time my brain does that.**
Your 'shitty idea' is actually the perfect solution. The solution is a rewriter.
Thanks for writing up this experience report! It would be great to see some changes to Go that would help the newcomer and smooth out some sharp edges. I don't speak for the Go team, of course, though I have been around since before Go 1.0 and taught C for over 4 years, and as a citizen of the Internet I have opinions :): **General** One thing that I was hoping to see in your experience report was the "sharp edges" of the language that nip nearby students. In particular, how often do they run into [accidentally binding to loop variables](https://golang.org/doc/faq#closures_and_goroutines), value-method "immutability," or confusion in shadowing variables? Things like this that confuse new programmers are also things that make programs harder to read even for seasoned programmers, and should be a prime focus for Go 2.0. One tool that you also have at your disposal as a teacher of Go is the fact that the language is trivially parseable and generateable -- you could make a linter that your students can use on their source code to point out common mistakes that you see or a generator to set up the skeleton of the project that you want them to make before they've been taught all of the magic (packages, imports, etc) that may be needed. **#1 - implicit `package main`** In general, I don't support "dumbing down" your "Hello, World!" program for students. When I used to teach C, I wouldn't go deeply into discussions of how `#include` resolution worked, but I would not say to ignore the `#ignore "stdio.h"` either -- I would point out that this line is required for all of their programs for the time being, because it tells the compiler where to find `printf`. Students understand (or, at least, they should) that you're going to be explaining the programming language to them a piece at a time, and so a certain amount of "We'll return to this topic in further detail later" should go unremarked (as it does in Physics, Calculus, Literature, etc). **#2 - slices** I find that slices can be taught either before or after arrays quite successfully. If you also teach the slice notation to alias part of a slice, you can (justifiably) point out that a slice is the result of such an operation. When you do cover arrays (which I cover immediately before structs) you can point out that you can slice them as well, and then later when you cover memory layout point out how they're related. **#3 int vs int32 vs int64** I agree that this is somewhat confusing. My preferred solution is to make `int` 64 bits wide in go2, but to leave it unaliased. The fundamental problem with aliasing, at least in the current "variably sized" `int` world, is that you would make the program fail to compile on the opposite sized architecture (I would be able to pass an `*int` to a function calling for a `*int64` on my machine, but you may not). I actually consider the stricture of Go's type system, particularly the omission of implicit numeric casting, as a huge teachability improvement over C where I managed to have students finding all of the confusing ways in which you could misuse implicit numeric casting. **#4 - int-to-string conversion** I wouldn't teach the `atoi` variants at all, actually... For reading input, I would lean on `Scanf` and friends. The `fmt` package is by far one of the most useful, powerful, and pervasive packages of the stdlib, and spending an entire lesson covering it will set your students up very well for future assignments. In my mind, the `strconv` package falls on the wrong side of "premature optimization" even in most production code because of the lower relative usage of the package when compared to `fmt`. **#5 - asserts** This is something that I think that tooling can solve, and that should not be solved in the stdlib. I would imagine a tool that looked for specially-formatted comments on/in blocks of code, injects the appropriate tests (which can then correctly do loop invariants, which can't be done with defer), and then runs your tests on the resulting code. **#6 - immutable map values** Pointers are fundamental to Go. I would restructure the curriculum to explain them before you explain either structs or maps. If anything, you should let your students think that structs are _always_ used by pointer, and maps _always_ point to structs (if the value type is not primitive) and methods _always_ have pointer receivers. You can explain later that this was an oversimplification, of course, but it's one that will actually serve them well when reading / writing "production" Go code, rather than teaching them or letting them use constructs that they won't see when looking around at publicly-available examples. **#7 - rand** You mention an `edu` package before, and I think that a new package might well be what you're looking for here, but not one in the stdlib, one specific to your course. **#8 - for** It's important for programmers to understand that code does not read like English or like mathematical proofs. I prefer the simplicity of having exactly one loop keyword; my C students were regularly tripped up trying to pick the right loop (while, for, do/while), and having only one choice just makes it that much more friendly. [edit: formatting]
You might want to take a look at [graphviz](http://www.graphviz.org/).
I already made a lot of researches in this area - sth like this does just not exist. The problem is mostly that tools like graphviz or plantuml are not very good when it comes to drawing custom icons. The main purpose was to draw a picture like you'd scetch sth on the whiteboard, using official iconset.
Thank you for the great feedback. I agree with you for 100% on the unsafe issue in flow_impl.go and I fixed as you said. So Flow keeps result as int32 and converts it into Result when that is required. No I have a feeling that piece of code is safe. Thanks. I agree with you that State allows developers to shoot in legs/arms/head/whatever else but I think Data() is still have the reason to live because the implementation provided by the library is simply a container with RWMutex but the contained data maybe concurrent safe or concurrent unsafe. So I want to give developers more control over the data contained in State. For example, someone can use map or slice as the contained and using Data() here can cause data races for sure so using DataWithReadLocker() and others should be preferred to use here. But the other developer may implement already concurrent safe data which has no need to use lockers so using Data() could be better choice here. Flow incapsulates context.Context and provide methods Complete() and Cancel() which can be used to signal cancelation. To say more Complete() and Cancel() can be called outside the flow (see example below). So I do not think that is a good idea to show incapsulated Context to the World. flow := floc.NewFlow() state := floc.NewState(nil) job := run.Sequence(...) go func() { time.Sleep(100 * time.Millisecond) // Time to cancel the flow flow.Cancel("Timeout") }() floc.Run(flow, state, nil, job) Sorry if I missed answering some of your comments. Anyway I always opened for ideas, comments, criticism, etc. And your feedback put a smile from on my face because that is really good feedback. Thank you again. Please feel free to contact me directly if you like.
Are you asking me or /u/martingxx?
There's a lot of thought put into this, that I appreciate, but I do disagree with most of the suggestions. For problem number 1, I would explain the differences between an executable and a library, and that 'package main' tells the compiler we're building an executable. I think having the ability to omit it will create more headaches in the long run. Number 2 is basically a bikeshed issue, but I don't think the reasons are compelling enough to have the whole Go community change vocabulary at this point. There's talk of [making int have arbitrary precision](https://github.com/golang/go/issues/19623) for Go 2, which solves the issue of number 3 in a different way (making them more distinct instead of similar), and one that I think makes more sense for real-world uses. The edu package can be written on your own, no need to make it part of the standard library. I actually like number 6. Does this works with slices? Would be good for consistency if so. I would solve 7 in a different way. I would have a separate goroutine handle all random number generation and have the goroutines that need it get the number through a channel, [example](https://play.golang.org/p/HxX1-npRHk). Number 8 is a bikeshed issue, but I can almost get behind it. For is used because of C's for loops, but C while statements are just if statements that repeat the block, and we already have initializing statements in if statements. In the end, though, I think it's not worth having the whole Go community change vocabulary for something like this.
My hunch is that "depth" is not correct. YACC (or probably bison) executes the code blocks after the rule has been recognized. declaration is left recursive, so it might be incrementing depth too late. It usually is not a good idea to shadow the state of a parser in a global variable. The cleanest solution is build the parse tree and then traverse the tree to perform the actions.
[removed]
"Yes, without inheritance."
I believe I failed to explain my point so allow me to try again. &gt; I don't understand why you are writing this. I already acknowledged that the package is popular. But programming is not a popularity contest. I didn't write this to showcase how popular the package is. The issue makes a point that pkg errors should be dropped from that library yet the author argues that it is a very convenient and popular package. So if we assume that pkg errors is a bad package because of all the reasons you mentioned, *"It's born out of an unreasonable rejection of debug logging, it encourages useless error messages (It equates "helps telling the user what went wrong" with "barfing out a stack trace") and with the language as it is, it just fails if not everyone is using it (and thus violates the whole "errors are just values" idea)."* and yet it finds its way so easily into libraries, then somewhere there's a problem right? More on this later. (To clarify, the way you described the problems of pkg/errors I tend to agree with you but then again I've never used the package so it is quite likely I am not seeing the whole picture.) &gt; Why not? The level of expertise doesn't seem to matter to me here. In general, the code of the stdlib and the tools by the go project itself seems, if anything, better suited for beginners, as it's of an above average quality (not saying it's perfect, though). Don't you find it strange that there's always the same questions that appear again and again from people that are new to the language at least here? "What's the best practice for X?" "What's the best way to do X?". Of course one can assume that the people that post these questions are just lazy. And I would agree if it was just a few cases. But they are not a few. Every other week it's the same questions that appear again and again. And this is going on for years. It's a pattern. And for me that pattern means that the ecosystem provides inadequate information or maybe it's hidden or otherwise not showcased effectively. No matter how you see it, sending someone an article that describes a good programming technique, teaching them how to do something properly, is much more effective and accessible than telling them to read the standard library and figure it out themselves regardless of expertise. &gt; The best practice is... Well thank you for explaining the best practice to me and the few people that will happen to read it. Does the general community know this best practice? I don't think so. And that's the problem! &gt; The basic issue here is, that people want a magical system that absolves them of the responsibility to care about error handling People want a solution. Nobody cares to show them how to do it properly. Cheney provides a solution. You do the math.
pretty sure that's what Go is for
I had a job interview a couple of years ago and near the end the Senior Engineer said "not at one point in this interview have you mentioned Objects". I expressed wonder at why a fast moving, forward looking organisation would even be focusing on OO, isn't the world action/event/reaction based? Suffice to say I didn't get the job - they're still blissful in their OO world, modelling abstractions and inheriting things. I'm in a Go and NodeJS world where things happen because things happen.
Also isn't 5 basically covered by tests plus a lib such as https://github.com/stretchr/testify/blob/master/README.md
I am aware, but that is a shell which can expand and may not have the desired affect with Go. 
What is the action oriented language?
Are you having a specific problem? I don't have any experience with Caddy. But for Heroku you must [bind your *web* process to the PORT environment variable](https://devcenter.heroku.com/articles/deploying-go#the-procfile). Furthermore, to do that with Caddy you'd use the [*-port* cmdline flag](https://caddyserver.com/docs/cli). So I'm guessing your Procfile would turn out something like this: web: caddy -port $PORT If you have other concerns, then you'll need to be more specific. Hopefully that helps.
To paraphrase Shakespeare, "First-class functions does not a functional language make."
that's definitely doing something, now it's saying &gt; error presenting token: Could not start HTTPS server for challenge -&gt; listen tcp :443: bind: permission denied Ug.
https://github.com/golang/go/issues/16968 there was a proposal to add it and Dave Cheney agreed with Brad Fitzpatrick that it should not be added. There's more useful discussion in https://github.com/golang/go/issues/18303
There is no such thing, go is imperative, structured programming language.
Oh I didn't know about this, thanks for sharing!
HEY FUTURE PEOPLE: Put "tls off" in your CaddyFile if you're using Heroku with SSL disabled, you're welcome
It's not that there's anything big wrong with it, just that it adds very little value. So little it's not worth having in the language.
I don't see how readability has anything to do with building the compiler locally. And yes, I build lots of Go compilers locally. $ ls -d src/go{,-*} src/go/ src/go-1.5/ src/go-1.6/ src/go-1.7/ src/go-1.8/ 
Depends on the definition of "to learn Go". (Yes there's no PL named golang). For some definitions, it could be easily be less than a hour (helloworld.go). For some other definitions it can take a couple of years. The thing is that for the later definitions Go is not much different to most other languages, but what makes Go IMO interesting is the low entry barier - after helloworld.go the journey to the first simple, but real program can be sometimes much shorter compared to some, if not most of other mainstream languages.
Do you have any ressource to look it up or a hint for an implementation? The idea of a parser tree is not new to me (tried it), but I faced several issues because of the limited number of non-terminals and the highly recursive grammar ... Depth is indeed my biggest problem - there's another branch (root_rewrite) which uses x and y (which depth and the number of the component in depth to add the component to). And as you said - under specific circumstances, this state is just not valid anymore ... 
I have to ask, how does this improve over `html/template`? I've looked at a few of the examples but it seems to roughly expose the same API as `html/template`. Slightly related, you're using string concatenation to build up paths, use [`filepath.Join`](https://golang.org/pkg/path/filepath/#Join) instead so it works correctly on all platforms. 
For state- I don't understand what value is provided by your API to users in Data() for values that are safe for concurrent use. You force access to your value through an accessor method that returns an interface which requires an assertion before use. Compare those drawbacks to: data := myConcurrentSafeData() As for Flow interface I suppose you won't actually understand my feedback until you build your first real world application trying to use it. Once you need to integrate with actual request workflows in http, grpc, net, all the places that accept context.Context the nuisance of your encapsulation will begin to surface. For each http request context you want cancelation respected for you will need to start a goroutine to call Flow.Cancel if needed.. which is also running in a goroutine. Not to mention we haven't even started error propagation which your library doesn't provide any of. Not a single error declaration in a library for coordinating jobs. So I have to make a channel to send errors too.. I have to handle synchronization of errors and handle how to deal with multiple errors occurring concurrently, requiring something like sync.Once, draining as well as deciding which error to propagate. Then upon receiving them call your Complete method with my error.. which all your Complete method does is call a context.CancelFunc and assign my Go builtin "error" to a eface which I then need to assert on after calling ANOTHER accessor wrapper for result fetching. I've now added a ton of complexity writing a shim for your library to work with standard Go patterns. All of this and what do I get? Some stuff in "run" package such as "RaceLimit()" to limit the number of concurrently running jobs.. at the cost of all these abstractions... yea no thanks. I'll prefer: limitCh := make(chan struct{}, 24) go func() { limitCh &lt;- struct{}{} defer func() { &lt;-limitCh } }() Now I have limiting and cause use something with proper error propagation like [errgroup.Group](https://godoc.org/golang.org/x/sync/errgroup) which accepts Context and is idiomatic easy to understand minimal and correct Go. Your code is not idiomatic or minimal, it also is not correct. Even your examples are full of [race conditions](https://godoc.org/gopkg.in/workanator/go-floc.v1/run#example-package--WithAtomic) like here showing a lack of understanding of the concurrency primitives this library is built upon- hinting this was a learning exercise for you rather than something purpose built from real world demand. That's great- this experience was invaluable and it's the only way to learn these things: through practice. Which is why I've taken the time to explain in the prior reply and this one as gently as I can the flaws in this library that miss the mark on idiomatic API design and concurrency. I've already implemented (much worst!!) versions of this and learned from my mistakes, so sharing this experience I hope you will not take as an attack on your hard work but an opportunity to expand on it. I'm sure this library will polish up nicely if you focus on using it from user space, integrating in real world problems and adapt as pain points emerge. Gopher on my friend.
If you hit the ground running or not 100% depends on your experience. Are you learning syntax or Go syntax? Are you learning concepts for concurrent programming or concepts for concurrent Go. Then how soon you begin writing [effective Go](https://golang.org/doc/effective_go.html) depends on if you are learning effective programming principles or effective Go principles. You get the theme here, it will vary by your past experience but generally speaking most Go programmers will agree the learning curve is pretty short in comparison to most languages due to languages fantastic minimal, yet concise syntax.
The questions are completely orthogonal. :)
So you replace switch which need fallthrough with if statements? Or go back to breaking every case?
It will be more verbose. What we need is type safe "struct tags", like attributes in C#
I'll try to have a look tomorrow evening.
RemindMe! 20 hours
What does this mean?
//go:generate go generate github.com/myhandle/myrepo/subpkg1 //go:generate go generate github.com/myhandle/myrepo/subpkg2 //etc
Are they profitable?
They're not setting the world alight, it will take a lot of effort for a consultancy in Cambridge to go out of business.
I know. I'm saying if you fix a bug in the generator and have like 10 of those generates to run it can be easy to miss one, and it doesn't always lead to a compiler error and it historically has been a slight pain to debug for me. Not the end of the world but not fun. It's kinda like dep management. Not so bad when it all works, but there are a lot of edge cases that can be a pain if you get caught in one.
Interfaces define what something can do, via methods, hence its action in Go. As a Java developer what helped me fully understand Go interfaces was to stop looking at the characteristics of an object (object/noun oriented), and start looking at behaviors and actions (verb oriented) to focus my thought process.
based on a cursory review of `tdoc.y`, i'm not sure why `root` is an array/slice. a generic root should be **an element** that contain all other top level elements (e.g. list) and all other elements (and blocks) should be descended from those (i.e. abstract syntax tree). evaluating the tree in order, depth-first, should produce the complete svg string. it's hard to tell from the grammar what is a definition and what is an invocation. i would consider making those more clear. for example, with something like: bar : vpc { EC2_Instance %1 } baz : vpc { EC2_Instance %1 } foo : cloud { bar %1 '-&gt;' baz %2 } $main : { foo "blubb" "quo" } assuming i recorded every definition in the registry, `registry["foo"]` will contain the root element for definition of `foo` (i.e. a `cloud`), etc. assuming i have a "runner" that takes a definition and a lookup table (i.e. registry), i should be able to do `runner.Eval(registry.Get("$main"), registry)` to evaluate the top level program. here's a good reference for a C version of something similar, the UNIX and Plan 9 [pic troff preprocessor](https://github.com/0intro/plan9/blob/7524062cfa4689019a4ed6fc22500ec209522ef0/sys/src/cmd/pic/picy.y) grammar.
Oh. /u/martingxx :)
This would solve all the problems I've had with wanting goroutine-local storage. The performance of a program with a lot of those may be difficult to optimize, although since they are a distinct type of variable I may be willing to pay. (You might get some O(log n) slowdowns on accessing or writing these things, using a simple implementation that springs to mind. There may be more clever ones, though I doubt you can get this without penalty.)
Oh wow, I'm very happy that my blog post generated such a grand response, thanks a lot for that! I haven't fully read it yet, I will likely write some thoughts when I read it. This definitely belongs to the experience reports as well and I'm putting it there myself right now!
For a language to be considered functional, it needs a lot more that first-class functions. Functional languages are immutable whereas Golang is mutable. I guess I should note that the amount of immutability depends on purity of the language. Haskell, a purely functional language, is entirely immutable (with the exception of using Monads). Rust, on the other hand, is immutable by default but can be "turned-off" per variable. There are other concepts that apply to functional languages but, in my opinion, immutability and the degree of it is a major concept that separates functional from non-functional.
It's not really OOP in a strict sense. It has some overlap with traditional OOP languages and combines an interesting mixture of OOP concepts as well as several features common to other language paradigms (FP, class imperative, etc.)
I think it could be done with each `dyn` being an index into a copy on write slice or something like that. So reads would be constant time, writes would be O(n) though. That's probably acceptable as the values would not be updated often per request and there should only be a handful per program. The language would then maintain a "stack" of these per go routine.
To be able to write stuff fairly quickly but not necessarily well, super quickly - like, hours to days.
I don't use an API framework in go, standard library has everything you need for developing an API server. ORM: https://github.com/jinzhu/gorm
Is this similar to other templating systems? It looks a bit like Handlebars. 
Hi /u/daenney thanks for pointing that out. I'll have to make changes and use `filepath.Join` to set a good example :) `html/template` is already an amazing package but I find it tedious to type all the filenames if I have template definitions in separate files. So, with [plet](https://github.com/steven-ferrer/plet), i just have to put them all in one folder and it will manage to find all the template definitions. By the way, I separated the template definitions in [examples/tmplt/content/simple](https://github.com/steven-ferrer/plet/tree/master/examples/tmplt/content/simple) folder to make my point clear :)
No its not, its just a wrapper for [`html/template`](https://golang.org/pkg/html/template/). `html/template` is the standard template engine for Go. If you are looking for other template engines, then you might wanna check [this](https://github.com/avelino/awesome-go#template-engines) out.
It's always better to start with the standard library because it gives you chance to learn the language. Please see [this](https://medium.com/code-zen/why-i-don-t-use-go-web-frameworks-1087e1facfa4) blog. At first, I also wanted to use a web framework, but as others say, stick with the bare metal until you get a good understanding of the language. But if you really are into web framework, then you might wanna try [Echo](https://echo.labstack.com/). Oh, and about the ORM, since you're going to use PostgreSQL, then you might wanna check out [go-pg/pg](https://github.com/go-pg/pg). It's focused on PostgreSQL.
You may need to write the keys to disk and restart the server. I'm not aware of any server out there that can do this live as it could cause all kinds of cryptographic race conditions.
Just remember that most companies are in it for the profit, not the tech. 99+% of the world is doing business NOT in Silicon Valley.
Are there any other differences? I find the directory issue can be solved easily with a glob.
Check out go buffalo as well just to see what is out there, but as others have said I suggest learning most of the Std lib a bit then branching out. Gives you a sense of what is possible without learning a third party api
You need to use the [GetCertificate callback](https://golang.org/pkg/crypto/tls/#Config) of a tls.Config and manage your own certificate cache.
Root contains only a reference to other components - it's just a kind of help to know on which component I have to append another component. vpc foo { EC2_Instance bar } The component foo will be used to add another component bar and creates a tree in this case. The root_rewrite branch follows the same idea, but multi-dimensional. The the end I have the program component, which is itself 1 component and is the first node for the abstract syntax tree. Thanks for the reference and your thoughts - will have a look!
You can just add cert to tls.Config and call ListenAndServeTLS with empty strings (https://golang.org/src/net/http/server.go?s=85702:85770#L2855). If server is already running, you need to implement GetCertificate like https://godoc.org/golang.org/x/crypto/acme/autocert#Manager.GetCertificate
Where would the slowdown come from? The kind of implementation I had in mind (though I am not really familiar with how TLS works) is to, in essence, have the compiler (who'd know about all dynamically scoped variables, as they can only be package-level) generate something like type bucket struct { data unsafe.Pointer next *bucket } type gls struct { net_http_DefaultClient *bucket log_DefaultLogger *bucket // ... } and then add a `*gls` to each `G`. When a `G` is scheduled, it's `*gls` is put into some thread-local variable. Reading and writing would then be // c := http.DefaultClient c := (*http.Client)(gls.net_http_DefaultClient.data) // http.DefaultClient = c gls.net_http_DefaultClient := &amp;bucket{unsafe.Pointer(c), gls.net_http_DefaultClient} defer func() { gls.net_http_DefaultClient = gls.net_http_DefaultClient.next }() Now, from what I can tell, both operations are O(1). There are things I am ignoring, though. Like the overhead of allocation, effects of caching or the actual implementation of TLS. But even with all of those, I still believe that this will essentially revert to something like adding an offset to a base-address. The base-address will come from TLS, but if it's hot will probably stay in a register. I'm not *terribly* sure about all of this, but I think given that the set of `dyn`s is statically known, I don't see where the added (computational) complexity would lie. Am I overlooking something? [edit] hrmpf. Of course I overlooked that this would still require a copy of that `gls` struct on every goroutine-creation. I assume that it wouldn't be terribly large. It only keeps a bunch of pointers, after all. But that would be interesting to quantify (e.g. how many package-scoped variables does the average go program have currently). Good to keep in mind and keep thinking on.
Question, does "go version" return the expected results in your terminal? I just ask because windows can be finicky sometimes, and if the issue is windows finding the binary in the path, that should be looked at first. I've found sometimes opening a new terminal window helps, other times a full reboot is what takes to get it working. 
Also, it's nice to see another Windows Gopher. 
This was also what I had in mind at first. In the [sibling-comment](https://www.reddit.com/r/golang/comments/6ti7ox/why_contextvalue_matters_and_how_to_improve_it/dlldjpq/) I described a pretty simple scheme of how reads and writes can both be O(1) (and a pretty small constant), at the cost of having goroutine creation be O(n) (with n being the number of `dyn`s used). It is this O(n) which would be interesting to optimize and COW pages might help. But I couldn't come up with a really good scheme which would not revert reads or writes to O(n). If I would ever seriously propose this, I would probably do a lot more thinking about this beforehand - maybe even implement a POC. For now, I think I'm reasonably confident that this could be made cheap enough to be worth to keep in mind :)
Oh right, thanks for pointing that out /u/joncalhoun. I have defined a type called `Templates`. this is just a map that holds the collection of templates. So, when you do `templates.Add(someTemplate)` it uses the base directory name of the content dir of `someTemplate` as the map key so that you can do `templates.Get("myTemplate")`. Please see [this](https://github.com/steven-ferrer/plet/blob/master/examples/templates.go) example. __EDIT__: I just realized that my explanation is unclear so I re-edited it. Also, by default it caches the compiled templates so that it wouldn't need to recompile the templates every time. But when developing, you would want your templates to re-compile every time you run `Template.Execute`. This can easily be done by setting `Template.HotReload` to true. 
No, go isn't in the $PATH, so any `go xxx` doesn't work. Still VS Code is able to do `go get` so I'd like to do it too.
+1
As Bryan mentions, the reason for sync.Map to be included in the standard library is that they wanted to use it inside the standard library. I do wonder why it’s not an internal package, though…
The dispatcher seems to be a potential bottle neck of GoWorld. Why use such design?
If the go binary location isn't in $PATH then your machine simply has no way of knowing where to get it from. Maybe VSCode has some built in locations that it tries if it is not in $PATH, I don't know, but you need to add it to yours.
Right, and how is this package different from tens of others, that do exactly the same?
Perfect! Thank you :)
A very intriguing concept. I only wonder how to solve the problem of replacing package-scoped variables with the dynamically scoped variables introduced in the article. Package-level variables are omnipresent throughout the standard library and in many 3rd-party packages as well. I cannot see an easy way of switching all of them to `dyn` variables. But still, the idea of `dyn` variables is indeed very convincing.
Yes, it would clearly be a breaking change, a priori. Which is why I wouldn't reuse the keyword. To make a clear distinction. I just wanted to make clear that a) we can get the same *benefits* as package-scoped variables by emulating them and b) thus buy back some of the complexity introduced. Take a keyword, leave a keyword, so to speak :)
Ewwww, gross. I almost forgot that stupid thing existed. Thanks a lot for the nightmares I'll likely have tonight.
I remember there used to be an idea that uses the import paths to get the packages which are instantiated on-demand from the remote server (LaaS/library-as-a-service?). That probably cannot work with the typical code hosting services out-of-the-box though. The creator of Erlang had a similarly interesting idea as well: http://erlang.org/pipermail/erlang-questions/2011-May/058768.html Not necessarily related to Grizzly, but I couldn't help but be amused by those rebellious ideas.
&gt; 2\. Rename “slices” to “lists”. The behavior of slices can be surprising when the underlying mechanisms are not fully understood. Think of the semantics of `append`, or the role of the slice capacity, or the fact that `byte.Split()` does not return new slices but rather adjacent slices of the same array, where `append`ing to one of the split slices can overwrite the adjacent one. Renaming slice to list would only obfuscate the nature of slices and would make their behavior just more surprising. And slices do not necessarily have to be introduced at an early stage of a Go course. I currently work on creating a Go course, and the first two sections do not require slices at all, yet enable the students to create a small command-line tool and build functions and packages. Then in section 3, arrays are introduced, then slices. All builds up naturally, step by step. &gt; 3\. Make int be a type alias to intXX (where XX is the machine word size) This leads to a very serious problem. Code like var x int var y int64 x = y would compile and run fine on a 64-bit machine, but fail on a 32-bit machine. This is the reason why `int` is intentionally *not* an alias type for `intXX`. (Edit: fixed numbering)
I don't live in Silicon Valley. If a company tells you that they're a fast moving, forward looking company in an interview, then they need to actually be it, not just say it.
Which is exactly why abstractions are a thing. There were several mistakes in a comment that kept saying Go's great because it makes code easier to write -- using broken code as an example. 
I would like to comment on the reason why you discard statically-typed context. You say that it makes impossible to write middlewares but that sounds like a type system limitation that could be solved. For instance, one could devise a way for an authentication middleware that writes into a CtxAuth structure to be wrapped around a handler that uses a static context that embeds an anonymous CtxAuth field. Or something like that. Another way to express it is that the goroutine local storage (aka Context) is an aggregate of structures of different types, and each middleware can access one (or more) of those structures. I'm not sure how the syntax would look like and how much can be checked at compile time vs run time. FWIW,I don't like dynamically scoped variables, they look very implicit and magic to me, it's very hard to read the code and understand how the variable will mutate as the functions are being called. I expect that refactoring would be very hard.
Thanks. Unfortunately, I don't quite get your joke as well as I wished. I would appreciate it if you could elaborate a bit further.
&gt; You say that it makes impossible to write middlewares but that sounds like a type system limitation that could be solved. I'd be open to suggestions. I couldn't find a way. &gt; For instance, one could devise a way for an authentication middleware that writes into a CtxAuth structure to be wrapped around a handler that uses a static context that embeds an anonymous CtxAuth field. I'm not sure I understand. But it would, at the very least, require the framework (say, `net/http`, or grpc, or whatever) to prescribe what can and can't be passed through it (in your example, it would need to define a `CtxAuth` struct to embed. And the thing it embeds into would need to have a finite, predefined set of fields). So I don't see how this would solve the problem of passing data through an API, without the API knowing anything about it. It is very hard to talk about these kinds of things without specifics. Which is why I tried to attach (even if only pseudo-) code to my examples and also talked about specific semantic and implementation questions for the actual design. It would be helpful if you could come up with some kind of syntax and semantic description to express your idea. &gt; FWIW,I don't like dynamically scoped variables, they look very implicit and magic to me That is a fair standpoint, but I believe these to be inherent properties of the *problem*, not so much the solution. i.e. the problem *is* exactly "how can me pass data through APIs implicitly". By definition, if an API explicitly mentions the type of data passing through it, it won't be agnostic to it. I also object to the word "magic". It seems like a catch-all phrase to dismiss things not fitting ones taste. You are typing on plastic buttons, swapping the color of tiny squares and sending fluctuations in the electromagnetic field through glass fibres along thousands of kilometers. We are already firmly in the realm of magic and seem to be able to adjust to it just fine. Dynamic variables don't seem any much "magic" to me than goroutines, a GC or… literally anything happening on your Computer right now :) &gt; I expect that refactoring would be very hard. TBH, I don't see why.
Since when adding features is only for increasing adoption?..
Try adding Go to your path manually? By default it's installed to C:\Go, is it there? If so you can add C:\Go\bin to your path.
Thank you, I will. Shame VSCode doesn't allow automatic installation of missing libraries though.
Why would it? It is a code editor, not package manager for Windows.
It automatically gets a lot of stuff though. Like goreturns and stuff. The go extension could also look for used libraries in code and automatically add them.
Well, that isn't done by VSCode per se but by the Golang extension. Also, I wouldn't like for an extension to compile random code from the internet because I typed an import path.
The reason that it works in VS Code is because the vscode-go plugin has [some heuristics to try and find the `go` binary. If all else fails on Windows, it falls back to `C:\Go`](https://github.com/Microsoft/vscode-go/blob/224c330327124c965cc9f9075d9a410181587cb4/src/goPath.ts#L91-L96), which happens to be the default install location. So unless you've installed it somewhere else vscode will find it just fine, but since you've not added it to your `$PATH` any CLI based interaction won't work.
Viper, https://github.com/spf13/viper Just a good solid config file parser. It might be a overkill to be a part of the standard library, but just a subset of it to read a file and parse it with ease.
Giving you an upvote just because you make a good point, but the solution to this is what forfunc and neoasterisk say.
Why can't anybody first layout the equation we try to solve ? What are the requirements we try to match and the constrains we have to respect ? This would be a tremendous help in evaluating how the actual Context is relative to that, and new very original proposals like this one. 
I'm pretty sure you want O(1) goroutine creation with penalties on the read and write of dyns instead, because the average goroutine will write only a few dyns if you keep them small, as they are designed to be. You need to plan for programs to contain many thousands of dyns, so O(n) on goroutine create is way too big. Reads and writes to dyns in that case can easily be O(log n) by having the linker give each dyn a unique id and using that as a key into an immutable tree for each goroutine. (Immutable tree keeps goroutine creation Of(1).) For efficiency's sale, in the common case of 10 or fewer distinct writes within a goroutine, a simple key/value array can be used as searching a small array like that on a modern system beats anything else. This optimized for the common case correctly, I opine, but won't break down if you do use millions of dyns within a single goroutine. A ton of writes would generate more garbage, but I think we have a good "don't do that" argument. The remaining problem, which may scotch this whole proposal regardless, is that in Go, x.y = 1 is not a write to x. Users do technically already have to know that, but this _really_ puts it in your face and make writing reached conditions really easy. 
&gt; I'm pretty sure you want O(1) goroutine creation with penalties on the read and write of dyns instead I think that very heavily depends both on the constants involved and the size of n. In particular, `context.Context` has O(1) copy and writes and O(n) reads and is pretty much universally disliked for being inefficient. That's why I would like to get some quantifiable data here, for example "how many package-scoped variables does a normal go program use". For example, some COW magic as an optimization will likely involve at least one context switch. From what I can tell, that is already a pretty comprative cost to copying over a couple of KB, enough to store several thousand `dyn`s. What effect dominates, in the end, will heavily rely on the actual access-patterns, how often new goroutines are created, how many variables will be used and all kinds of things. (also, all of this is pretty irrelevant, I believe. The question is "can this be implemented with reasonable performance" and I hope it is at least believable that the answer is "yeah, very likely")
There is dynamic dispatch, polymorphism and encapsulation. I don't think that Go is missing anything in terms of achieving the goals of OO, which were about scaling software development by allowing programmers to reason about smaller chunks, which were objects, and compose them into a larger solution. Inheritance is just a problematic form of code reuse.
The SysV IPC system gave me problems the last time I had to work with it, years ago. Hopefully it is working for you... good luck.
I'm going to use them on a project and I was wondering if I could know the kind of problems you encountered so I could look out for them before they happen! :) One challenge I can foresee is the maintenance of the queues. Queues could grow very large (number of queues and number of messages in them). We have to remove them, otherwise they consume resources.
Currently, we are using https://github.com/graphql-go/graphql for a project. So far: - Working good - Documentation non existant - But some examples are there - Only drawback so far: The GraphQL-Error-Object is not customizable as far as I can see. You can just set an error string.
So I want to callout in a separate message thread here that there is another thing I really like about this proposal, which is that it effectively eliminates the scourge of global variables, while retaining most of their advantages. As Wikipedia says, there aren't a lot of languages that implement this, but there is at least one in the "B-list" languages, and that is, of all things, Perl. The history of the `local` keyword in Perl is sort of interesting abstractly but mostly irrelevant for our discussion here; what's relevant is that it implements dynamic scoping within Perl. Perl's dynamic scoping is much simplified by the fact that it doesn't have to consider dynamic scoping in a threaded environment, so I believe it's implementation boils down to adding a bit of data to the local scope to reset the variable when the scope is exited. But it does mean I've had some experience with it, and even in a single-threaded context, as imperative programs become large you lose confidence in the nature of your environment in a way that is not entirely dissimilar to the way you may not be able to trust your threaded environment, because "a different thread you didn't account for writing to a variable" and "a function you didn't realize was writing to a global" have similar effects at scale. And one of my favorite Perl features (a language I mostly dislike) is that the `local` keyword means that anything anybody carelessly left as a global, I can carve out a scope in which I know what the value is, and that any code within this scope that may think it can modify it can't escape that local scope. It means that, in practice, Perl doesn't truly have global variables in the traditional nasty sense. When you have dynamic scoping, instead of global variables being _evil_, they're just another tool. They work particularly well with closures precisely because you don't have to add "contexts" to anything that takes a closure, which is particularly important because as nice as `context.Context` may be, it's not always what we're looking for. I've encountered this in Perl as well. (It's one of those things that is hard to see in a blog post because if the code is simple enough to fit in a blog post you can always just add the context in. But as the program scales up, that's not always possible.) You can still get a bit of action-at-a-distance where a something early in the call stack changes a variable, then we go down several calls where none of the arguments mention this value, and then a user gets an unexpected value. But at least the culprits for such an issue are still isolated only to the currently-active scopes, not "the entire program and its entire execution history" like a global variable. This is further mitigated by the fact that there isn't a "global" space that everyone shares in the first place, there are only packages, which eliminates the sort of accidental collisions you might get in C or something. I'd still suggest a community "best-practice" of not going crazy with these values and preferring explicit passing of values within one's own code. But when for some reason that doesn't work, which still comes up with some frequency, this would be available. I would fully support eliminating the current concept of global variable from Go 2.0 and consider that as an independent positive aspect of this proposal. I'd also observe that dynamic values can be _added_ to the 1.0 series without a problem that I can see... it would be 2.0 that removes non-dynamic globals. And that, on its own, would not be a _huge_ impetus to make a Go 2.0, IMHO.
Problem with that is we would break compatibility with 1.0, and as such split community, queue the python divide, we don't want that.
That is a nice question if you are in academics: The business of sorting, classifying and partitioning things; attaching labels producing taxonomies; coming up with novel criteria to judge things. For everybody else: Does it make any difference whether the answer is "Yes" or "No"? At least not for me...
I'm not so much in favor of removing `context` as I am in separating its two responsibilities for (1) request-scoped data, and (2) lifecycle management, i.e., cancellation. I don't understand why people believe the two belong together. It's like they took Java's Servlet Context or Ruby's Rack `env` and just bolted lifecycle management on to it. Maybe someone can shed some light on why these two responsibilities belong in the same API? At any rate, I'm partial to this recommendation of dynamically scoped variables for request-scoped data because it does address the type-safety problem I have with `context.Value`, however, I'd have to use it in practice before I formed any strong opinions.
Go is not a functional language.
What exactly breaks 1.0 compatibility? I already marked removing the current idea of removing global variables as a 2.0 issue. Presumably 1.0 modules would still work the way they do even after that; I'm sure this won't be the only thing removed from 2.0 but still supported in 1.0 compatibility (see also `new`, for instance). Fortunately, maintaining the "traditional" global variables for 1.0 compatibility is virtually no effort going forward. (I long assumed that Go 2.0 would simply support using 1.0 modules directly; recent comments from the dev team seem to back that this is the plan. Go has a _lot_ more information about what is truly going on in its modules that Python does, so it has a _loooot_ more options for having things directly interop without loss. I really wouldn't spend too much time worrying about a Python 2/3-type split; that was really brought on by the dynamic type system more than anything else if you deeply analyze the problems. The only place where Go has to look out for that is in the reflect module, which will have to handle both 1 and 2 at once, but even so, a lot of the changes we're discussing won't affect that. This one possibly included; it would affect the reflect module if you want access to the internal structures so you can crawl up the dynamic value stack, but if you don't want that, and I'd tend to consider it a bad idea, it doesn't affect the API of reflect at all.)
I've made a comment before that one of the reasons a lot of Go programmers don't see a lot of need for generics is that if you're writing a network server, a common use case for Go, you're often never more than one Marshal or Unmarshal step away from having a []byte in hard anyhow. If you're taking in a network stream and Unmarshaling it via any of the several methods of unmarshaling into a data structure, doing some stuff to that data structure, and then sending it back out, you don't do a lot of stuff that calls for generics. It looks like this use case gets a similar benefit. If you can represent everything with the `Stream` interface, you end up skipping around the generics issue. However, in practice, using this style of code in "general programming" will rapidly ram up against the missing generics, because in general if you've got a pipeline like that, you're going to be wanting to change types as you go, even something as simple as "turn a stream of bytes into a real `string`", and then, alas, it's way less fun in Go.
&gt;would my page automatically be available at http://123.123.50.50:8080/hello Yes ...how do you get it running on port 80?
It looks like a very interesting idea. I takes time to fully understand it and it's potential, or possible problem. 1. It clearly solves the context and cancel problem. 2. It could be added to a 1.x Go version because it is backward compatible. 3. These are global variables preventing side effects. ...(too long to list) So there are many +1 with this proposal. Searching for -1, the best I could find are below: 1. tempting to use global variables where it could be avoided; 2. code behavior harder to interpret because of the context dependency; 3. function behavior not fully parameterized by its arguments (context are hidden parameters); However, this is the best proposal I have seen so far. I also hope that the := behavior will be modified so that your example could be written as done.C, cancel := done.CancelFunc() defer cancel() 
Thank you! What did you base your decision of going with GraphQL &amp; Go on?
You can escape the period in the numbers so they appear properly: &gt; 2\. Rename “slices” to “lists”. ... &gt; 3\. Make int be a type alias to intXX (where XX is the machine word size) 
Thanks! Fixed.
From now on, whenever we say, "Go is about composition" we should link to this article.
That's a very good use of higher order functions. As for using it in your game library, I could see it possibly being useful for animation by transforming, composing and sequencing streams of vectors.
I can't quiet grasp all this code, but it fascinates me. To a neophyte it does indeed look like magic. 
&gt; Of course one can assume that the people that post these questions are just lazy. A *lot* of people who call themselves programmers these days really want answers served to them pre-chewed. They're not lazy as such, as they're willing to spend two hours arguing that they *should* be served pre-chewed answers -- especially for problems where building up understanding would take about 15 minutes of reading. &gt; But they are not a few. Every other week it's the same questions that appear again and again. There's a *lot* of those people. Something happening a lot != something is desirable, sustainable, etc.
LDAP is a great example of something that's way too complicated to go in the stdlib. (fwiw, I've implemented the LDAP protocol from scratch.)
Please don't generalize about and insult our fellow programmers. 
playlyfe/graphql claims to be x10 faster than node js graphql. I've been using both and can see that using graphql with go seemed a little easier to me, but that's just me ;)
The standard library CSV package implements RFC 4180. For variations and other stuff not included in the spec you'll either have to do [some manual work](https://github.com/golang/go/issues/12755#issuecomment-150670079) yourself or look for a 3rd party CSV package like [this](https://github.com/gocarina/gocsv).
Good point! Thought about this too and found [this benchmark](https://github.com/graphql-go/graphql/issues/119) that showed Go was slightly faster than Node.js and failed less overall. The question that comes to mind tho is - what is this speed advantage good for, if say a REST or database call is the limiter, slowing down the whole operation?
When you tie together multiple (let's say 100) streamers for playback, won't that risk causing audio delay as it processes all of the changes before it outputs? Is there a plan to add "compiling" chained streamers to avoid this?
Or they could just read golang.org/pkg/strings and get used to the godoc format?
I think you're completely right that the static nature of Go means whatever situation we end up in, it'll be much better than Py2/3. Speaking for myself, personally, I'd love it if there was a completely reliable Go 1-&gt;2 transpiler, so I could just convert my code and be done with it. And hopefully, there'd be enough community momentum that everyone else would do that too. And pretty quickly we'd all just be for the most part living in Go 2 land, without having to worry about Go 1 much or context switching a lot. Too optimistic? Maybe, but again the static nature of Go makes it at least conceivable. So in that scenario, if we added `dyn` and killed package-level variables, we'd need some way to handle them when transpiling. But if I understand the proposal correctly, it's pretty straightforward, though maybe kind of messy -- just create a new `dyn G struct { ... }` that contains all of the previously package-level variables.
Hi! I will be glad to get Golang specific feedback as my primary language is PHP.
Does `dyn G struct { &lt;package variables here&gt; }` work as a mechanical transformation of package-scoped variables? Maybe that's too ugly to want to apply to the entire std library though...
I don't think I understand. What do you mean by 'tie'? Sequence or something else? All of the streamers are as lazy as possible, which means they only ever process as much data as necessary for the given Stream call. What do you mean by "compiling" chained streamers?
I just use this: https://play.golang.org/p/RyqIVv-0vP the commented out line is if you have CR+LF line endings. You might also want to look at https://aadrake.com/posts/2017-05-29-faster-command-line-tools-with-go.html if you need to parse super large files.
Process the entire output and store it with the composed modifications to avoid having to process all of those modifications at playback time By tie, I mean compose. Related, how do you store a set of modifications? It looks like you would need to write a function like this: func ModifyStreamer(s beep.Streamer) beep.Streamer { s = effects.Gain{ s, 2, } s = effects.Pan{ s, 2, } ... etc return s } But if we have 100 effects to compose and we want to apply this to a number of streamers, that's going to take a good amount of processing time. It's also a lot of code to write.
No, but something like this: // note the pointers, for mutability. dyn G = &amp;struct { &lt;exported pkg variable declarations&gt; }{ &lt;exported pkg variable initializers&gt; } // separate struct for local variables, to prevent copying/assigning. var g = &amp;struct{ &lt;unexported pkg variable declarations&gt; }{ &lt;unexported pkg variable intializers&gt; } And yes, I agree that it would be too ugly :)
Ah, sure, that's possible (if I understand correctly) with Buffer. You can take any Streamer (composed of whatever) and append it to a buffer. buf := beep.NewBuffer(format) buf.Append(streamer) Then the buffer contains the pure processed data and you can stream it arbitrarily. And yeah, that's exactly how you store a set of modification. I don't think it's a problem, since you usually don't have 100 effects, just 2-5. Alternatively, you can always write a 'combined effect' yourself, if you encounter performance problems, but that's unlikely. Also, if you want to apply an effect to a group of streamers (e.g. SFX volume), which will be mixed, you can use a beep.Mixer, which dynamically mixes stuff and then you apply the effect once to the mixer.
Well done!
If there's anything specific you don't understand, I can try and explain :)
The more I read your post, the less I understand what it is you are trying to do. What do the newly generated certificates have to do with your HTTPS server? You shouldn't be trying to change the certificate of the server after it is generated.
What's the advantage of using composition over functions, though? If you used a function instead of a struct, then you could chain together arbitrary modifications with other functions and store it as a variable. 
Some effects are just function constructors, some are structs, it depends on whether the effect can be changed at playback time. If it can't, a function constructor is sufficient, if it can, you need to be able to change the parameters, so a struct if necessary.
So structs are used to modify the input variables after construction? Doesn't that assume that you know the exact composition of your Streamer, and you type cast them to the order they were composed in? You could get around that by casting to interfaces that just contained setters, but I don't see such interfaces in the project. That would also only let you modify the outermost layer of effects (whichever struct first satisfied the interface).
I need a good introduction to the design pattern your discussing. I love Go and find functional programming very interesting. It feels like I'm being introduced to pointers for the first time - do you remember that feeling. When I'm at my laptop I'm going to code up your examples and that will help. An introduction or perhaps coding itinerary for learning these skills would be greatly appreciated. 
&gt; It's susceptible to name collisions due to requiring a global namespace. Are you talking about the key passed to `ctx.Value` or something else? If you are, than it's only susceptible to name collisions if misused (e.g. `ctx.Value("myKey")` is wrong). The documentation clearly states "packages should define keys as an unexported type to avoid collisions" (e.g. `type key int; var myKey key = 0; ctx.Value(myKey)` cannot collide with anything outside the package). I'm not at all a fan of context values but I'm more a non-fan of FUD spreading.
It assumes that I know the exact composition, but no typecasts are necessary, I just save the appropriate streamer in it's struct form. For example: ctrl := &amp;beep.Ctrl{Streamer: music} volume := &amp;effects.Volume{Streamer: ctrl, Base: 2} speaker.Play(volume) Now, I can retain access to `ctrl` and `volume` to pause/unpause and change the volume of the music. Of couse, I don't have to retain them as simple variables, I can make them a part of a more complext 'sound architecture'.
Thanks
I don't like the idea that users need to store as many variables as they have compositions they want to modify in order to modify said compositions. This seems like a disadvantage as opposed to an advantage, because if you were using functions you could just reassign them with the new variables immediately, without storing knowledge of the previous composition. 
I don't see how it could work with just functions. Could you explain what would it look like? (Changing the parameters such as volume and pausing during playback just using functions)
 type EffectStreamer struct { effects FnEffect source beep.Streamer } You'd `And` together multiple effects, and the stream function for the resulting struct would put the source into each of the effects within it, in order, at each stream call. You would probably want a mutex around changing `effects`, but there would be no work processed during that reassignment (it would just be lock, set, unlock), so it wouldn't cause delay. `And`: func And(fns ...FnEffect) FnEffect { return func(s beep.Streamer) { for _, f := range fns { // whatever the stream logic is, I'm not as familiar with your syntax } ... } } I'm also not sure the lock is needed
&gt; The `SeqFunc` is a function which satisfies the `Seq` interface Shouldn't this read "is a type which" ?
Hm, this is actually not a bad idea at all! I'll definitely think about it. ~~This probably can't work for effects that change the number of samples, such as Resample, but can work for other.~~ Btw, a nice thing is, that this is not so hard for the user to do themselves :)
Instead of using an all or nothing system with all dyns stored in one block, one could split it in smaller blocks and group dyns that are modified together. There would be an optimally filled btree like index to these blocks constructed at compile time. There would never be insertions or deletions in this index because dyns are all known at compile time. COW is used when a leaf or node is modified. A dyn modification would imply duplicating the leaf block with the modified dyn and all index blocks that must be modified. The coroutine holds a pointer to the index root node. The dyn reference is a value that defines a path in the tree. It is a constant where group of bits correspond to the index in each index node. So accessing a dyn value requires to traverse the index. The performance could be adjusted (at compile time) by setting the depth of the btree and the number of dyns in leaf nodes to different values. By increasing the number of values per node, we increase the cost of a write and reduce the cost of read. Start/stop coroutine remains always the same and optimal since it implies only to copy a pointer to the index root node. I'm glad there would be is such price to pay for the dyn variables because it would incite to minimize their use. 
&gt; If you are, than it's only susceptible to name collisions if misused Exactly. It is susceptible to being misused. (Honestly, it's not my argument, really. I was citing the author of the original post and was trying to cast as broad a net over the criticisms as possible)
That's right, thanks! Fixed to '...`SeqFunc` is a function type...'.
Really, it doesn't matter. :) There is no point in optimizing performance until there is any indication that performance is a problem - so far, there is none. Even with the naive implementation I outlined, the overhead on goroutine creation would be pretty minimal (on the order of a couple of percent, with O(1000) variables, based on some guesstimates and naive benchmarks). This is a non-issue. Moreover, there is no point in talking about optimizing performance until someone is starting on the implementation work. Which won't happen until the proposal is accepted. Which won't happen until it exists. Which won't happen for probably at least one or two years. :)
Or, for the case of audio, a `[]byte` stream or a `[]uint16` stream
This is great work, but I do wonder why one wouldn't instead use gRPC and be able to communicate across many more languages. Edit to postulate: It's probably the "no deps" part.
[This](https://github.com/maxim2266/csvplus) one can handle headers.
Thank you. Things are a bit more complicated on PHP's side since you have to configure your environment with a proper extension version (including protobuf). I was looking for an easy to start solution which can be altered later on PHP side, haven't found the light one. :( Edit: yes, "no deps"
Why would someone use this/gRPC rather than using a message queue like rabbitMQ? Genuinely interested because I've never looked into (G)rpc.
Easy to read and understand, thank you. The godocs are useful, and important but sometimes just having a well thoughtout explanation can break a mental block in understanding and using something.
ah ok, so `dyn` is immutable, that makes sense i think. i was imagining it would be mutable but assignment would be special and push into the stack.
I can't get it to compile. When I run make, it says: go tool: no such tool "yacc", so I've got no idea what to do. Instead, I'll summarize what you might want to do. This is your basic grammar (I noticed missing semicolons BTW): program: statement_list; statement_list: statement | statement_list statement; statement: declaration | relation_assignment; relation_assignment: TEXT RELATION TEXT | TEXT RELATION declaration | declaration RELATION TEXT | relation_assignment RELATION declaration | declaration RELATION declaration; declaration: COMPONENT IDENTIFIER | COMPONENT IDENTIFIER ALIAS TEXT | declaration SCOPEIN | SCOPEOUT; There does not seem to be a relation between {, SCOPEIN, and }, SCOPEOUT. That's weird. Normally, when you parse, that's the most important anchor. I think your grammar would accept }}}}}. From what I get from your examples, you actually want something like this (but note that I don't know what TEXT, RELATION, COMPONENT and IDENTIFIER mean): document: declaration_list { document = $1; }. declaration_list:{ declaration_list = nil; } | declaration declaration_list { declaration_list = concatenate($1, $2); }; declaration: COMPONENT IDENTIFIER { declaration = MakeTerminalNode($1, $2); }; COMPONENT IDENTIFIER SCOPEIN declaration_list SCOPEOUT { declaration = MakeEmbeddingNode($1, $2, $4); }; That should give you your tree, which you can then process.
I would assume that the speed advantage would affect hardware decisions and overhead if you want to think that far out. Faster performance can have a positive impact on available resources.
In that case, it seems that an HTTP API would be most pragmatic. Were there other considerations (or something that I'm looking past in general)?
The one string function I want for a long time still lacks: strings.Clone(s string). This function can be used to avoid the kind-of substring memory leaking problem. Now I can use string([]byte(aString)) to clone a string, but it needs two memory allocations, one is a waste.
Seperating the example and the output is kinda irritating with 8 examples. With just true true true false as output its annoying to see what true or false is corresponding to what example.
Thanks, I'm gonna change the Makefile. Go tool yacc has been removed, so goyacc has to be used in your version of go: https://godoc.org/golang.org/x/tools/cmd/goyacc (Just *go get golang.org/x/tools/cmd/goyacc* and change go tool yacc to goyacc - my local version is a bit messed up compared to github due to all my experiments ...) You can use sth like EC2_Instance "This is an Instance" as ec2 In this case, EC2_Instance is the component, "This is an Instance" is TEXT and ec2 is the IDENTIFIER. RELATION is if you do that EC2_Instance instance1 -&gt; EC2_Instance instance2 Let's see if I got it right. concatenate will return a new Node with $1 **and** $2 MakeTerminalNode just creates a new **component** MakeEmbeddingNode creates a new component and adds it as a child - to what exactly? From where do I know to which component I should add the new Node? 
How though? What's better for Go and why?
Sorry, my mistake. I was initially going to exclude that call and mention it elsewhere. It was a simple one-liner, so I included it in the above list without thinking, but you're correct in pointing it out as a linear-time operation.
I just pushed a change. If you want to compile and test, just use make yacc go run main.go foo.tdoc 
The "EmbeddingNode" would be a container for other components or "embedding nodes". Suppose you make the struct as simple as this: struct Node { component string relation string children []Node* } you would have all information from the file. The parser then builds it the tree bottom-up. When it recognizes a simple component, it returns a struct where children is nil; when it recognizes COMPONENT IDENTIFIER { ... } it returns a struct with a list of Nodes in children. Wouldn't that take care of the basic nesting problem? I have to say I don't quite understand what you want to achieve with relation and text, but you can either shoehorn them in the same struct, or have an interface "ParseTreeNode" or something like that and create a list of those as children.
What about pb := &amp;aString copyOfString := *pb 
I generally find stack traces to be a poor way to get the one line of information I actually need.
Relation and text is just an Implementation detail. If you want to display a different text than the ID, you can use text. RELATION draws the lines between two related COMPONENTs and changes the placement. I don't see for now how the recursion works, but I'll investigate some time tomorrow and see if I can make it work. If it's really that "simple", I'd bite my ass xD That would indeed solve my nesting problem. Thanks!!!
Same goes for logging packages.
That would either require a consensus on config file format, or you'd end up with multiple formats being dragged in as dependencies. So politically difficult.
Very nice post. Liked every bit of it. 
To the best of my knowledge, `path` package is meant to handle forward-slash separated paths only, like URLs for example. You can still use `path` with the `/` to build up file paths too since that logic is abstracted away but you have to call `filepath.FromSlash(...)` if you want to hand that path to the OS, to open a file etc. for example. Using `path/filepath` takes care of this problem entirely and deals specifically in OS agnostic file paths. Those can be safely and directly passed to any function dealing with files at an OS level.
My pleasure!
If `:=` was literally just syntactic sugar for `var x = ` it wouldn't be so bad, but you can't do for var k, v = range foo { ... } so you _have_ to remember and use both syntaxes. Contrast with JavaScript, where I can just use `var foo = function (...)` everywhere and forget about the shortcut `function foo()`.
Thanks for sharing this. But there's something wrong with your webpage: if I load it in a browser tab, the browser goes nuts and starts allocating more than 2 GB of memory. This happens in the latest version of both Chrome and Safari. I'm on macOS Sierra. Do you have some kind of infinite loop allocating memory in JavaScript?
&gt; but you can't do &gt; &gt; for var k, v = range foo { ... } You can't do that *yet*. There's little reason to disallow the above syntax while allowing `:=`, since `:=` is essentially just syntax sugar. Make `var ... =` a valid replacement for `... :=` everywhere and you have a drop-in replacement. The only different in semantics I know of (besides where the syntax is legal) is with shadowing. `var x = 1; var x, y = 2, 3` is illegal, but `x := 1, x, y := 2, 3` is legal. So, basically we have to have `var` (e.g. so we can specify a type), but we don't have to have `:=` if we make `var` legal everywhere `:=` is legal.
Weird, that doesn't happen for me. It's a static website built with Hugo, the only dynamic parts are Disqus and Google Analytics (which are buggy and don't work). I removed the Google Analytics right now (for the time being), because I need to fix them anyway. Did that fix the problem for you?
I wanted to do this project in Go and I wanted to give GraphQL a try. Last thing was to choose the library: The one mentioned above is under active development, is widespread, has many Github-Stars. So I gave it a try and so far it works for me. :) For performance, this issue is interesting: https://github.com/graphql-go/graphql/issues/119 Bottomline: This library isn't optimized (yet) but already achives competitive performance.
I looked at this one, too but for a production system it seemed too young. But promising. :)
Action as in *Go*ing.
Viper supports a few already, so that is no need to pick one format over all others
Yes, but it presumably has external dependencies for all those formats, so they'd all have to be pulled in as well.
This was sorta how I did physics for a game server back in 2014. Closures and composition are super neat ways to assign physics triggers to game objects, and you don't have to be very knowledgeable about the concepts of higher order functions to use them. Need an object to have a special collision detection trigger? No problem, wrap the relevant physics in a closure and give it to the object's composite structure. It's a fast and intuitive way to have _very_ dynamic control flow. Maybe not the best performance but for cases where that doesn't matter it rocks, and the type safety and "comes before" nature of goroutines keeps you generally safe. More to the point, you can prototype very early because everything can be shifted to another composition paradigm without being changed.
I wasn't trying to imply that generics are useless, I am for generics in Go, I've actually posted some time ago how I could imagine generics in Go. But you can create many great abstractions without them.
Hey, cool arcticle. But for the love of god i cant come up with a clever way for your duplicate exercise. Maybe its just me thinking in a wrong direction or its the late hours. Got a little tip please? :) 
It's not particularly nice, you got to "buffer" the elements between the two returned sequences. So, let's say Dup(s) returns Seqs l and r. Now, if l is ahead of r (in terms of the number of Next calls), you save the elements it produced in a slice for r. Then r uses those saved elements until it catches up with l, and so on.
Ah i see. Thats where i was getting, but i thought 'nah that doesnt seem like its what hes getting at' because you used such a clean composition in the example directly above. Maybe add a hint like "you wont need amy of the above compositions for it". Also glad you did stick with the name beep, still puts a smile on my face :) 
Good point, then just let Google dictate how things are done like always :)
RPC is a much higher level abstraction than a message queue.
This is awesome! Love it.
How can one person do so many versatile and cool projects? Maybe cznic is some kind of robot or artificial intelligence? :)
That sounds really cool! I wonder if it's possible to make a really fast physics engine using the composite pattern.
I wasn't trying to imply that you were trying to imply that. I'm just observing that this is a case where you can sort of "get away" without them, but that style of programming generally calls for a lot more type changes in the pipelines as you go, which Go makes very difficult. And I write a rather a lot of code in Go without that ever being a problem. But I mostly write network servers.
Well, and this is just a hunch, it's just a bunch of check and jump type instructions right? And if you're talking about game physics then there's gotta be a lot of that every tick anyway so I'm not sure if it is even that much slower than a more linear code approach. From a programming side it's much better than massive switching. Jump tables are fast, but getting the alignment, ordering, contiguity, et cetera right means that the entire triggering scheme has to be essentially worked out ahead of implementation, making rapid prototyping almost impossible. I can't stress enough how beneficial RP is to game dev. Striding over a bunch of null checks thrashes less (_if_ you align your strides properly to the cache, which is tricky and mistake prone) but you're wasting a lot of time, why not better to sort of "depth-first" the physics by having game entities be responsible for their own access to environment variables? You can do it in an even more hacky way to simplify understand of the flow by using function pointers in entity structs for top-level composition. It's less "correct" but practically it works very well, and operations on composed function pointers seem so useful, especially in go. They're almost sublime to work with. Edit: on the voodoo side, does anyone know if branch prediction is good with jump tables or would the state be too large?
Your composite approach is definitely as fast as a linear physics engine, and that's usually sufficient. And doing it the composite way brings tremendous advantages. However, really fast physics engines do collision checks faster using smart data structures, various kinds of trees, to find overlapping objects much faster (in O(log n) time). I wonder if that's possible to accomplish with composite pattern.
Another important point, the physics engine part of a game server is almost trivially CPU intensive, it's the client IO preprocess that dominates everything. Edit:typo
My solution was to do simple things like overlap detection, bounds checking, bullet collision detection, etc in the speedy ways and all of the other more tricky stuff in the composite way. It all depends on the game really. 
Ah, that works too, smart.
Here's the kicker, you can prototype collision/overlap with composition and then just shift the pointers to, say, an octal tree based collision detection pseudo entity with essentially one line of code change. So you can work on and test your fancy per-polygon method while having the naive brute-force composed solution still running for the artists to test with. At the end of the day all you're left with as is a penalty is a few function-pointer nil-checks in entity structs. Pretty sweet deal.
Could you elaborate a bit on how such a one-line transition would look like? I can't quite imagine it right now...
So let's say you have entity physics handled through a function pointer in entity structs that calls into a composite that has access to the entire world through closed over environment arrays/vars/etc. And this updated the entity internals. Now you've built a new chunk of code that handles all of entity position updates through some other fancier method. In your entity creator/constructor/composer you change the line that kicks off this physics composition and assigns the positionUpdate pointer to one that points to an accessor method that retrieves updates from your new chunk, which can even be a channel type thing if you want position updated to be out-of-band. Essentially you're replacing the entity's function pointer to a physics updater with one that trivializes it into an accessor. If you have only one type of entity struct then this is overkill, but if you have many to save space on the simple ones then their struct definitions _and_ their update cycles remain the same. So you replace(pseudocode) thisEntity.positionUpdate = composer.CreatePositionUpdater() With thisEntity.positionUpdate = fancyPhysicsUpdater.getOutOfBandPositionUpdate The first one being a composition framework which returns a pointer to a closure and the second being a pointer to your new updater's accessor method. There are cooler ways to do this but this is so maintainable that it can even be a command line flag. 
I see, this is really good! If you make the fancy physics handler implement your composite interface, you can even put more composites over the fancy physics handler to make it even more fancy in a flexible way. I love this!
The best part is that in the simple case your entity struct types only need two methods on them: Update() and Serialize(), that's it. 
I won't say something that is maintained by the Golang team is "stable" in any respect. Adding more packages to the standard library only increases main maintainers more work, not necessarily high-quality product. I understand your concern with the fragmented ecosystem. Trust me, coming from Java background, having too many options is just as disgusting as having no option at all. However, as time goes by, I understand it's just come down to personal preferences like I mentioned in my original response: they all serve the same purpose, but each of them has its own strength and weakness. A good developer should be able to find out a workaround to overcome the shortcomings of certain packages/modules. If people are too lazy to even try to find a workaround, I would advise them to find jobs in other fields. There are no absolute well-designed packages. Having a package in "standard library" does not mean the package is "standard" and any other implementations are "counterfeit". If you take a look at the library of Java and C#, they all started small, but as time goes by, both Java and .NET became so bloated. If you examine the package that you imported in your Java and C# code, you probably not use more than 30% of the "Standard Library" while you still rely on a lot of vendor packages. In this case, I would just remove all those unnecessary packages out of the standard library of Java/.NET and let other people maintain them. I do not see any real benefits from adding packages that are used occasionally or situationally to the standard library.
When are we going to see faiface/gravity? *wink wink nudge nudge* ps. love the post and the library, actually planning on using it for a project in couple weeks or month, great job!
https://github.com/playlyfe/go-graphql
Hi all! I am trying to do service discovery with etcd following [this](https://coreos.com/etcd/docs/latest/dev-guide/grpc_naming.html) from [etcd](https://coreos.com/etcd) website. But I encountered an error similar to the question in stackoverflow. I don't know if this is an issue with [__etcd__](https://github.com/coreos/etcd/tree/master/clientv3) or [__grpc-go__](https://github.com/grpc/grpc-go). Any suggestions?
&gt; Slightly related, you're using string concatenation to build up paths, use filepath.Join instead so it works correctly on all platforms. I tried running the tests on Windows without using the `filepath.Join` and it still worked correctly. Any explanation why it still worked?
this is the same as "copyOfString = aString", they share the underlying bytes.
Use a struct and define the handler as method. This way you can store anything on the struct. But I'd prefer the github.com/go-kit/kit way: have a Decoder which decodes the http request, and returns a nicely filled struct; have an Endpoint which receives that processed struct, does its job, then returns another struct with the data to be returned; then have an Encoder, which outputs as needed from the struct into the ResponseWriter.
I have mixed feelings about the approach you are taking, but this article may help give you a few ideas - https://www.calhoun.io/pitfalls-of-context-values-and-how-to-avoid-or-mitigate-them/ Context values aren't bad by themselves, and they are pretty useful for middleware, but you just need to use some caution with them and not throw everything under the sun in a context value If you have follow-up questions let me know - I wrote the linked article.
Fully agree. This is an indication that the standard package may need an improvement
mine can do headers too: - https://godoc.org/go-hep.org/x/hep/csvutil/csvdriver - https://github.com/go-hep/hep/blob/master/csvutil/csvdriver/driver_test.go#L139 (and you can send SQL queries against it)
and there's also the panda inspired dataframe package for Go as well: - http://godoc.org/github.com/kniren/gota/dataframe
Agree about the optimization. But I'm sure a prove of concept is required before the decision to accept it will be made. As I now understand it, your suggestion is to move the cost to the coroutine start by copying the dyns. As a consequence, the write implies a simple push on the stack of the overriden value and the functions pops it back in on return. Don't know about panic. Are defer instructions executed when panic pops the stack ? That would indeed make read and write O(1) and light because they use the stack. The question is if the overhead on coroutine start is ok. Your suggestion to make all exported package variables dyns make sense from the user point of view, but this might be too much in price to pay for coroutine start. 
You know generics are for automating this shit right? So you don't need to do things like this.
May I suggest to write a more formal article presenting the principle of this new type of global variable, how it may be used and it's benefits, as well as some implementation strategies, abstract from any particular programming language ? This type of global variable may interest other languages and your article could be shared as reference. 
Website kept running out of memory on my iPhone, so I had to use safari's "reader" mode. Great article though, thanks :)
&gt; Are defer instructions executed when panic pops the stack ? [Yes.](https://golang.org/ref/spec#Handling_panics) (technically speaking, panicing doesn't really "pop the stack", though, because information about the original stack of the panic needs to be preserved. But details) &gt; The question is if the overhead on coroutine start is ok. &gt; &gt; Your suggestion to make all exported package variables dyns make sense from the user point of view, but this might be too much in price to pay for coroutine start. Yes. From what I can tell, so far there is no real indication that it's not. The amount of data is small-ish, the copy is fast-ish and starting a goroutine already takes long-ish. Relatively speaking. I ran a quick and dirty benchmark to see how long it takes to start a goroutine and the best thing I came up with to measure that came out with ~1μs. I benchmarked copying 4KB and it came out with &lt;0.1μs. That, to me, is enough of a signal of feasibility. And don't forget that you also *win* performance by this change. Reads/Writes being O(1) is not the only effect the linked list implementation has. It also removes possible data-dependencies of multiple threads. Right now, if you do `foo.X = Y`, the compiler has to assume, that `foo.X` might have been changed since the last time it got read/written (though the [memory model](https://golang.org/ref/mem) restricts that somewhat). If `foo.X` is implemented in this stack-form, there can't be any changes to it. So yes. As I said above: &gt; I think that very heavily depends both on the constants involved and the size of n. There are many effects working against each other here. Most of them are incredibly subtle and are only affecting a tiny time-slice of the total time taken by the 99%ile of go programs. Speculating over what effect will dominate in the end, is *way* premature. If you want to write a proof of concept, you should do the thing you should always do: Implement the stupidest, simplest thing that could possibly work. Benchmark and *if* it is too slow, optimize. Most of the time, it won't be, though. (FTR, a better argument than time would probably be *space*. And I say that, fully aware that this shoots my own arguments in the foot. Luckily, I don't intend to implement anything, so I don't have to care, for now)
&gt; May I suggest to write a more formal article presenting the principle of this new type of global variable It's not really new. Dynamic scoping has been around for a *long* time. There is no actual original thought in this article, when it comes to language design.
I suppose that happens b/c of your use of `filepath.Base()` and `filepath.Walk()` which knows how to deal with `/`-separated file paths.
For these kind of things I use code snippets which expand in my editor to these collection
Isn't it already possible to use the same port using HTTP2 and a connection upgrade? - https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#header.upgrade
Hm, if you have any idea what's wrong, please tell me. It doesn't happen for me.
Managing templates really is somewhat of a pain. I use [a simple makefile to discover and bundle them](https://github.com/hahanein/russenmagazin/blob/master/embedded-golang-templates.md) and [bindata](https://github.com/jteeuwen/go-bindata) to ship them with the binary. Hasn't failed me yet
I believe, that if a function contains only linear logic and two or three lines of code, it is meaningless to use defer statement. But there are some good (canonical?) examples, f.e: https://github.com/boltdb/bolt/blob/master/db.go#L588-L592 P.S. I really like defer and it often simplifies a function
Of course, let's blame the people that ask questions. Not the ecosystem. I'll say it again. The way the Go ecosystem presents information, guidance and best practices is lacking. Or we can assume that people are just lazy or bad developers and that's why they use pkg/errors.
I would be my worst nightmare to see the Go ecosystem end up becoming like the Java ecosystem. The argument that "hey this is normal and if you don't like it change jobs" is not good in my opinion. Sorry. Nope. I won't accept it.
Why do this as a middleware? There seems to be no actual value provided. I'd just use a function func ParseBody(v interface{}, req *http.Request, res http.ResponseWriter) error { if err := json.NewDecoder(r.Body).Decode(v); err != nil { // handle error here } return nil } and call that from a handler.
Good point! Seems like this is a standard way to switch protocol, but as far as I can see, there are at least two advantages of [soheilhy/cmux](https://github.com/soheilhy/cmux) compared to [Upgrade header](https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#header.upgrade): 1. It works, in contrast, [HTTP1 Upgrade is not supported in current gRPC](https://github.com/grpc/grpc/issues/1820) 2. It's transparent to the client (the client can either send gRPC requests or send HTTP1 requests)
So how far are you into development? Any roadblocks due to low documentation? I'm also eager to know what you think of [this JS example's](https://github.com/fkrauthan/hyperwallet-graphql-server) quality.
The actual "Design Patterns" book should be helpful
Many of them is enough long ago …such one before 2015 maybe not that useful. Many new useful things from Go1.5 dose appeared。
This is very interesting. Thanks for the article!
Good read. The channel option is the the special case where concurrency = 1, mutex version of the semaphore pattern described here http://jmoiron.net/blog/limiting-concurrency-in-go/ 
Came here to make the same joke and pay the same compliments. Good work, very interesting approach, very interesting libraries (faiface/pixel looks great too)
Thanks, this gave me a better understanding of Context.
I didn't know this was the best way of handling JSON request body in Go. Coming from other languages, they always had a JSON parser middleware. Thanks for the info!
Someone should make something like QT in Go.
Thanks. 
Interesting. They have different string addresses, and I couldn't think of any easy way to check if the underlying string storage address was different. I have to ask, though -- given that strings are immutable, what is the problem referred to?
Great story. It should inspire more people to try contributing - not just to go, but to any open source project. It is always great to be a part of a project and be able to help.
Thanks !
Is [this](https://www.amazon.com/dp/0201633612/) the book, "Design Patterns", that you were referencing?
This is full hipster. Amazing. 
Solarized.
Tomorrow Night. https://github.com/abiosoft/dotfiles/blob/master/tomorrow-night.vim https://twitter.com/abiosoft/status/890991630949515266
Gratz!
 func f() string { s1 := string([]byte{100000:0}) s2 := s1[:5000] return s2 } After calling this function, the 100000 bytes memory block will not get garbage collected, for s2 is still referencing it.
vim-moonfly-colors. https://github.com/bluz71/vim-moonfly-colors
Gruvbox! https://github.com/morhetz/gruvbox and then as a bonus, https://github.com/fxn/vim-monochrome is also very nice!
Well, yeah, until s2 is garbage collected.
That's it.
[Jellybeans](https://github.com/nanotech/jellybeans.vim)
Freya http://i.imgur.com/ncwZ9.png https://github.com/nomad-software/vim-posix/blob/master/.vim/colors/freya.vim
way to go :)
As someone who uses both, I'm very excited. Maybe someday I can use go as high level language and rust as a lower level for the most critical part. Instead of Python/C, there would be Go/Rust, performance and safety. 
[Dracula](https://draculatheme.com)
http://vimcolors.com/536/frood/dark
RabbitMQ is a message broker application which uses an interface definition language (IDL), a message framework, and implements a queue along with related tools (circuit breaking, clustering, routing, etc.). gRPC is a message framework which uses Protocol Buffers (proto3) by default, but can use others. Using plugins, for example, one can generate HTTP negotiation in front of the RPC server. It can be used as part of a message broker application. One may use gRPC to add an RPC API to an existing application, or to query multiple external RPC APIs as a client with load balancing. One may use RabbitMQ as a message broker that is integrated as RPCs.
I'd recommend https://github.com/neelance/graphql-go as one of the more idiomatic GraphQL libraries with active development.
Plot twist: a lot of those things aren't really about Go, and are actually pretty hard to present well. It's not like you could read a single (currently existing) book and instantly learn all you need. It's just software engineering, Go explicitly and consciously hasn't introduced anything new to the industry.
Tomorrow Night Eighties
I haven't seen much problems creating socket level protocol, it's basically a free performance.
I don't think the suggestion was to have dynamic-scoped variables *replace* package-scoped variables, but to rather be a new, additional type of variable.
Yeah, when I read the original article this drove me a little crazy - there are plenty of good complaints about Context but that's not one of them. And "susceptible to being misused" isn't a good argument against anything. If it was, all programming languages in existence would be "bad", because any language can be misused.
Sorry, I cannot help, but out of curiosity... what companies in CPH work with Go and are looking for more people? :-)
Awesome! Just out of curiosity, is it possible to do the same thing with C instead of Rust?
Why use a library for something that can be done in &lt; 10 lines? :) https://github.com/philips/grpc-gateway-example/blob/master/cmd/serve.go#L51
We do live, we do learn. You pointed many things to think about. If possible lets do step by step. Could you help me understand please why [the example](https://godoc.org/gopkg.in/workanator/go-floc.v1/run#example-package--WithAtomic) is full of data races? Maybe some links to interesting (blog)posts? I assume that is because of dynamic type casting of the interface returned by `state.Data().(*int32)`. If so I would like to say that `state.Data()` always return the same type of data which is the pointer to `int32` put in the state when it was created `state := floc.NewState(new(int32))`. Maybe I do not understand something? Thanks in advance.
&gt; there are plenty of good complaints about Context but that's not one of them. *shrug* TBH, not really. I think the disadvantages of context are pretty much overhyped. &gt; And "susceptible to being misused" isn't a good argument against anything. If it was, all programming languages in existence would be "bad", because any language can be misused. I strongly disagree. There is a difference between being *mis*used and being *ab*used. Programming languages can be easy to misuse or hard to misuse. For example, C is relatively easy to misuse. Go is comparatively hard to misuse. Haskell is even harder to misuse. `context.Value` is prone to name-collisions leading to subtle bugs, under misuse. Global variables (or `dyn`) aren't (comparatively speaking), as they are checked by the compiler. I'd be with you if you said that the potential of misuse is small enough, that it doesn't practically matter. But not when you reject the notion that there is an advantage in making it harder to shoot yourself in the foot accidentally.
Actually it's safer C functions.
Try to make anything idiot-proof and they just make a better idiot. All you have to do to avoid misusing context values is to *read the documentation* for `WithValue`. It's a whole three paragraphs. If that's too much to ask of someone, they're going to misuse any feature you put in front of them.
yes, you want CGo. but remember the wise proverb of Pike - "CGo is not Go"
I mean I was wondering if it is possible to use C without Cgo in the same way.
Plus a more efficient way of calling.
Rust is such a meme language. If it had C syntax and wasn't a meme language then fair enough but no it has to have interpreted front end typeless syntax which just doesn't make sense for a low level typed language.
you could just have another variable, maybe a boolean named "isWriting", that you lock around. Then you readers can lock that var, check if "isWriting", then unlock and send response
use context.Context - WithDeadline (though your database must support context related lock timeout triggers), if your database has SQL type interface driver and supports database/sql/driver package, then whenever you do Exec do ExecContext instead. You may pass your context from net/http.Request down the chain with deadline. I suggest you to have a look at some sql driver implementation of ExecContext. The problem you are dealing with is natural and timeouts, synchronization of shared resources and locking, idempotency, must always be on your mind.
sorry my bad!
[Bad Wolf](https://github.com/sjl/badwolf)
&gt; interpreted front end typeless syntax What kind of sorcery is this? In case you meant to describe Rust as an interpreted and typeless language, you would be wrong. Strong typesystem and compiled language.
I can't paste the example program on their homepage because their website manages to make their text all appear as a single unicode character when copied but look at that syntax and try and tell me that's not the kind of nonsense a typeless interpreted language would have. It looks like a worse version of javascript.
Take an actual look at the language, it is compiled, not interpreted. It is totally _not_ like javascript. Here is a [playground version](https://play.rust-lang.org/?gist=f02308edfa28f4828e23fd607b0e030c&amp;version=stable) of the example code on the frontpage, you can run it or view the assembler by clicking the button on the top left corner. I, personally, really like it because of its strong typesystem. Edit/Clarification: I inked to the playground because he wrote about the example on the frontpage, not meant as an ad :)
Peytz &amp; Co. Vivino, different hosting companies where performance is important, google have offices in CPH, iconfinder etc. Me and my colleague though are just two student developers at Andelsvurderinger, a company that built their own backend, IOS app, integrations with e-conomic. We are looking for two new Golang developers at the moment, which would make us 4 in total. 
&gt; Requests to read that happen while writing should fail, informing the http caller to try again later. That doesn't sound like the ideal behavior and it doesn't sound like the solution you're talking about implementing. I'm sure you'd rather just wait until the writing is finished and then perform the read which is how RWMutex works except maybe you want to time out if it takes too long. Maybe you can explain specifically what behavior you want the http handler to exhibit?
Sure Go hasn't introduced anything new but it comes with a different way of thinking. We need to present the "Go way" more effectively in my opinion.
&gt; Any code that is present between a call to Lock and Unlock will be executed by only one Goroutine, thus avoiding race condition. That's a weird way of explaining it.
Rust has an extremely strong, powerful, static type system. Just because the type names themselves aren't repetitively shoved down your eye sockets doesn't mean they're not there.
This example? fn main() { let greetings = ["Hello", "Hola", "Bonjour", "Ciao", "こんにちは", "안녕하세요", "Cześć", "Olá", "Здравствуйте", "Chào bạn", "您好"]; for (num, greeting) in greetings.iter().enumerate() { print!("{} : ", greeting); match num { 0 =&gt; println!("This code is editable and runnable!"), 1 =&gt; println!("¡Este código es editable y ejecutable!"), 2 =&gt; println!("Ce code est modifiable et exécutable!"), 3 =&gt; println!("Questo codice è modificabile ed eseguibile!"), 4 =&gt; println!("このコードは編集して実行出来ます！"), 5 =&gt; println!("여기에서 코드를 수정하고 실행할 수 있습니다!"), 6 =&gt; println!("Ten kod można edytować oraz uruchomić!"), 7 =&gt; println!("Esse código é editável e executável!"), 8 =&gt; println!("Этот код можно отредактировать и запустить!"), 9 =&gt; println!("Bạn có thể edit và run code trực tiếp!"), 10 =&gt; println!("这段代码是可以编辑并且能够运行的！"), _ =&gt; {}, } } }
Totally. Anything that can compile into a static self-contained `.a`/`.o` will do, really.
dns proxy for ad blocking: https://github.com/seedifferently/nogo
I had a bunch of relatively crappy D-Link DCS-930L wireless IP cameras that can dump pictures (upon sensing motion) to an FTP server. I wrote a Go program to provide an FTP server endpoint and write the images to the SD card. It would also provided an HTTP endpoint where it would provided aggregated thumbnails of camera footage. All of this was running on a Raspberry Pi 3. The JPEG encoder was relatively slow, but it served its purpose as a basic security base-station.
Congrats. I see you've submitted a few more changes since your first two.
When I started writing go, I created a project to control Pandora Radio on my Raspberry Pi. I have not touched it in a while, so it may need updating to work. https://github.com/biorisk/panpigo 
If you're not too worried about maximum efficiency at all costs, a condition variable can do it with another goroutine broadcasting after a timeout. If you read the source of that you may be able to specialize that farther, depending on your skill level.
[smyck](http://color.smyck.org)
If you want to cancel the read operation after a set amount of time, you could just start that operation in a goroutine and wait for a result or your timeout. Basically, your read operation would be something like this: type content struct { stuff string } type database struct { sync.RWMutex data map[int]content } func (d *database) Get(ctx context.Context, id int) (*content, error) { // Make a channel to send the result on result := make(chan *content) // Make a context to be canceled. ctx, cancel := context.WithCancel(ctx) defer cancel() // Start a goroutine to obtain a read lock and pass the result if the context hasn't been canceled. go d.read(ctx, id, result) var ( c *content err error ) select { case c = &lt;-result: case &lt;-time.After(time.Second * 5): err = errors.New("read operation taking too long") } return c, err } func (d *database) read(ctx context.Context, id int, result chan *content) { d.RLock() defer d.RUnlock() select { case &lt;-ctx.Done(): return default: c := d.data[id] result &lt;- &amp;c } } [Here](https://play.golang.org/p/3uTErRzvcJ) is a working example on the playground.
Do readers have to fail during the write or can they just return old data? I think either can be implemented fairly straight forward using an [`sync/atomic.Value`](https://golang.org/pkg/sync/atomic/#Value).
Great blog post. I'm definitely using that pattern more in the future. In the paragraph that reads &gt;Nice! There is a problem with this code, though. This program immediately exists and we won’t hear anything. _exists_ is misspelled, should be _exits_.
You can either put your application behind a dedicated webserver (nginx , caddy, apache) or you can use **setcap** $ sudo setcap CAP_NET_BIND_SERVICE=+eip /full/path/to/your/binary [https://www.insecure.ws/linux/getcap_setcap.html](https://www.insecure.ws/linux/getcap_setcap.html) 
I did the same newb thing. Stopped there though. Good job!
Yes I have ;) Thanks Joe ! 
I've heard Go isn't great for OS development since it requires garbage collection, but could this potentially bridge that gap and allow a kernel written in mostly Go?
For the example used, sync/atomic would be a good fit.
I suspect Arturus is using SNI (https://en.wikipedia.org/wiki/Server_Name_Indication) and wants to add to the pool of hostnames his server handles.
**Server Name Indication** Server Name Indication (SNI) is an extension to the TLS computer networking protocol by which a client indicates which hostname it is attempting to connect to at the start of the handshaking process. This allows a server to present multiple certificates on the same IP address and TCP port number and hence allows multiple secure (HTTPS) websites (or any other Service over TLS) to be served by the same IP address without requiring all those sites to use the same certificate. It is the conceptual equivalent to HTTP/1.1 name-based virtual hosting, but for HTTPS. The desired hostname is not encrypted, so an eavesdropper can see which site is being requested. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/golang/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Yes, for switching between gPRC and HTTP1, in essence, the cmux library does almost the same thing as the snippet you provided does :) I think this is more a matter of personal preference, and personally I prefer to use cmux since: 1. It's simple to use, and can save some keystrokes of repeated code 2. It's dedicated to connection multiplexing, and is lightweight
I'm guessing network related stuff in go and system level in rust? As someone who's interested in both languages I'm curious how you got started in both.
Whatever it is you're doing, presumably starting a goroutine that waits for a ticker, is probably the correct solution. A general solution could be created which accepts a function to execute and a timeout. That's my thoughts.
This library works pretty well for what you want: https://github.com/jinzhu/gorm
with [sqlx](https://github.com/jmoiron/sqlx): type Foo struct{ ID int Name string // etc } v := Foo{ID: 1, Name: "Ardie Savea"} db, err := sqlx.Open(...) // same as sql.Open stmt, err := db.PrepareNamed(`INSERT INTO data VALUES (:id, :name)`) _, err = stmt.Exec(v) 
This is the error I am getting https://ibb.co/htH9hF v := models.Data{102, "yoyo", 14, 67, 10745, 9592, 295, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}, sql.NullInt64{Int64: 42, Valid: true}} db, err := sqlx.Open("mysql", "----------------") // same as sql.Open if err != nil { panic(err.Error()) } stmt, err := db.PrepareNamed(`INSERT INTO data VALUES (:id, :created, :team_id, :status, :acceleration, :position, :velocity, :battery_voltage, :battery_current, :battery_temperature,:pod_temperature, :stripe_count, :pod_pressure, :switch_states ,:pr_p1, :pr_p2, :br_p1, :br_p2, :br_p3, :br_p4)`) _, err = stmt.Exec(v) if err != nil { panic(err.Error()) } Nothing is being sent to MySQL and is exiting. Not sure what to do. Any help would be great. Thanks
I was referring to this: &gt; There is some confusing overlap with package-scoped variables in this design. Most notably, from seeing foo.X = Y you wouldn't be able to tell whether foo.X is dynamically scoped or not. Personally, I would address that by removing package-scoped variables from the language. If package-scoped variables stay, then a new syntax is needed to avoid the ambiguity of `foo.X`.
I like https://github.com/robertmeta/nofrils.
It is full amazing. Hipster. 
Not really. The only thing this really achieves (while still amazing!) is to call functions from arbitrary machine code. The garbage collection would still exist. 
There are two plugins: the old community one and the JetBrains maintained one. The community one doesn't work with 2017.2+ platform and it doesn't have any of the feature Gogland, https://www.jetbrains.com/go/, has. The JetBrains maintained one is compatible with 2017.2+ and it has the features Gogland has. I suggest that you move to the JetBrains maintained one. It is available for all paid IDEs based on the IntelliJ Platform. The community plugin has not been updated in almost a year and I don't have any plans to further maintain it because I don't have the time or the resources to do so. Except a few contributions here and there, no committers have presented themselves for as long as the community plugin was actively maintained. Patches are welcomed. 
Unfortunately, the JetBrains plugin does not work with the Community edition.
:syntax off
Correct, as I've said in my statement: &gt; It is available for all paid IDEs based on the IntelliJ Platform. You can talk with the sales team for a student license, or open source contributor one (mind the usage terms) but you can also use Gogland while it's in EAP as it's completely free and vastly superior to the community one.
As I understand it, FIPS-140 is a commercial imperative when trying to sell into certain markets (I.e. government, military). It it's about having the perocess/technology in place to be able to certify that the machine code implementing this particular algorithm in this particular binary is the same code that a third party had previously issued a certification for. It is possible that a FIPS-140 certified product has only LESS secure primitives available compared to state of the art crypto systems because the bureaucracy had not caught up to the state of the art. If you need FIPS-140, you'll know it because your sales guys/gals will tell you. If you don't, you are not missing anything, security wise. 
We have now 3 queries (one of them with quite some big data structures) and 7 mutations. So far, using the library has been easy enough thanks to the examples. Only annoying thing is the limited error messaging mentioned above. But that is somewhat ok, our current workaround is to not send objects with codes and human readable strings but just an error string like "FOO_FAILURE" which the clients can easily consume. JS: No thoughts about it, I'm not in the JS-scene. :)
I've never seen it. After thinking more about it, I now believe the following. 1. This property is perfect for Context variables. So instead of calling them dyn, one could call them ctx. 2. Creating a new type of variable in addition to standard globals has the following benefits: - backward compatible - limited number of dyn/ctx variables in programs - possibility to introduce them in a go 1.x version - preserve the possibility to use conventional global variables 3. You suggested to implicitly define all package variables as dyn/ctx variables to avoid confusion. Another possibility is to use a sub-name space for these variables. Starting the subname space with a lowercase avoids collision with existing global variables. Collision can't be fully excluded. package demo ctx Level string = "visitor" ... if ctx.Level == "user" { ... } To access it one would have to write: import "demo" ... if demo.ctx.Level == "admin" { ... }
&gt; This property is perfect for Context variables. So instead of calling them dyn, one could call them ctx. But I like my bikesheds red! &gt; limited number of dyn/ctx variables in programs I don't see how this is an advantage. &gt; You suggested to implicitly define all package variables as dyn/ctx variables to avoid confusion. I suggested *removing* global variables, to limit complexity (and yes, remove confusion). I did not talk about doing anything implicitly. But that it's possible to get any of the same benefits as from global variables and then some. &gt; `if ctx.Level == "user" {` &gt; `demo.ctx.Level == "admin"` That just looks like a selector expression on a package or other identifier to me. It does not address the basic issue that it's hard to tell the two apart. But, in any case - bikeshedding syntax isn't really useful, IMHO. That was… *kind of* the point. There should be some discussion about whether we want the possibility of implicitly or explicitly (and which) passing data through APIs first or scoping data vertically in some way or another, before talking specifics. IMO [this golang-nuts post](https://groups.google.com/d/msg/golang-nuts/eEDlXAVW9vU/nhscyd4VCQAJ) does a great job in that regard by showing off some advantages of passing context explicitly.
A few issues: 1. You can use a .syso file to automatically include native code. No need for a makefile. See https://github.com/badgerodon/quadprog?files=1 for an example 2. The cgo overhead is actually necessary. Stack conventions are different (which he mentions) but there's also a lot of subtlety around goroutines... 3. Go defined a memory model that allows pointers to move. If you pass pointers to foreign functions your code may break in a future version of Go. Cgo warns you if you do this. 4. Its not obvious that the simple kinds of functions that lend themselves to this sort of optimization are going to be faster in Rust. Rust has more type safety, but really the only thing here that might help is it's compiler, which is very sophisticated and may generate better code. We need to stop thinking of languages at various levels. Go code is not automatically 2x slower than Rust code. 5. Assembly in most of these places is there for a reason. It's usually to take advantage of cpu features not available in the regular language, though there are some very optimized routines as well. Neither of these are likely to be served by any high level language.
I'm surprised to see Martini on the list of recommended frameworks. I thought people haven't recommended that framework for a long while. 
SysV IPC is a very old, rarely used technology. I would only recommend using it if you need to in order to work with another existing system. Maintenance of the queues is the primary challenge, yes. And unforseen implementation limits like "oh it turns out that we can only have 100 of these, but a workaround for another problem with them means we wish we could have 400 of them". Also, understand that SysV IPC is a local-only solution. If you later decide you need to talk to queues across a network, whatever work you've done to make SysV IPC work for you will be lost. Seriously, turn back now. Look into NATS or something. -jeff 
I enjoyed Spacemacs color scheme and found: https://github.com/liuchengxu/space-vim-dark
This. I was actually surprised how little I missed syntax highlighting almost immediately after turning it off. At least for go. I now like non-highlighted code better.
Context would slow down mutex-operations by a lot (as it uses channels). If you don't care about that slowdown, it's pretty simple to make [a lock with context support](https://play.golang.org/p/H5ETUaPxV3) yourself.
With [PyO3](https://github com/PyO3) the case for C extensions for Python is a lot slimmer than it used to be. :) Would love to see a tight integration like that for Go, too. I don't mind a little overhead, even.
Isn't it technically a rust extension for Python? I have been following closely this project anyway and hope to see real world examples in it soon :) 
I'm doing some freelancing beside my PHD. My client want something blazing fast (or at least a responsive ui). I mostly make webapps. Usually it's just some database fetching but sometimes it can get a bit cpu intensive. 
Some stuff I created for [BTC/ETH/etc](https://github.com/jdevelop/go-coin-ticker) [monitoring](https://github.com/jdevelop/go-coin-slack-bot), may be interesting to start with. Cool enough? ;)
Sorry, that's what I meant - PyO3 has made Rust extensions easy enough to write, that the only reason to use C is that the tooling is usually already there on Linux.
I got it, I just wanted to nitpick a bit :D
You could say the same for the existing ambiguity of foo.X - is foo a package and X a global, or is foo a struct and X a field? What type is foo.X? There's always a trade-off between ambiguity and verbosity, and the answer isn't universally "add verbosity to reduce ambiguity". A new syntax isn't *needed* to distinguish dynamic scope vs package scope references.
Frankly I have no idea what Trend Micro Locality Sensitive hashing is. But I will be checking out the paper you posted on the project GitHub repo when I find some time. Thanks for posting this. 
&gt; look at that syntax and try and tell me that's not the kind of nonsense a typeless interpreted language would have. It's not the kind of nonsense a typeless interpreted language would have. You're just completely unfamiliar with Rust and probably don't know what type inference is. 
[Monokai](https://github.com/sickill/vim-monokai)
What are some example applications for this hash type?
I had the same experience. Now I feel its distracting me from the code.
I've never changed the default theme.
I'm oversimplifying, but if you take the md5 of two very similar inputs you get two very different hashes. An LSH gives you two very similar hashes that allow you to even calculate a distance between the two inputs. You could use this in detecting known inputs or inputs that are very similar to previously known inputs.
Or you're a meme developer, I wonder which one is more likely
LSH is for example used in malware detection. Malware authors try to avoid detection by permutation of the binary representation of their malware. If you use LSH you can see the similarities between members of a malware family. Don't worry, this is not the only way AV products detect malware :)
I've always wanted to contribute but never could figure out how it works. Do you always have to find issues yourself or can you select a current issue and fix that? How do you get assigned issues?
The ones labeled `HelpWanted` are ones that the main devs aren't looking to work on themselves right now, so that's generally a good place to start.
1: I explain in passing that I can't `import "C"` or I can't write Go assembly, and without it I think the .syso won't be included. 2,3: I'm very well aware of the subtleties and difference in constraints of cgo and assembly.I gave a talk about it at GopherCon 2016. This will work for pure functions that I describe. 4,5: curve25519-dalek is faster because a number of reasons that are not reproducible in pure Go, but is pure Rust.
So... would a file made of every, say, 100th bit of the target file work as a primitive LSH?
fatih's custom **molokai**: [https://github.com/fatih/molokai](https://github.com/fatih/molokai) Other's I've liked: * (dark)- gruvbox,xoria256, vividchalk, inkpot, wombat, zenburn, candy, grb256, distinguished, candycode * (light)- papercolor, github
Every time you pass a variable to a function, you are making a copy of it. That means that any change you make to it inside the function is only being made to the copy and not the original. A pointer is simply a reference to the memory location of the original value, so when you pass that to a function, you're making a copy of the memory location (rather than the value), and when you make changes to the pointer, you're making changes to the original thing. A secondary reason for using pointers, especially for buffers, etc, is that the memory address of a variable is very small, but the buffer could be very big. If you copied the whole buffer every time you needed to change it, you would end up using all your memory in no time.
A what?
Theoretically yes but you see how this gets slight my complicated if you add a bit at the beginning and your 100th is now off by one.
Regarding receivers on function definitions: If a variable is in scope as a concrete type, any function that is defined on the type can be invoked. If the variable is a value and a pointer method is called, Go converts the variable to a pointer under the hood, so the method can be invoked. Value methods can be called on pointer variables without issue. The pointer/value receivers really make a difference when attempting to satisfy an interface. If a variable is in scope as an interface, Go cannot do the pointer conversion. This playground shows that pointer and value functions can be called on both pointers and values, and that when satisfying an interface, the interface must be satisfied by whatever is being used (value or pointer). If you uncomment line #31, you can see that the value object does not satisfy the interface, as the function is declared on the pointer: https://play.golang.org/p/PCpsTQFmvS
I know why we use pointers instead of values when passing them to functions and as receiver. But buffer aside, why is ast.ExprStmt passed around as a pointer? Non of it's methods change its value, but they still have pointer receivers. It's also not that big, only stores an interface.
Huh, I happened upon this about 2 weeks ago and was using it. This was the easiest LSH library I could find in either Go or Python. Thanks for making it!
Do I just comment that I want to do this task?
This subreddit... man I just don't get it. Why am I being downvoted? Maybe its time to move on.
Then `ast.ExprStmt` could easily implement `ast.Node` using only value receivers because its methods do not modify its value. But it's still used as a pointer. Could it be just to be consistent with other nodes? In .NET for example, we use classes (reference type - go pointers) almost all the time. And use structs if the value is small, behaves like a value, has no identity
Cool, I'd appreciate any sort of feedback and if possible some details on how you are using it (DM is fine)!
How is `models.Data` defined? There is some [documentation about named queries](http://jmoiron.github.io/sqlx/#namedParams) that may be helpful as well.
A new syntax is always an option. I don't say that introducing a new syntax is wrong. The author suggested that his personal preference would be to remove package-scoped variables, and I pointed out that there would be no easy way to do this. 
Why you have to be so salty instead of just having a normal discussion? You are more of a "meme developer" than everyone else here...
I'm not saying it's not an option, just that "a new syntax is needed" is not true - it's an option certainly, not a necessity. There will always be ambiguities and cases where you have to RTFM when using a package, and that's OK.
Oops, sorry, I got your last post wrong (overlooked the *isn't*). So scratch my previous comment. I guess we can agree that there are three options with pros and cons: * Do nothing and have one more ambiguity to resolve when reading `foo.X`. * Create a new syntax and make the language a little bit more complex. * Replace package-scoped variables by dynamic-scoped ones and spend some time fixing everything in stdlib and elsewhere that breaks when global `var X` suddenly is dynamically scoped. I really cannot say which of the three I would favor, but let's not forget that the author said that the post is far from being a proposal for 2.0, and if this idea ever makes it into a proposal, we surely will see some aspects of it changing anyway. 
&gt; I just use this: https://play.golang.org/p/RyqIVv-0vP can't load package: package main: tmp/sandbox785923211/main.go:1:1: expected 'package', found 'func'
and then `go get` 'delve' and it all magically works.
Agreed. Of the ambiguities around "foo.X", personally I would rather clarify whether "foo" is a package or a struct than clarifying anything about the type definition of X, but as you mentioned, it's a long way from being a serious consideration for the language. I do think that dynamically scoped variables could be a huge benefit to the language, but not if it means eliminating regular package scoped variables.
I like https://github.com/ikaros/smpl-vim
Here's how I have typically done mysql inserts in the past https://gist.github.com/KevBurnsJr/9abdeb0bfa0cf32cff4839da15e8c2a8 
`ast.ExprStmt` probably gets stored into `ast.Node` interface slots all the time, at which point it would need an allocation &amp; be stored as pointer anyway.
It is likely for consistency, yes. Remember, there are no reference/value types in Go. Everything is a value in Go, including pointers. A pointer is still copied when passed as an argument (a receiver is just another argument) but the underlying memory it points to is not. Additionally, the programmer doesn't have much say in whether a value is placed on the stack or the heap. The Go runtime decides, so a non-pointer may still end up on the heap. This is different from .NET where a class is always allocated on the heap, and a bare struct is usually allocated on the stack.
Yep, just say that you want to take up the task. Wait for one of the devs to give a green signal and then fire away your first CL !
Thanks. So what do you say about my `NamedMap` type? It doesnt have have any methods. Should I use it (for example as a field in another struct, or return from a function) as a pointer or value?
That's implementation specific though. 
To crudely paraphrase a passionate Go instructor I saw at a recent GopherCon... values / reference should be determined by readability and consistency. Look at the factory pattern - does it return a pointer, or a value? If it returns a pointer, ALWAYS use a pointer for that type of value. If it returns a value, ALWAYS use a value, and never convert into a pointer. This makes it easy to keep a more accurate mental model. If you are converting things between values and pointers and back again in various places, it will be impossible to maintain an accurate mental model of your code. As for optimization and performance considerations, they should all be secondary to readability.
Yeah, as this company has a very good reputation in mainland China. /s
It depends on how you use the `NamedMap`. If you have other functions that receive the map and modify it, then a pointer should be used. However, the `NamedMap` contains a real `map`, which is a pointer-like value. So even if something receives a bare `NamedMap`, modifying the map stored in the `Map` field will modify the underlying map that all copies of that specific `NamedMap` point to. [Here is a salient example](https://play.golang.org/p/eFBtckFvO-) One final point, you mention that `NamedMap`has no methods, but methods are simply syntactic sugar in Go. The receiver argument is just a regular argument under the hood, with some additional semantics that "bind" the value of the argument to whatever struct its attached to. edit: I realize I rambled a lot but didn't actually answer your question. To me, there is no hard-and-fast rule for returning a value instead of a pointer. Often-times I will return values for wrappers around things that I know shouldnt change, or that I want to enforce are immutable. The NamedMap struct in your example: if the Name field should never change after initialization, then return the value. But if Name might be really long, it might make more sense to return a pointer, avoiding copying the string every time you pass around the NamedMap.
Sure, but it's core code so it's not like the authors didn't understand the implications, or were unsure of them. The other good justification is "if some of ast.Nodes are *T, then for consistency let's make all be pointers". That makes the type switches easier to write.
&gt; it is meaningless to use defer Not if those 3 lines can panic in any circumstance or if you want to maintain the habit. 
This seems more like a gamified "must get all the points" scoreboard than something useful. Also, I have trouble taking a "code quality" tool seriously that itself has bad code quality.
It's called type inference dear
I think you are talking about the fact that you aren't required to point types for every single variable. The matter of fact is that everything is statically typed, it's just that compiler at compile-time selects the right type for the variable to spare you the typing. For example if you declare something as let x = "Foo"; it is pretty obvious that it's a string, so why bother typing let x: &amp;str = "Foo" when compiler can do that for you? 
Perhaps for consistency with other code, perhaps for performance: if there is no asynchronous access it's less work to pass a pointer than to pass the presumably larger struct.
This is a poor introduction to golang. Complex128? Really?
Could be wrong, but I don't think you're supposed to rely on the filesystem when using a heroku instance. 
I know this isn't Go, but it is a really cool bit of internet history from the first site on the [WorldWideWeb](https://www.w3.org/History/19921103-hypertext/hypertext/WWW/TheProject.html) and with the name being related, I thought it may be interesting to others. There was even a GopherCon.
Not sure why we need yet another "Getting started" tutorial. The [official documentation](https://golang.org/doc/) (Getting Started, A Tour of Go, Effective Go, etc.) is beginner friendly enough as-is. 
You're right I'm already reading about it! Guess I have to find a different solution to this
Reading files you deployed with your project to Heroku should work just fine. Creating files is not reliable as your application might just be thrown into a different container and the old one with your files is just gone then.
I see - thanks for clarifying!
I've used tebeka's library quite a bit. Works well with selenium server and chrome and Firefox drivers. If you're writing tests, I'd also recommend using goconvey (http://goconvey.co/) for slightly more descriptive tests that work well with a website's structure.
It's a cool idea to package it as a small microservice, but to have to run zookeeper in production seems like a deal killer. Especially when it's just to compensate for picking Mongo instead of something more sensible like postgres. 
I'm glad you liked it! And thanks for spotting the typo, fixed!
Yep, this is what I used in 1994 when I was a freshman in college. Then Netscape 1.0 came out around December that year and we never looked back. 
I just started using chromedp, it works as advertised the only problem I have had so far is apparently some websites can tell you are running in chrome headless and will act differently, this can lead to some gotchas.
Hey.. so both locker and storage are plugable so it's just matter of minutes/hours to implement new handlers like postgres and etcd instead of zookeeper.. If you have more ideas please open issues or discuss it over our gitter ;) thanks! 
Not sure if this is very precise, but this is how I did that in a test just to see how much time all of the subtests took. startTime := time.Now() // Do Stuff result := time.Since(startTime)
I just pushed a change to a separate [branch](https://github.com/iwalz/tdoc/blob/grammar_fix/parser/tdoc.y) and the result is kinda close to what I want: # make yacc &amp;&amp; go run main.go -d samples/basic.tdoc rule program: declaration_list never reduced 1 rules never reduced INFO[0000] COMPONENT IDENTIFIER ALIAS TEXT DEBU[0000] cloud DEBU[0000] third DEBU[0000] as DEBU[0000] blubb INFO[0000] COMPONENT IDENTIFIER ALIAS TEXT SCOPEIN declaration SCOPEOUT DEBU[0000] cloud DEBU[0000] second DEBU[0000] as DEBU[0000] bar DEBU[0000] &amp;{{0 [] [] false} cloud third blubb} INFO[0000] COMPONENT IDENTIFIER ALIAS TEXT SCOPEIN declaration SCOPEOUT DEBU[0000] cloud DEBU[0000] first DEBU[0000] as DEBU[0000] foo DEBU[0000] &amp;{{0 [] [] false} cloud second bar} But as you see, declaration_list will never be reduced and I'm not sure why ... The grammar looks as follows: program: declaration_list; declaration_list: declaration | declaration declaration_list; declaration: COMPONENT IDENTIFIER | COMPONENT IDENTIFIER ALIAS TEXT | COMPONENT IDENTIFIER ALIAS TEXT SCOPEIN declaration_list SCOPEOUT; I get the same error, if I follow your suggestion with the declaration_list example. 
time.Now(), returns system time, on the other hand, clock (C) returns an approximation of the processor time used by the program
How about using the same: //#include &lt;time.h&gt; import "C" import "time" var startTime = time.Now() var startTicks = C.clock() func CpuUsagePercent() float64 { clockSeconds := float64(C.clock()-startTicks) / float64(C.CLOCKS_PER_SEC) realSeconds := time.Since(startTime).Seconds() return clockSeconds / realSeconds * 100 } Source: [stackoverflow comment](https://stackoverflow.com/a/31030753/112129)
Okay good. I'm not the only one that thought this.
**EXCELSIOR!** package main //#include &lt;time.h&gt; import "C" import ( "fmt" ) func main(){ init := CpuTime() // .... fmt.Printf( "CpuTime %d\n", DiffCpuTimeByMS( init, CpuTime() ) ) } func CpuTime() uint64 { return uint64(C.clock()) } func DiffCpuTimeByMS( begin, end uint64 ) uint64 { return (end - begin) * 1000 / uint64(C.CLOCKS_PER_SEC) } compiling $ go build main.go
You can use time API with Go 1.9 https://tip.golang.org/pkg/time/#hdr-Monotonic_Clocks
To expand on his post, It's not as one dimensional of a decision as mutability. Structures that are read often but larger in size (ast) may make sense to be pointers as well, if every single implementation of Node had a Value receiver you would spend all your time copying around memory just to read it. That doesn't make sense of course, you wouldn't: cat file-I-want-to-read.txt &gt; copy.txt more copy.txt rm copy.txt
To avoid the possibly largish copy. Pointers are just a word(a few bytes) while the types pointed to could be many bytes. The indirection is possibly cheaper than the copy, so a pointer was used. Imagine a program moving that type through many function calls, that copying begins to add up. Now, imagine that type just being a pointer, the load has been significantly reduced. Your type stores 3 words. A pointer is 1 word. There is potential for both to be beneficial. Indirection vs copying. Pick one and profile. 
thanks for the info. I have updatedand and tested, althought the C clock() is more consistent. The official documetation... is a bit confusing 
Why are you discounting `Testing.B`? It's like you're asking "How to print to screen? (no fmt.Print)"
If history has told us anything it is that the core Go Team gives no fucks. They do what they do, and that doing is not influenced by anyone or anything outside of Google.
I believe the Go project is managed by Sameer Ajmani. ** so you down vote without rebuttal. Don't be a fucking tool. Actually research the information I gave you and you will realize it is fact. The Go project is managed by Sameer Ajmani. You brainless fucking twit.
From what I understand Pike had a hand in creating the language, and helped with the implementation. Pike now mostly just messes with big data within Google. So Pike mostly works with the language and sometimes on the language. However, his opinion is valued, and when Pike speaks up people mostly listen.
Rust syntax is unfortunate indeed, but it is actually one of the most strict languages available. Compiled rust program is practically working program.
b.Log....? No, it is to make comparisons of regexps with several languages and libraries (comming son \^-\^ (again...)), and the output of `test` is far from being appropriate for that work
Original: https://www.youtube.com/watch?v=hsgkdMrEJPs
&gt;[**Go for DevOps [27:19]**](http://youtu.be/hsgkdMrEJPs) &gt;&gt;Caskey L. Dickson &gt; [*^linux.conf.au ^2017 ^– ^Hobart, ^Tasmania*](https://www.youtube.com/channel/UCDMo9DyACXG62ak5cVgs3TA) ^in ^Education &gt;*^5,344 ^views ^since ^Jan ^2017* [^bot ^info](/r/youtubefactsbot/wiki/index)
Sometimes the language is chosen for you. There is very few times that I could pick the language.
Seems like we have different levels of trust in third party libraries ;)
Fixed link: https://github.com/PyO3
I believe the Go project is managed by Sameer Ajmani. ** so you down vote without rebuttal. Don't be a fucking tool. Actually research the information I gave you and you will realize it is fact. The Go project is managed by Sameer Ajmani. You brainless fucking twit. 
I'm sorry that downvotes makes you so angry, they are just fake internet points though. It seems stupid to argue about Google and Go, but regardless of the HR structure at Google, it's seems rsc has taken the lead on technical decisions for the Go project.
I don't know too much about TLSH. Does it perform well finding similar text documents? Or, it's mainly designed for finding similar binary mutations? From a quick glance at the paper, it seems like the distance function is not using hamming distance. Is that correct? One of the advantage of using hamming distance as a similarity metric is that you could use something like [multi-index hashing](https://www.cs.toronto.edu/~norouzi/research/papers/multi_index_hashing.pdf) for fast lookups massive amount of data. Are there known methods for fast TLSH similarity lookups on massive data?
Well I'm sold. Time to learn go. 
Expanding on this, docker swarm, kubernetes, nomad etc have a notion of services that the orchestrater will keep running, even across multiple hosts with some approach to dynamic routing of requests. The details vary per platform. 
Maybe :) Generally speaking, a dedicated library can solve problems more properly, especially there're some tricky corners involved. Maybe I have found one tricky corner here, see the use of `cmux.HTTP2MatchHeaderFieldPrefixSendSettings` in [this PR](https://github.com/RussellLuo/protoc-go-plugins/pull/1) and the [Java gRPC Clients limitation](https://github.com/soheilhy/cmux#limitations).
Not going to review this fully and I've been wrong before, but your implementation looks flawed at first glance. Your call to set [Set](https://github.com/OneOfOne/cmap/blob/master/cmap.go#L83) forwards to [shard](https://github.com/OneOfOne/cmap/blob/master/cmap.go#L67) which calls your hash function and mods into a bucket, which you return and forward to [lmap.Set](https://github.com/OneOfOne/cmap/blob/master/lmap.go#L10) which is nothing more than another map... with a rwlock. What is odd here is how the map key is a eface but the key is always a uint32 it seems? Anyways the major bug here is that you never check for value equality before overriding the hashes in the buckets. You simply assign to the underlying map. So in the event of collisions there is data loss. In addition using this map for anything security related is unsafe as any fowler-nol-vo hash or any derivatives I have seen lack most properties of a cryptographically secure hash function. Most importantly poor diffusion doesnt meet bit independence criterion, so using this in a web server for session keys for example would be unsafe. Regardless you put good effort into it, I just feel map implementations are too critical to aim for many "features".. iterations with call backs and cancelation etc. This extra noise makes it hard to form strong invariants around the implementation, it's why those tasks are normally left to users and maps remain a simple primitive serving solely as a building block.
You can use the [testing](https://golang.org/pkg/testing/) package as a regular package in a regular go file... this is an example: https://golang.org/pkg/testing/#example_B_RunParallel EDIT: There is a function `func Benchmark(f func(b *B)) BenchmarkResult {}` that gives you a `BenchmarkResult` without outputting it.
If you think there is a problem with the code, welcome you through PRs and Issues pointed out it, we will be very grateful to you.
brilliant! promising and interesting, I'll prove it tomorrow
You completely missed how it works. .shard hashes the key and picks a shard based on the key. Each shard is a rwmutex map. Fanning out the locks using the hash allows it to scale on highly concurrent systems. Also the shards are a slice of maps not a map. Collision is handled by the standard go map. I'm not sure where you got that the key is always uint32, that's the hash used to pick a shard.
I love go! I am a convert, except for the fact that it is a pain in the ass to do things dynamically, but I am learning as I go and having fun! 
Can we get a synopsis for those who can't watch the video?
&gt; You completely missed how it works. .shard hashes the key and picks a shard based on the key. &gt; I'm not sure where you got that the key is always uint32, that's the hash used to pick a shard. I see I didn't notice you were passing the original eface to the underlying map- it appeared to be chained into it since it's a call on the return value directly. My mistake. I understood everything else you just reiterated. I was mostly keeping my opinion to myself and just wanted to point out a potential subtle bug. I'll now point out that this is probably the most inefficient and naive implementation of concurrent map iteration possible. You are talking about performance and scaling, but to iterate your map you allocate shardcount times a total size of the length of the entire map [every single time](https://github.com/OneOfOne/cmap/blob/master/lmap.go#L73), you only hold read locks while copying to the transient slice then you lock... [every single individual element](https://github.com/OneOfOne/cmap/blob/master/lmap.go#L80) at least once. This will _not_ scale on highly concurrent systems. It will benchmark well on systems that are afk. On highly concurrent systems you are creating tons of scheduler pressure for mutex contention for even iteration. You could have multiple concurrent iterations happening in MULTIPLE requests causing multiple concurrent iterators thrashing against the same mutexes. Multiply this by the shard count and you have 10 concurrent requests iterating a 1k sized map- you suddenly have 10K syscalls in contention for something that would have taken a few milliseconds otherwise. We haven't even began to send values over the channels yet, which has a very unusual interface: &gt; // Iter returns a channel to be used in for range. &gt; **Warning** that breaking early will leak up to cm.NumShards() goroutines, use IterWithCancel if you intend to break early. &gt; It is safe to modify the map while using the iterator. func (cm *CMap) Iter(buffer int) &lt;-chan *KV func (cm *CMap) IterWithCancel(buffer int) (kvChan &lt;-chan *KV, cancelFn func()) Instead you could simply have: func (cm *CMap) Iter(ctx context.Context, chan&lt;- *KV) So I don't have to create goroutines to manage your cancellation "rules" that emulate context.
Yeah... "Martini’s best feature is its use of reflection" I am skeptical about the Framework list in its entirety, actually. Frameworks are kinda the "F"-word of Go; they don't really fit in with the language at all. Collections of libraries like gorilla seem to work out better long-term. The rest of the lists have a variety of things I recognize and don't, and I find the ordering of most of them to be questionable at best :).
I have always found that writing my own, purpose-built mocks is faster, more readable, and overall less fuss than using a mocking framework, especially in Go -- it's so trivial to mock something out: type fooMock struct { interfaceYoureMocking } func (fm *fooMock) FunctionBeingMocked() { ... } Now there's no "domain specific language" of the mocking library that anyone has to learn, and you can use Go code and standard go testing library idioms (subtests, table driven testing, etc) to define your behavior.
&gt; the output of test is far from being appropriate for that work No idea what you're on about. The output of test is perfect for that use case. 
Especially because the quiz distinguishes between (0+0i) and (0.0+0.0i) which are [identical](https://play.golang.org/p/gb_EU6T7NE)
I didn't read the code, just curious who cares about the "code quality" of a tool? It's not a package you import to your project. Did you review the source of Go compiler / go doc / go playground? Does it bother you they don't get "all the points" either?
** edit ** this is a mostly angry reply after a long work day, take a massive grain of salt. -- deleted -- &gt; . You are talking about performance and scaling, but to iterate your map you allocate shardcount times a total size of the length of the entire map every single time, you only hold read locks while copying to the transient slice then you lock... every single individual element at least once. This needs optimizing, yes, however I chose to do it to be able to modify the map live vs locking while using ForEach / Iter. *edit* Also, again, you misread the code and didn't understand how it works, this is *per* shard, not per map, lmap is a *shard*, I'll just rename it, maybe it'll make it easier for people to read that aren't familiar with Go. &gt; Multiply this by the shard count and you have 10 concurrent requests iterating a 1k sized map- you suddenly have 10K syscalls in contention for something that would have taken a few milliseconds otherwise. syscalls .....? &gt; func (cm *CMap) Iter(ctx context.Context, chan&lt;- *KV) This is actually a decent suggestion. So while I truly dislike your attitude, I appreciate the random tips, after all I posted it for feedback. -- deleted -- &gt; So I don't have to create goroutines to manage your cancellation "rules" that emulate context. -- deleted --
&gt; Compiled rust program is practically working program. As a user of some compiled rust program let me just disagree there. The advanced type system and borrowing semantics of Rust prevent a certain category of bugs. For any non-trivial program this category of problems is only a subset of those you'll definitely run into.
Will the doctor get sick? The purpose of the code quality report is to allow developers to write better code. Even if there are non-standard code and problems in the source of go, but this does not affect our judgment on good code.
I read this and I still don't know the thing about dates.
Martini got criticized for introducing "magic" behavior through heavy use of reflection that made it difficult to understand what's going on behind the scenes. The author heard the critics and developed Negroni (now maintained by someone else [here](https://github.com/urfave/negroni)), which basically is Martini without the magic. 
Excellent, I have been waiting for more tools to use with boltDB. Hope this development keeps moving forward.
That you need to declare a format that's based on a date placeholder and not some kind of relatively standard date tags from functions available in C (strftime, strptime?). Maybe I should make posts shorter :D
This must be the worst reason to advocate an ORM library. I'm pretty sure you don't need a bulldozer when you want to repot a cactus.
It basically explains how you can write one app and run it everywhere, and explains how you can change the filename to include it only in that build. E.g. main_windows.go will only be included if you set the variable GOOS=windows Further more, you can add the following header to any file, and it will only be included when building that platform. // +build linux, darwin, solaris ...
Oh no please don't. cgo has a rather large overhead, distorting your measurements. 
I'd recommend having a look at the result section of the paper. TLSH performed better on raw text than SSDEEP and SDHASH, but there might be other better performing hashing methods for text similarity. The distance function is a modified hamming distance with the addition that they consider outlier in the upper and lower quartiles. I haven't had time to look at the paper you mentioned properly, but if they base their solution of hamming distance features, I don't see a problem applying it on TLSH hashes.
I was rather polite in my first post, told you I only gave it a glance leaving room to concede to a honest mistake, which I then did. I don't have time to audit every single repo posted- but ones that could have security implications I try to at least one-over to get a general idea. I'm not having a bad day, I'm doing what I do here consistently and with the same tone and general demeanor. I omitted most of your design issues and stuck to what I felt was important. Had you corrected me I would have left it at the first sentence apologized and moved on. You decided. To post like this. With abrasion, letting me know. You were not sure how I got the key is a uint32, despite my opening sentence making it clear how it could come to be. I simply responded again but this time was more critical on the implementation itself due to your response inviting it. You can feel free to think I am a troll or having a bad day if you choose. &gt; This needs optimizing, yes, however I chose to do it to be able to modify the map live vs locking while using ForEach / Iter. There is no way to optimize it, you either lock each entry while iterating or lock once before you begin. &gt; edit Also, again, you misread the code and didn't understand how it works, this is per shard, not per map, lmap is a shard, I'll just rename it, maybe it'll make it easier for people to read that aren't familiar with Go. What about it did I misread? I stated that for each shard you have to lock once to build your slices, then you lock while visiting each slices element. Iteration calls EVERY shards ForEach method, so the number of locks is constant with the number of map elements, plus the number of shards as I stated. Furthermore the "new to go" comment is just silly, get over yourself bud I misread a single line of your code. &gt; This is actually a decent suggestion. &gt; So while I truly dislike your attitude, I appreciate the random tips, after all I posted it for feedback. &gt; Try some camomile tea or something. Try reading my initial post again weeks from now when the initial "trigger" of someone providing criticism of your work fades away. When there is nothing to make you "angry" and put up defense mechanisms you will realize it was really harmless with no hidden aggressiveness or trolling. &gt; syscalls .....? Yes. RWLocks can cause syscalls to. RWLocks have atomic fast paths but they ultimately branch into semacquire which may branch into spinning or ultimately futex system calls. The slow paths can be more costly while under high contention. This means a single writer updating each element while iterating concurrently while a larger group of readers iterate may wreak havok. &gt; This is again the wrong assumption, the intended usage (my fault for assuming people will look at the tests, I'll eventually add examples) Again with the defensive under-the-breath attacks. Even when your admitting your code has no tests, poor documentation, and a confusing API you're still trying to blame others. Don't post code here if you don't have the skin for it.
Really fun, but not practical imho. I'd love to see support for PlantUML syntax!
Or maybe longer ;)
Alright, we got off on the wrong foot. I'll reread what you said in the morning. Also I did few updates and switched to context.Context for iter and added ForEachLocked that doesn't allocate a slice with all the map's keys. And for iter it just calls .ForEach so at most it's one 1 shard RLocked at a time.
Want to give a stab at implementing it in assembly, or would it not work (ie, improve the situation)? :)
No. Just use the Go API. Though I guess you could tr to read the performance counters in assembly.
Generally, random projection using the dot product as a hashing method works well for text similarity. A good example of this in practice has been its use in Tweets: http://homepages.inf.ed.ac.uk/miles/papers/naacl10a.pdf
It would be faster to actually draw a diagram by hand and then take a picture of it with a mobile.
Build libsodium yourself and create a dynamic library (dll/dylib/so) then use it directly from golang.
Sure it's not super practical, but it's damn cool also ! Great work !
Thanks! Any constructive feedback is a positive feedback. ;)
I had a nice improvement for PGP implementation which doesn't let you pick the size of generated keys but my contribution was rejected after I refused to submit my full name and address to Google. I think that requirement is a huge deterrent and I think there is no reason to require it.
Before continuing on this path with the reasoning "this C library is audited, so I should use cgo to use it", please read this article: https://dave.cheney.net/2016/01/18/cgo-is-not-go It might be that using libsodium via cgo is right for you, but in general, cgo should be your last option, not your first.
&gt; learning as I go pun intended?
There's an example of using benchmark outside of `_test.go` in my article https://pocketgophers.com/exploring-alternatives-with-go-run/
&gt; Compiled rust program is practically working program. thread 'main' panicked at 'called `Option::unwrap()` on a `None` value', /checkout/src/libcore/option.rs:335 note: Run with `RUST_BACKTRACE=1` for a backtrace.
There's no point in re-writing a library from C to Go if said library already works. So I don't see you point for not using cgo here.
Well, it least it had potential to be reliable, before someone decided it is a good idea to make unsafe shortcut. I cannot say I am surprised: I see too much hipsters around Rust a too much attempt to include wannabe features in the language, just like these poorly controlable functional map(func, list) -&gt; list or filter(func, list) -&gt; list Also, direct adaption of ADT for error and optional states made reability a lot worse than it could be (without losing type safety) - pattern matching is too verbose. Personally, I believe the Rust is rather dead than alive, "thanks" to mindless syntax choices and poor decisions in stdlib like one you have shown.
Thank you. I kind of understand it now.
Yes, I do read source of many things I use. The compiler/stdlib are actually pretty darn good, most of the time.
I for one trust Adam Langley more than whatever CGo wrapper for Libsodium you can find. Use golang.org/x/crypto.
My favorite BoltDB shell is still my shell. https://github.com/bazil/bolt-mount Also, this tool doesn't seem to handle binary data.
Tldv pls
lol no generics!
Similar to the talk from Russ - they don't want a wishlist of features or problems to illustrate a feature you want, but a list of concrete problems with ecosystem, libraries, language which people who actually use Go have found when using it. So give them well described problems, not solutions. There's also a q&amp;a at the end which is pretty good and worth listening to.
Not quite what I have in mind for thunder, but nice indeed. Bolt should definitely link to those tools in the README.
If you read Dave's article and you still think that, then ok, let's agree to disagree.
It would be great to have sound directly from the mic and the slides directly from the computer feed!
I am looking for a unified format, to send tests and receive results between different languages. `go test -bench = .` and its output is not the most appropriate
Well, that's a good point. However, it does matter whether you can find these kind of errors with grepping for `unsafe/unwrap` or checking all dereferencing whether it's safe or not.
A practical application of this would be a part of a markdown parser. Write your markdown, add a ascii chart. Markdown is presented by a server, ascii chart is converted to pretty. So, pitch this to the Github/Gogs teams!
Liar liar pants on fire. Stdlib is good enough (by the synthetic "code quality" criteria; as I said these are the packages you import, so you check them). Compiler internals aren't. I would even dare to say that if developed not by a Go Core Team some characters here in /r/golang would "consider the compiler code harmful" whatever that means.
Absolutely 
No, that's not what I or they are saying. It's fine to say *I want generics*, but it's pretty pointless and an example of providing a solution not a problem. Better to say - containers in go suck because x,y,z, we want to use a red black tree for 10 diff types and it's painful, we generate go code and it's painful, I want to use range on my own types etc, or unboxing typed arrays is painful, or I find error handling verbose or difficult etc, because that lets them see what flavour of generics (from the many possible choices) are actually required, and which bits of the language/tooling etc need improved, specific to go. My bet is they will go with some form of generics in Go 2, the question that is completely open is what form it will take, which will be affected by exactly what problems they have described to them. Nobody is obliged to waste their time doing that, but if you want to influence the Go team, I'd suggest doing exactly what cloudflare did after the monotonic clock issue - describe the problem and why the current solutions don't work. They actually mention that in the talk and Q&amp;A here and say their initial instincts on the clock was wrong (smearing or separate pkg), and seeing the concrete problem helped. Lack of generics has not stopped my using go professionally (or you), and in my experience has been a minor wart rather than a blocker (this may vary for you). For me it's not a huge problem, I'd rather see errors or nil cleaned up. 
Well, `unwrap` et al are kind of evil, but necessary IMHO. How else do you propose to handle a situation, where you encounter an error but can't return a `Result` ? Even Haskell has an equivalent of `unwrap`/`panic` AFAIK. 
Awesome. Thank you!
It sounds like you're trying to use the [html/template](https://golang.org/pkg/html/template/) package here. Try reading those docs carefully; without more information, it'll be hard to help you here. Can you output the variables to the page at all? If so, that's how you output them into the `src` attribute.
[removed]
Tank you! the time of C.clock (time.h) is still more constant, but this is clean enough to replace it bResult := testing.Benchmark(func(b *testing.B) { b.N = 3 for i := 0; i &lt; b.N; i++ { // ... } }) fmt.Printf( "loops %d, average time %s\n", bResult.N, bResult.T / bResult.N ) 
Don't forget that your result will be bias if, say, one of those libraries isn't performing error checking while the rest are. ;)
Of course! Each library has different features, code and options, it's just a mere fun to compare my amateur creations
I never agreed to your "synthetic" criteria.
Although since testing.Benchmark has more characteristics, The output that delivers, puts it at a disadvantage with a simple measurement of time
https://github.com/GoKillers/libsodium-go appears to be the most up-to-date and the most well-maintained. And CGo is not evil. 
I am glad we have this video but I can't listen very well to the questions. Has the Q&amp;A been captured with the mic sounds and can we have it please?
To be more direct, I felt the article was short and that it ended suddenly. In fact I looked a few times in case it had a link to a part 2 or a link to the second page. Maybe it needs a small conclusion paragraph. On a related note, since I've been using Go and the time package for quite some time all the information in this article is "everyday Go" for me. So I might have had the expectation that I would read some kind of opinion or story in the end as a conclusion. Maybe the title gave me that expectation, I don't know. Instead it was more like "Hey people, this is new to me, it's different from PHP, here is how it works, the end". I think people new to the time package or the language will find the article useful though.
And this is why I hate the AWS SDK for Go. It uses pointers for all the base types for no apparent reason, forcing constant conversion back and forth between eg `string` and `*string`. Drives me nuts.
You link someone to something, they reply to it, and you start off *questioning* whether they actually read it?
The whole `sync.Map` debacle, and all the necessary usage of `interface{}` even in the stdlib should be great cases that generics, or parameterized types have good value. I even prefer the Java way of boxing everything to make generics happen, compared to type assertions on `interface{}`. Even so, I would much prefer support for ADTs and some kind of exhaustive pattern matching on types to aid in static verification of complex domains.
&gt; It's fine to say I want generics, but it's pretty pointless and an example of providing a solution not a problem. As you point out in your next sentence, we all know the problems and use case. Given that, the demand for experience reports with existing code comes off as partially disingenuous. I'm totally sympathetic to the idea that golang needs a design for generics or the equivalent that is "go flavored". We shouldn't repeat the mistakes of Java's generics, let alone the horrors of c++ templates. But at this point the discussion has become poisoned. Over and over I see people saying "I don't want generics but ..." and then they go on to exactly describe generics with no hint of awareness or irony. Additionally, there are some stupidly smart people wandering the halls of google. I'm quite certain that if people got serious about it instead of dismissive, we could find people who can go through the gory details of type theory and arrive at a design that makes sense in the context of golang.
This is the correct answer
&gt; I am looking for a unified format Dude, formatting is a problem that's been solved for DECADES; sed, awk, perl, etc. make formatting a non-issue. You can't throw a perfectly good tool away simply because you don't like the output. Also, you're not saying why `Testing.B` is not appropriate... you can't get good feedback if you don't state your problem. If you want testing AND formatting in the same package, you're making your own life more difficult. Separate those concerns, do your testing in one step, do your formatting in another step, you'll get what you need.
You can also try https://github.com/raff/godet (another "driver" for Chrome, not as complete as chromedp, that I wrote) In any case you should be able to control Chrome (via chromedp or Godet) even if it's not running in headless mode, assuming you do have a "screen" (just pass "--remote-debugging-port=9222" to Chrome when starting it up) 
Good info. Is there a way to get the entire HTTP response body with chromedp? I want to pull down a JSON file and have form fill working, but haven't found how retrieve the full body yet. Still looking into the others.
I think that tests in regular programs are a valid application, but for more explanation the test should recive: a) file b) loops c) regexes for example ./regexp-engine-x input.txt 5 "regexp-a" "regexp-b" "regexp-c" must deliver (in a file) with the results so bestTime matches regexp and then make a table with this data so I found it easier to do this in a regular program package main //#include &lt;time.h&gt; import "C" import ( "fmt" "io/ioutil" "os" "strconv" "regexp" ) func main(){ if len( os.Args ) &lt; 4 { fmt.Fprintf( os.Stderr, "re2 (Golang): wrong args" ) os.Exit( 1 ) } data, err := ioutil.ReadFile( os.Args[1] ) if err != nil { fmt.Fprintf( os.Stderr, "re2 (Golang): %v\n", err ) os.Exit( 1 ) } n, err := strconv.Atoi( os.Args[2] ) if err != nil { fmt.Fprintf( os.Stderr, "re2 (Golang): n arg: %v", err ) os.Exit( 1 ) } if n &lt; 1 { fmt.Fprintf( os.Stderr, "re2 (Golang): n &lt; 0" ) os.Exit( 1 ) } const fileOutput = "result.re2Golang" out, err := os.Create( fileOutput ) if err != nil { fmt.Fprintf( os.Stderr, "re2 (Golang): %v\n", err ) os.Exit( 1 ) } defer out.Close() for _, rexp := range os.Args[3:] { re := regexp.MustCompile( rexp ) var bestTime uint64 var oneLoop bool var result int for i := 0; i &lt; n; i++ { start := CpuTime() matches := re.FindAll( data, -1 ) time := DiffCpuTimeByMS( start, CpuTime() ) if !oneLoop || time &lt; bestTime { bestTime = time oneLoop = true result = len( matches ) } } fmt.Printf( "loops %d, best time %dms, matches %d, rexp %q\n", n, bestTime, result, rexp ) out.WriteString( fmt.Sprintf( "%d %d %q\n", bestTime, result, rexp ) ) } } func CpuTime() uint64 { return uint64(C.clock()) } func DiffCpuTimeByMS( begin, end uint64 ) uint64 { return (end - begin) * 1000 / uint64(C.CLOCKS_PER_SEC) } 
I haven't tried, at first blush after perusing https://godoc.org/github.com/knq/chromedp I would say perhaps ``` OuterHTML("*", cdp.ByQuery) ``` might work for some html pages... not sure about json. If you figure it out I would like to know! I'll try a few things if I get time after work.
Yeah, that was my point, you got it exactly. I'm not trying to reinvent the wheel (or write my own package for datetime handling), but only document something that I learned about Go after a couple of years of using it. I really don't know how this thing blew past me so magnificently, and in some ways I also don't understand the reasoning why it's departing from the (well understood) strftime/strptime (which are C stdlib). \o/
Let's not be pedantic here, generics are a single problem space out of a countless stream of features that trickle through the Go team. The point here is to promote this proposal structure for that feature stream in general. The Go team is fairly reasonable and don't want a regurgitated computer science topic anymore than the rest of us. But we should all recognize having overlap of those discussions in a open space does lead to new discovery of implementation details specific to Go. The monotonic clock is a fantastic example of this practice being a success. Had any contributor implemented the suggestions in early proposals we would have missed out on the discovery process that led to Russ's rather clever implementation. 
I agree, to start anew- good job on the changes and redirecting the conversation around my criticism into something productive. The interface sounds much nicer now so it sounds like this will clean up nicely. Because of this here is a optimization for you: since you only iterate one shard at a time, changing the signature to lmap.ForEach(buf []byte, ...) (since its private right) and allocating the temporary Buffer in the CMap entry point will reduce your allocations to 1 with a size of the max entries in a single shard plus maybe a single append depending on distribution. You could simply reset the buffer backing on entry to each lmap foreach call buf = buf[0:0]. If you only allowed one concurrent iteration or used a sync pool you could further amoritize this cost. 
Is your server running Linux or Windows?
Linux
I prefer multi-stage docker files and the alpine image, e.g. https://github.com/ipcjk/mlxsh/blob/master/Dockerfile.github Will build things in container 0 and copy only the binary and necessary files to the destination image. Its super easy. 
I think that Docker Multi-stage builds should be available now (in Docker 17.05) to simplify the build vs deploy issue. https://docs.docker.com/engine/userguide/eng-image/multistage-build/#name-your-build-stages FROM golang:1.7.3 as builder WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /go/src/github.com/alexellis/href-counter/app . CMD ["./app"] 
I'm trying to understand your idea, maybe if you elaborate a little bit, i would have a better understanding. 
Can we get a *command line tool* and not a *terminal user interface* app?
When parsing markdown to html certain things get changed, for example # becomes the H1 html tag. Ascii diagrams could be treated the same, just needs a start markup and end markup, similar to the Confluence markup language, {code} is both starth and end of the code macro that formats whats between with syntax highlighting. {diagram} ascii diagram {diagram} This in the markdown to html translator could send this part to this diagram tool that would in turn return a png file to render as an image.
The general strategy is that you run a Go app behind a reverse proxy, typically Nginx. The basic workflow is that you configure Nginx to listen on port 80 or 443, and pass calls to your local Go app, which may listen on port 8080 or 8000. You run your Go app as a systemd service so that the application can keep running even if you logout your server. Here is what I do: 1. Mirror your server environment the same as your development environment: install Go library, database (optional), other tools. 2. Install Nginx 3. Pull your Go source code to the GOPATH on your server, and build your Go app on your server. You may need to change your configuration since your server environment is typically different from your local development environment. 4. Write a systemd service file and run your Go app as a service using systemd. 5. Write your configuration for Nginx so that calls to port 80/443 are forwarded to your Golang app, which is on port 8080 or 8000. 6. Start your Nginx and Systemd, and you app should be running on your server now (if no error occurs). You may also need to configure your DNS, Firewalls, and SSL Certificates, but they are beyond the scope of this topic. DigitalOcean has a lot of good posts about how to configure all these settings. [Here](https://wiki.danceinsight.com/index.php/DAS_Application_Server_Setup) is a wiki for my own Golang app server settings. It's far from complete but should be an okay start: 
Bubble it up? 
&gt; but it doesn't work. How about you provide a little more information?
It's also the opposite, do we really needed sync.Map ?
Make sure you are cross compiling - if you do the app won't work locally but should work on the server. See example command below to cross compile. I usually host simple go apps on coreos without docker (I know :) as standalone apps - nothing required except a unit file and the go app + templates. You can use a go binary cross compiled and rsynced up with any support files required. Same process applies to ubuntu say. 1. Set up a service (usually systemd nowadays) on the server pointing to your binary location Then to deploy each time take these steps (I use a bash script): 1. local: GOOS=linux GOARCH=arm go build server.go 2. local: rsync the binary up to server 3. on server: sudo systemctl restart myserver.service I don't bother with these for go apps: * Compiling on the server (requires a full go build setup on the server) * Docker (unreliable, another part which can break, and go apps don't have dependencies) * Proxies (unless you want to share one server between apps) 
My apologizes for last night, no hard feelings I hope. I pushed a few changes, and few optimizations, one of them was major performance increase for avoiding escaping things to the heap. Also refactored Fnv32 to make it inlinable. 
Types must match exactly. You must implement the dot with signature func (a V3) dot(bIn R3) float64 { b := bIn.(V3) return a.x*b.x + a.y*b.y + a.z*b.z } And that will panic if you pass in a parameter that is not a V3. This is right where Go's lack of generics hurts the worst, IMHO. If this is the thing you are primarily intending to use Go with, rather than just something you're playing around with, well, it's all up to you of course but I'd take this as a sign of a real characteristic of Go.
sync.Map has nothing to do with general thread safe map. It even mentioned in the documentation. 
So far developing GO I have not had a "need" to use generics, yes they would be nice for some situations, but I really like a much simpler language to get my server written safely. I really like how the GO team are being very careful in feature choice decisions, because so far, I have found GO to be so much superior out of the box then any other development environment for server/services development and not to mention the awesome cross platform support. My only hope is in the future we have get better debugging tools, I really miss C++ visual studio debugging, but the benefits of GO simply out way me writing our server in C++. I have 110% confidence in the way they are handling GO version 2, as I think GO has definately been created as solution to a requirement, unlike other languages (looking at you Ruby, which I like by the way, but it really was created from a programmers wet dream :-)... And I mean that in a comical sense)
Yes it has. I'm actually working with Tim Ansell to get the sound sync'd from the room's mics
Rob's slides were mostly there, but Dave's earlier slides (the Go 1.9 Release Party slides) and audio recording weren't there (technical glitch)
And as a result of that, at every meetup now, we're going to do something called Lightning Experience Reports - 3 (or so) templated slides. Each lightning Experience Report will be recorded, transcribed and wiki'd
Hey all, we'll be uploading a full version soon with restored audio. I can't seem to upload 1hr+ videos on youtube.
Really awesome, thanks for your effort! Q&amp;As are a really important part of a talk and I am very glad you've made it available (a lot of conference talks cut the video right before the Q&amp;A).
&gt; But at this point the discussion has become poisoned. I sincerely don't see why it has to be - why not just assume good intentions? If you're bored, imagine how the go team feel :) There were legitimate reasons for putting off discussion as if it ever happens it has to be in go2 and it'd be a ton of work to do properly. &gt; As you point out in your next sentence, we all know the problems and use case. Consider this as an implementer, would you rather 100 reports saying 'I need generics', or 100 reports of which 50 are about custom containers, 20 about option types, 10 about too much casting detailing what people want to do? I think describing the problem would help a lot in narrowing down how to solve the actual problems caused by the lack.
Thanks. The Q&amp;A was unexpectedly interesting! 
&gt; I sincerely don't see why it has to be - why not just assume good intentions? I think most people do have good intentions. But this is not a new topic. This goes back years and years, to back when golang had a vector implementation in the std lib. Over those years it's become abundantly clear that some key individuals simply reject the need entirely. We are retracing the mistakes Java made, and more recently, heading down the exact same unproductive behavior as with monotonic time. We know this pattern, we should be smart enough to get ahead of it. &gt; Consider this as an implementer, would you rather 100 ... As an implementer I would not have arrived at this dilemma because I would have addressed it in the design of golang from the start. I also think you may be unaware of Ian Lance Taylor's 4 previous proposals and the very valuable discussions generated by that.
The stuff in golang.org/x/text and especially golang.org/x/text/message Be prepare for **real** **hard** work if you want to achieve your goal of "run literally everywhere in whatever languages". I'm sure you know there are languages with a very reduced concept of time or cardinality.
&gt; The monotonic clock is a fantastic example of this practice being a success. Had any contributor implemented the suggestions in early proposals we would have missed out on the discovery process that led to Russ's rather clever implementation. I strongly disagree with this. People told the team exactly what the problem was and exactly what would happen as a result. In return they got overt dismissive hostility. Go read the tickets. Only after CloudFlare got bit, in *exactly* the way people warned about, did the gatekeeping dismissal go away and people began to think seriously. &gt; Had any contributor implemented the suggestions in early proposals we would have missed out on the discovery process There is absolutely no reason that discovery process couldn't have happened as a result of the very first ticket. It didn't because key people sent a very strong signal that no action was possible on the issue. That's what I'm objecting to and what I want to see change. Part of leading is... leading. Edit: I glossed over this sentence in your post but want to respond to it: &gt; don't want a regurgitated computer science topic anymore than the rest of us. I *absolutely* want us to leverage the hard won knowledge of researchers of the last few decades. If your attitude is that computer science is worthless and has no value to the practice of programming I don't know what to tell you. That is a profoundly limiting perspective. If you program for a living you use the results of CS every day.
Agreed. I am just trying to wrap my head around what is available. Go is surprising less "Batteries Included" than expected given its popularity. I find plenty of 2 and 3 year old blog posts and semi abandoned code on GitHub. So it is never clear where the consensus lies.
If we apply same logic to generics, it means someone in Go team will finally implement it only after big cluster f*** like large number of people leaving Go and/or new people stop learning it because there are no generics. I doubt that will ever happen.
Try writing an array list that is type safe, oh you can't unless you have like 10 different copies of the same code. If you're happy to use interpreted meme code then you can use interface but it adds like 30% to the execution time of the code.
Yes, I wish I'd talked about this explicitly in my first post, but the biggest problem right now is that the opportunity cost of not having them is largely invisible to a lot of people. Java.util.concurrent has enabled a ton of high performance java applications. Likewise the Osaki/Bagwell structures made famous by Scala and Clojure. These just aren't possible with golang as it stands. I know, I've been trying continuously for several years now. There is no magic bright idea to make it work. As it stands people simply won't develop systems like Spark or Cassandra in golang. BoltDB is great, but is about the best you can do at the moment for an embedded database in golang. If your workload involves a high volume of concurrent writes to an embedded database, that's just not feasible atm. I think it's perfectly reasonable for the go team to want concrete experience reports to motivate additions and changes. But we need to counterbalance that by understanding that is a *reactive* process that does not work for everything. Some things require a *proactive* forward looking approach.
From what I read Go team spent lot of time designing language and calculating trade offs. They just didn't manage to fit in generics then and now. I think they are careful not to make bad deal if they try to satisfy too many people. Honestly I respect that approach as Go user with lot of legacy code that I have to maintain in decades to come. I don't want big waves ahead. 
&gt; Try writing an array list that is type safe, oh you can't [This article](https://appliedgo.net/generics/) describes ways you can achieve that in Go today.
Those solutions are inadequate. Interface with type assertions sacrifices static type safety. Reflection has massive overhead. Code generation is currently the best approach. It slows down compilation and bloats binaries in a way even worse than properly implemented generics. Note that one of the criticisms the author dismisses generics with is compile performance, but fails to apply the same criticism to their own proposal. Also note that the quote from Russ Cox is an opinion, not based on measurements of a real proposal.
I suppose when writing in GO lately, if I need a map of strings to structures or arrays of int, or a map of ints to string etc, I dont find that I am missing generics, and most code I write is in this kind of domain, for example UserNames map[string]*userStructure UserIds map[int]*userStructure in C++ I would of used the STL std::map&lt;std::string, userStructure*&gt; userNames std::map&lt;int, userStrucutre*&gt; userIds And either in GO or C++, this pretty much works identical for my needs, and arrays are pretty easy to use with defined structures too. I know that searching etc is implemented individually in GO, but I dont find it that difficult to usually write about 3 lines of code when needed to search etc, as GO makes this very easy anyway I know that in more complex situations, the GO way will probably not work as required, but I find if I need to branch past these kind of basic requirements, the container I am writing needs more thought, and usually I like to put in data constraints to ensure the data is correct, which is generally not going to be generic enough, so generics are kind of out the door for me then anyway. I really like the simplicity of GO, and it kind of removes unnecessary code that people write that is not really required anyway (ie over engineering because one can) Anyway, my simple outlook on life :-)
&gt; This is the takeaway here, you can disagree but be an adult and don't resort to trying to place me beneath you through weakly connected logical fallacies please. I do not believe I did this in any way. The rest of your post is past my limits of conversation. Cheers.
&gt; Those solutions are inadequate. They are just different solutions of how to solve the problem each having different trade-offs and so is generics. &gt; It slows down compilation and bloats binaries in a way even worse than properly implemented generics. I don't see how code generation does this in practice. You are writing a program that writes a program. The end result is normal Go code which you could have written yourself. I could understand slowing down and bloating if you generated code for say 100 types. But in practice I doubt most projects need code generation for more than 10 types. But even in the worst case, Go compiles really fast so I doubt the slow down would even be noticeable. About binary bloat, sure the story is not great there even without code generation. But that's a fine trade-off in my book. What is sad is that people simply refuse to provide good experience reports about actual projects where the lack of generics is a problem. Most people are like "Of course you need generics! Duh!" which is not helpful at all. &gt; Also note that the quote from Russ Cox is an opinion, not based on measurements of a real proposal. I don't care at all about Russ's quote. It was written in 2009 and back then the only examples we had about generics were the bad ones. Since then, there have been a number of new languages and solutions on implementing generics which the Go team now can study and consider.
&gt;Copy and paste or use interfaces https://i.imgur.com/bjXQawj.jpg
&gt; You are writing a program that writes a program. The end result is normal Go code which you could have written yourself. The same is true of generics, but with the additional performance advantages of happening at the AST/IR level vs textual manipulation. Implementations of generics can also make choices about optimization or de-optimization, selectively monomophize or combine subnodes, etc, with that information that the author of a textual code template cannot. &gt; What is sad is that people simply refuse to provide good experience reports about actual projects where the lack of generics is a problem. First, this isn't true. People have been for years. Additionally, it's hard to demonstrate an opportunity cost given the requirements the core team has for experience reports. If something is impossible without huge drawbacks atm, it simply won't exist in the codebase of an "actual project". Otherwise, I totally agree with you above.
No hard feelings at all, I have similar conversations at least once a week on here and it's well worth it when it leads to productive improvements to code. Well done.
&gt; The same is true of generics, but with the additional performance advantages of happening at the AST/IR level vs textual manipulation. Implementations of generics can also make choices about optimization or de-optimization, selectively monomophize or combine subnodes, etc, with that information that the author of a textual code template cannot. Nice, you are exceptional at describing the pros of the generics solution. Now try for the cons. &gt;&gt; What is sad is that people simply refuse to provide good experience reports about actual projects where the lack of generics is a problem. Most people are like "Of course you need generics! Duh!" which is not helpful at all. &gt; First, this isn't true. People have been for years. Duh! Right? &gt; If something is impossible without huge drawbacks atm, it simply won't exist in the codebase of an "actual project". But it's not impossible. There are many ways to solve this problem in Go today. Besides it doesn't have to be an actual, actual project. It could also be something like "we tried to create X in Go, the lack of generics made it impossible and here's why". It just has to be legit. 
I actually think they should have a monopoly etc, this way GO remains a controlled language to suite a specific design requirement (which it does perfectly in my opinion) and doesn't get littered with useless features that have to one day be supported because now it is in the language. I really don't understand why people that have such a big issue with this simply move to another language, because anyone I speak to that likes GO, like it for exactly the same reasons I do, and they don't want it to change much either, I thinks people are complaining for the sake of complaining as other languages exist for them.
&gt; Now try for the cons. Well first, it does add something new that developers have to learn. One of golang's advantages is that you can learn enough in a weekend to get something real done. I understand people wanting to protect that and agree with it. I think generics would be very much worth it, and can be implemented in a way that minimizes the learning effort for folks who are not familiar with parametric polymorphism from another language. Put simply, if you can understand `map[foo]bar` I think you can understand generics. Binary bloat is another concern, but again, I'm convinced that if we got real about implementing generics, the result would be more compact and better optimized than using textual code generation. As you might expect from someone vocally advocating for generics, I think the benefits far out weigh the cons, and that people are overestimating the cons of a good generics implementation in go.
cool. thank you!
100% into this. I would love plantuml server written in Go
This is amazing. However, I have to ask..how do you make the ascii graph in the first place?
You missed out in my opinion the most important con. Loss of readability/[noise](https://image.slidesharecdn.com/publicstaticvoidrobpikeosconjuly2210-121226060723-phpapp01/95/public-static-void-6-638.jpg?cb=1356502304). There is also another potential issue that generics could bring in Go. We need languages to be different so that they can allow us to approach a problem from a different perspective and so that we can solve different problems with the most efficient tool. For example if you are programming in Java you have learnt to think of certain ways to approach a problem and those can include type hierarchies and generics, (among other things). But Go is about composition. The idiomatic generics solution in Go is using interfaces. Of course it can't solve all problems but for the problems that it solves it is very, very, very good. If generics were to be added in Go then you probably wouldn't even think about other solutions because 'duh! generics everywhere!'. Then some years down the line we get a book about Go design patterns advocating for using interfaces when possible and not use generics everywhere. That story sounds awfully familiar doesn't it? &gt; and that people are overestimating the cons of a good generics implementation in go. Once something gets in the language, it can't get out. You can never be careful enough when adding such an impactful feature. Otherwise I agree with what you said.
I guess I'd respond by asking if you find the existing psuedo-generic types provided by the runtime to be a readability problem? One of the reasons I believe generics in go can be much more straightforward is precisely because golang eschews inheritance. I think Java and C++ both show pretty clearly that mixing the two forms of subtyping is pretty confusing. &gt; If generics were to be added in Go then you probably wouldn't even think about other solutions because 'duh! generics everywhere!'. This is a fair point, and I think one of the non theory/technical challenges for a golang generics design. I don't have a ready made solution for discouraging "preemptive generics" but I'll think about it some more. Thanks for putting that in clear relief.
&gt; I guess I'd respond by asking if you find the existing psuedo-generic types provided by the runtime to be a readability problem? They definitely feel verbose, explicit and clunky. It is very obvious that compared to the general feel of the language, they are [not orthogonal](https://www.airs.com/blog/archives/559). But to my knowledge, they have never let anyone create the noise and unreadable monstrosities we see in other languages because of generics. &gt; One of the reasons I believe generics in go can be much more straightforward is precisely because golang eschews inheritance. I think Java and C++ both show pretty clearly that mixing the two forms of subtyping is pretty confusing. That is also the reason why I have faith that the Go generics implementation has more chances to succeed. But it still a very complicated subject. We need more, useful experience reports and a lot of real effort from both the community and the Go team to make sure this is done right. The good thing is that the talk has already started and Pike said that it is going to be a "multi-year project" (what is going to be added in Go2). So I hope those years will be put to good use.
Thank you so much,this informations are very helpful
Here's how I do it on FreeBSD (which would work about the same as Linux). On the server I run the app behind nginx, so I can easily serve statically-gzipped assets and stuff. I use make to coordinate different build tasks, and one of those is `deploy`. My deploy task does the following: 1. Build the server and the JS client (using an NPM task) 2. Runs a command via ssh to put the app into "maintenance mode" (tells nginx to serve a static html page saying it's under maintenance) 3. Runs a command via ssh to stop the app's services 4. Deploy the app's compiled binary and public directory to the server 5. Runs a command via ssh to perform any necessary database migrations 6. Runs a command via ssh to start the app's services 7. Runs a command via ssh to stop maintenance mode I was thinking about posting more particulars of how I manage all this, since it works well for me, although it's certainly not as "cool" as Docker.
Not specifically written for Go but I have a work flow tool for internationalizing python applications. Im not very experienced with Go but in the odd chance it can be helpful to you please have a look. Hopefully it can be easily adapted to extract tags from Go source code. I wouldn't mind helping you adapt it if you can point out the areas that need work. www.github.com/lobocv/translation_factory
No.
A lot of "Here at Dgraph we love robustness" self patting, but otherwise an interesting article and an interesting tool.
&gt; I absolutely want us to leverage the hard won knowledge of researchers of the last few decades. If your attitude is that computer science is worthless and has no value to the practice of programming I don't know what to tell you. That is a profoundly limiting perspective. If you program for a living you use the results of CS every day. I don't quite agree. The following article is very relevant and says it better than I can. http://www.drdobbs.com/architecture-and-design/software-engineering-computer-science/217701907
You could do this lots of ways depending on your constraints. The most correct would probably be to make the docker daemon listen on a public port over tls and use client certificate authentication. If you can't do this for some reason such as not being able to configure the remote docker service, you could easily setup a ssh reverse tunnel to forward traffic from a specific port to a local unix socket. You could also use ssh tunneling from the client side as well using your typical credentials. Google any of "ssh tunnel" "docker tls public port" "ssh tunnel unix socket"
If you don't care about it being importable (and thus don't need more than one package), sure.
ReadWrite locks typically have [poor performance](http://joeduffyblog.com/2009/02/11/readerwriter-locks-and-their-lack-of-applicability-to-finegrained-synchronization) due to the overhead of maintaining the readers/writers, avoiding starvation, etc. For large critical sections the benefit outweighs the cost, but in your case the hold times are tiny. An exclusive lock may perform better, especially if it first acts as a spin lock before failing to a using a wait queue. You may want to give that a quick try. Unfortunately to really unlock concurrency at scale you would have to write a custom hash table. Then there are a lot of interesting tricks to employ, but otherwise this is a good pragmatic approach.
I looked into writing my own but honestly I got lazy, maybe I will eventually. 
Thanks, however to add support for PlantUML syntax would require a much bigger effort. Maybe in the future i will consider to to extend it, and to integrate PlantUML too. 
Sure, this won't be a problem. I will include it.
The answer isn't "No.", it's "Yes there is a way, but the Go toolchain/ecosystem will fight against it at every turn".
First off, if you haven't already, I'd suggest taking [the tour](https://tour.golang.org/welcome/1). Second, `http` is a package, not an object^1. The snippet you posted is actually invalid Go code, since there's no `package ...` statement at the top of the file (try running it in the [playground](https://play.golang.org) and the `http` (and `fmt` and `log`) packages aren't imported. However, as it's an example snippet I'm sure the author assumed the readers would realize this. Third, `defer` is never called because `http.ListenAndServe` _always_ returns a non-nil error, which means the branch containing the call to `log.Fatal` is _always_ entered. Internally, `log.Fatal` calls `os.Exit` which immediately exits the program, skipping `defer`s. If you want `defer` to run, use a function that doesn't exit the program like `log.Print`. Fourth, there _is_ a loop going on, but it's not in `http.HandleFunc`. The loop is inside `http.ListenAndServe` which blocks indefinitely. --- [1]: Strictly speaking, Go doesn't have objects like you'd find in JavaScript or perhaps Python. The only time you'll find the `foo.bar` notation is when accessing the contents of a package (a function, variable, etc.) or when accessing a field of a struct.
I'm on my phone so I can't get into too much detail, but 1. `http` is the http library included in the stdlib. As long as you `import "http"` in the file, it will be available. 2. `http.ListenAndServe` is the blocking action. It will stop right where it is and continuously listen on the address/port provided. It is listening on behalf of both of the `http.HandleFunc` methods, and routing incoming traffic accordingly. Those methods aren't necessarily doing anything until the `ListenAndServe` is run, before that they're just setting things up.
Thank you. I reviewed the tour and couldn't find where anything like ListenAndServe is described. I'll re-review. Thanks for explaining the blocking loop in http.HandleFunc
Eureka! Now I get it! Thank you. After reading your answer this passage in the [http](https://golang.org/pkg/net/http/) documentation jumped right out at me: "The handler is usually nil, which means to use DefaultServeMux. Handle and HandleFunc add handlers to DefaultServeMux:"
Using dep on most of my projects now and finding it to be a good improvement overall. Keep up the great work!
Ok, thanks. That's what I already suspected. But func (a V3) dot(bIn R3) float64 isn't a solution at all, because there might be other interfaces V3 needs to be compatible with. 
&gt; On a different front, we’ve also got a few folks working on a preliminary spec and very early prototype for registries. Just use godoc, problem solved. Took a single person less than a minute to solve this issue, no committee involved.
&gt; We have implicit \^ instead of = as a nudge: it makes the recommended approach also the obvious path of least resistance. This needs reconsideration. I don't want to read = in Go and have a meaning and then = in the tool that's supposed to make package management easy and have a different meaning. I also want the tool to do what it's expected from it to do without it surprising me. That's how good tools work. This applies to cases where there is no lock file as well as when you actually add a dependency. Think of the case where I specifically want to avoid a version bump because there's a performance regression in it but the tool decides for me what to do. That is going to upset a lot of people. &gt; Specifying a single version, =1.0.0, makes dep ensure -update useless for that dependency, and provides negligible value over simply relying on Gopkg.lock for stability in your versions. Unless you want / need / by mistake wipe your lock file. And the command should be smart enough to tell you that a newer version of the = constrained dependency is available. &gt; Unnecessarily tight constraints are the main cause of conflicts while resolving the depgraph They are necessary if you want to guarantee that the dependencies don't move between your machine and someone else's machine if you run the command with a delay on both machines. 
&gt; dep ensure -vendor-only and dep ensure -no-vendor No such ambiguity exists in any other Go tool, thank you for pioneering this. 
&gt; dep ensure -add was introduced, to facilitate adding new dependencies. This should address longstanding grievances about the chicken-or-egg problem of trying to add an import in order to add the dep, but autocomplete tooling not working because the dependency isn’t yet in vendor/ And now users will complain about the fact that their IDE doesn't suggest the path in the configuration file. The problem is still there but it's now shifted into a different place. It also breaks the original contract that dep ensure was supposed to have and be like this easy to use, straightforward, always the single command to do anything regarding dependencies. Now this complicates the user experience by adding another layer on top of it. 
Because of the lock file there's almost no reason to ever use a fully specified version in the toml file. So what's done now is really the best default. 
Except the case where there is no lock file, like when just starting a project or the case where you are just adding a dependency, like always.
Or unless you want to specify a pinned version because of a regression (performance or logic). Now the tool just caused you headaches for no good reason.
Yeah, I agree with the others here; multi-stage builds are a good option now. The approach you've taken is still quite sensible though. One other thing though: &gt; The container exited, because the app didn’t have access to the port we wanted to access. This is not why the container exited, the container exited because PID 1 finished, i.e. the `go get` completed. This will stop the container. If you had something running as PID 1 in the container that did not exit until it was signalled to, then the container would not exit.
In response to your edits I have mainly the following to say: = is basically useless except for in a lock file. It's much better to use &lt;, &lt;= if you want to avoid upgrading to after a specific version, or ! if you want to blacklist a specific version. Then in more detail to some of your points: &gt; This applies to cases where there is no lock file as well as when you actually add a dependency. I don't see any reason why you would want to add a specific version of a new dependency and not just the latest. The case when there's no lock file is only when initializing dep in the project. Here you should just use `dep init (-gopath)`. &gt; Unless you want / need / by mistake wipe your lock file. And the command should be smart enough to tell you that a newer version of the = constrained dependency is available. I really don't see a reason why the tool should work when you delete it's version manifests. This is equivalent to saying: If you wipe your Gopkg.toml and lock file by accident the tool should be able to recreate them based on which versions of dependencies fail to compile. &gt;Unnecessarily tight constraints are the main cause of conflicts while resolving the depgraph This is what the lock file and version ranges are for. 
&gt; And that will panic unless you use the comma, ok idiom. b, ok := bIn.(V3) if ok { return a.x*b.x + a.y*b.y + a.z*b.z } 
&gt; there might be other interfaces V3 needs to be compatible with. This is no problem. As long as V3 implements R3, it can also implement other interfaces as well and still be a valid argument to `dot()`.
This was one of the weird things that first struck me with dep, why do I have to import the library before I can manage it as a dependency? Coming from python where I can just pip install foo I found deps behaviour a little jarring as a work flow, surly if I want a dependency and I explicitly put it in my dependency file dep ensure should just get it? But then I don't really know the reasons behind the behaviour. Otherwise the tool is awesome.
I'll try to address what I believe is confusion about various parts of how dependency management works. &gt; It's much better to use &lt;, &lt;= if you want to avoid upgrading to after a specific version, or ! if you want to blacklist a specific version. Both of these leave room for downgrading the dependency, which in turn will cause unexpected behavior as well. Blacklisting a version, while it seems as a solution, it's really just more work for the user to workaround the tool doing what it shouldn't do if = would keep its meaning from the language itself. &gt; I don't see any reason why you would want to add a specific version of a new dependency and not just the latest. Because bugs happen and I don't want to use workarounds to force the tool to avoid a buggy version or a version where there's a performance regression. &gt; The case when there's no lock file is only when initializing dep in the project. Here you should just use dep init Which does not solve the problem at all unfortunately. In this case the tool will just do, wrongly, what's supposed to do and need to be explicitly forced into a behavior which should be the default. &gt; I really don't see a reason why the tool should work when you delete it's version manifests. I explicitly said the lock file, not the manifest file. I have no expectations from the tool to work if both files are removed (tho it could reverse engineer a manifest file from a lock file if needed). The manifest and lock files are different files and serve different purposes. The manifest is what the user declares, the lock file is what the tool uses after evaluating the manifest file and all the dependencies contained by the project. &gt; &gt; Unnecessarily tight constraints are the main cause of conflicts while resolving the depgraph &gt; This is what the lock file and version ranges are for. I haven't complained about this, the original author did. However it does show that instead of educating users, or simply being explicit about a problem, we instead want to hide it away and have it solved in ways that could be surprising for users. Go's appeal that it's very clear and straightforward. What you read is what you get. dep should be the same.
That is because of design decisions not following a route where the manifest is sacred. Instead, the tool is performing a lot more decisions than it should on behalf of the users.
But godoc is for documentation, no? He wants something like [crates.io](https://crates.io) or [rubygems.org](https://rubygems.org) which would be really nice _in addition_ to the current way of being able to put your code wherever...
Godoc is also a search engine for finding packages. One could argue that with very little effort, godoc can gain the knowledge if where a package is located vs its import path. Having godoc able to do this would mean that everyone benefits from a simpler deployment model, since it's just one tool that needs to be deployed instead of two, while everyone gains the documentation benefit. I also know from the feedback from the community that a central repository for Go packages is really not welcomed, so if we end up with that, I believe there will be a lot of uproar against it.
Really cool project. Especially the fact that you have such exhaustive GoDocs. I have a lot of knowledge gaps when it comes to the construction of parsers. Do you have some resources you suggest to read and on which your package is based or is it written purely from experience? Maybe you could provide some high-level description of how your parser and lexer work if you find the time for it.
Some IDEs let you specify project-level environment settings for the go compiler. This is about the closest thing you're going to be able to do to achieve what you're asking.
&gt; `Gopkg.toml` Holy Moly. Do they get their inspiration from Rust? Did they follow related [rust](https://github.com/rust-lang/cargo/issues/45) [discussions](https://internals.rust-lang.org/t/can-we-rename-cargo-toml/380)?
&gt;I explicitly said the lock file, not the manifest file. I have no expectations from the tool to work if both files are removed (tho it could reverse engineer a manifest file from a lock file if needed). The manifest and lock files are different files and serve different purposes. The manifest is what the user declares, the lock file is what the tool uses after evaluating the manifest file and all the dependencies contained by the project. I think the main difference in our opinion stems from what you're saying here. You seem to think that the lock file not part of the manifests and that it can be removed without issue. In the way I look at it this is not the case. You have a toml file which indicates how the lock file should be generated, but the lock file is the source of truth for package management. If you throw your lock file away you lose this source of truth and in that case regenerating might have a different outcome. For binaries your toml file can actually be empty and set zero constraints, because your lock file contains known working versions. There's only a need to add a constraint if you want to blacklist a version that you have tried to upgrade to, but it broke stuff. For libraries it is a bit differently as people will import it and resolution will happen based on the toml file from the library and the one from the actual project. However, here it's also the issue that if you set versions very explicitly, users of the library will not be able to upgrade the dependency even though it might work. As you can see in both cases a default that doesn't constrain is the right option. However, I do understand the confusion you mention because the toml syntax has an = sign as well. One solution that comes to mind is to disallow bare versions at all. This takes away the confusion, but makes the user think about what kind of version constraints are needed.
Ok, now i have package main import "fmt" import "math" type V3 struct { x, y, z float64 } type V2 struct { x, y float64 } func (a V3) dot(bin interface{}) float64 { b := bin.(V3) return a.x*b.x + a.y*b.y + a.z*b.z } func (a V3) sub(bin interface{}) interface{} { b := bin.(V3) return V3{a.x - b.x, a.y - b.y, a.z - b.z} } func (a V2) dot(bin interface{}) float64 { b := bin.(V2) return a.x*b.x + a.y*b.y } func (a V2) sub(bin interface{}) interface{} { b := bin.(V2) return V2{a.x - b.x, a.y - b.y} } type VectorSpace interface { dot(o interface{}) float64 sub(o interface{}) interface{} } func dist(a, b VectorSpace) float64 { d := a.sub(b).(VectorSpace) return math.Sqrt(d.dot(d)) } func main() { a := V3{1, 2, 3} b := V3{4, 5, 6} fmt.Println(dist(a, b)) c := V2{7, 8} d := V2{9, 10} fmt.Println(dist(c, d)) var e V2 = c.sub(d) // :( has type interface{} fmt.Println(e) } Now V3 does not have to know about the interface VectorSpace, but this prevents the natural usage of sub()
The things in the golang.org/x/name are usually where some consensus is.
Why did you switch from `R3` to `interface{}`? And if you want to avoid runtime panics, I suggest using the "comma, ok" idiom.
V3 is not supposed to know about the existence of the VectorSpace (former R3) interface. There could be a different three-space interface that also has dot(). The given code is only a concept to show my problem. The "comma, ok" idiom would only further bloat the code.
Good article. I have a question concerning the packages organisation. On which package do you put domain services that use both "auto" and "watercraf" models ?
Thank you :) It would depend on what the application needs those packages for. The important thing about how packages are organized is that they tell you something about what's important in the domain, so it would depend on what the domain services are doing.
&gt; You have a toml file which indicates how the lock file should be generated, but the lock file is the source of truth for package management. Aside from the s/toml/manifest/ in that sentence, I agree with with everything from it and I never said otherwise. &gt; For binaries... For libraries... See, that's where this goes sideways. For a package manager it shouldn't matter for what project type it manages the dependencies. Why should it care? I argue for a model where the burden of specifying the version resolution falls on the developer not the tool as a tool will not easily have the same understanding of the dependency and how different versions interact with each-other the same way as the users will be able to. &gt; I do understand the confusion you mention because the toml syntax has an = sign as well. I don't know what are the properties of the = syntax in toml specifically. What I say is: if I tell the package manager to get me the version X it should get that one. It's how all Linux package managers work, for example. &gt; One solution that comes to mind is to disallow bare versions at all. This takes away the confusion, but makes the user think about what kind of version constraints are needed. That is a bad solution to a problem that is artificially created by the tool misbehaving.
I see what you mean. But if you change var e V2 = c.sub(d) to e := (c.sub(d)).(V2) then you can change all occurrences of `interface{}` to `VectorSpace`, and the code still works. [Playground link](https://play.golang.org/p/KfLw3UucD4) The problem of having another interface with the same functions (`dot()` etc.) could be resolved by using different names (`vdot()`, `sdot()`, `mdot()`). Sure, this is not very elegant but it helps working around the fact that Go has no method overloading.
Thank you for not naming it `package models`.
Coming from .NET for 12 years, where I practiced DDD, BDD, TDD and most recently CQRS and MicroServices for many many years, when I switched to Go in 2014 I immediately went down the path of DDD and CQRS with strong OOO... ...and that is where I wasted months of time and effort. After reading what seems like 100s of articles and golang group messages from the developers of Go, I came to realize I was doing it all wrong. Struct alignment for CPU cache optimizations, lower memory graphs, stack vs heap, etc. After a solid 3 years in Go, there's very little DDD I use these days as I found the code I wrote less efficient and harder for other people to read. Just do the minimal and keep things decoupled in your same package for easier testing. Keep a close eye on your exported API to make sure it makes sense to others. The best packages are those that the API is self explanatory. If you flood it with 50 domain objects, 19 aggregates, 40 value objects, 70 specifications, and 5 modules - it is no longer easy to follow. DDD, while needed in OOO languages, tends to have an adverse effect in Go where things end up being more complicated with object graphs of several structs and aggregates - when you could have just wrote a single func that took parameters and returned an error. PS, to save you from falling asleep reading Mr. Evans' book, there is Domain Driven Design Quickly that I usually buy copies for for friends: https://www.infoq.com/minibooks/domain-driven-design-quickly Edit: I am not a DDD hater. Quite opposite really and really enjoy practicing and applying CQRS principals at scale. But Go's power really is it still minimalistic approach that keeps it easy to read, follow and function.
Even the jerks get it.
I don’t know what discussion there was around it, but it does seem to me like something that will cause friction for some people moving from Mac to Linux. 
Great to see some instructions on DDD in Go. Do you have any other readworthy resources on this topic by any chance?
Yeah I think that's solid advice. I think when coming to a new programming language it's important to throw out how things are done in previously learned ecosystems. However, I also think there comes a time when keeping things in the same package just doesn't work anymore. I think this is where some of the concepts in DDD can help, because Go itself offers very little structure as to how to organize code. That time may never come for a program. There are plenty of Go programs out there where this would be total overkill.
Following the tradition of how some changes have been implemented in the language and standard library, I think a better approach would be to always require an explicit operator, and make a decision about what to do when no operator is used once there is a large experience using the tool.
I couldn't really find anything. I would recommend the book though. It's got a lot more to it than just what I wrote up in this blog post. I think Go is still new enough and flexible enough that people are still figuring out the best ways to design things.
For my projects i was using organisation proposed by Clean Architecture: * somemodule * domain * auto * watercraf * app * [some use cases] * infrastructure * auto * watercraf * othermodule * ... i also made small validator, to keep cross layers/modules deps valid: https://github.com/roblaszczak/go-cleanarch Of course this structure have no sense for smaller libs or even small microservices, but you will probably either not use DDD.
yep yep, we did - here's our equivalent! https://github.com/golang/dep/issues/168
Very very small nit: You keep using "GO", where it should be "Go"; like most names and titles in English, only the first letter gets capitalised (unlike acronyms like SQL and TCL). It's like seeing an article about Python that uses "PYTHON" everywhere. (But at least you don't say "golang" which isn't even the right name!)
there's a mix of opinions on registries. some people wouldn't welcome it, and i've seen a lot of the specific concerns people have raised. others are quite in favor. as the post notes, this is something we're _prototyping_, and can abandon if it doesn't make sense. the value that registries provide is, as u/rgzr notes, code hosting, and the potential for properties like immutable releases, coordination around SAs, and light validation of inputs. (again, all hypothetical). retrieving projects from registries can be done over HTTP, obviating the need to have VCS tools installed locally. none of this is to say there aren't problems, of course. management and costs of the service itself, ownership models, name squatting, regionalized availability (e.g. DMCA takedowns), SPOF, etc. etc.
(Badger author) I suspect the slowdown might be because of the write pattern -- typically, this happens if you do Sets sequentially in a single goroutine. We have never tried to optimize Badger for that. But, seen it enough times that we have a FAQ for it.
(Badger author) BoltDB has never performed even close to decent for us; which is why we didn't even try to benchmark it. Having said that, we're working on benchmarks against LMDB, which is what BoltDB is based on. Expect that soon.
Thanks for the feedback! I agree that hand-rolled test doubles frequently suffice, and full-blown programmable mocks aren't always necessary. As you need more features of a mock object (as opposed to a [stub](https://martinfowler.com/articles/mocksArentStubs.html)), the burden of re-implementing mock library features increases - at some point, you want to pre-program a sequence of responses, check actual calls against expected calls, and assert call order and frequency. All this is possible without using a library -- it's Go code in any case, after all. However, it doesn't always make sense to re-implement the common logic for each mock object you need to build. It's in these cases that you end up either rolling your own mocking library, or using a pre-made one. I'm not sure I understand your point re: subtests and table-driven tests - you can use these with or without a mocking library, right?
indeed, this is a product of the model around which dep is based. it doesn't exist in other systems, because other systems aren't built around a sync-based model. the working hypothesis is that the benefits of being sync-based are worth having things like this. at the same time, these flags are generally just additive, and need not change your daily experience of working with dep at all. i think it's fair to say that most other extant tools have considerably _more_ knobs than dep. ¯\\\_(ツ)_/¯
 Hi! This is just a friendly reminder letting you know that you should type the shrug emote with three backslashes to format it correctly: Enter this - ¯\\\\\\\_(ツ)_/¯ And it appears like this - ¯\\\_(ツ)_/¯ --- *^If ^the ^formatting ^is ^broke, ^or ^you ^think ^OP ^got ^the ^shrug ^correct, ^please ^see [^this ^thread](https://www.reddit.com/r/john_yukis_bots/comments/6tr5vq/u_you_dropped_this_a_shrug_fixing_bot/)^.* ***^Commands:*** *^!ignoreme, ^!explain* *^I ^am ^a ^bot. ^My ^owner ^is, ^[John_Yuki](https://www.reddit.com/user/John_Yuki/).* 
&gt; V3 is not supposed to know about the existence of the VectorSpace (former R3) interface. This is kind of what I was getting at when I mentioned that this is Go biting you very hard. I can tell you right now that you can basically give up on having a nicely-typed vector library; the things you are trying to do are not implementable/expressible in Go as it stands now. (You can have a _not_-nicely typed vector library with vectors that _do_ know about R^3 and all other such things you may want to implement, but not one that is "proper".) You can hack a bit around this or you can hack a bit around that, but you spend your entire "type system design budget" long before your vector library is "done". I don't know exactly what you're shooting for here, but you might consider Haskell instead; it has the type system to be able to do all these things. Or, alternatively, if you're just trying Go out, I suggest taking on a network project instead. For instance, I recently bashed out a little media player server that reads a directory, sorts things by movies or music, auto-generates m3u files for directories with MP3s in them, and dispatches back to the default file server in the standard library for serving the media files themselves so I don't have to implement the range stuff. It's not on Github right now (maybe someday), but that's a nice bite-size project (I put like probably a total of 6-8 hours into it, you'll take more if you're still learning because I've been doing Go for like 4 years now, but it's still a nice project). But, you know, whatever you like.
(Badger author here) Is it reproducible? Can you point me to your code?
I think that it always depends of the project and you cannot say that DDD will always be best choice or not. In most cases your software should be focused to have extreme performance or good models design - not both. It has no sense to use DDD for system programming and and also is easier to write and mantain projects with complicated business domain when you are using DDD instread of more classical approach.
This is a great point.
Thanks for taking the time to point me this mis-explanation, I got it wrong. Fixed :)
Is there any reason to keep Car and wheel attributes public? For example you can do in your application layer something like this: car := auto.NewCar() car.Wheels = nil with will make entity invalid - with is against of DDD rules. 
&gt; And now users will complain about the fact that their IDE doesn't suggest the path in the configuration file. The problem is still there but it's now shifted into a different place. i honestly don't understand this point, could you elaborate? &gt; It also breaks the original contract that dep ensure was supposed to have and be like this easy to use, straightforward, always the single command to do anything regarding dependencies. Now this complicates the user experience by adding another layer on top of it. yeah, i'm not the happiest about the number of paths here - see, for example, [this TODO](https://github.com/golang/dep/blob/6ca8a4832dfa85ca827d7168656289a8c85fb650/cmd/dep/ensure.go#L548). however, the original contract - if we're talking about what the spec doc from last fall described - had far _more_ knobs than this does. prior to this change, passing arguments to `dep ensure` did, simply...well, who knows. but it wasn't what the CLI help docs said. and there was still the chicken-or-egg problem, which was a major concern people brought up regularly. it's maybe possible that we could work out removing the `-add` (or rather, deprecating it), so that you pass arguments directly to `dep ensure`. but this would inevitably just be more of the opaque magic i worried about in the aforelinked TODO. the thing i like least in this new design is how `dep ensure` on its own just takes no args.
Thanks, I was not aware of this, updated the article :)
You can also check *Domain-Driven Design Distilled: Vaughn Vernon* for more general overview and *Implementing Domain-Driven Design: Vaughn Vernon* for full knowledge with examples. Many people say that it is more affordable way to start learning DDD than Evans's book.
&gt; I don't know exactly what you're shooting for here I am trying to educate myself. I spend most of the time with C. I also know C++, Java and some Python. So i feel myself quite limited in the choices that are available to me and want to extend my toolset. I think it is always worthwhile to push a new programming language to its limits. But especially with a new language it is difficult to decide whether the limits come from my limited understanding or are inherent to the language itself. Thanks for the suggestion of Haskell.
for sure, you weren't/aren't alone in this confusion! hopefully `dep ensure -add` will help you out in your workflow. as u/dlsniper notes, the basic issue is that, unlike other systems, we don't treat the manifest as the canonical representation of requirements, but rather an additive set of constraint rules to be considered. this is compounded by the fact that we don't warn the user when there's an ineffectual constraint - which would happen if, say, you added something to a `Gopkg.toml` `[[constraint]]` stanza expecting it to then show up in `vendor/`. (we have an [issue about this](https://github.com/golang/dep/issues/302) - just, haven't gotten it done yet. contributions welcome 😁) this was a carefully considered design choice, though. if we a) treat the manifest as canonical and b) have one for an entire tree of packages, then you may end up suddenly with a whole hell of a lot of dependencies you don't need. like, imagine importing just one package from k8s, but you end up with _all_ of its dependencies. or, worse, conflicts between some of those dependencies that you don't actually need. i explored these problems more fully [in a gist](https://gist.github.com/sdboyer/b0813bf2b9dba58a335a85092085472f).
&gt; They are necessary if you want to guarantee that the dependencies don't move between your machine and someone else's machine if you run the command with a delay on both machines. they are not. that's what Gopkg.lock gets you.
possibly, but there's a problem with deferring the decision: while we're waiting for that information, we're also potentially populating the ecosystem with a bunch of unnecessarily tight constraints. the risk is such that we thought it worth taking this approach. i did forget to note in the post that we're not the first to do this - cargo does as well. so we're not breaking entirely new ground here, even if it is new to Gophers.
Thanks for pointing me to multi-stage builds, added to the article :) 
&gt; Both of these leave room for downgrading the dependency, which in turn will cause unexpected behavior as well. this is certainly true, and a thing we're considering - for more reasons than just unexpected behavior - is treating an unplanned downgrade event as cause for solve failure by default. (there'd then probably be a `-allow-downgrades` flag to let the behavior occur). &gt; Because bugs happen and I don't want to use workarounds to force the tool to avoid a buggy version or a version where there's a performance regression. i may be losing the thread of conversation here, but i don't think this is a problem? `dep ensure -add github.com/foo/bar@v1.0.0` &gt; instead of educating users, or simply being explicit about a problem, we instead want to hide it away and have it solved in ways that could be surprising for users. i chose to use the dep status update - the single most powerful tool i have for drawing focused attention - to point out this problem. it's also [in the README](https://github.com/golang/dep#semantic-versioning). so, the fact that we're discussing this is a pretty clear indication that we're _not_ trying to hide it away, or avoid educating users. the basic problem is that there'll always be a portion of users who _don't_ know these things, no matter how much educational work we do. and if the tool doesn't default to open constraints, then the releases they make _in that span of time before they learn about this issue_ will probably have closed constraints. and those old releases will continue to live in the ecosystem, even after they've learned about the issue. so, no, we haven't chosen this implementation _instead_ of telling people about it. it's just part of a multi-pronged effort.
&gt; I argue for a model where the burden of specifying the version resolution falls on the developer not the tool as a tool will not easily have the same understanding of the dependency and how different versions interact with each-other the same way as the users will be able to. dep affords you all of this control, if you choose to assert it - `[[override]]` everything. it just doesn't require you to do that work if you don't want to.
&gt; One solution that comes to mind is to disallow bare versions at all. This takes away the confusion, but makes the user think about what kind of version constraints are needed. that may well indeed be better! when toolchain transition time comes and we can revisit manifest format, this is something we could consider. we can't do it now, though, as it would break our backwards-compat guarantee.
Particularly https://github.com/golang/dep/issues/168#issuecomment-286583627
&gt; Sorry, capitalizations are a de facto standard for this class of file. /u/peterbourgon saying BS with a straight face as usual. What a neat character. I'd upvote every of the Dave's posts in the thread.
I wasn't trying to be rude. It is clear that reasonable people can listen to Dave's arguments and take away a different message. The one I took away is, "the point of rewriting a library from C to Go is to get all the benefits of Go, not the partial benefits of cgo". George took away a different message, I guess. Thus, we'll have to agree to disagree.
The optimizations you described works for only very simple types like int and type aliases. Otherwise there is no difference between generics and code generation in the resulting binary size. 
People like writing unnecessary abstractions. They tend to like sharing them even more. 
I would keep them public just to keep the code more idiomatic. I guess it just depends on how strictly you want to adhere to the guidelines in DDD, but I treat them just as guidelines. I think there's a balance to be found between strict adherence to DDD and Go's idioms, and I think that balance is going to be different for different people.
I always found it fascinating that people try and demand the language with every new shiny CS feature in it, but rarely do they use those features on the actual job. Or use they language itself. Case in that point - Rust. Lovely language from the CS and design viewpoints. Type system is superb. The tooling is quite nice. The cargo is really good. Yet when I talked with people, they tend to limit Rust usage in their actual business and when they do, they rarely even use generics (no kidding). Let alone more complex features. Makes you think... I strongly support the generics addition to Go, but I think people tend to overestimate their ability in writing generic code. You can see this just by glancing how many simply incorrect concurrent collections implementations there on GitHub. 
No. If you read the book "The Go Programming Language", you will learn that this is an intentional design to organize all the modules in Go in a structured way.
thank you for the reply :) Yeah that make sense, I did notice when switching from govendor to dep that dep resulted in much less packages in my vendor directory which I liked. I'm glad to see the --add flag, I'll see if I can contribute to that issue :D
&gt; I don't see any reason why you would want to add a specific version of a new dependency and not just the latest. There's plenty of reasons to not just pull the latest version of a dependency: * There can be conflicting versions of dependencies of your dependencies. This can be solved by using versions of packages that agree on the version of their shared dependency. * Maybe the newest one doesn't work with your current DB version. * Security issues in the newest release * Performance issues in the newest release * Your company's mirror of GitHub isn't up to date. You just pulled in the latest as of 9 months ago. * Anything after version X hasn't been approved by your security team and oh, by the way, their one Go expert left so you'll have to wait 9 months for them to hire a new one. The list can go on for quite a long time. These are the kinds of issues that would cause people frustration when always defaulting to latest release. IMO the model is backwards, the dependencies should be defined ahead of time and then used in the code instead of the code dictating the dependencies file. It's probably too late in the process to voice that concern, but I can hope that it reverses.
No, dep forces me to use an workaround for a problem that shouldn't be a problem in the first place. The fact that [[override]] is there is good, but now I need to more work. Why is this hard to understand? If I do apt-get install app=1.2.3 it doesn't install 1.2.4 because it knows better. dep however thinks it knows better than me, the user and that's a surprising behavior which will cause hours of confusion for developers when it shouldn't.
&gt; i think it's fair to say that most other extant tools have considerably more knobs than dep. I would propose that this is because they've been around longer and have seen more edge cases. It's unlikely you'll find the one true solution, so flags will become necessary over time.
There is a sufficient amount of commonality that moving between Go and an(other) Object Oriented language should not be difficult. A core principle of OOP is to distinguish protocol contracts from implementation details that do not alter the contract. In Go protocol contracts are defined by interfaces. Implementation details are the values that have methods satisfying an interface.
To put that into context, in May early testers reported data loss, OOM crashes, and no `fsync` safety. https://www.reddit.com/r/golang/comments/6b57aa/badger_a_faster_kv_store/ 
Based on previous experience with this so called just prototyping approach, it never got dropped. So you can understand my reluctance to yet another thing that we really really don't need. There are a lot of solutions out there to store repositories or blobs. We don't need another one. We need dep to be stable and usable. How about we have that first then prototype anything else?
Yes, but none of the tools have two knobs which do the same thing. So that's what I call pioneering in this space.
Great way to listen to the community: &gt; sdboyer locked and limited conversation to collaborators on 18 Apr
needs to be in the Go Font, but good effort nonetheless :)
Golang won't twist your previous conceptions of OOP. You know C so I will do my best to bootstrap you into golang from that: 1)structures in golang and structures in c are essentially the same 2)how you declare your structure, how you define your structure and how you use it are a touch different. 2.1) In C you could to forward declare your structure in advance without defining its contents. I have never done that in golang, but I believe you can. 2.2)In C, when you define the structure, you are simply defining it and you don't usually instantiate it at the same time. It's the same in golang in this respect. Where they differ is that usually in golang you are declaring and defining it at the same time without instantiating it in a variable. You'll do that when you do your implementation usage. 2.3)In golang, you may instantiate a structure and initialize it in one line effectively. In C, it takes more than one line. 3)C struct, golang struct and c++ class are very similar. The only difference is C struct/golang struct members are public in scope from the beginning while c++ class members might not be for whatever reason. C struct/golang struct may also contain pointers to functions which may possibly have the same behaviour as what C++ provides, but GOLANG differs greatly from C and C++ here when discussing the recommended way to associate functions with golang structs. ok NOW let's talk about the golang "type" keyword. In my humble opinion, the golang "type" is what you call a C struct and what I have mentioned golang struct previously. "type" is more powerful when using the golang-recommended of associating functions to specific user-defined "type"s. With those functions, it almost feels like OOP and certainly resembles python's and perl's way of implementing OOP. For me it feels easier to use. The other advantage are the tight compile/link cycles. THE GO COMPILER IS SO DARN FAST!!!! Have a look at the answer here about what books best summarize golang usage and how to stay up to date with it: https://www.reddit.com/r/golang/comments/6r1hmz/what_are_the_best_go_books_published_in_2017/