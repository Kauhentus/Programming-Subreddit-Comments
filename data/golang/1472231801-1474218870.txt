We use docker to pack all our web-applications and have the exact same setup with slightly different environment configuration running in dev, staging and production. Docker compose also helps tremendously with starting services. We did even create some hot-reload containers for the test-environment (https://github.com/dkfbasel/hot-reload). To take it one step further you could use docker swarm and have all your apps running as services over a network of multiple machines and with easy scaling functionality.
Interesting - will try this one
&gt;If we can't argue anymore (due to gofmt) where to put the '{' That's a compiler requirement, not some standard set by gofmt.
What do you use to build your docker containers? A custom script of some sort? And how do you get your docker containers to whatever production server you are using, or communicate with them as to what the database URL is or things like that? I have used docker some, but not enough to really understand the "correct" way to do all of this. IIRC the last time I had to bind some ports and then had like 3 docker containers running on the same server that could communicate via opened up ports (one being the DB, another the app, the last was redis), but I have no idea if that is "correct".
Thank you. I'm having a look at these links, very interesting.
CI/CD powered by drone and kubernetes, pushes to master get built, unit-tested, containerized, deployed, smoke tested and then traffic is cutover.
/u/jjheiselman is spot on. One thing to note though that if you're using go-routines, then you should let them have a chance to shutdown gracefully (to clear resources etc). You can provide a close channel that can be broadcasted to all go-routines.
Not directly answering your question and it's not Go specific, but it's a good article about deployment processes: https://zachholman.com/posts/deploying-software
Heyo, I'm a dev on luci/gae. It is under github.com/luci, but it is totally intended to be used stand-alone. In addition to the more ndb-like struct handling (inspired by https://github.com/mjibson/goon), and the caching (which is based on https://github.com/qedus/nds, itself based on python ndb), luci/gae also has some other nifty features: * A concurrency-safe in-memory implementation of the datastore * ~150x faster than dev_appserver.py, and no out-of binary dependencies (like python or the appengine SDK). This was actually the original motivation for the project. * includes all features like queries, index selection, zig-zag merge join, etc. * No secret Python GIL serialization to make `go test -race` useless! * in-memory fakes/mocks of a bunch of other appengine services (like memcache) * Per-service API filters allow things like the injection of [specific errors in tests](https://godoc.org/github.com/luci/gae/filter/featureBreaker), but could also allow you to implement things like ACLs, query filtering (e.g. removing entities that the current user isn't allowed to see), etc. in a single place, instead of implementing them in every handler. * Recursive buffered transaction support (via the [txnBuf filter](https://godoc.org/github.com/luci/gae/filter/txnBuf)) * per-field custom serialization (field implements [PropertyConverter](https://godoc.org/github.com/luci/gae/service/datastore#PropertyConverter)) * Sane PropertyMap type (which actually models the way datastore entities work... no more goofy Multiple boolean in PropertyList) * Expando-like functionality (by marking a PropertyMap field [with the 'extra' tag](https://godoc.org/github.com/luci/gae/service/datastore#GetPLS)) And some other stuff. Someone just opened https://github.com/luci/gae/issues/56 (:)), but I'd be happy to answer questions here while we work on sprucing the documentation up.
I'm testing enki on Android, will get go quizzes soon. Love the concept of enki, so well executed 
To me, the "avoid else" example is a great example of making the code more readable. Let's say you dive into this code, and you see that else line: else { return errors.New("something not ok") } what does that else hang off of? You have to scan way up the page and try to do mental brace matching... if there's more than one level of nesting it can get really confusing. instead, if you just flip the else: if !something.OK() { return errors.New("something not ok") } Now there's no mental brace matching needed, and the whole rest of the function becomes the else. You don't need to check the scope to see if you're inside a scope where something is ok or not... the whole rest of the function can rely on something being ok. I'm not sure I'm explaining it well... but I've definitely had a lot of code reviews (from both sides) where this advice was given, and the resulting code was way easier to understand, and even better - it was harder to misunderstand.
Why no foreach iterator via range? https://play.golang.org/p/Q36UC4yQ15
Not the person you're replying to (or even a professional Go developer), but I could shine some light since we use docker a lot. &gt; how do you get your docker containers to whatever production server you are using You don't deploy or build 'containers'. What you build is a docker image. This is simply done by calling a `docker build .` (+ the flags obviously) on a Dockerfile. Dockerfile contains the steps to build the image (can be simple commands, can be links to shell scripts, whatever. Part of the Dockerfile will also some kind of copying of the application binary (war, jar, go binary, whatever) into a directory inside the future container "file system" and a specified `ENTRYPOINT` which basically says "when this image is ran, call this command".. so the entrypoint will probably read something like`ENTRYPOINT ["./goBinary", "-flag", "flagValue"]` --- This docker image you have built is now in your local docker repository (think of it as git repo, it utilizes similar syntax). Now, what I would do (or a CI/CD script obviously) is `docker push company/imagename:imagetag` which pushes the image I have created into our docker repository (probably hosted on hub.docker.com) That is all I need to do from my side. Now, the production server can access this repository as well. That means that I can run `docker run company/imagename:imagetag` from there. This pulls the image (unless I have it locally already, e.g. by calling `docker pull company/imagename:imagetag` on that server) and runs it. That is everything to it. An image is like a "class" that I can run as a container "instance". I can for exmaple now run this image inside multiple containers (e.g. using the same image for master / slave containers). &gt; or communicate with them as to what the database URL is or things like that? - &gt; IIRC the last time I had to bind some ports and then had like 3 docker containers running on the same server that could communicate via opened up ports This is basically the right thing to do. You need to expose some ports in order to be able to connect to the actual application running inside the container. If I have a service running inside that container on port `8080`, I need to somehow access it from the outside, e.g. by originally running the service's image with `docker run -p 42080:8080 serviceimage`, which maps the port `42080` from my network to the applications `8080` inside the container. ---- That being said, you don't actually run several containers going around mapping their ports. What you do is use `docker-compose`, which allows you to specify individual services (e.g. their images, arguments, ports, dependencies on each other and **networks**) as shown [here](https://docs.docker.com/compose/networking/#/specifying-custom-networks). In that case, you can make the services (e.g. your application and the database) share the same contained network and allow them to communicate without exposing ports on the outside. Another nice thing about this composing is that it maps the IPs, so you don't actually need to specify some mongodb://17.54.78.61:27017 address (oh god I don't know the valid ranges, don't hate me for invalid ip, it's late), you can just refer to it from your application as **mongodb://servicename:27017** without worrying about the actual value.
GOOD! NOW WE KNOW WHO TO BLAME! j/k
So as someone who hasn't used Travis deploy or github release, this isn't clear to me. It isn't that I think it is a bad deploy process, but if you asked me to set it up right now I wouldn't know where to start aside form researching Travis, Github, and AWS S3 and seeing what they offer and how I might connect them. Can you break down what is happening behind the scenes for me? For example, does Travis or Github compile a binary? It sounds like Github compiles a binary, Travis pushes it to S3, and then the production server executes some file form S3 after using the s3cmd to get it, but I am still unclear as to how the server knows to get a new file form s3, or really anything else about this process, but I would love to learn more.
Thanks for the reply! I haven't read your entire post, but one question that comes to mind quickly is doesn't pushing an image to the docker hub make it accessible by anyone? This seems like a bad idea if you have anything that isn't open source.
It sure can. As I said, think of it similar to git. GitHub has private repositories, DockerHub has private repositories.
thanks everyone
I do similar with one addition, Docker (/u/tynorf does the same thing). 1. `git push` triggers travis CI (you setup the webhook in github) 2. completion of that, triggers Jenkins build (builds the actual image with the latest code, pushes it to docker registry. assets are pushed to s3 with git hash release 3. provision staging with the latest docker image. If looks good, tag github with the release, tag the image 4. stop services, update docker image with latest tag and restart the service. Docker makes it easier for deployment. Ping me if you want more info.
You could start the for loop at index 1, instead of 0
Why has this taken so long? And do we really need a committee? All packages are in version control. Just put a #&lt;tag/branch/sha&gt; at the end of the import and be done with it. Seriously. Why would that not work?
Because you probably don't want to rewrite all your imports at each version upgrade.
Will the dependency tool be independent of the toolchain like in cargo?
Generics obviously Edit: Thanks for the downvotes :( Should've added inb4 or /s
Depending on your load, you may be able to get away with an OpenVZ based VPS. Sites like ramnode.com have $15/yr options -- which is a good option to get your feet wet and start rolling. Golang doesn't usually eat up a lot of ram, and there are options like fasthttp to crank a little more out of CPU/RAM at the cost of development effort. Coming from a PHP background, if you go this route, you will likely want to deviate from the typical php model of interspersed php/html. Here are some configuration options to get the most out of a weak vps: * Serve small static html pages that will call back to rest api * Use an aggressive caching model to reduce load from repeat visitors * index.html shortest duration - only used to reference secondary resources. * 1yr+ cache duration for secondary resources - use timestamps in filenames so updates will be treated as new files. * Make use of free CDN options for your libraries * You don't have to use bandwith or processing to host them * Will likely already be in user's cache if it's a popular library * Use golang as a backend/rest api that is called by the static pages. * You may be able to make use of user info here for caching on a per-use basis * If you are generating content that will be the same across multiple users (dynamically generated static content), consider putting nginx in front and caching the result, so that it will be served as a static page. * For anything truly dynamic, keep the workload light, and responses small. As I mentioned, it really depends on your site's use/workload, but you can do a lot to make the most of cheap hosting. 
Transitive dependencies definitely makes package management a challenging thing to do well. If it weren't for that, then something simple like bigfoot's suggestion would probably suffice. It's a bummer that it has taken so long, but hopefully we'll end up with something really nice.
Is there a good resource on what SSA is and why it's important?
Is this committee official, as in the Go language official? Or, is it a 3rd party?
I too am interesting in learning what SSA is.
Can someone explain to me why this is even needed in the first place? Feels like an exercise to invent a solution for a non-existent problem.
On the other hand, Go has infinite stacks, unlike most languages, which encourages recursion. Python for example, pretty much forces you to rewrite everything imperatively, since you'll crash on big inputs otherwise.
We have around 60 different microservices on our backend and while a vast majority of them are written in Java, a handful are written in Python and--with my strong evangelism (Do you have some time to talk about our Lord and Savior Robert Pike?)--a handful (and growing!) are written in Go as well. I will save you the stories of our Java and Python deployment processes and speak specifically of our Go deployment process (Honestly, the deployment process is the same across all languages--it's just the build/release process that differs slightly): As a disclaimer, our deployment process needs a lot of work across the board. I DO NOT suggest you use this process in any of your companies. First of all, it should be noted that we are like the one company in the world who, for some god forsaken reason, is using Gentoo on all of our servers--both production and development, and FWIW, we are running hardened kernels on any public facing applications. I mention this for a couple reasons: * The "Gentoo way" is to build everything (and I mean EVERYTHING) from scratch--and we happen to employ a couple Gentoo maintainers on our Release Engineering team, so god forbid we don't do something the "Gentoo way." * One of the aforementioned RelEng folk found an old article or Google Groups post or something somewhere and raised a storm about how Go wouldn't work on hardened kernels without actually having tried it. After everyone calmed down, one of my coworkers at the time actually tested things out and was not able to find any issues with Go on a hardened kernel (at least the version we were using). This was around the time of Go 1.4, and since then (we are always up to date with our Go version--in fact one of our employees is often responsible for updating the upstream Go ebuild), we have two public facing Go applications in production and have not experienced a single issue related to hardened kernels. Now, with all of that out of the way, let's talk about actual process. We started out mirroring dependencies, and as you might have guessed, this was a huge pita. Now a days, we vendor our dependencies like everyone else. We use a self-hosted Github Enterprise for our VCS solution. We have a Github "org" for our team that has all of our release code in it under the master branch of a single project. (We have hundreds of repos on our org alone--some or production systems, some are just playgrounds, you name it). When a developer wants to work on a particular code base, they fork the repo and work on their own fork, before submitting a Pull Request to the release repo's master branch (or another branch if we are doing some sort of migration or something). Working in forks and working with Github Enterprise are both a pain with golang. The best way that we have been able to do it is, instead of 'go getting' the project, we manually git clone our fork into our GOPATH, but name it as if it was the real thing. This means that we lose the ability to 'go get' internal repos, but it allows us to iterate much faster. For example, we don't need to maintain a separate import statement in deployed code, vs development code. It should be noted that our source code also contains an ebuild for our project (remember, we are running Gentoo), and project developer(s) are responsible for maintaining this ebuild (By the way, I should also mention that our ebuilds build Go from scratch. Every...freaking...time...). Once a PR is approved and we are ready for a release, we tag the main repo's master branch. We have a daemon process running on our build hosts that monitor all of our projects for new tags, and when it sees one, it triggers a (custom) build task in [QuickBuild](http://www.pmease.com/). This custom task starts a docker container that runs go build, go test (with coverage) and go vet. Assuming everything passes, the task will create a new, versioned ebuild based on the project's ebuild in our internal Gentoo portage repo and gives it an "unstable" keyword. With this, we have an "unstable" release of our project, ready for deployment. Now, at my company, we do image-based deployments, currently. We have an active effort to migrate to a container architecture, but it's not quite ready yet. The deployment process goes (roughly) like this: * We coordinate with other developers to ensure that our deploys won't conflict with anything they are activly deploying. Once we get the go ahead, we use an internal tool to trigger a new deployment onto our AIT environement. (We have AIT, UAT, and Production environments, were Neither AIT nor our UAT envs are actually what they say they are... &gt;.&gt;) * Submitting the build job will trigger a task in [BuildBot](http://buildbot.net/) which starts by building a new image, based on a stable base image and containing all of the new, unstable packages for this deploy. Once the image is build, it deploys the image to all hosts (synchronously T_T) to all hosts in the target environment, and ensures that all applications start up successfully (we use open-rc). Finally, once all hosts are considered "deployed", we kick off the integration tests for the new image. We use a custom, in-house testing framework based on the Robot framework. Our TestEng team is responsible for maintaining this framework and writing all integration tests in during the course of development. If any IT fails, the build is considered a failure and we repeat the process. If tests pass, we move on. This entire step takes upwards of 30min per iteration... Because of this, if there is a failure after the first deploy (and it's not because the tests themselves are broken--which is often the case), we typically iterate by manually emerging package updates onto the AIT hosts for testing until we the issue is resolve, at which we do a real deployment again. This saves us SOOO much time... * Once the image is successfully deployed onto the AIT environment and tests are passing, we update the ebuild's keyword in our internal portage repo to be stable and the redeploy the exact same image and running the exact same tests... I don't know why in Robert Pike's name we need to do all that again, wasting another 15min just because we changed 4 characters in an ebuild that doesn't even touch the code, but yea... that's how things work here... &gt;.&gt; * Once the stable image is deployed, the developer who issued the deploy needs to MANUALLY validate the image (cause you know.. who can trust integration tests, right?) and sign off on it. Once this is done, the deploy is FINALLY ready for our UAT env... Assuming everything works first try, the entire process of deploying to our AIT environment typically takes around an hour. * Now... we have a validated, stable image deployed on AIT.. TIme to go to UAT, but there's a catch. Developers are not allowed to touch UAT, because for some reason, higher ups in other departments use UAT instead of just using prod. I have no idea why this is, but yea... for all intents and purposes, we have two prod environments (DO NOT mistake this for implying that our UAT environment is the same as prod; it's not--not even close) that only our SRE team can touch. So basically, at this point, we chuck the football over the fence and once an SRE member has time to do the UAT deploy, they do it, then chuck the ball back over to us so we can validate (again, manually--cause fuck integration tests &gt;.&gt;). * Once things are validated on UAT, it's read for prod--at which the above step is repeated exactly. * Finally, once things are validated in prod, we backfill on any other AIT envs (we have multiple AIT envs so that we concurrently play with multiple deploys) and it's considered deployed (no validation on backfill step). Now assuming you made it this far without shooting yourself in the face, one thing you probably noticed is that, not where in this entire wall of text do I mention a development environment. That's cause we don't have one (*gasp*). In fact, until recently, we mostly just coded it to spec, made sure the app started locally, and threw it onto AIT, hoping for the best--fixing any issues (on AIT, mind you) as the arose. Now, we actually have the ability to stand up dev VMs based on target images, so this was a huge leap forward, but yea... So there is our deployment process in a... well... whatever the hell that was... EDIT: How could I forget configuration management?! Oh yea, cause we don't have any... See at my company, configuration is code! To change the config, we have to release and deploy a whole new version!
As in Go language official. Andrew Gerrand Is on the committee.
This but I suggest simply using the context package to handle cancellation / shutdown. 
agree
Hold on a moment. /u/DoctrinalAsshole poses a legitimate question. AFAICT, no problem statement or rationale is posed in the [Go Package Proposal Process](https://docs.google.com/document/d/18tNd8r5DV0yluCR7tPvkMTsWD_lYcRO7NhpNSDymRr8/edit#) document. The document poses a goal: &gt; To produce, have accepted, and implement a complete proposal which addresses the concern of package management. "[T]he concern of package management" is exceedingly vague. What is the scope of this, and what does this concretely hope to accomplish? Without a few concrete problem statements, there is likely to be disagreement about what this even proposes.
Oh god... did they *have to* include the Masterminds? They are gonna sabotage the whole process, like the masterminds they are, just to sell us the package manager of another language disguised under a different name! I can already hear their laughter echoing in the distance! Don't fall for their misguided, mastermind-y intentions! They think they are making things easier for gophers but no! [It's a trap!](https://www.youtube.com/watch?v=4F4qzPbcFiA) Brace yourselves! The time when Go projects need to download half the internet in dependencies draws nigh! Okay, enough with the jokes. I sincerely hope that we see some innovation through this or at least something that works for Go. Good luck to the team!
What is the plugin execution mode?
Great. First we got Cockroachdb, now we got Trash!
Are you speaking of sed? Lol.
it is described here: https://docs.google.com/a/golang.org/document/d/1nr-TQHw_er6GOQRsF6T43GGhFDelrAP0NqSS_00RgZQ/edit# basically, "dlopen" for Go, the ability to dynamically load packages.
 go rename go format go imports 
I came across [this SO discussion](http://stackoverflow.com/questions/27625787/measuring-memory-usage-of-executable-run-using-golang#27628440) which may help you.
The document is extremely clear in what they're aiming for: - A synopsis of existing package management solutions, both for Go and other language ecosystems (c.f.; c.f.) - An enumeration of the important points of the design space, including possible decisions. (c.f.) - An exhaustive set of user needs/stories. (c.f.) In a nutshell: look at what everyone else (not golang) is doing in this space; look at what everyone is currently doing (ie. golang users), and make a concrete plan that addresses the needs of current go users, informed by the background in all package management ecosystems. I don't understand. What more do you want? (If the answer is, 'justification for why people are doing this at all', then go to the gopher slack, join #vendor and actually talk to people there, and read the problems they have. Some people don't need a package manager, totally true, but there are people who do, and it's killing them not having one. Just because **you** may not need a package manager, doesn't mean go doesn't need one. If you don't need it, then don't use it. Why do you even care if they add one?).
I just tried it and it works: a/ a.go vendor/ b/ b.go vendor/ c/ c.go // a.go package main import "b" func main() { b.Hello() } // b.go package b import "c" func Hello() { c.Hello() } // c.go package c import "fmt" func Hello() { fmt.Println("Hello from package c") }
Do you know what's the status of the [Transaction Oriented Collector Algorithm](https://docs.google.com/document/d/1gCsFxXamW8RRvOe5hECz98Ftk-tcRRJcDFANj2VwCB0/edit?pref=2&amp;pli=1) ? That looked very promising..
That's not what I mean. The issue isn't using a package once you have them *already installed in the correct folders*, it's installing the correct versions into the correct `vendor` folders without committing the entire source of `aws/aws-sdk-go` to your project, potentially multiple times (if say, two different packages depend on it, or depend on slightly different versions of it). In the abstract, if: - A -&gt; B -&gt; C#1.1.0 - A -&gt; D -&gt; C#1.1.1 - A -&gt; E -&gt; C#1.2.0 (if we're using semantic versioning, in which 1.1.* is 'no api change', and 1.* is 'no destructive api change'), Then your project should only have only copy of dependency C; version 1.2 is the 'best' match for all dependencies on C. Vendoring does not solve these problems: - Version resolution. - Downloading and installing packages of specific versions. - Handling conflicts (eg. A -&gt; C#2.0.0, and D -&gt; C#1.*) Yes, you can handle these problems yourself, by hand, or, indeed, use any of the various tools like glide to solve them; but 'out of the box' there is no automated solution to them in the go tooling. The goal of the working group is basically; standardize on a way of doing this and make the tooling for it official.
On SSA, see this: https://www.youtube.com/watch?v=D2-gaMvWfQY
&gt;[**GopherCon 2015: Ben Johnson - Static Code Analysis Using SSA [25:16]**](http://youtu.be/D2-gaMvWfQY) &gt;&gt;Single Static Assignment (SSA) is an intermediate representation of code that many compilers use for analysis and optimization. This talk will look at how the go/ssa package is used in existing Go static analysis tools like eg and oracle and how we can use it to build our own tools. &gt; [*^Gopher ^Academy*](https://www.youtube.com/channel/UCx9QVEApa5BKLw9r8cnOFEA) ^in ^Science ^&amp; ^Technology &gt;*^3,467 ^views ^since ^Jul ^2015* [^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)
From how I understand the question the answer would be no. While for the current running application you can call this function https://godoc.org/runtime#MemProfile (make sure you read the docs), Go is known not to immediately release memory to the OS. This is not a bug but a feature of Go in order to help out with the performance. Hope it helps. 
Of course I would commit the entire source of aws/aws-sdk-go to my project if that's what my project depends on. I avoid depending on the same project multiple times in different versions. But that's a manual process of weighing up. I usually add dependencies with a lot of care and thought. Copying them to the correct directories is the least amount of time of this. I often abstain from adding dependencies because they themselves have too many dependencies.
&gt; Just because you may not need a package manager, doesn't mean go doesn't need one. If you don't need it, then don't use it. Why do you even care if they add one?). Have you ever tried an application that requires you to use it's package manager to download "the whole net" in dependencies? Many developers do not consider the costs that dependencies bring. If they can save 10 lines in their own code by importing 1000 lines conveniently hidden in a dependency (which might also depend on other things), they will gladly do that, given that a package manager exists that will facilitate the process. Go on the other hand has created a community around a certain way of programming that usually favors [smaller dependency trees in order to make programs that compile faster, are easier to maintain and simpler](https://youtu.be/PAAkCSZUG1c?t=9m28s). I sincerely hope that we get something out of this committee that works for Go and not a mere imitation of something that happens to work for a different programming language.
Go and Erlang are really similar (baring the fact that that one is interpreted). Erlang is functional, but not pure, it has to manage state the same way Go does. Yes, the Go runtime is the one managing the state, it doesn't rely on kernel threads to maintain state (except, perhaps, in blocking system calls, but that is almost by definition what a blocking call is). The Go calling convention is caller-saved, stack based at the moment (ignoring some irrelevant details where registers are used), registers are killed on function call, so Go saves state (in the runtime, it gets there because the compiler inserts call to the runtime in the generated code; Erlang doesn't have to do this since the interpretor can do work between intrepreting bytecode, so to speak) by saving whatever is necessary to select an activation record (activation records essentially make a stack on modern machines, that is why we call it a stack, so it's enough to select the top-most frame). On most architectures, that means the program counter and stack pointer (it saves some other miscellaneous stuff that's not important in this discussion). On some architectures it needs to save the frame pointer too (on those architectures the stack is sometimes implemented as a linked list). Stacks are copied in order to grow them as needed. Fortunately, because there are very few pointers in the stack outside the stack themselves, this is easy to do (it would be harder to do with a more conventional calling convention where registers are used for passing arguments, and where some registers are callee-save). 
Kong does all of this very nicely. getkong.org
I find it funny that after you asked for help to hack a website [1] you now decided to _"teach"_ how to write programs in Go. To be honest, I find this childish, mostly because of the history in your Reddit account [2], but if you really want to contribute in the advocacy of the language I suggest you to post your videos in /r/learnprogramming/ good luck. [1] https://www.reddit.com/r/golang/comments/4xocy8/can_anyone_help_hack_a_torrent_site/ [2] https://www.reddit.com/user/chagatay
i asked to help hack a website because i don't do hacking. and i don't teach i just share the knowledge i have. thanks for the advice to publish my tuts though.
The downside of that approach is that you have to have git and go installed on your server. One of the joys of go is that you can build for different architectures and deploy single binaries. Has your team considered building locally and deploying the binary remotely? If so and they've decided against that approach, I'd be interested to understand why.
Rust prioritized package management very early and hired Yehuda Katz based on his experience with Ruby package management. (Also because he's incredible at getting stuff done.)
Caddy is probably what you want. Traefik looks interesting but I haven't tried it yet. 
Hey guys, Just a small tool that I made to solve a problem I have a lot while developing in the command-line and not thinking before rm'ing. Any feedback is welcome.
Yes, but this way they ended up with yet another bundler/npm/maven-like solution, which is the last thing I would want for Go.
Hey, it's not for everyone!
Why not just use the OS facilities e.g. https://github.com/andreafrancia/trash-cli ? as-is, your tool is a little dangerous IMO, because it silently overwrites older files that just happen to have the same name.
TL;DR how to load-balance with classic HTTP1.x a gRPC service, using multiplexing. Useful for most load-balancers e.g. AWS ELB https://github.com/gdm85/grpc-go-multiplex (a better alternative to http://collectiveidea.com/blog/archives/2013/01/11/running-haproxy-behind-amazons-elb/)
That is probably the better way to go, but for us there isn't a whole lot of difference. We already have a full unix machine on GCE, and I took 5 minutes when we first got it to install go (I think git was already there, which we need for something else). Every few months I need to install a new version of go. So modulo a small amount of upkeep, the methods (in our scenario) are basically `ssh user@domain &amp;&amp; git pull &amp;&amp; go build` vs `go build --build-flags &amp;&amp; scp project user@domain:/some/dir` I'm willing to bet both lines run in under 15 seconds, so it's kind of a non-issue. Until this becomes an issue, I think worrying about it would be an instance of bikeshedding (I hope I'm using that term right, I just learned it). I've even used the first method on my raspberry pi to host a second development copy of our server, and it also works quickly. I will say that the frontend is more of an issue. It takes a lot longer to compile typescript/react/etc down to html/css/js files than it does to build and run the server. I have to wait almost 2 minutes sometimes.
They have to agree on things first. Being a committee may (or may not?) help, but it's not sufficient on its own. 
They've implemented it and it seems to mostly work. Last I heard it was so full of debug checks that they couldn't measure its speed yet. But then I saw Rick dump a load of patches on the mailing list, so it seems like they've hidden debug checks behind compile-time constants and they're starting to land it. Also, it's been renamed "ROC" now, because I guess that rocks more. I don't know what the R stands for.
As much as I like vendoring and avoiding deep dependencies - and not really liking package managers that are language specific - I think that a package manager emerging is inevitable (people really, really want it), and once that happens, sooner or later a library you really don't want to rewrite will become dependent on it. So if that has to happen, I'd rather have something well considered by a team that makes sure to learn the lessons of previous efforts, than something cobbled together by someone who didn't even expect their prototype to gain traction.
So true! I also found the rust community very unwilling to learn anything from Go. I find Rust's import/packing quite ugly compared to Go. Once we have the "standard" vendor file I see no reason why cargo would be in any way better than Go packing.
I used to think we needed a package manager. I am not sure it is a real need but a desire to do things I no longer think are a good idea.
This is great news. While a lot of people will say it's something that should have been done a long time ago, it's a chance to do things the Right way rather than the first way, the easy way, or the way other languages did it. There is still huge debate on what that Right way is, there are multiple systems created by the community to use to solve it in different ways, and that experience is quite valuable for an official Go way.
Go isn't really a language I would want to use for scripting. I'm not sure what would be the point of a Go interpreter.
it's unlikely: there are no warnings in the Go compiler(s). having said that, what would be the use case for that?
Sure, but pythonista use python for scripting, not go, obviously. 
It replaced bash/shell in many cases. What makes you think it's not good for scripting? Few years ago I was writing bash just to rewrite it in Go shortly afterwards. Now I don't bother with bash unless Go is not option.
It's all about the interactive prompt. It really is fundamental to have such a thing for e.g. exploratory work, data analysis and a bunch of science-y workloads.
Have you taken a look at dwhitena's [gophernotes](https://github.com/gopherds/gophernotes) package? Go is being adopted for Data Science-y things and processing pipelines, so there have been multiple REPL projects. Gophernotes is the most complete one that I've seen.
Yes. Gophernotes is already in the loop :) But its repl has known deficiencies, that our (future) repl is set to fix.
&gt; While much remains to be decided, a few things are clear. The package management solution, whatever it will be, will take the form of one or more **official Go language proposals**. And the proposals (and their eventual implementation) will be driven by a small committee of dedicated Gophers. from [the forum post that started all this](https://groups.google.com/forum/#!msg/go-package-management/a55P0_FU_jA/Y9t_mBroBwAJ) So yes, this is Go language official (assuming the proposal gets accepted, which it almost certainly will)
Transport Layer Security. Go continues to not expose any concept of thread local storage (on purpose). Outside of maybe sync.Pool indirectly.
Generically, I agree with your sentiment. &gt; In general, you shouldn't ever care what type something is. Not the case for this application. In fact, this app doesn't care what the "behavior" of the thing is, *only* the data type. The things, in this case, are literally just data, they have no behavior, at all. To be a bit more concrete, imagine a ETL system that needs to be able to migrate data from one database to another, but it doesn't know ahead of time what the database driver will be (although it'll implement the sql.driver interface), or what the DB schema is, what columns will be needed, what the type of those columns are, etc. The go sql API doesn't even allow you get get the datatype of the fields of a row, even though all the drivers necessarily gather this information. JDBC has its quirks, but given the current go sql API, it's nigh-on impossible to build generic database tools (without resorting to reflection). Edit: spelling
I think they could just have omitted the "range" element entirely. I still have to backspace to put that in almost every hour of the working day.
This is the umpteeth rant about the same points that an umpteeth people have ranted about in the past. That is not to say that some aren't valid, just that it's all a repeat of things that have been said already, both here and in many other places.
I find most of those features, when used correctly. Most of the authoritarian aspects are there to intentionally remove creativity. You will think it when working in a team, or coming back to an old project. For package import, there is a work around for packages that pop in and out: import ( "fmt" ) var _ = fmt.Println But it's still forcing you to make that exception, so you won't over use it. Go is a strict teacher, it's making you a better programmer. I think of it similar to dead code and unused assignment errors, slightly annoying during development, but good for code hygiene. And if I could turn them off during development, they are not turning themselves back on.
&gt; The gRPC server conforms to the http.Handler interface, which you can multiplex with grpc-gateway. You don't have to open two ports. What do you mean by 'gRPC server'? gRPC does not come with a http server, does it? Did you mean the gRPC gw server? The gw server does have a mux satisfying http.Handler, but registering a gw generated service requires the gRPC server address in one way or the other. Do you have a working example of serving with a single http port?
It's an introduce to what LLVM is, how it works and what can be done in Go with this. Maybe it can even be the basis for the Go VM or a REPL, who knows? 
If you're talking about a SQL DB why not do a join between tables and then return all the data at once? SQLx has the select method that can make it easy to populate your structs from multiple row results. 
I'v seen, but I wanted to make my own library during learning Golang :) 
It's a trade-off. A REST-ful API is a better first pass if you have multiple clients. But if you're only writing the one client, and/or the data doesn't update often, websockets will be many times more efficient. Websockets will also be a lot simpler on the client, and not too much different on the server.
The whole idea behind the error handling is to force you to deal with it in the context of where the error is retuned and to do it just once. Sometimes the right thing is to pass the error back to the caller, sometimes it will be to retry at the point of the error, or any number of variations. Having a generic error handling routine makes things more complicated because you're losing all the local context about what the right thing to do is in that particular situation. Are there more elegant ways of doing this in other languages? Of course, especially for function composition. But the one good thing about it is that it's explicit and easy to follow in Go when you haven't looked at a code base in a while. Try the same thing in Scala where you can have any number of fancy abstractions which cause you to hate your former self when you try to figure out what implicit magic you did and where the error is actually handled 6 months later. 
"Who knows" summed it up about right.
Yes, wasm is really interesting. On the Go-&gt;wasm front, I think the gopherJS will provide that eventually, so we could switch to that in due time. Depending on how fast we move forward, maybe the first version of our VM could actually be based on wasm :P
There is already a llvm-based repl on the market: llgoi. https://github.com/llvm-mirror/llgo/blob/master/cmd/llgoi/llgoi.go But coming from Go and 'go get', I must say I have always felt the installation process and developing against LLVM as a very frustrating process.
I am unsure, what the point of your post is, TBH. There is nothing new here, these things have been complained about ad infinitum and have been replied to and discussed ad infinitum. So you are not going to change anyone's mind, really and the views of the go community on them are readily available elsewhere. As such, the point here seems mainly to vent and I don't think this is the best forum and audience to vent about your frustration with go. A personal blog or your friends seem to be a much better suited forum for that. &gt; Errors: This dead horse has been beaten to a pulp by now. Exceptions lead to hard to follow control flow, unreadable error messages emitted to the users and do not even really *have* the advantages touted by it's supporters, because now you need to litter your code with `try { … } catch { … }`, instead of `if err != nil`. It's also similar to Conversions (see below); error handling is hard and complicated and if you think the world would be better by hiding it away, you are not being rigorous enough with your error handling. &gt; So, as to why exactly we need to be careful with the packages we import in a world that's heading for personal computer with a few Terabytes of RAM in ~5 years, I don't know. Go was developed as a replacement for C++ at Google, where builds already have Terabytes of RAM and thousands of CPUs available today, so this is clearly a red herring. Tiny programs that you write on your home PC might compile quickly and not suffer from the difference, but even moderately sized programs will. Fast compile times are a key distinguishing factor of go (if you doubt that, just read this subreddit whenever a new version of go doesn't significantly bring them down, people are going *nuts* over it) and things like not importing stuff you don't use are a key strategy to get there. And as mentioned elsewhere, goimports can almost entirely eliminate the annoyance of it. &gt; Conversion Again, this is learned from experience. A conversion almost always has caveats and can lead to bugs, so doing it implicitly hides a *lot* of bugs (and a lot of them are security-related and can lead to DoS, e.g. because of failing bounds checks).If you believe that conversions can be done implicitly, you are not rigorous enough with your checks. &gt; Authoritarian format: It's not Authoritarian, it's opinionated (also, it's not required by the compiler, so at best you are complaining about the community here). And it's the favorite single feature of go for a lot of people. So you might not like it, but you are in a vanishingly small minority then and are unlikely to change anyone's mind. The things gofmt are opinionated on are exactly the things programmers in other languages spend *lifetimes* in lost productivity on debating about (case in point: Right now we are spending engineering hours debating about debating about them). *It doesn't matter*. If you really can't stand it, rewrite it to your favorite special snowflake style on checkout and gofmt on commit, done. But you could also just realize how it *really doesn't matter* and how much time and energy you save by not worrying about things that don't matter. After working with go for a while I pretty much refuse to work in languages without good auto formatting, because when I do, I immediately realize how much time I wast reformatting my code and how much energy I waste worrying if the linebreak should be before or after this HTML-element and how to best deal with inline-js. It's tiring.
great idea. thanks!
What about the new application load balancers that recently came out? They claim http2 support. https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/
&gt; Go is now catching up with 90's compilers Haha :-D But we have to agree on one thing, Go is amazing at what it does, and it is just getting better with each release! I especially love the fact that there are no language changes, reminds me of your talk in GopherCon India, "asymptotically approaching boring"
Let me give you the answers to your points: **Errors** Having explicit error handling is a great thing for larger projects. You still can choose to ignore them as exceptions usually are, but it doesn't feel good. From my point of view that is a good thing, handling errors is an important part of your program. If error handling feels repetitive, you can program around it. For instance by creating a wrapper type that saves errors and makes followup actions on the wrapper noops until the error is read and reset. **Package import** I recommend here to use goimports, which formats your code and handles the imports. There is even a workaround forcing the import. var _ = fmt.Println **Conversion** All what Go requires you is to make conversions explicit. That forces you to think about it and will prevent errors. BTW an int is not similar to a float64. For most integers they are similar, but not all 64-bit integers can be represented exactly by float64. **Authoritarian format** Every project involving more than one developer will have to agree on a single coding style. It is quite some work to develop and document one on your own. Go comes with a single coding style and a tool to support it. It's time saving. You can immediately read code somebody else has written and you don't need to care, because a tool is doing the formatting work. Your complains are pretty common for people coming to the language. But those features are there for a reason. Go's objective is to make it easy to produce production-ready, stable code in large projects. Lack of error handling, implicit conversions, unused dependencies and inconsistent coding styles don't support this objective and are therefore not supported or encouraged.
This is an article for anyone interested in writing a compiler in Go. Intended target is anyone who's taken a compiler writing class (or who is, like me, a self taught compiler writer). LLVM is a tool kit that makes it far easier to target native code because it eliminates the need to learn how to generate code, write out symbols to an object file, write a linker, etc. Modern processors are wicked hard to handwrite code for, and each has wildly different optimization paths. Debuggers are also super challenging. LLVM has done all that for you, and lets you stick to the fun stuff: designing the syntax of your language. 
Have you tried running this with the -race detector? You have unguarded sharing of your client slice. Bad things could happen if multiple goroutines are connecting and disconnecting 
I meant it as: sky's the limit / who knows what else people will come up with? 
Had seen that already. As far as I know, it's about the (HTTP2) traffic of the application themselves, not for the health check; and even if it were, you cannot possibly use this for gRPC as any response returns 200 OK and does not inspect/parse the protobuf output. (HTTP2 != gRPC)
Go interpreters would be very nice for applications with plugins. Atm the options basically are "compile all together", "RPC/IPC", "API" and probably "something we made up 5min before sleep" I'd love to run go apps in a sandbox and interact with them. Opens up plugins that use any comm system they like or none if they code their functionality within the sandbox. I'd love to see this happen!
oh i didn't consider about that. Thank you for suggestion
If keeping RAM down matters so much then you can get into a whole other can of worms about how to best optimize each fucking loop and what un-heard to flags to use at compile time. I'm not trying to compare GO with the ressource hogs that are scripting language like Python, Node, PHP, Perl... etc or ,god forgive, Ruby. But tbh I've spoken with some amount of C and C++ programmers over my lifetime and I've never once heard "oh god, if only we wouldn't have forgot to remove that &lt;iostream&gt; include from that one class, man would that have saved us a lot of ressources" Because, lets be honest, most useless packages that stay over are for debugging purposes and a warning would be enough
Because it would be too secure.
Recommending not using multiple defers is ill-advised as the current execution order is the only one* that makes sense
&gt; And you are using TLS? In fact, assuming you are using a modern browser, I find it a bit odd that you received an error about it not being HTTP/2 earlier. Not using TLS. Those errors are not by browser to server, they're originated from server to server, that is /gw talking to /. &gt; What then lead you to gRPC in the first place? All of its requests are POSTs as well. Seems like an odd concern given your tool selection. [You comment](https://www.reddit.com/r/golang/comments/4z7kcp/grpc_is_now_officially_ready_for_production_use/d6urgl4) gave me the impression that pRCP is the right tool for building a RET API: &gt; On the other hand, if your intent is to build a REST+JSON API from the start, but want to use gRPC semantics (perhaps with an upgrade path later), then pRPC may be more appropriate. I want to build a REST API, and I really like how easy it is to define service interfaces with proto 3 and it goes very well with gRPC in Go. All I needed was a thin layer of REST in front of it, and I would have a go-kit code with 90% of it being autogenerated. - One way would be to mount pRPC on a path, and use a REST proxy in front of it. Something like the method you described for single port gRPC-GW, with the difference that no http/2 is required. It's a simple http/1 proxy to pRPC. The proxy simply merges url query params with http body json (if any), and POSTs it to the pRPC endpoint. - Another approach with less overhead is to implement `prpc.Sever`, unfortunately it has many unexported creatures like: https://github.com/luci/luci-go/blob/master/grpc/prpc/server.go#L166 . - A 3rd approach would be what your code demonstrates. But it completely ditches pRPC. Why did you add `// Don't actually do this!`?
Embedding. Right now your choices are Lua or JavaScript. Evaluating Go inside Go would be huge. &gt; There are much more important things to take care of first. This is a community project, not something blocking the core team release process.
Using multiple defers follows the order of popping in the stack, so I think it usually makes perfect sense, especially when later variables are dependent on earlier ones. Imagine some kind of database access: db := connect(mydb) defer db.Close() query := db.NewQuery() defer query.Close() // ... construct the query return query.Execute() This way, the query is closed before the db is closed.
A few notes from the top of my head: * source code building and runtimes should be separated (See also: reproducibe builds) * runtimes should be minimal images, see also busybox + Go binary images * both source code components and deployment artifacts should be versioned, this will help you so much later on In reply to 3: graceful migrations when possible (and service APIs don't change), otherwise planned downtime, no need to build extremely tall card castles In reply to 5: Docker allows specifying all the configuration for a container as environment variables. The only problem I have with this is that it's non-validating e.g. if you still keep around wrong configuration settings, the binary won't complain about them and use defaults, hence the suggestion to always use defaults which are safe for production (failure is more acceptable than data/systems corruption).
You shouldn't change your coding style.
Agreed, but I'd say its situational. If you think the execution path is confusing and likely to result in a problem later, maybe just defer one anonymous function that does multiple things instead of multiple deters? I'm not sure if that would really be better as honestly I haven't ran into a codebase where I felt it was needed yet, but I could see it being useful to remember. 
Definitely. So much cleaner than `try … finally`.
Disagree with a bias to use `fmt.Sprintf`...you move some errors to execution time instead of compile time. 
It's a shame she missed out on [the crochet gophers](http://imgur.com/gallery/dDJxJ) made for me by my friend. The pattern for it is open source, but stuck on one of my old hard drives. I'll be retrieving and uploading it soon.
I hope a mips32 port happens for 1.8
What did I say to imply that I had a bias towards `fmt.Sprintf`? I'd like to update the post to not imply that, as my intent wasn't to say that it is preferable to use `fmt.Sprintf`, but to simply present it as another option since many developers are already familiar with `printf` in C.
[Issue 4341](https://github.com/golang/go/issues/4341) (image/jpeg: correct EXIF orientation). Supporting jpeg without supporting orientation isn't really supporting jpeg. You can't do anything with an image if you don't even know which way is up.
Worth noting in point 3 that `fmt.Sprintf` is much slower and more memory- and allocation-heavy than `strconv`: $ go test -bench=. -benchmem -v strings_test.go testing: warning: no tests to run PASS BenchmarkFmt-8 5000000 251 ns/op 16 B/op 2 allocs/op BenchmarkStrconv-8 20000000 67.2 ns/op 5 B/op 1 allocs/op ok command-line-arguments 2.942s Benchmark code [on GitHub](https://gist.github.com/pswaminathan/a51cda43cba2f7ea2194c90e3f3e1136). `fmt` is certainly more versatile, so in cases where something more than simple conversion is needed, `fmt` will probably be more useful. But maybe worth noting, especially to emphasize how straightforward it is to test these things.
Welcome to the club.
Yeah, I typically only use `Printf`/`Sprintf` when I want to embed a value inside a string with some other kind of formatting. Relatively speaking, these are also expensive functions in C.
Valid point. I updated the post to note that `fmt.Sprintf` is slower. Thanks for the feedback!
Maybe I'm not understanding what a transitive dependencies are, but what's wrong with bundler's approach? Allow multiple versions of a dependency to be installed and each package import the version it requires. Forgive me if I am being naive.
to be honest, I thought it was the easiest way ... but probably I should have tried it too! good point
I did not notice about the code disappearing! I'll fix! Thanks
Ditto, upvote, agreed. . . yes.
Niceeeee
as a (happily-former) C++ programmer, i'd have to disagree. RAII does have the nice property that you can't fuck it up, but it's such a PITA to use. defer is easy to use, and easy to see when it's missing -- i like it.
Hardcoded weird shit date format (not even ISO standard ...) with the standard logging.
I use multiple defers often enough. I don't think it's ever been part of a bug or confusion. Missing cleanup is far more likely to be a problem than doing cleanup in the wrong order... especially if you're using defer (which does last in first out, as it should... if B depends on A, you have to create A first, then B... then clean up B then A). 
So ... if a webpage has more than the hardcoded 10 redirects, the defer doesn't run and I have a memory leak?
&gt; Then why do you need an interpreter ? you're already using Go you don't need Go to act like an interpreted language. You asked me why someone would use Go for scripting. I hope that answered your question.
You're not using Go for "scripting" you're compiling then executing a go binary. You don't need Go to be interpreted. You're not interacting with Go with a REPL. 
That could explain why my webcrawler goes OOM. How could this "special handling" look like? Example code from my webcrawler url := "http://www.example.de/ req, err := http.NewRequest("GET", url, nil) if err != nil { Error.Printf("REQ Error: %s\n", err) return } req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1") // SEND HTTP REQUEST var resp *http.Response = nil maxRetries := 4 for i := 0; i &lt;= maxRetries; i++ { Debug.Printf("start HTTP Request with: %s\n", url) resp, err = client.Do(req) if err != nil { if i == maxRetries { Warning.Printf("RESP Error: %s\n", err) return } else { time.Sleep(time.Duration(i+1)*time.Second + 1) Debug.Printf("sleep for %d seconds @ %s \n", i+1, url) continue } } else { break } } defer resp.Body.Close()
What about it?
Instead of a 64-bit random number, you only get 63 bits of randomness. 
UC?
It seems there is some progress: https://github.com/golang/go/issues/4254
Go doesn't have casts! :-) It has type conversions. &lt;https://golang.org/ref/spec#Conversions&gt;. One can convert between strings and slices of bytes, but the bytes are copied, unlike C's notion of casting. UTF-8 isn't required. https://play.golang.org/p/lvfyjNlJUE
My current "favorite" obscure bug in `godoc` binary: https://github.com/golang/go/issues/16183 It treats a relative import path parameter as if it were absolute if its value happens to match a `cmd/...` import path.
&gt; As such, the point here seems mainly to vent and I don't think this is the best forum and audience to vent about your frustration with go. I understand that reading the same questions over and over again can be frustrating, but this is not helpful.
yes, the `buildmode=plugin` is a cornerstone (or, will be) of the VM implementation: it will allow to dynamically load pre-compiled packages (e.g. the stdlib or any 3rd-party package) from the prompt, w/o having to deal with interpreting ASM or cgo. `buildmode=plugin` and the interpreter are not the same thing. they are related though :) (and David Crawshaw is working on providing `buildmode=plugin` for `linux/amd64` for the 1.8 cycle)
&gt; each package then returns types of different versions I don't think that would be a problem. gopkg.in already works this way. &gt; Another issue may be larger binary size This I understand and yes, it should be a concern. &gt; slower compile time Package are all compiled separately anyway. The only thing I can see that would be slower would be `go get`ing the extra versions. edit: formating. And thanks, I will check out the forums.
probably because putting the date really far back makes difference calculation always being about huge numbers, which can cause an overflow...
On that subject, I actually hit a wall trying to use a c-shared Go library that gets loaded into a process with a bunch of other dynamically loaded libs. It was a thread local storage issue https://groups.google.com/d/msg/golang-nuts/EhndTzcPJxQ/i-w7kAMfBQAJ
In practice it's not a big difference. Instead of `bash ./run.sh` you use `go run main.go` but of course I can't run go code interactively as I can in a shell. 
Why are the comments in chinese but the identifiers (i.e. function name in english ?) 
logger: 2009/11/10 23:00:00 main.go:12: Hello, log file! Not the worst I have seen...
Date format is PURE SHIT. ISO standard or Go developers can fuck themselfs.
As far as I understand the code and its comments, the response body will have been closed before returning the response and error.
Russ summarizes it quite nicely: https://github.com/golang/go/issues/222#issuecomment-66048620 
I would say if you are worried about cleanup order and code clarity, defer to a single function that does your multiple cleanup in the correct order.
i can't edit the link, but I found out that the book is present on Github. https://github.com/jannewmarch/Network-Programming-With-Go
You can specify the values you want with: s := []int{0, 1, 2, 3} You can't, however, specify one single value and the size of the slice and have that value to be applied to every position. You have to loop for that.
foo := [10]int{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, }
There are many ways. It also depends on whether you want an array or a slice. var ( a1 = [...]int{3, 4, 5} a2 = [5]int{2: 15} s1 = []int{3, 4, 5} s2 = []int{5: 10, 10: 20} )
Author here - Let me know if you have any questions or have any concerns about the article. I'm always open to feedback and want to make this series as educational and idiomatic as possible. 
@wjkohnen pointed out that the code shows the resp.Body being closed prior to that error being returned; I could be mistaken about the bug.
ah, I believe you are correct. Thanks.
Go maps are safe for concurrent reads. You just can't concurrently read+write or write+write. Once you need to, use a Mutex or RWMutex, etc. 
There's no hard and fast rule, and the [guideline](https://github.com/golang/go/wiki/CodeReviewComments#line-length) is "avoid uncomfortably long lines". In any event, in the example given str will include the newlines of the multiline string, and using a literal multi-line string is more readable than str := "This is a\nmultiline\nstring."
:) So one at a time: * We (Chrome Infrastructure) developed luci/gae primarily for our own uses (building open source continuous integration software for Chromium on AppEngine), but pulled it out separately because it was completely independent of the rest of luci-go (barring some very minor common library dependencies), and we thought it would be useful for other folks. In comparison to the official SDK(s), it's substantially more magical, which may be part of the reason its not in a merge-into-SDK state (ostensibly more difficult to maintain/support). Go also arrived most recently to AppEngine, and so taking advantage of the existing AppEngine development stuff (e.g. dev_appserver.py) probably made sense from a getting-things-up-and-running-quickly standpoint. However, this is all speculation on my part, I have no special insight into the Go-on-AppEngine team. I know that the Go-on-AppEngine team knows about luci/gae (I told them about it internally and externally), but past that I don't know of any other plans. * There's a [bug on file](https://github.com/luci/gae/issues/23) to 'invert' the luci/gae model and implement all the memory backends in terms of the actual appserver protos. This would allow building luci/gae as a drop-in replacement for dev_appserver, which would make it easier to use luci/gae with existing codebases. I also speculate that it would make it easier to integrate the fast testing bits into the official SDK since aetest could transparently use it. It would, however, require implementing the 'admin' frontend pages in luci/gae (which I'd love to do, but definitely don't have time to right now). * Yeaah... So we could definitely use some help here :). Our primary responsibility is to support CI for Chrome/Chromium, and so we practically have a very small amount of time to devote to advertising/SEO/documentation :(. We would love to spend a bunch of time on this and make it a Glorious Open Source Project (because it's really useful and pretty spiffy), but unfortunately we can't promise to make those time commitments (right now). As Chromium CI improves, we'll have more time to devote to secondary deliverables (like documentation and a proper luci.github.io site, etc.) * I definitely agree, and we try to improve this where we can as we go along, but it will very likely be 1-2 quarters before we can devote a lot of time to this. * I'm not sure what you mean by this? * Much of the documentation is geared at chromium CI development. luci/gae does NOT need gae.py, and can be imported/used/uploaded with gcloud as part of a normal go appengine app without any special tools. * We look at all of the issues/pull requests (even if we can't work on them quickly)! If you see a piece of documentation that's particularly lacking and can think of a better thing in 5 minutes, we would be MORE than happy if you could file an issue and/or pull request for it :) I know that this sounds a bit like "please do work for us", but it's more about engineering priorities; we DO want to comb through the documentation for this (I even had an OKR to do this last quarter) and make it really approachable, but it's very difficult to prioritize that in the face of the other responsibilities we have (right now). So in summary; we use luci/gae in production for our continuous integration apps (e.g. https://github.com/luci/luci-go/tree/master/logdog which is a log streaming/aggregation/cloud storage service). It works well for us and we intend to continue developing it/supporting it/fixing bugs as we find them. Eventually we're going to make a big push to have all of the luci project externally approachable (via documentation/landing site/etc.), but we're not at that phase of development yet. However, in the mean time if external folks find luci interesting/useful, we're always happy to take external contribution (anything from casual issues filed all the way up to code, or anywhere in between), particularly when those contributions make it easier for other external folks to use stuff.
Adding to the write scenario, to avoid contention, although **not advised**, you can use atomic [as explained in this example](https://golang.org/pkg/sync/atomic/#example_Value_readMostly). Or trust the untested code generator from a random internet guy (me): https://github.com/ninibe/atomicmapper
It's probably worth referencing https://github.com/golang/go/issues/12580 as some of the context to the motivations.
This was an incredibly inspiring video, Francesc... it really makes me want to go hack on my own just-for-fun project. I've been so focused on writing high quality packages and/or widely useful tools, I forgot about the joy of writing something just to have fun and make something cool for yourself.
Thanks! I'm not sure where the syntax came from, but I suspect it was at least partially inspired by templating libraries like [mustache](https://mustache.github.io/)
it is indeed used: $ cd /usr/local/go/src/net &amp;&amp; pt aLongTimeAgo fd_unix.go 134: fd.setWriteDeadline(aLongTimeAgo) fd_windows.go 346: // cause fd.setWriteDeadline(aLongTimeAgo) to cancel a successful dial. 354: fd.setWriteDeadline(aLongTimeAgo) net.go 450: // aLongTimeAgo is a non-zero time, far in the past, used for 452: aLongTimeAgo = time.Unix(233431200, 0) btw, you can use [deadcode](https://github.com/tsenart/deadcode) to detect unused variables and functions.
Binaries only support one architecture, but you can usually compile for one from another.
I do agree that premature abstraction causes a considerable amount of evil, but I do not agree to blame DRY for that. Because first, there are far more (and better) ways of not repeating myself than introducing abstractions. E.g., as Uncle Bob says, "prefer composition over inheritance." And second, doing DRY by premature abstraction means to fear the possibility of repeating myself, even before I did so. However, to me, the most important thing about DRY is just being lazy, and introducing complexity just because of fearing stuff is the opposite of being lazy.
It was enjoyable but to be honest my advice is to avoid RegEx in Go especially on the backend for web apps. You noted the common problems with RegEx but I would also give Go beginners an link where the idea came from [History](https://en.wikipedia.org/wiki/Regular_expression#History) So people can figure out the best use case for RegEx is when they just want to have an embedded practical pattern matcher inside their programs. In my view the only good case to use Regular Expressions is in the frontend because using them to search and replace informations from strings is usally faster then writing own functions in JavaScript.
&gt; It was enjoyable but to be honest my advice is to avoid RegEx in Go especially on the backend for web apps. Why? &gt; In my view the only good case to use Regular Expressions is in the frontend because using them to search and replace informations from strings is usally faster then writing own functions in JavaScript. What...
 return (&amp;multiplexer{}).New() That's just weird.
This entire comment is nonsense and fluff.
Why does it require login/authentication?
If you range over the string, you will get runes instead of bytes.
this series is fantastic, thanks Ben ....would make a great eBook etc once complete
i think go performance is bad better than java! 
can someone mirror this content?
https://blog.golang.org/strings is a good place to start
Thanks!
Are you just executing the templates (good) or parsing them in every request (bad)? Saying that, template rendering is expensive regardless, but you should be able to do several thousand requests per second per core on a modern CPU. DB connections are usually a much larger bottleneck.
What license is fine? And why GNU is not good?
Would love to see this, but the app wants to access to my Google account with no indication of what permissions I would be granting. Denied.
Here's how you can implement your own concurrent map and slice types. http://dnaeon.github.io/concurrent-maps-and-slices-in-go/
&gt;&gt; It was enjoyable but to be honest my advice is to avoid RegEx in Go especially on the backend for web apps. &gt; Why? Backend web apps serve thousands of clients, and much of their code runs in small, fast loops. Performance counts. Regexp matching, on the other hand, is not particularly efficient, compared to specialized string matching algorithms that are optimized for a particular use case. I mostly write command-line apps where performance does not matter that much, and I find myself using regexps quite often. In on case, however, I did decide to not using regexps but rather a specialized algorithm (Aho-Corasick in this case). The commandline app I wrote had to filter millions of lines of log files in a short time. In the same app, I used regexps to parse the command line arguments, and I found I was hittng a wall because the argument syntax had evolved into something too complex for regexps. It's all about applying good judgement before deciding to use regexp matching.
Executing. I parsed it once, and referred to it by name. I was only able to do about 3k requests per second on a six core i7 machine clocked at 4ghz. All data was in memory.
When he is writing about encoding from byteslices: &gt; These functions [VarInt and UVarInt] decode the data to the largest available signed and unsigned integer types — int64 &amp; uint64. I don't understand what he means with **largest available** integer. Do those functions don't know the endianess en decode using both endianesses en pick the biggest value?
Use something like MIT. Google "permissive license"
I think this is a bad practice. Coming from Java world, business logic in templates harmed us more than it benefited. It's a lot harder to maintain the code. Business logic should remain in the code files, templates should only work with values. But I did like the tips about html comment insertion, nice work. 
*This might sound rude, which is not the intended tone* I don't think you answered my question. You just kinda changed the subject by stating what I'd consider to be truisms. * "specialized code/algorithms are more efficient than generalized regexps" - if that weren't the case then there would be no need for the specialized code to exist * "matching a regexp against millions of lines in a short time makes the regexp overhead more visible" - if you're doing that on your backend for every request you're probably opening yourself up for DoS or using the wrong tool for the job or both... but even ignoring that, I still consider it to be a weak argument given my own anecdotal evidence. i.e. for all we know your regexps are *written* or *used* poorly or you're measuring/benchmarking far more than just the regexps. Just a few days ago I saw someone give the same advice to avoid regexps and was highly upvoted for it. I couldn't believe what I was seeing when I looked at the sample regexp. * "regexps are inappropriate when they become too complex for the problem and vice-versa" * "apply good judgement before deciding to use $tool" *In-case it's not obvious: you don't have to answer, but I will re-state the question more directly below* Why should one avoid regexps in Go, especially on the backend of a webapp? 
since Go-1.5, the compiler is indeed written in Go but it doesn't expose an API nor a (public/exposed) package to actually perform the compilation. it's of course doable to fork or vendor what is under `cmd/compile` but a bit impractical (because the Go team hasn't committed to any stability API for this part of the Go tree) so you can't really have a single binary to iteratively compile a given Go expression w/o actually having a Go installation somewhere. that's for your last (very insightful) point. see: https://github.com/golang/go/issues/15108 as for having a web of sub-processes connected via, say, gRPC or something, is of course technically doable. although it seems the scalability of such a setup doesn't look good, especially at the granularity of the interpreted lines. there are other some performances issues: we already all think cgo is slow but it's "only" 10-times the usual cost of a normal Go function. serializing, sending over the wire a part of the whole environment of a given interpreter vm to a sub-process, deserializing, applying the expression, re-serializing, de-serializing, ... doesn't bode well, really. (the nice thing though is that it would be somewhat harder to corrupt the VM memory from a CGo call for example as it would happen inside the sub-process)
It's not the largest available integer, but the largest available integer *type*. People coming from dynamic backgrounds might assume that the returned integer size would match the architecture (32 or 64 bit). It's just clarifying that the returned type is always 64-bit.
Thanks.
I like the subject because I am always struggling with regex. Should use it more maybe to retain the knowledge I have to look up every time again. However, I do not like to format you have chosen: video. Too much content nowadays is only created as video. Impossible to search through, impossible to skip parts without missing info. Text can be skipped through very fast and still have an idea what parts are about. So its easy to only read those interesting pieces. Maybe you could add transcript of the text. Even if it is without code. For a 'best of both worlds'. 
I return a custom `type ErrorHandler func(w http.ResponseWriter, r *http.Request) error` with a ServeHTTP method on the type - much in the style of returning a http.HandlerFunc, but if there are many common dependencies / groupings, structs can be just as good :) It also helps to define a custom `type HTTPError` or similar, so you can pass a response code alongside an error message, and then pull it apart in the `ServeHTTP` function for the `ErrorHandler`
I'm usually doing the second. The first I'd find acceptable too. I'd discourage the third, as it makes testing and everything much harder.
I do something like the first one using closures, except I pass a service interface with methods that do whatever DB or data fetching that needs to be done to return the response. This allows you to easily mock the service when you are testing your handlers. 
I use the struct method, where all dependencies are interfaces so they can easily be mocked.
I was always struggling on the best way, so I built [lars](https://github.com/go-playground/lars) so you can use your own struct as the context, hope it can help.
Thank you for the feedback. My next video will definitely have a transcript. Also, I plan for shorter videos, so that there is less urge to skip parts of it. I just need to find a way of turning the mix of keynote slides, slide notes, voice-overs, and (in future videos) screencasts into a suitable transcript without too much effort. 
Thank you for being a polite user on reddit! --- *This bot was created by [kooldawgstar](http://reddit.com/u/kooldawgstar), if this bot is an annoyance to your subreddit feel free to ban it. [Fork me on Github](http://www.github.com/kooldawgstar/PoliteUsersBot) For more information check out /r/Polite_Users_Bot!*
&gt; Package design in **GOLang** Please, tell me I didn't just read that. Tell me it's just a bad dream! &gt; Packages are the organisation unit for code in **GOLang**. &lt;GOLang intensifies&gt; &gt; NO!. In **GOLang** this is more simple Ooh, the horror!!! Seriously, do we have some kind of curated list "Horror stories for gophers" to put this article in?
I use the first one, but there are cases when I store certain dependencies in context when the dependency either stores per-request data, or when the middlewares should be able to override the dependency for certain handlers / on certain conditions. It is easier to add something (e.g. a db connection) globally, and then being able to override it on a per-handler basis.
Beautiful. Thank you! :)
This seems like a neat alternative to nano. One thing that I think nano does better though, is that it includes the keybindings for common/important commands at the bottom of the screen as an easy reference for people who aren't familiar with the editor. I know you have the "ctrlG for help" text (which is useful!), but a new user needs to see that, read enough of the help to know to type ctrl-E "help tutorial" or ctrl-E "help keybindings", and then read enough of that to figure out the keybindings for something as simple as saving or quitting. If your goal is to make an intuitive/easy to use editor like nano that you can just throw people into without any experience, I think it's important that the basic operations like Save/Quit/Find are obvious right up front. (But that's just my opinion..)
How would you suggest doing things like a sign in/ sign out button depending on whether the user is logged in? I agree that you don't want much logic in the view, but a little bit is pretty common so id love to hear how you generally avoid any at all.
I'm not sure what you're looking for exactly, but Go has its own reflect package: https://golang.org/pkg/reflect/
Agreed. 
Didn't know it exists before, seems pretty much powerful.
yes, we'll probably have to do that (or, at least, recommand to have a valid and up-to-date Go installation) in order to support `import "github.com/foo/bar"`
I've used libusb (which is wrapped by gousb) earlier when I needed to send a couple of predefined bytes to a device, and I think it really is low level and not the best approach for this problem. YMMV, but the libusb FAQ also kinda confirms this: https://github.com/libusb/libusb/wiki/FAQ#Can_I_use_libusb_to_open_a_file_on_an_USB_storage_device My idea would be to identify where the device you've plugged in (you can still use gousb to detect this) is mounted (or mount it if it's not happening automatically), and use the usual filesystem operations provided by the OS. This way you don't have to worry about various partitions and filesystems. On Windows the devices should be mounted automatically (after a small delay, if the drivers need to be installed). On most Linux systems with a desktop environment there should be some D-Bus API you can communicate with, or maybe you can use the [mount](http://linux.die.net/man/8/mount) command. Have a look on [udisks](https://www.freedesktop.org/wiki/Software/udisks/), its command-line tool was quite useful for me for programmatically mounting disks on my desktop machine. Unfortunately I'm not that familiar with OSX, maybe someone else can help you with that.
There's no need to be rude.
https://www.reddit.com/r/golang/comments/4f8e0q/micro_a_modern_and_intuitive_terminalbased_text/ Original post from the author of Micro
May I make the suggestion to checkout Goa [0] and look at it's generated code? I like the way it handles this and it does it similar to the first 2 methods you describe, but takes it to a different level in a good way if you ask me. One I never thought of. [0] https://github.com/goadesign/goa
Neat. Wish it had vim bindings though.
Oh wow! Hah, learn something new every day, i'll have ponder some other go-ified name. This project is still super young. Thanks for heads up.
What's more, I already created a genetic algorithm library in Go called gago: https://github.com/MaxHalford/gago!
When it will have most useful features of vim
I guess haha, I didn't even know for the name thing. I like the fact you created an `Organism` interface. I did the same thing at first but then I got stuck. For example if you don't want to apply crossover then it gets tricky because the interface obliges you to implement the crossover method.
There is no magic in Touring complete languages.
I don't think this applies in all situations, but it's a great idea. If careful, I think it's actually a great way to start out if you want to learn more about routers and understand best practices. A switch on `req.URL.Path` is the simplest thing that works [0], you can always upgrade to something more heavyweight once you want/need/feel ready. But it's much harder to start out by using a really heavyweight, complex thing and simplify that. Anyway, it's definitely worth experimenting with and iterating on. I'm doing that here [1]. One weakness I've discovered is having to also take the method into account. Ideally, I want a mux that can automatically detect when a route is registered but doesn't handle given method, set "Allow" header and return http.StatusMethodNotAllowed. [0] https://github.com/shurcooL/play/blob/a2441b05dadc2698cc34fe87ffb1169cab3d085c/170/usercontent.go#L14-L15 [1] https://github.com/shurcooL/home/blob/a2aefd82ef2b5e52c7431dff99525994557bb8f5/sessions.go#L261-L264
I was about to post that when I decided to double check which links you posted. :) This article made everything pretty clear to me and is what I'd recommend anyone too! If you come from a language like JavaScript where you never deal with bytes, it makes little sense to ever drop to a lower level concept than a unicode string--at least on the surface--but there are a lot of good reasons to deal with different types once you're doing things that need any sort of performance.
Thanks! Interesting point. It definitely suits the usecase of my library I need. Did you abstract out the concept of an operator?
If I can make a suggestion that will save you at least 6 months of your life, and your sanity - use React or Vue.js. Angular is trying to build an entirely new MVC framework on an already existing MVC framework (a web browser and your server). Your question shows a flaw in Angular right away, in that it's trying to handle routing via the client. What is your server there for? Just go take a look at how easy to understand Step 1 of their ['Quick'start guide](https://angular.io/docs/ts/latest/quickstart.html) is.
A router in Angular is handling the user's interaction with the SPA state. A router in Go is handling any client requests for Go-application endpoints. Go would help you construct a back-end/API and serve out static resources. Angular would help you construct a front-end/GUI. In other words, they both have routers.
You will typically route in both places. Your golang app will most likely expose an API with its own controllers and routes, e.g.: * www.example.com/api/users * www.example.com/api/teams Your single page app will most likely mutate the browser's querystring without necessarily making calls directly to your server; you will often separate the portions of your javascript application, a portion of which might be controllers that are invoked when a particular browser querystring is encountered. This is where you might have client-side routing occurring. E.g.: * www.example.com/teams * www.example.com/users
You need both to varying degrees and life is easier if you have a clear separation between api (preferably on an api. domain) and 'views' to make sure there is no overlap. You need the latter to serve up the index.html page for your SPA, whatever framework you're using. You can go further and add some server-template rendering to the generated index.html page to add page titles and other metadata (open-graph etc...) and then you'd need to have some mirror of the client-side routing on the server side as well. There's an example here https://github.com/CaptainCodeman/go-poly-tenant of doing that for Polymer (more for the multi-tenanted aspect) but hopefully it gives you the basic idea and something to look at. Having used Angular 2 for the last year, I can't recommend Polymer enough ;)
That is some new bug. It certainly doesn't need two ports.
Well the scenario is as follows. Say package A is ConfigService and Package B is some HttpService. B depends on A. When bootstrapping the tests we initialize the Ctx, which has all the application services in it. So what happens is that both A and B get loaded. I can manually bootstrap a different context for each package ignoring the services that belong in that package, but that sounds very repetitive and I wasn't sure if thats typically the way to write a common service container thats reusable for testing and prod purposes.
You will have both. The usual setup is to have client facing routes from your framework of choice: Angular, React, Ember...etc. You will also have endpoints(routes) for your API that exist in the backend. In this case, it would be implemented using Go. The only weird exception is if you use Universal React, but that wouldn't work with Go. 
&gt;What is your server there for? API server? You know you can use the angular router to change states/views on the client and then use an API written go (aka backend routes) to get/send info from the cleint. This is the exact premise of the MEAN stack (mongodb, expressjs a web framework for node, angular, nodejs). Instead here, http server in go, use the router to just send back json data to the client, use angular client for the web application/user interaction instead of server rendered templates
There is this: https://github.com/deepakjois/gousbdrivedetector/ It is incomplete. It shouldn’t be too hard to contribute to it, because it is basically a port from Java of: https://github.com/samuelcampos/usbdrivedetector
So it's chosen because it is nice to write in basically? This makes sense as these projects are open source so it probably helps to get them more contributors. And because there isn't existing work in the domain with a standard language they basically got to choose? 
I'm quite happy with Angular 2, especially now that the angular-cli has adopted webpack. To your points: 1. 6+ months is way too much. I learned Angular 2 in about 2 weeks. Depends on the learner of course and I'd recommend to wait until it hits 1.0 2. With tree shaking support, the build can be actually quite small 3. That's true 4. That model becomes worthwile when your API starts to serve more than 1 client. And even for only a browser client, it feels pretty snappy Using React comes with many similiar caveats and like you said -- it's only the view layer so chance might be that you have to invest plenty of time in learning the whole ecosystem around it. And personally, I'm fed up with React after using it for quite some time.
Yes, you would only use React where it is actually needed. Dashboard that live updates? Analytics/stats page that lets you filter? Adding a new todo item? Any page in your application that requires live updating via AJAX is where you would use React. These are the requests that hit the server API. At that point, each page can be its own, much smaller, React application. Using webpack with babel gives you component reuse. To me, this is a much easier, more straight forward solution.
First time hearing about it tbh, just googled polymer and I'll have to read up on it. Edit: Check out this post: [https://facebook.github.io/react/docs/webcomponents.html](https://facebook.github.io/react/docs/webcomponents.html) Seems web components are designed to handle creating reusable components, while React of course can do that as well but is mainly there to keep the view up to date (which web components doesn't seem to do, and what Polymer is there for, like React?). As I'm typing this, kept googling, and yeah it seems like Polymer is trying to be like... React? [https://www.polymer-project.org/1.0/docs/devguide/data-binding](https://www.polymer-project.org/1.0/docs/devguide/data-binding) This seems like a really shitty way of Google essentially copying ideas from React without having to change their previous poor implementation (Angular). Don't get me wrong, I love Google and I really, really love Go, but this is hilarious. My view on web components and Polymer? It was made by Google strictly to compete with React, cause they see it's the better way. Edit 2: Truth is, just keep an eye on it, if it's a better implementation then of course it should be used and React will follow suit. I'd have to study it more, but it at least seems like it's going in the right direction.
Looks promising - but you should at least provide some selling points here on Reddit: * differences with existing libraries (tealeg/xlsx comes to mind) * what is implemented, what is not, i.e roadmap
&gt; I've been looking into blockchain recently and I found that both ethereum and hyperledger seem to be implemented in Go As an aside, if you're looking into "blockchain", I'd be curious to know what aspect of the technology interests you, because I'd argue that both Ethereum and Hyperledger are not very good examples of the tech. I'd suggest taking a look at the [original Bitcoin whitepaper](https://bitcoin.org/bitcoin.pdf), and would point you to the Go implementation of Bitcoin: [btcd](https://bitcoin.org/bitcoin.pdf)
&gt; and where maintainability must be high (you come back to this code in 1 year you should be able to figure out what it does pretty quickly) This is very very important for projects that have long life. It also very helps that Go is simple and has relatively small number of quirks you have to remember/rediscover after coming back to it after a pause. Also Go doesn't brakes language specs with new minor versions (1.x works without a hitch on all upgrades last 2 years). This makes Go perfect language for project you will maintain for a significant future. Most comparably new languages fail miserably in this regard (I'm looking at you D) while Rust have its dose of criticism too (they introduced big changes after 1.0 release and subtly broke specs).
Kind of a deceptive title. This isn't an implementation of an S3 alternative in Go. It's just a different library to interact with S3.
But it looks like there is also [server](https://docs.minio.io/docs/minio-quickstart-guide). Maybe I missing something? Update: I tested it and there is a server with S3 compatible API and also web client.
&gt; tealeg/xlsx I noticed the mention of tealeg/xlsx in the README and am also especially curious if this is a different approach (and why it's an advantage) or if it's simply a learning exercise. From my looking around at the examples, it looks like you followed a procedural style, where tealeg/xlsx used receiver types, giving a somewhat oo feel. That means tealeg/xlsx operates on objects `sheet, err = file.AddSheet("Sheet1")` and you operate via functions that pass back the new file object after taking it as a first parameter: `xlsx = excelize.NewSheet(xlsx, 2, "Sheet2")` Is there anything else I'm missing?
I got the impression that some people were staying away from the specification until it stabilized and had widespread non-polyfill support with the shadow dom / styling issues settled. That said, CSS Variables look pretty cool. I seem to remember that polymer changed quite a lot ~1.0. https://blog.revillweb.com/web-component-challenges-a09ebc598d65#.tlbt8a4ry See also: http://www.backalleycoder.com/2016/08/26/demythstifying-web-components/ Note: this isn't my area of expertise!
Don't use this or self or anything similar. It's uninformative. func Speak(a Animal) Is the same as func (a Animal) Speak() Literally the same, the latter is just syntactic sugar. I hope you'd never write func Speak(this Animal) So please don't do this: func (this Animal) Speak() Also, it marks the code as unidiomatic, and raises red flags for people who have been coding in go for a long time. If the author ignored such an easy convention, what other conventions are being ignored?
Sure, all that is fair and I don't expect it either. However, the title says this is an alternative to S3. It's not. So this is either mistakenly deceptive (as I expect, thus my comment) or click-bait. 
If using 'this' makes sense for you and your team, then use 'this'!
The name "a" isn't any more informative than "this". They send slightly different messages depending on your perspective.
gofmt testcases? ;) &lt;/sarcasm&gt;
It's easy to write in, and statically linked binaries with easy cross compilation means it runs basically anywhere with no worries about a VM or runtime on the target machine. i.e. it's easy for end users to run it themselves, even if they're not developers. You can do similar things in other compiled languages (and even in some interpreted languages, with the right tooling), but it's not as straight forward. With go, it's easy and it's the default. 
I learned about it today and gave it a spin and really liked it (checkout minio: the server) - API is compatible with S3. So if you have code which uses S3 but some installation cannot use AWS S3 (say no public cloud) - this could be a good option without the need to re-write. - There are other S3 API clones but this can also store and distribute object replicas to different drives to provide some form of redundancy. - I also looked at the repo and liked how it has been packaged as a Go program: single executable which also includes static assets etc. This is also a project with a good amount of activity. Overall I am glad this exists. Both for what it does and also as a good example of what makes Go shine.
S3 is a solution for secure, durable, and highly scalability distribution of assets. This might be a cool product, but not a solution for that. This mimics the API, which is great, but that's like calling a picture of a hamburger a hamburger.
Your Makefile is 40% of the repository's code... Anyways, if you want to have multiple commands, the most common way I've seen is to have a `cmd` folder then two or more folders, one for each command which then will contain the `main.go` of the command. By following that layout, when someone go gets your repo, they will also get all the commands. So in your case, instead of `cli-chomp.go` and `cli-chop.go` you would have a `cmd` folder, containing `chomp` and `chop` folders containing `cli-chomp.go` and `cli-chop.go` respectively or you could even rename them as `main.go`. Helpful article: [Five suggestions for setting up a Go project](http://dave.cheney.net/2014/12/01/five-suggestions-for-setting-up-a-go-project)
Honestly I don't think it matters either way. Go chose this convention; Python, Java, etc, chose another. Both seem to work fine. I can't perceive any harm or benefit in either direction.
I vaguely recall someone writing a command line Google search client called s as well, also in Go if memory serves. Just a heads up.
looks like a fork of c9
It is but you mentioned javascript size and while I understand that React isn't as big as Angular once you add in libraries or whatever code you need to perform your business logic and calls to the server then you are probably at about the same size as an Angular application. I'm not a proponent of Angular but client side routing in my opinion is vastly superior to wiping the page and loading an entirely new one.
The font is all garbled - can't read anything.
i would argue that you should still treat receiver variables like parameter names. if your convention is to name parameters long form names (animal *Animal), you should do so, if it's to use shorthand (a *Animal), then do so. The idea is to treat them like parameters, so that when you refactor code, the locality of the type it is attached to doesn't matter. self.Speak(), for example, is specific to code attached to a struct that has a Speak method, not an animal with a Speak method
Except in other languages 'this' is different from Go's receivers. Go's receivers are syntactical sugar for the first argument; that is, `func (t T) Foo(t2 T2)` is essentially the same as: `func Foo(t T, t2 T2)` If you wrote this Java method: `public void Foo(T this)` people reviewing your code would take issue since 'this' is a completely nondescript name. Plus, like I sort of mentioned above, calling the receiver 'this' implies that it's the same as 'this' in other languages---it's not. In JavaScript, for example, 'this' defaults to the global object (Go has no such thing); is undefined or the global object during a function call with and without strict mode, respectively; or, if it's an object method, it refers to the object the method is called on. This is distinctly different from Go. Write Go in Go. Don't try to write Java or JavaScript or Python.
You can write apps using golang but you will have to use low level graphics API calls to create all the GUI. What I have done and found most useful is to use gomobile as an SDK that i use to handle most of the business logic and use java, in your case, as the GUI layer. 
Thanks for the pointer. At a glance it looks like it could simplify the cookiestore setup step for clients too. I'll have a proper look into it.
I don't think you touched on this, but I'm principally excited about the opportunity for a REPL. The existing solutions I've tried have been clunky and slow, and I don't expect "improving the compiler" can be reasonably relied upon to bring performance down for this use case. Maybe I'm wrong?
You don't know what you are missing. * You can search for a.foo or b.foo, where in a large code base there can be many more this.foo. Especially in Go where a method can be in any file. * Follow conventions mean it is easier for others to read your code and you to read others code. Please follow conventions, if you ever collaborate with others. * this is a keyword in other languages that does object oriented programing differently from Go. It act as a trigger word for me to change my programing model. You should remove all this from your .go files now, and train yourself a habit of writing idiomatic Go code. Otherwise you won't get the benefit of the above.
&gt; but not both at the same time. I disagree; tabs for indentation, spaces for alignment. ;) But, I wasn't actually trying to debate tabs vs. spaces. I care so little about that, that I was genuinely surprised go uses tabs; If you'd asked me yesterday whether gofmt outputs tabs or spaces, I couldn't have given you an answer. I only wanted to point out that to attribute the uniformity of go here to gofmt is a fallacy. Mentioning any opinion of the issue itself was just gentle trolling :)
The problem is, tabular alignment breaks if the tab-size changes.
I myself prefer spaces, but I love standards more.
The closet thing to a REPL is of course play.golang.org I use it every now and then as a lab for things but here you are in essence writing small programs. I never really understood the REPL approach. It's just forcing you to have everything on one line. If you really need dynamic evaluation there are other programming environments. Maybe you can use parser.ParseExpr and Gohpher.js to run Go within Node? It's very expensive to do so but it will give a REPL with Go syntax if that is what you want... and even if it is expensive it would probably run just fine.
That's why Go aligns things after the beginning of the line (e.g. comments on structure members) with spaces.
I should be safe? I am just worried about SQL injection. All my db.Exec are like the one listed above and none of them are like: _, err := db.Exec("INSERT INTO books(name) VALUES("+university+")")
I already answered that question. However, I recommend you read: https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet
You're not missing anything, you're just deviating from the convention. People in this thread are getting religious about it and downvoting you (here, have an upvote). It's not a big deal, it's just a little annoying to look at for those of us who have acclimated to the Go convention. If you want your code to be a little more reader-friendly to Go programmers, avoid using `this` or `self`, but contrary to the impression given by other commenters it's not going to miraculously improve your productivity or make your algorithm O(1) or something.
Indeed. I completed it but it was daunting.
I found this project the other day while Googling GlusterFS/Ceph. My use case is fairly simple, I have a few leftover servers on a rack and I want to have a backup service which is not a SPOF (NFS), and I don't want to buy an expensive NAS. This project is fairly easy to get up and running and I can still upload using s3gof3r. Thumbs up. Will try it again later.
Interesting survey. I'm going to repeat what I said at the bottom here because I'm curious what people's thoughts on it are. Basically, I'm starting to wonder if maybe the whole approach is wrong. Every dependency manager I've ever dealt with uses a version-based approach. You tell the dependency system that you want to use a specific version of a package, or a version range, or something similar, and it takes care of it for you. But maybe that's not the right idea. Most people, I think, are looking for two specific things when they ask for a specific version: Compatibility and security. But these are not things that are tied to a specific version. What if a dependency management system was able to determine the best version to use via a combination of a small amount of metadata and how you actually use the package in your existing code? Go's interfaces basically do this at a language level; is it possible to do at a dependency level? There would need to be some metadata so that, for example, a library could tag stable releases, or so that a function's functionality can be tracked, rather than just the signature. But, other than a few things, couldn't a dependency manager automatically figure out the best revision/tag/version to use based on how your code and your other dependencies use it?
Automatically figuring things out is an ideal. But, we are a long way from that. For example, a function signature gives us an interface but it does not tell us what features or bugs live inside. How do we handle that level of compatibility? Imagine a parser that finds 3 things in one version and 4 in another. The input and returned values can be the same but what it does are different. Or, not everyone keeps the tip of master in a stable healthy state. These are just a couple examples. It's worth re-thinking dependency management but when we sit down to go from concept to meetings peoples needs it's more difficult that it first appears. Anyone who wants to go down this road I challenge to try and figure out the how while dealing with the real and complex situations. I've love to see an answer.
+1 The survey was too long.
Dependency management is really just about fetching the right dependency with the right version. It shouldn't even try to worry about API compatibility. Go get DOES fetch dependencies, it is unable to track versions of dependencies. These versions have to be described somewhere, like in a manifest. I think what matters is WHAT format that manifest will take and HOW will it resolve versions. 
It should definitely be possible to configure specific versions if necessary, although I think it would be more useful for if the automatic dependency detection failed. In terms of conflicts, isn't that the point of the Go 1 compatibility guarantee?
Hmm, looks pretty thorough in features. I'll have to take a closer look, but it's nice to see. Having said that, may I suggest using a bit more descript commit messages? It's really hard to read through a project when it's commit messages [look like this](http://i.imgur.com/Ttf49qX.png).
There are stdlib features in Go 1.7 that are not in Go 1.4, right? The compatibility guarantee does not cover that. FWIW PHP's Composer does have a way of addressing interpreter version, which I found cool. It's actually the most straightforward tool I've used (from a UX perspective).
Enforcing something that 80% of programmers disagree with. :)
In some cases that defeats the purpose. For instance, the entire objective might be to learn something about the data, not necessarily to come out with a reproducible program. You need to be able to dynamically poke and prod at the data, and interpreters are very useful for this.
PEP8: &gt;Spaces are the preferred indentation method. &gt;Tabs should be used solely to remain consistent with code that is already indented with tabs. 
Huh, didn't know that.
Your import statements should just be: import ( "fmt" "log" "net/http" "os" "strings" "github.com/dimfeld/httptreemux" ) Also, take a look at [goimports](https://godoc.org/golang.org/x/tools/cmd/goimports).
Left a lot of blanks. Too long. In terms of the questions about what I wanted/needed, I found some of the questions interesting. To satisfy all the needs they seem to be considering, you can imagine them recommending `import` is extended in a later version of the language to have `Gemfile` style semantics for being able to track tags (incl. versions) with a default of `master`. If `$GOPATH/src/` is mostly git repos, that would actually be easier than how Ruby (with rvm) does it with actual Gemfiles.
What about breaking the API between commits to master? Being tied to the API you designed 3 years ago with no room for change OR being forced into creating a new package isn't fun. There's a spectrum between 'breaking things every week' and 'making sensible changes to the API over time', and the lack of widespread versioning/dependency management facilitates neither.
I thought the survey was excellent! I am really curious what comes out of it!
How I do dependency management? &gt; go get -u -v &gt; go build -v Then fix anything that broke (and things ever only broke once, a function removed one of its arguments). How do I fork without changing namespace? Use git to push and pull from alternative remote repo.
And again, this is a tool that is probably very useful for every developer and maybe even operations guy. Yet it is only posted in /r/golang. How is it relevant that it is written in Go?
I'm afraid that it will become a competition of existing tools. Or a challenge to accept most of the features. I would prefer that we keep only one way, the most simple, even i have to change my own workflow. Please KISS and opinionated!
One of the statements in the article isn't quite telling the whole truth. Interrupting idle connections can be done just by calling conn.SetReadDeadline(time.Now()). This interrupt will only take place upon the next Read operation and so for long lived connections suck as WebSockets that may be idle or only being used to push data, you may have to wait a long time before this happens. P.S. to account for a long lived connection being used only to push information, you should also set connection.SetWroteDeadline(time.Now()) if this is still the approach you wish to take.
Check out this: https://github.com/xlab/android-go
Safe
You can use letsencrypt to create your key and crt files then pull them into Go with ListenAndServerTLS() To get the .key and .crt file you will need to run Let's Encrypt in manual mode Docs here: https://letsencrypt.readthedocs.io/en/latest/using.html#manual GoLang Example With TLS: https://gist.github.com/denji/12b3a568f092ab951456 
Yes I have done it that way before. But I was hoping to use Russ's package - github.com/rsc/letsencrypt
I am the dev of this keyboard, hope you enjoy it ʕ◔ϖ◔ʔ. If you have suggestions for more gophers to add, I'd love to hear them!
I looked a bit and it seems this isn't dependent on the rsync tool itself, right? However, does it implement the same protocol as rsync or just a similar type of thing? Does (or will) it support Windows file permissions and shadow-copy? Can this backup to a local network server somehow? I'm looking for a replacement for rsync on Windows hosts for running back ups.
The Go package https://github.com/xenolf/lego has all the code in it for getting back and updating the Let's Encrypt .key and .crt files. There is a command line main as a nice example and a library. You set up a go routine that runs once every X days to update the .key/.crt files. Then periodically you re-load the server. This can be done cleanly with https://github.com/fvbock/endless.
Maybe we'd like to see how it's written.
Ah thank you I didn't know Reddit had markdown, thanks for the heads up!
I believe [this](https://play.golang.org/p/ZnZoIFgANJ) should work. Add the proper imports obviously.
"tabs" I give, you (i.e. if gofmt would use spaces, they would use spaces), but not "universally". Again, ruby uses just as universally spaces.
awesome \ʕ◔ϖ◔ʔ/
&gt; Your Makefile is 40% of the repository's code... Lol yeah, that's been my crutch while I learn Go idioms. When a user runs `go get ...`, does Go automatically build the .go files, and even place the binaries on PATH? Or what would be the typical steps for a Go user to begin using the cmd/... programs from a Go package URL?
Was thinking of making a twitter bot some time ago but never got to do anything, now I feel the urge is comming back with this nice library (and docs). It's lacking tests though, even simples ones.
It being written in Go tells a lot about a program, which go coders know: it's a single binary without a dependency mess, that's probably portable. It also means that if you would want to use any of the underlying tech, there are probably good packages for it, and you now have an example on how they're being used.
No, you should never implement bubble sort. ;-)
The memory graph is probably misleading, it looks like the author didn't normalize for number of connections, so Ruby appears to use almost no memory while languages with lots of clients appear to be using lots of memory.
I filled out one before with a bunch of questions. Didn't see a followup post about the survey results. It's sad we have spam surveys in programming groups.
In my opinion only "invoke" (https://github.com/zx48/twitter/blob/master/core.go#L41) and "makeUrl" (https://github.com/zx48/twitter/blob/master/core.go#L67) should have tests. But even "invoke" is too simple so i skip the test (i believe http.NewRequest and ioutil.ReadAll well tested by the Go Team). All other function are simple wrappers around "invoke" and "makeUrl".
Should really have done .net and signalr, and then the same with .net vnext on Linux 
Yeah, I noted the normalization issue too, but the verbal description of the results seems to still be very good.
For sure, thanks for the tip!
For me, having well defined subroute groupings is really helpful. My typical setup is something like: `/static/`: all js/css/images/whatever. Usually bundled into binary with [this tool](https://github.com/mjibson/esc). `/api/`: all of my api routes. `/`: anything else just serves my index.html. Client side app does all the rest.
taught this old dog some new tricks
[removed]
This is exactly the kind of problem where C++ excels: the programmer has detailed control of memory allocations and de-allocations. For a problem like this, the `std::getline` interface actually encourages performant code, with a single, re-used buffer. Throw in some `string_view` objects to give views into the buffer, plus some template functions for conversion (ahem, generics).
If a fast CSV parser were the primary criteria for this project, I might have chosen C++. EDIT: Controlling allocations via profiling in Go has been surprisingly easy.
&gt; But there's probably not too many Go generated public keys in the wild. I wouldn't be so sure, the last year or so would have seen a significant uptick. Just looking at Let's Encrypt adoption, I know that &gt;100k active certificates came from a tool I wrote, and other offerings like acmetool/Caddy/random integrations in Go-based software are getting fairly popular. Certainly nothing of the scale of OpenSSL though. Anyway, I clarified my original post, I'm not too sure what the ability to classify the origin of an RSA key has to do with it.
&gt; Anyway, I clarified my original post, I'm not too sure what the ability to classify the origin of an RSA key has to do with it. Arguably security. I would rather lack of classification at the expense of a marginal speed loss (if its a choice between openssl/gpg/whatever and using Go). I'm not sure why speed of key generation is that important; for all we know, OpenSSL could be taking shortcuts and not implementing proper random key generation.
I always thought that streaming CSV reading should be in the standard library in the first place (feeding back through a channel so you can range over it, etc). Maybe add that as another helper to your version?
I was on HackerRank and they wanted me to write one D:
Doing it this way actually has a lot of side benefits I hadn't considered - thanks!
just want to chime in, a channel is less performant than a mutex.
&gt; for all we know, OpenSSL could be taking shortcuts and not implementing proper random key generation. You really think the most used opensource SSL tool on the planet could be taking shortcuts &amp; literally no one has noticed? I mean, it could be the case. But it's pretty unlikely. I trust the OpenSSL implementation more than I trust any closed source or small project's implementation.
Perhaps a similar API to `bufio.Scanner`? scanner := csv.NewScanner(r) for scanner.Scan() { line := scanner.Row() ... } if err := scanner.Err(); err != nil { ... }
&gt; You really think the most used opensource SSL tool on the planet could be taking shortcuts &amp; literally no one has noticed? [Yes.](https://en.wikipedia.org/wiki/OpenSSL#Notable_vulnerabilities). Perhaps shortcuts was a bad word. But perhaps rotting code that hasn't been looked at because it's far too obscure and nobody bothered to review the implementation because it worked. Until recent years, that's pretty much how OpenSSL operated.
All pattern links end in 404
Why no C?
cool. will have a look 
A Singleton is not an awesome pattern, neither in go, nor anywhere else on this planet. :) I would comment on the implementations, but - as already mentioned - all links point to missing resources (404).
Vim has session persistence.
Is that only for x/net/websocket or are other parts of x/net also abandoned/deprecated/not maintained?
The real treasure were the gophers they met along the way!
What if newContextWithRequestID only took a request and got the context from request.Context()?
You honestly almost never need to think about it. Lots of times I use neither... I dump a bunch of garbagely formatted - but valid - go code into my editor and let gofmt sort it out. 
This doesn't require synchronization of any kind, so the only benefit to using channels would be perceived ergonomics (being able to use `range`), but I've never found channels to be as ergonomic as scanners/iterators even though there is no syntax support for those patterns. Never mind the performance hit.
&gt; Why is ethereum and hyperledger not good examples? If you define a "blockchain" as an immutable public ledger, both ethereum and hyperledger do not properly satisfy that definition, whereas Bitcoin does. I'd be interested in finding out what makes you think that they're "vastly more advanced" btw :)
There's nothing wrong with the singleton per se. Usages and implementations of singleton are often discutable though.
I was so excited..
I've taken a look at go-pocketsphinx but was not able to find a way to do a stream rather then run and exit.
I'll be sure to check with you before posting anything in the future.
First, make sure your prototype exists in $GOPATH/src. I like to use the full $GOPATH/src/github.com/peterbourgon/prototype path structure even if it's not committed yet, because it's nice for consistency. But you can also do something simple like $GOPATH/src/prototype, if you want. Then, just use `go install &lt;pkg&gt;`, e.g. `go install github.com/peterbourgon/prototype` or `go install prototype`.
If you have no intention of touching your source code, why do you care if your dependencies are out of date?
GHOST (14s block time), transactions per second, full VM on chain, asics resistant POW.
http://i.imgur.com/JxErC67.gif Edited- There is only one handler func and http.HandlerFunc is it's name. On a serious note, and with kudos to the author for their work (really, it's admirable), I am inclined to avoid this sort of indirection.
This is the final part in this series, but I may end up doing a followup showing how a controller would work alongside views like this. If you have any interest let me know!
Cool beans. Thanks! I just discovered gRPC. . . it looks so useful for the kinds of things I need to do, planning to play around with it soon.
interesting picture, try it on https://github.com/pressly/chi .. it's stdlib-only
I'd love to, but it looks like `chi` moved on to 1.7 already. I'm on 1.6.3 still, which makes chi not possible to install ATM. Would you mind adding the graph? It should be as simple as ``` goviz -i github.com/pressly/chi | dot -Tpng -o png/chi.png``` edit: to clarify here, `x/net/context` automatically imports `context` in 1.7, which allows everything that has `x/net/context` as an import path to still work in 1.7. But the opposite is not true if you're on 1.6. If a project imports `context`, then users who have 1.6 and earlier can't `go get` your project. https://github.com/golang/net/blob/master/context/go17.go
I like how "micro" is basically the dependency clusterfuck.
what is your take away about built with net/http example?
note that "go get" is the only thing that knows that import paths are URLs. go install, go build, go test, etc only know that 'import "github.com/foo/bar"' refers to $GOPATH/src/github.com/foo/bar The answer is to just make folders on disk in the spot where go get *would* put them, and then just use go build or go install normally.
Please don't draw the wrong conclusion from these graphs. Vault is a complex, production ready application which does much more than your typical CRUD app. I find the vault http code to be of good quality, and written in an idiomatic style. If I was learning how to wrote APIs in Go today, I'd look at the vault http package to learn from.
No, the finalizers on os.File are not to be relied upon. You really need to close your file descriptors.
Looking at Revel, I'm wondering: - why does it depend on gocolorize ? Realistically you'd never need to colorize your terminal output on a production app - why does it depends on fsnotify ? if it's to simplify development with stuff like hot reloading, it kinda sucks that even in production you have to carry that dependency. - is it a good idea to use klauspost/compress/zlib and klauspost/crc32 instead of the stdlib ?
Duh, good point.
As a fellow C++ developer, I fail to see how this is relevant in any way. It's about object reusage and not about controlling allocations \ deallocations. Go allows this, Btw getting memory from OS and returning it back, is actually more expensive then getting it from managed heap. Yes - on some scenarios languages with GC are actually faster. So, to say again, I see no point in your comment.
https://github.com/groob/goviz-frameworks/blob/master/svg/echo.svg It's there :) There is a bug in goviz on master, but there's a PR to fix it. You can download the .go file from the PR. Ran into this muself.
A singleton is just a single instance of an object. If you want that, just create one object and inject the same object into all the other objects that depend on it. Elevating this simple principle to a pattern, or creating bombastically misguided implementations that essentially create global state while violating the law of loose coupling, is unnecessarily giving singleton too many credentials.
The singleton specifically address this "just create" to make sure you have only one instance. If you create a library, you may want to make sure you have only one instance of something. Instead of letting everyone decide how the "just create one" and only one object, this pattern gives clear rules making it easy for everyone to understand what's going on. And that's what patterns are most about: a common way to solve a problem. Like a common language. As for loose coupling, it's not a problem of the pattern at all. If you inject your non-singleton object into everything, you might end up with coupling problems anyway.
I think it can be an indication of "some frameworks have a lot of dependencies for no good reason". e.g. in the echo framework somewhere else in this thread, a bunch of color/tty dependencies are brought in because the logging middleware wants to be able to print colors to the terminal. Perhaps if the author had considered how much extra code that functionality would require (esp. considering that 90% of the time you are not logging to a tty and therefore colors are irrelevant), they may have reconsidered color support, or put color support in an optional package. (Not picking on echo, it's not an egregious example by any means. I love echo!). Perhaps it is a more useful tool for maintainers rather than users, as transitive dependencies might be overlooked during development. You might accidentally bring in a cgo dependency and then you're really in trouble.
Because you are passing a slice of slices, you could just as well set those to point at the internal buffer, rather than copy. The problem with this approach is that you're returning internal buffers, which can be dangerous. &gt; The user will have to take care to understand that the slices they provided might not be the ones that contain their data. I'm not sure I understand this fear. Either way, you end having to [allocate when encountering a large field](https://bitbucket.org/snippets/weberc2/goEb7#csv.go-40). With the API proposed above, the user could control when this allocation happens, or let the call fix it up with an `append`, as shown in the example. On average, there shouldn't be any allocations once the largest fields have been encountered and there will never be more memory allocated that it takes to handle single row.
Please could you let us know when the 404 errors are fixed
Wanted to put this here to help others find it. For reference, this was built because the existing Firebase API implementations didn't make it easy to use with Google service account credentials, and were not very Go-like with their APIs and made some strange decisions to forego using the standard packages (ie, ignoring http.Transport, etc). This does use the github.com/knq/jwt library for signing JWTs, as the oauth2/jwt package does not provide a way to encode custom claims on a JWT -- see the open issue I have here: https://github.com/golang/oauth2/issues/198 -- if the issue gets closed, then I will modify the package to use the standard JWT/Google provided oauth2 implementation.
 func handler(w http.ResponseWriter, r *http.Request) { bootstrap.ExecuteTemplate(w, "bootstrap", nil) } Is calling bootstrap that way thread safe ? 
I haven't got time yet to dive into your code, but quickly looking at the code you linked to I wouldn't say `invoke` is simple (even if it's just a bunch of calls to the stdlib. btw, there's an ignored error on line 58). Since all other functions depends on it, there's even more reason to make sure it's well tested. Personally I would also had mocked all the calls to the external API and at least test that all the functions in your own API behave ok. Sure it would be a lot of pain and work to even do that, considering the size of the twitter API. This reminds me of why I never got started on that bot, too much work to get started with the twitter API and testing it D:
After all, isn't Go the most popular language in China now? :trollface: I'm kidding, don't take me seriously, please.
This is the equivalent of building a part of the atomic bomb that blew up over Hiroshima. How do you feel now that you have become a destroyer of worlds and all that?
I am quite despondent, as it is clear that this trojan is truly deadly, and could not possibly exist without my ingenious code. 
Hold on, I've almost completed my own full-featured bindings, http://github.com/xlab/pocketsphinx-go with almost entire API exposed. Need to write README and an example.
Actually, that repository contains a few different variants on the idea, but the exported Encode and Decode functions do not use gob at all. The reason I was experimenting with gob was that in one version I tried to store a "descriptor" for the encoded object type, so that I could verify a match at decode time. This descriptor ended up containing some slices and pointers, so the most straightforward way that I could find to encode it was using gob (binary only works for objects stored contiguously in memory).
Yes. OTR should be fairly easy to add.
This looks fantastic so far, well done! Looking forward to the docs. 
Is that still a thing? I thought the Go team is going for repeatable builds.
Mostly. But there are a couple issues about it that are still open. https://github.com/golang/go/issues/9206 https://github.com/golang/go/issues/16860
It might be just standard lib stuff, everything on library level is an 'api' so libc things like 'printf' and other calls have a consistent fingerprint. But internals could be obfuscated, even post-compilation. Or with a pre-processor. I always thought internals were represented in some kind of low-level notation that wouldn't allow for such verbosity, but obviously there would have to be a reason to do it in the first place and I can't think of a legitimate reason myself. Seeing how we're discussing trojans here, it would apply :(
Well, Go is great for AV-Avoidance as AV-Engines detect it to be a native application. Due to this normal 'C'-kind of rules apply for the heuristics engine. Therefor the Go-Memory management makes the AV-Heuristics Engine time-out and allow the application. This bundled with using Go packages allows ease of writing complex malware. Although for pentesting you really want to have an interactive shell, with network pivoting, screenshotting the works. In order to get this the preferred way is to get meterpeter running on the 'victim' box. As this is a very commonly known piece of code (being from the Metasploit project) this is flagged everywhere. What we do, and this is what makes Go awesome for AV-Avoidance, is use 'net/http' (or any other package that allows bufio) and parse it to a "import 'c'". Then in C you parse the buffer you get from Go and make a 'mistake' in order to trigger a vulnerability. That allows you to load meterpeter in memory. So, I expect that AV-Vendors will first wrongfully tag a number of Go libs as bad and then learn that they need to learn to create a separate heuristics engine for Go. ps. People might not understand why it is a good thing for people in my profession to beat AV. But it is an extremely important thing, as it lets people realize that AV is nothing more than a fall-back on attacks that are VERY common. Even Check Point inline IPS / AV are easy to bypass, given the right tools.
Shiva would like a word with you.
You can also use [`unused`](https://github.com/dominikh/go-unused#readme).
&gt; I've been using go to write backdoors for easy av avoidance That is surprising to me, given the fact that Go programs are occasionally detected as viruses (false-positive) for years. EDIT: I'm too lazy to find links, but this is also not the first known piece of malware written in Go, so I assume that if anything were to change, it would've changed years ago. 
&gt; Destruction is always far simpler than construction. I hold no virus writer in esteem. Writing something to cause pain is no accolade whatsoever. &gt; &gt; You want points with me? Construct something that makes life better or easier for people. Or increases security. Where do we draw the line? what about all the people that only ended up in the *virus writing business* because that's the only job that pays to put food on their table. I'm not trying to glorify anything or imply that writing viruses is a good thing, But to me, it's not much worse than working for $company writing shady code designed to track users against their will online.
I built something like what you're describing a few years ago and use it for a variety of pipelines. It has some areas around logging and buffering that could be improved, but it works great. https://godoc.org/github.com/jboelter/pipeline https://github.com/jboelter/pipeline 
I don't know anything about the book but I would think it's just a random extension to indicate that it's a "go template language" file. The extension shouldn't really matter. Just the contents. 
I don't have a problem. It's works, though I am wondering if there is better/go-to pattern. Or maybe something that is written better. I feel like my version has room for improvement...
&gt; I once got physically beaten by a group of niggers because I interrupted one of them trying to hustle another white guy into a dodgy bar. &gt; If this is the kind of person you are, I'm not sure who on this planet would be surprised about what happened to you. &gt; The excuse? "This is my job! You interrupted my job! This is how I make money!" &gt; What's your excuse? &gt; Now, I don't give a fuck if you need money or not. You don't do it by going around assaulting people as if it is okay. &gt; Yeah, but what's your excuse? What were you hoping to accomplish by inserting yourself into a conversation between me and someone else? &gt; You can bet your bottom dollar I would murder those niggers. &gt; That's funny because it seems to me that you only have the courage and such a big mouth to say this online. &gt; And I'll sure as anything fuck up you if you think hurting people is acceptable as a means of making money. &gt; Hmm, many young men and women get duped into doing exactly that every day by this thing we call *the armed forces*... but it doesn't matter, because we all know you can't do anything more than run your big mouth online. &gt; You piece of shit. I've been called this before, so I don't know why you're telling me this again... I already knew that! 
Of course it won't work. It's parameterized. That's the whole point. Also no you don't need the mysql option. The user can't modify the queries so there is no point.
Absolutely read this. 
Thanks Fatih. You're awesome. Really appreciate all your hard work on vim-go.
https://medium.com/@lapingvino/pre-shared-keys-over-a-distributed-key-value-store-e84f248eaefe I really hope someone manages to create a real open source Bittorrent Sync alternative some day
Upon this guy's request: https://www.reddit.com/r/golang/comments/50qr24/offline_voice_to_text_options/
I had been thinking about outsourcing that part to snowboy (https://github.com/Kitt-AI/snowboy )....but was just in the "looking around" stage. Did you find good Go library for the alexa voice services API? The API itself seems fairly straight forward to build, but if someone's done the work or started, I would rather contribute to a standing project than build something from scratch... edit: fixed link
In the providing case, isn't this the intention of context.Context?
I know there is a lot of ORMs around, but this one looks really neat! I especially like the idea of making it "db first", compatibility tests, automatic timestamps, and the readme is well done and has a lot of info. Well done! The only thing I would suggest is sqlite support, but I know that takes time and it probably less popular for these things. So I understand going for postgres.
context.Context is not intended to pass application-level dependencies such as "db_conn" (https://github.com/augustoroman/sandwich#providing), but is meant to pass request-related data such as "User" (https://github.com/augustoroman/sandwich#handlers). However, it is possible that the hypothetical author of User got a little carried away with it's design (OOP zealotry) such that it has a field containing a DB connection. So, be careful. Consider it in this way: If you pass the context type to another service (e.g. logging), will that service be able to access the values? If it's a DB connection, obviously not. To pass application-level dependencies, http.HandlerFunc functions which require dependencies should be methods on properly defined types. type server struct { db DbInterface } func (s *server) indexHandler(w http.ResponseWriter, r *http.Request) { // use s.db } DbInterface can then be mocked in tests. Automagically managed dependency injection is far too clever for my (and probably a majority of Gophers) preference.
Not sure if I'm doing this wrong, but running the installation command in the README is failing? There doesn't seem to be a cmd directory in the repository? Project looks great! Looking forward to trying it out.
what would be the advantage of this approach over ldflags? https://www.atatus.com/blog/golang-auto-build-versioning/ 
Oh, thanks, I didn't know that was possible!
Awhile ago when doing some CSV reading I opted for doing strings.Split() over encoding/csv using a line reader (none of my fields had the newline character). I remember it being considerably faster. I think incorporating a loose mode parser (as opposed to strict) for efficiency with the caveat you're restricted to certain delimiters would be useful to the standard library. Then again, writing a CSV parser isn't particularly difficult that you can just build your own if performance is needed.
Thank you. This was perfect.
Yes. I read about the convention. However I, and I suspect many others group their code by project and not language. These scripts allow me to be consistent with my code organization. I think Go is unique in this requirement. Perhaps some more exposure will help me get used to the standard way.
daveddev is exactly right here, though I do want to add that context.Context has it's concerns. Consider this example handler: type server struct { db TaskDb tpl *template.Template } func (s *server) Home(rw http.ResponseWriter, req *http.Request) { rw, pResponseWriter := sandwich.WrapResponseWriter(rw) // Log the request logEntry := sandwich.StartLog(req) defer func() { logEntry.Commit(pResponseWriter) }() // Check authentication &amp; log the user if authenticated. user, userId := ParseUserCookie(req) LogUser(user, logEntry) // Render the home page. err := Home(rw, req, userId, user, s.db, s.tpl) if err != nil { // Ooops, something went wrong. Respond with an error and log it. sandwich.HandleError(rw, req, logEntry, err) return } } This is ye olde plain vanilla request handler. It's clear here that some vars (s.db and s.tpl) must be provided external to the request while some data (logEntry, user, userId) is derived from the request itself. As written here, the request handling is very explicit and context.Context is unnecessary -- conceptually, sandwich helps construct this kind of request handler. Written manually, however, it rapidly becomes unwieldy when you have multiple requests that all do the same logging &amp; authentication checks. Instead, middleware paradigms provide a means to easily consolidate this repeated code. A classic approach is having functions that wrap http.Handlers, and using context.Context leads to: type key int const LOG_KEY key = 0 func logRequest(h http.HandlerFunc) http.HandlerFunc { return func(rw http.ResponseWriter, req *http.Request) { logEntry := sandwich.StartLog(req) ctx_with_log := context.WithValue(req.Context(), LOG_KEY, logEntry) req_with_log_ctx := req.WithContext(ctx_with_log) h(rw, req_with_log_ctx) // Call nested handlers logEntry.Commit(rw) } } // Use different values or different types. Usually these will be in // different packages, so a different type is easy. const USER_KEY key = 1 func ParseUser(h http.HandlerFunc) http.HandlerFunc { return func(rw http.ResponseWriter, req *http.Request) { user := ParseUserCookie(req) ctx_with_user := context.WithValue(req.Context(), USER_KEY, user) req_with_user_ctx := req.WithContext(ctx_with_log) h(rw, req_with_user_ctx) // Call nested handlers } } So far, so good. The functions-returning-functions takes a little getting used to, but not bad yet. So how do we use the values embedded in the context (log &amp; user)? type server struct { db TaskDb tpl *template.Template } func (s *server) Home(rw http.ResponseWriter, req *http.Request) { user := req.Context().Value(USER_KEY).(auth.User) err := Home(rw, req, user, s.db, s.tpl) if err != nil { logEntry := req.Context().Value(LOG_KEY).(*sandich.LogEntry) sandwich.HandleError(rw, req, logEntry, err) } } In this example, I've taken pains to explicitly pull out the values from req.Context() in a single top-level handler rather than letting Home(...) assume that req.Context() has a user in it. In my experience, dumping things into the request context makes it difficult to keep track of which functions need what when refactoring and leads to run-time failures of the server for that one function that didn't get tested with the right set of inputs. In this example, if you didn't test Home returning an err, then you might be surprised when the server crashes because logEntry isn't there and the type assertion fails. If you're paranoid, you can do the `_, ok` pattern when extracting values from the context, but that just buys you a nice error message in your server logs or maybe a pager notification, not correctness. Finally, what does it look like to use this in main()? s := server{myDb, loadedTemplates} http.HandleFunc("/home", logRequest(ParseUser(s.Home))) // And other handlers each use the same wrap sequence: http.HandleFunc("/profile", logRequest(ParseUser(s.Profile))) http.HandleFunc("/foo", logRequest(ParseUser(s.Foo))) Thus, there exists several other middleware packages that allow you to replace logRequest(ParseUser(s.Foo)) with fn(logRequest, ParseUser, s.Foo) (e.g. https://github.com/stephens2424/muxchain, https://github.com/urfave/negroni, etc) When things get more complicated, you may not have a single server struct since some handlers may be defined in different packages or require different initial data, and the wrapping sequence will definitely be different between various endpoints. For example, some paths will *require* a user to be logged in (e.g. "/api/me/...") and some will just want to know if a user is logged in (e.g. "/home"). ------- Phew. Ok, that ended up being longed than I intended. But, for completeness, there's another common approach for injecting dependencies via closures. Instead of defining a server struct, instead you can make a function: function ServerHome(db TaskDb, tpl *template.Template) http.HandlerFunc { return func(rw http.ResponseWriter, req *http.Request) { user := req.Context().Value(USER_KEY).(auth.User) err := Home(rw, req, user, s.db, s.tpl) if err != nil { logEntry := req.Context().Value(LOG_KEY).(*sandich.LogEntry) sandwich.HandleError(rw, req, logEntry, err) } } } And this is called in main doing something like: http.HandleFunc("/home", logRequest(ParseUser(ServerHome(myDb, loadedTemplates)))) I think defining a struct is the most popular approach, but both work. I find closures are conceptually more difficult to keep track of in my head. ------- Alright, so given all that, it didn't seem too bad, right? Why did I bother making sandwich, anyways? For a couple reasons: * context.Context is nice and provides some really cool functionality, but I don't like type-asserting my parameters. Also, usage of dependencies extracted from a context tends to leak deep into subpackages and leads to difficult-to- test dependencies and unexpected failures unless you are always vigilant. Sandwich verifies that the arguments to your functions can be called before your server starts listening for requests, so you avoid run-time dependency failures. * The examples I wrote above are basically all boilerplate, including the context.Context per-request state management. The code I had above skipped some functions such as the LogUser(...) call in the original handler. That's another context.Context type assertion and wrapping boilerplate. * Error handling is always a tricky topic, and sandwich has a bit of a unique approach to it. In the manual examples I had above, error handling is deeply embedded into each handler. However, in my experience with larger servers, you typically want to consolidate error handling to allow: - Keeping a consistent user-facing error HTML page for user-visible endpoints. - Responding a consistent JS-friendly JSON response for programmatic API endpoints (e.g. for AJAX calls). - Obscuring internal error details and presenting a simplified error message (and sometimes an error code) to the user. - Logging lots of internal error details for forensics. Sandwich was explicitly design to support consolidating the error handling to support these goals. Hope this helps!
Not only is a workspace useful for building code, but the packages and tools which you 'go get' reside in that workspace. For example, I use goconvey, gometalinter, whatever vim-go requires, etc. With multiple workspaces I would need to redo the effort each time.
Would it be considered a weird bash file if it were only one line that tacks on the ldflags approach to the Go command? 
Thanks for the update! :)
Dependency injection has a well-deserved stigma, IMO. I've worked in a large Java codebase with Guice automatically injecting stuff and it was maddening for me to figure out how and where things came from. If a dependency was missing, the only way for me to figure it out was to sift through the codebase looking for the right source... or just ask somebody. Sandwich is, technically, dependency injection. However, if you use sandwich in only a single function to setup the routing and middleware ordering, it is more like a helper that just calls your functions in order. 
Of course I don't mind a rant, or criticism as long as I'm allowed to retort! There is of course exceptions to where this library will be useful. Even ActiveRecord has it's limitations where you fallback to raw queries to get the intense things done, and of course that's a use case we support. No ORM can cover every use case, the point is (as the name implies) to remove boilerplate from projects. That's to say I don't want to have to create a models folder, and start rewriting all the code that this would generate for you. I don't want to put a bunch of magic SQL strings that aren't checked at compilation time, and send all the whitespace inserted by Go's backticks string literal syntax to postgres. And of course you have to do that unless your project is tiny and you only ever fetch from your users table in one place, otherwise you're violating encapsulation, code re-use and general good coding practices. To me this is the sqlx way you describe as "clean", but I can't help but consider it very dirty and very repetitive, especially if you've just done all that work for one database in a project and then you move on to a new one, time to write SQL again! So you see it's not that this is necessary, or that it covers all the use cases, it's just that it makes for far less code because very likely I will end up writing all of this code by hand, something to load my users, something to update them, insert them, look up their relationships, count them, etc. You could also compare this to sqlx itself, why do you use it? Because Scan() is boilerplate you can do without. I feel like the basic CRUD operations are boilerplate I can do without. And of course there is no holy grail, so we should all stop searching if we haven't already ;) PS. Check the benchmarks, we do benchmark against sqlx to compare our raw binding speed because that's all sqlx does. We beat them. And we compare to the other ORM-like libraries out there because our featureset is much closer to them than to sqlx which has only one real feature: binding to structs. If we tested our inserts against them it wouldn't be fair because we do more work than they do eg. auto timestamps. GORM/GORP/XORM/SQLBoiler all accomplish the exact same task with the same features, the implementation and approach is all that differs which is exactly when you want to be benchmarking, to compare implementations solving the same problem. Thanks for your comments!
I've really enjoyed this series. Thanks Jon
https://godoc.org/net/http#ServeMux
I'd be interested in seeing how the view(s) can be used as a non-global 
We found that using a [build.go](https://github.com/bosun-monitor/bosun/blob/master/build/build.go) script to stamp builds works pretty well in a cross platform repo. We generate multiple binaries from one repo, so a shared [version](https://github.com/bosun-monitor/bosun/blob/master/_version/version.go) package is used to set/get the version number and the build script sets the git hash and build date. The only thing I didn't like was local builds via "go build" didn't get the hash or build date (just the preset 0.6.0 version and a -dev modifier), but I recently changed the version package to pull the file last modified date in that case, which works fine for testing. 
What about go-gettable binaries? None of the approaches discussed here (ldflags, go generate, go/bash build script) would work with pure `go get`. Only hard-coding (and maybe using some "setversion" script prior to git-committing a new version) seems an option. Or did I miss something?
A good compromise between a single GOPATH and per-project GOPATHs is the two-part GOPATH as described in [this awesome best-practices writeup](https://peter.bourgon.org/go-best-practices-2016). TL;DR: &gt; Some Go developers use a two-entry GOPATH, e.g. $HOME/go/external:$HOME/go/internal. The go tool has always known how to deal with this: go get will fetch into the first path, so it can be useful if you need strict separation of third-party vs. internal code.
IMO `net/http.ServeMux` is flawed enough to basically only be useful for simple static routes, and even then it can be annoying (see point 2 below): 1. It does not support path parameters (eg. `/resource/{id}/subresource/{srid}`) 2. Trailing slashes match anything under that root. For example, `/resource/` would match `/resource/foo`, `/resource/foo/bar` and so on. This adds extra unnecessary work.
np. Thanks for the kind words :D
But that's a script file. The beauty of the go tooling is that it needs just the source of your code. Zero scripts and configuration. So I am trying to find a way to keep it like that.
http://gophergala.com/ seems a yearly event, in January. So if you want to organize another one then please do it in Summer. 
Just found this post, I'm the author of go-sctp. I moved the project to [go-sctp github](https://github.com/cyberroadie/go-sctp) It's in a very minimal state (sctp branch). If there is enough interest I'm happy to continue working on it and throw me any questions you want and I do my best to answer :-) 
&gt; unbuffered or buffered channels ? is there any throughput limitations with synchronization of go channles? I see many random deadlocks with my samples :( 
Unbuffered or buffered is a decision that's influenced by your requirements, but as a general rule you want to prefer unbuffered channels. There are no intrinsic throughput limitations. If you're seeing deadlocks it's because you have logic errors in your design. Feel free to link to a small, self-contained example on play.golang.org which demonstrates the problem and I'll see if I can help.
No, it's faster the same way a car with rigid suspensions is faster on good roads/highways but may fail early &amp; hard on roads with potholes.
Given that 1., 2. actually makes sense. That way you can implement parameters inside the handler. Agreed that this would probably quickly turn tedious to work with, though.
You are trying to add in metadata, something by nature outside of the source code. Same source code doesn't care what it's versioned as.
This is a good thing to consider. I haven't professed to using variables in my URL path
Summer event, yes please.
Perhaps not in the Golang barebone spirit, I still think this is one of the better frameworks out there now.
There is tons of documentation on this topic on the net. JFGI ;)
You can just use plain os/exec and pipe commands by setting (\*exec.Cmd).Std\* fields. There are also higher level wrappers like https://labix.org/pipe.
Those who want to get things done and have a task at hand where its toolset is handy.
It is general purpose language(just like python,c ,c++) you can do almost anything. But most people use it to make network apps(high performance APIs, ) It provides the speed of static typed language and expressiveness of dynamic. It is one of the few languages where you won't need third party frameworks/libraries most of the time. It has robust standard library.
I'm in, where do I sign up :)
Go is a general purpose language. It was designed for building web services but it is quite useful for other purposes, too.
Or whom...
It has built-in tools for concurrency that are sensible and easy to use. Some say it occupies a spot between C and Python. You might agree. If you already know Python and C, Go should be pretty easy to learn. Give it a try!
It fails because the type of a isn't an interface type: https://golang.org/ref/spec#Type_assertions
I'm going to do that :) Thanks EDIT : Oh last question, is it a high level langage or a low level ?
I updated the slice library and made it safe for modern use in all versions and variants of Go. 
Yes, it's very hard to build a CSV parser which parses non-CSV files, and even harder to do so efficiently. This library won't probably sacrifice performance to support deviations from RFC-4180, but that depends on the prevalence of the deviation and the magnitude of the performance cost.
Splitting on commas is "faster" in the same way a random number generator could be faster by always returning 4. Definitely faster. Definitely not correct.
http://labix.org/pipe
Nice! It looks like you have made quite a bit of progress already, but if you are doing Go &amp; React you might be interested these two server-side rendering with go/react projects: * [olebedev/go-starter-kit](https://github.com/olebedev/go-starter-kit) * [augustoroman/go-react-v8-ssr](https://github.com/augustoroman/go-react-v8-ssr) --- A few other suggestions: * Use [goimports](https://godoc.org/golang.org/x/tools/cmd/goimports) run every time you save instead of commenting out imports. Easier and faster. * Minor suggestion: Consider using [json.RawMessage](https://golang.org/pkg/encoding/json/#RawMessage) instead of interface{} / mapstructure for the data field in your message struct. This allows you to explicitly decode the json into a destination structure according to the message type (instead of re-converting it a second time via mapstructure). Anyways, looks like you're making great progress, good luck!
I think all languages these days are high level. Go is more higher level than C/C++ due to garbage collection. 
Is it centralized or distributed/p2p?
I would definitely recommend adding some tests. For example, have the update functions accept an io.Reader which is a file under normal operation, but could be a byte slice in tests. This way, you can validate that your code works as expected with a variety of input. This is very cool and I look forward to giving it a try!
https://github.com/golang/go/issues/11462 It's necessary only when you are running in tight for loops with no preemption points, but still want the scheduler to perform normally. EDIT: also see https://golang.org/doc/go1.2#preemption
Hi, I'm currently working on the other end of the spectrum, a Image Booru based on Ember.js and Go, focusing on images instead of posts. Great to see there is more effort going into this, the existing software on both sides of the spectrum is either closed source or helplessly complicated to install and use. -.- probably the dustiest and legacy-est niche of software atm. I wish you good luck.
I'm just going to leave this here: https://github.com/postman0/govnaba
net/http is very well suited for real world production applications. You can handle any type of parameters with a little extra code. It may require a little more effort than other routers, but it's dead simple.
The author had design errors from day one of the project. It's a shame he never sought advice earlier. I haven't done a deep dive to his project, but from a cursory skim, the problems were of his own making, not any intrinsic problem with channels.
and s/Golang/Go/g :)
The alternative is of course, type conversion (AKA casting). Where you can wrap values in a type to convert them to that type, like so: var i int = 42 var f float64 = float64(i) var u uint = uint(f) Source: [A tour of Go: Type conversions](https://tour.golang.org/basics/13)
AFAIK go channels are just an mutex and an pointer (in buffered channel there is an array/slice). So you're just passing pointers to the data, there is not real data sending like eg. trought an tcp socket
I agree that requiring block_index is a bit awkward. The reason I have the block_index field is because of the way the config is unmarshalled. If you look at the BlockConfigs struct in [lib/modules/configure.go](https://github.com/davidscholberg/goblocks/blob/master/lib/modules/configure.go), each block is either a struct or a struct slice. I could, of course, define the block configs in a YAML array so that the index is inferred, but then I'm not sure how to unmarshal that into the distinct block config structs.
I just had a quick look at the main server.go, and it's not bad for a first stab at Go, but there's a few minor nits: 1. Why are you import/discarding "net/http/pprof"? 2. You're discarding errors instead of handling them. Instead of returning *testHarness, you could return (*testHarness, error) and then return the err where applicable instead of ignoring it. 3. Code like this is unnecessary: var tdrr = testHarness{ next: rebalancer, port: 8096, } return &amp;tdrr You can just do: return &amp;testHarness{ next: rebalancer, port: 8096, } 4. You're not consistent with if you range through the balancees and then return a *testHarness, or declare a *testHarness and then range through the balancees, then return the *testHarness in getRoundRobinHarness/getDynamicRoundRobinHarness.
I thought that it was good practice to use pointer receivers only if the method needs to modify the struct, unless the object is too big. I guess my question is: how big is too big in this case?
If you're already sorting the hash keys when Marshalling, then you can use `json.Indent()` on both items you want to diff, and then throw them at [difflib](http://github.com/aryann/difflib). Which is a general text diff package. It returns an array of diffs so you can work with the text as needed. It can also output HTML so you could use that to visualize the diff. I see that the library you wrote sorts the hash keys, so the above approach probably wasn't applicable to your situation. *[Edit: fixed link]*
Looks like very clean code imho. In jsq.go line 120-121, it's quite common to instead use http.Error(w, http.StatusBadGateway, "msg...")
Personally I would have the balancer constructors take in a `[]url.URL` rather than `[]string` since then you can handle any URL parsing errors outside the constructors. Another option would be to have the constructors return errors, but I think it's always nice to have functions that can't fail (and hence do not need error return values).
I use pointer receivers almost exclusively, just to be consistent - though if the struct has a size similar to a couple of pointers, maybe it's not worth it. See https://github.com/golang/go/wiki/CodeReviewComments#receiver-type if you want more guidelines.
You're right, my mistake. Just meant you can't type assert a concrete type. Thanks for the example :)
reimplementing the setIndent method could in fact save the you the trouble of having to rely on go1.7
finally I don't have to sleep alone anymore
&gt; Clear is better than clever. &gt; -- [Go Proverbs](https://go-proverbs.github.io/) `p.SetName("anna")` is clear, abusing variadic paramaters is not (and IMHO it's not very clever). And what about someone that does `p.Name("anna", "WTF?!")`; compiler type checking and function/method signature checking is a good thing. 
Why stop at a plush? I'm I the only one who think it would be killer if the gopher had a raspberry pi or arduino in it's belly and some cool programmable LED's in it's eyes and servo's in the arms and ears?
Sounds like you already know the answer, the trainwreck of `method().method().method()` is not preferred. Can you ask about a concrete case? Setting a cookie looks like `req.Header.Add("Cookie", "blah")`.
Sure. I'm creating a package that I'll use for myself (the code is probably crap, and no test cases are up yet..). [link](https://github.com/natdm/httpFuture) If you look at the second example where I've got two requests, it would be nice to be able to do.. req := New(...args).Exec() right off the bat, if I wanted to. That's really it. But I can't use Exec as a value, so I can't add cookies, exec, update headers, etc, before I assign it. edit: Also, feel free to critique anything you see.
This is left as an exercise to the reader.
You're saying I shouldn't implement libraries to concurrently do tasks when it's easy enough to write them when the caller needs it in the first place? It was going to be used in a few of my "for fun" projects, I guess. But I'd like to keep with the standards, and what you're saying makes sense.
It's behaviorally similar to rsync, but because of limitations in the APIs of the cloud storage services it doesn't do all the clever things rsync does to reduce data transmission.
1. Bluemix. 2. PostgreSQL. 3. [github.com/mattes/migrate/migrate](https://github.com/mattes/migrate/migrate). 4. Yes, it's standard for Bluemix, which is based on Cloud Foundry. I haven't yet got enough of them to need a manager for them.
As /u/theGeekPirate posted in the other thread, there's now an official [model sheet](https://pbs.twimg.com/media/CnSJB8EUIAAIiCK.jpg:large) from the creator. This third party gopher doesn't look like the official one nor does it follow the model sheet. Where is the cute and chubby bean-shaped body? #realGophersHaveCurves There are also many more issues described [in the other thread](https://www.reddit.com/r/golang/comments/51g8tt/go_gopher_toy_by_sean_tasdemir_kickstarter/d7bx3p5). Do not back this up (unless it's fixed) and hunt down an official gopher plushie instead.
That was a long survey, for anyone who have some experiences with dependency managers.
Thanks all! I'll let you know when I have something setup.
Now you got two problems. ^sorry Try adding more () to group things, like ((phone)|(email)). Also tlds can be more than 4 letters, like [.horse](http://www.theverge.com/2013/8/7/4596982/dot-horse-is-here-and-the-internet-will-never-be-the-same) or http://icannwiki.com/.verm%C3%B6gensberatung
&gt; Holy crap, I just realized it looks exactly like http://itstuff.com.ua/shop/gopher/ minus the black around his eyes, which is ~$11.14. Ouch. The guy on the video (Roman Pronskiy) is actually one of the creators of ITStuff and this gopher. As I understood, the idea to reach international gophers is a result of my feedback after the GopherCon, where many people've been asked about Classic Gopher. 
Renee French has this gopher and said it's much closer to the original design, and she's absolutely in love with it ;) https://twitter.com/idanyliuk/status/752927380960784389
Isn't it the same survey that was posted a few days ago here by glide authors ? that survey is way too long. There are like 30 questions, it's ridiculous. If you want a lot of answers shorten the survey.
Phone Numbers come in all sorts of formats internationally, really a job for a small lexer and parser. 
That was a long ass survey, but the questions being asked by the end are the right questions to be asking, IMHO.
This was much too long, especially the parts about other package managers. Why were the answers "Usually, Sometimes, and Never" in the multiple choice at the end? Why is there no always? "I usually want reproducible builds" seems to say "Sometimes I'm okay with unreproducible builds," which is very much not what I mean. I _**always**_ want: * reproducible builds * `build` to not talk to the Internet * to be forced to solve diamond problems I don't think I can express that in this survey and I can't skip filling out essays on other package managers, so hopefully someone reads these comments.
See, they are listening to us ! /s
What, did you expect her to say she hates it to his face or something? I'd be enamoured as well if someone made a plushie with my art. The only difference as far as I can tell is that you've removed the black from around the eyes, as well as enlarged them, which is of course closer to the original design, but only because the originals looked so bad. Still leaves a lot to be desired.
That explains the similarities in designs (see: them being the exact same besides their eyes), but it still doesn't explain the insane price hike. I don't see how it could possibly cost $15 to ship internationally from Russia.
Oh it's for a web crawler, not an input. 
howdy, i'm the author of [goi3bar](https://github.com/denbeigh2000/goi3bar), which does something awful similar to goblocks. nice work on your project :) i found that the best way to avoid boilerplate code in my plugins was to decouple the framework from the interface, and take out all the common functionality i could. i was able to take out most of the communication between the app and its' modules, which seems to be most of your module bloat (`GetUpdateSignal`, `GetUpdateInterval` etc.), by thinking of my app as a pipeline where updates are created by the modules, pushed to the app and aggregated, then a new i3bar update pushed out at intervals. structuring it this way pretty much forced me to remove all explicit signalling from my modules. this worked pretty well for me, but i found out i was instantiating tickers in every module, and copy-pasting a lot of code between modules. to fix this, i went and considered module in two categories - those that need to keep state between updates (e.g., usage counter) and those that don't (e.g., disk space), and provide one interface for each. my main application struct only accepts [Producer](https://github.com/denbeigh2000/goi3bar/blob/master/component.go#L22)s (stateful), so i can just receive updates from channels instead of polling. alternatively, there's also the [BaseProducer](https://github.com/denbeigh2000/goi3bar/blob/master/component.go#L97) struct which wraps a [Generator](https://github.com/denbeigh2000/goi3bar/blob/master/component.go#L15) (stateless), and handles all timing for stateless modules. these allow me to write stateless modules with a single function, and keep stateful modules pretty simple. i have a couple of pointers having gone through this before: 1. use a framework for getting system info, like [gopsutil](https://github.com/shirou/gopsutil), it makes things much easier and less error-prone (you wouldn't have the lingering open file problem from opening /proc/meminfo) 2. decouple your modules from the rest of your application if possible - it makes it much easier to add new functionality, and for new users to contribute modules 3. (imo) ditch the 1.7 dependency for json.SetIndent, and use jq for debugging instead 4. use a dependency management tool like glide so that you don't get bitten when your deps change. it's very easy not to update your installed go packages for a while, and users who `go get` your package will be disheartened if it doesn't compile after a fresh clone. (still todo for goi3bar, but it's happened once)
problem with channels is lets say you have a channel that collects log lines and writes it to a file. If the data writing to the file is slower the amount of message generated on that channel what you will have is a backlog. But now you'll need to keep track of log messages in flight in that channel incase application receives a terminate signal.
Hmm.. I thought lots of Korean programmers use English identifiers. And I learned to use English identifiers in school too. Thank you for your wondering :)
I'm sorry, but I could not make it through that survey. As soon as I realized you were going to ask multiple open-ended questions about every single package manager I've used, I closed the tab.
A slice is basically a pointer to an array - see the section labeled "Slice Internals" here: https://blog.golang.org/go-slices-usage-and-internals So when you pass slice A to a function, it copies that slice as slice B. The internal pointer of slice B is pointing to the same memory that slice A is pointing to (as the pointer is copied), so modifying an element of B is modifying an element of A as well.
[deleted] ^^^^^^^^^^^^^^^^0.2338
I'm sorry. My responses are limited. You must ask the right questions.
Seems like a nice one, even though it's still bare-bones. Though it's not what I'm ultimately aiming for.
&gt;Discontinued.
"Great pattern" because of the position of periods? I think the problem with those patterns is they encourage use of exceptions as error handling. 
I have more opinions on this survey than on package managers... The question "What did/didn't worked well" and "Why did/didn't it work well" could be combined into one question. And be more like "What makes it good/not good" instead. And as others are saying, it's too damn long.
Well, there's nothing wrong with requiring the latest stable. It takes 5 minutes to fetch and build. There's a *LOT* of reasons to use 1.7. New code (context!), better perf and smaller binaries.
id like to help but i know nothing about react. 
Or that you most likely need to use their command line tool to build your app. If you like revel, take a look at [mars](https://github.com/roblillack/mars).
Good call! I'll be changing those tonight hopefully :)
Ooo, I haven't seen that before, will be changing, thanks!
Why do you need to know which interfaces a struct is implementing? If you try passing it somewhere that you think it should be implementing and it's not, it'll be a compile error and you'll know right away. If you need to be sure it's implementing an interface, you can assign it to a variable of that type interface so that it generates a compile error. ie: var _ io.Reader = myStruct{} will die at compile time if myStruct ever stops implementing io.Reader.
If you are looking for this information, because you want to call some function which expects an interface and want to know how to get an appropriate value, [guru](https://godoc.org/golang.org/x/tools/cmd/guru) should be able to provide you with an answer.
Go Guru. "The implements query shows interfaces that are implemented by the selected type" — &lt;https://docs.google.com/document/d/1_Y9xCEMj5S-7rv2ooHpZNH15JgRT5iM742gJkw5LtmQ/edit#&gt;.
You can use go guru. You can watch the talk of Alan Donovan during Gophercon 2016 on youtube. Here the link http://m.youtube.com/watch?v=ak97oH0D6fI
strings.IndexRune returns an int, not a bool. if len(input) &lt; 3 || !strings.ContainsRune(input, '@') { // Validation failed } Best practice is to then send an email to that address with a link that can be used to validate the email address.
I'd like to strongly suggest providing a client in Go (and example of such). There are quite a few socket.io dropins for Go, but almost none of them include using Go as the client. It's not the most common use-case, but still a very valid one.
I'd like a plastic gopher to sit on my desk next to my plastic Android.
That's going to be tough, at least as far as phone numbers go. People are needlessly creative with how they punctuate them. You'll probably need to do something like `\+?[\d\.,\(\)\-]*` -- i.e. an optional +, then any number of characters which are either digits or a punctuation character from `-(),.`.
Just checked and no one mentioned it so far. The biggest problem with chaining is that there is idiomatic way of handling errors in `go`. If an error occurs in one of the methods/functions you need a way to communicate it back to caller or type. If any error in these methods is a panic then it's like an exception and chaining will work. Otherwise you'd have to create an error field in the type and for each method to check if it was set by previous call before proceeding. Very awkward. Better to return an error from each method in idiomatic way. 
In the first test, using the real test package, you used: `t.Error(fmt.Sprintf("Expected the sum of %v to be %d but instead got %d!", numbers, expected, actual))` You can actually [use `Errorf()`](https://godoc.org/testing#B.Errorf) which has the `Sprintf()` format. 
[removed]
Hi. The project looks interesting but how does it relate to go?
On mobile right now but google "go doc pointer analysis"
Almost anything else. JSON, TOML, delimited text, XML even. YAML is human _readable_, but it's incredibly complex and fiddly to _write_ by hand unless you restrict yourself to a small subset -- in which case, you might as well use something else that's smaller to start with, so you don't have to deal with [object deserialization security holes](https://www.sitepoint.com/anatomy-of-an-exploit-an-in-depth-look-at-the-rails-yaml-vulnerability/) and the like.
I think you can also get this from godoc using something like: `godoc -http=6000 -analysis=type` I don't know if that is exactly what you want, but these may be helpful: https://github.com/golang/go/issues/2751 https://golang.org/lib/godoc/analysis/help.html
You should also check out the [gophers slack](https://invite.slack.golangbridge.org/) specifically the #reviews channel. 
Some notes: * Drone.io supports Github and Bitbucket, Mercurial and Git, but its Go version is 1.1 - Fetching the Linux Go 1.7.1 binary tarball takes less than a second (network caching?) and untarring only takes a couple seconds; definitely doable every build * TravisCI supports Github only * Werker supports Github and Bitbucket, but only Git (not Mercurial) * MagnumCI supports Github and Bitbucket, Mercurial and Git, but its latest Go version is 1.3 * Bitbucket has a beta feature called pipelines; about 5 hours after requesting the beta invite it arrived in my inbox - Pipelines is awesome; took me &lt; 1 minute to set up even though Go isn't a supported language yet out of the box. Here's my config file: `$REPO/bitbucket-pipelines.yml`: image: golang #:tag pipelines: default: - step: script: - go version - go test -v 
Well, do you have to use bitbucket? You could just push your repository to github as well... 
Github doesn't support Mercurial.
Drone spins up a new docker image with each build, and I don't especially want to fetch the tool chain every time. This wouldn't be the worst solution, but I was hoping there would be some simple CI system with 1.7 installed by default.
TOML seems the best option to me. JSON sucks due the lack of comments and JS backward compatibility.
http://readme.drone.io/usage/caching/
Oh, cool. I'll play around with this. Thanks for the heads up.
Everyone has been telling you to use go guru, and they're not wrong. That is the correct way to get this information in go. That said, you may be wondering why go doesn't have an implements keyword. Why does go need a whole separate tool like guru to give you information that java gives you right in the source code? Well, consider this example: type MyStruct struct{} func (m MyStruct) Read(p []byte) (n int, err error) { return 0, nil } func (m MyStruct) Write(data []byte) (n int, err error) { return 0, nil } func (m MyStruct) Seek(offset int64, whence int) (int64, error) { return 0, nil } func (m MyStruct) Close() error { return nil } Wanna know what that implements according to go guru? &gt;$ guru implements main.go:#70 C:\Source\Go\src\github.com\psywolf\GuruTest\main.go:9:6: struct type MyStruct C:\Go\src\io\io.go:91:6: implements io.Closer C:\Go\src\io\io.go:117:6: implements io.ReadCloser C:\Go\src\io\io.go:136:6: implements io.ReadSeeker C:\Go\src\io\io.go:129:6: implements io.ReadWriteCloser C:\Go\src\io\io.go:148:6: implements io.ReadWriteSeeker C:\Go\src\io\io.go:111:6: implements io.ReadWriter C:\Go\src\io\io.go:70:6: implements io.Reader C:\Go\src\io\io.go:106:6: implements io.Seeker C:\Go\src\io\io.go:123:6: implements io.WriteCloser C:\Go\src\io\io.go:142:6: implements io.WriteSeeker C:\Go\src\io\io.go:83:6: implements io.Writer Woah! Check out how granular and still useful these interfaces can be when we aren't forced to explicitly implement them, and that's just the ones in the standard library. Anyone could create their own private "SeekCloser" interface, and MyStruct would work with that too! Functions that need to do IO can be very specific about which IO methods they actually need, and MyStruct will just magically work with all of them. If go forced us to explicitly implement our interfaces like java, I would have had to know about all of these in advance (which honestly, I didn't), and it would have made my type declaration stupidly verbose. Something like: //not real go code type MyStruct struct implements io.Closer, io.ReadCloser, io.ReadSeeker, io.ReadWriteCloser, io.ReadWriteSeeker, io.ReadWriter, io.Reader, io.Seeker, io.WriteCloser, io.WriteSeeker, io.Writer {} Ewww. And even then, MyStruct would never be able to work with that hypothetical private "SeekCloser" interface I mentioned earlier. I'm really glad we don't have to do that. In practice, languages like java trend towards larger less granular interfaces so that they they don't have declarations quite this verbose, but that's a trade off we don't have to make in go. P.S. There are actually more important benefits when it comes to decoupling and testability, but they're more subtle so I figured I'd go with this example instead. see https://golang.org/doc/faq#implements_interface for more details
It appears this configuration (the ability to read .drone.yml files from the repository) applies to the open source drone version, but it's not enabled for the hosted service. This feature may make its way into the hosted service, in which case I wouldn't need to cache because the configuration files permit you to specify which docker image you'd like to boot. In other news, it appears to be very fast to download the linux binary (it takes less than a second, compared with a couple of seconds to untar it). So this is a more acceptable solution.
It's interesting you know about guru, but don't know it can answer the question OP is asking and think it can't be answered
I believe go get uses https rather than ssh and hence can only pull from public repos.
https can pull from public repos just fine. I use it all the time. edit: private.
Check this package: https://github.com/PuerkitoBio/goquery
Yes I know it can pull from private repos but there's no HTTPS 'keyring' so go get doesn't have any credentials to fetch with.
When choosing what to learn, I was choosing between React and Angular. I like React's philosophy. I don't know anything about web components or Polymer at this time.
Thank you for changing my view. I've never considerd these benefit before. I just frustrated that it take somewhat blind guess when I have to learn unfamiliar library. 
Like ROFLOLSTER says, go get passes https URLs to git when accessing remote repos. What I do is configure git to rewrite https URLs to github as ssh. In my .gitconfig I have: [url "git@github.com:"] insteadOf = https://github.com/ 
I really like CircleCI and used it at a previous job.
Ah - no idea about mercurial - hopefully you find what you are lookig for
Go version with pools (arenas): https://groups.google.com/d/msg/golang-nuts/sy8IuQQOQSI/MtDor18hCQAJ
I've been using GopherJS for a while and I'm pretty familiar with its history, so I can answer this one. GopherJS started out more like a transpiler, where it'd do something more akin to TypeScript compiler and create JS code that was roughly 1:1 to the Go code. However, the goal of GopherJS was to fully implement the entire Go spec and support the standard Go library. The challenge was in supporting the more advanced features like channels, goroutines, select statement, defer, goto. Also, to be able to compile standard library packages like io, bytes, strings, crypto/rand, reflect, net/http, fmt, etc., and all the basic types including interfaces, utf8 strings, runes, range loops, etc. To make all that possible, it was no longer viable to simply translate Go syntax to equivalent JS syntax, since many of the features of the Go spec I mentioned above do not have a direct equivalent in JS. That's how GopherJS evolved into a more sophisticated compiler. Creating a seemingly short package that imports fmt needs to include all the supporting code for Go types, and fmt package itself imports quite a few others [0]. However, as you add more lines of code to your program, the increase in generated output scales linearly with that. [0] https://godoc.org/fmt?import-graph
What do you see as being the most common use cases for GopherJS today? 
Not only do I know about guru, I actually use it. What it can answer is "Which interfaces in my workspace does X implement?". Which ignores all interfaces out there.
The fn is "func() error" vs "func()" and Do does call the fn until it return nil error. Think sync.OnceSuccessful. Idea is good, but there exist libraries for exactly this purpose, which support additionally retries and backoff. I doubt someone will import this package (add a dependency) for just a few lines of code.
That was just an exploration of Go's ability to handle offheap objects, I never submitted it to shootout.
CircleCI certainly looks user friendly, but it doesn't support Mercurial. I'll keep it in mind for future git projects. :)
It makes, moreover Do should return an error.
Personally, I'd use the .go format.
It seems like that would have been a more useful answer than "You cannot", because it's unlikely OP meant "Which interfaces will my struct ever implement from now until forever" and more likely they meant "How can I know which interfaces my struct implements right now in the context of my work"
whats the best mocking library in golang? i was creating my own mock classes recently but i presume there is a decent mocks library out there? 
Care if I do?
At this point I'm not able to point to any mocking library and say "this is definitely the best right now." One reason for this is that in most cases where a mock would be a good fit, you should be using an interface, and due to the fact that interfaces can be implemented by any type, it is often just as easy to just write an implementation of the interface for your tests that captures whatever parameters you need from method calls and is more expressive/customizable in the long run. That said, some mocking libraries do exist, with https://github.com/golang/mock being the one I see most often, so if you really want to use a mocking library that is where I would start.
Well, yes, anything which _can_ be derived from the source code alone, should be. However, there's no standard way in Go source at present to indicate semver or commit of an import, so there's a need for some way to provide that information -- though I'd rather that whole issue was optional, with the default being "latest version".
The best workaround IMO is creating the directories and cloning it yourself. Then you will be able to go get -u all you want. Better than fiddling with Git global settings.
Uh... the author replied to him before you stating it was okay, what was the need to mention this?
&gt; Can anyone recommend me books/articles about writing horizontally scalable servers in GO? Thanks. no need for books. if your server is immutable it will scale horizontally. It you persist states and data it will not. It has nothing to do with Go specifically.
&gt; In my personal experience, Go does not like allocating a lot of memory. Do you mean "Go doesn't like allocating large blocks of memory"? Or "Go doesn't like many allocations"? I understand the latter is true, but I've not heard anything negative about the former.
From the application instructions: &gt; Please don't implement your own custom "arena" or "memory pool" or "free list" - they will not be accepted. http://benchmarksgame.alioth.debian.org/u64q/binarytrees-description.html#binarytrees
allocating about 1.2GB of memory in go takes quite a long time. This was actually after optimizing from making many small allocations, which was many times worse. In general; both are a bit bad on go, allocations take a long time compared to how fast other languages can do it.
Very nice series and a pleasure to read. I am picking up useful bits and pieces in each part.
Yes, I think if it were part of the standard library, and I agree that pool allocators in any language defeat the usefulness of the benchmark.
I'm not exactly sure why it takes so long, it's probably some underlying mechanic. It would be nice to have some compiler flag to make allocation faster at some other cost.
This is quite simply amazing.
Hmm, I got 75ms on 1.6. I wonder if there wasn't some other problem with your benchmark?
1. If you are only concerned about horizontally scaling the Go app then you would have identical servers running the Go app. Then you have a server that runs the database. In the Go app configs you provide the ip of the database server as the database host. 2. You would need a Load Balancer, which is a server that distributes your client traffic across the available Go servers. The Go server server will either grab the data from the database and return it to the client or post data from the client to the database. Your questions are more related to system admin and devops than Go, so I would recommend looking for topics and forums related to that. You can also play around with setting up servers for free with AWS https://aws.amazon.com/free and read their tutorials https://docs.aws.amazon.com/gettingstarted/latest/awsgsg-intro/gsg-aws-tutorials.html . Just be aware that they got plenty of custom services that can add confusion. If you plan on using another hosting company stick with building an architecture with just EC2, which in simple terms is a server you manage. 
It's the first reason that it will not be accepted.
In `Subscribe`, you have a map of event name to a channel, which you loop over, and inside the loop you re-check that the `value.Name == event`, which you know it already is because the message came over the channel for `event`... isn't that redundant or am I misunderstanding the code?
The "verbs" are just symbols that you can use as placeholders in the format string, so that your additional arguments can be "formatted" into their place. Again if you are familiar enough with Python, there is a similar thing with being able to do this in Python: "Hello %s" % "World!" or "Hello {0}".format("World!") But in Go they are telling you that their approach to the formatting verbs is similar to C. And yes, those verbs are a convenience for doing different types of formatting to the objects you provide.
&gt; The benchmark is terribly flawed. And yet you found an *"apples-to-apples comparison"* ;-) &gt; Java is allowed JIT warm-up [Not true](http://benchmarksgame.alioth.debian.org/how-programs-are-measured.html#time), never has been.
http://benchmarksgame.alioth.debian.org/how-programs-are-measured.html#time
&gt;Go does not let me use a package imported within the same package if the import occurred in another file. What? This is definitely not the case. EDIT: Okay people, there was some room for interpretation, please read below before downvoting.
Are you sure? Maybe I'm not explaining clearly. . ├── b.go └── main.go in b.go: package main import "fmt" var _ = fmt.Println in main.go: package main func main() { fmt.Println("foo") } go build: ./main.go:4: undefined: fmt in fmt.Println
Yeah, exactly this. Go encourages you to build medium sized files in your packages. Most related functionality should be within a single *file*. I can't see the use for the behavior that the OP is describing, outside of putting a ton of very small files in a package.
If anything I would say it's unexpected that declarations are shared at a package level, but not that imports aren't. Which seems like maybe something you agree with, just would expect declarations to stay in step with imports.
Yep, exactly. I'd be happy with declarations being kept to file scope (thrilled actually, I hate that it doesn't work this way). It's just odd that file scope is mostly only for imports. Especially because dependencies are usually discussed in terms of a project or packages dependencies, not in terms of the dependencies of the individual files in that project. The crux of what I'm trying to get at is: file scope exists. It could be really useful for keeping variables from leaking between files in a package (I struggle with this problem constantly). Instead it's used to force files to explicitly declare their imports*, which doesn't seem like meaningful behavior to me because who cares about the dependencies of a file when we should be more concerned about the dependencies of the package? \* And while it succeeds at this, it fails to achieve the more meaningful goal of keeping dependencies (in the DI sense) explicit. Variables are still declared anywhere in package scope, so while the file's imports are explicit, its dependencies are not.
&gt; And yet you found an "apples-to-apples comparison" ;-) A flawed design doesn't invalidate data points, it invalidates extrapolated conclusions that require the design's assumption. &gt; Use default GC, use per node allocation or use a library memory pool. So, in order to draw valid conclusions about "per node allocation", we can only draw conclusions from the data points that adhere to this requirement. A proper test would segregate the per-node and memory pool data, because the meaning of the data is completely different. As the code /u/VifArdente posted shows, a memory pool completely changes the benchmark; an order of magnitude change in times for the same language and compiler should be a huge flag that the data has caveats. &gt; Not true, never has been. Cool, that's changed. They used to run all the benchmarks in one session, too. Granted, I first dug into these benchmarks a decade ago, so I should have expected some changes. I usually compare source code before comparing numbers, anyway.
I understand now what you mean. The phrase I quoted has another interpretation: If b.go imports fmt, you wouldn't be allowed to import fmt in main.go. In any case, per-file imports instead of per-package means clarity, which is one of the major design goals of the language. What we edit are the files, not the packages. With the import line in every file it becomes clear that you're dealing with an external package and not a package-level variable.
*That* answer makes sense to me. For some reason I thought you could assign an imported package to a new package-level variable, which would have defeated this argument somewhat.
awesome read
No, I was aware of aliasing. I thought you could also assign package 'references' to package-level variables (thus allowing you to share packages between files without importing them). I'd just never tried it because it doesn't sound like a good idea.
&gt; Where are those extrapolated conclusions on the website? The data is presented as fulfilling the same requirements, ranked and compared directly. The only way to identify what the time actually means is to look at the code. So, the extrapolated conclusions are the presentation that does nothing to show important details. How are people like OP expected to come to any other conclusion than " Go clocking in at 39 seconds, when Java do the same in 11 seconds, Rust in 4 seconds, and C in 3 seconds."? &gt; No! That is not true. Java was not allowed JIT warmup. I'm not trying to argue history with you, it's been a damn decade. It doesn't change how we read the results from this specific benchmark. JVM is collapsing those small allocations behind the scenes. That's not a bad thing! That's a solid piece of engineering! But, in order to make valid comparisons we shouldn't be comparing code that's allowed to combine allocations with code that's not. My memory might suck about the exact setup of these benchmarks, but the lesson learned is still valid.
I've used it quite a few in for chrome apps; as far as I know it's the only way to get a fully functioning http server running IN the browser without writing your own. (the software for my CNC requires a websocket server on localhost, and I wanted to use my chromebook). I also used the standard libraries for a Go plugin in the atom editor (for parsing and whatnot), using gopherjs to build it into .js. It's definitely an edge case I'd say (the http server); otherwise someone would have created one in (pure) javascript by now. Go lends itself well to portability that way. For a javascript-only environment gopherjs makes for a very convenient tool to leverage a lot of ready-made libs. In my case, nothing from the node.js core libraries was usable (they use native extensions); same for socket.io. but the Go net/http package didn't require extra work once you implement the required interfaces for the listener and connection, essentially making the entire project feasible.
Hadn't noticed UnquoteChar and needed exactly that.
Consider how the current design allows different dependencies when your package has build tags to work with older versions of Go, or to support different OS/Platform combinations. For example, you may need to import golang.org/x/sys/windows in a code_windows.go file, but doing so in a code_linux.go file would make no sense.
Some things I see: You're using a RWMutex but not using the reader side of things. Line 81 this whole function should take the Lock() reading from the map not under the lock isn't safe. Imagine 2 gorotines publishing at the same time. They will both pass the test, the 2nd go routine will stomp then channel in the 1st go routine. Line 63 you're accessing the map again not under the lock. If you got go routine #1s ptr (in the example above) it would never produce anything. You should take the RLock before the go func(), and cache the channel in a variable, then release the RUnlock and range over the variable in the go proc. Line 45 you should use the RLock here for publish. createlubscriberIfEmpty should probably RLock check if exist, RUnlock, if you have a channel return it, then everything calling this method just has the channel and doesn't have to lock again.. If you don't have a channel take the Lock and then check again , if its not there add it, then Unlock and return that. If it is there Unlock and return that. Then you need to think of a way to cancel a subscriber.
I'm all seriousness, what's the benefit to another function like that?
&gt; And yeah, I'm aware of and use goimports. It is a good solution to the problem, but its existence shows that there is maybe an actual problem to discuss. Hm? No, it's a *solution* to an actual problem (namely, that unused imports being an error is annoying, but we don't want to allow them to improve compile times and basic code hygiene). Saying goimports' existence shows there is a problem, is like saying git's existence shows that there is something fundamentally wrong with version control.
Pretty sure you never stop saying this for your entire career do you? I only am 12ish years in but I've been calling "one year ago me" an idiot ever since. Come back to this library in four years and let me know :-)
Well Atoi() only exists as a convenient wrapper for ParseInt(). I suppose Atoi() was added because it was such a common use-case, needing to convert strings to base 10 ints. I imagine ints are used much more widely than int64s in the standard library. I'm also fairly sure base 10 integers are by far the most common. It would be more consistent to have Atoi(), Atoi8(), Atoi16(), Atoi32() and Atoi64(). It would make code easier to read and would make the package docs also slightly clearer, in my opinion (even if they are rarely used). Readability is probably the biggest benefit.
**Just being curious** Let's assume my shell is zsh. Which one would be *loaded* faster and which one would *run* faster: a bash script (*#!/bin/bash*) or a bash script written in Go (the same way this one is)?
How about "when the allocation shows up as the biggest hotspot in the alloc'ed heap profile and the use case makes using a Pool possible". Usually if you're exposing the allocated object to a user of your package, a Pool is too riskly, because you never know what the caller did with the reference to your object. So Pool works best for private objects. You can also search the standard library and see how sync.Pool is used (IIRC fmt uses one)
Fetching public github repos via ssh just works. There is no problem. You can't, of course, push commits to them, but that's no different from https.
Being a heavy user of twitch and livestreamer, I was looking for a stream display that isn't written in .NET or Java, doesn't require a web browser and displays useful information - so I was forced to make my own. Suggestions for improvement are welcome.
My best guess would be that your server just coincidentally died of natural causes. Recommend performing an autopsy.
Go executables on Linux require a decent new glibc and that's it (well kinda, depending on DNS lookup in net). If it is there it will run, if not, not. Probably your machine just died.
thumbs up for that!
 curl https://www.teleconsole.com/get.sh | sh Yeah, exactly, that's what I'm gonna do.
Your machine coincidentally died.
I'd be wary of _premature optimization_ here.
Go sucked the life out of your machine. Joke aside, I run linux and windows statically compiled executables on all sort of OS's (win xp, 7, 8, 10... centos, ubunty, mint), and only problem I found that sometimes when running on newest linux kernels strange panics happen with some libraries (mysql connector etc.)
I don't get it, that's pretty much exactly what I said and got downvoted. Bizarre.
But there is "Readme.md" file. I need to rename it to "README"?
Thank you! I Mark it for itself
&gt; JIT's warmup time is included in the benchmark Whatever time is needed for "JVM start-up, JIT, OSR…" is included in the reported time measurement. Which is to say: no warmup time is allowed *before* the measurements are made. Which is to say: the measurements are "cold start" "Single Shot" measurements. That will make more difference for binary-trees than for some of [the other tasks](http://benchmarksgame.alioth.debian.org/sometimes-people-just-make-up-stuff.html#jvm-startup-time).
Thank you very much! I will fix it.
Ah, I understand your meaning. On a related note, I'm bummed that those benchmarks don't have warm/cold comparisons for JIT and interpreted languages. :(
Thank you very much for review! Your feedback is very useful for me!
What difference would that make for interpreted languages? Here's [a cold/warm comparison](http://benchmarksgame.alioth.debian.org/sometimes-people-just-make-up-stuff.html#jvm-startup-time) for JVM JIT.
Try it?
[gofmt your code](https://blog.golang.org/go-fmt-your-code) (or [goimports](https://godoc.org/golang.org/x/tools/cmd/goimports)). Read through general [Go code review comments](https://github.com/golang/go/wiki/CodeReviewComments); from a quick glance you violate many of these.
pretty awesome I've been using [tmate](https://tmate.io) &amp; [ngrok](https://ngrok.com/) for this I kinda dislike the install instructions but great either way
&gt; …all run for over a second? Depends what you run them with :-) iirc CPython is an interpreter; [the shortest time at the largest workload is ~2 secs](http://benchmarksgame.alioth.debian.org/u64q/measurements.php?lang=python3) and the longest time is ~45 minutes.
no way, `apache` doesn't convey what the package does.
If stripping goes wrong the binary usually doesn't even start. That was at least my experience.
Ekranos, thank you very much again! I have created request with changes, following your review. https://github.com/iamsalnikov/socache/pull/4/files
&gt; Where is it ranked? The x numbers simply show the active measurement as a ratio, they are not a ranking. The display table is sorted by performance. Just because the counting is left to the reader doesn't remove the ranking inherent to such a display. Don't act like a list ordered by time isn't a ranked list. We know that all but the most skeptical readers are going to read it that way. &gt;&gt; The only way to identify what the time actually means is to look at the code. &gt; Always (and to be fair to the OP, that's what they did). The OP posted a table of measurements, but what is the context for those measurements? &gt; On the contrary, the presentation shows every detail. What it doesn't do is tell you to draw general conclusions: rather the contrary. If the reader is required to read the source code to understand the meaning of every measurement, why are the results separated by benchmark? The reader is entrusting you to give enough context to read the results. Simply blocking by actual group by displaying "binary trees benchmark, memory pool allowed" and "binary trees benchmark, per node allocation" tables would allow the reader to simply parse the measurements. Instead, the data is displayed as "binary trees benchmark" without the necessary caveat "You must carefully read the benchmark description, notice the 5 words that completely change the benchmark design, have enough understanding to know how the benchmark results are changed by those 5 words, and read all source code before drawing any conclusions from the following data." Like scientific and statistical experiment data, the design of the presentation/report is as important as the design of the experimental procedure (just ask Mendel). You aren't telling the reader incorrect information, but you're giving just enough information to draw bad conclusions. You're expecting every reader to have the statistical understanding and programming chops to rehash the entire table of results before drawing any conclusions. That's just not a reasonable expectation for general internet traffic. 
Correlation != causation. If there is a way for a Go binary to wreck your machine so bad it won't even POST, I'd be seriously impressed.
&gt; left to the reader Thank you for acknowledging that no program rank is shown. The programs are presented *ordered by* elapsed time, *ordered by* memory use, *ordered by* source size, *ordered by* cpu time. &gt; why are the results separated by benchmark? The results are separated by task. &gt; statistical understanding What *statistical understanding* for [the table](http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=binarytrees) referenced by the OP? &gt; before drawing any conclusions Again, *what is the context* for those measurements? &gt; for general internet traffic Lest we forget, this discussion is taking place on `/r/golang/` 
I'm no friend of automatic formatting tools and "style guidelines" are just that: guidelines. If a projects demands a certain style, then that's fine with me; I prefer 8 space indentation, an 80 character hard limit and somewhat consistent structure. However, I did notice my `.vimrc` was missing, as I forgot to copy it to my home directory after a recent distro swap, which resulted in a few formatting problems, which are fixed now (among indentation).
What actually would rock is a service, that makes a fingerprint of the API of the HEAD of the default branch of the repo and ensures that it does not break for the current major version. Similar to Go API checker (https://golang.org/src/cmd/api), but as a service and for arbitrary Go projects. Any badges generated this way would better reflect reality.
* bounds check Xlat since it's public * I would not panic in this lib, since it's Auth based you want to avoid panics as it could become an unintended point of DoS * With above said, Encrypt would return an error as well * You want to implement private encryption helper funcs that do any index access and return errors for safety * Hashpair looks correct where it sits now, but I would still guard it's access with bounds check. * cypherBytes access assumes no error means we have an index to access, but hex decoder can return zero length string * Same type of feedback as above for decode too
I'm not a big fan of this style of installation either, but it's trivial to go to the URL and look at the script yourself. Granted, I'd prefer the standard `go get` method of package installation, considering `$GOPATH/bin` should be in the user's `$PATH`. There's no good reason to distribute a tarball.
Now only if we could do this with a simple bash script and a decent package manager. Guess we'll have to wait for the package manager first.
gta is based on the gps engine, which will soon be powering glide (with any luck). that'll get you to at least halfway decent. hopefully the committee will get us the rest of the way there.
actually, let me be clearer with this: gps is written as a library for package managers. it can power a full-blown one, like glide (or others), or it can support one-off specific tools, like gta. so, the fact that gta CAN exist means we're a lot closer to decent package management than we have been in the past. having a library like gps also means you can write something that does what gta does in &lt;300 loc (and half of that is setup/flag parsing/etc.), rather than having to complicate the package manager itself. this might seem easy to do, but there's actually a lot going on in there to get it even in the vicinity of correct; doing it in bash would not be a good idea.
Nice too and exampes!
You may be using the method through reflection. You may be creating a global variable to be used by another package. Local unused variable never really make sense.
I'm not super pleased with "one week ago" me, but I must march forward! I'll never forget the time I interviewed a company where I'd worked 5 years earlier (never should have left) and they asked me what a specific chunk of Python code did. Turns out, it was my code and it very Java-esque Python code because I had just started using Python. I remember my first though when I looked at it - "why kind of idiot write Python that looks like Java???"
You might want to show how a closure is implemented. Having a picture in your head of what the compiler is emitting is always better than abstract definitions when things don't work the way you'd expect them to.
&gt; You may be using the method through reflection You cannot use reflection to call unexported package-level functions. &gt; You may be creating a global variable to be used by another package You cannot import package main.
That is in fact an argument against the current behavior. It is confusing to the casual reader which will be you in a few weeks. Package aliases are there to handle collisions.
1. don't have to multiply inside, i is a byte offset: for i := 0; i &lt; (len(hash)-2)/2; i++ I'd use for i := 2; i &lt; len(hash); i+=2 2. Use plainBuffer.WriteByte, not WriteRune - you're working with bytes everywhere! 3. The seed write-out is "%02d", which may be 5 digit for an int16! Use seed%100 if you need digits in the output. 
The script is 20 lines long. It takes 10 seconds to inspect what it does.
I believe there was an elegant answer by Ian Lance Taylor somewhere, but I can't find it now... :(
&gt; I'm no friend of automatic formatting tools and "style guidelines" are just that: guidelines I feel the same (not in the case of Go though). And yet at some point in life I realized that if you want something from others, then you have to accommodate them ;-) 
cheers at present im just implementing own mocks. seems like a fine approach to me. just exploring other possibilities at present. 
Oh, I did not. I actually have been evaluating vault and console in an enterprise setting. I was just curious why you choose such a complex example to showcase the http aspect.
Also, the reason that AppEngine provides such a strict runtime environment is not *only* security, but also that it makes it next to impossible to write apps that don't scale horizontally. It forces you to put all your state explicitly into a backend-layer.
I've never seen any lag on my first gen Pi, it's very snappy. How many users do you have? How fast SD card do you have?
In the same package name for different things. When I glare over a file of a project I don't read everything top to bottom. When I saw a certain package 'fs' imported somewhere and then switch the file first intuition is to expect the fs package to be the same. 
Doesn't the AppEngine SDK just transparently come with Datastore? I thought I used that at some point. I'd assume the emulator is only useful if you want to develop for a VM or something. Especially as, AFAIK, you can't use gRPC on AppEngine (might be wrong about this). You should just go through the [tutorials and stuff](https://cloud.google.com/appengine/docs/go/quickstart) for go on AppEngine, it should walk you through everything you need.
Really enjoyed the article. Looking forward to the next one. 
I think something like the equivalent C could be useful.
Yes. C is generally more familiar than assembly but still very analogous.
I see. It's the same package name for very similar things. Both provide filesystem-based implementations. It just wouldn't make sense to put all the services into a single `fs` package, so they're split up with one `fs` package for each server. But logically, you can think of `fs` as a single package providing multiple service definitions. So I think it's okay if you thought they're the same package upon cursory look - the point is that the details can stay out of sight until you need them. Only if you need to investigate the fs implementation details would you descend into that package, and then you'd see exactly where each one comes from. Put simply, this setup may cause you to miss some unimportant information at first, but if it becomes important to you, it's easy to see.
&gt; readers would need to perform the near-impossible task of counting If they wish to make a ranking they are welcome to do so, and to take responsibility for doing so. &gt; Okay, word games are fun. You seem to be having fun with your word games. &gt; You answered your first question by asking the second. Evasions. &gt; Correct, we're on a programming language sub, not a data science or statistics sub. Your original comment was about "general internet traffic". Apparently you wish to quarrel with someone about something. Oh well.
I'm going to go with the universe lighting some candles, trying to make the mood sensual before making sweet love to the transistors of your ancient hardware. Sparks were in the air, you could cut the sexual tension with a knife. But the universes gentle kiss was too much. Alas, your hardware is in deaths hands now - but at least it died happy, knowing, feeling.
Never in my life has it been suggested that I should replace caps, and I'm not sure if it's $10 per cap or total. Had I known, my trusty Cyrix 200Mhz might still be heating my apartment in the clutches of winter.
I cant read minds, so I don't think I can help you without more context.
Both fixed! Thank you again for your help.
I mean what pseudo-code showing what the compiler emits to make a closure. Once you know that you can understand the allocation and lifetime implications of placing a "local" variable inside a closure. And how a single function pointer can expand into the address of the opcodes and a pointer to the closure's environment.
C would be good, though I don't think the call to a closure through a single function pointer can be done in pure C.
This code is not very idiomatic. One example is the order of your imports (and that you have comments): https://github.com/fitzy101/api-template/blob/master/main.go a few other points: * The SetDB api is very odd and hides a global variable. * Your function naming doesn't follow conventions (and is inconsistent). I don't mean to be harsh but I don't think this code should serve as a starting point for new Go developers. 
Wow, this is genuinely a hard one. On one hand, the value of inciting HTTPS by default is huge. On another, we're talking about technology that isn't 100% set in stone and that really isn't *necessary* to have in the standard library. But then again we do have some things in the std library that feel like they would be better off of it, like base32 or utf16, and they certainly get less use than ACME would. I think I'm leaning on yes, but it's going to be interesting to follow how this one goes.
Slices are implemented using arrays which means they have similar performance characteristics. This really isn't a go specific question since nearly every langue has both array lists and linked lists. Here's a good summary of when you may want to use one over another. http://stackoverflow.com/questions/393556/when-to-use-a-linked-list-over-an-array-array-list
Brad, I just wanted to thank you for taking GOARCH=js into account and making it not fail there, along with all other "safe" environments (i.e., where importing unsafe isn't possible). +1
New returns pointers. Make doesn't. See also https://golang.org/pkg/go/constant/#MakeBool , https://golang.org/pkg/reflect/#MakeSlice , etc...
oh, cannot believe I didn't notice that. Thanks!
I'm not too keen on your usage of "at runtime" because it's inaccurate. Everything in Go happens at runtime an is known and processed at compile time. The only thing where "at runtime" would be acceptable when discussing Go would be with type checks. As with those the compiler either knows the type at compile time, or won't until the program is executed. And each of those scenarios are drastically different. I get why you use it, but if you're writing articles targeted to teach it's best to make sure even the jargon is accurate. 
Looks like it! I would note that Go has been compiling to MIPS for a while now with gccgo too. 
Perhaps the point is not that a doubly linked list is the right answer to many problems, it's that it's a foundation to building many other data structures that is easily explained and understood and implemented. A Trie or Red Black tree might be more practical, but they take a lot more work, even if they're based on similar core skills and understanding from the point of view of the language.
Here is a simple way that I personally pipe commands without using any external packages. This won't run using the Go playground though since it is running actual commands: https://play.golang.org/p/pYJ_DY9_ri.
Unfortunately I don't think I have the knowledge to do this justice. :(
No, you're not missing anything. Traditional complexity analysis of algorithms using Big-O notation is based on the outdated assumption that all memory accesses have the same cost, but this is simply false for a modern computer. Locality is crucial. So, even though the number of instructions to insert an item into the middle of a list is O(n) for a slice and O(1) for a linked list, the slice runs faster because memcpy is extremely fast, and because the more important operation for a list is iterating over the elements, which is basically free for a slice but requires sequential pointer chasing, i.e. cache misses, for a linked list. In short, slices and hash tables (maps) are all you need in almost all situations.
By the time the caps usually puff up the hardware was usually so outdated that an upgrade was a better option. That 200MHz cryix today is laughable compared even to a raspberry pi.
For anybody that's interested, gobeat's process functionality has now being mostly moved into it's own api for anybody that wants to use it. Here's a link to gobeat's new process package: https://github.com/radovskyb/process Edit: package is now fully functional and in it's own repository.
Unless... Not related to Go but C, a year ago I had some work on high performance element iteration, and linked list outperformed array accessed by incrementing index (compiled on linux, gcc). I was surprised. Seems if every ns counts, just benchmark it.
By default use slices, period. If you have performance problems, it is possible, depending on the concrete program and program input, a linked list to perform better. It's just another, rarely used, tool. 
If you want to use App Engine Datastore in Go, use luci/gae datastore package: https://github.com/luci/gae/tree/master/service/datastore This is like python's ndb, and even better. I think they are going to make the documentation better, meanwhile, I found this issue where people ask questions: https://github.com/luci/gae/issues/56
I really like this. Because I keep forgetting the most simple commands. I suggest you put something on borg.crufter.com:80 describing the project (readme of github) plus a list of all current definitions. To get some idea of the volume and things missing. And yes, then next also a safe way of adding new definitions... 
Hey, cheers, I had the exact same issue, originally I just wanted a portable way to save and query snippets then I thought why not aim for something more complete, instead of having my own 50 snippets we could share thousand times that... given the system is well thought out (with good interface for search etc). In terms of breadth, at the moment the database contains the bash snippets posted as an answer to 8k stack overflow questions. Adding/editing/rating is something that will be very useful once I figure out, if this gets some traction I will definitely deliver, so keep the upvotes/stars coming, it's free :P
Interesting. . . just came across your comment here, I'm working on a Go project, my first one and I've been using Swagger for a bit, primarily with Python. While I love the idea of Swagger and I love what it *can* do, I keep feeling like it's more effort than it's worth, at least on the codegen side. The UI documentation and sandbox however, are fantastic.
As a LE user myself, I don't understand why we need to add support to every software. Why not configure it into my default webserver? And even when implementing it in go, why add it to the standard library? It would be enough to add a third party library imo. 
&gt; As others have mentioned it's not stable enough yet. Wouldn't go in until Go 1.8 which is not until next year, which is also when ACME should be finalized. &gt; As someone who has written an ACME client, it's hard to find the right interface. This is SOOO true.
&gt; don't have to multiply inside, i is a byte offset: for i := 0; i &lt; (len(hash)-2)/2; i++ I'd use for i := 2; i &lt; len(hash); i+=2 True, but I do need access to a 0-indexed counter in order to access the correct key, see the `keyByte := Xlat(seed + i)` assignment. &gt; Use plainBuffer.WriteByte, not WriteRune - you're working with bytes everywhere! Good point, thank you! &gt; The seed write-out is "%02d", which may be 5 digit for an int16! Use seed%100 if you need digits in the output. In the context of my application, seeds are understood to be 0 to 15, so they'll always fit in `%02d`, no need to mod 100. This algo is laughable for many reasons, but an interesting shortcut they could have taken in the hash format is recognizing this constraint on seeds (might as well encode the seed with a single hexadecimal digit rather than waste two characters for a silly decimal representation). Especially since the rest of the hash is already hexadecimal.
&gt; bounds check Xlat since it's public The mod in `return xlat[index%len(xlat)]` accomplishes most of that. &gt; I would not panic in this lib, since it's Auth based you want to avoid panics as it could become an unintended point of DoS &gt; With above said, Encrypt would return an error as well Updated `Encrypt()` to return `(string, err)` for a cleaner API. The lib no longer panics. The command line script will keep panicking on errors for now, as it seems to be a reasonable and direct route to displaying useful error traces, setting the program exit status, etc. &gt; You want to implement private encryption helper funcs that do any index access and return errors for safety `Xlat()` does this with a mod check. &gt; Hashpair looks correct where it sits now, but I would still guard it's access with bounds check. The `i &lt; (len(hash)-2)/2;` check performs the bounds check, in particular, allowing the decryption to gracefully output as much as it can in the event that the final characters are an odd number (successfully restoring as much plaintext as it can from a partial, corrupt hash!) &gt; cypherBytes access assumes no error means we have an index to access, but hex decoder can return zero length string. Yes, the code checks for this via the error multi-return value.
I think describing it as "dynamic" is sufficient. A named function is that named function for the lifetime of the program but a function stored in a non-const value is just like any other. It can change and be modified during the course of the program making it a "static" vs "dynamic" situation. 
Good work man on the changes you made. If this is meant to serve as an academic example than you should diligently implement bounds checks and not expect prior context to ensure safety. Your modulus constraint only ensures safety for positive numbers, exposing a panic to a public interface. Again for the DecodeString it can return an empty slice, without an error. I.e. DecodeString(""). Right now it's control flow prevents error, but control flow serves as a weak proof of correctness in a security context. Just food for thought.
Cool project, didn't look very closely but did notice you should replace your map access directly to form with FormValue to prevent map access panics.
I updated the article to reflect your suggestions, because I agree that getting the jargon right is important. What I struggle with here is that there are subtle differences that are difficult to explain between a regular function and an anonymous function. For example, the `newCounter()` example isn't returning a static function; Each call to the function `newCounter()` returns a closure that references a different `n` variable. I hope people new to closures are getting that point, but at this point it is hard to verify.
Updated `Xlat()` to require `uint`, to guard against negative indices. I like your approach to robustness, thanks again for the suggestions!
The guy committed .swp files to his repository ...
Could be good to integrate with peco (if possible) if you ever do interactive search.
Aren't there other established solutions here? gobindata, go.rice, another simpler one I'm not recalling off the top of my head. why write this?
For reasons, I prefer not to keep all my projects in $GOPATH. Could I soft link or something?
There are a variety of applications for a doubly linked list. For example if you are building an LRU cache the standard implementation is to use a data structure constructed on a Hashtable and a Doubly Linked List. https://www.quora.com/What-is-the-best-way-to-Implement-a-LRU-Cache 
You do know this line: https://github.com/zowhoey/gologin-gpg/blob/97bf4d3437436653f8108859e9f82f57a6ccd787/login.go#L17 just overwrites the previous `p := ...` line, right?
It needs a new name. There's already a backup utility named Borg. 
I'm surprised too. Did you find out why it was like this? Generally, data structures where elements are consecutive in memory are considered to be better for cache locality.
**If** ACME support was to be included, it should be as generic as possible and not include LetsEncrypt by default. If LE was included in the standard library as default, that would probably be a big step backwards, however, the documentation may suggest using LE by default, the language itself and it's standard libraries should try to remain neutral.
Also reminded me of Google's own Borg: http://research.google.com/pubs/pub43438.html
Hey I tried! 1) Generate http.Dir vars and Handlers function in ./internal/asset 2.1) Call vfsgendev -&gt; failed cause cant read internal 2.2) Wrote to non-internal -&gt; worked 2.3) Found out tool cant choose package name and write to specific directory. 2.4) Switching to directory and calling from there works, albeit still cant change package name this way. Hitting https://github.com/shurcooL/vfsgen/issues/23. 3) vfsgen.Generate() requires to write a temporary main package cause http.Dir directory is non-main to be usable and I tried doing that but was not so successful, so continued with 2.4
Why some receivers work with a pointer and some without a pointer?
I didn't really have anything else handy in mind here :)
Exactly, the example loads the entire file into memory before passing to a buffered reader.
So, is there any reason why I'd want to switch to this over neovim?
I got scared for a second 10 years but no release notes? Just have to look [elsewhere](https://groups.google.com/forum/#!topic/vim_dev/CmiGxtJ7fn4). 
If you are asking as a general question you should check out https://golang.org/doc/faq#methods_on_values_or_pointers for good some reasons. Otherwise, in this case, they could all be using pointer receivers and for method set consistency I might change it. However like the Go docs says, `For types such as basic types, slices, and small structs, a value receiver is very cheap`.
Well I certainly don't need both and I will probably change it to use only pointer receivers since my struct is bigger now than it originally started off being and this had actually slipped my mind so thanks for even mentioning it. Whilst my struct was smaller, I just used the general rule of using pointer receivers on methods that needed to modify state while using non-pointer receivers on non-mutating methods, since value receivers are cheaper to use AFAIK.
Glad I could help. Generally, I would put the consistency rule above the mutability one, that's why I mentioned it.
Changed it. Was a good catch honestly. Thanks!
Everything else aside, I already know where I will put it in use. Seems interesting.
That's great! I actually just created `process` within the last few days, since after creating https://github.com/radovskyb/gobeat, I realized that all of the process functionality should be extracted to it's own package since I thought it might come in handy in other places besides `gobeat`.
Sure, you can run SQL queries here: https://data.stackexchange.com/stackoverflow/query/new. It's awesome! I will commit the SQL queries in the repo as well (although they are very simple).
[Polymer](https://www.polymer-project.org).
I have experience with Angular, Backbone and jQuery UI/Mobile. I'd avoid the later two if possible. If you like Go, you may not be particularly thrilled about the fast and loose nature of JS, so you may be more productive with something like TypeScript. I plan on trying out Angular 2 soon because of out of the box TypeScript support (it's written in it). Go has re-inforced my desire to use typesafe languages with good compiler support. I'd love to have Go in the browser or even something like C# (meh), but for now will have to live with transpiled languages. I could use something like gohperjs but I wouldn't want to get too far off the of mainstream because everything becomes a special case in a hurry. I've never used React, so no input there. For UI compoenents I've used UI Bootstrap (Angularized Boostrap) and Ionic Framework (mobile) and have mostly been happy with them. If your web{site|app} is simple, you could consider vanilla JS which does have it's merits. I've done this for a simple Go server I wrote for work. Go + Angular2 is not a wrong choice IMO. Both will get you some nice experience and neither will disappear overnight. Just make sure you spend some time on your build process for the frontend. Go dev is so nice with the go tools. JS dev build is just an endless list of continuously changing dependencies. My $0.02.
I tend to avoid it if at all possible as I don't like the reliance on JS, specifically for any applications which help the disabled browse the web comfortably.
Not right now, but I think the most important thing to watch is which fork gets the most love from plugin developers. Hopefully it's not too hard to have a compatibility layer for async / job stuff, if so hopefully the plugins that take advantage will work well in both. If not, it's all about plugin author mindshare.
From my experience with Vue and go: if you are working with *.vue files you are fine. But if you use Vue templates in HTML then Vue delimiters will collide with go template delimiters. Just be careful there.
Huh didn't know of that service, thanks. I've manually downloaded a hard DB copy from SE before, but it's quite big(!) and bulky to handle manually. This might be quite nice for when I would like to quickly play around with SO data.
I'd add my vote for Polymer as well. Although you can of course consume a Go powered API from any SPA framework, Polymer just seems to remind me of Go and has a similar ethos to it. Instead of trying to create a big, heavy, complicated framework, they have built on web-standards and created something relatively lightweight, simple and pragmatic that works well. Can't recommend it enough ... and v2.0-preview is just out, ES6, WebComponents v1 etc... (more browser vendor support). Here's an example to demonstrate using Go + Polymer to power a multi-tenanted site (where each tenant gets their own progressive app): https://github.com/CaptainCodeman/go-poly-tenant
Vue
Right. What they mean is that you need to make sure you end the transaction. A way to do that is to defer the end immediately after you start it. But in your approach, it is wrong. You are waiting until the end of the function to call defer. If anything goes wrong when you call next() and the request panics, you won't have your transaction ended. And my other point was that if you are going to run it at the very end of the function, then defer is not doing anything to help you because either way it's the last thing that is called. 
Also known as [COND](http://clhs.lisp.se/Body/m_cond.htm) (lambda (foo bar) (print (cond ((= 0 foo) "foo equals 0" ) ((= 3 (+ foo bar)) "foo + bar equals 3") ((= 5 (- foo bar)) "foo - bar equals 5") ((= 7 bar) "bar equals 7" ) (t "nothing is true" )))) 
Tests were fixed to use reflectutil.Swapper. 
I'm currently learning Elm. http://elm-lang.org/ /r/elm
fixed
you can make the volumes_attributes field a map, ie map[string] volumes_attributes. then you can make a volumes_attributes array the same size as the map and then for each key value in the map, add the value into the array at the index (only if the order matters). if it doesn't matter you can ignore what i just said an append each map value to an array of volumes_attributes
afaik you can't do what you want with json tags. As you pointed out, changing the slice to a map of int-&gt;v_a can mess up other code which expects a slice. Perhaps the best way is to make a custom type for []volumes_attribute and override its MarshalJSON() and UnmarshalJSON() methods. For starters you can simply build up the map the API wanted and marshal/unmarshal it. 
You can either type-alias it as a map[string] volumes_attributes or make a wrapper type: type VolumesAttributesW []volumes_attributes and implement the JSON Marshal/Unmarshal method on the wrapper type, depending on how you want to encapsulate the functionality. I think I would implement Marshal/Unmarshal on a slice-type because the slice maintains ordering, where as a *map [string]* does not have any ordering guarantees while traversing.
Really digging Vue. Easier and faster than React.
Soft linking isn't a hack. It's a unix tool like any other. I can access all my projects at $HOME/code but many of them exist in a different place, including all the Go code I touch which lie in a $HOME/.go folder. NPM has its own downsides, and if you want NPM's way of dealing with things you vendor absolutely everything and stop complaining about go getting stuff.
The SPA's built with it would suggest otherwise.
I'm going to try ELM on my next project. I could never get into React because it seemed [overly complex](http://www.cli-nerd.com/2016/09/12/functions-are-better-than-components.html).
Of using the SDK method or using the graphics packages?
Just so you know, if you don't want outsiders to be able to guess what the next number will be, seeding an RNG with the local time is undesirable. In such a use case, you should use the OS's entropy source, which in Go is exposed through the Reader variable in the crypto/rand package. Something like this: https://play.golang.org/p/ticW5xkU4c
Any good references to back this up? Heard a lot about polymer when it was released, followed swiftly by nothing else
https://www.youtube.com/playlist?list=PLNYkxOF6rcIDnSm7bZRJC36Ca1DYXSQ70
Be nice if it would do libraries as well…
The reason I use this tool is to find out if certain binaries I care about are in need of updating. But it doesn't matter for libraries since they always get compiled on the fly if they're out of date. So how would you use this information for libraries, if it were available?
I believe it's just mirrored there in the same way Linux is, i.e. no actual development goes on there, and it's just a public read-only snapshot. But I'm not sure.
Isn't it just a library?
Wrap the external type with your type. type ExtendedType struct { *externallib.SomeType someExtraField string } Helpful read: [Go Object Oriented Design](https://nathany.com/good/#composition-by-example)
 type ExtendedType struct { *externallib.SomeType someExtraField string } This is also how you would add new methods to a type from a different package. More details here: https://golang.org/doc/effective_go.html#embedding
Use an embedded type. The new struct will still adhere to the interface. See https://www.goinggo.net/2014/05/methods-interfaces-and-embedded-types.html 
When you embed a type, its members are treated as members of the extended type - they're effectively merged, as you say. You can also access them as if they were nested, but that's primarily only useful if you override some of the original members in your extended type and then want to access the embedded member later.
ok that makes sense, however maybe I am still missing something. in this example https://play.golang.org/p/nxP2n97TwU shouldn't this be working? 
ok this makes sense now. thank you for the help!
See the documentation of the json marshalling: &gt; Anonymous struct fields are usually marshaled as if their inner exported fields were fields in the outer struct, subject to the usual Go visibility rules amended as described in the next paragraph. An anonymous struct field with a name given in its JSON tag is treated as having that name, rather than being anonymous.
You are embedding via pointer and that pointer is nil. `test.field1` is equilalent to `test.SomeType.field1` and `test.SomeType` is nil. Do this instead (it's better from a memory-layout perspective anyway): https://play.golang.org/p/C9kC3tlS91
you're missing the initialization for the embedded type: https://play.golang.org/p/Q4iFGRlc_g
The GOPATH has a very specific layout and it does not match what you want. Specifically, I wouldn't recommend mixing code from other languages into your GOPATH. That would be like putting non-python code in your PYTHONPATH. It might work, but it would be weird. The way I see it, you have 2 options here: A) Embrace the GOPATH as it was designed. If you can explain to us why you think you need your python projects mixed in with your go projects, then we may be able to find an alternative that lets you use the GOPATH as it was intended. If your need for this layout is simply preference and not a technical requirement, then put that preference aside for a while and learn the language as it was designed to be used before you try to do something non-standard. Down path A lies less surprises and better community support. B) Forsake the GOPATH. This is the road less traveled and we may not be able to help you as much with it, but technically, only the go command line tool requires the GOPATH. The language itself doesn't. There is an alternative to the go tool called [gb](https://getgb.io) that may be a better fit for you. P.S. In case it wasn't clear, I strongly recommend option A. edits: lots of small additions to improve clarity
And his fantastic talk at GolangUK https://www.youtube.com/watch?v=twcDf_Y2gXY
It just seems weird that for every other language out there, I can clone the source from github/gitlab/bitbucket to my repos folder, acquire its dependencies and build the project in place. Having a gorepos/ for go repos and repos/ for all the other languages just seems weird. Having go get scan/index through thousands of files in version control ignored folders such as debug/ and cache/ in order to resolve dependencies of 1 project is also weird. I often clone random repos just to observe the code, so my repos folder has a "large" amount of files. edit: gb is probably what I'm looking for, after looking at their rationale and faq page. I don't like how the go community is hogging the 2 letter namespace for shell commands though.
For chrissake, the name of the language is "go". I know that this is rude and doesn't have anything to do with the content, but can we *somehow* please get rid of this "golang"-meme? Using "go" works just fine, even in search engines, these days.
Damn, this sub really is toxic
[deleted] ^^^^^^^^^^^^^^^^0.5612
Awesome, thanks a lot for the great work /u/farslan! I am also looking forward to the VIM 8.0 features ;)
Anything not from Google? ;)
If you see a fork in the road, take it. 
If you are not doing something to artificially confuse a search engine (like using a proxy and an incognito instance or something like that) that looks like something that should be fixed. I tried typing "go" into various search engines from incognito tabs and without and always got reasonable results (except from bing), that is, golang.org on the first page of results (which is pretty amazing for such a ridiculously bad search query). If you want, you can send me a private message with your circumstances (e.g. location and such), I'd be genuinely curious how you can get those results only and there might be the possibility to fix the situation. But it's also not a very representative search query anyways. For example, typing "golang" won't give you any blog posts about go either, so using "golang" in your blog post won't actually help your discovery under that search query. So I'd consider this specific query a very weak argument at best (and a straw man at worst).
&gt; Better tell those meme spreaders at golang.org… Yeah, in the PR I created I mentioned that as "that's to deal with name-clashes". I know where the meme comes from, I still find it annoying. &gt; No need to get mad about it though. I agree. I just snapped a bit, after going through the text for two or three paragraphs and stumbling over the 12 or so golang's (which is a high number even if it would be the correct name) I had to read even for that. Honestly, this has little to none to do with this specific post, it's just the one that pushed me to snap for the first time :) I really wish we could do something to have the term "golang" finally die (if anything, it makes things worse, because now the search engines have to learn the association between the two terms).
[deleted] ^^^^^^^^^^^^^^^^0.6484
[deleted] ^^^^^^^^^^^^^^^^0.7227
On some systems the entropy pool for the system wide `/dev/urandom` (or equivalent) is limited. Unless you really need cryptographic secure random numbers you should **not** use `crypto/rand`. A really good compromise is to use `crypto/rand` to seed `math/rand` (as both you and sleepydog [said above](https://www.reddit.com/r/golang/comments/52faku/seeding_randintn/d7k9oxh)). IMO, there should be a convenience function to do this in the standard library (ala BSD's [srandomdev(3)](https://www.freebsd.org/cgi/man.cgi?srandomdev) which also can set the PRNG state to more values than a single int64 can).
A library to build web components which can be assembled to create a SPA.
Indeed, when I tried using that query from an Australian web proxy it gave me similarly bad results. I'll try to report this.
I've heard of people having success with using sym links to get full control over where the code for the project resides, so perhaps that could work for you without the need to use gb (which I haven't heard of, but perhaps it's well maintained and also a great choice). I haven't tried any workarounds myself. I used to want my folder structures to be standardized too, but as I learned more languages, I gave up since there is so much variance inside the projects anyway. It is an opinionated choice but doesn't really affect my workflow in any way.
I've been using Angular with Go but for my next project I'll try React. I'm not yet ready in jumping to Angular 2.
I am a big fan of React. Although not a SPA framework in and of itself, putting it together with a few other micro frameworks is the best approach I've found. In my case, I simply use it with an MVVM or flux style architecture and I've never needed anything more complicated. Routing is just another state change so I avoid any "router" plugins. I've used Backbone + JQuery UI / Mobile and Angular in production apps and did a pretty deep dive into Polymer a while back. Polymer is the only one I'd consider over React, and I've got a not-yet attitude on that one. 
I assume this still works okay with vim 7, right? :)
Yeah, this and all upcoming versions are compatible with 7.4.x
&gt; Your reasons are probably bad reasons. I don't like how Go treats me like I'm retarded in that sense. Go is a cruel, but very enjoyable mistress.
Well, I was being lazy (though I think farslan caught on) and I did mean 7.4 :)
Just out of curiosity, why do you want to do this? I've been using Go for a large variety of different types of projects for years, but I've found very few good use cases for embedding, although they definitely exist, so I'm wondering if maybe there's a better way to solve the underlying problem you're working with. The main reason I'm asking is that attempting to emulate inheritance is one of the biggest mistakes that a lot of people make when coming from a classic OO language like Java or C++, and this question kind of makes me think of the Stack Overflow issue of asking implementation questions, rather than solution questions.
That's one of my favorite little tweaks Go has made to a standard construct, although I've actually never had to use `fallthrough`. Combine it with the ability to evaluate non-constant cases and comma-separated cases and Go's `switch` is easily one of the best I've used. Rust's `match` isn't terrible, but trying to use it for anything complicated turns it into a syntactic nightmare, and, at least for now, it does *not* get along well with the borrow checker.
You can watch at [mattermost repository](https://github.com/mattermost/platform). This project use go backend and react front. :)
Interesting, it's good to know that the go tool ignores other languages as expected. I wonder what (if any) performance impact that has on tools like goimports and guru that have to look through everything in your gopath.
&gt;Also my opinion: Angular is a pile of technical debt that should be avoided. Yes pls.
&gt; I've never needed anything more complicated. IS there anything more complicated the React+Redux?
I personally do not try to massage the structs as they come in from JSON. I have two layers to my model. The raw (sometimes denormalized) structs defined exactly as they come in from JSON. I then have a proper model with constructors that take the raw structs as params. 
Looking at https://github.com/radovskyb/process/blob/master/process.go#L89 What is the use case for using the channel write to notify that the process has started? The method returns right after the channel write call. This requires the caller to listen on a channel (or use a buffered channel, which seems un-necessary for this situation) or else the method blocks and can't return. 
An article on applying the Clean Architecture to Go: http://manuel.kiessling.net/2012/09/28/applying-the-clean-architecture-to-go-applications/ A bit dated and quite long but I think it still can give valuable insights. EDIT: Just found the same link in the Readme of CaptainCodeman's clean-go!
Why is this downvoted? It's an undeniable fact that the programming language is named Go. Consistency is a good practice in programming. Yet you are not consistent with the usage of language names. You do not call C, clang do you? Do you call Ruby, rubylang maybe? How about D as dlang? Inappropriate usage of the language's name speaks volumes about the meticulousness and depth of their authors.
This is my GOPATH https://gist.github.com/dlsniper/5eeb713fd9b00cd8d40e#file-cloc-txt and I don't think I've seen any problems with it. Intellij gets a bit grumpy from all the Javascript at times but I don't have problems on a 16gb, 8 core i7 machine.
So I have run across a scenario where the embedded type has a method unmarshalJSON that is overriding the default json unmarshal when decoding. the problem is that this custom method does not take into account my new fields from my wrapping struct. Is there a way have a function that first unmarshals the new field then uses the custom method and puts them back together? 
what I am trying to do with this is simply be able to use the already existing struct with a few new fields, I am not trying to mimic inheritance so much as just re-use of code. I have am implementing a rest api that wraps a library, the library contains the structs and the data being passed by the user matches these structs, however I may need to add a few additional fields that the library doesnt need by my rest wrapper will use. so rather than building a new struct that is an exact replica plus the 2 new fields I just need to have a struct that leverages the existing fields plus a few more.
Happy to answer questions here if anyone has any.
To clarify, the first answer to that question is correct, i.e. you should (must) always import the fully-qualified path of a package, e.g. `github.com/yourname/yourrepo/mypkg`. The second answer is incorrect, i.e. you shouldn't (can't) import in a relative sense, e.g. `./mypkg`. 
Interesting post! In your first section, I think it's worth noting that returning a function that closes over data is fairly uncommon and not especially idiomatic (at least I've not seen much code like this). It's more common to create a struct and define those members on it.
Yes, but with Go the whole process is as easy as `gofmt -w -r '"github.com/foo/bar" -&gt; "bitbucket.com/foo/bar"' *.go` (Note that the whole argument to -r is in single quotes, with double quotes inside it.) From [this golang-nuts thread](https://groups.google.com/forum/m/#!topic/golang-nuts/RyhFfhB_kd8)
Alternatively, if you really need a level of indirection, check out [vanity and canonical import paths](https://texlution.com/post/golang-canonical-import-paths). It's overkill for most people, but if you actually expect to change repo hosting, that's how the go team managed their move away from code.google.com
Just want to say I love vim-go and thank you :)
Nothing complicated about react and Flux. I won't comment on redux.
Do you know how efficiently closures are implemented in Go? Like amount of extra object code, memory, etc? I am used to C programming and writing tight code so trying to find the right balance.
Yes. (Or use a vanity import path, if you own a domain.) In practice, I think this has happened approximately never.
I have a special type in a bigger app I'm making: type dbHandler func(db.AppDB) http.HandlerFunc This type is a function that takes an AppDB instance and returns an httpHandlerFunc. Also, I have this function for middleware: func attachDB(DB db.AppDB) func(dbHandler) *negroni.Negroni { return func(handler dbHandler) *negroni.Negroni { return negroni.New( negroni.HandlerFunc(jwtMiddle.HandlerWithNext), negroni.Wrap(http.HandlerFunc(handler(DB))), ) } } Both are instances of closures. attachDB being used: auth := attachDB(DB) r.Handle("/auth_user", auth(BidLogin)).Methods("POST") Then all my route implementations are like this: func BidLogin(dB db.AppDB) http.HandlerFunc { return http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) { defer req.Body.Close() ... logic... }) }
Thanks. Those benchmarks are pretty useful to review. Need to experiment with them.
Oh.. I just realized that you mean't for the StartTty method. I see what you mean now. It may not actually need that I see what you are saying. I'll have to reconsider that, but I am still going to change the other notify call in the regular start method so glad you mentioned it in the first place. Thanks.
You'll have better luck getting an answer by submitting an issue to the repository with your question, kindly asking the authors of the repository, or by asking your question in a DotA 2 developer community, rather than trying to ask the general Go community a question specific to a DotA 2 library you're using.
It should be possible. Just define an unmarshalJSON method on your struct that does a normal unmarshal followed by calling the unmarshalJSON on the embedded struct. I haven't tried this myself, but in theory, that ought to work. If you run into problems, I could try to throw together an example tomorrow.
i have looked at aurelia. a project by rob eisenberg which is known for its convention over configuration frameworks. typescript supported too. http://aurelia.io
This person is on the money.
Because people never learn?
If your project moves from github to bitbucket, it becomes a different package (as packages are identified by their import paths). This is a feature, not a bug. If you don't want a different entity to own your package name (because that's what's effectively the case, when your package name begins with `github.com` or `bitbucket.org`), use a vanity import path.
Imagine I changed github-&gt;bitbucket. So, I can't event build my project before I commit my changes to bitbucket? How you live with this, people?
As the name indicates: a **DHCP** load-balancer. I'm not sure a lot of people have issues with their DHCP server's load. Still interesting though.
Use the force, here the https://github.com/facebookincubator/dhcplb/
I've seen it happen once when googlecode shat down.
Tell it to the D guys, dlang saved their bacon. Before it you had to type "digital mars d" or "d programming language" or just "D" which sometimes produces phallus results. I agree with you, but dlang, nimlang, golang, rustlang etc. are all in popular use for a good reason (unambiguity).
Yup. Context: I'm a freelancer travelling around the world and often deploying over 3G/Edge :P
You should instead be whining about "What if I fork a popular project on github, and it won't build because the import paths include the username?" It sucks, yeah, but *other than that*, the system actually works kind of well.
I'll second the "satisfying io.Readers/Writers interfaces"... Would be nice. 
The build system itself doesn't require any code to be checked in anywhere. You can technically write, compile, and run code that is 100% un-versioned. Obviously "go get" requires your code to be hosted somewhere, but "go build" does not. It only looks at the folder structure in your $GOPATH.
Let me ask a very important question here. Are you actually expecting to move this repo somewhere else in there near future or is this just a hypothetical question? If the current location is only a temporary holding place for this code and you know that you're going to move it soon, there's actually a very straightforward solution. If, on the other hand, you're just complaining that go isn't exactly like every other language, I can't help you with that.
Thanks guys, I just fixed the code :) 
Any downside to using ```FROM SCRATCH``` and utilizing netgo instead of libc DNS resolution?
There's are subtle differences between how DNS resolution is handled between the two, search for issues on GitHub.
Go its amazing and it combines simplicity and performance. However I would not recommend go for anything that has to deal with strange data formats. Irregular JSON, SOAP or XML for instance. 
I would not use it for iOS or android apps, or any kind of frontend. I know there are wrapper etc and tools to translate code, but I don't want to mix technologies.
That should be fixed in 1.7.1 IIRC.
https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1
This is hilarious, "oh no we have too many computers!" but now virtual computers are solved.
&gt; I'm not sold that it would be great for large web apps because I've had issues with it's DB package. It's a quite large step from "I have had issues with Go's DB package" to "Go is not great for Web apps"... (I am sure you have a reason to say that; it just does not become obvious from those few words.)
What's your definition of a large web app? I've been using go for web dev the past few years without issue, one project handles about 50k req/min on a dyno on heroku.
In your opinion, what're the reasons somebody should use this (and universal-translator) over https://godoc.org/golang.org/x/text/language?
&gt; I would not use Go for scientific or statistical computing because that area is dominated by R, matlab, and python. Julia didn't think that way and look how it is growing and taking mindshare off R, matlab and python's turf. I believe Go has a part to play in that game too. especially with its "no magic, but clear and understandable code" philosophy.
He likely means large code/feature base. &lt; 1,000 rps is a) not particularly impressive and b) not indicative of the size of the app...only the scale at which it is running. For all we know, it's a hello world that just spits out `200 OK` for any request. the fact that it is running on heroku is equally as irrelevant. 
some very specific cases have caused me headaches. some bitcoin libraries are not great. was using one that i discovered was completely re-written as i worked on my project. i discovered this by running into a bug from their then-deprecated code and encouraged to upgrade, and it cost me some time re-writing everything (new version wasn't backwards compatible). that's an anecdotal story of course but the lesson is applicable to any project: using new libraries is risky. similarly, i can imagine those who work more in pure math/science have python/java libraries they depend on and can't ditch their projects' underlying language as a result. this isn't strictly a argument against go, as going all-in on a young library in any language comes with similar risks. however, go being young itself naturally leaves the programmer with fewer mature options when researching libraries to work with. in general it's hard to make an argument for choosing a library written 3 years ago in go by a small team over a library written 10 years ago (assuming it's well maintained) by a similar-sized or even a smaller team. i really like this go sqlite3 driver: https://github.com/mattn/go-sqlite3 it has enabled me to create many small projects that compile to a binary i can immediately distribute among diverse systems. one advantage of go is not needing a runtime environment (cough java cough), deploying binaries is easy as pie, and sqlite is just a perfect match for go.
I agree, if it's a hello world app and just spitting out a `200 OK` for every request and taking 500ms, that's not impressive and a static site hosted on a small server running apache could handle that load without skipping a beat. Some of the applications I developed are as you described, very large and packed with features. It's analytics/advertising software so there is a lot of complexities that come with different configurations of things being flighted and response times need to be sub 10ms to ensure the latency Heroku adds doesn't mean we just lost an opportunity to make a deal. Large codebases, large logistical problems. Go handles it without skipping a beat, it has never been the bottleneck. I appologize if I came off arrogant in my original comment, but I've never worked on a project and thought "go can't handle this, it's too complex." of course I chose go for its ability to scale, but I've never and will never take that performance gain over ease of use to work on a codebase.
Is that per dyno? If so that's very impressive, depending on what the requests actually do. 
I know, I wrote a lot because I just felt like giving a little bit back to the community after I've spent a good amount of time learning about it and leveraging free software other people made. 
I am very happy to see this open sourced, but of course opening up your source also opens up the source to code review. The Facebook folks could use iota to declare those sets of sequential constants (enums) without so much boilerplate. (With none, really.) Unless, of course, they have a style guide that dictates otherwise, which might well be the case. 
Not trying to bash on you because the word is in common usage, but I fucking hate the "word" transpiler; it's just a compiler.
SSA?
I'm confused, I thought Go 1.7 already shipped with the new SSA backend..
They've shipped the SSA backend for some architectures. Most code compiled in 1.7 will use SSA. However, there were some architectures which didn't have SSA yet. Now that all of the architectures have SSA, the old backend can be deleted.
Similar-ish in spirit to my project, although I never thought to generate Go templates out of it, which is kinda neat! https://github.com/quii/mockingjay-server
&gt; For example, this seems to be buggy: Of course it's buggy. It's bad code. Refactor your function like this: f, err := os.Open(filename) if err != nil { // error path } // happy path Relevant Effective Go part: https://golang.org/doc/effective_go.html#if Also this talk: https://talks.golang.org/2014/readability.slide#27 Also if you have worries about shadowing, it's trivial to find using the go tooling: `go vet --shadow`
The rule is simple: on the left hand side there must be at least one name, not previously declared **in the same block**. If a name was already declared **in the same block**, that declaration is reused, i.e. the name is not shadowed. Now, you just have to know precisely where are the block. Besides obvious blocks, evident from the use of curly braces, there are some implicit ones: In `if`, `for`, etc, statements, there's an extra implicit block, surrounding the **whole** statement, including `else` parts, so for example: if x, y := f(); g() { foo() } else { bar() } is the same as { x, y := f() if g() { foo() } else { bar() } } In `switch` or `select` statements, each case is another implicit block and variables, declared in receive cases are declared in that block: select { case x := &lt;-in: foo() } is the same (as fast as scoping is concerned) as: select { case someMagicForBlockingOnChannel: { x := &lt;- in; // does not block here foo() } } This is specified in https://golang.org/ref/spec#Blocks
Yes, I know about these, being part of gonum and having been pushing for ndim slices myself. But while ndim slices are (will be) great, you can do without right now and still write great scientific libraries and do science. C++ didn't have them back when it was known C++03 (and a less than stellar compiler story) and still, it took my field (high energy physics) by storm, displacing FORTAN77 in a decade. When not every scientist has the time (or will) to become a C++ master but still want reasonable runtime punch (bye Python), Go is a reasonable choice and a good trade-off.
Not every scientific analysis out job can be solved with a quick Python script or a jupyter notebook. For some, you need reproducibility (useful when your peers investigate the results of your paper) and scalability (in terms of people using your library, or in terms of scientific problem size, or in terms of the number of people working on a given scientific problem, or in terms of the number of machines your application can be deployed on.) I know first hand (having worked on software developed for CERN's LHC physics analyses) that both C++ and Python are ill suited for these tasks. Go has a better story, here. And for quick exploratory work, there's 'go run' and even a jupyter go kernel. And, perhaps, at some point, even a real REPL :)
I'm not sure what you mean by irregular, but usually you should have very little problem marshalling to or from any JSON.
So, what it solves to an average programmer?
Go is not great for 99% of CRUD web apps, you inevitably reinvent the wheel over and over (users, authentication, etc) for very little benefit, when you can create the exact same application in ~10m w/ Ruby on Rails, Django, etc. 
Ahh I didn't know you were dropping neovim support! I'll probably look at moving back to Vim...
I cannot build it from source! Have anyone managed to do so?
Potentially better insight into how to optimize code too (as you can look at what the individual phases produce, right now, and see which one may have shortcomings). But yeah, for the median programmer really nothing will change.
I looked briefly at SQLite, and it appears that foreign key support is not enabled by default: https://www.sqlite.org/foreignkeys.html &gt; Foreign key constraints are disabled by default (for backwards compatibility), so must be enabled separately for each database connection. I'm not sure if this has changed in more recent versions or how up-to-date this document is. This is a bit of a problem for us though, because foreign key constraints are what we rely on to build an accurate model of your relationship structure. I'm not sure if this is something most sqlite users enable, but it is something that will probably have to be enabled (and used) for us to support sqlite effectively. It's also possible that we skip the relationships side of the ORM, but then we're severely limiting the usefulness of the API (because we believe that's one of its big advantages). Personally we don't use sqlite ourselves (not yet, anyway), so it'd be helpful to hear from the community how they think this support should look, and how relationships should be treated.
https://github.com/pschlump/log-consolidate/blob/master/lib/lib.go#L59-L75 After parsing the config and there is a lot of code to set defaults if values weren't set. I think a cleaner way would be to set default values in the config and then parse the user supplied config file.
I would suggest that it is perhaps premature to be dropping neovim support.
The idea behind the post isn't that you have to make a trade off between the two - it's that rushing into a microservices approach without understanding a lot of things up front will only be to your detriment later on. You could have one Monolithic-y app, and a handful of smaller services as well, and that's a fine approach. Monoliths aren't great, but they also aren't directly equivalent with terrible codebases, either.
I think Haskell is great, it's number 2 on my "Languages to learn" list (after Go). I would argue Haskell is not considered a "rapid" "web application" tool. RoR and Django both have generators built for spitting out shitloads of reusable web boilerplate, as well as shitloads of libraries to redo various pieces of web functionality.
Glad I could help :-)
Well, I think for showing what can go wrong it is quite good code :) I would be curious how you would code the fragment from above so that the flow as values is not only correct but also as clear and as concise as possible. There are several possibilities, I tend to chose added var declarations and changing ":=" to "=" over auxiliary return values. What is better? Apart from that I've tried go vet and then grepping for all ":=" and found that go vet does not find all such cases - it found one in about 300 lines but there was another shadowing bug. However what turned out to look really helpful was gometalinter and from this the check of cyclomatic complexity. I would not have thought that but both places where error return values were shadowed, also had a complexity wcich was above threshold. Interesting. 
&gt; Unless, of course, they have a style guide that dictates otherwise, which might well be the case. Haha, nah. Nothing like that. We mainly pass the code through gofmt + golint. But since it's taking production traffic quite well, now would be a good time for these kind of improvements. p.s. if anyone out there feels like it: PRs are welcome :)
Nice idea, especially for stuff like text/template, or other complex packages.
Thank you, this is awesome! Is there a way I can donate to you? 
The message was pretty good, just should maybe write a derivative where the tone reflects the message and it would read really nicely. Thanks or the polite response. Take it easy.
Right, which is why I trolled a bit with the obvious conclusion that when things are just perfect, nothing is bad...I think we all get the need to avoid orthodoxy
I'm going to type congrats on every social media site this gets posted, because it's that awesome.
Lets be more specific. Ask yourself these questions first before going choppy choppy and splitty splitty on your application. * Can you separate, or refactor any components from your system so that they can function entirely on their own without any other part of the system running? If not, don't even bother, a system with distributed parts that are still interdependent on each other is still a monolithic system. Just because different parts of your application is running on different servers and you are using networks and RPC instead of direct procedure calls, doesn't mean you are suddenly using microservices. You will only add more problems to your system while solving nothing. * Do you have fully automated infrastructure, deployments, testing and monitoring? If not, don't even bother, your organization is not ready to handle the mess you are about to make. It would be better to keep all of your manually managed system on one box. 
In the early development phase we didn't have all DHCP options or message types defined, so the option numbers were not consecutive. Looking at it now, iota will reduce boilerplate. Thanks for the suggestion.
I posted this here a while back and got lots of great feedback. I've been working to incorporate lots of what I heard and am ready to release this next version! Thanks for all of the great feedback last time. In this new version, you can: * Use symmetric or asymmetric signing methods, including RSA, HMAC-SHA or ECDSA * Use cookies or bearer tokens * Use separate issuing and verifying servers (if you want to have an independent auth server that has access to the private key and shares the public key, for example)
Thank you! :)
Thanks! No need for donating, it's complimentary! :-)
I have to admit contributing to Go does require way more steps than I'm used to with everything on Github now but I understand why they chose to do it that way.
This is exactly why I started using Code by Microsoft 
I will take a close look at that.
Yeah, Sublime tooling for Go is light years behind Atom and VS Code
I just want people to know that lots of credit should definitely be given to websites such as https://www.socketloop.com/ since when I first started, lots of these examples came directly from websites such as them. I think I will eventually re-write a lot of the examples to be more proper and fix all of the lack of error handling though!
Thanks for the suggestion - I changed it. Much better.
yes, it's per dyno in peak time, will probably grow from there. in total it handles almost 300k req/min across 6 dynos.
good luck buddy.
Congratulations /u/spf13! Good luck! After the success of Hugo, you should slot in well. 
awesome!
Congratulations. I really appreciate your work and hope you will have time to continue with your open source projects.
Congrats [spf13](https://www.reddit.com/u/spf13)! and good luck.
I fully understand the desire to eliminate boilerplate code that is used over and over again. However, I see two general problems with this approach. Most functions are one-liners; the rest consist of a few lines only. Yet, many of them have `interface{}` parameters that can cause runtime panics. So you sacrifice stability in favor of a little bit of readability. And the readability increases only for you but not necessarily for others who first have to find out what these functions do before they can understand any code that uses this library. (Imagine if everyone rolled their own slice library...)
Andrew talks about this in https://talks.golang.org/2014/testing.slide#1
The pprof that ships with Go is an older version of that pprof project. I've filed https://github.com/golang/go/issues/16184 to keep them in sync. You can use that pprof tool with Go code and I frequently do. Be aware the commands and output are slightly different. 
&gt;Can you separate, or refactor any components from your system so that they can function entirely on their own without any other part of the system running? Authentication of Users does not need to interact with any other part of the system. Simply issue a JWT Token. The Static File Server can work entirely on it's own, it's a SPA app only interacting with the API, it can be seperated. So very quickly the system is seperated into API, Frontend Servicing and Authentication. &gt;Do you have fully automated infrastructure, deployments, testing and monitoring? No, but I think I can handle 3 servers/services.
&gt; it was really hard to debug what the issues were I understand. Once such a situation arises, the frustration level increases dramatically. I had similar experiences when working with some JavaScript code that obviously relied heavily on some of the more obscure features of the language. Good libraries should always feature sane error handling that makes deducing the cause of an error as easy as possible, plus (esp. for large libraries that do complex operations under the hood) an easy way of tracing the flow of operations. &gt; once you rely on other open-source packages as dependencies, you tread into dangerous territory True, although this is not specific to Go. And on the other hand, if the packages are open source, you can look into them to see what's going on there. The bigger problem are closed-source packages but luckily Go does not do much to support this kind of package.
A good analysis of the hurdles that new Go users may be facing (and that may draw them away from the language before they even get started), and a very well thought-out proposal for a tool that makes the onboarding easy and painless. Thumbs up.
Consider using a "time multiplier" as proposed on [slide 61 and following](https://speakerdeck.com/mitchellh/advanced-testing-with-go?slide=61) from Mitchell Hashimoto's great [talk](https://youtu.be/yszygk1cpEc). (Update: [exact position](https://youtu.be/yszygk1cpEc?t=1h2m3s).) TL;DR: Instead of using a "mock" system clock, add a multiplier variable to every timing operation. During tests, set the multiplier to a small value so that the 3min timeout becomes a, say, 3sec timeout.
I'm not a huge fan of this idea instinctively, esp for my server in particular, where I don't have many areas where I need to test timeouts (as opposed to Mitchell's case with distributed systems). I'd want to write a time mocking library which allows me to test parts of my server where the time you make the request is a factor relative to the data in the database. i.e. if you make a request to "give birth to baby" today I'd want to validate in my tests that you can't make that request again for the next 9 months So in my tests I'd want it to look like: res := POST "/baby/create" expect.equal(res.code, 200) //fast forward 6 months res = POST "/baby/create" expect.equal(res.code, 422) expect.error("can't have another baby 6 months after first one") 
But why?
Brilliant news. Big fan of both Hugo and Cobra.
This reminds me of similar tools for other languages, I have never used them since they always seemed like pure fluff. It could make sense as part of an IDE but as a standalone tool it seems really marginal. I also don't see how this will drive down questions about "how do I commandline?", it just delays it a little bit.
You could take a look at how net/http/cookiejar tests it's functions which depend on wall time: Exported function/methods call unexported functions which take an explicit time parameter and thus allow simple and fast testing. 
What direction would you take to help improve the new-user experience?
&gt; I also don't see how this will drive down questions about "how do I commandline?", it just delays it a little bit. The fact that the tool tells the user exactly what "commandline command" it uses for any specific action should help them get started with that, don't you think? Today I use the terminal almost exclusively for this kind of stuff but at first it was really annoying reading a 'do this or that' in a guide only to have to google 'how to do this or that'. I think this will help beginners with that.
I am interested to see how such a tool will be created without a cross platform Go gui library.
None, new-user experience is great. Plus, why even bother? What's the point in having more Go users? It's not like Go team sells ads or has a subscription fee on their tool set..
Instead of building such a tool why not add sane defaults as part of the installation? Go is opinionated with an opinionated structure/workspace. I bet a brand new user who finds it difficult to set up GOPATH etc. will be delighted if they don't have to do anything at all about it. As the go proverb goes: "Make the zero value useful." On the other hand, people who do understand how to set up GOPATH, their workspace etc. will simply complain because it's not what they are used to from other languages. These users do not need guidance. They are just used to a certain way of working which is different from the way Go works. Instead of following the instructions, they will use what they already know to make Go work according to their expectations. Once they start struggling to achieve that, they will start tweeting about their bad experience. 50% of those "New user" tweets in the slide seem to be falling into this category. And they will most likely still going to complain even after you build this tool. The way I see it, the root of the problem is described already in these slides: _"There is no centralized point of reference for our tools"_. Even someone who has been using Go for a very long time like myself, sometimes I struggle to remember/find where a certain answer is in order to guide new users with questions towards that point. Is the answer in the official docs? Maybe it's in the wiki? Maybe the answer is in some kind of "official" talk? Or the worst case, maybe the answer is not in any official document? The Go team has written a lot of excellent articles which we still use, even after all those years, as reference for best practices etc. But those articles do not come often. How many more new users that want to write web apps in Go will we have to guide to [Writing Web Applications](https://golang.org/doc/articles/wiki/)? While the article is excellent, it only guides users till a certain point and avoids discussion of certain important topics (database usage). Where should we guide new users for that? Where is part 2? Now I am not saying that you should not build the "Go workbench" tool. All I am saying is that there should also be focus (maybe of higher priority even) to improve, tidy up, update and extend, what we already have.
Exactly my curiosity as well ! One more reason to support the initiative ;-) But it probably ends up being a wrapped up html/js app 
unsafe - I understand you, assertion is unsafe if you don't check, but all the assertion in the code were checked. If interface{} in parameter is a unsafe problem, you need complain with gin, viper, echo, pq, etc. So I think you just need verify the type to do the assertion correctly, as golang community recommends. Not that tests are guaranteed not to have problem, but this project has 100% coverage. Readability - All the functions were documented, so if you have any doubt: https://godoc.org/github.com/lucasvmiguel/list I made this library because I'm tired to search for slice tricks...
&gt; Readability - All the functions were documented, so if you have any doubt: https://godoc.org/github.com/lucasvmiguel/list Sure they are, and good for you. But that's the problem in the first place: While reading some code, I suddenly have to go somewhere else and lookup some docs for something that is ultimately pretty trivial. It takes time and effort and takes my mind off of the code in question. Sure, there's no way to avoid it completely, but "less is more", or sth ;-).
Add generics? That's .. new
dude....generics are the bee's knees
I agree with you in simple cases like: list.GetAt -&gt; list[1] list.Add(x) -&gt; list = append(list, x) but of course this library is not for simples cases, look that and tell me what is more readable: for i := len(list)/2 - 1; i &gt;= 0; i-- { opp := len(list) - 1 - i list[i], list[opp] = list[opp], list[i] } X list.Reverse() or this... list = append(list[:i], append(x, list[i:]...)...) X list.InsertRangeAt(x) 
GOPATH is good. Trying to use Python, Ruby, and Node has convinced me that not having the equivalent of a GOPATH is a disaster in terms of coding importing conventions.
All of it looks wonderful. The only thing I would change is Slide 29: https://puu.sh/rdhIO/3f389d06a7.png This is supposed to be a beginner tool, as in, run-a-hello-world-and-setup, kind of beginner tool. I think there are too many options here. I would just keep the individual tests, lest you scare away new people. 
I think if the written docs were collected and explained well enough, it would eliminate the need. Part of the reason that Go is great is because all of the additional flags and tools shouldn't be something you need to get going. And adding them all in in the beginning would confuse a lot of people about their necessity. "I'll run this tool with *all* the flags because I like all of the buttons." It would end up like MySQL Workbench, which is horribly confusing for a newbie to use and does not lead to a quicker understanding of MySQL and DB Administration. Improving the docs and combining necessary setup information would solve a lot of the problems people have getting started.
The facepalm photo made me cringe. Not everything has to be presented like a reddit comment.
&gt; I bet a brand new user who finds it difficult to set up GOPATH etc. will be delighted if they don't have to do anything at all about it. This is a big barrier to sharing Go projects with people who don't ever work with Go. I could easily say &gt;`brew install go`, then setup your `GOPATH`, then run `go install &lt;myproject&gt;` and the binary will be at [...] ...but for someone just trying to get up and go with the project and not really set up the full environment because they'll probably never use it again, it's a lot of steps. In Python I'd just have to say clone the repo, `pip install -r requirements.txt`, and run the script.
Thanks for the response. I'll go through that exercise, before running through the second article. Thanks man. 
I really like the overall idea of the library and the implementation! Great work!
Good point, faking wall-clock time &amp; date seems a bit more demanding than tweaking timeouts. But maybe the package you need exists already. I searched on GitHub for language:go time mock and found these projects that look promising: https://github.com/benbjohnson/clock: "Clock is a small library for mocking time in Go." https://github.com/101loops/clock: "Go package to return the time; and mock it" https://github.com/WatchBeam/clock: "Time utility with lovely mocking support" 
Examples in the readme.md are essential in my opinion, similar to situation with PHP vs Python documentation - PHP includes examples and therefore cuts away the necessary "find this on stackoverflow" step that Python almost always necessitates. edit: looks nice and clean code wise
So much yes. I'm in the camp of proponents of command-line centricity. Go workbench should not replace command line (at least for now), but rather help newcomers learn the basics and and start working with go tooling as quick as possible. PS. The presentation is super clear and awesome.
It's not at all. But it doesn't break evey other day 
Great, will check back on project in a few days time; be sure to update comment thread for publicity :)
This may be out of scope on what you're trying to accomplish but may I suggest adding details on FAQ section at the end on topic such as IDEs and their packages required to run go? also list some of the most popular frameworks/libraries out there. 
I think on any system pprof requires some separate gv utility to do the rendering, but I could be misremembering.
The Getting Started docs could at least mention `go run`, since you can run a simple Go script without setting any `GOPATH`. vi hello.go go run hello.go 
This seems like a perfect use case for Electron.
these days the only reason I have for nginx is as a reverse proxy but even then now I use caddy
Well that's a simple approach but how do you know if after you send a request the response you read may or may not be the one you expect (because of the unsolicited responses) and whats to stop the other event thread from sucking up a request response?
[deleted] ^^^^^^^^^^^^^^^^0.8841
I'll second this. I only learn by example. Because I am a dolt.
Wrong sub?
1.7 had only x86_64 for SSA, the rest of the architectures weren't finished yet. /edit already answered, should have read before I post ...
Yep, unicode trickery: https://play.golang.org/p/ugO0B8rKlI Got me for a second. Redeclaring `S` was too suspicious. Plus, I guess I could've removed one of the declarations and it would've errored in the compiler...
Shouldn't there also be "BenchmarkReusingNormalFunction-4"?
It looks like a good start. Good comments and code clarity. I was a little surprised that basic feedforward networks had memory, but sure. As a side note, I think you might have trouble with speed and with crossover for non-linear structures when putting this or other types of algorithms into your evolutionary library-- it looks like everything needs to convert itself into a binary string to be able to be selected, which will make tracking information like tree structure difficult and adds a lot of overhead between the different steps of the evolution. (disclaimer: I'm developing my own evolutionary computation library in go right now) 
This is very insightful and very true. I love to default Go way and tools, but I also see the strong point that `gb` makes when it decouples projects to some sort of general Go workspace, sometimes that's just what you need.
The first `S := 1` is actually `\xd0\x85` while the second one is `\x53`.
This is a great idea. The most compelling part, in my opinion, was the proposal to have a central place for Gophers to see go tools and their docs. Right now I have an error prone mental map of where they are, and some bookmarks for from when that fails :) This project could make the language more awesome to start with and make people productive and enthusiastic easier/faster.
I'm doing something similar with the [Asterisk Manager API](http://www.voip-info.org/wiki/view/Asterisk+manager+API) (a.k.a AMI) both receiving events from the interface and sending requests to it that return responses. In the AMI, requests sent down the line are tagged with a client-generated unique ID. Any responses related to the request include the tag. The pattern I've been using is to create a channel that I store in a map[string]chan (map[string]string) and spawn a goroutine that waits on the response in the chnanel. I grab the event packet from the AMI, see if it matches an outstanding request in the map, and dispatch the packet down the channel if it matches: // Anything with an action binding if action, ok2 := packet["ActionID"]; ok2 { resultChannel := pendingRequests[action] if resultChannel != nil { resultChannel &lt;- packet } } Since I spawn requests inside of a goroutine, the request logic is simplified: func (a *AMI) DBGet(family string, key string) string { // Need a unique action id to send along with the request actionId := fmt.Sprintf("id%x", rand.Uint32()) // Build up the request packet packet := make(map[string]string) packet["Action"] = "DBGet" packet["ActionId"] = actionId packet["Family"] = family packet["Key"] = key // Channel that the response packets will come back on responseChannel := make(chan (map[string]string), 1) // Install the channel into the pending requests for dispatch pendingRequests[actionId] = responseChannel // Send the request a.out &lt;- packet for { // Check each received packet to see if it is our response packet packet = &lt;-responseChannel if packet["Response"] == "Error" { log.Printf("astdb key %s/%s not found", family, key) break } if packet["Event"] == "DBGetResponse" { break } } delete(pendingRequests, actionId) close(responseChannel) return packet["Val"] } Hope this helps. 
Get out.
Well, "simply issue a JWT Token" is a rather simple example. In case of some corporate internal app, it's sufficient and I'd say it's also sufficient for some internation applications. Usually what you do is issue three tokens. One is for CSRF and reissued per request with a short lifetime. The second is the current authentication token and has a short-ish lifetime, let's say an hour. The third is the refresh token and is used to obtain a new current token from the auth server. It's true you can't revoke a JWT, but using three tokens you can fully circumvent this problem. The second token only lives a short time after the refresh token is revoked. The refresh token is authentication against the auth server itself but all services can check it, it does not authenticate alone so it's no concern.
This looks really cool, I've tried looking a bit through the code but how is locking handled? Does it need a separate locking table?
Capture the traffic and compare.
While I think it's awesome that the workbench would display the equivalent CLI commands, I'm still sceptical and think that a lot of these problems directly stem from GOPATH and vendoring being too complex. Are there important reasons why your project layout must be $GOPATH/src/yourpkg? Can't we get rid off GOPATH entirely with vendoring? The other thing is that the semi-official tools should be easily accessible from the website. Currently I have no idea where to get goimports, golint, etc. without searching first. (Fortunately, vim-go handles this for me.)
Congrats from a happy Hugo user!
Hi, locking is implemented by setting the txID field of a to-be-locked-item to the ID of the locking transaction in https://github.com/fumin/dtx/blob/master/transaction.go#L581 of course upon the ConditionExpression that the item has NOT been locked by other transactions in https://github.com/fumin/dtx/blob/master/transaction.go#L587 The locking per se is done of the item itself, but the algorithm does require two additional tables, one for storing the transactions themselves, and other storing the snapshots of item after they are successfully locked, but before any changes are applied. The original design documentation contains a higher level view of how this locking behaviour interacts with the other steps in the algorithm https://github.com/awslabs/dynamodb-transactions/blob/master/DESIGN.md
FYI: The `WORK` var has nothing to do with `GOPATH`. It will always point at some kind of `/tmp` (actual path might differ between systems). It's a temporary directory where things like object file creation and cgo compilation take place. They are (or "supposed to be") moved to your `GOPATH` place afterwards. Now, if you show us the whole output of the `-x`, it might be more useful.
TL;DR Use tcp proxy and proxy your http requests to an existing Tor installation. The code is really sloppy. E.g. instead of commenting code, you could do: var proxyURL = &amp;url.URL{ Scheme: "socks5", Host: "127.0.0.1:"+defaultPort(), } func defaultPort() string { if runtime.GOOS == "windows" { return "9150" } return "9050" }
This was my thinking. Aside from being confused, I'd probably close Workbench not wanting it creating and pushing a repo for "Hello World"
&gt; The code is really sloppy. It is a one-function example snippet. How exactly is it sloppy though? I'm willing to improve it if it actually makes it better. &gt; instead of commenting code Again, this is a snippet for learning/reference therefore it is overcommented to explain everything to reader. Feel free to delete every comment in there and rewrite it in a way that makes you more comfortable. &gt; if runtime.GOOS == "windows" It is not as simple as that. If they are in Linux and using the Tor browser bundle then the port will be 9150. If they modified the torrc at all, it can be a different port. It is up to the programmer to manage that. 
What does packaging, distribution, and installation have to do with "coding importing conventions"? You can always override module search path with PYTHONPATH or NODE_PATH (not sure about Ruby) if environment variable is your thing. Or you can programmically change module resolution as well. Those are dynamic languages. Yes, those languages come with arguably more feature rich package management tools so that you're not too concerned about including all dependencies in your package: one command to install your cli program and all of its dependencies. I'm not sure if it is planned, but once Go supports linking to runtime shared libraries, you'll end up with the same problem with packaging, distribution, and installation regarding where the runtime shared libraries are looked up from (LD_LIBRARY_PATH, for example) 
https://play.golang.org/p/5il3KeC4i6
same!
It was one the first one i tried, sadly it just says its unable to discover the router... Though UPnP is on and other programs can use it...
So glad to see this. Users have been banging hard for more latency!
warning...long blog post ahead. OP it would be good to have TL;DR
The TL:DR; is here https://peter.bourgon.org/blog/2016/07/11/context.html
I've updated the original post with the output of what's going on and some more information. I looked in both `$GOPATH/pkg` and `$GOPATH/src` and neither had any new files in it. I had to create the folders last time manually and then do `go install` in the `$GOPATH/src` directory. Then it created the paths in the `pkg` folder. 
He comes to it in the [last sections of his article](https://paddy.io/posts/contexts-and-dependency-injection/#composing-ourselves:a02204ad746afb5ed34fd2c044a0278c). Edit: removed the big code snippet, you can just find it in the article
[It's been done before](http://bouk.co/blog/idiomatic-generics-in-go/) 
Okay, this is sweet depending on how it's implemented.. will be on mobile for rest of night so anyone familiar with it fill me in? Mainly curious high level if Plugin here is a Go concept or a minimal abstraction around dynamic loading (POSIX dlfcn)? Because if the latter it could be used for things not possible before depending on the tax of reaching into it. Is it like, import C cost? Or is lookup similar to normal runtime heuristic's.. cause you could do some cool stuff like make a database server that compiles views into Go code or something. I dunno. Cool stuff.
All the messages are newline delimited
FFS, there is a reason the http server takes an http.Handler interface and not a handler func. Handler func is just a convenient wrapper to allow a function to satisfy the Handler interface. If you are writing a system with context then have your context struct declare a ServeHTTP method and voila it all freaking works like magic because that's what interfaces are freaking for. Then by the power of interfaces you also have your middleware take this interface and it still freaking works. Even more amazing, none of your code depends on each other. [be explicit](https://play.golang.org/p/GQPd_oPgPo) Now if you have some poorly coded middle ware that takes a func(writer,request) rather than an ServeHTTP then you can just take you object and do o.ServeHTTP and it casts to that function type. Likewise if you need the badly coded middle ware to be wrapped in your awesomesauce code you can up cast the function to a ServeHTTP by using HandlerFunc. For all go is touted for its goroutines, the real power of go is in the interfaces. use them.
This is awesome. This is terrifying.
Minimal abstraction around dlfcn. Read (it's ~120 sloc): https://tip.golang.org/src/plugin/plugin_dlopen.go 
I'm waiting for the first "Blogging platform written in Go with plugin support!" 
It isn't what I meant. Here there is a dependency struct which itself satisfies the handler interface, this way you pass the struct to the http.Handle. But you also need one handler struct per endpoint. What I mean is creating on dependency struct for a set of endpoints, declare multiple handler functions on it, so they can access the dependencies, and later pass those functions to the http.HandlerFunc's
This is huge
Woah, so looking forward to that.
So, * `go install` works fine * `git clone` works fine * but `go get` does not work at all. Since `go get` uses `git` under the hood, it looks like `go get` fails to call `git` for some reason. Or (as no error message is seen) it does call `git` but the results of the git operations get lost somehow - either discarded silently, or written to some wrong place.
You may want to hide the implementation/ provide a system which accepts extensibility even while being closed source.
Hey, thanks for that, so reading it looks like calls to Plugins will pay cgo tax if I'm not missing anything? Not a big deal really since most plugins will probably be something you call into for larger segments of work vs many tiny calls. Still think this is going to open some cool new possibilities with Go code compiling Go code, so you can run Go code while you run Go code. Okay Go home Chris your drink. 
... and inferior enums.
The inability to unload a plugin defeats the use I would have for this, unfortunately. A long running service with plugins may want to load new, updated versions of an existing plugin. Not being able to unload the old version, ends up creating stale references which will keep piling up for the lifetime of the process. As I understand it, this is essentially a memory leak. Still, I can think of plenty of use cases where these plugins would be nice to have.
Looks okay, pretty useless without a plugin Close() to unload plugins though.
&gt; The inability to unload a plugin defeats the use I would have for this, unfortunately. I second this.
Having just spent some time working with a custom implementation of the RPC method, I wonder how this compares and what problems arise. 
Will this make Go conquer Java big data?
ohh no its all for learning
Try https://github.com/syncthing/syncthing/tree/master/lib/upnp it's a subpackage to the nat package, but might be usable standalone.
I first started wanting the ability to do this back when I was writing a simple program for pulling and displaying web comics. I wanted to be able to add support for comics by just writing a plugin and dropping it in a folder. Something like this: The program would have a separate package that worked something like this: package comic type Interface interface { // Stuff for getting info about a comic, such // as URL format, URL of specific comic, and // so on. } Then, I could do this: package main import( "comicviewer/comic" ) func Init() comic.Interface { // Initialize and return something for the // plugin's specific comic, be it XKCD, Dilbert, // or something else. } And then in the viewer itself somewhere: package main import( "comicviewer/comic" ) func loadPlugins() { for _, file := range(pluginFiles) { p, _ := plugin.Open(file) i, _ := p.Lookup("Init") addComic(i.(func() comic.Interface)()) } }
Another Go proverb will arise, I assume: "Go plugins are not Go"
share memory among plugins.
This may just be an early implementation detail? Because POSIX dlopen is complimented by a dlclose. Or they are positioning theirself for a Go specific implementation and closing is a complicated thing to get right first pass. I am not well acquainted with the Go internals at all, but curious how viable it would be to execute plugins from memory and if any speed up could be obtained. I.e. map() with PROT* flags including EXEC. That would probably be a large technical undertaking but might even have the way to use Go as a scripting language to be used within distributable apps.
Close() would be nice it's but that doesn't mean it's useless at all.