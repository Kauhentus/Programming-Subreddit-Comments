&gt; it's a mere demonstration that the language could be improved (yeah) by adding some few hundreds of lines. Under the assumption that addition == improvement. (I can prove a lot of things if I may choose the assumptions freely.)
Well, there are situations where it's not possible for a function to return a `Result`. Here's a few off the top of my head: - top-level function of a thread - functions executed asynchronously (in various ways) - arithmetic operators (panic on overflow) - many iterator member functions Arguably all of the above _could_ technicaly be converted to `Result` returning functions but at the cost of lower convenience/readability and higher code verbosity... 
His effort in the article could be respected, but the blatant hostility towards the open source community should not. I cannot support someone who actively disrespects the community in various ways, but seeks praise from it.
What do you mean "boundless implementation of a bytes.Buffer"? Where do you think it bounds you?
Here is also the post on his blog about it: [Package Oriented Design](https://www.goinggo.net/2017/02/package-oriented-design.html)
I was thinking more of a `type foo struct { field string; func bar() {} }`. But your point about build tags is right. 
But now you just changed the syntax of defining the type to depend on whether the type is a struct type or not. That's not what simplicity looks like.
I forgot about MANIFEST.in. There's also [pyproject.toml](https://www.python.org/dev/peps/pep-0518/), which I've never seen in the wild yet, but why not add one more thing?
Yes, but that's not a linked list; it just has a linked list as part of its implementation. ;-) I'm mostly just complaining about linked list because it is the bubble sort of data structures: it is well known because it is pedagogically useful, but there are very few cases where it by itself is a good choice absent some unusual mitigating factors.
This wouldn't work in functions without named variables or zero return arguments, based on what you say it replaces. 
It is a linked list though. It is a linked list + a hash map to elements in the linked list, so if we had generics and linked lists we could use the built in map to make an LRU cache.
[`testing.(*T).Helper()`](https://golang.org/pkg/testing/#T.Helper) is my favourite addition. Definitely gonna go through my tests and use it. **EDIT:** Thank you for the gold, kind fellow gopher!
Yes, this was mentioned on the golang slack as well. The workspace will always be clean, before and after. Anything else that happens outside pollutes everything, that's why you have to either be careful or put more effort in by using docker ( I want to, gradually). I'm new to the language and what concerns me the most is the go version you mention.
If you want to make it harder for you to debug issues, sure, go for it. As an example: https://play.golang.org/p/heF7hr9_24
https://golang.org/doc/go1.9#vendor-dotdotdot This is a real life saver. `go test ./...`
As much as I like William's talks I don't agree with some of the advice he gives on this subject. He is essentially "abusing" `internal` at a very high folder level and throw a bunch of packages inside that are going to have looser structure. Sure this is going to work if you follow strict guidelines but it also opens up the team to not care about package structure inside the internal folder. I think `internal` should be used rarely and with care, just like the standard library does. I am also concerned on how easily he makes small packages especially inside `internal`. I don't know how much code is there but from the example he gives it seems he is treating packages like a Java class and so he makes package `orders`, `customers`, `tags` etc. Those could be just files. Another issue I find is that he tends to do dependencies in the reverse way than the standard library does. He says something like "If `orders` have to use `customers` then put `customers` under `orders`". As far as I know, the standard library uses subpackages for implementations. It is rare for a package to import a subpackage. It is usually the reverse. I much more prefer [Ben Johnson's advice](https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1) when it comes to package structure which more closely resembles the standard library. But otherwise William's advice about packages communicating purpose etc. is very sound.
Life is good!
I don't understand what is the issue. Aside from non-descriptive message, but that has nothing to do with the new method.
I'm not sure I understand the point of the Helper function. Can someone explain it to me?
[Release note](https://golang.org/doc/go1.9)
&gt; The workspace will always be clean I've worked with Jenkins enough to be weary of this claim. Best to code your Jenkins jobs defensively: have each job verify the workspace is clean before proceeding. This way, if someone accidentally flips the "Clean Workspace Before Running" switch, your job doesn't develop subtle errors as a result. &gt; the go version you mention Don't forget the versions of anything that's been `go install`ed in the same `$GOROOT`
Let's say you have a package that implements some sort of a data structure, and you write a `func checkFoo(t testing.TB, f *Foo) { if f.level &gt; 9000 { t.Fatalf("level must always be &lt;=9000: %v", f.level) }}` that makes sure its invariants still hold. Imagine you have dozens or hundreds of those calls in your unit tests. If you now make it call `t.Helper()`, the resulting error won't claim that it happened in `checkFoo`, it'll claim the error happened in the actual test, at the correct line. `t.Helper` makes it easier to find the spot where to start debugging.
I actually dislike this solution a lot. I've always been fond of Go taking a stance against those weird quirks that only apply in special situations. I would have already liked it a lot better if the caller would decide when to mark some function call as a helper, like: ``` func TestFoo(t *testing.T) { ... t.Helper(someHelperFunc(...)) ... } ``` That being said, our Golang overlords probably had a good reason not to implement it like that.
my concern with this approach is that the /internal/platform folder is just the /pkg anti-pattern in disguise. any time you are making a folder that wont be imported, it should slow you down.
I assume you're referring to templates, not macros?
The popcount in math/bits should speed up my sudoku program. Thanks Go Team!
the internal pattern is used in a lot of places, or are you specifically talking about nested packages in internal?
&gt; Hi ! I'm now making a web project. &gt; Model struct fields map to database fields and sometimes differ (if i always use some tables together i create embedded structs or create fields that are pointers to other models). &gt; And all this worked perfectly until i decided to introduce natural composite primary keys for some models... I do not understand why you need this abstraction. Is this some kind of mini ORM-ish library that is going to be used by many other projects of yours? I am assuming no since you are clearly stating that you are making a web project. I recommend to keep it simple. Use one of the techniques described in [this article](http://www.alexedwards.net/blog/organising-database-access) (preferably the technique /u/everdev described), write your database functions along with their tests and move on. If you want some help to reduce the boilerplate you could use a package like [sqlx](https://github.com/jmoiron/sqlx). Sure you are going to have to write boring code but there's nothing fundamentally interesting about this problem anyways. By the time you have finished and moved to a more interesting problem or even another project, you'll be wondering why you even bothered with the abstraction in the first place.
it's the nested "packages", yes. i'm not suggesting that use of /internal is an anti-pattern. /internal/platform won't be imported by anyone or anything because it's empty. it doesn't serve a purpose, houses no code itself, it is just a folder. with a name like platform, it is highly unlikely that platform will ever provide code, so it is no different than using a /pkg folder at the root.
Compilation times are slightly better for my project: Go v1.8: real 0m6.542s user 0m12.727s sys 0m1.524s Go v1.9: real 0m5.502s user 0m11.103s sys 0m1.447s 
Now that 1.9 is released we're logically just one more minor release away from 2.0!
sorry, the next release will be 1.10
/u/bradfitz has talked about this before. &gt; Go 1.9 &gt; Go 2.0 !!! &gt; Generics! &gt; Sum Types! &gt; List comprehensions! &gt; Immutability! &gt; Rationalizing []byte vs string &gt; Rationalizing new() vs make() &gt; Rationalizing := vs var &gt; VFS layer, os.File becomes an interface &gt; ~memory @ownership! &gt; Data races statically impossible! You can find the rest in [his talk](https://docs.google.com/presentation/d/1JsCKdK_AvDdn8EkummMNvpo7ntqteWQfynq9hFTCkhQ/view#slide=id.g118cf9b85c_0_258).
I tested an old version of a project of mine against Go 1.4 (the fastest Go compiler so far) and Go 1.9, and the results were 2.1s for 1.4 and 2.5s for 1.9 -- so 1.9 is within 20% of the speed of 1.4. Not bad! (That's for about 30K lines of Go, running on a old MacBook Air, dual-core 1.7 GHz i7.)
Oh my summer child...
Same here. I read the article, and I can tell there is a huge lack of real world experience. When the author said he was new to the language, yet writing an article on how to. It confirmed my thoughts on lack of experience in the real world. 
Yes, templates.
just run go get -u golang.org/x/tools/cmd/goimports solves the problem
One thing has stayed the same. The constant criticism about lack of features.
Pprof labels looks interesting. 
Interestingly they don't use the popcount opcode for x86 processors yet. Its all written in Go with table lookups. 
&gt; Uses local files, packages, and **automatically uses go get** if the remote package doesn't exist. Eww.
I think the argument laid out in https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1 is pretty darn strong, for the kind of app that has a significant amount of business logic. Seriously, I'm picky as hell and my only complaint about that article is the word "standard".
EDIT A comment below has ***completely accurately*** summarized the spirit of my comment: &gt;it just makes Go slightly less broken. (Their words, not mine.) I added this quote because of the huge negative backlash to my comment. Personally, I'm super, super excited!!! Of course, full generics should need to be added to the language later!! ORIGINAL VERSION OF MY COMMENT: -------------------- [ EDIT: copied from original bottom of comment: ***Obviously this just reduces copy and pasting in certain specific cases, letting you keep the type information in just one part of the codebase - it's only a small part of what generics enable***. ] Holy holy sh#$ is type aliasing huge!??? I only just learned Go a couple of weeks ago so forgive me if I'm wrong but this SEEMS massive!!! ~~Like, almost the same as generics~~. [EDIT: that goes way too far. I was just excited at how useful this new feature might be.] You can now write a set of functions and data types where in only a *single* line you write: type MyPony = int; Then change it to string. You can write it against MyPony!! That is absolutely massive if this is true! Let me give you a very specific example. Here is a Queue: queue := make([]int, 0) // Push queue := append(queue, 1) // Top (just get next element, don't remove it) x = queue[0] // Discard top element queue = queue[1:] // Is empty ? if len(queue) == 0 { fmt.Println("Queue is empty !") } Beautiful, isn't it (I got it [here](https://stackoverflow.com/questions/2818852/is-there-a-queue-implementation).) Makes my eyes water. Awesome. Except wht's this - queue := make([]int, 0). damn. Can't use it for anything but an int! But now these two lines: type queueType = int queue := make([]queueType, 0) mmmmmmmmmmmmmmm. That's just the beginning. You can make functions and whatnot, right!??? And they say Go wouldn't get generics :D [&lt;--- this was a joke. This is not generics.] I mean, maybe I'm misinterpreting but it sure looks to me like this can save a TON of copy and pasting... [&lt;---- only copy and pasting. And search and replace. Not full generics.] What am I missing? (***Obviously this just reduces copy and pasting in certain specific cases, letting you keep the type information in just one part of the codebase - it's only a small part of what generics enable***.) [&lt;----- this was part of my original comment but should have gone on the top instead of the bottom.]
Unless there is a bug in the compiler the popcount instruction should be emitted for amd64. math.Bits only contains the generic go versions of the implementation. The compiler can replace those on a per architecture basis. Since not all amd64 capable cpus support popcnt the dispatch of the popcnt instruction is guarded by a check for the capability. relevant code for the compiler intrinsic: https://github.com/golang/go/blob/master/src/cmd/compile/internal/gc/ssa.go#L2879
Yeah. This is the one I'm excited about 
You're bound by reads, on grow() bytes.Buffer forgets previously read content, so it can't be used to implement io.Seeker. https://github.com/golang/go/blob/master/src/bytes/buffer.go#L140 
There's no blanket answer. Generally these things depend on what you're doing. Build some prototypes that are as close as possible to what you actually want to do, and test them out. That's really the only way to know for sure. 
You're right, of course, but unless you're a NodeJS master and a Go novice, Go is very likely to blow the doors off NodeJS in every respect. 1) memory usage 2) request latency, average and standard deviation 3) throughput, particularly on a multi-core system 4) high open connection situations 5) deployability It's hard for me to wrap my head around how well suited and brutally efficient Go is for API handling.
&gt; unless you're a NodeJS master and a Go novice Out of all the things you listed, I would say that's the one that matters the most. At the scale that 90% of use cases operate at, what matters more is how fast someone can execute.
&gt; unless you're a NodeJS master and a Go novice Out of all the things you listed, I would say that's the one that matters the most. At the scale that 90% of use cases operate at, what matters more is how fast someone can execute.
?¿?¿?¿?¿ ¯\\\_(ツ)_/¯
Didn't realize they hard coded those library functions into the compiler! 
Are you using dep instead of glide now?
Indeed.
Concurrent Map in the sync? Yay! Totally missed it was going to happen!
You don't want it to automatically use `go get`?
I'll be interested in seeing benchmarks on it. 
Why do some golang threads have everyone golded? Who keeps doing this lol Edit: oh god damn it lmao
An exported function of a package is technically an API you know
really great stuff. What i don't see here is any benchmarks regarding speed?
Hey, can I havz gold too?
I'm gonna take this seriously and help lift you up, and also so when someone finds this in a search later, they understand what they need to know. *_"Generics" in Go 1.x_* First, if you needed to declare a new `int` type, you'd do it with: type Color int You do this because you want to strongly type `struct` fields, function parameters, and return values. Using that specific type (`Color`) instead of a plain integer makes it very clear to your human readers (and the compiler) what you intend to use. Then to make it act like an enumeration: const ( COLOR_UNKNOWN Color = iota COLOR_BLACK COLOR_RED COLOR_WHITE COLOR_BLUE ) Now, we wanna do something with the color, so let's add a function with a receiver (aka a _method_) to it. func (c Color) String() string { switch c { case COLOR_BLACK: return "black" case COLOR_WHITE: return "white" // . . . } return "UNKNOWN" } Now, let's say we also need to do some common functions on types like `Color`. We'll define an _interface_ that describes what those behaviors or functions are. type Squawker interface { Squawk() string } If we want to make `Color` implement `Squawker`, it needs to provide a method. func (color Color) Squawk() string { return "SQUAWK " + color.String() } I'll assume I have some other types that also implement Squawk(). If I want to work with that type generically, I'll declare the container (a slice or a map) to hold that interface. var squawkers = make([]Squawker, 0) squawkers = append(squawkers, COLOR_BLACK) squawkers = append(squawkers, Duck{Name: "Donald"}) When I later retrieve a `Squawker` from the slice, I know it will implement the `Squawk()` method, but I won't know anything else about it. See also a [Playground link with working code](https://play.golang.org/p/3H3APhEE2P). *_The point behind type aliasing_* Type aliasing is intended to be a way to allow you to migrate from one version of an API to another without breaking compatibility with existing code. Go programmers are _very_ conservative in how they change APIs -- or they should be -- and we try to keep the same signatures in place for as long a time as possible. Type aliasing allows you go develop a new version of a type, give it a new name to differentiate it from an older version of the API, and alias it to maintain name compatibility. You already had limited aliasing by naming your imports. This just adds a new facility to allow you to alias (or rename) existing types.
Awesome.
I don't know about you, but I picked up Go very much faster than Node. It took a few months before I completely discarded Node because of every bullet point, and some others which just deal with language libraries and syntax. Most of the time spent in node was choosing between the mess that are npm packages and I'm going to say poor documentation of stdlib. Trying `yarn` which is touted as a good replacement to npm (*edit), but constantly gives issues forcing you to rebuild npm packages, like [node-sass](https://github.com/sass/node-sass/issues/1918). Don't let the issue being closed fool you, that means nothing in Node world. *Nothing.* Go shines with it's excellent standard library and the choices third party packages aren't trivial like some often mentioned examples in Node (isarray, is-array, left-pad, etc.). Even with non-trivial node apps, it just becomes ridiculous to understand even how many times you can declare a closure in Node and what's the difference between those declarations. Syntax wise it's a mess. Callbacks, promises, async/await, closures and arrow functions, no concept of concurrency (pcntl fork? Is it the 90's?) and you have to bolt on process managers to scale *beyond a single cpu core*, like PM2? REALLY? Go can scale to whatever number of cores without adding a process manager on top. I think that's a fucking great benefit to anybody who's even thinking of Node today. Or even PHP, which is kind of the same, [*but worse, much worse*](https://serversforhackers.com/c/php-fpm-process-management). Here's like [10 free copies of API Foundations in Go](http://leanpub.com/api-foundations/c/FREEDOM) for fucking *anybody* who's a Node guy/girl and is looking at Go. Just take the plunge and notice that your mood improves, you're sleeping better and there's no ambiguity to [which type](https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Functions/Arrow_functions) of [closure syntax](https://developer.mozilla.org/en/docs/Web/JavaScript/Closures) you should use. Binding functions into an object context? I think the reason why I survived so long as a programmer is the ability to put these random facts and nuisances very deep in my psyche so they only come out to play during my worst nightmares. Who doesn't like night terrors? Edit: p.s. I feel like I should add the fact that Go has a fucking *standard library testing and benchmarking toolchain*. It wasn't mentioned in the bullet points specifically, but given that node has so many different ass libraries that deal with this shit (sencha, ava, mocha, ...), and people actively advocate moving from one to the other for I don't know which reasons, and I even find it ridiculous that there should be a *third party* package that implements *code coverage* (hello [istanbul](https://www.npmjs.com/package/istanbul)). F... just... I have 99 questions and all of them are ELI5 "Why do planes fly?" but "Why does Node even work?". Edit2: copies are gone. That was 20 copies like poof. If anybody genuinely wants to learn Go (as opposed to people who just collect free ebooks and never read them) and have somebody who you can ask questions about how to do X or Y, send me a DM and I'll probably send you some extra copies. Also Discord Gophers is a nice location to rant/be serious, you can join me there: https://discord.gg/PxwHvBS ;)
&gt; What am I missing? Well, pretty much the whole concept of generics actually. Consider this C# code (it's pretty straightforward even if you don't know C#, don't worry), a language that actually has generics: class Foo { int[] data; int first; } class Bar { double[] data; double first; } You have two classes (Foo and Bar) that are essentially the same, except for the type of the data in it. This code is *obviously* duplicated, except that it's not, because those classes store different data types. So, how do we solve this? With generics! class Baz&lt;T&gt; { T[] data; T first; } This Baz class can work with any data type. So if you want an array of various data types, you just do this: var x = new Baz&lt;int&gt;(); var y = new Baz&lt;string&gt;(); var z = new Baz&lt;SomeCustomTypeYouDefined&gt;(); and the compiler automatically generates the necessary classes for you. How does this differ from your reply? Well, suppose that I publish my class Baz in a library somewhere. Then, Alice, Bob and Eve all download my library and they want to instantiate a Baz with some custom data type. Well, no problem: var a = new Baz&lt;AliceCustomType&gt;(); var b = new Baz&lt;BobsCustomType&gt;(); var e = new Baz&lt;EvesCustomType&gt;(); They can just import my library and use it. In your case, each of them should *modify my library* to change the type alias. It's not the same as generics at all, and you need to recompile everything if you change a type. EDIT: woah, my first gold :D Thanks, kind stranger!
Since he's coming from Node, he might have issues understanding what you mean by "exported" or "function" or "package"
I don't like that very much. 1. Too much underscores (very python, much unreadable, wow) 2. It hides to me the explicit handling of errors (that is one of the reasons why i hate try/catch constructs, even if this isn't one) My 2 cents :)
Sure, here is the issue with it, I was hoping that running the test would make it clear but by the amount of -1 I guess people don't understand that. Without the helper method ``` --- FAIL: TestTrue (0.00s) demo_test.go:9: failure --- FAIL: TestTrueTwo (0.00s) demo_test.go:12: failure FAIL FAIL command-line-arguments 0.395s ``` With the helper method ``` --- FAIL: TestTrue (0.00s) demo_test.go:21: failure --- FAIL: TestTrueTwo (0.00s) demo_test.go:25: failure FAIL FAIL command-line-arguments 0.395s ``` It would be more useful to be: ``` --- FAIL: TestTrue (0.00s) demo_test.go:9: failure demo_test.go:21: failure --- FAIL: TestTrueTwo (0.00s) demo_test.go:12: failure demo_test.go:25: failure FAIL FAIL command-line-arguments 0.395s ```
I don't think that's gonna run very fast, but maybe in 1.10... :D
Have you never heard of goldlang ;-)?
&gt; The code speaks for itself I'm not sure about that. It looks like the sort of stuff Go was designed to avoid: ws("${JENKINS_HOME}/jobs/${JOB_NAME}/builds/${BUILD_ID}/") { withEnv(["GOPATH=${JENKINS_HOME}/jobs/${JOB_NAME}/builds/${BUILD_ID}"]) { } } If it was Go, it would look something like: buildPath := strings.Join([]string{JenkinsHome, "jobs", JobName, "builds", BuildId}, "/") ws buildPath { withEnv("GOPATH", buildPath) } Far more readable! 
I sill don't understand why is this a problem. :P If you know what line failed, you know what function failed. Knowing what line failed is way more useful, at least for me, rather than function. The message you proposed could be better, but that doesn't me this thing is unusable. :D Still, maybe we can change/improve it in future versions. I don't know how it works if you have a lot of functions callings, nested function, or how it is called. Can't provide example or test now tho.
It's party, we got best Go release by far*. :tada: :tada: * every release is the best release. I'm wondering why I got forgotten tho. :/ xD Edit: Thanks for golds kind gopher redditor :D 
Why i need an abstraction? To be able to refactor at any stage without facing O(n) complexity (sometimes it's even worse). Compare changing 1 function and 200 functions.
Sqlx is useless as it only supports embedded structs and doesn't understand pointers to structs at all.
bye bye `glide novendor` 
Damn these releases are coming in fast. Wasnt the last one only a month ago?
I love Go.
Thanks for the article, it's a good read but my problem is different: how to pass multiple variables from request body to db-specific code without writing redundant code for Atoi, error handling, etc. Gorilla/schema can solve this if those variables are organized as struct fields. But sql.Exec need those variables one-by-one, comma-separated.
Has anyone tested yet how the performance of the concurrent map holds up versus just using some mutex manually? While the addition of a concurrent map is nice, I didn't really have many problems with maps before. Especially since the great updates to the race detector in 1.6 and 1.8.
Not really, that were release candidate versions. New Go releases come up every 6 months. [Release History](https://golang.org/doc/devel/release.html).
What solution are people using for installing several versions of golang at the same time on their system? I've mostly been using debian packages myself, but they lag a bit behind.
Why are newer versions of the compiler so much slower than 1.4? Seems odd?
Is the gold a flair, or are people really getting gold?
If I recall correctly &lt;=1.4 compiler was written in C ~~or C++~~, and then was rewritten in Go for 1.5. [Go 1.5 release notes](https://tip.golang.org/doc/go1.5): &gt; The compiler and runtime are now written entirely in Go (with a little assembler). C is no longer involved in the implementation, and so the C compiler that was once necessary for building the distribution is gone.
I haven't seen pip fail to talk to the packaging server enough to see it as a real problem, npm has been down far more than PYPI. I do however always assume that repository servers can be down so I always run CI configured with caching proxies for npm/pypi/gem/apt/... so that the outside world can't mess with CI jobs regardless of why. I don't really mind compiling and having the right libraries installed. I don't know how windows does it these days but the complete lack of standardization on library/source/headers/version is a really problematic design. Every time I install windows I start by installing mingw and the microsoft c++ toolchain and then some libraries but I don't use windows as a development workstation operating system so I have it easier. I think we need more well designed packaging tools good enough that everyone wants to use them: 1. C/C++ library manager which all other package managers can use for the .so/.dll/.h connection. 2. A functional way to define locations of packages. I have not tried it myself is nixpkg but the way it works is a very good step towards something simple and universal which can be shared among package managers https://nixos.org/nix/about.html . In this specific case it's not windows compatible but it's the way designs may be heading which is interesting. 
That would be great if something like that existed logging as well. C has __ FILE __ , __ LINE __ which can be used with macros.
I would not call 20% too much slower. Still a lot of work that can be done to improve this. The compiler was translated from c to go. Cleanup of this mostly automatic translation from c'ism to more idiomatic go has been done over many releases. Also newer compilers do more optimizations which involves more work. Compiling programs with different go versions can also be a moving target as they are also compiled with the newer runtime and std lib which can also contain more code to compile than before. Likely does not matter as much for large code bases.
The premise is flawed and the premise doesn't rely on poor syntax, Go doesn't need try in such form. And even more awkwardly, if it did have try, a catch should to go with it. And you'd still have to throw errors which means you'll have the same amount of code, all the `if err != nil { throw err }` don't really improve anything. The issue with the above: 1. chooses to return which means only named returns are possible, 2. doesn't somehow ensure that __err is passed forward (it doesn't just hide the handling of errors, it prevents actual handling of errors) The only nice thing about this example is just to see where you'd add a language based feature, preferably one that's more thought out. But then again, seeing how exactly none of them have a chance to be accepted upstream without an accepted proposal before any development, it's just some sort of self-satisfaction, to know that you *can* (but to lack the foresight to know that *you shouldn't*). 
Unfortunately there is slight binary size increase for some of my work ( &lt;~10%).
One thing I like a about Go is btw that a project almost always can be written either in pure Go which means that the only thing I have to do to secure that a reproducible build can be done is to check in my `vendor/` directory into VCS.
Asking the important questions
That's just wrong on so many levels... I hope you are trolling.
Good stuff!
Those are all valid points as to why Go is better. And to be clear, I used to be a JavaScript developer. Now I'm a Go developer. I like Go much better, for all the reasons you stated and more. But I think it's silly for us to assume OP's situation will absolutely call for Go just because we like it better for all the reasons you stated above. It really depends on what OP is trying to do. There's never a straight up answer to "Is X better than Y?" Pointing out the flaws in JavaScript and Node doesn't really answer OP's question, which is about performance. 
Is next 2.0 or is it 1.10... 1.99999 before that
Amazing. I love the concurrent map.
Congratulations on the type aliasing! a life saver
Well if you want to be abstract about it, literally any premise can be answered with "it depends". Go is set up for scaling and performance that doesn't require the use of external process management, thus being less complex (and relevant to OPs question). If you'll ever need that complexity is another thing, so whatever people tend to implement in Node or PHP still somehow seem to work, even if you're, you know, not as critical to all these things. Or somehow you managed to pick a better tool for some specific job - which I guess you also can't, without knowing all these bullet points :)
Correction: As CLR supports generics in the run-time, the type specialization is done at runtime. C# compiler only emits a generic type in IL.
Lots of people implemented some generic implementations of this to have typed maps, and literally all of them used some rwmutex, it's confusing even to see a sync.Map that's storing interface{}s. I know without generics there's really not a better way, but why include it at all... TBH, I'm more interested in `Mutex is now more fair.`? Who's the fairest maiden in this land?
i can't wait to see the updated garbage collector graphs!
[blue Gopher with original label](https://imgur.com/gallery/XvTHw) 
Before installing the new version on Linux, delete the old folder: sudo rm -rf /usr/local/go
[Pink Gopher original label + squishable brand label](https://i.imgur.com/1RrLHHF.jpg) 
So what? It's a PoC. It also doesn't track positions of injected nodes, doesn't modify the "go/parser" package and so on.
First, I think it's great that you wrote up a prototype. That is really valuable and much more effort than most people (me included) are willing to do. Thank you. :) I disagree, though, that this is in any way an improvement of Go. If all you do is pass on an error, you are very likely doing it wrong (there are a few exceptions, but they are rare). At the very least, you are going to want to annotate that error with useful information. Far more likely, though, you should replace it with your own, better error message. For example, I literally do not care which syscall failed when I run `go get example.com/foo/bar`. I neither want "connection refused", nor do I want `getting "example.com/foo/bar": fetching meta-tag: parsing "https://example.com/foo/bar": extract vcs-info: import prefix "github.com/foo/bar" is not a prefix of "example.com/foo/bar"`, nor do I want a stacktrace. I want an error message to the effect of `domain example.com does not exist`, or `broken go-imports tag on "https://example.com/foo/bar": import prefix "github.com/foo/bar" is not a prefix of example.com/foo/bar"`. (The *developers* of the go tool might care about a stack trace or more detailed debug information when a bug happened, but that's what a "-debug" or "-v" flag is for) *Yes* writing the code to produce readable, helpful error messages is hard and tedious. But that's because *it's code*. Constructs like these would have exactly one effect: People would think, that not handling your errors is blessed by the language, so they wouldn't handle their errors.
Thanks for the example - a caveat: if you run go build there, make.go is overwritten with a binary (due to folder name being make.go?). Maybe force a compilation error somehow in the case?
`__try` is an example. It cannot be `try` because it would break existing code. It could be `try!`, `try?` or even `?!`. However `__try` is good enough for PoC.
It's a code-rewriting "try", as in Rust. Something close to the Maybe monad. It has nothing to do with exceptions, catch and throw.
Why so much gold...?
Musta missed the freebies :( I'll be quicker next time! One minor thing. From the description of the book: &gt; This book is the destillation of more than a decade of web development experience \*distillation 
In my experience, and judging from results of `go/src $ git grep -A 1 "if err != nil {" *.go`, most of the error handling is in fact an error passing: `{ return nil, err }`. Also we use stacktraces in our production code (server back-end) and it shows us exactly what we (its developers) want to see from an error message.
godoc on golang is not up to date: https://github.com/golang/go/issues/21601
It's clear enough that it's quite useless if you want to handle `__err` and you should. I mean if you're throwing it away, which is exactly what's happening with the return, you can pretty much just write `res, _ = ...` and bleh.
Maaan, I have to run a spellcheck through this. I'm gonna give you a special link in the DM as a thank you :) * Also: maybe I can just pay somebody some reasonable amount of money for editing services? If somebody does that, DM me. Keep in mind that the book is in markdown, and *just* running a spellcheck through it is not exactly what I have in mind when I say editing :)
At a company where I work we have a pretty large number of projects, and as it turns out, everyone knows how to run `make`. So if you look from standpoint of larger OSS project I think it's very beneficial to provide a quick way to start tinkering with a project. If you have to install a large number of tools just to get started. From that, I kinda wrote the makefile that can be used for any project, for example, https://github.com/dz0ny/gobu/blob/master/Makefile.
[For the guy who keeps gilding everyone; a golden gopher](https://i.imgur.com/wm1lyVQ.png)
Looks like there's some good stuff in here. The test helper method is a nice to have to keep tests clean - the sync map looks useful, too.
Nice Makefile. You don't seem to be leveraging make's prerequisite tracking. For example `all` will do _everything every time_. How has that worked out for you in practice?
Thanks for the support, I will continue to improve it.
Be aware that it's untyped. `sync.Map.Get(interface{})` returns `interface{}`. I can't think of when I'd use it over normal map and a mutex.
&gt; most of the error handling is in fact an error passing "Being common" isn't the same as "being good". And yeah, that explains why so many people believe that this broken idiom (I still consider "error handling" a categorically wrong term for it) should be improved. &gt; Also we use stacktraces in our production code (server back-end) and it shows us exactly what we (its developers) want to see from an error message. Do you also send stack traces to people sending you a request? Because that's what an error message is for. There is nothing wrong with logging stack-traces - as long as they go to the debug-logs, where they belong. The developer of a package, the developer of an *importer* of that package and the user of a binary are all different audiences. Use debug-logs for the first, error values for the second and error *messages* for the last. Either way, don't just blindly pass on your error. (this set of audiences is of course only an example. Depending on your structure and use case you might have different sets of audiences. But that doesn't change that a) they in general need different ways to communicate failure to, b) those should use different mechanism and c) in general, error-messages should target humans with minimal knowledge of the intrinsics of the software and tell them specifically what *they* did wrong)
1.10 almost certainly
For what it's worth, at least the API is usable for implementation in your typed implementation. I'm confused as to why this even got into stdlib, esp. with all the holywar generics discussions in the last months.
That's true, although I think that this is more of an implementation detail that does not affect the concept of generics in C#. A cool consequence is that since the compiler only generates the generic version of the method the assembly does not get bloated with multiple "copies" of the same method (the downside of course is that the CLR needs to re-instantiate the method every time you relaunch the application).
It is [almost certainly not going to help you](https://www.youtube.com/watch?v=C1EtfDnsdDs).
Nooo I want the copy. Thanks for your points. I am really open to moving to a completely different language, and based on your comment, I might transition to Go!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/negativewithgold] ["Holy holy sh#$ is type aliasing huge!???I only just learned Go a couple of weeks ago so forgive me if..." \[-10\]](https://np.reddit.com/r/NegativeWithGold/comments/6vy8lo/holy_holy_sh_is_type_aliasing_hugei_only_just/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Yeah or with smaller projects I don't use anything 
I should have said Restful API...
As I mentioned "I only just learned Go a couple of weeks ago". My name here is "throwawaybeginr". Please explain why it's wrong for me to write code against queueType = int but then later change it to queueType = float32 ? In the current case I would have to change all of my code that I wrote in my queue (or any other) function, whereas in the case I suggest I can just change the one line. So please explain clearly why it's "wrong on so many levels", does not match best practices, and that I shouldn't do it. By the way someone gave me gold for the above comment - so either they really massively agree with me or thought that I was making a very, very funny joke. Well, I wasn't joking *on purpose* but maybe I am very wrong, so please explain why: as I mentioned I literally learned Go a couple of weeks ago, so be kind! Thank you.
I understood this perfectly. Yes, each of them should *modify your library* to change the type alias. Yes, you need to recompile everything if you change a type. This is why I said at the end of my comment: &gt; (Obviously this just reduces copy and pasting in certain specific cases, letting you keep the type information in just one part of the codebase - it's only a small part of what generics enable.) Can you read carefully the sentence above, saying "obviously this just reduces copy and pasting in certain specific cases, letting you keep the type information in just one part of the codebase - it's only a small part of what generics enable" and suggest how I could edit it to match my intention? What I meant to write was "Obviously this just reduces copy and pasting in certain specific cases, letting you keep the type information in just one part of the codebase - it's only a small part of what generics enable." The only thing I meant is exactly what I wrote: that it can *reduce* copy and paste in certain cases. After this addition: You write: " each of them should modify my library to change the type alias" Before this addition: You would have had to write: " each of them should modify my library by searching and replacing every single occurrence of that type. Well, maybe not EVERY occurrence, just the ones that refer to the part that needs to be changed. Maybe there are other uses of the type that are unrelated. So, you just have to go into the library you are modifying, click "find" on every single occurrence of the type that you are replacing, and decide if it refers to queueType or some unrelated usage of an int. For example even floats could use an int in the course of the queueType function, as part of simple iteration or for unrelated reasons. So you just have to spend a few hours, or days, or if it's very large, a couple of weeks, reading the library you want to change, and then find and replace each instance. If you misunderstand a particular specific line you might accidentally break the whole library. " So this is the sense in which it seems to me that the limited generic programming this enables - forcing you to change a line in the library you are including - is still a small subset of generic programming. Let me close by [quoting wikipedia](https://en.wikipedia.org/wiki/Generic_programming#Programming_language_support_for_genericity): &gt;Arrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs. I hope you understand that when you change the type of an array, you have to change the code all over the place. So it is not a general "generics" mechanism. The reason the article says "arrays and structs can be viewed as predefined generic types" is simply, because arrays and structs can be viewed as predefined generic types. The reason "type queueType = int" can be viewed as "generic programming" is because I can write a two thousand line library that does something very complicated with queues, and use the word "queueType" throughout. When you change "type queueType = int" to a float, my entire library might still work. Because it was written in a style that uses the word queueType. It can also use integers, in an unrelated sense. If you don't understand how this fits the definition of generic programming, then, given that I'm the one who has been learning Go here for a couple of weeks, not you, let's say one of us is wrong and you should teach me why I am wrong to consider it a small subset of generic programming. Certainly it is not a general Generics solution. I don't want you to put words in my mouth. Thank you.
I use [Go Version Manager](https://github.com/moovweb/gvm).
Great work! Earlier in my career I worked on automation that might have used this if it had existed. Perhaps I'll be writing that type of thing again eventually.
There's no way I'm ever reading a rant that long, sorry.
Hi, you gifted me gold. Could you explain whether this was because you were making fun of my comment, or thought my comment was a joke? The reason my comment was not a joke is because as I wrote, it reduces copy and paste. Let me be very clear. Before this addition, I might have copied one library into another, then have to do a find and replace. Only I couldn't replace every instance of "int" because maybe some of them are unrelated or doing something different. After this addition, I can copy one library into another and change a single line, to change the entire type of that library. Because in some sense it was written in a generic style. While it is not a complete generics solution, bear in mind that Wikipedia says "Arrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs." When you change the type of an array, you will have to recompile and so forth. Yet in some sense it "can be viwed as predefined generic types." In some sense I can write an entire 2000-line library around a queue, and have it apply only to integers. At the top of my library I can put `queueType = int` and someone else (or me) can change it later. Throughout my library I can use the word queueType and it will refer to just that part of the construction. In this way I can very clearly program an entire function or library using the word queueType without deciding the exact type that it will refer to. If you don't see that this is a small kind of what generics programming is, i.e. reduces copy and paste in some cases, then you are not seeing me: - Before: copy the library. Click "find" in your editor. Type "int". Decide for every single integer, whether it refers to queueType in a logical sense or is some unrelated or different use of an integer. - After: copy the ibrary. Click "find" in your editor. `queueType = `. Change the end it from an int to a float and see if it still works. Maybe keep clicking "find" for instances of queueType and see if the library makes other assumptions about it that break. But what you will not do under #2 above is replace unrelated usages of int. So I view this as enabling a very small subset of generic programming. If I'm as wrong as you suggest, then please correct my understanding.
See, for example: https://github.com/buildout/buildout/issues/211 and the advice to use allow-hosts=*.python.org Compiling libraries with C dependencies and other libraries that need installation, especially on Windows (which just never got any attention) was really just miserable.
It is simple and you can try it.
Please remove or edit your comment. My comment fits in 1 paragraph in Word - it's 200 words (1064 characters) and a code snippet. According to [reading times](https://www.google.com/search?q=average+reading+time) it takes 12 seconds to read with 80% comprehension. I get that you don't want to address my beginner comments (I mention that I just learned go a couple of weeks ago) but I think it's very rude of you to call my comment a very long rant - it's a little over twice the size of this comment. Please change or delete your comment.
Oftentimes you can use runtime.Caller(): https://golang.org/pkg/runtime/#Caller
&gt; haha 
Added 5 more copies, grab one quick. Edit: boy they Go quick. For the people left out, consider buying [it bundled](https://leanpub.com/b/golang-app-bundle) for $12. My wife buys books from this income, so go buy her a book.
[Here's a golden god](https://c1.staticflickr.com/9/8164/7593355284_029a33643e_b.jpg)
:)
/r/theydidthemath Fine, I'll schedule 12 seconds. [A short time later] &gt; and suggest how I could edit it to match my intention? You could remove these two phrases: &gt; Like, almost the same as generics. &gt; And they say Go wouldn't get generics :D Which are seriously misleading given your intention. &gt; Before this addition: [...] You would have had to write: [...] Oh god. This is horrifying. I get it why you are excited about the alternative, but it just makes Go slightly less broken. &gt; When you change "type queueType = int" to a float, my entire library might still work. ... what if you need a queue of integers AND a queue of floats? &gt; If you don't understand how this fits the definition of generic programming It's still not a generic queue, it's a queue that supports a single specific data type and you have an almost-not-terrible-but-pretty-dumb way to change it to another datatype. Because seriously, changing what the alias points to is in the end just a very fancy search and replace. &gt; given that I'm the one who has been learning Go here for a couple of weeks, not you, let's say one of us is wrong and you should teach me why I am wrong to consider it a small subset of generic programming. Considering that: - I've been using C# on and off for some years and I am currently working with it professionally; - C# has generics; - Go doesn't; please excuse me if I don't consider your 2 weeks of experience with a programming language that doesn't have generics as a valid experience in programming with generics. 
 /r/theydidthemonstermath [^^^Contact](https://www.reddit.com/user/alienpirate5)
Edit: &gt;it just makes Go slightly less broken. This is an accurate summary of the spirit of my comment. I will put it at the top of my post. I agree with everything you've written.
Peace, bro :) &gt; I am currently working with it professionally and if I want to keep my job, I'd better get back to it. 
Someone who really misses writing releases to golden cd's
:)
I hope this little tool helps you. If you have any questions (or problems of course) I'm here to help!
If you look at the source code of the sync map, it's doing far more than a map with a mutex around it...
I was convinced it was a fairly adorable troll, add to that now elaborate lol. If your being serious, this feature doesn't enable Generics in any sense of the term. No more than your code editor does, I.e. :%s/queueType/int/g Edit: I did not give you gold to mock you or reflect my perspective. I gave you gold as I did everyone before I went to bed just as I've done in past releases because I think it makes the release more fun. I also feel I did not personally attack you- I didn't call you a troll I said I was convinced it was. I still feel you could be trolling but if you are genuinely excited about that feature than I apologize. Though I would suggest if you're passionate about Go you avoid using Aliases for use cases outside the spirit of their design: refactoring code. What most gophers value about Go is the limited feature set which prevents having to explore their unusual experimentation during other people's learning exercises. When I see a alias I should immediately think this type is migrating and I should use the type it points to. Tooling should eventually do the same. If you use it a different way than I and anyone else has to become familiar with your usage. Edit2: I see your edit, I'm not accepting your ultimatums and I am not striking through my text. I am instead choosing to exercising my right to disagree with you on both points. * 1) I did not personally attack **you**, I raised question to your **ideas** which is well within my right in a forum dedicated to exchanging opinions in public setting. My original reply was to your post when it said "this is like generics" which was false without room for objection. This under a downvoted throw away that said "who said Go won't get generics" at a time generics is a heated source of debate here. Under this context my replies are not unreasonable nor is my apprehension to taken you seriously. If you want serious responses post in serious tone. * 2) I am not mistaken on the points of your post, I disagree with them on multiple fronts even after they have been edited. You can claim the alias feature is a form of generic programming and I am genuinely happy if you find a lot of value in it, but computer science does not support your claim and I choose not to as well. I won't reply to you further we can disagree from here. My apology from the first edit stands if you were being genuine- not because I feel my actions made it necessary but because in the off chance you are not being disingenuous I don't want to discourage you from your journey to Go. Happy coding. 
So good to see that they removed vendored folders from the `dotdotdot` commands!
hello automatic and audited testing! &gt; Env &gt; &gt; The new go env -json flag enables JSON output, instead of the default OS-specific output format. &gt; &gt; Test &gt; &gt; The go test command accepts a new -list flag, which takes a regular expression as an argument and prints to stdout the name of any tests, benchmarks, or examples that match it, without running them. Automatically run the test, output to json, send it to the elstic stack for easy reporting.
Well thanks! That one managed to stay below my radar.
Well if the c compiler then existed today with the same sophistication in the toolchain then it would be much slower I imagine. The SSA backend and all the optimization passes didn't exist in 1.4.
Awesome. sync.Map would help me a lot. Can't wait for Go 2
If ur gonna handle it yes you should. But how many Go libraries do that for every error ? (Poor man's stack trace). 
No, I don't want anything to automatically get new source from the internet. That's why `go get` is a separate operation from `go build`/`go install`/etc.
To answer the question of "Is X language Object Oriented?", envision a sort of object orientation scale, where to the very left is "pure object oriented (everything is an object, everything is message passing)" and to the right is nothing is an object, and no message passaging. Most languages lie somewhere in the middle. Java for all it's fame in OOP is probably left-center of this scale. Ruby would be more left than Java, and Smalltalk would be the canonical example for purely object oriented language. Is everything in Go message passing, and is everything represented by Objects? Obviously not. Where does it lie on the scale? Somewhere on the right, for sure. **However**, the *principles of Object Orientation code structure / problem solving* can be applied to Go using existing language mechanisms. I personally think if you approach Go and treat everything like an object nail with your OOP hammer, you will find yourself fighting the language in some situations or doing things that are anti-patterns in the language. You will be building almost your own DSL inside the programming language. 
I am so excited to try this tool out. I have wanted a solution to this problem for some time; thanks for making this!
Oh that's a weird way of using "bounded". It sounded like you thought bytes.Buffer had a max size it refused to go above. bytes.Buffer is a "write data in, read data out" buffer, think of it like a buffered pipe not a file-like object.
I remember reading a few years back that popcode intrinsics weren't always faster than doing it in normal code. I also very vaguely remember there being some issue with popcount related feature flags on at least one generation of Intel CPUs that made it hard to determine whether or not there was popcount support.
I agree 100%
&gt; I think it's silly for us to assume OP's situation will absolutely call for Go I'm not sure what you mean by "absolutely call for Go" means. It's not exactly the nuclear option...
I answered OP's question: &gt; Does golang provide better performance for backend api end point than nodejs? OP didn't ask "should I use NodeJS or Go?"
It got in so it could be used by other bits of the stdlib.
While that's true for scale, it's NOT true for latency. Tons of evidence says latency is a critical ingredient for end users...
Someone is *really* happy about 1.9.
Thats the exact same thing I have said in the article. "Go is not a pure object oriented programming language". " In the upcoming tutorials we will discuss how object oriented programming concepts can be implemented using Go."
Hey, docker engineer here. Can't say I've touched this exact API but educated guess is it's a tar file of the directory you want to use when you "ADD", COPY" etc in the Dockerfile. For example, if I have the line "ADD config.json /var/lib/foo/config.json" the first path (the "from" location) is looked up in what we generally refer to as the build context. 
Thank you, I'm glad it helps you! This issue has been bugging me for a while as well and I had some crazy shell scripts running which didn't work on windows, obviously :)
That's very true. I don't disagree. What I'm saying is that what's even more important than latency is actually having the application and that works correctly. It's better to get something implemented first, and then solve the performance issues when you actually know you have them, than to try fixing latency issues that don't exist yet by using tooling that you aren't familiar with.
You said it was brutally efficient and well suited for it. There's no doubt that it is. That's why I love it. What you didn't say was that it's faster, which is what his question was, because you can't possibly know the answer to that. OP didn't even say what he was trying to do.
That's a valid point, I'll make it require a flag.
I mean that we can't assume his use case will be worth him using Go over Node.
We don't need to, he didn't ask for that. &gt; his use case will be worth him using Go Why the word "worth?" Does Go have some hidden costs -vs- NodeJS that I'm unaware of?
There's nothing abstract about "it depends." It really does depend on what he's trying to do. The question is open-ended.
I'm not sure what you mean by this. He said he was developing an API -- which I assumed (my bad, but I was right!) meant HTTP API. &gt; What you didn't say was that it's faster &gt;&gt; 2) request latency, average and standard deviation &gt;&gt; &gt;&gt; 3) throughput, particularly on a multi-core system 
&gt; Does golang provide better performance for backend api end point than nodejs? If you stick either in front of a database that crawls that needs to be accessed over a spotty VPN and communicate with three other services to help aggregate its data, they are both going to be slow. Go might not be better in that case. That's why it depends.
You get gold! You get gold! Everyone gets a gold! 
&gt; What I'm saying is that what's even more important than latency is actually having the application and that works correctly Not in all cases. If we're offering our wisdom about what's most important for OP, rather than simply answering his question, I'll counter your assumption that working-and-correct is more important than working-and-correct-and-fast/quick by suggesting perhaps he has needs you don't fully understand.
&gt; I'll counter your assumption that working-and-correct is more important than working-and-correct-and-fast/quick by suggesting perhaps he has needs you don't fully understand. Read the thread man.
&gt; That's why it depends In that case, it's OP's responsibility to ask a better question. You're stretching to make your point. Again, I just answered his question.
I have. OP has added zero details.
https://github.com/OneOfOne/genx/commit/8b1d42a39496d67a63082a94e72c6eb1111dcfb5 there ya go.
I'm not stretching to make my point. My point literally *is* that OP needs to provide more details, because which one performs better depends on his use case. If anything, you're the one stretching by making assumptions (which you already have admitted to making). I haven't assumed a thing.
*Exactly*
&gt; how many Go libraries do that for every error That's something we should aspire to improve upon, not exacerbate. That said, this is a cool PoC, thanks for doing the work!
Is it impossible for you to imagine simply answering OPs question, rather than treat him like a child?
And you've not answered his simple question either, which **absolutely** has a general, simple answer.
I don't think telling OP "it depends on your use case" is treating them like a child. I think answering honestly by saying that there isn't a silver bullet is actually treating them like an adult.
It doesn't though, because performance depends on the use case.
coz gold is made from go, I see myself out
Mind elaborating what's different than just an underlying map[string]interface{} and a mutex? I didn't look at the code yet but some comment made me think that I should.
epiris, the reason you are mistaken is as follows. (1) We take my fully given example. I gave it in full! queue := make([]int, 0) // Push queue := append(queue, 1) // Top (just get next element, don't remove it) x = queue[0] // Discard top element queue = queue[1:] // Is empty ? if len(queue) == 0 { fmt.Println("Queue is empty !") } You can consider this being in a function, or whatever else we like. Next we make it a little bit more complicated. It doesn't really matter what the complication is, this is just an illustrative example. For our example, we will simple keep track of its length in a dedicated variable called myLength. queue := make([]int, 0) // Push queue := append(queue, 1) // Top (just get next element, don't remove it) x = queue[0] // Discard top element queue = queue[1:] // Is empty ? if len(queue) == 0 { fmt.Println("Queue is empty !") } var myCachedLength int //tracks length, only updated manually // update length (call this manually) myCachedLength = len(queue) Why might someone do something like this? Oh, I don't know, can you possibly imagine a tree that is expensive and traverse the elements of? If not, then can you please read this article: https://en.wikipedia.org/wiki/Binary_search_tree Above I completely implemented a queue. Can you imagine me implementing a more esoteric data structure? No? Well, it doesn't matter if you can imagine me doing it, because we are doing generic programming here. (2) You claim: No more than your code editor does, I.e. :%s/queueType/int/g But before this change, `Go did not have the possibility of defining queueType to mean int.` Therefore before 1.9, if you wrote a program with queueType, ***it wasn't a Go program until you ran:*** No more than your code editor does, I.e. :%s/queueType/int/g It simply wasn't Go! Is this a Go program? ###this package is:main require "fmt" because you can write :%s/###this package is:/package/g :%s/require:/import /g So, really, Go included both the words "require" as a synonym for import, and the more verbose "###this package is:" as a synonym for package? No. You must present a valid Go program, not external search-and-replace shenanigans. So, ***if we start with a valid Go 1.8 program***, then your external search-and-replace shenanigans result in this: var myCachedLength string //tracks length, only updated manually // update length (call this manually) myCachedLength = len(queue) This is a type error (the mistake in the code is at the first line, which now defines the cached length as a string, and then it gets triggered at the third line which is illegal). The function is now broken. This applies to my example, but it doesn't apply to yours because in *your* example you were already generically programming albeit in a limited form, using your external tool to let you do so, and not writing valid Go. But if we removed this and start with a valid 1.8 program, then you had to use this to replace it: No more than your code editor does, I.e. :%s/int/string/g So what Go 1.9 did is move the limited form of generic programming ***that you were doing*** from your code editor, into the Go language. In fact, I will go farther: In your example of writing an illegal queueType which was not a valid Go program, then using search and replace in your code editor (or an automated tool) to change it, ***you were engaged in Generic programming.*** This is because you were writing code without regard for its type. The difference is, in Go 1.9, you can write ***valid*** code without regard for its type. In Go 1.8, you had to write ***invalid code, that wasn't Go*** to write code without regard for its type. This is the change I refer to. Your claim is like saying that adding "require" as a synonym for "import" does not expand the language to include the word require, because it was already part of the language if you wrote using it and then did a search and replace. Bear in mind that I have only been learning Go for a couple of weeks, so I may have Syntax errors above, and my point may be wrong. I am open to the idea that I am wrong, if you express it clearly. (3) I am unhappy that you have attacked me as a troll, then an elaborate troll. This derails the discussion. Another person, who uses generics heavily in C++, already started off by disagreeing then amended their [point to say](https://www.reddit.com/r/golang/comments/6vuhkz/go_19_is_released/dm3yl92/): &gt;&gt;Before this addition: [...] You would have had to write: [...] &gt; Oh god. This is horrifying. I *get it why you are excited about the alternative, but it just makes Go slightly less broken.* They saw my point. It's just a limited form of generics programming, that saves *some* copying and some search and replace. I have [amended my comment](https://www.reddit.com/r/golang/comments/6vuhkz/go_19_is_released/dm3eqs6/) to make this blindingly clear. Likewise in order not to derail the discussion please do the following: 1. Add strikethrough to your comment where you call me a troll. (A personal attack.) 2. Add "I was mistaken and now understand OP's example." Certainly instead of 2 we can continue this discussion. But you must perform 1 as it's very rude of you to attack another Go user this way. As you have seen, I also do not accept your point. Go 1.9 now includes a very limited way to write a valid Go program using a somewhat generic code technique, that before could only have been done through copy and paste or search and replace. Now the starting program can be valid, and by changing the type definition, in some cases it can remain valid. This is huge in my opinion, of course you and everyone else here are right that it's extremely limited - but it certainly (100%) meets the definition of adding a very limited form of generic programming to the language. Please amend your comment as I have requested before replying. I will message mods about your calling me a troll, otherwise. That is a personal attack. Then I am happy to continue this discussion with you.
**Binary search tree** In computer science, binary search trees (BST), sometimes called ordered or sorted binary trees, are a particular type of container: data structures that store "items" (such as numbers, names etc.) in memory. They allow fast lookup, addition and removal of items, and can be used to implement either dynamic sets of items, or lookup tables that allow finding an item by its key (e.g., finding the phone number of a person by name). Binary search trees keep their keys in sorted order, so that lookup and other operations can use the principle of binary search: when looking for a key in a tree (or a place to insert a new key), they traverse the tree from root to leaf, making comparisons to keys stored in the nodes of the tree and deciding, based on the comparison, to continue searching in the left or right subtrees. On average, this means that each comparison allows the operations to skip about half of the tree, so that each lookup, insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/golang/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.26
I wonder if it works with helpers more than one level deep.
This is actually really useful. Great explanation, thanks!
[gocov](https://github.com/axw/gocov) might also be useful for those interested. The [gocov-xml](https://github.com/AlekSi/gocov-xml) helper is very useful to get coverage report in CI or other places that support the Cobertura format.
Difference being the discussion for what Go 2 is going to be, has started. So the "never" from that talk is no longer valid.
PoC? Whats that? Stack traces are one of those things you should let the runtime do for you. You only wrap errors providing more information when you need. It's a pity Go does not support stack traces natively for errors although its runtime can do that. Now, you will probably point me to panic but most gophers use errors and panic only when it's a truly exceptional situation like programmer atupidity, out of memory and so on.
Thanks for sharing your project! It looks like a great example and well structured. I'm new to `go`, so this is very helpful. Looking through the "repository" folder, I notice you're manually creating prepared statements in the update/insert queries. Can you explain what the benefit of that is compared to just calling `db.Exec(...)` with these one-time-use statements?
List comprehensions are the one thing I miss from my Python days. They would make me very happy in my trousers. 
Ah ok thanks! 
Yes, nothing is 100% certain. Perhaps you should point out it's all irrelevant since a meteor may destroy the planet before he launches. 👀
I didn't know a meteor obliterating us all was a use case.
OK, i have figured it out. As always the decision is in using **closures** (really, i did this trick so often that i start to think Go interfaces are pretty useless (unless for decoupling packages) ). Anyway interfaces are gradually disappearing from my project. If someone cares, the new code: https://play.golang.org/p/HSMO_XdCN3 
Pardon my wording then. I agree about bytes.Buffer, that's why I said a type with similar api but with different impl would be a better fit for real usage that OP's WriteSeeker.
Thank you for this. Can you please read [my other comments](https://www.reddit.com/user/throwawaybeginr/). Can you reply to those comments (in a reply to this comment)? Thanks.
[How to Ask Questions](http://catb.org/~esr/faqs/smart-questions.html). It's generally good stuff, and if perhaps some of it rubs you the wrong way, well... to some extent that may be because you are the one who needs to be reading the document. :) I've actually been relatively pleased with at least the people posting on /r/golang; there's a lot of "how do I do {{ .highLevelGoal }}" here, and a good amount of posting of real code, and not a lot of the typical "how do I do this small part of the crazy plan I'm not telling you about where in fact there's an easier way to do the whole thing that you can't guess because my question is too specific?"
Really great! I've been looking forward to these changes, especially the type aliases. Thank you Go Team!!!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/negativewithgold] ["If you want to make it harder for you to debug issues, sure, go for it. As an example: https:\/\/play.golang.org\/p\/heF7hr9\_24..." \[-10\]](https://np.reddit.com/r/NegativeWithGold/comments/6w0c5v/if_you_want_to_make_it_harder_for_you_to_debug/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Interesting you use the phrase "lazy programmer". I actually consider that a very important part of developing code (see [Why Good Programmers Are Lazy and Dumb](http://blogoscoped.com/archive/2005-08-24-n14.html)). This is just a balance of what you think is enough for your use cases. Some languages (like Idris or Coq) have (dependent) type systems that even let you have compile time checks that your indices are not out of bounds. Personally, I find that reasoning about the types of things (including generics) lets me have a deeper understanding of what is going on. All the way at the end of that spectrum are dependent types, where you sometimes have to prove to the compiler some property or invariant holds in your code.
If the compiler is written in Go and to use the compiler you need to compile it using itself then how does someone without the Go compiler compile the Go compiler?
Yeah sorry I wasn't implying you didn't say it, I just think a lot of people get confused between the two concepts -- object orientation as language mechanics, and object oriented principles or design, which is this whole M.Fowler/Chad Fowler/et-all style development they have dominated consultancy gigs with for the past 2 decades.
Since you're talking about REST API performance, I'll add my "it depends" hat to the ring. You can look at the tech empower benchmarks (https://www.techempower.com/benchmarks/), but here are some things to think about: * REST API performance is significantly affected by latency * Doing most useful things in an API is affected by DB and latency to other services * The first connection to an TLS enabled endpoint gets a lot of overhead establishing the connection. Costs decrease for subsequent queries where connections are reused. * Using a CDN and creating an API that enables aggressive caching can have a huge effect * Using a distributed database can have an effect in cases where there are results that cannot be cached (private api calls etc) and there are compute resources used through geoip balancing. * If you have to do a lot of computation, a truly multi-threaded language will have a larger advantage 
Except you, apparently.
Damn, maybe next time 
Definitely, you can use go-acc (the tool from the blog post) together with gocov for comprehensive and complete coverage reports :)
It's a shower of gold! A veritable golden shower!
Do we have gene... I mean, woohoo \o/ 
Oh.
I don't know technically what's different but I remember a few of the patches to use sync.Map resulted in performance improvements for some reason, and sometimes more readable code.
How one compile gcc toolchain without c/c++ compiler ;-) A classic chicken-egg problem. I'm not sure how go toolchain does it but here you have example using gcc: http://gcc.gnu.org/install/build.html
You are correct that you can not write a compiler in Go without having a Go compiler. The solution to this chicken and the egg problem your referring to is called **compiler bootstrapping** if you want some reading material.
The main point of this patch is improving how code looks, so choosing a really ugly PoC is a bad idea. Also you want a lot more examples, Eg. how do I pass the err up the stack and what about any other return values? Maybe show a before/after of a couple of real Go code examples (anything big on github would be fine, IMO). Edit: Also a comparison vs. errWriter: https://blog.golang.org/errors-are-values
If you don't already have a Go compiler on your system, you need to download [1.4.2 first](http://golang.org/s/go15bootstrap). 
PoC being proof-of-concept
I mean, this exact same idea has been suggested at least a dozen times before, so defending your implementation by saying it's just a proof of concept doesn't inspire confidence.
Hey SkaterDad, Im glad you like it. Im coming from PHP and are used to prepared statements, there is also a security aspect to it.. but long story stort under the hood db.Exec, db.Query does a prepared statement under the hood for you as far as I know so not much of a difference. Im new to Go as well and I could be wrong but what I have found on Google about the difference between db.Exec, db.Query and Prepare is they all do a prepared statement under the hood. What is your understanding of this? Do you have any projects on Github/Bitbucket?
Thanks Go Team. When I saw [net.Buffers](https://golang.org/pkg/net/#Buffers) I thought they were new. Turns out they were added in 1.8. What's the use-case for this type?
`errWriter` is a joke. This idiom doesn't work in real code when you'd have to write `errFoo`, `errBar`, and `errBaz` wrappers in every function, `errLog` included.
You're free to contribute, as the patch shows - it's fairly easy. 
I'd rather work on any of the things that I'm already working on, than on a misguided and incomplete solution to error propagation.
There's a word next to never.
LOL was meant to be rather open ended discussion because I am an indeed Go novice 
That's my understanding also, that `database/sql` automatically does prepared statements if the db driver supports it. This is partly why I asked about your code explicitly preparing the statements. I suppose one benefit is you can catch query errors before trying to execute, to isolate if your SQL is the issue or the inputs? [The code](https://golang.org/src/database/sql/sql.go#L1265) isn't the most intuitive to read thanks to short names for everything, but [ctxDriverPrepare](https://golang.org/src/database/sql/ctxutil.go?h=ctxDriverPrepare#L13) is called in each of the Exec and Query functions.
Most of reddit people treat people with less experience as children, hence that's why they are on reddit 
7PM PDT happens when this comment is 4 hours and 59 minutes old. You can find the live countdown here: https://countle.com/3Q441276R --- I'm a bot, if you want to send feedback, please comment below or send a PM.
My comment was in no way intended to be negative toward you.
_My comment was in_ _No way intended to be_ _Negative toward you._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^tmornini ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
Yes, it's an uncompressed tar stream of the filesystem contents at the root of the Docker build (in other words, everything that would be added in a `COPY .` Dockerfile command) https://github.com/dollarshaveclub/furan/blob/master/lib/builder/builder.go#L384
"The code isn't the most intuitive to read thanks to short names" Im trying to adapt to this as well in my project im used to camelCase stuff like e.g. in my router I have uc short for usercontroller but yeah I guess using the long term would make it easier to read (maybe) Well I would always validate my inputs before doing anything DB related :) the .Prepare should handle everything but e.g. sending a user registration with a email that we know already exists doesnt make any sense because it will fail thanks for links to the official db lib 👍 Do you have a Github?
Hey, I just tried `go get -u github.com/ory/go-acc` but a binary is not installed so I can't run the command. Would love to try this tool though!
I love it more!
I have to say, I love Go, but the go subreddit have got some of the most toxic people on reddit. That being said, that's very interesting, a lot of time I really wanted to do something like, even have the always fun `ret := func(err error) (xx, xx,xx error) { return nil, nil, nil, err }` at the top of the function to do that. I know this will never get merged, at least in Go1, but you've done an excellent PoC.
Indeed, running `go build` replaces `make.go` with the compiled `main.go` binary. I hadn't noticed that before! It is just an unfortunate coincidence because the project name/folder name is `make.go` and the script (guarded with `// +build ignore`) is also named `make.go` so the compiler ignores it and replaces it. I don't think there is a way to force a compilation error as this is how the go toolchain works. In any real case scenario where the project will have a different name I don't think that will be a problem. Nevertheless if there is a solution that doesn't involve changing the `make.go` name, I'll be happy to add it. Thank you for reporting this!
Oh that's cool I never noticed somehow. It says it uses the writev syscall if it's available which makes sense because the signature is a slice of slices. I haven't seen the net.Buffer implementation, but the Linux writev syscall is for vectored I/O which allows a single syscall to read from or write to a single Buffer, and write to or read from multiple buffers sequentially and atomically. You can probably extrapolate that to potential use cases.
That's weird, did you add $GOPATH/bin to your $PATH?
Constant performance improvements and sync.Map are the highlights for me! Huge thanks to everyone involved.
yep, no binary gets put in $GOPATH/bin and the $GOPATH is set correctly. Just used a fresh install of go-fuzz to test
If you do go by variable name, just use `err`. It's already conventional. No need to make it a keyword: just have `try` apply to return values called `err`. It won't break any existing code, because no existing code uses try. I would actually have it apply to any return variable of `error` type, but I understand that's much more complicated than just hooking into the parser. Some other ideas: *Allow/require specifying annotations/wrappers.* // e.g. Transform into fmt.Errorf try "Couldn't get widget: %s" http.Get("http://yo.yo") It's generally considered bad form to just blindly return errors. Everyone does it though, because it's the easiest option, and it achieves the *fundamental* goal of stopping the code when there's an error. So, why not make it so that something not as bad as blindly returning errors is easier? Then, people would do that instead. :) *Something with switch?* switch widgets := try getWidgets() { case sql.ErrNoRows: try errors.New("Not as many widgets as expected") case sql.ErrTxDone: try errors.New("Database failure while extracting widgets") case nil: return widgets, nil } 
It's because `main.go` says `package accurate_code_coverage` but it should say `package main`.
[xyproblem.info](http://xyproblem.info/) is a useful link to remember when explaining this to people.
👍🏻
The -list flag is for test discovery for large multi-language test systems. JSON output for tests are have been proposed but i haven't seen code for it yet.
It looks like the root package is not "package main", so it can't be built into a binary.
If MoveMouseSmooth works as advertised, I could definitely see myself using this along with a screen capture tool to make a snappy product demo gifv. I'm always annoyed by jerky, impercise mouse movements in [normal screen capture product demos](https://img.labnol.org/di/tweet-bookmarklet.gif). 
I can see where you coming from, but the problem is - if I need, for example, binary tree for both int and float implementations - I will be forced to duplicate the source and edit type aliases. Code generation became easier, but it is still not a proper solution for a language generic facilities. For example - there will be difficulties constraining the type. If I don't want user of my code to use it for pointers or structs which have pointers - because it will break equality operator - how can I achieve that? 
`sync.Map` is poorly named imo in that it is specifically optimized for cases where you have a high read:write ratio. The closer you get to a balanced number of reads and writes, the more you should start looking for another solution. `sync.Map` is not a generic concurrent map, which is what I originally thought it was based upon the name... until I looked at the code.
come up with actual testcase that exhibits the strange behavior (assignment changes value of right hand side).
The only thing this seems to be doing is combining multiple files, skipping the first line of all but the first file. And the code kinda stinks, for that. What is everyone hyped up about?
Just upgraded today on my local machine, so far no issues! Would love to see the alpine linux package updated to go 1.9, I might even do the PR to the aports repo but they look to be... a bit behind in their merges.
I have the actual compiler which I am writing.
I get to throw out my hacky architecture-specific bit twiddling!
Are there other goroutines running?
I have not started any with the "go" keyword, the program should be running sequentially.
Don't use `println`. Use `fmt.Println`, see the comment on https://golang.org/pkg/builtin/#println
Yea, It's for debugging purposes, the code does not normally have println
But we can't see your actual code, and the example you've given us works properly. This is not an actual test case because it doesn't exhibit the behavior you're describing. If you can't reproduce the bug reliably, there's nothing we can do to help you diagnose what's happening. EDIT: Thinking about it a different way, there's something different between your code and the example you've given us. Did you write the playground example from scratch, or did you copy-paste your code in and it magically worked? If the former, then likely you have a typo in your code.
Great talk by /u/peterbourgon. Its an updated version of Bryan Boreham's [talk](https://www.youtube.com/watch?v=yCbon_9yGVs). More of this pattern can be seen [here](https://github.com/weaveworks/mesh). Fun fact, you essentially get this pattern for free using Erlang's [gen_server](http://erlang.org/doc/man/gen_server.html). We first implemented our custom protocol server using Erlang and it's great, however the cpu utilization was off the charts. A prototype/rewrite in Go using this pattern yield significant performance gains. I usually make fun of Go's warts but as /u/bradfitz said it's a [get shit done language](https://www.youtube.com/watch?v=sYukPc0y_Ro#t=10m40s).
i mean the clear path to debugging this is finding assignments of the subtype 'list'. it seems that ic.ExpressionType = t is just a red herring. Something is concurrently modifying the SubType pointer and you just so happen to have noticed it at this location.
There is only one goroutine running. The only concurrent logic would be from the Garbage Collector.
The livestream was fun to watch. Thanks a lot for making it happen!
I love listening to Peter talk, but every time he says "you've all seen this", and I have no idea what he's talking about. I need a version of his talk for dummies.
right, sorry about that, looks like my IDE did something stupid while refactoring it. It's fixed now!
It says in the readme what it does and why it does it. If you think you can do a better job why not create a PR instead of complaining on reddit?
Thank you, it was a refactoring accident caused by my IDE - fixed now :)
if you try again it should work :)
I think the more important point is the last half sentence of the `println` doc: &gt; it is not guaranteed to stay in the language. I usually use `davegch/spew` for quick, readable data dumps. Easy to add, easy to remove (as, like with `println`, you would not use `spew.Dump()` for anything other than debugging.) Also a great debugging help: [`kr/pretty`](https://godoc.org/github.com/kr/pretty), a pretty printer that also can diff data structures. (Buz, if you read this: Thanks for pointing me to this one.)
it seems you have already figured it out, but yeah the next step would be to check your pointers for shenanigans.
Learned a few things from the profiling talk.
Fantastic release!
Try to adjust the speed.
Any new libs that use the testing.(*T).Helper? 
It is supposed to be that way, the first run should give you some result. But if you want something specific, you can just run that (build, upload, test). But I agree parts could be optimized :)
Looks to me like entire app :D Not just to make a reproducible build. How is this helpful for novice users or even developers?
Just want to say, the commentator that I got into an argument with is /u/hell_0n_wheel. Deleting comments = cowardice!
This thread is a gold..congrats for 1.9 release 
&gt; I have to say, I love Go, but the go subreddit have got some of the most toxic people on reddit You haven't seen r/programming then.
I've recently been working on a game in Go, and it bothered me that with Go's popularity, there were still no bindings to the awesome C++ GUI library, imgui. So I decided to make my own. (or at least try) At first I was going to use cgogen to generate bindings to cimgui, but cgogen didn’t like how it was a C++ project using C linking. I eventually settled on using SWIG to generate bindings to the C++ library. This the first time I’ve ever actually made bindings, so **expect bugs**. Everything may explode when you use it. If you find any bugs or issues though, please report them! You’re also welcome to contribute. I’m also working on a rendering backend for it, using SDL 2 and OpenGL 3.
All three talks were great. Thanks for streaming.
I think the issue is that := will cause the if scope to have new variable named metric that shadows the outer one
The `metric` in the if statement (as in the `if metric, ok := ...` one) shadows the variable above it, so when you assign to `metric`, you're not overwriting the one you're trying to. You don't actually use the `metric` variable you look up, so you can replace it with a `_` to discard it. (Or, you can make use of defaults, and check `if found := metricMap[metricName]; found == nil`, but using `ok` is more clear to a reader.) https://play.golang.org/p/KeX1EVeR4S
Damn it...I thought I was assigning the outer metric in the if statement not creating a new one. That does make sense though thanks.
I saw this on Golang News and found it interesting because it actually documents a use case where type checking at runtime and code generation don't work well. Most of the discussions in the past have been hypothetical, while this is based on actual experiences rather than what ifs.
The only thing: go-bindata seems to be a dead project by now. Several issues and pr are stuck since 2 years. jteeuwen haven't committed or reacted to anything since 2015.
&gt; At the very least, you are going to want to annotate that error with useful information. The [rust version of this](https://github.com/rust-lang/rfcs/blob/master/text/1859-try-trait.md) the `?` operator implicitly calls `Try::from_error(From::from))` before returning which allows you to build up conversions of error types to your local error type by implementing the `From` trait. Of you course you can expliticly handle errors at the call site or do one off conversions using convenience functions like `map_err(|| /* conversion */)` before using the `?` operator. To me this strikes a nice balance. All erroring calls are explicitly marked with the `?` operator, conversions only exist for what I explicitly supply so that lower level error types don't leak up the stack like exceptions. I think you can implement a more simple, open ended version of this using something like [`errors.Wrap` from from `pkg/errors`](https://godoc.org/github.com/pkg/errors#Wrap). Though without generics I'm not quite sure how to thread in those conversion helpers easily. Maybe something like this: // every module that wishes to supply converters provides this func (same type) // if they don't sub it some default one // maybe some way to make this an iterface? // what if instead of a hardcoded function name you had some kind of transform comment? // eg, go:generate Try.wrapError func wrapError(err error) error { // convert } // original func DoIt() error { foo := try pkg.ErroringThing() // ... } // desugared func DoIt() error { foo, err := pkg.ErroringThing() if err != nil { return wrapError(err) } } Integrating having a magic function in a package as part of the language is bizarre, generics could get around this their might be some interface tricks that get you there.
Check out https://github.com/dave/courtney ... Merges coverage files but also removes error return blocks...
There is also tegola which serves MVT tiles from PostGIS and is written in pure Go. https://github.com/terranodo/tegola and http://tegola.io 
chroot
Yes, interesting post and a very valid point. What I like, there is no suggestion on how to implement generics in Go. Kudos to Sameer. That said, may be we should move away from generics and invest in traits for basic and user defined data types. Traits align with Go's philosophy of behaviorial composition. Yeah I just did what I said we shouldn't, sorry. :(
There shoudl be also an else statemenr in case it returns an error doe..
How would you envision 'traits' to look like in Go? Genuinely curious...
Cheers for the links. I mainly posted this project as I had it knocking around it bitbucket for ages and thought it ought be of use. I'll go check those out
It's sentences like these that scare away new programmers.
Go.rice unfortunately seems to suffer of the same. Seems go-bindata was stable-er enough in my tries :)
It's best not to rely on other packages to test code in this package. Then if you delete the other package or use this package from another executable without the test package, then you won't be running those tests. 
Rob Pike probably has a budget to spend on stuff like this on every release.
&gt; it requires the user define a new key type for each metric Yet a little earlier, you were willing to vomit static Counter2&lt;String, Long&gt; httpRequests = MetricFactory.getDefault().newCounter( “/myserver/http/requests”, new Metadata(“HTTP request count for MyServer, broken out by HTTP method and response code.”), Field.ofString(“method”), Field.ofInteger(“code”)); Defining a Go struct type reads so much nicer to me.
This reminds me of [zap](https://godoc.org/go.uber.org/zap)'s [Fields](https://godoc.org/go.uber.org/zap/zapcore#Field). With that inspiration, the example could look like httpRequests.Add(1, metric.GET, metric.StatusNotFound) or httpRequests.Add(1, metric.GET, metric.Code(404)) or httpRequests.Add(1, metric.Method("GET"), metric.Code(404)) 
For a pointer type you don't even need to check the ok, just check the value against nil. https://play.golang.org/p/SGcHXoViBM
To isolate the error, try it's simplest form: Send a POST request via Postman to your API endpoint. If it comes through fine, then it's your JS. Your Go snippet is simple enough that I can't see an error in those couple lines.
The problem is that, as usual, the Go devs will look at this and say "well it looks like you can work around this in several perfectly valid ways!" Case closed. They're not looking for examples where generics would be helpful -- there's a million of those. They want someone to present a problem that absolutely cannot be solved without generics -- there is no such beast.
Another good option, true. I was careful to say that my examples were only *some* of the options available. :) That said, using a chroot essentially falls into the same bucket as #4 (appending to the host system trust store) from the perspective of our application.
this was where my mind went first after seeing the Field.ofX() for the java example.
 func StrInt(name, meta, field1, field2 string) (v struct{ Add func(int64, string, int) }) { v.Add = func(n int64, v1 string, v2 int) { MetricAdd(name, meta, n, Field(field1, v1), Field(field2, v2)) } return } httpRequest := StrInt("/myserver/http/requests", “HTTP request count for MyServer, broken out by HTTP method and response code.”, "method", "code", ) httpRequest.Add(1, "GET", 500) .... func StrStrStr(name, meta, field1, field2, field3 string) (v struct{ Add func(int64, string, string, string) }) { v.Add = func(n int64, v1, v2, v3 string) { MetricAdd(name, meta, n, Field(field1, v1), Field(field2, v2), Field(field3, v3)) } return } someOtherThing := StrStrStr("/mysever/some/otherthing", "something else", "other", "things", "here") someOtherThing.Add(1, "Some", "Other", "Value") There are ways to do this that don't require generics.
It's not a good argument at all, actually. Generics implementations have unboundedly large numbers of expansions. The answer is, don't expand all of them, just the ones you need. There's no scenario where anyone would ever write either code generation or generics code to preemptively generate all possible combinations of type arguments before they are needed; nothing anywhere works that way.
I'd suggest that the Go-like way to do it is the Struct key they showed, and to the extent that there is still a problem with types there the solution is that HTTP response codes and HTTP request types should be made explicit types: type HTTPResponse struct { // note this is private, meaning only values provided by the package // can be used code int } type HTTPRequestType struct { type string } const ( GET = HTTPRequestType{"GET"} POST = HTTPRequestType{"POST"} // ... R200 = HTTPResponse{200} // ... ) And with code like this, while you may initially type Incr(1, httpstats.HTTPResponse{httpstats.GET, httpstats.R200}) eventually what you would end up with for this particular case is a middleware that records it for you in a way you can't forget, since _any_ requirement that a given handler records its own results is rather fragile. For the other uses, if it really became a hassle I think there's nothing wrong in internal code with expecting people to import this package as `"hs" "httpstats"`, to shorten the typing, or if it's really fundamental, importing it directly into the main namespace, at which point most of the verbosity arguments goes away. I wouldn't expect the Go authors to agree that declaring a type per metric is too much work; declaring types in Go is meant to be cheap, and a basic thing that you do without much cognitive overhead. (I remain pro-generics, though, and consider the "I want to pull down well-written and well-tested value-based datastructures for my high-performance network server rather than bashing together my own crappy data structures code" to be a sufficient case on its own. Go's data structure situation is embarrassing.)
You do get that what you just described is what implementations generics usually do, right? 
This is why shadowing should be a compiler error, IMO.
That is why it is a bad argument. Nothing anywhere just pre-emptively expands everything. Every possible technology in question, from code generation to generics to anything else, is going to expand on-demand. It's not a valid argument that you'd need ~300K metric types pre-generated; the answer is simply "don't do that". It's not an argument in favor of generics, because no code generation or any other technique would have that flaw anyhow, so generics aren't offering any advantages on that dimension.
Yeah, I don't get this either. I don't know of any generics implementation that doesn't work lazily, where it generates only expansions you use.
Yes, you "got around it" by doing what generics does for you, and it's incredibly verbose. You basically just did code generation by hand. I don't know what your point is here - no one is claiming generics make previously impossible things possible, just that it makes your code more succinct in these scenarios.
There may be a valid reason to insert a nil for that map key, making a simple nil check not quite descriptive of the issue. The presence of the nil might be an important distinction to make. Make the zero value useful :)
Shadowing is useful in many conditions, especially when checking errs, like when you do: something, err := doSomething() ... if err := somethingElse(); err != nil { ... } I don't think making it a compiler error is the best idea, but it's not the worst either, since one could make the argument that you could just use a different name. At the same time if you're writing straight-line code that's just "do something, if error deal with it and return", which is a common theme in Go, then the errors will be short-lived anyway. There are linters out there that check for issues relating for shadowing, which can help in lieu of any compiler help.
Yeah, I'm curious about this as well. I've always thought of struct embedding as a sort of trait implementation.
oh yeah, i forgot about it, been years since i went there.
That is true.
The author is the manager of Go devs.
Nobody said we need 300k metric types. Rather,this is the upper bound and demonstrates the number of methods and types that need to be generated to support the full range of the API for every client. It means that in a generic solution, those 300k would not be necessary
The Go devs asked for use cases to be able to distill a possible solution out of them. They never said, "show us a case that is absolutely impossible without generics". There are dozens of ways of implementing generics, and the use cases can help finding out which way is the best, or if there is a possibility of addressing these use cases with something else than generics. Because [every generics concept does have drawbacks.](https://docs.google.com/document/d/1vrAy9gMpMoS3uaVphB32uVXX4pi-HnNjkMEgyAHX4N4)
Or docker
here's to hoping they listen to him
`//monarch-gen:counter -fields method=string,code=int HTTPRequests` The Criticism is, that when people say "use codegen", they don't usually mean "generate every possible permutation of API anyone could ever use", but "create a generator that can generate the necessary API". In that way, talking about the number of needed functions is a red herring. (and yes, what I described is *exactly* what a generics-implementation might bring you as a language-feature, instead of as a bolted-on generator. But there is no argument in the post why a bolted-on generator isn't sufficient for this use-case)
I think if I where to use codegen for this, I would generate the type-safe function he talks about towards the top. That is, I would write a tool that traverses the AST, looking for calls to `metric.NewCounter`, extract the arguments and generate a suitable function of the correct signature to increment it. I think there is room in Go to make meta-programming of this sort easier (not as a language-feature, but in the tooling). It has the potential of alleviating a lot of the pain of lacking generics, while also soothing the concerns of people like me, that they might be overused.
The `cat` trick to transfer a binary over ssh definitely deserves some respect.
You could try a Jinja like templating engine for Go, I'm not sure what the differences between Nunjucks are, but it appears to be quite minimal: https://github.com/flosch/pongo2
Again, this is the upper bound and it's also the number of methods you have to support. Can you guarantee that *every single one* of the methods work? Generics make this far far simpler and integrate it into the go compiler so there is no external dependency and you can make a good promise that it works for every possible type you could stuff into it. That simply does not hold with code generation, no matter how much you improve it. Templating is a crutch for this, a working one, but still a crutch.
&gt; Again, this is the upper bound and it's also the number of methods you have to support. It's not, though. That's the point. You can generate them as needed. &gt; Can you guarantee that every single one of the methods work? The fact that you are saying "methods" indicates that you are still misunderstanding what I (and, AIUI, /u/jerf) was saying. I wasn't talking about generating methods, I was talking about generating *functions* which do the type-safe wrapping that he suggests towards the beginning of the talk. I was suggesting generating this wrapper: addHTTPReq := func(method string, code int) { httpRequests.Add(1, method, code) } That is, what you'd *usually* do, when you use code generation. Even if you take some existing, naive, purely lexical tool - if you implement an RBTree using them, it won't generate a gajillion wrappers in your rbtree package for any type-combination. It will provide you with a tool that you can use to put a type-checked RBTree in *your* package. &gt; Generics make this far far simpler and integrate it into the go compiler Well yeah, that's out of the question. But it's not a strong argument for why it *needs* to be in the go compiler. The criticism of the post is, that it assumes you are generating code for methods on `metrics.Counter` and then says that's impractical because of the number of methods that would need to be generated. But that's a red herring, because you should generate for the *instantiations* of Counter with an appropriate set of type-safe functions to increment it. It is not a good argument against code-generation, because *that's not how you'd implement code-gen*. &gt; Generics make this far far simpler and integrate it into the go compiler Everyone understands this. No one is arguing this. But again, that is not the point. The point is that the post is making a weak argument for why it *needs* to be in the go compiler, when talking about generating methods on `Counter`. &gt; That simply does not hold with code generation, no matter how much you improve it. The most common suggested implementations for generics *are* code generation. Both rust and C++, for example, implement them that way. Think about it this way: Whatever you suggest as a generics implementation - you can write a tool that takes the code as you suggest with the semantics as you suggest, type-checks it under the semantics you suggest and then transpiles it into Go 1.9 code and feeds that to the compiler. That is all that code generation means and thus, trivially, code-gen is at least as powerful and safe as generics.
I checked Jinja's file format and It really is mostly same. So, I 'll try this(https://github.com/shawnbot/meta-template) and see if it converts files correctly or rewrite all the templates. Thanks. 
&gt; Prometheus metrics use string Labels, which can be specified as a list of settings (here called “label values”) or via a map An advantage of the Prometheus approach is that the result of the `WithLabelValues` call can be cached, as thread-safe map lookups aren't exactly free. I've [written](https://www.robustperception.io/label-lookups-and-the-child/) on this in more detail.
Well Nunjucks basicaly is Jinja for Javascript. 
&gt; emailRe := regexp.MustCompile(`^[a-z0-9._%+\-]+@[a-z0-9.\-]+\.[a-z]{2,4}$`) You've already had your main question answered, but note that this is a __terrible__ regular expression for email validation. For example it would reject: * bob@example.photography * user@free.email etc. Best way forward is to send a mail to the user, and ask them to confirm they received it by clicking on a link.
&gt;It's not, though. That's the point. You can generate them as needed. Again, this is not the problem. The problem is the upper bound, of which you have to support everything. &gt;The fact that you are saying "methods" indicates that you are still misunderstanding what I I mostly do not care because I find it to be an insufficient solution. Wrapper methods don't look very pleasing in the code and feel clunky. They should not be necessary. &gt;But it's not a strong argument for why it needs to be in the go compiler. Because compiler support means it's better integrated into the language and works without having to resort to making third party tools a requirement. I hate libraries that make me jump through 20 steps before the installation is complete. I want to `go get` and be done with it. `go generate` is still a seperate action to that and should not be necessary. &gt;But that's a red herring, because you should generate for the instantiations of Counter with an appropriate set of type-safe functions to increment it It's not a red herring, it's an upper bound. It doesn't matter you only generate the necessary functions, generics will do the same without involving crutches. &gt;The point is that the post is making a weak argument for why it needs to be in the go compiler, when talking about generating methods on Counter. I do not find it to be a very weak argument. It's a rather interesting example where generics *can* be useful and it would be neat if the compiler supported it. &gt;The most common suggested implementations for generics are code generation. Compiler-level code generation would still simplify and generalize the solution, including making others capable of using just `go get`. And again, if you don't make running `go generate` a requirement then you'll have to generate *all* methods. Additionally, code generation decouples line numbers in the compiler output from where the code actually is, the template will falsify the position of any code, thusly not improving on debugging at all.
&gt; The problem is the upper bound, of which you have to support everything. I literally do not understand what you are talking about. So, let me ask directly, do you understand what the point is, that jerf was making? That saying "we need to generate 700K Methods" (even if it's "up to") just isn't true and coming from the wrong direction? As you wouldn't generate the methods, but would instead generate type-safe functions as needed (*exactly* like C++ or rust would do with generics)? &gt; It's not a red herring, it's an upper bound. It doesn't matter you only generate the necessary functions, generics will do the same without involving crutches. Generics will generate *literally exactly the same code*. I am not being facetious or overstating things. The mechanism is 100% equivalent in the number of methods generated in either case. I don't know what you mean by "crutches". If you mean, that you'd want that code to be generated by the compiler, instead by a separate tool, that is fair. But, again, a separate argument from talking about needing to generate 700K methods. &gt; And again, if you don't make running go generate a requirement then you'll have to generate all methods. But… no one is talking about "not making go generate a requirement". Quite the contrary, we are talking about generating code, that that'll involve go generate should be pretty clear. The criticism is, that the post says &gt; Alternative: Generated code &gt; One alternative would be to generate type-safe stubs for all likely combinations of field and value types. But that is not how you'd normally use go generate. You wouldn't generate type-safe stubs (in the metrics package) for all likely combinations of field and value types. You would generate *exactly* the combinations you'd need (in the server package). That doesn't run into the problems mentioned in the article, but it *is* the way go generate is usually used. &gt; Additionally, code generation decouples line numbers in the compiler output from where the code actually is, the template will falsify the position of any code, thusly not improving on debugging at all. See, that's the kind of argument the article *could* have made about code generation that *wouldn't* be a red herring.
One thing I hate in JavaScript is that ESlint errors if you shadow variables. And because there are lots of callbacks, you need to check errors a lot. So you end up with errRequest, errCheck, errInsert, errDelete for each callback where you check the error
"maybe"
https://i.imgur.com/Bdy3cOQ.gif
&gt;I literally do not understand what you are talking about. There are 700K *possible* methods. Each one needs to be supported or atleast have documentation of any bugs encountered. &gt;Generics will generate literally exactly the same code. I am not being facetious or overstating things. The mechanism is 100% equivalent in the number of methods generated in either case. If it uses templating, yes. Difference being the first-level compiler support. I'd have less of a problem if the compiler can properly support templating if that means you can track back the proper line numbers and properly support all those 700k methods. &gt;But… no one is talking about "not making go generate a requirement". Quite the contrary, we are talking about generating code, that that'll involve go generate should be pretty clear. Unless it's part of the compiler. &gt;You would generate exactly the combinations you'd need (in the server package). That doesn't run into the problems mentioned in the article, but it is the way go generate is usually used. But again, you still need to support all those method. Just because *you* don't use those methods doesn't mean someone else is not using an entirely different set of methods. If somebody has a problem with 1 of those functions, support is difficult compared to first level compiler integration (if templating becomes the way generics are implemented at all) &gt;See, that's the kind of argument the article could have made about code generation that wouldn't be a red herring. Simple because it's a red herring makes it wrong. A bit clickbait-y maybe.
Yeah, I used the tool i linked to above and I didn't had to make any significant changes after that except replacing " ' " with ' " '. 
Their current API allows data type mismatches, which are likely to slip through code review in my experience: Incr("GET", "HTTP/1.0", 200); vs Incr("HTTP/1.0", "GET", 200); Fixing the original API design to truly enforce types like Incr(Method.GET, Version.HTTP10, Response.R200) or Incr(Method("GET"), Version("HTTP/1.0"), Response(200)) already makes it Go-like. I think a simple codegen approach would be best in this case and not just for Go, although C++ has macros so separate codegen step is not needed.
&gt; There are 700K *possible* methods. Each one needs to be supported or atleast have documentation of any bugs encountered. In *literally the same way* as there would be with generics, in no way more and in no way less. If you don't need to support 700K different posssible things when you write package metrics type Counter&lt;T...&gt; struct { // whatevs } // foo.go package foo var httpRequests = new metrics.Counter&lt;string, int&gt;(…) then you don't need to support 700K different possible things when you write package metrics type Counter struct { // whatevs } // foo.go package foo var httpRequests = new metrics.Counter(StringField(…), IntField(…)) // foo.gen.go package foo func incRequestCounter(method string, code int) { http.Requests.Increment(method, code) } literally zero difference between the two. &gt; If it uses templating, yes. No, that does not matter *at all* what it uses. A tool can generate boxed-runtime-generic code just as it can generate name-mangled-compiletime-generic code. &gt; Difference being the first-level compiler support. To make that abundantly clear, this isn't about whether or not code generation is good or not. It's not about whether generics are good or not. This is about whether the article makes a good case against code generation, which it doesn't. You don't have to like code-gen to acknowledge that. This thread is about the observation, that the original article made a claim that code generation to solve the problem would imply generating hundreds of thousands of methods. Which is an untrue claim. The problem presented in the article to counter code generation *does not exist* (or rather: It is self-made by the unnatural and counter-intuitive way that the article proposes to use it). Arguments about whether a *different* kind of code generation would be good or bad too are misplaced. Anyway. This doesn't seem to go anywhere, there is just an infinite loop of repetition here. Maybe you can actually explain what you mean by your "methods to be supported" or "potentially to be generated methods" argument without just repeating the same words? Otherwise I'm just giving it up.
&gt;literally zero difference between the two. Well, one has first-level support from the compiler and the other does not. There is quite a difference. Additionally, I find the first example using generics easier to read than the second. &gt;No, that does not matter at all what it uses. A tool can generate boxed-runtime-generic code just as it can generate name-mangled-compiletime-generic code. It kinda does because different types of generics function differently and not all of them function like templates, not at all. &gt;This is about whether the article makes a good case against code generation, which it doesn't. Well it does, so &lt;/case&gt; I guess?
Ha, thanks! Honestly, the "right" way to do it would be using the scp protocol, but at the time I either couldn't find a library or didn't want to pull one in for some reason.
I don't know, I think it's elegant, because `cat` exists just about anywhere. I tend to use `rsync` for deploys, but it does require some installation. Using `cat` is also useful for things that might not be ssh sessions, but for example `docker exec -it [name] [command]` if you want to inject a binary/file into running containers... Anyway, good trick to know :)
I think you're right for dependencies to other libraries or programs. If we look at packages within a complete system (e.g. docker) I think it's ok to rely on inter-package tests (and by the way common for any other programming language).
Is it possible to return the output from go test to stdout? I ran courtney github.com/arekkas/accurate-test-coverage/... but it didn't print anything out.
Why is it good that this article doesn't make suggestions on how to implement generics in Go?
I’m still a k8s newbie, but this definitely seems like it would be useful!
Thank you! I really appreciate the feedback!
thanks
You seem to be sending a multipart body, but claiming it's `x-www-form-urlencoded` instead.
&gt; That is all that code generation means and thus, trivially, code-gen is at least as powerful and safe as generics. What you describe is clearly totally different from adding generics to Go, because what you describe inevitably results in a hundred incompatible generics implementations all transpiling to Go. That's about as much help as creating a hundred different programming languages instead. No single generics implementation will gain traction unless blessed by the language designers, *especially* given a large fraction of Go's userbase are hostile towards generics.
&gt; Recently I faced with task: Guarantee that data will be consistent for mongoDB(anything which can’t be blocked but have unique ID for separate parts) . That is mongodb's job as a database which provides atomicity and transactions is it not? That said I wouldn't recommend this solution for anyone over a global lock, reasons: - Solution fails to enforce it's only purpose if you have more than 1 application server (real applications do) since the lock is not distributed. - For each document your mongo client accesses it now holds 8 allocations that the garbage collector won't recover until the program exits since keys are never removed. - Creating a safe removal mechanism using this pattern without a global mutex is impossible since it can never guarantee a key is not being currently read. - Since it does not report if the key was being read the application doesn't even know if it's working on an existing document or not diminishing the value since the DB already provides atomic guarantees. - I see at least 8 (maybe 7, but I believe 8) allocations for each Lock() &amp; Unlock() call made for a unique key. - That number will still be 3 for keys that already exist. - I imagine due to the above it's around 70-100x slower than a global mutex Now if you for some reason have a single app server you can avoid all these downsides and have 0 allocs, 70-100x faster with zero complexity using a single global Mutex. The only "benefit" you lose is locking individual id's, but since mongodb provides document level atomicity you can use that instead. Finally if you absolutely need a distributed lock I would use redis, as it scales across your web tier and can have a expiration set.
Yes :)
Reading material for new people to the discussion https://docs.google.com/document/d/1vrAy9gMpMoS3uaVphB32uVXX4pi-HnNjkMEgyAHX4N4/mobilebasic https://research.swtch.com/generic https://news.ycombinator.com/item?id=9622417 
Based on what you've said it's possible you may be missing how your solution works, though it could also be me misinterpreting your solution since you ONLY showed server code I am left to make assumptions based on your description. If you want me to take a look to see if my concerns are correct just post your client code and I can be certain I have a lot of experience in around PKI/TLS/x509. &gt; On application start, both the WebUI and API check for existence of a cert/key pair on the filesystem; If one does not exist, a self-signed CA cert is generated (using code extracted from here) with a Subject Common Name: localhost. &gt; This change, along with an optional -insecure-ssl flag (which falls back to InsecureSkipVerify: true) means our applications can still be deployed simply for trials or into otherwise-secure lab environments, It sounds like both the webui and the server generate a certificate if one is missing and append to the system cert pool. This is redundant and provides no value to either the server or the client Since the WebUI and API generate their own certificates. So adding the cert as a trusted CA for each one is only going to ensure they could establish line of trust to themselves. You aren't establishing the ability for the WebUI to securely contact the API by ensuring the endpoints certificate is trusted. The only reason you are able to connect to the API form the WebUI is from the second quote- *InsecureSkipVerify: true* flag which omits validating against the systemcertpool all together. To test if your code is configured how I believe it is you may remove the systemcertpool portion and leave only InsecureSkipVerify and the code should work exactly the same. Remove it and the code will break. The server side code (the code you posted) is 100% redundant regardless of any scenario since you make no mention of client certificate auth- even if you used client certificate auth it would not work because the server does not trust the client CA. That said this sounds mostly like a PKI management problem that falls outside the concerns of an application. Solving your PKI management for local/lab environments and production to maintain an identical configuration in the application is the most correct approach in my opinion. For local development / testing what I do usually is create my own local CA [gist](https://gist.github.com/cstockton/025bc5c3d17e806463020544ea74920b) for this purpose. You could use this to establish produciton-like configuration and line of trust across multiple machines in a lab setting and it works with client certificate authentication as well. If you're set on solving it in this way I suggest removing all modifications to system cert store for the server. Then either establish line of trust for the client to the server through the servers CA (this means client must use the servers .crt file) or make no changes to system cert store for either client or server and simply use the insecure flag on the client side to gain simplicity while understanding you must mitigate the risk of MITM for any external traffic by network acls.
There seems to be nothing there except README and good intentions? Why is it pretending to be already released?
devil is in details. could you provide buildable code example?
To answer your question, no string comparison is as you expect. There must be something causing your issue that your example is not showing us.
Seriously check out https://gobuffalo.io/docs/getting-started ... I'm not telling you this because "beego is not the go way", rather because you say you want something like Rails w/ migration, an ORM, and all the other stuff ready to go. Buffalo provides that. You are correct that db migration in Go is a bit immature right now. https://github.com/mattes/migrate or https://github.com/pressly/goose mostly work, but definitely, have their issues. Buffalo implements their own migration tooling https://github.com/markbates/pop/blob/master/fizz%2FREADME.md that you can use on your own.
@vpol is right, please put a self contained reproducer on https://play.golang.org/.
looking at your example code there's a lot missing. I was not able to reproduce the issue you're describing with this reduction: https://play.golang.org/p/IH04cbFPjB you'll need to provide a more concrete example
Given what you have shared there is nothing wrong with your expectations. We may need a more exact set of code to look at. When posting code here you can either look into reddit's code formatting style OR paste a functioning snippet into https://play.golang.org. Golang related tips, feel free to ignore these at your leasure just intending to help out a bit. :) Error strings tend to not contain capitalization or punctuation. This comes from the common practice of: func Foo() error { err := funcThatReturnsError() if err != nil { return fmt.Errorf("foo error: %s", err) } return nil } Although not really required by anybody, the Go community tends to use camelCase variable names.
I'd probably drop the versioning stuff, and remove anything that depends on make. The rest of it seems like a good skeleton to start from if the code was made available.
TBQH, I don't consider this an ergonomic solution at all. Any given binary has literally hundreds of metrics each of them incremented at potentially dozens of call-sites. Having to jump through all of these hoops just to add a counter really is no good. &gt; eventually what you would end up with for this particular case is a middleware that records it for you in a way you can't forget I just looked it up: From the ~800 metrices one of our services exports, ~100 are service-specific, the rest happens in some imported library/middle-ware. Point is a) even if it happens in a library/middle-ware, *someone* has to write that code, b) even then, there will be a non-trivial number of service-specific metrics and c) we are talking about a *lot* of boilerplate :) &gt; I wouldn't expect the Go authors to agree that declaring a type per metric is too much work; declaring types in Go is meant to be cheap, and a basic thing that you do without much cognitive overhead. Note, that in your code you are defining one type per metric *field*, which is a totally different thing than one type per metric. You are also defining several constants and wrappers, though I'd argue they are mostly unnecessary. I think defining a single type per metric would be fine. Indeed, the rough overhead of that is the current recommendation: Defining a type-safe wrapping-function to increment it (which could be automatically generated, if need be, but it's probably fine to just write it down). PS: Your example is also broken, as you are using const on struct values. But that is besides the point, I don't think those constants are really useful anyway.
well, It actually looks pretty solid, definetly gonna give it a try, thanks!
Don't let users name files on your server. 
Use fmt.Sprintf to print the two strings using the %q format. My psychic powers suggest one string has a newline on the end and the other does not.
Your psychic powers are working great mate. There was a rogue '\r' at the end of the returned string. Thanks a lot!
Could you explain your view more? The way I see it versioning is a fundamental part of deploying most Kubernetes applications. The makefile also wouldn't hide anything and it would help implement a few of the requirements in the readme. If this were a straight application in Go I might be inclined to agree but this project seems to be combining the operational aspects of deploying to Kubernetes with Go's code.
I dare say this is the first experience report about generics. Everything else, so far, was either not concise or lacking substance. 
There are many runes which appear the same but to a computer are rather distinct at a bit level. See the Ruby fiasco with allowing UTF-8 invisible characters as variable names. When in doubt, print the numerical byte values, hexdump, etc.
sure, my view is that I'd like to see modular components that help me build and manage binary lifecycle within k8s. It should not prescribe how my build system operates. They are independent concerns.
Thanks for your feedback - but it appears that my writing style didn't communicate clearly enough exactly what's happening. &gt; The WebUI and API share some packages, including a “settings” package which handles some basic shared config for when the applications are both running in the same environment. When running on the same host, the applications also share the same SSL certificate. Both the WebUI and API use the same cert file path on the fs. So only one of them will actually generate the file pair (always the API when both are installed together), the other will simply load the pair and use them to serve HTTPS and to trust the other app's server. The *-insecure-ssl* (InsecureSkipVerify: true) flag is provided only as a way to have the server and API running on different hosts, but still allow (insecure) communication without setting up a CA. This is how the application will be used in our trusted lab environment, where HTTP plain text was previously employed anyway. It defaults to *false*. That said, it may have been clearer if I had actually provided a the **right** code sample (/facepalm). The whole point of the post was share the process of appending a cert to the pool provided by `x509.SystemCertPool()` and use the new pool in your client. Most other guides I came across seem to replace the `RootCAs` in the `tls.Config{}` of the client with just the self-signed cert, or mention `InsecureSkipVerify`. I've updated the code sample. Thanks again!
Hi- thanks for clarifying- the fact they have access to the same certificates doesn't change my feedback, it just places this solution farther way from being the correct one. I probably threw a bit too much information at you, I'll try to clarify the key point. - The reason you probably don't see any tutorials of people appending to the systemcertpool is because it's not a good idea. You should form strong guarantees around your netflows in your services, so creating a new cert pool and separate HTTP client for your internal endpoints that validates against a single CA is the most correct and secure option. Placing exceptions to these guarantees in your code opens you up to the potential of a configuration problem more easily affecting your production deployment. Logic for magic such as: **if certExists() { useCert() } else { generateCert() }** opens up to MITM attacks (since you currently set InsecureSkipVerify in second case) or at the very least possible customer requests receiving tls negotiation errors. **tldr** Moving parts around security is a bad idea. - The entire process as it stands now is redundant despite the clarification you provided. The reason is because the InsecureSkipVerify flag doesn't validate the certificate chain (i.e. use your newly appended cert in the systemcertpool) anyways- so it might as well have not been appended. To correlate this to the new code you posted: // Trust the augmented cert pool in our client config := &amp;tls.Config{ InsecureSkipVerify: *insecure, RootCAs: rootCAs, } tr := &amp;http.Transport{TLSClientConfig: config} client := &amp;http.Client{Transport: tr} The rootCAs is the systemcertpool by default, so the option is redundant. The option is then IGNORED when InsecureSkipVerify is set, making the process of appending a cert to the system store pointless as well. You may be getting all of this and making a conscience decision for this design, but I just wanted to make sure I was clear because I feel like the article is advertising a poor practice from a security perspective and would suggest this problem be solved with PKI management (gist I posted for example). Using that you can issue certificates for local development or your lab with a .test domain and maintain a identical configuration. I route all traffic to .test to a loopback interface (you can do this with a simple host file entry and not worry about dns) and have a predefined set of common names that I create like in the gist (client.test, server.test, with numeric suffixes) and then destroy the CA private key so I can safely trust it on my machine in the system wide store, no new certificates can be signed by it, so I can even share the private keys for other developers to test clients with if I choose to without any real security implications. Just food for thought. Thanks for being polite in your response, take care.
1. I didn't describe my problem correctly. For example I read document from collection "A". After it I extract from document "a" id for documents in collections "B" and "C". After all data in memory of application I do logic and update related documents in "A", "B" and "C". During query to "B" and "C" some different gorutine can do query to collection "A" to the same document! And mongoDB don't allow block document. Different gorutine will read from "A" then read from "B" and "C". Then update "A" and overwrite changes from first gorutine which already was finished! 2. Yes. It's not distributed lock. I didn't wanted to do distributed lock. Maybe I can misleading somebody. 3. Key always removed. Guarantee!! https://github.com/im7mortal/kmutex/blob/master/kmutex.go#L24 No! Global mutex absolutely wrong solution. First process read from global resource by id "A". It take 2 sec. Second, third and fourth reads from "C", "B", and "D" in the same time With global mutex "D" will be handled for 8 sec! but we need block only "A" not "D" Mutex allocation is latest thing which I want to think on machine with dozen of memory.
Was working on a large parser/expression evaluator that needed lots of tests, and also on solving Cracking the Coding Interview problems, so I came up with this library based on boilerplate code that I'd write every time. Would love to get some feedback/suggestions.
Useful library, but the code is not very idiomatic (constant distance multipliers...) and the API signature could be much more concise (more types like BBox...).
Maybe consider splitting the template into two easily distinguishable parts: 1. minimum requirements for a functioning, reachable application within Kubernetes. 2. All the other stuff. Then the people who visit your project to learn about Kubernetes can focus on 1. first and then pick and choose from 2. later
&gt; The entire process as it stands now is redundant despite the clarification you provided. The reason is because the InsecureSkipVerify flag doesn't validate the certificate chain (i.e. use your newly appended cert in the systemcertpool) anyways- so it might as well have not been appended. Not true - we only set `InsecureSkipVerify: true` in the case where the cli flag was explicitly passed in. The appended cert is trusted in the default *secure* mode. Good PKI management is obviously going to be the "best" solution in a case where we want client behaviour to remain unchanged; but this was the best balance for our use cases, which, while often in lab environments, are not tests of our application. For further clarification, this solution enables a specific "3rd mode of operation". The primary mode is *secure*, requiring CA-signed certs, deployed by the administrator. Secondary mode is *insecure*, ignoring server certs completely. In the default, secure mode, the WebUI would be essentially unusable in a new installation (without certs deployed), as it would not trust the local API server. Our spec was that a user must be able to use the application from a new installation, on a single host, without having to disable the security. Therefore, for a *multi-host deployment* our users must either deploy trusted CA-signed certs or run in `-insecure-ssl=true` mode; But, for single-host deployment, this practice lets them keep security enabled, while explicitly trusting the API endpoint at `localhost`. 
I would be interested in one blue one. How much and would you ship to Switzerland?
The reading of the file and passing it in via the &lt; is part of the shell, not SSH, so it's no wonder you're having trouble replicating it using just the SSH package. I'm guessing the best approach is to read the whole script in, then pass it all as the cmd to Session.Run
What about self deploying application like that: https://www.youtube.com/watch?v=XPC-hFL-4lU
Of course, I have already tried to do it more than once in my real apps. I would like to make this useful for myself and for many people as a template, in which you do not need to spend a lot of time on the deployment and access to the environment. Another goal is to help beginners run an already-ready application with the implemented requirements of Kubernetes
Good idea, I will think about it
You can use strings.TrimSpace() to remove all leading and trailing whitespace characters from your strings. If your strings have leading or trailing whitespace and only want to remove line feeds (\n) and carriage returns (\r), read up on strings.TrimFunc().
https://golang.org/pkg/strings/#Compare Why not just use this 
I don't know I'm having difficulty at this point following you requirements, why even include it in the example code you edited if all you wanted to show was how to add certs to the system pool? Anyways regardless of what your requirements are you can satisfy them while avoiding the practices I mentioned. This just raised a few red flags and was trying to be helpful, if you're a SME in this space and understand these implications then I certainly can't argue if the risk is acceptable. Have a good one. 
Here he is, folks, the guy that wants you to work for him. 
while it looks like a neat toy, i wouldn't use this for anything beyond testing and rapid dev.
As /u/JHunz said, the `&lt;` is not really part of the command line sent to the other machine. It just tells the *local* shell to connect `dummy.sh` to the stdin of the `ssh` command (which is why it only works if the file is locally available). It's pretty much exactly as if you typed the contents of that file into ssh, then pressed Ctrl+D to simulate EOF and close the command. Now, as for how to replicate this in Go: the interface for `crypto/ssh.Session` seems to be similar to that of `os/Exec.Cmd`, so my guess would be something like: file, err := os.Open("dummy.sh") if err != null { return err; } defer file.Close() session.Stdin = file session.Run(cmd) Disclaimer: I've never had occasion to use the `crypto/ssh` package.
the whole 'check everything is equal by stringifying' feels like a relatively weak guarantee of equality where you could easily make mistakes.
Good point, but what would be a better solution for the general case?
&gt; No! Global mutex absolutely wrong solution. First process read from global resource by id "A". It take 2 sec. Second, third and fourth reads from "C", "B", and "D" in the same time With global mutex "D" will be handled for 8 sec! but we need block only "A" not "D" [partitioned locking using global mutex and sync.cond](https://play.golang.org/p/9fPUDm3ftU)
Check pull requests' tab, there is at least one to solve this.
Anyway, these are planed as separate components which exist separate on one project. Somebody will use codebase, somebody whole parts. In the future, it can be developed as a code generator with optional components. Do not need versioning - switch off from the project, do not need to deploy - switch off.
Ignoring best practises (ie error handling), the basic way I do this in our server is as follows : signer, err := ssh.ParsePrivateKey([]byte(connectionInfo.PrivateKey)) sshConfig := &amp;ssh.ClientConfig{ User: connectionInfo.UserID, Auth: []ssh.AuthMethod{ ssh.PublicKeys(signer), }, Timeout: time.Second * 5, } conn, err := net.DialTimeout("tcp", connectionInfo.AddressString(), sshConfig.Timeout) .. c, chans, reqs, err := ssh.NewClientConn(conn, connectionInfo.AddressString(), sshConfig) .. client = ssh.NewClient(c, chans, reqs) So to run a command session, err := client.NewSession() var b bytes.Buffer session.Stdout = &amp;b session.Run("ls -l") // or what ever you want to execute b contains output Copy a script to the server for execution: session, err := client.NewSession() bytesReader := bytes.NewReader(bytesToWrite) scp.Copy(int64(len(bytesToWrite)), 0644, filename, bytesReader, path, session) I hope this helps. 
You should execute it in sh -c - otherwise you'd pollute the system with temporary scripts.
This is a useful technique where as the author points out "if the functionality change in the newer version of Go doesn’t cause the source code to fail to compile on older versions" The author uses the example of requiring Go 1.9's monotonic time, but there's other examples, such as if you're using text/template's trim feature or block action in 1.6 https://golang.org/doc/go1.6#template - people could compile but they'd quickly get a runtime error when executing the template. So it's niche, but something useful to have in your arsenal if you think it's appropriate.
Have a look at [gotests](https://github.com/cweill/gotests) that generates test code from a given function. `gotests` uses `reflect.DeepEqual()` for testing equality of multi-level data like slices.
I'd read it, but the website keeps interrupting me with stupid crap like asking to send notifications or asking me to join the newsletter. I shouldn't need to do any of those things to read the article, so why not just let the readers read?
Did you read the problem description? Did you read [the documentation for `strings.Compare`](https://golang.org/pkg/strings#Compare)? &gt; Compare is included only for symmetry with package bytes. It is usually clearer and always faster to use the built-in string comparison operators ==, &lt;, &gt;, and so on. 
&gt; TBQH, I don't consider this an ergonomic solution at all. Any given binary has literally hundreds of metrics each of them incremented at potentially dozens of call-sites. Having to jump through all of these hoops just to add a counter really is no good. In a sense, you've overshot the goal here; even if generics are added to the language you're probably _still_ going to need code generation in that case, in which case you might as well bite that bullet today. &gt; Your example is also broken, as you are using const on struct values. I always forget about that.
&gt; ven if generics are added to the language you're probably still going to need code generation in that case, in which case you might as well bite that bullet today. No, I don't think so. Both in terms of it being needed and in terms of it helping. With generics, doing something on the order of this (details of syntax nonwithstanding) is totally feasible: var requestCounter = metrics.Counter&lt;int, string&gt;( "/path/to/metric", "Help description text of the metric", "code", "method") // wherever requestCounter.Inc(1, 200, req.Method) Now, you might say that writing a thousand of these definitions is still going to be a PITA and you'd be totally right. But it's very close to the minimum amount of manually provided information needed anyway. So code-gen doesn't actually *help* with this - you'd need to feed the same amount of data into it. The problem with your approach is that I need to provide significantly *more* than the minimum information needed to tell Monarch. Now, code gen *can* help here, by simulating generics. E.g. what I suggested [here](https://www.reddit.com/r/golang/comments/6w6v6a/go_experience_report_for_generics_google_metrics/dm6mstz/), by generating the type-safe wrappers needed from the types used in the declaration. Alternatively (also a suggestion that I wished they had considered or discussed) you could also have a tool which only *checks* correct usage statically. That is, have a `monarch-vet`, which finds all usages of `metrics.Counter` and all usages of `(*metrics.Counter).Inc` and compares the arguments for compatibility. You could then integrate that into your CI (at Google that doesn't pose a huge problem). Now, this would still not catch everything, but at least the most common pattern of assigning the result to a variable/field and then referring directly to that would be covered. Anyway :)
The cmd here should be something like "bash -c" for crypto/ssh, or "ssh user@host bash -c" for using os/exec, as a slice ofc. +1 on the explanation. It's also possible to copy the originating script to the remote machine by running `cat &gt; /tmp/outfile.sh` as the cmd. The file will be sent to Stdin, cat which executes on the remote host will read from it, and then write the contents into a file on the system. It might be better do so it like this, just so there's no weird tty issues or whatever (I haven't hit them myself, but some things require a tty and break if you don't have one).
Thanks, I have been trying to avoid doing that but looks like this is the only way out. Let me try it.
From https://golang.org/pkg/time/#Time.AddDate AddDate normalizes its result in the same way that Date does, so, for example, adding one month to October 31 yields December 1 From https://github.com/uniplaces/carbon/blob/2ed00d8a452957d017135066f4e9bda33821d426/carbon.go#L419 : func (c *Carbon) AddMonths(m int) *Carbon { return NewCarbon(c.AddDate(0, m, 0)) } 
I think I solved it. Instead of creating a slice of values and then a slice of pointers to those values, I'm creating a slice of pointers to values on the heap: pointers := make([]interface{}, len(wantedRow)) for i, v := range wantedRow { pointers[i] = reflect.New(reflect.TypeOf(v)) } And then pass that into `rows.Scan()`, and then compare those values to the values in `wantedRow`: for i, v := range wantedRow { actual := reflect.ValueOf(pointers[i]).Elem().Interface() if v != actual { return fmt.Errorf("wanted %v; got %v on row %d", v, actual, i) } }
The example on the readme seems like way more boilerplate to test a simple function than the standard library. 
watch every single video on these channels: justforfunc https://www.youtube.com/channel/UC_BzFbxG2za3bp5NRRRXJSw/videos Golang UK Conference https://www.youtube.com/channel/UC9ZNrGdT2aAdrNbX78lbNlQ/videos Gopher Academy https://www.youtube.com/channel/UCx9QVEApa5BKLw9r8cnOFEA/videos
Wow! Thanks!! I felt I should use *ConditionVariable* but I wasn't so good with understanding of it. I will update my article with your name! Looks like a lot of people face with such problem but can't solve it elegantly. I still have questions. **I.**About OOM. Sorry I again didn't describe it fully. My logic: 1. Sometimes occur a event. [ ID in mongoDB] 2. Users start to do task for the event. [request to ID] 3. In 99.9% cases only 2 users can access to the same ID (logic specific). (It's that I want to prevent) [**DEEP OF recursion == 2-3**] 4. After some number of users used the event. The event got expired. [**Remove key from map. GC remove all mutexes**] **In my case it's not possible to have OOM with my implementation?** Thanks a lot! 
If you're using `reflect` you're probably doing something wrong. Time to stop, step back, and think about what it is you're trying to do. &gt; want to build a test helper that takes a SQL query and some expected rows and asserts that they're equal Why? What exactly are you trying to test? Why do you need to compare the results of Scan() directly? Doesn't your application have a model of the data in the DB (as necessitated by the Separation of Concerns principle)? This would make comparison easy: just populate your models and compare the models. Done.
Hah. It is rather long. Perhaps the most critical part of the files are the 300 lines of the `main` function. 
http://www.gopl.io
&gt; Now i want to try wiith Go Then try, and get back to us when you have something to share.
In regards about OOM question I thought you were being snarky when you said "Mutex allocation is latest thing which I want to think on machine with dozen of memory." but I realize now you were not and it may of been a language barrier. So responding with the satire was completely uncalled for and I feel rather bad for it, so please accept my apology. That code is all yours buddy and no credit to my username is needed, happy coding.
&gt; the code is not very idiomatic Yeah, this is Ball-of-Mud design here: putting everything in one package, functions that do several things at once. There's also a distinct lack of comments on some rather opaque algorithms. At least, for the uninitiated. I'd recommend, at the very least, running `go vet` and golang's `lint` and fixing those issues.
Main advantage in this situation will be Goroutines to drastically speed up the process.
I did it like this const ( stmtCreateUser = iota stmtGetUser stmtUpdateUser ) var stmtPairs = []struct { id int stmt string }{ {stmtGetUser, `SELECT u.email, u.data FROM user u WHERE u.id=$1 FOR UPDATE`}, {stmtUpdateUser, `UPDATE user SET data=$1 WHERE id=$2`}, } func (d *DB) prepareStmts() error { for _, stmtPair := range stmtPairs { st, err := d.DB.Prepare(stmtPair.stmt) if err != nil { return fmt.Errorf("Failed to prepare statement %s: %s", stmtPair.stmt, err.Error()) } d.stmts[stmtPair.id] = st } return nil } // NewDB creates new Postgres connection func NewDB(dataSource string) (*DB, error) { db, err := sql.Open("postgres", dataSource) if err != nil { return nil, err } res := &amp;DB{DB: db, stmts: map[int]*sql.Stmt{}} if err := res.prepareStmts(); err != nil { return nil, err } return res, nil }
I'm confused. Isn't the entire purpose of SQL to use a specified query to pull out the data you need from a database? Why are you iterating through scan returns instead of specifying in the initial query what it is you're needing? Let T-SQL (or the like) do the processing for you. Don't pull bulk queries from a SQL database and then process the return in Go. You're doing it wrong...
The API is wrong, the Declare, Load and Register functions should take named and non-empty interfaces. Just like [sql.Register](https://golang.org/pkg/database/sql/#Register).
Okay thanks alot, this looks a lot cleaner. Will take a look at it tomorrow. Thanks
Those kind of helpers are not refactoring-friendly. The Actual and Expect methods take interface{}, lots of errors that would have been caught compile-time now needs to be found in the runtime. If tests take non-trivial amount of time, lots of it is going to be wasted in the refactor-build-test loop.
That seems useful, but it also seems like it should be easier. Perl has this functionality using a simple "use &lt;version&gt;;" include at the top of your script.
The intended usage of the function you are refering to is to register drivers for a specific interface ([sql.Driver](https://golang.org/pkg/database/sql/driver/#Driver)). On the other hand, the purpose of the given layout is to be generic on this aspect so one is able to register drivers for any interface he declares as a driver group.
&gt; If you're using reflect you're probably doing something wrong. Time to stop, step back, and think about what it is you're trying to do. Fair enough; I'm open to suggestions. &gt; Why? What exactly are you trying to test? Why do you need to compare the results of Scan() directly? Doesn't your application have a model of the data in the DB (as necessitated by the Separation of Concerns principle)? I'm building a compiler for SQL. As such, I have no data models. I want to assert that my high-level query objects compile to SQL that does what I think it does. I want a helper function because I don't want to deal with the boilerplate involved with mucking around with `*sql.Rows`. I can't create a type that represents any row returned by any query I might want to test, so `reflect` seems like the next best thing.
&gt; I'm confused. Isn't the entire purpose of SQL to use a specified query to pull out the data you need from a database? Why are you iterating through scan returns instead of specifying in the initial query what it is you're needing? Let T-SQL (or the like) do the processing for you. Don't pull bulk queries from a SQL database and then process the return in Go. You're doing it wrong... I'm not sure how you got that out of my posts. I'm writing tests for SQL queries, and the only way to adequately test SQL queries is to run them against the target database and assert that you get the right thing back. As such, I'm not "processing results" as you describe; I'm asserting that the result contains what I expect it to contain.
&gt; At first I was going to use cgogen to generate bindings to cimgui, but cgogen didn’t like how it was a C++ project using C linking. It works only with C headers, yes, unfortunately there is no automatic tool to generate C exports for C++ classes, I was going to implement it using LLVM but had no time for that unfortunately.
Wait, https://github.com/Extrawurst/cimgui/blob/master/cimgui/cimgui.h looks like a legit C header, are you sure c-for-go won't parse it?
I think that's premature generalisation. 
&gt; To be save I used prepared statements for everything that uses variables that are not created inside the program. You don't need `Prepare()` for this. [`db.Query()` takes parameters](https://golang.org/pkg/database/sql/#DB.Query) and can use placeholders to provide the exact same level of safety. `Query()` literally prepares the statement and immediately executes it, unless the driver provides a `driver.Queryer` interface with equivalent semantics. `db.Query()` and `db.Prepare()` provide the exact same safety from SQL injection. Why two interfaces, then? Prepared statements allow the database to cut down on query planning time, a cost which can get significant for queries that are executed many many times. Unfortunately, moving all your queries to a central place like a `func createStmts()` can have a significant negative impact on code structure and general readability. Is there some option in the middle? `pgx` thinks so, namely [an idempotent `Prepare()` function](https://godoc.org/github.com/jackc/pgx#ConnPool.Prepare). This lets you write code that prepares and execute statements as needed -- keeping the SQL queries in the code that uses them -- but subsequent `Prepare()` calls for identical queries are handled from a cache. This gives the performance benefits of preparing once and executing repeatedly without the maintenance drawbacks. If you need prepared statements, I would encourage you to do something similar on top of `database/sql`.
Why is GopherJS awesome?? mainly for isomorphic uses?
&gt; I want to assert that my high-level query objects compile to SQL that does what I think it does You're using a database engine to test a SQL compiler? That's quite, erm, awkward. I'll admit, compiler design isn't my strong suit, but this test has some smell to it. Can you do this without invoking a database? Seems like that'd be a better way to go. &gt; I'm clearly not violating SoC here. Interfacing with a DB, marshalling the data returned from the DB, and inferring equality, I see three different concerns right there.
Code sharing between client and server and strong typing for JavaScript
Also using [almost any pure Go library](https://deedlefake.github.com/wdte) in JavaScript.
Google "Todd McLeod"...
It was complaining about some syntax errors in some of the standard library headers (stdio and stdarg iirc), the problem wasn't in the actual cimgui headers. Also, I used my imgui 1.51 PR/fork (https://github.com/Armored-Dragon/cimgui) instead. It parsed and compiled fine though.
Yeah, definitely an easy header, just generated bindings using [c-for-go](http://github.com/xlab/c-for-go) in less than an hour. https://github.com/golang-ui/imgui However, Go refuses to compile the C source, it doesn't like the incomplete types. cgo-gcc-prolog:161:17: error: field has incomplete type 'struct ImVec2' ./cimgui.h:30:8: note: forward declaration of 'struct ImVec2'
Next time please report that into issues please, only collective effort may move the progress :)
Can someone please do a feature-complete React-wrapper-thingy for Gopherjs so the frontend/backend can be bridged once and for all? (roughly: I wouldn't wish a single line of javascript onto my worst enemy)
I'm not understanding why you can't use reflect.DeepEqual. I'm not saying you can, just that I don't understand why you can't. Possibly with a quick but of preprocessing to do some slight type tweaks.
Yes, all of this. Also, you should be aware that prepared statements may be slower, sometimes drastically slower, than non-prepared statements. You save time on the planning, but the tradeoff is that the "generic" plans that are produced may not be as efficient as one that's planned with the knowledge of the parameters. Parameterized queries are always a good idea, if you can use them, but prepared queries aren't *always* better.
Ah, gotcha. Are you using the standard SQL library, or constructing your own? I ask because if the standard one, there is already test files included to ensure the queries are implemented correctly. Just don't want you to put in a lot of time on something that may be redundant.
I'm using the standard SQL library, but I'm not familiar with those test files. I already have a working implementation, but I appreciate the effort! :)
People have been doing exactly that. See: - https://github.com/gopherjs/vecty - https://godoc.org/myitcv.io/react - https://github.com/bep/gr
can't wait for the wasm-based version of GopherJS :)
&gt; You're using a database engine to test a SQL compiler? That's quite, erm, awkward. I'll admit, compiler design isn't my strong suit, but this test has some smell to it. Can you do this without invoking a database? Seems like that'd be a better way to go. Not reliably. I can test that it produces the SQL that I expect it to produce, but that's fragile and I care more that the SQL output *does* what I expect it to do. &gt; Interfacing with a DB, marshalling the data returned from the DB, and inferring equality, I see three different concerns right there. To be clear, this is a test helper function; my compiler doesn't depend on a SQL engine. It will spit out SQL strings all the livelong day; whether or not you pass them to a database is your business. My *tests* do pass them to a database, because that's the cheapest and most reliable way to validate my compiler. You can bet your ass that at some point, the Go compiler tests actually *run* the generated assembly as part of the test cycle--it doesn't just assert that the output assembly looks a certain way. :)
Being able to write your backend and frontend code in the same language is indeed nice. You can share validation or rendering logic. You can effortlessly move some functionality from backend to frontend, or vice versa. You get to use same tools, same practices: gofmt, goimports, go vet, godoc, go test, etc. Refactoring frontend Go code is a pleasure. A game you write in Go can run on desktop, mobile, and in browsers. Basically, here's the list of languages you can use on frontend without GopherJS: - JavaScript - &lt;other languages that compile to JS&gt; With GopherJS: - JavaScript - Go - &lt;other languages that compile to JS&gt; Given the choice of JavaScript and Go, I'd much rather do my programming using the latter.
Me too! The performance, reduced load times, efficiency will go through the roof! And they're already surprisingly good. That said, it won't be GopherJS, probably gc compiler or a new compiler.
The problem was building a list of pointers that I can pass into sql.Rows.Scan(); comparing is the easy part. In my sibling comment (downvoted to the bottom, for some reason), I explain the solution I found, which is to build a slice of pointers into the heap, pass them to sql.Rows.Scan(), and then iterate over them, dereferencing each and comparing it to its corresponding expected value. As you mention, I could build a `[]interface{}` of values and then do the deep equal with the expected row, but that would be more work for no obvious gain.
I definitely agree with you. I think my idea behind this library was to showcase the problem / need. I just opened an issue about this: * https://github.com/golang/go/issues/21673
Strange. I guess c-for-go just didn't like the STL headers used. The incomplete types could be fixed. Does c-for-go not support variadic arguments between Go and C? The absence of `Text` and `BulletText` are rather crippling, as Text is one of the most used functions (formatted text, so it could be implemented with `TextUnformatted` and `fmt.Sprintf`).
&gt; Not reliably In my view, depending on a 3rd party to do your validation is less reliable than testing inside your own source tree. &gt; I can test that it produces the SQL that I expect it to produce Shouldn't that be enough for a compiler? How's it fragile? 
Another alternative is to use a package like this: https://github.com/jmoiron/sqlx You could then use `.StructScan()` on the returned sqlx.Rows and compare to your static test data, which would be in a struct.
There are a couple of problems with not testing against a database. First is the fragility problem--my compiler should be free to optimize the SQL however it likes so long as the output is the same, and I don't want to go and rewrite a bunch of tests every time I add an optimization. Secondly, my users don't care that my compiler produces the output I think it should produce, they care that the compiler produces correct SQL. On the other hand, the downside of testing against a database is a relatively tiny bit of complexity in your test process; it would be foolish *not* to make this tradeoff.
&gt; my compiler should be free to optimize the SQL however it likes so long as the output is the same Ah, if your output is dynamic, then your unit test should have some simplified version of the compilation logic in it... that's what's specified in the golang book! No use trying to hit a moving target. &gt; my users don't care Uhh, you writing a requirements test, an integration test, or a unit test here? Seems you're trying to do all of the above... 
&gt; https://github.com/jmoiron/sqlx I didn't know that existed, but wouldn't it be subject to pretty much the same problem? If I understand correctly, I'd need to use reflect to allocate a struct of the right type and get an address to it to pass into StructScan()?
database/sql will always use prepared statements when you pass in parameters; no need to make them manually.
&gt; Ah, if your output is dynamic, then your unit test should have some simplified version of the compilation logic in it... that's what's specified in the golang book! No use trying to hit a moving target. I'm not sure what you're proposing. Why would a unit test contain the logic it is validating? This all seems terribly complicated compared to simply passing the sql into the database. &gt; Uhh, you writing a requirements test, an integration test, or a unit test here? Depends on your definitions. I'm writing a lot of exhaustive tests with a database in the loop to assert that my compiler behaves as required.
FWIW, I like the sound of this approach
I may talk out of my ass but didn't prepared statements also reserve a connection right away?
Let's not call the concept of exporting metrics "JMX". Also, might as well make it a Prometheus exporter.
For any of this function there are options provided by the library such as TextV, BulletTextV, etc So it takes a little bit of effort to make wrappers by hand instead of generating variadic. In fact, I have plans to support "bridge" autogeneration for `...` arguments, but have no time to do that :) Maybe only if enterprise clients would ask. 
Do you need it for Java? Not as far as I can tell.
What benefit would this have over just using CloudFront or deploying Varnish servers in your data center with origin to S3
Hi, I guess these will make a good basis for a FAQ section :) &gt; just using CloudFront CF is a good thing but it's still Amazon and it shares the same problem that forces people to avoid cross-datacenter traffic to S3: when the amount of outbound traffic and request rate go up, the Cost of Service may rise insanely high. You can check out this calculator and see what's the monthly cost of transferring 500kiB files with 100/s sustained rate. S3 is about $10K+ and CF just doubles that. https://calculator.s3.amazonaws.com/index.html &gt; or deploying Varnish servers * Varnish is not zero-config, someone should support it, it has a scripting language and a couple of books written about it. * Varnish is complex, so it has bugs like this https://varnish-cache.org/security/VSV00001.html#vsv00001 * Objstore on the other side is very simple inside, it just serves the files using io.Reader or io.ReadSeeker, backed by os.File or a file body from S3. * The projects have different feature sets — Varnish doesn't handle writes; * My favourite: «Varnish High Availability is available as part of a Varnish Plus subscription. If you’d like to get these components up and running now, please contact one of our Varnish Plus Sales Executives» I hope there is enough reasons to have a good replacement for Varnish implemented in pure Go and designed to be failure-proof for free.
&gt; Why would a unit test contain the logic it is validating? Specifically, a vastly simplified version of the logic. Why do this? To provide a verification of the unit's output, and ensure the unit's output doesn't drift over time. And as a bonus, it's not a fragile test. &gt; This all seems terribly complicated compared to simply passing the sql into the database. Not when you consider your simplified logic is several orders of magnitude less code than "the database". Also "the database's" code isn't exactly well controlled, in comparison to the logic in your test. Another factor is the portability / ease of use of your test code. That's all much improved by using simple, local code for your tests, and not pulling third party dependencies. &gt; Depends on your definitions Not my definitions. They've been around for quite some time; see the classic Art of Software Testing (Myers) for one reference.
 type Backend struct { db *sql.DB insert *sql.Stmt delete *sql.Stmt } func NewBackend(db *sql.DB) (*Backend, error) { var err error var s *sql.Stmt prep := func(q string) *sql.Stmt { if err != nil { return nil } s, err = db.Prepare(q) // err = errors.Wrapf(err, "prepare query: %s", q) return s } return &amp;Backend{ db: db, insert: prep("insert ..."), delete: prep("delete ..."), }, err } If you have a lot (more than one "Backend"-type struct) it works better to break it out into a struct that has db, err and a "Prepare" method (whatever you want to call it). The idea is to store the first error and jump out if it's != nil. It makes subsequent calls after the first failure cheap enough that it doesn't matter that you don't check the error until the end. It's not the right choice for *every* situation, but it's good for initialization and places where you can tolerate a little inefficiency to improve clarity. Inspiration came from bufio.Scanner and how it "stores" the error state: https://golang.org/pkg/bufio/#Scanner There may have been a blog post at some point in the past too, I vaguely remember reading something. If I find it I'll post the link.
are you full-stack or did you just do the back-end work for someone else? Doesn't Node have a robust package system (npm) where lots of things that you would want to do with users are already built by someone? Are your web apps user-facing? Do you work on the interface parts too?
I'd be interested in a blue one shipped to Colorado, USA. What's the price you want for it and the shipping?
I use it to run `gofmt` on the generated code at https://mholt.github.io/json-to-go/ -- because `gofmt` is written in Go 😉
&gt; Specifically, a vastly simplified version of the logic. Why do this? To provide a verification of the unit's output, and ensure the unit's output doesn't drift over time. And as a bonus, it's not a fragile test. I still don't follow. In particular, I don't know what you mean by "a vastly simplified version" or how I would use such a thing to validate the actual implementation, much less in a manner that precludes drift and fragility. &gt; Not when you consider your simplified logic is several orders of magnitude less code than "the database". Right, but I'm not maintaining the database; it's well tested and dependable. &gt; Also "the database's" code isn't exactly well controlled, in comparison to the logic in your test. I don't know what this could possibly mean; the database's code is almost certainly much better than mine. It's certainly better-scrutinized. &gt; Another factor is the portability / ease of use of your test code. That's all much improved by using simple, local code for your tests, and not pulling third party dependencies. This whole appeal is beginning to seem like dogma, all to avoid bringing in a dependency that we're going to need at some point anyway. It took me a few seconds to go get the test dependency and apart from spinning on the test fixture, I'm flying through tests. Beyond that, the tests will run on any platform with only DATABASE_DRIVER and DATABASE_CONNECTION_STRING variables. Plus I get a high degree of confidence knowing that everything works against the actual databases I'm targeting. Maybe what you're proposing is really a lot better, but the vague terms, lofty promises, and appeals to authority are tripping my BS detector. &gt; Not my definitions. They've been around for quite some time; see the classic Art of Software Testing (Myers) for one reference. I didn't mean to imply that you defined them, only that these terms get thrown out by different people meaning different things to the point that they're hardly clear. Frankly, the distinctions don't interest me that much any more; I care more about writing working code than I do about abiding by a particular testing philosophy.
I think goimports can get you halfway there. https://godoc.org/golang.org/x/tools/cmd/goimports 
This ^ Works with all editors. What many miss in the beginning is that goimports = imports + gofmt. Find the setting in the editor that formats your code o save, and change it to use goimports instead, and you'll have both. Unused variables: ``` // TODO remove _ = somevar _ = someothervar ```
No. If I read correctly, you indicated that you're testing SQL queries and you know what the return values /should/ be. In that case, you just manually create a struct that represents the row you expect. Then let sqlx do its thing (which does involve reflection.. but you don't have to write it); and finally compare the scanned struct to the expected one. type myRow struct { ID int Name string Value string } expect := myRow{ID: 1, Name: "foo", Value: "bar"} var got myRow err := db.QueryRowx(query).StructScan(&amp;got) if err != nil { t.Fatalf("blah blah: %v", err) } if got != expected { t.Fatal("yada yada") } You may also need to set db tags on the struct fields, or set the Mapper (all documented, if required).
IDE is gogland, obviously for the refactoring facilities
Thanks for sharing. I have a few comments: * Packages like this should never panic. * Handle errors properly or return them to the user. Swallowing errors makes this package less usable, not more. * Examples in the readme would help. How do you do the same thing with the stdlib and your package? Why is your package better?
goimports, and use the vars by assigning them to _ _ = otherwise_unused_var 
Largely agree, except for one point: &gt; Packages like this should never panic Specifically for generating a random number, I think that the default should be to panic on error. On modern-ish Linux, the `getrandom()` syscall will generally ~never fail unless there's no available entropy and you call it in nonblocking mode (if you call it in blocking mode, it'll just wait until you do have entropy). For most programs, not being able to generate a random number in these cases is essentially unrecoverable - there's no safe thing to do except to `panic` / `os.Exit()` the process.
Without adhering to a strict driver non-empty interface, there are various elements of this program which are decidedly unsafe. Casting to a known interface which might not be satisfiable, resorting to reflection for a runtime interface check without actually checking any satisfiable methods,... If your point was that you need generics, I think you're not the only one. Various applications, like yours and even the underlying `sync.Map` would benefit from having a *typed* implementation. In your case however, you'd be better off declaring a non-empty interface which hipone suggested. I mean if it's good enough for the Linux kernel, it's good enough here. In example, a simple linux kernel may declare only two functions, the init function and the exit function. Possible communication with the kernel module may be extracted to a number of other interfaces which they may use. It may be worth it to think how you can communicate to your (prederably typed) interfaces over some sort of data channel, which will allow you to implement all your logic in a safe way (ie, state machine that takes specific payloads like protobuf or something)... I mean usually drivers have their own client pairs, and there is some example work as to how this communication can happen without resorting to unsafe generalisations.
I think I only maybe started to understand why type aliases were introduced when you showed the context example. I remember running into some minor annoyances combining packages which used the new context type and packages which used the old context type. I was able to work around them though. I think I found the introduction of type aliases confusing because semantic versioning seems to solve a very similar problem. If you version a library and you move a type between two packages of that library you will not break the build of a package which imports it since it should be pinned to a version or range of versions. When you want to increase the pinned version of the library you: increase the pinned version locally, fix the compile errors, and commit the changes. This results in master never being in a broken state. It seems like one of the benefits of type aliases over semantic versioning is it allows libraries which are tied to different versions of an external library to interoperate temporarily for some period of time (since the old type will eventually be removed)? With semantic versioning if you want use both a library which requires version 2.0.0 of library X and another library which requires version 3.0.0 of library X there is a chance it may not work. Maybe I still don't 100% understand why they were introduced.
&gt; I don't know what you mean by "a vastly simplified version" or how I would use such a thing to validate the actual implementation Let's imagine a spectrum of test complexity. At the far left, you have your compilation engine. At the far right, you have a static lookup table. The far left wouldn't do you any good, because you're comparing the engine against itself. The far right, as you noted, produces too fragile of a test. What you want is something in the middle: just enough complexity to make a more stable test, but not so much that you've coded your entire engine into the test. Yes, it'll look odd writing a mini compilation engine just to run a compilation engine test case, but it accomplishes the goals of unit tests: to verify correctness of output and protect against drift. &gt; I'm not maintaining the database; it's well tested and dependable. "I'm not maintaining the database" is exactly why you shouldn't use it for testing the correctness of your code. You can only assume it's dependable, and assumptions don't prevent bugs. As I said, your home-grown test code has orders of magnitude less code, and every LOC is an opportunity for another bug... &gt; I don't know what ("the database's" code isn't exactly well controlled) could mean You don't have control over when "the database's" code changes (patches, releases, etc.). You have exclusive control over your own source code. What this means is, your test may never be repeatable: say you find a bug one day, want to try to reproduce that bug some weeks later, but you don't remember / can't determine / can't reload the version of "the database" you were running at that time... not going to happen without pulling in the source tree of "the database". &gt; This whole appeal is beginning to seem like dogma Hell, yes, software testing is dogmatic. If you're going to test software, but only do it in an ad-hoc manner, you're not going to collect the full value of your efforts. If you're the numbers type, the classic text Code Complete offers some statistics on various testing methodologies. &gt; to avoid bringing in a dependency that we're going to need at some point anyway Not really clear on this, but all the testing I'm talking about above can work in conjunction with external dependencies. The Myers text I cited above has a couple chapters on that if you're interested...
Yes you could. Logging into a site with plain golang code would be something in the lines of: // Store session cookies after login jar, _ := cookiejar.New(nil) // Setup HTTP client, and give it access to the cookiejar client := &amp;http.Client{ Jar: jar, } // Create POST data to send to login form log.Println("About to login user", *username) urlData := url.Values{} urlData.Set("sForm", "login") urlData.Add("username", *username) urlData.Add("password", *password) // Prepare http POST request req, _ := http.NewRequest("POST", "http://www.mysite.com/login", strings.NewReader(urlData.Encode())) req.Header.Add("Content-Type", "application/x-www-form-urlencoded") // Do the http POST to login resp, err := client.Do(req) if err != nil { log.Fatal(err) } // ..and we're logged in log.Println(*username, "logged in succesfully") resp.Body.Close() In conjunction with https://github.com/PuerkitoBio/goquery this is very powerfull, especially if you use Go routines like Morgahl said. I don't know Scrapy, but from quickly looking at the Scrapy documentation the overall experience/usage looks quite similar. Using goquery might be somewhat more verbose, but you could easily create convenience functions for that..
all_test.go: `func use(...interface{}) {}` foo.go: `use(bar, baz, qux.Foo)`
Aha, not needing the prepare statement makes the code a lot cleaner. Each database function gets its own go function. Removes a bunch of statement arguments from my functions. Thanks a lot for your advice.
Never knew prepared statements could be slower. I thought that prepared statements optimized the query in advance somehow.
I love the effort you put into this. Not saying I would use it, but I use nginx for a similar use. As long as your cache dataset can fit on disk, there's little need for a multi-master setup. As soon as I will be dealing with TBs of data on S3, woo boy, I'm running to you :)
You can export pprof data to prometheus, so everything you need is already there, you just need to enable it. In fact Prometheus profiles itself with pprof, since it's written in Go.
As far as I know the Prometheus Go client makes no use of pprof.
JMX is much more than metrics. But sure what OP is trying to say.
This is a dangerous assumption. IHMO libraries must never panic. If you really want this the lib should provide a Must function that can panic.
Have you read Russ' [article on the topic](https://talks.golang.org/2016/refactor.article)? IMO it makes a good job of explaining both what the problem is, how type aliases solve it and how that solution relates to versioning.
We might have to agree to disagree, but it's generally my opinion that a library can panic when there's no way to handle an error. An error is just a return value like any other - if you're going to return an error and the caller will exit/panic every time, then `panic` is the right answer. Additionally, it's good practice to make potentially-unsafe code more obvious - for something like this, it's best to have the default (i.e. shortest/most understandable code) do the right thing on failure (i.e. panic). If someone needs to handle the error, then exposing a function like `BytesWithError`, or a similarly long name, makes it more obvious that this isn't the default case.
Thanks for sharing this. This is very interesting.
The easiest solution I know is to install VsCode with Go plugin.
You're welcome !
Reading comments... no one advantages. Many of them talks about velocity of golang without check default scrapy conf to be a good citizen. Their are unfair (and wrong) metrics. Scrapy works with event programming, Go with n:m threading system. So probably go consume more ram (2k or 4k of stack size by each connection) but you don't worry about if your code is blocking or not. If your are doing complex things in your spider, probably go consume less cpu. But if your bottleneck its net your language is not important. Scrapy its a complex and mature framework with a lot of side projects (for example frontera). Go its a language without a comparable framework. Maybe the answer is: use scrapy for complex parser and go for toy or specific ones. Maybe in a future something comparable to scrapy exists in go, but not for now. And no, no and no, scrapy is not comparable with http+goquery. PD: I was working with a modified scrapy with prioritized queues and a custom scheduler. Make it in go will be easier than in scrapy (program with twisted its hard), but i need anti-thotling stuff, xpath and css selectors, ... and many things with came with scrapy. Sometimes I dream with a scrago ;-)
I agree with both of them. Provide a "Must" variation that panics. Libraries should never panic. &gt; ...when there's no way to handle an error You can always decide. There's never "no way to handle an error".
yep. BTW, that's tracked over there: - https://github.com/golang/go/issues/18892
The second I heard type aliases were coming in 1.9 My instant thought was "yeah that context bullshitery I've had to deal with definetly pissed off enough Google engineers too". 
Platform: Linux. IDE: Emacs. Solution for imports: goimports Solution for unused variables: 95% of my unused variables are bugs so I'm happy the compiler detects them for me.
Thank you for the constructive criticism. I was hesitant to use panic but decided it was better to stop if there was no entropy. Using panic also cleaned up a lot of redundant error handling code in the package. I'll revisit this as soon as I can. I would like to note that some functions in both math/rand and crypto/rand also panic under certain conditions. The standard library provides most of these functions with math/rand but math/rand is not considered cryptographically secure so I decided to write this package. Honestly, I don't know how useful this package would be to anyone else but it's useful to me and I thought I would share it. I will try to get some examples in the README as soon as possible. I have a couple of other functions I plan to add as well.
&gt; I was hesitant to use panic but decided it was better to stop if there was no entropy This is not libraries' decision to make. 
&gt; With semantic versioning if you want use both a library which requires version 2.0.0 of library X and another library which requires version 3.0.0 of library X there is a chance it may not work. Yes. Semantic versioning allows you to signal breaking API changes. The type aliasses essentially allow you to deal with those changes more gracefully and still interoperate across those versions. Though I'm all for semantic versioning I like the idea of being able to carry over usage gracefully into a new (major) version without necessarily requiring action on their part, especially when what's happening is that we're renaming/reorganising things but not necessarily changing the functionality they depend on. The example given in the Just For Func episode is a simple one but a pretty good one too, with the alias you don't lose the methods of the type you're aliassing. And lets not forget that not everyone follows semantic versioning. Though it's common place for open source or public components this doesn't necessarily happen everywhere and is a lot harder to do in a mono-repo style environment.
gogland IDE + goimports, configured to run goimports on file save and using the format shortcut (cmd-shift-f iirc) to go fmt my project helps a lot.
Ah, I see. I think the overhead of defining an extra struct is not ideal for my use case, but it's nice to know that exists. Currently, this is what my tests look like: test(query, []string{"expectedCol1", "expectedCol2"}, [][]interface{}{{"foo", 42}, {"bar", 24}})
Can you do this video in English too!
&gt; Yes, it'll look odd writing a mini compilation engine just to run a compilation engine test case, but it accomplishes the goals of unit tests: to verify correctness of output and protect against drift This sounds like more work to maintain when a database is virtually free. It's also going to be much less reliable than a database, despite your claims, which I'll address in turn. &gt; "I'm not maintaining the database" is exactly why you shouldn't use it for testing the correctness of your code. You can only assume it's dependable, and assumptions don't prevent bugs. As I said, your home-grown test code has orders of magnitude less code, and every LOC is an opportunity for another bug... Do you write a mini operating system or CPU emulator for every program you write? Or do you just assume that the rigorous testing and billions (trillions?) of hours of use do a better job of validating their correctness than you could by writing a small fake? Yes, there's a teeny tiny chance that my tests will report a false positive due to some bug in the database, but that chance is many, many orders of magnitude smaller than that of me building a faulty fake. By my estimation, a database is faster and also more reliable, though as you pointed out, not infallible. &gt; You don't have control over when "the database's" code changes (patches, releases, etc.). This is absolutely false, I don't have to upgrade the test database if I don't want to, and dependency management tooling means I have everything I need in source control to provide strong reproducibility guarantees. &gt; If you're going to test software, but only do it in an ad-hoc manner, you're not going to collect the full value of your efforts. I don't know about that; my back of the envelope calculations suggest that testing against a database is a much better value than the dogma. I'm sure in many cases, the dogma delivers a really good value, but I have a hard time believing it delivers the best possible value in all cases, especially this one.
* goimports solves half of your problem * Unused variables are usually bugs so even though they can be annoying sometimes I actually feel thankful the compiler works that way. I use two different setups. One is very basic, opening 3-4 tabs on my Ubuntu terminal and use 1st for the editor (vim), 2nd for compilation, 3rd for git etc. It's so simple that it needs almost zero configuration. The other involves using the [i3 window manager](https://www.youtube.com/watch?v=j1I63wGcvU4) and similarly I open 3-4 tabs/windows for the different stuff I need. The advantage of those setups is that I can switch between tabs incredibly fast, especially with i3 and I don't need to move my fingers away from the keyboard too much. I think the complete solution is a combination of getting used to the unused variables, increase your speed of changing them through your setup and learning to mentally not leave unused variables.
Come back when you can speak as many languages as him.
https://godoc.org/time#AfterFunc
To add to this. You'll need to take the current time modulo your interval and use that as an offset first. After that you can call `AfterFunc` and your timing will be properly aligned.
Perhaps, but gmx ain't.
Add a yearly cron job firing on Sep 1st, check if current year is 2017 or later, if so add the hourly cron job?
I think you meant to make that a question. You should end with a question mark in that case. De nada.
Nice, I might actually switch to using c-for-go to generate the bindings.
There is also https://github.com/jasonlvhit/gocron
&gt; It's also going to be much less reliable than a database I get what you're saying, with the DB having reviews and bugfixes and such, you get the impression that it's more reliable. But that's a false sense of security. The LOC count alone is a sign that it won't be so. &gt; here's a teeny tiny chance that my tests will report a false positive due to some bug in the database, but that chance is many, many orders of magnitude smaller than that of me building a faulty fake How did you calculate that chance? FWIW, I've been down that road, letting a third party do the lifting, and have been bitten. Not going there again. &gt; dependency management tooling means I have everything I need in source control to provide strong reproducibility guarantees There's yet another layer of complexity you're introducing into your test environment... instead of keeping all the source code in the same tree now you have to wrangle with dependency management. &gt; my back of the envelope calculations I'd love to see your work here. &gt; I have a hard time believing it delivers the best possible value in all cases Sure, there are no absolutes, but for those of us who've made software test our bread and butter, our job is to to be dogmatic and to share the dogma as gospel. You're free to chart your own path, handle your own customers as you wish... that's no butter off my bread.
I hear ya. I've been bitten twice by shadowing outside of error handling though. So have a few coworkers. It's just really frustrating to debug and not always obvious.
Try this: c.AddFunc("0 0 0 1 * 9 *", func() { c.AddFunc("@hourly", func() { fmt.Println("Every hour") }) }) Should get you your Sept 1st start. Of course add in checks for the year.
The instruction encoding of AVX instructions is a bit tricky. Read the Wikipedia article on the VEX prefix to understand how it works.
&gt; How did you calculate that chance? FWIW, I've been down that road, letting a third party do the lifting, and have been bitten. Not going there again. It's an estimate. I've never ever encountered a bug in any of the databases I'm targeting, and they have very strong reputations for robustness. Some of the world's most valuable data is stored on these databases. Even better--if there *is* a bug in the database, and my compiler generates code that does the right thing in spite of the bug, my users are happy. On the other hand, I frequently write code that is buggy; I'm not comfortable testing against an untested system, and you've provided no rationale for why I should change course (only that you've been bitten trusting a third party, but presumably never been bitten by code you've written). &gt; There's yet another layer of complexity you're introducing into your test environment... instead of keeping all the source code in the same tree now you have to wrangle with dependency management. Any reasonably sized project has to deal with dependency management anyway, and projects that aren't reasonably sized can necessarily live without reproducibility. In any case, I'm solidly in the former camp. &gt; Sure, there are no absolutes, but for those of us who've made software test our bread and butter, our job is to to be dogmatic and to share the dogma as gospel. You're free to chart your own path, handle your own customers as you wish... that's no butter off my bread. I sincerely appreciate your honesty. Your incentives are very different than mine, and I think this is why we have different opinions here.
the alternative is to simply slap Cloudflare in front of S3 bucket - even free plan works nicely, or if you want the extra features and/or have to support non-SNI compatible clients the $100/month one is plenty.
Exactly. I find that somehow I need two way communication, e.g. enable the detailed debugging on this host for a minute. AFAIK, Prometheus is not capable of "Consulting and changing application configuration" https://docs.oracle.com/javase/8/docs/technotes/guides/jmx/index.html &gt; Typical uses of the JMX technology include: 1. Consulting and changing application configuration 2. Accumulating statistics about application behavior and making them available 3. Notifying of state changes and erroneous conditions. 
Sorry for the ambiguous description. I need a JMX like thing which works for Go so I can view and change my application stats.
Rather than implementing this directly in the application, I would model this using Kubernetes: https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/ And have your app do the thing once via a CLI command that would be executed in a container.
At the database level, typically yes. The statement is prepared on a single connection and can only be used there (at least on the RDBMs I'm familiar with). But the Go db library - database/sql - provides pooling, and abstracts away the prepared statements. The db.Prepare() you run just tells the library about the statement, and it'll then run real prepare statements on each underlying session as needed (or it won't - there's no obligation for db.Prepare() to map on to an underlying PREPARE statement, though I'd usually expect it to). I've not looked at the code, but I'd expect it to do that lazily.
This will sound snarky but it's not meant to be. Go is not a language that's suitable for this type of programming. You should arrive at your editor with a quite concrete plan of attack, and go about implementing your idea—as opposed to toying about with experiments so informal that these aspects of the language become burdensome. If you can change the way you think about prototyping, and change your behavior in turn, you're going to have a much better time.
This is actually extremely valuable insight. Because the language/syntax are small, it seems clear that you can write programs quickly - so your advice tempers this expectation. Do you have any other examples which show that this is the way Go is "meant to be"? After all, the approach you outline can be done with any compiled or interpreted language - there's no language you can't approach with "a quite concrete plan of attack". I'm curious what other language or tooling features in Go make you think that Go is more suitable for the type of programming you outline? Thank you.
The comments here seem to focus on panic vs. error (I have a strong opinion on that too), but I would be more worried with how this is implemented and if such convenience functions should be tied to `cryto/rand` at all. Another concern is poor implementation, e.g. `Uint64Range` doesn't implement a fair selection for ranges that do not divide a uint64 evenly. `math/rand` takes care to get such things correct. IMO, a much better solution (that can also solve/work-around the panic issue) would be to split the problem into two parts. First, implement a [`rand.Source64`](http://golang.org/pkg/math/rand#Source64) using `crypto/rand.Read` and preserving/exposing any error (since none of the `math/rand` routines can fail). I.e. something like this: package crypto import ( "crypto/rand" "encoding/binary" ) type Source struct { Err error } func (Source) Seed(int64) {} func (s *Source) Uint64() uint64 { var buf [8]byte _, err := rand.Read(buf[:]) if s.Err == nil { s.Err = err } return binary.LittleEndian.Uint64(buf[:]) } func (s *Source) Int63() int64 { x := s.Uint64() return int64(x &amp; (1&lt;&lt;63 - 1)) } Second, any desired convenience routines that generate random things can take a `*rand.Rand`. This is highly desirable to avoid the abuse and overuse of `crypto/rand` where it is not necessary. Where actual cryptographically secure pseudo-randomness is necessary, a caller can just put these together something like so: src := new(crypto.Source) rng := rand.New(src) // use rng with convenience functions and/or `math/rand` methods _ = somepkg.AlphaNum(rng, 8) _ = rng.Perm(8) // check for any errors and respond in a sane way (a user visible stack dump is not sane IMO) if err := src.Err; err != nil { // if we're `main` just exiting with a simple error report is sane log.Fatalln("crytpo random failure:", err) }
&gt; only that you've been bitten trusting a third party, but presumably never been bitten by code you've written I've been bitten by both. But as I said, it's better to be bit by one's own code as it's much quicker and easier to fix... I'm sorry, but I can't provide a better rationale for my methodology without better knowing your user stories, your stack, etc. and that's more work than I'm willing to do pro bono. I'm always looking to learn and improve my methods, and I know that there's a balance to strike between being pragmatic and dogmatic, but the numbers support leaning towards dogmatism... error rates always spike when teams play it fast and loose, and bug fix costs grow near exponentially the later a bug is found. Anyways, you've heard enough, thanks for the exchange.
&gt; I would like to note that some functions in both math/rand and crypto/rand also panic under certain conditions. Importantly, these conditions are all for programmer errors, and not user input or other possible errors.
quite sure it's not the function call overhead, but rather the switch from normal instructions to avx instructions. Loading from "normal" register to AVX reg incurs a transition penalty. The best thing you can do is to saturate out a AVX register at the start (using some broadcast strategy), and then perform your FMA from there. The key is to change the MOVSDs to VMOVxx at the top of the function call, and don't use MULSD. Use the AVX version of MUL (VMULPD/VMULSD) 
Microservices rock. 👍🏻
`man cron`
Is `vmovd` really that slow?
Yes, and my point was that it's a lousy idea in Java, which makes it twice as lousy an idea in Go, because it doesn't even have the peer-pressure excuse. For viewing, you might want to use expvar, which is in the core and has integrations with some things. For changing configs... no, don't just casually do that. Don't even think about using a thing that makes it easy :)
As the others have said: goimports. I use Atom with the go-plus plugin that installs that and a bunch of other nice things by default. For unused variables: for me, I usually only have these if I'm in the middle of a new function I'm writing and wanna test what I have so far, so I usually only have one or two variables hanging out at the end. I either assign them to `_` instead of names, or just assign the names to `_` later, so that the variables are "used". e.g. file, err := os.Open(filename) if err != nil { return err } _ = file // here, `file` gets used so its not an error
Sounds like you're trying to write Python, but using Golang instead.
no. but you are moving constants into the Xmm registers which are SSE registers. And the AVX instructions really prefer it if you use the Ymm registers. I'm no expert at this - I just cheat off agner fogg's work all the time but I've ran into these situations before
I posted this on HN as well, but for those here I will quote it: &gt; If anyone was curious why the instruction address didn't match up, it's because he read the instruction identifier wrong. It's also totally counterintuitive and you have to essentially read the full documentation to get to any real information about it. Anyway here we go. VEX is a prefix notation with specialised encodings: &gt;C4 - For a three-byte instruction (which vfmadd is) this is C4, if a two-byte instruction this would be C5. &gt;E2 - This one is more involved but basically for this instruction the first three bits are 1 for setting some modes on it (R, X, and B). Then, because this is an 0f38H instruction, the next 5 bits are 00010. Altogether you get 11100010 (E2) &gt;E9 - this is also involved, but the first bit is a W mode. This is 1 for the instruction he used but it's pretty much ignorable. for bits 2-5 it has to do with the first source register the instruction uses. This is 1101 because it uses XMM2/YMM2 (it's by lookup in a table) first. bit 6 is set to 0 if it is a 128 bit vector (it is). bit 7-8 are set to 01 based on a lookup table for the "pp" value which is 66 in the instruction identifier. Altogether that's 11101001 (E9). &gt;A8 - this one is easy, it's the opcode. &gt;C3 - this is the ModR/M byte which is actually used in this case for reporting the register/memory operands. first 2 bits are 11 to indicate the first operand is the register itself (not displaced, or truncated). The next 6 bytes are the register codes. 000 is actually not used since the destination is overriden for float maths), and the 011 is EBX as the base cpu register to source the data. Altogether it works out to be C4E2E9A8C3. Not intuitive at all really. &gt;Edit: Please someone correct me if I missed something. I hate this kind of stuff and I'm sure I made a mistake. E: formatting
Not really, you can use xmm registers just fine, but you should use the VEX encoded variants of instructions that use them as they zero out the high part of the register.
Note that there is no `vfmadd123pd` because two of the operands commute, so `vfmadd123pd` would behave just as `vfmadd213pd` but with write back to the second operand instead of the first.
was your bytes not VEX encoded (commuting and on mobile)
I didn't write the article, but in the article, OP indeed mixes legacy SSE instructions with VEX encoded SSE instructions which is a huge no-no.
I really disagree with this. Goimports on save resolves import problems immediately, and once you've used go for a month or two you'll reflexively delete or comment out unused variables as you code.
Windows VSCode goimports on save (on by default with the Go extension) and writing sufficiently small functions that unused variables are never a problem (always easy to tell when they come up)
&gt; you'll reflexively delete or comment out unused variables as you code Very simply "or comment out" can be done programmatically. It is possible to run a script htat says `/* commented out as unused by script xyz on sfdf*/ //` such as by a script that repeatedly tries to run to run the compiler and comments out those lines. I'm not going to be the 1 person in 300,000 that has thought of this.
if by that you mean that I make many iterations on working code while I prototype, yes, I get shit done :)
Ah. Which is why you use a proper assembler to write out the bytes.
I feel like you'd spend more time deleting those comments than it would save you
November :)
Great article, interesting read.
Every comment from those more knowledgeable such as you and FUZxxl makes me really want to get off my lazy arse and start reading up on all this stuff again.
I'm far from knowledgable. Like I said, I mooch off agner fogg's work a lot
I believe that 1.9 blocks on lack of entropy which is a probably d appropriate way to handle it. 
should also be an automatic step (I thought this was obvious, sorry.)
I use (GNU) Emacs for most of my Go coding. Goimports solves the unused imports problem by standing in for gofmt, since I almost always gofmt my code before running tests or building. Some people have Emacs set to auto-gofmt (ie auto-goimports) when they save files, but I don't like that because there are some situations where goimports can get it wrong. To spot unused variables and other problems before running tests or building, I like Emacs' flycheck mode. This will more or less immediately highlight a whole collection of problems (not just unused variables) as I'm writing code or saving the file. Provided that I pay attention, this works great. The other thing that you can do in Emacs is to run `go build` and/or `go test` from inside Emacs. When you do this and errors happen, Emacs will let you immediately jump to the location of the error. This reduces fixing these problems to `M-x compile`, click on things, apply edits, save again, repeat. 
How do you automatically remove a comment, how does it know what you want to remove?
I don't know, hence this thread? It could have a 'remove-by' date (like an expiration date on old milk) in some automatically read format and just get deleted after that date. I don't have a specific suggestion here. I was asking what hacks people actually use.
&gt; Though I'm all for semantic versioning I like the idea of being able to carry over usage gracefully into a new (major) version without necessarily requiring action on their part Based on reading Russ Cox's article it doesn't seem like that is what he intended: https://talks.golang.org/2016/refactor.article#TOC_5.2. In his example he describes adding the type alias in version 5.2.0 and then finally removing the old API in 6.0.0. So it seem like the intention is not to carry over usage gracefully into a major version. From what I gathered its more about not having to change all the places it is used within a single repo in one commit. That seems a bit odd to me since if you only change a few places and leave a lot of them not changed within a single repository you are taking on some technical debt. This seems like something that maybe should be avoided if possible? You now have a lot of places that need to be updated that haven't been updated yet. I guess it might be useful if you have a several million line code base and you want to avoid constantly fixing merge conflicts since other developers might be constantly changing the code? 
You want your code to automatically delete itself if you don't make sure you uncomment it? OK, because you say you're looking for what other people are using: No one is using this idea of a script that automatically comments things out. It is infeasible for a number of reasons. I already mentioned what I do.
I gave it another read! I think reading it again might have helped. Correct me if I'm wrong but the problem is: updating all of the locations where an old API is used might be to difficult to do in one go. I think I can see why it would be a problem if a repository is large ( &gt;= 500k LOC) and very active. For example: you want to change all the instances where an old API is used but everytime you try to merge the change you run into merge conflicts because the repository is very active. Does that sound about right? If that is the case it sounds like type aliases might be a bit overkill for those of us who work in smaller ( &lt;= 100k LOC) or less active repos where we can easily change all the usages of an old API in one go. 
I print all variables that aren't used yet in a fmt.Println(r, err) since an underscore almost guarantees that the next bug will be what I'm ignoring. Then once I think I'm using all my variables correctly, I also search for fmt.Println since when I'm done, I should convert to log.Println even if I am still printing to a console. I've seen others favor snippets to expand an "if err != nil" right away. I don't like snippets because it's more to remember. One of my coworkers just types out the err checks in the beginning and it doesn't bother him. A junior dev on the team does nothing and gets bitten. :) I'm not sure what they do for non-err variables that aren't used yet. I lean heavily upon Git, happy to delete code, and combined with my Println, never get bothered by the language feature. My other details probably aren't important in this case since what I mentioned isn't very fancy, but I'll still be happy to share: Win 10 and deploying to Alpine in Docker; VS Code; just updated to Go 1.9 and Angular 4.3.6 for ongoing work. Running apps are mostly Go 1.8.x and no frontend, an Angular 4.2ish app, and then the other teams are mostly .Net Core, one massive AngularJS app with .Net Framework, two apps with Knockout, and statistics scheduled in Python.
I see ! You clearly know much more than I do. Frankly, I have no idea on some of the things that you are talking about here. :D The whole blog post was just an experiment. I'm afraid it will take me lot of time and effort to do what you are saying :(
Finally !! I get some explanation !! Thank you so much. All of this is some magical wizardry to us highlanders. I have never dealt with assembly before. No wonder I was doing it wrong. The fact that I even managed to get it work amazes me :D
You are right, I guess. I tried to overthink that there might be some deeper reason.
Maybe check out expvar.
[removed]
[removed]
Essentially, the order of execution here is: 1. First 'pow' call 2. Second 'pow' call 3. fmt.Println using results of first two The Println in 'main' doesn't evaluate until both of it's arguments have been evaluated. Thus, the results of the pow calls won't be printed until the very end. So: 1. pow(3, 2, 10) is evaluated. Nothing is printed to screen, function returns 9 2. pow(3, 3, 20) is evaluated. '27 &gt;= 20' is printed to screen, function returns 20. 3. Now that both arguments have been evaluated, fmt.Println(9, 20) is evaluated, '9 20' is printed to screen. Ninja edit: I just realized that your confusion may stem from the difference between printing text to screen and the return value of the function. The first call to pow returns a value (but doesn't print anything to the screen) to the caller, main. The second call to pow prints a line to screen and also returns a value to main. Then main goes and prints the return values of both calls to screen.
pow(3, 2, 10) and pow(3, 3, 20) are arguments to fmt.Println. They will both finish running before the Println in main() prints anything. That's why "27 &gt;= 20" prints first. After both functions return, fmt.Println "9 20", which are the return values of pow(3, 2, 10) and pow(3, 3, 20), respectively. pow() always returns a value, whether or not it prints something.
I am not sure what you mean. Note, I did not write the article. OP had to manually specify the bytes that make up `vfmadd213pd` because the Go assembler does not support this.
&gt; Correct me if I'm wrong but the problem is: updating all of the locations where an old API is used might be to difficult to do in one go. No, the problem is unrelated to go. The issue is, that "all the locations where an old API is used" can be spread over many, many independently developed repos. So you can't commit an API change and the fix to its users in one commit, because they are not in the same repo. So you need to sequence them in a way that it's fine that they are split. And even if you *can* submit both in one change, because you use a mono-repo - in practice, that one commit will need reviews and approvals from many people, so will take a lot of time and anyone developing during that time will get merge-conflicts when you submit - so you are effectively still blocking a lot of work. &gt; If that is the case it sounds like type aliases might be a bit overkill for those of us who work in smaller ( &lt;= 100k LOC) or less active repos No, they are *far more* useful in smaller or less active repos. If a PR to migrate to a new API takes long to merge because it's inactive, it will stay broken longer after a breaking change was introduced. So making fewer changes breaking will give people more time to apply changes. Not the size of the *repo* is important, but the size of the *codebase*. And every single open source project works in an *gigantuan* codebase, split over many, many repositories with tens and hundreds of thousands of developers.
&gt; No, the problem is unrelated to go. I should have maybe used another word. I meant the word "go" not the programming language. &gt; The issue is, that "all the locations where an old API is used" can be spread over many, many independently developed repos. If the code is spread over many repos then semantic versioning can help solve the problem (and is generally how its been done). &gt; No, they are far more useful in smaller or less active repos. If a PR to migrate to a new API takes long to merge because it's inactive, it will stay broken longer after a breaking change was introduced. So making fewer changes breaking will give people more time to apply changes. I don't think I understand. I don't see how type aliases are more useful for small repos. If you have a small repo that is versioned and you have two places where a function is called it seems trivial to bump the major version make the change without the use of type aliases. Why would you need to spread changing the two lines across more than one commit? &gt; Not the size of the repo is important, but the size of the codebase. And every single open source project works in an gigantuan codebase, split over many, many repositories with tens and hundreds of thousands of developers. I think you might be exaggerating here. Hundreds of thousands of developers? I don't believe the Linux kernel has had 60,000 developers contribute code.
I like it, very clean. The one thing that bothers me is your tiny utils package. In general it's not a good idea do have utils package in your repo. It's often just a bunch of functions for which you didn't take the time to find their right place. For example utils.Run is used only once in the cmd package. In my mind it's fine in the cmd package. Hope that helps Cheers 
Right, I didn't think it through completely yet, since I was sure others have. However it is also interesting that I [got the feedback](https://www.reddit.com/r/golang/comments/6woy10/if_youve_solved_this_what_is_your_secret_hack/dman9pl/) in this thread that this wasn't really the "Go approach". Any thoughts on that?
thanks, this was really interesting.
This thread started with me giving my thoughts on that. Once you've used go for a month or two you'll reflexively delete or comment out unused variables as you code. An IDE that highlights unused variables is usually sufficient, but if you think ahead of time you can iterate fast without worrying about it.
&gt; I should have maybe used another word. I meant the word "go" not the programming language. =D No, you did it right, I just can't read, apparently :) &gt; If the code is spread over many repos then semantic versioning can help solve the problem (and is generally how its been done). See Russ' article for how semantic versioning relates to this problem. In short: It's orthogonal. Semantic versioning is about preventing incompatible versions to be installed. Gradual repair is about making more versions compatible. &gt; If you have a small repo that is versioned and you have two places where a function is called it seems trivial to bump the major version make the change without the use of type aliases. If you bump the major version, all users of your package will now have to be fixed (to bump to the new major version). Until then, not only will they fail to get any bugfixes, they will also not be able to be used with *other* users of your package that are already bumped. If, on the other hand, you don't *have* to bump your major version to do the change, not only will users of your package just continue to get all upgrades, you also reduce version conflicts. The "small less active repo" that is helped in this scenario isn't *your* repo (the one with the API change) but your downstream users and their users, which will take a while to apply the fix - because they are small and less active. &gt; Why would you need to spread changing the two lines across more than one commit? Because a commit can only ever apply to a single repository, whereas the definition of an API and the user of an API are usually split over multiple repositories. So you'll never be able to apply both the change and the fix in one commit. &gt; I think you might be exaggerating here. Hundreds of thousands of developers? I don't believe the Linux kernel has had 60,000 developers contribute code. Github has [23 Million users](https://github.com/home). Again, I'm not talking about repositories or projects, I'm talking about *codebases* and "the open source world" as a whole is a codebase (Russ calls it "go get's codebase" in his article). People should start realizing that they are not working in a vacuum when doing open source. You are using other people's code and other people use your code. That is what open source is *about* - collaboration. People should feel empowered and responsible to keep the ecosystem as a whole healthy, instead of siloing themselves off of it. The whole idea behind SemVer and Vendoring is to *not* collaborate and instead isolate, which is why it's fundamentally not solving the issues we are talking about. You should always keep in mind, that while you are solving the problem of "how can I develop my shit without interference by other projects", distributions *and your users* have to solve the problem of "how do we integrate all this software by people who ignored each other". You have sets of people working *against* each other, wasting enormous amount of brainpower. The idea of gradual repair (as furthered by type aliases) acknowledges that open source is an ecosystem as a whole, that changes don't happen in isolation and that we should try and keep our projects and packages compatible and not break them, to stop wasting so much energy on integrating them again later. (This rant brought to you by a person who tried - and failed - for months to get a new version of jekyll into debian a few years back, having to untangle license violations and version breakages created by people vendoring libraries and files and then just letting them rot)
I agree an IDE that highlights unused variables sounds like it would be sufficient. does yours? which one?
&gt; So it seem like the intention is not to carry over usage gracefully into a major version. The great thing if all refactorings and API breakages can be made gradual is, that you can give a stability guarantee akin to something like this: If you develop for version N and adhere to all deprecation notices defined for it, we guarantee that you'll be able to build with version N+1 (but potentially not with version N+2). That is, we promise to a) announce any breaking API change at least one major version in advance (thus at least $release_cycle time before it happens) and b) provide you with a way to be compatible with any two consecutive versions. With a promise like that, any user of your package can, when you release version N, safely declare a dependency on *either N or N+1*, even before N+1 is released. Thus, when you finally release N+1, they will continue to be able to co-exist with other libraries that already take advantage of APIs introduced in N+1, even without action on their part. They can then, in their own time, come and apply the fixes announced in N+1 and bump their version to N+2 and continue. That way, if we can at least kind of agree that every software should get some maintenance at least every $release_cycle (say, 3 months), the ecosystem as a whole will literally have gone rid of version conflicts. A major version bump will no longer mean you are breaking your dependencies - it just means you are resetting the clock until they need to migrate to the new version to avoid a breakage. Of course, all of this is oversimplified. In practice, not all software will always be fixed in time. In practice, not all breaking changes will always be planned this way. But, in practice, even if only a *subset* of all software would work like that, it would already be a significant win. In that sense, it's like SemVer - SemVer is a social contract, you are saying "I promise to plan and announce API changes in this way and if you adhere to these rules, your stuff won't break". In practice it's not perfect - there are projects that don't use SemVer and there sometimes are changes violating the strict SemVer semantics by accident. But it's still a win for compatibility and makes the job easier. A gradual repair social contract works very similar.
Fuck off 
Go is nice, but it is a wrong tool in your case: your tasks are easy and it is not a big difficulty to write the same using C++ or even C.
Expose an interface that has the method you need from whatever method instantiates your collector struct. This will then hide behaviour that is not declared within the interface definition.
Was thinking the same. Love go, but it does seem at odds with what's really needed in this sort of embedded work.
If the field is unexported, users from outside the package can't access it anyway. I've had the same situation as well, it turned it out it didn't matter in practise. The only time you access the unexported field in a method not belonging to the struct is in a `New` function. I think you are overthinking it.
Encouraging results. Well done
The way to deal with this is to simply not access the field. This discipline will only apply to you, because users of your package won't see the unexported field. Accidentaly accessing a "private field" of another struct just doesn't happen.
Turns out programmers are perfectly capable of managing internal state within their own package without private/protected/public directives. Who would've thought..
I'm afraid of accidentally using it after a few months of not working at this project. With "others" I mean people which will probably modify my code. A colleague of me (sysadmin like me) which is closely working with me at our infrastructure is starting to get his hands dirty with Go. My fear is when he gets starting to work on one of my projects, that he does not read the docs well enough. I think I stick with the solution from /u/elsyms. Seems reliable to me, but I was hoping for a simpler approach like just change the first char of the field :-). 
I would not recommend that as that leads to too many unnecessary interfaces. I would just solve the problem when it actually appears and I believe it won't really appear.
Thank you. I think I stick with this solution. Had this already in my mind but I was hoping for something simpler like just modifying the first char of the field like in my approach. But life isn't always a bowl of cherries. :-)
Thanks for you answer. I know how to export/unexport. My intention was to hide fields of a struct from other funcs inside of the same package.
Thank you for answer. I was talking about access from inside of the package :-). For example method "a" in the same package cannot access field "c" of struct "b" because field "c" should be only available to the "b" struct himself. I tried to make a suggestion to improve Go. Look at /u/elsyms answer. This is exactly what I need but I want it simpler.
Yeah I'm capable of this as well but I prefer to enforce things like this. It can not hurt when the compiler has my back in such situations. This was just a simple example and in bigger packages things can get more complicated. I'm just a human and humans do errors. Who would've thought...
That is my fear as well. But for this small project it just does the job. I would prefer something like I suggested in bigger projects. Just to have the compiler to watch my back in situations like this.
Inside the package is like inside a method. Inside a package you should know what you do and unit test it. Get used to it. Is the best advice I can give. There is nothing to improve or change here.
&gt; I was hoping for a simpler approach You could try trusting your colleague.
The fact that `ipRanges` is not exported (it starts lower-case), to my eye clearly marks it out as a private field within the struct that I should deal with very carefully. If you want to make it only available inside a `collector`, separate that out into a separate package if you must, but YAGNI. Probably. If somebody decides to add a function to that package that uses `ipRanges`, they probably did so for a reason you have not yet anticipated. At that point, you either refactor, or you realise you were over-designing by preventing such access right now. I'd say all is well, but if you're being paranoid, pull it into a separate package.
Agreed, just because you can, doesn't mean you should. Best tools for the job, etc.
But why not try to make things more clear or tough against false use. I like to tackle possible bugs before they occur :-)
Yeah that's true. Maybe I just didn't think simple enough like the Go idiom even after 3 years of Go (I'm not a full time programmer). I started programming with C#. Maybe some leftovers from the past.
Because it requires hoop-jumping and adding complexity. I don't know your colleague. Perhaps he really can't be trusted to read and follow the docs. But it sounds a lot like premature optimisation to me.
If you're that worried, which is fine, put it in it's own package. I end up with a lot of small packages. My co-workers have said that combined with the godoc this seems to be easy to understand. Also bear in mind that even if you do that, anybody can come along later and move it back anyhow, or capitalize the field later. It's not like you have any options other than "strong suggestion" anyhow.
Is 128MB a "low memory device" these days? Genuinely curious. 
It really depends. Using a blanket terminology like "low memory device" doesn't indicate what the device is for, what sort of architecture it has, interfaces, etc... A raspberry pi is a low memory device when compared to other general purpose computers, but it's far more than the 512K found in a TI MSP430 based device. Running a camera along with the presented image management system would probably be doable in 8MB or so with some optimized C/C++.
yeah, the "you" wasn't specifically you.. it was a more general "you". In the past what I did was basically write the assembly then used an assembler (`as`) to print out the bytes and re-write them as go assembly. That way all the encoding etc are handled properly
You're talking about the right tool for the job but you don't work there or know the long term goals? So, this is more just like.. your opinion man.
Yeah. However, this rule is fairly easy to obey without resorting to this kind of trick: All vector instructions beginning with `v` are VEX encoded, all other instructions are not. Almost each non-VEX SSE instruction has a VEX-encoded counterpart with the same mnemonic with a `v` in front and a third operand.
Using Go for embedded does look like a cheap workaround and more like temporary solution if they have long term goals. Also, embedded coding is not something that really needs high level programming language with GC and reflection. These are trivial pieces to code, the challenge is to make them smaller and sometimes as fast as possible. Go is nowhere near as good as C or C++ (or Rust) for this purpose.
K
If they could use mmap-ed memory for those 16MB, then they need no GC hacking.
That's what I was thinking - an Arduino or something is VERY low memory!
Or they can do the complete opposite by such clever things as:- #define public private #define private public
Sadly the fact that this comment, which most embedded engineers would easily agree with, is downvoted to hell speaks volumes about this subreddit :( EDIT: When I wrote this, the parent comment was at -5. I'm more than happy to be proven wrong.
Cool article, kudos! :)
Thanks, I'm planning to drop the dependency on the git command and replace it with the [src-d/go-git](https://github.com/src-d/go-git) library after releasing v1.0 so util.Run and util.Git will no longer be necessary, I'll nuke util.FileIsLink later.
It's just two assignments on one line, equivalent to n = z z = z-(z*z-x)/(2*z)
Sorry for mobile formatting in advance. 'n, z =' is starting a double assignment. See that the right part of he equal sign is split by a comma too? The left will be assigned to 'n' and the right to 'z'. The advantage of doing it like this rather than in two lines is, that the right side is evaluated first and thus the old values are used and not the potentially changed ones. Imagine you have to ints 'i' And 'j' and you want to swap their values. You can do this easily with 'i, j = j, i' 
https://golang.org/ref/spec#Assignments
Since a lot of stock routers and APs have anywhere from 8-32MB, I'm going to say that 128MB is a generous amount of memory. As you see, running Go isn't feasible under something like 16MB. Running it on some embedded devices takes quite some effort. If you're thinking of something like [NodeMCU](https://en.wikipedia.org/wiki/NodeMCU) then I'm going to have to dissapoint, there's zero chance you'll get any Go program running on 128KB of RAM, even if you would manage to fit it onto the 4MB available storage...
**NodeMCU** NodeMCU is an open source IoT platform. It includes firmware which runs on the ESP8266 Wi-Fi SoC from Espressif Systems, and hardware which is based on the ESP-12 module. The term "NodeMCU" by default refers to the firmware rather than the dev kits. The firmware uses the Lua scripting language. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/golang/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
I'd strongly suggest against using this library. If you use sprintf-like functions you'd need to safely escape your input before passing it to DB.
Could you provide an example of each? It would make discussing it a little easier without worrying about the definition that people are using.
If it's functionality that you feel that you're particularly likely to want to use outside of the class, maybe you should be exporting it. If it's not, then I wouldn't worry about it. Even if someone else does start using a method you considered private, it's still internal to the package, and shouldn't be difficult to change later. Also, even if there were access level specifiers, if you're worried about people who have access to the source code of your package, they could just broaden the access level specifier and use it anyway.
The example clearly shows variable binding; The concept of sprintf-like functions is about templating, when implementing one, you may choose to handle value escaping. 
Then what does it say when it's the top voted comment?
As nobody had mentioned it yet, I'd just like to add that this has nothing to do with loops. You can do multiple assignment anywhere, and it's one of the basics of Go's error handling system. The `result, err := DoThing()` pattern you see everywhere is multiple assignment. The important thing to remember is that it's *only* for assignment.
VSCode
I find it funny that people use Go for low memory devices. A wise man once said - "fit the tool around your problem, not the other way around". Trying to solve everything with Go is just taking it too far. Go is a bad choice for low memory devices. Use C for these devices.
Have you looked at something like https://stackimpact.com/ ?
That's what I thought. 128MB didn't seem like a huge challenge if you had it under a lightweight Linux OS
https://tour.golang.org/basics/6
Yes, 100x yes. This is often called method chaining, where a method returns the original object just so that you can call another method from the return value of the first. e.g. a.Foo().Bar().Baz() However, this is really bad practice in Go (and honestly, in other languages, but that's another story). In go, this means that your methods can only return a single value - the receiver. Not only does this prevent you from returning other useful values (like errors), it also means that your methods now "lie" about their return values. func (f *Foo) Bar() *Foo What does this method do? I don't know, but it looks like it might transform f and return a different value. That's usually the only reason you'd return a pointer. But if this is just for method chaining, then it doesn't return a different value, it returns the same value. This is making the signature of the method misleading. Often times, these methods will be able to fail (most interesting things you can do can fail) which means that then people do heinous things to get around the fact that they don't have multiple return values anymore... like stashing an error value away in the receiver, so then you have to do something like a.Foo().Bar().Baz() if a.Err != nil { return a.Err } This is hugely non-idiomatic, and just a bad idea. What part failed? Foo, Bar, or Baz? You don't know. It also means that now there's internal state that needs to be tracked around that error.... it's just a disaster. All this just to save a few line returns. even assuming these things can't error, just make them boring old functions: a.Foo() a.Bar() a.Baz() ^^ this is not the end of the world.
It depends. If your embedded application is on 16-bit MCUs with 8KB RAM, your only choices would be C and ASM. There are also boards with ARM11 and 32MB RAM that are capable of running full-fledged Linux. Some LTE modules are even equipped with a Python interpreter already. Don't forget Erlang, as an even higher level language than Go, was designed for implementing telephony systems which are generally categorized as embedded systems as well. 
Just any place you would use chaining, but I guess a concrete example would be with database/sql, say you had a map of prepared statements and you choose one to call `QueryRow()` and you chain `Scan()` so, `line 15: err := statementMap["somePreparedQuery"].QueryRow().Scan(&amp;someDest)` Then you run this and get a panic - invalid memory or pointer access. Is it a: you have a typo in your map accessor? b: your destination variable is not defined or nil/something else? I am afraid by giving a specific example the conversation will drift towards the one specific instance. There are some other places I have encountered this. I will say however the panic does typically end up being memory access related 
My Rpi has 512MB!
OP: Read the spec. It is comparatively short and you'll learn a lot.
No, by that I mean you're writing code that's peppered with unused variables and imports and expecting the interpreter to deal with it. &gt; I get shit done When "shit" is the product that's called for, that's fine...
From the README "Please note that the import paths have been moved from github.com/mvdan/sh/... to mvdan.cc/sh/... for 2.0. This will help future-proof the project by making it depend less on GitHub." Can you explain what you mean by future proofing? 
If you're worried that you might forget about the restrictions on collector when further developing this package in a few months, then just formalize collector into it's own package. That way you are forced to use it via the API it provides. Put this `collector` package as an internal subpackage of the package you are developing. Voila, you are now safe.
The day you stop learning as a dev, that's the day you know your career is over. :-)
Of course. If I was forced to move my projects off GitHub for some reason - for example, if it shut down, or if the terms turned bad - I would need new import paths. I would no longer be able to tell people to import `github.com/mvdan/foo`, and code importing my packages would have to modify their imports to adapt to my new import paths. However, if I use a vanity import at `mvdan.cc/foo` pointing at `github.com/mvdan/foo`, that's not the case. I can simply make `mvdan.cc/foo` point at something else. A vanity import is just a small HTML file that `go get` and other tools consume to know how to download the source code. In this case, it simply points at the existing GitHub git repo. Of course this may never happen, but better safe than sorry. It wouldn't be the first time people have suffered this with Go - see when Google Code shut down.
In /u/calcifer's defence it was "downvoted to hell" earlier.
&gt; Go is nice, but it is a wrong tool in your case: your tasks are easy and it is not a big difficulty to write the same using C++ or even C. It's also not a big difficulty to write Yet Another IoT Device That Has Buffer Overflows Leading To Arbitrary Code Execution if you write it in C or C++. Go isn't a bad choice if you don't mind having to keep track of a few things that you might normally otherwise ignore. If your program basically consists of opening up a few static buffers and moving bytes around between them with some care, it's not that hard to write something that garbage collects once every few weeks or something, and that GC running very, very quickly too. (Of course Go doesn't magically secure your code; you'll still need authentication and authorization code. But at least you're not inviting arbitrary code execution in Go every time you type `make([]byte)`.) I think people are getting a bit hung up on the word "embedded" here; better to just look at the specs. Yes, Go may be a bit of a tight fit here, but that's all... "a bit of a tight fit", not some sort of unmitigated disaster.
Yes. The library in fact doesn't even allow you to get an escaped SQL query. You have to use parameterised queries.
This is like [fatih/astrewrite](https://github.com/fatih/astrewrite) except it gives you more control and gives you access to the parent node(s). Also on deleting a node, it will automatically nuke any comments associated with it. Used mainly for https://github.com/OneOfOne/genx but I'd like some feedback.
Meanwhile in the linked doc: conds = append(conds, sqlf.Sprintf("name LIKE %s", "%"+filter+"%")) Correct me if I'm wrong, but if you want to use a literal "%" character, judging from this example you should escape it yourself. Bindvars for clients escape % that's why you see things like: "select * from users where uname like '%" + db.Escape(pattern) + "%'" If you need to use escaping, rely on your clients method of escaping. If you need a consistent abstraction like named variables, implement something like sqlx (NamedExec, etc.) on top of your database client. Don't do your own escaping, and if your database client has an escape function, use that. There's subtle nuances which the author here obviously missed.
&gt; However, this is really bad practice in Go Yes, it "works" much better in languages where all functions implicitly have two return values, the thing they return and the exception they may throw. If you do want to do something similar to this, I'd suggest that [functional arguments](https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis) is a much better way to go. I don't find it useful very often because A: having something that takes dozens of parameters is already a bad sign and B: functional options come into their own when you need validation checking on the arguments as well, which makes it even more unusual (if you don't need that, just use a struct and call it a day). There's a certain amount of controversy around functional parameters, so note I'm only claiming _they're better than method chaining_, not that they are necessarily "good". Still, I've got a couple of these in my code bases. The other thing I've come to hate about this pattern, even in other languages, is that something about it makes people's brains check out and they think somehow they're working in a different language all of a sudden. They stop considering the chains themselves as abstraction targets, they stop realizing it's perfectly valid to: val := X.Chain1().Chain2() if cond { val.Chain3() } commonChains(val) val.DoTheThing() func commonChains(val *val) { // "Consider this as a single thing for my purposes" val.Chain4().Chain5().Chain6() } Alternatively, you get chain-based APIs where the API authors themselves forgot those are all syntactically valid and things blow up if you take copies of a half-instantiated chain or something. I find method chaining distasteful for reasons I'm still not sure I can 100% articulate, but I _loathe_ what it does to programmers. I can't explain why it does that to programmers; perhaps I just encountered my first chaining API far too late in my development (I think I was ~7-8 years in before I first saw one) so they were never the limits of my abstraction capability because I could always "see through" them to the implementation underneath. It's hard for me to explain why something happens when it never happened to me.
You are right, that is a bad example for the like statement since the filter could include `%` or `_` (in the case of postgres). However, it is equivalent to your example with db.Escape, since db.Escape just escapes a string, not a like expression.
Is your username a reference to The Third Policeman?
That makes sense. Good argument.
ATMega328 which is commonly used in Arduino has 2K of RAM.
Running the standard Go runtime isn't feasible under 16MB. Using Go is, with something like emgo: https://github.com/ziutek/emgo Admittedly it's a pretty niche and immature project.
&gt; The problem is how do I enforce myself and others not to add to collector.ipRanges directly and instead use FeedIPRange Personally, my policy is to never access unexported fields outside of methods of the type itself (and maybe tests); even if I can. There are exceptions to this, but as long as this is my default policy, I am always *intentional* about doing it if there is an exception. Like, I think "why am I accessing this unexported field? ah, right, it's okay here, because…", preventing problems.
&gt; Over 79% of vulnerabilities found in Go Open Source libraries aren’t public. This means that if you’re not a customer of SourceClear you are only seeing the tip of the iceberg. The tip of a large and scary iceberg. Besides this alarmist non-sense that you have there, which by the way someone should be fired for, if you'd care about the Open-Source community at all you'd help it improve for the most frequent projects used. And you'd make the service free for the maintainers so that they can use it and then promote you. But instead, you chose to be all alarmist and no show. And fyi, I really can't wait to get home and see where that int overflow can cause a denial of service because that would be a good one to see if your tools are any good or not.
w in this case is something that implements the [io.Writer interface](https://golang.org/pkg/io/#Writer), so it has a Write method. when they say "write to w", they mean call the Write method
Ah, I see. Cool. Thanks!
Its expecting an instance that have an implemented Writer interface: `type Writer interface { Write(p []byte) (n int, err error) }` An instance will read the stream of p []byte and will write it into a sink, e.g. buffer, terminal, memory.
&gt;Go isn't a bad choice if you don't mind having to keep track of a few things that you might normally otherwise ignore. Couldn't that same care be taken to secure a C++ program? 
Not a fan of the style in general, though it is sometimes useful. Regardless, this is legal (and how gofmt fmts it) a.Foo(). Bar(). Baz()
Misuse of runtime debug interface doesn't sound as a profound solution either.
It is indeed, we could all learn a lot from the wise philosopher de Selby.
Fear isn't a good signal for making design decisions.
&gt; That's usually the only reason you'd return a pointer. Not necessarily, you are assuming too much based on the signature, method chaining is also a possible use cases (which predates Go, btw). &gt; This is making the signature of the method misleading. Only if you assume that No True Method will ever do method chaining. &gt; Often times, these methods will be able to fail [...] which means that then people do heinous things [...] like stashing an error value away in the receiver &gt; [...] This is hugely non-idiomatic, and just a bad idea. What part failed? Foo, Bar, or Baz? You don't know. It also means that now there's internal state that needs to be tracked around that error.... it's just a disaster. Isn't that the whole point of [Errors are values](https://blog.golang.org/errors-are-values)? Find a way to manage errors like you do with other values, using state variables if necessary? Maybe the receiver is able to keep track of which method failed. &gt; All this just to save a few line returns. even assuming these things can't error, just make them boring old functions: Agree, the fluent interface is useless when used only for style, but there could be good use cases for it, including handling errors.
And what happens if someone compromises your website and changes the go get source to something malicious?
The "few things" in question in my quote there are things like keeping track of your []bytes and reusing them and making sure not to allocate too much that you don't need to. Different, and substantially more, care needs to be taken to secure a C++ program. I'd actually resist anyone's suggestion to use C++ at this point... it's just too magical. In C++ it's very non-trivial to figure out what `a = b` is actually doing; is it allocating a new buffer for a, or is it copying, or is it doing something even more exotic? And the language just gets more magical and bizarre with every standard they release.
Yes, if you are extremely skilled at C++, it is safe to use. I'd still want to add some static analysis tools to be sure, though; it's _real_ easy to turn out not to understand what your C++ code is actually doing. The language has basically gone batshit insane [1], and shows no signs of slowing down. But it can be done. The path from "not knowing C++" to "extremely skilled" has gotten fairly long in the past 10 years. By contrast, Go is safe to use in almost all cases (just about the only thing you can muck up here is reusing buffers and leaking their old contents, which is a thing you need to watch out for), and with just a bit of care, its performance will be fine. The skill required to use Go successfully in this case is literally years of experience less than the skill required to use C++ here. (Which is itself probably a good 2-3 years less than the skill to use C here, which I would consider basically suicidally insane at this point.) [1]: Since this is /r/golang, what may be the simplest language in common use right now, I'm not just saying that as a Go user. I like Haskell. Pretty much all of it. It goes crazy sometimes but it at least considers experimentation an explicit goal. But C++ has totally gone round the bend in complexity. Its committees should be talking about what they can remove from the language, not what they can add, and since "what they can remove" is pretty much nothing, I think by far the sanest course the C++ committee could take right now is to just declare the language done.
Why would you ever not want parameterised queries? Then the escaping problem goes to the DB, where the professionals do it correctly. Sprintf and co have no place in building a strong for a SQL statement (admittedly I've sprintf'ed templates together to.jse in a bound query)
That's why vendoring and reproducible builds exist - you want to rely on third parties as little as possible. You're right that this adds an extra point of failure, but if anyone wants to prevent such scenarios they have ways to make sure they can't happen at all.
It is definitely not equivalent. In my escape example it's clear what is being escaped and what is not. The example in project docs implies that either % (and possibly _/.) isn't being escaped, or that they will be escaped with the rest of filter, as everything is passed as one string.
This is not `fmt.Sprintf`. `sqlf.Sprintf` will return a `*sqlf.Query` which you then use to get out a parameterised query string and a slice of args (q.Query() and q.Args() respectively). That you pass on to your db layer. `sqlf` does no escaping. It just helps with composing the format string and argument slices. Look at the example again, especially the output. The first line is the parameterised SQL query, the 2nd line is the arguments. If you have ever had to dynamically build a parameterised query string in Go you probably noticed it is annoying to keep track of the parameters in the SQL and the corresponding slice of arguments. This package solves that problem.
And exactly how many developers are going to do this?....
_And exactly how_ _Many developers are_ _Going to do this?...._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^kidovate ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
Can you link me to a client package function which escapes like patterns? Or an example of a real db.Escape? I think you may be misunderstanding what this does. See my comment here https://www.reddit.com/r/golang/comments/6wzce1/build_and_compose_sql_queries_without_an_orm/dmcejwk/
Vendoring (or other forms of dependency locking) are very common in Go, at least amongst popular projects. Reproducible builds per se not so much, but you're pretty darn close if you vendor everything.
&gt; And exactly how many developers are going to do this?.... All of the security conscious developers.
I wasn't really expecting the compiler to deal with it. I was asking about hacks - something outside of Go, the source code, and its compiler - to let me one-button something to both remove that stuff out (by commenting it out), comment it back in if I do need it (by removing the auto-generated comment), or remove it entirely if I don't need it anymore (by removing the autogenerated comment and the line it referred to). Just a way to manually speed up what I do anyway. So it is about speeding up the speed with which I do something that people do anyway. You can't really tell me you've never encountered an unused variable error, because you always just type in your final product? :)
thanks. which vscode extensions do you use? (can you list all of them or the major ones that have helped you anyway)?
vscode will automatically recommend that you use a default Go extension (https://github.com/Microsoft/vscode-go) and it is sufficient
[Full Changelog](https://github.com/containous/traefik/releases/tag/v1.4.0-rc1)
For the ones into quick math: that is ~833 requests / second / server
I'm not. You produce bindVars via q.Args(). And in your example, the complete value for the %s parameter for LIKE will be escaped by whatever client will be used. This means the resulting query will be "name like '\%filter\%'", meaning the % would be treated as a literal character. Given the usage patterns for LIKE, this would be *unwanted behaviour*. Work on Quoters for database/sql is something that's currently lacking/in progress: https://github.com/golang/go/issues/18478 - specifically, client libraries provide different quoting mechanisms: &gt; MySQL has configuration options for quoting, so the quoting is also dependent on the connection. &gt; &gt; As a user I would expect the function to be on the sql.DB object, since I probably already have a reference to it when I am constructing queries and sql.DB already has all the information to set up the quoting. Thats also where you find them in similar packages for other languages like Perl/DB, PHP/PDO, PHP/Mysqli. The problem with such a query builder is basically that it doesn't provide a faculty of escaping but not quoting the string (neither is there a reasonable way to do that in Go at this time, I accept). I can point you to [PDO quote() in PHP](http://php.net/manual/en/pdo.quote.php) as also the cited example suggests.
Per.. minute? Meh.
Looks like the author took Go and threw away most of the stdlib leaving only some crucial parts. Hard to gauge from the README how correct I am about that statement,... +1, would play with :)
Much faster to write correct code from the get-go than to write a bunch of crap, lean on tooling to fix your mess, then figure out why it doesn't work the way it did. &gt; You can't really tell me That's not an argument in favor of what you're doing.
Some parts of the stdlib don't really make sense on an embedded system, some he probably didn't get around to implementing yet.
It's easy to argue about choices when you aren't the one making them. While using C/C++ has benefits in such a system (or even Erlang or Rust), Go was part of their skill set. Using a different language, compiled or interpreted, doesn't exclude you from facing the same problems regarding memory usage. I see this article as more of an example of system management, than actual development. p.s. I love LUA. In terms of resource usage and code size, the whole thing is so so tiny. Any other interpreted language I picked up in my distant and not so distant past, suck. Hard. Choose your pick: PHP, Ruby, Python, Perl, Bash, Node. Exactly 0 of them should live on a MCU-sized device (okay, maybe bash). Unfortunately Go isn't feasible there, but I really wish it was. Setting up a cross-compile environment for C isn't exactly the easiest thing in the world as well... at least not in comparison with GOOS/GOARCH...
Sure, no use for database/sql. I completely agree. It was just a partial observation tho, I think a lot of non-stlib things should be thrown out for such a build target as well. Definitely a challenge!
Since status code checks are common for high volume microservices, I wonder if you could shave off a few ns by using a static hashmap of error code -&gt; SUCCESSish (all 2xx values to true, other values to false) rather than a pair of integer comparisons?
It doesn't matter much for method calls anyways because the stack trace will show the line in the method that panic'd: https://play.golang.org/p/PDTgvn4mjh
I wonder how this compares with the language spec changes bringing in FMA in 1.9 https://golang.org/ref/spec#Floating_point_operators
I came up to slightly over 30k requests/s for sonyflake, an ID generator service like twitter snowflake, scaling on three docker swarm nodes ([article](https://blog.codeship.com/running-1000-containers-in-docker-swarm/)). But that's a raw benchmark and not real traffic, if anything is serves as the maximum capacity... I suppose they also can go that high, so they are at about 10% of their capacity. When they 5X their traffic, they can add another server, which is cool :)
I was actually thinking of a circular buffer as well, but it might be a limitation of whatever framebuffer/video capture library they are utilizing. Who knows. As you say, being an armchair general is easy and fun :)
Good post, but I'm very interested in why the author recommends not using assert libraries at all. In my opinion it makes test code much cleaner to read and the errors have a decent description automatically. They are especially nice when using the suite package from testify. It makes sure you don't have to pass *testing.T to each function that you call to be able to fail the test from within, because you can simply make them methods on the suite. The only downsidee I can think of is using Equal which changes some compile time errors to runtime ones, because it uses DeepEqual on interface{}. In theory this can be a bit annoying if your CI runs take long, but in practice I almost never have issues with this. It's good practice to run a test that you changed locally anyway before pushing to CI. 
An integer division + an integer comparison should be a lot faster than map lookups, or even array lookups.
I have written once here and I repeat: typical task for embedded hardware doesn't need fancy algorithms or architecture. It is pretty straightforward and the technical challenge is its size and sometimes performance. There aren't much places in C++ code for typical embedded tasks what can cause massive headache.
&gt; func Fprint(**w** io.Writer, a ...interface{}) (n int, err error) &gt; &gt; Fprint formats using the default formats for its operands and writes to w. 
Holy shit this sounds like total scam. I just got flashback to 2000s and those web banners "Your PC is slow? Have trojans/viruses/registry problems? Just install this ultraboostfixer.exe to fix your computer! Its totally OK and does what we say, trust us, wink wink!". Or it may be only me paranoid.
https://docs.google.com/document/d/1Zb9GCWPKeEJ4Dyn2TkT-O3wJ8AFc-IMxZzTugNCjr-8/edit?usp=drivesdk
kops to make the cluster or use GCE go in docker - compile the program, then package the static binary in an Alpine container. CI - pick one - Openshift Origin, Spinnaker, other proprietary options. 
I guess even personal blogs have reposts, I remembered the title. :p http://marcio.io/2015/07/handling-1-million-requests-per-minute-with-golang/ I prefer the style of your other blog, it have less crap around and the reading area is bigger. 
http://www.telit.com/fileadmin/AppZone_Guide/az-python-user-guide/index.html#!Documents/runpythonscriptsonthemodule.htm
No. I can view that link. Otherwise you can search by yourself the keywords "telit python".
bad bot
Thank you ar1819 for voting on haikubot-1911. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Forth would work too
thanks!
jenkins (ugh) running on k8s, runs tests/builds binary in golang build container then add binary to minimal Docker, always looking like so: `FROM scratch ADD bin/mybinary .` and then start rcs/whatever using k8s scripts/custom cli written using k8s `client-go`.
This is interesting. Actually it's pretty insightful. So let me learn a bit from you :) Do you think through the architecture you will have? Do you go as far as write it down on pen and paper? Or do you work it out in the IDE, and only try to hit "compile" when you think you're ready? In other languages, I have had a very fast turnaround cycle so that I always have something that compiles/works. I spend only short amounts of time writing new code before I test and make sure it's all still working. curious how you approach Go instead of this.
Any idea why I get this [bunch of errors](https://pastebin.com/XyvWeeFq) when I do just that? This is from [vscode](http://i.imgur.com/cfgLHUg.png) -- in the first link I copied that whole output and I already installed git. Thanks.
Congrats on coming up with a system that worked well for your load! Some of the logic at: https://gist.github.com/mcastilho/e4cbbc18b5ddb9636758b7f5c7a73bdf#file-medium_dispatcher-go-L21-L36 could be simplified by using a for range on the channel for job := range JobQueue { go func(job Job) { // try to obtain a worker job channel that is available. // this will block until a worker is idle jobChannel := &lt;-d.WorkerPool // dispatch the job to the worker job channel jobChannel &lt;- job }(job) } 
Try getting each of them individually and see what the error is, it's probably all the same thing like a bad version of go or something. So no, I don't know.
To all the people meh'ing about the requests/server, keep in mind servers are only 2 cores each (c4.Large is 3.75GB and 2 vCPU)
Is this legit? Has anyone tried it?
I'm processing about 20k messages per second with an unoptimized code on a c4.large same kind of workload, receive a request, process some json, dump to S3. I'm not saying that what the author did is not good or anything but some of us are used to other kinds of workloads.
Github is a multi-billion dollar operation. It's lifetime is more assured than the lifetime of your website.
Speed != Efficiency I don't see what the interviewer did as any better than: const false = true Code must not lie and code must not be magic
From my experience, and I'm not expert, but there's a lot you can do in Go to make it use very little RAM. Maybe not quite as low as you can with a well crafted C program, but you can do pretty well.
this is dumb.
If you see at the bottom of the article we ended using only 4 servers after the refactoring. So we are talking roughly ~4166 Requests / server Those were expensive requests that received a big json payload and save to S3. 
I was going to say that I thought I remembered reading this before.
thanks. not a great experience for me :) we'll see...
Google is a multi-billion dollar operation, yet I had to move my projects from Google code. Atlassian is making a lot of money too, and I had to move my projects from BitBucket. And I had my projects on SourceForge before that. Remember that? I don't know what the future will hold. Maybe GitHub will still be around in ten years, or maybe it won't. All I know is that I've had to move my projects three times already in the last ten years. Better safe than sorry, and since domain names are so cheap that they're basically free why not use it to give you more freedom and better guarantee [Cool URIs](https://www.w3.org/Provider/Style/URI.html)?
This comes off as if you are sarcastically telling me you think my ideas are bad, and I'm not sure why you'd be that rude.
Good point. I was thinking the payloads were large, but upon re-reading, the size isn't mentioned. If the payloads are small, there is very likely a lot of performance that could be uncovered.
I agree, and this is how I put it: Code must do the expected thing. If it does something unexpected on purpose, there should be a comment noting that. Without such a comment, later on, it's impossible to tell whether the unexpected thing is done on purpose or if it's an unintentional mistake.
How big are we talking? A few hundred kilobytes? A few megabytes? A few hundred megabytes?
I believe the point was not about using additional testing packages, but not to write your tests such that they keep going after a failure, on the grounds that it isn't very useful in practice to find out about the second and third and so on failure because the probability that it's just about manifestation of the first failure, rather than a distinct second failure. Don't use testing packages that do that is the idea, I think.
Thank you, I was curious! Assuming an array held nothing but 500 booleans, small enough to fit into L1 cache, the context switch triggering loading this data is probably worse than just doing integer comparison.
Sure but your definition of little RAM may be different from little RAM C is adept at running in. That said, C is also a lot faster than Go. For modern Servers, Go is one of the better Garbage collected languages out there. Anything with constraints and Go is certainly not the right choice I think.
http://www.golangprograms.com/
I feel like I may have seen this pattern explained before, but forgot what its purpose is. Why have a separate JobChannel per worker and a Dispatcher to fling jobs from the general queue to individual workers, as opposed to just having the one job queue and all workers consuming from it? Is the read contention a big problem?
I'm a fan of drone for the CI. GitHub.com/drone/drone. Easy self-hosted, and once you get the pattern, it's really flexible. Commit to github builds, tests, deploys. 
GitHub may not want to host your project, they're a multi-billion dollar operation, they can treat their customers how they want. Personally, I'd rather not depend on the benevolence of a multi-billion dollar operation and rather depend on myself.
I'm sorry to belabour the point, but I think you are conflating escaping strings and escaping wildcard characters in LIKE expressions. `%` has no special meaning in SQL query strings, it only has a special meaning to the SQL engine when evaluating a LIKE condition. The linked issue is about escaping strings, identifiers or binary data, not like expressions (or any other special strings like regexes, JSON, etc). If there was a `db.EscapeLike` you would use it like this "select * from users where uname like '" + db.Escape("%" + db.EscapeLike(pattern) + "%") + "'" EG: `PDO::quote` in PHP will not escape `%`. `PDO::quote` for MySQL src is here https://github.com/php/php-src/blob/php-7.1.9/ext/pdo_mysql/mysql_driver.c#L300 and if you follow it down to the actual escaping you can see which characters are the important ones for escaping https://github.com/php/php-src/blob/php-7.1.9/ext/mysqlnd/mysqlnd_charset.c#L862-L880 So to bring it back to Go db.Query("SELECT * FROM users WHERE name LIKE ?", "%" + filter + "%") is how you would do a `LIKE` query on `filter` with wildcards on either side in Go. In the sqlf package q := sqlf.Sprintf("SELECT * FROM users WHERE name LIKE %s", "%" + filter + "%") db.Query(q.Query(sqlf.SimpleBindVar), q.Args()...) This is the same, since `q.Query(sqlf.SimpleBindVar) == "SELECT * FROM users WHERE name LIKE ?"` and `q.Args() == []string{"%" + filter + "%"}`.
I have to production environments running on Kube: one on AWS with kops and the other on GCE. GCE is easier to get started, while kops feels, at least to me, more flexible. On the AWS cluster we have Jenkins (I know, I know ;)) running tests and building the containers on every git push and we then have a multi-stage deploy (dev, staging, production). On the GCE cluster we build using Google Cloud Container Registry's build triggers. We tried using Drone instead of Jenkins, but we have complex pipelines and requirements which were almost impossible to support. As for containerizing a Go app, you can either use the go image and build the app inside of the container, or build the binary somewhere and then just add it to a bare bones image as suggested by /u/recurrency
&gt; then package the static binary in an Alpine container. Why alpine vs "from scratch"? 
Yeah we just use scratch. 
it's not always necessary depending on your orchestration layer, but sometimes being able to ssh onto a prod instance is a godsend.
Since you have more cores, why don't you let your process using all the cores?
Shit. I have about 15 years of code to fix now. In a way, it's the best compliment I can give you :). Edit: well, I did say "correct me if I'm wrong". You have :)
I agree. It seems unnecessarily complicated. I'm not a go expert, but I feel like you would just need a initialize a single channel, and a pool of workers all reading the channel. The dispatcher would initialize every job and add it the job channel. Wrapping the dispatcher loop inside a go routines gives a really deep queue, although probably could be optional.
I would gladly, I am curious what should be strategy? Let OS handle each instance or restrict each instance to a fixed number of cores? I am leaning towards not touching anything and see how it goes. I am assuming even though runtime among the instances cannot communicate with each other, OS would fairly allocate resources for all 4 of them.
More comments here https://news.ycombinator.com/item?id=9845820 I think the concensus is this code works, but can be improved on.
You have to move it in its own package as it was said before and in an internal package if you don't want it to be available outside. If you start to think you need to "hide" some elements of your implementation it means you have a public and private part so you need to move your code in a package. I guess we make a big deal of that but it is not, creating a module is nothing especially if you scope it as an internal package, no one outside will notice about it and you make a clear statement that the code in your internal package works as its own.
Interesting article on how to design something like this. I like when articles post their failed attempts, and describe why they didn't work out. However, if I were to design this I'd use Lambda and Kinesis, instead of implementing my own non transactional queue. Edit: I just did the math to figure how how much this would cost in Kinesis/Lambda- About $1k/mo. So running this on a few c4 servers is probably cheaper. 
Well, If you only want to limit the number of cores per process, it will require you to setup CPU scheduling. Let's imagine you have 40 cores and you want to launch 4 processes, * P1 =&gt; core 0-9 * P2 =&gt; core 10-19 * P3 =&gt; core 20-29 * P4 =&gt; core 30-39 As you see, each process will not share core resources (the system will consume some processing power still) but it's a bit harder to set up. From my point of view, I would just let my one process using all the cores, It's easier to setup. if you still want to go with your way, then check [cgroups](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch01.html)
Looks like that is what I am going to do.
Two points: * The result of pinning your instances to `taskset` is *almost* the same as specifying `GOMAXPROCS`: the instance would see exactly the number of cores it was pinned to, and that would make it create that many `P`s ("processes"—thigies used to run goroutines on OS threads). Of course, the remaining difference is that in the case of using `taskset`, the OS won't schedule the instance's threads on the other 16 cores. * The scheduler in the Go runtime is itself concurrent: that is, it is not implemented as something monilithic protected by a global lock—quite on the contrary, as much as can be done concurrently, is done concurrently. Specifically, each `P` has its own run queue of the goroutines, and different `P`s are able to steal goroutines from the runqs of other `P`s w/o touching the global scheduler state. That said, one another point to possibly consider is that goroutines are not **fully** preemptible (that's actually a good thing but read on): this means that long runs of Go code which do not call any functions could effectively "pin" a goroutine to its underlying `P` (and hence to its underlying `M`—the "machine", an OS thread) preventing fair distribution of CPU quanta across goroutines. In such cases, the more `P`s ther scheduler has, the better, but such cases are pathological anyway. You could try to see whether you have such a case by inspecting a so-called "scheduler trace" captured over a run under a typical workload—see [this](https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs).
Not at all! This was more just a random complaint about VSCode. Of course VSCode is the most-recommended editor (by far), I am just sharing that I expected the experience to go more smoothly, as I did a bog-standard installation of Go, Git, and also VSCode, and then installed its recommended extensions. I was surprised that it didn't "just work" given how popular VSCode is and how I'm doing things the bog-standard way. Of course this has nothing to do with you :) Thanks for the conversation, you were quite helpful.
An alternative using templates https://github.com/Davmuz/gqt
For people who (like me) have more time listening to stuff on the commute or so over sitting down and reading stuff, there was an awesome 'go time' episode about restic: https://changelog.com/gotime/48 They do a very good job in breaking down the concepts. Going into the details like this blog does is of course a valuable addition to that.
Looks interesting. Is it possible to do something like the linked example? IE build up a slice of where expressions, and correctly compose it to have a sql query and argument list to pass to the db?
Opt-in security, isn't that risky?
There are some places I think this pattern is useful. For example in a query builder, where you're just setting state in a memo, and don't need to return errors. It's nicer to do something like this: query := users.Order("status desc, name asc").Where("status is not null") if q { query.Where("name=?",q) } if tag { query.Join("tags").Where("tag_id=?",tag) } than to build the sql by hand IMO (remembering to put order last etc), and there's no reason for a struct like that to return errors, it just stores state in a convenient way. Another example might be to filter collections - sometimes it's nice to be able to define some filters on a collection which can be chained so that you can combine different filters succinctly in order to filter a collection down to the elements you want (for example a dom filtering language). That said there are many cases where it is not a useful pattern and gets in the way, and it's not something I'd use often. It shouldn't be used with code that might cause errors or panic. 
Docker and docker swarm for me. [12 Factor Apps with Docker and Go](https://leanpub.com/12fa-docker-golang). CI and CD are usually done with anything you like, but there are a number of services available. Travis, Jenkins, Codeship, Buildkite, GitLab CI, Bitbucket pipelines,... you can use any to build your stuff and set up a deployment process. Packaging your app is as simple as choosing an appropriate docker image (alpine is a good choice if you need some additional software), and then just add in your binary. For example: [titpetric/pendulum](https://github.com/titpetric/pendulum/blob/master/Dockerfile);
I see a lot of comments saying "C/C++/Rust is a better choice". But I think Go can be good solution in the embedded Linux world. I also develop services for small embedded Linux system with 128Mb of RAM and 256 MB of NAND. The device has around 20analog and digital inputs and outputs and it's used in the HVAC industries. It runs a few small services to configure the IO's and interact with them using several protocols. Go turned out to be a good solution for us. * Most our dev's where more familiar with Go than C or C++. * cross-compilation in Go is easy and fast which makes the feedback time quite short. * 128MB RAM is quite a lot memory. Our apps use &lt;10MB of memory per app. With 4 apps that is at most 40MB. If you include the memory used by the rest of the system only less than 50% of our memory is used. * 256MB NAND is huge. If we remove debug symbols and compress the binary with UPX our binaries are &lt; 2MB per binary. * In HVAC it takes sometimes up to 60 seconds to ramp up fans or close valves. Also temperature or humity doesn't change much in 100 milliseconds. So our systems doesn't have to react with the speed of light. A drawback of choosing Go over C/C++ is that the C/C++ ecosystem has more support for some protocols we use. 
Nice. I guess this makes use of iTerm2's unique inline image [feature](https://www.iterm2.com/features.html). BTW, is there a way to detect if stdout is connected to an iTerm2 shell? If so, then it would be possible to write code that uses `imgcat` when iTerm2 is detected, and [pixterm](https://github.com/eliukblau/pixterm) otherwise.
[removed]
Values reported by Range reflect any mapping for that key during the Range call. There are no guarantees which mapping will be returned. Range can be O(N) even if the user-provided function f returns false after a constant number of calls. Range copies dirty values and the effort for this cannot be controlled by returning false in f.
I agree, it's why I don't like Go's "packaging model". It's far too easy and insanely common that hosts, usually Github, remove repos because the project changed owners - or the author decided to just remove it. Then of course, we could run into external domains that are far more likely to be compromised than "trusted" hosts like Github/etc. I love Go, but the entire package model, from the GOPATH to distribution of packages, is just.. awful. Imo.
Perhaps it is early in morning, so I kept reading it as 'show cat images' on terminal and after seeing no cat images on link I was disappointed.
I said it was another blog post :) For test suites, here's my opinion from using them for 3.5 years on a large project: https://npf.io/2017/03/3.5yrs-500k-lines-of-go/#test-suites
Nah, I really did mean "don't use assert packages" .. i.e. just use the std lib testing package and if statements. I think you get better tests that way. Mostly, I think people are more deliberate in what they test and what their error messages are when they write them by hand rather than just throw two variables into an assert function.
So, you have a good point about it not being misleading if you expect method chaining. I do think that usually when people try to do fancy things with caching errors, what happens is that you get a weird non-idiomatic interface that confuses people, and thus gets used incorrectly (totally forgetting to check error returns, or checking them at the wrong time, etc).
Just to clarify further... w is the name of one of the arguments. Go documentation often does this, where it will refer to arguments by name with no other context. Once you get used to it, it's nice, because it avoids having the javadoc thing where you have to define the argument but then the description is just useless. // @param w the io.Writer to write to 
Every other week, there is a library or proposal to "solve" error handling in Go. At least method chaining does not require language extensions, it already works within the language. It *is* unfamiliar, but if you cannot assume that people at least read the documentation, you can't write anything. Could it help if the object (a) is of a type which implements the error interface? 
Bit of a tangent, but will encryption ever get to a place where morons can encrypt their own code _(I say morons, to avoid any debate about "normal humans" haha)_? Eg, I know *nothing* about crypto. Well, I take that back - I know just enough, to know I shouldn't touch it. I know that I can introduce vectors of attack, based on the data I provide it, that undermine a chosen algorithm. I know that it is difficult. To use an explicit example: I'm writing a pet project to handle synchronization, distribution, blah blah - like Restic/Camli, with some features - intended for myself. With that said, I want to encrypt it on untrusted hosts. Is there likely to ever be a sane way people like me to encrypt things easily? Namely, to avoid introducing weak points due to how the data, and the type of data that is stored? Pretty broad question, mostly rhetorical. Just looking for thoughts/discussion.
It looks like to be the exact same post as someone from malware byte did years ago... Why repost it? http://marcio.io/2015/07/handling-1-million-requests-per-minute-with-golang/
I've updated my article a bit and reposted on Medium. It has aged well and still can help lots of people that have missed it before.
Benchmark both scenarios and see which one performs better.
Most things trying to solve error handling are just trying to avoid writing if statements, which to me seems like a silly thing to worry about. Method chaining seems to exist to avoid writing line returns... which is possibly even more silly to worry about. We have idioms for a reason, it means that you can make assumptions about how code will work and generally be right. When you stray outside those bounds, you introduce opportunity for misunderstanding, even if there is sufficient documentation. Docs don't help when you're looking at the code in a diff or a code review. Would it help if a implements the error interface? Yeesh, no, please don't do that.
hahahaha, that can totally be done though!
Good question, could file an issue to the repo?
Even with a comment in one file, code could still lie in another file. I think I I'd just stop at code should do the expected thing.
Done.
Well, given there is a chance it could be O(N), that implies the worst case scenario is that it takes N time, or in other words, it's always O(N).
With that many go routines, and assuming it bakes out all cores, simply running it on a larger machine may well result in higher throughout. If not, I'd wrap the entire pipeline structure and launch as many pipelines (the current set of go routines) as you desire...
This.
I stand corrected. I work that way myself. If I didn't have reflect.DeepEqual on hand, I might operate differently, but I do. (Occasionally I do have to get one of the packages that tells you _what's_ different rather than just _that_ it is different, but that's about it.) And writing a function to take a `*testing.T` sometimes just isn't a big enough deal to justify a package.
Wow just love this!! Keep the examples coming :-) One feature request: would it be possible to provide even more context to all concepts introduced? 
The "premium data" vulnerability in the screenshot is probably [this one](https://github.com/square/go-jose/commit/789a4c4bd4c118f7564954f441b29c153ccd6a96). Looking [at the issue](https://github.com/square/go-jose/issues/151) you just get back an invalid response (not even a `panic()`), although it may be possible to abuse this to make code panic (maybe?) Either way, it's just a bug. You give a function some input and you get the wrong output. Not good, but a "Medium risk" security problem? Probably not. If this post was intended to showcase the added value of SourceClear's database then I'm afraid you've chosen a poor example.
In the last couple years there have been multiple projects that are targeting exactly this. Things like NaCl and libsodium tend to have very high level encryption APIs that use strong defaults and config params that make it easy for average developers to have crypto that "just works" with fewer possibilities for mistakes Edit: typos/wording
I would rather not depend on you to keep your server secure versus GitHub.......
You still cannot argue that this guy's vanity URL is likely to remain registered longer than GitHub, a code platform used by hundreds of companies including Google, will. 
That's like your opinion then. On the other hand, you do not have to depend on me securing my server, all my commits are signed with PGP and my Git server is up to date and enforces HTTPS. You don't really have to trust me, that's what reading the source code is for. On the other hand, again, I do not trust Github enough to be the sole source of truth for my source code. A centralized source of truth for code is no better than some rando's unsecured git server.
Very nice!
That may be the case, you're absolutely right that trusting a single source is bad, which is why Go's packaging model needs some work (the primary thing I'm trying to point out and am getting downvoted for). Don't get me wrong, I love Go and use it for the majority of my work these days, but that part of it needs improvement and is being actively worked on. However. You and I both know that if GitHub goes down, a TON of projects will need to migrate. Logrus for example had issues when the creator renamed his username from Sirupsen with a capital s to without. A lot of people and companies have a big stake in keeping GitHub up and secure. That cannot be said for some random dude's domain name. His server could easily be insecure as hell, and to be honest I would prefer a GitHub url to a random .cc domain. 
Maybe not, but it's not just about the domain, it's about the entire URL. Example: Stack Overflow has its "Jobs" section. I got my Go job through that last year. I also used it for my CV. Then they changed it to the current Developer Story, and now my CV looks like crap, so I just disabled it, which has the side-effect of invalidating all the CV URLs I sent out. Since I don't control the "stackoverflow.com/cv/mtournoij" URL I have no way to redirect that to somewhere else. Had I continued using the arp242.net/cv URL I could have changed to to anything else. Like I said, I don't know what the future will hold, but greater companies than GitHub have fallen from grace. Another possibility might be that next year someone will launch a competing service which is so much better that everyone will migrate to it, like what happened with SourceForge -&gt; GitHub.
Your CV url isn't being depended upon by hundreds of companies that would all have to migrate given it changing or otherwise going away. I've commented a longer reply lower down in the thread.
True but a status code lookup is not the only thing this service would be doing so L1 would ha e to load those values first by looking at L2 then L3(if present) then fall back on memory at worse.
Love this !! I wanted to learn about assembly for a long time.
Well, `dep` handles dependencies and vendors them, and is most likely going to become the standard Go package manager.
&gt; AFAIK currently I need to open a file and run log.SetOutput() to that file. If you're doing this, you only need to do it once per process. But you probably shouldn't be doing this at all. Why are you logging to a file descriptor and not stdout?
I want the log to be in a file so I can review it in the future?
Similar to [k-sone/critbitgo](https://github.com/k-sone/critbitgo).
This should not be your golang application responsibility but your application environnement with a stream redirection 
I mostly agree that some random domain is not as secure, however with vendoring this should not be an issue. Almost all good tools allow you to specify alternative repo sources while keeping the import path. In the even that the author has abandoned the domain, they will most likely to provide any updates to their code either, so you'll likely have forks elsewhere. The beauty of git is that it *is* decentralized, so you can simply fork off the repo and use it yourself if you don't trust me to keep my infrastructure running.
Nice! What made you start this project?
It would be more useful if it could contain values like https://github.com/chromicant/iptree can. iptree works it just uses Radix32 and only supports v4.
You call this once (err handling not included): f, _ := os.Open("my.log") log.SetOutput(f) and after this you can use log.Println and so on for every Go file in your whole program because log.SetOutput sets the output of the package global default logger of the log package. Do the upper logger initialization for example in the first few lines of your main func and it does not matter in which file of your project you call log.Println. This is fine when you only have one log file. With more log files (e.g. access.log and error.log) you should consider initializing a *log.logger using log.NewLogger and then pass it through your program where it is needed.
Whatever your using to launch your app (systemd, upstart, supervisor, whatever) should be able to capture stdout to a log file for you.
Unless you're doing something extremely niche, you won't run into problems finding libraries to do what you need. Third party libraries aside, the standard libraries are comprehensive enough that most projects don't really require anything else. Just dive in.
Indeed the data structure is similar. Although initial glance indicates that at the library interface level, since it accepts array of bytes as input, it wouldn't be able to differentiate CIDR blocks with IP mask more granular then 8 bits, e.g. it is unable to store differentiate 192.168.0.0/16 and 192.168.0.0/15. That said it is certainly possible to add support for the use case, just at maintainer's expense. I could even switch cidranger's internal storage to use critbigo. Some of the value in this project also lies in the production ready aspect, that it is fully tested, has insightful benchmarks, and supports both IPv4 and IPv6 out of the box.
 I was thinking of adding this initially but left it out because I thought it could be achieved just as easily by the library user's by storing an additional map. However, this certainly had crossed my mind and I would love to add this to the interface, do you mind opening an issue on github so I can attend to it later? Thanks!
In general, Go is amazing for setting up a backend server. What are you trying to do specifically?
&gt; Why are you logging to a file descriptor and not stdout? Not every program is a web application or system daemon. Sometimes, you want to write standalone software that is fully-functional on its own. 
A web and mobile! So Go will be used to fetch data ..!
This is what I do as well. I came from Laravel, but Go feels so much better! Good luck with your adventure. 
net.go provides a (minimally documented) net.IPNet based interface to critbitgo that seems to work fairly well. I've been using it for a while, but I'll definitely take a look at cidrranger the next time I need that.
+1. I need something that does queries into a list of CIDR addresses a few times a year, and I can't think of a case where I've not needed data of *some* sort attached to the route.
&gt; Do you think through the architecture you will have? The code I write follows a set of basic principles / best practices: separate data and logic, build functions and classes with a single responsibility, etc. How that code is architected depends on the use case. If it's a production service, of course I work it out on paper... if it's just tooling I might sketch out some quick pseudocode, sleep on it, then fire away the next day. In either case, I write code in smallish units, and write tests for each unit. Code is usually reusable, especially when written dogmatically, so there's little excuse to write throw-away code. And none of this is specific to Go. I've done this in C++, in Python and in Go.
Thanks, this was interesting. You say "dogmatically" but can you share some of the rest of your dogma? I am listening attentively to your approach. Thanks.
Do you intent to integrate legacy enterprise systems that rely on SOAP? then I'd say no. Otherwise it's fine. The question is more where do you come from and what kind of libraries do you expect to find in Go. And yes, the libraries are mostly focused on REST API/ Devops and just go to https://stackoverflow.com/questions/tagged/go to see that you'll get very limited help there. No matter how people try to inflate the community it isn't that big.
Damn new pprof ui looks hala good
The company where I work has in the past year really opened up to go and are loving working with it. The biggest complaints I've heard so far are that there aren't really options for mirroring libraries (to remove external dependencies at deployment time) &amp; version control, and its goofy directory structure requirement.
I've done a little bit of Go Assembly - so it's nice to see this! I'm not sure about you, but when I was writing assembly code from scratch the most difficult thing for me was swapping the operand orders and just finding a concise listing of all the instructions as Go implements/names them. I would love to see a reference if you know of any!
Can I recommend gRPC. It makes it a breeze to exchange messages between services. https://grpc.io/
The problem is that it's already really verbose compared to the original www.gobyexample.com It was way more verbose and I had to cut it down so that it wouldn't become a hassle to read. BTW if you've stopped on the hello world, that one is intentionally non-verbosed.
I'm trying to implement KangarooTwelve, a new hash function based on SHA-3, in go Assembly. What I need is to transform [the SHA-3's permutation](https://github.com/golang/crypto/blob/master/sha3/keccakf.go#L46) into a permutation that doesn't take one state, but several states and perform the same operation on several states instead of one. It's do-able via SIMD instructions (that perform operations not on values the size of normal registers but on values of the size 128-bit or 256-bit or 512-bit depending on the arch) but the Go Assembly is still tricky to me so I figured I would do this to motivate myself.
the documentation clearly sucks and that's why I've created this project. My main source of information has been [this video](https://www.youtube.com/watch?v=9jpnFmJr2PE)
The best thing to do is look up the Plan 9 assembly syntax. Go assembly is pretty much the same thing with minor variations. It's easier to remember that it uses AT&amp;T syntax when you remember that Go was largely written by the people that AT&amp;T syntax was named after.
What is RPC??
remote procedure call. Check the link I sent in my previous comment. It's two process from different Lang communicate with each other in an standard way.
I like your project and I'm glad you made it. But I'm not sure that it's really what most people that would look for this sort of thing would actually want. * It doesn't appear to support any normal HTTP caching semantics (Cache-Control, Expires, ETag, etc.) * It doesn't have any cache expiration. (I see that LRU is on your radar) * It doesn't support the same GET/HEAD/PUT/POST/DELETE api as S3-proper, so now I have to write an entirely from-scratch API client to "read-through" or "write-through" to AWS S3? Why would I do that? Easier just to do my writes to S3 and my reads through something like Varnish. Just my perspective. I like this project and think it has some great potential!
Thanks!
Blasphemy!
Thank you for all the feedback. I removed the modulo bias from Uint64Range and removed the panic. All functions now return an error if rand.Read fails.
We've started a dual licensed AGPL/commercial library for creating and editing OOXML documents (.docx, .xlsx, and .pptx). It's fairly new, but the initial focus is on implementing .docx support. I added an [example](https://github.com/baliance/gooxml/blob/master/_examples/document/fill-out-form/main.go) that shows how to use our library to: - list all forms in a document - fill their content - save the resulting document It's available on GitHub, [github.com/baliance/gooxml/](https://github.com/baliance/gooxml/). We use a vanity import path, so to install it run: go get baliance.com/gooxml/ 
I was wondering what this repo was even doing, thanks for the clarification.
Wait, does it still require graphviz? So it's still a hassle on windows?
Yes, this tripped me up too. Apparently FMA is added only for s390x and ppc64. The issue I linked to from the blog is the one which tracks the addition of FMA for amd64.
Peter is right. This is a common convention espoused by the "12 factor rules" that all your application should do is just log to stdout. The environment should capture the stream and store it in files. I wrote a tool exactly for this purpose. - https://github.com/agnivade/funnel. Probably that is something that will fit your use case ?
In that case, I'd wager that you will find pretty much everything you need in the standard library itself. Regarding "community" - hey, that's what this sub is for ;) So give Go a chance !
Completely awesome. Well done!
I believe this discussion became so long and heated because there is no single "right" way to do it. And moreover, different parties have different interests. * Package authors do not want to depend on some third-party service to keep access to their packages available for everyone. Their best choice: Go vanity imports. * Package users do not want to depend on too many different third parties. A github.com import path breaks if Github goes out of business. A vanity import path breaks if the package author decides to stop working on the package and to delete the domain. If you projects contains three vanity imports, then you have four points of failure - github.com plus any of the three vanity domains - instead of only one. So there is no right way. Both centralized github.com imports and vanity imports have their pros and cons. The only solution IMO would be to make import paths independent of other people's domains and URL's, and have a *distributed* set of repository services resolve the import path and serve the repo. If one repo service goes out of business, the others would still be there, and new services could join the party transparently. After all, Git was created to be distributed, not centralized.
Is having an sshd running in every container a good idea, considering security and maintenance efforts? (I have [this blog post](https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/) in mind.)
This maybe a stupid question but since it's on the same box any reason to have 4 instances can't you just have 1 larger?
Seems like your link for 12 factor apps is broken 
Fixed, ty
You don't need sshd in every container, just a shell binary, and kubernetes can exec into it. Hopefully in v1.9, you won't even need the shell - we can inject it dynamically!
&gt; A drawback of choosing Go over C/C++ is that the C/C++ ecosystem has more support for some protocols we use. Which could be alleviated by having domain experts like you writing the missing Go packages for those protocols. &lt;wink wink&gt;
Ah, thank you for the reply!
Make sure that this is only in package main. In library code, do not mess with the setting of the logger, because your caller knows better than you how he/she wants to get the logs.
Seems like a nice guy. Glad to have him hacking on Go.
Can this really interpret bash scripts with full compatibility?
Short answer - yes. I haven't yet found a real-life bash script that it can't eat. See https://github.com/mvdan/sh#caveats though. Some things cannot be done in a completely static parser. They all have very easy workarounds, though. Edit: I just noticed you said interpret, not parse. Interpreting is a work-in-progress and experimental, so definitely not yet. It's also limited by some of Go's nature, such as how I can't fork processes. But I hope that it will do a good job once finished, as long as programs don't depend too hard on these limitations.
Thanks for the update.
Hi, thanks for your feedback. 1. It's not a HTTP cache in any way and will not be compliant to HTTP caching standards. 2. I decided to leave expiration/evocation mechanisms out before the store is properly tested, also at this point I can't decide if LRU will be sufficient or it's worth to implement ARC. &gt; so now I have to write an entirely from-scratch API client API client is on roadmap, for Go and Ruby at least. I will think about adding S3-like API, so it'd be possible to have the store as a drop-in replacement for S3. &gt; Easier just to do my writes to S3 and my reads through something like Varnish. Answered that question above, in short, if you're OK with Varnish, nothing will affect your decision :)
Which we do. Some of the implementations are already open source. Some of them are not yet. 
Awesome! Highly appreciated! By the way, can you link some of them?
I am thinking of using go for a web app I need to write. This will not be a trivial app, it will need to make full use of postgres abilities, CORs, API authentication, full gamut of user functionality like invites, registrations, I forgot my password, login with facebook etc, robust loggins, interactions with various cloud services, background jobs, scheduled tasks, payment systems, yadda yadda. Coming from a ruby background it seems to me that Go is a terrible choice. I spent most of the day reading up on the various frameworks just to have people talking about how you should not use any of them and just roll every freaking thing yourself from scratch. I have read articles like this https://blog.golang.org/error-handling-and-go#TOC_3. Which frankly made me want to poke my eyes out rather than sit down and write that. I read this article http://www.alexedwards.net/blog/organising-database-access which proposes a solution to insanely complicated and verbose that it's shocking to me. I have to tell you from what I read coming from Ruby go is not a dream at all. I would like to learn the language because /r/programming hates it and as far as I am concerned that's the highest praise anyone can heap on any language and of course there seems to a lot of successful apps written in it but man I am really dreading diving into this swamp. The useless verbosity of the language is almost offensive to me. I could understand if the verbosity lead to better readability but it really doesn't. It's just noise you have to skip over while reading. Take a look at Elixir or Rails and tell me that developing web apps in Go is easier or more pleasant. It certainly doesn't seem that way to me at all. 
We thought about this as well but require a web client beside the apps. So we decided against gRPC in favor of GraphQL: - gRPC is binary and so not that easy to debug (via Postman, curl and co) - Not all browsers can deal with it without going through some hoops
Why? Rails and Phoenix provide a robust set of functionality you will need in most if not all of your apps. Sessions, secure cookies, logging, cross site protection, configuration, management of development, production and testing environments, testing, asset management, routing, template rendering etc. Why do you need to roll your own on every project? 
Of course! Glad you asked: [it is on Github](https://github.com/AdvancedClimateSystems)
Relevant, a twitter thread Feross shared recently about Ryan: https://twitter.com/feross/status/899044389883789312 &gt; 1/ Ryan Dahl (creator of Node.js) wrote an epic rant and then quit writing software for a while. I want to repost it here now. ...
Replying on a 4 months old thread? :P Anyways... &gt; Why? It's a complicated topic. So I am gonna present some random thoughts in no particular order. * When a framework becomes so dominant as Ruby on Rails or Java/Spring in an ecosystem it can cause a lot of problems and headaches for developers. Ever since that dominance I don't think I've ever seen a job description that says "Ruby" or "Java". No. It is not enough anymore. Now it is "Rails" and "Spring". * Bureaucracy at its finest. Your team can no longer hire a Ruby or Java developer. Nope. * Spend thousands to receive official Spring training for your team members. Become certified! * Because of the nature of a framework (trying to be very generic and solve all problems) they end up being quite complex so the bar for entering a project for a new developer is raised significantly. * Also these frameworks tend to have a lot of magic inside which causes maintenance problems. Try to debug some obscure bug in a large project that happens because of the magic of these frameworks and suddenly you need a "Rails or Spring expert". * You are no longer writing Ruby. Nope. You write some custom DSL (Think tests). * You are no longer working with the "standard" language tooling. Now you have to learn a bunch of framework specific commands. I think a lot of people have been burned by the ecosystem that has been created by these dominant frameworks. That's why the simplicity of Go looks appealing. &gt; Why do you need to roll your own on every project? Why roll your own when you can handpick specific libraries that solve your problems? The difference is that you start with nothing but the standard library and you add only what you need vs having to grok all the framework complexity from the start. I believe this also leads to better understanding of a project. &gt; Rails and Phoenix provide a robust set of functionality you will need in most if not all of your apps. Sessions, secure cookies, logging, cross site protection, configuration, management of development, production and testing environments, testing, asset management, routing, template rendering etc. Rails is amazing there's no doubt about that (I've never tried Phoenix). In fact if you asked me what would I pick for a large/complex web application, I'd probably pick Ruby on Rails or Java/Spring anytime over Go. But I don't think that matters anymore. I think we have long passed the phase of building traditional server side web apps. Sure they have their place but man they don't scale. Throw any solution you want on top, be it Rails, be it Spring be it Go but as soon as the project starts getting large and complex it becomes quite difficult to iterate. I believe nowadays a much superior approach is to split the application into API + UI. That scales much better as you can use the best tool for the job (My choice is usually Go for the API) and for the UI well you can pick your poison between one of the billion frameworks and solutions that exist.
Well said, and good point about SOAP. A definite no go if you need to work with it. 
I can't speak to your experience, but I can tell you mine. What I consider pleasant in terms of web development has changed over the past couple years. Ruby and RoR was the first server side langauge / framework I picked up. I loved Ruby. Still do. I love javascript too. They can be truly wonderful to write in. But I grew to hate Rails. Errors would occur for the silliest reasons, little typos. Functions that would only work in the controller but not the model, or vice-versa. Being able to do things very quickly with gems, but not being able to modify them - because the logic for the gem was buried so deep within a dozen classes containing nothing but the word 'super'. jQuery not working on 50% of the apps I wrote without special configuration. Not to mention every app ran slower than blackstrap molasses after a few hundred thousand records. Sure RoR is fun to write and to read, but its no fun to debug. Its no fun to optimize. Its no fun wondering if the 4 year old gem your app is dependent on is being maintained, or if your app is silently hitting errors and just chugging right along. Its no fun writing unit tests for the endless possibilities that such a 'magical' language can produce. Go has lot of error handling. It looks repetitive, because it is. After a few days, you don't even notice it. You just skip over the error handling when reading unless its relevant. But all that error handling means you know exactly whats happening in your code. You know exactly what edge cases your checking against. You can guarantee that a certain block of code is going to do what you think it will do, with relative ease. Best of all, you can do that with other people's code too. Go lets you look directly under the hood and see what your code is really doing. I'm very interested in Elixer and Erlang. Functional Programming is my favorite style and I'm interested to see how the industry takes to Elixer. Good luck on your app. If its as advanced as you say it is, I highly recommend you give golang a real shot before you dismiss it. 
Don't use gRPC if your client is a browser, because gRPC doesn't support the browser.
To address some of your points. I haven't had any serious issues with rails other than the slowness and memory usage. Memory usage only because I tend to run multiple rails apps on the same server. I am using Rubymine and it catches most typos for me. I also tend to have a pretty decent test suite. I am not saying I don't have any errors mind you but nothing I consider to be unreasonable. Most of the slowness of rails is from Active Record. I tend to not use that unless I have to. I use Sequel for most things and lately I have been using micro frameworks like Rhoda as well which are much faster. I have also used Crystal which in my opinion is much much better than go. I mean ten times better when it comes to syntax, error handling, package management, type system etc. The language itself is much better than go but of course the compiler is not as fast and the community is not as large. It really has everything go is missing and more including what I consider to be saner error handling. Elixir is also great and phoenix is a well baked and robust web framework. It has great community but again not as large as go so you tend to run into things you need which are not there. I would give elixir a try if I were you. It's fun. Having said that..... I think I can summarize my research into go this way "don't use go to build web apps, it's only really suitable for building APIs". That seems to be the consensus on the various mailing lists and reddit. As of right now I have not yet made up my mind. I do want to try something new but readings those pages has really left a sour taste in my mouth. I hate Javascript but even Node + Typescript seems more pleasant to use. 
&gt;Why roll your own when you can handpick specific libraries that solve your problems? The difference is that you start with nothing but the standard library and you add only what you need vs having to grok all the framework complexity from the start. I believe this also leads to better understanding of a project. But this is an immense cognitive overhead. First of all you need to do research all of them to see which ones are still actively developed, which ones are documented, which ones have a mailing list you can ask questions on (hint almost none of them), which ones are secure and which ones will work well with the rest of your code. A framework is simply a carefully chosen set of plugins which are known to work together. regarding the API. That seems to be the consensus for go development. Don't write web apps in go, write APIs instead. I don't know how I feel about that frankly. I hate javascript but if I am going to be forced to write most of my app in JS I might as well use node don't you think?
Crystal looks interesting but it doesn't look production ready. "Only good for the writing APIs", not sure what you mean here. APIs are the web...you can do a view layer with Go as well, but most people prefer to use something like React or Angular. You can also do realtime via websockets, or microservices, whatever you want... The idea that you have to go around your framework to achieve standard speeds is worrying. Wouldn't you prefer a system that does what it's supposed to without extra configuration?
jeez, he could have just linked to the original post, no? http://tinyclouds.org/rant.html
&gt; But this is an immense cognitive overhead. First of all you need to do research all of them to see which ones are still actively developed, which ones are documented, which ones have a mailing list you can ask questions on (hint almost none of them), which ones are secure and which ones will work well with the rest of your code. It is indeed but so is learning all the little intricacies of a complex framework. And if you are going to say "But I am gonna learn the framework" well you can also learn the libraries you need. I think it's more or less the same thing from two different angles. &gt; A framework is simply a carefully chosen set of plugins which are known to work together. Sure it is but due to all the reasons I mentioned in the previous comment I'd rather have small sharp tools that combined together make something great, than an over-engineered generic solution. Oh and as for the "are known to work together" well that is great in theory but not so much in practice. For example try to do something custom that the framework doesn't support or plug in a different dependency that fits your needs and chances are you will encounter a lot of trouble which will also be hard to search for. Which reminds me that all this dominance makes searching for solutions harder too. Just try to search for a vanilla JavaScript solution for X problem. Chances are, the top answers will include jQuery. &gt; regarding the API. That seems to be the consensus for go development. Don't write web apps in go, write APIs instead. I don't know how I feel about that frankly. I hate javascript but if I am going to be forced to write most of my app in JS I might as well use node don't you think? As always it depends. If you are a sole developer that has to build everything then sure Node and Ruby on Rails make a lot of sense. But if you have at least 2 people or more, then you can stick with writing pure Go on the backend and enjoy all the advantages of the Go ecosystem. P.S. I am not the biggest fan of JavaScript either so I share your pain. I truly hope webasm will evolve enough to allow us to build our web UIs with any language we want.
Look at the digesting a tree part of: https://blog.golang.org/pipelines
https://github.com/pocketgophers/pgviewer is &lt;200 loc, but useful for my work :) Happy to answer questions about it.
I feel like centralization is the wrong solution to the problem, even with safeguards to avoid the kinds of pitfalls Ruby and NodeJS have run into. Hopefully dep can get some cryptographic signature infrastructure soon though.
[flactools](https://github.com/lpar/flactools), [trashman](https://github.com/lpar/trashman2).
&gt;The idea that you have to go around your framework to achieve standard speeds is worrying. Wouldn't you prefer a system that does what it's supposed to without extra configuration? The framework is modular. You pick and choose which parts to enable and disable. 
&gt; for a certain class of application, which is like, if you're building a server, I can't imagine using anything other than Go
Look at the [coreutils in Go](https://github.com/ericlagergren/go-coreutils).
There are no objects in Go, only values per spec. So define 2 structs, the "object" and the DTO. &gt; I have a long list of domain objects, and each of them can be mapped to a variety of DTOs depending on the use case, and writing these mapper functions is tedious. Write a generator ( a go program) that will look at Go ast and generate your mapper function from the struct definition ? 
Interesting article, thanks for posting. A lot of what they achieved can be done another way with existing tools : 1. use [protoc-gen-swagger](https://github.com/grpc-ecosystem/grpc-gateway/tree/master/protoc-gen-swagger), which gives you a [Swagger](https://swagger.io/specification/) specification file. 2. use [swagger-codegen](https://github.com/swagger-api/swagger-codegen) on that file to generate a Typescript client Tested it recently, it works well. It's still JSON on the wire, so you still need the gRPC REST Gateway. But you get all the type safety and codegen from a single specification. Also you can integrate the gateway directly into your grpc server, it doesn't have to be rolled out in a separate binary. I'm not sure how it compares to their solution, I haven't looked into the details of the tools they had to write. Just thought I'd share an alternative.
Very cool! Thanks for this. Probably a naive question, but: what architectures will this example assembly code run on?
I mean, you can have both. Most of my code is double hosted on my own service and github. I consider my own service the source of truth but you can always point your vendoring tool at github if you want. Or any other code mirror. Thanks to git being decentralized and most vendoring tools supporting fork-urls, you can easily setup your own local mirror of important code that never goes down.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/full360] [Goby - GO programming with ruBY syntax : golang](https://np.reddit.com/r/full360/comments/6xfhgr/goby_go_programming_with_ruby_syntax_golang/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
This is pretty intuitive if you consider that a slice starting at the end of some other slice will have zero elements, and is therefore not out of range. No indices past the end of the slice were ever accessed.
Hey, look, you completely ignored everything the parent commenter said in his well thought out reply and said basically the same exact thing in the same way you've said it 2 comments up! He did just say that the best approach would be to use a decentralized, host-decoupled approach to resolving package sources. This is not how Go works at the moment. Sure, it's how some dep tools sitting on top of Go work, but it's not how it was originally designed. It's at the moment objectively less secure to use a vanity URL due to the initial first step of pulling literally from the import path as the pointer to the source code without any further signing or verification. Don't say https, because that's also incorrect - if someone compromises your server, they can either take the existing cert private key or replace it with something else. Pointing out a flaw in a language is not an attack on the language's utility. Trying to shove things under the rug like you are here is just irresponsible, and is how things go years without being fixed. 
in the context of OP's question (new user getting started), no it's not the best idea. Having a shell is also mostly unnecessary if you have the infrastructure in place for logs/monitoring. a few places I've worked pay the cost of carefully maintaining images and networking for stuff like sshd because they already have systems in place for secured ssh access to prod environments. that said, it's definitely not where I'd *start* w/ k8s in GCE.
This might seem weird when you think about it directly, but when you think about working with the slice it becomes somewhat obvious. Eg. [...]string{"a", "b", "c", "d"} ...here [0:] will have len==4 and cap==4; [0:0] has len==0 and cap==4. Then at the end we have [3:] with len==1 and cap==1; [3:0] len==0 and cap==1; thus. when you move to consume that last capacity you get [4:0] has len==0 and cap==0.
&gt;It's at the moment objectively less secure to use a vanity URL due to the initial first step of pulling literally from the import path as the pointer to the source code without any further signing or verification. Less Secure? no. You do the same at github or any other code hoster. Difference being in the size of the operation. Github doesn't verify shit. More failure prone? maybe. depends. &gt;Hey, look, you completely ignored everything the parent commenter said in his well thought out reply and said basically the same exact thing in the same way you've said it 2 comments up! Is there something wrong with repeating myself to amend some information? I merely completed the comment, I was missing were you can have both of the presented options so I presented that. &gt;Don't say https, because that's also incorrect - if someone compromises your server, they can either take the existing cert private key or replace it with something else. Which is why I have a seperate key with which I sign my commits. You can verify those if you want. Https is only a transport security if you weren't aware. On the other hand, if someone compromises Github, same problem. &gt;Pointing out a flaw in a language is not an attack on the language's utility. Trying to shove things under the rug like you are here is just irresponsible, and is how things go years without being fixed. What?
Well like I mention in OP, the code uses CGO, so it won't scale as you are expecting. Also, redundancy is nice to have. Plus scaling via process model is not only simple, it is easy to reason about. 
Thanks, we decided to move ahead with default options and let OS choose what is best for the 4 processes. 
Don't have this luxury as we do not own the Cloud, hence the question here. :) 
First struct maps, then commute.
&gt;Less Secure? no. You do the same at github or any other code hoster. Difference being in the size of the operation. Github doesn't verify shit. Nope. GitHub has a lot invested in keeping accounts secure. Your domain does not. &gt;Is there something wrong with repeating myself to amend some information? I merely completed the comment, I was missing were you can have both of the presented options so I presented that. Yes, it's been said that with additional tools on top of Go you can paper over these issues. You did literally just repeat the exact same thing the parent commenter was replying to. &gt;Which is why I have a seperate key with which I sign my commits. You can verify those if you want. Go does not enforce pgp key signing on commits. &gt;Https is only a transport security if you weren't aware. On the other hand, if someone compromises Github, same problem. GitHub is significantly less likely to be compromised than your random GoDaddy vanity URL. 
You know what, I am getting a little annoyed by gRPC being advertised around here like next best thing in web services when the Go bindings for it are poor right now. Might be ok with browser or apps but not nice for distributed services for which gRPC is a perfect fit. Heck I am not happy with the way default protobuf compiler generates go data structures for proto files. Who in their right mind decided to have structures generated with even boolean and int with a pointer for Go, seriously? 
Either rename the string method or wrap it in a type that eliminates the string interface, but delegates the error interface, like this: struct{ error }{ yourVal }
&gt;Nope. GitHub has a lot invested in keeping accounts secure. Your domain does not. My domain keeps only my account and accounts of close friends secure. I literally could not give less of a shit. And you shouldn't either because you're only pulling from there. &gt;Yes, it's been said that with additional tools on top of Go you can paper over these issues. You did literally just repeat the exact same thing the parent commenter was replying to. Well, yes, because that is the current situation. &gt;Go does not enforce pgp key signing on commits. /shrug Then maybe you should make it a habit to check? You can't fault other people for a lack of security on your behalf. &gt;GitHub is significantly less likely to be compromised than your random GoDaddy vanity URL. I don't use GoDaddy, I use Cloudflare for DNS and have a Dedicated Server and a VPS on DO. I use SSH keys and 2FA everywhere I can on my own service unless there is a reason not to. Not that it's anything significant because my Keepass is set to 64 character passwords most of the time. Github is the target of hundreds of hackers a day. I get script kiddies on my port 22. I'm simply no target for any big guy to take down so I'd say the probability is the same. Unless you can cite evidence that this is not the case.
What other languages besides java and c# have good soap support ?
I understand the redundancy reason, but what about cgo calls that make the single process instance scaling up difficult? Not arguing just genuinely curios.
Anyone have an opinion on gRPC vs. graphql in Go? 
In Go1.9, just use https://golang.org/pkg/net/http/httptest/#Server.StartTLS and https://golang.org/pkg/net/http/httptest/#Server.Client
How complex are the relationships in your data? Last time I checked it was not worthwhile implementing graphQL unless you had complex relationships and shit tons of data that particular clients might not be interested in.
Thank you!
Oh my.. you want to serve bytes right? So do that. Add abstractions when doing that becomes tedious. Don't start software design at the abstractions. If you need help while serving your bytes than explain the **problem** here, not the solution you have chosen.. or in this case the entire abstract domain of the solution you have chosen. Questions in [XY problem](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem) form should be avoided in general, but in Go even more so because you will find the solutions are much different (simpler) than your current background.
Hi guys, OP here. All this talk is very interesting. And I would like to apply some of the optimizations that you talk about. But frankly, all of it is going above my head. :) Could any of you dumb it down to simple english so that an ignorant like me can understand ? ;) I get that I have used old instructions where I could have used new. And something related to vectors which help to do operations in parallel. I will read up obviously. But any help will be greatly appreciated. 
I like https://github.com/nsf/godit, not sure how many lines.
Go is one of the best languages for this purpose. My elevator pitch for it is "80% of the productivity of Python, 80% the performance of C". My company started with Python and when we outgrew it, switched to Go. We handle 800Mbps of traffic and over 2 Billion API calls per day. As for the size of the community, it is middling but growing fast, and more importantly the quality of both members and contributed code is very high. No architecture astronauts over-engineering and over-complexifying as with the Java weenies, or noobs creating security vulnerabilities by cluelessness as with PHP. IMHO, the three general-purpose languages any programmer ought to learn are JavaScript, Go and either C or Rust.
"my website is obscure so it won't be attacked" You do you. I'm never pulling anything from any of your domains, but after all, why would I in the first place? With this kind of "no u" logic, nothing would ever be fixed. I'm happy that the Go developers do not share your limited view.
I don't but I am also interested in hearing from someone who has an (informed) opinon.
Graphql is a schema for data association and query, there is no transport specified in it. gRPC is a schema for services you can provide in a distributed manner along with data schema with transparent support for transport. So they both can be used together, just pass around your graphql queries as protobuf. It depends on your use case. If your use case is about changing schema on the fly use grpahql with gRPC. If your data schema doesn't change quite often, use just the gRPC.
Fields having pointer types lets you tell the difference between a field not being present vs the empty value. That is the same reason most of the fields in the AWS APIs are also pointer values.
Hmm I'm not quite sure what you're talking about here. I use the Go bindings and haven't noticed anything wrong with them, and also haven't noticed primitives being given pointers. Are you using proto2 or 3? Try it again with proto3 and see if you still see the issues. 
The best solution would be a mix of both. I'm working on something like that at http://github.com/rgraphql/magellan
I just figured out a way to mapping structs in Golang, particularly for mapping Model to DTO. I would like to know your thought on this. Thank you very much!
I applaud your enthusiasm, but if you've started this project for reasons other than learning/horsing around, I suggest you contribute to Crystal instead, if you can.
&gt;Initially we went with REST and HAL, but the burden of re-implementing and maintaining clients for the plethora of languages (C++, Java, Go, C#, Objective-C) started to eat into our time to do what we really like Isn't that the whole point of swagger? Write up your API doc and then generate all the clients automatically without any burden.
So basically, Intel specified the SSE instructions to only manipulate the SSE registers, leaving the upper 128 bit of the corresponding AVX register untouched. Leaving half a register untouched is a bit tricky to do efficiently. Intel did this by giving the vector floating point unit two modes: In SSE mode, the lower 128 bit of each AVX register is detached from the upper 128 bit. This way, SSE code can be executed efficiently. In AVX mode, the lower 128 bit are attached to the upper 128 bit. When you execute a legacy SSE instruction while the vector FPU is in AVX mode, an expensive partial update is performed which costs you an extra cycle or two. The vector FPU enters AVX mode when any AVX instruction is executed that sets the high 128 bit of any vector register to something not equal to zero. SSE mode is entered when the `vzeroupper` instruction is issued to clear the upper 128 bit of each vector register. Now what if you want to use both SSE and AVX instructions at the same time? To do this, Intel added a new encoding for every SSE instruction called the VEX encodin. VEX encoded SSE instructions behave the same way as legacy SSE instructions with the following differences: a) when they write to an SSE register, they zero out the high 128 bit of the corresponding AVX register, avoiding the partial update penalty outlined above, b) each instruction that overwrites one of its operands gets an extra operand for the result and c) their mnemonics have a `v` attached in front to distinguish them from their legacy counterparts. If you use AVX instructions at all, you should always use VEX instructed SSE instructions, not just to avoid the partial update penalty, but also because you can lower the register pressure by not having to destroy one of your input operands. You should also issue a `vzeroupper` at the end of your function so other code using legacy SSE instructions isn't slowed down. Let me give you more information later when I have more time.
You have to have the parallel of `[:0]` EDIT: you have to have the empty array at the end of it just as well you have the empty array at the beginning of it. I agree the index of the empty array at the end of it is not a valid index when accessing an element. See my reply below yours...
On the dynamic side, a webapp in Go is going to look no different than an app written in any other language, except that it also provides HTTP/2 out of the box. HTTP/2 is no different from HTTP/1 when you're looking at vulnerabilities in the application, but if you were going to do protocol fuzzing then there would need to be that. On the static side, I'm not aware of any static analyzers (yet) for Go that have a really strong set of defensive programming rules. However, the lexer and parser are available as API's so writing a static analyzer specifically for defensive programming type of things would just be a matter of understanding what API's need to be modeled, what incorrect or unsafe things look like, and what safe usages of those look like. Doing proper dataflow analysis and taint propagation, however, would take a lot more time.
I wouldn't think it's very common, just because it's not that much easier to do in Python than it is in Go. Sure it's a bit easier, but is it so much easier that it's worth adding another language to a project/projects just to run the tests? I don't think it is. 
I've certainly never seen it. And the whole "Python being faster for X" thing is a highly subjective statement. Write your API tests as Go tests, using the standard [net/http/httptest](https://golang.org/pkg/net/http/httptest/) package. Then you can share your model types, get code coverage reports, and more.
It's almost ridiculous how much more easier the webpage version is to read compared to the twitter thread. Thanks.
Wow that sounds really promising. What material did you use to learn go? I just went through go tutorial on the go page, until gorotuines, but I don't think I'm ready yet...
it's a good question, and I should probably mention it somewhere but all these examples are for the amd64 instruction set. The `sqrt` example is where I introduce the different architectures.
There's a few simple command line tools here. https://github.com/averagesecurityguy/30DaysOfGo
The idiomatic solution here is not to use reflection and simply write out the DTO mapping function. It's a little extra clerical work come refactoring time, but it's simpler and more efficient. Of course, nothing morally wrong in writing Go in a Java/C# style, it's your pregorative; but reflection can easily be avoided here and thus using it is unidiomatic.
I've done the same thing and can't recommend it enough. You end up with the ability to write .proto files and generate strongly typed frontend TypeScript/backend Go plumbing automatically. You can iterate very quickly in the early stages of a project knowing that TSC and go will catch all of the type errors.
Funny, I kind of did the inverse. I wrote a go program that outputs a fairly a complicated XML string. I pipe it to a python script which sends it to a SOAP interface. I could probably just as easily write the XML generation in python, but I enjoy writing Go. However, Python has a ridiculously simple package for interacting with SOAP interfaces and I had to mock something up quickly. This was just for systems integration testing, not production by any means. 
Agreed, but that puts unnecessary pressure on GC. They perhaps could have solved this problem by keeping a bitmask of fields as unexported member and provide APIs around it. 
I wrote [dfm](https://github.com/chasinglogic/dfm) which is a dotfile manager. Very straight forward code methinks.
Lol ikr?? His time would be much better spent on reddit leaving shitty comments on other people's open source contributions.
~~I'm not very familiar with Node, but isn't that back end (i.e. server) JavaScript? Seems a little strange to have its creator say something like that. Or am I missing something?~~ Edit: from the article: &gt; That said, I think Node is not the best system to build a massive server web. I would definitely use Go for that. And honestly, that's basically the reason why I left Node. It was the realization that: oh, actually, this is not the best server side system ever.
That's a pointless interface: https://godoc.org/github.com/yl2chen/cidranger#Ranger
What do you use to make sure you have different configs for different environments and dynamically compiling and reloading your app while you develop? 
One random thing I noticed is the order of all your slice assignments like [this](https://github.com/esimov/stackblur-go/blob/master/stackblur.go#L99) may benefit from the upper boundary being assigned first. In something like this bounds checks can add up and it may help the compiler with BCE if it isn't already. If it still generates bounds checks you could probably get gains by assigning the four values on one line at the alight cost of readability though. 