You can use a lock block whenever accessing the list. Only one thread can use a List&lt;T&gt; at once. So by using lock blocks around portions of code using the same lock object, you can ensure only one thread is inside a lock block locked with that lock object (probably the list, as long as you don't make a new list or ever let the reference be null). The key from there is to minimize the time spent in the lock to keep the time spent blocking other threads to a minimum. And of course once you leave a lock block you should expect the list to change before you reach the next lock block. Examples: T newItem = somethingSomethingGenerate(); lock (list) { list.Add(newItem); } T[] stuff; lock(list) { stuff = list.Take(10000).ToArray(); // Side note: LINQ evaluates just in time so be sure you don't let any LINQ objects generated from the list be operated on outside of the lock... convert them to an array or something instead first. } lock (list) { int index = list.IndexOf(someItem); if (index &gt;= 0) { list.Insert(newItem, index + 1); } else { list.Add(newItem); } }
&gt; Interpreting my post as ‚Äúyou should cast everything to IEnumerable‚Äù is pretty wild. That is the topic of this conversation; whether it's better to be explicit about the return type or always use IEnumerable regardless of the underlying type.
I currently have a Chromebook that supports Linux apps. I'm planning on using it to test my .NET code with .NET Core and see if its compatible. In the long run, I'm hoping to make quick code changes/additions on the Chromebook and migrate them to my .NET. For now it's just testing. &amp;nbsp; I'm hoping I can use the Chromebook to make small code changes
Oh we were discussing a false dichotomy? Yeah I‚Äôm out. 
Unless you 100% need to go to Framework, stay with .NET Core.
Definitely. Hopefully you will find us helpful üôÇ
This looks like some kind of chain of responsibility implementation. https://en.m.wikipedia.org/wiki/Chain-of-responsibility_pattern
I have literally no idea where you got the notion I was shoving business logic into the UI layer, which again is why I‚Äôm telling you: dude, you‚Äôre misunderstanding me.
Thanks for this, I'm trying to get my Blazor project off of the nginx server in my basement. :)
Ah, yes! I totally glossed over this one. I think this is the path I should actually be taking. Instead of a test method, I can use an approach more akin to the [ASP.NET](https://ASP.NET) middleware architecture. Seems much clearer and I can actually reference a GoF pattern when needed. Thank you!
Hey guys, you were right! my company was null, like pajej811 said, i debuged the controller ant it was not getting any data from the database. noob problem but i learn from it, thanks again :) 
Your welcome :)
Interesting. didnt know about the Vector4 vs Vector3. &gt;he was able to speed up his raytracer by 7x over the old System.Numerics version I wonder how he sped it up if System.Numerics are already that optimzed. &amp;#x200B;
Still there is the question of how to set up the chain. The sequence of handlers might be important. I've used an approach where the handler gets the receivers injected by the DI and then based on attributes on the receivers first sorts them using a graph sorting algorithm. The receivers were decorated with [Before(other type of receiver)] and/or [After(other type of receiver)] attributes so that it became possible to dynamically add plugins to the app that extended the behavior of the handler
&gt; if it makes sense to revert to .NET Framework No. &gt; I'm still on a learning curve, but have had some struggles getting a few things working. Sounds like the main problem is that you don't know ASP.Net Core. The Core runtime, ASP.Net Core, and EF Core are plenty mature for production. It's not a big question at all. It's pretty cut and dry. For new web development you should use Core unless you have a necessary dependency on something that's not Core compatible (which is not all that much at this point, beyond desktop UI frameworks).
Then just update after 3.0 RTM's.
Nice. Fortunately in my current scenario, the order will not matter. Exactly one or zero handlers will handle each request.
2.2 is not LTS so 2.1 definitely. 3.0 probably won‚Äôt be LTS as well.
Unpopular opinion but unless you 100% need to go to Core (portability or specific use case), stay with Framework, which is much more stable and may make more sense for some enterprise use cases. Core is still moving sands, Framework had a lot of breaking changes from 1.1 until 4.0 and even 4.5. This being said you can use ASP.NET Core on top of Framework. Would not use MVC for sure at this point.
RemindMe! 9 hours ‚ÄúCheck out this UI lib!‚Äù
Are you saying not to use MVC core? I just want to make sure I understand correctly before continuing. 
Put the handler in the same file as the model. Or put the handler inside the model class which works fine too, see example: [https://github.com/JasonGT/NorthwindTraders/blob/master/Northwind.Application/Customers/Commands/CreateCustomer/CreateCustomerCommand.cs](https://github.com/JasonGT/NorthwindTraders/blob/master/Northwind.Application/Customers/Commands/CreateCustomer/CreateCustomerCommand.cs)
Don‚Äôt forget Netlify. I think it‚Äôs better than Github Pages.
Not exactly what you want -- but it's easily to setup and you can provide a link: https://github.com/ChangemakerStudios/Papercut Papercut is a localhost SMTP server + UI for testing.
&gt;What is Blazor? Blazor is an experimental .NET web framework using C# and HTML that runs in the browser. Blazor is the WebAssembly tooling for the Microsoft .NET stack. I feel like this routinely gets less emphasis than it should, and I think anyone interested in what Blazor brings to the game should also be getting up to speed on how webassembly does what it does. There are other implementations of WASM, and keeping separate what is 'Blazor' (the tooling) and what isn't seems important for devs as they get started. My two cents, FWIW.
That is true. Tried not to spend to much time telling about it. With post being about hosting it. I assumed my target audience already knew about it :)
Was thinking about adding them and google
Fair enough. I am a WASM fanboy (because like everyone else I've had to develop using evil, wretched javascript for years instead of a real programming language), so I want knowledge and hype for WASM to spread regardless of implementation. That said, I too am stoked for Blazor as .NET is my workhorse.
If you have the time, you can add them to the hosting GitHub repo ;)
.NET Core 2.2 is working surprisingly well. I just finished an ETL program to load 2TBs worth of data into PostgreSql. It was built on Debian using Visual Studio Code and C#. It is multi-threaded, creates processes, handles files (open, read, write) and interacts with the database just like in Windows. Absolutely flawless. So exciting. Still in shock this is happening. Disk IO is faster on Linux than Windows, at least for ETL programs with big data and Linux boxes are much cheaper to rent. Visual Studio Code is a bit buggy on Debian/Linux. Some Keyboard shortcuts are broken and it lacks the quality of Visual Studio on Windows, but it is so much better than anything else I've tried on Linux (all offerings). It has support for C# .NET debugging too, although it has some quirks and its limited compared to VS. I haven't figured out how to attach to a process, move the stack pointer or move the variable window down horizontally, though I didn't research how to do that. One thing .NET Core lacks and sorely needs is a logging mechanism. For Windows we have the EventLog classes, but for Linux there is nothing of the sort. I'd like to log to the system logging like all the other programs do and utilize the existing Linux tools to search/filter logs. 
Yes. Please use ASP.NET Core (which is not called MVC anymore) but the runtime can be NET Framework.
MediatR is a implementation of the Mediator pattern. This is common in simpler languages like Javascript where you don't have strong typing and dependency injection, so instead everything you want to do is a message (basically an object with some data) sent to a pub/sub channel and a subscriber will listen to that message and respond. Now you can have various classes all communicating with each other without having to manage complex dependencies, they just have to have the ability to send and receive the message. You can also think of this message-based architecture as an abstract request/response model. Everything is a Request object, which runs some logic, and then gets back a Response object. Since all the logic is abstracted, everything can live in core library that just deals with the logic of what to do for things like CreateCustomer or BuyItem or EmptyCart instead of worrying about any other application specific details. The app then becomes a thin wrapper around the core logic and can expose the Request/Response as HTTP requests and responses using mvc/webapi, or you can also use a message queue for a background worker, or use anything else as the outward facing API. CQRS is a more complex version of this that separates commands (writes/modifications) and queries (reads) but it's overly complex for most apps. The fact that Requests and Responses are just objects/messages means that you can also use an event sourcing pattern to store all the requests and responses in order so that there's an immutable and archived history of everything that happens in the system but this is also overly complex. &amp;#x200B;
Again you are saying to target .Net Framework 4.5 &amp; the like within a dotnet core project? [Documentation](https://docs.microsoft.com/en-us/dotnet/standard/frameworks)? If so that runs counter to what .Net core was intended to be used for. Although Microsoft allows going backwards compatibility for Core, it was them caving to major 3rd party vendors. It was also the same reasoning they went from using projects.json over to csproj because old school developers wanted msbuild like tooling. With this being said dotnet core is where the entire framework is going to go and not fully moving over to it for new projects is foolish. Having the ability to run your applications within Docker containers on Linux systems is a huge game changer. This was only made possible by having strict cross platform support. I have been developing with Core since rc2. At my company we used it for production systems and at the time core was super buggy. The platform is stable and imho only has one major feature not implemented, xplat Directory Services. This can be solved by using the novell ldap library port. There are also issues with ef core that should not have been implemented but were due to old school developers, lazy loading. This is a huge grip of mine but it is just because I prefer to load related entities as needed. The main point I am trying to make is do not go backwards or mix new with old. There are libraries that will not work with the Core but there are other ways to accomplish the task. 
I understand your point. I agree portability is the main selling point right now. However, both Core and Framework are superseded by .NET Standard which defines a common set of API. ASP.NET Core is a Framework built on top of .NET Standard. And there is no plan yet to have .NET Framework not supporting the next .NET Standard version. As you mentioned, you probably had to port/rewrite your application a few times going from version to version, and things are still changing a lot between versions. It will settle eventually and become more stable, but it is not an easy choice to make now on which platform to use.
huh? you can use the operator inline: var slice1 = list[2..^3]; // list[Range.Create(2, Index.CreateFromEnd(3))] var slice2 = list[..^3]; // list[Range.ToEnd(Index.CreateFromEnd(3))] var slice3 = list[2..]; // list[Range.FromStart(2)] var slice4 = list[..]; // list[Range.All]
There is just no way im visiting a url with the word 'swag' in it. Sorry, OP.
[Here's the csharplang proposal for Ranges.](https://github.com/dotnet/csharplang/blob/master/proposals/csharp-8.0/ranges.md)
It's okay :) I am just in love with my URL. Think it's a Danish thing with not taking your self to serious :)
[ASP.NET Core won't run on anything other than .NET Core anymore after 3.0.](https://github.com/aspnet/Announcements/issues/324)
Visual Studio Code works too. Free.
Hmm. So everything I know about migrations is what I read in this book so far. My tutorial app still seems to be working, but I am still not knowing what to do to add the missing migrations and only them. 
Fantastic! That‚Äôs a good one. Thanks. For the FTP server, I was thinking of using [FubarDev.FtpServer](https://www.nuget.org/packages/FubarDev.FtpServer/) and hosting one in the console app to demonstrate. 
Well, that's kind of the point... they're optimized in that the SIMD-accelerated intrinsic versions of them are faster than the scalar C# versions (which you get as a fallback on older versions of the JIT), but you can still build better/faster code with direct access to the individual SIMD instructions because you can optimize your entire algorithm instead of working with/around a few optimized primitives.
I believe I read somewhere that current release of ASP.Net Core would be the last one compatible with .NET Framework. Future releases would need .NET Core. That would be a reason for not suggesting that path. Either ASP.Net Core with .NET Core or ASP.NET MVC 5 with .NET Framework It could very well depend on the specific functionality that you need, but I believe .NET Core is the future, is stable and mature enough, has performance benefits of its own, and enables some interesting scenarios (hosting on Linux, docker... yes I know you can have docker on Windows, but not nearly as stable) Anyway, even if you stay with .NET core, please please please anything that is not the MVC app, use netstandard library instead of regular library. That would make your migration to .NET Core considerably easier
Also looks like [netDumbster](https://www.nuget.org/packages/netDumbster/) is an smtp server I can also easily configure inside the console app. After thinking about it, I really want to have the demo as self contained as possible. I‚Äôll probably install paper it on one of my dev machines to test it out later tonight. 
Yes, I was planning to write about that tomorrow :p
&gt; This being said you can use ASP.NET Core on top of Framework. ASP.NET Core 3.0 won't run on top of the .NET Framework, so with that choice you have a blocked upgrade path.
&gt; Again you are saying to target .Net Framework 4.5 &amp; the like within a dotnet core project? No, he suggested to use ASP.NET Core, but target the .NET Framework. You can run ASP.NET Core on both .NET Framework and on .NET Core. But note that .NET Framework support is dropped with ASP.NET Core 3.0.
I still think my example above was cleaner than the new way in the blog examples. As for these cases, .NET Core already has LINQ TakeLast(). These all seem to belong to LINQ scope to me as they are about collection extractions. E.g. for the most complex case of yours var slice = list.Skip(2).TakeLast(3); 
Thanks, I'm really looking for a foolproof clone / run, with as little reconfigure as possible. I've found nugets that should allow self hosts. I'll try them out and report back. 
Thanks, I'm really looking for a foolproof clone / run, with as little reconfigure as possible. I've found nugets that should allow self hosts. I'll try them out and report back. I'd never seen that write to disk approach, that's really interesting. I'll have to dig into that when I've got more time. 
ok ok, which tool is this that was used to browse the changes animated?
I'd say this is a mix of the Strategy and the Factory pattern
Have a look at [https://github.com/aspnet/Logging/issues/873#issuecomment-435135702](https://github.com/aspnet/Logging/issues/873#issuecomment-435135702)
/remindme 7 Days (with solution please :))
*Hey just noticed..* It's your **3rd Cakeday** adamhathcock! ^(hug)
You know Microsoft's official recommendation is that developers start all new projects with Core and even to change existing projects that are actively developed to Core, yeah? 
Right. There are apparently changes along the way from .NET and ASP.NET core 2.1 to 2.2 to 3.0, but this is the smoothest path that you will get; any other upgrade path will be a lot more breaky.
GitHistory
GitHistory
You can use Visual Studio Code as I explained here: https://www.sergeydotnet.com/get-started-with-net-core/ But It's better to use Visual Studio and the community edition is free just download and enjoy :-) https://visualstudio.microsoft.com/vs/?ranMID=24542&amp;ranEAID=je6NUbpObpQ&amp;ranSiteID=je6NUbpObpQ-xvXlof0eaXmRZyvi1ZHlBQ&amp;epi=je6NUbpObpQ-xvXlof0eaXmRZyvi1ZHlBQ&amp;irgwc=1&amp;OCID=AID681541_aff_7593_1243925&amp;tduid=(ir__bygt1fsczskfrw1hxkpoxiopbn2xmxl66pldfh0p00)(7593)(1243925)(je6NUbpObpQ-xvXlof0eaXmRZyvi1ZHlBQ)()&amp;irclickid=_bygt1fsczskfrw1hxkpoxiopbn2xmxl66pldfh0p00
Thanks! I will request VSCode at work and hopefully they don‚Äôt raise a stink. 99% sure corporate IT will make it nearly impossible for Visual Studios though. I can probably get it but it will take 6 months lol
TIL about double.Epsilon. What would be a practical use case of such a small number?
You can create Virtual Machine(VM) in Azure portal with Visual Studio installed if you can't install it on your pc.
I tried this, although it's giving value of Logging section as null. Is there a way to fix that?
If you have buy in, why not look at spinning up a test instance of the site (load balanced) in AWS for a day or two and run some load testing? There's plenty of tools out there that will let you record user interaction with the site, and then replay it using test agents to simulate load (versions of VS have this, and it's worked well for me in the past)
Thank you, that fixed the issue.
Read the code and determine if there's anything that is stored on the server rather than in a DB of some sort. If not, then you should be good. Also figure out why your site is crashing, don't just think load balancing will fix that problem.
Have you got any specific tools I could look into? Problem is I've got buy-in but partially because I said "it won't take TOO long to set up" since they really care about time spent since they always think we're trying to eff them over in costs, and don't understand that the way we built their site 7 years ago is allowed to have had some design flaws / scaling issues that need resolving down the line.
It's the CPU getting maxed out on the EC2 instance, when I say the site down I mean 'not responsive' - we have a pingdom tracker every minute that checks we can get a response and if it takes too long it's considered "down". But whatabout session state - I worry that might cause us issues? Would I be able to configure ESB in testing to say "serve these requests as server 1" and then say "now serve these requests as server 2" and see if i get the response I expect? thanks for your help
In 2019 you shouldn't even be using sessions. Am I misunderstanding what you mean by session state?
so many videos. why dont you just write blog posts. its impossible to follow so many videos. so my question is: do you have a way of debugging containers locally ? 
We deiced to record videos, as the blog posts would be quite long, however, at some point, we might create articles/ebook about these topics. You can debug containers using VS, Rider or VS Code AFAIK :).
Im aware of debugging. The comple ity arises when projects depend on each other. I would like to download all the dependencies, start a local docket network and use my local code. Do you show this in your videos ?
In that scenario there isn't much you can do, basically, you need to create custom Nuget packages including PDB and the debugging should work just fine. You could also create a Dockerfile that contains all of the source code and start the project using dotnet SDK within a Docker container, instead of .NET Core runtime.
I already did it with docker registry. Nuget wont work because of container specific settings. I was wondering if you did something similar.
If you don't have access neither to Nuget nor source code, I'm afraid there's no (easy) way to resolve this issue.
If the session info is in a cookie, you should be good. If you're storing just the I'd and loading session info from memory, you need to change storage, but that would be odd since restarting the server would also lose all session data.
&gt; the ability to us "sticky sessions or store Session in database and access it from every instance
I dont think you understand. I have my local project. It has dependencies to other projects. I just want to debug my project and download other containers to my computer and start kubernetes. But i solved it anyway
Ok, if you have the full source code I'm not sure what was the problem, but glad to hear you found a solution. 
Comodo sells code signing certificates for 70-85 US$ per year.
I needed to send this link to myself to personally thank you for putting together something like this. It will definitely be of help towards me
I'm glad it'll be helpful! It's annoying not being able to document some of the stuff we do at work for sure. Feel free to hit me up if you run into any roadblocks / have questions!
This does not explain how to detect global mouse hooks.
The easy way to load balance any site is to turn on "Sticky Sessions" or "Hash Based Routing" which will route the same user, or set of IPs to the same server. It is a common way to deal with applications that do not like to be load balanced and AWS Elastic Load Balancer supports Sticky Sessions. To test, spin up to EC2 instances, load your app, add an elastic load balancer in front, enable sticky sessions, and test. &amp;#x200B; Other things to consider is anything taking a lock in SQL server, and since an ecommerce site, how the order ids are generated. I've seen developer get "fancy" with number generation which would break if two servers were creating numbers at the same time. Check for any use of shared Session State, or anything written to the disk. Cookies are not the same as the in-memory session provider that trips up load balancing. &amp;#x200B; Skip ElasticBeanstalk until you are more comfortable with AWS, you could spend days trying to automate 15 minutes of manual work you only do once. &amp;#x200B; &amp;#x200B;
Will do. I've followed you on github and started the project. I may have some ideas down the line, this is definitely gold though. 
Again you dont understand. Nevermind
W/e
Awesome! Looking forward to hearing them. If interested, I do a lot of front end prototyping / notes stuff on [StackBlitz](stackblitz.com/@JaimeStill).
\+ showing me about closure caching issues.
I think everyone consumes informations differently. Videos, docs, code. I'm a fan of reading, but I very much appreciate the videos since they allow me to multitask a little better
Under the hood, binary-based floating point numbers are constructed out of a sign, exponent, and significand. You can then compute the actual underlying value using the algorithm: `-1^sign * 2^exponent * significand`. This blogpost by Fabien Sanglard actually does a good job of explaining this in terms of the exponent being a window and the significand being an offset into that window: http://fabiensanglard.net/floating_point_visually_explained/ `double.Epsilon` represents the delta between values in the first "window". Each window above that has a delta between values that is ~2x the previous delta. So it is really just the baseline from which all other deltas can be computed. 
Thanks for sharing, that was interesting. üëç
Thanks for this, good to know that EBS supports this. For now, it might be worth just using Elastic Load Balancer and testing. I assume I could also do things like shut down VM1 and see if things still work when traffic is routed to VM2. I just wanted something that could scale way beyond our normal usage. Case in point, we had a piece of marketing go out that unexpectedly caused a huge surge in traffic (i.e. a typical months worth of requests in ONE HOUR) and I don't even know if two EC2 instances would have handled that. We haven't even touched or thought about DB scaling either, that's a kettle of fish I am completely clueless on and will cross that bridge only when we get to it. Good shout on SQL locks, I suppose I will have to try and find any potential race conditions, but I would hope since EF Core is being used there are built in mechanics to not wait indefinitely and try and avoid these type of issues.
Bad hat, bad hat, bad hat. It gets really interesting [at this point](https://youtu.be/YwezzKWrFuo?t=1570) where they discuss what it's good for and what it *isn't good for*. That advice is gold. It's so hard to find good advice that says "This is not our use case".
Huge thanks for sticking through and helping me get my docs right!
I am not sure where Session state is used, my boss just thinks it does use it. I think it might actually just be cookies that read a value and then lookup in the DB. I'll have a look and report back! thanks 
If you have VM1 and VM2 behind the load balancer, there will be a brief part where some users could be down. This is the Health Check part, usually 3 in a row for up, 2 in a row for down with checks every 30 seconds to a minute (3 up * 30 = 90 seconds of potential downtime). You can tweak that, just something to be aware of. You can also manually remove a server in case you want to restart it for updates. Good for the marketing team for generating traffic! You may want to check the DB to make sure common mistakes are not being made, lack of primary keys, lack of indexes, those will slow down all your web servers and generate inefficient load. Use something like loader.io to load test your load balancer test. I've maxed out their free tier (10,000/min) on a single dotnet core linux server, just takes tuning. If your web server CPU is high, tune your web server code, if your DB is high, tune your DB schema, indexes. If both are low, check memory and disk IO. 
Yeah it was a good job by them, a shame we weren't prepared but my boss has been trying to get the client to modernise for a while and this finally brought them onside to upgrade areas of the site. We use the cheapest, weakest aws instance available to serve the website so I wasn't surprised usage maxed out! Thanks for that, I'll digest and make a plan of action tomorrow!
tl;dr floating point math can result in small errors; Epsilon is good value to use as an error window. Eg if you've done a bit of math on a it might end up being 1.000001 even if in a perfect world with perfect math it would be 1. If you have a b float that is exactly 1, a == b will be false. But you can do Math.abs(b - a) &lt;= float.Epsilon to have a tolerance for this.
OP tried to help you. Could try being a bit politer.
Integration testing with RabbitMq is an interesting concept. Saved for later :)
Will still implement netstandard2.0, only specific libraries might change. (In a way that's hopefully better than the previous version)
Exactly, we record videos (quite long) as we can discuss the topics as detailed as possible, but I get your point and totally agree :).
Can be done, just got to be careful :).
That‚Äôs awesome!!! 
I'll check that out, currently using https://github.com/djfarrelly/MailDev
Unfortunately `Math.Abs(b - a) &lt;= float.Epsilon` isn't actually a sufficient comparison to handle cases like this. `float.Epsilon` and `double.Epsilon` are the smallest possible deltas and you can't have any value that is less than it other than zero. The delta between values also changes based on the internal exponent being used. So since `double.Epsilon` and `float.Epsilon` are the smallest possible delta, they only apply to the first 2^52 and 2^23 presentably values (those with exponent 0), respectively. The next set of values (covering exponent 1) have a delta of `epsilon * 2` between them (the next exponent covers a range of values that is twice as big as the previous exponent, but with the same number of unique representable values). The next set of values (exponent 2) after that have a delta that is again twice the previous delta, etc. * It is worth noting that the exponents given above are the "biased" exponents, and you need to adjust them to get the actual "unbiased" exponent used by the `-1^sign * 2^exponent * significand` algorithm. So you would find that, for any two numbers in a given window, they are the same distance apart and unless you are in the smallest window, that delta will always be larger than `double.Epsilon`/`float.Epsilon`.
Reminded me about this amazing post: https://medium.com/@indy_singh/strings-are-evil-a803d05e5ce3
How is that form going to be accessible? This seems like a really bad example: &lt;EditForm Model="@person" OnValidSubmit="@HandleValidSubmit"&gt; &lt;DataAnnotationsValidator /&gt; &lt;ValidationSummary /&gt; &lt;p class="name"&gt; Name: &lt;InputText bind-Value="@person.Name" /&gt; &lt;/p&gt; &lt;p class="age"&gt; Age (years): &lt;InputNumber bind-Value="@person.AgeInYears" /&gt; &lt;/p&gt; &lt;p class="accepts-terms"&gt; Accepts terms: &lt;InputCheckbox bind-Value="@person.AcceptsTerms" /&gt; &lt;/p&gt; &lt;button type="submit"&gt;Submit&lt;/button&gt; &lt;/EditForm&gt; @functions { Person person = new Person(); void HandleValidSubmit() { Console.WriteLine("OnValidSubmit"); } }
Can someone tell me what this new ‚ÄúWorker Service‚Äù will be? Is it meant to be a windows service-like app that is running on .net core?
I think it is this: https://github.com/aspnet/AspNetCore/issues/6817
That article sounds like it could be improved even more by using `Span&lt;T&gt;`.
@Html.DropDownList("NoteType", new SelectList(ViewBag.NoteTypeList, "Text", "Value", ViewBag.NoteTypeListSelected), htmlAttributes: new { @class = "form-control" }) &amp;#x200B; In controller: List&lt;SelectListItem&gt;ddList= new List&lt;SelectListItem&gt;(); ddList.Add(new SelectListItem { Text = "type1", Value = "type1value" }); ddList.Add(new SelectListItem { Text = "type2value", Value = "type2value" }); ViewBag.NoteTypeList = ddList; &amp;#x200B; I would store the static list data in model but this should work for you.
That probably means that labels, or references to other controls, are not implemented yet.
I havent done any razor components yet, but that syntax looks awfully like web controls from web forms. why isnt it &lt;input instead of &lt;inputcheckbox so that we dont have same issues as before in web controls back in the days 
It'd be nice if it was something like this: @using(Html.EditForm(person, HandleValidSubmit)) { @Html.EditorFor(p =&gt; p); &lt;button type="submit"&gt;Submit&lt;/button&gt; } @functions { Person person = new Person(); void HandleValidSubmit() { Console.WriteLine("OnValidSubmit"); } }
jfc, that code looks horrible. "Innovating" to simply do something different is really dumb and that line of thinking needs to die.
Razor Pages &amp; Razor components is such an obnoxious way of doing front end work, I don't even know why MS even tries.
Is SignalR the right tool for doing long running background uploads? Like reporting the progress from server to client? Or ist there something better suited for that use case? 
I was lied to! I think. I got that advice a looong time ago. Makes sense I guess as the rounding error gets bigger Epsilon ain't gonna cut it for an error window. It's worked fine for me in the past but I guess I've gotten lucky.
I like this guide: [Continuous Integration and Deployment with Gitlab, Docker-compose, and DigitalOcean](https://medium.com/@codingfriend/continuous-integration-and-deployment-with-gitlab-docker-compose-and-digitalocean-6bd6196b502a) Basically for release/deploy you can use docker:dind and ssh to run commands on your deployment server to pull from the dockerhub repo.
Good point. I have a co-worker switching away from it though because it causes hitbox issues for him when Windows' interface scaling is active. I might give it a chance though.
The improvement on this release on Razor Component is MASSIVE.
&gt;Is it meant to be a windows service-like app that is running on .net core? That's what it sounds like. &gt; this template is designed as a starting point for running long-running background processes **like you might run as a Windows Service** or Linux Daemon. &gt; &gt; In the coming days we will publish a few blog posts giving more walkthroughs on using the Worker template to get started. **We will have dedicated posts about publishing as a Windows/Systemd service** &amp;#x200B;
When is the final release?
&gt;We plan to ship .NET Core 3.0 in the second half of 2019. We will announce the ship date at the Build 2019 conference. &gt;Visual Studio 2019 will be released on April 2nd. .NET Core 2.1 and 2.2 will be included in that release, just like they have been in the Visual Studio 2019 preview builds. At the point of the final .NET Core 3.0 release, we‚Äôll announce support for a specific Visual Studio 2019 update, and .NET Core 3.0 will be included in Visual Studio 2019 from that point on.
You've got to be kidding me. I finally decided to download the .NET Core 3.0 preview earlier today. I must have missed Preview 3 by a matter of minutes.
Darn, .net standard 2.1 won't be supported by .net framework. We have a ton of .net Framework code at work. Just started moving code to .net Standard libraries this week. Our first .net Core web application is going live in about three weeks and it shares code with the .net Framework apps that will never be upgraded. But the tooling for .net Core in Visual Studio is so good now. 
Anyone that doesn't like typing\\seeing f%\^&amp;ing curly braces everywhere... for one.
With .NET Standard the version number is the highest number it supports. You can have a library target a lower version and it will work with both framework and core. 
And Preview 4 is out...NOW. j/k
Hey do you know what causes files sizes to be different? I assume you used the same exact package for all 3 If I go to network tab on and refresh with cache disabled the Azure and AWS ones come out at 4.9MB The github pages one is only 3.5MB The mono.wasm on git is 810kb, but on AWS/Azure its 1.9MB? I've noticed when running the blazor app locally that mono.wasm and mscorlib are less than 1MB each however in the dist folder after making a packing they're like twice the size. 
That is actually a really good question. I will try looking into it :) didn't even notice it. 
That sounds interesting, this could be a *more* future proof LOB service framework. 
Don't joke with us like that Scott! I have features I'm supposed to get in Preview 4!
TOO LATE 5 IS HERE
AAAAAAAAHHHHHHH
My first option would be to use (remote) attach to process, to the application pool running the application. Failing that, adding additional log information to help to track down where it hangs. Or given that it might be something like a long GC, maybe checking performance counter information for ongoing memory consumption, JIT and GC events.
Check your application pool in IIS for the site and make sure it's properly recycling.
Is it possible to add a validation on the Action method to ensure searchString is not null or white space? I‚Äôm worried about doing it with routing will lead to some issues. It‚Äôs easier to just return badrequest on null 
Does anyone know why MS keeps releasing new major numbered versions of Core at (what feels like) this high pace? It feels like .Net classic 4 has been getting a lot of updates as well, but always minor version numbers, making it easy to upgrade. I like updating the .Net versions in my projects. In a little over two years, I‚Äôve ordered a book on Core 1, saw the release of 2 and now 3 is almost ready. Maybe I‚Äôm being fooled by the numbering, but that big 3 indicates to me a lot of breaking changes with version 2 of Core. How will I know when Core is stable/feature rich enough to use for a longer term?
Check out perfview and procdump
it's a new product, so it will rapidly progress in the first few years, then start to slow down in terms of large changes. [.NET Framework was similar](https://en.wikipedia.org/wiki/.NET_Framework_version_history#Overview), although not quite as fast mostly due software being more difficult to update back then. and they're using the major version numbers to indicate a large number of additions, less so breaking changes. 2.0 is largely compatible with 3.0. Same with 1.0.
It's `&lt;InputCheckbox&gt;` due to requiring it to be a component to access validation metadata for the validation to work. Which is the Cascading value `EditContext` which is made from the `&lt;EditForm&gt;` parent component. You can't encapsulate or access any validation metadata with a plain html input element. Though you can make a component wrapper (kinda like how `@Html.EditorFor()` works) with a `type` parameter and pick what validator to use via that type. `&lt;Input&gt;` as a component name doesn't work right now since the razor compiler doesn't differentiate casing so it'll treat the component as plain html instead, though it's in their road map to fix that using the same way react does (Components are starts with a capital letter while html elements are lower case).
Some great changes in there, really like to look of the gRPC template!
good bot /s
Are you sure about that? Because I am 100.0% sure that headyyeti is not a bot. --- ^(I am a neural network being trained to detect spammers | Summon me with !isbot &lt;username&gt; |) ^(/r/spambotdetector |) [^(Optout)](https://www.reddit.com/message/compose?to=whynotcollegeboard&amp;subject=!optout&amp;message=!optout) ^(|) [^(Original Github)](https://github.com/SM-Wistful/BotDetection-Algorithm)
/u/WhyNotCollegeBoard While technically right, did you not see the `/s` after it? Gosh, thought AI would be smart these days /s &amp;#x200B;
Ask for a refund!
Agreed, this sounds like it should be easily detected in code without having to do custom routing. The other option is to bind the query paramater to a model and use BindRequired https://stackoverflow.com/a/43897434/1267231
Ahhh ok, thanks. For some reason I thought MS were using semantic versioning for Core version numbers, as they recommend using SemVer with library version numbers. With SemVer, a change in the first number means breaking changes. 
Ahhh ok, thanks. For some reason I thought MS were using semantic versioning for Core version numbers, as they recommend using SemVer with library version numbers. With SemVer, a change in the first number means breaking changes. 
2.1 is LTS, you can safely use it for a longer term.
The recommended approach would be DebugDiag [https://www.microsoft.com/en-us/download/details.aspx?id=49924](https://www.microsoft.com/en-us/download/details.aspx?id=49924) Take a look here for info about how to use it [https://www.cantabilesoftware.com/support/DebugDiagTool](https://www.cantabilesoftware.com/support/DebugDiagTool) &amp;#x200B; You can also look at the behaviour an get some clues as to what's happening...is memory increasing (so you have a memory leak...make sure you dispose / close especially unmanaged objects). 
They do use SemVer, there just aren‚Äôt many breaking changes, but there are some.
10 years ago I would say .NET Framework as MS keeps stuff around longer than it should so long term support would not be a surprise support forever, ASP/Vbscript/VB ;-) .... but seeing the new of a final Framework announcement, shout to the heavens CORE is great by Microsoft, the EOL of Core versions. The EOL of earlier Core versions is interesting to me, as I dont' recall eol of Framework versions so quickly. Top all this off with younger developers coming in with the newest tech knowledge and some other experienced developers want to always jump on the latest and greatest buzzwords. Trying to stay with the latest official release that your current dev environment has is the best option. This is a new Microsoft , everyone can debate the pro/cons but MS doesn't want to be caught again with fighting uphill some new ROR becomes the great new thing and people forget MS... they want to stay in headlines with NEW NEW NEW, be it saying Open Source, New Version of this or that, Cross Platform Support. I get a kick out the fact they have essentially a last place console but they announce live on Switch and they get in the news for something not even really a "GAME" not available anywhere that is a must have, but just a service on other Game Platforms. But they know what they are doing know, might be aggravating at times, but they make more money now than ever without selling a boxed product anymore.
Check the number of worker threads running. Also look at the IIS logs and see if there is a URL returning non-standard status codes, like 500s
&gt; Problems with AutoMapper &gt; In the real world, mapping between identical or similar classes is not that common. What? Why is that a problem with AutoMapper?
GET /location Returns state object
Cool. Just which of the 7 points that are wrong with automapper did you actually solve? &amp;#x200B; " If you have similar classes you are mapping between, there is no guarantee that they will not diverge, requiring you to write increasingly complex Automapper code or rewriting the mapping logic without Automapper." That is the biggest issue for me and I just avoid any mapping tools because of that. Especially in solutions with hundreds of projects and complex dependencies.
Definitely need more info. Can you share Git Repo? 
What do you mean with state object? 
Is it possible you're in an infinite loop somewhere? If you get another user to start a new session, are they impacted as well?
You can use LINQ on whatever collection you're pulling from. For instance, for an Entity Framework context: ```cs public static async Task&lt;bool&gt; IsLastOrder(this AppDbContext db, int id) { var last = await db.Orders.LastOrDefaultAsync(); return last == null ? false : last.id == id; } ```
Have it return an object that lists the state information of the location
You are going to find lots of ways of doing this and many people that disagree. Myself, I would expose it as query parameters at the list level: /Orders?find={some filter} I would then have the result be a redirect to the corresponding order's URL using an HTTP 307, so that a client that didn't want to download the order could call a HEAD request to /Orders?find={some filter}, or so that followed get requests would work exactly as they expect.
Asp.net core changed a lot between 1 and 2.
I would avoid doing this server side. You've already got the count and index. If the order index (assuming zero based array) plus 1 is equal to the count it's the last entry.
Keep your api's simple. Don't over complicate them. Ask if you really need something and whether it can be achieved by the caller.
`GET /location/acme-corp/orders?orderby=date,asc&amp;limit=1` // acme-corp orders [{ "id": 1 }] If `GET /location/acme-corp/orders` returns \[\] / NoContent, there is no entry, and if it there is an entry, you get the first (by timestamp) or last (if ordered desc)
Thanks for the idea. So, you mean something like this { OrderCount: .., }
Yes
Good idea, but I have one issue with this solution. How can I handle this, when the user doesn't have read rights on all orders (or on the location)?
Sorry, if this was unclear. I meant how to expose it on my API.
Call it from an API controlller: ```cs using {Wherever the above extension method is defined} [Route("api/[controller]")] public class OrderController : Controller { private AppDbContext db; public OrderController(AppDbContext db) { this.db = db; } [HttpGet("[action]/{id}")] public async Task&lt;bool&gt; IsLastOrder([FromRoute]int id) =&gt; await db.IsLastOrder(id); } ``` This can then be accessed at `/api/order/isLastOrder`.
Can't you just make an `isLast` flag that is part of your order resource?
Pretend you‚Äôre showing the user HTML. Whatever information you‚Äôd show there, or links, is what you‚Äôd return with whatever representation you‚Äôre returning.
If the user has no right to know the orders *or* the location, he should not know if any order is the first or last one **I**n **M**y **O**pinion If the user is limited to /Location, then /u/cresquin already gave a good hint
Check the system event logs for application error. .Net exceptions should show up there. Plus, I've seen stack overflows (bad recursion) cause hung app pools.
This is genious, why did i never think of that before? I should question my dev existence ... :D
Hey I fixed that one. Now I have another interesting problem, the css doesn't load. What might cause this? 
Cool blog! &amp;#x200B;
Submitted - not sure how long it takes to get approved, though.
Yes, I also think so. This is my favorite solution.
Is this only in production? I still need to see some kind of source code. From my experiences, it could be a certificate issue. It could be that your CSS needs to be minified (doubt this but possibly). But I'm just speculating without code. 
Consider using the [EventSource API](https://developer.mozilla.org/en-US/docs/Web/API/EventSource), which is ultimately what SignalR uses, but that doesn't necessitate you using SignalR yourself.
I have been following this example -&gt; ([part 1](https://altkomsoftware.pl/en/blog/building-microservices-on-net-core-1/), [part 2](https://altkomsoftware.pl/en/blog/microservices-net-core-cqrs-mediatr/), [part 3](https://altkomsoftware.pl/en/blog/service-discovery-eureka/)) which uses Ocelot. Seems to suit our needs pretty well.
The latest edition of the book widely regarded as the single best C# book, written by Jon Skeet, famous for having over a million reputation on Stack Overflow. According to this page, the paper and eBook editions are only out later in the month. But if you purchase it directly from Manning you should be able to read the "livebook" and PDF versions immediately.
Time to get that fiber connection so you can download releases before the next is out! 
It's approved - https://www.stitcher.com/podcast/net-bytes
As others have said, take a dump and analyze it. Another approach is to code review. Are you using threads? It could be a deadlock.
Thank Christ, seeing it postponed time and time again was starting to hurt my soul. It feels like I've been watching it run away since somewhere November Now only "Dependency Injection in .NET" 2nd edition.
Have you looked at using the ILogger interface configured to go out via EventSource to lttng?
Manning also has PDF, epub and Kindle formats already there. (you can email the Kindle version to your Kindle...). 
Thank I will try it.
Great tip Thanks!
DI in .NET 2nd edition has also been released. It's a great day!
Will do that! Thanks.
Ocelot is great for me.
Nice it‚Äôs very good look at it and I like the way you built on the examples. How does this compare to say spoiling up an array of tasks and then dumping them into a parallel.foreach limited by maxconcurrent (something I only recently started using in some automation tasks)
Thanks and I'd say it's a good implementation for a one-time thing or with a set interval like once an hour. But, if you want something continuous that will start working as soon as a job appears, with a limited number of threads, it's not the best route to go. So it can be good depending on your needs.
Ugh, you'd think manning would have it cheaper than you can get it with Amazon prime. Sadly not. Shipping 6.85 +44.95 vs 47.49 on Amazon...
But aren't you just writing the mapping code yourself?
Amazing, will be picking up a copy and brushing up my C#. I work with it every day but there's always something new to learn from Mr Skeet!
At some places ConcurrentQueue can be used instead of Queue with locks.
I suggest you to use [PdfProLib](https://www.officecomponent.com/products/html-to-pdf) HTML to PDF Converter library. It is best to convert your HTML into PDF so you can easily convert list, images and much more because it generates PDF from HTML. SO you don't need to write complex back-end code. With simple lines of codes you can achieve your desired task. 
I looked at this last year and feel that this book, like many others, go the wrong way. They work on the assumption at you, as a new user need to know the intricacies of Core and force you to basically hand code everything. I wrote another manual for my students, and if you message me I will give you a copy, it just uses the built in templating of VS to quickly generate a site and then work to modify it to your needs. 
Is it just me or is this book basically a history of C# going into details about what has changed and is available in newer versions?
I write mappings myself. I use a couple interfaces and register all the types that implement them in the di container, then just have them injected as needed. public interface IModelMapper&lt;TSource,TDestination&gt; { TDestination Map(TSource source); } public interface IDualMapper&lt;TOne,TTwo&gt; { TTwo Map(TOne source); TOne Map(TTwo source); }
Check developer console in the browser in the network tab.
Unless you need an abstraction around your mappings why would I choose this library over creating an extension function to do this work? Your benchmarks clearly show it's slower and it will even take 'longer' to code using your library, as it is basicaly manual mapping the properties plus making use of your library. 
Probably because you need to start mapping some properties by yourself and mapping some ignores, starting to slow you actually down instead of winning time in coding it. 
yes and no. it introduces the language features from basics to advanced while showing why these features were introduced. showing the features isn't what makes this book great. it's how john shows WHY the language designers choose to add these features and what problems they solve and what advanced things these features allow you to do.
Added to my podcast rotation :)
&gt;Now only "Dependency Injection in .NET" 2nd edition. It was released yesterday actually. The title has changed to "Dependency Injection Principles, Practices, and Patterns".
&gt; I couldn‚Äôt name the second place contender and looking on NuGet, nothing else comes close I was curious, here is a comparison: Project | Downloads ---|--- Automapper | 33000000+ ValueInjecter | 837000+ Mapster | 298000+ ExpressMapper | 256000+ TinyMapper | 122000+ AgileObjects.AgileMapper | 49000+ Boxed.Mapping| 14000+ OoMapper | 8000+ Top 3 discounting automapper seems to have a fairly decent usage too.. why not go with that?
Honestly I love format like that. Because you can come into it with just basic OOP knowledge and come out knowing everything
I was one of the people recommending the authors change the title. So glad they did. I do 90% of my dev work outside .NET and I found the book so useful (bought it in MEAP and read it already). It shows doing DI with and without containers. Changed the way I write my Node.js Express apps.
One to five minutes is a really long time. I'd probably look into optimizing the query first, if possible. Even if your users are OK waiting that long, you putting a lot of strain on your database if that is a commonly used Dashboard. Could the data for the dashboard be pre-computed and stored in a table so that when the user accesses the dashboard they could quickly get the results? Without knowing the purpose of your page it is hard to make recommendations for performance. Performance is definitely a 'details matter' kind of thing.
Thanks!
Thank u! Will check it out :)
System.Threading.Channels are soooo much better for multithreaded producer consumer. They are like a breath of fresh air. Go get some. https://sachabarbs.wordpress.com/2018/11/28/system-threading-channels/
&gt;https://stackoverflow.com/a/43897434/1267231 Yes that was it followed stack overflow article and remembered I have done this before in the process lol. Thanks again!
So apparently Github pages automatically does gzip compression for you Looks like it did it for mono.wasm but not for the .dlls? Wonder if the files in dist folder can be gzipped before putting them up in Azure/AWS
!remindme March 30 2019
I will be messaging you on [**2019-03-30 02:58:36 UTC**](http://www.wolframalpha.com/input/?i=2019-03-30 02:58:36 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/ayegnc/fourth_edition_of_c_in_depth_has_been_published/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/ayegnc/fourth_edition_of_c_in_depth_has_been_published/]%0A%0ARemindMe! March 30 2019) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
&gt;I should question my dev existence You wouldn't be a dev if you didn't ;)
Well, the new Razor Pages in .NET Core is MVVM. And most UIs, I believe, also follow the MVVM pattern. - UI ViewModels - Service classes encapsulating business logic - Data models representing the database tables So I think there's a good bit of class re-use potential and only the UI layer models would differ.
Noted, but that's not apples to apples. Manning also gives me the PDF and ebook formats along with the paper copy, and they let me download it an unlimited number of times from my Manning account, which I've used quite a bit over the years to grab something I didn't physically have with me. 
Anyone trying to use this with VSCode, you need to change your omnisharp version to 'latest' (or one of the recent beta versions) or your C# extension will be broken.
There are plenty of Code Signing certificate providers who offer Code Signing Certificate from the most trusted brands such as Comodo, Symantec. I think you‚Äôre searching for alternatives of GoDaddy Code Signing certificate, so that, I think you can go with Comodo Code Signing certificate which support multiple platforms using the same certificate. If you really want to find the difference between code signing certificates from multiple brands, you can get help from Google Search Engine by searching keywords like ‚Äúcompare code signing certificates‚Äù where you will compare the features of Code Signing certificates.
How could one implement 1000 concurrent queues? I'm guessing threads wouldn't work.
Typically if you buy the physical book, you can register it with your account and still get all the benefits.
Coming from php and js/node (and a little python here and there) would this be a good starting point to learn c#. I‚Äôve been lurking around this sub for a good book tonthumb through. 
Sounds awesome :)
also: [Razor Components Roadmap](https://github.com/aspnet/AspNetCore/issues/8177)
It would be nice if this blog post said a few words about what those changes mean. What is improved about event handling? Bugs fixed? Speed/memory optimizations?
Can't do wrong with the book. Probably will create an infoglut when just new to .Net though. There are probably better books for starters. No need to immediately know how about reflection, expression trees, Span&lt;T&gt; and whatever 'advanced' topics there are in my opinion. 
Well then you should model your domain entities and DTO's correctly and not blame the tools.
You don't want your clients to receive domain/ DTOs and that's where the whole object mapping happens. Often these domain / DTOs contain more data than the clients receive. I don't get where these are modeled wrong. 
We're using Azure Functions so the choice was limited but we used MigraDoc for a while. It was incredibly bad, with a ton of "3 cm" string literals, bad formatting and overall problems so we decided to move onto Html To Pdf solutions. We've been successfully using [https://www.browserless.io/](https://www.browserless.io/) for that for a while now. It's quite fast if you use REST Api they provide. Just don't use .NET version Puppetteer-Sharp, it's so bad that it opens connections multiple times and just setting it up before every pdf generation takes a few seconds. Just use the rest API, send an html string and you're done. It's also fairly cheap and they provide a Docker image if you want to host it yourself.
Cool, thank's!
Yes, we both said exactly the same thing in the first sentence. AutoMapper maps what is there, if your domain entity has 10 properties and your DTO has 9 properties, it will map the 9 matching properties. If you decide to go and put a Password property in the DTO for some reason it's a failure to model the domain, not a problem with the tool. 
I'm using wkhtmltopdf since a few years and it works fine for our use cases (generating invoices / contracts).
Thank's! 
I can't recommend this book enough. I read the first version of this book cover to cover and for me it has been the most informative book i have read on c# and is also written in a way that you want to read it. 
why? &amp;#x200B;
Why what? It explains how to detect global mouse events by registering your own hooks, but it does not explain how to detect global mouse hooks.
A bit of a bulky but easy solution is to use a headless Chromium to generate the PDFs. It gives you full control over page breaks etc. using [@page](https://developer.mozilla.org/en-US/docs/Web/CSS/@page) CSS and is free. You can use [PuppeteerSharp](https://github.com/kblok/puppeteer-sharp) for an easy wrapper around the Chromium API, it also takes care of Chromium executable retrieval etc. Take a look at their [PdfAsync](https://github.com/kblok/puppeteer-sharp#generate-pdf-files) functionality.
Thanks! 
It looks amazing! I also want to do something similar but just for fun (and learning as well). Can you share with us what resources you used for have an idea on how build a parser and a CLI? (Like books, articles, podcasts, and so on). Cheers! 
Could you recommend a couple good books? Thanks
I still don't "get" Blazor. Why on earth would I want a runtime (.NET) on top of another runtime (WASM)? Is it truly not possible to just compile it straight to WASM? It just feels wasteful.
Thanks. It's actually quite simple, basically a simple parser + a little reflection to create the objects. Didn't really use any resources except some Google for reflection, which I don't use too often and it's a bit different in .NET Standard compared to the .NET Framework. 
. Net brings the ability to leverage years and yearsof existing code to client side apps. 
You will want to minimize what data is transferred across all channels. Think about this, your potentially pulling many mb of information from whatever data store you use to your server side code, then it is in turn potentially doing something with it and passing that down to your client in which that is doing the aggregation and such needed for visualization? What if the client side doesn‚Äôt have a lot of power? Now your making that client do all the work on a single thread which could look pretty poor for a user experience plus you miss the opportunity to cache the results making it faster for multiple users / requests. (Sure, you could cache via the client side...) Define the use cases for the data you need. See about getting that data from the source so the data transferred across all pipelines is substantially better. From there you can usually improve performance via indexes or through better query plan execution. This works well for typically line of business style apps. If you have to deal with various data sources across different locations and/or tech then this solution changes a bit! 
Why can't that code be compiled straight to WASM? Also, as a user, I don't want a damn runtime running on top of another runtime just so you can reuse code. Websites are already slow enough.
The queries are essentially used to compute values over multiple XML files in the XML-database, the number of files are at least 30 000 if not more. The values I get from the queries, I want to visualize. The values can have a hierarchy as well. For instance, one file has multiple different values. &amp;#x200B; The purpose of the dashboard is to visualize the values of one or multiple files, like statistics. One approach I was thinking is to pre-compute the values only once every day, map it to entities and then store it in another database. When the client accesses the dashboard, the API controller would access that database instead of the XML database. The question is performance and memory constraints in that case.
Hi, thanks for the PM. This is my answer from the thread to get a bit more context: The queries are essentially used to compute values over multiple XML files in the XML-database, the number of files are at least 30 000 if not more. The values I get from the queries, I want to visualize. The values can have a hierarchy as well. For instance, one file has multiple different values. The purpose of the dashboard is to visualize the values of one or multiple files, like statistics. One approach I was thinking is to pre-compute the values only once every day, map it to entities (e.g. map each file) and then store it in another database. When the client accesses the dashboard, the API controller would access that database instead of the XML database. The question is performance and memory constraints in that case.
Well, dont use it then? 
I think direct compilation to WASM is the end goal. I just watched [this presentation](https://youtu.be/Qe8UW5543-s) last night, and one of the points they get to at the end is performance and client download size. They show a simple example of a prototype AOT compilation feature, but they also made it sound like it‚Äôs not very reliable at this point in time. But I‚Äôm guessing this is the holy grail for them, as it looks like getting AOT comp to work reduces execution time on the client by several orders of magnitude, as you‚Äôd expect.
I‚Äôve used ABCPDF for years with some complicated use cases to great effect.
Well, I stand corrected I guess. So, order with Prime and profit? I dunno, someone is taking a markdown there I guess.
He links to [this page](https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-core-3-0-preview-3/) that goes into more detail about the changes. 
Why would you need 1000 concurrent queues?
I think I follow you, but realize that this situation is just bringing the browser runtime experience more inline with "server side"(might be a bad comparison).... What I mean is, non-browser apps.. .net and java when running on a server are runtimes that are on top of the OS layer For native apps I could always write everything from scratch using only ANSI standard C or C++ (did that for years), but .net and java give me an additional level of productivity in the dev side that I usually am ok with trading "performance" at runtime. But if that's not what you are after, then maybe you'll tend towards more 'pure' solutions?
!remindme March 30 2019
What is an XML database? Do you mean a database that is storing XML data? If the value can be precomputed at the time that the XML is stored, you might want to create a separate table in the database that is updated everytime the XML is updated and then you could use lighter weight SQL aggregations (compared to pulling XML and parsing out values).
Another thing I don't get is _why_. Sure, for some applications such as Discord, Slack, and other _apps_ this is fine, but are we supposed to use this instead of normal web pages? The very sole purpose of the web is to publish information and link it using hypertext, where with these patterns we are approaching having applications running on top of the browser with their own run-time. With these kinds of applications we are basically at the beginning before the web was born, with no standardization in terms of how content should be displayed and rendered. I think it is very cool having .NET running in the browser and all, but this is _really_ reminding me of the problems that everyone had before the web was born.
One obstacle to adoption I can see is that you don't have the ability to support a list of values (e.g. `git add file1.txt file2.txt`). In keeping with your design philosophy, you could probably use the following style: class Options { public List&lt;int&gt; Numbers { get; set; } } - program.exe Numbers=1 Numbers=2 Numbers=7 &gt; Because running the application with no arguments at all is also considered an error (if not why would you use CCP?) Configuration files? I wouldn't expect CCP or any CLI parser to do configuration files, but it would make sense for some scenarios to have a configuration file with "standard" settings and a CLI that allows you to override specific settings on a single run without having to edit the standard config. &gt; For valid JSON, strings need to be surrounded by double quotes, which makes the syntax a bit ugly. Also note there cannot be any whitespace between the values in the JSON. Most shells (including the command prompt and powershell) accept both single and double quotes. Since JSON requires double quotes for strings, could you surround the JSON with single quotes and avoid the ugly syntax of escaping double quotes/outlawing whitespace?
Is it possible that the host stopped supporting http? In client-side Javascript, URLs like this are fine because the browser interprets this as "use whatever scheme *this page* uses." However, in a C# application, you're not running in a context where that makes sense. So it has to default to one or the other.
Probably an fx update. I'm surprised that it ever worked without a protocol.
&gt; I think I follow you, but realize that this situation is just bringing the browser runtime experience more inline with "server side"(might be a bad comparison).... What I mean is, non-browser apps.. .net and java when running on a server are runtimes that are on top of the OS layer That's precisely my point. .NET and Java are already executed on a runtime that runs on top of the OS layer. Javascript/WASM are also executed on a runtime that runs on top of the OS layer. Blazor, however, is a .NET runtime on top of ANOTHER runtime (WASM). It just feels incredibly wasteful. I am familiar with the productivity benefits provided by working in a managed language (I write C# for a living). Those benefits are not as significant when comparing C# to Javascript (and even less significant when compared to Typescript). If it were possible to compile C# straight to WASM, then it would be fantastic. But forcing your users to run Blazor just so you can write C# instead of Javascript/Typescript feels wasteful, even disrespectful of your user's computer resources.
The good news is that .NET already performs Tokenization for you, so what you put on the command line (`program.exe arg1 arg2`) becomes a string array in your Main method - *some* of the work is already done for you. The standard book for compilers (parsing is integral to compilers, so you will find a lot of overlap in concepts with CLI parsing) is [The Dragon Book](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811) but it can be difficult for people who haven't spent time learning academic CS theory. [This series](http://cogitolearning.co.uk/2013/03/writing-a-parser-in-java-introduction/) of blog posts is a bit of a more gentle introduction to parsing. It's in Java but the code is very, very similar to C# (the biggest differences are `foreach` loops, `implements` instead of `:` to implement an interface, and naming conventions for interfaces (Java doesn't add an `I`) and methods (Java `camelCases` method names instead of `PascalCase`). A lot of parsing involves creating a kind of "state machine" that allows you to loop through Tokens (items in your args array) and decide, "what do I do next?"
I wouldn't use it for my personal projects, but there's no way for me to know if I'm visiting a website that uses it, except perhaps if it starts taking up even MORE CPU than modern websites already do.
The modern web is just a delivery mechanism for applications. I don't know if that's a good or bad thing but that's how it is
Great work! I like very much the opportunity to dispatch argument basing on "verb" (the name of the IOperatio), this helps to keep code clear. I used many other parser and, if I had to find something missing, I can suggest to add the alias/shortcut (es. -v or --verbose). Do you have any performance benchmark, to prove performance against other framework?
Thanks. The shortcuts are a good idea. Also thought about it. Performance wise - I didn't do any benchmarks, but this is something you run once at startup and it's basically a few loops over a very small N, so it's not really an issue. 
Great comment. Totally agree adding support for some kind of multi value container is needed, but I think I'll start with simple arrays. Could add collections later. Not throwing an error in cases when there are no parameters could be an option. True about the single quotes with JSON. If that works than it just will. I don't do anything special here. It works with what it gets from the args array. This is also the reason whitespace is not permitted. It will get stripped in the args. An alternate approach would be to have the actual full parameter string passed, instead of the args[] and force people to read it from the System.Environmeny, which might be confusing. The args[] you always get from main(). 
WASM is very new and version 1 is just the MVP to get all the browsers makers on board. It's still missing threading, garbage collection, exception handling, DOM bindings and many other specs needed to actually handle .NET natively. [https://webassembly.org/docs/future-features/](https://webassembly.org/docs/future-features/)
Charge them upfront for the system. Then have them pay for support.
Cost is not very much
Any chance it could be interpreting this as a UNC file path on a server that is no longer shared? Asking this without even looking up the basics of WebClient, of course. 
&gt; If that works than it just will. I don't do anything special here. That's what I was thinking as well - no change to the code needed, just an update to the README to guide users. I did some testing though and Windows/PS1 parsing is really F&amp;*ed up. My results when simply looping through `args` and printing each arg to a new line: &gt; .\ConsoleApp1.exe Person='{Name:"John Doe",Age:123}' Person={Name:John Doe,Age:123} - &gt; .\ConsoleApp1.exe Person='{Name: "John Doe", Age: 123}' Person={Name: John Doe, Age: 123} - &gt; .\ConsoleApp1.exe Person='{Name: "John Doe", "Age": 123}' Person={Name: John Doe, Age: 123} - &gt; .\ConsoleApp1.exe Person='{Name: "John" "Doe", "Age": 123}' Person={Name: John Doe, Age: 123} I think the .NET side of the tokenizer/parser doesn't distinguish between single and double quotes - either can open or close a "string", and only whitespace within a pair of single/single, single/double, or double/double quotes is not treated as a new arg. So unfortunately, it seems like any JSON in the arguments will be a bit complex to add :(
I think a lot of this type of software would still be expected to sell using a perpetual license.
Why not look into something like jQuery DataTables or similar libraries to handle table pagination / sorting / filtering rather than building your table in a partial view? This would be a better alternative than building your own client side application for this.
You can find all code related to static file serving here on github: [https://github.com/aspnet/AspNetCore/tree/master/src/Middleware/StaticFiles/src](https://github.com/aspnet/AspNetCore/tree/master/src/Middleware/StaticFiles/src)
You'll find the code [here](https://github.com/aspnet/AspNetCore/blob/dc718f66027e126b3eb6c3aabb0f907b44dc7a7f/src/Middleware/StaticFiles/src/StaticFileExtensions.cs). It registers [this](https://github.com/aspnet/AspNetCore/blob/dc718f6602/src/Middleware/StaticFiles/src/StaticFileMiddleware.cs) middleware. Just keep looking around from there.
Thanks! Somehow my eyes scanned right past that.
If that‚Äôs the entirety of the requirements. It never is. 
&gt; Yes, it doesn't follow popular syntax for command line arguments Nice work, but this is a dealbreaker for me.
wow thanks
Just press f12 and see if there are *.dlls among downloaded files
XML database is eXist-db, so not exactly in ASP.NET Core. 
If you use the string array param for main, doesn't it just basically do a string.split on environment.newline? So that means your args couldn't use quotes to denote that an argument is supposed to have a space in it, right? 
about tree fiddy
I've done what are you are talking about by using jQuery Datatables for the grid and then I create a details page that is just a modal popup template with the data from needed for my details view. Then just set the Select button or row click or whatever to trigger the popup. 
Vue is particularly well suited for a gradual integration in an existing mvc project. I've done this over the past year with my team, and it is a big success. There is not a lot of guidance on this though. I actually am about to host a meet-up presenting this on Thursday, if you want I will send you a link to the samples and slides as soon as I finish writing them. Also, you can already have a look at vuetify, a free component library that has a lot to offer, like an easy to use data table component. 
I can't build the template project, it says it needs 'Microsoft.NETCore.App', version '2.0.0' , but I have 3.0.0-preview3-27503-5 
Those changes to dotnet publish -o relative pathing need me to change all my scripts üò≠
No, `Environment.Newline` is equivalent to `\r\n` on Windows, but it splits on spaces, tabs, and other whitespace. In addition you *can* quote around whitespace and it prevents splitting on that whitespace. From my tests it has a bit of a weird behavior, in that any contiguous buffer of characters or quoted whitespace will be parsed as a single argument, so the following command lines would each be parsed as the single argument/string `hello world`. hello" "world "hello world" he"llo w"orld hell'o "world"' hell'o" world"' I believe the actual Windows API used to convert to a string array is still [CommandLineToArgvW](https://docs.microsoft.com/en-us/windows/desktop/api/shellapi/nf-shellapi-commandlinetoargvw)
Ah, I gotcha, thanks. That being said, I'd still just use the main(string arg) overload and parse it myself tbh
I still let Windows do it for me since, despite its quirks, it's the standard across anything Windows does (batch, powershell, VB.NET, C#, etc.), but there are some edge cases that have me considering doing it myself. In particular, users who (correctly) quote a path to a directory that contains spaces, but leave the trailing backslash at the end (which functions as an escape for the double quote: `"c:\Program Files\Microsoft\"` &lt; `\"` escapes the quote
That would be fantastic!! Thank you for the offer! &amp;#x200B; &gt; There is not a lot of guidance on this though. No kidding! I've been scouring Reddit, Stack Overflow, and various blogs the last couple days and I haven't found any discussions or examples that are beyond the basic adding Vue to a single page and interacting with an api endpoint.
FYI... You can add your own app.Use() method before the static file handler to handle the request however you want. They just go in order, and pass on to next handler if not handled...
So you downloaded the source code, modified and now you wanna use it? There are many way to do it. The main I remember are: - add new DLLs as reference and remove old Niger - create a local nuget repository, rebuild the same nuget from command line and add it locally Hope it helps
Add the code into your solution ? Compile it. 
First you do not want someone to ‚Äúprogram‚Äù that. ‚ÄúProgramming‚Äù a new software cost a lot of money (hundred of thousands of $) There are already many programs on the market that exist to do just that. I‚Äôd suggest evaluating them and choosing the best for your need. 
It looks to me like ``` services.AddSingleton&lt;IStringLocalizer, StringLocalizer&lt;SharedResource&gt;&gt;(); ``` should instead be ``` services.AddSingleton&lt;IStringLocalizer&lt;SharedResource&gt;, StringLocalizer&lt;SharedResource&gt;&gt;(); ``` Otherwise, you are registering an `IStringLocalizer`, but trying to get an `IStringLocalizer&lt;SharedResource&gt;`.
I just had this problem and the way I resolved it was by adding another GetSection(‚ÄúAzurewhateveryounamedyourkey‚Äù) to get the next key in the Json file. I messed around with it for a while and gave up so there might be a better solution than that but it worked for me. 
I would also like to check out the slides and samples as well. Been wanting to get my feet wet with Vue and see if it‚Äôs worth integrating to current apps I support.
hi, thank you for your input, I tried this but unfortunately I get the same error... this is really bumming me out.
Thank you for responding, but after some help with a coworker, it got figured out (: 
Core with Mvc is the best and default
Sounds like Classic ASP.Net. So it will be WebForms and .Net Framework. Classic ASP.Net predates MVC and Core by many years. You can learn some things by looking at that code but it is an obsolete technology. There are still TONS of classic ASP.Net apps out there (I still maintain one I wrote 15 years ago) but it's not going to be much use to you in your career.
Net core if you can build new apps. use the Microsoft docs. But if these apps are public domain id strongly advise not getting involved until you feel you know the framework and security
 These are internal applications. I still have no intention of trying to work with the current apps. I am just trying to get an idea of how to best support the business and make their lives easier. It is a great cause.
I see. Thank you! This is very helpful. I do not really intend to get too involved with the current apps. I mainly would like to figure out how to provide support and tooling for them. In the future, it would be cool to contribute small projects. I am currently working on a web front end for a Powershell script that will Automate their onboarding process. Just want to do everything the Microsoft way so I don't rock the boat.
Absolutely! That is what I am studying for my own projects. I just wanted some insight into the infrastructure I am supporting.
I went straight to asp.net core. I never actually learned classic asp.net. Keep learning and progressing. I started core maybe Oct of 17 and have been doing it since. I'm building intensive applications and it's definitely a challenge since I'm solo. Just keep progressing 
Same! Sounds very interesting. Have a nice weekend!
Couple of suggestions. Add an IDefaultOperation and if you find one, execute it without requiring the dispatch class name. Eg. public class Application : IDefaultOperation { .... } Second - mark up aliases for properties using attributes Third - for a boolean add the ability to have it set true if present without a value. This makes it really easy to map command line switches. So you could do this: myapp.exe -v and in code it would be public class MainRun : IDefaultOperation { [Optional] [Aliases("-v", "--verbose")] public bool IsVerbose { get; set; } } This still fits with your philosophy of convention over configuration and follow similar patterns. What do you think?
You are on the right path. Dot Net core have lot of potential. As you are starting now, Microsoft docs would get you a long way. In my experience and No offense, i believe Pluralsight might throw you off becoz their tracks are out dated and not much of organized dotnet core mvc content in a consumable format for a newbie. If you are wondering,i have been member of Pluralsight sight for 4+ years. Once you finish your basics from the docs then use Pluralsight for digging deeper into concepts and how to build an application in a structured manner. Kudos to Microsoft and Community for getting the docs together so well. 
From a support perspective, I‚Äôd find what you can of the source code, any/all documentation and start reviewing it to find out exactly what‚Äôs being used and what versions. Build up your own documentation of what everything is built with, is doing, and if anything is a future risk (server upgrades etc). If something breaks you‚Äôll have a head start on googling the right versions, and specifics of what you need to research. You don‚Äôt want to waste time on learning the old stuff. As for new development, just focus on C# for starters, even if it‚Äôs replicating some simple PowerShell scripts with console apps, I did loads of these before work let me loose on the more serious web apps. Things like move files with Email/SFTP, connect to Active Directory and pull out lists of users etc. Once your comfortable (of your not already) just focus on doing all new development in .net core, Entity Framework. MVC or razor pages, Understanding Entity Framework Core first, then Web API 2, so being able to do basic a REST api to a db and back again will get the backend basic solid and allow you to do plenty before learning MVC &amp; the newer Razor Pages. Pluralsights fine, prefer sites/text guides over videos, easier to cover at work and easier to refer back to while working on somthing new.. docs.microsoft.com LearnEntityFrameworkCore.com LearnRazorPages.com Are pretty good.. That‚Äôs from my experience mostly working on internal .Net apps and learning on the job. Hope it helps! 
Open source is not the same as "redistribute at will" Check their license terms. I'm sure you'll find something on the project page
Yeah true, they always have a grand vision in their head 
Thanks! I will try the docs first then!
Sorry if this is a dumb question, but what do you want that https://github.com/commandlineparser/commandline doesn't support?
Wow! Thanks! This is extremely helpful. I will check out the resources you mentioned. I am excited to get started!
I just stumbled on to this course. Not sure if this course is good but it is free with coupon. if you need here is the link. https://www.udemy.com/c-net-for-beginners/?&amp;deal_code=UDEAFFBR219&amp;utm_source=aff-campaign&amp;utm_medium=udemyads&amp;utm_term=Content&amp;utm_content=Textlink&amp;utm_campaign=All-3Mar2019&amp;ranMID=39197&amp;ranEAID=VkwVKCHWj2A&amp;ranSiteID=VkwVKCHWj2A-wytzL2GPGDtiSyjkppLSfw&amp;LSNPUBID=VkwVKCHWj2A
Interesting. Looks worth a shot. I have had success with Udemy for other technologies.
What happens if you try to run dotnet --version? Or are you running from Visual Studio? If so, you need the preview version of VS 2019. If you are using VS Code, then you might need to configure omnisharp to use the latest beta version instead of the stable.
The CPU usage should be decent in most cases (after they can put corert on WASM), its the memory usage that would be noticeable.
Its mostly going to be for LOB applications, enterprise and heavy web applications (games, paid online services, large dashboards, etc...) where functionality, developer productivity, and correctness are more important than download size, RAM usage, and startup time. They also haven't optimized the download size or moved to the corert on WASM, two things that are going to be necessary before the use case becomes acceptable. If the startup time and download size were within a factor of two of Angular, then it could be viable. I think thats why client side Blazor is still 'experimental'. Razor components, however, have all of the benefits and none of the downsides, if you can stomache a websocket connection to each client, and don't need PWA functionality.
&gt; With these kinds of applications we are basically at the beginning before the web was born, with no standardization in terms of how content should be displayed and rendered. I don't understand what you mean. The HTML, CSS, JavaScript, and DOM interactions used in the browser in a Blazor application are all based on modern web standards. Even the WASM that runs the .NET code is an accepted web standard. Its no different than if you had run PHP on the server to provide the UI and used a web socket to tie to DOM events to rerender the UI (except they found a way to avoid the network round trip). 
If you just want the first record, use `.First()`, // if there's always at least one record and you want it to throw exception` `.FirstOrDefault() // if it's possible to return no record `.Single() // if there is only ever one record and you want it to throw exception on 0 or &gt; 1` `.SingleOrDefault() // Only one record, or 0 will be null In your case above, you would need to take the result of your query and do the above method on it. The SQL equivalent would be Take(1) and it does not exist in query syntax from what I know.
Yup, that's the solution. Run that query and then run a second one against the result with .First(). Thank you good sir!
I've never used migrations. I always did database first though. I highly recommend that route if you have db experience. Software first is great if you don't care how it's setup or don't want to touch the DB.
Some of its wrappers: [https://github.com/topics/wkhtmltopdf?l=c%23](https://github.com/topics/wkhtmltopdf?l=c%23)
Need more information. Have you tried restarting vs? Where was the database hosted? 
Use Shopify, it‚Äôs not worth it to pay someone to custom build something so basic. 
Just restarted it, same error. database us just local at this point
This project is Code First EF core, but now I need to alter an entity. I have database experience however I think I'm in to deep to to start making changes in the DB after entity changes
Do you use version control? Can you get the deleted migrations? If not I guess you could do add migration and update based on that. It should create a new db for you.
Yes it can. Dapper is a library that can be used in .NET applications, regardless of where they run. &amp;#x200B; [https://dapper-tutorial.net/](https://dapper-tutorial.net/)
You get this error when you add-migration Initial? 
In this case no, nothing was saved via version control. That second option Ive tried, which is the root of my question. The add-migration and update is running in the correct project
Yes
Have you tried a global search for the migrations namespace in your solution? 
Explain, where should I look?
Do a find in files search for Migrations 
Searched 'Migrations' in the solution and nothing was found
ü§∑‚Äç‚ôÇÔ∏è
Thanks a lot for for your response!
Well how did ya do it? I‚Äôd sure like to know for myself!
When you do add-migration does it not create the migrations folder and classes? 
The philosophy is different. The library you pointed to has 1000x more configuration options and is much more complete, but it works differently. What is does is fill an Options object for you and leaves you to do whatever you want with it. The idea behind CCP is different. It doesn't only parse the input, but also runs the created "operations" automatically. This produces much cleaner code. This is a bit similar to how controllers / actions work in ASP.NET MVC. 
i learn a lot from [this guy](https://www.youtube.com/watch?v=y0iyLwBrmRE&amp;index=2&amp;list=PLrW43fNmjaQUBZv0OiliNY4fStb4Vj1u4&amp;t=0s)
dotnet --version 3.0.100-preview3-010431 It's Fedora 29 64bit. I'm running dot net build from console. It's the only version I have, installed by unpacking tgz and changing system path.
That `[Component islands](https://github.com/aspnet/AspNetCore/issues/8214)` feature looks like an excellent solution for an MVC website looking for minor interactivity without having to migrate everything to an SPA (not everything is suited to be an SPA). Advantage of this is to do away with `jQuery` being littered on the page which is a pain to maintain. Plus points since it can do prerendering as well. Since this can be used via Razor components (hopefully even in Blazor you can choose some components to run on the server only) you can do some data sensitive operations (payment processing, complex business validation) as that component island can run completely on the server.
What have you tried? [And this should be in the FAQ](http://jonskeet.uk/csharp/debugging.html) 
There's plans for`AOT` for that (the have a prototype that's quite fast as C++ running on `WASM`), but for managed languages you'll always need for a runtime else everyone would be required to code ANSI C or C++ for everything. Also `WASM` itself has plans to help those apps that need managed runtimes to be smaller in the future since they have plans having first class support for managed languages. Right now the reason why it's a bit slow is because it's interpreting the IL instead of JITing it to `WASM` also the runtime packs a lot to support stuff that WASM doesn't support yet natively but is on their roadmap (e.g. Garbage Collection, native DOM API access through JS interop). It's the reason why it's labelled experimental for now, as `WASM` matures the runtime will get smaller and better as there will be lesser features it would need to bring to fully support managed languages.
What tools are.you using? Unity?
Just Visual Basic
Oh ok and I tried using SetBounds, which did nothing. The other thing I‚Äôm trying is using the rectangles intersectswith property, but I need to figure out how to implement it so there will be a rectangle for each bullet. That‚Äôs basically about it. I also tried doing bullet location = target location but needless to say that didn‚Äôt work.
I think you need to study a bit more what is asp.net core transmitfile will send the page as download if you want to send as a page you need to see what are views and how to render them
Visual basic is language, so it doesn't really answer the question. You should clarify which rendering platform / framework you are using, since they have different ways to track the controls . Are you using windows forms? WPF? 
Oh sorry I tend to get confused on this, but i‚Äôm using Visual Studio, not Visual Basic haha Also Im using Windows App Format
I'm not thinking about which APIs or standards you are using but the architecture itself. We are getting thicker and thicker clients/front-ends that contain more and more business logic. Our Angular/React/Vue/Blazor/whatever app is attempting to replace parts of the responsibilities of the browser and the backend at the same time. The web was made was to use a markup language to indicate properties of text/information, then have a style-sheet that expresses how this should be styled/presented to the user through a browser. Now we are effectively scrapping that whole idea and instead build an application inside the browser that should typically use a REST-API (which has no standardized way of how information should be linked, except HATEOAS which almost nobody follows) and large layers of abstractions (this is where the SPA comes in) for presentation which eventually talks to the same APIs (e.g. DOM) that traditional web-pages talk to. Yes, all the APIs are still _standardized_, but we are effectively trying to solve the same problems that the browser already solves at the cost of complexity and performance. If Flash became a web standard and a native part of the browser, those horrible Flash web-site applications that was made soon 20 years ago would be almost no different to these kinds of applications. I think it makes sense for more advanced applications like games and tools, but I fear we are going to get many websites that are an re-incarnation of these Flash applications in WASM/HTML5/Razor/whatever in the near future.
So you are using c# and UWP? 
Any news on DbGeography?
Integrating how? I must be missing something. If you just throw a div element with the right id in a cshtml page and reference your react app's bundle, you're good to go. You can still retain your layout cshtml and any other shared items. Furthermore, you can create links on the server-side by creating window-scoped javascript variables that your react will reference. &amp;#x200B; &amp;#x200B; **Here's an example.** &amp;#x200B; In the body of the cshtml, I'll throw my react dom container in: &lt;div id="root"&gt; &lt;div class="container"&gt; &lt;h3&gt;Loading...&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; Lower I create a link at the bottom of the page passed in from the view's model and add my react bundle. You can pass in anything from your model in this manner. &lt;script type="text/javascript"&gt; var buyItLink = @Html.Raw(Json.Encode(Model.BuyItLink)); &lt;/script&gt; @section PostDOMScripts{ &lt;script src="/Scripts/_react-bundles/mypage.bundle.js"&gt;&lt;/script&gt; } My PostDOMScripts section is referenced in my layout page. Next in the javascript side, I grab my variable where relevant so I can use it: const buyLinkUrl = window.buyItLink; and then to render react, it's just the normal fashion: ReactDOM.render( &lt;App /&gt;, document.getElementById('root') );
Could you make a post on here with the slides? I'm also very interested.
I‚Äôve personally found it hard to find the right approach with DDD and repository design. Take for example a simple ordering system. My aggregate roots are most likely Customer and Order. OrderItems loves as part of Order and CustomerAddress lives under customer. I want to get a list of reference data for OrderStatus (I.e. New, Processing, Complete). Which repository would that sit under. Similarly, I want to list all distinct shipping addresses for every order and the associated customers. Is that under Customer or Order?
I've tried using dapper on another copy of my project, and still end up on the same question. Also, when I use EF, I am having single complex request when I do Include, if I do a DTO, I'm going to have multiple request on the database server. If Im going to use dapper, I'll also end up with the same problem. What I plan to do now is to do a DTO, to avoid having a single complex request, and I think DTO is more flexible. &amp;#x200B;
You can't really do this. Here's a [stack overflow question](https://stackoverflow.com/questions/1041542/how-to-download-multiple-files-with-one-http-request) that's on point.
Was the nuget download of Microsoft.NETCore.App failing during/before the build? Can you make sure you can do a 'dotnet restore' in the folder successfully first?
Except in the cases of security concern, how is it any different if the templating, routing, and state management occurs on the server or on the client? For those applications they are just pushing down certain responsibilities for UX or performance / scalability reasons. You are disliking the trend of thick web applications in the browser, but its the closest thing we have to productive cross platform application development that doesn't require C++ or some toolkit language to use properly.
One thing that no one actually defines is at what scale you need "scalability". Scalability is not free. Implementing solutions takes developer resources. The infrastructure will cost something in the cloud. You or a team can't just jump into developing a highly distributed, kubernetes coordinated microservice architecture backed by some nosql store without some serious ramp up on the technologies themselves and the implications of those architectural decisions on the code base. So at what scale do you need "scalability"? I've seen some significant load go through a handful of servers behind a load balancer and a beefed up SQL Instance with a readonly node. IMHO this configuration is a fairly small step from a "typical" infrastructure. Figure out how to share you session data (which there are out of the box solutions for) and it's mainly configuration from there. I'm not debating the benefits of a distributed architecture, but the reality for most brownfield projects is that it won't happen overnight and there's got to be serious justification for going through this kind of transformation. What are the indicators that my app might be getting close to the limit of my current architecture? How do you identify what are the areas of the app that are dragging down the performance, and are therefore prime candidates to split out onto their own infrastructure? How do you define the level of effort and cost for implementing a more scalable architecture in compared to scaling vertically by provisioning bigger hardware? I'm not saying that the technical side of things is easy, but without the answers to these questions, its challenging to justify the effort needed to implement a more distributed system from the business side. And if it turns out that the app doesn't really need the scalability, then the resources spent implementing these changes might be better provisioned on developing features and functionality that will actually improve the product and experience for the customers.
Browsers will only show one html file at a time. The closest you could get to that would be to make an HTML file that has multiple iFrames where each index.html is bound and you setup your server side to serve those files at a URL that is bound to the iFrames (either with static file middleware, or with a controller method that returns the html files based on their associated id, if any).
I'm late to the party, but I figured I'd add my 2 cents. We use Vue at my job and I've done a decent amount of Vue work since I started. 1. The benefit of Vue is that you can literally just drop it in. Just include a reference to Vue, and begin creating your app 2. Vue doesn't have to be a full blown SPA. It's very easy to create small components and use them where you need them. We typically have isolated areas of the app that make either a page or a section of the app with the level of interactive that we need. We're constantly updating pieces, but we don't need to, and haven't, migrated the whole app to Vue. 3. With Vue, you can start small. You don't need a routing library. You don't need a state management library. Bring in those libraries when you absolutely need them. Vue is very powerful without them. 4. You can put Vue components in pure JS or you can "embed" them in HTML. When we start out with components, we'll typically put them in their own CSHTML file in the typical ".vue" structure of &lt;template/&gt;&lt;script/&gt;&lt;style/&gt;. This makes it easy to include them in other pages by using Html.RenderPartial. It's not much of an advantage, but you still get syntax highlighting on the HTML this way, and everything in the CSHTML file is ready to become a .vue component pretty easily. 5. If you have or are willing to setup a build process, then you can compile .vue files into javascript packages. This gives you good things like linting and compile time checking, but you really don't need that right out of the gate. This is more important as your usage of Vue matures in your application 6. Vuex, the recommended state library for Vue, isn't hard to pick up or work with, but you likely won't need it getting started. Learn Vue's reactivity model and get how to properly split and use components down first. Then bring in Vuex when you need the complex. Feel free to ask follow up questions if you have them. I've been really thrilled with using Vue and I think it makes a lot of sense for ASP.net apps.
The title of the article is poor. These are excerpts from a book, not overlooked scalability rules.
Like I said, this architecture has its usage-ages, for example in applications like Slack. And this is why I think Blazor is a good thing since we won't have to write so much JavaScript any more :) But then again, just look at how resource-insensitive Slack is. The problem is when people use this architecture for web-_sites_ primary used for displaying information (information pages, company websites, forums, wikis, etc.) which greatly benefits from having linked information that for example search crawlers benefits from. Another downside is that developers tend to forget these days that not everyone has the latest and greatest smart-phone to render their company info website through the thick layers of JavaScript frameworks. A side-effect of this is that the browsers themselves have gotten such a large attack-surface of APIs that I am really not interested in running JavaScript on your websites just to look up your company's phone number.
I stated that it is the ones I see as the most overlooked. Didn't include the entire book in my blog post :)
Did you even read the blog post (not just the title)? * Has nothing to do with the cloud. * Has nothing to do with kubernetes. * Has nothing to do with highly distributed. * Has nothing to do with NoSQL. &amp;#x200B; It about Working is about * Don‚Äôt overengineer the Solution * Design Scale into the Solution (Not implementing from the start) * Using the right tools. * Leverage Content Delivery Networks. * Database -&gt; Don‚Äôt Select Everything * Learn from Your Mistakes -&gt; Discuss and Learn from Failures
I did read the blog post. Frankly, it didn't go into much (don't select * and cache stuff, got it). The point of my response was there's all this talk about scalability, and a lot of engineers who want to do implement solutions based on the fad of the day without actually assessing whether the solution even needs it, or thinking about the trade offs. How do you assess when scalability is an actual issue for your app vs an issue on your resume? &gt;Don‚Äôt overengineer the Solution The simplest solution is likely a typical 3 tier solution; client, server, db. However, that will have scalability issues *at some point* &gt;Design Scale into the Solution It's likely that the whole solution doesn't need the same level of scalability. How do you identify the areas that need more scale, and others that don't? Also, designing scale into the solution and not overengineering the solution are diametrically opposed to each other. &gt;Leverage CDNs Great idea if the app in question can benefit from it. If it's content and resource heavy, then great, but if its a lot of processing, then this won't have much effect. &gt;Database -&gt; Don‚Äôt Select Everything This has been sound advice for decades. It's highly doubtful that this is the route of your scalability issues
 If it's not in memory and you don't have an SSD or SAS drive, then 10-20ms sounds about right, especially if you're running in debugging mode. If you're doing something in production you'd probably want to be behind a caching server anyways (I just use nginx).
&gt; designing scale into the solution and not overengineering the solution are diametrically opposed to each other. Strongly disagree. Seen far to often people putting all their logic directly into Controller classes instead of building Services or using CQRS pattern. Little planning goes a long way. Building code that scales exists in all kind of formats. But not thinking any scale into your product will leave you with a rubish product people dread working with. I too hate resume driven development and wrote that you shouldn't go of implementing Messaging straight ahead. The whole idea is building some that fits your need now, but also enables you to scale in the future. It's fine if your app doesn't need loadbalancers, but you still shouldn't put state directly on your app servers, which will give scaling problems in the future. 
Wrote this in notepad, no idea if it compiles, but you should get the general idea. namespace Web.ActionFilters { public class RequstHeaderFilter : ActionFilterAttribute { public GetRequestHeaderFilter() { Order = 1; } public override void OnActionExecuting(ActionExecutingContext filterContext) { // Get header from Request and stuff it in the viewbag filterContext.Controller.ViewBag.YourHeader = filterContext.HttpContext.Request.Headers["YourHeader"]; } public override void OnActionExecuted(ActionExecutedContext filterContext) { filterContext.HttpContext.Response.Headers.Add( "YourHeader", filterContext.HttpContext.Request.Headers["YourHeader"] ); } } } &amp;#x200B; namespace Web { [RequstHeaderFilter] public class BaseController : Controller { } } &amp;#x200B;
Your posting implies the article will contain 6 overlooked scalability rules, not that it will contain references to the topic in a book. Your title is disgenuous. Its a clickbait title (though most marketing blog posts are).
Would you rather it was just scalability rules?
&gt;Strongly disagree. Seen far to often people putting all their logic directly into Controller classes instead of building Services or using CQRS pattern. Little planning goes a long way. People putting all the logic into a controller is the very definition of not over engineering. No patterns, no indirection. Just the code doing what it does. You can complain all you want about it, but the reality is that a lot of code is written that way because it's the simplest way for many programmers to devise a solution. The ultimate point is that there's a level of engineering which is insufficient for a base solution (fat controllers and spaghetti code being prime examples), a level that is sufficient for the solution, yet not poised for scalability, and a level of engineering that keeps options open for implementing scalability down the road. Designing for scale requires some level of additional engineering, because it's not simply about how to solve the issue at hand, but how to solve it in a potentially scalable manner. &gt;Building code that scales exists in all kind of formats. But not thinking any scale into your product will leave you with a rubish product people dread working with. What about code that doesn't scale in a way that's compatible with what the application needs. Not all approaches to scalability are implemented in the same manner. Implementing an actor model and something like Akka.Net or Orleans is a VERY different way of thinking and approaching a solution than something like an Event Sourced CQRS. Without thinking about which approach is applicable and makes the most sense, it's entirely possible to go down a rabbit hole for nothing. &gt;The whole idea is building some that fits your need now, but also enables you to scale in the future. Just build what you need now. Try and do it in a way that has a reasonable separation of concerns and level of engineering involved. Then deploy it, monitor it, and make decisions based on the data that you capture. This will tell you A) what aspects of the application need to scale, and B) what levels of scale it needs to respond to. It's entirely possible that the code can be scaled just fine by doing some basic load balancing. And if you did all this engineering work in your code base to allow for things to be scalable that you turned out not to need, then you just spent resources over engineering a solution, which is the very thing you warned about. &gt;It's fine if your app doesn't need loadbalancers, but you still shouldn't put state directly on your app servers, which will give scaling problems in the future. I completely agree that you shouldn't have state in your app servers. If there's no shared state, then a key aspect of scalability becomes figuring out the which part of the code can be split out into it's own discretely deployable units. Funny how that didn't make you 6 scalability rules though. 
You can select multiple result sets in one call to the database server using dapper. Typically if the logic is really complex on building the DTO, you could implement it in a stored procedure which are easy to populate using Dapper as well.
What are you trying to do? A browser can only load one .html file at a time
Not looking for lengthy discussion here, but ‚Äúnot over engineering‚Äù and ‚Äúabsolutely no design patterns‚Äù are two very different things. Logic in controllers is bad design 101, and should never be done. Base schemes are necessary. Bad code isn‚Äôt ‚Äúnot over engineered solutions‚Äù, it‚Äôs just plain bad code. 
Please stop marketing blogs here. We‚Äôre largely not interested. 
It depends on what sort of authentication you're using. If you're using Windows authentication, a set of claims describing the user and their Windows group memberships are loaded when IIS signs the user in. If you're using e.g. a JWT, the token you are issued by the authentication provider will contain various claims that your app will extract from the token and assign to the ClaimsIdentity for the request. You can also add any custom claims you want to the user on the server side. A JWT will for example include a claim that allows you to uniquely identify the calling user, so you could e.g. query a database to retrieve additional claims to sign to the user when they make a call to your API.
Look up the game Rogue. Better starter project.
https://en.m.wikipedia.org/wiki/Rogue_(video_game)
http://jasonwatmore.com/post/2018/08/14/aspnet-core-21-jwt-authentication-tutorial-with-example-api Check this out,its pretty impressive HOW-TO for API with jwt +ASP.Core It helped me land a job and get upper hand on rest of juniors in my firm! Its pretty much a template project. I didnt write it nor do I know the dude that did it, but boi oh boi do I owe him much. Might not fit your needs but its a valuable bookmark,and pretty nice intro to JWT and its integration. 
&gt; I mainly would like to figure out how to provide support and tooling for them. There's very little difference in supporting an old ASP WebForms app vs a ASP MVC v3-5 app from a sysadmin standpoint. There's a bit of difference with ASP Core apps in that the integration with IIS has changed (or an app could use the Kestrel webserver).
I am starting to see that. I just published both a simple asp.net WebForms all and asp.net core app to IIS in my homelab. Very cool stuff. VisualStudio makes the basics pretty idiot proof.
I wrapped the file provider with IMemoryCache for live.asp.net to bring the latency times down: https://github.com/aspnet/live.asp.net/blob/dev/src/live.asp.net/Services/CachedWebRootFileProvider.cs
Nope, literally using the most useless language; Vb.NET in Visual Studio and windows app format. 
thanks for that bro
[removed]
I would have titled it: "Takeaways on Scalability Rules - Principles for scaling web sites" It wasn't a review, but the ideas aren't your own, and you are describing some of the most common scalability rules, so its not accurate to call them 'overlooked'.
Not a marketing blog. Try look at the other posts. Just wanted to share a book I love...
I created a template to solve exactly this problem. It integrates Vue as a multiple mini spa's inside razor pages. You can also start using it with existing projects. &amp;#x200B; GitHub: [ASP.Net Core Vue Template](https://github.com/danijelh/aspnetcore-vue-typescript-template) Medium: [Multi-page .NET Core with Vue.js, TypeScript, Vuex, Vue router, Bulma, Sass, Jest and Webpack 4](https://medium.com/@danijelhdev/multi-page-net-core-with-vue-js-typescript-vuex-vue-router-bulma-sass-and-webpack-4-efc7de83fea4)
You‚Äôre definitely trying to market your own blog here, hence the ‚ÄúHey, check out my blog post!‚Äù We‚Äôre not interested in your spam. If you for some ridiculous reason really feel the need to talk about scalability issues you‚Äôve seen, you can paste it into reddit. Otherwise, you‚Äôre just trying to market yourself. 
his react tutorial for consuming a jwt token was sooo good! I wish there was more real-world enterprise open source examples 
oooo ok I‚Äôll check it out, thanks!
We did this as a school project in 2nd year using Java and Swing. Should be similar with VB/C# with windows forms
Can't really help for better books specifically about .Net to be honest. I learned mostly just by programming and during my college degree, at work or at meetings. A book I would recommend and think one of the best books about programming ever written is Clean Code, all though it is writen in Java (if you know C# you'll understand, it doesn't cover that advanced Java topics). 
TBH dude pretty much chewed up whole fullstack job and spat it out so nicely for us starters. You get the whole back end start kit and how to consume in every major front end framework. Dude is the hero non of us deserve. Just pull his repos,choose your framework and follow the article. There's no way you wont pick up stuff super fast and easy. Really magnificent HOW-TO,if I dare to say,the best one I found so far. You get everything written and explained + examples that are easy to follow and understand. 
Make sure you are building Release. I get 5-6ms for small static html files with Kestrel. Are you on Linux or Windows?
Didn't know sharing my blog was a bad thing. No need to be a dick. Had plenty of good feedback on the other posts. 
windows,debugging mode
I‚Äôm not being a dick, but you‚Äôre trying to sell yourself here, and many of us are tired of this kind of content. 
Why would you need it? It's okay for user, if request is taking like 1-2 seconds. 
no it must be asap
Not trying to sell my self. Just trying to share knowledge. Have no reason to sell myself. Have my dream job and work with the tech I love every day. I am making no money on my blog. And will keep writing my blog and keep sharing it here. Had plenty of good feedback in the past and most likely in the future as well. People like you are welcome to downvote all you want. I will keep sharing :) 
Thanks for the awesome feedback :) Will take that in mind for my next post. Had no intention on making clickbait. Just wanted to share an awesome book and some points in it.
Try using AddMvcCore() instead of .AddMvc() if you don‚Äôt need Razor, CORS and some other middleware that is added with that. The ones you need can be added specifically. Static file delivery performance shouldn‚Äôt really be an issue in most applications unless you have huge traffic volume or need geo distribution. And that is preferably solved by CDN, caching proxies or maybe even as simply as going with client side cache headers.
i dont use mvc tho
Source code: [https://github.com/JoaoBaptMG/PathRenderingLab](https://github.com/JoaoBaptMG/PathRenderingLab)
Ok. Well, what numbers do you get with Release build? Because that‚Äôs how your app will run in production. Debut is for development and has no optimizations. Also try turning the logging level to error. I think it defaults to info and has some overhead.
Thanks! This seems like a really good resource. I want to use jwt for authentication and I know you can transfer user claims in the token but I don't know what to do with those claims afterwards. I'm still pretty noob in that aspect of web development. I want to build an auth api just for giving tokens as like a single sign on endpoint so the users can access all the other APIs I could offer.
Is there an simple way to tell an API controller to say if the user has a validated token and claims to access X data then authorize them? I know there was a way with attributes when it comes to role based auth. Would I need to create custom attributes?
You should mention if the solution has multiple projects. The only way you can continue to get this error is if: &amp;#x200B; 1. You have a using xxxx.Migrations 2. You have [xx.Migrations.xxx](https://xx.Migrations.xxx) to fully reference a type for a variable (bypassing the need for a using statement) 3. Your migrations are in a separate assembly and you have referenced that in the main project &amp;#x200B; You should never have to delete migrations if you understand what they are: &amp;#x200B; 1. A class that is "transposed" to sql statements i.e the migration 2. A class that is a "snapshot" so that EF can determine what has changed 3. A table called \_\_EFfMigrationsHistory that holds what migrations have been applied &amp;#x200B;
Okay, this is what I was thinking. And from your point, How much does it cost ? As the Location is Egypt .
So you think yearly subscription is useful than upfront payment
Shopify will cost me monthly, I don't want this hustle, the client needs to pay once and has his software 
Okay, I will search for open source programs, And what do you think of the cost that I will ask the client for ?
Language popularity is complex and is predicated on more than just speed.
How much exactly ? 
Most people I've worked with associate c# with windows and IIS. 
I don't get it, sorry !!
Can't work it out myself, but I wonder if some people can't be bothered with trying to decide which framework or platform to use when they see the dozens of project types offered by .net dev tools. Or it could be all those enthusiastic "anyone can write software" groups pushing training in simple scripting but never explaining the difference in compiled languages and how different run-time environments might benefit from them. 
Well you pay up front and always own that version but renew support and newer versions annually etc 
I get it, FYI I'm the developer not the client. I am trying to enter this field tho 
Speed is hardly the only factor in how "good" as stack is, or hell, even the most important one. We'd all be writing C++ if it worked that way. Yes, the ability to have both your frontend and your backend in the same language is certainly one reason people use Node over other backend stacks.
Why doesn't everyone write websites in C? It's much faster than C# and Node. C should really be the most popular language. Some reasons Node is great is that you can share code between front and back end. For example, you can create validation methods that can be used on the client and on the server. This can't be done in C#. I also feel Node has WAY more open source MVC frameworks to choose from. .NET Core doesn't really seem to have as many open source options in regards to MVC. Choosing a language is complicated stuff so everyone has their own motivation. 
I would pay someone to do this as a side job and then pay them more if you need updates. Exact amount would depend on the details of what you want exactly
The system does the following: 1- print an invoice for the purchased items 2- Database for the in store items, and the possibility to search for them 
Not enough detail 
Can you lighten me up, I am not following!! This the information I got from the client 
I've been trying to do web programming with .NET for some time now. Although I love .NET, have been using it for 13 years and have a huge body of my own code to draw upon I find web programming on .NET is like pulling teeth. Perhaps the best example is JSON serialization. This is a simple problem (I've written [a JSON parser on .NET in under 60 lines of code](https://gist.github.com/jdh30/50741cd6d094004203b1dce019726ebb)) with a simple solution (functions to convert any value of any type to and from JSON) but .NET fails at it spectacularly. Microsoft failed to ship decent JSON support with .NET even after the world had moved on from XML to mostly JSON in the wild. .NET developers standardized on a third party library instead (Newtonsoft's) but JSON has moved on so .NET developers have been lashing together ad-hoc, informally-specified and bug ridden reimplementations of JSON serialization. I tried over a dozen of these libraries this week and found serious problems with all of them. For example, not a single library was capable of structurally serializing the F# value `Map[(1, 2), 3]` to and from JSON. Specifically, most libraries try to convert dictionaries into JSON objects when dictionaries support any equatable key types but JSON objects only support strings. Finally, .NET is great for multicore number crunching but its performance sucks when it comes to typical web programming like microservices. The most common JSON library for F# is FSharp.Data. We've been using it and performance has been a problem. A quick benchmark shows that it is ~40x slower than other JSON libraries like RapidJSON. There are much faster libraries like Utf8Json but they only do half of the job and the design of .NET makes the serialization layer either slow (due to pathological allocations of temporaries) or complicated and non-portable due to run-time code generation. 
There will be a ton of additional features to implement before you can be up with a remotely useful system, therefore it is impossible to tell you how much it will cost. From what little detail you have provided, I suspect that you actually need a complete e-commerce platform. Which is a good thing, because there are off the shelf solutions available. 
oh i see, is json serialization used often in corporate webdev world ?
Oh yeah. Even outside webdev it is used for configuration files. Hence all the complaints about the JSON format not supporting comments. I've even written a low latency server for HFT using JSON over HTTP. 
Over my .NET career from .net 1.1 i found that this is a bit lazy and strange expectation that MS must provide you all facilities like all serialisation libs with best performane, bugs-free and in a first release. Joyent doesn't write all node modules, community does, same is everywhere. Only MS is expected to ship everything in one package. C# has ups and downs and performance depends on what you're doing in your code. My main struggle with web dev in it was those attempts to make web dev similar to desktop apps dev with all asp.net things. Mvc and Webapi were quite good attempts to break that weird model and move everyone to raw http and api shapes. .NET Core is doing quite well on all sides now - microservices are fast and coding in modern c# is very cool. Legacy associations with iis and windows only are still there, for sure. But you know that haters gonna hate regardless any facts...
&gt; Over my .NET career from .net 1.1 i found that this is a bit lazy and strange expectation that MS must provide you all facilities like all serialisation libs with best performane, bugs-free and in a first release I find that statement quite odd. In Javascript you can convert any value of any type into a string in JSON format simply by applying the `JSON.stringify` function to it out-of-the-box and back using `eval`. .NET developers have published hundreds of thousands of lines of code trying and failing to solve the same problem. Is it really a surprise that most developers are "lazy" and choose to use the simpler working solution? 
How is this different than what Adobe has had for years? Honestly wondering.
I think this is the biggest reason. I'm not saying C# is perfect or anything but the majority of people I talk to who aren't into it, associate it with Microsoft, IIS, expensive MSDN licenses and SQL Server. 
&gt;In Javascript you can convert any value of any type into a string in JSON format simply by applying the JSON.stringify function to it out-of-the-box and back using eval. ¬Ø\\_(„ÉÑ)_/¬Ø and .NET has a binary formatters and many other things that are available out of the box. This was not the point of message. The point was that .NET is the only ecosystem to which people apply an "ultimate argument that wins the internet" - Hey, Microsoft didn't do this for us - so this means product\framework\language sucks!". There is a marvellous Json.NET library that solved the 99,5% of JSON serialisation problems in .NET . MS hired James Newton-King (who is the author of the library) and also using this library internally and as a dependencies in many core frameworks. I think it's a much healthier way to go with ecosystem, than old MS-way to provide everything as part of a huge framework, but this is again - off the topic. JS was ahead of everyone in JSON use and adoption, thus it's has better support, .NET introduced async/away that was borrowed by many other langs including JS itself. This doesn't make one language superior or better in everything. Also there is no "DYI disaster" (and it's strange to hear this sentiment from a "left-pad" ecosystem :troll:) and those broad statements are not supported by reasonable arguments. In my network, I hear more and more positive sentiments towards Microsoft and their "developer experience" - visual studio, visual studio code, many live debugging features, working integrations and with .NET core - heavy focus on performance and cross-plat support. JS community is bigger and stronger - that's for sure, MS contributes to it too - TypeScript for example. PS: And I would not suggest to use `eval` for parsing any externally-supplied data. All forms of eval (not only in JS) are quite common guests of "Top X security bloopers of the internet".
&gt; .NET introduced async/away No it didn't. C# got it from F# which got it from OCaml which probably got it from Lisp/Scheme. Async is an ancient idea that predates .NET by decades. &gt; there is no "DYI disaster" How do you explain the existence of dozens of incompatible and buggy .NET libraries implementing JSON serialization on Nuget: FSharp.Json SpanJson.Fsharp.Formatter Utf8Json.FSharpExtensions FsPickler.Json Falanx.Proto.Codec.Json Fleece.FSharpData EdIlyin.FSharp.Elm.Core FSharpEnt.Common FsJson Giraffe.JsonTherapy Kephas.Serialization.Json Vertigo.Json FSharp.Azure.Storage FSharp.Data FSharp.SimpleJson Newtonsoft.Json.FSharp FSharpUtils.Newtonsoft.JsonValue Fable.Remoting.Json 2.2.0 Fable.SimpleJson Thoth.Json 2.5.0 Thoth.Json.Net 2.5.0 JsonFSharp Jet.JsonNet.Converters &gt; I hear more and more positive sentiments Then you're in an echo chamber. Look at the objective statistics: developers have flocked to JS. Hence the OP's question. &gt; PS: And I would not suggest to use eval for parsing any externally-supplied data. All forms of eval (not only in JS) are quite common guests of "Top X security bloopers of the internet". Agreed. 
You have the source code for this.
If you‚Äôre a sysadmin, maybe one way would to see if you can do things to make their deployment easier. I was a developer, but moved into the sysadmin side a few years ago (database specifically), and I had to publish a Nuget package for our developers to use. It was about a million times easier now than when I was actively doing development, because our dev support team has made a ton of improvements in that process. Automated code pull from SVN/git, builds through TFS, and automated deploys through Octopus Deploy. My part was even a little more complicated because we had to do some additional transformations between the compile and the publish to Nuget, but all that was automated, too. I was floored how easy it was. Basically just one click to build and deploy! If your shop doesn‚Äôt have something like that, it could be something to look into. Hopefully they are at least checking code into version control to start!
No obviously. But OP seemed like there was something new with SVG in general. This is not a "is it FOSS" topic. I'm not saying "herp derp use Adobe". It's a simple question.
If you're using JWTs, then the JWT middleware takes care of validating the token. When it comes to the authorisation on the controller, you have a few options: 1. If your controller or method has a plain [Authorize] attribute on it, any authenticated caller can invoke it. You could add code to the method to look at the calling user and check if they have the correct claims. 2. You can create a subclass of the AuthorizeAttribute, put your custom authorisation logic in there, and then decorate your controller (or specific controller methods) with your new attribute. 3. If you're using ASP.NET Core, you could use authorisation policies. In your Startup file, you can register any number of custom authorisation policies (e.g. "My policy" = user must have claim X). The [Authorize] attribute gives you the option of specifying a policy name instead of a role name e.g. [Authorize(Policy = "My policy")]. This option is not available in regular ASP.NET.
Yeah that's bugged me a little, how ordering the print book from Manning includes the ebook editions, yet the print book also comes with a voucher for the same ebooks. It just seems redundant.
Based on what I've read about mapping complex objects in dapper, it can only map 1:1 Object, may Address Class depends on three Objects. Here's the [link](https://medium.com/dapper-net/multiple-mapping-d36c637d14fa) that I've red. 
!remindme April 1st 2019
You should definitely put up more info about the project and background. I wasn't sure what this post was referencing, at first. After seeing the repo, I'm very impressed with the progress you've already made! Definitely excited to see where this goes.
For the costs you need to set an hourly rate for yourself and then estimate the amount of hours. I can't really give you a price. It depends what you want to make. Same for support. Think how much time you are likely to spend on it a month and price accordingly. Bit of advice. Write a complete spec and get them to approve it. If they want anything added or changed respec and estimate again.
I've thought about this a bit. Isn't this just the by-product of dealing with a distributor (Amazon; but Walmart does this too) that just demands a certain negative price per unit relative to the rest of the marketplace? Amazon is trying to subsidize their own delivery fleet and the extensive, and expensive, facilities that go with it. Whether or not their whole business model is even sustainable is even TBD. Worst case scenario is that Jeff Bezos becomes a modern day Napoleon; virtually exiled by his own country for the perceived greed on his part, but widely hated simply because of the greed and hubris. Time will tell. Best case scenario? I'm not sure. His model will prove somewhat profitable ... finally, but maybe everyone will decide they don't want to work that way anymore. It will be [interesting](https://en.wikipedia.org/wiki/May_you_live_in_interesting_times). 
Thanks a lot :)
No problem. I can almost guarantee they will want extra stuff/changes. So make sure you have it all documented to protect yourself.
I will of course :)
What actually you are referencing? The ‚Äúmeat‚Äù of the code is actually the primitive generation code that turns paths into primitives to be used on GPU. The SVG part is just to extract the paths from the files. Anyway, I do not plan it to ‚Äúreplace‚Äù anything; for now it‚Äôs a study project on how I could make vector art viable to video games. If you are referencing Flash, well... Apple killed it some years ago.
I am sorry. It is difficult to provide a good summary on what this thing is doing, and I ended up building on q past post. I will take this advice the next posts I do.
Are you saying NPM doesnt have the same issue? Perhaps not for Json, being that it is already built in to javascript, but for other libraries / features?
This is the path I am looking forward to. I wanted to identify what technologies were being used so I could do a POC and evaluate options. Unfortunately, I think the biggest hurdle is going to be getting buy-in from the team. I am fairly certain that I am the only one using source control.
Lol. If speed was the most important, no one would have used rails.
Well, every web stack uses JavaScript. So even the .NET folks have a vested interest in it. So the JS number is not really relevant. The real comparison is ASP.NET vs Node. .NET Core is still young and before that ASP.NET could only run on Windows servers. This made it a no go for most of the big players on the web. I doubt it even made it onto the whiteboard. A number of big players switched to node and reported creating their app in less time and it being faster/more secure than other frameworks (Ruby/rails, python/Django). That means more money. Look at Netflix. If ASP.NET Core proves to be as fast, stable, secure, and developers take an interest in it, then more big players will use it. This would drive adoption.
&gt; Are you saying NPM doesnt have the same issue? Sure. I'm not saying that. I'm saying that JSON specifically is an elephant in the room in this context. You cannot talk about the relative popularity of JS+Node vs .NET without talking about JSON support. &gt; Perhaps not for Json, being that it is already built in to javascript, but for other libraries / features? Sure but JSON is so ubiquitous these days that it is a hugely important feature. Let me turn that around: what does .NET do better than Javascript that is in as much demand as JSON? &gt; Secondly, most if not all C# .Net Json users use Newtonsoft. True but Newtonsoft only solves half of the problem. Even with Newtonsoft I still cannot serialise any value of any type. &gt; But what is the problem in other people taking a crack at making a package? Nothing wrong with it but it tells you there is something wrong in the ecosystem. &gt; You can pretty much tell the quality of competing packages via its download count anyway. Sure. I've tried 12+ more JSON libraries for .NET this week and they're all slow and buggy. I've literally spent months working just on JSON support on .NET. If I want a performant solution then I have many months work ahead of me. So I can sympathise with the huge numbers of people flocking to Javascript. 
nevermind you all have been useless, apparently i updated the virus to 4.7.3, disabled net4.5 in optional features, rebooted, then re-enabled and boom 2 minutes later the bug was gone.. i kept saying for the last 4 months its netframework but you all didn't listen.. :poop: emoji is needed here.
Thanks for the links! I'll try to give it a look later this afternoon.
[https://www.youtube.com/watch?v=ld8eAH0MW00](https://www.youtube.com/watch?v=ld8eAH0MW00)
&gt;For ASP .NET Core 3 Microsoft is dropping Newtonsoft [Json.Net](https://Json.Net) as a dependency and making their own using the new Span&lt;T&gt; &amp;#x200B;
I was seriously wondering what's different about your process. Not sure what's so anger inducing about my question. Is it faster is it better, does it solve any particular problems? There's absolutely no wrong answers here! I'm not an Adobe fan trying to get at something. Apparently I've poorly worded my question and "triggered" people.
I‚Äôm sorry if I got triggered by this. I‚Äôm just trying to fight the _‚Äúeverything I try to do is already done somewhere‚Äù_ mentality that comes to me sometimes. About your question, what exactly Flash did I do not know (at least not on its GPU pipeline), but I am applying [a known technique](https://www.microsoft.com/en-us/research/wp-content/uploads/2005/01/p1000-loop.pdf) to render the curves using the fragment shader. The idea is to preprocess the paths in order to have non-overlapping curve primitives and other triangles in between. Probably what Flash did was something other than it, with scanline rendering or the Stencil-then-Cover technique.
Hey, if we all didn't trying to do something that's was done somewhere else, we'd have absolutely no improvements, and sure as hell no Linux. Never worry about what anyone has done before. :) I see, that's pretty cool. Thanks for addressing the question! I was interested in seeing the polygon generation in your screenshot there and it seems quite logical. Something I've seen in the past is SVG generation that does end up with overlapping and becoming erratic. I'd have to manually edit it. Good work so far üëç
Because if you do anything on the web (and many do) then you have to know JS 
"they probably meant [VB.net](https://VB.net).. " &amp;#x200B; \*checks link\* &amp;#x200B; "Well damn, it's V8.net"
Okay, people are getting confused by what I mean with "actual SVG rendering". *Actual* is referring to my project itself, which then only rendered single paths and restricted to a proprietary file format (just for test). The actual is emphasizing that I finally managed to make the project to read SVG and extract paths from it.
u/antlife, I guess I found out why you were confused. Sorry for the intentional clickbait title.
i do notice the difference
1:1 only refers to the cardinality of the relationship (i.e. if one user has multiple addresses). No matter how you solve this it will either need to be 1) flattened so the user data is repeated in the result set with each address, 2) multiple queries, or 3) a stored procedure that returns multiple result sets that you then join on the client side. However one object having multiple 1:1 mappings to other objects does not require any of these approaches, only selecting multiple. That article explains how to multi-select in Dapper at the beginning.
What would the purpose of this be? When I first read the headline, I was thinking .NET was supported first-class in V8 but this is the reverse, where you can host a V8 js runtime within a .NET Core application, right? So is this if you want to create an application with scriptability exposed in JS?
I've thought of hosting chakra on .net to expose a specific piece of custom behavior. The app currently relies on an extremely esoteric pseudo language that is not documented. 
Chakra was kind of hard to google and I hadn't heard of it before. Are you referring to this? https://github.com/Microsoft/ChakraCore So you're saying V8.NET would be an alternative to Chakra. That makes sense. My gap in understanding is mostly around the purpose of including a JS runtime in the first place. If you want scriptability of your .NET application, why wouldn't you just make an interface that users could implement and then dynamically load the plugin DLLs at runtime and instantiate the classes that implement that interface? Seems a lot easier.
Yes, the link you posted is what I was referring to. The problem with users needing to implement am interface is that it's harder for the users. At the end of the day, yes, they're implementing an interface (more specifically, they're complying with a contract), but the barrier to entry for including JavaScript just seems lower. There isn't any native filesystem access or anything like that. You don't need to understand how everything goes together as well. It just seems more simple. For an end user of my system, I could give them a bunch of code samples, and the number of samples required for them to achieve their goal is theoretically lower. 
The thing about ‚Äúactual‚Äù is because just now I began to add support for SVG, because before it just rendered single paths. I accidentally created a click bait title üò∞
You can only have one version of the desktop .Net Framework installed on your system, which means they have to keep 100% backwards compatibility; .Net 4.8 still has to be able to execute .Net 2.0 applications flawlessly (even if they do terrible things like use reflection to access private members of framework classes), so breaking changes even when well justified often aren't feasible. .Net Core supports side by side and self contained deployment for backwards compatibility, so low-but-not-zero risk breaking changes like reworking private implementation details can be made much more freely.
A low barrier to entry isn't necessarily a good thing. You tend to just get a lot of badly written, insecure projects that are just doing something (badly) that already exists in the framework. Plus, people aren't learning the language any more, they're learning the new layer. We've all seen a million "how do I add two numbers in jQuery" posts, not to mention NodeJS etc. .NET isn't a hard technology to learn, and it teaches you to be a FAIRLY good programmer too by recommending the bare minimum of good programming practices.
I only partially agree with this. While I understand, empathize with, and would like to sell leaders on this argument, the reality is that in a lot of organizations, that last part is hard. I can build a small portion of my system to run JavaScript and still make it reasonably secure, particularly based on the need. For example: don't allow sensitive information to enter whatever your engine is; allow a simple language that gets transpiled either at runtime or when users save changes. At the end of the day, any user extensibility is going to create a security risk. You're trusting someone else's code. That carries risk. I can hijack any interface and use it to execute malicious code if you're going to call whatever delegate, function pointer, or interface method I implement. 
Uh, ok I can address most of what you are saying with this fact: Newtonsoft json library is ubiquitous in .Net and satisfies most people's entire Json needs. What on earth are you trying to do that would cause you to type: &gt; Even with Newtonsoft I still cannot serialise any value of any type. With &gt; Newtonsoft I'm operating at the level of lexical JSON tokens. I need to &gt; add an entire reflection layer to that to convert between .NET data and &gt; JSON tokens. I mean this library is in hundreds of thousands of production web services...
I think .NET Standard libraries that you want to test against both Core and Framework are probably an exception, but I agree with this for straight Core projects.
Interesting and worth a read, but I disagree with the premise of the author. I don't want to learn dozens or hundreds of command line interfaces and libraries when Cake has everything wrapped up and documented in one place, and can output sane build output in the same format across all. I get the authors point, but I don't want to develop and support my build system...I'd rather develop and build my application.
&gt; I found packages folder for nuget in program files, but these folders lack csharp files. Nuget packages have compiled DLL assemblies - not source files. The source code would either not be available or would be hosted on an SCM server like Github. The point of packages is to use recognized (and signed, if possible) builds with clearly defined behavior. It's not to hold a bunch of code files to be built, modified, and added directly to the solution. &gt; How to add package if i have folder with csharp files of middleware If you have a bunch of `.cs` files, compile them into a library and reference it. If you're wanting to make your own package, the easiest way is to once again compile it, but add a Nuget build step with all the metadata. You can do that in Visual Studio, and then you can host a private Nuget feed either with a server or on a shared folder. Check the package licensing to see what the rules are around distribution and make sure to version bump. If your changes add significant value, consider submitting them to the original package project and make the effort to get them included for others. I'd recommend against just pushing your changes to the public Nuget server with a new name and version. It adds a lot of confusion - especially if you're not willing to maintain a package for others. 
I just use Visual Studio or Azure Devops. 
You mean, you add "dotnet build" right in your build definitions? What of you want to build locally?
No, I right click on the project in Solution Explorer and click build.
Have you ever had to manage version numbers? Upload build artifacts? I think you are being a little naive here. Surely there is *something* that your IDE can't do.
I don‚Äôt know. I‚Äôm old school. Been coding for almost 20 years. I just have always used VS to build my projects. Never really used anything else until we started to automate deployments with Azure Devops.
If I had to guess, I'd say your complaint is with \`\`\`msbuild.exe\`\`\`. It wasn't designed to be invoked by users from the terminal. &amp;#x200B; This is a good reason to use NUKE/Cake's built-in way of invoking \`\`\`msbuild.exe\`\`\` for traditional projects. They both have support for finding \`\`\`msbuild.exe\`\`\` and invoking it with a friendly C# API. But when all things considered, this single thing alone isn't enough to take on the burden of Cake.
Ah, well, then this post isn't for you. :)
Apparently not.
Wait, someone just pointed out that I posted this on my cake day!
Author of nuke here. I agree that having a wrapper can sometimes be very helpful. So instead of invoking signtool manually with dozens of single letter parameters, for instance, it‚Äôs easier to use a wrapper API with actual human readable names. Nuke takes this to another level and generates the wrapper APIs, which makes them consistent, powerful, and most of all, easy to update for new features. Parts if that are described here: https://nuke.build/docs/authoring-builds/cli-tools.html
&gt; What on earth are you trying to do Serialize any value of any type to JSON with a single function (like `JSON.stringify`) and back again with another function to recover the original data (like `JSON.parse`). Very simple. Very useful. &gt; I mean this package is in hundreds of thousands of production web services, happily serialzing / deserializing json all day every day, &gt; Let me add that I'm a .Net guy first... Why? Why are you not an assembly guy? I mean, there's a lot of assembly out there happily running... You use .NET because it is productive, right? You don't have to worry about register allocation and calling conventions. You don't even have to remember to call `free` because you have a garbage collector. You have high-level user friendly libraries. Wonderful. Let's make a dictionary, add a key-value binding, serialize it to JSON and back and look that key up in Javascript: var dict = {}; dict[[1,2]]=3; var dict2=JSON.parse(JSON.stringify(dict)); dict2[[1,2]]; Piece of cake. Now do the same in any .NET language. Not so easy. Although `Dictionary&lt;Pair&lt;int, int&gt;, int&gt;` is a simple type it crashes every serialization library for .NET that I've tried including Newtonsoft's: let stringify value = use writer = new System.IO.StringWriter() Newtonsoft.Json.JsonSerializer().Serialize(writer, value) writer.ToString() let parse&lt;'a&gt; str = use reader = new System.IO.StringReader(str) use reader = new Newtonsoft.Json.JsonTextReader(reader) Newtonsoft.Json.JsonSerializer().Deserialize&lt;'a&gt;(reader) Map[(1,2),3] |&gt; stringify |&gt; parse&lt;Map&lt;(int * int),int&gt;&gt; I get: Could not convert string '(1, 2)' to dictionary key type 'System.Tuple`2[System.Int32,System.Int32]'. Look at [Jet.com](http://jet.com). A big .NET shop with over 2,000 employees. Lots of web dev on .NET. Despite the pre-existence of Newtonsoft's library they chose to write several JSON libraries of their own including this their [Vertigo.Json](https://github.com/jet/Vertigo.Json) library that is designed to mimic Javascript's `JSON.stringify` and `JSON.parse` but when I tried it on the above example it also crashed. I've tried these libraries from Nuget so far: FSharp.Json SpanJson.Fsharp.Formatter Utf8Json.FSharpExtensions FsPickler.Json Falanx.Proto.Codec.Json Fleece.FSharpData EdIlyin.FSharp.Elm.Core FSharpEnt.Common FsJson Giraffe.JsonTherapy Kephas.Serialization.Json Vertigo.Json FSharp.Azure.Storage FSharp.Data FSharp.SimpleJson Newtonsoft.Json.FSharp FSharpUtils.Newtonsoft.JsonValue Fable.Remoting.Json 2.2.0 Fable.SimpleJson Thoth.Json 2.5.0 Thoth.Json.Net 2.5.0 JsonFSharp Jet.JsonNet.Converters For various reasons, they all fail on this simple example. An example that Javascript did with just 4 lines of code. 
I can‚Äôt see myself needing this. The advertisement does very little to explain why I would use this other than it supports ‚Äútargets‚Äù and that it can replace Cake. I don‚Äôt use Cake and from your example code ‚Äútargets‚Äù means it can invoke the CLI... but I still have to know the shell command to invoke. It‚Äôs not strongly typed and totally wraps the normal compiler invocation. Can you tell me why I would want this over something a Roslyn analyzer plus simple invoking shell script?
Same as him, there‚Äôs really nothing i can‚Äôt do straight from VS UI when developping, i guess it‚Äôs different for people using VS code but as a real VS user for 15+ years i haven‚Äôt had to do anything dev related with CLI, aside from the EF commands and that feels like a step back from design first EF from 10 years ago
[This](https://www.youtube.com/watch?v=HZFTUtbn1RU&amp;t=1m19s) seems relavant.
I wish the author of the article had focused on those points instead of just trying to smear cake. The premise was 'Cake is painful, you can use these instead', the premise should have been 'Cake is full featured and has had its place, but check out what these tools can do'. I love me some loquatious interfaces where the parameters and method names make sense. There are so many tools out there that have terrible CLI parameter conventions. Thanks for the link, that sounds worth reading up on.
&gt; I can‚Äôt see myself needing this. The advertisement does very little to explain why I would use this other than it supports ‚Äútargets‚Äù and that it can replace Cake. Needing what? Bullseye? You are missing the point of the article. My point is that Cake comes with *other* baggage that you shouldn't burden yourself with. Cake's "targets" just isn't worth it. There are alternatives without the baggage. &gt; but I still have to know the shell command to invoke. It‚Äôs not strongly typed and totally wraps the normal compiler invocation What about the points I addressed in my post, about how the type-safe wrappers are error prone and not in feature parity *with the CLI it's invoking*? What do you think about my ```npm``` example? Maybe there are people who just *hate* the CLI? In that case, there is NUKE. It wraps up many commands as well, like Cake, but without the baggage, full IDE integration, debugger, etc.
I just use the pre/post build steps in the solution files. Or a powershell script if it gets really bad. Didn't even know cake was a thing.
&gt; I wish the author of the article had focused on those points instead of just trying to smear cake. I get why it came off that way. My main goal wasn't to promote a single technology, but to just persuade people to use anything that is just a simple console project, which includes NUKE/Bullseye/etc. I guess, deep down, I'm trying to break this hypnosis I see in the .NET community with it's beloved Cake. I feel like I'm in a twilight zone with the number of people still absolutely swearing by it. Cake just isn't worth it these days, I believe objectively.
Ok you've found an edge case. There are multiple ways round that rather than rely on reflection - serializing the keys &amp; values lists lists for one surely?
I‚Äôm trying to let you know that your post does make clear certain key things that should be the lede of any introductory technical copy: What, Why, How. To demonstrate I will answer these questions based on everything I‚Äôve read (blog, and this comment section). &gt;1. What is Nuke? &gt; &gt;Nuke is a invocation engine for CLI commands during build. It fully wraps the entire build script including ‚Äòdotnet build‚Äô. It is run as an exe. &gt; &gt;2. Why use Nuke? &gt; &gt;You don‚Äôt want to write highly complicated conditional scripts in a shell language you don‚Äôt know. Nuke let‚Äôs you invoke commands within a larger C# program so you can stop worrying about how to write a for loop (or something much worse) in bash, powershell, or whatever else you currently use. &gt; &gt;Nuke is not a think abstraction though. Keep the CLI invocation simplicity with all the power of easy procedural programming around it. &gt; &gt; 3. How do I use Nuke? &gt; &gt;I don‚Äôt actually know you go to some repo on GitHub. I‚Äôm not sure I‚Äôm write but hopefully you can understand that your current ad doesn‚Äôt answer these questions well and make it better for potential users. This project isn‚Äôt one I feel like I need or want but I hope you can find your audience. Cheers 
I‚Äôve never heard of it, used it either. Now psake... *grumbles in inscrutable powershell*
I mean, what's the point really? I've never had the need to use cake or make for any reason to build .net applications, even in a CI system. So, why would I use cake? Honest question.
No, seriously Visual Studio is **built to build for .NET**
why are you using the word _ad_?
Because you are advertising a solution. It‚Äôs not a shameful thing, it‚Äôs the point of the post. Would you be more comfortable if I called it a blog or post? Also I‚Äôm on mobile so ad is faster to type ü§∑‚Äç‚ôÄÔ∏è
This perspective amazes me.
Lol, what perspective is that?
No I just wanted to make clear, that Paul is not affiliated with any of the other proposed solutions. 
Right. I assumed it was an ad for Nuke and he was affiliated.
Build automation encompasses a bit more than just building. Like pushing nugets, generating documentation, calculating version numbers, creating github releases, etc pp. Go and check some bigger opensource projects on github, like roslyn, avalonia, nodatime. This is really nothing you do from visual studio.
A coworker and I had a conversation about this recently. Visual Studio is great, but it is very intimidating to developers who are on a different stack. VS Code is just a way for Microsoft to branch out to coders. VS Code and the dotnet CLI is easier for a Node.js or python developer to get into. VS Code users are the beta testers for us coders who use Visual Studio. I am working on my first .net core app and the tooling for it / GIT is SO GOOD. Working with.net in Visual Studio is simple and awesome. I have no idea why the kids think the command line is cool. It like a huge step backwards to me. VS Code is like what Microsoft uses to try out new features for Visual Studio.
This is not about different stacks, nor being cool by using the command line (btw nuke has integration with VS), nor beta testing anything. You seem to have a different idea about what build automation means. As mentioned in another reply here, have a look at some GH repositories, like Roslyn, BenchmarkDotNet, NodaTime, etc... 
Okay.
I'm not addressing that. I'm addressing op in this thread up a few from my response, literally confused how you can build a dotnet core app in Visual Studio. Not build automation. I use Appveyor
The thing with appveyor (and any other CI system) is that you can‚Äôt execute your build locally. For instance to troubleshoot an issue with test execution, or the service is down, and you want to publish from local machine. Also you‚Äôre experiencing a lock-in. You can‚Äôt just move to another CI system. You have long waiting queues. You can‚Äôt easily execute a single part of your build and repeat it (lets say from build-analysis-package-test you want to debug the test target). On a general level, these are drawbacks. Not saying it applies to you.
So you'd rather type "Then" and "End" in an If statement than a single curly brace?
But for all projects that use solely VS, build automation literally IS just hitting the F6 key, at least that‚Äôs the norm, like everything there are exceptions but they are rare, you could go a whole career and never touch any CLI tools to build your apps from the day .net was released untill today working as a .net dev. That includes somewhat large solutions with dozens of projects in them, still just a F6 away
FWIW, we introduced our devops team to automated build and deployment using Psake. It's not as great as NUKE, which was my first choice. But we're restricted by other technical debt due to which we cannot use a more modern build system yet. It still beats having to make a build through the VS interface on production servers.
That's interesting. What's a better alternative in your opinion?
Because you want to over engineer a system so that you can appear busy to your manager when it fails or has hiccups.
It's gaining traction albeit slowly ever since `.Net Core` came to be, but .NET is not really known to be that "hip" platform when you compare them to say Go, Node, etc... You can compare C# to Java, it's used quite often on a lot of business critical apps though you don't hear that much about them. Most people I know usually stereotype it as "enterprisy, windows, IIS, licences, etc..." which is slowly changing but old stereotypes typically die slowly. Then there's the argument for Full stack as if you're gonna want to deal with the web you'll need to know JS regardless of your back-end language which means more training time for the dev to learn two languages than say one full stack JS (though imo I'd prefer C# every time when there's a chance). Hopefully `webassembly` changes all that and allows other languages to compete full stack and end JS' monopoly on the web.
So I'm kind of new the world of build automation and I checked out what we use at my work. We use 'Nant' by the looks of it. I Googled it and seems old, really old. Anyway we have lots of .build filed and .includes files for lots of various build configurations. Is something like Nuke able to replace this kind of set up??
...Cake?
My visual studio does not do this. My build/release definitions in the cloud do these things. I think u/thilehoffer means the same thing. I do not know anything that VS build can't do that an extension can do
If I have it, can I eat it too?
To those liking my posts, liking my facebok page (it's embedded in my blog) will really help me out and give me some confidence for the future :) 
Out of interest, as a Cake and Azure DevOps user, can we replace our Cake build with what you‚Äôre describing because it would certainly be interesting for us. We have some more complicated build scenarios which is why we are using Cake at the moment rather than a big standard dotnet build. 
Haven't tried it, but this seems like wht you're looking for https://github.com/Zeugma440/atldotnet
Looks really promising, thank you 
Version control for your build scripts is the killer feature for me honestly. 
I see. Whats webassembly tho, is it like js react?
lol, this is me, and my "version controlled build scripts" are the history and tagging of these solution files with build bits in source control.
It's a binary format that can be used as a compiler target. Basically you can take any language, and if it has a webassembly toolchain, you can run it in your browser and access common browser APIs. For C#/.NET that toolchain would be Mono and (maybe in the future) CoreRT
[removed]
Great article! Very useful
honestly i see *way* more people ranting about how everyone is using it wrong than i ever see people actually running into socket exhaustion problems and in my experience all it does is make people scared to use it when it's really not that scary. just don't keep creating instances of HttpClient, keep one and reuse it
The problem with that is the DNS records will never be updated, so this could be problematic if your software is designed to keep running for hours or days at a time. The route I've taken for applications with a long session lifetime is to wrap HttpClient with "CustomHttpClient", with its own logic to regularly recycle instances.
Try this one: [https://github.com/JeevanJames/Id3](https://github.com/JeevanJames/Id3)
yes exactly, and that's why Microsoft created the HttpClientFactory, so that you don't need to care about that anymore, they do it for you :)
That doesn‚Äôt address the stale DNS issue though.
you do you, if you don't care about the DNS issues that could occur, keep doing what you are doing :)
Right, it's contextual. If I'm writing some tests, that are going to e.g. run 100 tests over 5 seconds and then exit, then I am perfectly happy to make and discard 100 HttpClient instances. It works, and it's not going to stop working any time soon. However, when wiring that API under test that has to sustain 1000s of requests per minute, then things are different.
I believe TagLib is the standard: https://github.com/mono/taglib-sharp
Or if you really want to annoy your team, try fake. F# is scary apparently.
You're a developer with 20 years experience and you seem to hate automation. That's amazing.
I like to be able to run what's running on the build server locally. Complicated builds especially.
Wait, so you literally just hit F6 then what? Drag and drop the binaries somewhere? Manually upload a nuget package somehow? How do you avoid having a build system on any team of more than one person?
make, rake, fake, cake, psake, etc are all just variations on a theme.
If it works for your team don't touch it you'll just end up owning it and every build issue will be your fault. If you want to ignore this advice then yes, there are much better tools available today. Nuke is one of those. Personally I use fake but that's only because I wanted to learn a bit of F#.
We were talking about building, your questions here are about deployment, in most case that too is only 1 click away in the ide (right click project =&gt; publish) for most web/console/gui apps
You‚Äôve got it backward, it‚Äôs not hating automation, it‚Äôs not having to automate something that your IDE does by itself to begin with.
Is it dragon based? Or science based? . . . maybe both?
I don't think many people use cake for just build. That would be silly. I just hit F5 and go as well, when you're doing something like building an API, a frontend, an API client nuget package and you want to do all that on your build machines and locally in the same way then you use something like cake. I'm not arguing that it's useless for single project inner loop.
You don‚Äôt understand. If the IDE already does it for you, what is there to automate? 
Build scripts? What are they for? Never had a build script in my life. Also that isn‚Äôt a feature? Any file change in the repo will appear as a change in Visual Studio. To get version control on a file all you have to do is save it in the repo folder.
Unless you‚Äôre still using the full framework.
I mean, yeah, there are a lot of insurance where someone could say, "don't worry, you'll never run into that" but programming is deterministic, and knowing as much as you can about what you are using will never hurt you. And then when you work at a company with any amount of scale, you aren't shooting yourself in the foot. I spent about a month at one point going through a lot of the code another team wrote at my company that my team was taking over because they were creating an http client for every request in any service that called another. Some of which were receiving roughly 1000reqs/sec. They tried to be smart about it but that just made it harder to fix cause it was weirdly wrapped in two layers of factory methods in every service. They were regularly battling socket exceptions by doing an app pool recycle after an exception threshold was hit in the logs because they couldn't figure out what the problem was. I think people assume these kinds of blogs at just for people starting out, they aren't always. Even engineers at a fortune 200 company fuck things up regularly. But maybe I'm just bitter about working with a bunch of lazy devs.
You are spot on.
&gt; Ok you've found an edge case. There are infinitely many types for which this library (and all other JSON serialization libraries for .NET) fail. &gt; There are multiple ways round that rather than rely on reflection - serializing the keys &amp; values lists lists for one surely? Fantastic idea. Also crashes: Map[[|1;2|],3] |&gt; stringify |&gt; parse&lt;Map&lt;(int []),int&gt;&gt; with: Could not convert string 'Int32[] Array' to dictionary key type 'System.Int32[]' Pick a type. Any type. Will it work? Who knows. Do you want to rely on this for your production code? Not really. &gt; I've got to say using a tuple as a dictionary key is not really a very .Net way of doing things... What types of keys in a *generic* `Dictionary` do you think are ".NET ways of doing things"? 
I‚Äôm doing all these things from the IDE too (except creating nuget packages, never had the need to make my own). I‚Äôve never had the need for CLI tools to run / debug / publish and release large multi project solutions, including solutions with a mix of console/web/API/WPF etc.
"full" framework is such an odd term, considering it's basically deprecated at this point and it will be getting fewer new features compared with its successor.
All you need to know is probably here: https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/implement-resilient-applications/use-httpclientfactory-to-implement-resilient-http-requests
I don't think we're talking about the same thing unless visual studio recently got some feature I wasn't aware of.
Welcome to the party! I appreciate all the info! My other team member and I have both really enjoyed the tinkering we've done with Vue. We're mostly exploring what's the best way to bring it into an existing project and it sounds like you are doing what we were thinking. &amp;#x200B; &gt; If you have or are willing to setup a build process, then you can compile .vue files into javascript packages. This gives you good things like linting and compile time checking, but you really don't need that right out of the gate. This is more important as your usage of Vue matures in your application This specifically is something I'd like to explore more. Are you creating these Vue "mini SPAs" using the Vue CLI to create these projects or building them from scratch yourself?
Hey! I'm back after reading your article and poking around your repo. This looks awesome! I may explore starting our next project with this template. I'll need to learn a little bit more about Webpack just so I can support and troubleshoot any issue I could potentially run into. Luckily it looks like Colt Steel recently released a [10 video series on YouTube](https://youtu.be/3On5Z0gjf4U). Thanks again for the links and putting the effort into creating a template and sharing it!
So this is one of those things you don't realize you need until you need them. Before we migrated our builds to use Cake we set up Bamboo (our build system) configurations for each project. So one config for one project for example could include building the project, running the unit tests, collecting test coverage data, creating a NuGet package etc. The problem with this approach is that now the Bamboo configuration defines how and what is done when we do a build, but the configuration itself is not a part of our repository. This means that when we, let's say, change our test framework from NUnit to XUnit, we'll have to change the task in Bamboo to run the XUnit runner instead of the NUnit runner. This in turn means that we can't build a previous version of the code, which still used NUnit without again making a change in the Bamboo build configuration. Cake solves this problem by allowing us to specify, in code, checked in to the repository, what happens when we build. 
If you're targeting .NET
What about WebRequest.Create()? Does it suffer from the same issues?
First of all kudos for trying to build something yourself in an effort to learn, it's a great way to get your hands dirty (so to speak) and is a testament to your mindset. I suggest you have a look at MS Orleans, the back-bone of popular franchises like Halo and Gears of War: [https://dotnet.github.io/orleans/Documentation/index.html](https://dotnet.github.io/orleans/Documentation/index.html) It's a great read and there's a lot to learn from its underlying architecture (virtual actors, etc... ) Have fun!
When you say "throttle" do you actually mean Lock here? Throttling to me usually implies that more than one is getting through at a time, but the example makes it look more like a traditional lock. Also does any solution need to be potentially distributed across different hosts?
If you're writing software for a distributed microservice/cluster architecture it could be a real issue. 
\&gt; When you say "throttle" do you actually mean Lock here? \`\`\`TriggerEvent\`\`\` should be none blocking. awaiters of \`\`\`AwaitEvent\`\`\` will be doing whatever the "event" is, when released. &amp;#x200B; \&gt; Also does any solution need to be potentially distributed across different hosts? In memory, single host.
Best type of writeup: simple with lots of code.
Socket exhaustion options aren't necessarily the problem that this can cause. I was working with an API library for an online service, and when I was really hitting it hard periodically my program would just hang for no apparent reason. after a lot of debugging I eventually sorted out that it was deadlocks stemming from the HTTPClient. I fixed the issue by rewriting the entire library to be async, and to only rely on a single httpclient.
&gt; in my experience all it does is make people scared to use it when it's really not that scary. Instructions unclear, reimplemented HTTP using System.Net.Sockets.
I would argue for a `[Default]` class level Attribute over an `IDefaultOperation` interface. Being a default is not part of what a class _is_, but rather how a program uses it. I would also add `Type DefaultOperation` as a property to `CCP.Options` to override the `[Default]` attribute.
And the question for most projects is likely to be ‚Äúhow much do I care if I‚Äôm looking at perhaps 1000 requests per day?‚Äù
I‚Äôd like to see these blog posts as ‚ÄúMicrosoft did a bad job when creating HttpClient, here‚Äôs how you should handle their mistake‚Äù HttpClient implements IDisposable, it‚Äôs hard to blame individual developers, therefore, for wrapping their code in a using block and never thinking beyond that. HttpClient is not disposable and should never have been created to implement IDisposable.
If you‚Äôre using Core, sure
It depends on if: 1) that will ever change (the scalability concern) 2) that code will be used as a template for other code (the best practice / habit concern). 
I'm just sitting here using RestSharp...
Thank you for that, I will take a look at it. :)
Are you trying to concatenate fragments of pages together to form a full page? I recommend you look into Views. They are made for constructing full pages from fragments dynamically. It's probably what you want. To answer the specific question, you don't want to use TransmitFile because IIRC that tells ASP.NET you want the user to download the file, and you can only do this once per request. Instead you just want to send the data and control the headers of the response yourself. This is easy enough to do by opening a FileStream to the file and piping it to the response body stream.
What makes your approach better than [Dynamic Data](https://docs.microsoft.com/en-us/aspnet/web-forms/videos/aspnet-dynamic-data/)
Haha, sorry i'm not creating a game, just an MMORPG Emulator
Or https://www.llblgen.com. Or any other codegen framework?
Are you using pure .NET or are you using something like Unity? It doesn't really make sense to use pure .NET unless you're committed to making your own game engine. Even large game development studios rarely go that route now. Traditionally basic collision works by checking to see if two bounding boxes intersect, as you said. It sounds like you are using WinForms with controls. This will not work will if you try to scale it up for a "bullet hell". I recommend you add 1000 controls and then try to do something like resize the window. It will be very slow. Winforms are not made for gaming like that. You will probably want to manage your own internal lists of rectangle boundaries and just draw everything in the form's OnPaint override function. You should learn how to use System.Drawing.Graphics for this purpose if you don't already know how to. You also seem to be using VB.NET, which I find it is a lot easier to make mistakes in due to all the legacy design decisions. Keep in mind .NET was pretty much made side-by-side with C# (AFAIK?) so it is a better match. But if you have already started coding or if the project is VB.NET specific obviously you should keep going with that. And of course you shouldn't add additional tasks onto your plate like learning a new language right now, but it's something to keep in mind for the future. Now to get to your actual question, Bounds is definitely a property on a Control, you can see the documentation here: https://docs.microsoft.com/en-us/dotnet/api/system.windows.forms.control.bounds?view=netframework-4.7.2 What control type are you using? If there is no bounds property perhaps it's not a Control. Try adding a Button to your form to test and it should have a Bounds property. SetBounds should change the position and size of the control on the form. I think you have the general idea of how collision works, the tricky part will be building a system that can keep up with hundreds of bullets and run fast. Good luck!
nuke :)
The way I look at it is the underlying sockets are an unmanaged resource that needs to be disposed of, but that disposal doesn't need to happen as quickly as you would intuit. 
I see you've been here a while :-D old meme is old. Thanks for the laugh
Unless you have a reason not to just use the factory. Would be my advice.
Same thing here! Put into a project, and to the dev team the locks were normal. After looking into it the entire app was not using asynchronous properly. 
I'll also throw my business out there as well https://www.codenesium.com Your big competitor will be air table. If you want to talk about this some more just DM me your email or something. 
Well, there's only one way to find out. First, list the visual studio features that you know of. Then I'll tell you if it recently got anything that's not on your list.
Which uses a new instance of a HttpClient every time, unless you inject a static one.
HttpClient SHOULD be disposed before dropping out of scope. Just that it's scope should not be 'one usage'.
Absolutely!
&gt; and it's not going to stop working anytime soon Eh, it could be sooner than you think. All that has to happen is for people to add tests over time, eventually you'll run out of resources, and it won't be at all obvious why,. Someone will then spend a lot of time figuring out what's going on, and even more time fixing it because it's now duplicated more times.
I suggest you to use [Html To PDF](https://www.officecomponent.com/products/html-to-pdf) library. It is easy to use and has built in methods and implementation so you can easily use this to generate PDF from template. It is fully supported in .Net Core MVC.
It's because it's easy to blog about.
So if a person uses this one class, and this one method for all service calls, is this the proper way? I'm a tad confused here. 
A use case that doesn't fit IDisposable, which is intended to be a one-use interface used with the `using` block Take the interface away and put a big warning on the documentation page saying "You must dispose of this properly!", or just don't expose HttpClient to the user at all and only make it available by a factory method... whatever you like, as long as it's intuitive. I'm okay with the framework either doing all the legwork (ideal) or passing responsibility to the developer... I'm not okay with .NET half passing responsibility but designing the class in such a way that obfuscates that responsibility. The problem isn't the "You have to do something to make this safe" - that's part of being a developer - the problem is that it exposes a problem to the developer in a way that makes it seems like there isn't one.
But should it even work if I see .net core 2.0 set in every configuration file, and building tells me explicitly it needs .net core 2.0 and I have .net core 3.0? I did successfully "dotnet restore" and haven't had any fails during downloading. Full log is here (sorry for few polish messages but I don't know how to change it): https://gist.github.com/ptrwis/21cc9c20b09bede22c88a8cd8654845f I don't have those ".NET Core prerequisites".
No, you shouldn't new up the HttpClient for every call. If you can use HttpClientFactory you can inject the HttpClient, if not, you can try something like version two, new up the HttpClient in the constructor and then store it as a field (if singleton), else store it as a static field. Just beware of the DNS issues that could ocur.
What the hell is that pricing model? I can't see myself buying something with that kind of model, I wouldn't even know where to start pricing up my project
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/latexandloaf] [You're (probably still) using HttpClient wrong and it is destabilizing your software](https://www.reddit.com/r/LatexAndLoaf/comments/azxv4q/youre_probably_still_using_httpclient_wrong_and/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
HttpClient isn't a good example of design. But just because something implements `IDisposable` doesn't mean you must absolutely use it with a `using` block. The latter is just syntactic sugar for try/finally with short-lived objects.
I looked it trough. The main difference is that [GridQuick](https://www.gridquick.net/?rref=rdotnet) supports Angular, Xamarin, WPF, has field validation and custom field controls (MultiDropdown, Editor, SearchDropdown). It doesn't generate any code, it's a control basically like &lt;Grid Name="nameOfYourGrid" ... /&gt;. You bind it to stored procedures not tables (I think I could implement tables too though). That way you can change the grid definition or procedures and changes will happen on all platforms without altering the code.
Ok, i'm going to look into your examples further and see what I can come up with. Whats up with the cons always being returning the object as a string? 
If you have very large responses and store them in a string they could end up on the large object heap, so it's just a matter of memory optimization more or less.
So in my pic i linked, do i bascially do the same thing by taking the response and calling the "ReadAsStringAsync();? I don't think my responses will be very large, but I think this is very close to what i'm currently learning, so i'd like to hopefully get it right :D Thanks for your responses. 
Yes, when calling ReadAsStringAsync you are storing the whole response content in a string. It isn't always bad, but it's good to know that it could potentially end up on the LOH if you have large responses so I would recommend you to have a look at streams :)
&gt;Or https://www.llblgen.com. This looks similar but it generates ORM code it's not just a control. But thanks for pointing out somewhat similar competition :)
&gt;Your big competitor will be air table. You mean this? [https://airtable.com/product](https://airtable.com/product) That looks like a team management framework which is not very similar. Anyway nice business of yours. I'll drop you a message and we can talk.
With my specific case, My http request is in my UI project. I basically make a call to a service class, that calls the class containing the http method and then that calls my api. I don't think i can use Ihttpclientfactory in this setup. 
I would use Rx to manage the stream of events. You can throttle, buffer, etc. Just use "Observable.Synchronize()" to synchronize across threads if needed.
LOL
Just trying to understand the code, what happened to HttpClientFactory from version 5 onwards? Does service.SetHttpClient do the same thing?
In the same line of thought as Orleans there is also [Akka.NET](https://getakka.net/), not the easiest thing to learn, but the documentation and extensions are top notch.
I literally did this when I had to get my .NET Core 1.x app running on RHEL 6 which was not officially supported. I had to manually dump in newer versions of missing or outdated .so files into the app folder. And even then I couldn't get anything that used CURL to work (such as HttpClient) so I ended up reimplementing the bits I needed. Glad to see official support so I don't have to do it again. Amusingly the .NET Core 2 guide for RHEL 6 support roughly matches what I figured out myself, for the most part.
Which is bad design
With IDisposable you should allow the _owner_ to clean it up. Did you create the HttpClient? You are its owner so you should call dispose, maybe with a `using` block. Did HttpClientFactory create the HttpClient? Let HttpClientFactory worry about calling dispose. HttpClient is a perfectly fine use of `IDisposable`, devs are just inefficently creating it for every request instead of having a central owner like HttpClientFactory.
I think it's LINQ code that prevents continuing. I don't know why that should be the case.
I think the discussion you both meant to be having is the need for artifacts or not. I myself work at a smalls start-up and we just use web deploy for deployments. No artifacts are saved, nor version numbers are used. We could use that, but such red tape would only lead to decreased efficiency. However, as we are rapdily growing, I'm looking into Azure devops, to do what you are describing.
I agree this would be better. Great suggestion!
What would the payoff be for using this?
well if you need to build/package/sign/upload a project over multiple repositories your ci script will be huge, so I guess writing it in plain C# is way better than using powershell, etc..
thanks for the detailed response! It addressed some helpful information for me. I am actually using Windows Form App, not XAML. The framework is 4.6.1 if that makes a difference. The controls I am using are primarily picture boxes and labels; I also tried adding a button as your suggestion and I get the same result as with every other item: the only available property for ‚ÄúBounds‚Äù is ‚ÄúSetBounds‚Äù and even after declaring a ‚ÄúSetBounds‚Äù, there is no ‚ÄúBounds‚Äù property. Frankly if this doesn‚Äôt work I‚Äôm probably just gonna simplify this and do a ‚ÄúOne Finger Death Punch‚Äù style game. 
God, that sounds positively relaxing.
Yeah, although that's changing with .NET Core particularly, SQL Server is still very expensive: most people automatically associate Express as being a cut down version, even though it's actually plenty for most non-commercial products The big problem is the perception though: even if it isn't expensive and tied to Windows, it's still more restricted than FOSS options and more tied to Windows than most other ecosystems. That, plus the general "tech follows trends" thing
Also even with .NET Core, it's far more "involved" than Node.JS: there's more configuration and learning involved. That puts hobbyists off, and a lot of these "popularity/trend" type discussions forget that there are a LOT of hobbyists I'd venture that .NET is still used in far more commercial projects
There is no Bounds property in XAML according to the documentations.
Recently made a move to an open source stack. Just seems crazy to me now that you have to either dream this up yourself or copy 100 lines of code from some clever blogger in order to do it 'right'. Why isn't this encapsulated in the framework? Isn't that the word point of a framework? 
I‚Äôm not working with XAML though :3 Windows Form App is the one I‚Äôm using. 
:)
Really? How? Since they both are converted to the same CIL, how would rewriting it in C# make it "work better"? Are the 0's &amp; 1's compiled from CIL which was converted from C# somehow ***special*** 0's &amp; 1's? 
Well you could start writing some tests. I won't start using a library that I know is untested or has a extremely low coverage. I'd recommend Unit and Integration tests for the size of your project. Also, there is no NuGet Package available which is nowadays the only real way to consume external libraries. But still, it looks nice
It could be improved with some unit tests so existing functionality is broken by later additions. 
On the code aspect I would also say that I wouldn't use partial classes. They make it difficult to get a grasp on the whole class. Your table class is also quite big. 
You can't beat LLBLGen. That piece of software has been continuously improved since the big bang happens. Wait until Frans Bouma announces his retirement. 
Awesome. Orchard Core also has a headless mode via its GraphQL support.
This library is also new https://github.com/dotnet/command-line-api/wiki 
Wait until Razor Component is completed then there is less reason for corporate world to use JavaScript libraries for most of their needs.
Yes it does! You could either inject the HttpClientFactory or use the AddHttpClient extension method. The difference is that either you inject the factory OR the HttpClient (ehich the factory has created for you under the hood)
Why use this over Azure Devops?
&gt;If you're using Core, sure ~~Core~~ .NET Standard2.0+
it may not be enough reason to stop this JS train, js users will need a solid reason to switch, and even if they will, most job offers will always be for JS for another 5 years or more
That further forces webmasters to learn JS and ignore other languages, because they need to pay bills
OTOH the only reason why I would ever use tuple as dictionary keys would be if they were coordinates containing a special value Eg. Dict&lt;(x,y), isOccupied&gt; (could've just used a hashset in this bool example)
This is the problem which can be solved by CQRS. You should apply DDD tactical patterns only for writes. For reads, you can query database as you like. You can read more about CQRS implementation on my blog: [http://www.kamilgrzybek.com/design/simple-cqrs-implementation-with-raw-sql-and-ddd/](http://www.kamilgrzybek.com/design/simple-cqrs-implementation-with-raw-sql-and-ddd/) or you can check my repo: [https://github.com/kgrzybek/sample-dotnet-core-cqrs-api](https://github.com/kgrzybek/sample-dotnet-core-cqrs-api)
How you structure the solution depends on which common Architecture do you want to follow. For example for Clean Architecture/Onion Architecture/Hexagonal Architecture you have specific layers defined. &amp;#x200B; DDD Aggregate is "a cluster of domain objects that can be treated as a single unit". More information you can find here [https://domainlanguage.com/ddd/reference/](https://domainlanguage.com/ddd/reference/) 
Funnily enough I‚Äôm using that, but DDD sits in between the commands and queries and the repositories so a command queries a repository for a domain object, you call a method on that domain object DoSomrthing() and then it updates that repository. What gets confusing in this is say for example you want to update a customer. You object a ICustmerRepository so you update a customer but you also need to grab a list of customer statuses from the database. For that you need repository access. Should the query that gets all customer statuses from the ICustomerRepsoitory because it‚Äôs the aggregate root, or do you make CustmerStatus a new repository?
Thank you for the information. I also used partial classes to help break up the size of the table source file that way the code is a bit more easier to work on. Logic that is table related stays in the table file, everything else is broken into other files. But after compilation those classes do get merged together. I appreciate your feedback!
If this doesn't predate [ASP.NET](https://ASP.NET) MVC, then whoever named it did a terrible job. &amp;#x200B; \*.cshtml is already the razor template view file extension.
I would agree, on the grounds that 1) Practice. It helps to get into the good habits. Chances are you'll do the next one the same way that you did the last one. So you might as well start with the best way. 2) Consistency. You don't want 2 different ways to do the same thing in the same code base. You can need `HttpClientFactory`, so if you have that one, use it consistently.
&gt; All that has to happen is for people to add tests over time I disagree. Adding test over time is linear growth. e.g. from 100 test to 200 and then to 300. It takes very high load to reach socket exhaustion limits, the test would have to increase exponentially, e.g. tenfold, multiple times. Even then, moving up to a "re-use the HttpClient" strategy is fine for a process that starts, runs for a few minutes and then exits. The DNS issues show up in long-running processes.
&gt; I‚Äôd like to see these blog posts as ‚ÄúMicrosoft did a bad job when creating HttpClient, here‚Äôs how you should handle their mistake‚Äù Http client is not even the first go-around of the "working with http" concept. See https://docs.microsoft.com/en-us/dotnet/api/system.net.webrequest To be fair, each iteration sucks a lot less than the previous one. 
Too late. The train left already.
Be **very** aware of enabling this in a production environment. Depending on your use case, your data and your local laws you might not be allowed to log the values (sensitive personalized data), or you need to make sure access to the logs is tightly controlled (not even every dev).
Lol God why? Stop fighting HTML and JavaScript, people. There is a life outside of C# and it's ok to code in other languages they are best suited for the job. Also, the naming is terrible and misleading as cshtml is the extension for razor template files already. 
Plus if your app ends up needing to scale more than initially thought you'd have to rewrite it any way. So may as well just do it right off the bat.
 //=============================================================================== // // IMPORTANT NOTICE, PLEASE READ CAREFULLY: // // ‚óè This code is dual-licensed (GPLv3 + Commercial). Commercial licenses can be obtained from: http://cshtml5.com // // ‚óè You are NOT allowed to: // ‚Äì Use this code in a proprietary or closed-source project (unless you have obtained a commercial license) // ‚Äì Mix this code with non-GPL-licensed code (such as MIT-licensed code), or distribute it under a different license // ‚Äì Remove or modify this notice // // ‚óè Copyright 2019 Userware/CSHTML5. This code is part of the CSHTML5 product. // //=============================================================================== FYI, your `NOT allowed to` bullets don't agree with the GPLv3 license, which strictly forbids placing additional constraints on the software. As is, your license is neither GPLv3 nor free. To be specific, you may use GPLv3 licensed code in proprietary or closed-source projects so long as the source is made available to those to which you distribute the software, so any software used internally to the company which produced it is in compliance provided the software isn't used by non-employees. [Source](https://www.gnu.org/licenses/gpl-faq.en.html#GPLRequireSourcePostedPublic). If this is your intent (which is fine!) you'll have to change your license to a proprietary (and non-free) license, such as "The CSHTML5 License". Next, the clause prohibiting licensees to `Mix this code with non-GPL-licensed code (such as MIT-licensed code), or distribute it under a different license` should be reworded to state that the code can't be "incorporated into" non-GPL-licensed code in such a way that results in a non-GPL-licensed work. So, you can't add CSHTML to an MIT licensed project and retain the MIT license; the project must be licensed under GPLv3 if it is to include CSHTML. This is a slight difference, but the original wording suggests that the incompatibility is a strict constraint, which is misleading. For posterity, IANAL.
probably cause they are rapidly losing market share, and a fight to stay relevant. who besides the one off mom and pop shops with a legacy app would use this? wish these guys best of luck, however i would not touch this even if it was 100% free. &amp;#x200B; You are right, stop fighting html and javascript. Its the front end language of today.. XAML lost a long time ago, I would run from xaml, other than a 
Trash license, no thanks.
This is a dumb position and assumes a world where there is a one size fits all solution to every UI problem and that's hilariously naive. Also, why are you advocating for a technological monopoly? What a terrible idea. CSHTML is a perfectly valid solution for certain domains and can be much preferred over a javascript framework for multiple reasons. The first being a nice cohesive environment. Not every project affords developers a chance to learn how to build out a webpack build file and setup nodejs to build their front end reactjs/angular/vue/ecma6/typescript front-end project. The amount of noise in front end development is nauseating and CSHTML is providing a simple alternative for people who need to create a simple data dump app in short order. Secondly CSHTML gives a templating solution that is great for some LOB apps that don't need fancy UI elements and is arguably just as easy to set up as any other JS framework that does much the same. 
Two solutions: \- Add ICustomerStatusRepository so you treat CustomerStatus as different Aggregate (not preferred) \- Customer Status can be treated as Value Object in DDD terms. Customer status could be simple string or id so you don't need to take all statuses from database - you can have all of them in code (as list of string, enums etc) (preferred).
&gt; I also used partial classes to help break up the size of the table source I know why you did it. Don't spit up the class across many files, split up the class into smaller classes. The fact that you can already divide the class into separate concerns hints that you're violating the single responsibility principal. &gt;But after compilation those classes do get merged together It doesn't matter what happens after compilation, no human sees that. 
ITT: developers with "15+ years experience" don't know what build automaton / continuous integration is.
For the love of god please stop using PowerShell
Yeah... I guess I should have been more explicit. I thought it was clear that when I say "build", I don't mean "right click project in vs and tap build".
I made the exact thing here in the company but faster. Why not cache the properties of the 2 objects and then you can do the dictionary lookup instead of doing reflection all over again. Also, method that takes IEnumerable as a direct param is better than having {smtg}.Select(x =&gt; PropMapper&lt;SourceType, DestType&gt;.From(x) ) &amp;#x200B;
Are you aware that automapper also uses expression trees?
[removed]
You haven't made a very convincing argument.
NSFW Controller and Application Service Interaction
The idea has merit, but the implementation on the SQL side has some issues. Your usage of a GUID as PK will create a lot of table fragmentation on insert (Maybe on purpose to reduce lock contention). Then you query on ProcessDate which is not indexed which triggers table scan on every query (and taking a read intent lock on the whole table). These unprocessed messages are queried with the server side cursor kept open which will lock all the pages where the unprocessed messages are stored. This will create more blocking on the insert side. Having this 1 table wrapped in every transaction will also become a bottleneck very quickly. Potential improvements that I would explore are: * Change the ID column to non-clustered (Since this table has one single insert, read, update per record it will most likely perform better when it is a heap with non-clustered indexes) * Create a non-clustered Index on ProcessDate * Switching to numeric id's, pre-populate the table with empty messages and using a sequence to find the next available slot. * change your query to: SELECT TOP (100) " + "Id, " + "Type, " + "Data " + "FROM OutboxMessages with (rowlock, readpast)" + "WHERE ProcessedDate IS NULL * maybe apply table partitioning on Type 
&gt; There is a life outside of C# and it's ok to code in other languages that are best suited for the job. Okay....and it's okay to code in cshtml, which by the way...uses html and Javascript. So it's not fighting it... &amp;#x200B;
It does cache the whole expression (instead of just properties) and NOT doing reflection "all over again". The reflection bit is executed only once - in the static constructor.
That looks really awesome, nice to have some competition for MDinXAML.
I was under the impression it uses reflection Emit (from some StackoverFlow discussions), but never looked at their source code no... Still the benchmark says it all.
Very good database level performance advices! Thank you very much, I will definitely look at it deeper.
Sorry for the delay, yesterday was a busy day. I wasn't here when the work with Vue initially started, but in general we don't use the Vue CLI for much. We're not doing separate projects for each "mini SPA" simply because we haven't found a need to it. The "mini SPAs" are just components with possibly some stronger expectations about their context. We're not really getting into routing yet, but you can get a lot of isolation and separation just by properly breaking down components and using namespaced Vuex stores. Overall, Vue is very much a "take it or leave it" type framework. You can quickly add a component to a page by just including vue and the code necessary for your component. Many times, we'll start rather organically not knowing how extensive the usage of Vue will be. Other times, we know we're going to have a bit wider scope, so we add a little more infrastructure, but there still isn't much. If you're just starting out with Vue, then I wouldn't worry too much about the CLI or setting up a build process. Bypass that until you start to see some usage patterns in your application. I think a build process is helpful, but not a huge necessity unless you need linting, bundling, running a test suite, or any transpilation (like if you want to try typescript or something). Also, if you don't have a CI pipeline that will run your build for you, then you'll be reliant on deploying the built files from source, which is not really recommended.
For the love of xenu?
Cool article. Does anyone know some solutions to the 'fat service' besides CQRS? I have moved from making everything an interface/service to only when I need to, because it seemed like I was just moving the bloat from the controller to the service while adding a bunch more code in between - now my justification for using a service is where I know I need to execute that code in multiple places (say in my startup, or if I'm using MVC and exposing REST at the same time) or have some testing concerns (i.e. doing something more than just CRUD with EF)
Oops, if that is the case, sorry. It just seems a little slow nevertheless. Hmm, maybe it's up to the case. &amp;#x200B; I cached it in a way that I stored properties between the two in a dictionary, then i lookup a dict, if properties are there I just map them. &amp;#x200B; Btw. is this thread safe?
Well, i got a similar issue last year. I used DotMemory and DotTrace for find it. An application process was converting xml-string files to xml-byte arrays within a for block. On very small files there is not problem, big size files (5MB or bigger) was creating memory leaks. Big size objects was disposed too slow by Garbage Collector. I solved problem avoiding redundant format conversions (byte to string).
Something tells me the problem isn't the fact you have so many DLLs, but rather the way the software is implemented in the first place.
Yes you are right. Also, go tell JS to stay in the browser and dont try to escape from the sandbox :)
Can you be a bit more specific? Are you talking about the fact it's a winforms app? .Net Framework itself? The bad pattern of having a parent app call up user controls within user controls within user controls that basically have their front end to back end defined by themselves? It actually loads a lot faster once all the dlls are into memory, which is where the 'too many dlls are slowing it down' theory comes from.
You're right in the sense that once all the DLLs are in memory it's going to be quicker, but not in the region of \*Minutes\* quicker, hence I suspect there's something else at play here in the way it has been coded, but difficult to say without seeing the source itself, or get some architectural overview.
Shameless plug r/monogame
Do you mean loading, compiling or executing?
&gt; they are rapidly losing market share Who's they? lol if you mean the company with the largest market capitalization in the world.. 
Not into cults. 
We know issue is with parsing string. We are in the process of reducing the size and need some breathing room as we work on the change. 
I really doubt that compiling all your assemblies into a single dll will help you speed up the loading of the application. Don't assume that assembly loading is the problem. You should measure the loading times and compare them to the total loading time. I don't completely know the details of assembly load profiling but it's worth your while spending some time investigating. Maybe this will work: You could use fusion log to find the times that the assembly loader started resolving an assembly. And you can use AppDomain.AssemblyLoaded event to find the times when the assembly was actually loaded. Still this only shows you the difference between starting to load and have an assembly ready for use. This can be a longer period of time than that it takes to load the assembly from file. Use process monitor (https://docs.microsoft.com/en-us/sysinternals/downloads/procmon) to find the file load time. Now you can use all this data to analyse if the loading is the problem. My guess is that the actual issue is related to instantiating the datasets and grid components. You will not solve that with merging assemblies. Use a memory profiler like the one from redgate or jetbrains to profile those kinds of issues. 
Thanks for the tips. Sadly the second one wouldn‚Äôt work in this scenario because the statuses are tenant based and each tenant has a different list of statuses. Leasing off your answer should a ICustomerRepository only return IEnumerable&lt;Customer&gt; or Customer, or can it return other objects/object lists?
Useful stuff. Thanks.
You have no problems on recycling app pool cases. Obviously, very long processes will die. The big problem with recycling on memory size is initialization, like MemoryCache usages or Lazy loading usages. If a big size request is processed and causes OOM, w3p process will be killed and reloaded, then heavy initialization process will create a bottleneck.
Do not do development based on "possible causes"; use a profiler to measure what is causing slowdown. Visual Studio has one built-in. In my experience, most slowdowns are latency caused by network or file access, or deadlocks in code. I have never seen a slowdown caused by "too many DLLs".
Oh bb inject those dependencies.
&gt; our software is still using DataSets Do those datasets pre-load the xml schemas? Because the difference here can be 10 minutes load for a few MB of data without the schema, instead of 10 seconds with the schema. &amp;#x200B;
Might be Large Object Heap fragmentation if you're churning lots of strings of varying size over 85kB. Object pools can help with this. You can get pretty tricky with strings and unsafe code like like creating a pool of fixed-size strings and copying content into them then writing behind the string's pointer to modify the Length property to match the length of the content, leaving the spare room in the fixed-size string hidden and still allocated.
We build the datasets in the designer, for the most part.
Right. Just checking. [I have this to deal with myself.](https://www.reddit.com/r/programminghorror/comments/8p93b5/code_reviews_no_we_dont_why_is_there_something/)
It's too late.. it has run amuck all over the NodeJS landscape! Although my buddy codes typescript and I have to say that is a pretty cool language.
That topic frightens me. But programminghorror is probably also not a rabbit hole I should jump into. Haha.
Everyone thought code reviews was a way to throw people under the bus. I was just a newbie wondering if they reviewed code some actual code and functionality implementations in our product at least once a week, so that everyone was on the same page, and maybe learned from each other. Ever since I got into software I've been looking for mentors. But I got left with a senior writing this kind of code. Not much to learn here.
In DDD terms: 1) you should not process more than one Aggregate in one transaction (aggregates define transaction scope and protect invariants) so returning list of Customers and processing them is not preferred solution. Of course sometimes exception is unavoidable. 2) only Aggregate Root is visible to clients so you cannot get nested entities from repository because you cannot protect Aggregates invariants then. You should always get to nested objects through Aggregate Root. See [https://domainlanguage.com/ddd/reference/](https://domainlanguage.com/ddd/reference/) &amp;#x200B;
DI 
I find it's easier to test my domain logic when I don't have to worry about the MVC stuff.
I am gonna go ahead and guess that this will change. They only open sourced it today, the license might not have been fully updated. 
&gt;The source code is now available on GitHub under the GNU General Public License (GPLv3) license. No, this _is_ the license.
Excellent article, very easy to comprehend .. I would just like to see "best" practices for return types for web API
i'm not a fan of CQRS either....seems way too overengineered for little to no benefits.. but that aside you should never have application logic in the last layer of your application (controller if its an API). you should ideally be able to use your API via CLI without duplicating any logic of the core application. basically a facade for the user to be able to interact with your API via CLI is all that is required. this kind of plug and play feature of "service oriented" architecture is why it's the defactor way of building things (in .NET world at least).
Wtf? I have not seen that specific issue anywhere else other than JavaScript developers.
ACTUALLY WebAssembly is the future of the web and everyone is clawing their way out of the JavaScript nightmare half-trained developers have put ourselves in. I'm sure your teacher told you web apps are the future. They were during the Web2.0 era. 10 years have passed and coding bootcamps have completely fucked what these young developers think the industry is actually doing.
It will eventually get out of NodeJS someday. Till then, I can't touch it.
yeah .net framework is backwards compatible so you'll be fine. 
Cause Typescript makes JS a more serious approach hahaha
You have to take into account the size of the application and what it was built for. A small application built for .NET 4.5 might function fine on 4.7.2, but a large enterprise web framework will probably break in subtle or spectacular ways running on a .NET Framework version that it wasn't tested for.
Forgot the licence. C# 6. No thank you. 
I‚Äôd take the time to see the breaking changes between versions and consider if it would impact the things your app does. You can upgrade it and deploy that to a QA or test environment and have knowledgeable users test it too. 
The industry is definitely not writing xaml and C# for front ends though and web assembly isn't quite ready yet.
Yes they actually are, but only in WPF or Xamarin. Not using this, this isn't enterprise ready. But WPF and Xamarin are very very big in the financial industry. For desktop applications of course. WebAssembly is actually ready and in every browser. Just tools to use with it are being ironed out, such as Blazor still being in a beta/test kind of state.
Used to do this at my old place. Worked perfectly fine for what we had. At my new place we have build automation stuff and I understand why it's necessary as well, since it is on a much large scale and more complicated
For the love of his noodley appendage?
You will want to go through this: https://docs.microsoft.com/en-us/dotnet/framework/migration-guide/ This part specifically was helpful to us: https://docs.microsoft.com/en-us/dotnet/framework/migration-guide/runtime/ Select your current version, and new version. We upgraded a massive enterprise desktop application from 4.5.2 to 4.7.2 and there were a few gotchas with security changes and what not. But they were all listed in that guide.
The size of the application has nothing to do with it. You've either used APIs with breaking changes between the two versions, or you didn't, and there aren't that many, and even fewer commonly encountered ones, between 4.5.1 and 4.7.2. OP, /u/aorin gave you perfect advice.
No, it's going to have to be something related to the actual merits of powershell. If it's solving a problem for me, I need to see some specific reason to stop using it, and probably an alternative.
I‚Äôd performance profile the app using tools built into visual studio via debugger. If it‚Äôs specific pages I‚Äôd target my investigation to those pages. Either way, this would tell you where it is spending the majority of its time in. Assuming you have the debugging symbols loaded it will show you specifics as to which method or call stack is the bottleneck. 
WTF....why didn‚Äôt they MIT this like every other duel license project?
SOLID is always overkill until you wish you had it. You should still use interfaces to reduce your change surfaces even if you are only using logic in one place. In an enterprise environment: software requirements change. If you don't abide by SOLID from the start, when those changes need to be implemented, you will either have to start adding bloat and complexity to the existing code (which will usually cause tech debt). Or you will have to refactor before making changes, which means you have now down twice the work. Personally, I will happily take the few extra lines of code upfront over the large amount of extra work for the rest of the application lifespan.
1. You should be able to see in the performance counters for that worker process and use that to answer your question. 2. I wouldn‚Äôt bother with increasing the shut down. In my experience doing worker process cycling like your describing but just using the app pool configuration, the terminating worker process would finish its requests before shutting down As I started to say in #2. In recent IIS changes, you can do a lot in the app pool / site to allow it to cycle within the web server itself. This or your solution could be a stop gap while you investigate the root cause. For solving the root problem (which you absolutely should do, unless this legacy app is being phased out now), I‚Äôd get a crash dump of the worker process so you can run windbg or other crash inspectors that will allow you to explore where the memory is going and address the root cause. Switching to 64 bit isn‚Äôt a stop gap, you‚Äôll still run into the same problem and your likely to run into comparability issues with some assemblies only working in 32 bit mode. (Pdf libraries in older apps have commonly been 32 bit exclusive for example) Good luck. 
are you asking about risks for upgrading the .NET Framework version installed on a machine, or for changing the targeted .NET version when building an application? If it's the first one, .NET is backwards compatible - I've upgraded thousands of machines from 4.5.x to 4.7.2 without any issues. If it's the latter, then you need to consider some of the language changes as well as where the application is going to be installed - is it an environment where you know 4.7.2 will be installed?
I've been doing it the SOLID way by default for a long time now at work (5+ years,) just in my recent hobby project did I try being lazy with fat controllers (to basically test your principle.) So far out of 6 controllers I only 'needed' to make one with a service layer after the fact. In this experiment I also am trying more of an integration testing approach than a unit testing approach, to see how fast and far I can get. Any tips for preventing service / logic from growing too fat? Most of my actions/functions are pretty tight but every now and again you get one where it seems like you can't help but have a decent amount of logic going on - not sure if it is better to split up the operations into multiple API calls or wrap the logic up into some other patterns.
Does it make sense to move all interfaces and models to a separate class library, something like "...DomainContracts" so it would be easy to replace the services via dependency injection, or is that not necessary?
I would think a webAPI (and MVC controllers) would return ViewModels or some kind of projection class. I don't like to get crazy with mapping objects to other objects but to me it makes a lot of sense for the client project (api or mvc) to return model classes that have no dependencies. Sometimes you might want to change the domain models and not affect or break your api.
&gt; test environment what does this mean?
Though some of it feels a little preachy, it's a great informational piece. Though this repeated line in the examples was killing me. var loanQuoteResponse = _loanService.CreateQuote(request.Amount, request.CreditScore); return Ok(response);sponse); 
this simply isn't true. socket exhaustion doesnt just go away when your process exits. because of the way tcp works, ephemeral ports will remain open for a period of time (4 minutes by default on windows) . The number of ports is really not that many, like 15000 (again, windows). don't bother with proper usage for one off programs. but if a remote system will ever run your code, just do it the right way. it's worth a few extra minutes to get it right than to chase down why your test system dies every 6 hours. 
Seems troublesome and tends to fragment code that otherwise would have similar dependencies and be logically grouped together.
Does it actually affect anything?
Yes, it does.
What benchmark? I only see a table. [This is a benchmark](https://github.com/AutoMapper/AutoMapper/tree/master/src/Benchmark). Automapper only uses emit when mapping proxies so it's not relevant to your needs - but even if it did, emit is equivalent to compiling expressiontrees... it's not the reflection you're thinking about.
It's an environment setup for testing.
&gt; ephemeral ports will remain open for a period of time (4 minutes by default on windows) A full test suite typically doesn't get run even close to that often.
Mind referring which books you used? Not sure which books to use since the exam ref 70-486 books are a bit outdated. Or so I've been told. I also saw that on the microsoft page [https://www.microsoft.com/en-us/learning/exam-70-486.aspx](https://www.microsoft.com/en-us/learning/exam-70-486.aspx) they also refer to the test exam. Is it really worth buying the test exam? 
Thanks for your response. It's about the upgrade and not about the changing the code. 
Thanks, yes I am planning to do the Development environment first, than Test, Acceptance and Production.
I dislike these when they happen due to assembly dependencies not found because it doesn‚Äôt bloody tell what it was looking for in the exception. So you need the Fusion Log Viewer or something to assist just because of that, which isn‚Äôt very fun if it‚Äôs on the customers end. 
Totally agree. The error message could be so much better in this case. I didn't even mention the Fusion Log in the blog post, but there is definitely content enough for another post :)
It is unclear who your target audience. Developers? They prefer to use scaffolding for simple CRUDs generation, or rich UI widgets from DevExpress (= and usually they hate visual designers). Non-developers, so this is code-less CRUD-app builder? A lot of strong competitors here too (MS PowerApps? Caspio? Appian?) . BTW, grid-views also compete with (embedded) reporting/BI tools that connect to user's DB and offer powerful table reports - not only grids, but also pivot tables and charts. 
Thank for a detailed answer. When it is transitioning to a new worker process , other than the application cache . Does it need to load any information into app domains memory. What is your take on web gardens scenario for the above problem with out of memory . We do use many 3rd party libs and are working on getting a 64 bit version. 
Azure has a free tier for app services hosting plans.
Azure app service free tier for getting started, but for anything else I think it's highly beneficial to have access to the underlying server, otherwise you end up restricted and paying a higher price. If you can't hack IIS and Windows Server then stick with App Service. So what I do is have an application server and a database server, and then I can fully manage the servers to my requirements, e.g. for backup strategies and any supporting applications I want to run on the servers. Windows Server VPS on AWS Lightsail is pretty good as they provide a bandwidth allowance. For high volume apps you're going to need provisioned storage and would have to move to something like AWS EC2. I like the look of Digital Ocean but have never used them as they don't do Windows Server, but I think they support BYOL.
I'm using Core so I'm not constricted to Windows Server, that's why I mentioned DO, as I've seen it being mentioned here from time to time. I'm generally not a big fan of Azure and I'm not sure how pricing-effective is Postgres hosting in Azure. Pricing seemed a bit higher than their regular Azure SqlServer-like DB.
It does have to load all the dlls and such into memory as you pointed out. After that it will go through any cusomizations you may have injected into the startup process through the web.config and also via the global.asax‚Äôs app startup lifecycle. 
I am just playing with cheapes linux VM and TICK stack in docker. Runs well for ~10$/month on Azure. Any cheapest cloud VM should be enough for experimenting. I am not aware of any free provider. PS: No matter what you choose, google for promo codes. You can get down to 2$ monthly (at least for some limited time)
MVC of course. Or I don't understand you?
I'm using the repository pattern in the training course. I'm creating a web api so theres no 'View'. The api is consumed by a seperate Angular app.
I don‚Äôt understand, you will have projects, api and angular?
CQRS with non-generic Repositories and Units of Work. We skipped Repositories once, got stuck with EF everywhere, never tried that again.
That's fine; I stick with Windows anyway because of other software that I use and general familiarity, but Core is fine if you don't use the Windows features. I personally would host Postgres on a regular VPS, I'm not really a fan of managed database services, but if that's what you're looking for, you could also consider AWS RDS or AWS Lightsail managed databases. Managed database pricing is high anyway, vs. regular server.
I have a .Net Core WebAPI application. This is the logic of the application. There is no 'View' here. &amp;#x200B; The view is my Angular application which consumes the WebAPI.
REST (actual REST, not just JSON over HTTP) and small Onion, with a thin controller.
I'll have a look into that. Any good sites to read up on this.
Ok, you need read about RESTFul. See [https://www.djamware.com/post/5b87894280aca74669894414/angular-6-httpclient-consume-restful-api-example](https://www.djamware.com/post/5b87894280aca74669894414/angular-6-httpclient-consume-restful-api-example)
Really depends on the user experience and resources available for your team. Angular/react and such can offer a rich/fluid experience because it can avoid the constant page refreshing that could be found in some of the conventional methods of razor. In cases like cheap line of business entry apps, razor may be a perfect choice for a quick solution. I‚Äôd ask yourself, what are the long term goals and support available on this project. If it‚Äôs just you, what are the time constraints and are you comfortable with one vs the other to deliver the product on-time with appropriate quality? 
I'm actually kind of afraid of hosting any kind of database by myself in a case the project grows into actual product. I think I have not enough experience managing that stuff. I do agree that the price is usually high.
I actually got into CQRS thanks to [this guy's](https://radblog.pl/2017/08/19/cqrs-first-step-split-to-commands-and-queries/) presentation(and blog posts). But I guess any source you fancy should be fine as long as you understand why and how.
Your post has been removed. Self promotion posts are not allowed.
Your post has been removed. Self promotion posts are not allowed.
This question comes up fairly often, [so I wrote a blog post](https://mking.net/blog/cheap-and-easy-aspnet-core-hosting) to outline some of my favorite options. The gist is as follows: [Azure](https://azure.microsoft.com) is your best bet - it's easy to use, and has the full weight of Microsoft behind it. Deploying can be as simple as a `git push`. There is a [free tier](https://azure.microsoft.com/free/) for 12 months. After that, you can get 'free forever' Azure App Service instances that have a few minor drawbacks (shared hosting, no custom domains, etc.), or you could pay for some of the cheaper shared instances. If you want more functionality, moving up to the other paid plans is definitely worth it, there are some great features there. Scott Hanselman has a great post - [Penny Pinching in the Cloud](https://www.hanselman.com/blog/PennyPinchingInTheCloudRunningAndManagingLOTSOfWebAppsOnASingleAzureAppService.aspx) - that covers the best way to get a good bang-for-your-buck from Azure App Service hosting. The next best bet is to host on a [DigitalOcean VM](https://www.digitalocean.com/). You can use [Dokku](https://github.com/dokku/dokku) to get your own mini-Heroku PaaS, or manage the VM yourself ([following Microsoft's documentation](https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-2.1)). You can get anywhere from $10 to $100 in credit from a [referral link](https://m.do.co/c/aac4e1b54a04) - this will last you a year and a half with a small VM. [Heroku](https://www.heroku.com/) is another good option. Their product is rock solid, easy to use, and has a wide variety of pricing tiers (including a free tier and an inexpensive hobby tier). You can deploy ASP.NET Core apps using a [.NET Core buildpack](https://github.com/jincod/dotnetcore-buildpack), or using their [Docker](https://devcenter.heroku.com/categories/deploying-with-docker) functionality.
Thank you!
So stick to repository pattern/unit of work instead of CQRS. I think the main point is that you should break it down. In a system where you are working with events, that data access layer may end up very different with completely different dependencies between each command, so CQRS makes a lot of sense. We tend to do controller-&gt;service-&gt;repository(ies), for basic crud services. All logic/orchestration, (maybe with stateless helpers split off for complicated business logic) is in service layer and data access of domain models in repository.
The real power is decorators that he didn't show. Having cross cutting concerns across all types in one class greatly aids in development. https://www.skylinetechnologies.com/Blog/Skyline-Blog/January_2019/cqs-simpleinjector-and-power-of-decorators
You can mitigate that by having well defined folder structures ahead of time so everyone knows where to out and find things.
The database management services are where they slug ya. If you don't want to look at Azure, Do have a look at RDS which u/Liam2349 has suggested. Look at using Docker to host your demo apps and database on Linux. I have used the SQL Server Docker image with some success for demos and small apps. You can use SQL Express edition which is free. https://hub.docker.com/r/microsoft/mssql-server-linux/ You can always swap your database to a managed solution when you scale. You're not locked into a platform here. At the end of the day, it's just a connection string to somewhere. Do consider the value of your time. You will spend more time getting things working on a Linux VM.
Even with a managed database service, you still need to manage the database. The only difference is that you do not manage the hardware, operating system, and any associated software.
If your response and your domain model are so decoupled, it gives you (the developer, not the domain) a dependency between them when one changes, as that change has to be accounted for at the integration point. If they‚Äôre more tightly coupled (I like to have response models extend domain models and add API specific things, like href) then those changes are automatically propagated. That just pushes the integration layer further out, of course. Basically, if your domain models change that‚Äôs going to affect your API one way or another, as you have to account for that change.
i‚Äôll check it out thanks 
The books were Programming in [ASP.NE](https://ASP.NEt) Core (Dino Esposito), [ASP.NET](https://ASP.NET) MVC 5 (Jon Galloway, etc.), and I read some of 70-487 reference book for Azure stuff. There was a fair bit of Azure Architecture stuff. The practice exams weren't exactly like the real test, but the important stuff from the practice exam is the types of questions and which areas they ask. There was plenty of overlap between the 2 to make it well worth it. The Exams are more important than the books. As you know .NET is very large, hard to remember everything, so the exams give you a better direction.
Thanks for the input. It's not an app builder, it's a single grid control generator. One of the advantages it has to scaffolding is that once you define a grid, it's defined on all 5 platforms. With scaffolding you would have to generate forms and lists (actual code) for every platform (Xamarin, WPF, Angular, MVC, Core) and every view not to speak of form validation. With [GridQuick](https://www.gridquick.net/?rref=rdotnet) you can load grids dynamically at runtime you just change grid name in your control meaning you can basically have one view with one grid control that changes the name property and displays different grids based on that. **Example:** If we are on the web you could do something like [domain.com/Home/Index?gridname=students](https://domain.com/Home/Index?grid=students). Index page will have bunch of other components along with one grid control where you delegate the grid name from query params. You can of course do a static reference like &lt;Grid Name="nameOfYourGrid" ... /&gt; My target audience would be a head developer or an architect in a company that needs to create output from a legacy database. That output would be very repetitive meaning a lot of tables and CRUD forms. I probably need to add some more explanation along with docs on the page so the message is clearer. &gt;BTW, grid-views also compete with (embedded) reporting/BI tools that connect to user's DB and offer powerful table reports Competition is never bad. It just proves that there is a market. How my product fits it I have to find out. &amp;#x200B;
I'm gonna go against the grain a little bit and recommend DigitalOcean. They have all the tools you would need for hobby projects all the way up to medium sized business infrastructure. It's very competitively priced, and I find their UI to be much more straightforward than services like AWS and Azure. Currently I have everything (API, Nginx, Postgres) in a single VM but isolated using Docker containers. Eventualy I'd like to separate them out to each have their own VM, but that costs more and I haven't had any problems with it so far.
Can I just ask out of curiosity how you deploy your containers there? Kind of noob on this but would be interested in knowing.. thanks!
Whatever it is AutoMapper deos it takes 32 seconds for the same very task (and 325ms if I add some manual caching optimizations). OK, so it's probably NOT because of Reflection, point taken.
as .NET developer, I don't see strong motivation to use your solution, but maybe I'm just not your TA. In any way, good luck!
Sure, there's probably a more elegant way to do this but my CI pipelines handle deployment. After it successfully builds, tests, and publishes the Docker image to Docker Hub, it then will SSH into my server and run an "update.sh" script located in my VM that basically does the following: 1. Stop the web API container 2. Pull new image from Docker Hub 3. Restart the web API container I also use [Docker Compose](https://docs.docker.com/compose/) to make managing multiple running containers a bit easier.
You are right, we are going to reword the document asap. Thanks for pointing this out.
I'm a fan of mediator. 
I tend to see this pattern a lot so it seems fairly common, I'm also using it in my projects at work. I think its usually referred to as the Repository pattern. API Controller -&gt; Service -&gt; Repository &amp;#x200B; However, I know CQRS is a popular approach as well and it can be added to a project using this architecture fairly easily based on my experience. &amp;#x200B; &amp;#x200B;
I'm currently using Linode for my side projects, but Vultr and Digital Ocean are also excellent with similar prices.
This is why Microsoft naming server-side Blazor as Razor Components is a problem. It's too easy to mix things up. That said, naming things is hard. I don't have a better suggestion. No offense either u/theSeanage; it's an easy mixup to make! OP: Unfortunately I've not really played with it yet, although I have been tempted to make something with it as well. I'm hoping someone has some first hand experience with it and can share what kind of lag or delays a user will actually see.
the joke is that lots of places just do everything in production and qa/test environments are a luxury. 
Clean Architecture + CQRS + DDD [https://github.com/kgrzybek/sample-dotnet-core-cqrs-api](https://github.com/kgrzybek/sample-dotnet-core-cqrs-api) &amp;#x200B; But I have to warn you - this is only one of the multiple solutions. You always need to choose appropriate architecture and implementation according to problem which you want to solve. General speaking, there is no "best solution" and there are many "architecture drivers" (business requirements, technical requirements, resources, tools, frameworks etc). Architecture drivers define you what is the best solution **in particular context**.
And my joke was I tried to explaining using the words that compose it. Making it a useless explanation.
Id don't know any solutions besides CQRS to do that. Maybe using AOP but is not preferred. The best way to avoid fat service is using Command Pattern together with Pipeline Pattern. In this scenario you are separating concerns which Application Service is responsible to take care - logging, validation, orchestration. You can check how it is working in my article about REST API validation - [http://www.kamilgrzybek.com/design/rest-api-data-validation/](http://www.kamilgrzybek.com/design/rest-api-data-validation/) &amp;#x200B;
Great article. Your problem is called analysis paralysis. This can be solved by writing code, compiling, running. Ensuring fast feedback over all. Then, refactor only once you see how you can make what already work simpler. You win by going bottom and refactoring as you go up and height make things crumble. Instead of going up and never reaching the bottom.
I cannot recommend Octopus Deploy enough. For enterprise scale products, it is unbeatable at the moment. Especially for managing the whole deployment pipeline as well as just configuration values.
damn. i need a coffee.
Learn WPF?
&gt;‚ÄúThere are three Windows DHCP Client Remote Code Execution vulnerabilities with a 9.8 CVSS score in this month‚Äôs release,‚Äù &gt;‚ÄúThese bugs are particularly impactful since they require no user interaction ‚Äì an attacker send a specially crafted response to a client ‚Äì and every OS has a DHCP client,‚Äù See, this is exactly what I was afraid of, this dynamic host control protocol is out of control. It can execute scripts or modify anything without any control! With the good comes the bad and I think that NuGet has solved way more problems than it has created. Also, you left out this tidbit from the article which is literally the next paragraph from your quote. &gt;If successful, [an adversary] could modify files and folders that are unpackaged on a system,‚Äù ZDI wrote. ‚ÄúIf done silently, an attacker could potentially propagate their modified package to many unsuspecting users of the package manager. **Fortunately, this requires authentication, which greatly reduces the chances of this occurring.**
We've all been there haha
I love Vultr. We have five servers running there and haven't had any issues. I also really like that they let you add records to their DNS servers as part of your hosting. Free redundant DNS for the win!
I'm not entirely sure what you're asking here - NUnit should work fine with NUnit since it isn't solely .NET Core. How are you using .NET Framework on Ubuntu, though? .NET framework isn't cross platform (unless you're using Mono) which may mean you're using the dotnet CLI (which I don't think supports running .NET framework - correct me if I'm wrong though) so it may be that you're running your tests as .NET Core tests.
Do you have experience in Silverlight? If yes, Silverlight is really just a subset of WPF. The biggest problem is if you have 3rd party control libraries and don't have the WPF ones. Basic process for going from Silverlight to WPF is to move all the code into WPF projects and fix stuff until it works. Silverlight has some stuff you'll need to rewrite, and if you use RIA, you're pretty screwed, but it's fairly simple overall. 
Ah I see. I think it's quite okay :). About your app settings though. You keep them in the VM as env variables? Say like connection strings and so on? 
Kimsufi, dedicated linux machine for 4 euro a month [https://www.kimsufi.com](https://www.kimsufi.com/)
.NET Standard shows you the support between DLL Framework, Core ... [net-standar](https://docs.microsoft.com/en-us/dotnet/standard/net-standard)
&gt; We skipped Repositories once, got stuck with EF everywhere, never tried that again. Many people advocating not to abstract EF DbContext with Repository and UOW miss this one. Using DbContext directly binds you to EF, as you say, all over the place. We are migrating from EF to Dapper and simply need to rewrite the repositories and we're go.
Thank you for this! I've scrolled by it at least a hundred times and never actually looked at it.
You are describing Razor which is different to Razor Components which does not require full page loads and posts. It uses SignalR to communicate page diffs. 
 First of all, thank you for this awesome tutorial, it works like a charm. But I have a problem. I just don't why files that are in ClientApp folder doesn't show up when i try to publish this app to exe file. I have to do this for my university project so any help would be appreciated :) 
Sorry it's been so long, but would you be willing to show me how to do it with a model? This is what I have so far //ViewModel public class GenericNotesViewModel { public List&lt;SelectListItem&gt; NoteType { get; set; } } //Controller note.NoteType = new List&lt;SelectListItem&gt; { new SelectListItem { Text = "Operations", Value = "1" }, new SelectListItem { Text = "Management", Value = "2" } }; //cshtml @Html.DropDownListFor(m =&gt; m.SelectedCity, (SelectList)GenericNotesViewModel.NoteType, "Select a Type", new { @class = "form-control chosen-select" })
Yea I currently use environment variables. I think there is a way to include variables in automated deployments in Azure DevOps but I haven't looked into it yet.
Those people don't really understand the Repository pattern then :) but yeah, I saw that thread here a while ago too.
&gt; Service layer can probably be omitted until its actually needed if its just acting as a simple passthrough to the repository layer. Eh, I always just add it in anyway. Even if it is just a pass-through, it is there if you do need it, and not terribly difficult to add at the beginning and much harder to add after the code-base has fermented for a while. 
I don't get it. As long as your project is a .NET Standard 2.0 project type, you should be able to just install the [`Microsoft.EntityFrameworkCore.Sqlite`](https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.SQLite) package and you'll be all set to go.
Very true, it doesn't harm anything by being there. 
Is there a good guide out there that you can recommend on how everything goes together? I've seen demonstrations where they set up microservices, angular/react front ends, and use docker containers, and some kind of manager like kubernetes to scale things up and down as the workload changes...but the big mystery is how it all goes together. How you spawn up or destroy containers on the fly, but keep your actual data from being destroyed, etc... I have yet to find a decent guide. I know some people that just figured this stuff out on their own, but they have a team of about 25 developers. If I'm working on my own, I don't really have the time to learn all of this on my own. I'm very knowledgeable with IT, and have been programming for a long time as well, so it shouldn't be too hard. I could just use some guidance on how the nuts and bolts of this new software paradigm goes together.
raspberry pi sqllite db &amp;#x200B;
&gt; We suspect this is used to log people using virtual private networks during exams. Could also be to try and detect when VM software is installed or in use, whether the software is running on a host or guest VM. That is a big way possibly to bypass most of what this software checks for.
Well there is my blog www.topswagcode.com Aiming to do one blog post every second week. Would love feedback. 
This tool is reportedly good at telling you how compatible your code is: https://docs.microsoft.com/en-us/dotnet/standard/analyzers/portability-analyzer But I would also go with /u/aorin's suggestion as well to manually check things.
&gt; In another scenario, say a product is sold just after we ran out of inventory. We could automatically cancel the order. A better option is to automatically re-order stock and let the customer know it's been back-ordered. Brb gonna go short AMZN then place an order for 8000 of those $20k giant TVs and then immediately cancel it
You are right .Net Framework isnt cross platform, but we use a interpreter called "mono" and it runs exe. My problem is that NUnit needs to use .Net Framework for our Solution. And I dont know how I can run that
I‚Äôm describing razor as in the cshtml razor sense where it is rendered at the server side and emitted down to the client. If that clarifies anything. Definitely not talking about any blazer style stuff. 
From what I can tell, the NUnit console runner should work with Mono. I'd suggest installing that and trying to run your tests with it - that should work without .NET Core.
Unfortunately I'm not familiar with any good guides that cover all of those aspects. Hopefully someone else reading this thread has one, because I'd also love to see how others are putting everything together.
I think "you don't always need" is more accurate. Sagas don't always satisfy the need either.
I just went through this recently. I couldn't find any guides and had to just learn by doing some samples.
I setup a pipeline in Azure DevOps to automatically ssh into my Droplet and redeploy the latest docker image.
What do you mean not resizing accordingly? Can you provide screen shots and reproduction steps? I have experience with UWP and I'd like to try to help. 
Time for Linux to shine! 
Orchard CMS.
Luckily no RIA
&gt; Any tips for preventing service / logic from growing too fat Things growing ‚Äútoo fat‚Äù, or even just ‚Äúfat‚Äù is a violation of the SRP - they do more than one thing. So break it up into things that have only one responsibility and thus are in harmony with the SRP.
If I remember they have a benchmark solution on the github repo. As for real world performance that's yet to be checked as .net core 3 is still in preview. Though I do think some are using it on small production apps from what I heared from gitter.
I just spent the last two weeks learning about concurrency in C#. I read through a lot of articles, and sampled many books. The best resource I found was [C# 7.0 in a Nutshell](http://shop.oreilly.com/product/0636920083634.do), Chapter 14. Stephen Cleary also has a lot of great articles on the subject (and a good book too if you‚Äôre looking for a more in depth treatment).
Don't know if this is exactly what you're after but Azure Notebooks have F# support https://notebooks.azure.com/
If you only use Linux then you must take the exam under "strict supervision".
Richter. 
Nice to see VS for Mac getting some love!
OP was talking about server side Blazor which was renamed to Razor Components 
Try this: &amp;#x200B; @Html.DropDownListFor(m =&gt; m.NoteType, new SelectList(Model.Items, "Value", "Text"), "Select a Type", new {@class = "form-control"})
That's a really cool and useful project, thank you! 
That really doesn't bother me. The more people that use linux the less enforceable that notion becomes
This is great! Thank you.
Yes. I think you will be impressed with the speed once you stop using Resharper. Plus, almost all of the features from Resharper have been replaced by VS (and the few that aren't you can get from a couple of micro extensions that are much more efficient). As painful as it is to memorize new shortcuts, I think in the long run it will be a win. ...and if you hate learning new shortcuts, you can remap, but that is kind of a pain, especially if you have to do it in VS and VSCode).
Oops, meant to post more information. Updated with link to stack overflow. 
I know that ocelot supports response aggregation, but what's the use case for a gateway doing it? Does response aggregation not couple services and require some business logic to determine the use case?
Microsoft's SQLite library (that EFCore uses) works fine for me. What's your problem?
if it makes any difference to you, I just installed a vscode extension that sets up resharper-like keyboard shortcuts
.NET Framework is not slow and .NET Core is not some magic speed up. Nor is ILMerge. Also don't use .NET Core 3 yet, it is not ready for final release. If you are interested in porting to .NET Core I recommend using 2.2 which is stable. Porting to .NET Core 3 from there when it is released should be trivial. As others have said the problem is more likely in the code itself. You will have to identify the source of the slowdown. If the app is not running IIS must start it which can cause slowdown on the first page load, however subsequent loads will be full speed. If all page loads are slow then it is almost certainly a code issue. You can try making a REST API endpoint that does nothing but return some static value, and try timing that. It SHOULD load instantly. If it does then the slowdown is likely due to code in the request path. If not then the issue may lie in IIS or some other part of your codebase. You can turn on request tracing in IIS (briefly for testing on a development server) to see where all the time is going. I would expect it to go to your app but you can check to be sure. If you attach the Visual Studio debugger and step through code it will tell you how long each line takes to execute. This can greatly help identify slow parts of your code. For a more crude method you can store DateTime.Now into a local variable, and then later on in a method subtract that value from a new DateTime.Now to get the interval between the two fetches. Then you can log it which can help identify if the code block you've placed them around is slow.
You are right, it is not a magic speed-up button, but I am not the boss and my boss is convinced that it is and that loading too many dlls (thousands) is the problem. I looked at a profiler and identified a couple of obvious problem areas, which my boss seemed not overly interested in fixing. Just said 'I'll look at it' and dropped it.
I like it. Are you going to do anymore posts from what you learned from books? That scalability post is nice, I'd like to see something more on that one. A good book to cover is The Pragmatic Programmer. &amp;#x200B; Not trying to be ... that guy ... but on Serverless payment with AWS Lambda and Stripe - Step by Step guide ..... Joking is misspelled. I like the blog.
Yes, i know this is a day or so old, but for anyone coming across this... The mixups persist because people continue mixing it up. Nothing was *re*named - a name was *given to* the component model used Blazor. This same model - Razor Components - is usable on [client and server](https://docs.microsoft.com/en-us/aspnet/core/razor-components/hosting-models?view=aspnetcore-3.0) and in non-Blazor contexts (Razor Pages, MVC). Server-side Blazor is an ambiguous misnomer. The docs refer to it as "Server-side Razor Components." Source: [docs](https://docs.microsoft.com/en-us/aspnet/core/client-side/spa/blazor/?view=aspnetcore-3.0) and loads of interviews/conferences with Dan Roth
https://www.reddit.com/r/dotnet/comments/b0kymj/razor_components_performance/eiisbdg/
I've never managed to have that app service really be free. Not sure what I did wrong, but they charged me.
&gt; " Razor Components - is usable on [client and server](https://docs.microsoft.com/en-us/aspnet/core/razor-components/hosting-models?view=aspnetcore-3.0) and in **non-Blazor contexts (Razor Pages, MVC). "** The part in bold is not correct and the docs in your link explain that. This may change. But at the current point in time that is not true. Razor Pages and MVC can ***never ever*** use Razor Components. Yes they are written in the same **markup language** that Razor Components uses. That markup language is called **Razor. B**ut you cannot use the Razor Components in an MVC or Razor Pages app. At this point in time, you can only add a Razor Component file to a Razor Component project (File -&gt; New Razor Component Project) . That project, currently implements the Server Side hosting model in your link. &amp;#x200B; In fact, due to the confusion, Microsoft is planning on changing the file extension for files that can contain Razor Components, from ".cshtml" to ".razor" so that you never try to add a Razor Component file to an MVC or Razor Pages app. &amp;#x200B;
The plan is to do more posts on books :) I have pragmatic programmer book :) So most likely coming a book on it. I don't mind getting spelling errors. I will fix it :) thx for the awesome feedback. 
WebStorm is so much better for Angular projects than viscose. You should give that a shot.
It's not that you can't use dependency injection, you just can't use the current Microsoft provided library. The library you're trying to use requires a .NET Standard 2.0 compatible target framework. If you check against [this](https://docs.microsoft.com/en-us/dotnet/standard/net-standard) table, you'll see that .NET Framework 4.5 is only compatible with .NET Standard 1.1. If you want to use the newer libraries and frameworks, you'll need to jump to a newer version of Visual Studio or use their free editor, VS Code (though you would likely be abandoning the full .NET Framework in this case and be using .NET Core).
No, it is time to move from VS+R# to JetBrains Rider.
Thanks!
All the articles you've referred to are for .NET Core and ASP.NET Core, not ASP.NET MVC, so you need to make sure you're creating the right type of project. While I can't figure out for sure what version of .NET Core is supported in VS2013 (if any), it's very likely you won't be able to use VS2013 to use any of the new frameworks.
Im not a fan of repos for each table and implementing get / update / insert methods. far easier to just use IQueryable on the dbsets in the service layer, yes it leads to a db dependencies in the service layer but you can use sqllite to do unit tests instead. All these repository patterns just make life harder, i havent seen one good implementation that doesnt bloat out the code base massively and adding new sql operations a pain in the ass.
Seems to me that client and server do not share code. So instead of Xamarin you could easily use Kotlin. Get a bit down to earth!
I believe when you are using a gateway API as a BFF (backend for front end), the API is really dedicated to just that client or group or clients, so any aggregation you do is specific to those client use cases. I'd think most of the business logic would be presentation concerns, aggregating multiple services on the gateway improves performance by saving multiple round trips from the client. So, the gateway API is very coupled to the needs of the client. Keep in mind that you will tend to have multiple gateways, either for different clients or business areas, as opposed to having one mega gateway monolith.
&gt; I realized how much more faster it was to throw gang-signs at my keyboard instead of mouse-clicking Okay so I laughed so much harder at this then I should have. However I agree. Consistency, for me at least, definitely increases efficiency. I have a few custom key bindings of my own in VS (nothing to fancy). Anytime I try out a new editor I just find myself frustrated that the key bindings don't work the way I expect them to. Then I spend more time then I should remapping things. Finally after the headache is over I'm back to my normally pace. &amp;#x200B;
Yes, i know the difference between Razor and Razor Components and Razor Pages/MVC and .razor and .cshtml and so on. The link you posted is a duplicate of one i linked. VS doesn't (AFAIK) expose any "use Razor Componenets in Pages/MVC" whatnot yet as it is currently only one-way (and has been demonstrated) - where the intent is two-way. As i understand it, swapping filename extensions is just as much to ease the burden on the backend as the developer - how is it to know which sort of Razor it is looking at without a hint of some kind. Again, this is all coming from docs and the likes of Steve Sanderson and Dan Roth, nothing i'm pulling out of my ass. I'll find a link about the multi-use thing.
If learning is the goal, have a play with Unity 3D talking to a dotnet core backend on Azure. I find Unity more fun than console apps for exploring code ideas. Core on Azure should scratch that corporate deployment experience itch. 
Hey I‚Äôm the author of one of the articles above. I‚Äôve been using VS2017 (and a little VS2019) for my entire blog series and haven‚Äôt tested anything with VS2013. Another comment here also pointed out that the OP is working with ASP .NET MVC (and not Core) so those are 2 different things, in different namespaces. 
Use monogame.
The error message, at least part of it, is coming directly from the Windows standard error messages. So the error handler may not have the context needed to figure out what file was being accessed; it just sees it was the internal windows error code that means a file is missing, and assigns a FileNotFoundException instead of the generic Win32Exception used for generic Windows API errors.
Both of those methods AddMvc() and AddAuthorization() are available in Asp.Net Core 1.1. It's in the documentation. Intellisense is imperfect. What was the build log error?
&gt; i wanted to learn I would recommend learning current technology, rather than 5+ year old stuff.
Jeffery Richter is the best author out there for .Net threading. 
VS codes keyboard shortcuts are not exactly the same as the ‚Äòvisual studio‚Äô resharper layout. I switched from the IntelliJ layout to the visual studio one because my machine was often being used for code reviews with colleagues who didn‚Äôt use resharper. VScodes c# is almost but not quite there. I find I miss the automatic brackets and ibeam placement that resharper adds to intellisense the most.
Looks pretty neat! But do you have any quick tip as to how to resolve the error below when trying to load the visualizer? Unable to load the custom visualizer. Could not load file or assembly 'ExpressionTreeVisualizer, Version=1.0.6.0, Culture=neutral, PublicKeyToken=null' or one of its dependencies. Operation is not supported. (Exception from HRESULT: 0x80131515)
[https://www.manning.com/books/entity-framework-core-in-action](https://www.manning.com/books/entity-framework-core-in-action)
Thanks I will look into these
AFAIK, Visual Studio 2017 [doesn't support custom visualizers for .NET Core apps](https://developercommunity.visualstudio.com/content/problem/99999/dataset-visualizer-missing-for-net-core-20.html), although it's supposed to have been fixed for 2019. When debugging a .NET Framework app, were you also not able to use the downloaded DLLs? And were you able to use them from the instance folder, instead of the My Documents folder?
I only got the error mentioned above when downloading the DLLs. When I did a bit of Googling for the error 0x80131515, the results talked about Windows being unhappy with using downloaded DLLs. Apparently in *some* situations Windows is polite enough to display in the DLL's properties window that the DLL is "blocked", which you can then fix. In other situations, it just silently doesn't work, which is apparently what I encountered. I'll try your suggesting regarding folders. I was just following the instructions linked from "Copy the .dll files into the visualizer folder as described [here](https://docs.microsoft.com/en-us/visualstudio/debugger/how-to-install-a-visualizer?view=vs-2017)". I don't mean to sound too negative - it's precisely because I thought this was pretty interesting work that I took the time to write that up, in case someone else trying it encountered the same issues.
I have to keep Resharper installed for my work computer because none of the NUnit 2 tests work without it. Depending on how VS is feeling it may or may not detect them, but if they're detected and you try to run them it builds and says "No test with name X was found" (or whatever the error is). Resharper runs them no problem. It's driving me crazy because VS is so much slower with Resharper.
I know VS can run NUnit tests consistently, so it makes me think there is some setup issue going on there... but I use xUnit, so I am less familiar. I do know that is one selling point of R# on VS... the test unit stuff was a bit more friendly.
Really late to the party here but I do the technical interviews at my company for .NET development; and completely agree with this answer. I do ask lots of questions and I ask them to tell me about using the technologies that I asked about, then even to tell me about a project of their choice outside of what I asked about and what they pieces worked on, issues they ran into, and how they chose to resolve them. I do find my questions are hard and some indepth but I try to stay away from obscure; but lead in by telling them this is a senior dev/team lead level of questions and I don't expect all of the answers to be given. They are conversation starters and jumping off points; and some are key things in our apps we deal with every day. I really want to find out what they don't know and if they have strong foundation and can pick up what we need. You will spend a long time interviewing candidates if you are looking for the unicorn that knows you entire tech stack. If you find someone with a good foundation and the ability to learn they will do well. I find it hard to find someone who has the passion I do just to learn new patterns and architecture but if you find that they will learn a lot and strive to learn more; and that I feel is the unicorn you want. Your tech stack may evolve or grow; you need someone who can do that.
Perhaps you are missing the nunit2 test adapter extension? Works for me at my work. Also AxoCover works for code coverage.
You might want to consider migrating them to .net.
Those are the kind of applications you want to just never change again, to avoid the pain of having to put it back together again. 
Could you implement this as a Visual Studio Code extension? Admittedly it wouldn't be cross-platform, because of the VB6 dependency.
Why would I use Kotlin? I have stressed in the post that either I use dotnet and csharp or if they are not a good fit, I skipp the project. 
I agree, is so much confortable
I think that's the point mate - working with legacy applications in large enterprises means you don't necessarily have the option to do the mass migration/refactor. The approach OP discusses at least enables you to crack on with future development without being hamstrung by ancient tech, and actually enables that migration option safely.
I think that's the point mate - working with legacy applications in large enterprises means you don't necessarily have the option to do the mass migration/refactor. The approach OP discusses at least enables you to crack on with future development without being hamstrung by ancient tech, and actually enables that migration option safely.
Extension? https://marketplace.visualstudio.com/items?itemName=k--kato.intellij-idea-keybindings
Thanks. I‚Äôll look into it next Monday at work. This might actually be useful to our use-case. Thanks again for open sourcing you solution. :thumbsup:
Great book
I need to write a blog post on this as I keep saying it but it's good to get your head around App Service resources. They are effectively one machine and you can have many Web Apps running on that App Service - you're not restricted to one. That way you can save money and still have web apps independent of each other.
[removed]
[removed]
Here are some slides about Vue: [http://vue.bwets.net:8000](http://vue.bwets.net:8000) &amp;#x200B; And the samples: [http://bit.ly/vue-net](http://bit.ly/vue-net) &amp;#x200B;
[removed]
[removed]
Still want to burn it to the ground... &amp;#x200B; I quit a job because they wouldn't migrate a vb / asp application to .net. I spent a long weekend doing 18 hour days getting the entire backend converted and still couldn't sell it to them, all we needed to do was convert the lightweight asp html to webforms. &amp;#x200B; &amp;#x200B;
True on being able to crack on, but it doesn't move any more forward with safety on migration. Visual studio has a wizard to convert your app to .net pretty easily, but regardless it's a decent end to end after its converted.
Its milliseconds not seconds and no you don't notice it.
Julie Lerman's EF Core courses on Pluralsight
It kind of sounds like you need to know how a database / SQL works. Work that out, then you can work out the twists that EF does to it. Query analyser works wonders for this. I worked with a guy that just knew hibernate and had no actual idea about databases, he could stumble through but his life would have been much easier if he actually knew the fundamentals. &amp;#x200B; n.b. Use the in memory provider instead of SQL lite?
I don't know if you actually ever tried to do this on a large application. Admittedly I haven't done this since maybe 2005 or so, but it didn't workout very well for us.
Yeah the conversion wasn't very..smooth in the 2005 era for myself either. Particularly if the VB6 application used a lot of Win32 API calls, DotNet doesn't like that one bit..
It's actually not much different. I recently had to do one, and actually had to use VS 2005 (or 08?). Then was able to step up to .net latest. Not exactly pleasant.
Sure. However, in the real world, when you have a 100,000 lines of VB6 code in a project, you actually DO want to change it. Slowly. One bit at a time. As you move it over piece by piece, and patch it up so it calls back and forth between .NET.
Why? It's MSBuild.
&gt; what could POSSIBLE be wrong Typo here
I wonder if you can be a little more specific about what's throwing you off so far? Or is it just pretty much all of it? &amp;#x200B; MVC can be a little weird when you first pick it up, due to all the moving parts and multiple ways to implement things. Saying that it is also possible to keep things relatively simple so you can get up to speed quickly. &amp;#x200B; What kind of application are you building? Is it mostly forms and CRUD operations or are there more complex business logic to tackle?
But [bing.com](https://bing.com) (from microsoft) has been going to netcore
I wish I wasn't interested in this.
Following up on my comment yesterday, I did get it to work with .NET Core in Visual Studio 2019, Preview 4.0. I had to put the visualizer DLLs in C:\Users\&lt;username&gt;\Documents\Visual Studio 2019\Visualizers\\*netstandard2.0* I did try putting them in the bin directory of my project, but VS doesn't look there. But I think that makes sense when you consider that the visualizer DLLs are more of a plug-in to Visual Studio, rather than really part of your application.
I just specced out my own dev PC. It's a bit more than you specced here. For \*me\* I got a Zotac 1060 6Gb with 3-DP, one HDMI (2.x) and DVI. It comfortably runs 2x 1440p (one 21:9) and 2x4k monitors. In fact it's probably over-specced for dev if I'm honest. You really don't need anything crazy if you're not \*gaming\* in 4k...look for \*fairly\* recent (so 1060 / 980 at a pinch) but just lots of ports on the back. Yeah using a service like NZXT BLD will get you a decent spec without anything you \*don't\* need (so no crazy GFX / gaming motherboard etc...) This is the spec I recently got [https://1drv.ms/u/s!Ap2OEgQ4A-EOgfICJhQuEACp6U0f8A](https://1drv.ms/u/s!Ap2OEgQ4A-EOgfICJhQuEACp6U0f8A) came in at ¬£1800 GBP...but I DID add a 500Gb NVMe 970 Pro for about ¬£140 boot / code drive as it's super fast. Depends on who's building it but often getting base Memory &amp; SDD and buying from Amazon later can be cheaper if that's a major concern. 
EF is an abstraction that will pay a performance penalty compared to custom SQL code. I've been down the route of having EF query an object with many includes and look into the performance only to find out we were returning too much data. Returning too much data is from the SQL performance tuning best practices from decades ago. You also have to factor in ethernet bandwidth because the amount of data returned over ethernet includes the size and performance of the ethernet itself (You can return a 1gb table faster on a 1gb ethernet connection than a 100mb connection). &amp;#x200B; Repository patterns break down in larger applications because you start to have to include so many to get your data. Look into domain driven design to build services and data access around domains versus tables. &amp;#x200B; Lastly, I do love EF, it has a use in almost all .net application I write these days. I still have to know how to tune SQL. I recently tuned a query that looked over 5 million rows from 1 minute to 2 seconds. I had to alter the SQL Schema to increase that performance, nothing I could do to optimize the EF code without changing the storage of my data.
NVidia GTX 1060 6gb with do 2 4K monitors at 60hz. I have that and another 1080p monitor connected with no issues. I had an older video card that could do 1 4k@60, it had weird issues with some Windows 10 and Office animations. 32gb of ram is a lot, 16gb I would say is minimum. I run 24gb, have a VM running all the time, SQL server in the background and the biggest memory usage for me is Chrome. 512gb ssd for your main drive is best, 256 is easy to fill up with installs and updates. Any processor with 4 or more physical cores are fine. My build is a Ryzen CPU, motherboard, and using an existing case and power supply I built the PC for about $600. 
Im taking 70-487 in two weeks, good reminder for me to practice
Yeah it seems like you can't get an MCSD now unless you're an azure guy or a mobile app guy. I got my MCSA on Saturday just past, as a web dev not using azure presently I'm kinda "meh" about getting through the MCSD now.. And yes, revision resources for most of the exams are a joke, no recent book for 486, and the 483 one was riddled with mistakes!
If you looked at it, let me know. Been at this issue for a while now. 
If it were me I'd stick with the 32gb, visual studio gets heaver every year, and VS + SSMS with chrome open on stack overflow is a fairly real use case
I have a 1440p and a 4k 60hz on an ultrabook with an old i7 no graphics card and it works fine.... I don't think the graphics card is that important for development, I do not do any video or picture editing though.
is this confirmed for C# 8.0? i thought it wouldn‚Äòt make it yet.
It is not confirmed for 8.0 but is possible for some minor 8.x patch as they have started implementation.
Ah fuckit. Dude I completely empathize. I ain't even gonna argue. I'm sorry. I hope you're in a better place now
i really like this. saw the proposal a couple years ago. hopefully it comes soon üòé
Part of my issue is that my laptop supports an older HDMI spec, and that the switching from hybrid to discrete card is not in my control; there are often stutters and pauses while the machine automatically switches. Further, on resume from sleep or even unlocking, the windows move around and take *forever* to readjust at the 4K resolution.
Me too. My company has a shit load of VB6 in a legacy project that we're actively replacing, but still have to support the legacy stuff as it's implemented. What a nightmare. 
Misleading headline. It might happen but it's not on any roadmap that I've seen. 
 [https://github.com/dotnet/csharplang/milestone/8](https://github.com/dotnet/csharplang/milestone/8) 
I'm not familiar with VB6 interop with .NET. How does that work?
Yep üòÇ
Unfortunately that tag is misleading for C#. Items get that before they decide if it actually is going to be part of the actual roadmap. At best it means "we might think about it". 
VB6 is COM. So, it's COM interop.
You are 100% wrong about that. &amp;#x200B; Records are way past proposal status, and was originally considered for 7.0 but was postponed because of the amount of work needed. [https://github.com/dotnet/csharplang/blob/master/proposals/records.md](https://github.com/dotnet/csharplang/blob/master/proposals/records.md) 
From what I heard, 70-357 is primarily just Windows Phone development, which is unfortunately considering WP is basically gone. If you still want to go that direction, your best bet is going to be to get familiar with UWP/WPF development, since they're basically the same, and then get a [practice test](https://us.mindhub.com/70-357-developing-mobile-apps-microsoft-official/p/MU-70-357?utm_source=microsoft%3futm_source%3dmicrosoft&amp;utm_medium=certpage&amp;utm_campaign=msofficialpractice) and look up any topics you're unfamilar with.
Look at the status of the ticket: https://github.com/dotnet/csharplang/issues/39 It still needs to go through the LDM process and have a formal specification written. Adding something to C# is an intentionally hard process and this is a very complicated feature. 
Visual studio is still x86 and 2GB limited though.
Gray text on a white background is painful to read.
Forgive me if my understanding is incorrect, I thought the x86 addressing limit was 4gb, and I also thought that vs post 2015 was 64 bit. I can say for definite that the standard work issued laptops with 8gb ram cannot comfortably run vs 2015, SSMS 2012 and outlook at the same time. It works reasonably well in 16 (which I'd agree is the minimum). I've not used 2017 for real yet, so I was assuming the worst. 
70-487 is a bugger, but achievable. Best advice is to read the requirements list and work through MS tutorials on those subjects. And practice, a LOT
Yea I know I need to change the styling. Haven't gotten around to it yet üôèüèª
So I was a bit wrong. x86 programs by default are limited to 2GB of ram. The 4GB is possible but requires the program to be compiled as LargeAddressAware. Visual studio did not compile like that until 2015, and it requires the OS to be x64 to use that flag. There's still no x64 version of VS
Use routing attributes instead. Much easier. Seems to work a lot better than config. 
So the title is a lie.
No but if you have trouble reading the xin 8.x, then I can understand your confusion.
You said yourself: &gt; but is possible for some minor 8.x patch as they have started implementation. It's possible. It's likely. But it's not certain.
Fair point üëçüèª
No problem. It's a classic case of click-bait articles.
We had 4 people that were going to do the MCSD by taking the html, c#, and mvc exams. Now that they've changed this yet again, we've decided to just forget it. We tried azure for 6 months and had to ditch it and retool on AWS. No sense in us doing this cert if its really useless for us. MS really needs to get their heads out of their asses with their cert paths. No good exam resources, their classes are a joke, and they keep changing the requirements...
I'm gonna have to disagree with you there as I am the author. I had no intention to create a click bait title, just wanted to share some exciting news üòä
The inclusion of advertisement and affiliate links is just a bonus.
Yes. And it is an honest recommendation for a good book. :) &amp;#x200B; During the lifetime of the blog about 3 months, and about 4.000 page views, I have made a whopping 1.02$ on the book recommendations. If you want to feel butthurt about that, please go ahead.
I really like this feature and the one line declaration. The only thing I think that is may be missing is the ability or option to guard against null being assigned in the constructor. Does anyone know if records will support such a constraint some how?
I hadn't heard about this. What is the exact language? On the MCSA page it says you get 70-480 or 70-483 and 70-486 in either case. So it seems like for the MCSD you could just get the other one (if you got 70-480 you could get 70-483 for example). Am I reading this wrong? 
That is _worlds_ better. Thank you.
No thank you! I appreciate the feedback!
That‚Äôs actually a hosted version of IFSharp
[Official MCSD Page](https://www.microsoft.com/en-us/learning/mcsd-app-builder-certification.aspx) &gt; If you have satisfied Step 1 ‚Äì Skills by earning an MCSA: Web Applications certification, you will need to take an exam other than 70-480 or 70-483 to earn your MCSD: App Builder certification. They only added this restriction a couple weeks ago. Until then, their own [official forum](https://trainingsupport.microsoft.com/en-us/mcp/forum/mcp_cert-mcp_certif?tab=Threads) "Microsoft Agents" were giving out wrong information. So some people took these certs and are now being told "Lol too bad." Which is ridiculous. Luckily for me, I only prepped for the exam and caught the change right before registering for it. Still shitty. With how challenging 70-486 was, and since you can't take 40-783 anymore, it looks like the easiest path to MCSD is 357 + 783 then 480 as elective imo.
I‚Äôm looking to replace my 8 year old Dell Precision tower. This spec looks to be pretty much what I‚Äôm looking at. Where did you manage to buy this for ¬£1800?
Hey I got it on PCSpecialist they seem to be pretty reasonable for custom machines. [https://www.pcspecialist.co.uk](https://www.pcspecialist.co.uk)
Thanks, I failed to read the top blurb and went straight to the cert steps. I've actually developed and published a native Windows Phone app and I still don't want to take that mobile apps course. 
Looks really useful, I actually have a need for this for a project my team is developing. FYI I'm getting a 503 @ http://www.tyrrrz.me/Projects/CliWrap, and you should consider setting your site up for https. If you'd like some help with setting up your website, PM me. I'm working on a SaaS product to automate web server setup on VPSes and I am currently looking for alpha users!
Does it work with UWP apps?
Slightly off subject, but in my opinion if you haven't bought your monitors yet, consider 2560x1600 monitors instead of 4k ones. When you are programming, vertical screen real estate is everything. What would be even better would be if someone actually produced a monitor that worked well in portrait mode.
I used to work at Epic, where the millions of lines of client code are written in VB6. A transition to a C# web framework began more than a decade ago, but the conversion for such a massive codebase is slow, and in the year that I left we still produced more new lines of VB6 than C#. I would have been super interested in this (using the VB6 IDE was pretty painful at times), although I'm glad to no longer be in that position.
i started reading this today, really good book. thank you
Do you mind if I potentially use this in an article I'm writing? What an awesome library!
I've got one that rotates and isn't awful, but i do most work on a 4k tv that's got decent inputs because my sight isn't great
Thank you!