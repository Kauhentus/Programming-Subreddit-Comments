Depends on what you mean by 2nd job. If you only have like 1 year of experience, the expectations are going to be roughly the same, likewise if your 2nd job is 10 years later, you will be aiming more for a senior level job and need to be an expert in certain things. If you are looking for more of a middle tier 3-5 year type job, you will need to be more specialized. The job posting itself will tell you what you need to have experience in. When working in your first job, keep an eye on what jobs pop up in your area. Pay attention to what skills they are looking for, and what sort of jobs you might be interested down the road. If your current position isn't giving you the exact type of experience you want, you may need to start some side projects. For example, if you want to get into web development but your current job is more back office data stuff, start some hobby websites of your own. You should be able to give examples of projects you worked on, problems you encountered and your solutions to them. 
I'm not sure what you want me to fix ? EDIT: Oh sure I get it, I declare an empty object. Fixed, thanks.
I can only guess but in a couple years I would think it would shift to being the norm for new projects. There will still be a lot of apps under continued development in 4.x though. It took 2-3 years for people to shift from classic ASP to .Net when it first came out.
Yeah I was thinking around the 2-3 years of experience timeframe for a 2nd job. Good advice to keep an eye on local job listings, as I guess the industry is going to demand different things in different locations :)
Cool, thanks for the insights :)
Seems to me like using a dictionary would be more elegant. foreach(var propName in PropertyNames) { myDictionary[propName] = Tags.ExtractFromText(text, Tags.SectionName) } If the data file is line-by-line, then should probably loop through the lines of the file instead of re-scanning the whole file for each property name. 
Could you do something like this in your startup? #if DEBUG app.UseHangfireDashboard(); #endif 
See this: http://docs.hangfire.io/en/latest/deployment-to-production/making-aspnet-app-always-running.html
my tried and true way is to attach to the process, and then trivially modify the web.config file. next request will hit your breakpoints.
I'm stuck on 4.0 still because my damn web host refuses to upgrade and the president of the org doesn't want to switch to a new host. SIGH. 
I think it only works on localhost by default anyway?
I was literally looking at exactly that today. No there isn't, the default implementation of the Dashboard adds the LocalRequestsOnlyAuthorizationFilter to the Authorisation collection in the constructor of the DashboardOptions. The quickest way that I can see to remove that is to use the overload that has dashboard options app.UseHangfireDashboard("/hangfire", new DashboardOptions() { Authorization = new List&lt;IDashboardAuthorizationFilter&gt;() }); 
Without configuring authorization? No. See http://docs.hangfire.io/en/latest/configuration/using-dashboard.html#configuring-authorization &gt; To make it secure by default, only local requests are allowed, however you can change this by passing your own implementations of the IDashboardAuthorizationFilter interface, whose Authorize method is used to allow or prohibit a request. We use something like: app.UseHangfireDashboard("/hangfire", new DashboardOptions() { Authorization = new[] { new AllowAllDashboardAuthorizationFilter() } }); public class AllowAllDashboardAuthorizationFilter : IDashboardAuthorizationFilter { public bool Authorize(DashboardContext context) { return true; } }
Yeah I agree that this is kinda annoying. Trying to find stuff for 1.1 and all you find is for 1.0 which is very different, same thing with most tutorials being made with 1.0 so you have to figure out how to configure stuff yourself. That being said, can't wait for 2.0 and let it mature a bit.
Currently the 2.0 tooling isn't out. So targeting .net standard above 1.3 (I think anyway but I'm on my phone and don't feel like looking up the version chart) causes it to not work on .net 4.6.1, let alone earlier versions. Once the tooling update happens, and assuming you don't need anything outside the standard .net library, you should be targeting the lowest level of it that you can get away with. It makes porting between platforms about 10 times easier than previous approaches that they've come up with.
Does it still not work if you specify a duration greater than zero? It also seems like you have to specify the TargetProperty as (Button.Foreground).(SolidColorBrush.Color)
That would be a good way to model it for sure. I only chose a _part_ of the problem to illustrate the solution to. It gets more complex with multiline tags. So consider: #BookName#Foobar #ChapterName#Adventures of Foo #SectionName#Introducing Foo #SectionText# Once upon a time, there lived in the village of zoosha[1] a girl named Foo #SectionFooter# - [1]: Short for Zoola Shateau #SectionName#Foo's Parents #SectionText# . . . #SectionName#... To top it all off, these aren't plain text files, they're actually part of a word document.
I still can't see any way to tell which frameworks a package supports unless it lists the dependencies by framework. e.g. https://preview.nuget.org/packages/Akka/ only supports net45, but I can't see that anywhere on the page.
I've tested it with a time in there. I think my target type was probably just wrong. I'll test it in the morning and let you know. Thank you for the assistance. 
It's because that package did not specify any dependencies, and it does not provide a dependency per framework. It's up to the package authors.
You have a lot of choices here. I have modeled the caching layer in several ways in the past: * As a dependency or private state on a controller * As a dependency or private state on a business layer service * As a dependency or private state on a data access layer service To me the decision that matters the most here is how your application and layers will be used. In general though there are several tradeoffs to keep in mind: * Modeling at the Controller level: Strongest tie between the data you cache and the reason why. Gives you strong control to evict or refresh the cache when CRUD operations in your application take effect. Doesn't scale well (in developer productivity) with many views (assuming different controllers per views), can cause more confusion than other approaches, and does not work well if many data changes occur in the data outside of the web application. * Modeling at the business layer: Assuming business layer services manage the crud operations for your application, this gives you the best control of keeping the cache up-to-date with data changes. The caching can also be seamless to the application code (I have often created caching service wrappers that wrap the underlying service, so that caching can be added or removed with one line of code for a given service and no application changes). This is a good approach if you have a web application and a stand alone API on a different server/instance that the web application defers to for all of its business logic and data access. * Modeling at the data access tier: If you have multiple business logic layers that share a data access layer, then this approach for caching might be appropriate. Its very similar overall to caching at the business logic layer, and keeps your business logic clear of more data access concerns. The data access layer modeling for caching is the one I see used the most often, though I have favored modeling it at the business logic layer myself. I find being able to manage caching concerns at the business layer makes the business layer easier to reason about as what you cache often depends on the process you are trying to accomplish. Caching at the data access layer usually involves a lot of heuristic guessing for what ends up in the cache and for how long because that code doesn't have any understanding of what is done with the data or why. Example: Caching active products in a product query service is probably about as efficient as caching all recently accessed products in from the data layer, but the latency will occasionally be higher accessing active products if they haven't recently been used with the data layer approach.
It would be possible to multi-target .net standard and .net framework and then later just target .net standard once 2.0 is released
JSON is better in most cases. You can still have strong typed interface that interact via JSON. If you are working on lower level, JSON is better in most cases. If you are working on higher level, you don't mind what format is used under the hood. In this case JSON is again better because it is smaller and can be serialized/parsed fairly faster. And finally, it's a native format for JS which is frequently is a single consumer of this service.
It would be fairly trivial yes. For Queues, offload to rabbit message queue or MSMQ. For the service... something like this: using System; using System.IO.Ports; namespace SerialPorts { class Program { static void Main(string[] args) { SerialPort myPort = new SerialPort(); myPort.DataReceived += MyPortDataReceived; myPort.PortName = "COM45"; myPort.BaudRate = 4800; myPort.DataBits = 8; myPort.Parity = Parity.None; myPort.StopBits = StopBits.One; myPort.Open(); Console.ReadLine(); myPort.Close(); } static void MyPortDataReceived(object sender, SerialDataReceivedEventArgs e) { var myPort = sender as SerialPort; Console.WriteLine(myPort.ReadLine()); } } } 
I tend to agree with the other post. I took our 4 year old service framework and rewrote from the ground up and the dependency injection pattern for pretty much everything is BL components. There are some basic caching abstractions so we can change our cache to be memory, redis, mongo, etc, but generally it's abstracted out to how the framework and services use it. A good example is the actual implementors OF the cache. The controllers make a call to the DI component that handles requests to the back-end and determined is it or isn't it in the cache and how/what to request. It's hard to explain fully but the re-write came from how we were actually using the services and framework. The best I can say about DI is take a step back and think how you will use each interface you're creating. If only one thing is going to use it then maybe it's overkill. DI helps prevent code sprawl and helps creating functional units. Whatever you're going to build isn't going to be perfect and will be used slightly in a different way than was intended on initial implementation. Just try to think of the best case scenario for when you need to change something, what's the most straightforward way to create whatever you're doing that changing it won't be a headache. 
JSON has no standardized schema yet, case closed. XML can be also parsed faster, using **proper** tools.
&gt; don't feel like looking up the version chart) Life used to be simpler
This is intended behavior. Remember the saying "If a tree falls in a forest and nobody is around, does it make a sound?" If nobody is using your website, does it need to run? The answer is usually no, so ASP.NET waits until someone loads a page to actually start it up. The best solution, I think, is to accept that you'll need a first page load, but then you can configure an AppPool in IIS to ensure the website is never shut down, so your website can always be running, if you want to do things like have background threads schedule background work at arbitrary times in the future. I use this approach myself. As far as the first page load goes, remember that even if your app isn't running and you miss something you wanted to do, you can check on startup to see if you missed anything and catch up, and that's usually good enough in my experience. The exception is if the work you wanted to do is intensive especially if it blocks the page load (if it doesn't need to, throw it into a background thread).
`a ?? b` is basically shorthand for `a != null ? a : b`, however in a special case if `b` is a throw statement and `a` is null the throw happens. VS will automatically detect null checks before member accesses and offer to convert into `object?.property` shorthand, but I am not sure if it does `??` for you. It sounds like ReSharper tried to do the logical thing and extend the built in correction to add `??` support but went a little too far.
Well it seems I might get a workaround by using HKEY_CURRENT_USER\SOFTWARE\Microsoft\Windows NT\CurrentVersion\TWAIN and modify the Default Source key Edit: A link worth visiting about this subject: http://www.gssezisoft.com/main/2013/06/selecting-the-default-twain-source/ I'll work on this tomorrow and will tell you if this helped. 
Have you tried .\img\image.png? If you are not using a '.' (which means the current directory) and simply using '\' first, you are telling the program that the image is in c:\img\image.png, for example. 
I just got so excited reading your post and tried it, but it didn't work... :( Thank you so much for the idea, though! I was for sure that was it when I read it. Maybe it has to do with the template or the button? Here's a sample of the XAML code for one of the buttons. The template: &lt;ControlTemplate TargetType="{x:Type Button}"&gt; &lt;Grid x:Name="Container" HorizontalAlignment="Left" Height="20" VerticalAlignment="Top" Width="20"&gt; &lt;VisualStateManager.VisualStateGroups&gt; &lt;VisualStateGroup x:Name="CommonStates"&gt; &lt;VisualState x:Name="Normal"/&gt; &lt;VisualState x:Name="MouseOver"&gt; &lt;Storyboard&gt; &lt;ColorAnimationUsingKeyFrames Storyboard.TargetProperty="(Shape.Fill).(SolidColorBrush.Color)" Storyboard.TargetName="Background"&gt; &lt;EasingColorKeyFrame KeyTime="0" Value="#FF464646"/&gt; &lt;/ColorAnimationUsingKeyFrames&gt; &lt;/Storyboard&gt; &lt;/VisualState&gt; &lt;VisualState x:Name="Pressed"&gt; &lt;Storyboard&gt; &lt;ColorAnimationUsingKeyFrames Storyboard.TargetProperty="(Shape.Fill).(SolidColorBrush.Color)" Storyboard.TargetName="Background"&gt; &lt;EasingColorKeyFrame KeyTime="0" Value="#FF282828"/&gt; &lt;/ColorAnimationUsingKeyFrames&gt; &lt;/Storyboard&gt; &lt;/VisualState&gt; &lt;VisualState x:Name="Disabled"/&gt; &lt;/VisualStateGroup&gt; &lt;/VisualStateManager.VisualStateGroups&gt; &lt;Rectangle x:Name="Background" HorizontalAlignment="Left" Height="20" Stroke="Black" VerticalAlignment="Top" Width="20" Fill="#FF323232"/&gt; &lt;Image x:Name="Image" HorizontalAlignment="Left" Height="16" VerticalAlignment="Top" Width="16" Source="{Binding Content, RelativeSource={RelativeSource TemplatedParent}}" Margin="1.89,1.718,0,0"/&gt; &lt;/Grid&gt; &lt;ControlTemplate.Triggers&gt; &lt;Trigger Property="IsKeyboardFocused" Value="true"/&gt; &lt;Trigger Property="ToggleButton.IsChecked" Value="true"/&gt; &lt;Trigger Property="IsEnabled" Value="false"&gt; &lt;Setter Property="Foreground" Value="#ADADAD"/&gt; &lt;/Trigger&gt; &lt;/ControlTemplate.Triggers&gt; &lt;/ControlTemplate&gt; The button itself: &lt;Button x:Name="buttonClose" Height="20" Margin="501,2,2,0" VerticalAlignment="Top" Style="{DynamicResource ImageButton}" Click="buttonClose_Click" Content=".\Resources\clear_16x16.png"/&gt;
Ah, sorry I couldn't help. I haven't messed with WPF / XAML in a while. The only thing I could see working for you is setting the background image of the button &lt;Button Content="button" HorizontalAlignment="Left" Margin="87,103,0,0" VerticalAlignment="Top" Width="238" Height="89"&gt; &lt;Button.Background&gt; &lt;ImageBrush ImageSource="pack://siteoforigin:,,,/Resources/myImage.jpg"/&gt; &lt;/Button.Background&gt; &lt;/Button&gt; Probably not what you're looking for.
I think you're approaching this a bit too manually, in that it sounds like you're trying to wire up these nested dependencies manually rather than leveraging the appropriate patterns and approaches that are supported within the DI container. Ultimately, if any part of your application knows that their dealing with a cache rather than the actual implementation, I think that's a code smell and you should try and find a better solution that doesn't have that additional dependency. Caching is a cross cutting concern, as are things like logging and security, that is generally needed in multiple places throughout an application. There is a pattern that is very useful for these concerns called the Decorator pattern which allows you to take a class that has some base functionality and "decorate" it with some additional functionality provided by the decorating class. Essentially you implement the same interface and do some work for a given call before calling the same method of the base class. An extension to the decorator pattern that a number of DI containers support is called Interception, which allows you to get in the middle of a call in an indirect, generic manner and not have to worry about implementing a specific interface. Both decorators and interceptors are well supported in most modern DI containers and allow you to have separation of concerns between your caching logic and the base functionality. There are also a number of benefits to letting the DI container handle this for you. First, its easy to have the same cache used in multiple places and across different consuming classes. Second, the determination of whether to use the cache is not handled by individual classes but by higher level application logic that is closer to the composition root. Third, is that is is quite trivial to wire this up, sometimes as simple as MyContainer.Decorate&lt;IRepository, MyCachingRepository&gt;() with MyCachingRepository being having an IRepository that is constructor injected with the "original" resolution of IRepository from the container. http://blog.ploeh.dk/2010/04/07/DependencyInjectionisLooseCoupling/ http://structuremap.github.io/interception-and-decorators/
Try to use TemplateBinding inside your template for the content property. https://blogs.msdn.microsoft.com/jitghosh/2007/12/27/wpf-control-templates-an-overview/ 
If you really want to be current you should also look into building UWP apps.
Does UWP use C# and XAML? 
Use build server to create builds (installations), keep your local Visual Studio for development only. The biggest issue is NuGet because it is not reliable and makes too many dependency and version conflicts, not mention the disk space bloat due to downloading tons of useless target versions. Unfortunately it is a "cool" technology we have to live with. As for Visual Studio itself, using SSD instead of HDD is strongly recommended. There is also 'Lightweight Solution Load' feature in VS 2017 with mixed results. Maybe it is time to reconsider need of one big solution file and split it into multiple logical modules.
I'll look into this ASAP. Thank you for the suggestion!
Yes it does but it’s a totally different way to work from WPF desktop apps I’d recommend to create a few apps using WPF and then move on to UWP. Make sure to read up on data binding in XAML. That’s one of the biggest features of it
Came here to write exactly your comment.
That's like the basic stuff ( using teamcity/Jenkins for 10 years) . Still didn't answer my question because you still need to setup the build process on the build server. What is the optimal msbuild flags? What is the best scripting tool (make, cake, fake, powershell, bat???) what is the best way to organize the bin output? How to speed up the build on ci?
&gt;Null References There is no option type in the framework itself. F# has them and there are various third-party implementations, but I can't make any specific recommendations. There is ongoing work in providing C# with functional-style pattern matching (partially available in C#7) and non-null references (still in the planning phase). &gt;Enums Still just constants. The only remotely related change is the addition of extension methods a while ago, which can be used to attach pseudo-instance methods to enums. You're free to create your own enum-like types of course, but there is no language support. &gt;Functional Programming As far as I know, LINQ provides equivalent methods just under different names. `map` -&gt; `Select`, `filter` -&gt; `Where`, `reduce` -&gt; `Aggregate`, etc. The equivalent code (substituting null references for the `Optional&lt;T&gt;`) would be something like var journal = journals.Where(j =&gt; j.FlagNormalCompletion.Equals("1")).FirstOrDefault(); return journal?.Select(j =&gt; j) ?? journals.FirstOrDefault()?.Select(j =&gt; j);
Yes it does. UWP is the new UI framework for Windows 10 apps.
Lightweight thing works like a shit ( don't work at all) I've always using ssd (local and vm's on azure) still sucks until you have &lt; 4 projects:)) Yep the splitting is the good way. But here comes the pain with the versioning of your own libraries. Btw one more good option to speed up the build on huge solutions is remove shitty "copy to output" in reference properties. 
I use MSBuild scripts only (having thousands lines) but this is very individual. If you're MSDN subscriber I'd definitely consider to try [Visual Studio Online build services](https://www.visualstudio.com/en-us/docs/build/get-started/ci-cd-part-1) You can speed up the CI build by [MSBuild parallel features](https://msdn.microsoft.com/en-us/library/bb651793.aspx?) or just by brute force HW configuration, especally the SSD. As for bin output, I usually create many different configurations that are behind the solution configuration scope so I have my own build scripts for that. Basically I use solution build just to create binaries output while configuration files are created completely different way using templates.
**Null References** - a lot different stuff in nuget, you can use what you want. But mostly in use is "elvis" operator "?." and checks for null. Also for C#8 exist possibility of introduction the "not nullable reference types". **Enums** - just constants. **Functional Programming - map, flatMap, filter, reduce, apply, unapply** - LINQ supports all this stuff. **Database Support** - EF works with different databases... and there are a lot of other ORMs **Environment / Profiles** - it exists in .Net
Keep in mind UWP only runs on Win 10. WPF has still more features and is more complete compared with UWP. I would stay on WPF for a while. Learn about databinding (not a standalone feature of UWP) and MVVM then switch to UWP. 
Databinding is actually a key feature of WPF.
You should also familiarize yourself with [.Net Standard and Xaml Standard](http://www.dotnetcurry.com/dotnet/1377/dotnet-standard-2-xaml-standard). Just so you're aware of where things are heading in the near future. Good luck!
That's good options. Btw you'll faced the issue with parallel build on the build machine. New versions of msbuild forget to shut down processes. 
It looks like the target property suggested still yields no results. The foreground color of the buttons content does not change when you click.
The parallel build has (had?) many unexpected issues so I actually never used it. The issue with MSBuild are external processes spawn from it that don't behave correctly. On build server another issue is account you're running the build under. For example, it does not have required share folders access so it hangs and so on. COM privileges (used by Wix) is another nightmare. I also use many own MSBuild tasks that's considered oldfashion these days :-)
BTW You don't have to pay for cloud build time when use VSO. You can run your own [build agent](https://www.visualstudio.com/en-us/docs/build/concepts/agents/pools-queues) for free (that connects to the Microsoft server and receive build tasks) on your server and still take advantage of all VSO build services.
Have you changed it to fit the control you're trying to animate? (Like Textbox.Foreground instead of Button.Foreground)
It is a button i'm trying to edit so no need to change it. I've edited in the entire style if that helps? the mouse leave and mouse enter work without issues
It's been such a long time since I used WPF sadly. ~~Are you sure the storyboard is *starting* it's animation? Maybe try to also animate some other property to see if that's the issue.~~ Nvm I now saw that you know the animation is *firing* You can also try to remove the setter, it maybe doesn't work because it's a reference to a StaticResource?? If any of the things I mentioned aren't the problem then I'm afraid I don't know what to do next.
That's fine. the other animation works without issues so I know it correct. You do have a point that changing the setter might be the issue. but its only a button style so I can't change a button foreground I can only change the setter that sets the buttons foreground. I appreciate the help. Thank you
Go with cake, get the basic build process working locally and then use the cake script on the build server too. That way you will have a repeatable build and it will be down to managing dependencies on your build server. That to me beats fancy gui build steps every time. Try and use something like proget or any local nuget mirror to manage your nuget feed if you are building on prem. You can manage versions and are not pulling deps over potentially slow internet connection. Once that is working, try and play with flags. It is all msbuild underneath and that has plenty of documentation. I don't think there are as many compile options as something like c or c++. Oddly turning off verbose logging saved a noticable amount of time even in a headless build. Tests can be the hardest area to run in parallel. Anything more than basic unit tests that set up some state can interfere with other tests. This may just be because our team did unit tests that were more like integration tests though.
Check out https://github.com/GregRos/OptionalSharp There's several other projects but I can't remember them off hand. If you search for larger used ones, the word monad might help you 
Yeah sounds a lot like our setup. We use cake running msbuild, proget for nuget and npm, and tfs as the ci server Versions, Restores, builds, tests, packages, publishes all consistent across our dot.net projects. Both locally and in the server. 
Entity Framework Core can work on _both_ .net core AND .net full. Use of the word 'core' is a silly branding exercise or something. And Entity Framework Core is a complete from scratch rewrite, and lacks many of the things you would expect. I thought I mostly didn't care for Entity Framework and wouldn't miss all the things it did which I hated anyway, but just yesterday I found out it doesn't support querying views. Like views are some newfangled fad that they're waiting on. Good god. So, caveat emptor. EF6 is certainly still an option. But also, there are other ORMs. Also, regarding .net core, as it sounds like you've been out of the loop. It's stable and good, I'm using it for some production stuff now, with asp .net core, but it's also very much a work in progress. Whether you want to target .net core or .net full is a decision you should make after some consideration, based on what exactly it is you want to do. I've been frustrated by the occasional library that is still hooked to .net full and thus can't be used. .net core 2 should allow such referencing, but still, do your research and it very much depends on what you plan to do as to what the right answer is. 
I've been using fake for a few years now. If it were today, I would take a serious look at cake. I haven't taken a serious look, but it's getting huge traction from the looks of thing. Meanwhile any coworker who goes near fake is scared of the F# syntax. I like F#, but that's me. 
I am not familiar with Optional&lt;T&gt; but if I am seeing this correctly it looks like a class that encapsulates the target value so you are certain the outer object is not null, and it can handle things like generating a default object to replace the null reference. There's a different way I know in .NET that gives a similar result: the `??` operator, which in the expression `a ?? b` will return `a` if a is not null or `b` if a is null. You can also use the `?.` property accessor operator like so: `objectThatMightBeNull?.property` which will do nothing and evaluate to null if `objectThatMightBeNull` is null, otherwise it will evaluate the member reference, which can be a property or function call. You can also combine them, for example: `object?.property ?? defaultValue` which will grab a property on an object, or a default value if you have no object (or the property value is null). I am not familiar with Java enums. The most advanced things I do with .NET enums is to serialize and deserialize the values to strings to store in files. The Enum class has a static method to Parse a string into an enumeration value, and `.ToString()` on an enumeration value will give you the name of the value. You can do all sorts of nifty things with reflection as well, though it's not recommended if you can avoid it since it is slower than not using it in most cases. I am using .NET Core (rewrite of .NET for cross platform and splitting the framework into modular libraries) and the corresponding Entity Framework Core works great for SQL Server and SQLite for me. There are also third-party libraries to plug in a few other databases. The main problem is that SQLite does not support some of the database migration operations EFCore tries to do, but I understand the EFCore team has workarounds for this planned (maybe already implemented, I might be behind in versions).
The local build agent is a nice option, I started using it when I needed a VPN to see a private nuget server.
Cross cutting concerns are something that I'm quite fascinated with. To really have true separation between things like caching or logging, and the code that they're wrapping is certainly a challenge, but its even tougher without the right approach. Although I think AOP frameworks like PostSharp are amazing for handing some of these use cases, modifying the post-compile IL really seemed like just a bit too much "magic" for my tastes. It wasn't until I really got into DI containers and started to truly leverage them that I found a much better solution for cross cutting concerns. Its amazing how much your classes are really doing that you don't realize because its fairly standard to have logging code just peppered everywhere. The other interesting thing is that once you get into interception, most DI frameworks do that by dynamically generating a "Decorator" on the fly, which means that a framework for dynamically generating objects is already included in your project. Although its not something I need often, when I do need it, its a god send.
The json you're parsing is not an array, but rather a single object. Here are a couple changes: DeserializeObject&lt;RecipeDetails&gt;, change ingredList to Ingredients (and you can remove the ingredList class). I am on mobile so it is possible I missed something but you should be good. If you would like for it to come out as a RecipeList, modify the JSON code above by adding square brackets around the outside curlies in Recipe.
I agree with everything you say, except he is just asking which layer to put the caching code in. The decorator pattern is good, but he needs to know if he should decorate a BL service, his DAL, or a controller.
&gt;which layer to put the caching code in OP doesn't give enough information to determine that, and depending on the structure of your services/repository, you could likely leverage caching within each layer. Your DAL could be caching your calls to the DB, which could be leveraged by either a service or a controller. A service could be making API calls of its own, which could dictate caching the service itself (although I'd argue that those calls should be abstracted out and the resulting interface cached). Regardless of what's cached where, wiring it up in the DI container should still be the same; the base implementation of the interface should be configured, then the decorator/interceptor associated with that interface so that the container knows about both implementations and can pass one into the other.
&gt; OP doesn't give enough information to determine that, and depending on the structure of your services/repository, you could likely leverage caching within each layer. My other answer to OP was pretty much that..."Here are the different levels and some pros and cons of each". &gt; Regardless of what's cached where, wiring it up in the DI container should still be the same; the base implementation of the interface should be configured, then the decorator/interceptor associated with that interface so that the container knows about both implementations and can pass one into the other. That's a good approach, though I'd definitely prefer the decorator over an interceptor. Most interceptor APIs/implementations I've seen (which is admittedly not many) make me cringe at the cognitive and runtime overhead involved.
Has anyone used it in a production project of theirs yet? I like how it looks, but the documentation leaves some open questions for me. What is the difference between a C# like DSL and C# in this case? The cake site doesn't have any good examples of build scripts on it that I could find outside of really basic samples.
Yes, I have used it at work. Also, the NUnit project uses it too. AMA.
I've seen it appear in a couple of projects by Octopus Deploy, e.g.: https://github.com/OctopusDeploy/Octostache/blob/master/build.cake
We use it at work and we all love it!
Xamarin also uses it on a few projects, Nancy uses it and several other OSS projects. We use it at work for all our .NET /.NET Core projects 
Really great tool for C# dev. Using it to automate any action related to a project: releasing, building, managing docker, publishing and so on. Thanks!
https://github.com/dotnet/csharplang/blob/master/proposals/nullable-reference-types.md
&gt; I'd definitely prefer the decorator over an interceptor. Most interceptor APIs/implementations I've seen (which is admittedly not many) make me cringe at the cognitive and runtime overhead involved. Unless it's truly generic, then decorators are certainly preferred over interceptors. If there's a ton of runtime overhead for interceptors, then the framework is taking the wrong approach. The ideal way for interceptors to work is that dynamical IL is generated that winds up effectively creating a decorator on the fly with a little reflection and some small template code. Interceptors can be done with more reflection and a system of delegates, which does tend to slow things down.
Why not use a class library project to regroup your basic controllers ? If you're sure you need a private nuget package, you'll have to host [your own nuget feed](https://docs.microsoft.com/en-us/nuget/hosting-packages/overview)
https://www.cuttingedge.it/blogs/steven/pivot/entry.php?id=92 No need to follow exactly his classes and layout, but the idea should be clear from the article. This is an application of the SOLID principles. The key here is that the CLIENTS of your services should define the interfaces that contain the methods they need to call (Dependency Inversion Principle). For example, in an MVC app, you might have a EmployeesListingController which takes an IEmployeeLister which has only a List method that returns a List&lt;EmployeeListingResults&gt;. To implement that IEmployeeLister, you then may have the Data Access Layer implement it directly, or create another layer of code that uses your DAL repositories to implement IEmployeeLister. Now, your high-level details don't depend on low level details. You can create decorators of IEmployeeLister to apply cross-cutting concerns such as logging and validation. It becomes easier to unit test each class too, since you followed the Interface Segregation and Single Responsibility Principles, without losing the ability to add new functionality via decorators or coupling yourself to IQueryable.
&gt; Still just constants. The only remotely related change is the addition of extension methods a while ago, which can be used to attach pseudo-instance methods to enums. You're free to create your own enum-like types of course, but there is no language support. &gt; There's also the ['type safe enum'](https://blog.falafel.com/introducing-type-safe-enum-pattern/) pattern which I've found is gaining popularity, although as you noted there is no language support for this one.
&gt; However now I have moved the job of querying to the repository for a given entity how would I achieve this as the DAL has no knowledge of the domain models, and rightly so. That's a bad practice. If you sure about performance the repository damn well better be giving you the objects you actually want. The domain objects should be at the very bottom of your dependency stack, used and shared by all other layers. Not only for performance, but also so that the repository can use it for validation. 
Is this the only way to approach it, surely there's another way, developers can't be settling for returning everything from a query.
If you build your repository with Tortuga Chain, it will use generics to build SQL statements. You pass in the name of the class and it uses its properties to select columns. There are limitations. For example, it does not natively support joins. Instead expect you to use views and stored procedures. basically you need to think in terms of projections instead of object graphs. https://docevaad.github.io/Chain/Introduction.htm
What do you mean?
If you download Automapper's source it will have an example. But the reason why I had to download Automapper's source was to remove the functionality. Our build server is on an older version, and errors with a nuget that supports core.
This design seems so heavy - Im assuming it's just cause of lack of experience. I don't see how this solves his initial upfront problem of having to change code in 2 places still. If he adds a new method he still has to add a new interface. It's 3x code for the same end result. The only benefit I can determine is its easier for a Test framework ??
It's not the same end result. Yes, you do end up with more classes this way, probably more lines of code that declares classes and inherits interfaces and injects dependencies, all infrastructure. But the amount of logical code doesn't change, and it becomes split into logical places that make it easy to change, read, test, and understand small bits of code at a time. Done correctly, the "glue" code is separated from the code you care about.
I didn't click the link but my repos return Iqueryable so I can filter as required and it seems fine what is the bad part 
For me at the moment Linq2sql still works fine and I don't currently have a reason to switch. If we end up making the leap to .net core then we would need to find a new ORM :) and we will just pick the best one we can find at the time :)
&gt; How to get only required data from a repository method You have a couple of options here. I'm assuming you're using EF. You can create repo methods that take projection expressions as arguments that project the query result into the desired output. E.g. void Main() { var vs = SelectVendors(v =&gt; new { v.VendorId, v.VendorName }); vs.Dump(); } // Define other methods and classes here IEnumerable&lt;T&gt; SelectVendors&lt;T&gt;(Expression&lt;Func&lt;Vendor, T&gt;&gt; projection) { return Vendors.Take(10).Select(projection).ToList(); } The same approach can be used to allow calling code to specify a custom `where` clause. If you're using EF and AutoMapper, you can simplify this further with the use of the [IQueryable extension ](https://github.com/AutoMapper/AutoMapper/wiki/Queryable-Extensions)`ProjectTo&lt;&gt;` which will automatically restrict the query to only select the fields that match those in the generic type parameter. &gt; Another question about the repository, they are only aware of the associated dbset so how would you query cross entity? This is one reason why I'm not a fan of table-centric repositories, personally. I think task-centric ones are more appropriate. Tables in a well-normalized relational database are *related* so it makes no sense to apply an abstraction that fits so poorly with that fact.
Visual Studio is and has always been the easiest Dev environment to set up.
We decided to roll our own authentication system. It wasn't too difficult, and we finished it really quickly. We are finishing testing and going to roll it out to production soon.
Disclaimer: I work on the .NET Agent Team at Dynatrace. Our Agent is fully capable on monitoring .NET Core on Azure. My team implemented support for it a couple of months ago. Here is a guide how in install it for Azure PaaS: https://help.dynatrace.com/infrastructure-monitoring/paas/how-do-i-monitor-microsoft-azure-web-apps/ Not sure what you meant by "Dynatrace didn't seem to understand the app itself", but if you explain, I might be able to help you.
You make a new ASP.NET MVC project. Add your basic controller(s). Add an extension method that register the routes and other configuration for your basic controller(s). Alternatively you can add the WebActivator Nuget package and you can have it auto-register and stuff. Make a NuGet package out of that like this: https://docs.microsoft.com/en-gb/nuget/quickstart/create-and-publish-a-package
This article should be titled something like "Making a custom offline installer for Visual Studio 2017"
Projection sounds exactly what I was looking for, thank you!
Definitely. I've installed environments like Eclipse and well, never again. It's a pretty simple IDE to install. Sure there are a lot of options but it supports several language and usage scenarios but it's pretty easy.
Unless you've got a stupid company that gives you a tiny C drive, and a proxy for all that lovely online installer stuff :( At home it's a few clicks and a big wait. At work it's a few days of fiddling around. Not VSs fault though, but I do which you could download an offline installer.
You can choose to download the ISO from MSDN and drop it onto a disk (DVD/USB). We store our installers on a local network resource. I usually install VS this way because then my colleagues don't have to redownload it from MSDN and can instead copy it across the network in seconds. Also, you're not required to install to C: and can easily install it elsewhere, but this should be done on a case-by-case basis rather than creating a custom installer.
Isn't this pretty much identical to - https://docs.microsoft.com/en-us/visualstudio/install/create-a-network-installation-of-visual-studio ?
Nice to know the aspnet:UseHostHeaderForRequestUrl setting. How about the different in URL schema between Load balancer and IIS setting? In my past project, we use http schema in IIS and https in load balancer, then Request.Url always returns http URL. We mitigated this by using another header or appSetting key. If your solution can work in this case, it would be great :)
Hmm not really. Certainly it's better than Eclipse, NetBeans and other crapware but compared with some of the more polished alternatives I think it's a bold claim. JetBrains products install easily and it's only recently I didn't have to take a lunch break for VS to install.
Not as easy to fully uninstall though. Just fully uninstalled VS 2015 Community so I can have a completely fresh install of VS 2017 community. Definitely took me longer than I expected.
It's not wrong, but there's almost certainly a better way. The goal should be to write a query, either in SQL or another language like LINQ, and get an object back without having to do boilerplate checks like ISDBNULL(). If you want to continue writing SQL or use stored procs, look into [Dapper](https://github.com/StackExchange/Dapper). For your own professional development, I'd recommend you learn EF just because so many places use it.
Yeah... Easiest method seems to be using online installer with options lol.
you might also want to look at [DbUp](https://dbup.github.io/) for dababase migrations. We are using this to deploy migrations for SQL Server and Postgres databases in two different applications.
You need an ORM. Examples include nhibernate or dapper. Google them, read about them and try them out. If you decide dapper has too much sql writing for you, and wanna go down the nhibernate route, look for fluent nhibernate which makes your life really easy when mapping your objects to SQL. Good luck :)
At this point, you gain little over adding extension methods to a DBcontext directly, though. Furthermore, the use of Expressions in this way becomes a lie to the rest of the application - there are many valid Expressions that you could pass as a parameter to that sample function that will throw exceptions when used against any IQueryable that is not an in-memory implementation in the first place. Mark Seemann has a great article about why IQueryable is Tight Coupling on his blog. 
Not for VS2017, thats the problem
Title already taken: https://www.hanselman.com/blog/HowToMakeAnOfflineInstallerForVS2017.aspx ;)
Uh.. I beg to differ. It's a bit of a mess with VS2017, especially if you don't have Win 10 Creator's. For example, whether or not components such as .NET 4.6.2 and .NET 4.7 show up depends on the version of Windows and if you installed them stand-alone or through the installer. In the former case, they might show up as not installed even though they are and work.
Yeah, we had an ISO for 2015 and it was so much easier. I was aware you could install a part of it on a non operating system drive, but for most of VS it's certainly not easy and involves "hacks" that are probably impossible on my work laptop: https://stackoverflow.com/questions/32029751/how-to-install-visual-studio-2015-on-a-different-drive There's also no simple way to get an offline instsller for 2017: https://docs.microsoft.com/en-us/visualstudio/ide/create-an-offline-installation-of-visual-studio 
Like the others say, an orm could help you. Maybe start with [sqlfu](https://github.com/sapiens/SqlFu) since that still close to what you're doing.
Exactly what i was going to write. IQueryable is an implementation detail of your specific implementation of the repository abstraction. Try avoid creating "one for everything" repositories. My suggestion to my team is to keep the dbcontext open to map the whole database (read as "the portion of database actually relevant for the specific application") and use repositories as connection between business logic holders and the dbcontext. An example. Let's assume that we are working on a asp.net mvc application and that the controllers are the holders of the business logic (wrong, but for the example sake). Let's also assume we have an `OrderController`. This controller, whose responsibility is to display a list of orders matching a certain filter and display a detailed view of a given order, will have as a despondency a `IOrderRepository` (it should use a service...). This repository interface has a method to retrieve orders matching a set of filters and to retrieve a certain order by identifier. Note, the Order I am using here is your business entity. It has no knowledge of its storing media, it might even have some behaviors. Also, the list of Orders is returned into a *finalized* collection, my preference is `IReadOnlyList&lt;Order&gt;`, but make sure the list is finalized and non mutable. When it comes to `IOrderRepository`, we can have several implementations. One of these could be a event source storage based on MongoDB, Azure DocumentDB or AWS DynamoDB. But we assume we are working with Entity Framework, so we create an `EntityFrameworkOrderRepository` that implements the interface and accepts a dbcontext as dependency. This class will transform the list of filters into valid expressions to be combined to generate the query. Also, the dbcontext will use an Order entity that is not the same as the Order business entity. Mapping tools like AutoMapper come to help. The advantages of using this pattern (although verbose) are: * no leaking of implementation details through the abstraction * possibility to use different storage media without affecting the layers above * real DIP compliance since the `OrderController` is the one dictating what it needs to perform its job. * the repository is acting as an adapter between the controller and the dbcontext * possibility to have a single (db) transaction flowing across multiple repositories 
In case I was unclear, I was not advocating exposing IQueryable outside the repo. To use and expose `ProjectTo&lt;&gt;`, you would just expose the generic type parameter as a parameter of the repo method, which would call `ProjectTo&lt;&gt;` internally. Your point about the `Expression&lt;&gt;` parameter being a lie is fair. My response that would be that while it is *literally* true, that most .NET developers who are familiar with LINQ to Entities will implicitly understand that in the context of a repo, an `Expression&lt;&gt;` parameter is going to be passed to LINQ to Entities. Extension methods on the DBContext are another way to skin that cat, but harder to swap implementations via DI compared to the more straightforward approach of a task-specific repo class that implements an interface.
What you're looking for is an ORM tool (the wikipedia article has a pretty good explanation https://en.wikipedia.org/wiki/Object-relational_mapping). Entity framework is the most popular in the .NET ecosystem but others are available. EF presents a virtual object database which you can query using linq. Those queries are converted into SQL by EF behind the scenes (though you can manually write the SQL if you wish). There are plenty of resources available online for EF and for your own professional development you'd probably benefit from understanding it.
that's what the X-Forwarded-For, X-Forwarded-Proto, and X-Forwarded-Port are for. Load balancers usually set those request headers (AWS ALB and ELB do and you can configure nginx to do so as well) https://stackoverflow.com/a/41335628/3093703
&gt; One issue we're having is that the Kestrel / IIS server shuts down / goes to sleep after 24 hours no matter what we do. That is because your IIS is configured to do so. You must adjust the tools you use to your requirements. Configure your IIS. Your default application pool is by default set to recycle every 1740 minutes.
IMHO - I would suggest looking at Dapper prior to EF if you are using an existing database. EF is more convention over configuration and its highly unlikely your convention matches theirs...
Accurate advice despite condescending delivery. 
Is there anyone using Windows Docker containers? I have seen a lot of Linux containers for .NET Core Apps but not Windows.
I think people are still doing exploratory work. It's still pretty new. For example, I think windows server nano is now becoming the preferred OS for containers where as the Linux world has had Alpine for ages. It still feels v.bleeding edge.
You're right. I've had a look and it's absolutely ridiculous the hoops that you have to jump through to install it. I've asked them on Twitter and sent an email to my MS contact in the VS team about it because it's insane. Why would anyone want to install local certs in order to install a piece of software? Sounds like a massive hack to me.
&gt; In general it is advised to learn the basics and configuration of the software you use. Ha! That'll teach him to learn by seeking aid from other possibly more experienced developers!
Just having a bot request the homepage every 5-10 minutes is easy and will keep the AppPool alive.
Interface methods! Wow, this is first I've heard of that. That just seems like a bad idea, I always thought of interfaces as contracts i.e. the what, not the how. Surely this blurs the line between interfaces and abstract classes, and introduces similar testing nightmares. Am I missing something? Change my view.
That was Java's solution for extension methods, I guess it might be driven by Xamarin requirements to better interoperate with Android APIs, now that Google is adding Java 8 support.
The use case for default interface methods is to resolve the problem where adding new methods to an interface is a breaking change. Currently new methods can only be added to interfaces via extension methods, but extension methods cannot be overridden. As an example, if the extension methods of IEnumerable&lt;T&gt; were implemented as default interface methods instead, then classes the implemented IEnumerable&lt;T&gt; could provide overridden implementations of those methods. Currently that isn't possible. Obviously people will find other ways to use default interface methods. Some of those ways might be good and some might be bad.
That is my impression from industry trends as well. Container and container management software on Linux are very mature. Especially if you utilize something like OpenShift.
Interface methods can't change the state of the implementing class in other ways it's consumers could. They can't access or introduce fields. They are just like extension methods but more targeted. They solve otherwise hard to fix issues that arise when you want to extend an interface that is already used by many classes. Just like ref, do while, reflection, struct... you can find entire libraries that don't use/need them but they have their places that would suck without them. 
&gt; &gt; They can't access or introduce fields. That's technically true, but in practice wrong. See we've got this thing from the DLR called the `ConditionalWeakTable` that allows us to add fields to any object. It plays games with weak references so you shouldn't have to worry about memory leaks. 
&gt; The use case for default interface methods is to resolve the problem where adding new methods to an interface is a breaking change. And that's why I don't like the proposal. I want interfaces to be small, well defined contracts, not something that they can bloat over time.
Here's my writeup of the topic: https://www.infoq.com/news/2017/04/Clr-Default-Methods
Extension methods is a use that I hadn't thought of. I am genuinely interested in the idea. I did look up the docs for the [Java implementation](https://docs.oracle.com/javase/tutorial/java/IandI/defaultmethods.html). I wonder if C# is also planning to include the idea of static interface methods. 
Completely agree
**Readonly ref** is interesting but I have always thought this is optimized by the compiler because you can infer such information from the code, to some extent. Moving it to developer is a bit C-ish approach (the const parameter).
It is also relatively extremely slow.
Even if this was something that could be optimized by the compiler/jitter it would still be nice to be able to explicitly identify places where an optimization is expected.
It could be a parameter attribute (similar to MethodImplAttribute for methods) rather than a new language keyword.
Most of these features are just C# syntax sugar or generated internal code behind. I'd like to see more MSIL/CLR changes. I am still missing enums inheritance for instance.
Yes, static methods are included in the proposal. https://www.infoq.com/news/2017/04/Clr-Default-Methods
While you're not wrong, there is merit in being able to say "compiler, tell me if this optimization isn't going to be applied because I'm counting on it". That's why I think we should be able to explicitly indicate that we want tail calls. I've avoided a lot of designs where tail calls would have been required, not just a nice to have.
That would be nice. And it's a safe change because the size of the enum would be consistent. (Unlike value type inheritance, where new fields could be added.) I'd also like enum methods. Having a separate module full of extension methods is annoying.
There will be also some unexpected (or rather more complex than known yet) boxing issues with that.
Getting rid of extension classes everywhere? It's not something that you couldn't do with just one using before...
I second that. Once I started using Dapper I also started missing such a library for the JVM :-(
The fact that you need user to access the site is a indicator of a different problem. However; look at Azure web jobs. https://docs.microsoft.com/en-us/azure/app-service-web/web-sites-create-web-jobs If you are using user access to control jobs or events may I suggest just replacing that with the webjob. Plus it will keep the server warm too 
Can someone give me a 10,000 ft view of what Docker is?
Absolutely probably.
This was precisely my thought! We have abstract classes and interfaces for a reason. 
Most likely certainly 
Ohhhhh, thanks.
It's a way of packaging up your environmental dependencies with your code into an image. Running the image on a host launches a container. As the OS kernel is on the host start up time is real fast. Like having the flexibility of a process with the configurability of a VM. 
Since your team is already comfortable with .NET why not just stay with it? Technical differences between .NET and Java aside the fact that you won't need to learn a new platform alone is a big plus.
The whole .NET/Java discussion is a moot point nowadays. If you can solve your business problem using a technology stack that you know, you will do it faster and probably better. It's more a question of architecture, than a question of technology stack.
With Java you also lose generics (true ones on VM level) and many other things like LINQ. Since .NET is ported to Linux, there is no need to consider any switch.
Will we get proper pattern matching before C# 8?
Without seeing any code it will be impossible to give specifics. But it sounds like you're doing a lot wrong. You should be able to use dependency injection to get the data context in any service or controller. I'm not sure what the singleton is for. But I bet you don't need it. Hangfire would allow you to schedule tasks every X mins to run some code. You could also do it as a web job in azure and use the message queue to trigger updates to single items or the whole database. I'd suggest you look for a boiler plate or tutorial to see how you're supposed to structure everything and use dependency injection.
The only good idea I can come up on the spot is implementing a default ToString method for your interface... But even then it is still probably not a good idea.
https://docs.microsoft.com/en-us/aspnet/web-api/overview/advanced/calling-a-web-api-from-a-net-client This article should illustrate the way to do it. Cheers!
If you only need a couple of calls, Microsoft's [WebApi Client Library](https://docs.microsoft.com/en-us/aspnet/web-api/overview/advanced/calling-a-web-api-from-a-net-client) is pretty simple. For more serious use I usually use [Refit](https://github.com/paulcbetts/refit).
Thanks. Is restsharp still a thing in .net core?
Like everything in a programming language, it can be abused, but it has its purposes. I personally won't use them until I find a need to. Same way I treat "dynamic". IE: I'd rather have it than not have it for that edge case where I need it, rather than complain about potential abuse which hasn't happened yet. 
Also Reflection should be pretty capable of altering state. But we're getting into "deliberately bad design" territory here. I believe the GP's point was that it's hard to accidentally blow your foot off with interface methods; but nothing will prevent you from doing so if you're determined. 
https://docs.microsoft.com/en-us/dotnet/api/?view=netcore-1.1
Awesome, thank you!
Doubtful. I don't think an interface method can override a base class method.
Hangfire will let you schedule when to run a function. Add it. Set it up to run the update function from a service with the data context DI into it. Then schedule it every 5 mins. Get away from the singleton. I assume you're just sleeping on a thread X mins at a time. Which is bad.
Really? Lol.. that's the only good use I could come up with hahaa
It's on Azure, so you pay for what you use. Typically size per runtime. I used DocumentDb (Cosmo is a superset of DocumentDb) in some experimental apps and it works much like Mongo. The cost can be worth it depending on your current infrastructure vs your expansion needs. https://azure.microsoft.com/en-us/pricing/details/cosmos-db/
I'm not seeing any added benefit of that Refit project vs WebApi2 for .net. Care to elaborate?
Er... it handles basically all of the boilerplate for you? URL formatting, headers, serialization/deserialization, etc. Their examples seem pretty straightforward to me.
I'm seeing nothing more it can do than webapi2
Looks pretty good to me! I haven't decided how feel about defaulting to making separate projects for separate layers yet. It seems good from an extensibility point of view but so often I only end up using the data layer project in the MVC site it was built for.
You should absolutely always separate your layers with separate projects, otherwise you're not really separating your layers at all. It's not only about extensibility but also about access control. For example, your web layer cannot touch your data layer directly. Instead it must access the business logic layer, which connects to the data layer. Except for temp projects, I don't think there is any excuse not to do it that way.
Came across [this Channel 9 video](https://channel9.msdn.com/Events/Build/2017/C9L13) and I think the answer is NO - UWP apps will only run on Win10 but the guest panelists allude to such a thing in the future. Class libraries can be shared but not a UWP UI. It does seem that Xamarin.Forms apps will run on all 4 OS platforms if it meets your needs.
Unlikely. What it means is that it can target the improvements from .Net Standard 2.0, but continue to run on Windows.
Yeah, I second that. Also, a great idea to have separate project for DTOs for same access control reasons.
Looks like a great implementation of Dependency Injection and Repository to me. :) A couple of points: * I don't know if this pattern is "overkill" for this project. I would have implemented like this for anything. * In the Create action, consider moving call to _unitOfWork.CommitAsync() into the URL service. Then remove the dependency to IUnitOfWork in the controller. That creates a nice separation between the controller and the functional logic. If you want a method that just creates a slug without saving to the DB, there might be room for another Service Class. * I don't like the name UnitOfWork. Seems to vague, but if it's an EF thing, I guess that's cool. (I'm more familiar with other ORMs) * DI is great for lots of reasons! One is to make your code easily testable. Your code is easily testable, so if you wanted you could add unit tests! 
No, that's not how it works. UWP supporting .NET Standard 2.0 only means, that code (mostly libraries like NuGet packages etc.) that target .NET Standard 2.0 can be used in a UWP app, because UWP 6 garantees to support all those APIs. UWP itself will continue to be a super set of .NET Standard. This superset contains all the UI stuff. .NET Standard 2.0 doesn't contain any UI stuff, so UWP 6 apps can't run on iOS, Android or whatever.
We use CosmosDB for many projects in production. Lately I've used it to store conversation history for a botframework project I'm working on. One thing I've found though, is that the SDK is not intuitive at all IMO. It was extremely hard to decouple my models from CosmosDB (Almost every article you read is about making your models inherit from a base "Document" object). Also I found the query language to be much more limited (Mongo seemed much more powerful). 
Thank you for taking the time to review my code! You mention moving the save changes call into the service, however I'm not sure what to think of that. If you've got various methods which handle the creation of entities because something special happens, but then you want to have a seperate method to utilise those other service methods without saving until the end I can't see how that would work? I'm not too experienced with unit tests, I'll give it a go :)
Stupid expensive. It's charged per-collection (not per-database). GridFS support falls over &gt; 2MB. We are looking at transition to Azure and I blew through my free $200 credits running unit tests with out service that uses ~20 mongo collections. 
Are you not supposed to be using the local cosmos db emulator for unit tests, and only using cosmos db for prod?
The idea is that the controller has very few dependencies, ideally only one. It should have a single responsibility, which is to return the content of the web request. To do this it must invoke the method that does the actual work/business logic (located in it's single dependency). If you're invoking methods off more than one dependency, you could argue that your method has more than one concern. Food for thought when considering the DI pattern in conjunction with Single Responsibility. Hope this thought makes sense! 
no.
Sure i suppose but there's no replacement for the real thing. Otherwise I wouldn't have found out that the pricing was worse than what the documentation would have you believe. 
It does not support aggregations. So features are not the same as regular MongoDB. 
Doubt this will go down well but I disagree and think you're just over complicating stuff. https://vimeo.com/131633177/description
Did you learn that or did you learn that unit tests don't fit the model. I haven't looked through the pricing myself, but your comment stands in stark contrast to others in this thread and to common sense. I'd hazard the guess that your unit tests are creating and destroying a lot of collections and that that's what actually ate your money. On most products it's really easy to rack up stupid bills if you use the system in a way that wasn't intended.
We tried pointing our existing code to the CosmosDb with the Mongo driver, we found it broke in interesting ways when using aggregations. {aggregate([{"$match":&lt;condition&gt;}])} Simply gets ignored for array elements, but works on a flat structure {aggregate(&lt;condition1&gt;,&lt;condition2&gt;)} It treats condition1 and condition2 like OR statements, whereas native mongo treats them like ANDs Having said that, it is lightning fast, and convenient if you're already in azure, so will be keeping an eye on it.
I've only had a quick scan through, but a couple of small things: * In your controller you've required IUnitOfWork from the DI system, but what happens once you've got multiple implementations? Bear in mind, design patterns are designed to make an application grow less painfully, and this may break once DI holds multiple implementers of that interface. You should request the type you want, or an interface guaranteed to only have one implementation. * Also I spotted `IUrlRepository UrlRepository { get =&gt; _urlRepository; }` which for a simple getter, you just need `IUrlRepository UrlRepository =&gt; _urlRepository;`
&gt; UWP itself will continue to be a super set of UWP. While that's technically true, I don't think that's what you wanted to say.
Oh... Was written in a rush. Fixed that. Thanks for pointing it out :)
This. We suffered from this also.
This is quite interesting, but I am struggling to find any example projects which use this pattern. Do you know of any?
Is there a changelog / new features list for UWP6?
docs.microsoft.net is the new location for all .NET docs, msdn is the older docs
A collection in DocumentDB is not the same as a collection in MongoDB. I think this was an incredibly bad decision on MS part to use the term collection, specially if they were trying to attract MongoDB users. Basically, you can store entities of different types in one DocumentDB collection. What makes DocumentDB interesting is that you can partition all you collections just by choosing a field as a partition key and it will take care of scaling it up for you.
It's a good video! And an great idea. I think it depends on the complexity of your project. For most of the projects I've worked on it would have been a mistake to not separate layers in deployables. For simpler ones maybe it's cool. Truth is it's usually far more complicated, so I think separating into deployable projects is a good way to start. You might find these links interesting: http://www.ustream.tv/recorded/86186174 https://www.thoughtworks.com/insights/blog/bff-soundcloud http://samnewman.io/patterns/architectural/bff/ 
I totally agree, however as a bridge between the two architecture how would you feel about retaining the repository and service layer and then introducing queries and commands as a way to move that logic from within the repositories? Instead of the controller using the mediator, the service layer uses it instead.
Collections in mongodb are the same, just containers of documents. Schema-less means both database systems can hold any kind of document in the same "collection" although its easier to split for organization - and mongodb has no price overhead for this which is why people are used to it. It just doesnt port over well to cosmosdb.
I love appveyor and didn't realize you could rdp into the build agent. 
Yup! That's pretty much what I would do :)
I use redux so I've actually reduced down To one api controller that forwards everything to mediator. Works well for me. 
I was thinking the same thing. It is a direction being mulled at the management level. If I get an opportunity to be involved involved in the decision, I wanted to make sure I knew what points to make.
Do find generics very useful
Collection in CosmosDb is equivalent to database in SQL. If you're using one collection per 'table', you are still solving the problems in SQL mind.
It really is a killer feature! I have used it a number of times to track down weird things going on with my builds.
FFT. Have a read of this, using the transform you should be able to get most of the information you need from waveforms: https://www.codeproject.com/articles/1107480/dsplib-fft-dft-fourier-transform-library-for-net Also look into the Goertzel algorithm, You are entering fun territory here my friend... good luck!
So here is how I dealt with it. I have to deal with specific folder and file organisation depending on the manufacturer. (Lucky enough, we give every only one or two Brother models + Lexmark for MFP) I'm still worried about the renewal of equipment that occurs every few years. Cause it's hard coded... I would still be glad to be able to do it in a more integrated way with TwainDotNet. [...] const string twain32Dir = @"C:\Windows\twain_32"; [...] //Action to leave the loop when needed Action getRegistry = delegate { if (scanner.Contains("Brother")) //Si marque = Brother { //#BrSc11c (modèle 8520) if (Directory.Exists(twain32Dir + @"\BrSc11c")) { var BrotherDir = Directory.GetDirectories(twain32Dir + @"\BrSc11c"); foreach (var dir in BrotherDir) { if (dir.Contains("8520")) { var dirIniFiles = Directory.GetFiles(dir, "*.ini"); var dirDSFile = Directory.GetFiles(dir, "*.ds").FirstOrDefault(); foreach (var file in dirIniFiles) { //Si l'un des fichiers ini contient le nom du scanner if ((File.ReadAllText(file)).Contains(scanner)) { registryValue = dirDSFile; return; } } } } } //#Brimm15a (modèle 5750) if (Directory.Exists(twain32Dir + @"\Brimm15a")) { var BrotherDir = Directory.GetDirectories(twain32Dir + @"\Brimm15a"); foreach (var dir in BrotherDir) { if (dir.Contains("5750")) { var dirIniFiles = Directory.GetFiles(dir, "*.ini"); var dirDSFile = Directory.GetFiles(dir, "*.ds").FirstOrDefault(); foreach (var file in dirIniFiles) { //Si l'un des fichiers ini contient le nom du scanner if ((File.ReadAllText(file)).Contains(scanner)) { registryValue = dirDSFile; return; } } } } } } else if (scanner.Contains("Lexmark")) //Si marque = Lexmark { if (Directory.Exists(twain32Dir + @"\Lexmark\NetworkTwain")) { var dirDSfile = Directory.GetFiles(twain32Dir + @"\Lexmark\NetworkTwain", "*.ds").FirstOrDefault(); registryValue = dirDSfile; return; } } }; //J'appelle l'action. getRegistry(); //Je modifie la clé de registre RegistryKey myKey = Registry.CurrentUser.OpenSubKey(@"SOFTWARE\Microsoft\Windows NT\CurrentVersion\TWAIN", true); if (myKey != null) { if (registryValue != "") { try { myKey.SetValue("Default Source", registryValue, RegistryValueKind.String); myKey.SetValue("Source par défaut", registryValue, RegistryValueKind.String); } catch { //On conserve dans le cas la valeur initiale } finally { myKey.Close(); } } } Thanks for any help.
What type of information are you looking for?
https://github.com/saad749/BeginCollectionItemCore - That is an option that will append unique guids to fields automatically.
The way that I did it was a very manual way. Basically you clone your current project, delete the a few files, modify your sln file, change your tfignore to a gitignore and then push your changes up to your new repo. Keep in mind that this does not move your work items and such. It will only move your source code and history basically. 
You can use a git-tfs bridge to convert your history to git and then push that to a new project. https://github.com/git-tfs/git-tfs
 [git-tfs/git-tfs](https://github.com/git-tfs/git-tfs) &gt; *Description*: A Git/TFS bridge, similar to git-svn &gt; *Stars*: 1153 &gt; *Forks*: 413 &gt; [Issues](https://github.com/git-tfs/git-tfs/issues) | [Pull Requests](https://github.com/git-tfs/git-tfs/pulls) *** ^(This is Earth radio, and now here's human music ♫) ^[Source](https://github.com/anaskhan96/github-stats-bot) ^| ^[PMme](https://np.reddit.com/message/compose?to=github-stats-bot)
As piku17 said, git-tfs is a good tool, allowing a full conversion but it takes time and you may need to resolve conflicts by hands. I already used it and after 2 or 3 trials, you will find the right configuration for your need, there is an interessting links on git-tfs I used : [ogvolkov.wordpress.com](https://ogvolkov.wordpress.com/2016/04/02/migrating-from-tfs-to-git-in-visual-studio-team-services/). There is a feature built-in visualstudio.com but it's limited in how far in time it recreate the history (As I said, it's an expensive process). In both case, you only need to create a new repository, so it will not require a new project or to create all your issues again. There is on git-tfs an option to migrate the work items references (it add some texts on the commit comments, and vs.com do the rest). It actually make sens to keep the old repo and only migrate the last iteration, or last 2 iterations. A full migration provide theorical benefits but it's a lot of work for nothing.
It doesn't matter most would probably run install just because it would be slightly faster than checking all packages 
Seems almost like a fake rather than a mock. Using an automocking container might help clean it up a more, you can get rid of the "new" statements.
Maybe I am old-fashioned but all these mock frameworks made things more complex than necessary ... in my view. I always use simple private class that implements the interface in a unit test class.
My advice (that will get downvoted) is, **do not do it**. TFVC is still much more reliable and easier to use than Git despite of all its hype. The VS Git user interface might be part of the problem.
&gt; I always use simple private class that implements the interface in a unit test class. Its not that different in the end, but I've found creating mocks is easier.
Remember kids, don't feed the Troll !
It is based on my personal experience. I use both TFVC and Git repositories, both hosted on Visual Studio Online. There are endless issues with the Git one, in most cases I have to checkout all repository again in order to push changes. While there are no issues with the TFVC. I am not alone with such experience. Don't mention you lose features like incremental version number I use automatically in all files version info to clearly match the build output with source code version. Oh yes, in Git you "don't need it" :-)
Hmm, sound serious. Do you use a branching strategy adapted to your needs? Git does need an adapted work organization. It's usually for the best 
Yes, branches as well. Most of issues is error message that there are changes you have to push first to perform pull but there are actually none. Sometimes helps to open the command-line git and perform status command (but why do I have to bother? TFVS always works), sometimes I just do checkout to a different folder, manually copy changes from the old one and push. Terrible tool, negative productivity.
The system works best if everyone has a full copy of the repository. If your repo is too big, you might want to look at dedicated solutions, like git-lfs. MS has developed a solution for the windows repo now that they switched to git for every thing based on virtualized files, I don't remember it's name you could look into it. 
The drawback of that is that you need to manually modify the mock every time the interface changes
[removed]
Ah some of my co-workers had those issues. Specific to Vs.2015, try the 2017 or use another client(bash or source tree) 
Not sure about 2017 but they restarted the git implementation from scratch due to those kind of issues. So it should work. 
Ok, I still use VS 2015 for this project, I'll try 2017.
What's the difference between a fake and a mock? 
If I remember well, it may be related to the fact that you have multiple instances of Vs team explorer running, so multiple git exe in parallel. I don't remember well, I didn't had that problem since last year. 
Which is good to "remember" to change corresponding tests as well :-) Type safety first.
I use NSubstitute because it's less code to read. It's clear what I'm implementing, and what it should return and what I expect. Certainly less code than handrolling it all.
Uh. Nobody's really sure. Stubs, fakes and mocks are terms for a bunch of pretty similar things, and I get the impression that everyone has a different idea about what the differences between those *are*.
Correct. Versions below 2017 use libgit2, which is a subtly broken implementation. Newer versions just use git.exe and scrape the output. It's been bulletproof for us.
Its a spectrum. https://stackoverflow.com/questions/346372/whats-the-difference-between-faking-mocking-and-stubbing 
Thanks, it looks promising.
While there's debate about using the repository pattern on top of your data abstraction, I think most would agree to layer your UI, business, domain and data layers. Otherwise, not much has changed in the separation of concerns aspect of web applications in the .Net MVC space that I know of. Try [this architecture doc](http://aka.ms/WebAppEbook) by MSFT. It starts off looking like an Azure brochure but it actually lays out some good solution structure when you get into it.
I have been reading about architecture and project structure of .NET web apps a lot lately. It seems that everything someone recommends someone else find good arguments on why this particular structure/architecture is bad. Beside the classic layered structure often people seem to recommend organizing project by feature: Good summary with references can be found [here](https://www.reddit.com/r/dotnet/comments/5yf5qa/how_do_you_break_up_your_mvc_solution/deq24fi/) . Also yesterday someone recommended [SOLID Architecture in Slices not Layers - Jimmy Bogard](https://vimeo.com/131633177) . I did some research and found a lot of positive comments abot that architecture but havent found any bigger projects using it. Havent tried it but I dont see how would this architecture work on any complex project but I am too inexperienced in this field to make statements like that. I have mostly used classic layered structure along with repository pattern in the past and I like it but I often end up overcomplicating things and whole repository pattern seems kinda redundant. In the future im definitely going to try structuring project by feature cause currently it seems like right way to go.
This will really help us. Keep these Appveyor articles coming 👍
I put this together from a bunch of other random sources to try to get an understanding of the ZB64 that's nearly undocumented in the Zebra Programming handbook. I used this to have better control over QR codes, as they scale dynamically in size when more information is added. This is a major problem when you have limited space. Using this I am able to increase the QR code density rather than scale. This can also be used for logos, or unsupported zerba barcodes, or whatever else you could want to print.
Whether you migrate or start fresh, you'll be glad you moved to git.
I just like the Git workflow and branching much more, but I‘m not even pretending I was going to use the VS integrated tools for my repo after switching to Git. Also when you create a new project on visualstudio.com Git now is the default choice and Microsoft switched to Git recently for their Windows codebase, so I think it‘s save to say that even they think it‘s the better vcs.
Just as a side note, Bower is deprecated, you should look to switching to npm or yarn depending on your needs
What were you putting in the 2D? Just curious, no real reasoning behind the question 
Package names don't always imply that things are related. `Microsoft.AspNetCore.Mvc` happens to be dependent on `Microsoft.AspNetCore`, but it doesn't have to be that way. I could make a package called `icefall5.HeyThere`, and someone else could make a completely unrelated package called `icefall5.HeyThere.HowdyDoo`. Microsoft is really good with its naming conventions, so all of their packages should be related in that way. So, knowing that packages don't have to be related, perhaps it makes a bit more sense. In package versions before Core, a lot of them were bundled into one monolithic package that included everything. That's not the case anymore, as you've noticed. Microsoft packaged all of the features separately to keep Core lightweight. This way you only need to include the actual code that you use, instead of including _everything_. Namespaces are the same as packages for the naming conventions--any code can have any namespace. It obviously doesn't make much sense to not do it the standard way, but you could hypothetically give each of your classes completely unrelated namespaces. That's why `Microsoft.AspNetCore` doesn't automatically include everything underneath--those namespaces aren't _technically_ "underneath". Hope that makes sense, let me know if not!
Wow, I going to rewrite our parking officers app tomorrow and this is wonderful! Thank you!
The barcode (lack of a better word)? Data that helps automate my company's receiving process. We issue out return labels for items and those labels need customization per client that we work with.
Glad it will help! I was frustrated when I found so little documentation on ZPL other than "download an image to the printer using our application", even after extensive google searches. This eliminates the need to download anything to the printer, which is perfect for networked printers with dynamic data being printed to the label.
impressive results, well done! If you can update the performance figures when you migrate to .Net Core 2.0, please do.
I can certainly try :-) Is there anything in particular that you are interested in hearing more about?
Anything about speeding up builds, I'm hoping this caching will help us.
The labelary link is very helpful as well. Awesome work!
Alright, so that brought a few more questions to my mind :) What is `Microsoft.AspNetCore.Builder` if not a package? I get access to it by importing the package `Microsoft.AspNetCore`? You say `Microsoft.AspNetCore.Mvc` happens to be dependent on `Microsoft.AspNetCore` but when I look at the [page](https://www.nuget.org/packages/Microsoft.AspNetCore.Mvc/) for `Microsoft.AspNetCore.Mvc` I can't see `Microsoft.AspNetCore` among the dependencies? Plus, if Microsoft.AspNetCore.Mvc were dependent on `Microsoft.AspNetCore` wouldn't that mean that we only needed to include `Microsoft.AspNetCore.Mvc` in our `.csproj`-file. Since we would get `Microsoft.AspNetCore` the same way `Microsoft.AspNetCore` gets us `Microsoft.AspNetCore.Hosting` (by being dependent on `Microsoft.AspNetCore.Hosting`)? 
Sorry, I did not find a tutorial for what you want to do, but I can give you some hints. One way to make this kind of inline popup, is to conditionally hide parts of your UI. This means that you put all dialogs for this form inside and hide/show stuff depending on the current state. This article shows, how to hide/unhiode a button, but nothing keeps you from hiding elements like panels or grids: http://www.technical-recipes.com/2015/showing-and-hiding-controls-in-wpf-xaml/ You said, you wanted to keep it super simple, so you probably do not want to do MVVM like the articale does. In your case you can just use regular event handlers and then set the corresponding show/hide properties on the elements you want to show or hide.
Its just a namespace. The package is a single or collection of assemblies. Assemblies can have as many namespaces as they want and they don't have to have anything to do with the assembly/package name. For example the System assembly has the namespaces System, System.IO, System.Media, System.Collections.Generic etc.
Considering I never even heard the term "MVVM" before, yeah... =P The trouble with using element hiding is that it's difficult and tedious to work with. Let's continue with the example I gave above: there is a main window. In it are five elements. Clicking one element will change the entire window to display something else until a "back" button is pressed, which returns the user to the list of five elements. Problem is: The element list and the five submenus, that's six different views, all of which occupy the same space. At runtime, five of them are hidden at any given time, so this is no problem. But how do I work with a form in the editor where, on some parts of the surface, there are five to six elements occupying the exact same spot? Like, there would be five different "back" buttons, which have the exact same coordinates, look exactly the same, and feature exactly the same text... maybe even link to exactly the same code routine. That sounds like a royal pain in the behind to select and edit individual elements. "Just reuse that button"? Then I start to get into the case where I cannot simply hide the entire UI for one view with one command, but have to hide every individual element for every view individually. That's even *more* work, not less. There has to be a way that just switches out a view for something that I can make separately in the editor. Maybe I should start googling MVVM, just in case that's the actual thing I am looking for... EDIT: Nope, it isn't. Holy crap, the wikipedia page on that alone makes me doubt that it's written in English.
Thank you, is there any easy way of identifying something you're importing through a using statement. Like what assembly it belongs to and what package that assembly belongs to (if it belongs to a package? Does the System assembly for example even belong to a package)? EDIT: Also: how do I explore a package? See what assemblies are included and what namespaces are included in those assemblies (and what classes are included in those)?
When the widget set doesn't do what you want it to do, you have to make all of that stuff happen manually. This has always been true, back to windows 3.1 days. Five hidden views, swapping them around, reusing buttons, etc is all par for the course. You make it work in a clean way by wrapping all that stuff up in your own custom components. Try not to make one giant component that mimics your UI completely otherwise you are back where you started. Think about what your app needs but the widget set lacks and make just that component. Maybe you need a custom ViewContainer to manage swapping them around for you. Maybe you need a common View base that share the same callbacks for the button events. 
Ah, got it. Always curious about more use cases for them. I work for a print/fulfillment shop and we put 2D's on every piece of mail (and every page of it) that is produced. It's used to audit that every page leaves our building and, also, used to tell the machines how to fold/insert them into the envelopes along with automatically pull Insert into the container.
That sounds disappointing and hard to believe. You would think that in twenty years of UI development on Microsoft's side, someone would have chanced upon the concept of making more that one window in the editor and switching which one's displayed at runtime. That seems like an incredibly fundamental kind of capability. But, very well. If I have to do it myself, then I would like to learn how to do that. How do you recommend I should start approaching this? It goes without saying that this is 100% new territory for me... I am self-taught via drag-and-drop and online tutorials, and have little grasp of software development lingo and concepts.
Exactly the kind of thing I'm after, thanks very much!
Haven't had the chance to watch the video yet, but thanks for your input. If you haven't tried it before, I'd definitely recommend having a look at Onion Architecture - http://jeffreypalermo.com/blog/the-onion-architecture-part-1/
I like to describe it as having a lunchbox (your primary bare metal server or VM) with several Tupperware inside of it (docker containers). All your applications go inside of the Tupperware rather than thrown into the lunchbox, each one can be packaged as needed, while not getting your main server dirty. If anything were to go wrong with the server, just spawn up a new server (new lunchbox) and load the containers (Tupperware) in it.
I'm in a very similar boat. I've got a personal project that I've been working on, and everything WPF that I know has been self taught. I've found that open source projects have been one of my best resources when teaching myself some of this stuff. There's quite a few on githib if you search hard enough. One of the best ones for in depth examples on behind the scenes stuff like this has been MahApps.Metro. I didn't want to use their UI, but I did want to learn how they did it so I could make my own. I cloned the repo and I think that has saved me many weeks of stress trying to learn from MSDN tutorials. They don't have a ton of fancy effects, but I'm sure there's another repo that does
Sounds like what you want is to create an app that supports navigation. It's not that hard to implement. All the basic components to do it are already part of the framework. The docs here should get you started: https://docs.microsoft.com/en-us/dotnet/framework/wpf/app-development/navigation-overview
Oh man, can I upvote you ten times? That does sound like the right thing! The linked page specifically talks about XAML browser apps, but it does say that it's also available in standalone WPF applications, so it shouldn't be hard to find out more.
Yep, you can apply the exact same concept to a WPF app and it'll work the same way.
Old tutorial, but should still be relevant: http://paulstovell.com/blog/wpf-navigation
I don't know the details of packages. When you do a using that is a namespace. If you open the object browser it will show you a tree view with all assemblies, if you expand one it'll show the namespaces, and if you expand a namespace it will show you the classes.
[removed]
Thanks!
The material design in xaml project is fancier, with animations and stuff, if you ever want a peek at something similar.
Probably because in their eyes it still has the lowest barrier to entry for css + js dependencies. Those templates originated from a time when bower was still very popular. I remember when the first core templates came out I always used bower for front end dependencies. At that point they also had grunt as part of the template, which was later replaced with gulp. I'm sure they'll change it at some point. It's always the first thing that I remove from the boilerplate and replace with just npm.
Don't try to understand it, just accept it lol..
Awesome indeed, I googled this for the longest
Nice! There are a lot of useful applications for using 2D codes. The ease of use plus the awesome error correction that comes along with them is really helpful. Originally we used the standard 3 of 9 barcode and had several of them. But as things get scratched the less the codes were able to be scanned. Also the fact that someone had to scan multiple codes per package was a little too much. One QR code with delimited data and a parser is all it takes to automate multiple actions at once. We thought about using NFC chips at one point. Personally I'm still very much into that idea, but buying all the equipment is a bit much. Though the idea that multiple items could be in a sealed box and they can all be scanned at once while leaving onto a truck is awesome.
The editor should not be the problem. You could for instance put the forms in a stack panel so they are shown below each other in the editor. If you make sections invisible, the panel will just snap the other items into place. MVVM is a frontend design pattern, which is often used in WPF and many Javascript frameworks. It more or less allows you to bind your UI to data, so you flip flags in your data and make parts of your UI hidden with it. This does not give you additional benefits in this case, it'a more about code maintenance.
Because Microsoft is always behind on the latest JavaScript trends. To be fair, the trendy JS tools seem to change every two weeks. 
Everyone is always behind when it comes to JavaScript.
whats wrong with nuget
Bower is included along with nuget in the .net core template. The question is why is Bower there at all Also they host very different packages
Have you thought about rendering multiple partial views, and, to avoid reloads, use angular or react to switch them? 
PartialViews are the answer. I haven't used angular or react but I retrieve them with jQuery using a simple get and assign the results to a div.
I’d recommend KnockoutJS or Vue for this, rather than a full blown frontend framework. 
The host you build using the WebHostBuilder is what calls the Configuration methods in your startup class. These methods are not part of an interface or base class because you can inject services into those methods (just like you can the constructor), so there is no set signature. This is the one part of ASP.NET that is basically "duck-typed". As far as order, the host is built first in the Main method (as you noted), then the Startup constructor is called, followed by ConfigureServices() (if present), then finally Configure().
Have you zero'd out the idle timeouts too? You could also just setup http monitoring of the site, since it's frequently hitting it to check its status it should keep on resetting IIS's shutdown timer.
I find this really interesting (perhaps mostly because I don't understand it), but when you say &gt; These methods are not part of an interface or base class because you can inject services into those methods (just like you can the constructor), so there is no set signature. How and where do I inject services into them? Are the parameters they have in the template code injected (like env, services, app and loggerFactory) like this?
This page should explain it: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging#log-scopes
IntercoolerJS It's pretty much magic with MVC and razor. 
This is how I'd do it. ReactJS and Angular are nice, and if OP has a lot of time to fuck around and learn those for this project then great, but if he has a tighter deadline, just use Javascript/jQuery. Way less overhead. And then basically just turn the divs on/off with css display: none or something like that. 
Ok, I'm going to get some hate on this one - but for extremely complicated UI logic (heavy business logic) - Web Forms over MVC / Razor isn't always a bad choice. If you're going to end up with a mess of interweaved jQuery, WebForms may provide a more easy to maintain solution (but not as light-weight, obviously). 
They do market it as RAD now.
Indeed! The sample code leverages dependency injection for those services and you can inject any framework service into those methods. I've not tried injecting app services into Configure(), so I'm not sure if that works or not. But the docs do explicitly say we can inject any "built in framework service" in those methods.
Really? I must have missed that somewhere, but maybe because "rad" + "asp.net" always ends up on the Telerik website. 
What kind of apps are the clients looking towards? Maybe try a WPF application if they are looking for desktop app. Then you could get some GUI experience. Otherwise try building a asp.net MVC app and progress into WebAPI.
they're definitely mostly web apps i would say. more for like helping businesses run their businesses more efficiently, if that makes sense. "business solutions in software", so to speak.
Makes sense, most software is made to help do things more efficiently. You still have lots of options in the web side of things. When I started learning web development on .net, I started with getting a solid grasp on asp.net MVC. From there I learned better web page design, SVG, and developing APIs to consume via javascript or native .net.
I would try making some simple MVC and Web API apps. Then make sure you have a good grasp of Linq and Tasks/async/parallel code EDIT: A windows service may also be useful
Are they hiring more peoole? Sounds like a great entry level gig 
Sounds like you're basically just asking for an ASP UpdatePanel, wrap a bunch of controls in one and they will all do AJAX updates without the page refreshing, no manual jquery/javascript required. You can just place divs with runat=server in them and set them visible=true/false in code behind if you're trying to hide/show chunks of controls at once.
DependsOn does exactly what you want via jquery. https://dstreet.github.io/dependsOn/
Find out what they are using and build something with the same framework. No sense learning razor if they have an angular front end.. 
Thank you so much! This is exactly the type of thing I was looking for. I'm technically an audio programmer, but I just started a couple months ago, and haven't gotten to work with actual audio processing yet, so I'm working on this side project to get me familiar with everything. 
&gt;And then basically just turn the divs on/off with css display: none or something like that. this is the easiest solution. I would go with this.
This. See if someone will give you access to at least view the code repo. Delve into documentation, the code, and whatever ticketing system they may use. That should show you a clear path.
Okay dokay, I will put my thinking cap on :-)
Using the MVVM approach is recommended for WPF and will help here. You can select current window using a property binding and for designing the window itself you could use a Page contol for each view nested in the main window as a container so they are separately defined.
Ah alright! But where is it done? If I'd like to inject the built in framework serviceX into the ConfigureServices method and inject the built in framework serviceY into the Configure method, where and how would I do that? EDIT: Just watched [this](https://www.google.se/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://m.youtube.com/watch%3Fv%3D8Cwb0d_WeKs&amp;ved=0ahUKEwisgsSQzKbVAhWqJ5oKHaDMCioQo7QBCB0wAA&amp;usg=AFQjCNF6mmPzJpLRvE9d1Zps2LspD_1i8w) and I believe they explained it well. Since the framework services are built in, I just add their interfaces to the constructors and voila, I get them injected to the method. :)
I had thought about partial views a little and it seemed like it good route. I think I'll try to incorporate them and clean up my existing code then see if it leaves me wanting something different. Thanks, all the answers here have been really helpful!
There is no `runat=server` in MVC. You’re thinking of the archaic Web Forms.
The only way to prevent Web Forms from eventually devolving into an unmaintainable mess of spaghetti code is to just not touch it in the first place. I have yet to see a Web Forms project that didn’t end up in tears and epic alopecia. MVC needs a lot more up front planning, but it is worlds ahead of WF in terms of maintainability and extendability.
Is there any particular reason you want to configure dependency injection in the `Configure` method? `ConfigureServices` is meant to be used explicitly for setting up dependency injection mappings.
May be worth noting, that they *are* part of an interface - `IStartup` - but you don't have to implement it. If you do make your `Startup` class implement the `IStartup` interface, there'll only be injection into the constructor, not the methods. If you don't implement `IStartup` yourself, the methods are found through reflection, and wrapped by methods of a class that *does* implement `IStartup` (`ConventionBasedStartup`) and which uses the IOC container to resolve any additional parameters. `WebHost.Run` uses the `IStartup` interface to call `Configure` and `ConfigureServices` in either case.
Interesting. Thanks for that info!
Lol honestly I can't complain. I would have preferred to be working with more people but oh well.
What kind of simple web API apps would you recommend?
Yes they use angular for the front end
No there isn't (though there might be, I just wouldn't know about it) I'm just trying to learn what/why/where everything is/does in the template code :)
Do you have a recommendation on a good Vue tutorial, or better yet a Vue/MVC tutorial?
Is OpenShift similar to Kubernates?
OpenShift is built on top of Docker and Kubernetes... Check this post: https://blog.openshift.com/red-hat-chose-kubernetes-openshift/
True, he didn't specifically mention MVC though. Just said he was making "html forms" and that he was new to web design, and a lot of people who are new start out there so I figured it might have been what he was looking for.
Do you mean compilation time of C++ code? Because if yes, you might be better off at /r/cplusplus. Concerning. NET languages I doubt there is a significant performance hit, since. NET languages don't directly compile to native instructions. So there is generally less to compile.
We saved 1/4 of our build time using the cache - thanks 😁
How is this related to .NET?
I'm curious about this too; and specifically about .net languages. Does anyone know if C# Visual studio workloads are particularly amenable to skylake-X (as opposed to say, a 7700k or some ryzen or threadripper variant)? Anandtech has a chromium compile benchmark: http://www.anandtech.com/bench/CPU/1858 - that appears to be *very* Skylake-X friendly, with it outperforming 7700k's and ryzen's quite thoroughly. But C++ has some pretty quirky aspects that perhaps make it quite different from a C# workload (in particular, a lot of necessarily serial work).
Definitely my case. Lower salary, but way more vacation days, better pension plan, less stressful environment and shorter commute. You gotta figure out what are your lifestyle goals. If you prefer making loads but less vacations for example. Commute is a big one too... you don't realize how much time you waste in your car until you get a job that has a much shorter commute. It makes life much more enjoyable, more time to make meals or do activities after/before work.
My wife is an engineer but had something similar. The company she was working for was huge, high pay, but wouldn't let her get the training she needed. She took a pay cut (from 90k/yr to 80k/yr) and is very happy. Part of the reason though is she wanted out of the big company. She still doesn't make as much but is much happier. I would say if you like the job, stay at it. If you feel you are stagnating, try to either find work time or your own time to learn new skills. 
If you want to use Entity framework 6 (not Core), I recommend trying this https://visualstudiogallery.msdn.microsoft.com/ee4fcff9-0c4c-4179-afd9-7a2fb90f5838/view/Reviews/2 fantastic tool to generate "CodeFirst" model from your existing DB. It's been a life saver for me.
.NET Core is cross-plat and WSL is super handy for testing your .NET apps on Linux on Windows.
&gt; NET languages don't directly compile to native instructions. Not paying attention to Microsoft .NET tooling? https://docs.microsoft.com/en-us/dotnet/framework/net-native/getting-started-with-net-native https://github.com/dotnet/corert
Fiddler is super handy for testing .NET web apps but I wouldn't expect to see a post in this sub about a new release of it.
Skimmed through the Xamarin architecture PDF and I have to say, I'm impressed. The reason I've stayed away from it in the past it because I just could not get a "good" architecture working with it leaving my app feeling glued together, but this MVVM paradigm documentation they lay out looks great, may give it another shot shortly. 
I did it once (20k pay cut). I regretted it. The job is good, vacation is nice, good commute, etc I find that you can find that and still get the money. For me, I always regretted it, in my head "I was making X more and now I'm making less". It depends on you and your market. You're my market there's so many jobs that if I'm not happy I could just find another one so at that point money is King.
Finally!
 I've been digging into the repo over the past few days. The Location Service in the eShopOnContainers uses MongoDb as it's data store. They've also got Redis, RabbitMq, and others Color me impressed with the technology mix. We done MS.
Contribute to an open source project off hours? Work on a hobby project using a new stack?
You don't use `dotnet add package bootstrap` do install bootstrap. That command is used to install NuGet packages, but you want NPM packages. So you do `npm install bootstrap --save`. This will install the NPM module in a `node_modules` folder, from where you either copy it to your destination folder or use a build system like Webpack to create a bundle.
Oh wow! That is a massive saving! I am very glad to hear that you have had great success with this!
It's been pretty much superseded. All you will maybe still find are slight bugfixes.
In about 10 years there will be plenty jobs converting ASP.NET MVC applications to ASP.NET MVC Core.
I took a 20k cut and am so happy I did. New job is full time remote and the company is 100% family first, and the projects that we work on are very diverse (wpf, mvc, xamarin, db only, etc). My stress level is now 0. I did not even realize I was stressed at my last job until I started working for a company that truly cared about its employees. So I guess what I'm saying is, there are a lot more factors than pay. I know it's stupidly cliche, and 3 years ago I completely thought otherwise, but money is not everything. 
Yes. I'm earning half of what I could elsewhere, but I have almost absolute freedom. I'm working at an online retail business I started with a friend 10 years ago. There's just no bullshit in my job. I come and go when I want, I take holidays and appointments as I need them, wear what I want, get the freedom to develop how I want, etc. etc. I don't have to worry about pleasing some shitty manager or sucking up to customers.
ASP.NET is still being develop only at a slower pace. ASP.NET Core is much smaller and new innovative features can be implemented in a shorter timespan.
I have feeling it will be about 3 years at my workplace.. but it will most likely be pushing even older internet apps to be os agnostic which we have started being pushed towards.
Definitely learn how to build proper RESTful APIs. Learn your HTTP verbs and status codes. 
In all honesty down to 2 minutes from 3. We found the nuget cache was slower than just restoring nuget packages but caching npm worked well.
Because it can does not mean it does with most project types (templates). But thanks for the links I was way behind on that info myself .
That might be true about NuGet, but you have to remember as well that the cache isn't just about speed, it is also about resilient builds. If NuGet.org goes down, your builds, with everything in the cache, will continue to work :-)
They probably wanted to show that the services are independent from each other. I'd like to see one service implemented in Perl, just for the (horror) show.
Do you have any links or documentation that says that? I can't find anything on the web about ASP.NET being in active development.
Yeah, I'm trying to do an cost/benefit analysis about moving to ASP.NET Core. It'll be a big plus on the ASP.NET Core side if ASP.NET isn't in active development anymore. 
Salary isn't everything. Benefits add up quick, especially shorter commute, bonuses, cheaper insurance premiums, 401k matching... 401k matching can easily make the difference right there. And quality of life is easily worth losing $10-20k a year, if you get more days off, less stress, better work environment, etc.
My current employer matches my 401k at 150% up to 6% of my wages plus 4% profit sharing bonuses to it annually. At $85K/yr in salary that's a lot of extra money to the tune of $10K annually, plus I really enjoy the freedom I get due to years of trust I've built up with my team and managers. Salary isn't everything, but a shit salary with nothing else to gain is still just that. Edit: oh, and I'm 100% remote.
It's possible, but you can do it via controller too
how? 
Http client allows you inject some kind of handler, which is a middle ware. I know of no tidy project, but a team at my current place has written some tools to nicely inject multiple middlewares in a pipeline. It works well for stubbing HTTP responses in end to end tests too.
What?
I wonder if the Azure platform could automatically implement this via a service.
Does anyone have a very simple console app that demonstrates these vulnerabilities?
Wouldn't that be distributing malware?
Myrtille provides a simple way to connect Windows based remote desktops from a web browser. As a web UI, it’s device and OS independant. Server side, it uses the .NET (C#) framework, IIS and the RDP protocol through an HTTP(S) gateway. Its particularity relies in the use of the .NET/C# framework to offer a better integration with Windows (equivalent solutions, Guacamole to name just one, are running on Linux/Apache/Java). It supports HTML4 (one of the few to do that), HTML5, file transfer and WebP compression (in addition to PNG and JPEG). It's primarily aimed at system or network admins and devs but everyone can use it. I hope you'll enjoy it! feedback is welcome :) Thanks!
I don't know if I'd classify a calculator launcher malware.
The basic vulnerability seems to be in letting the caller specify the type to deserialize to. Bet you could do it pretty simply with a crafted payload and a simple JSON.NET JsonConvert.DeserializeObject&lt;Object&gt;(payload). He specifies some of the interesting target classes to use in an attack.
Not if you explicitly say it's a vulnerability.
Realcode.co.uk could do it but not immediately, lots of work on currently, if you can wait get in touch
-$10k, -80% stress, 50x importance of work, 10x happiness of family, 100x better management chain. I feel like I won the powerball. Of all the metrics there, money is the one I care about the least.
where to get freerdp already built?
It is as simple as the templates were shipped with VS before the announcement. 
I'd suggest following [some of Aaron Bertrand's advice](https://dba.stackexchange.com/questions/99334/migrate-from-sql-server-2000-to-2012-without-a-2005-or-2008-instance/99340#99340). It looks like you can download the evaluation version of SQL Server 2008R2, which can upgrade a 2000 database--you'd then load that into 2016 and upgrade it again.
Very neat project ~ the legacy support is especially impressive.
I was profoundly disappointed by this article: I was hoping to see if NotImplementedException was ranked in the top 5.
Thanks for the input! I'll need to check out the size of the database. I haven't seen it yet. It looks like the DB can't be larger than 10 GB.
You seem to mix up namespaces and assemblies.
Well, I wouldn't say I have them mixed up. Since this module loads .dlls, it is loading assemblies, but since it also figures out appropriate "using" statements, it's taking care of namespaces too. The idea here is that you can just throw a list of strings (not knowing if they're namespaces or assemblies) at the Load-Assemblies function, and it'll adjudicate.
[This blog post](https://www.alphabot.com/security/blog/2017/net/How-to-configure-Json.NET-to-create-a-vulnerable-web-API.html) includes a simple sample using ASP.NET Core and Json.NET as serialization library. Be aware that most of the published gadgets in this presentation do not work with .NET Core (yet). An example of such a gadget is on page 11 of the presentation that uses a WPF class.
https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds
Doesn't guacamole exist?
choice is good by nature :)
Or because it's not written in the holy language we'll do our own. 
I'm not for reinventing the wheel of course, but the first myrtille PoC started back in 2007, at the same time as Guacamole...
I try to keep them as separate as possible to help identify failures. 
I came into it also thinking `NullReferenceException` would be #1.
Test only one *scenario* and one *result* in each method. Sometimes this leads to multiple asserts, but NUnit provides a syntax: [Test] public void ComplexNumberTest() { ComplexNumber result = SomeCalculation(); Assert.Multiple(() =&gt; { Assert.That(result.RealPart, Is.EqualTo(5.2), "Real part"); Assert.That(result.ImaginaryPart, Is.EqualTo(3.9), "Imaginary part"); }); }
So doing it like this is too much? And I should put these asserts in new methods? What's the benefit of using Assert.Multiple? [Test, Category("SelectListHelper")] public void SelectListHelperDropDownListEx1() { var helper = MvcHelper.GetHtmlHelper(); List&lt;GenericIdNameVM&gt; list = new List&lt;GenericIdNameVM&gt; { new GenericIdNameVM() {Name = "Name1", id = 1}, new GenericIdNameVM() {Name = "Name2", id = 2} }; var result = helper.DropDownListEx("name", list, 1, new {@class="form-control"}).ToString(); Assert.IsFalse(string.IsNullOrEmpty(result), "helper should not have returned null or empty string"); Assert.IsTrue(result.Contains("&lt;select class=\"form-control\" id=\"name\" name=\"name\"&gt;"), "Select list was not created with name or attributes"); Assert.IsTrue(result.Contains("&lt;option value=\"\"&gt;All...&lt;/option&gt;"), "All... option was was not created"); Assert.IsTrue(result.Contains("&lt;option selected=\"selected\" value=\"1\"&gt;Name1&lt;/option&gt;"), "Name1 Option was not created and selected"); Assert.IsTrue(result.Contains("&lt;option value=\"2\"&gt;Name2&lt;/option&gt;"), "Name2 option was not created"); Assert.IsTrue(result.Contains("&lt;/select&gt;"), "Select list was not finalized"); }
I would be inclined to use something like Html Agility Pack (http://html-agility-pack.net/) to parser the generated HTML. Your test should be validating that the generated &lt;select&gt; element has the correct attributes and options. HTML doesn't really care about whites pace, quote characters and case. Your test should ensure the elements and attributes have the correct values Here is example. Note that I didn't compile or run this, but should give you an idea. var select = = helper.DropDownListEx("name", list, 1, new {@class="form-control"}).ToString(); var html = "&lt;html&gt;&lt;body&gt;"+select+"&lt;/body&gt;&lt;/html&gt;"; var htmlDoc = new HtmlDocument(); htmlDoc.LoadHtml(html); var selectNodes = htmlDoc.DocumentNode.SelectNodes("//select"); Assert.That(selectNodes.Count, , Is.EqualTo(1), "Should be only one select"); var optionNodes = selectNodes.Descendants().ToList(); Assert.That(optionNodes.Count, , Is.EqualTo(3), "Should be 3 options"); Assert.That(optionNodes[0].Attributes["value"].Value, Is.EqualTo(string.Empty)); Assert.That(optionNodes[1].Attributes["value"].Value, Is.EqualTo("1")); Assert.That(optionNodes[2].Attributes["value"].Value, Is.EqualTo("2")); 
In addition to the other comments look into AAA syntax. Three phases of the test: Arrange Act Assert(s)
So, this is one of the odd things doing asp net core mvc dev right now. The "baked" in functionality for dealing with things like bootstrap is currently bower. [This](https://docs.microsoft.com/en-us/aspnet/core/client-side/bower) should be what your are looking for i believe. 
Hmm, I did look at bower but I've gone through a series of blogs/posts/comments [(1)](https://www.reddit.com/r/javascript/comments/5ls30m/is_bower_dead_should_we_start_using_other/) [(2)](https://community.plone.org/t/bower-is-now-officially-dead/4246) [(3)](https://www.quora.com/Is-Bower-dying) [(4)](https://www.reddit.com/r/dotnet/comments/6pgrur/why_is_bower_included_in_new_mvc_projects/) that seem to unanimously agree that Bower is dead and deprecated and responsible developers should use npm instead?
Most people use something like webpack to build/bundle those packages that get out into your wwwroot 
It’s interesting to be sure! Wonder how it will play out?
We did the migration and it's not bad at all. Remember you can switch to asp core but still target the full net framework.
In general I would say yes bower is on the way/already left the building. Unfortunately I cant say to much more on using npm with MVC. Personally I love to use asp net core for writing an api, then having a separate project for the UI which uses for example React and Webpack. To be honest, even knowing that bower is is pretty much dead, if I were to do net core MVC with razor right now I would probably just use the in built bower functionality for its convenience and support in the docs, even though it does not feel very good to do so : /
This is what we use http://github.com/okhosting/OKHOSTING.UI
You can definitely stick with .net for the backend stuff. A lot of people like MVC, but I prefer just using webapi and a front end framework to visualize the information. Modern web development is a complicated business these days. No matter what you need to get familiar with javascript, as quickly as possible. Past that you need to look at some of the frameworks that are available and figure out what's best for your needs. For example, angular, knockout, vue, react, and many many more. Then there's package management so you're not re-inventing the wheel at every step. You'll want to read up on bower, jspm, and npm. And there are task runners such as gulp and grunt. This may all seem overwhelming, but once you start working with these tools they'll all kind of snap together. If you're serious, I would recommend picking up a subscription to pluralsight or safaribooksonline and start on a learning path lead by experts. 
https://media.giphy.com/media/1M9fmo1WAFVK0/giphy.gif 
The usual reason for this is when you have a code base in a language which doesn't scale well or is difficult to manage and you can't rewrite it easily. They're also sometimes used by companies when a particular language is substantially easier and cheaper to hire for or is much more productive for developers. PHP has a lot of really cheap developers and a lot of developed code, but is simultaneously a steaming pile of shit. There are more transpilers for it than any other language.
If you have a good naming convention for your unit test methods, a lot of that is taken care of for you. I think the below style is a recommended Microsoft practice, but I'm not sure; someone at our company told me to name a unit test as (method being tested)\_(input)\_(expected output). So for example the business wants the code to only do something if a user's input in a text field is a non-negative integer. /// &lt;summary&gt;Returns true if the string is an integer and greater than (or equal to) zero&lt;/summary&gt; public bool IsPositive(string input) { int parsedInteger = 0; if(int.TryParse(input, out parsedInteger)) { return parsedInteger &gt;= 0; } return false; } Syntax might be a little off, and the whole method feels a little smelly to me (maybe should parse it into an integer elsewhere since we will likely be using it as an integer if we're doing this check, and the name leaves people unsure how it handles 0). Let's ignore that and focus on testing. Given the business rule, the obvious test cases are inputs like "5" and "-4". The test method names are: IsPositive_positiveNumber_true IsPositive_negativeNumber_false Now you also want to verify some edge cases like "0", "Hello", and null. IsPositive_zero_true IsPositive_nonNumeric_false IsPositive_null_false So that's a simple one where you have a single string input and a single bool output. What if you have a method that does something that sets multiple fields in an object? /// &lt;summary&gt; Converts the viewModel with all our data annotations into an equivalent address that the backing API will understand. &lt;/summary&gt; public ApiAddress ConvertToDatabaseAddress(AddressViewModel addressViewModel) { ApiAddress apiAddress = new ApiAddress() { Name = addressViewModel.Name, Country = addressViewModel.Country, // ... and a bunch of other fields. }; return apiAddress; } The business rules for this method are, "All the fields should be copied. If the input is null, the output should be null." I see two test cases right away: a good address is fed in and a null address is fed in. ConvertToDatabaseAddress_goodInput_allFieldsCopied ConvertToDatabaseAddress_nullInput_null In the first one, you would have a single method that has asserts on all the fields you expect should be copied. The second one only asserts that the output is null. The second one would currently throw an exception the way I wrote the method, but that's fine! The unit test will fail and the issue will be caught right away.
Could also use extension methods to accomplish the same thing. public Mock&lt;IUserService&gt;MockAuthenticate(this Mock&lt;IUserService&gt; mock, string username, string password, Guid output) { mock.Setup(x =&gt; x.Authenticate( It.Is&lt;string&gt;(i =&gt; i == username), It.Is&lt;string&gt;(i =&gt; i == password) )).Returns(output); return mock; }
For phone development in C#, take a look at Xamarin. 
As much as I agree with the above, it's accurate and well informed, you can just roll a new MVC project and move with that, the lovely front end frameworks have a huge learning curve for beginners. What we take for granted is going to be an uphill struggle. The concept count is insane. At least knowing C# you're surface area is going to be smaller. So yes, all of the above but just start at MVC, learn the pattern, about HTTP, get it done and then iterate with the front end frameworks. Get pluralsight and get moving :)
I was going to ask why would anyone want something like this, but that actually makes sense and proves the point of how bad PHP is.
It's not necessarily how bad PHP is. The big players have been doing this for more than a decade. Facebook was transpiling PHP to C++ because C++ developers were expensive. Lots of start-ups do the same thing. They don't choose long term languages, they choose fast to develop easy to staff languages.
Use Yeoman to fire up a suitable generator. From there learn about project structures and get started with a good foundation to your project. 
Hmm, install instructions for Yeoman say I should install it with npm
Then I'd say I'm stuck at the fact that bower tells me to install it with npm? Edit: But thats the Bower page, perhaps I should look at the link you sent. Its just that they often assume you're not building it from the ground up and that the things are there automagically
Install yeoman with npm does not mean using npm in your actual project. Have a quick experiment and install it, then search for a generator that fits your purposes here; http://yeoman.io/generators/ The end product is a fully working starter app for your given technology stack. The generators all have separate GitHub repos explaining what you'll end up with. 
For something a bit different, I'm recommending my open source project: https://gitlab.com/hodgskin-callan/Invention Happy to assist a fellow hobbyist if you are interested.
Are you sure the reason is not PHP being bad? I don't really think the reason was that C++ developers were expensive. Mark realised soon enough that it was a mistake choosing PHP was a mistake and that's why he hired one of the best people in the industry to create a language of their own.(Hack)
Thank you for your very insightful comment.
just use another endpoint in your new app and call that, then use middleware there that then calls your legacy app
I would change the dist folder to point to your www root. That way you can use default files middleware and static files middleware to serve Angular. Also, you can create a 404 handler that redirects all 404 errors to index.html so that you don't take care of reloads. 
this said, if you're using MVC and a forms based front-end, you're not going to have the kind of coherent application experience that we take for granted in WinForms/WPF. That sort of experience is why we have single-page-application frameworks like Angular/React/Vue, and so once other immediate hurdles have been overcome, it probably makes sense for OP to pick up one of these.
Compared to PHP developers, C++ developers, especially ones that aren't shit are much more expensive, both in terms of salaries, but also recruiting costs. A decade ago every mouth breather who could turn a computer on was learning PHP. That's why there are so many bloody WordPress plugins. You could find a PHP coder in under a week. PHP as a language was also shit, but there are transpilers from pretty much ever dynamic language you can think of that were written when the language didn't scale.
Sound point ;)
I find developers on Upwork
You should probably crosspost this to the popcorn subreddit.
Make first sure to contact your employer. Most contracts forbid side-work.
`Lazy&lt;T&gt;` has absolutely nothing to do with the C# language. It's just a class in the framework.
I also tried this and gave up, seems to be fuck all in the UK. Like you say Freelancer is bad, it's full of indians who can't type more than a couple of words of english posting shit like "need c# dev apply here" like wtf?
I guess my best option is just work on side projects and hope one of them earns me money.
Develop a mobile game or app using Xamarin. Free with ads or paid. A friend did a Xmas app that all it did was tell kids if they were naughty or nice from santa and made a few hundred on ads.
I've never used Xamarin, will have to check it out. Thanks
[removed]
Is this a UK thing? 
Hey guys plz check this simple and easy tut on svn basic
Hmm. If you are trying to stay away from EF then I would suggest using [Dapper](https://github.com/StackExchange/Dapper) for calling your stored procedures and hydrating your models. As for generating models from an existing database without using EF perhaps [this](https://github.com/ericdc1/Dapper.SimpleCRUD/wiki/T4-Template) would fit the bill?
Can we get a Dapper demo with better audio? I would really like to learn alternatives to EF
I'm in the UK and every job I've had says you cant make apps if they have anything to do with what your work does. I'd imagine that was the norm, otherwise you could compete with the company you work for.
This is common in many countries, including Germany and the United States of America.
We should be specific here as in the US I've never seen a contract which forbids side work. 
The USA is a large country. There are plenty of contracts out there that even claim "whatever you do in your free time belongs to the company". I do not know how legal or enforceable such claims are.
yes there is one more video showing about database connectivity in that ...here is the link https://www.youtube.com/watch?v=YxvpBee4NBc 
in dapper we writes sql queries nude in our code ...but in EF writing queries is easy compared to dapper
Why would you want to do this instead of using git?
see...it depends on client
Sorry, I don't see it. If your client doesn't already have VCS, suggesting SVN is just bad counseling. If they already use VCS, and SVN is their system of choice, you don't have to set up a server. Just starting out with VCS? Use git. SVN served its purpose back in the day, but it shouldn't be your goto VCS in 2017. 
I'm assuming this is your video...I'm not sure where your microphone is but your keyboard is louder than you are. I had to turn volume way up to hear your voice but made complicated by the background noise. PC Fan? Hairdryer?
Do you know if T4 will work to map stored procedures?
Generally, they are considered un-enforceable, but its never been rigorously tested. My employment contracts states that I need a waiver from the company for any side work I do, every time I've asked for one they always just asked the scope of the project / work and have never made a fuss over it. They just don't want me working in the same space I currently work for them in
Nice. Maybe Microsoft will include it in the next Windows 10 update! :D
I like both Dapper and EF and I would say that they are rarely alternatives for each other.
I just installed it, how would you even use it?
Why would you use dapper?
I can't get enough of low quality videos narrated by someone with a thick Indian accent. I've seen so many now but thankfully they still keep coming :-)
Create an .edmx database first file (ADO .NET data model in templates I believe), and point to your DB. This should generate your POCO's, and you can edit or replace the T4 from there.
Speed and control. https://github.com/StackExchange/Dapper#performance
When using an ORM like EF, you will often run into a performance wall around bulk, batch, and streaming operations where you will need to get closer to the database to solve your issue (e.g. using database bulk import operations and other database features). Dapper is a good tool for getting closer to using the database directly without having to write a ton of boilerplate ADO.net and object mapping code. I find that in most projects where you would want to use an ORM like EF or NHibernate, that it probably wouldn't hurt to also use a micro ORM to cover the higher performance sections. Dapper really hits a sweet spot for that use case. Really, Dapper is a great drop-in replacement for most cases where you would want to write ADO.net code directly.
Thanks! I'll look into it
That sounds very intriguing thanks!
I know it's not a popular alternative but I use Fiverr and I make some money. I am also a dot net developer but I use Javascript and PowerShell on Fiverr. 
Fantastic write up. I am working on a very large project right now where many random reads and writes would be happening in a DB. Is it worth using dapper, or utilizing the caching abilities of EF?
The rule that everything you do in your free time belongs to them is indeed un-enforceable. But the rule that you're not allowed to have side-work without permission is absolutely enforceable.
What service do you offer?
seek a better paying job
Google sheets and PowerShell scripts. 
Not really possible, I'm a home owner so I don't really want to move and commuting would cost me to travel.
I wrote a blog post a while back on using Dapper for creating a simple API. In it I'm using DB2 and showing how to use it with that, but it's not far off on what you'd have to use for something like MSSQL as well. http://jrcook.net/2016/10/24/using-dapper-with-db2/
there are jobs that allow you to work remotely
When you don't want all the unnecessary stuff that EF has - which is very nearly all the time unless you're writing some huge complex enterprise system. Dapper is also incredibly faster. There's a reason why Stack Overflow created it and uses it instead of EF.
Yeah you can go with dapper.... Am using it in in one of my social site where I have huge database. Dapper is giving good performance there
Instead spreading hate you could have added your thoughts and knowledge on this topic ...this gesture would be appreciated
Normal assert will exit the test once it fails and repport that assertion as an Failed test, while multiple will collect all of the failed assertions and report them.
I literally could not understand a word you said. Audio is too noisy. 
Thanks...will work out on audio :) 
Literally do not bother doing this. There's absolutely no money in it. As a learning experience, sure, but read *this* if you have *any* illusions about the possibility of making money with a little app / game: http://enterprisecraftsmanship.com/2017/07/17/a-story-about-how-i-tried-to-get-into-game-development-and-failed/ tldr; Even a technically excellently executed app / game whatever will take a significant amount of time to do, and, realistically, make no meaningful income. So like, sure, do it if you're bored, but: &gt; would like to supplement my income using my skillset. No. Just no. This isn't the solution you're looking for. Realistically you'll have to fail and waste 2-3 years of solid effort trying and failing to get into a position where you're familiar enough with the space to make something that even has a *chance* at being successful.
You may have some success offering a patreon account similar to https://www.patreon.com/AlanZucconi or https://www.youtube.com/channel/UC2CRYvS0FWRkTpCU3l4j8Mg that teaches people how to do various things. It's a very tight margin market, where even the highly successful folk only make a few hundred dollars a month, but if you only have a limited (ie. out of normal work hours) amount of time to invest, you could do worse than picking some kind of specialist area that many people are interested in (ML in C#, shaders, whatever) and writing tutorials about it. The options aren't amazing, no matter which way you look at it: - freelance for bargain bin prices against internationals - educational blog / video blog / whatever for pitiful ad / sponsorship revenue - invest your time in an app / game which will probably never make any money - get a better job / become a contractor via a recruitment agency Realistically, if it was that easy to just make some extra cash on the side, everyone would be doing it.
thanks..i will work out on audio 
It's hardly hate - it was light hearted, I even put the smiley at the end to indicate it wasn't to be taken too seriously. It's testament to India's growth as a country that it supplies the world with so many software engineers that in turn see so many video tutorials by them available online.
As a general rule, EF is fine for smaller projects... but not larger ones. It still mystifies me that ASP.NET Identity is built on top of it by default.
Perfomance against a huge database is more to do with the DB schema design being good and smart use of indexes rather than the database layer in the application. Micro ORM's are almost always quicker than EF or NHibernate regardless of the DB size.
excellent 
Each unit test should ideally test each "rule" that the method must abide by, and each rule should have it's own unit test (so that it's easy to work with if a rule changes down the line).
&gt; educational blog / video blog / whatever for pitiful ad / sponsorship revenue If going this route be advised that getting the number of people to generate any ad revenue or to seek sponsors will take a lot of time.
&gt; educational blog / video blog / whatever for pitiful ad / sponsorship revenue If going this route be advised that getting the number of people to generate any ad revenue or to seek sponsors will take a lot of time.
This is a subreddit for discussion of the .NET framework, I don't think that's an appropriate question.
Got my mistake it was my bad.... thanks for kind words about india
&gt; should all of them be registered in the IoC container nope. If you have multiple implementation of `IFoo`, and at runtime you have to pick which `XxxFoo` to use, consider something like https://en.wikipedia.org/wiki/Strategy_pattern The examples make the 'how do I pick the concrete implementation?' part of the pattern seem somewhat obscure, but it's really not complicated. You have basically a `WebScraper` (or pool of them) that holds a single `IProcessWebData` every time before you invoke `scraper.Process(remoteUrl)` you invoke `scraper.SetStrategy(...)` or, more likely, something like: var process = HostNameChecker.GetStrategy(url); worker.SetStrategy(process); worker.Process(url);
**Strategy pattern** In computer programming, the strategy pattern (also known as the policy pattern) is a behavioural software design pattern that enables to select an algorithm at runtime. The strategy pattern defines a family of algorithms, encapsulates each algorithm, and makes the algorithms interchangeable within that family. Strategy lets the algorithm vary independently from clients that use it. Strategy is one of the patterns included in the influential book Design Patterns by Gamma et al. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
It sounds like you want an IDictionary&lt;domain,IWebScraper&gt; you can populate the dictionary in your IoC container and inject it where you want it.
Yeah, it's pretty much the same problem as making an App; it's just that the outlay of making an app (say, 3-6 months maybe) is significantly more than making a video (3-6 hours maybe). So you can generate more content more quickly... but ultimately, you're completely right. It's hardly a 'quit your job' kind of thing.
I like using jQuery DataTables to accomplish this on the client side to reduce server overhead. https://datatables.net/
What kind of project is it? What is 'many' random reads and writes? If you are working on a web project and plan to do hundreds of database hits per user request, then you have a problem than an ORM / micro ORM choice isn't going to make much difference for. Reorganizing data requests into set based operations and caching wherever it can be safely used can help mitigate needing hundreds of bits of data per each user request. I don't think I would rely on any EF caching though. I was under the impression what EF had was more of an identity map than an actual cache, anyways. Unless you are talking about one of the second level caching providers you can plug into EF? 
I'll second this. DataTables works great for this. Lots of ways to customize it as well.
Well you need to answer a question first. Do you want to sort on the front end or back end? If backend, pass some value or query param to the server when rendering the page to tell it to sort. You can use the order by in linq to do this easily. If on the front end, I would recommend just going with jQuery DataTables, it is most definitely possible to do it yourself but the implementation would take a while and I personally wouldn't do it unless I absolutely needed it. As always, posting some code would be best. We could then begin to guide you based on what you already have.
I'll third this. DataTables is amazing.
You can register a special service, something like IWebScraperFactory, that will select/crate IWebScraper based on domain name. Then your controller or other logic code will use IWebScraperFactory from IoC, create scraper and use it. interface IWebScraperFactory { IWebScraper GetScraper(Uri url); } var scraper= _webScraperFactory.GetScraper(url); var result = await scraper.Scrape(url);
Ah! I wondered when the factory pattern would come in useful. Thank you.
Came in here to mention Strategy pattern as well.
Somewhat irrelevant question, but what area of the UK do you live in and what are your rough earnings?
Sorry, I'm on mobile and about to go to bed, but have a look at this thread https://stackoverflow.com/questions/6864438/access-control-list-acl-abstraction-layer-in-net You're correct in saying that roles alone won't suffice, unless you can get away with simple recursive parent child relationships and can work with role inheritance.
Nice to have this workaround. But it would have been better if this was built in. See https://github.com/dotnet/cli/issues/3773. It has been months and basic usability issues like this still persist :(
Thanks for the suggestion, but I am worried: * Not updated since 2011: will this even integrate with MVC 5 / Identity 2.0? * Evaluant (company that built this) no longer in operation. * Hosted on CodePlex: about to vanish.
Note that when combined with an IoC container, this doesn't preclude registering all of your implementations of the common interface with your IoC container. Remember, even inside of a factory, "new is glue" - try not to couple your types if you are interested in testability or runtime flexibility.
I'm distracted atm with some other things, but I've recently architected a very similar project (IdSrv 3, angular 4, web api, Repositories), and I was thinking about how I would achieve what you need in my solution. You're right that you have more than just roles at play here. You also have business logic which will need to live in logic layers somewhere. That logic may be aided by roles, though. What you'll really need to leverage roles for are to secure your endpoints. Have you considered the "Resource/Action" authorize pattern? I think you could achieve what you need, if you treat "groups" as a resource. Following this pattern, you could decorate your web methods with attributes like: [ResourceAuthorize(Actions.Sale, Resources.C)] [Route("api/c/sale"] public HttpResponseMessage Sell(HttpRequestMessage request, Order order) { // get the caller principle from the httpcontext here if you need to look at users claims // call logic layers here } This way you can secure this endpoint so that the user must have the "Sale" action against "C" resources. To accomplish this, we'd created roles in the idsrv database with names like "Sale.C", which, if you had that role, would mean you had the "Sale" action against the "C" resource. You could give this role to your B2B to secure that endpoint so only B2B's get the ability to call this method. We had to take security a step further too, and we implemented our own version of the ResourceAuthorize which would also secure id-specific roles. We generate roles for all our tenants, like "Write:Tenants:1", which means "I can write to tenants, where the Id is 1". This allows us to secure endpoints against individual resources now, dynamically based on the resource being requested, and the user's claims. [More information here if you go to "Resource Authorization" section.](https://identityserver.github.io/Documentation/docsv2/overview/mvcGettingStarted.html) The other stuff you mention, like commissions and things.. those should happen in logic layers, and your logic layers can be aware of further "things" to do against your resources, while checking the caller's claims principle for claims and/or querying the db using the caller's identity ID to gather further meta information. You can trigger this logic during other web calls, or schedule them with web jobs. 
Considering the complexity, I would look into claims based authorization.
A couple diagrams might help communicate the problem you're trying to solve. It might make the players and their interactions more apparent than all the text you wrote, and elicit more participation.
Wow, that's sounds ugly, and perhaps like one of those MLM schemes? Just wait until they start asking for customization for different companies! A few thoughts. Sounds like there might actually be several different applications that should be separate in there. I'd be especially careful with sales commissions/territories. My experience is that they change often and randomly and have lots of special cases because sales team structure and incentives seem to always be in constant flux. You likely will need to store/compute on trees in the database, which are exponential in complexity. Look into [hierarchical data in the database](https://www.sitepoint.com/hierarchical-data-database-2/) You no doubt will need custom auth, with complex permission system. If you're putting multiple companies data into a single database, I've had good experience creating a system level repo/dbcontext, then another company level repo/dbcontext that appends the company specific record restrictions. You can then setup the correct client repository based on the user making a request. This to me is a key piece of data compartmentalization because it's damn near impossible to never make a query mistake and leak some data from client to client. A couple discussions on multi-tenancy [https://www.dotnetrocks.com/?show=1332](https://www.dotnetrocks.com/?show=1332) [https://www.dotnetrocks.com/?show=1431](https://www.dotnetrocks.com/?show=1431). You might think hard about how to use the modern cloud to your advantage. It's an attractive path to push the mutli-tentant complexity out the application itself and onto the deployment system. Meaning use a different app/database for each customer which gives great isolation and ability to scale/customize if necessary. 
What you're looking for are Custom Role Providers, which allows you to extend the base Identity classes and managers into a more fully-featured (or horribly twisted) system. The last system I worked with used a system of Users, Companies, Roles, and Permissions, with the latter being an enum drawn from the database via a T4 template (thus it never changed unless the application was recompiled) and the former three were mapped between each other. What they called Permissions were actually what was stuffed into Identity as Roles. Controller actions were decorated with an overridden version of the [Authorize] attribute which determined access based on Permissions. If you're wondering why they used a T4 template, it was because the wanted to keep relational integrity in the database for a variety of reasons, so the T4 tied the application to the database at compile. Wouldn't have been my first choice, but it worked. Also kept magic numbers and strings out of the authorization process and attributes so...I guess it was OK. The SaaS admins would handle user creation, soft admin perms (Company admins), and global perms, but the Company admins were free to create, edit, or delete Roles within that Company and assign Users/Permissions to them however they saw fit (we had extensive auditing trails to cover our rears). It worked quite well, although I suspect the Company admins were certainly cutting corners (making groups with blanket permissions and assigning all users to those groups...), but that's not our problem, and for a nominal fee we could fix just about anything they could screw up.
Good write up. I'm pretty keen to use this stuff myself. Can't wait for 2.0.
How?
Actually I've already implemented my own backend pagination so I don't really know if I can have backend pagination and front end sorting with data tables as suggested by u/HokieFreak
Hello! This pretty much hits the nail on the head. Additionally, there are plenty of features one platform can borrow from the other, e.g. consuming/distributing really useful PHP libraries (FPDF for example) or entire frameworks (Wordpress for example) as NuGet packages.
I went too a great talk last night in Liverpool where Gary Pretty a Microsoft MVP gave a great talk on Microsofts .NET Bot Framework. He has loads of stuff on chat bots on his blog if you want more information http://www.garypretty.co.uk/
Sorry this is a bit old, but here's my two cents. Bower is pretty simple, even if it is "deprecated." It still works, and is closer to just manually adding scripts to your project or pointing at a CDN and getting off the ground quicker. If you don't need crazy packages with many dependencies, by all means you could keep Bower and not worry. NPM (or Yarn if you want to jump on that bandwagon) are much better now that bundlers are easier to come by. If you do use more complicated packages (e.g. an entire framework) you would be better off with npm and webpack. There's some great documentation on webpack and even better examples out there, but it is definitely more work than the simple Bower management.
Fair enough. What are you displaying the results in? Repeater? Grid?
Seconding this. I've done something similar in my projects and we have a shared IDS3 instance that we use between our APIs (and to manage identities across the various identity providers).
 @foreach (var item in Model.AnalyticAccountLines) { &lt;tr&gt; &lt;td&gt; @Html.ActionLink(item.WorkDate.ToShortDateString(),"Details", NavConstants.Accounting.AnalyticAccountLineController, new {id = item.Id}, null) &lt;/td&gt; &lt;td&gt; @Html.ActionLink(item.Project.DisplayName, "Details", NavConstants.Projects.ProjectController, new { id = item.ProjectId }, null) &lt;/td&gt; &lt;td&gt; @Html.DisplayFor(model =&gt; item.Description) &lt;/td&gt; &lt;td&gt; @Html.DisplayFor(model =&gt; item.Quantity) &lt;/td&gt; &lt;td&gt; @Html.DisplayFor(model =&gt; item.Amount) &lt;/td&gt; &lt;td&gt; &lt;div class="label @Html.AnalyticAccountLineStatus(item.Status)"&gt; @Html.ResourceEnum("AnalyticAccountLineStatus_" + item.Status) &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; }
If that's what you're referring to
What version of MVC? &gt; I'm trying to integrate my API from a completely separate site into my main site &gt; I'm looking to merge the sites because I have a monolith It sounds like you're going to make it more of a monolith.. Is that what you want? &gt; I've had to do weird RPC things to clear caches on the main singleton instance when API calls are made This is what messaging is for. I have a system with a dozen nodes, all sharing Redis caches (as well as in-memory caches). I use Redis pub/sub to fire "clear this item" messages to all the nodes. It's a high traffic system and it works well. 
I would suggest having: * One Web API with resource versioning * Per unique subdomain * With an independent branching structure * With an independent CI/CD pipeline You mentioned wanting to strangle the monolith, so breaking things out instead of combining them would be advised. If you need to share components, version them and publish them as NuGet packages to a feed. You can use an API versioning scheme to help with the cache clearing problem.
Any .Net devs who can't find work don't live near Nashville.
Put API controllers into own folder. /website/api/(controllers) /website/(controllers for web content) Without having all 3rd parties update paths to new API, use [IIS URL Rewrite](https://www.iis.net/downloads/microsoft/url-rewrite) to send request to correct path when the domain is "api.mydomain.com". The website would use your wildcard cert, so it could handle api.mydomain.com and mydomain.com. 
Hey, I think you could do better by using InstallShield to call one of the commonly available CLI applications which can execute scripts for you. Or better yet, just use the CLI tool and drop InstallShield. Why is that in the picture? https://github.com/lecaillon/Evolve https://dbup.github.io/ https://github.com/chucknorris/roundhouse/wiki/GettingStarted
Or mercurial.
Mainly because I was asked to do so, my inexperience definitely shows when it comes to things like this. Basically the people who I am doing this for just want the scripts to be run together as one batch so there's not 20+ steps of "Run the following scripts", which is really easy to screw up. They simply asked if Installshield could do it, since that's what we've use before; I didn't know, so I tried to figure it out. I will look at those tools. In case I still have to, how do I make Installshield call one of those tools? Thanks!
https://stackoverflow.com/questions/24177181/execute-command-line-statement-in-installshield-installscript
Code-behind pages, old friend. Looks like we've come full-circle. 
Thank you!
Or Charlotte
This sounds a lot like ASP.NET Web Forms on .NET Core, with some goodies from MVC and without the old Web Forms controls. &gt;This is NOT a new implementation of old ASP.NET Web Pages Ok 😉 
Huh? This is nothing like web forms.
Front end pages with code behind pages doesn't sound like Web Forms to you? 
More like status of /r/dotnet blog spam.
Gotta be honest dude, as a modern day dev, your options would open up if you had ability to do more than c#. 
Usually only work in the same field. You can do programming for unrelated fields. He would have little legal trouble doing embedded software development for instance. You'd have to stumble into a real unique contract to get in trouble for any work for other companies.
With monthly travel. Very common.
Yes. 20k less, and far more interesting job with a slew of opportunities already opening up for further advancement.
Personally I'm an Autofac fan, so I would use thier keyed-services approach: http://docs.autofac.org/en/latest/advanced/keyed-services.html?highlight=iindex#resolving-with-an-index This is very similar to u/scherlock79's [suggestion](https://www.reddit.com/r/dotnet/comments/6qoawk/utilising_multiple_implementations_in_a_single/dkyqvbo/), with the pros/cons of being a bit more specific to the container technology in use. If you don't like to use container-specific types inside of your own code (possibly out of some sort of superstition that it might be hard to change containers later), you could alternatively write a "meta"-factory that looks something like: class WebScraperFactory : IWebScraperFactory { Func&lt;WebScraperImplementationA&gt; _scraperAFactory; Func&lt;WebScraperImplementationB&gt; _scraperBFactory; public WebScraperFactory(…) { _scraperAFactory = scraperAFactory; … } IWebScraper GetScraper(Uri uri) { if (…) return _scraperAFactory() else if(…) return _scraperBFactory(); … } } This of course assumes that your container will provide the inner factory implementations (either automatically, like Autofac can, or manually wired up). Personally, I would use the first approach I suggested. Regardless as to which container piques your interest, find time to familiarize yourself with its bells and whistles - you'll find that these kinds of "problems" are so common that people smarter than either of us have already spent a lot of time solving them. 
No....web forms generated shit tons of bloated asp code with each drag and drop control. Native html pages calling razor pages.....whole different ball game.
Usually for **any** *higher* work. You're expected to rest in your free time - not to work even more.
Don't know what higher work is. But every dev I know does something on the side, often for fun and not money.
I see this being useful for a SPA. We are currently building a React application at work that only has a single view and does all communication over a websocket. So in essence it is not an MVC app so using an MVC pattern is a strange fit. Add some routing magic to mix and you have a nice base for a SPA. Could have easily done that with Web Forms aswell but that wouldn't be nice and shiny.
With a bit of Rx and the help of LINQPad support for IObservable, you can add autorefresh :)
What made Web Forms shitty wasn't the code behind model, but rather how it tried to shoehorn the client-server model of the desktop Windows Forms world on the web. It resulted in a complicated event driven page life cycle with ugly (and large) hidden values to keep state in an otherwise stateless world and ugly bloated auto generated html.
I started programming on the web with classic-asp, VbScript. When I finally switched to .NET, WebForms was all we had. For years, I hated creating webpages with it. Then Asp.net/MVC came out. What a game changer! I've never been happier.
Now try to do some performance testing around the if-else approach you discarded and your exception throwing code; and get mesmerized at up to 20 times performance loss with your approach...
Now I get &gt; do some performance testing around the if-else approach you discarded and get mesmerized or &gt; do some performance testing around your exception throwing code and get mesmerized But for some reason my brain hurts from reading: &gt; do some performance testing around the if-else approach you discarded and your exception throwing code and get mesmerized Is there some comma missing or am I just slow?
But in your case shouldn't default(T) be used instead?
I'm trying to find docs on how to implement the drop-in-place button: http://imgur.com/a/SL3vf I can't remember where I've seen it in the wild, but it's similar to the Log On Using Google button, explained here: https://developers.google.com/identity/sign-in/web/sign-in I'm looking through the Kantana library examples, and not seeing what I'm looking for. Am I barking up the wrong lightening bolt?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/KEBuxyB.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dl26olg) 
Yeah, good point. Actually I have some other code that does that. This is an old code block that I remembered used it and is the only example I have (deleted now, stupid example).
Maybe &gt; Now do some performance testing around the if-else approach you discarded vs your exception throwing code and be mesmerized because your approach is up to 20 times slower
Very helpful, thanks
If you have to instantiate at runtime a class dependent on external data, you might not really have much choice. You might be able to emit code to make that creation faster, but if you are not doing the Activator in a tight loop, it's the simplest way to get a class. Alternatives: Lambda, Code Emit (see https://stackoverflow.com/questions/4432026/activator-createinstance-performance-alternative )
This has dangerous security implications - see (pdf warning) [this black hat talk](https://www.blackhat.com/docs/us-17/thursday/us-17-Munoz-Friday-The-13th-Json-Attacks.pdf) about what can go wrong when you allow attackers to initialize whatever types they want. That talk deals with json deserialization which can be a bit worse than your stated use, but the general premise is the same. If you must do this, I would strongly recommend you to whitelist the set of permitted types. 
Thank you for the link, and actually he's doing this database dependence to not instantiate the classes directly and use interfaces so the code looks like this: var a = (ITheInterface) Activator.CreateInstance(Type.GetType("ClassName")); a.InterfaceMethod(); I know the use of interfaces is recommended but I'm not sure about this way of instantiating.
Why is this necessary?
He wants to avoid the use of a switch to instantiate the object needed.
I started with classic ASP as well! ASP.NET Web Forms was such a breath of fresh air after that shit show that I was blind to its problems until MVC showed up and I saw the light LOL.
Need to understand the larger context but there's definitely no opportunity to use a dependency injection pattern and use a factory to generate the dependencies? See the question and answer on this stackoverflow; https://stackoverflow.com/questions/10285946/ioc-ninject-and-factories
Wait, all of this is to avoid a switch statement? What is the argument against the switch? I'm assuming there is a well know set of classes you could be instantiating, it would be dangerous to just trust your database to only have safe values.
1. It is a factory. 2. You can inject the dependencies using property setters or an initialize method 3. It probably shouldn't have dependencies in the first place. 4. If you really need it later, you can always change it to a `System.Type.GetConstructor` call that passes in the possible dependencies. I swear, the obsession with dependency injection is ridiculous. 
Are edmx files still a thing?
My solution is to store the class names in a read-only lookup table. If you have access to edit that table... well you can already do anything else you want to the database.
I've done it countless times in the past. It's not my favorite technique, but it works quite well. `Activator.CreateInstance` is slow. So if you run into performance problems you'll want to use call `System.Type.GetConstructor` instead and cache the function it returns in a dictionary. `System.Type.GetConstructor` is also useful if you later decide you need to use a non-default constructor. 
1. Switch statements are annoying to maintain. (Ok, not a good reason but its the most common one.) 2. The classes are found in plugins created by other teams. This is used when your building a framework, especially commercial ones like CMS and CRM systems. 
Well, if something exceptional happens in your code, you should probably throw an exception, if/else is good for the C.
Database changes and dumps are one thing, pseudo-arbitrary code execution is an entirely different manner of beast. Instantiating *any* existing class is dangerous. If you want the class name in the database for plugin management, which it sounds like OP is doing, you're going to want to validate that the custom type contains a specific whitelisted interface *before* instantiating it instead of waiting for a cast or dynamic access call to throw an exception after the constructor has already run.
You don't know much about databases, do you? Admin access to the database usually includes the ability to execute OS level commands as if you were the database. Depending on how badly the database is configured, that may be root access on the server as well.
I personally use a pointer in the database to a dictionary with the function or class names, then build off that with create instance.
Database-driven dependency injection! :-)
This is great news
You can also go towards reflection.emit or system.linq.expressions to create factory delegates for zero overhead object creation.
No sure of your exact situation, but I have a couple of situations where I need to reference implementations of a class in a database. My approach is generally: * Use an enum with values for each different implementation. Bonus points here because storing the underlying values is slightly more performant than storing strings. * Create an attribute which stores this enum. * Add this attribute to each implementing class. * Use reflection to build up a dictionary of enum values to types. Depending on specific requirements, I'll spin these up either with Activator, the IOC container or by compiling a delegate. 
Exactly. But the article says that instead of returning values the OP chooses to throw exceptions. It can cost up to 20x more due to how exception throwing is implemented in dotnet runtime.
Added a semicolon, does that help?
...or the type sits in a strong named assembly and you control the snk...
Thanks for sharing this. I've read through one of the guides, and they're a great read! Finally a doc that gives direction vs googling for best practices. 
Then run. I've seen far too many projects come close to failing directly (and indirectly) by that horror. Thankfully it is usually pretty easy to rip it out.
I've been on MEF projects where MEF itself was failing on startup 5% of the time. The developers swore everything was fine and there project was "successful" despite this fact. I've also had the displeasure of trying to debug applications that had bugs in object constructors. While this isn't directly MEF's fault, it certainly didn't do me any favors when it trashed the stack traces. And don't get started on the performance hit you get if some idiot thinks the DI framework should participate in all operations instead of just creating the decency graph during startup. But again, this is an indirect problem and you can't blame MEF for people using it as an alternative to `new`. There's really no reason for MEF. It's less reliable and harder to use than pretty much any other DI framework or even just straight reflection.
This. Also.. - Exceptions should be exceptional. - Modules should catch their own exceptions. The ASP.NET conversion of an exception into a 500 response may be nice, but it`s also sloppy. - Well... this... &gt; catch (Exception ex) &gt; var myCustomExcecption = ex as MyCustomException; vs &gt; catch (MyCustomException ex) Yeah... Its a bunch of inventive code that should not exist. Yuck. 
This.
Came here to say this. (are you a THIS bot ?)
Yes! Not even sure how the semi colon is supposed to be used in this context, but it helped!
A blog post by the PR author with some background: https://cockneycoder.wordpress.com/2017/08/02/some-words-on-nugate/amp/
That's may not be what it's meant for, but that's always how I see it used in the wild.
I'm sorry but this entire PR thread appears to be full of whiners.
If the class doesn't have a public parameter-less constructor then you will run into problems. Maybe consider using some dependency infection framework, and only allow certain classes to be added the the container - then only those classes can be initialised via the dependency injection framework, along with their child dependencies.
Don't have much experience using MEF. But seems there's a new version of it [VS MEF](https://github.com/Microsoft/vs-mef) that's actually used in Visual Studio. Which one would you recommend ?
I had no idea Packet even existed before this so I appreciate that this brought it to life. That being said, you are correct. It is hard to take people seriously when you read things like &gt;Don't forget that ignoring people is a form of abuse too so this is advice for all.
We actually use the exact same code path on the front end to do Google and Microsoft login. I don't know exactly what you mean by "drop-in-place" button. Is there a link to some actual html/js logic behind that image?
I can understand why the PR author submitted this. I can understand the frustration by the PR author it's declined. I don't really understand why everyone else go so up in arms about it being declined. Honestly, if I were MS I would decline this for one simple reason: it makes it seem like that's an official tool from Microsoft and they'll start getting support questions, etc if something goes wrong, or people asking my running "paket ..." is giving errors, etc.
Maybe... But on the other side, if you want the .NET Ecosystem to flourish beyond the MS sanctioned tools / frameworks, they have to open their mind a little and welcome contributions from outside Microsoft itself. Also, as noted in the blog post, NuGet is part of the .NET foundation, not MS itself (the difference is probably minor.. but still). My feeling is that people who reacted on this PR are long term .NET developers that tried (and still try) hard to propose new tools to the .NET Ecosystem. In the end, it can only be beneficial for all of us. 
I agree, which is why I don't understand the community backlash in that ticket because the response from the team was (emphasis added was mine): &gt; We want to keep the content on NuGet.org focused on the NuGet and dotnet CLI scenarios in the **short term**, given the client usage we are currently seeing in the wild. Hence, we won’t be merging this PR into the repo. **Over time, we intend to make further changes to improve NuGet.org’s protocols with an explicit consideration towards promoting usage of other clients, and as a part of that effort, we will have guidance around this topic**.
Point taken. They also close the PR without further discussions, which I believe irritated some. Also, it sounds like MS have their own agenda and are not open to external contributions. That's just own I see it...
Closing the PR doesn't disable discussion though, and can be re-opened at any time. Honestly, this pull request is the equivalent of a pull request to the Windows Store website to add a Chocolatey tab if there's a package for that. Or the npm team adding a yarn install information. To me, this seems like the paket team wants more exposure, and I agree that [Microsoft has set precedent with this with how they are promoting Rider and OmniSharp](https://github.com/NuGet/NuGetGallery/pull/4437#issuecomment-319996459). I think the right thing to do here is for Microsoft to somehow promote paket but just adding a paket tab would be insufficient for that in my opinion. What the proper solution is, I don't know. Perhaps it's adding a top-level "tools" page to the nuget.org site, which would help paket get off the ground a bit more.
Amusingly, VS Code has had a terminal for a while now
Further discussion is warranted indeed, however the place for that isn't in a PR. Generally speaking if you have a feature suggestion the polite thing to do is open an issue first and submit a PR only once it has been discussed and agreed. I would treat any PR against any of my repos the same way regardless of value or merit simply for breaking protocol.
YESSSSS! Been waiting for this forever... picked up the All Products subscription back in Feb 2016 in anticipation of this release. Or rather, it is one of the big reasons that made me finally bite and get the full pack instead of just ReSharper.
So you find it better than VS + ReSharper? I actually have not a lot criticism currently for this this combination 
Very cool. I played with the pre-release versions earlier in the year before VS 2017 came out and liked it, but it still had some wonkiness so I couldn't use it full time. After a few months of VS it'll be interesting to play with it again in comparison.
no, visual studio is definitely still way more feature rich. but it's a start, and enough to get by on. being able to quickly do some work on my primary linux desktop without having to remote into a windows machine alone is worth it to me. not to mention load times, etc. it's not often that i need the features that visual studio has anyway, and it's better than VSCode.
I've been tinkering with it a lot more since RC release, as i had the same experience before - too buggy. there's really only a couple things that annoy me for daily use, but they're being addressed and i can work around them. namely: debugging async calls does not work properly (skips over them) no way to update all nuget packages, have to do it one at a time
Congratulations to the entire Rider dev team - the community has been waiting for a decent VS competitor for years and we finally have it! My Resharper license is not up for renewal for a while, but I will definitely be looking at the Rider option. To the Microsoft VS team: please let send these guys a cake. We know what a positive thing the friendly rivalry in the browser market has been for the Web. Right now, we should all be working towards a better future for .NET and Jetbrains is certainly part of that effort. 
Can someone explain why this approach is better than having an actual implementation of a factory?
Does anyone know if this will come as an IntelliJ plugin at some point?
highly doubt since it's sold as a whole IDE product
No, there are no plans to make an IntelliJ IDEA plugin.
It amuses me that Maven is pointed out as an example of working well with the community. Maven Central doesn't even have its own web site or search, they're completely provided by third parties. Hence, said third parties can show how to use Maven Central with a bunch of other Java dependency managers. Having said that, Maven itself has supported transitive dependencies for over a decade.
It's only available as a subscription. Thanks but no ! I've been using Visual studio 2015 and it works great. i don't feel like paying for the same software every year just to keep using it. What if I'm fine with y current version and I don't need your new «features» ?
Reading through it now and it's a good read. Very detailed.
When did you last use vscode. It's getting better and better month on month and you really can't beat the price.
Great news! Finally we have another IDE for .net
Does this work with Xamarin at all?
oh i agree it's awesome. i still use it fairly regularly for web development, working with json, quick text editing, etc. and i've certainly used it to write some C# as well - but using it for any kind of large project i'm pretty sure i'd go insane.
honestly, you get a lot for the price. i pay for the all products pack and use datagrip and resharper pretty much every day. i've also recently been trying to get up to speed with java for a potential project, so having access to intellij ultimate as well is a nice bonus. not to mention webstorm and the upcoming gogland. they definitely keep making improvements, so i don't mind the subscription. the price is discounted every year as well. i realize not everyone feels the same, but it's a no brainer investment to me. for less than half the price of an MSDN subscription i get access to a ton of tools that definitely make me more productive and able to make more money. and i don't have to awkwardly beg my employers to provide the tools i need.
The post says it does!
But is the cost of rider worth minor editing on your Linux machine? Without resharper turning it into a slow buggy steaming pile of shit VS 2017 is actually really pleasant to use. I'm basically at the point where except for a couple of really edge case refactorings there's literally nothing resharper or any of the jetbrains products do that's not done better by something else. Often for free.
to me it is. ideally it wouldn't be minor editing - i'd prefer to just ditch windows altogether. not there yet, but it's definitely getting closer and this release is a big step toward it :) edit: also, as i described in a comment in a separate conv on this thread - the price is not just for rider. get plenty of tools that make it well worth it. datagrip is a pleasure, having intellij ultimate came in handy. webstorm would be awesome if i did more ui work, etc.
I don't think Microsoft is against Rider at all. On their .NET Core page for Linux they even have a link. https://www.microsoft.com/net/core?WT.mc_id=Blog_CENews_Announce_CEA#linuxredhat
I think you get a key to use versions within your sub period after your sub expires if you don't renew it, you just don't get updates anymore.
Select all packages and right click for update. I have seen issues where Rider just deletes the reference from my csproj though, haven't tested on the release version.
I wonder if it's also written in Java?
You would need to provide bigger scope of code e.g. where do you store secods variable? And where and how do you use countdown function?
Yes, it's based on IntelliJ IDEA with a fitting set of features and plugins from ReSharper, WebStorm, DataGrid. 
I don't know a lot about licenses, but I do know [.NET CORE is under the MIT license](https://github.com/dotnet/core/blob/master/LICENSE.TXT).
So someone tried to merge a competitor to nuget, into nuget, and everyone looses their shit? Wow.
So does NuGet. They have had project.json for about 2 years and Visual Studio 2017 has introduced Package References. The Paket community just seems to ignore that. 
Agree or disagree with the decision, this had no business in turning into a flame fest. In addition a lot of posters do not seem aware of the fact that NuGet has support for transitive dependencies, which is weird since they took the time to actually comment on the thread. Lastly, just because something is OSS, that does not mean that everything should be accepted and merged. Might be just me, but I feel like the author could have reached out to NuGet/Microsoft in a different way. 
The idea is you get your boss to pay for it :)
I've played around with Rider as I love jetbrains tools and use them where I can and yes its great, the issue comes with older .net projects. In my job we have to maintain some pretty old projects and they don't play ball with rider, and while the new ones do, switching between rider for the new ones and VS for the old ones isn't much fun. 
Fixes for the async debugging problem have been merged, and are currently being tested, expected to go public in a bugfix update later this month.
Since it's ReSharper in the IntelliJ shell, we're talking about a mixture of Java, Kotlin, C# and some VB.NET.
What kind of projects are they, and what version of Visual Studio do you currently maintain them in?
vdproj installers. Microsoft dropped support in 2012/2013 but due to popular demand there has been a semi official extension that gets them working in later VS versions. https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.MicrosoftVisualStudio2017InstallerProjects
Thank you for the article. I have used mock a number of times but I would always end up copying another tests code, now I know why they are calling certain methods. I have never spent the time really learning MOQ.
I actually find it better than VS+R# in a lot of ways. I've been using it almost exclusively for the last month or so.
I edited the post. Thanks! Also, sent you a PM.
&gt; new to web-dev and learning MEF How did this come to be, an out of touch senior dev making you use it?
There was a [statement from a member of the NuGet Project Management team](https://github.com/NuGet/NuGetGallery/pull/4437#issuecomment-320285321) posted earlier this morning.
I think the best way would probably be to use an M2M protocol such as OPC UA. The Raspberry Pi would be an OPC UA server with which the SQL Server connects. Microsoft is doing something similar in the Azure IOT Cloud. https://opcfoundation.org/wp-content/uploads/2016/10/Microsoft-OPC-UA-5-Clicks-To-Digital-Factory.pdf For a hobby project that is too much effort, but if you want to make it professional this is the best solution.
Nope. It's a subscription, you pay monthly or yearly and you can use it (with all upgrades included). If you stop paying you stop using it.
haha well sort of I am in no sense new to web-dev more so the practices of MEF, I am second year in college and have a Internship somewhere where they are asking me to create modules that are plug and play essentially no need to bring down the web site to insert it on runtime. It is not that bad actually I have got to the point of Composing the dlls, right now Im just stuck at being able to access the methods within the external dll's, but yes I am in a pickle and have only a week left to complete this project and MEF isn't as straightforward as MVC was. GOD would I do anything to get this done. I also need to create a technical documentation, I guess much more of a pickle than I thought hahaha rip me.
They tried to merge a pull request to have the NuGet website give directions on how to pull packages using said competing tool.
Glad it helped.
I went down that road. I now run node red on the pi to read the pin states and forward messages over mqtt to an mqtt server running on the pi. I actually abandoned .Net and just do my whole home automation system with node red now. This way, anything else I want to integrate just needs a service that talks to it over whatever API it needs, and translates back and forth to mqtt.
I generally use Azure IoT hub as the interface between applications &amp; devices. You'll need to tack on some extra stuff if you want long-term, historical data, but it's great for current &amp; recent data, and supports bi-directional comms.
You are right with an assumption that &gt;It appears as if there are multiple instances of the timer running. This code below chat.client.timer = function () { means that code of this function will be called on each click as it is being called from singalR if I understand signalR logic correctly. And that causes that you create multiple instances of setInterval. Easier solution is having some global variable like here https://jsfiddle.net/w33aLsme/2/ But more elegant solution is introducing class with property containing timerInstance and onclick it would create new instance of timer if not exists or reset a current one. Let me know if you are intereseted then maybe I will find some time to write it down
Thank you for your response and the explanation! I am definitely interested in a more elegant solution. I have only been coding for about 5 months so I welcome any feedback and ways of doing things better! I will give your current solution a shot when I get home!
Just looked up setInterval, I think I understand now!
You can check the more elegant solution here https://jsfiddle.net/ckh0k7qj/1/ I use typescript on my day to day basis so in this jsfiddle you can see my original ts code which was transpiled by ts compiler
Use procmon
From what I have heard, from a C# (and .NET Stack) perspective, MS is very excited to have more ways for people to use their programming languages. From a broader perspective, they are cautiously optimistic but apprehensive that it might not be as streamlined for tying into other products (like deploy to Azure, for example)
If it's deploying to local iis, it has to be run with administrator privileges. Did you check the output window?
I use VSCode for C# all the time. It’s a good lightweight IDE and understands .NET Core better than VS2017 imo. At a certain point in the size of a project, I need features that basically live in Resharper like refactoring across classes and code clean up. Since going macOS only, I use Rider for that. Rider still has issues but it’s a good full IDE. So I hover between it and VSCode.
Your .cshtml is the View and communicates with the Controller using the Model. this should give you a fairly decent idea what to do: http://www.c-sharpcorner.com/UploadFile/cd3310/using-mvc-Asp-Net-tools-create-simple-login-form/
Honestly ... There's great ways to get a more direct connection - either a webapi if you're device can easily make a http request, or an iot hub from azure or otherwise - but if it's already writing to a database, just check the database! 
Test the important stuff first. This looks pretty easy to test and looks fairly important. It's probably work testing that you didn't screw up the logic. Mock out your various policy properties to test HasPolicies. Looks like it would take a couple minutes at most. The reason you want to test this is because you do have logic in here that would be easy to mess up (because of all the NOTs)
The important stuff I can't seem to test yet. It's the service layer/repositories that really throw me for a loop. I don't want to hit the database/call APIs directly with my tests. At least that's what I've been reading to avoid.
You are right. Those would be integration tests, not unit tests. You should be using interfaces which allow you to mock things out.
How do you mock out things like the EF database context?
Just check techempower test to see dotnet core performance
Are you running core or framework?
typically you use repository pattern instead because its hard to mock ef db context
How would this compare to using something like [Polly](http://www.thepollyproject.org/) with HttpClient? It seems like your project allows you to do retries and open a breaker, but once it does, what happens? With something like Polly, I can wire up a chain of conditions so that I can handle the failure of any number of services.
Framework
In the context of integration tests, I would recommend visiting the DbMigrator class. It might be of use. 
You could use a test database and/or transactions to roll back changes after each test.
I was coming in to say the same thing. Re-use Polly's circuit breaker as it's not an easy thing to get right.
excellent answer! Thanks!
Any good sources of info about it?
The idea is: Make call -&gt; success. Make call -&gt; Fail count failure. Do not retry same message. Make call -&gt; Fail count failure. Do not retry same message. Make call -&gt; Fail count failure. Do not retry same message. --&gt; open circuit. Make call -&gt; Cancel call and return failure instant Make call -&gt; Cancel call and return failure instant Make call -&gt; Allow to make call again. If failure return to open and extend timer. Else return to closed stated. Don't want retry. Want instant failure so the client / server can continue on working path's of the code / services. So if a service is flakky and often looses connection we stop working with it for some time before giving it a chance to recover. This also help against heavy load / ddos attack's. If a service cant keep up, we simply stop calling it and wait for it to complete existing calls. This would stop cascading failures of systems.
For something like important messages, I would use a message queue. Set event on queue that I want to call X service for Y data. Have a other proccess to access this queu and try to make calls. If something fails, put it on a error queu and try to make call again some time later. Main queu would run every 500ms. while error queu would only run every 30 seconds or even more. If message fails more than say 3 times, it would then be sent to dead letter channel for dev's to check with all prior stacktraces and exceptions.
Agreed. This project is surely well-intentioned but is mixing concerns and will only end up with both reduced capabilities and increased maintenance headaches.
Unit testing will, in the future, tell someone how your application works. If they make a change and tests start failing it will make them question whether the change is correct, and the test needs updating, or whether they missed something critical. So if this is something that an individual could want to change behaviour on, then having a test to lock in the current behaviour does make sense. That said, first test anything you think is complex enough that you can't be 100% sure it works, locking in behaviour is slightly less important than making sure the code actually works.
The same way, use an interface.
You might be able to create a COM wrapper so that .net can Interop with you c++ code https://www.microsoft.com/com/default.mspx You may also be able to copy your c++ code to a managed c++ project in visual studio so that asp.net can call the objects using the CLR https://msdn.microsoft.com/en-us/library/68td296t.aspx 
You want pinvoke https://stackoverflow.com/questions/38202027/p-invoke-in-net-core-with-linux
Look up User Controls and Custom Controls. User Controls allow you to aggregate existing controls into a single, reusable control. Custom Controls allow you to design a control from scratch.
Sounds reasonable. I had a Fortran app running like that on a asp.net full framework some time ago. - Render HTML page with input form and post button - Receive data in asp.net controller - Write data out to input file for external program in a temporary folder - Launch Process.Start (from system.diagnostics) to run your executable with the path the the temp folder as parameter - Read output data - Delete temp folder - Return output data in the view bag Should be pretty easy depending on what you have in your Cpp part. Asp.net core is fine for this, should be no issue. The basic stuff should be less than a day's work, the only question is how you want to input and output to the users. Input Fields, output charts, data validation, reports, user accounts, history of runs, billing of computer time, etc. 
Google "run Db Migrations programmatically"
Let me know if you do eventually figure it out. Keycloak is great. A few years back I was trying to write a .NET client adaptor for it. I didn't have time to finish it.
You're not going to be doing anything where the performance of the .NET stack (or any stack) matters. They can all handle thousands of requests per second without issue. If you actually need more than that, you need to hire someone to administer the infrastructure you'll need for any stack. 
I'd suggest avoiding COM and instead using C++/CLI (what used to be called "Managed C++"). You can have a C++/CLI project that acts as a bridge between .NET and C++ - it'd export a .NET class that calls the C++ code. Having said all that, I just noticed the OP mentioned that it needs to work on Linux which rules out both C++/CLI and COM. The only cross platform choice is P/Invoke, or running a separate executable via Process.Start.
No if you stop paying you get the version that you had at the start of your annual subscription as long as you finish at least one year of payments.
&gt; Is my idea possible? totally~ &gt; Or is using ASP.NET Core for this a complete overkill? ...probably. I'm a big fan of .Net core, but two points: - netstandard2.0 isn't out yet for a couple of months (sept?) You really probably don't want to go down the road of using &lt; 2.0, because it'll be obsolete before its even finished. If you want to use net core, use the preview, and be prepared for minor changes when the full release version swings around. - Net core applications are a pain in the ass to distribute. I won't go into the details, but in a nutshell, read https://blogs.msdn.microsoft.com/luisdem/2017/03/19/net-core-1-1-how-to-publish-a-self-contained-application/ You build a deployment bundle, and it'll be a few hundred files you need to roll out onto the server, and involves manually editing your config files and know exactly what platform you want to roll out onto. It almost needs an installer to roll things out correctly. For your purpose... I'm not sure it's a great fit. What you probably want is a quick easy web service that you can expose your api on that can run on AWS, or locally on a windows or linux machine without complicated setup and install. I'd probably use golang to deploy a single static binary web server. Have a read of: http://golangcookbook.com/chapters/running/cross-compiling/ https://golang.org/doc/articles/wiki/ https://golang.org/cmd/cgo/ The ability to roll out a single binary (eg. `server.exe`) that encapsulates your entire application in a robust http/2 endpoint is really compelling. Once you need to build a fully featured web site, the benefits of golang sort of fade away; the database, templating, package management and authentication systems are really not up to the same standard as the .net equivalents; but you don't sound like you really need any of that stuff. 
COM on Linux? He wants to run there.
Moq is awesome. It sure as hell beats the "old fashioned" mode of creating mocks manually. Sometimes readability suffers when testing complex methods or interactions, but the capability of the Moq library is impressive. Love using it.
Nice advertisement for Go, but a bit drastic "oh, use a completely different ecosystem" :-) Alsi you didn't explain how he should call his C++ from there. It's not a big deal (same as p/invoke).
Ideally you'd want to have a repository interface that defines your data access operations, an abstraction in front of the underlying EF code. You would then mock that repository interface and not the EF code directly. 
If you want to run on asp.net, you "just" wrap your C++ in C and use p/invoke. If you want to stay with C++, you "just" use e.g. boost.asio to get http server.
Is your requirement actually to use MEF? IMO it's fucking obtuse and overkill for some applications that just need to load a plug-in dynamically.
https://golang.org/cmd/cgo/ &lt;-- calling c from go ...but good point, if you want a single static binary artifact, checkout https://github.com/shadowmint/go-static-linking I mean, I'd love to be to recommend .net core the same way, but its just not there. The assembly embedding tools still dont work with core, and much as I enjoy https://github.com/dodyg/practical-aspnetcore, I just dont see it being the right choice in this case. Its a big heavy stack for a very simple job. I mean, what would you recommend? rust? python? The only other top tier choice I can think of would be like https://github.com/ipkn/crow, but the OP specially said they didn't want to use C++?
He says that he doesn't want to write an http server or xml in c++, not avoid use of C++ completely. He already has the "whole" application in it, after all :-). I don't quite see why is writing xml a problem, and he doesn't **need** to read it, that is self-inflicted at this stage :-) for his description (post an html form, send an xml reply). An HTML form is a name=value collection. So simple solutions in C++ ecosystem are * httpd (apache) and cgi or a module * boost asio (tl;dr In both cases, all "http" C++ is written for him; with httpd he needs an exe or an apache module; with asio he only needs... asio). ASP.NET core will work through p/invoke and a C wrapper. You are arguing golang from the perspective of deployment, which just isn't his consideration. The difficulty in integration is the same as with .net (a C wrapper). But I agree, the deployment should be a consideration. The problem with your idea is - golang is not a subject for discussion in this subreddit :-) In the end, his needs are extremely simple. If so, the reality is that the best solution is one he is the most comfortable with, aka "use what you know".
Thank-you for the response, I will take a look now! Best regards, Elliot
I will probably write a blog post about it when I have figured it out. Will part it back here.
I prefer NSubstitute.
&gt; I don't quite see why is writing xml a problem, and he doesn't need to read it Since further down the line the application is very likely to be integrated into a larger system, its input and output need to be in some relatively standard format so that both humans and machines can make requests and get results.
We've done something similar at work and ended up doing it with SignalR. We have a server side method which runs in a loop doing some stuff. We cache the result of this method and if it has changed, we send a message to the client to update the UI. This means we don't do a pile of HTTP polling and the server does the heavy lifting.
Yes, but: * poster says nothing of that * writing a good thing out is still leaps and bounds simpler than parsing * if it's "standard", there really should be APIs to do it, not a generic xml lib.
Sure, just learn about the robots.txt file: http://www.robotstxt.org/robotstxt.html
The better option is to create an "Empty web project" and start adding stuff to it.
Depending on which version of VS you're using, you might need to [uncheck 'MVC' and check 'WebAPI'](https://media-www-asp.azureedge.net/media/4410055/getstarted02.png) when creating your project. The HTML stuff comes from MVC.
If you're doing it in core you can just use the command line to do a dotnet new webapi and it should give you the minimal stuff. See the doc on the cli for more info: https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-new#options
For many of the reasons you've outlined, I've started with [Nuke](https://nuke.build). I have a strong background in build automation and tooling. So to speak, I do know about the important of consistency and ability to reproduce a build. But it also has "glitter on top", like you said :) It provides full IDE integration (auto-completion, navigation, debugging, package referencing) without requiring to install an extension or reading several how-tos. It's all native. For supporting command-line tools, I wrote a very powerful generator that works on metadata (e.g. [MSBuild](https://github.com/nuke-build/tools/MSBuild.json). This generator can even generate strongly typed methods for enabling / disabling the "RunCodeAnalysis" flag. As a reminder, this is actually a property that is usually specified via "/p:RunCodeAnalysis=true".. so we provide the methods EnableRunCodeAnalysis and DisableRunCodeAnalysis. There are numerous more advantages if you look under the hood :) This metadata is something that I hope can be shared between all the other build automation systems in the future. About PowerShell support. I'm aware of the problem that the Cake plugin is lacking. I will soon release a similar API very but with more best-practice handling. Would be great to get some feedback. Anyway, happy building :)
Yeah, sometime getting the number and type of right can be tricky, lol.
Ah yes. Not a good option then.
"Empty web project" is the way to go, then just add `ApiController` instances. ...but fwiw, the .net core `dotnet new webapi -name Project` scaffold is really very minimal: it's literally just ValuesController.cs, Program.cs and Startup.cs. No other scaffolding.
But then you'd have to read the docs and not just click a button!
There was an issue with quicktour button component that required login and is now fixed. Sorry for that, it was not intentional. What happened is that after few steps it was trying to show how an user page looks like and that requires a logged-in user (i mostly use the app logged in), and if you went back from the browser the quicktour component tries to resume from last step even if you try to navigate to another page. And since last step was a request to see another user profile it just popped up the login window. So from an user perspective it seemed like the site wanted to force you to login...I now removed those steps from quicktour.
Thank you all for the information.
aye, i've got the subscription and if i didn't renew it i'd still have a license for 2016.3 of all tools.
OK, after 1 day of fighting and trying to figure out that thing, I managed to get the ASP.Net Core webapi securing part working correctly. And it's frustratingly simple, I got "saved" by the [Auth0 documentation](https://auth0.com/docs/quickstart/backend/aspnet-core-webapi/01-authorization). Since Auth0 uses OpenId Connect, I thought it would be the same with Keycloak, and I was right. It's frustratingly simple, the only thing that needs to be done is to configure the *JwtBearerAuthentication* middleware with an authority being the Keycloak instance. Add the Microsoft.AspNetCore.Authentication.JwtBearer package from NuGet. Then just add this to your Startup.cs Configure method. var options = new JwtBearerOptions { Audience = "***your-app-id***", Authority = "https://KEYCLOAKINSTANCEURL/auth/realms/YOURREALM/" }; app.UseJwtBearerAuthentication(options); Then, add the [Authorize] attribute to any API method you want to securize and you're done. If you want to use the secured API you'll then have to ask for a new token to Keycloak and pass it through the HTTP header. Now I need to figure out how to do the implicit flow with Angular 2 and add the header to my requests and I'll be done.
Thank you for your reply! I have been looking into SignalR as I just got a prototype completed and put on GitHub but it forces the browser to refresh. I have completed the SignalR chat tutorial and I am using what I learned there to build a hub for my PingBot. You can check it out if your interested https://github.com/magnusdarkwinter/PingBot
That is great that you were able to figure it out! I can't wait to try this when I get a chance. Thanks for sharing.
I'm not an expert on this stuff so i suggest you be sure to look into everything yourself to be sure. Assuming you're using nuget package management you should be OK. Anyone who has uploaded their project to nuget is fully aware other projects are going to be pulling it in. That said you should still look over the licenses of these projects just to be safe. But if you're using nuget your project folder does not contain any code or binaries from referenced packages so there shouldn't be a problem. Only real problem is if you're forking a library and including the source code in-line (or in a separate project), then you have to be extra sure the license will allow what you're doing. It's probably best, unless the project you're forking is also MIT-licensed, to keep such things in a separate project/repo and keep them under their original license if possible.
Since this is being deployed on a MEF platform, it needs to be MEF main application and all the other parts implement MEF so it can be discovered dynamically on runtime. I am finding it very difficult no doubt since it requires alot of configuration and half the time fails for no reason, also having to shadow copy the dlls so as to be able to access it Views. Its a pain in my ass and I have about a week left to impliment this from a Sequence Flow Diagram to a fully functional software. "Internships amirite?"
Are you currently using SignalR? What would you like to do that requires SignalR? If you're just using an MVC controller without XHR/AJAX you could post the data to an UploadImage() controller action and upon successfully processing of the file (you determine what success is) redirect back to your image gallery page which will now include the new image based on the updates you made in UploadImage(). If you're using XHR/AJAX to upload the file to an MVC controller action then you could have UploadImage() return a JSON object that includes information on whether the upload was successful or not. Then in your JS client code you would process that JSON object and decide whether or not to refresh your image gallery, which could include a call to a SignalR hub method. I would suggest keeping SignalR communication between the client's JS code and the server's SignalR hub.
Didn't know that. I remember i read somewhere about the "valid until you pay", but it seems i'm wrong about that.
Maybe link to a screen shot of what you are trying to do. I'm personally more a fan of the second method which you have marked works but you feel it is a hack. Maybe I would need to see a more complicated use case. http://www.wpf-tutorial.com/common-interface-controls/menu-control/ In this tutorial they show adding images, I'm not sure how that would shake out for the Resources method long term. Your menu will hold multiple menu items, each with the internal menu items so that they can be nested. I'm not sure if you have thought through if a menu items expands to another item tree below it. 
The second part is what I thought I'd have to do and that's fine. But what I was going to try was the first suggestion but then notify all other users of the change.
There are ways to call hub methods outside the hub, but it varies depending on what version of signalr/asp.net you are using. You need to get the hubcontext I've done it before with something like this: _hubManager = Startup.ConnectionManager.GetHubContext&lt;ChatHub&gt;(); _hubManager.Clients.User(rUser.UserName).newNotification(); And I've seen this method suggested from stackoverflow: var hubContext = GlobalHost.ConnectionManager.GetHubContext&lt;AdminHub&gt;(); hubContext.Clients.All.foo(msg); And i think the new version of Signalr for .net core might be doing it a different way
Or if you just want to see what is in it: https://github.com/Microsoft/dotnet/blob/master/releases/net471/dotnet471-changes.md
Looks like upgrading nuget packages sometimes messes up the references in the csproj file. I've had a few instances of double references from both versions and sometimes it deletes it altogether.
This is very cool! I love all the features and the experience looks amazing, but most of all, I just LOVE the cultural change that a tool like this could bring to a team. In most C# teams, everybody's concept of "how our build works" is a foggy cloud of confusion, fear and doubt, so they defer touching it to the one guy or gal in the team who's actually familiar. With this, the build becomes a first class citizen, no more scary than any other bit of C# code.
What's the difference between this and CAKE? I've already implemented and used CAKE and it's a very good tool.
Most importantly it integrates as normal C# project, meaning that no extensions or plenty of how-tos are required to use all the basic IDE features like auto-completion, navigation, debugging and refactorings. During setup the build project is even automatically added to your solution. Also, targets (like Compile, Test, Pack) are defined as expression-bodied properties, which allows superior navigation and avoids typos. In Cake however, you would keep to the scripting experience, like noticing compile errors only after invoking build.ps1 or looking up commands on the website. Another improvement is that every support for CLI tools (like msbuild.exe or paket.exe) is almost completely generated from [metadata](https://github.com/nuke-build/tools). That is less error-prone, allows to implement more features, keeps the API consistent and is more likely to be extended by newcomers. In Cake you'll see a lot of different naming, like I tried to point out in [this issue](https://github.com/cake-build/cake/issues/1406). Unfortunately, nothing has changed this then :/ For all the stuff that NUKE doesn't yet support, there is exists a Cake bridge, meaning that you can call Cake addins from Nuke. Of course this is not a long-term solution, but it's helpful during the process of catching up :) Another thing that was just released is the extensibility model for the [command-line interface](http://www.nuke.build/command-line.html). Besides that, I have plenty of more ideas for very customized tooling. But that will take some time :) Time that is in Cake generally spend for even the basic tooling (syntax highlighting, auto-completion, debugging).
&gt;How is that the variable k has object B and variable l has object C? Because the 2nd time the method 'setMyClass' is invoked on object a, you are changing the value of myClass. 
Then both k and l should point to the changed object, which is C. Isn't it?
No, by declaring and assigning the variable K, it is allocated its own spot in the PC's memory. The same with L. This is why both variables will not hold the same values.
I'm torn now, I just recommended two weeks ago to change our build process to use Cake. Now Nuke looks like a strong alternative. Unfortunately I am no longer the person building the build server scripts so I guess I can just pass them along with the alternative. I would like to see a better comparison of the two tools. Hopefully there are long term plans to keep improving Nuke.
This is great. I am so disappointed our devops team can barely handle nuget packages, and stuff like this comes out. 
 var a = new A(); Create a new 'A' in memory. Create a variable called 'a' that points to the memory location of this object. myClass = new B(); Create a new 'B' in memory. Set the variable 'myClass' to point to the memory location of this object. var k = a.myClass; Create a variable called 'k' that points to the same memory location as 'a.myClass' myClass = new C(); Create a new 'C' in memory and change the variable myClass to point to the memory location of this object. (Note that here, k still points to the original memory location of the 'B' that was created) var l = a.myClass; Create a variable called 'l' that points to the same memory location as 'a.myClass'
From the same code sample above, if I do Y y = new Y(); var r = y; y.a = 15; The value of a in object r also changes to 15. By your logic, shouldn't r point to a copy of y? If you say a copy is created only when new() is invoked, then the approach given in my question is causing any memory leak?
I literally a few days ago started using Cake. Damn only if you posted this a few days ago i'd have tried it out.
&gt;By your logic, shouldn't r point to a copy of y? No. R doesn't point to a *copy* of y, it references the same memory location that y does at the time of the assignment. So in Line 1, you create a new 'Y', let's say this is in memory location 123. The variable 'y' (behind the scenes) holds '123'. (The compiler understands that this is a reference to a memory location, not the literal value '123', so when you use the variable 'y', you are actually using the object in memory location 123). Line 2, you are assigning y (which is a reference to memory location 123) to r. So r now also references memory location 123. In Line 3, you're using the object in the memory location pointed to by 'y'. Since 'r' *also* references this memory location, 'r.a' will also hold the value 15. &gt;If you say a copy is created only when new() is invoked, then the approach given in my question is causing any memory leak? By 'copy' do you mean 'new instance of'? If so, then yes - doing new() will create a new instance in memory, doing: var a = new Table(); var b = a; Only one object will exist in memory, with two references to it. As for memory leaks, it sort of depends on the scope, and how long objects *should* stick around for. Basically, the garbage collector will look for objects that are no longer referenced by anything that's in scope (or by extension, anything used by anything that's in scope, etc). So in your example code, an object of type 'B' will exist in memory as long as 'k' is in scope. Once 'k' goes out of scope, there will be no references to that object any more (since the reference a.myClass got changed to reference something different), so it will be eligible for garbage collection.
Nice effort! As others have said, this looks really useful. I noticed in the 'Getting Started' section one of the Build Execution steps doesn't work outside Windows so my question at this point would be, how much does the user experience differ depending on what OS you're using?
Formatting failure, will try again in a minute
Thanks for pointing that out! Actually it's just about wording. The experience is the same on Windows and Unix. However, so far I've found that 'mono' is always reachable from just calling 'mono'. Tested with some CI servers and my own macOS environment. Otherwise the MSBuildLocator already contains some [UNIX specific bits](https://github.com/nuke-build/nuke/blob/master/source/Nuke.Common/Tools/MSBuild/MSBuildToolPathResolver.cs#L35-L39). If there are any issues on other systems, I would be glad about a notification!
I was thrilled by Cake once too. But now I really can't recommend it any longer (even if there wasn't NUKE). I've reported some very severe issues related to security and determinism and unfortunately, I wasn't happy how they were handled by the team. And yes, there are long-term plans. I'm looking forward to get feedback to make it better :)
There is always the possibility to make a step back :D And please see my other [comment](https://www.reddit.com/r/dotnet/comments/6sbvj6/nuke_build_automation_with_c_dsl_and_full_ide/dlcauyk/).
I see that the target metaphor is still in use for this framework. Does that mean the framework generates MSBuild artifacts or does it leverage MSBuild and/or CSC directly?
It orchestrates MSBuild only if you specifically ask for it. For instance by calling one of the [MSBuildTasks](http://www.nuke.build/api/Nuke.Common.Tools.MSBuild.MSBuildTasks.html). For everything else it is just the same metaphor. I've found it reasonable to reuse it :)
Thanks, I may give this a closer look. I have been carrying around a mishmash MSBuild based framework I have built up over the years and I tire of maintaining it. 
great guy
This is not the answer you're looking for, sorry, but... Consider your requirements. Less than 100 visits per day. That's very very few. To be sure, I would whip out your favorite load testing software (for you scenario, maybe just a couple of browser tabs) and see if it stays alive. 
I would very appreciate any feedback :)
I have been implementing stripe for the first time and it has been more difficult than it needs to be for two reasons. The first is the lack of documentation for .net. the open source library has examples of how to do most things but there is still a lot to learn from trial and error. This news should greatly improve this first challenge. The second issue however was that the general documentation has key information spread across multiple pages. For example, understanding that their checkout.js doesn't charge the card was one thing that wasn't clear enough. Or how making a subscription automatically charges the customer credit card but updating the plan leads to a proration that is only billed next month. Or that invoices are used to force those subscription updates to charge immediately. I think I'm pretty clear on how the system works now but it definitely could have been much easier for me with .net examples and a more complete "quickstart" guide. 
Thanks, I'll take a look. Would I have these same problems if I ran using NGINX?
I'm confused. If it's a REST API, why does it need a .NET client library? 
It's super nice to have all of the request/return objects already defined as classes in a library. Additionally, it's also super nice to just call `await StripeLibrary.SaveSomeObjectAsync(object);` rather than having to keep straight all of the endpoints, HTTP methods for those endpoints, etc. It basically saves a few days of code writing all the boilerplate for this particular API.
Ah yes, the "client libraries are bad, lets use REST. REST is bad, lets use client libraries" cycle. Do we have a ~~WS-*~~ ~~RAML~~ Swagger API contract yet?
https://github.com/dodyg/practical-aspnetcore 
What a silly argument. REST and client libraries are not mutually exclusive. The client libraries are not enforcing a two way contract. They are simply providing an abstraction over some of the .net tools used to access the API and they are providing type definitions for interacting with the API. The API itself doesn't care at all what client you are using to access it. You can write everything manually if you'd like, I'm sure you'll find some justification for the monumental waste of time and bad code that results. God forbid that developers provide tools for accessing their services.
&gt; What a silly argument. REST and client libraries are not mutually exclusive. Of course I don't believe that, but it wasn't too long ago people were telling me that raw HTTP was the way to go, REST is the application layer (in the classical network stack), and you don't need any abstractions over it. 
I'm a bit of a build automation newb, but my impression is that any decent tooling should be relatively trivial to implement. Why not setup Cake and Nuke side by side and go from there?
NGINX could barely be bothered to roll out of bed in the morning to handle 100 requests a day. But in all seriousness, 100 requests a day is nothing for a server. The problem is that people tend to use averages without discussing how "bursty" those requests are. There's a big difference between 100 requests at the same time once a day and 1 request every 15 minutes. If you're worried, you'll find no better answer than the one you prove yourself. I think Kestrel in ASP.NET Core is still too new to have any good feeling for; you'll just have to hammer it and see what happens.
The REST API is the application layer on one side of the exchange, to be sure. The side consuming the API, on the other hand, most likely implements calls to said API on the service layer (or others depending on architecture) Let me provide a similar example. A web server can serve a website without a browser. You can interact with a web server using any number of tools. The entity providing the website (server) does not care if you use curl, wget, etc. The user consuming the website uses a client, i.e. a web browser to consume that service. Libraries such as this can be seen as the browser on this case. There are any number of ways, in any number of languages, on any number of platforms that you could interface with this API. Using a client library that is maintained by the provider of the API allows developers to build more robust, maintainable code. If someone told you that you should only use an Http client when accessing web services via .net, you got bad advice. Even IF you were going to implement your own code to interact with this API, wouldn't you most likely create a single helper to access the API and then reuse it?
Hello, We have plenty, take a look and fell free to ask any questions if you find something interesting: https://github.com/okhosting?utf8=%E2%9C%93&amp;tab=repositories 
https://www.codetriage.com/
Mainly time constraints. It's an open source project that I work in in my free time https://github.com/tidusjar/Ombi
Contribute it! Open source is awesome, and your experience in what confuses beginners is really vital to any project.
100 visits a day is puny. Developers working on their machines would have made more visits a day.
Thanks for the reply and the link. I look forward to using this :-)
Which tooling do you mean exactly?
I strongly recommend to change [this](https://github.com/tidusjar/Ombi/blob/DotNetCore/build.ps1#L125) to https (in build.sh as well)... It is the security issue I was talking about. This could allow to inject harmful code.
http://up-for-grabs.net
We have been using kestral in production for 6 months with no nginx / IIS. We do however have an AWS ALB in front of the servers. 
Thanks. Maybe I'll try nuke when I get some spare time :)
It seems to have been installed during the night, causing a restart. I never seem to get any notification when Windows 10 is going to restart.
Maybe you were spending too much time with evangelists. ;)
Sadly yes. But I'm a reporter so I don't have much of a choice.
The language is called C#, not C sharp. 
Is this free? This program is geared towards software development, if that's your goal then its not going to hurt. 
Yes, it is free. I am only worried because it seems like 18 weeks isn't enough time to learn everything needed for the designations. But who knows....!
Just make a quick project and try it? It's free. 
The UI asks you to specify a database when connecting. You can select form a dropdown (of databases.. not linked servers) or enter manually. If there is some special syntax to use to specify a database on a linked server, I don't know what it is.
Does this mean that .Net Core 2.0 is final and ready for production also or are we still awaiting Core 2.0's final release?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [.NET Standard 2.0 is final (Ready for production)](https://np.reddit.com/r/programming/comments/6smkgu/net_standard_20_is_final_ready_for_production/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Not necessarily. They need to lock down the .NET 2 Standard first, then test to make sure Core actually adheres to it.
Great progress and (for me at least) the first usable version of the standard- not having System.Data built out in 1.x made it a non-starter.
Heya! I'm Andrew (the guy who wrote the blog post). Is there something specific you're looking for? If you give me a template, I can see if we can whip something up like that! We built a few getting started guides like this one-- https://stripe.com/docs/checkout/aspnet And we tried to expand our existing documentation to include .NET examples as well.
to be honest, I surprised how .net core is working on linux (i switched to linux 1 year ago after first release of .net core). it is working so well. It is really good performance from .net core team. good work! 
Some guy that works on kestrel said .NET Core 2.0 is going to be released at the end of next week on the linked discussion.
I've done queries through EF through synonyms against Linked Servers. Readonly queries work out of the box, non-readonly queries break EF's optimistic concurrency assumptions (the query executes correctly but does not return the correct number of rows affected). I believe that for database first, you will need a single database frm which to generate your EDMX. So your database migrations need to be applied to two places. I would really suggest against it, synonyms and linked servers have left an awful taste in my mouth.
It'll be released on Sept. 18 confirmed [here](https://m.youtube.com/watch?v=fGxGeP7b8j0)
Awesomesauce. Thanks.
What part of EF do you want to use? If you are just using it as a materializer with raw SQL, then you can write `SELECT a, b, c, from server.database.schema.table`.
Just dropping in to ask because I can't check myself right now. Is there a net standard 2.0 download available for vs to Target already?
.net Standard 2.0 is the specs, so I'm guessing you're referring to .net Core. It's not production ready, but there's the preview versions of .net Core 2.0 and tooling within Visual Studio ~~2013~~2017 Preview. You can download them from MS' site. Edit: wrong VS version.
How is this free? I used to teach at an 18-week bootcamp, and they're expensive (and questionably worth it unless you have an aptitude for programming). Also, several of the topics mentioned in the course, especially database programming, could have a whole course to themselves. I always view these kinds of programs with some degree of skepticism. There seem to be a lot of promises of "We'll make you a real developer!" that only come true if you do a shit-ton of extra work outside the course.
2013? You mean 2017?
I was let go by my previous employer and turns out my county has a career program where I can get IT training for free. This is one of the participating institutions. I officially request the grant tomorrow through my county's career center. 
Well hey, can't argue with free! That's great. Just be sure you don't get suckered into some expensive arrangement. Read the fine print to make sure that there's nothing that says "Well if you don't pass, you pay $10k." Not everyone has the mind for coding. It's not strictly a matter of intelligence. Find a balance between giving it 110% and realizing that this isn't clicking. 3 months in ought to be a good time to make a self-assessment.
No I meant the specification. Vs allows you to Target specific standards instead of net core or framework. The runtime then figures out if you have an implementing framework at hand. So the question is if the dropdown already shows net standard 2.0 in the library properties
If there is, it will be offered in Visual Studio 2017's installer.
Fuckin hyped to go to work tomorrow.
Can you clarify, "not having System.Data built out"?
No DataTables are the biggest issue. And much of the System.Data.Common classes for accessing the data are missing. All of that now exists in 2.0
build automation/scripting. 
Same, I switched to Linux at the same time I discovered .NET Core and it's been awesome! :)
I don't think it's easy to implement. The crucial part is to support all the hundreds of tools and their arguments correctly. And also to provide a meaningful framework that can adapt to your needs.
404
Look for references. If someone is capable enough to be self-reliant, they will have a portfolio of previous projects to point to. Ask them about them: how did they work with the client? How did they translate business challenges to technical challenges and how did they solve them? And of course, call the references and here it from them as well.
Anyone have a suggestion where I would find developers that could convert older .NET code to use this? I'm a Linux Systems Engineer and I need a Windows developer for a project coming up.
back
As they say, Microsoft gets it right on the third time. (although you could argue this is easily 4th or 5th now) 
Stack overflow jobs of course. If you list a full time remote position with proper pay, you'll get 25+ qualified applicants in days. If you need consultant(s) who can get job done and move on, pm me. 
It's nice that the example uses F# to show it works. Usually F# is left behind and gets platform support last or not at all.
I'm talking easy to implement by the consumers of your tools. Not suggesting that a build automation framework is itself easy to make.
Okay now I understand. Well these two tools differ very much in their approach. Cake is basically C# scripting + a few preprocessor directives, meaning that it requires a certain learning curve. Many IDE features are not supported (or only in vscode). Nuke takes the normal C# project approach, where everything works as usual. Using them side-by-side wouldn't be very helpful I guess. Transitioning from Cake to Nuke should be relatively simple, as soon as I release the required "bridging" package.
This post is not getting a lot of love. I started out with very basic roles - Administration, Vendor, Installer -- using the Authorize(role="Administrator") attributes and this worked really well in the beginning. We needed fine grain control over the application -- some people can view this, some people can update it etc etc. I switched to Policies -- it took me some time to get my head wrapped around it. One of the main issues I had was that we needed view only access for some users. This is what I ended up doing -- this is not how you want to do as it does put in the static strings in the startup file which I first I was very against but in the end I accepted it. Some notes * We have 135 different claim types. ~50 are used as attributes to control controller access. The other ~85 are used to control view layout/ui and user options. We also use the Claims to control what forms the user has access to * You only need to place the ~50 attributes in the startup class - not all 135 * You place the lowest claim in the attribute e.g. [Authorize(Policy = "Vendor_PO_General_View")] * The startup file as this AddPolicy line options.AddPolicy("Vendor_PO_General_View", policy =&gt; policy.RequireClaim("Vendor_PO_General", "Full", "View")); * On the UI I hide the Save button, make input readonly, in the save function I check the ClaimValue just in case the user tries to bypass my security. I do this with this logic if (POModel.HasClaimFor(User, "Vendor_PO_Permit", "Full")) * In the database I can store ClaimType: Vendor_PO_General_View with ClaimValue: Full or Read to control whether the user has full access to read only access All the above points are not reason as to yeah this is really great I am switching to this. What I really like is that if you use Microsoft Identify services (asp.net core) that is made up of the following tables user, role, userrole, userclaim, roleclaim -- you can populate these tables and in the controller/View the User object has a List of claims "User.Claims" which has all the claims from the UserClaim and RoleClaim table. One thing that I just discovered and what has me worried is that the claims are stored in a Cookie which should not exceed 4K -- if your claims are long and you have a lot of them you may exceed this limit. 
Validation attributes on the server Model maybe? 
I haven't messed with winforms in ages but I seem to recall another method like ShowModal for opening child windows like you want. Sorry nothing else to add but I have seen that issue before. Edit: Worst case you could sub your main form to the closing event of child windows and set the focus manually on trigger
It starts like this: If you have a form with the properties "username" and "password" that you want to be filled out, how do you prevent someone from just submitting a blank form, and submitting null data in your database? The answer is you want to force the guy submitting the form to enter data before submitting, and if theres no data you want to redisplay the form with error messages like "the username field is required..." the way to accomplish this is to go to the class or view model where the properties are and require the properties you need filled out. By doing this: using System.ComponentModel.DataAnnotations [required] public string UserName {get; set;} This "require" data annotation creates a contract with the user that he must fill out his username. Now in order to enforce this contract you need to go to your action method that accepts the post and return what you want only if the info is valid (in our case, the username is required) by wrapping it in If (modelstate.IsValid) { Return where you want to go }; Otherwise redisplay form Return View(). In your view, under the input for username, you put this in a span: asp-validation-for-UserName tag to tell your code that this is where you want the error for this property to appear. The form will redisplay with the error message "the username field is required" right where your span is. If your validation-summary=All, It will display on top too. If your validation-summary=modelOnly, any errors associated with the property "username" or any other specific property will not be shown on top. What will be shown? General errors that are not specific to any one property, like, "Could not log in". How do you add that error to the model? In your action method like this: If (modelstate.IsValid) { Return where you want to go }; //Otherwise add general errors Modelstate.addModelError("", "could not log in") Return View().
I'll just go ahead and repeat myself: &gt; So I guess I wonder what they mean by model level and property level, what are these in my aspnetcore mvc application? EDIT: If your answer remains the same: How do they differ?
Thank you for this reply! It explained the things that confused me very well.
I'm not sure I understand. It sounds like you have a database on Server A but want to get to it through Server B. Does Server B have a database you want as well or is it just acting as a proxy connection? I often have applications that consume "warehouse" data that lives on a different server (Server A) than the application database (Server B). For those, we create a linked server and use synonyms to query through the application database to access the warehouse data so the two can be joined together, filtered, etc. EF won't list linked servers to scaffold your models and context but, using the example above, if you expose specific tables from Server A as Views in your application database on Server B it should pick that up. The Views are essentially proxies to the remote data at that point if that's all you need.
Huh, so people really do write static policy names into their controllers? That seems really unsafe to me. I have tried to avoid it as much as possible. Did you read my methods? Do you think that would work for your projects? 
Just out of curiousity, what is the technical reason that requires you to use DataTables? I thought they went out of style like over 10+ years ago.
So it looks like one type of validation message applies to the whole model , and one applies to just one property. So I think the next step is figuring out what the different type of validations are intended for. Sorry I'm not more help and good luck.
Server A is MS SQL with a linked server pointed to an oracle database on server B. I'm not familiar with synonyms, I'll have to do some reading up on that.
I'm on a Mac so I don't have access to visual studio. I use vs code when Im writing smaller stuff or just have to fix a quick thing, but when Im implementing larger things I use Jetbrains Rider IDE, which has just released its first full version, it's still a bit wonky on some parts, but overall a very good IDE for my purposes
VS Code + Omnisharp 1. I completely switched to Linux so I need something that works natively (no Visual Studio) 2. I'm still a student so it needs to be free (no Jetbrains Rider) 3. It works great with all the other languages I use (Python, HTML/CSS, JavaScript, shell scripts, etc) Sure, it's not as feature-packed as full VS, but with some configuration I think it can fit perfectly in many workflows.
https://www.visualstudio.com/vs/visual-studio-mac/
Just started using VS Code for learning asp.net core and angular. Find it useful to learn this stuff from the bottom (cli/terminal), without all that visual studio magic. Dunno if I will switch back after the initial learning phase.
https://www.jetbrains.com/buy/classroom/?product=rider it's free for students
JetBrains Rider is free for students. Not that there is anything wrong with your setup, just an FYI in case you were not aware.
Translate your entity result to a DTO or view model then serialize || ask jsonconvert to ignore cyclic references. This is happening because of the cycle caused by your child&lt;-&gt;parent relationship. They will infinitely refer to each other's navigational property.
wrapbootstrap.com You can apply any of those templates to your asp.net core app. I remember the Inspina actually having an asp.net core starter project.
For me it's the simple stuff that I miss when using VSC over VS. things like creating a class not automatically adding in namespaces off the bat. I use both happily but prefer full blown VS for larger projects with many classes.
Thanks. That's what I was afraid of... my models are the same for ef and web api, don't really want to duplicate them. How do you ask JsonConvert to ignore cycles? Is there some attribute I can put on the property for JsonConvert to ignore it?
Thanks I'll check it out.
Nevermind, got it - JsonIgnoreAttribute... :)
Also, Rider is pretty nice, IMO. I’ve been using it for the last 3-4 weeks at work instead of Visual Studio as an experiment, and it’s been mostly great so far.
Oh wow, I didn't know that! In that case, I'll definitely take a serious look at it.
[There's a plugin for that.](https://marketplace.visualstudio.com/items?itemName=jchannon.csharpextensions) :)
table valued parameters for one. 
VS 2017, since both VSCode and Rider don't support Edit and Continue(aka Hot Swap), Class View, Object Browser
HTML forms don't submit values for controls that don't have names. Open up your browsers dev tools and go to the network tab. Submit the form, then look at the form post in the dev tools. Your value will be missing from the JSON. Give the control a name, and re-submit. It will now have a value submitted to the controller. 
Using a constant or enum instead of static string is a good practice -- it prevents typos -- I like that approach -- as typos always creep into static strings.
You can do it on a attribute basis or in the serialization configuration somewhere like startup.cs meaning it would apply to every serialization even if you don't use the attribute. I think most people would recommend you stop returning entity objects and instead return classes that do not reference each other in the first place. DataResultClass Prop: HeaderRowDataClass Header Prop: List&lt;DetailRowDataClass&gt; Details The details class does not have any reference to the header class and no cyclic serialization is attempted. This may seem like extra work but is a recommended over returning entity objects in API or standard controller methods 
You could use dotnet watch if you are running .NET core. It isn't exactly the same though.
Cool! I did search but it's knowing what it's called!
&gt; This may seem like extra work but is a recommended over returning entity objects in API or standard controller methods Yeah, I know... But when you have two weeks to deliver POC app you tend to cut corners :)
I've been using Visual Studio for Mac for some time and it's fine. Before that I was using Xamarin Studio for ASP.NET MVC stuff and though less polished than VS on Windows it still got the job done.
Not sure where I should look, this is how my network-tab looks after submitting the form: https://i.imgur.com/PZd7ms1.png Where do I find the response json? Also, what do you mean by the control? I initially think about the BlogController.cs but I suspect you're referring to something in the form? 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/PZd7ms1.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dlfvl0l) 
The synonyms are to avoid putting another server name directly in your views, stored procedures, etc. If the other server moves you'd have to update all your objects to replace the name. They don't impact what you're doing. To be sure I understand, you want to point EF to MSSQL server and query through to Oracle. I'm not a DB person so I don't know what troubles that might present purely from a database perspective but if it can be configured, you can use it but the tooling won't scaffold objects for you and the context won't know how to use them. If you create a View on the MSSQL server that has your fully qualified name to the Oracle table you want to use, EF will consume that. You alternative is to create stored procedures in MSSQL that get the data from Oracle and use EF to invoke those.
Nothing wrong with that until that demo is running in prod still in 5 years :) at least that's what always happens to me lol
Does VSCode work with .net core?
[It does!](https://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp)
Click on the request. The request method you're looking for will likely be a POST. I've taken a screenshot from a Reddit request: https://i.imgur.com/O6s594g.png 1) Click on the desired request 2) Go to the Headers tab 3) Scroll down, the payload is below all the headers It can be a GET request, in which case there will not be a body. The value will instead be sent in the query parameters.
Yes, I'm referring to the form control. &lt;input type="text" class="form-control" /&gt; Doesn't have a name. The other controls on the form have names using the `asp-for` attribute. You can either use that or use the `name` attribute directly. Both will have the same effect of getting the control's value into the form submission body.
Bootstrap as already mentioned , materializecss and others all provide out of the box modernized styling that can help people like you (and me) who have no artistic talent. If you want to find others just google something like bootstrap vs ... the article will most likely be shit and not worth reading but you will find alternatives. 
If you have access to VS I can see no reason why to pick the others and I've tried code and rider but they are just not as good in my eyes. There is no huge amount of info being hidden by visual studio with regards to commands like npm or webpack. If you can't be bothered to learn about things unless your IDE is too stupid to do them for you then you have other issues in my eyes. 
LOL yeah I've read plenty of crap vs articles. I use bootstrap all the time, I don't think there's anything out there that can rival it. What do you think? Any alternatives I should know of or at least be aware of?
I think you're looking for Browser Link which **I think ** is part of VS Web Essentials: http://vswebessentials.com (on mobile right now so I can't check)
From what I can tell, Browser Link refreshes pages on project save. What I'm thinking of is re-building/starting the project when the browser loads, if that makes sense. (Sorry for phrasing/describing this awkwardly - I'm not really sure what it's called.)
I like materializecss but there are some odd things here JavaScript wise and there mostly the same as bootstrap slightly different class names 
I'm absolutely lost. I've gotten a basic authentication path happening with one of the examples using OWIN against my Azure AD, but I'm not sure how to authenticate someone coming from say, O365. I can't find any clear documentation explaining how to allow users with any of the various MS accounts (Live, 365, etc, etc) to authenticate into a web application. I keep getting errors mentioning admin consent, or that the user isn't found in the directory, needs permissions, etc. I've added an external test user into my Azure AD, clicked on the button in the 'invite' email, etc. Didn't work. For my purposes, having an external administrator approve our application is impractical; I'm just looking for a path similar to the Google ID approach: auth against user store (Google), get response payload upon successful authentication, my application takes it from there and deals with authorization, etc. The MS offerings seem to dance just outside of what I need. I'm running out of time and starting to gasp for air. Any help you could offer would be seriously welcomed.
Turn off 'Edit and continue' and then your app keeps running in IIS express after debug stops. Then a normal build action will allow you to refresh and see changes in your browser 
Visual Studio Professional 2017
Try this: https://www.thomaslevesque.com/2011/03/21/wpf-how-to-bind-to-data-when-the-datacontext-is-not-inherited/ I've used that a few times and it really helps when your DataContext is a moving target.
This comment has just saved me from punching a wall. Trying to use a Google api that depends on NETStandard1.6.1 in a C# Lambda running on netcoreapp1.0. The direct dependency solved it. Although VS2017 had a moan, the AWS VS extension published fine to Lambda. Thanks!
I use a MacBook at work - Full VS in a VM when I need to do full framework projects, but for core I'm 90% VS Code. Most of my newer projects are angular &amp; angular-cli with .net core WebAPI/EF backends, so I usually don't miss the other features in full VS. I've been meaning to try Rider, since I've liked their other IDEs.
I have the strong feeling that this could end in "plugin hell"... For sure VSC is very suitable for smaller projects and it feels fast and neat, however, in the end it's a matter of scaling.
The whole jetbrains suite is free for students. I don’t know how I’ll live without it after I graduate.
Definitely some useful information there. Not sure the answer I'm looking for is in there but I bookmarked it anyway, thanks! 
That's a fair point. I think people who are tied to windows will just use Visual Studio no matter what, but on Linux and Mac I think hobbyists/small-projects will end up using VS Code and professionals will end up with Rider. Although, now that I've learned that Jetbrains products are free for students, I will definitely be giving Rider a serious look. :)
In a certain way the decision which IDE to choose is very similar to the question whether to apply unit testing or not :) Matter of scaling for both. 
Newtonsoft.json has a nuget package, some of the others might as well. Could you add the decompiled files to a new project and add references that way? That's my initial idea at least. Recompile through your IDE and away you go. Another idea, where did the exe come from? Can you check the program's root and see if any of the dll's it references exist there? Move your new exe to the directory with all the referenced dll's and try running from there. If you didn't touch any of the other code, it *should* still be able to reference those libraries. Take what I say with a grain of salt, I'm still a budding developer, but hopefully I can spark some ideas. My initial thought would be to just imitate the project as best you can and compile from there.
I would write the functions: M(params Foo[] foos) M(IEnumerable&lt;Foo&gt; foos) plus two extension methods: Concat(this IEnumerable&lt;T&gt; items, T item) Concat(this T item, IEnumerable&lt;T&gt; items) I guess the implementation is clear. Usages would be: M(foo1, foo2, foo3) M(foo.Concat(foos)) M(foos.Concat(foo))
They're fundamentally different objects, so not really. What would the code of those methods actually look like? You can of course do something like void Bar(Foo foo) { foo.DoSomething(); } void Bar(IEnumerable&lt;Foo&gt; foos) { foreach (var foo in foos) Bar(foo); } 
Well, first, you should use `IEnumerable&lt;Foo&gt;` instead of `List&lt;Foo&gt;`: public [ReturnType] MyFunction(IEnumerable&lt;Foo&gt; myFirstFooList, IEnumerable&lt;Foo&gt; mySecondFooList) { // ... } Doing so would allow you use a single function for any way you want to try calling it by just initializing a single-item array when necessary: MyFunction(firstFooList, new[] { secondFoo }); You could also just make an extension method that would take a single `Foo` and return an `IEnumerable&lt;Foo&gt;` containing that single item. Or you could create additional overloads that will take the single item parameters, create an `IEnumerable&lt;Foo&gt;` for the single item parameter, and then pass it on to the primary `MyFunction(IEnumerable&lt;Foo&gt; myFirstFooList, IEnumerable&lt;Foo&gt; mySecondFooList)` function. 
Think I can't find the desired request? Even if I debug it and have a breakpoint before the controller redirects to the Blog/Index this is all I have: http://i.imgur.com/JkHYguI.png I simply can't find the desired request =/ Though perhaps the lowest `1`, it didn't contain a payload but a `Form data` which has the url in it? http://i.imgur.com/9ge9Vpb.png If I click `View source` on the form data I get this: `BlogId=1&amp;Url=http%3A%2F%2Fetherealgears.wordpress.com%2F&amp;__RequestVerificationToken=CfDJ8H_EzDFh5CpKtvxuCbq8GOadDeq4B03QfSON73hD9mF1n2LdPm3S6EVvSto9DrUgREXM1-wcicU0fNuUHU7BFFhEsSQlA1xlqPWRWC0qZopLOJ1DFMVaSRW6kTiUShi5PhqZZKnlDXdM1b3e5dTDgoU` Now I'll try to add the name like you said! Alright! By writing it: `&lt;input name="TestInput" type="text" class="form-control" /&gt;` it works! :) The data is both in the Form data and in the debugger as a local variable. It didn't work with asp-for="TestInput" and could this perhaps have anything to with TestInput not being a part of the model that gets passed to the view? EDIT: Thanks for the help! :)
You may try just replacing the resource with dnspy (https://github.com/0xd4d/dnSpy). This would not require decompilation.
^ this. Look into the `params` keyword to make the syntax nicer, and internally you just deal with enumerations of items. Also be aware that [there is a Concat extension method built in - it's slightly different, it concats two enumerations ](https://msdn.microsoft.com/en-us/library/bb302894(v=vs.110\).aspx). You can make your own version. You are dealing with enumerations, that's the general case. If converting items to enumerations inline e.g. `MyFunction(firstFooList, new[] { secondFoo });` is too much overhead for you, [you can make the extension method mentioned here](https://www.reddit.com/r/dotnet/comments/6szv90/having_a_function_that_accepts_either_a/dlgtsf6/) to abstract that, and convert any object into a enumerable containing that object. e.g. `public static IEnumerable&lt;T&gt; ToEnumerable&lt;T&gt;(this T item) { return new [] {item}; }` Though that's going to show up in intellisense *everywhere*. 
&gt; It didn't work with asp-for="TestInput" and could this perhaps have anything to with TestInput not being a part of the model that gets passed to the view? Not sure, I'm not too familiar with the `asp-for` attribute. It was added in .Net Core, which I haven't used much. I would assume it would work like your other form controls. Yeah, form data can be sent in a second format, which you found. I neglected to mention it as a possibility. It's a set of url-encoded name-value pairs separated by `&amp;`.
Or keep Edit and continue and detach the debugger instead of stopping the debugging.
I can't think of any good reason not to use the packages/DLLs. 
VS enterprise is also free for students. Through MSDNAA, now called Imagine.
Thanks all, I'll use IEnumerable and overloads then. ;)
Nuget packages are Great Because they have versions, which source code does not. So I would recommend allways go to nuget for your stable releases, and leave the source references for development and debugging
See what the project maintainers are recommending. Chances are that if a project is releasing nuget packages that's the preferred way to consume their product. If you use source and build yourself, you might have trouble asking for help later. 
Stick to the NuGet packages. If you feel like something's missing you can write your own extensions or reach out to the creator of the package. The only reason I can think of to use the source code would be if you find a bug and it would take less time to fix it yourself than to wait for the creator to fix it.
I've been using the packages lately for my own libraries and it has been a very pleasant experience. 
&gt; For example, common/utility libs might well be integrated in source code form and possibly be even adapted to your coding conventions (namespaces changed, etc.). This can lead to more problems than it's worth. What happens when the original utility is updated with new functionality? If you've already made changes it becomes onerous to bring those changes into your modified version of the code. 
**Update:** You appear to be correct; it's a feature of the BrowserLink.Loader package in ASP.NET Core. I was working on an MVC 5 project, hence the confusion. Thank you.
This is also useful, but I've realized what I'm describing is only available in Browserlink.Loader for Core.
Other than Materialize, I really like both: * [SemanticUI](https://semantic-ui.com/) * [Clarity](https://vmware.github.io/clarity/documentation) (based on Bootstrap, but better) They have a much cleaner and more polished look than Bootstrap, imo. 
Thanks for the tips!
While I frequently use ILSpy to look at code some inconsiderate dolt forgot to check in to find bugs or understand some undocumented behavior, I generally use Telerik JustDecompile when I intend to recompile the code. Not sure if this will solve your issue, but it's another free avenue to investigate. 
Clarity has the most features of the bundle, including an existing login page. But regarding: &gt; And maybe something with a login already coded in so he can login and post stuff himself if he wants to. This would be a feature handled by your ASP.NET app, regardless of the CSS template you use. I would highly recommend you start off with Core's built in templates, including Identity for login/etc. If you want to learn how to make a functional CRUD application with properly authorized login/administration, I would highly recommend this book: http://www.apress.com/us/book/9781484203989 If nothing else, you can look at the source code for each chapter for free and see what he's done.
Wow awesome! That is great info. I'm at work right now but I'll definitely take a look at it tonight. Thank you so much for your help!
Here are 3 possible answers: * use params keyword to accept a variable number of Foo args. (You just want a bunch of Foos in this function) * the functions with these different signatures are not actually the same. myFunction (FooList, Foo) will not be processed the same as myFunction (Foo, FooList) so you should probably create these overloads instead of adding a bunch of branching based on the argument type (Foo, FooList, and arg order are relevant to your processing) * If none of the above seem right to you, my guess is there is something messed up with your design (somehow Foo and FooList *are* the same in your application)
Better yet, use ssh and the local git config to select the appropriate private key. 
Best practice depends entirely on how it is used. If there is no sharing between different solutions, you should keep your code in the solution in appropriate projects. But, if there is sharing, do use the NuGet way. Personally I like Packet because I can incorporate http and git sources alongside NuGet and because it is far more explicit what packages is used in what projects. There is nothing wrong with tiny dlls for abstractions or small but significant functionality. This is the route .net itself is moving. For 3rd part libraries always include them from NuGet, including the source code means you now have to include bugfixes and enhancements by yourself. Only when you find an old abandoned project you need to update does it make sense to include the source code.
Create your parameters of type Object. In the function get the Type of the object(s), and perform operations based on the Type. In this case specifically, I would use GetType to see if it is a Foo. If it is, I would create a new List&lt;Foo&gt; and add the parameter to the list. Then perform operations. Somethign like the below. public string myFunction(object myFirstFooList, object mySecondFooList) { List&lt;Foo&gt; firstFoos = GetList(myFirstFooList); List&lt;Foo&gt; secondFoos = GetList(mySecondFooList); //Do Things } private static List&lt;Foo&gt; GetList(object myFooList) { List&lt;Foo&gt; foos; if ((myFooList.GetType()) == typeof(Foo)) { foos = new List&lt;Foo&gt;(); foos.Add((Foo)Convert.ChangeType(myFooList, typeof(Foo)); } else { foos = (List&lt;Foo&gt;)Convert.ChangeType(myFooList, typeof(List&lt;Foo&gt;)); } return foos; }
&gt; smart people 
Welcome to reddit where most votes are not deserved ;)
This is pretty much where I'm at. We have msdn vs pro at work, and I use Windows on my laptop, so I just use vs. I also have vs code replacing all the cases I would have used atom for, though. 
Adding voice to the choir, NuGet packages. If you need to extend then do so by extending the class.
You shouldn't have one function for two fundamentally different data structures. ClassA and list of ClassA share no common members except the object members but then the function you are writing should be usable for classb as well and then the question doesn't make sense in the first place. Tell us a little something about what exactly you are doing. Because yes using interfaces two widen it's functionality is good but just saying overloading is solving your problems may be syntactically right but catastrophic for the cleanness of your code. Two functions should only ever have the same name as in overloading when they fundamentally do the same thing.... Like bootstrapping a private functioning that does the actual work but needs type specific parameters t work correctly.
Just be aware that a `params` argument allocates an object. If having a single `Foo` is a common case, and your application is at all heavily-used, I'd strongly suggest keeping two methods around.
You could just change the signature to accept a List&lt;MyObject&gt; and pass it a list with one or more MyObjects. Then in the method, just iterate over the list and do what you want with MyObject.
4 clicks in and I still have no idea WTF Akka is for
But the required .NET Core 2.0 is not. :P https://www.microsoft.com/net/download/core edit: To elaborate, because some people (*caugh*anonveggy*caugh*) don't know/get it. In order to use the ASP.NET Core 2.0 framework you need to have a runtime that implements .NET Standard 2.0 (an API specification). This is because this framework targets only .NET Standard 2.0. Currently the only runtimes that support this standard are .NET Core 2.0 (which is not yet released), or .NET 4.6.1 with a new tooling set (which is not released either). So while ASP.NET Core 2.0 is released, there's no way to use it yet (unless using preview software). That's likely the reason why Microsoft made no announcement about this yet.
http://getakka.net/
[Akka.NET](http://getakka.net/) is a .NET port of the [Akka](http://akka.io/) framework that deals with the many complicated aspects of a distributed concurrent/parallel exchange of message objects. An example of a relatively simple application implementing Akka.NET could be this [Akka.Remote Remote Deployment Sample](https://github.com/petabridge/akkadotnet-code-samples/tree/master/RemoteDeploy). A more advanced example could be their [ASP.NET and Windows Service Microservices with Akka.Cluster](https://github.com/petabridge/akkadotnet-code-samples/tree/master/Cluster.WebCrawler) example, that comes with some very good documentation.
Honestly, the only real reason is that someone did the work to add those methods to .NET Core, but no one did it for the Framework. That's all there is to it, really. I suspect that adding generically-named extension methods to existing namespaces could be considered a breaking change since it would likely create ambiguous call situations for many users who created their own variants of these methods, but I couldn't find any discussion on that. You could relatively easily use [the Core implementation](https://github.com/dotnet/corefx/blob/master/src/System.Linq/src/System/Linq/AppendPrepend.cs) in your own projects, if you wish.
Enumerable.Append is not included in .Net Standard, so if i write a library that use it in .NET Core, it won't work for other platforms that implements .NET Standard (.NET Framework, Mono) right?
Yes, that is correct. If you want a cross-platform compatible library, you'll need to forgo that method or include an implementation yourself.
Thanks for this, does sound very useful indeed
Would an MMORPG be a good use case?
This is wonderful news, and I'm so glad they included Solarized as an option. Now the command line can be functional *and* pretty!
Which just begs the question, how can you finalize a product on a platform that isn't? A bit of a cart-before-the-horse situation in my eyes.
Hope it is, im sick and tired of Javascript. It's being pushed everywhere, from web apps, desktop apps, mobile apps down to microcontrollers. Disgusting!
18th September
The platform is .net standard 2.0. The implementing net core 2.0 will be released on the 18th September
There are definitely many aspects of an MMORPG that would benefit from a framework such as Akka.NET, but I think that there are also parts that wouldn't. The client-server communication over the internet will assuredly offer better timing and latency with an UDP based framework.
Why don't you like JavaScript?
I don't like that javascript is single threaded in 2017. I like types which help me with readability and debuging my code. Hoisting? invoking a function before it is declared? WTF? (i understand the concept, execution context and its creation phase, but i dont understand the reasons behind such a design) Coercion - i prefer explicit conversion. Sometimes i have to use var self = this to make sure im working with the right object (example - function inside a method). No interfaces. Feels to me javascript is forced into areas where it just doesn't belong (Electron - web tehnology for desktop? Oh please). 
But ASP.NET Core 2.0 is already released as a final version. That's what he meant. Framework is fully released, but the runtime not?
ASP.NET Core 2.0 target .NET Standard 2.0, not .NET Core
This. 4.6 is already out and implements net standard. There you have your run-time.
.NET Standard is no runtime, it's an API specification. ASP.NET Core is no runtime, it's a framework.
To your code net standard is a runtime yes. Targeting net standard 2.0 compiles your as though as any implementing runtime/framework combination can run your code. The app then determins on start up whether there's an implementing runtime given. So targeting 2.0 with aspnet core 2.0 will currently give you a net framework 4.6 runtime until net core is out and you choose to run with net core. Net standard is promise that asp relies on. Right now that promise is fulfilled by net framework. On the 18th September this promise will also be fulfilled by net core 2.0 the asp net core 2.0 framework doesn't care who delivers that promise.
The whole frigging point was about .NET Core, not .NET 4.6. Are you trolling?
Is this the Xamarin for web applications? Or am I getting this web assembly thing all wrong?
But the platform Target is net standard 2.0 not net core. You are misunderstanding what aspnet core 2.0 is. Aspnet core is not "net core"..... it's the core framework of asp.net... which now targets net standard 2.0 to allow cross platform implementation via net standard promises.
eventually, idk who is the one trolling here 
I'm not trolling tho. dude's issue is just that s/He mistook aspnet core for a net core thing because of the name. Aspnet core is just a net core thing because the actual platform is net standard which net core is there to implement.
Okay, either you don't understand because English is not your native language, or you're trolling. Either way, I'm done with you.
Another dumb question: In my project, i changed `TargetFramework` to `netstandard2.0` and then use `dot build`. Since netstandard2.0 is implemented on dotnet framework 4.6.1, how can i run it with dotnet framework 4.6.1. Of course i can change it to `&lt;TargetFrameworks&gt;netstandard2.0;net461&lt;/TargetFrameworks&gt;` and then use `dotnet run -f net461`, but what is the mean of using a netstandard if i can't choose a runtime to run with.
English is not my first language. Yes. But either way you falsely assume that asp.net.core's platform which it is not.
I'm not mistaken at all, I'm not confusing anything. You're constantly assuming I do. In order to run any ASP.NET Core 2.0 project on the supported platforms you need .NET Core 2.0. Yes, the framework targets .NET Standard 2.0 - but that's just an API specification. You need an actual runtime that implements the standard, which is either .NET Core 2.0 (not released and only option for Linux and Mac OS X), or .NET 4.6.1 with the 2.0 tooling (not released either). So the framework is released, but the required runtime or runtime tooling is not.
I would argue that going online to find the answers shouldn't be an issue. Most developers do that hundreds of times a week. Have them take the tests, and schedule some sort of phone/video post-interview and talk through his answers. Why did you make this decision? Did you take this into account? Being able to explain his decisions is a big indicator that he understands his answers and didn't just have a friend do it.
What tooling are talking about specifically?
https://docs.microsoft.com/en-us/dotnet/standard/net-standard Look at the section ".NET Implementation support".
Typescript is the answer to most your frustrations. My preferred way to work is make a backend in C# and generate typescript interfaces of my rest endpoint (using the typelite nuget package). That way I'm sure my backend and front-end are always in sync. The only thing you can't fix with typescript is the singlethreaded issue.
But none of your previous comments ever mentioned the tooling. The runtime is there. The framework is there and the nuget package requiring the runtime and standard being implemented. Just now you added the question for the tooling.
I know this opinion is controversial, but please bear with me for a second: For line of business applications HTML still seems very underpowered to me. Especially since all we really get for user input is the &lt;input /&gt; tag. I'm not saying it's not possible to make great apps with it, but I kind of seems quite ugly with all the JS used for simple controls. Is there anything in the pipeline in that regard?
I mean, there's nothing stopping you from rolling your own controls using events and canvas. 
I thought it was odd that VS wanted me to upgrade the EF package on a brand new project I started. I blatantly clicked ok and was told it targets .net standard 2.0. Curious to why these packages are rolling forward without the proper target runtimes in place. Unless I'm missing something, which I probably am.
It doesn't work if your language isn't English (technically, if your CurrentCulture uses commas as the decimal separator instead of periods). I'll see if I can do a PR. EDIT: [PR here](https://github.com/Microsoft/console/pull/10) EDIT 2: I've removed the PR, the [very first pull request](https://github.com/Microsoft/console/pull/1) handled the exact same problem.
Typically Professional (or even Community) is enough for a standard dev and Enterprise is required for architects or seniors. I'm sure Ill get comments disagreeing saying they need all the tools, but from my experience most don't use the enterprise tooling. If you wanted to play it safe you could always get a few licenses of Professional (or maybe a single Enterprise). Then start the rest on Community and slowly upgrade as they ask.
If you have to ask, you probably don't need enterprise. At my work, we use professional. It's rare we find ourselves missing a feature from enterprise. Depending on the size of your team, you might even be ok with community, which is free.
Interesting perspective. Thanks for the input, I'm looking over the Community edition and it doesn't seem to be so different from the Professional version... Looking at the licence terms, we might even be eligible for Community.
Understandable, what's your take on the difference between Professional and Community?
In real terms the biggest difference for me is CodeLens. I'd say Resharper (or your preferred plugins) and Codelens is perfect for 99% of the time. Finally, there are some weird ways of getting CodeLens for free in community (google around, it involves installing some SQL tooling and seems random).
Cool, thanks for the help :)
Enterprise has some interesting features for debugging multi-threaded programs that I may one day learn how to use. And continuous unit testing is a nifty idea. But pretty much everything that I actually care about has been pushed into the community/pro version.
We use professional at work. The only feature I wanted out of Enterprise was the code metrics but we picked up some Ndepend licenses to make it up. With that and Resharper, I'm really happy. 
They are actually identical, it's just the license terms which are different. If you are not covered by the license terms for Community, have a look at the Microsoft Developer Action Pack, which is a cheap way to get VS Professional.
Enterprise comes with development licenses for a lot of Microsoft software, which is otherwise quite expensive. If you use any of that software and need development environments for individual developers enterprise is a no brainer. There's also things like release management in TFS (if you do this), intellitrace and a handful of other features that can be quite useful. If you don't qualify for community edition, and lots of businesses don't, it's probably worth talking to the Dev team about enterprise. It's not super critical, but there's a lot of value in it and it's not dramatically more expensive. In the end it's really important to get the tooling that makes your particular team productive. Enterprise isn't that expensive in the grand scheme of things, and if it makes your team work better it's worth it.
I'm a team lead of 7. I have enterprise, the rest have professional. Useful for me to have the higher grade MSDN tools more so than the VS itself . Professional + Resharper Ultimate will be just as good for most developers anyway.
Sorry mate but you're flat out wrong. No need to insult his/her English skills (which are great, by the way /u/anonveggy) just because you're misinformed.
If I recall correctly the rule is: * up to 5 developers OR * $1million revenue per year Once one of these limits is broken you "have" to start paying for licenses. If your use of .Net and company are quite young, then you will likely find community edition is just fine, for now.
it's like 1k vs 6k. honestly i feel like pro is a no brainer. there also not that much software available with ent that isn't available with pro,.
SharePoint, a production office professionsl plus license, a handful of others. SharePoint alone pays for the difference if you need that license. The features in enterprise are really awesome, honestly. It's not by any means a no brainer, but it's a decision that absolutely shouldn't be made by someone who isn't a developer. 
Whichever way you decide make sure you allocate some budget for sundry tools (ndepend, resharper, jira, trello, etc) and/or components. Few things are more moral crushing than having to jump through lots of hoops to spend a thousand bucks, or even two, on a tool that will make your job so much easier. Here's a good example: red-gate.com. Some of the best tools out there for futzing with databases (my god the schema compare and data compare can be life savers). They're frigging expensive... but if you need to **know** that two databases are truly the same these tools will tell you that in minutes. Without them... it would take at least a day or two of soul crushing work. Btw... I prefer VS Pro. I'd like to have VS Enterprise (the built in code coverage would be so nice)... but it's just not worth the price (for a small small company). Also I'd love to try the testing tools (not sure if these come with VS Enterprise or if you need VS Testing or some such)... but again the price is just ridiculous. Once upon a time MSDN Universal was the goto product. You got MS Office Pro, VS Enterprise, SQL Server (dev) and you got access &amp; license keys (for dev, not prod) for all pretty much all MS software. You could download ISO's of various operating systems so you could build out a nice set of VM's for testing. I used to subscribe when the price was around $3k/year... expensive... but vital. You should track down a MS Licensing person. There's deals to be had. You need X * (win10, ms office, visual studio or msdn) and Y * (win10, ms office) and Z * (windows server + windows CALs). However... if you're not 100% up to date with all the licenses needed (eg. using SQL Dev for in office applications... whoopsie) then this might turn expensive. But then again... you really ought to be up to date on licenses :)
The only feature most would miss is code coverage for unit testing, and that is something that you can easily replace with dotCover. The dependency management stuff is cool, but most shops don't have the process/perseverance to make it useful. 
Mostly identical, but there's no CodeLens in community. Some people are indifferent about it, but personally I really love it. Probably not worth paying for Enterprise out of your own pocket for it.
Yeah I like codelens a lot, somebody else in this thread mentioned a way to get it in Community by installing SQL Server Tools.
Code coverage for unit testing is important, but there are workarounds for Pro. I say give them Pro, but cut your devs some slack if they want paid plug-ins or something to supplement.
It depends on what you need. I rarely use the enterprise features. The real benefit of the enterprise license is all of the other software you get, which is quite a bit. If you need any of that software you might want to add up the prices and see which way is cheaper. Also, get the subscription not the standalone license. 
A really, really useful comment. Thank you for the effort. I am taking all of these tools into consideration, the devs should have anything that makes their life easier.
Continuous unit testing is indeed a a nifty idea, but it's not very practical in an age when basically everyone is using laptops for development. If you're lucky enough to work on a reasonably powerful desktop, maybe...
Is code coverage still enterprise exclusive? I didn't see it on the comparison page and I'm losing track of what's getting shuffled down the feature ladder.
We all get by just fine with professional licences and Resharper subscriptions, doing mobile development with Xamarin. I think one of the guys has an enterprise licence for as/when we need to use the profiling tools, but it's so rare that it's not worthwhile everyone having it.
businesses will always prefer you just do the 2 days of soul crushing work to save on capital costs, they are paying for you anyway and if they need something else done quicker, you'll just work weekends for free anyway.
You might look at how to Uninstall a project template from visual studio . That's all you probably need to do it seems though I've never tried. 