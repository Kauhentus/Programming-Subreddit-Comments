My go-to https://www.connectionstrings.com/
C# isn't significantly harder than VB.net except for the case sensitivity and an excess of syntactic sugar.
It does :)
&gt;public virtual DbQuery&lt;ReturnFMIDHierarchy\_Result&gt; ReturnFMIDHierarchy\_Result { get; set; } &gt; &gt;public IQueryable&lt;ReturnFMIDHierarchy\_Result&gt; ExecReturnFMIDHierarchy(DateTime date, string fmid, int maxDepth = 16) { &gt; &gt;return Query&lt;ReturnFMIDHierarchy\_Result&gt;() &gt; &gt;.FromSql("dbo.ReturnFMIDHierarchy @FMID = {0}, @pDate = {1}, @MaxDepth = {2}", fmid, date, maxDepth); &gt; &gt;}
1. Get the database from the .asp 2. start a new asp mvc C# application 3. search for "database first entity framework" tutorials 4. create CRUD (models, controller &amp; views) 5. Modify application to function as desired 6. $$$
```csharp public virtual DbQuery&lt;ReturnFMIDHierarchy_Result&gt; ReturnFMIDHierarchy_Result { get; set; } public IQueryable&lt;ReturnFMIDHierarchy_Result&gt; ExecReturnFMIDHierarchy(DateTime date, string fmid, int maxDepth = 16) { return Query&lt;ReturnFMIDHierarchy_Result&gt;() .FromSql("dbo.ReturnFMIDHierarchy @FMID = {0}, @pDate = {1}, @MaxDepth = {2}", fmid, date, maxDepth); } ```
Thanks for that. I actually googled Xamarin forms for macOS previously but never came across those docs, only the blog announcement. Cheers!
Get half the money up front waiting till the end to get paid is never a good idea. Charge double what you'd be ok receiving and get half up front. The better the family friend the more likely they are to fuck you over.
You need to learn the entire .net web stack.
This is unfortunatelly true. I am saying unfortunatelly, because you really shouldn't have business logic in USPs. Only exceptional stuff.
I would recommend watch the Youtube series "[ASP.NET Core Tutorial for Beginners](https://www.youtube.com/watch?v=4IgC2Q5-yDE&amp;list=PL6n9fhu94yhVkdrusLaQsfERmL_Jh4XmU)"
&gt; Then you start a complete new project in ASP Core, not a single line of code and not a single html tag should be used of the classic asp site. This. You just can't convert easily. You need to do a whole re-write. All of our internal web apps were written in .NET 3 about 10 years ago from the previous developer. I'm re-writing all of them in Core. It's really the only option.
I don't see why UWP would be dead? It seems great to me as well as the window 10 IoT platform. If they do abandon it it would be really sad because its damn convenient.
I concur!
Well that's one way to solve the concerns about npm in Javascriptland.
First, open your desk drawer. Next, put your head in the drawer. Finally, repeatedly slam the drawer until you lose consciousness. When you come to, repeat the process. After 6 months of this kind of training, you will be ready to open the asp code.
Downvotes aside, this is the best possible advice. Asp, very dangerous, you go first.
Exactly, nothing to learn there. Get the requirements as if it was a new project and write it from scratch using a a more modern technology that will help you in the future. Waste of time unless you are going to start a business that refactors old ASP pages into new ones.
Consider the Problem Details specification for the errors. RFC: [https://tools.ietf.org/html/rfc7807](https://tools.ietf.org/html/rfc7807) Example implementation: [https://github.com/christianacca/ProblemDetailsDemo](https://github.com/christianacca/ProblemDetailsDemo)
Is there any reason you prefer to avoid RESTful practices and rely on HTTP status codes?
Thanks! I’ll get on it
Therer is no stablished pattern, we are migrating from an old .Net Framework to ASP NET Core with Web Api's but the only "rule" was keeping the Stored Procedures
Currently using it, was hard to convince the Veteran devs to move to Dapper, but other veterans from other team back me up
&gt; Mainly when everybody else who worked on that project longer than you tells you to. You mean those people who don't want to lose their jobs to someone else? It's called job security when other people "can't touch your code."
Oh yeah. I think you're totally right. I guess I was thinking about some other cross platform stuff they've been developing recently like vs code and dot net. But I didn't think about the fact it's all targeted towards developers.
I've never gone down this road but Docker + Asp.net core is the answer here as far as I can tell. Check this out. https://www.hanselman.com/blog/BuildingRunningAndTestingNETCoreAndASPNETCore21InDockerOnARaspberryPiARM32.aspx
I personally would avoid the TVP solution. It's surprisingly inefficient. (remember that unless you're using SQL Server 2019, your tvp will have a cardinality estimate of **1** row). There's also the concern about marshaling your vins and stocks into table types. If you're using sql server 2016+, you can probably just use either `openjson` or `string_split`. These TVPs are efficient and come with a baked estimate of 50. perhaps you could restructure your query like so:: update v set visibility = 0 ,archived = 1 ,archiveDate = getdate() from vehicles v where subscriptionId = @subscriptionId and not exists ( select 1 from string_split(@vins,',') where item = vin) and exists (select archived intersect select q from (values(convert(bit,null),convert(bit,0))_(q)) and not exists ( select 1 from string_split(@stocks,',') where item = stock) and not exists (select left(stock,3) intersect select q from (values('TAC'),('CON'),('SIL'))_(q)) Once you have it parameterized like this you can now actually pass parameters rather than concating things, and potentially running a fowl of sql injection. You can then use it like this:: var subId = new SqlParameter("@subscriptionId", _subId); var vins = new SqlParameter("@vins", string.Join(",",_vins); var stocks = new SqlParameter("@stocks",string.Join(",",_stocks); dbContext.ExecuteSqlCommand(query,subId,vins,stocks);
wouldnt using the http status codes be using RESTful practices? 404 bad request means the payload was incorrect e.g. bad parameter as in my example. but id like to bring back a message why. perhaps im completely retarded and have this backwards, have you got any good documentation on this?
If it's an old webform application you might want to check into creating a .net core razor pages web app as the structure is similar.
Which book was it?
This.
Maybe. It is possible that there exists this type of thinking in real, not only as a joke. What is more probable is they now have some way which works with USPs and don't want to end with system where 80% of system in old USPs and 20% in new guys way they. Thats how it ends usually, because some 'priorities' came in way. It's a rewrite, not a greenfield project.
Code Complete 2.
Literal answer: as little or as much as you want to. &amp;#x200B; In practice: Probably a bunch, implicitly. You probably won't want to touch the container itself much except for registering components.
I second this. Also, go with Alpine based images, if you can.
This.
The biggest concern remains (same in NuGet land): The provided source code and the published package are not related. What you publish and what source you provide can be vastly different.
It is more complicated, yes. I only recommend you use it in advanced scenarios, where you notice the graphics are out of whack or not presented correctly
asp.net seems to get closer to classic asp every day. If it's a small site, which you better hope it would be, it honestly probably wouldn't be too hard to port using razor pages etc. That being said, if you don't know classic asp and you don't know asp.net you would probably have to learn at least the basics of both.
From the post, it's not web forms. Classic asp is even older than asp.net web forms.
From the post, it's not web forms. Classic asp is even older than asp.net web forms.
If they have any current API for calling the stored procedures, use it. If not, just put some repositories on top to at least abstract away the fact that they exist from the rest of the code. If they ever want to migrate away that layer will make it much easier. For what it's worth we use stored procedures at my work and it's never been a problem. We will migrate away from Azure SQL entirely at some point because it doesn't scale cost effectively but that's the only reason we will move away. The stored procedures themselves don't really cause us any problems.
I bet r/GitHub is enough. Hate to see same news on 1000+ loosely coupled subreddits.
I'm glad it was posted here, I'm not subscribed to /r/GitHub
I would use some of the hw features of Pi. - monitor CPU/Memory usage and show history. Readonly, simple not really Pi specific but more fun than just another todo/eshop - track wifis around. Could be done with Pi(zero) running on batteries. - control something - LED, room light, fan via rellay. TV with IR LED, .. That would need additional hw and some manual/electrical skills. - Remote control of Pi/Linux. Check for system updates, execute shell comands, browse disk. There is a tool like this, but i don' remember the name. - a robot/car controlled via web. You can buy cheap parts on ebay - webcam + servos so you can look around remotely. Maybe with some AI to detect stuff/people and make a photo when someone enters the room... Pi is a nice small computer but if you don't use its hw extensibility it is just a slow PC
I have a raspberry pi zero w downstairs with a motion sensor upstairs that calls a raspberry pi upstairs. You could do something similar. Maybe hook up a camera and button. Push button. Take pic. Send picture to server, which logs time and stores pic. Set up front end to view logs and pics.
[removed]
I mean relying on the status code as delivered by the HTTP response, instead of redundantly putting it in the response body.
/r/dotnet has 4 time as many subscribers, I don’t think a post on an unrelated subreddit is enough.
Well then it should be marked as a crosspost....
[removed]
Has anyone tried it yet? Will it publish a NuGet package to NuGet gallery or a Github sourced feed?
Yea I saw that but I thought there was probably a slight chance the OP might actually be referring to a webform application.
If the package is published from Github using Github actions, with source code on Github, it stands to reason that there is a greater accountability in the package manager since each part of the chain can be verified. What am I missing here?
**If** it's published that way, and then it needs to be signaled in a way. But it doesn't has to be that way, you can manually publish just fine.
IMHO, You'll probably learn more by doing Microsoft's tutorials and googling stack overflow when it breaks. As far the book, anything .net core is a good start.
We handle our packages locally (locally hosted Mercurial repositories, actually), and use tags for each version, and a script to build the package from that. This helps ensure that we know exactly what code ends up in the package.
Thank you.
I wasn't talking about being aware yourself what commit relates to what version. I mean as a user being sure that what the author claims to have released actually is what's released. As a malicious author I could add a tag to a commit and tell everything "this is version 1.5", even pointing to an automatic CI pipeline, but what I have actually deployed from my local machine is something entirely different. With some languages it's easier to figure out than with others, but honestly.. who checks it? It's all a huge trust-system. Would be nice if GitHub offers an integrated solution where the author could not fiddle in-between anymore. GitHub builds and publishes the packages automatically, and then it gets some kind of "verified" flag.
Old? You mean current. 2.2 came out in December. If you like learning from books then yes it is extremely relevant. Nobody is using 3 yet, it’s not even released. And the planned been features are focused on a lot of fairly experimental. Don’t postpone learning. 95% will be the same when 3 eventually comes out and you have a good base of knowledge.
Seconded, do, struggle, solve. &amp;#x200B; But books and blogs fill some gaps and express ideas you may not have been party too :)
Love it, although the worst mistake people make is not doing anything without knowing everything. (Generic version :) )
I didn't knew how different the versions were so i was afraid it may have been outdated, but you're right i shouldn't postpone learning. Thank you for your advice.
It's not even out yet so we don't actually know how big the differences will be. Some things have been announced, but it's still in development. However, things are almost always incremental changes. Even if you go back a few years to asp.net 4 (not core) you'd find a lot of similarities.
What makes this better than just publishing packages to NPM? They both apparently charge for private packages. And wouldn't NPM be more discoverable anyways? Who's going to look for published packages on GitHub?
What's an internal author.
Updated ;)
Not what you asked, but consider Keycloak. You could even run it in a docker container. Has everything you need, and unlike Identity Server doesn't require you to custom write / maintain a bunch of screens and code which defeats the whole purpose and benefit of having an off the shelf IAAA framework. And by everything you need, I mean everything - admin screens, login screens, user federation, LDAP/AD integration, auditing, out of the box SAML and OAUTH/OIDC, X509 screens, a robust and extensible API .... And Microsoft has good middleware tools and examples for tying it into SPAs and traditional NET Core web apps.
All you have to do is change the database connection to the environment you want to migrate, then run "update-database", just make sure you have the correct project as the start up project.
or give you ideas and approaches for future problems you might not have yet
This looks amazing. Thank you!!! Identity server has cost me my hair lately :)
My advice is to do what you like to do. Because you'll be better at it than otherwise.
I implemented something like this just recently. Have a central store of the domains that can connect to the site, and a central store of user accounts, with username, hashed password and the SID of the user in their domain. I used Mixed-Auth (https://github.com/MohammadYounes/OWIN-MixedAuth), which allows you to grab the domain, username and SID of the current Windows user when inside their domain. You can use this to authenticate against your site. Alternatively, they can login via a username and password. It's working well for us.
During your career you will likely learn new technologies and languages every few years. When I started out 20 years ago, C# and Angular didn't exist--now all I do is angular, C# and SQL. In a few years, hopefully the SQL will be less and the back ends will be more NoSQL. Even though I have a lot of experience in Angular, I do like React and Vue quite a bit because of the flexibility. I wouldn't worry too much about picking the right one.
I have been teaching myself react and redux as there is a core template for it. Apart from the absence of tutorials it's been good so far.
From a pragmatic viewpoint if Angular is so popular in your area go ahead and learn it. If you're thinking about other frameworks I'd suggest taking a peek at React because firstly it's more popular than Vue in commercial space (from my experience) and secondly because it's easy to learn and comprehend thanks to its _minimal api surface area_ - it's a relatively simple library.
&gt;but I've heard a lot of negative opinions about it and in the recent stackoverflow survey it was one of the most dreaded technology, ignore that - angular is fine. Just about every developer I have asked that said that they hated angular was someone who used angular 2. Programmers have long memories and are biased to when they last used a product. Angular 7 is fine the fact that you like it. most opinions of it are NOT about any recent version and thats where the jobs are means yes - you SHOULD use angular
You know, back in the day, we called "full stack" developers for "potatoes", because you could use them for everything, but they weren't really good at anything. That being said, there is nothing wrong in being polyglot or knowing your way around the front and backend. But at the end of the day, fullstack is an unprotected hype term, and something people label themselves. I'd take the hypes with a pinch of salt. Most devs I meet drum their chest and recite their claims for glory, but it is all smoke and mirrors. Love what you do, spend 15 minutes at the beginning of each day keeping up with the industry and don't get pinned down in a limited role, like becoming the datawarehouse guy or something like that. You should have a field of specialty, though. Mine is security and they keep coming back for more. Thats my 20 cents.
It'll be missing a couple things such as SignalR, but the bulk of it is relevant. I, too, have an ASP.NET 2.0 book.
Angular is fine. Do what you need to do to find work. If you tried it and liked it, why not? There is plenty of time to learn plenty of other frameworks. Plus who knows? Maybe blazor will come in and you will be making all your SPAs in C#.
I think the lumping of angular js and angular on the survey invalidated the results for angular on that survey. Angularjs is not Angular.
I love Angular (I don't like angularjs). Most Angular devs I talk to love it. That survey included angularjs and Angular together, so I think it really invalidated the results. Just do what you love to do, learn as much as possible so that you can be well rounded.
I recommend using App.config transformations. Create a transformation that contains the connection string for each environment. Then just switch the build configuration before executing Update-Database. If your transformation are in a file other than a Web.config you may use SlowCheetah plugin.
Personally, I prefer Razor Pages (or [asp.net](https://asp.net) MVC) without or any front end framework. I use BootStrap and Telerik (mostly just for the grid). I prefer many simple screens rather than SPA applications, but that's just me. Angular is popular right now, so you may as well learn it. But things change, front end frameworks come and go. The best thing for a good career as a developer is really domain knowledge. Get a .net job and learn the business as well how to code and then you can become the MVP at work and make bank.
If you can, i'd recommend getting a ALM tool with automated releases like Azure DevOps (free for up to 5 users). That way anyone in the team can release it reliably when needed.
alpine just recently also shipped arm32 alpine images too so good time to check it out.
Don't listen to those guys. What programmers like is not always what the market wants, and the market loves angular, so if you like it too, just use it, is not going to go anywhere anytime soon.
Agreed. Being full-stack should just mean that you are comfortable and familiar with every level of a solution, but likely not a master in most. With that being said, you aren't too valuable unless you have a specialty. That specialty could be a concept area, languages, tooling, or general skills like problem solving and/or mentoring. Be flexible, open-minded, and willing to adapt as the industry needs you to. That's how my best teams have formed, when people checked ego's and boxed-roles at the door, and collaborated as a self-organizing, growing team.
Technology is just a means to an end, focus on whatever makes sense for your job market / future career development
I’m already using those, so depending on what i pick in the dropdown on the top it will also change my update db location? Nice
&gt;Keycloak within 3 minutes I was able to get a docker container running with this, only bad thing i noticed about the container was that in idle it was using 600mb of memory which is pretty high. I do like it though, everything is done for u on the identity server side, havent tied it to a .NET core app to see how well it works
Just use Xamarin.Forms if you want a crossplatforn UI desktop framework. It has WPF, Cocoa and GTK backends, using shared XAML XF UI.
I would say 1 React, 2 Vue and 3 Angular as a bet for the future. I am a full stack myself, and I love React. Haven't tried Vue but every time I begin to read their docs, I get annoyed and it feels like the poor man's react. Vues place in the market feels like PHP in the sense, that it's popular cause it's easy to get started, and it might even overtale marketshare but no one capable would like to actually work with it.
I do full-stack development, but try to concentrate on the frontend side of the stack. So that means I mostly do the front-end/web development part, but am comfortable with the backend side as well and some devops. So most of my experience is in the front-end part and I will be most useful there, but have the advantage of being able to alter, adjust the backend if needed. But when serious backend work or devops work needs to be done I turn to more experienced colleagues. These days Angular seems to be the flavour of the month, but I see a steady rise in Vue development. Every technology has some advantages and some disadvantages, so it's not so much a question of choosing the right technology, but rather the right one for the job. There is nothing wrong in learning one technology like angular more extensively and experiment with some others. Learning a technology/framework/library is good, but in the beginning concentrate more on concepts of development than the actual tech. Most concepts are equal everywhere but just have a different syntactical coat over them.
There was a confusion of Angular 1 and Angular 2+ in latest stackoverflow survey so I wouldn't look at that at all. Angular1 was a slow ugly piece of \*\*\*\*, however Angular 2+ is quite nice. As for job offerings for .net developers in Poland, it is almost same also in Czech and Slovakia... one of my teams is now working on new project based on .net core and their chosen front-end was still Angular 2 (well right now 7 actually) even though they could have chosen anything... Typescript and how angular works in general is simply natural for .net developer. Same dependency injection, same interfaces, etc. The only problem I have with Angular is compilation time (I don't really care about size as with increasing speed of connection this is getting irrelevant + this can be solved by better lazy loading and AOT compilation). Also building in CI in docker is just slow (due to 'NPM install' though so this is Javascripts issue not Angulars in particular). What also my developers liked about Angular was the sort of all in one (it is like .net, you have everything you need and don't need to search for good maintained package for routing, for auth, for http, etc.) As for Vue, yeah, I love Vue for small and personal projects, Vue is just awesome, however for big project where several people/teams work on 1 code base, angular seems to be better due to more strict rules by default. Honestly if you learn Angular, any other framework will be a breeze for you, Vue is amazingly simple to learn and use. If you are doing it for work possibilities and not just for fun/side projects, definitely invest your time in Angular as that is what .net based companies are choosing. And if you want a great course for Angular --- this course is probably the best: [https://www.udemy.com/the-complete-guide-to-angular-2](https://www.udemy.com/the-complete-guide-to-angular-2) by Maximilian Schwarzmuller (any course from him is great)
ah yes i completely agree, i mentioned that one thing i didnt like in the pattern in the article was returning the status code in the body as its redundant. the other comment, i think is just a bit tad over engineered. i just want something as simple as possible, it doesnt have to conform perfectly to api standards like the google api or microsoft standards.
People definitely overestimate their need for a full on JS based front end solution. In a large number of cases just using things like you’ve mentioned are more than enough. The nice thing about something like Vue or react is that they don’t have to be a complete replacement to that. You can just use them in places where it’s necessary.
Yep...
Avalonia even lets you to use CoreRT. Which produces a single small native executable.
And if you want to play with a lot of already written samples, check this repo https://github.com/dodyg/practical-aspnetcore
Yep, there's a GPIO library for .net core you could hook up any sensor you want to the rpi.
I see, to be honest I'm planning on focusing on backend and be really good at that (the speciality or domain focus will come later, I hope so atleast) but as you said I just want to know my way around the front and be familiar with some frontend technologies.
I've got my NET core app running on Raspian, Pi 3
Hello \`ArrayList\`
I use my pi as a webserver but its not really an IoT project. I've thought about making some things (pun intended) but I havent found the one project really convincing.
VSIX files are just ZIP files containing a manifest and various files used by the extension. Visual Studio IDE extensions most likely contain one or more managed DLLs (VS Code extensions use JS), but they can also contain non-executable stuff like project templates. There is no universal way to just make it work, it'll depend on the specific extension. An extension DLL is going to reference VS-specific DLLs to register menu commands, interact with the IDE etc. The types and methods required for the core functionality may or may not be exposed publicly, and may or may not be contained in separated managed or native DLLs that you can easily reference elsewhere. If the functionality you're looking for is written in managed code, it needs a .NET runtime and referencing it from non-.NET languages is possible but more complicated.
People use Node in production :)
And no problems making HTTP calls out to a back-end server from the pi? It has wifi I'm sure. And it's all just normal .net core, not some weird subset, correct?
Wow, thanks a lot for so good and complete answer. \&gt;calling it from non-.NET languages is possible but more complicated. &amp;#x200B; could you tell a bit more about it or just paste some useful links, from where i can find more info about that topic? &amp;#x200B; another option i also see (correct me please if i'm wrong and it's not possible to do) is to write some kind of wrapper with .net language using this extension, which will expose all the methods and build it as a DLL to use anywhere else.
If you want a cheaper option than a Raspberry Pi, you could, depending on the sensor, use an ESP32. Then you'd use C++ or Python (IIRC).
[Meadow](https://www.wildernesslabs.co/meadow) should be coming out soon too for another option.
Care to link me this library? The one I found (System.Devices.Gpio) seemed to be unfinished and I couldn't even get it built, so I gave up and used Python and WiringPi.
&gt; Python *Gasp*
https://github.com/dotnet/iot here’s the github. It might be a .net core 3.0 release but everything is there.
Pure .net core, it should be very straightforward. Here’s a blog post from Scott H: https://www.hanselman.com/blog/InstallingTheNETCore2xSDKOnARaspberryPiAndBlinkingAnLEDWithSystemDeviceGpio.aspx
I cannot believe (well I can actually) someone downvoted this sincere question from a 14 year old. What a sad person.
I don’t really care about karma anymore tbf I just want answers and I got them so thanks to everyone who helped! And you too😉
Have an upvote :-)
Have a silver award? Lol it’s all I hit but thanks :)
Side note: If you want to be full stack, learn about databases and SQL.
You want two ASP projects to communicate? That's a dream come true! Each can expose endpoints for posting updates to each other. SignalR is also amazing.
There's no reason your application should run in a sdk-based Docker image. Your Dockerfile is very poorly.
I am only exposing the ports in docker-compose which is only for dev.
I used the SDK image since I am doing the build/publish in Docker. Which image would you recommend instead?
1. Use Multi-stage builds to separate building and running into different images. 2. Copy **only** the sln/csproj files and run `dotnet restore` first. 3. **Then** copy the source code and build your application. 4. Make sure shit like `bin/` and `obj/` and `Dockerfile` is part of your `.dockeriginore`file.
I've used a Raspberry Pi for this with .NET Core. It had sensors and was connected to Azure IoT Hub which then output to Azure Stream Analytics. Making an application to read from a sensor and calling a HTTP endpoint is really not much different from other day to day C#.
It's not exactly "unfinished" but in active development. I've used it for real world stuff.
Ok, yeah those a great points. I have updated the article.
there are some pretty good resources for how to marry aspnet core and spas. check out Microsoft.AspNetCore.SpaServices and Microsoft.AspNetCore.SpaServices.Extensions for some helper extensions. You'll see things like UseSpa on IApplicationBuilder and what not. There should be a react based template with the latests sdks as well.
If kestrel is production ready and can be exposed to the public , why is the suggestion to use nginx to reverse proxy ?
Here is the reasoning from the documentation: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-2.2
Thanks !
To me it seems like the main argument is load balancing. Sounds like Kestrel doesn't support that.
I'll take a look, thank you.
Ugh my mods tab doesn't load either unless I keep jamming the refresh in top left until it finally does. Why doesn't twitch fix this??
yeah that and angularjs to angular 2 was a bumpy transition that a lot of developers hold against Angular. Those years are behind angular now.
No need to know anything about .NET for a .NET dev job. Just memorize everything about O(n) and complexity. And how to traverse a b-tree. And traveling salesman shit. And how to implement data structures that already exist in high level programming languages. And of course about heap vs stack.
Thanks for sharing. A good write up.
In the latest update they bumped the timeout to 30 seconds (3 x 10) instead of 9 (3 x 3). Given that the agent takes about 20 seconds to load for me and that my computer is quite fast, I can see how it can be insufficient for many people. I don't understand why they insist on using a small timeout instead of bumping it to a much higher value.
Esp32 and micropython are a good combination.. I know you can use c# nanoframework but it looks to have less backing/maturity than micropython by a mile. I hope the meadow framework will get ported to esp32 when meadow is complete, having .Net standard 2.0 would be awesome but not for $40 on their board.. The going rate for esp32 is $4-$12
I love the pi for UWP Windows 10 IoT. The GPIO on the PI 2/3 has a built in resister meaning you need almost no bread boarding. I would highly suggest a breakout board as well. I can't find the exact one I use but its like this https://www.amazon.com/Alchemy-Power-Pi-EzConnect-Raspberry-connector/dp/B01FE9EQ88?ref_=fsclp_pl_dp_4
Here is what you need to do. In your Startup.cs add [this](https://github.com/NotMyself/webpack-dev-middleware-bug/blob/6c61d7666ef90a66a86cfe03e750048c62c4429a/src/server/Startup.cs#L34-L37) to ConfigureServices and [this](https://github.com/NotMyself/webpack-dev-middleware-bug/blob/master/src/server/Startup.cs#L70-L82) to Configure. You may not need the /api filter. You will also need to add a [fallback controller](https://github.com/NotMyself/webpack-dev-middleware-bug/blob/master/src/server/Controllers/ValuesController.cs). Note that configuring your app this way means that anything that 404s serverside will serve up your Spa app. So you will need to handle true 404s client side.
Have you got an example of this? I'm looking to exactly this
That was a nice process description. I will share it with some of my fresher collegues (and in passning mention it to the older ones).
Yeap. I must agree with you. It will costly if you use big database size, if only host simple websites, then Azure is good option. I recommend [asphostportal](https://asphostportal.com) for shared hosting. For deployment, I really like Azure, but it is costly.
What I usually tell people is to just skip the Basic step and go straight to C#. It seems like a bad idea to learn the language without the syntax, just to go unlearn the bad habits later when you transition to the preferred language. These days both do basically the exact same job, both are based on the .NET framework, and both are very similar. I started with C++ at 13 so you'll have no problem at all with C#.
I haven't specifically had any experience with this but a quick google for "dotnet core dynamic assembly loading" returned this link which might be helpful https://natemcmaster.com/blog/2018/07/25/netcore-plugins/
Thanks this (sort of) works, hovewer i have a problem, When i use that fallback controller code, my app wont load anything from the database, it just cant for some reason.
Thanks for this, I jumped into his repo and the samples directory had the MVC demo too, however he essentially does the same thing, loads up dlls on startup using ApplicationParts (which is what I'm doing currently, which requires a manual server restart), I want a way to load plugins without a server restart or restarting kestrel pragmatically. I'm leaning towards a small program to watch the plugins folder and restart kestrel on file changes, does that sound like a good idea or do you have any better recommendations? Again, thanks!
Sorry didn't see that. Maybe this so then? [https://stackoverflow.com/questions/46156649/asp-net-core-register-controller-at-runtime](https://stackoverflow.com/questions/46156649/asp-net-core-register-controller-at-runtime)
Nothing prevents you from doing that watching from within your server. And .NET 3.0 will bring a feature that allows you to unload assemblies, so no restart is required.
How are you executing the scans? Not sure if this is an option for you but we integrated SonarQube scans into our CI/CD pipeline so we just rely on Azure DevOps to send an email once it finishes.
That is stage 2 for me, while I try to integrate with Jenkins. It may just be that I get the pipeline to email once a scan has been run. I know there’s a Jenkins plug-in for SW for sure Still, the stubborn side of me wants to understand why the hell this can’t be done straight from SonarQube. Seems like quite the oversight imho!
Yeah that might be the only way then. I don't think there is a built in way to send emails for a completed analysis. Then again though would an email for a failed quality gate be sufficient? It seems like if it doesn't fail the quality gate then you would consider your app as passing right?
Apparently they want notifications every time an analysis is run My thinking now is that I will have senior devs subscribe for quality hate changes and *every dev* to get an email via Jenkins That could be best of both worlds for what I’m tasked with
Thanks, I just implemented it and it works great, I didn't have to restart the server, it loaded and unloaded seamlessly.
Cool, I'll look into it, but for now ApplicationParts did the trick for me, I just injected the ApplicationPartManager to my PluginsController and went forward from there.
You need to create an exception route for your api. That is what [this](https://github.com/NotMyself/webpack-dev-middleware-bug/blob/master/src/server/Startup.cs#L54-L57) line and [this](https://github.com/NotMyself/webpack-dev-middleware-bug/blob/master/src/server/Startup.cs#L70) line are about. This means all of your controllers will be a sub-route of the app. In my example they are behind /api. So you will need to change your client to call there as well.
That makes sense. Yeah I think having both would be really nice. We started with emails but are now in the process of just using gates in Azure DevOps to hit the SonarQube API to check if the App has failed it's Quality Gate. If its passed we let our release go on to the next phase. We realized most of our Devs just ignore emails but do pay attention to where their release is at in the pipeline 😂
If you're sending HTTP calls, you can do it from all kinds of devices and it really isn't dependent on the platform that will be receiving those calls. Raspberry PI is fine. It's probably more powerful than what you need but you can definitely use it for that, and it's nice to own a Pi. Try asking a subreddit like /r/arduino, though, or some other device-related sub. There are a million different devices that can send HTTP calls.
&gt; but I've heard a lot of negative opinions about it and in the recent stackoverflow survey it was one of the most dreaded technology, "There are only two kinds of languages: the ones people complain about and the ones nobody uses." \- Bjarne Stroustrup
Welcome to my world. &amp;#x200B; I am also rewriting a 10 year old application. Ours is in vb and webforms. All I can say I feel your pain. Good luck!
I do maintenance on a project where we have lots of old stored procedures. They aren't really documented and are fairly complex, so even though nobody really wants to keep them, replacing them would take time and introduce risk.
I would not recommend this, since you will end up serving index.html for requests that are expecting something totally different or an error page but not a real web page. Obvious ones are /favicon.ico and /robots.txt. Plus if you decide to add additional pages or even simple things like images, CSS, JavaScript, etc, it won't be compatible with your idea. What I would recommend trying instead is changing your 404 error page to include a redirect header to the index.html page. This is less likely to confuse requests for hard coded special urls (though I am not 100% sure).
Yeah i added that line but now i get a No file provider has been configured to process the supplied file. net core static, this shit is 1000% more painful than Express
This seems to be more of a database question than dotnet. Someone here may be able to help you, but there's probably not enough info here to guess at a solution. &gt; try to update database again, it will always show me this error In particular, it's not clear what you mean by "update database". The problem is likely to be inside the process you're using to update the database. So try changing that.
Oh sr, what i meant was run the Update-database command in the Nu-package
That is pretty nice. Does it work when using modals (e.g. bootstrap / sweetalert2)? I had to do a lot of workarounds to get client side and server side validation messages working correctly when using modals as Jquery validation does not work on dynamically added forms (unless you manually trigger it).
That probably means you have a code-first migration that's attempting to create a foreign key that creates a cycle. You'll have to modify that migration to fix the problem.
Particle is another good one. Program with Arduino-like libraries and do web stuff.
Oh it works, thank you very much!!
This is definitely one of the uses of Roslyn. You can read more about it on the [docs website](https://docs.microsoft.com/en-us/dotnet/csharp/roslyn-sdk/compiler-api-model) for the API structure, or read more on what looks like a [blog](https://josephwoodward.co.uk/2016/12/in-memory-c-sharp-compilation-using-roslyn) about what you're doing. TBH, I didn't read the blog in it's entirety, it's just a resource I came across while trying to find helpful links.
[roslyn sample scripts](https://github.com/dotnet/roslyn/wiki/Scripting-API-Samples) might be helpful.
I'm using similar structure to what you presented. 'Data' which is a dynamic model depending on the request, 'Errors' which is a list of objects with 'Code' and 'Message' fields and additionaly some other properties related to pagination when they're needed(Results\[on this page\], TotalResults, Page, PageSize, Query). I add error whenever I feel that HTTP code can't convey the message I want it to, like if there's two possible cases for returning 403 Forbidden, I'd like to clarify to the User, why I've forbidden that. So that in most cases client can just switch on Http codes, but when he needs exact cause, he'll look for the error codes(f.e. for property names when 400 was returned from a form submission).
Yes
We did this before Roslyn with Reflection.Emit and Linq.Expressions. https://docs.microsoft.com/en-us/dotnet/api/system.reflection.emit.typebuilder?view=netframework-4.8
Fuck off with the ads.
It might be a challenge to provide enough level of security and guarantee server stability if code comes from 'untrusted' source (say, entered by end-users of the application). Very often full power of C# is excessive, and it is enough to provide some kind of expressions with explicitly controlled operations/functions and parse them to expression-trees; no need to write a parser by yourself - you can use existing open-source libs this.
Your post has been removed. Self promotion posts are not allowed.
+1 for Dapper There are many reasons not to use Dapper, but they all basically come down to "I'm using a more advanced ORM" If you aren't using something like Entity Framework then Dapper is by far the best option I've ever found for wrapping SP calls and simple SQL statements: lightweight, simple, and gets rid of all the messy looking boilerplate. It's not always the right solution, but it's a great solution where it fits - which is, as I said before, anywhere you aren't using Entity Framework or similar.
Yes, it does
Before Roslyn came out, I had success with this: https://github.com/oleg-shilo/cs-script That was for internal use only though - be careful!
I am switching to .Net Core from MVC 5, however, I share the sentiment on SPA applications. It is a lot easier to make a super reliable application with more screens using MVC. &amp;#x200B; I implemented a bunch of JavaScript/Ajax on my companies intrasite and it doesn't seem to make any difference to our employees. I would rather make something more functional, tested and reliable.
Cool. Especially for a corporate intranet, a SPA is probably not worth the complexity.
http://rbwhitaker.wikidot.com/c-sharp-tutorials This guy also has a book that is excellent in my opinion. I learned from his book and ended up becoming a professional developer. There is something I find particularly engaging about the way he teaches stuff. http://starboundsoftware.com/books/c-sharp/
Yay! Finally made the changes to the post.
I'll take a look tomorrow. Been working with vueJs + dotnet core for a bit now, so I'm pretty interested.
Cheers, yeah just keep in mind this package is specifically targeting web components which are slightly different from a typical Vue instance
When you use the term web components, do you mean proper [Web Components](https://en.wikipedia.org/wiki/Web_Components) or just Vue components?
I also work with episerver and I've wanted to do a vue integration for a long time. This is much appreciated!
Yes, proper web components, created in Vue and built with \`--target wc\` or \`--target wc-async\` in the vue-cli build command, see [https://cli.vuejs.org/guide/build-targets.html#web-component](https://cli.vuejs.org/guide/build-targets.html#web-component)
No problem! With this integration library EPiServer blocks and vue.js web components work together extremely well. Let me know if you have issues, but the example should be pretty straight forward.
Well I'll be darned. I'm still kinda new to Vue, and I had no idea that package existed.
why would you use docker to do dev directly from visual studio with a vm? i presume the vms are for testing for when you do deployments in prod? I'm just getting started with docker too, but essnetially all you need to do is run the docker containers on your local machine for dev, no need to use another vm. once you do that you can debug directly with your local docker containers. youre lucky youre on a mac as docker in windows, i had a few teething problems with docker not working when using linux containers. once youre done with dev with your local docker containers then you can publish them to your vms, but really i dont see the point. why not just publish them to your dev / uat environment?
The issue is - as far as I can tell - because my machine is a Mac, and I do my .net development on a windows vm running on a Mac, that they way docker desktop works by default it actually spins up another nested container using the windows hypervisor (a process that is mostly hidden from you) - and I have been unable to get the nested virtualization working despite spending days on it - so I trying to figure out if I can 'point' visual studio to another vm running docker, and still take advantage of the integrate debugging in VS
ESP32 does support [nanoFramework](https://nanoframework.net/). So C# on ES32 is still an option.
The submission runner will need to run on the server which isn't really addressed by any of the front end technology you mentioned. It's also probably the hardest part. You may be in a bit over your head if you have only been exposed to basic front end web technologies. The general idea is that the browser will send a request about a particular submission and the server will run it with the specified test cases. The total application is beyond the scope that could be explained in a reddit comment even if I knew how the rest of it worked.
If you can make it an xbap then that's extremely easy to do. [https://stackoverflow.com/questions/927688/wpf-switching-from-wpf-app-to-xbap](https://stackoverflow.com/questions/927688/wpf-switching-from-wpf-app-to-xbap) Otherwise conversion to silverlight is the next easiest option. Beyond that nothing will be easy and you will have to recreate the front end from scratch.
U could use vs code on the mac but u probably know that already. I can't remember but I think u can get visual studio on macs now, have a look. Coding in a vm would suck
Exactly what we were looking for. Thanks!
Not a concern here. I will have absolute control over the resulting code that will be compiled.
Oh, you wrote a data generator cool... wait interface what? why? dependency injection, services, controllers? Generics, okay he went there.... Oh no you didnt. &amp;#x200B; Lets start again: DataGenerator.FillWithData(item); // Fills the object with test data, the text is even Lorem Ipsum! &amp;#x200B; Crap, I need to simply my data generator.
 log4net is still one of the most commonly used logging frameworks for .NET. This post is a collection of descriptions, examples and best practices I would have liked to have had when starting out with log4net 10 years ago.
You are confusing frameworks (.NetFramework/.NetCore/NodeJs...) with programing languages (C#/Java/javascript...) &amp;#x200B; With .NetCore running on linux, I can practice with free-for-open-source tools like CircleCI... It's a small little thing that I really enjoy.
Fair points. I have been developing in C# since 2003, but acknowledge that Java is the king. Nobody in silicon valley will use c# because why. I don't get the blazer hype since web assembly is for performance critical code and not needed for most apps. However, core is better than .net framework in a lot of ways. The project structure, cli stuff. Libraries are starting to care more about core than .net framework. The one thing that I think is legitimately huge is that I can run core APIs on linux. Windows and IIS sucks so I am stoked that I can deploy to linux and run behind nginx or Apache. Again, nobody is going to leave java because of this but for c# people this is pretty cool.
log4net is really not being maintained and hasn't been for years no? Any reason to use this instead of NLog?
This tutorial is meant for people already using log4net or for developers moving to a project using log4net. I don't see any reason what so ever choosing log4net when starting new projects. Both NLog and Serilog are superior frameworks, which I've written about here [https://blog.elmah.io/nlog-vs-log4net/](https://blog.elmah.io/nlog-vs-log4net/) and here [https://blog.elmah.io/serilog-vs-log4net/](https://blog.elmah.io/serilog-vs-log4net/).
Uh, I'd gladly use .NET over rails, nodejs, or django and flask. Rails is a fucking monster, nodejs is an abomination, and I don't particularly care for Python.
That's cool that you wrote that post to help people but nothing in there mentions that log4net isn't maintained and shouldn't be used if possible. For security reasons software that is using log4net should migrate to a logging framework that is maintained. If that isn't practical people should consciously be making the trade off, money vs security.
That's a good point. And I already wrote a comment in the post after your original reply. While it is clear to both you and me that log4net shouldn't be used for new projects, it isn't for new developers. So good thing you pointed that out. Thank you for the input.
C# / .NET / Visual Studio is great for businesses for a ton of reasons (faster/easier dev times and good speed for data processing are the big ones, imo), so making it cross-platform is huge. The ability to be put on anything is the only reason Java's had such a hold over that market. There won't be a reason to use Java outside of old codebases and Android development -- not to be understated -- once they work the kinks out. But a huge factor in the hype is just new-age Microsoft. They're easily one of the cooler tech companies now, which is a big change from being "the man." Anymore that title is much more fitting for Oracle and Apple. Hell, you might even think they're more chill than Google if you were on the receiving end of Gmaps' latest price changes (our bill is literally bumping up 10x in a couple months).
Deploying to Linux docker containers is in my opinion the main benefit.
WASM is not just for performance critical code. One of its main goals is to allow other languages than JS to be used in the browser in an efficient way.
Java is still used by more companies and by more top tech companies... it’s 20 years older than Core which has only been out for 3 years so of course Java is used more. I don’t understand the point you’re making with this statement.
Your post has been removed. Self promotion posts are not allowed.
I need dis
I'm in the same boat. Gets a bit dreary at times.
That’s just the way it is. At least half of web-dev is going to be repetitive and generally boring work.
&gt;Decent beginner to intermediate online C# course that isn't videos. &gt; &gt;I've been trying to learn C#/.net with a couple of Udemy's online C# courses, but because of crappy internet and restriction where I live/work, and the fact that I just tend to zone out while watching videos and not absorb much, it's really difficult for me to use a video based course. Does anyone know of an up-to-date online option for learning C# that isn't video based, but instead text based? Thank you friend. I just bought the book The C # Player's Guide. Maybe with his online lessons I will finally learn at least something. And although English is not my native language, it is written clearly not even for English speakers.
Fantastic!
Can't you just use the scaffolding that comes with visual studio? It sets up all the Razor pages you need based on a db context. [https://docs.microsoft.com/en-us/aspnet/core/tutorials/razor-pages/model?view=aspnetcore-2.2&amp;tabs=visual-studio](https://docs.microsoft.com/en-us/aspnet/core/tutorials/razor-pages/model?view=aspnetcore-2.2&amp;tabs=visual-studio)
That is indeed what I am currently using, however I need to add a lot of functionality that is the same on each entity. Which would then require me to create custom scaffoldings which I’m not sure if that is possible at all
If you can find a way to abstract the common stuff so it's not specific to the app you're building, maybe consider publishing an open sourced EF extension/plugin on your own GitHub and nuget account? That way you can easily reuse it on your projects, and build up your own portfolio.
Allows you to deploy c# code cross-platform. C# is a fantabulous awesome language. And it's fast.
A bit of googling, and you can supply a ton of args to the scaffolding tool: [https://www.learnrazorpages.com/miscellaneous/scaffolding](https://www.learnrazorpages.com/miscellaneous/scaffolding) and [https://gavilan.blog/2018/04/28/asp-net-core-2-doing-scaffolding-with-dotnet-cli-aspnet-codegenerator/](https://gavilan.blog/2018/04/28/asp-net-core-2-doing-scaffolding-with-dotnet-cli-aspnet-codegenerator/) The latter sets up an automation script. And even though it's undocumented, you can actually add your own scaffolding templates: [https://stackoverflow.com/questions/38382954/custom-scaffold-templates-in-asp-net-core](https://stackoverflow.com/questions/38382954/custom-scaffold-templates-in-asp-net-core)
Very interesting! Thanks man!
I'll probably do that in combination with dinkelhopper's suggestion on using custom scaffolding templates :)
Please see [AdaptiveClient](https://www.nuget.org/packages/AdaptiveClient/) and its related library for EntityFramework. While it does not automate anything per se it it provides a highly reusable pattern for constructing services. It also wraps much of the tedium related to registering classes etc in some very easy to use method calls. The pattern itself reduces the service layer problem to approximately this: Create a class-per-entity for CRUD ops (or whatever you need. Class per entity seems pretty common.). Create a service manifest. Register services with Autofac. Inject client where needed: controller, viewmodel, etc. The [EF utilities](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework) abstract away much of the tedium of creating / dropping the database. [Web and WPF demo here.](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon)
I've never met a dotnet developer that hated writing dotnet. In fact anyone using it typically are enthusiastic about c# and the framework. The same cannot be said about Java or php devs. These developers always seem apprehensive with the tools they have to work with. There is a big dotnet industry. The addition of core makes it even larger. The hype is because dotnet has all the features and more then we've been asking for for years.
I for one have reduced my hosting costs and increased my performance 10-fold since moving to .NET Core. Newsflash, .NET on a shared hosting with IIS is incomparably more expensive and less performant than .NET Core on a Linux VM. So I am *extremely* grateful for .NET Core. &gt;Java is still used by MORE companies, and not only that, more TOP Tech Companies. This is fact. There's no changing this fact. On the contrary, this is constantly changing. To say that .NET Core can't ever replace JVM as the go-to cross-platform solution is ridiculous. .NET is far more popular in the back-end web space and with .NET Core it will continue to expand its reach into middle and small businesses as well. &gt;No one is going to suddenly say let's use C# for any willy nilly startup The truth is again the complete opposite of what you're saying here. .NET was not viable for startups because of the hosting prices. .NET Core on the other hand is as cheap to host as any other back-end stack and performs a lot better than the technologies you mentioned whilst providing better tooling as well. You're also completely disregarding new developers that are coming into the industry which will now have more reasons to start with .NET (Core) rather than Node/Express or Django/Flash. .NET Core is a long term plan and it's already gained a lot of marker share. It used to be that .NET was only viable for large enterprises that could afford the costs but now that it's pretty much free smaller businesses and startups will opt for it as well. Considering Oracle's power over Java and their licensing costs it has no chance of competing with .NET Core in this domain.
I'm confused as to why you associate C# with old school. Old school would be something like ASP web forms. We've had so many iterations since then. .NET is constantly evolving and reinventing itself to stay relevant and modern. Blazor is simply the evolution of .NET into the SPA and Component-based architecture territory. Think of it like javascript. Javascript itself has been around forever, but it has been reinvented and modernised through frameworks like Angular/React/Vue and Typescript (ironically a MS product). That's exactly what Microsoft have been doing continuously with C#/ASP for many years.
Look, I agree with all the points above but if I was a Front-End Developer or Full-Stack Developer starting out interested in building modern web apps and I didn't specifically want to work for any type of company most likely I will start with Javascript and Node. End of story. Most of the people coming from Bay Area or bootcamps will never touch.NET.
You keep repeating the same points. I already know about the benefits of.NET Core. The point I'm trying to make though is that for most people not already using C#, they won't likely care and will stick to Node. If my end goal was to work for a Bay Area tech company or startup, then learning C# specifically will kinda be useless.
I'm not associating C# with old school. I'm just trying to ask why anyone not already using C# would go out of their way to learn it if they weren't required to use it for their job.
And there are also people, given the same situation, that would avoid JavaScript and Node like the plague. It’s just different strokes for different folks.
Your question was "why is there so much hype for .NET Core?" and I think my comment answers it. Throughout your post your sentiment generally revolves around "why so much hype when other people aren't going to use it?" and I explained that other people *will* use it and they will use it a lot more than they used .NET. Another thing to say is why should we not be hyped about a technology that will greatly improve our current and future professional lives, regardless of how many other devs use it. &gt;The point I'm trying to make though is that for most people not already using C#, they won't likely care and will stick to Node. And front-end developers will continue to use JavaScript and data-oriented devs will continue to use Python. .NET Core is not hyped because it's a solution to all of the world's problems but because it's a solution to many of *our* problems. You're thinking about this in a very short term manner. .NET Core proves Microsoft are willing to invest in .NET and make it the future for any software industry, from desktop, to mobile, to web, to gaming. I don't know many other technologies backed by large companies that provide this.
Learn Dapper, inheritance, and polymorphism. Combine all 3. You can write one repository that returns T, and only write your queries once.
Plenty of reasons. The .NET framework is very rich and refined which makes for faster and less painful development. C# is also similar enough to Java that the learning curve for anyone coming from Java would be very small. Plenty of reasons for hobbyists too. The language is very exciting. I'm having a ton of fun exploring the future of non-js browser code. C# is also great for game developers that prefer unity or other engines that interact with it. There are good performance reasons to choose a .NET Core based server for a website as well. In saying all that, if someone is already comfortable with say a full stack node and react website, there is little reason to leave and learn .NET. it is really just a matter of preference and flavour there. I personally prefer Blazor as I can share code between the client and server via a shared library, there's no need for 100 npm packages and webpack, and I like the richness of the .NET framework.
Being able to deploy your apps to any environment, and not requiring the .net framework to be installed on that machine to run said deployment, is pretty awesome.
Depends. Was previous design MVVM? If so, basically, you can keep models, data access layer and most of the services. Only thing to figure out will be FE. You can use MVC and sprinkle it with Razor components where you can use C# syntax. Or go full FE framework like Angular.
Do you think all Web Developers HAVE to learn C#/.NET Core in the same way that all Web Developers HAVE to learn HTML5, CSS3, and JavaScript?
https://github.com/App-vNext/Polly/wiki/Cache
Absolutely not. That'd be like saying web developers HAVE to learn php, nodejs and every other server-side language. It's just about choosing which one is right for the team and the project.
Makes sense. In your opinion, if I had to choose one enterprise web backend to learn in 2019, what kind of scenarios would make me choose Java Spring Boot vs. .NET Core in 2019?
This is a very well-written and detailed post, thanks a ton for sharing it here. If only all blog posts could be written like this one!
I can't really speak for Java Spring Boot as I've no experience with it. I think the issue with a comparison here is going to fall to the fact that both solve the same problem with similar advantages. I could cite the pros of .NET Core such as speed, framework richness, cross-platform, etc. But I imagine Java Spring Boot probably has the same things going for it. If someone was already well versed in Java Spring Boot development and was happy with how it was performing, I'd really have no reason for them to drop it to learn .NET.
My situation is this: I learned Java in college, as do most students. I use Java as my main OOP language. Should I specifically go out of my way to learn C# and .NET Core in 2019 for building hobby projects OR should I stick with Java and just go with Spring Boot. I'm just curious as to how most people not already in C# get started with .NET Core if they don't need to learn it for a job. Like there has to be some incentive or marketing that gets a brand new developer to learn C# / .NET Core for solely hobby projects.
imo c# is "cool" compared to java
I started with C++ prior to formal education, then did Java and C# in university, and stuck with C# because the language simply felt much more refined and efficient to develop in. For me, I was sold on it because I felt like I was writing cleaner and better code in less time. The framework just worked for me (things like date/time in java and anything asynchronous just felt like a battle of semantics to me). The incentive for me was simply the language and framework itself. I also love how much easier and faster creating a new web application is with entity framework (for handling a code-first database approach) and identity (basically pre-built user accounts, authentication, tokens, etc). I also love linq (a great part of the .NET framework for handling collections of data, such as sorting, selecting, etc) which can also be used with entity framework to automatically convert back to SQL for data access. Strongly typed data entities are a godsend. As you already know Java, I would highly recommend learning (or at least giving it a try) as C# is very similar to Java in many ways. If the potential benefits of the language and the framework aren't enough to sway you then there's absolutely no harm in sticking to Spring Boot. As for incentive/marketing from MS to get brand new developers over to .NET, there aren't any that I'm aware of. The closest to an incentive is that Visual Studio Community and VSCode are free, but that's no different to most other development tools.
Aps.net core api, windows service or scheduled task for looped checks and mvc for data representation? 3 independent modules. Easy to maintain. With c#8 async streaming with async ienumerables and blazor around the corner, if you have time - I would wait a bit.
When you say "continually monitor" I'm guessing you are essentially meaning "polling" e.g. repeatedly reaching out to check every so often also known as a "pull" model. This is also synchronous: it happens at a set time and in between those times, nothing can happen (as you're finding to your detriment). What you really want, it sounds like to me, is more of an asynchronous "push" model, where the "workers" are "pushed" some data when there is work for them to do. I'd suggest looking into the [pub/sub pattern](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) personally, but there are a number of ways to accomplish this overall design pattern.
Probably the easiest way to solve this to separate the `File` type into two, both deriving from a common base class. So you might have a BaseFile class: public abstract class BaseFile { // insert the "irrelevant properties" from your File class here } And then derive a `SpecificFile` and a `File` class from it: [Owned] // this tells EF to make a one-to-one relationship between this class and its owner public class SpecificFile : BaseFile { // no primary key required, since this is an 'owned' class } and public class File : BaseFile { [Key] public int FileId {get; set;} [ForeignKey("ArchiveId")] public Archive {get; set;} public int ArchiveId {get; set;} } Then you can include these files directly in your `Archive` class, like this: public class Archive { public virtual SpecificFile SpecificFile {get; set;} public virtual ICollection&lt;File&gt; OtherFiles {get; set;} }
I haven't tried the later versions, but I believe it's possible to change the scaffolding templates to fit your shop's conventions. In general, MVC seems to kick D.R.Y. in the pants. Use of scaffolding is automation of replication (duplication), not factoring. A bigger-scaled problem is that we should be using something like data dictionaries (tables describing tables) and set theory to customize and tune field-sets per local usage (specific screens) rather than replicate. But OOP appears ill-suited to this approach so far. I've been experimenting with such on and off, but still have rough spots to work out.
Yeah, it would be great to have something that I could have running on the server that I could somehow have different processes fire to say "hey, there should be work for you to do" and then it does it. I've been mostly working in my WPF/WinForms, Entity Framework, SQL Server stack but I'm trying to branch out. I'll take a look into this pattern and what those tools might be able to do for me.
Indeed, and nobody with power seem to want to solve this, perhaps believing it unsolvable. My suggestion is multi-part: First, the industry should come up with a CRUD/GUI-friendly browser standard to replace or supplement HTML browsers. This would include state-handling and the option of letting the server compute layout coordinates so that we have more choice of UI layout/flow engines. Second, better use of data-dictionaries, which I describe in a nearby comment.
&gt; MVC seems to kick D.R.Y. in the pants. I don't think that is necessarily true. You only run into trouble if you start putting business logic in your controllers. That is a major bad thing to do. A better approach that is commonly used is put all your business logic in your service layer and inject your services into your controllers.
Is there a sample you could perhaps link to? A common problem is that "business logic" and UI issues are often intertwined such that putting a solid wall between them makes for more cross-interface-wiring busywork. "Separation of Concerns" is sometimes a wrong approach because concerns interweave in practice.
Thanks!
Hi, a few thoughts. First, no one ever made a compelling point with their only evidence being "End of Story" or "Period" or "For the people in the back" and that's it. Node has an awesome ecosystem, and Javascript, whether we like it or not is the language of the web right now, but to frame it as a default platform is not accurate. It is extremely popular, new development is not a monolith of Node. Second, I know several new shops leaning on .Net Core. It is very fast, very flexible, and the debugging in .Net is second to none; anything a seasoned dev will tell you is more important than the other 2 factors combined. Some of these new shops coming out from the west coast. Third, it has been a long time since a new company I have seen has stuck with just one language/platform. Many new places are hybrids focusing on the strengths of different platforms. Using one is dominant, but python and node seem to be popular things sprinkled into Java and .Net ecosystems.
Yes sometimes you have to repeat business logic in client side code. Its ugly and I hate doing it but that's the way it is. Fortunately it's rare. On the server side it is entirely possible and highly desirable to encapsulate your business logic in a reusable service layer. Please see the AdaptiveClient link I posted for working examples. I know concerns interweave in practice - this is why you encapsulate code and inject dependencies.
I have little experience here so pardon my ignorance. Would something like Azure Logic Apps be useful in implementing the pub/sub pattern?
Based on having started with both stacks fairly recently: .NET aims to stay out of your way and java doesn't. For one, because Spring docs suck. They use a lot of non-standard terminology, don't properly cross-link, etc. What's more, the Spring framework lags behind current technology, even in Java terms. Try getting it up and running with JUnit 5 instead of 4, for instance. The Java philosophy seems to be "life is a hassle, and so should our tools be".
Well, no. To all of that. CRUD is repetitive simply because it’s back and forth, and there’s no way to change this without throwing security to the wind. If security wasn’t an issue, you could do sql calls right from JS and that would be that. &gt; a CRUD/GUI-friendly browser standard That already exists. HTML is easy to make GUI’s with, and isn’t somehow unfriendly to CRUD operations.
I have no experience with Logic Apps myself but reading about them quickly leads me to believe that yes, you’re absolutely correct, they would indeed be very useful (and seem somewhat model after, in fact) implementing a pub/sub pattern. Great find, thanks a lot for introducing me to Logic Apps, excited to explore them more on my own!
AdaptiveClient link I posted for working examples. The examples seem about managing database connections rather than in simplifying business logic and form field management issues. To start simple, let's assume we have a single database server. How could it help build say a college class and grade tracking system? (a common sample/test/scenario domain).
Awesome, happy to hear^\(read?\) that! I’m still new to Azure and found out about Logic Apps literally today haha.
I don't believe security is really the bottleneck to better factoring. Some info will indeed have to be "echoed" to the client, but the developer shouldn't normally have to manage this echoing. For example, if the LastName field is limited to 30 characters, the HTML will need to "know" that fact in addition to server-side checking of size, but the developer should only have to encode that limit in one place. (POC "data annotations" in MVC kind of do this already, but it's clunky and awkward.) &gt;HTML is easy to make GUI’s with Sorry, I have to disagree. Among other things, it lacks native combo-boxes, dialog boxes, tabs, drop-down menus, tool-bars, data tables, collapsable trees, MDI handling, and has crappy date-pickers and multi-select boxes. With enough Javascript libraries, these can be emulated, but often break 2 years down the road as browsers change. One get "good enough" CRUD ui's with less JavaScript, but it's a lot of effort for basic stuff that wastes screen real-estate and requires a lot of back and forth screen-jumping by the user.
Connecting to a PI from a remote device using bluetooth, and then controlling the pi via headless interaction(cmd, or application). Was a great learning process for me.
Security definitely is. Like I said, if you don’t care about security, 99% of server work would be replaced by SQL queries from JS. But organizations like OWASP do state that independent validation does need to be done on all 3 layers of a web service, and there’s no way around that. An annotation is 1 layer, and if there’s a bug or it fails, so does your database. Lacking a few native items is irrelevant. If you can build with it, it’s still a good tool.
&gt;independent validation does need to be done on all 3 layers of a web service I don't dispute that. It's a matter of automating the implementation at all 3 rather than hand coding it at *each* layer/spot. Something that reads and processes a data-dictionary knows to put such validation at all 3 levels (or at least 2 if it has no control over the database schema itself).
That’s the point, you can’t rely on something doing that for you. Automating the implementation means you’re really only securing once. And not if your company is within or operates within the EU, no. Server side validation isn’t enough, ever.
Does Channels supersede this?
Sorry, I'm not following you. &gt; And not if your company is within or operates within the EU, no. Server side validation isn’t enough, ever. I'm not in the EU. Is there some law there that *all* validation rules must also be done client side? Let's use a concrete scenario. Let's say the biz rules of a form require 2 out of 3 per phone-number, email-address, and home-address. Are you saying EU law requires that the client (JavaScript) must also perform this check (in addition to its server-side incarnation)?
&gt; Microsoft has 2 solutions 2 different NuGet packages for caching. Both are great. eh.. "great" is stretching it a bit in my opinion. Though to be fair I haven't had the opportunity of using Microsoft.Extensions.Caching.Memory any significant amount, and System.Runtime.Caching.MemoryCache has managed to handle 100's of millions of sets and billions of gets a day for me without being too terribly slow. System.Runtime.Caching.MemoryCache is also all but impossible to extend, and has some significant long standing bugs with trimming. Almost anything will work fine when your system doesn't see a lot of traffic. When it does see a lot of traffic, you'll find you have to be a lot smarter about caching. I'm also curious about the `WaitToFinishMemoryCache` example - are you ever removing items from that `_locks` dictionary or is bounded only by the number of unique keys locked during the lifespan of the application? Though, the way it appears to be written, you're always `new`'ing up a new instance of `WaitToFinishMemoryCache` and so that non-static `_locks` won't actually prevent stampeding your origin source across threads unless somewhere else you're sharing the instance of `WaitToFinishMemoryCache`.
The devil's in the details. Merely using those is no guarantee by itself of getting practical [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself). All three can be misused or misapplied to make a mess; I've seen plenty in the field.
**Don't repeat yourself** Don't repeat yourself (DRY, or sometimes do not repeat yourself) is a principle of software development aimed at reducing repetition of software patterns, replacing it with abstractions or using data normalization to avoid redundancy. The DRY principle is stated as "Every piece of knowledge must have a single, unambiguous, authoritative representation within a system". The principle has been formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer. They apply it quite broadly to include "database schemas, test plans, the build system, even documentation". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I looked into this in the early days and ended up rolling my own assembly line structure. I'm sure it's improved since then, and the value is quite good for long-running jobs that can be broken up into concrete steps. By using an 8 block pipeline on an 8 core CPU, 1000 items went from taking 8000 seconds, to roughly 1000-ish seconds.
What hype? No one other than ms talks about .net. Certainly no startups.
Hi, In `WaitToFinishMemoryCache`, the number of locks is indeed bound by the number of unique keys. I guess you could rather easily add code to remove the lock items in a post-eviction callback if that becomes a problem. My intention was for it to be a shared instance since the locks dictionary and cache do need to be shared.
“Securing” is a verb. It means “to secure”. No, with GDPR, you will get fined into oblivion if there were avoidable data breaches. Using only server side validation isn’t enough. JS needs to perform a check, along with the server, and the database. 1 isn’t enough for security and safety purposes. Relying on just server side, or as you suggested, a way of generating all 3 layers, is not enough. Each implementation needs to be independent.
I'm not familiar with Channels but I did find this GitHub issue about Channels and Dataflow: https://github.com/dotnet/corefxlab/issues/1126
Yup, no startup ever has used .NET or .NET Core. /s
I think the main pro of F# is it's neat. The main pros of C# is that it's mature, well supported, and there will be plenty of C# devs around for years to come. &amp;#x200B; It sounds like going with F# would be a mistake. Better go with boring old C#.
F# is a very good choice for startups as an all-round language and the combination of concision, type-safety, closeness of objects to the domain, linear code, and availability of .Net libraries will help you to manage a large project with few people. However rewriting a codebase from scratch is rarely a good idea. Perhaps you can rewrite parts of it from C# to F# and migrate gradually? &gt;lively community Yes, mostly on Slack. When making specific tech choices though you need to be clear headed and not get carried away with other people's enthusiasm. &gt;better tooling F# tooling in Visual Studio has got more powerful over the last few years and has always been good. Just not at the level of C# which has very fancy and specialized tooling. (Non-Windows users complain more, but the tooling is obviously good enough for them not to want to switch.) &gt;we will be moving majority of data from SQLServer to DocumentDB You need extremely high throughput?
&gt;I will start with Javascript and Node. End of story I think your amateur-ism is showing pretty hard here. A node backend is the weakest out of your list above, which is funny because everyone here knows you had no idea about that. Why do you think kubernetes exists? FYI - a .net core backend is significantly faster and more powerful than a node api. This is essential if you ever wanted to build an app that had more than say 50k users. That's why people would choose it.
&gt; “Securing” is a verb. It means “to secure”. Yes, I know. But I still don't know what *you* originally meant. I'm trying to use concrete examples to avoid confusion created by speaking in generalities or jargon that may not be understood the same by all readers. &gt; if there were avoidable data breaches...1 isn’t enough for security and safety purposes. What specific breach scenario are you imagining, using the 2-of-3 scenario I laid out?
In my opinion the hype is from: 1. Many things are just overhyped in our industry. It's a general problem that creates waste by "encouraging" people who to throw away road-tested ideas or tools, or apply them where they shouldn't be. But that's an entire topic in itself. 2. MS's push for Core suggests existing frameworks like "traditional" MVC and Web-Forms may lose practical support from MS faster than anticipated, making people concerned about existing applications and practices. 3. People are happy to see a potential alternative to Java for Linux-based shops, being that the company which starts with "O" has proven heavy-handed with their Java lawyers.
&gt; C# is technically better than Java in some respects. I used to believe that C# and Java were largely similar. Sure, there's a little feature variation, but fundamentally, they both do the same things. Boy, was I wrong. As time went on, C# proved itself to be _vastly superior_. In my eyes, they aren't even close neighbors. I cannot use Java anymore because I lose way too many features and APIs. (It's fine if you disagree with this assessment. I'm simply framing .NET Core's significance. C# is a really good language.) The release of .NET Core was a major event. Proper cross-platform support and open source development means I do not have to be shy about using C# in just about everything. For years, I was on a team working with C# clients and Java servers. We never pushed using C# in the server because even we were uncomfortable running Windows servers (and Mono was no good at the time). Once .NET Core became a thing, we quickly dropped all Java pursuits and ported everything to .NET Core. We were able to share code across both systems. It was wonderful. I do personal projects in C#. If I ever attempt a startup, I absolutely will use C#. &gt; But at the end of the day, fundamentally speaking, Java is still used by MORE companies, and not only that, more TOP Tech Companies. This is fact. There's no changing this fact. It's been around longer, and C# stifled its own growth with its closed ecosystem. This is another reason .NET Core is a big deal: it removes those historical issues and sets .NET free. Sure, Java is used more places... I hope to never work those places... or I hope to enter those places will full intent to start over in C#. :D
F# is one of the highest paid programming languages out there but good luck finding a job for it
It's probably highly paid because companies like Ops have to throw tons of money out to get them.
There are languages people love and then there are languages that people use. F# is in the first camp and C# the second.
This is why it's not a great idea to use for a startup, using an esoteric language isn't a good idea when you need to hire quickly and are usually on a tight budget.
On windows iis and kestrel and ms sql reporting server use http.sys to listen to a port Kestrel is run as an app not as a service what is the difference anyway on multitasking is?
https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener
Got access to a web server? .net core api + hangfire or .net cores new scheduler. You could then write a simple admin panel in vue, angular or react... Fun. Get off the desktop, you will thank me for it one day.
Finally somebody‘s talking about cache eviction. I hate how everybody is just saying „Use a concurrent dictionary. Done“ Any plans to elaborate on invalidation based on update events? I.e. avatar has changes for user xyz so cached avatar needs to be evicted (and rebuilt)?
Haha thanks. I knew it should be easy.
F# isn't going away, it has a fervent community and Microsoft has shown renewed interest in it. Check out the f# projects community on GitHub for some active serious projects written in F#. F# takes advantage of most of the improvements in tooling and performance that has come with .Net core, and if you like discriminated unions it is pretty much your only CLR choice. It can be harder to hire F# debs and they tend to cost a bit more. However as something of a self-selecting community, anecdotally I've found your average F# dev is more skilled than say a C# or Node Dev. Lastly, it doesn't have to be all or nothing you can choose C# or F# on a case by case basis
Your last point is a great one. My company uses F# and C#, and as long as there are no leaky abstractions, it works great.
The two .NET projects that currently have the logic for steps 2 and 3 (verification and distribution) are .NET Framework libraries, but could probably be converted to .net core (maybe, idk, haven't tried it yet). Right now I have access to a Windows Server that I can put stuff on to run jobs and stuff. No one needs to see it, it just needs to autonomously do work on Entities in a SQL Server database when certain criteria are met. &amp;#x200B; I know that desktop isn't my path forward long term, but it's what I've got now so trying to make those WPF apps the best darn desktop apps they can be. I've done some mobile and just started poking into web stuff. &amp;#x200B; Appreciate the response. hangfire and the .NET core scheduler are new to me and I'll have to take a look at those.
Similar situation to you. Launched v1 of our app last year, cut every corner there was to get it up (internal app). Been developing v2 for a few months, we bought aspnetzero to enforce better standards and benefit from things like multitenancy and user accounts without needing to re-invent them. We'll do some refactoring later this year / early next when dotnetcore 3.1 launches specifically so we can use Cosmos (not going to go with 3.0, it's worth waiting a couple of months for 3.1 IMO). We are sticking with C# and I'd recommend you do the same. Finding good devs is hard, don't add another restriction to your list.
thanks...I want to test NopCommerce , or if anyone can recommend something else?
I would recommend trying out a CMS. It allows you to easily create content types (tables) and provides a standard UI for CRUD operations. It will also provide roles and permissions, logging, reporting and a lot of the common functionality required by applications. Having all this in place, allows you to focus on the features and the more interesting pieces of the application.
Nice article. A few typos, but still a lot of valuable information. We're looking to implement a container-based ETL system in C#, and I've been looking long and hard (heh) at Dataflow for that purpose. Have you used this in a production environment? How would you say it compares to a naive, queue-based processing pipeline that parallelizes over a single job at a time (vs parallelizing at the larger granularity of jobs themselves)? Obviously, I need to benchmark some of this stuff, but I was just curious what your experience with it has been. Thanks for the post!
&gt; This time I want to “do it right” or at least make better architecture &amp; more importantly better domain. And I have stumbled upon F# (I’ve known that it exists but never paid much attention to it until now). Good luck doing it right when you're hopping on a new language and paradigm you have (it sounds like) no experience with. Odds are you'll just write another mess. It's not going to be some magic bullet that inevitably results in wonderful code pouring from your fingertips. You'll be far better served iterating on what you've already done and spending your time improving rather than starting over from scratch. &gt; we will be moving majority of data from SQLServer to DocumentDB (ie Cosmos) Word of warning: model your costs before you commit to this. &gt; I am working for the startup company which I have co-funded and I am in charge of all technical decisions. Our only product is B2C app and we have pretty small tech team F# *will* handcuff your hiring ability. The Big Rewrite™ is often a very bad idea for startups with huge opportunity cost.
&gt; There are languages people love and then there are languages that people use. F# is in the first camp and C# the second. So you say... I don't think you could pay me enough to program in F#. I'll gladly code C# for my current position and salary (where I'm currently coding Python3)
&gt; F# will handcuff your hiring ability. The Big Rewrite™ is often a very bad idea for startups with huge opportunity cost. This can't be emphasized enough. Take Reddit for example... Originally written in Lisp. Then the Big Rewrite? Python. The difference with F# and C# is that you can probably blend them nicely since they both compile to the DotNet CLR. I read an article the other day by some guy who absolutely had to have some certain section of code be reliable (immutable) and avoid side-effects. So he wrote it in F#. But the rest of the code was written in C#. Makes sense to me.
I agree. Whenever I'm forced to use Java I almost have to stop and cry because trying to program without Linq, async/await, C# DateTime and the vastness of features that come with .NET feels so tedious after having them there.
I'm reminded of a project that I starred recently called RawCMS: [https://github.com/arduosoft/RawCMS](https://github.com/arduosoft/RawCMS) "RawCMS is an headless CMS written in asp.net core build for devopers that embrace API first tecnology. RawCMS uses MongoDB as data storage and is ready to be hosted on Docker containers." I have to say, other than starring it, I've done nothing else with it.
&gt; There is no changing this fact. Time will change this fact.
&gt; I would imagine if any person on this sub had a choice, they would not choose C#... Yeah that’s probably why they’re subscribed to the dotnet subreddit
Pluralsight has a whole course on DDD. It's not free.
I was in the same boat. I made https://www.codenesium.com to try and solve it. It's geared towards a SPA app though. You can use t4 templates to build pieces of your app you need to generate.
**tl;dr** .NET Core 3.0 is faster than .NET Core 2.1
The article states that **SimpleMemoryCache** is thread safe. Isn't it possible that two concurrent calls to GetOrCreate with the same key (when there is nothing in the cache yet), would both call createItem and compete over inserting it into \_cache?
Way faster.
The nice thing about F# is that it is built on .Net framework. They’re totally (I think) interoperable. Almost every feature of C# is part of F# and vice versa, so choosing between the two is largely a superficial choice based what you prefer. The new C# version (6) has more features that reduces boilerplate code. So check that out.
Excited to find out more of the Linq to SQL performance enhancements, since I know they are still catching up .NET Core for some of the more complex translations.
From your description the "auto-grader" basically takes a file/directory, do some processing and spit out the an output(grades and cheating analysis). You didn't mention having to re-do the implementation of the processing, I assume you can just grab the old code and modify it? If so, what's stopping you from simply turning the WPF into a console program by adding support for some command parameters(in particular the student codes path)? Lastly, with a console program you can simply run a page with a single button and filepath textbox and basically Process.Start the program and scrape the output to the page?
Have you looked at 3rd party code generators? This is exactly what they are good at. Generating source code from a database table. I made my own code generator years ago and it has saved me many hours of writing the same basic code. You could make custom templates to fit your backend or interface requirements. The templates are usually pretty flexible so things like this are easy if field is datetime then output datepicker code &amp;#x200B; There are a few free tools around or a few hundred dollars is cheap if it saves the boredom.
Devmentors.io has a good video series about this stuff.
I have to strongly disagree here. I use C# because I absolutely adore it.
Indeed, there is some competition. I think it should also use `MemoryCache.GetOrCreate` with a factory, such as: public class SimpleMemoryCache&lt;TItem&gt; { private MemoryCache _cache = new MemoryCache(new MemoryCacheOptions()); public TItem GetOrCreate(object key, Func&lt;ICacheEntry, TItem&gt; createItem) { return _cache.GetOrCreate(key, createItem); } } Inconvenient is that `createItem` now needs to deal directly with `ICacheEntry`.
Thank you, although, we merely touch DDD, it's more about microservices in general :).
Hi, **SimpleMemoryCache** is thread-safe in the sense that you can use from multiple threads without risk of exceptions. The issue you're describing is absolutely possible, and the 2nd thread to finish will override the **createItem** of the first thread. For a *complete thread safety*, where **createItem** is created just once, use the last implementation **WaitToFinishMemoryCache**.
https://github.com/OrchardCMS/OrchardCore/ It's a fantastic CMS with flexible model, etc.
Lol, that's probably not the best approach to start a task. Updating based on events is definitely an issue to consider. Now you have to consider whether to refresh it immediately or wait for the next request, which is not trivial.
Now .net core 3 supports WPF so you should start looking at converting those projects to .net core, it would be remiss of you not to. If you did it right your libraries could still interact with your .net 4.X framework by implementing them as .net standard. So you can port them across slowly. It's not that hard, but there are a few gotchas along the way. e.g. any external libraries might cause you issues. &amp;#x200B; Do me a favour, create a .net core web api and add hangfire (the docs are pretty good, you should be able to follow your nose). Then have a look at the dashboard it provides. I suspect it will be all you need and I would be surprised if that doesn't convince you to use it. &amp;#x200B; If you get stuck stick the project on github and send me a link.
The talked of LINQ and not Linq to SQL which I assume is pretty much dead.
Thank you for your answer :) &gt;You need extremely high throughput? &amp;#x200B; Not really, but business requirements change often and it is much easier to change smth. which has *implied* (as Fowler nicely put it) schema than strict relationships SQL requires. And we have much more reads than writes and rarely any updates (which is good for Document like DBs). I think that we will be much faster with NoSQL solution than we are now with SQL
I respectfully disagree. Thread safety is not only about having exceptions or not. It is also about having consistent observable state. It the code on the article if two threads attempt to create the item, while one will override the other they will both get a different value: the one that was created for them. And if they themselves cache that value locally then you have a defect because one thread is holding a value that is now gone from the cache. On the other hand, using the proper GetOrCreate method fr the underlying cache and while several instances might ne created in the factory method, only one will ever get into the cache and the other will be dropped. But in the end both competing threads will receive the same value. That behavior is consistent with ConcurrentDictionary and as a user of your API, I would expect the same.
&gt;Lastly, it doesn't have to be all or nothing you can choose C# or F# on a case by case basis &amp;#x200B; Thanks for your input, this is very much true :)
&gt;We are sticking with C# and I'd recommend you do the same. Finding good devs is hard, don't add another restriction to your list. &amp;#x200B; Agreed! But I think that restriction wouldn't be "you have to know F#", but "you are willing to learn it and to work with it" - but that still *is* restiriction :/
Thank you very much for your answer! You raise some good points, I have edited my original question to add bit more context :) &amp;#x200B; &gt;we will be moving majority of data from SQLServer to DocumentDB (ie Cosmos) &gt; &gt;Word of warning: model your costs before you commit to this. &amp;#x200B; Thanks for the warning! I have been thinking about this (ie moving to some Document based DB) for more than a year, and I think this is the way to go. I am C/P(ing) this from my comment below: &amp;#x200B; &gt;You need extremely high throughput? &amp;#x200B; &gt;Not really, but business requirements change often and it is much easier to change smth. which has *implied* (as Fowler nicely put it) schema than strict relationships SQL requires. And we have much more reads than writes and rarely any updates (which is good for Document like DBs). I think that we will be much faster with NoSQL solution than we are now with SQL
&gt;The difference with F# and C# is that you can probably blend them nicely since they both compile to the DotNet CLR. &gt; &gt;I read an article the other day by some guy who absolutely had to have some certain section of code be reliable (immutable) and avoid side-effects. So he wrote it in F#. But the rest of the code was written in C#. &gt; &gt;Makes sense to me. &amp;#x200B; yeah that is really awesome and one of the reasons that I am even considering F#
I have no argument about that, except for maybe terminology. The basic **MemoryCache** implementation from Microsoft NuGet is **not** thread-safe according to your description. That's why I proposed the last implementation which answers your requirements.
Nevertheless it's a great course to get your feet wet on the whole topic. Highly appreciated.
Looks interesting, didn't know about this library. Thanks for sharing.
I gave a look at the code and I am puzzled. I assumed Microsoft implementation would be thread_safe, but in fact, it isn't for the same reasons as above. Given that the cache does use a ConcurrentDictionary internally, I fail to comprehend why they made the public interface unsafe. They might have had some performance trade-off when designing it, as it was first and foremost for Asp and only later moved to an independent package.
My guess why is as good as yours, maybe MS didn't want the performance overhead. Looked at the code now, and saw some locking but no ConcurrentDictionary.
Hmm. We are not looking at the same code apparently. Here I can see a ConcurrentDictionary: https://github.com/aspnet/Extensions/blob/master/src/Caching/Memory/src/MemoryCache.cs
You're right. I decompiled the NuGet and somehow missed it. I see it now
Thanks!
You can use with modals and dynamic partial modals (like jquery get or load). You can't use in sweetalert2. But you if you want to show the result messages with sweetalert2, you can.
Since you like being negative from the start..... well you probably deserve the backlash here. &amp;#x200B; Just so you know though, I personally did the C++/Java jump to .NET and I do not want to go back. And if you're good with Java you'll feel quite at home in the .NET library it's a very simple transition in case you ever need to do it.
I'm taking some wild guesses. Like you, I'm starting with .NET (being developing for 10+ years). - Azure is MS's Cloud service. So, quite likely you are expected to make a web site/service. - .sln is a Solution file. It's used to "join" several projects. This is more for development. - You'll need to make a Production build. That's what you upload to Azure. I think VisualStudio already has a recipe included for such case. - The code running on the Raspberry Pi would most likely have to export its results somewhere instead of saving them internally. Your application in Azure should be able receive that information and store it.
1) Yes we need a website to display the data visualisation and a place to upload the images 2) So does that mean I cannot use the above project from GitHub? 3) How do I go about doing this? 4) Yes and if I am not wrong I am supposed to create a database in .NET. Thank you so much for replying! You don’t know how w I am
Also, how do I connect all of this platforms together to make it work? I’m a total noob.
2. GitHub is a code repository. You use it to keep track of the changes to your code. GitHub might not run it, but it will assist you in other, equally important tasks. 3. You'll need to play around with the build settings of the project. Sorry, I haven't reached that part either. I did once saw something about publishing to Azure. 4. If you're using Azure, there's an SQL version that running on it. You can use the Entity Framework with Code First to handle the database. HTH.
I was making a general point. All languages have their fans.
It's hard to tell without knowing more about what your requirement are, but if your processing jobs are independent from each other parallelizing will always win when compared to a single job at a time.
This is how I would do it. It might not be the most optimal way. I'm open to improvements. 1. From the Raspberry Pi, open an http connection to the .NET app to move the data. 2. The .NET app would have a REST API endpoint to receive the data and store it to the database. 3. The .NET app would read from the database to process and display the data.
About four years ago I rewrote my desktop C# application in F#, but I kept WPF GUI as C#. I did not regret that decision one bit. Modeling business logic in F# is much easier, is much more expressive and requires far less code. I also felt I was able to code in F# much faster, once I got to know the language. However, it was kind of one-off thing and maintaining the codebase for a long time was not a consideration. As such, I did not have to worry about such thing as hiring people with F# knowledge or teaching F# to anybody. Sadly I have not been able to use F# anywhere since then, and it truly does bug me. It is a damn shame MS keeps treating F# as a mere afterthought. And while I have not been keeping tabs actively on F# community, I don't feel like the community has grown in the past few years. Instead MS focuses on implementing half-baked F# features to C#. Bah. I feel Kotlin even beyond Android ecosystem has quickly surpassed F# in momentum. It tells me there is a need for languages such as F#, but that F# never caught sufficient momentum to truly make it. Now it remains largely obscure, but favored by select few who have found its beauty. Alas, I am afraid that is as far as F# shall go, unless something significant happens that gives its momentum a nice boost. Now, whether or not to use F# is a difficult question. It is true it is much harder to find F# devs, or C# devs willing to learn F#, but if nobody takes those risks, then F# won't ever gain even a bit of momentum. F# lacks a high-profile supporter. JetBrains did well in supporting Kotlin, since they developed the language and their Rider IDE has first-class support for it. Then Google also started to support it in official fashion, and now Kotlin is the recommended language for Android development, by no other than Google itself. In contrast, Visual Studio never had first-class support for F#. In fact, VS had had a lot of issues with F#. Ironically, JetBrains' Rider supports F# better than VS. A language with no strong corporate backing almost never goes mainstream. And F# has no such backing, even if it originated from MS. Sigh. I love the language and I would love to use it everywhere in place of C#, but it is hard indeed to justify unless you're solo.
Lots of mentions of removing stringbuilder for performance reasons. I thought stringbuilder was fast. What kind of alternative are they doing?
I thought about splitting them up, but I thought it was possible to achieve my goal without doing so. &amp;#x200B; I never saw a ComplexType class, interesting. Thank you for this. Which properties should it store, because I don't want the Archive class to be the dependent class between itself and SpecificFile. &amp;#x200B; Thank you for providing me with this solution. Much appreciated.
I completely agree with everything you said :)
StringBuilder is a lot faster than repeatedly using += on strings, but if you have a look at the PRs, they're mostly being replaced with `unsafe` code blocks that use pointers and spans.
When we look for people I don't know that certifications are that important to us. We're more concerned with your abilities and knowledge and how well you interview in general. In my experience, most people can pass a standardized exam, but that doesn't mean you'll be a great developer.
[C# in a Nutshell](https://www.oreilly.com/library/view/c-70-in/9781491987643/) by Albahari brothers.
ok sooooo the best way to approach this would probrably be 1. Create a .Net Core Web Api and host it in App Services this will be a central communications bit that your PI will send data to and anything else will pull data from. 2. It will need some type of data storage I would suggest something like MSSQL or Redis(use stackexchange.redis library for the api) if your not real familiar with SQL. 3. Now make a website this can be hosted in a second App Service container or in an azure storage account and will get its info from the API you just made. I would suggest Angular or Vue.js because they are easy to get up and running quick.
Is there a specific reason why you need all this? Because DDD and CQRS contradict each other, they might work together, but DDD focuses on business oriented applications while CQRS is all about great performance getting and setting data. I'am not saying they dont coexist, but when they do is in VERY big enterprises wih very good developers, if you want to learn about architecture this might be the wrong place to start.
Thank you for looking, read it on my phone so wasn't really keen on digging through pull requests. &gt; StringBuilder is a lot faster than repeatedly using += on strings, This we all know, but I was curious if the method for improvement was potential `String.concat(span1, span2, span3, span4)` so that it's all batched in a single operation with the inputs being spans instead of strings.
And if I am a great developer with 10 years experience in Oracle. Any advice on how to stand out?
And if I am a great developer with 10 years experience in Oracle. Any advice on how to stand out?
Can’t I make a database from .NET?
If specific_fle is something easy to derive from other_files (such as the first file in the collection) you can exclude it from the database storage and just do something like: [NotMapped] public virtual File specific_file =&gt; this.other_files?.FirstOrDefault(); I have a similar situation where I had a base class with one file reference, but a derived class with multiple. In my case I was just using a string for a file reference, but I still had to make a class for the multiple files since the version of EF Core I used couldn't handle a collection of strings. public class Thing { public string MainFile { get; set; } } public class Subthing : Thing { public ThingFile[] MoreFiles { get; set;} = new ThingFile[] { }; [NotMapped] public IEnumerable&lt;string&gt; Files { // Make sure you .Include(x =&gt; x.MoreFiles) when accessing the DbSet or this property won't work (MoreFiles will be null). get =&gt; new[] { this.MainFile }.Concat(this.MoreFiles.Select(x =&gt; x.Value); set { if (value == null) { this.MainFile = null; } else { this.MainFile = value.FirstOrDefault(); } if (this.MainFile == null) { this.MoreFiles = new ImageFile[] { }; } else { this.MoreFiles = value.Skip(1).Select(x =&gt; new ImageFile { Value = x }).ToArray(); } } } public class ImageFile { public string Value { get; set; } } }
you can indeed thats what MSSQL is.
I have been asking these kinds of questions recently, having been in the F# ecosystem commercially for 12 years now. The answer is complicated. F# is going through some turbulence but the worst is (I think) now over. VS2017 was a disaster for F# and introduced a huge number of show-stopping bugs into the core tooling. Updates have fixed many but it is still quite buggy (I was playing with F# in VS2019 today and quickly found another bug). The F# event scene has become fragmented across more events but the recent F# Exchange in London was apparently the largest ever with ~200 delegates. Microsoft are investing in F#. For example, VSCode is obviously hugely popular right now but the F# tooling is lacklustre so Microsoft are paying to guy who wrote it to sure it up. I think that is a *very* good sign. Finally, I'd note that we have [a mobile app written in F#](https://play.google.com/store/apps/details?id=idtechex.app.droid&amp;hl=en_GB) using Xamarin and it has worked very well for us. Our CTO is very happy with the tooling. I'm keen to do things with it that would be very hard to do in other languages. On the other hand, I think F# is so tied to .NET that F# for client-side web is much harder than necessary so I'm working on a minimal ML dialect that is designed to run in the browser. Although it is a hobby project I may well end up porting it to native code on Linux using LLVM but that's a long way off...
Thank you very much for your answer, it is nice to hear that you have had success with it and that things are starting to look good :)
I'm currently studying for the az-203 And if you take the az-400 after that, you get an Azure devops cert
**N\*** is from .**N**ET, same as **j**\* for java software
I assumed this was the case, but I couldn't find anything definitive.
I'm not personally a fan of that convention. I think I have only seen it used by packages that are ported from or inspired by Java libraries that start with J (like NUnit being named similarly to JUnit).
I must have read that wrong! Yes I understand the MSSQL part now. Thanks!!
The CQRS bit is ok, but you lose the ball a bit on the ES side of things. First, you don't define many of the terms. Projection is completely missing, and as you only have one projection in your diagram, you miss the subtle advantage of having multiple projections. &gt; Next advantage is that saving your domain aggregates as series of events greatly simplifies your persistence model. You no longer have to design tables and relations between it. You are no longer constrained by what your ORM can and cannot map. During working with extremely advanced solutions like Hibernate we found cases, when we had to resign from some design concepts in our domain because it was hard or impossible to map to database. No, this is horrible. Events sound great in principle, but you are utterly screwed if you choose your events badly. It will make a more traditional ORM refactoring exercise look trivial in comparison. Speaking of which, projections may well result in records in a relational database which look exactly like the same system built without ES. You still have the same relational mapping issues. You can't just bung everything in Elastic unless Elastic is the right fit. So there should not be a case where you couldn't fit a domain into a database. There is a fine (or coarse) line between when an event is considered too fine or too coarse in ES, or how to deal with the event history when an event's context changes. It would also be nice if you mentioned that ES works on completed events not intentions, and this has a load of complexity in itself. A rule of thumb, and it's a loose rule is "If it ends in ed you're going in the right direction". So UserPasswordChanged is a good event, UserRequestedPasswordChange probably isn't, but really what you're want is UserPasswordChangeReceived. See the subtle bit? &amp;#x200B; :)
I will start that after MCSD. Your username seems appropriate 😊
That seems to be the trend. I don't have strong opinions either way but it does lead me to try to think of clever package names.
I don't want to detract from the great work happening on .Net Core but most of these improvements are measured in nanoseconds. That's just not going to result in much of any measurable improvement for the vast majority of applications.
I think it's pretty clear the poster above means entity framework's linq to sql translation, not the ancient and definitively not Core compatible L2S library.
Use CQRS and use both? Tbh, if sql migrations are a bitch, then nosql schema versioning is a proper motherfucker.
I don't agree with this, at all. These types of nanosecond improvements are being made to the core building blocks which the runtime (and in turn, your application) depends on. If one of the runtime functions calls one of these methods that got a 100ns boost, and it calls it ten times, that's very likely going to have a noticeable impact on your application. Sure, not all of these are to these core types of functions, but enough of them are. Or, in other words, you're right that shaving a few nanoseconds off of a database call isn't going to matter. It may not even be measurable. But, shaving a few nanoseconds off of the functions (which are called many, many times) that marshall the values returned from the database into C# objects sure will. Time will tell just what impact this has in real life, but I do think you're really underselling the point.
For this, `SpecificFile` should be an empty class. All of its properties are already declared in `BaseFile`.
&gt; If one of the runtime functions calls one of these methods that got a 100ns boost More like 5ns. &gt; Time will tell just what impact this has in real life, but I do think you're really underselling the point. The system I lead dev on serves near StackOverflow levels of traffic. I've written plenty of custom high-perf .Net code to replace BCL stuff. I'll stick by my original statement that the vast majority of apps will not see any measurable difference.
&gt;Use CQRS and use both? &amp;#x200B; Actually I haven't even thought about it, thanks for suggesting this! Just to be sure, you are suggesting Commands for SQL and Queries for NoSQL ?
Honestly I'm not sure about anything anymore. I have this absolutely awful professor who thought practicing microservices through his student was a smart idea. This is his idea, I have zero knowledge of DDD. I might just listen to you and not focus on DDD. I'm self educating in his course cause I'm not learning anything from his ramblings but I'm obviously trying to learn the concepts he throws at us. Now it was DDD, Event Sourcing, CQRS and so through microservices.
Liked it. But was hoping for actual example / demo of it. Seen way to many people hyping up Event sourcing, graphql and what not. But never reallyd seen any good sample projects showing it in action.
I started MCSD at my first job 5 years ago. It was a contracting firm and they used it as a selling point for clients. I only completed 70-480 before moving to a new job and haven’t looked back. Hasn’t seemed to hurt or help me really, you can learn a lot by studying for the tests but personally I think side projects using the involved tech is a more effective way to learn. The only word of caution I’ll extend is too many certs in the absence of side projects or general work experience looks mighty suspicious. Good luck with the transition!
Obviously, for programs that don't call these particular methods over and over, yeah, there won't be an earth-shattering perf change. Still, the fact that most of them lopped off 60% to 80% of the time means several classes of applications _will_ see crazy improvements. Parsers, etc.
Dont start there, your professor shouldnt start with those things, and if he does he probably doesnt understand them, get strong grasps on .NET and SQL Server, if you want to go full stack learn some basic js, html and css and you will eventually get better the more you use those. After you dominate the stack you will face some issues when it comes to builind and maintaining an app, thats where you should dive into architecture, what your professor wants you to learn is very, very recent architecture, thats has a lot of hype behind it and is not always applied correctly because everyone wants to use it. Good luck on your journey man, feel free to ask anything!
No.. consider an app servicing thousands of simultaneous users. Or a farm of servers serving tens of thousands. As they describe it, the peanut butter effect is very noticable.
Thanks. What would you consider a good side project?
The best side project is one you are passionate about or at least interested in. My advice would be pick something you have to do regularly and design a tool to make that task easier, then look into how to accomplish that with the specific technologies you’re interested and go from there. You can of course learn by doing the standard “make a planner, make a budgeting app” but you’ll get a lot more out of it if you’re invested in the outcome of the project rather than just doing the same canned tutorials everyone grinds through. You’ll also end up with a couple pieces of code you’ll want to show off, which definitely helps in interviews.
Actually that's where I am. I am a decent web developer using the Angular/Net Core 2/EF/SQL stack capable of building E-commerces and such. Honestly, I am kinda stuck. I feel like I have a good grasp of what I know and I am also getting top grades in my web developer courses but I feel kinda lost how I'm gonna develop from here. I looked online for advanced courses and finished (this one)[https://www.udemy.com/csharp-advanced/] albeit it didn't have much new stuff it definitely had some usefull techniques for me. I have also thought about the possibility to simply leave my stack and focus differently but I'm done with my studies in about 10 month and I would prefer to keep working in this stack - in 6 month I have my internship and until then I just wanna get better at everything. I've always developed 4 somewhat decently sized applications in my stack and in the latest I've included stuff like global error handling, helper classes and such. I just so want to be better at this stack but I can't find a course that'd fit my needs and my professor is an absolute joke. Any suggestions would be greatly appreciated. I talked to a guy the other day and his suggestions was focusing on the devops part - things I am not confident in. I think I want to do that (as I understand that's also what you're getting at) but then I would have to completely go the other way of my professor - not a big thing since I don't need a good grade here or anything but I need to feel confident in what I'm doing is really useful to me then.
How will we cope without the ten-level-deep nested `BsonDocument` initializers??
That does give me a little more to work with though, which is helpful. Thank you!
I'll take a look at it! Thank you!
Perhaps you should have paid more attention in class?
You say you are confortable building E-commerces, but those are not big applications, eventually you will lead into some challenges, performance, scalling, huge data and code bases. Just finish your studies and do your internship, you will eventually find bigger challenges and learn along, but discard the DDD stuff for now, it will hurt you more than it will help you.
On the contrary, there should be more from the open source community. In some areas, dotnet needs more OSS libraries. Go has so much more open source activity.
IIRC there's like one page each for 2.1 and 2.2 covering needed changes to projects to migrate from the previous release. They're pretty straightforward. Other than that it's mostly new features in each.
If you're using [JSON.NET](https://JSON.NET) you can set a handler for errors in the json serializer settings and tell it to ignore them. It's just an event handler. I can't remember the details, but someone will figure it out if this is what you're using :)
You can use System.Data.* namespaces to issues queries directly to the database instead of using something like Entity Framework. Though for SQL Server the latest package is Microsoft.Data.SQLClient (System.Data.SQLClient is usable but isn't getting new features).
Yep, if you're porting JUnit to .NET, NUnit makes sense, or go the other end. Akka in Java became Akka.NET on the .NET end. I think it's nice to give a nod to the originators, and it helps acclimate developers coming over from that side.
Optimize the app so you use SQL query less and you have already gained more performance. But all bits help I guess.
I wouldn't have powershell scripts in a project like this. Use [System.Management.Automation](https://docs.microsoft.com/en-us/dotnet/api/system.management.automation?view=pscore-6.2.0) instead. There is a Powershell object you can create.
Thank you for your response! I have been using the System.Management.Automation namespace to create Powershell objects, then I add scripts/commands, create a runspace, and then invoke the Posh scripts (I am not directly opening powershell.exe and running scripts that way :)) My question concerns the overhead in invoking Posh scripts like this, creating the runspace/Posh environment, invoking the scripts, mapping the PSObject results, etc... versus directly using the Web.Administration API to deal with .NET/C# objects. At the end of the day, I'm still manipulating IIS objects, but one method involves the overhead of invoking Posh (but much easier to manage), and the other uses .NET provided APIs in C# (but harder to manage). Also, when going with the Posh/Management.Automation namespace way, I'm not sure where to store all my scripts so that I can add them to the Posh object to invoke. Hope this all makes sense.
When the length of a command overall is measured in fractions of a second, nano seconds are important. It likely won't be noticeable when you're writing software that only runs for under a minute or stuff you're practicing, but when you get down to programs that are sifting through SQL tables with thousands or millions of records, nanoseconds add up fast.
OT: What's up with all the downvotes? If yer going to downvote, at least explain *why*.
You're sure Grafana + Application Insights can't do what you need?
Depending on the level the OP is, the moving parts aren't obvious.
An Instructor is highly unlikely to just assign a project like this without having previously talked about the moving parts.
I agree. Sometimes, the project group will decide on a project that is outside their areas of knowledge. The instructor can and should try to dissuade them, but at the end of the day, it's their call. Failure is part of the process. I saw this with a group during my capstone. The group decided on a project, and the professor raised his objections. They still wanted to do it, so he let them. The group got surprisingly close to finishing the project (only issue was at the hardware level).
Curious, have you evaluated any off the shelf solutions for this?
No future. It is dead because Microsoft killed the Windows 10 Mobile (the best mobile OS so far) and no longer care about small 8" tablet devices either. No Windows 10 tablet mode fixes, WoA is also dead as there are no devices on the market. The ecosystem is dead, Store is also dead (MS Office is no longer there). Sad story.
A lot of misinformation here. F# is actively and continuously maintained, every single day. It is a first-class citizen of dotnet. Many of the perceived tooling issues are because people who use C# see that F# doesn’t have feature parity, but many of those features are not critical for using F#, since 99% of your code isn’t brackets and objects. There is no issue with momentum; if anything, F# has been steadily gaining momentum. F# receives updates with each release of VS, just like C#. As many people have mentioned, C# eventually gets F# features. F# and C# aren’t meant to replace each other. Leverage the strengths of each - you have .NET to tie it all up. Most of my projects are F# backend, C# frontend. As far as the comment about corporate backers: https://devblogs.nvidia.com/jet-gpu-powered-fulfillment/
Piranhacms
Thx. I've heard good things.
Thank you very much for your answer! I have been playing today with F# for the first time (till now I have just been reading / watching some videos) And what you said &gt; Many of the perceived tooling issues are because people who use C# see that F# doesn’t have feature parity, but many of those features are not critical for using F# &amp;#x200B; Is in my very limited experience - true. At first it was super weird for me how much less things VS does for me whilst typing, but after couple of hours I have realised that I actually do not need bunch of fancy stuff because everything is basically spaces and/or tabs and it is super easy / fast to type!
YouTube
Nagios does pretty dang good job, but what I don't know is if Nagios can introspect into the IIS server to view stats on individual applications served up by it.
You can use any filesystem as a simple nuget package host. See https://docs.microsoft.com/en-us/nuget/hosting-packages/local-feeds
https://fsharpforfunandprofit.com/ Definitely check this site out. It’s like the “Learn you a Haskell” book - easy to digest and full of simple, descriptive examples. This is also a nice resource: https://devblogs.microsoft.com/dotnet/get-started-with-f-as-a-c-developer/
Microsoft.Web.Administration is pretty straight forward to use. I'd think much more so than trying to run PS from .Net.
If you have tons of scripts then you might consider just storing them in a database and retrieving them to add to the Posh object. This makes it a bit of a pain to edit scripts though. If it's a small enough number of scripts for you to manage them in folders and files then you can check them into your source control and dynamically load them from disk in your application.
You don't give any context - are you an experienced developer with multiple languages under your belt? OO languages, functional, etc? Are you just getting started? Are you aiming to work in the desktop world, server-side, building webapps, mobile apps, games, what? You can learn a lot just by grabbing Visual Studio Community Edition - it's free - and playing with the starter project templates there while having the docs on Microsoft's website up in a browser window. Youtube has some good videos, but for a more focused approach I'd get a Pluralsight subscription, around $30/mo. and go through courses on the various .Net related paths. They cover a lot of material, everything from the basics like data structures, variables, etc. up through very advanced tasks with particular tools.
*Remember: WPF is about 20 year old tech by now.* .NET isn't even 20 years old
Using powershell scripts like this is the right way, actually. Anything else that can do things like get "specific details about the servers/apps" is going to be privileged and probably need to interact with [wmi](https://docs.microsoft.com/en-us/windows/desktop/wmisdk/about-wmi). You don't want to go down that rabbit hole though.
I wouldn't use SQLite DB for anything with many users like a blog could have. It's not well suited for web apps because it's file based. You should work with something like MySQL, MariaDB, PostgreSQL, and MSSQL. It's easier to secure those databases and they are much better suited to multi user environments. Note that people viewing your blog in this case are users even if they're not logged in.
If you are really serious take a pluarlsight course. It is worth the money and you can cancel any time. i think they might give a month free.
Because it's a homework question and a bad one at that I'd say. It comes off as the classic "I didn't pay attention at all in class and now I have an actual project which I need to do and haven't got a clue. Please guide me through the whole thing so I basically don't learn anything again."
I can understand downvoting the post. It certainly has that vibe. But downvoting the answers of anyone willing to help? That's unnecessary.
&gt; MFC How about Qt or Electron?
Okay I can wire up a MySQL dB then! Thanks for the heads up.
Did you use the [Authorize] attribute or the [Authenticated] attribute for your controller? That will usually redirect you to the login url, mentioned in the JWT authority. Refer this: https://devblogs.microsoft.com/aspnet/jwt-validation-and-authorization-in-asp-net-core/
You had me going with a simple example at the to where a service is broken into two one that does reads and the other that does mutations. However as the top poster said I got lost in events, why do I need them? My thoughts when at the start of reading the article was wow great. I can use dapper in the read interface implementation and efcore in the mutators interface implementation. If I do this, is this a good pattern?
Are you trying to create tokens? Verify tokens? Or call another service with a token you already have? If you already have one, the JWT goes in the 'Authorize' header like so: `Bearer ecysdf.sdfesf.sdfsdfsf`.
I'm not sure exactly what specific information you need about servers and applications but it sounds like Retrace covers most of it. https://stackify.com/retrace/
No, Electron is the worst what could happen :-/ Web crap, slow, ugly, non-native, no threading etc.
They are focusing UWP now on being a UI framework rather than an app framework. All new .NET component created by MS will continue to be built using UWP. However, for the overall app framework, MS is directing users to use .NET Core. This is part of what XAML Islands is about.
I don't want to sound like a troll, but did it ever have a future? It's first iterations couldn't come close to the capabilities of WPF/WinForms - and although it seems to have matured, it's still tied to the Windows Store (I personally don't know anyone that uses it, except for some gaming titles that are tied to the Windows Store).
Thanks for the links, I am familiar wit FSharpForFunAndProfit - it is pure gold. Also I have been reading Scott Wlaschin's Domain Modelling Made functional - one of the best DDD books I've seen.
Genuine noob question. &amp;#x200B; Will things change with .NET Core 3 ?
UWP is not tied to the MS Store. Adobe XD is UWP and not in the MS Store. Many people use the MS Store. For example, the best touch &amp; pen optimized Surface apps are UWP and on the MS Store.
Not for desktop UI frameworks.
Apparently, ConcurrentDictionary is not thread-safe in the sense that it can actually call the creation delegate multiple times: [https://andrewlock.net/making-getoradd-on-concurrentdictionary-thread-safe-using-lazy/](https://andrewlock.net/making-getoradd-on-concurrentdictionary-thread-safe-using-lazy/) That might explain it.
[removed]
Not a course but you can learn a lot here https://github.com/dodyg/practical-aspnetcore
I didn't know you could distribute UWP apps outside the MS Store - thanks for the info.
Ive used this in the past, seems to work pretty well. [https://github.com/chriseldredge/Klondike](https://github.com/chriseldredge/Klondike)
As others have mentioned, there are some really good tools already available for a lot of this. Prometheus and grafana. Datadog. New relix. Etc etc. All have some level of app/infra monitoring and also (mostly) will have alerting built in as well, or easy connection to pager duty or other alerting tool. Just something to consider.
Somehow half of the apps I use on my PC is based on electron or similar technology.
These are no real apps, just poor local websites.
Looks like an app, feels like an app. How do I know it is not an app? Well, everyone like using these, so who cares about the name.
It has no native Windows controls, it doesn't support custom ClearType settings in (terrible) font rendering, it is terribly slow and resources hungry, it spawn tons of external processes instead of using thread we already have 20 years .... It is terrible crap.
This is quite shitty.
Not going to dive into the same discussion for the Nth time, just saying that the only one thing that is relevant is high memory consumption. That is the single problem that should be solved before I could call it a great technology. Everything else... 99% of the users are totally fine with it. Trying to build an app on something else would probably bring much greater issues, and bad code will be bad no matter what framework you use. &gt;!And thanks god I will not see these terrible native Windows controls in most of the apps, lol. But that's subjective.!&lt;
&gt; bad code will be bad no matter what framework you use JavaScript is not able to deliver anything safe and and with decent performance (no threading, no memory model control etc). It is completely wrong technology for that purpose. No way to fix it.
JavaScript has threading, even in the Web and it **absolutely** does in node. Christ most electron apps don't even run as a single process, let alone a single thread. JavaScript is also a fully garbage collected language, it's as memory safe as C# so I have no idea what you think you're talking about there. JavaScript is also JIT compiled to assembly, so beyond a slightly slower start up, it's not particularly poorly performing either. And electron exists because it's a really cost effective way to write cross platform apps, not because those damned Web developer kids won't get off your lawn. You do this in every thread. No one gives a shit about native windows controls. Not even Microsoft and they make the bloody things. Chrome, and therefore electron supports clear type just fine, and I've never had a single font problem in an electron app anyway. There are some really fucking awful electron apps, but they're awful because the design and development practices behind them are awful not because the technology is bad. There are also some really awesome ones. Visual Studio Code is an excellent development experience.
&gt; JavaScript has threading No, no threads, no synchronization objects, no concurrency memory model. Its "threading" is like cooperative multitasking known from 16-bit Windows 3.1. Ridiculous. &gt; electron apps don't even run as a single process, Yes, because it lacks essential threading support. The Skype electron crap needs six separate processes and allocates 500 MB VM while the same native Skype app that had more features and could use threads was single process and 70 MB VM. Why I should use such shit? &gt; Visual Studio Code is an excellent development experience Not at all. That's why I still have MSDN subscription.
VSCode is an electron app, and it's a pretty wonderful product.
Maybe, but it is poor compared to normal Visual Studio written in C++/.NET/WPF.
That's because it was built as a lightweight text editor at it's core. They let the developer community handle the rest of the work. I'm sure there's a debugger extension out there for most of that stuff though.
Yes, and we are facing the wrong technology (Electron) limitations again.
My coworker is working on a UWP app for some tablets that employees will use in the field, and this news has kind of defeated him. Really sucks to sink all that dev time into a product and then know it doesn't really have too much support going forward.
 [https://www.pluralsight.com/paths/aspnet-core](https://www.pluralsight.com/paths/aspnet-core)
Is this good one for starting up with .Net ?
Yeah, a few times. Transferring from one source control to another can usually be done fairly trivially with a tool, including copying over contributor information, but bear in mind that you may have to run both side-by-side while you set up CI with the new system, and then run the final migration when everything is ready. CI support is actually a good argument for modernising your source control system, and many like AzureDevOps now have built-in CI systems anyway - but this does mean that you'll be rebuilding all your pipelines in a new system. You'll also have a stronger case if your fellow developers have experience in github outside of work, as otherwise there will be a learning curve for that, so it'd be a good idea to poll them. Also, if your old repo is in more of a mono-repo style, you will likely want to split it up along application/library lines and have one pipeline per repo, adn this will also make it easier to transition one application/pipeline at a time.
When I started at my company in 2014, I inherited several VB6/.NET apps that used Visual SourceSafe for version control. I soon migrated everything to a local TFS server. I was connectivity issues with it, so I migrated to TSVS cloud about a year later. After the Azure DevOps name change and some issues with my Microsoft account, I decided to test drive a Git repository using Atlassian BitBucket as the server and Sourcetree as the local Git application, which I chose over GitHub after comparing the benefits for small teams. &amp;#x200B; So I'm currently still using the Git method and loving it, much more so than TFS/TSVS/Azure. On Microsoft's source control, I used to have issues with projects that were added to TSVS then later removed, but if I would start a project over and try to add it back to source control with the same name, the server would throw a fit. Lots of issues like that. I got fed up with it after the DevOps name change and moved to Git. So far, it's much easier to manage all-around. Also, I feel like Microsoft's product was more difficult to manage security, which I never fully figured out. Overall, it just seemed bloated and over-complicated. Maybe it's not so bad, but I didn't want to use my time to learn the ins and outs for my small team that just wanted to push and pull code. &amp;#x200B; Long story short, I much prefer the Git source control method over Microsoft's ever-name-changing source control. It seems better for small teams with a simpler security system and easier code management.
For small (private/hobby/single developer) projects I a private github repo. For serious projects (at work) we use Azure DevOps to manage the workflow (mostly scrum), store our "blessed" repository and create PRs. We also use the build and release pipelines from azure devops to automatically deployed, release and test stuff. IIRC you can try (with a few limitations) Azure DevOps for free.
Yeah we're a small shop, and don't have any CI/CD stuff in place at the moment. Implementing CI is something we want to start doing, and hopefully upgrading our system will make it much easier to do that.
You can use git in Azure DevOps
JavaScript has threads, service and Web workers are threads. They're threads. It also has fully asynchronous IO, which is the most important thing. Synchronisation is done through message passing, which is what the most parallel languages do anyway because managing shared memory state is really horrible. And no, they don't use processes because they can't thread, they use processes because processes and memory isolation are the best model for what it's trying to solve. Visual Studio uses separate processes for much the same reason. Threads aren't magic, they're one way that you can solve concurrency. Outside of Windows, multiple processes have **always** been more common. Semaphores and locks are an awful way to do concurrency. Look at elixir or Rust, or any of the other languages that perform incredibly well in parallel and they don't do concurrency that way. Skype was in a situation where the Windows client was OK and the client on every single other platform was dogshit. That's why they moved to electron, because being able to deliver the client on something other than Windows at a reasonable quality level was more important. Just because Microsoft uses native controls in one of their apps doesn't mean they care about them. Most of the new software they're writing doesn't. You're writing software the way it was written twenty years ago and you're viewing anything that doesn't work that way as inferior which is pretty much everything.
We just started the update from TFS 2012 on-prem to Azure DevOps Services, and we also use TFVC. The move to Azure DevOps doesn't seem like it's going to be bad, but we *just* started so we're not too deep into it and we also have corporate support doing most of the heavy lifting. From what I can tell, that move will be fairly trivial and worth it to keep up to date with the latest that Microsoft offers. Even with that though, it sounds like your problems are mostly with TFVC so this might not help a whole lot. &amp;#x200B; Moving from TFVC to Git is a different story. We've talked about it because everyone here hates TFVC, but our code base is huge enough that that transition gets pushed off in favor of other projects with a higher priority.
&gt; Outside of Windows, multiple processes have always been more common. I don't care. On Windows such design is shit, period. &gt; Semaphores and locks are an awful way to do concurrency No, it is essential to make true concurrency. The only issue are clueless (web) developers who are unable to "think parallel". Message passing is a poor way that have nothing to do with true parallel processing.
Yeah it comes with 5 free users. We also have MSDN subscriptions (if that's even what they're still called), and those come with the 'basic' tier on Azure DevOps for free. We're not taking full advantage of those licenses for sure, and it's something I'm hoping to remedy.
When I started at my current position last summer, we had all of our repositories set up in Bitbucket. We didn't really do any workflow management. Since then, we've slowly been migrating them over to Azure DevOps Services, and we've starting managing our work more formally and taking advantage of DevOps' Pipelines feature. Git is the de facto industry standard for source control, so I would highly recommend moving your repositories over to it if you can. You'll have a wider range of options as far as actual repository hosts go, and there's a whole lot more community support for issues with Git. And like you said, the people who made TFVC are phasing it out, so there's really no reason to cling to it at this point. I don't know your manager, but if it were mine I was trying to convince, I'd identity tangible ways Git could enhance our workflow now and in the future, thus saving him and his bosses money.
Yeah it seems like if you do switch from TFVC there are tools to help you migrate repositories, but even MS doesn't recommend you bring your complete history over. So it seems like if we would want to keep our history before we switched to git, then we would have to keep our on-premise server running just for that. Not what I would consider optimal.
The decoupling is ultimately needed but it will hurt like hell for the transition, our desktop team wasn't too happy either at first until realizing that in the long run this will be better overall if we'd want to see XAML moving forward and not stagnate (it was already stagnating from what I could feel). I feel like this is gonna be like how we migrated all apps from winforms to WPF and to UWP. It's gonna cause headaches, but hopefully the WinUI 3 port would be the only big breaking change after this. And also hope that the migration path is as simple as namespace changes, would appreciate a migration tool to boot that does this and checks compatibility. I always felt the Windows OS release cycle really held back XAML from innovating as it's locked to every 6 months for improvements to show up. This time if this takes off with it's decoupled design and independent release cylce it could do alot more. Kinda like how when Edge went chromium their feature releases became quite quick. For an OS the slow release cycle made sense but for a component / UI framework it didn't. I'm kinda hopeful that since the renderer would most likely be decoupled from XAML itself it might open up Xplatform capabilities depending on how hard it is to make a different renderer for it, be it MS sponsored or community made. And maybe improvements like not having to make a lot of converters for simple tasks like bool negation, bool to Enum etc...
That was the biggest thing holding us back as well, there didn't seem to be optimal ways to keep history. On top of the learning curve for those who haven't used it and rebuilding our CI pipelines, it didn't feel like it was worth it at the time since we're not having many troubles with TFVC, it's just annoying sometimes.
Elixir can handle hundreds of millions of connections without the slightest problem. It's built on the Erlang OTP which was literally written to run phone exchanges. It uses message passing, and it's about as far from *clueless* Web developers as you can get. The reason being is that the very best way to solve concurrency problems is to not have shared state at all if you can help it. You know incredibly little for someone with such strong opinions.
&gt; The reason being is that the very best way to solve concurrency problems is to not have shared state at all if you can help it It helps to make it easier but not better. &gt; You know incredibly little for someone with such strong opinions. Yes, I use Win32 thread synchronization objects since 1996 only ;-)
[removed]
I think the learning curve for others will be the only factor holding us back. We don't currently have pipelines set up, but researching and figuring that out is part of this project I'm heading up.
You've listed a lot of complaints but they're mostly minor nitpicks except for "terribly slow" which isn't necessarily true. Visual Studio Code is one of the fastest and most responsive Windows IDEs available right now. Sure, it takes a lot of memory, but have you opened Visual Studio the native app recently? \_Everything\_ takes a lot of memory nowadays.
&gt; Everything_ takes a lot of memory nowadays No. My services running 24/7 processing hunreds of connections takes 100 MB VM max. Why? Because it is written properly :-)
\&gt; No, it is **essential** to make true concurrency. The only issue are clueless (web) developers who are unable to "think parallel". Message passing is a poor way that have nothing to do with true parallel processing. The result are terribly slow and resource hungry applications just because of clueless developers using improper tools This comment is particularly weird because "true parallel processing" would seem to indicate that two threads are operating at exactly the same time, which means they are not waiting for each other with locks or semaphores... There are certain places where locks can be a good solution, but the more cores your processor has and the more threads your app has, the fewer those places are.
&gt; which means they are not waiting for each other with locks or semaphores You don't have to always lock/wait when access shared data. That's why non-blocking concurrent collections and similar object exists. You just have to understand things inside the modern CPUs. Small locked blocks are still better in many cases than tons of wasted memory resources to make copies of data because you aren't capable to handle the shared access properly. All of this it is completely beyond scope of simple typeless scripting JavaScript language and runtime :-)
&gt; I am glad I don't have to use it :-) Sure, whatever, use what you like. &gt; My services running 24/7 processing hunreds of connections takes 100 MB VM max. Does your 100 MB service have a consistent GUI across three or more OS platforms, a programmable plugin system, a text editor with multi-select support, and an embedded terminal? If so, I'd like to hear more about it.
No, you always have to target the particular plaform. Cross-platform development never works and won't deliver *proper* results.
K. I mean, it does work, and we have specific examples of it working, but I guess you can ignore those if you want. You are also welcome to have your own personal opinion of what is "proper," but it would be useful for you to understand that it's just your opinion and many people don't agree with you.
&gt; many people don't agree with you Yes, and I don't care. For me it is sad to watch how current software development turned into total crap just because clueless incapable people makes software using improper tools. We have very powerful hardware with great specs but the user experience is worse and worse.
&gt;Yes, and I don't care. Maybe if you cared a little more you would learn something. The world has moved on. The sad thing is, *you* are the one who is likely using improper tools. Enjoy whatever it is that you enjoy, but personally I am going to enjoy learning new technology and using and promoting (some) electron-based apps and other modern tools that are better at quickly delivering better user experiences and good performance on modern hardware.
What are you talking about? UWP is still being improved and supported.
&gt; hat are better at quickly delivering better user experiences and good performance on modern hardware. Definitely not the Electron. And it never will, it is easy. I am not going to learn something that is 20+ years behind, scripting language is good for web but that's all. I'd rather quit the industry than use such crap.
That's not quite true. https://twitter.com/gcaughey/status/1129043912595787777?s=17
Except that MS doesn't treat F# as a first-class citizen. https://www.reddit.com/r/fsharp/comments/6tdrwq/after_so_many_years_still_no_net_native_nor/
I'm aware of that, but I had enough bad experiences that I wanted to move on.
NuGet feed support was added to TFS2017. Upgrade, or use a file share.
Why does it not surprise me that you've been doing exactly the same thing for almost 25 years without any change.
No, I adopt new things, if they are better or even on the same level. But it isn't the case of JavaScript (over .NET, Java, C++) nor Electron.
I hadn't read any of the rebuttal's to that original article. Thanks for sharing.
This is a really shaky premise to base a claim that MS doesn’t care about F#. I can also claim that MS doesn’t care about C# because it doesn’t have type providers.
That's awesome! Nice work.
If the query is too broad, then it will hit AD. In the query, use the LocalAccount property and set it to true. https://docs.microsoft.com/en-us/windows/desktop/cimwin32prov/win32-useraccount#properties
We published our examples in GitHub. This is quite simple, but I hope that is useful. [https://github.com/asc-lab/java-cqrs-intro](https://github.com/asc-lab/java-cqrs-intro) [https://github.com/asc-lab/dotnet-cqrs-intro](https://github.com/asc-lab/dotnet-cqrs-intro)
IMO could be useful. Did you see this example: [https://github.com/asc-lab/dotnet-cqrs-intro/tree/master/SeparateModels](https://github.com/asc-lab/dotnet-cqrs-intro/tree/master/SeparateModels) ? You can try improve this example and check that is good pattern for your problem.
I agree with you. Part about ES is really simplify, could be too simplify. In this article we focused on CQRS, we just wanted to mention ES and expand the topic in another article. Maybe it's our mistake. Your comment is very valuable, thanks!
UWP and Windows On ARM are a long-term projects. You can't need WOA devices when WOA isn't ready yet, it needs more optimization and UWP needs to mature more. About the tablet market died, it isn't true, the Windows tablets market is very active today, more than Android, more yet in Chinese low-end tablets like the Nuvision and Chuwi.
I think XAML is done compared to the holograph API. I never tried mixing the two.
Don't forget Edge + Chrome, that is be big for UWP too.
Qt is pretty darn good at cross-platform development.
Unfortunately there won't be any interest to make any UWP apps then :-/ Depends on what tablets. It is rather bigger 12" 2-in-1 devices with detachable keyboard than 8" mobile one with celluar internet connection. You can more or less use desktop applications on it while the smaller ones needs UWP touch screen optimized apps and this is what has already died on Windows.
I have a 7' tablet and can run Win32/UWP without issues. In the other hand, give to Win32 features from UWP is part of the strategy.
It can run but there is no way to use a Win32 app on a 7" touch screen :-)
Qt is definitely much better option than Electron because it is based on mature technology, but I don't care about cross-platform development. Native app for each platform is the only good solution. Yes, you can't save on development resources :-)
Just a heads up, Microsoft uses GitHub. I use git as well for small, weeklong, and large production style projects. One of them consist 3 teams and 8 containers in docker. We all use git for this. I work almost exclusively on Macs but it's a complete preference now. Everyone works with Linux though. So I dunno. My team lead is on Windows.
Having used Git and Mercurial for a decade now and TFS, SVN and VSS before that, I can say: * If you aren't using Git or Hg, you are suffering. * Git and Hg are at exactly the same level and neither has a technical feature improvement over the other. * Git is more common and more software integrates with it, particularly VSCode and VS. * I barely use the integrated features with Git. * Centralized Version control is orders of magnitude slower at reviewing changes, comparing revisions, merging, updating to historical revisions, branching, and identifying if you have made any changes locally. We upgraded from SVN to a single "monorepo" Hg a decade ago, importing all our changeset history into Hg (30k changes). It was painless, but useful for about 6 months because at the same time we were working on a new application and the old one was being EOLed. We are going to switch from Hg to Git in the next year or so because part of our company is using VSS still and will be upgrading to TFVC hosted on DevOps (and that is as close as I am willing to get to their madness) and there is a desire to move all source code, testing, issue tracking, and CI into one platform in order to reduce the load the current system has on internal audits. As part of this move I'm going to migrate our Hg history into a Git repo (I've already done this, there is an Hg extension that lets Hg work with Git repos since the wire protocol is 1:1 translatable; we are able to seamlessly push and pull with a Git server and an Hg client). However we are going to mothball this repository when we make the switch and not push it to DevOps, instead pushing a single/small number of revisions containing just the current working copy and eliminating 10GB+ of history. I would recommend: 1. Get (migrate) all of your historical commits in a git repository so you can reference it if you need to. 2. Put this on a long term storage somewhere where you can access it (eventually it will be useful maybe once a year or so) along with your current bug tracking database 3. Start fresh in DevOps, fixing your username policy, project structure, issue tracker structure, whatever else. 4. Create 2 powershell scripts in the repository root: Start.ps1 and Build.ps1. Start.ps1 should be a file you can put on a new machine(without already having the repo; ie. fileshare this file and run it) and it does everything you need to have done in order to fully configure that machine to work on the code. Build.ps1 should orchestrate every build/test process in the repo and then exit with appropriate warnings/errors or success code (this is the 1st step to CI). 5. Create a Disaster Recovery plan that involves being handed a fresh VM on the "oh shit" box and ends with a developer being able to get a change out of a workable dev environment. If that plan is not "1. get Start.ps1; 2. run Start.ps1; 3. (go for a walk... then) make the change," Improve your plan. 6. Once a month a random dev should be selected to perform the DR plan. Everyone should be able to do it.
NativeScript has potential. Javascript but Native compiled
Make something. Anything. If it applies at all to you that you can use. It will likely stick much more than normal.
great stuff this is something i need to solve with sql server, will this work on linux? as in does sqllite work on linux containers, im going to be using centos. few questions for you, if im developing a .net core api and putting into a docker container. im also developing an angular app to consume the services of that api. im thinking they should go in the same docker container. or should they be in two? do you know of any good resources on how to wire this up properly?
Azure dev ops and git
Why not put the historical repository in Azure DevOps too? You can keep it as a separate repository and start fresh with your new working copy. That way you don't have to worry about maintaining that 10 GB blob on-premises.
You could. I noticed with our svn history I looked at it a bunch the first year or so and then pretty quickly didn't use it anymore because of how much harder it was to understand. It was useful that first year to have locally on our machines because we could update to revisions and then search the working copy for stuff and follow history back more. Searching on a server copy is a bit annoying so you probably want it local regardless of if you put it on devops for a while. Then once you stop wanting it locally, you stop caring for having it on the server. Our svn source was pretty hard to traverse without following paths from source to bugzilla and back to source. I expect that is pretty common so if you break those links you would quickly wind up with history that is difficult to track the reasons for doing things.
If it's truly throwaway, then that makes more sense. We unfortunately need to keep the source longer for legal reasons, so having it on a server so it doesn't get lost is important to us. (We're actually going to end up with *three* historical archives soon - state of VSS at the time of TFS conversion; TFS archive #1, because my predecessor chose not to start over when moving to TFS 2012 instead of upgrading; and TFS archive #2 because Azure DevOps doesn't allow importing multiple collections yet...)
You might adopt new things, but you're not adopting new ideas. You can only see the benefit in technologies that let you develop exactly the same way you ways have. The industry is moving away from monolithic single process executables. Even Visual Studio doesn't work that way anymore and it's moving to **more** processes, not fewer. For a dozen reasons, including security, performance, and flexibility. Office is changing and may not even have a thick client in a few years. Cross platform is basically a necessity now. When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous.
You might adopt new things, but you're not adopting new ideas. You can only see the benefit in technologies that let you develop exactly the same way you ways have. The industry is moving away from monolithic single process executables. Even Visual Studio doesn't work that way anymore and it's moving to **more** processes, not fewer. For a dozen reasons, including security, performance, and flexibility. Office is changing and may not even have a thick client in a few years. Cross platform is basically a necessity now. When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous.
The reason why I don't recommend keeping your complete history is because you will lose historical context of why changes were made if you are unable to move your issue tracking history completely as well. Even if you are able to move your issue tracking history, if you break links between your source history and the issue tracker, the work to follow changes and discussions between them becomes annoying. Then you are left with a bunch of dead weight that just makes it take longer to initialize a new dev environment or build server. If you are able to migrate your changes without increasing the amount of work it is to determine the context of whatever random bug you find from years ago, then by all means keep the history. If you can script a graph walk of TFVC, you can script a full import. That is probably the easiest part of the conversion to care about.
The reason why I don't recommend keeping your complete history is because you will lose historical context of why changes were made if you are unable to move your issue tracking history completely as well. Even if you are able to move your issue tracking history, if you break links between your source history and the issue tracker, the work to follow changes and discussions between them becomes annoying. Then you are left with a bunch of dead weight that just makes it take longer to initialize a new dev environment or build server. If you are able to migrate your changes without increasing the amount of work it is to determine the context of whatever random bug you find from years ago, then by all means keep the history. If you can script a graph walk of TFVC, you can script a full import. That is probably the easiest part of the conversion to care about.
You might adopt new things, but you're not adopting new ideas. You can only see the benefit in technologies that let you develop exactly the same way you ways have. The industry is moving away from monolithic single process executables. Even Visual Studio doesn't work that way anymore and it's moving to **more** processes, not fewer. For a dozen reasons, including security, performance, and flexibility. Office is changing and may not even have a thick client in a few years. Cross platform is basically a necessity now. When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous.
If it's truly throwaway, then that makes more sense. We unfortunately need to keep the source longer for legal reasons, so having it on a server so it doesn't get lost is important to us. &amp;#x200B; (We're actually going to end up with \*three\* historical archives soon - state of VSS at the time of TFS conversion; TFS archive #1, because my predecessor chose not to start over when moving to TFS 2012 instead of upgrading; and TFS archive #2 because Azure DevOps doesn't allow importing multiple collections yet...)
The reason why I don't recommend keeping your complete history is because you will lose historical context of why changes were made if you are unable to move your issue tracking history completely as well. Even if you are able to move your issue tracking history, if you break links between your source history and the issue tracker, the work to follow changes and discussions between them becomes annoying. Then you are left with a bunch of dead weight that just makes it take longer to initialize a new dev environment or build server. If you are able to migrate your changes without increasing the amount of work it is to determine the context of whatever random bug you find from years ago, then by all means keep the history. If you can script a graph walk of TFVC, you can script a full import. That is probably the easiest part of the conversion to care about.
If it's truly throwaway, then that makes more sense. We unfortunately need to keep the source longer for legal reasons, so having it on a server so it doesn't get lost is important to us. (We're actually going to end up with \*three\* historical archives soon - state of VSS at the time of TFS conversion; TFS archive #1, because my predecessor chose not to start over when moving to TFS 2012 instead of upgrading; and TFS archive #2 because Azure DevOps doesn't allow importing multiple collections yet...)
If it's truly throwaway, then that makes more sense. We unfortunately need to keep the source longer for legal reasons, so having it on a server so it doesn't get lost is important to us. (We're actually going to end up with *three* historical archives soon - state of VSS at the time of TFS conversion; TFS archive #1, because my predecessor chose not to start over when moving to TFS 2012 instead of upgrading; and TFS archive #2 because Azure DevOps doesn't allow importing multiple collections yet...)
I read visual novels there, and they're programs Win32-based. Your argument is invalid (?)
I read visual novels there, and they're programs Win32-based. Your argument is invalid (?)
I read visual novels there, and they're programs Win32-based. Your argument is invalid (?)
You might adopt new things, but you're not adopting new ideas. You can only see the benefit in technologies that let you develop exactly the same way you ways have. The industry is moving away from monolithic single process executables. Even Visual Studio doesn't work that way anymore and it's moving to **more** processes, not fewer. For a dozen reasons, including security, performance, and flexibility. Office is changing and may not even have a thick client in a few years. Cross platform is basically a necessity now. When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous.
Indeed... pretty slick. I am impressed how straight forward blazor looks. I have been away from programming for a bit and starting to get back into asp. This is pretty exciting.
You might adopt new things, but you're not adopting new ideas. You can only see the benefit in technologies that let you develop exactly the same way you ways have. The industry is moving away from monolithic single process executables. Even Visual Studio doesn't work that way anymore and it's moving to **more** processes, not fewer. For a dozen reasons, including security, performance, and flexibility. Office is changing and may not even have a thick client in a few years. Cross platform is basically a necessity now. When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous.
&gt; For a dozen reasons, including security, performance Can you elaborate how the split to multiple processes improves the performance? To create a new process is **very** expensive operation, compared to a new thread. &gt; When you're using synchronisation primitives you're doing message passing via shared state, and every time you hit one your code becomes synchronous. No, you don't understand basic things. Recommended reading [Concurrent Programming on Windows](https://www.amazon.com/Concurrent-Programming-Windows-Joe-Duffy/dp/032143482X)
Visual Studio 2017 is much slower than 2015 because you're running Resharper and Resharper runs in the main process which is grinding Visual Studio to a halt. If you uninstall Resharper or when Jet Brains finally moves into its own process (or onto Roslyn which is already in another process) it will improve. And no, I understand just fine. Shared state allows communication between threads, that's **literally** what it does. If you're sharing state between two threads and it's not being used to communicate data between them you're doing it wrong. Shared state just allows you to do it more sloppily. And yes, synchronisation primitives create synchronous code. When you put a lock around a section, that section is synchronous, that's what the lock is for. And a book from 2005 is not state of the art. In 2005 process forking in Windows was slow, and no one gave a shit about security or even particularly robustness. The kernel has been rewritten several times since then and the world is different. Running processes instead of threads isn't a massive performance hit anymore on Windows and it never was on Linux. Security of your code, especially if you've got any kind of plugin architecture is much more important than it was in 2005. The world has changed and you are a dinosaur.
&gt; Visual Studio 2017 is much slower than 2015 because you're running Resharper I don't use any Resharper. It is slow because of the silly multi-process design. &gt; When you put a lock around a section, that section is synchronous, that's what the lock is for. No, you don't need to always lock when access shared data, that's my point you are still missing. &gt; And a book from 2005 is not state of the art. Of course, because it is out of scope of script kiddies :-) &gt; The world has changed and you are a dinosaur. So why all these "better new ideas" delivers worse results only?
Visual Studio 2017 is much slower than 2015 because you're running Resharper and Resharper runs in the main process which is grinding Visual Studio to a halt. If you uninstall Resharper or when Jet Brains finally moves into its own process (or onto Roslyn which is already in another process) it will improve. And no, I understand just fine. Shared state allows communication between threads, that's **literally** what it does. If you're sharing state between two threads and it's not being used to communicate data between them you're doing it wrong. Shared state just allows you to do it more sloppily. And yes, synchronisation primitives create synchronous code. When you put a lock around a section, that section is synchronous, that's what the lock is for. And a book from 2005 is not state of the art. In 2005 process forking in Windows was slow, and no one gave a shit about security or even particularly robustness. The kernel has been rewritten several times since then and the world is different. Running processes instead of threads isn't a massive performance hit anymore on Windows and it never was on Linux. Security of your code, especially if you've got any kind of plugin architecture is much more important than it was in 2005. The world has changed and you are a dinosaur.
&gt; Visual Studio 2017 is much slower than 2015 because you're running Resharper and Resharper runs in the main process which is grinding Visual Studio to a halt. I don't run any Resharper, it is slow because of the silly multi-process design and tons of JS crap. &gt; And yes, synchronisation primitives create synchronous code. No, you don't have to always lock when accessing shared data, that's the point you're still missing. &gt; And a book from 2005 is not state of the art. In 2005 2008. Well, it is out of scope of script kiddies :-) &gt; or even particularly robustness. Robustness was done by code quality. Has it become a problem lately? &gt; Security of your code, especially if you've got any kind of plugin architecture is much more important than it was in 2005. What's the point, if all the unnecessary processes are running in the same security context? As for plugins, that's what AppDomain is designed for. &gt; The world has changed and you are a dinosaur. So why all these "better new ideas" delivers worse results only?
&gt; Visual Studio 2017 is much slower than 2015 because you're running Resharper and Resharper runs in the main process which is grinding Visual Studio to a halt. I don't run any Resharper, it is slow because of the silly multi-process design and tons of JS crap. &gt; And yes, synchronisation primitives create synchronous code. No, you don't have to always lock when accessing shared data, that's the point you're still missing. &gt; And a book from 2005 is not state of the art. In 2005 In 2008. Well, it is out of scope of script kiddies :-) &gt; or even particularly robustness. Robustness was done by code quality. Has it become a problem lately? &gt; Security of your code, especially if you've got any kind of plugin architecture is much more important than it was in 2005. What's the point, if all the **unnecessary** processes are running in the same security context? As for plugins, that's what AppDomain is designed for. &gt; The world has changed and you are a dinosaur. So why all these "better new ideas" delivers worse results only?
You don't have to lock if you're accessing immutable shared data, but you don't need synchronisation primitives to do that either, if you're accessing mutable ones then you do. And my point about that book is that it's based on best practice for programming Windows XP. The author started writing it in 2005 and it's based on state of the industry at that time. At that time, process forking in Windows was **dramatically** slower than spawning a new thread. This isn't the case anymore. Robustness is not done by code quality, that's the height of insanity. Even if you personally wrote all the code, your code has bugs. You create robust code by coding defensively. This includes moving components which are more likely to crash into a separate process so they don't take down the application. Robustness is a function of design, not of the delusional belief your code is perfect, because it's not. On security, the point is that those processes don't have access to each other's memory. That means that if you've got plugins or any other kind of uncontrolled code or data can be isolated from each other. Threads don't work for that. And again, they don't deliver worse results.
&gt; Robustness is a function of design Yes, so separare it to processes won't resolve anything. &gt; On security, the point is that those processes don't have access to each other's memory. Again, AppDomain. &gt; And again, they don't deliver worse results. So the Skype 8 versus 7 version having half of features, consuming seven times more system resources and rendering blurry fonts is better ?
There's still alot of room to modernize on the XAML dialect itself and trim some boiler plate and verbosity to make it cleaner especially when up against current UI frameworks. I see alot of good feature-requests coming in that's in plan for WinUI 3 on their github page, and their looking for inputs on their 3.0 roadmap. You might wan't to contribute your sentiments on what to innovate there as well. As for mixing the two e.g. Holographic mode and 2D Xaml mode it's possible now via a Unity generated UWP csproj solution but requires alot wire up on transitioning between two modes. There's been feature requests there as well for a 3D Model component to ease this problem at least. I'm actually a bit optimistic now that it's planning to decouple from Windows OS release cycles and is open source as we can now track and contribute as needed. Though I guess there's no choice but to bite the bullet on the transition on our current UWP apps to WinUI and hope on smooth sailing moving forward.
Your argument was that JavaScript threading wasn't proper threading because it lacked synchronisation primitives. Now you're trying to argue for immutable state, which doesn't require synchronisation primitives. Separate processes is **part** of design. It's not a poor fix for bad implementation, it's a design. And Skype 8 was built for a completely different purpose than Skype 7. It lacks features because Microsoft doesn't want it to have those features anymore, not because electron can't. It's also **one** product you keep harping on. And that reference is to enabling clear type even if it's off at the system level. Chrome supports clear type just fine if it's on.
No, you don't have to lock for concurrent reads, just for writes. That's what [ReaderWriterLockSlim](https://docs.microsoft.com/en-us/dotnet/api/system.threading.readerwriterlockslim) and similar are for. And it is still the simple solution. Better than make hunderds of data copies. So it is a bad design. No, Chrome doesn't support ClearType at all (unlike Firefox), it renders ugly blurry fonts.
I had to stop very early in the article - Domain models aren't DTOs, they should have all the business logic in them if you are using object oriented language, and shuffling data back and forth between db shouldn't even be on the list of concerns when designing domain objects. While domain and persistence model can composed by the same objects, it is by virtue of infrastructure that allows pure mapping (such as EF and NHibernate does, in a way that doesn't force a dependency upon the model). What you've described is a well known anti-pattern, known as anaemic domain model, which breaks SOLID principles and is a poor advice to recommend to build domain model this way. Common perception is that classes, that are looking virtually the same when you take a look at their properties, are violating DRY principle - which is not the case, as the objects have different responsibilities - for example it's quite common to add property to DTO that isn't in the domain class, or make changes to the persistance (which shouldn't impose changes to your domain model). Someone mentioned that separation of concerns is sometimes a wrong approach - I would argue it's never wrong, it's just misunderstood (in the terms of what concerns are separated) or improperly applied. It's of course more work for something that may not justify it, but that doesn't mean it's wrong.
Perhaps books such as C# in Depth, CLR via C#, Effective C#, Pro .NET Memory Management and similar.
Thanks. I have read the first two, I think there is a drop (or two) in abstraction I need to get to that isn’t covered in c# books.
What are you looking for specifically?
You keep saying it's better than making data copies, but why is it better than that? Memory is cheap, memory copying is cheap and checking the status of a lock is, in and if itself, a mutex event, so even if you can read code concurrently, you can't check if you can read it concurrently. The win32 synchronisation primitives also play really poorly with async code, because they weren't designed to and mutex code is really, really complicated under the hood. You have a particular design model in your head, but it's a design model based on assumptions that simply aren't true. Skype 8 is bad for you because Microsoft doesn't give a shit about people making free video calls. They care about people being able to do calls into their enterprise comms server on whatever device they want. The non Windows Skype clients were awful, especially on Windows, and now they're not, because of electron.
You keep saying it's better than making data copies, but why is it better than that? Memory is cheap, memory copying is cheap and checking the status of a lock is, in and if itself, a mutex event. Edit: And again Chrome does support clear type and has supported it for years. I know you're obsessed with it not being there, but it is. You linked to a request that had to do with activating it when it's off at the system level, not with having it at all.
All 4 of the books you listed do not contain a single word about platform intrinsics.
Off course it is. I'm assuming your company is paying for the exams so it's a no brainer. Having them in your CV is always a plus. I'm not saying having certs will change your life but you always learn something while studying for them. BTW your attitude is quite bad. "Free shit"... come on. Looks like your totally disconnected from your companies well being. I was/am grateful for my company to pay for the exams and give me an hour or two off for passing them. We have totally different attitudes.
There should be an incentive in it for you. Check if the company will up your wage for completing the certification. Also, make sure they're covering the cost of any materials and exams.
lol it was a quick off the cuff post. It was more a colloquialism than bad attitude. Either way I've edited that part away because it detracts from the purpose of the post. Thanks for the input. Upvoted.
I got mine earlier this year because the company needs it for partnership. On the job hunt now and no one has commented or asked about it.
Have a look at [this](https://github.com/aspnet/AspNetCore/issues/5833) discussion.
I really liked this website: https://software.intel.com/sites/landingpage/IntrinsicsGuide/ It uses the C++ terminology but it’s the same material.
Yep, that Intel guide really is the most complete documentation out there for what's available. The .NET intrinsics map 1:1 to those native intrinsics for the supported ISAs. You can find the mappings in the .NET source code here: [https://github.com/dotnet/corefx/tree/master/src/Common/src/CoreLib/System/Runtime/Intrinsics/X86](https://github.com/dotnet/corefx/tree/master/src/Common/src/CoreLib/System/Runtime/Intrinsics/X86) This doc might also be helpful for understanding the bigger picture of how it all works: [https://github.com/dotnet/designs/blob/master/accepted/platform-intrinsics.md](https://github.com/dotnet/designs/blob/master/accepted/platform-intrinsics.md)
That is a good point, I didn't think about the Unity wire up issues -- but sounds possible. Now if Unreal was compatible with hololens. Oh, would you look at that https://www.unrealengine.com/en-US/blog/epic-games-announces-unreal-engine-support-for-microsoft-hololens-2 Lots of ideas now.
You can try something like https://github.com/phatcher/CsvReader/blob/master/README.md to do the underlying CSV reading, then use reflection to find all the properties of a POCO type and try to map each incoming row to the POCO properties.
Is there something particular in "reversable encryption" that I don't know about, or you just calling encryption the "reversible" for some strange reason?
Well the 'reversible encryption' is the easy part something like Bouncy Castle [https://www.nuget.org/packages/BouncyCastle.NetCore/](https://www.nuget.org/packages/BouncyCastle.NetCore/) works well on .NET core. Really 'reversible encryption; just means 'not a hashing function'...otherwise it's almost exactly functionally equivalent to writing random data to disk. Not sure what else you need; are you looking for some sort of 'secret store'?
There are many approaches - you can go all in, use event sourcing and EventStore, cache projections in Redis or something similar, store projections in nosql, use materialized views for projections, it really depends. If you don't go with event sourcing (which is rather non-trivial to implement properly, especially for the first time), I personally like SQL for write, after which you update read side NoSQL with projection or cache the result. Simplest to implementation you can use to dip your feet in IMO is using materialized views (IIRC they are called indexed views in MSSQL) and use those to provide for read side. It honestly really depends on how likely are your projections to change, how much effort are you willing to spend to make it performant (performance experiences kinda logarithmic growth when correlated to complexity). https://cqrs.nu/Faq may answer some of your further questions ^^
[ASP.net](https://ASP.net) Core Data Protection?
Thank you very much for detailed answer :D
I think it's pretty clear from the context that OP meant "symmetric".
&gt; What you've described is a well known anti-pattern, And by far the best anti-pattern I've ever used.
Excellent. Thank you
There is not, and likely will never be, cross-platform DPAPI support. https://github.com/dotnet/corefx/issues/22510 There is also not a library providing similar cross-platform functionality that I am aware of (I certainly could be unaware). You will likely have to determine your own solution for other platforms and create your own API to provide a facade over various platform implementations.
Excellent. Thanks
Are you talking symmetric vs asymmetric encryption?
With Oracle changing the licensing model for java recently in a very predatory way its going to cause companies to look for alternatives
Lol ado... now that's a trip down memory lane
Symmetric, but specifically a facility similar to DPAPI where our the key would be unknown to the application and the ciphertext useless even when an attacker gets the application and all of its configuration. This app also is not intended to run in Azure.
Saw that, but did not see anything similar to DPAPI where key management is not in scope of the application.
DPAPI is an API that allows you to store secrets with encryption and key management completely handled by the OS outside the application (on Windows it uses keying material derived from the process user account's login password; if you get the ciphertext etc., the data is safe unless you also get the password and context from this particular system; key is specific to this particular user on this system).
Symmetric encryption is technically what DPAPI provides, but it provides more than that because the key management and protection bit is completely handled by DPAPI and thus not something your app has to worry about. Reversible encryption is actually a term commonly used in these scenarios. I am not looking for any symmetric crypto API but specifically for functionality similar to DPAPI.
...with no requirement to ever handle keys...
Sounds like your looking something to this https://en.m.wikipedia.org/wiki/Homomorphic_encryption
**Homomorphic encryption** Homomorphic encryption is a form of encryption that allows computation on ciphertexts, generating an encrypted result which, when decrypted, matches the result of the operations as if they had been performed on the plaintext. Homomorphic encryption can be used for privacy-preserving outsourced storage and computation. This allows data to be encrypted and out-sourced to commercial cloud environments for processing, all while encrypted. In highly regulated industries, such as health care, homomorphic encryption can be used to enable new services by removing privacy barriers inhibiting data sharing. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Desktop link: https://en.wikipedia.org/wiki/Homomorphic_encryption *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^257936
No. Symmetric encryption with key management handled by the API, not the application. Symmetric encryption requires that the key used to encrypt be kept safe. With DPAPI that key generation and protection is a function of the OS + library. It looks like .NET Core has no similar functionality.
On second look, it looks like a lot better and is the equivalent! Unfortunately to use it as transparently as DPAI it actually chains down to DPAPI and therefore has a Windows dependency... but it can also use a certificate to generate the keys for key encryption at rest. The docs are unclear about this but it probably uses the private key to derive the master key. On some OSes that can be protected a little better than a plaintext secret lying around in the file system.
It can use DPAPI itself, or Azure keyvault, and probably more. Pluggable backends.
It does not use the certificate to generate keys. It generates its own keys and then uses the certificate, or dpapi, or azure key vault to protect the keys it generates before they are stored. Both protection and storage are customizable and you can write your own providers. Thing is you can’t be transparent cross platform like dpapi is on Windows because Linux has no os support for this, or even protection of private keys. It’s pretty fully documented including the key process so you may want the read the docs before ruling it out.
If it's truly throwaway, then that makes more sense. We unfortunately need to keep the source longer for legal reasons, so having it on a server so it doesn't get lost is important to us. (We're actually going to end up with *three* historical archives soon - state of VSS at the time of TFS conversion; TFS archive #1, because my predecessor chose to start over when moving from TFS 2008 to TFS 2012 instead of upgrading; and TFS archive #2 because Azure DevOps doesn't allow importing multiple collections yet...)
That's exactly what I wrote.
Yes I'm aware of what it is...
We keep ours (the final state of the svn repo, and the hg repo when we move to git), it is on a tape and at a secure 3rd party backup provider along with a vm we can boot to look at it if we need to. We'll keep the bugzilla database as well, but configuring an environment to make bouncing back and forth between them is not trivial. And then there are tens of thousands of xml files containing build artifact logs from ccnet, each with dates and references to both bugzilla and hg/svn that you could peruse... in zip files by year. If you so desired and were determined enough you could figure out exactly when a code change made it into any environment, what other changes went with it, what bug it was fixing, descriptions of how it was tested and so on for any change that has ever been made since we started using svn. I can't say we have attempted to retrieve the svn instance in a few years. Source code snapshots also exist of the current production release branch each month (I think we keep those for 5 years or something) as dumb zip files in a shared escrow with some of our clients for in case we close doors. The data is all there and theoretically could be used at any point in time. It just isn't very useful to have the old source history online without maintaining the infrastructure that was related to it.
I'm guessing if(!isUpdated) &amp;#x200B; is actually true. Put a breakpoint on the isUpdated = true; line and see whether it's hit before the if line.
We're posting this again? It's only been a week. https://www.reddit.com/r/dotnet/comments/bp4br3
Obtain your access token and then use HttpClient to attach it in the header of each request (eg, see https://stackoverflow.com/questions/14627399/setting-authorization-header-of-httpclient/14628278#14628278)
Use attributed like Required, StringLength and Display in your view models and attributes like Key and ForeignKey in your database models. You can then write two extension methods that convert your data model to your view model and vice versa. That way, you separate your UI classes from your data classes. `public class MyViewModel` `{` `public int MyViewModelID { get; set; }` `[Display(Name = "Name")]` `public string Name { get; set; }` `[Display(Name = "Age")]` `public int Age { get; set; }` `}` `public class MyDataModel` `{` `[Key]` `public int MyDataModelID { get; set; }` `public string Name { get; set; }` `public DateTime DateOfBirth { get; set; }` `}` `public MyViewModel ToViewModel(this MyDatabaseModel model)` `{` `return new MyViewModel` `{` `MyViewModelID = model.MyDataModelID,` `Name = model.Name,` `Age = DateTime.Today.Year - model.DateOfBirth.Year` `}` `}` Then you can do: &amp;#x200B; `MyDatabaseModel model = repo.GetDatabaseModel(id);` `return view(model.ToViewModel());`
Use attributed like Required, StringLength and Display in your view models and attributes like Key and ForeignKey in your database models. You can then write two extension methods that convert your data model to your view model and vice versa. That way, you separate your UI classes from your data classes. public class MyViewModel { public int MyViewModelID { get; set; } \[Display(Name = "Name")\] public string Name { get; set; } \[Display(Name = "Age")\] public int Age { get; set; } } public class MyDataModel { \[Key\] public int MyDataModelID { get; set; } public string Name { get; set; } public DateTime DateOfBirth { get; set; } } public MyViewModel ToViewModel(this MyDatabaseModel model) { return new MyViewModel { MyViewModelID = model.MyDataModelID, Name = model.Name, Age = DateTime.Today.Year - model.DateOfBirth.Year } } Then you can do: &amp;#x200B; MyDatabaseModel model = repo.GetDatabaseModel(id); return view(model.ToViewModel());
From strict design perspective, you should implement validation twice. ViewModel represents a screen while model is meant to represent your business domain concerns. They are not necessarily one to one and validation may differ, though in practice they are usually very similar. If you want to stay pure architecturally, externalize your validation into validator classes using something like FluentAssertions.
You should take a look at the FluebtValidation NuGet package.
&gt;FluentValidation I recommend [https://fluentvalidation.net/](https://fluentvalidation.net/) as well !
+1 Validation within a Model itself seriously clutters the classes. I'd rather validation be in [separate classes like with FluentValidation](https://fluentvalidation.net/). Very intuitive and keeps things separate.
+1 here for me as well. It's more advanced and flexible (e.g. supports DI so you can validate using a Db as necessary) and can be used to replace WebAPI's default validation on Action methods which cleans up the Action methods.
This isn't what the question is about
The question was why is the test passing through the MRE. &amp;#x200B; There are two obvious options. The code is throwing an exception or the MRE is being triggered early. I'm guessing the latter. You run it under debug and see what is happening. &amp;#x200B; So, totally on point. But thanks for your input.
Regardless of the implementation, valid as soon as possible and validate before you save
AddCourseViewModel would have additional validations that apply in this case, but not all cases.
[removed]
NotImplementedException was being thrown. Thank you, sir. After removing the exception, the test code path worked as expected. Start() also had a NotImplementedException that wasn't handled, but the Test continued. Commenting out Start() made no change either. Per TDD rules, I should have moved my focus to implementing the functionality of the Harvester much sooner. The slick features of of Visual Studio where you can ctrl+. and implement got the better of me.
Yea now that I think about it mine are duplicated. I have models that were generated from Ef core scaffolding, then I have the actual view models which are different than the database models (I’m working a legacy webform project converting it over to MVC) somewhat but do share a lot of the same stuff usually.
Im going to give fluent validation a go when I get home, done some reading on it while on my phone &amp; I'm excited to give it a go.
Yay :D Glad you got it sorted :D
I have had a read up on my phone &amp; it looks really interesting, we where only ever shown data annotations in college so I'm excited to mess around with this when I get home.
I didn't see it before, so thanks OP.
&gt; Memory is cheap, memory copying is cheap OMG :-) &gt; The win32 synchronisation primitives also play really poorly with async code Have you ever checked the async pattern implementation? &gt; Skype 8 is bad for you because Because it is Electron shit.
Quick question regarding Fluent Validation, how do I define my pk &amp; fk ? From what I'm seeing you can't use 2 validation frameworks within the same model if I'm using Fluent Validation I can't use the [Key] data annotation.
Quick question regarding Fluent Validation, how do I define my pk &amp; fk ? From what I'm seeing you can't use 2 validation frameworks within the same model if I'm using Fluent Validation I can't use the [Key] data annotation.
Quick question regarding Fluent Validation, how do I define my pk &amp; fk ? From what I'm seeing you can't use 2 validation frameworks within the same model if I'm using Fluent Validation I can't use the [Key] data annotation.
2-cents, not sure if it'll help. Could it be that you are making call to [https://localhost:5001](https://localhost:5001) and not [http://localhost:5001](http://localhost:5001)? It states "Request starting HTTP/1.1 GET [http://localhost:5001/](http://localhost:5001/)" but you said you got an error when trying to make a call to [http**s**://localhost:5001](http://localhost:5001) (also in your last log file). It seems you may want to force https. Not sure it'll work but in your startup.cs you can try two things: 1) in Configure: `app.Use(async (context, next) =&gt;` `{ if (context.Request.Host.Host.ToLower() != "localhost") context.Request.Scheme = "https"; await next.Invoke();` `});` For this one if you have IIS pointing to localhost take out that if I have there. For me, I have the best luck when my certificate is signed for the actual domain I am using. or maybe: 2) In ConfigureServices: `services.AddIdentityServer(options =&gt; {` `if(Environment.IsProduction() || Environment.IsStaging())` `options.PublicOrigin = Configuration["Appsettings:BaseUrl"];` `})` Where appSettings:BaseUrl is the actual [https://mydomain.com](https://mydomain.com) you have your IIS DNS records pointing to. Not sure if it'll help, but I guess worth a shot. 3) Also, maybe to your IdentityServer you can add: `.AddConfigurationStore(options =&gt;` `{ options.ConfigureDbContext = builder =&gt; builder.UseSqlServer(Configuration.GetConnectionString("DefaultConnection")); })` To get rid of those errors about the grant store. They could very well be causing the issue. I've found when you use the dev credentials locally it allows you to be less secure but upon deployment it forces you to be more strict with how you're making requests. Of course you will need to configure the corresponding code/DB for this snippit. Hope it helps.
\+1 Model validation scales poorly as the project becomes more complex. Personally I'd like to see it removed altogether and stop teaching it like its the **only way** in MVC
You dont. You can make a class that ensures that the properties value is valid (E.G A product ID belongs to a product in the product table). leaning on a FK/PK relationship couples your solution to a db implementation, and is often over simplified E.G What if you cannot order product not yet on sale? FK relationship doesnt allow for this
Ah this makes sense. Thanks.
Be cool if you and/your company got some credit for your advanced techniques by submitting the code or the ideas behind the code that improved on the BCL as PRs or Issues on CoreFX on GitHub
I'm still learning identity server, but are you suppose to use inmemory when deploying to iis?
You shouldn’t be using data entity models in the web contracts. Separate your concerns. Take a look at DDD. You should have specific models for the API that aren’t related to your database models.
You can. For example, you could use it for non prod environments.
so edgy, aren't you cool. Full Framework isn't openly developed. I have more important things to do with both the time my company pays for and my personal time than submitting PRs for CoreFX. Some of our performance code is open. A large swath of optimizations are replaceable with Span&lt;T&gt; when the runtime fully supports it. Many optimizations are usage specific and become possible when you can apply additional or different constraints. Others rely on compiler behavior that cannot cross assembly boundaries. Yet others rely on our specific environment like 64-bitness or Windows OS. If you're going to be a dick, try to at least not be clueless.
What are you talking about? .Net Core can consume WCF services. Look up SvcUtil, this will generate your service proxy from the WSDL. Then use Dependency injection to bind the proxy interface to the wcf proxy client. It’s so easy...
It depends what Start() on the IndeedHarvester does. If that starts a Thread/Task then your test method _should_ be continuing. I’d also encourage you to assign the completed handler, and any other handlers, BEFORE calling Start(), otherwise there’s always a chance your method finishes before you assigned the handler.
You might also want to look into Fluent Validation. Much more flexible and powerful. It blows the built-in validation out of the water. You can even do `if not empty/not null, then validate via this rule` statements.
It's not that obvious. WCF is not supported server side and there are a bunch of tickets open requesting it. Client side there is limited support.
My biggest concern is how will I be able to deploy such a setting. WCF still uses .NET Framework and is incompatible with .NET Core framework.
.Net core can consume soap services. You just can’t write it (server). The deploy of your Api will be just through docker .net core image.
What do you mean by 'just can't write it'. Do you have an example of such a setting perhaps? Because I couldn't find a microsoft supported solution for the problem.
Are you just trying to consume a soap service? If so, google “svcutil” it is a Microsoft utility that helps you create the proxy service to consume soap services. I consume both soap and rest services from .Net core at my job. My code is deployed as a container
Hmm, okay. Thanks for the answer. Do you create one solution together with net core and wcf in a solution in visual studio or somehow else? The communication is supposed to be bidirectional, that is my app should consume from the main app, and also have services that can be consumed by the main app. I suppose it's a matter of creating a client proxy for consuming and creating a service contract in wcf for offering a service. If you have a sample project where net core consumes a SOAP service, I would be more than happy :)
You do not need WCF at all. You will have one project, your WebApi. Please google SvcUtil. This utility will create a class that you will include in your .net core project and your code will just use that class to make that service call. Please refer to this link: https://www.google.com/amp/s/thinkrethink.net/2018/06/08/access-xml-soap-services-in-net-core-and-client-certificates-ssl/amp/
Thank you very much this seems useful, I will try it as soon as possible! I have another question, how should I approach creating a SOAP web service in NET Core. Is that possible?
IIS has its own cert. You may need to add it to your certificate store to avoid the error.
You cannot create soap services in .Net core at this time. Nowadays people write RESTful services.
My project specification for university disagrees :( Do you have a suggestion for solving this situation? Or should I just give up and succumb to Spring
If you’re filling a page with data from the database you DO NOT USE THE SAME MODEL AS THE ONE THAT DEFINES THE DATABASE TABLE. This is a FUNDAMENTAL TENET OF MVC. You use a completely different model, known as the ViewModel, to accept data from the database and display it in the view. It is * there* that you implement validation of any kind.
That's easy to answer because I am dealing with it now. Basically it is just a pain in the ass. Either you have to do a lot of full, server side refreshes or you have to write all of the AJAX calls and deal with all of the bookkeeping without any help. SPAs take all of the ad hoc stuff I need to deal with and wrap it up in a nice framework. It's a lot of up front complexity, but it pays off over time by increasing consistency.
No, seriously, you're arguing that this particular design pattern (multiple processes) is bad because it makes copies of data, that's your sole argument (well aside from a book from when performance characteristics were wildly different). Why exactly? Because Linux software pretty well only uses this pattern. And have you tried using win32 synch primitives in async code?
I don't care about a Linux software. It is nonsense on Windows as it has native thread support for ages. The async/await pattern in .NET has a very little to do with parallel processing. So I use **either** the async/await because it comes for free and might improve performace in some scenarios or parallel processing in separate threads or tasks using .NET (not Win32) synchronization objects.
You don't care because you don't have any idea why, and so you understand what it means or why. You're a dinosaur, you've learned nothing in twenty years. Enjoy retirement.
Me neither.
Me neither...
Everything is easy when you know how.
Also updated this: [https://www.nuget.org/packages/Open.Threading.Dataflow/2.0.0](https://www.nuget.org/packages/Open.Threading.Dataflow/2.0.0)
This is *important*. Now... For difficult problems once in production , DebugDiag is great, IntelliTrace is great. I didn't look particularly hard, but neither works with .net Core, I think. If I am right - that is a shame. Conversely... in lieu of having Core-specific tools, going through traditiinal system tools on Linux (valgrind, callgrind, gdb...) would be preferable, no?
We have multiple instances of IS4 running in our environments, so I've seen most of the problems at some point. Two questions for you: A. Did you have it running and issuing tokens using the developer key material (which also allows you to run over regular http)? B. Is the certificate you're using trusted at the root of your certificate authority or (preferably) a public CA (like Verisign, Godaddy etc)? Also be aware (you probably are, but still) that: .AddSigningCredential(new X509Certificate2( Configuration.GetSection("Addresses").GetValue&lt;string&gt;("RSA"), "password")) Has nothing to do with the SSL state of the kestral instance or your clients certificate trust chain. It ONLY applies to the jwt tokens and cookies issues/created by IS4's STS. If you have had it running in HTTP mode and it was issuing tokens using the developer key material, then you likely just have run of the mill cert trust issues on your dev box. If you have not had in running in dev mode, do that first, you'll know exactly where the breakage happens after that.
No (because you asked ^^) Preferable would be the same tooling for the same technology regardless of the platform. It's like asking java developper to learn cmd because java also run on windows. Why you want us the same experience regardless of how/where it runs If you think you can provide idea and support for "dotnet diagnostic" that add support of valgrind and that's a REALLY GREAT IDEA but if you are the one that will do diagnostic i probably want a similar approach on such tasks. Can i force Linux /Mac dev to learn DebugDiag/PerfView from sysinternal ? i don't think so even though they are amazing tool
https://reddit.com/r/dotnet/comments/ble2lw/introducing_net_5/
This obviously needs more emojis ⚠️
Looks really nice!
Downvoted for the emojis and the bullshit headline alone.
Many do, probably. But computational expensive applications that usually run on multiple cores suffer A LOT when ported to JS. This computation can not be done Server-Side because it would stall the Server, instead each client can do his simulations based on Data it retrieves from the aforementioned Server. &amp;#x200B; Yes, WASM will, in the future, support "threading" (Synchronization primitives and other goodies) but until then, JS is not a solution.
Thanks for this, I didn't know about svcutil. Admittedly I don't know much at all about wcf, but I have some services I'm pulling from that I managed to get to work in .net core. I just used that generator for them in vs, however I also have two that it won't work for. I'll go back and try using svcutil this week and see if I can clean some of that up with svcutil, because my method feels really janky.
Does this qualify for r/titlegore ? The title is pure cancer.
It does. Personally I would ban /u/hemantjoshi_in to prevent from doing this again.
Hi please let me know how to title this i thought it would be nice to have some emojis to make it look nice . I did not wrote something bad abt anyone
Its a helpfull content and the video talks abt how .net 5 is coming and whats microsoft tend to do with that. What happens to other framework and whats not coming into .Net 5 . I just wanted to share this with others whats bad in this? Just because its has emoji :( Will remove this if all have so much problem with it
It turned out to be in the end. The frustrating thing is that I knew this existed but overlooked it on the account that ''I'll find a better solution''
If you want to host SOAP services in .NET Core, look at SoapCore project. It's actively maintained, and is highly compatible with WCF (you can reuse your service contracts and implementation even). It's basically "WCF-ish" for .NET Core. It only supports SOAP over HTTP, but it supports SOAP very well, using Microsoft's own System.ServiceModel libraries for .NET Core as the implementation of the SOAP Message &lt;--&gt; Object translations
Validate in the models and raise structured exceptions. On the front end using a js framework it can take care of its own world... This question is only really a problem with mvc, so dont use mvc?
To be clear, we're not commenting on the video, purely the title. People tend to get enough emoji-overuse in facebook, discord, twitter, etc :)
I find validation in the model helps provide the one source of truth. Having to hunt multiple places to work out the business rules is annoying. But i am rewady for the downvotes.
I mean your youtube video has a concise title: ".NET 5 - A Unified Platform", why not go with that?
Can i edit the title i dont see that option. But yes point taken will take a note of it.
I'm not going to downvote you but validation is rarely so simple. Depending on which section of the application the data is coming from there might be different validation rules. That's why they're usually applied on the ViewModel/DTO level rather than an entity. Not to mention custom validation messages which may need to be localized. And localization is presentational logic so you can't put it in the domain code.
Thanks for the detailed response! Great points!
I tend to do backend / viewmodel / modern javascript frameworks, not mvc... If the code gets to the db layer something has gone wrong, so raise a descriptive error (which can be localised you poor bastards) and display it or not depending on your consumers. I choose to duplicate my validation in javascript, its a cost but it makes sense for the mostly crud stuff i write. n.b. it i have to write another application that supports multiple languages but is really only ever used in english im going to become a market gardener.
Your application is configured to run IIS, not IIS Express. Check your project properties.
When i view the properties pages for this application, I just get a popup that says unhelpfully "There are no property pages for the selection". I've tried the solution properties too, there is nothing in there that shows any way to check, let alone switch, which of IIS or IIS express is configured. &amp;#x200B; Furthermore, if I go to create a brand new [ASP.NET](https://ASP.NET) web application, I get this gem: &amp;#x200B; So I am pretty sure there is more going wrong here, as I would assume a new web application would not inherit any settings from existing applications?
&gt; "There are no property pages for the selection" Sounds like.. you don't have a csproj file..? Something really weird. Screenshots would likely help.
That is big news, also not needing to parse at runtime is really good for performance
This is actually a vb web site project. I checked the solution file, this specifies iisexpress ProjectSection(WebsiteProperties) = preProject UseIISExpress = "true" Thing is, if I create a new web application, I get another nasty error, so I am pretty sure there is something wrong with this install. Thing I can't understand is I reset windows and installed everything new today. All I had left was the files, so struggling to see how this issue is still here. Works fine on my desktop.
Oh, then no idea. I've never used Website Projects, always just proper ASP.NET projects.
When it comes to web programming there are many ways to skin the cat. Heck you can even go 'web assembly'. I enjoy the C# server side programming over, say, php, python, ruby, etc. Enterprise libraries can work then as well, knowledge is transferable for those not 100% onboard with the website but have .NET experience, and Razor pages provide a whole host of control in a nice environment.
I think you'd do well to wrap your mindset around what is client and what is server before you worry about the technologies involved and the billion ways to obtain the same result.
In my experience, **the more you do on the server, the more reliable the application is over time.** Browsers change over time, breaking JavaScript and CSS libraries. Plus, the more business logic you put on the client, the more "levers" you give hackers to bleep with stuff. The idea of many frameworks is also to separate UI work from business (domain) logic so that the UI can change to fit the latest style craze without overhauling the application. And UI specialists can focus on the UI separate from other application and database programmers. At least that's the theory; the practice is often messier. That being said, there are many annoying things about Dot-Net MVC that should be reworked in my opinion, but that's another story.
Alright. I have a brief idea about it. JS and the related technologies provide stuff like client side validation, etc and asp.net actually handles the server requests made by the client, am I right?
Yep. That’s it. ASP.NET can do front end, but if you want to do modern client-side things you need Javascript, or WebAssembly. You could use Blazor when it comes out to basically not use JS at all, but that’s not here yet. And there’s a lot of JS libraries and frameworks out there to be used. Rarely will you build anything with just one thing anyway.
That clears things up a little bit. Thank you. I just find it frustrating sometimes that tutorials and books don't make it clear enough of the larger picture of what things are and how things work for a beginner.
&gt; And can I say that the equivalent server technology for JS is node.js as asp.net is? Not quite. The node.js is a JavaScript runtime, it is equivalent to the .Net runtime. Express is a JS web application framework, it would be more equivalent to ASP.Net.
Validation in the view-model works right up to the point where you need to start doing more complex validation. Like compare property A with properties B and C. Or do database / API calls. Or only require property A based on the value of B or C. We always convert over to FluentValidation these days (for projects that used to be attribute validation).
It's not about the same experience, it's about tapping into existing maturity and capabilities. It will probably be some time before the Core implementation catches up. DebugDiag *works* for both native and .net code, and it does nice things. So do other tools. So they could work for Core as well.
I can build a house with just a hammer!! Said no construction worker ever.
First of all you've got to get your head round the concepts of client side and server side rendering. Don't worry about specific technologies just why do we render on one side and not the other. You mentioned html and CSS - they're only any good if you have a completely static page which makes up only a tiny proportion of the web. Since most pages have at least dynamic content at load you need something that changes the content before it gets sent to the user - this is server side rendering. Server side and so asp.net / razor or any other tech is good because it means the page structure is predefined before it gets to the client. This means all the client has to worry about is downloading the resources and displaying them. So server side is providing us with dynamic content and taking the load off the client in performing all the logic to produce the necessary html before it's rendered. On top of this, server side is great for caching. This means that if the same page is requested many times we're not having to perform the same logic over and over to render it, which if we relied on client side technologies then we'd have to do it every time the page was loaded, just on the client not the server. The importance of this is speed, this is also important for SEO. If your pagespeed is quicker your page ranks higher on Google. Server side rendering is also helpful for SEO because Google has two goes at loading pages, the first one being where it just reads the page and indexes that content as it is. Server side rendering produces a fuller page, therefore it gives you better SEO rankings quicker. So in summary, server side technologies like asp.net and razor allow you to produce dynamic pages, faster and give better SEO. There's more but you can only type so much and you can Google this stuff. Okay so why bother with any client side of technologies... Well JavaScript Frameworks like react, and CSS stuff like bootstrap allow you to do clever stuff on the client. Things like dynamic page changes when users interact with the page is good and gives a better user experience, for example an item appearing in your basket without a full page refresh by calling the server for a new page. A product variant changing on Amazon without reloading the page. Sometimes we just need to change the page around without reloading everything. This is where client side rendering is important and so frameworks like React do this job really well. To make a good web experience you generally need a combination of server and client side technologies. I'd forget about what those technologies are and focus on understanding implementations and doing it well. The starting point is reading up on when you do stuff server side and when you do stuff client side.
Feel free to submit a PR to Msft if you think you need it right now. Still i understand they want to re-think their previous way of diagnostic to make it work XPlat first and not "linux first"
I'll give it a go. As we intend to only use HTTP for communication, this will be sufficient. Thank you very much for the suggestion :)
It will make sense soon, so don't lose faith in it :) &amp;#x200B; Imagine a page on Amazon that shows a product. It's a cat collar. Then you browse to a different product, say a... mouse collar. I'm sure they exist :) The page is the same for both (mostly) but the product details are different and the recommendations are probably different. You'll see a different selection of mouse collars on the second page. &amp;#x200B; That's what the server bit is doing.
Every piece of the puzzle adds functionality. &amp;#x200B; .NET MVC does its work server side. It serves the front end with data. If you were making a static website then you would have no need for MVC. However, when you work on a large complicated project with data access over a longer period of time you will start to appreciate something like MVC. As long as dev/team adheres to the general conventions of MVC you would be able to easily pick up the project and continue to work on it. &amp;#x200B; Some functionality is not available with just regular .Net views, that is why in some cases people utilize jquery/js.
Yea the solution is simple, don’t use webforms.
Or perhaps a 3rd option... Just use `.HasFlag()` and let the framework authors duke it out.
The difference is front end and backend. Vue and react js are built to handle displaying UI. Backend code such as c# asp.net framework is for handling things like business logic, data manipulation/look up. It’s true you can build both in JS and that why the created frameworks like node so that front end dev could reuse their skills for backend dev. However in most cases it’s just choice which one you want to use. Each framework has its strengths and weakness
The solution was simple, but it definitely wasn't easy.
Pretty impressive, nice to see the Avalonia project constantly improving. With Xamarin.Forms, Eto, Uno, and Avalonia the GUI space has healthy competition to bolster .NET Core.
I’m sure there are other fancy tools - but we just use some freely available utilities (or vs edition). Performance - we use logparser to identify problematic areas - and then try and identify reproduction cases from that. VS profiler once the reproduction case has been found if still in doubt to the actual issue. Occasionally- windbg - but rarely unless we need to take a snapshot of prod. Memory leaks - take crash dumps when the memory use is high - then use windbg and sos to inspect. Pretty sure there is a tool called debugdiag or something like that that can take the dump automatically- but it tends to hang the process while writing the dump so we end up doing it manually when the machine can be isolated. Windbg/sos is a bit of a beast - but once you get a hang of the commands and how to setup the symbol paths it’s a lifesaver and is basically applicable to any .net development. Lots of info around on how to use it as well as a few cheat sheets.
Thank you for your thoughtful reply
HasFlag calls are generally slower, the paths that I've seen use reflection.
Not sure why you are getting downvoted. This is the correct answer.
Thanks a lot. Are there any books or tutorials that I could read to know more about the whole architecture of web?
Those are not equivalent. The first will give incorrect results for some enums and you can't tell if it's correct or not without looking up the definition of the enum.
Those are equivalent (for single flag checks). What enum definition you have in mind? The [Flags] attribute? Without it the whole check is kind of pointless.
When you need a database... .Net gets you there. When you want a web application instead of a website, .Net comes to the rescue. .Net is just one of many flavors of backends as others have said, but it is generally very easy to get what you need without too much fuss.
I'm always wondering (but never enough to bother checking it): Can I add custom options to this connection string to use them in my `DbContext`? I rarely see any articles related to expanding the `DbContextOptions`.
Well, since the connection string is specific to the database, I don't see much chances of that. What kind of options would you like to specify in the config? I assume it is some kind of configuration for Entity Framework itself?
True, I guess the connection string is not the appropriate place. But I often want to customize the behavior of the `DbContext` based on configuration values, and injecting the `IConfiguration` is often not applicable (e.g. when using db context pool). But there's a lack of examples for extending the `DbContextOptions` with custom values.
To extend on the code already available for \`DbContextOptions\` I think I would either use existing extension methods or create my own. These methods should take parameters that you want to specify through config and then just pull either single variables or entire sections through \`IConfiguration\`.
Your post has been removed. Self promotion posts are not allowed.
I would recommend a traditional database. Your expected outcome (100k transactions per day) averages 1 transaction per second, meaning peak traffic is probably less than 1000 operations per second. These volumes are easily handled by stable, reliable, and well-documented technologies like MySQL and Postgress. If your transactions are orders of magnitudes higher, then congrats! Business is booming, and you can afford to run a couple more servers in parallel or hire fancy engineers to build fancy databases.
so in what scenario ES will benefit? 10k transactions a second? also - in your approach - will the project benefit form denormalized, read side DB?
[removed]
Many people working on problems of similar scale to you have found traditional databases useful, so that's my broad recommendation. I don't have enough information (both about your project and about databases) to make specific technical recommendations, sorry.
[removed]
thank you blurr for your replies
Oh, so you're recommending SQL, because you don't know anything else :/
UPDATE: &amp;#x200B; Tried running the vs2019 installer, removing IIS express, then clearing out all temp files, etc. rebooting, and installing it again. Still no joy. &amp;#x200B; So I decided to do a full fresh windows install from the 'reset' options in Windows 10. This appears to have done the trick. I installed vs2019 and when I open webs, they load ok, and I can run them on IIS Express. &amp;#x200B; So whatever, the issue was, it seems to have been carried over even after doing a 'reset' on windows where I opted to keep files. Seems I needed to do a total fresh install (though I did not opt for the option that does a full disk wipe and clean as you would if resetting the PC to sell or give it to someone).
Introducing new patterns and architectures to a team can be a tricky process, and really depend on how much appetite the team has for learning and working with something new as much as balancing the benefits and drawbacks of the pattern. I am a big fan of CQRS because it is nicely scalable without leaking the storage concerns to the rest of the code, and ES encourages domain logic to be packaged up into discrete and non-branching handlers which generally makes it easier to write less error prone logic that is testable. Both of these come at the cost of adding complexity, so the question is whether that is necessary complexity to solve the problem and requirements you have I think you could do some small tests to see how it can work; spend a few days trying a few different things in some projects that won't become production code, and discuss with the team To answer some of your questions: \- First two questions are probably answered by using one of the ES frameworks, just google for one you like the look of \- Probably 1 aggregate root per account. The slight issue here is when you have two accounts that need synchronised events, like a transfer of credit between them. So you might need some validation step to know whether an command event would be successful on each before committing them \- For schema changes the answer is don't. Once you have written and released an event type that data structure (and also the data) should be immutable. If you need to add fields then that is a new event. Same with the event handlers, you don't want to be in a situation where replaying a stream after a change ends up with a different state in the aggregate than before. I know we developers talk about DRY all the time and it might feel weird to create an entire new class with a single extra field but here we are specifically using the pattern for being able to reproduce the state. NoSQL / document databases shine here since they don't have a fixed schema in the way that a traditional db does \- Event number leads to ordering and synchronisation issues, eg. say two people with access to the same account both make a purchase at almost the exact same time, there are only credits to complete one of them. Your code would have to lock on some sequence id generator, sounds like an annoying problem to solve. Or, just stick a timestamp on the event and it gives you a way to order the events while solving the incoming synchronisation issue
&gt; A key example is [Enum.HasFlag](https://docs.microsoft.com/en-us/dotnet/api/system.enum.hasflag). This method should be simple, just doing a bit flag test to see whether a given enum value contains another, but because of how this API is defined, it’s relatively expensive to use. No more. &gt; &gt;In .NET Core 2.1 `Enum.HasFlag` is now a JIT intrinsic, such that the JIT generates the same quality code you would write by hand if you were doing manual bit flag testing. Source: [https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-2-1/](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-2-1/) &amp;#x200B; HasFlag is only slower on pre-Core 2.1 (*Hello .NET 5, thanks for deprecating .NET Framework*)
Question: does it or can it support DI when constructing the object graphs? The thing that has consistently annoyed me about XAML is with dynamic views there is a lot of manual wiring up services into the view model and then the view, and I end up writing some view factory and there is a bifurcation between views that can be composed, and those that can't
- "and ES encourages domain logic to be packaged up into discrete and non-branching handlers" - but you can get it with CQRS itself, even without ES, don't you? - 1 aggregate root per account? can you explain? I was asking if we have e.g. "Account" aggregate root and "Merchant" aggregate root, there should be 2 separate event stores per each AggregateRoot class? resulting in 2 separate tables (one table per event store, per aggregate root class) - timestamp can also be same (in rare scenario, 2 events in the same microsecond) so we will relay on the order that is in DB? thanks :-)
Learn C# at first... Do some console app or winform app, use entity framework and some basic db. Then read about best practices for coding in C#, there will be a lot of things that you are used from PHP but won't be a good way to do things in C#. Then start with [ASP.NET](https://ASP.NET) or [ASP.NET](https://ASP.NET) CORE. You have understanding of web development so it will be easy for you I believe, once you learn C# at least a little, but you shouldn't take practices from PHP to [ASP.NET](https://ASP.NET). So study a language and .NET Framework first and [ASP.NET](https://ASP.NET) will be really easy for you. Btw good choice, it's not only about the job opportunities but also there can be really huge gap in salary between developing PHP and C#. MS technologies are usually much more expensive also to host, that means they are usually used by bigger companies, bigger projects, which can mean much more money for you. Good luck with this transition of yours!
It helps you bring in and manage 3rd party dependencies that your project requires. Before NuGet you would have to manually download and import your other libraries (for example Entity Framework). You were also responsible for making sure you were getting the latest patches. This handles pretty much all of this for you.
It is a way of obtaining and managing third party software components that you can use in your code. [Newtonsoft JSON](https://www.newtonsoft.com/json) is probably the most relatable package out there. Sure, you could write a bunch of code to work with json ( just a string after all ) but this library already does it. Nuget gives you easy access to such things as well as notifying you when new versions of it have been released.
Do you know what [NPM](https://www.npmjs.com/) is? Or what [ruby gems](https://rubygems.org/) is? [Rust Crates](https://crates.io/) ? [Python package index](https://pypi.org/) ? Like that, but for .NET.
\- Fair point, it is more about interface design which neither enforce unfortunately \- Transactions are events, an account is a materialised record of those events. You have a choice for the rest, a merchant or user is a property attached to that account, or you conflate an account with a merchant, it depends on the requirements, e.g. could multiple merchants use the same account or a merchant have multiple accounts. You could either have merchants as their own aggregate with their own events, or just simple objects that are connected through IDs. An account might have a CreateAccount event that sets the merchant reference ID. Just because one subdomain benefits from ES doesn't mean another would. True microservice design might say that merchant management is a separate domain from transaction processing, it depends on your requirements... design is more art and philosophy once you get the engineering out the way &lt;/pretentious comment&gt; \- Fall back to comparing the Guids to resolve matching timestamps. There is no 100% guaranteed way to avoid a conflict, but you can get it down to the likely hood of once in a few billion years, which works for me
in the old day you would go to Blockbuster, select a DVD, bring it home, place in dvd player and then you can watch the movie. Now you have a Netflix app that lets you watch that movie without much effort. Nuget = netflix, the movie = some 3rd party package you want to integrate into your project. Previously you would download some zip file from some website, unzip, place dll's in directories, edit web.config file etc.., and then you would be up and running to use the 3rd party library. Nuget allows you to do this easier now
What's your level? Are you working as a dev, student or just a hobbyist? Something like this might be good - https://www.amazon.co.uk/Full-Stack-Developer-Essential-Everyday/dp/1484241517/ref=mp_s_a_1_2?keywords=web+development&amp;qid=1558434553&amp;s=gateway&amp;sr=8-2 But it's more for people who either work in the industry or want to at some point. It's not technology specific but that might be a good thing so you can focus on the general principles
I did miss the word single in the article, but they are still only equivalent if each possible flag only has a single bit set. That won't be the case if there are combination values like `ReadWrite = Read | Write`, values that may later become combination values, or some hidden structure to the values. I prefer HasFlag or '== mask' because they work correctly for any flag or combination of flags, except 0. // Original [Flags] enum ParseOptions { None = 0b00000, AllowWhitespace = 0b00111, AllowSign = 0b11000, } // Possible future version [Flags] enum ParseOptions { None = 0b00000, AllowWhitespace = 0b00111, AllowLeadingWhitespace = 0b00001, AllowInternalWhitespace = 0b00010, AllowTrailingWhitespace = 0b00100, AllowSign = 0b11000, AllowPlusSign = 0b01000, AllowMinusSign = 0b10000, }
I can suggest MemoScope (https://github.com/fremag/MemoScope.Net) for Memory Leak &amp; Crash Analysis. You use it to analyse dumps and can drill down into object instances. If you want to spend some money, there's DotMemory by Jetbrains.
One thing that is very confusing to me is the signature of the string.Create method (it's not explained) and specifically what 'chars' is and where it comes from in the delegate.
*click* So, does NuGet handle installation, or does the Netflix analogy continue past that, and NuGet lets you run off a "steaming service"?
It's the built in one for .Net 4 Hm, maybe it is designed similarly.
Installation and versioning. You can see when updates are available, and choose to upgrade or downgrade when appropriate. You still need the dependencies locally, of course.
See it like this: You wanna program something, but in your program your require, for example, a json-file. Now...handling json-files isn't the core of your program and many people have already written code to handle json-files, so you go and find and just "copy and paste" the code of someone else (or just copy the files of this code into your project). Now, the guy who wrote the code to handle your json-file sees "dammit, i have a bug in this code"...so does your program now, because it uses this code. So the code for the json-file gets updated and you now have to download and copy the code into your program again. And a few months later, you decide to not use json anymore, so you don't need the json-code anymore and now need to find and delete all the source-code you've copied into your program to handle json. But because you are a programmer, and like every single one of us, lazy as hell, you write software that does all of this for you. But you are also not the first to think of this, there is already software out there that does that and more. Like NuGet.
I started working as an intern last month.
You've probably already added a reference to a DLL (like System.Web) in your project using visual studio, you might have added a reference to another project in your solution (like a class library helper) using visual studio. With NuGet you can add a reference to a third-party DLL (in its simplest form) from the internet and use it the same way. When installing with NuGet it may even run an installation (PowerShell) script against your project, it may include any kind of file such as a cshtml file or, text file, or image file, and it may even include other NuGet packages that the package itself requires to run (depends on).
Steaming service in the sense that it has a config file that is part of your codebase. You generally use a .gitignore to exclude the binaries themselves keeping your project small. When you share your code with other devs it downloads the dependencies down before compilation.
*the more you do on the server...* The more expensive it is to run your app :)
[removed]
Correct. There's a lot of cases where the two comparisons not match. Hope people who read my blog will realize that. Or they will use `HasFlag` anyway (which is what I would do). :)
It's a package manager. It's a convenient, centralized way to distribute libraries for .NET projects. Instead of having to go manually download libraries you need for a project, you add dependency references to them and the .NET tooling automatically handles downloading and setting them up, updating, etc.
it's a tool/feature in visual studio that lets you install a library to your project. Once installed you can reference classes and functions from that library in your project.
Something like that will be good then. Try and find resources on full stack development. Even if you only work on one part of it, understanding the rest of the structure will only make you better. Also understanding the roles of other people you work with. So ask questions, ask to pair with people and talk to them about what other parts go into creating sites. The best way to learn is to get involved and see the work being done at all levels
Something like that will be good then. Try and find resources on full stack development. Even if you only work on one part of it, understanding the rest of the structure will only make you better. Also understanding the roles of other people you work with. So ask questions, ask to pair with people and talk to them about what other parts go into creating sites. The best way to learn is to get involved and see the work being done at all levels
Something like that will be good then. Try and find resources on full stack development. Even if you only work on one part of it, understanding the rest of the structure will only make you better. Also understanding the roles of other people you work with. So ask questions, ask to pair with people and talk to them about what other parts go into creating sites. The best way to learn is to get involved and see the work being done at all levels
This.... Might be the best analogy I’ve seen for NuGet.
Connection strings are so underrated! You can add attributes to them and do all sorts of neat tricks. See [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient).
XamlIl injects IServiceProvider if class lacks a parameterless constructor. Provider instance is read from here: [https://github.com/AvaloniaUI/Avalonia/blob/11af891f036cb78a4f534ee701ea2945e804e8a5/src/Markup/Avalonia.Markup.Xaml/XamlIl/Runtime/XamlIlRuntimeHelpers.cs#L127](https://github.com/AvaloniaUI/Avalonia/blob/11af891f036cb78a4f534ee701ea2945e804e8a5/src/Markup/Avalonia.Markup.Xaml/XamlIl/Runtime/XamlIlRuntimeHelpers.cs#L127) &amp;#x200B; It's the same service provider that is passed to type converters and markup extensions, but potentially it could be extended to use DI.
Perfect timing for a question my team is facing: We have an onshore team, all with total admin access to our PCs, and an offshore team that is severely restricted. We're having an issue where our global nuget packages are being installed in our AppData/Roaming directories, which works fine for onshore developers, but our offshore are facing problems because they can't access packages in those directories. Instead, they can only use the packages in the solution directories. Is there a way we can disable global packages on their PCs or force all solutions to use local packages?
Not completely like that, don't some of these handle transitive dependencies well and some don't? Can be quite an annoying difference
It handles installation, dependencies, etc. So say package X requires packages Y, D, and F v1.533, and those have requirements of their own, instead of having to grab each specific requirement and install them, you click install on X, and everything else comes with it automatically (after acknowledging that it comes with them on a pop-up window) It's actually pretty great overall, makes getting a specific version of a library a snap as well. And the install is usually pretty quick and you can go straight from installing it to referencing it in your code in seconds.
but generally they're all package management systems
They all have small differences, yes.
If you're writing some code and think "Hey, this task I am trying to do seems like a common thing that someone else has already solved" then you might think about using a _library_ - a piece of standalone code someone else wrote to do the heavy lifting. A popular library (for example) is Newtonsoft.Json, as there's not very many ways to cleanly handle JSON in .net, yet it's a really common format. How would you "consume" that library? One thing you could do is download the dll somewhere on your machine and add a reference to it. It'd work, but means anyone else using your code needs to do the same thing (And put that referenced library in the same location on their machine). Now, what if you want to use another library that relies on yet another library? Say you aren't using newtonsoft.json, say you want to use SomeCoolLibrary but _that_ requires newtonsoft.json, well you'll need to download _both_ and ensure they're both in a place your code can access. What if Newtonsoft.json has _another_ dependency on something else? This can quickly become a nightmare. Nuget solves _all of this_. You just tell nuget "Hey I want to use SuperCoolLibrary" and it'll take care of finding and downloading it, as well as any dependencies it has, as well as any dependencies _it_ has. It'll even try to find the best _version_ of those dependencies if there's a mismatch. Nuget is awesome.
Context: you want to use the code which was written by someone smarter than you. Solution #1: Smarty pants writes the code on a blog post. And you manually copy/paste the code into your project. Cons: no versioning, no guarantee that code even compiles and lots of manual (read: error prone) effort Solution #2: Smarty pants writes and publishes the code to a source code repository. And you use a tool to copy/paste the code into your project. Cons: no versioning, no guarantee that code even compiles Solution #3: Smarty pants writes the code and compile it into an dependent dll. And publishes the dlls to a dll repository. And you search the dll repo, for your needs and use a tool to download and install the dlls into your project. Nuget is #3 with few more benefits
Either because you're told to or because you are more comfortable with it. That's about it. Currently, both apply to me. I was told to use .net c# for the current project and I'm more comfortable with it than JS.
Good NuGet make package discovery and installation go fast vroom vroom
It depends though. If you can guarantee that this package will still be around for some time, yes, that is appropriate. But it's generally safer to include the dll of a third party lib in your codebase if you are not exactly sure. Ofc you don't want to bloat your code but if it's a critical dependecy I like to keep them around in a special folder in my repo.
&gt; if it's a critical dependency I like to keep them around in a special folder in my repo. This policy was once common, but is now pretty rare. For several reasons it's not recommended, including that: * Large binary files don't work well with source-control systems. * Nuget packages are immutable. i.e. `Newtonsoft.Json 12.0.2` should be the exact same bits every time so there's no change issue with getting it again. If you don't trust the public feeds, or are concerned about downtime, then many companies have internal, private nuget feeds that can cache in front of nuget.org
NuGet is just one in a pool of similar applications known as "package management software". It's been used forever on Linux and imo it's the one HUGE advantage Linux has had over Windows. To install software in Windows, you typically download an installer, run it, and it installs to Program Files. But, this is not standardized. Maybe you just get a ZIP or EXE and have to find some place to put it yourself. Or maybe that installer doesn't install to Program Files. Maybe it installs shortcuts to the start menu, maybe it doesn't. Maybe it dumps a shortcut to your desktop and another on your taskbar for you to delete. A package manager removes all of this and provides a standard mechanism. A program can provide information about itself in a standard format and get uploaded to a server that is configured so the package manager can work with it. The package manager can then, on request, pull that software download, and use the information contained to extract it, configure it, and so on and so forth, automatically. This also means it can do automatic updates without any support from the software itself since that's just a new package that gets uploaded, which the package management software can look for, find, and download and install for you. NuGet works just like this but for the narrower scope of .NET libraries. You can tell NuGet you want to use a library with a specific name, and optionally a version range, and NuGet will get and install it into your project for you automatically. So for example if I want to use SQLite in my project, I can pick out an SQLite library on NuGet and use NuGet to install it. Then I can use it in my project right away and it'll get updates and so forth. And critically, if I send my project to someone else or put it in GitHub or something, I do not need to include the SQLite3 files. Just my project files which have the list of nuget packages it requires. When the next person tries to build my project, nuget will automatically fetch the packages for them.
Servers are much cheaper than humans.
Nuget also handles most dependencies too. Continuing the analogy, if you want to watch Endgame it would also download all the relevant movies leading up to it.
Yep, very true. Forgot about that part.
Make an endpoint in your webapi that does this work, then, yes, you can easily set up something like webjobs to call it periodically or just manually trigger it whenever you want. Having the same app do this work will avoid the shared database anti-pattern.
Number of options here. Firstly though, I'm going to assume by DB we're talking an SQL database of some description. If you're talking Cosmos there is a much better way than what I'm going to suggest. You could go down the Logic App route but honestly, they are probably more hassle than they are worth. Great for little admin tasks like a once a day clean of some blobs or something, but I wouldn't put any proper logic in them. Function app is really the way to go here from an Azure standpoint. As you're saying you're not fussed if it's real time, the easiest solution would be using a [timer trigger](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer) on a schedule. This picks up whatever it is you've inserted and sends them off somewhere. It's simple and works. Will also be incredibly cheap. Another option if you later decided to add real time into it would be to put this message onto a queue of some sort (could use service bus or queue storage) with a message saying "this thing needs processing now". Maybe passing along the ID of the thing that needs to be picked up. Then you'd have the Azure function triggered off that, which is then processed. This is a much neater and arguably better way of doing it. Definitely do not need a VM or IIS.
I feel there are probably a number of alternatives. You can use the sign bit for one, something like: &amp;#x200B; \~\~\~ int bit(int x, int n) { return (x &lt;&lt; n) &lt; 0; } \~\~\~
yes currently testing with this method, trying to get logic apps to call it based on a timer
Don't know what kind of database you have there but this article could get you started which initially sounds like exactly what you want and a would be a perfect match. Connect cosmosdb with azure functions and from the function post to your receiving api. https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-functions
yup DB is SQL. ah right function app(you got me at cheap), let me read up about it. &amp;#x200B; yes i hear you, let me get this going with a scheduler to see and see if the business benefits from it. Then I will invest more time on work on real-time. &amp;#x200B; thanks for the inputs!
Is storage in a database necessary? If not then you can just use some queues. Receive the web hook calls, storing the data in a queue. Pick up that data on the other side of the queue in another function to process it and store it in a separate queue. Then lastly use a function to send it to the other API. This is scalable, near time, simple and has no processing when no data is received (as opposed to polling the database). You also get to not have to administer in the database which piece of data is already processed, is processing it still needs to be processed. I don't have much experience with azure but queues and logic apps and event grid look like it could work.
You can also run a function app off the same app service plan as your Web API if you want and it won't cost you anything extra. However consumption plans are so ridiculously cheap it won't make much, if any, difference. I'd also steer away from using a web job directly if you can get away with it. It's what functions use under the hood anyway so unless you have to do something really specific, there's almost nothing to gain.
this is really cool, /u/dantheman999 also mentioned cosmos db. I think if this gets bigger I would want to take a look at it. I am not using it, cause I havent used it before and want to get something up and running first. &amp;#x200B; thank you for the article!
Probably wouldn't use Event Grid for this. Sounds like there is a definite `this happened -&gt; now this needs to happen` rather than just a `this happened, but I don't care what happens now`. Definitely something for Service Bus of Storage Queue.
for this particular function, yes you are right but the DB is used for other purposes which I didnt mention here so yeah no choice. &amp;#x200B; Wondering if i can do a hybrid, where I send the message off to the queue as well as get it written to the DB. this is something I can explore.
Sure you can do that. Having multiple processes reading the same message from the queue is a bit more involved so I would not go that way if you don't need to. But writing to a queue and storing it in a database from the API that receives the web hook doesn't complicate things much.
In my experiences these very advanced techniques are not found in masses in the internet. you need to pick up different learning materials on this (books, blogs, conference talks) and then experiment/get experience with some concrete technologies (the ones that will work for your teams skillset and actually can provide a solid foundation for your proposed CQRS or whatever architecture). there is no single best practice and best implementation in this constantly moving industry.
1. Pick the first option to store in-app. 2. Create application 3. Change connection string to point to your SQL database 4. Add-Migration (might need to create a dbcontext before I can't recall but you can figure it out) 5. Update-database 6. $PROFIT$
The latest version of NuGet does not run installation scripts, mostly because it was impossible to write a correct uninstall script.
I would say that you should never use “== mask” because it makes refactoring very difficult. If you ever convert a regular enum to [flags], then you’d have to refactor everything to make sure that you don’t ruin all of your checks
Welp, I don’t think it’s that good. It made me thing that nugets are not actually downloaded but magically works via the internet like Netflix. It’s more about finding the package and ease of integration. I would compare it to searching, instead of going to library, finding the category, the author, then the letter, the book, bringing the book and finding the page you just type the question into google and the information is right here.
&gt; Continuing the analogy, But it’s honestly not a very good analogy, so why?
I would say when to use is simply when you are concerned about performance or memory usage. And even then it may be worthwhile to just get the code written and come back to it later to optimize only if needed.
https://docs.microsoft.com/en-us/aspnet/core/fundamentals/routing
Maybe instead of looking at new frameworks / methodologies, start looking at something like containerization via Docker, which is actually pretty interesting. &amp;#x200B; Also, off topic I guess, but not sure why MVC world is considered "stuck..." Personally hate the JavaScript frameworks, they have a place, but that place is not "everywhere."
https://github.com/dotnet-architecture/eShopOnContainers
You're building web applications without javascript frameworks?
It actually depends a lot on the scale of your app. Further, you can usually CDN js and have it cached on the client.
AutoMapper, NLog, AppMetrics, Identity Server, Autofac.... ASP.NET is itself open source so you could read the source code
\&gt; I wonder if after repeat processing if I'm trading memory for extra processing or some other detrimental effect The purpose of \`Span&lt;T&gt;\` is not reducing memory utilisation, it's reducing memory allocations. Memory allocations cause GC pressure, which increases latency overall. Regarding your example above, using \`Span&lt;char&gt;.Slice\` is pretty much always a win compared to \`string.Substring\`.
You sound exactly like what a lot of open source projects are looking for. Let me make a case for helping with my project, Entity Signal (https://entitysignal.com/) - It is a small code base, so "Taking it apart" will not take much time - There are lots of low hanging fruit items to add, for example support for mapper files, unit testing, documentation, a good readme.md, etc... - It has been getting quite a bit of traction, and was recently tweeted about by Scott Hanselman https://twitter.com/shanselman/status/1128194960979677184?s=20 In terms of quality well that is debatable, I even have "I KNOW IT'S NASTY" as a comment in one of my functions lol. https://github.com/dustout/entitysignal/blob/master/EntitySignal.Server/Services/EntitySignalDataStore.cs
Sideload bruh
Some cool projects: &amp;#x200B; * [BaGet](https://github.com/loic-sharma/BaGet) \- A NuGet server implementation. This is my side project :) * [Try .NET](https://github.com/dotnet/try/tree/master/MLS.Agent) \- Lets you develop and run C# in the browser. For example, check out [this demo](https://readme.dev.nugettest.org/packages/Newtonsoft.Json/).
And when you feel like you understand Docker and Containers, you could dive into Kubernetes and Helm, Amazon EKS, Azure AKS. Maybe dig into Azure and AWS Serverless options. For a slightly more low-level, make sure you've got Asynchronous down cold since that's the direction the wind is blowing. Look into Span&lt;T&gt; and [IO Pipelines](https://devblogs.microsoft.com/dotnet/system-io-pipelines-high-performance-io-in-net/).
Support for install.ps1 was removed as running arbitrary Powershell on users' machines is a serious security concern :)
Your post has been removed. Self promotion posts are not allowed.
&gt;If you don't trust the public feeds, or are concerned about downtime, then many companies have internal, private nuget feeds that can cache in front of nuget.org [NuGet.org](https://NuGet.org) is replicated into several Azure regions and is cached at the CDN level. We do our best to avoid downtime :)
People have built web applications without modern javascript frameworks for years. They aren't always very responsive or fancy, but they get the job done.
Responsive and fancy have nothing to do with it. The question is do you want to make your users pull an entirely new page every time they do something. So yes you can still build websites using inferior tech but it should never be preferred. It will be a worse experience for your users.
For me, I am primarily focused on developing for Azure, and even though my web front ends use React, bouncing requests off an ASP.NET Core API which can then leverage basically every NuGet package for Azure resources just makes it so easy.
You can see if any of these projects are interesting to you https://code.msdn.microsoft.com/
It's a public website where you can build your code into a "package" and host it there. You or anyone else can then use it in their projects. It's a way to share code and functionality without everyone writing their own or copying the source files around. Package managers exist for many different languages.
The video stream from Netflix is still downloaded to your machine.
Something I stumbled upon earlier today: - eShopOnContainers - Microservices architecture on Docker. they also have a free ebook I started to read today - NorthwindTraders - CQRS, EF, Web APIs
Please take a look and if you see something you like, Im here to answer any questions: &amp;#x200B; [https://github.com/okhosting?tab=repositories](https://github.com/okhosting?tab=repositories)
React, Angular, or any other virtual DOM diffing technology isn't exactly superior. It's a hack to get around underlying limitations of JS. Since you're in the .NET world, check out Blazor. Its C# compiled to WASM. It's a more sane and superior world to the spaghetti mess in the JS world. I say this as someone writing a TypeScript/React app.
Which can make upgrading the software a pain in the bleep. And yes, it does depend on various things, but *usually* the bottleneck is the cost of developers and related maintenance, not servers.
Markdig by xoofx: https://github.com/lunet-io/markdig It's not a big project so it's easy to navigate through. Code quality is good and there are unit tests.
Thank you for the reply... Are there any Blazor or Web Assembly quick starts you would recommend? I don't see DOM diffing as a limitation of JavaScript. JS is a language. DOM diffing isn't a problem in and of itself that JS is trying to solve. I see Angular, React and Vue as ways implementing some form of MVC in the UI. In angular the DOM is the view, my component in typescript is the controller, the model is whatever. No? I guess it's kind of inelegant because there is a lot of brute force change detection, but in angular you do have more elegant options like on push change detection.
There are good reasons to not use a frontend framework in a number of scenarios. Maintenance, team experience, and intended audience just to name a few. Implying you are doing something wrong by not using React, Angular, Vue, or whatever is quite short-sighted in my opinion. I am sure we can agree that a majority of web applications are going to benefit from using a frontend framework. No doubt. I don't remember the last project I started or worked on without one. However it is certainly not a requirement.
I can see a few legit scenarios... but maybe 10%, maybe. Maintenance - Are we comparing JS framework to plain HTML or some form of MS ASP.NET? Have you seen the pain of the people on here asking about web forms? If you don't keep evolving your dotnet stuff with MS releases over time the maintenance gets pretty rough. Team experience - I think in general you're better off developing expertise in the best technology, than leveraging expertise in a less perfect one. If you follow your logic, your team just doesn't evolve. They become obsolete. Intended audience - Who wants to make a full request when paging through a result set? ... I'm not saying everything should be a SPA or script heavy. But who are these people? I'm not trying to rant or hate. I am legitimately curious and interested.
As long as it gets the point across …
The [official documentation](https://docs.microsoft.com/en-us/aspnet/core/blazor/?view=aspnetcore-3.0) should be a great place to get started.
&gt; I can see a few legit scenarios... but maybe 10%, maybe. Right, and 10% of web apps is a lot of web apps. I just tossed out a few scenarios I have seen, not some exhaustive list. &gt; I'm not trying to rant or hate. I am legitimately curious and interested. No I hear you. If you are promoting frontend frameworks you are preaching to the choir with me. My point was only to assert that not using one is somehow wrong is certainly not always the case. You can still make a great web app with ASP.NET Core MVC and a bit of vanilla js. In some cases doing so may be a requirement.
check out [Jellyfin](https://github.com/jellyfin/jellyfin) - The Free Software Media System
I made a small contribution to the Saule project, found the code base to be extremely clean and nicely done.
Interesting project. Am I wrong to think that this is a bit Firebase inspired?
True, 10% is a lot of apps. So foregoing a JS framework isn't going to be wrong all of the time, but it will be wrong 90% of the time... which is a lot of the time, no? So the guy who started this thread off said he felt stuck in MVC world. There's a 90% chance that he is totally justified in feeling that way. And probably more than that if he's been in MVC world on multiple projects.
Great. Now we need a tutorial that shows token refresh using a local database. For my toy apps, I have set the token expiry to 5 years because I can not figure out how to properly set the refresh token in Asp net core.
This is actually somewhat complicated. You will need to do the following: 1. Identify someone uniquely (probably a cookie). 2. Have some middle ware (before or after the controller logic) that records which page that person is requesting. Look here as to how middleware work [https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/?view=aspnetcore-2.2](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/?view=aspnetcore-2.2). You will need to persist this in the database somehow. 3. Then put some logic into your controller / services that displays the trending articles to order the most clicks by unique users. However you may want to also consider the bounce rate, is it really trending if people only read the first paragraph and click back? It starts to get complicated quick.
That is a good analogy, where Firebase is a NoSQL real-time Database, Entity Signal is a SQL real-time database
[Here you go](https://github.com/matthewblott/simple_aspnet_auth/tree/master/examples/api). It's actually pretty straightforward.
Isn't identity server 4 supposed to be baked into ASPNET core 3 projects? https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-core-3-0-preview-3/
It isn't, it's a cheap cop-out. Rewriting might cost so much time that the business will sink. You're (both) presuming otherwise without a shred of context.
1) The application coupled WebForms to the business logic. In which case the correct answer is to slowly refactor this away. 2) The application didn't couple WebForms to the business logic. In which case refactoring the frontend shouldn't be a time sink.
Never heard any of this. Nor do I understand how those technologies are “nimbler.” Maybe you’re just in a dome of pretentious people?
Well technically, you need to know JavaScript to build a front-end, but you don't need to learn .NET to build a backend since there are other options. JavaScript holds more cards than .NET.
Is OP begging the question, implying that existing .NET devs don't create side- or hobby projects? OP is correct, I imagine in the fact that devs do what they know. So .NET devs would do .NET systems, php devs do php, etc. Who says it would/should be different?
You seem to have a lot of issues with C# and .Net. It's just a single language in the end. Use whatever works for the situation at hand. There are advantages and disadvantages for every single language. You mix and match what you know vs what you feel is best for the situation. Just because you are a startup or a large corporation does not automatically mean you have to use node, php, or .Net. A lot of people spout nonsense because people are biased towards specific things.
No, you can do your front end without any JS if you want. We did front ends years before JS became hip. And with Blazor we can do even more. And it’s a completely different thing to learn some front end than to handle also backend with JS. And you’re assuming “a person knows X but not Y so Y is always useless.” Not a good start. Take a step back, remove the pink glasses, and then look again. So let’s assume the person doesn’t know JS to begin with. Why would they bother? Or PHP or Python?
Yes some people are fine pulling a "new" 4kb page in 10-40ms
There's a difference being saying Node is the best, C# sucks and pointing out actual facts and statistics. YouTube and Reddit are both developed in Python. This is fact. I would assume most people, if given the choice, would want to work for a company that is more similar to YouTube or Reddit rather than a generic financial/medical/old school company that treats its employees as IT people.
A lot of people want to be Front-End / Full-Stack Developers. According to the actual job boards, most of the jobs require JavaScript experience. You can avoid C# / .NET entirely if you want. You can't go up to an employer and refuse to learn JavaScript because you did front ends without JS years before it became hip. They will just laugh at you. lol I say this again, JS holds more cards in terms of employability than C# / .NET. There's no way to ignore this.
*In front-end/full-stack positions*. Again, be specific and don’t just throw claims out there. You can also avoid JS entirely if you want. Again. Take a step back and look at the big picture, or be very very specific. Of course you won’t go to an employer looking for a JS developer and say “I won’t” but you don’t have to apply for that position if you don’t want JS. It’s that simple. Duh. So no, JS doesn’t hold anything over .NET overall. Only in jobs where JS is needed. And vice versa. If you apply for a .NET job (of which there are plenty) you can’t go “I know JS that’s all I need.”
What about the business context around the application? You don't know how much time that rewrite\*, will cost (time and money) and whether the boss wants to give it. So please... *\ refactoring is a rewrite of a part. By now, it became a weasel word for schmoozers to sound knowledgeable.
You realize that most .NET Backend-only jobs are slowly transitioning to more Full-Stack. ASP.NET Developers are more likely expected to know how to do JS in 2019. There's a reason JavaScript and Python are the TOP and hottest languages in 2019 and not C#. Go on any poll. StackOverflow. TIOBE. Multiple polls have C# / .NET listed 4th or 5th at the highest, whereas JavaScript and Python are nearly always #1 and #2. There's a reason for this.
Seems like you are a troll who has nothing better to do than argue that one language is better than another. What a waste of my time even trying to reply.
By posting this, you are literally proving my point in the original question.
&gt; static typing. Compile time checking. Mature. Well designed. Performance, talent pool, support &amp; community, user experience in IDE/tooling Also, OP, it's quite common to hear said that the best language is the one you already know.
You are welcome
Jesus that page is aids, so many ads, things popping all over the place that persist on mobile. Good article tho.
This is not relevant to Visual Studio 2019, but to the project type. If you create an ASP.NET MVC applicaton, then you will get a RouteConfig. If you create an ASP.NET Core application, then you will not get a RouteConfig.
That's odd, it's usually the opposite on what I hear here in our area. Granted Node.js is pretty in demand here for companies (big and small). Usually they regard .Net as "enterprisey" in the same league as Java and that JS/TS is the future since it runs on everything (Electron, React Native, Web etc...). It really depends on the area you are in and what in and what's the in demand tech stack for that area. If you're looking as to why I prefer .Net, then it's because it's usually Convention over configuration (it's why I love Ruby as well), Mature, assured support over the years, Runtime and compile time type enforcement, statically typed, and C# overall is an elegant language and there's also F# for those who loves a functional approach. Also with WASM (especially when it gets Host bindings for DOM access) JS's monopoly on the web stack is hopefully going to end with Blazor and the likes going for a web stack on their own. One of the best things about doing back-end is that there are many ways to skin a cat and many languages with their own set of pros and cons. E.g. if I want do to ML there's python with it's good support of libs for it and ML.NET seems to be interesting if you want to stay in .net land, if I want a webserver there's tons to choose depending on your preferred stack. This isn't the case on the front-end sadly though that's starting to change.
Cool. Handy. Thanks
We do full-stack (IoT, Mobile, Desktop, Hololens AR, and Server). There's more than just the web and with WASM that's also changing. Popularity measures isn't really relevant when finding a tech stack that fits your requirements. We use python for ML related stuff since it has better support for such loads, otherwise it's .NET since you can share code for Hololens / Desktop / Mobile without problems and in the future we plan on using Blazor for web. Not everything is a nail when you only have a hammer, there's alot of areas that JS isn't suitable for especially on resource constrained IoT devices where C# or C++ is preferred. The fact that C# is 4th / 5th means that there's plenty of devs as an employer to leverage. You don't need to be the most popular to be great, devs just need to be plenty enough to not have problems looking for them (C# is in a good spot and ratio of devs from the looks of it).
Just curious, what company are you working for (the work sounds pretty exciting)? :)
It is pretty good! Even shows how to add custom claims to the token. All that's missing is showing how to use the claim in the client side in Angular/React for links/routes.
Pretty easy to do. Just decode client side, and use some sort of state management to decide things like user roles or what not. Template from there.
OK: how about: It's an "app store" for code libraries.
As far as I can tell, there are roughly speaking, 2 categories of package managers - Application package managers and Library package managers. Application package managers are there to add, update or remove apps to the machine, or to the user. Windows has lagged on this, the only good example is [Chocolatey](https://chocolatey.org/). Maybe the "App Store" on your phone or PC counts as well? That's just Application package management plus a nice GUI, right? So you have that subcategory on Windows, iOS and Android now. On the other hand Library package managers (such as NuGet) are there to add, update or remove code as source or binary to the app that you are working on. I know the line is now blurry - both NPM and NuGet can be used to install commandline tools. But it's clear that the main focus of NPM and NuGet is on code libraries for apps in their respective ecosystems. That is where they started, that is their core strength
A software solutions company in Japan, the reason for the varied tech stack since each Client has differing needs. I can't really state who are the clients though for reasons XD but yeah they're quite proactive on wanting to try out new tech to get an edge out of the competition. We suggest and provide the software / service as needed. The ML stuff isn't really in my domain sadly since I'm no python expert, we have another team that works on that. I'm more on the Backend services using .Net core, though I'd love to learn some ML myself in the future. It seems Japanese companies still loves desktop apps where the world moved to the web xD so our desktop team is alive and well here. I've dabbled with Hololens too for an industrial automotive client we work with, it's a pretty awesome tech and learned alot on 3D development on my short time working with the team under it. Ruby is actually still popular here to my surprise (though not Rails but rather on embedded stuff using mRuby). .Net though ever since .Net core was able to run Linux is slowly gaining popularity due to cheap hosting I'd assume.
Right. NuGet is very reliable these days. It wasn't always this way, but it's important infrastructure now.
I have actually found a very simple way around this and it works fine except that the bounce rate isn't considered and I really don't know how to factor that in just yet
So the logic I used was to add a new field to the database table called visitors of type int and I made it null able. So when a users clicks on an article to read jus before i return the article for the user to read I check the database to see if that article visitors is null if true I set the null value to 0 then I increment it by 1
So as visitors keep reading different articles their visitors count keeps increasing so I have used that to get trending articles ordering by descending
If there is a Beta way to do this while factoring in the bounce rate I would be more than willing to know how
[ASP.NET](https://ASP.NET) Core is very different to [ASP.NET](https://ASP.NET) (let's call it 'classic') It is much more streamlined and - by default - uses all kinds of "conventions" to reduce the amount of code it needs for you to bring up a simple/medium difficult service. The "RouceConfig.cs" is "replaced" by (Startup.cs) `app.UseMvcWithDefaultRoute();` (see: [https://docs.microsoft.com/de-de/dotnet/api/microsoft.aspnetcore.builder.mvcapplicationbuilderextensions.usemvcwithdefaultroute?view=aspnetcore-2.2](https://docs.microsoft.com/de-de/dotnet/api/microsoft.aspnetcore.builder.mvcapplicationbuilderextensions.usemvcwithdefaultroute?view=aspnetcore-2.2) ) &amp;#x200B; You can be more specific if you replace "`app.UseMvcWithDefaultRoute();`" with "`app.UseMvc(routeBuilder =&gt; { /* Configure the routes */ })`"
The reason companies still have shitty things like WebForms around is because they don't do refactoring. &gt; The word “refactoring” should never appear in a schedule. Refactoring is not a story or a backlog item. Refactoring is not a scheduled task. Refactoring is immediate and continuous. It’s like washing your hands in the bathroom. You always do it.
To anyone looking for the solution, here it is (Just translate the page) [https://medium.com/@maxamorimdasilva/asp-net-core-swagger-adicionando-header-parameter-na-sua-documenta%C3%A7%C3%A3o-984bfebd1dc7](https://medium.com/@maxamorimdasilva/asp-net-core-swagger-adicionando-header-parameter-na-sua-documenta%C3%A7%C3%A3o-984bfebd1dc7)
This sounds snarky, but thats not how I mean it, but by dirt cheap do you mean starting at € 620 like this shows: https://azure.microsoft.com/en-us/pricing/details/power-bi-embedded/ Or is there a different way to embed this?
I think it's only when you create a new project via a template, it will pull IS4 as a dependency. Otherwise, no.
Sorry, but you keep mentioning 'standard' then list 3 javascript frameworks that, in the grand scheme of things, sprung up overnight and will probably be replaced by something else in a few years
Nice effort :-)
Because CodeDom hasn't been ported? But if you really want one I can help you with setting up CS-script in Core. I use it on my ORM so I've seen the usual problems already. Just give me a ping next week as I am on vacation.
Thanks u/Kirides ! After doing some poking around I discovered this and it made more sense. I'm finding that there are a -lot- more tutorials and resources about [ASP.NET](https://ASP.NET) than there are of [ASP.NET](https://ASP.NET) Core.
Thanks u/AngularBeginner
Thank you kind sir
My experience is that Castle.Core does work on core. We use it in combination with .net core 2.2 for custom proxies in production and it works as intended.
Sounds to me you are pretty senior already. More so than me in any rate. What I use to keep me on my toes are podcasts: \- [https://cynicaldeveloper.com/](https://cynicaldeveloper.com/) \- [https://www.codingblocks.net/](https://www.codingblocks.net/) But there are Books too: \- Clean Code (Uncle Bob) \- Clean Architecture (Uncle Bob) \- The imposters Handbook (Rob Conery) &amp;#x200B; But... Who am I to judge. &gt;&lt;
Wow I now remember the old days before NuGet. Younger developers these days take these kinds of things for granted
I was mostly pointing to the fact the core featureset surrounding the data store, in terms of integrating the data store with an observable pattern and permissioning. SQL VS NoSQL seems to be a bit consequential in that sense, you could probably integrate NoSQL stores pretty easily.
Googling Apart Oriented Programming yielded nothing. Did you mean [Aspect Oriented Programming](https://en.m.wikipedia.org/wiki/Aspect-oriented_programming)?
Desktop link: https://en.wikipedia.org/wiki/Aspect-oriented_programming *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^258727
There are plenty of environment specific optimisations already, so I wouldn't worry that they aren't generally applicable, because they may still have value. Like I say you could simply lodge issues instead of pull requests, that should be even easier if you have published code you can refer to. Obviously if your optimisations aren't as effective as Span&lt;T&gt;, that is an excellent example of where low level changes can offer real perf wins. I'm not sure why you bring up the full framework CoreFX is the BCL for .Net Core. As for your negative comments against me, sorry you feel that way. I've seen evidence from MS and others for noticeable performance improvements, I've not seen anything from you so far, so to be honest I'm starting to think you are trolling. Hopefully not, and you one day feel motivated to give back something to the community.
Thank you for the feedback! Yeah those podcasts seem interesting enough! Thanks. &amp;#x200B; I might be senior level but I want to feel confident taking on higher paid jobs because they'll definitely require a higher calibre of a developer. That's what I'm aiming for.
The repository layer isn’t obsolete, that’s just where you implement dapper/EF
Hmm. I think I thought that because I recently read this blogpost: [https://cockneycoder.wordpress.com/2013/04/07/why-entity-framework-renders-the-repository-pattern-obsolete/](https://cockneycoder.wordpress.com/2013/04/07/why-entity-framework-renders-the-repository-pattern-obsolete/) &amp;#x200B; It's a bit old but I think I agree with it. What do you think?
Yes, it's not bad.
Those are both terrible practices. We separate concerns so that items and layers can be replaced if need be. Not saying it happens often, but that’s the basis of MVC (the concept, not Microsoft’s brand). Having a repository layer means you can easily switch out the way you connect to a database, be it with EF (as much as I hate it and discourage its use) or any other DB tool. Mixing it in with the services is going to be a hell of a job to fix if the application requires you to do so.
So from your real world experience, how applicable is that? Do people, in your experience, employ the use of a repository layer? Or do they just use EF/Dapper/Other DB Tool instead? &amp;#x200B; Thank you for your input :)
Both. Using EF is fine, but do it in a repository layer. I have had project where we needed to pivot, and being able to go through a repository layer makes it a dream. If you have your interfaces set up well, you won’t even need to touch your service layer. Just switch out the repository implementation. Don’t get me wrong, that’s still time consuming, swapping out EF for SQL or flavors of SQL, but it’s better than looking through your services and figuring out which methods actually connect to the database, which might just be data manipulators, etc.
Doesn't AOP stand for Aspect Oriented Programming? Also, my worked used PostSharp but we fucking hated it and tore it out of everything possible.
&gt; To extend on the code already available for DbContextOptions Do you mean using `DbContextOptionsBuilder.IDbContextOptionsBuilderInfrastructure.AddOrUpdateExtension` and then `DbContextOptions.GetExtension`? There's surprising little information available regarding the extension of `DbContextOptions` with custom properties / values.
what made you hate it?
So in a Controller, Service, Repository pattern you're trying to isolate each layer as much as possible. Your controllers are your entry points. Keep these light weight. They're used for authorization and authentication, model validation, and mapping the result of the service to whatever your response should be, whether that's an IActionResult or a JsonResult. In my opinion anything to do with your web framework should be in your controllers so you can reuse your services or upgrade to newer web frameworks in the future. Services are your business logic. There's not much to say about them, this is where you do your bog standard coding. Do not make SQL calls here, do not return web framework specific stuff here. Repositories are where all your SQL calls go (dapper or EF). There should be little business logic here. The most business logic you'll do here is generally mapping input to output. An important thing to remember here is to use a connection or context factory or pass the connection or context so you can assure it'll be disposed (if you pass the connection or context then the services will have the factory). Just remember that these are abstractions and you don't want your abstractions to leak into each other otherwise they become useless. Without following some sort of rule set or standard you might as well be doing everything in the controllers.
We hated two things specifically. I don't really care to argue over the validity of opinions so keep the peace. AOP - It hides parts of the code so when a dev is reading a method/class signature they may not know this AOP functionality is being applied at compile time and so they *miss* party of the operations that will execute. This leads to difficult to identify bugs. It also is a can of worms if someone accidentally makes an AOP operation that performs great against one method but poorly against another. We had this case with a logging AOP. The dev used reflection to read every param property being inserted and log it. Well that was fine for all the methods with value types but we had one method that took an object with DEEP nesting and that AOP opp ground to a halt. Unit testing didn't catch it, QA didn't catch it, Integration testing missed it, Performance testing missed it because the method was not hit often enough. Production caught it and it was a massive week of effort to figure out what the fuck was happening because part of the code was obscured by AOP. PostSharp - They have a free version and a paid version. If you try to library with the paid classes it will fail to compile. If you use the free version they give you access to the paid version on the second Friday of the month (or some dumb shit like that). Well some Dev didn't know (no one did) and they used parts of the 'paid' version. Worked on his box, checked in, deployed to Dev, merged to Test, deployed to Test and was pushed to production branch for a weekend deploy (we have a GREAT process for rapid dev here), anyway, Sunday night build/deploy runs... except it fails... Monday we come in, no one's shit went out, weird build errors in all environments, no Dev can build. Why? Because the fucking license only builds PAID versions on the second Tuesday of the month.
Watch interview.io videos on YouTube. There are videos of senior devs on there asking other senior devs about optimized paths to solving problems. This helped me to start thinking about bringing down complexity and really finding the space and runtime trade offs between approaches. What I will do is watch an interview up to the question. Then I’ll think about it and come up with clarifications and edge cases. I’ll then watch how the interviewee responds. But before they write code, I’ll pause and come up with a solution and do it on paper. At your level, you should also look at systems design. What are the trade offs between DB types? How do you scale out only certain parts of your application? Design a tinyUrl application from scratch, etc. Good luck!
It depends on what medium you get the most out of. I personally think physical books is just blah for retaining information. If you enjoy videos and get a lot from them, I would highly recommend one of the online video subscriptions. If you are really wanting to learn the front end side of things, an [egghead.io](https://egghead.io) or front end masters subscription are very much worth the money. Egghead is super short, straight to the point videos. FEM is more workshop-ish, and usually dives into the why/how/theoretical side of things. Pluralsight is also great. There's also lots of free content out there too :). Don't think you have to spend money in order to get 'continued learning'
Users pull an entire new page every time they click on a link if you're talking purely about data transfer.
The way I have my repo layer set up is like the other guy is saying. It houses my Dd context classes and inherits from an interface I created that contains dbsets of the tables, and a save method. The repo layer implements that functionality but my core layer only needs the interfaces. So theoretically I should just be able to swap out my repo layer as long as it implements those same interfaces.
I just assumed it was because most people are building web applications and most of the interesting things to do with AOP are typically implemented as request middleware or as part of ORMs and few people use AOP proactively in their application design.
Down voting people because you disagree with them isn't in anyone's best interests. I never down vote people unless they are being jerks. This is a C# forum not an [ASP.NET](https://ASP.NET) fans forum. By down voting because you don't agree, you are creating the internet as echo chamber, you are discouraging debate and so on. It's in everyone's best interests if differing opinions are voiced and considered--not up voting is a more than adequate way to express disagreement. Ideally we would express disagreement by responding and not up voting.
Yep. I hated it within an hour or two of playing with it.
What about System.Reflection.Emit? I personally find codedom to be pretty useless anyway.
I'm actually trying to learn this kind of stuff right now, however, I am using the JSON variable substitution, so I'd be interested to hear the answer to your question. &amp;#x200B; I've managed to get my JSON variable subs to work, but I'm having problems getting the actual api to run on the azure services :(
Where is your hang up? I had problems with actually setting up azure correctly and creating all the necessary resources that were needed. My first big hangup was not having an azure active directory.
Is this in an MVC API or an Azure function? Honestly either way though, it is way easier just to store them as App Settings. There is relatively no difference. Then you can just access them as you would any other configuration if you are using MVC (through IConfiguration) or using Environment variables if you are using Azure Functions.
MVC API. Won't utilizing App Settings go against the level security i'm trying to set up?
There's no security difference. How are you currently deploying? You could just set it manually in the portal, it shouldn't be in code at all. You can use user secrets to do local development and that requires no changes to the .gitignore file. At runtime those app settings act as environment variables and will be picked up as standard.
Currently I have them in the AppSettings and when I push to github, my connection string information is public information because it lives in a publicly accessible file. I'm deploying VIA github: merging into master kicks off a build.
In the past I’ve used KeyVault Secrets to securely store connection strings, there an MS package to integrate them directly into your IConfiguration at runtime. There’s a working app using that here: https://github.com/rpj/et
But in Azure you could put them directly into the App Settings and they will act as if they did in the `appsettings.json` file. Then they don't need to go into source control at all. Then locally you put them in [user secrets].(https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2&amp;tabs=windows)
Those values won't have any effect in local. They only take effect in the remote hosted version. They aren't compiled into literal values either. You would ideally create the connection strings in your local config file - pointing to a local dB with different credentials. The Publish dialog in visual studio has got some options for setting up local and remote versions of certain variables these days IIRC
This is a guess, but I'm guessing (see the guess bit) that you have the mdf as a copy if newer item. So when you run the program it builds to directory bin/debug, then copies in the empty database. When you run the app again, it writes over it.
So how do I work around that then? Sorry, I generally work with full on sql server so I've never ran into this.
BTW I think you're on the right track. I added a bunch. Then closed VS and just ran the exe in the debug folder. All the data was still there. However, when I built again, it was gone.
Ah ok, this is what i'm confused about, and ultimately attempting to accomplish but not fully understanding how to implement it.
&gt; Marketing. Growth Hacking. At least you aren't trying to hide it?
It's fine, it's a common problem working with a local database. It's actually preferable in some cases, you can preload data and run repeatable tests for example, or deploy it for each user without worrying about them getting your data. The easiest way to solve it is to move the the database to a "database" directory under the solution and change the build option to "do nothing". Then you change the connection string to point to that location. Anything under bin will get wiped out, so it has to be outside of there. When you eventually deploy, simply use a different connection string to point at the correct file location. If you add a debug and release configuration file you can add do it all fairly transparently :) hope that helps.
So if I already have variables in my App Settings (above screen shots) how do I access that in my code? This is where i'm not understanding how this works. How do I tell my code to look at that when azure picks up the deployment and begins to compile?
what if you're deploying directly from github using another service like heroku or any other service that doesn't provide app settings. how will you typically handle this, seeing that you've excluded appsettings.Production.json in your gitignore file?
That does help. I think I can get away with hard coding the path and then telling my friend to put the file in the same path. Thanks
I'm not as concerned with local as I am with setting it to point to the correct values in azure app settings. Although I agree, the insights should be set up that way, reality is I don't have them set up that way. And it's rather obvious I don't understand app settings, so understanding insights and how to configure it seems like a entirely separate frustrating task.
[Read this](https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=sr_1_1?keywords=Code+complete&amp;qid=1558553840&amp;s=gateway&amp;sr=8-1). IMHO still the single best book on programming. The market seems to agree, as the price for used copies remains so high.
There are implementations. Here is an example https://nearsoft.com/blog/aspect-oriented-programming-aop-in-net-core-and-c-using-autofac-and-dynamicproxy/
If this is a .NET Core application, then it will be part of automatic `IConfiguration`. So in a simple Service... public class MyService { private readonly IConfiguration _config; public MyService(IConfiguration config) { _config = config; } public void DoSomething() { var connectionString = _config["DBConnectionString"]; } } Or something like that.
:) You can use ..\..\.. for example to go back up three levels of directory. It's better than doing C:\repos\kittens\database for example :)
So this is pretty similar to what I initially had attempted. Here's my code, that is still returning a 500 public IConfiguration _config { get; private set; } public Startup(IConfiguration configuration) { _config = configuration; } public void ConfigureServices(IServiceCollection services) { services.AddMvc() .AddMvcOptions(o =&gt; o.OutputFormatters.Add( new XmlDataContractSerializerOutputFormatter())); string dbconn = _config["DBConnectionString"]; services.AddDbContext&lt;HouseInfoContext&gt;(x =&gt; x.UseSqlServer(dbconn)); services.AddScoped&lt;IHouseInfoRepository, HouseInfoRepository&gt;(); services.AddScoped&lt;IFurnitureInfoRepository, FurnitureInfoRepository&gt;(); services.AddScoped&lt;IImageInfoRepository, ImageInfoRepository&gt;(); }
Haven't tried it. I like CS Script because it is real C# code, making it easier to debug.
You know. I only truly l, really understood KISS while trying to decipher convoluted, smart ass, spaghetti code written by devs over engineering stuff. I just write simple , well named code , using some n tier pattern and keep that fucking resharper linq refactor mentality out. Life as a dev is now good
When you say it 500s, does it give any indication a to what is wrong? You could add application insights to get a better idea. Have you moved the connection string to the AppSettings now?
That’s a great start. Good luck on your journey as a dev.
I have app insights added but it doesn't give me a very verbose error message. No stack trace. Or at least the verbose error is not very obvious or standing out
Hmm I was wondering what the shell/terminal had to do with Xamarin.Forms :)
If it's all set up right then, I can't imagine it is the IConfiguration not working, done this dozens of times with APIs and Function apps and never seen it not bind. I'm surprised there are no exceptions logged in Application Insights if it's returning a 500.
oh, there probably is. They just aren't standing out to me. I'm trying to go through each step to make sure i'm not doing something stupid, which is highly likely.
why? What real problem does this solve?
 System.Exception: at Microsoft.AspNetCore.Hosting.Internal.ConfigureBuilder.Invoke (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Hosting.Internal.ConfigureBuilder+&lt;&gt;c__DisplayClass4_0.&lt;Build&gt;b__0 (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Hosting.ConventionBasedStartup.Configure (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at AiHostingStartup.ApplicationInsightsLoggerStartupFilter+&lt;&gt;c__DisplayClass0_0.&lt;Configure&gt;b__0 (AiHostingStartup, Version=2.8.14.190, Culture=neutral, PublicKeyToken=31bf3856ad364e35) at Microsoft.ApplicationInsights.AspNetCore.ApplicationInsightsStartupFilter+&lt;&gt;c__DisplayClass0_0.&lt;Configure&gt;b__0 (Microsoft.ApplicationInsights.AspNetCore, Version=2.6.1.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35) at Microsoft.AspNetCore.Server.IISIntegration.IISSetupFilter+&lt;&gt;c__DisplayClass4_0.&lt;Configure&gt;b__0 (Microsoft.AspNetCore.Server.IISIntegration, Version=2.1.7.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.HostFilteringStartupFilter+&lt;&gt;c__DisplayClass0_0.&lt;Configure&gt;b__0 (Microsoft.AspNetCore, Version=2.1.7.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Hosting.Internal.AutoRequestServicesStartupFilter+&lt;&gt;c__DisplayClass0_0.&lt;Configure&gt;b__0 (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Hosting.Internal.WebHost.BuildApplication (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) Inner exception System.ArgumentException handled at Microsoft.AspNetCore.Hosting.Internal.ConfigureBuilder.Invoke: at System.Data.Common.DbConnectionOptions.ParseInternal (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.Common.DbConnectionOptions..ctor (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.SqlClient.SqlConnectionString..ctor (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.SqlClient.SqlConnectionFactory.CreateConnectionOptions (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.ProviderBase.DbConnectionFactory.GetConnectionPoolGroup (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.SqlClient.SqlConnection.ConnectionString_Set (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at System.Data.SqlClient.SqlConnection.set_ConnectionString (System.Data.SqlClient, Version=4.4.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a) at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerConnection.CreateDbConnection (Microsoft.EntityFrameworkCore.SqlServer, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.Internal.LazyRef`1.get_Value (Microsoft.EntityFrameworkCore, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.get_DbConnection (Microsoft.EntityFrameworkCore.Relational, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open (Microsoft.EntityFrameworkCore.Relational, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerDatabaseCreator+&lt;&gt;c__DisplayClass18_0.&lt;Exists&gt;b__0 (Microsoft.EntityFrameworkCore.SqlServer, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions+&lt;&gt;c__DisplayClass12_0`2.&lt;Execute&gt;b__0 (Microsoft.EntityFrameworkCore, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerExecutionStrategy.Execute (Microsoft.EntityFrameworkCore.SqlServer, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute (Microsoft.EntityFrameworkCore, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerDatabaseCreator.Exists (Microsoft.EntityFrameworkCore.SqlServer, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerDatabaseCreator.Exists (Microsoft.EntityFrameworkCore.SqlServer, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.Migrations.HistoryRepository.Exists (Microsoft.EntityFrameworkCore.Relational, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.Migrations.Internal.Migrator.Migrate (Microsoft.EntityFrameworkCore.Relational, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.Migrate (Microsoft.EntityFrameworkCore.Relational, Version=2.1.8.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at FHIStorage.API.Entities.HouseInfoContext..ctor (FHIStorage.API, Version=1.0.0.0, Culture=neutral, PublicKeyToken=nullFHIStorage.API, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null: D:\home\site\repository\FHIStorage.API\FHIStorage.API\Entities\HouseInfoContext.csFHIStorage.API, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null: 13) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitScoped (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.DynamicServiceProviderEngine+&lt;&gt;c__DisplayClass1_0.&lt;RealizeService&gt;b__0 (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.GetService (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService (Microsoft.Extensions.DependencyInjection, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService (Microsoft.Extensions.DependencyInjection.Abstractions, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Hosting.Internal.ConfigureBuilder.Invoke (Microsoft.AspNetCore.Hosting, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60)
You keep the app settings for the environment in Azure directly. At work these are managed with a combination of Terraform and Key Vault.
what if you aren't using azure
Yup, certainly looks like it doesn't like it. The only other thing I can think of at this point is that the `IWebHostBuilder` is set up in custom way. What does your `Program.cs` look like?
 public class Program { public static void Main(string[] args) { CreateWebHostBuilder(args).Build().Run(); } public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt; WebHost.CreateDefaultBuilder(args) .UseStartup&lt;Startup&gt;(); }
If I were to put a folder called data in my root of my sln could I just do /data/SomeMdfFile.mdf ? If so, that would make it so it's always relative, no?
I'm afraid I don't know much about how the other providers behave. I'd be surprised if they didn't have something similar. Otherwise you'll be stuck with either using appsettings.json or maybe an open source secrets management system. You could still use KeyVault even if you're not hosting the web application in Azure.
thanks, I'll check out keyVault or any other alternative out there
I'm pretty stumped at this point. Last thing I can think of. Remove the DbContext for the time being, and anything that relies on it. Make an API controller and inject `IConfiguration`. Make a GET end point and return the `config["DBConnectionString"]` Then at least you can see easily what the value is.
It would be better to put all the effort and manpower to the performance issue instead of shell, no matter how simple the ui is, you can't reach 60fps with xamarin.forms...
I like Blazor but man that is some spaghetti code.
OOOOK so, I added a test controller and commented out the dbcontext. Hitting the test endpoint throws an error and returns this stack trace System.InvalidOperationException: at Microsoft.AspNetCore.Mvc.ModelBinding.Binders.ComplexTypeModelBinder.CreateModel (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Mvc.ModelBinding.Binders.ComplexTypeModelBinder+&lt;BindModelCoreAsync&gt;d__6.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.ModelBinding.ParameterBinder+&lt;BindModelAsync&gt;d__11.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.Internal.ControllerBinderDelegateProvider+&lt;&gt;c__DisplayClass0_0+&lt;&lt;CreateBinderDelegate&gt;g__Bind|0&gt;d.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker+&lt;InvokeInnerFilterAsync&gt;d__13.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker+&lt;InvokeNextResourceFilter&gt;d__23.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Rethrow (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Next (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker+&lt;InvokeFilterPipelineAsync&gt;d__18.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker+&lt;InvokeAsync&gt;d__16.MoveNext (Microsoft.AspNetCore.Mvc.Core, Version=2.1.3.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Builder.RouterMiddleware+&lt;Invoke&gt;d__4.MoveNext (Microsoft.AspNetCore.Routing, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Diagnostics.StatusCodePagesMiddleware+&lt;Invoke&gt;d__3.MoveNext (Microsoft.AspNetCore.Diagnostics, Version=2.1.1.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Server.IISIntegration.IISMiddleware+&lt;Invoke&gt;d__13.MoveNext (Microsoft.AspNetCore.Server.IISIntegration, Version=2.1.7.0, Culture=neutral, PublicKeyToken=adb9793829ddae60) at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e) at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http.HttpProtocol+&lt;ProcessRequests&gt;d__188`1.MoveNext (Microsoft.AspNetCore.Server.Kestrel.Core, Version=2.1.7.0, Culture=neutral, PublicKeyToken=adb9793829ddae60)
And this is what my test controller looks like [Route("api")] public class TestController : Controller { private IConfiguration _config; public TestController(IConfiguration configuration) { _config = configuration; } [HttpGet("test")] public string getTestController(IConfiguration _config) { return _config["DBConnectionString"]; } }
The Pragmatic Programmer: From Journeyman to Master
Haha. I’m 10years+ been through DDD, CQS,CQRS, no pattern, pure luck ... it’s mental. Just KISS now.
Yes, but bin/debug is higher, so you'ld hve to ../../database. Have a bugger about, find something that works for you. If ti doesn't it's a simple fix :)
Your post has been removed. Self promotion posts are not allowed.
In my company we decided to use dryioc in .net core due to performance and features(dynamic proxy was in the top 3 reasons) This example gives you a great start https://bitbucket.org/dadhi/dryioc/wiki/Interception
Hey thank you for your suggestions. I am reading 2 books already, 1 for c# and another for [asp.net](https://asp.net) core mvc. I managed to get a copy of the book you suggested but it seems to be from 2017. Do you think it's still viable for 2019? Since core is kinda new.
Nice! Love seeing how open Microsoft has been lately.
XML Bdsm is probably best left off the ol’ resume
I thought they did this months ago.
They've been doing it piece by piece. The first part of WPF was open sourced several months ago, another big chunk of it was added now and I believe some pieces still remain.
But running whatever they put in a DLL is fine??
LightInject targets .NET Core/Standard and supports dynamic proxies. [http://www.lightinject.net/interception/](http://www.lightinject.net/interception/)
That's a hell of a diff, damn.
Fody is a PostSharp style AOP tool, supports NET Core. [https://github.com/Fody/Home/blob/master/pages/supported-runtimes-and-ide.md](https://github.com/Fody/Home/blob/master/pages/supported-runtimes-and-ide.md)
Just a different template syntax that I find to be more friendly.
AspectInjector supports NET Core [https://github.com/pamidur/aspect-injector#requirements](https://github.com/pamidur/aspect-injector#requirements)
I feel like if you're seeing hype, said hype is probably providing an explanation of what it is the hype is about.
Almost 2 million files, my god that's a big commit.
Nearly 4,000 files, almost 2 million *lines changed*.
I think my active directory group is managed by my organization, so all I had to do was add myself as admin on the database. I actually got it working and i was able to submit requests to it after getting it all configured. Pretty sure the problem was I deleted what I assume was the default index file, so I was getting a 404 from my root directory, but the service was actually running. Might just FTP in and out a redirect page to the documentation!
Woops hahaha seems I didn't read the labels that clearly there xD. Still a big ass commit though :P
Other than the documentation, I haven't found any good resources on ASP.NET Core. Usually I have to poke around in the source code to figure something out. Regarding books, I swear by Dependency Injection in .NET (or the new edition, Dependency Injection Principles, Practices, and Patterns). While it does focus on dependency injection, I never quite knew how I should be architecting things until I read it, and then it all made sense.
Someone needs to make a coffee cup with KISS on one side and YAGNI on the other. I would never need to speak in the office again.
You don't separate out your data layer just so you can replace it, you do because it is a logical way to simplify your application. Need to do something database / datalayery? Its over there in that logical block of isolated code.... The whole 'can switch out' and unit test stuff is just a bonus.
haha, yeah auto correct gone wild
To be fair, reference source has been available for a while now. Allowing outsiders to improve and bug fix is a win win for everyone and a huge step.
They hope someone can fix their blurred text bug...
Hey, I’ve just checked and it seems like your version is the latest one by this author. The .NET Core platform is still growing, so yes, there can be minor differences between what is in the book and the current framework. I don’t think this is critical, especially if you learn C#, but just keep this in mind.
Ok, thank you. Do you recommend any particular books for that?
Not really. I was thinking more about creating custom extension methods to set properties which are not already available to the fluent API. But I must admit that I has been 3-4 years since I worked with Entity Framework last, so my memory is definitely blurry :)
That doesn't look quite right. Should be something like [Route("api/test")] public class TestController : Controller { private readonly IConfiguration _config; public TestController(IConfiguration configuration) { _config = configuration; } [HttpGet("")] public string getTestController() { return _config["DBConnectionString"]; } } You can inject into controller methods but it's usually not necessary. But as an example you could do [Route("api/test")] public class TestController : Controller { private IConfiguration _config; [HttpGet("")] public string getTestController([FromServices] IConfiguration config) { return config["DBConnectionString"]; } } And that should also work.
Where do you experience blurred text? &amp;#x200B; Most of my application have no (real) issues regarding blurred text. I run them on a 24" Desktop in 1080p and a 13" Notebook with 1440p 150% scaled. They all look ok to me. I run them on .NET Framework 4.7.2 (soon core ;) ) and have applied the settings for Per Monitor DPI, so that they work across different DPI (eg. projector and notebook)
So... You have actually no clue about this. You can't add properties to the `DbContextOptions`. Inheriting from it will lead to various incompatibilities in the EF Core eco-system.
I'm not sure I understand the problem. Are you getting NULLs when your app is deployed and running in Azure? Or do you want to run your app locally while retrieving the connection string from Azure?
Afaik. Microsoft never found a bullet proof solution to managing ClearType in WPF ... I might be wrong. Occasionally the text would become blurry or harsh.. I haven't coded WPF for years so im just referring to what i remember being a big issue over the years. https://richnewman.wordpress.com/2011/12/20/blurry-text-with-small-fonts-in-wpf/ It's an old article i know, just wanted to point to what i mean.
To be fairrrrr
That confirms my thoughts, WPF is a bloated library
So when you say KISS and then say you've tried all of these patterns (I have a senior developer in my company who likes CQRS pattern), which pattern would implement KISS? n-tier? Or spaghetti code? haha
Well, now's your chance to fix things up...
I believe I’m getting NULLs when deployed and running on azure. Not so much concerned with how it runs locally.
try: public partial class LoadTable { \[Key\] public Guid Id { get; set; } } eg use data anotation to let EF know this is a keyed value....
I’l really surprised they MIT licenced this like they did for the rest if net core. This means even if they stated they wouldn’t accept PR for non windows support it’s possible to branch out. WPF everywhere would rock
Sorry, but no. That’s poor code. You should have an independent layer. Separation of concerns.
It looks like you're attaching a brand new `LoadTable` object to the context, and then setting it as "Modified". You can't do that. Modified objects must already exist in the database.
As the object already does exist in the database, what would be the correct way of doing it? Does `Modified` not track some or all of the values? Or am I thinking of it the wrong way?
Thanks, unfortunately that didn't work. First had the error still, second didn't have the error but also didn't update the database. But also, would that cause issues when creating a record?
CQRS ... is nice but it’s must be driven by business. CQS is a very nice option for things like MVC and can be dev driven. KISS is just about keeping it simple in whatever pattern. Usually when you find yourself pulling hair out because of something it’s because somebody bolted KISS.
Your controller needs a constructor for the SyncRequestPort to be passed into. Try: `public ReservationController(SyncRequestPort client)` `{` `_client = client;` `}` The controller factory will see the parameter and know it needs to instanciate and inject that service into the controller.
Why not skip JS all together and go straight to Web Assembly, by using Blazor?
Would love to see performance improvements between core3 and .net framework
Storing each visit in database seems really expensive and not scalable. You should consider using something like Redis, you could use keyspace notifications with Expire for debouncing.
Which exact line triggers the error? I assume it's `_context.Attach(LoadTable).State = EntityState.Modified;` based on the error message, however your phrasing indicates it could also be ` await _context.SaveChangesAsync();`. Have you checked the actual value of the `Id` property to ensure it really is what you're expecting it to be? As a solution, consider using the context to retrieve the record again, then copy the values from LoadTable to the new instance of the record and then save that one: LoadTable table = await _context.LoadTable.FirstOrDefaultAsync(m =&gt; m.Id == LoadTable.Id); table.SomeProperty = LoadTable.SomeProperty; await _context.SaveChangesAsync();
That might be an option. Seems like it has a lot of overhead though, what's the performance like comparatively?