Adding others for reference IDesign C# Coding Standard: http://www.idesign.net/Downloads/GetDownload/1985 Coding Guidelines for C# 5.0, 6.0 and 7.0 (Open Source): https://github.com/dennisdoomen/CSharpGuidelines/releases 
My boss had to find a cheap hosting option for some little site and asked me to take a quick look at Azure. I quickly realized that it was going to cost more for me to figure out how much it would cost on Azure than it would cost for a year of hosting on the cheap host.
Oh my god yes! The first few were not my style but the one of the woods snd the shot if the field, uhhhh, I need it please! Why the water marks though?
I thought it was the SignalR demo with a login added, but its pushing some service/product.
I didn't notice the watermarks originally. That's the site where I downloaded them from. I hate watermarks, but the pics are high res (I downloaded them for my Surface which is crazy rez), so the watermark was smaller than normal. Here's a link to my desktop album. They're not all amazing, but I haven't had time to curate it in a while. There may be some dupes. I like low-contrast nature images, it's less strain on the eyes, less distraction, and maybe a little de-stressor from work. https://imgur.com/a/ReJVv 
OData is great, especially Visual Studio Scaffolds for creating OData Controllers. But I would use it for an internal service to make things easier, for example admin related end-points. Also would not choose GraphQl, it's weird. 
I joined midway in a project that uses it with asp. Net mvc api. It was absolute shit tbh. Could be the way it was set up, but we had problems since day one. And documentation and stackoverflow were sparse. We now have a Frankenstein system where some endpoints are odata and others are plain mvc. 
Yeah, I want this! You need beta testers right?
What do you mean by updated realtime?
Considering that most displays are 16:9 I just took the photos I liked and cropped and scaled them down to 1920x1080 thus for getting rid of the water mark.
Making changes outside the software automatically gets updated.
Later down the line yes. I will need a ton of testers. The software is to assist in game development and time management.
I'm not going to lie, I have never used a code repository or source control before. How exactly does GitHub work? And didn't Torvis invent it?
Just use the github gui. Sign up for an account and download the github desktop client. There's a small tutorial that gets you going.
Now would that be the full source code I'm uploading or the compiled project? And if it is source code what happens if a revision is made? I mean wouldn't it be a cluster fuck if 10 people are editing at the same time? GitHub seems very valuable and would like utilize it.
For the main repo you'll be the sole maintainer. In other words you'll be the only person directly making the changes. People will submit "pull requests" that are changes in code. You can use the desktop client to do code merges from pull requests. Git is a wonderful thing to the open-source community (and private repos with multiple contributors ). I'd recommend spending time to learn it a bit. 
Is there a link without signup? 
Jesus..winforms?
Nice, thanks!
Nice. What versión Of Core is? 
Yup lol. I thought it was a play on "lit af" or something.
You can still write one, but it isn't nearly as "universal" anymore without Windows Mobile. You could also write a traditional desktop app in C#/WPF and utilize Centennial to port it the store.
Currently it still looks like the future is UWP and the desktop bridge is only a stop gap solution to bring Win32 apps into the store. Everything on Channel 9 is about UWP, and the majority of new Windows 10 APIs, including the Fluent design built on top of the new visualization layer, are only available to UWP apps.
Fair chance that it will get dropped or stale. History teaches you that, doesn't it?
latest
For Web Forms I totally agree, especially the RadComboBox is a control developed by the devil himself. Buggy, unstable and very bad UX. However, Kendo does provide a few alternatives here and there. If you're building dynamic user interfaces kendo widgets work well, as well as the grids. Though some features are obviously still build for the online demo and once you start using it you find all kinds of limitations.
And that javascript engine running a VM of Windows 98?
I've been switching between VS and Rider. A couple things I find annoying about Rider: * You can't dock the test and build output windows as Docked Tabs like in VS. Instead you get these pop up windows * Built in source control tooling is another pop up window with pop up windows for diffing. You can't dock the source control window as as a tab. So you have a floating window in the middle of Rider with pop ups all around, while you diff and add comments * Also in the source control tooling, no option to sync. Instead you need to commit and push, which opens another window where you confirm and push to source control. In VS, this is a selection of links: commit, push, sync. Rider has too many clicks for this part of committing * Darkula scheme needs to be more like Visual Studio Dark. The fonts are too light and thin in default scheme. I had to edit the scheme to Consolas Bold and increase font size to have them show up better in the menu and toolbars I'm still messing with Rider, but so far I like that it is less resource intensive than VS and runs much better on my i7-6600U laptop with 16GB RAM and an SSD. My CPU isn't pegged at 100% and half my RAM isn't gone. VS2015 with Resharper in comparison is very resource intensive, but with better usability. 
So a pull request in essence is someone say "hey over here! I made a change!" And once you accept their request it gets added to the original source?
There was also a time when everything on blogs was about winforms
Once upon a time Apple docs were all about Carbon as well.
This is really the case here. The benefit to targeting WPF, is that you can also distribute your app to anyone running on x86 or x64 hardware, regardless of Windows version. So that means windows 7 users who are still a sizable market share. If you use UWP, you can only target Windows 10 users. UWP has other "issues" with it as well. First, UWP is distributed by WinDev, so it only sees updates when Windows does, this also means that if you use the latest and greatest APIs, you can only distribute your software to users that already upgraded their OS, with MSFT rolling updates, this can take some time, where as WPF is maintained by DevDiv, so it is part of the .Net Framework, which can be upgraded and installed as part of an installation process. Also some of UWP's API for doing what a traditional desktop app might do are, well, different. With the coming Win32 ARM transpiling stuff, I think the case for UWP is even more diminished (IMHO). Theoretically, x86 Win32 apps will run on this new platform without recompiling and they claim battery life should be fine which was once an argument for UWP, that it plays well with extending battery life. 
[Here's the same guy (Stephen Cleary) making a async http request that updates the UI using the MVVM pattern.](https://msdn.microsoft.com/en-us/magazine/dn630647.aspx) If you read his async mvvm articles you will also learn how to make a cancellable command that notifies the UI when its status changes.
The Windows community show done yesterday used C++ UWP applications as an example of remote debugging.
Pretty sure they won't drop UWP, it will just keep on evolving. Isn't the future pretty tied up to .net core and .net standard now?
Worthless blogspam.
I am not spamming. I am just a beginner and shared what I learned today. I might not be as expert as you might be but that doesn't mean I am spamming :(
ASP.NET Web Forms? Sure, let me just fire up my Model A Ford and I’ll be right over…
GC.KeepAlive is the problem and seems unnecessary. I've never actually used windows forms but this is my naive crack at it: static class Program { /// &lt;summary&gt; /// The main entry point for the application. /// &lt;/summary&gt; [STAThread] static void Main() { Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); using (var context = new AppContext(new Form1())) { Application.Run(context); } } public class AppContext : ApplicationContext { private Mutex mutex; public AppContext(Form mainForm) : base(mainForm) { mutex = new Mutex(true, "MyMutex", out bool createdNew); if(!createdNew) { MainForm.Close(); MessageBox.Show("An instance is already running."); } } protected override void Dispose(bool disposing) { base.Dispose(disposing); mutex.Dispose(); } } }
GC.KeepAlive is the problem and seems unnecessary. I've never actually used windows forms but this is my naive crack at it: static class Program { /// &lt;summary&gt; /// The main entry point for the application. /// &lt;/summary&gt; [STAThread] static void Main() { Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); using (var context = new AppContext(new Form1())) { Application.Run(context); } } public class AppContext : ApplicationContext { private Mutex mutex; public AppContext(Form mainForm) : base(mainForm) { mutex = new Mutex(true, "MyMutex", out bool createdNew); if(!createdNew) { mainForm?.Close(); mutex.Dispose(); MessageBox.Show("An instance is already running."); } } protected override void Dispose(bool disposing) { base.Dispose(disposing); mutex.Dispose(); } } }
Can't see them dropping UWP. It's very much in active development (with .NET Standard 2.0 making life a lot easier). I've been developing for UWP since a couple of months after it came out (mostly personal projects, and a few work projects (touch screen devices, mobile scanners etc.). It's honestly been a beauty to work with. Backwards compatibility is super easy also (conditional code and condition xaml allow you to support newer features on newer versions of Windows 10, while still keeping support for older versions.
Outsystems is disgusting. You can do better with RAD tools and frameworks that give you full control.
Not to be rude, but can you be more specific? What didn't you like about it?
Since you deleted the whole database, you don't need those autogenerated migration files anymore. Try removing everything under the Migrations folder and try to update-database again 
http://www.earthclassmail.com are scamming people.
http://www.earthclassmail.com are scamming people.
[Windows Community Standup, Always Connected PC ](https://channel9.msdn.com/Events/Ch9Live/Windows-Community-Standup/Kevin-Gallo-January-2018)
Waay more developer friendly and following more "standards" than sitecore does!
As a traditional Content Management System I think Umbraco is quite nice. It's very "close" to ASP.NET MVC, meaning integrations are "easy" as they are on .NET in general. I don't know any other CMS as close to .NET as Umbraco. Also, it can run fully headless if you want something for SPA. However, I don't think it can run on .NET core yet.
We started using OS little over a year ago, while we're a mostly a MS / .Net shop with over 70 devs. OS lives up to its promise when it comes to apps that mostly stand alone; those in charge of their own data and with little integration with other apps. So mostly application in a business' supporting / secundairy processen in out case. But it lacks source control like you're used to. It knows versions, but no true branches. Trying to automatically deploy is a pain, although the OS claims to be working on that. And it, by design, does not support the concept of null when it comes to consuming webservices. This almost guarantees integration issues. My personal opinion: see it a webbased / mobile ready, more mature version of Access, rather than a way to speeds up .Net / webapplication development. 
First of all, that’s your model, not your controller. Secondly, has your model changed since the last time you created a migration? Maybe you should delete the Migrations folder and create another “InitialMigration” before updating the db.
I wonder what the time would be for a database style B-Tree.
Either use Fluent Migrator if you want to stick with CLR bits or use Liquibase if you’re open to JVM for your migrations.
Doesn't support null? What does that even mean? If something comes back null from an external service, it crashes or gives it a default value?
The latter, if a response gives back null, OS converts it to the default value defined for that type. You _can_ work around it, but you'd have to build a 'XIF component', witch it essentially a .Net based dll used like an extension / plugin. In other words: by writing the client side yourself in .Net, defeating the whole RAD / codeless point of using OS in the first place. For the record, I didn't work with it directly, but it was used a client for a WCF service my team made, which turned out te be &lt;understatement&gt; not the scenario where OS really shines &lt;/understatement&gt;. The combined end product (OS + wcf based backend) was however received positively by it users. 
[shmig](https://github.com/mbucc/shmig) is my tool of choice for managing migrations. You still have to write all of the queries yourself, but it makes applying them much easier.
If you delete all the migrations then you'll have to do a `add-migration` first to rebuild your initial migration.
Nice article! I just had a lecture in my team to introduce the reasons why we are slowly moving from TeamCity-generated build configurations to ones simply executing a cake script. 
I worked at a place that used it 7 - 9 years ago, so some aspects might have changed ... Background: the company had hired a new CIO who wanted to use it as a way of being able to produce features faster. We ended up hiring someone from Portugal to do the actual work - we still had our existing web app and didn't really have time to become experts with something so completely different. Spoiler alert: it's not so easy that "even a librarian can use it" (per one of their video tutorials). I think it would have been fine if we were just starting and we were using that tool, but since we already had an existing app, we had these challenges: - integration with existing asp.net app. Like another commenter said, you basically have to write the extensions in C# anyway to make that work - slow start up time. Granted this was only once per app load, but it was extremely slow. - price was around $100k / year ... which didn't seem that cost effective. Plus in our case we had to hire the specialist from Portugal - documentation was video only - the deployments were a huge problem because of how differently OS worked compared to a conventional web app. We eventually had to have consultants from OS itself come in to help with getting it to work right. Again, this might not have been an issue at all if were just using OS exclusively. When we were talking to their sales people they said most OS clients used it in a mixed environment, but it was clearly designed to work as a stand-alone app; integration with an existing web app didn't have first class status. - trying to use CSS or JS was very non-standard, and another pain point as far as re-use of existing assets. 
Hopefully op doesn't mind this side question: I'm using dapper as well, but I don't know how to store my SQL queries. Right now I'm saving them in a *.SQL file in my solution folder. This is all internal, used by coworkers so it's not a huge security risk, but I'm wondering how the pros handle this? 
I’d write a separate worker, have it listen to the queue, and call the api over http. 
What drove costs up so high? The pricing right now is, I think, $2500 * 12, so $30,000. That covers 200 "objects," they call them, basically any table, page, or endpoint. I assume the number of objects balloons quickly. 
I found the `dotnet` CLI to be great for basic CI integration. It does most of the basic tasks perfectly. Along with bash/powershell and Docker, it satisfies the needs of most small projects. But if you need more control, like control nuget caching or clean up, you would probably need more. 
I don't know what the details of the deal the company made with OS was, but I do know that they got it down to around 5 - 6K / month after a new CEO took over. I don't recall hearing anything about pricing per objects ... 
We use RoundhousE for our DB schema changes. It does require a decent SQL knowledge but it's approach is quite straightforward.
Do you mean DB migration scripts? You want to look at RoundhousE.
Fluent Migrator is great!
Oh. I needed more detail. All the SQL scripts I call from my program and from dapper are stored in txt files. I save them as SQL files. I was wondering how people store all their SQL queries. I know stored procedures are popular with some, but how do the others do it?
Stored procedures are the way you want to be doing it. Stored procedures are analysed by the server and execution plans are generated so they are as efficient as possible. Doing it your way means the server has to generate an execution plan every time it runs the query which will be inefficient and add load to the server.
This is interesting. I'll read into it further. Does this matter at all when it's less than a hundred queries a day? Thousand? Million?
i normally use a SSDT project in visual studio to handle all of my schema changes, migrations, script generation etc. The only download of this is that your poco's and schema aren't synced so you have to update 2 places. I also like to use this because then i can use powershell to apply the DACPAC to my SQL server DB, which i've created as a data tier application. Then updating my database with a well defined, versioned schema is as simple as a single command. In VS2017 they also have RedGate ReadyRoll, but i haven't personally used it.
My code looks the same but my SQL query is very large so putting it in code is terrible looking. Visual studio allows for pretty SQL code if you put it in a SQL file. 
If you can switch your API to ASP.NET Core, you can use the IHostedService interface. [https://www.stevejgordon.co.uk/asp-net-core-2-ihostedservice](https://www.stevejgordon.co.uk/asp-net-core-2-ihostedservice) ASP. NET Core sites are basically self hosted. When that interface is implemented you can run your app as a background service and an API endpoint in the same process. This would allow you to also listen form Azure messages. Something similar probably exists for WebAPI/Owin but I can’t think of it off hand. 
I actually still use Entity Framework for that. While I prefer Dapper for my repository classes, I leave EF around simply to manage migrations.
Possibly not, but it'd still be a good habit to get into.
It might be simpler now, but it will come back and bite you the first time you have to tweak a query in a production system and realise it will require a new release of your codebase. It's much easier to tweak a stored procedure in realtime without any downtime. It is not good practice to embed SQL in your code.
If staying the .NET Classic, usually you would build your message queue listeners as standard Windows Services. (I'm looking to build a small framework that abstracts the differences between Windows Services and IHostedService.)
Is there a particular reason you want to use a web api project specifically? I mostly use Azure Webjobs for this purpose. You can just run it as console app that hooks up to various things like Azure Storage, Azure Storage Queues, Service Bus Topics and Queues, Azure Event Hubs, and many other such event generating sources. This console app can be run manually on Azure Websites and be managed through the portal and a KUDU module on the App Service. You could also run it in a distributed fashion on some other kind of infrastructure. You don't even necessarily need to use Azure at all, but the built in KUDU monitoring module is fantastic.
And if using MSMQ specifically, you can build a COM component that is hosted directly by the OS and attached to a queue. Please don't. It is as brittle as fuck. I just want you to know that it exists in case you see it in the wild.
There was a cool OSS library called https://github.com/aeslinger0/sqlsharpener but the only update in a long time was my was my PR from early last year to make it work on VS2015. It works alright for some basic use cases, but failed in some more further down the line. I'd like MS to release a tool to auto-gen a POCO model from an SSDT .sqlproj.
If you're already proficient with .net, I don't think OS will buy you a whole lot. When you program, what takes more time? The actual typing, or the thought that goes into design and architecture? If you're only making a generic web app, OS might be feasible, but for anything that strays from the norm, OS doesn't really have an edge, but you still have the monthly cost + overhead of having to learn and maintain a platform that you will probably never encounter again once you move on from your current job. And if you have to integrate with an existing app it gets really hairy.
Yet another PowerShell build tool is [Invoke-Build](https://github.com/nightroman/Invoke-Build). 
.NET is really just a compliment of library’s ASP.NET is a compliment of web library’s .NET alone can’t be used for web development but useful for command line tools or services. These may or may not rely on databases or IO access.
Use a database.
This winforms still works an will probably work till the end of time. You will not get new features. That’s all
[Please mention crossposts](https://www.reddit.com/r/learnprogramming/comments/7rx4vh/what_is_net_is_it_a_web_development_framework/)
For integration tests you don't need RestEase nor TestHost. Just use Kestrel in the test, and a hand built Rest client relying on HttpClient, that way you not only test the API but also the client.
&gt; .NET alone can’t be used for web development What do you mean? ASP.Net will run in IIS and works just fine, ASP.Net Core can run as a self-hosted process
Yes, not all of .Net is ASP.Net, but all of ASP.Net is .Net, so what's your point? You can still do web development completely within .Net
No.
You can do web development completely in asp.net The op asked about .net not asp.net
ASP.Net is a subset of .Net Am I missing a joke or something?
Nah, I think he just dumb enough not to understand subsets 
No to what? 
It’s additional library’s over and above the .net framework. It’s not included in vanilla .net but requires it as a dependency 
.NET is a subset of ASP.NET not the other way around. ASP.NET is it’s own framework which uses .NET framework Writing web applications using just .NET simply isn’t feasible
https://stackoverflow.com/a/3103433 &gt; ASP.NET is a web application framework from Microsoft, which is part of the .Net framework.
Help others and give answers instead of shitreplying.
The title?
Look at the accepted answer with 7x the votes instead, ASP.NET is not part of the framework. It's an add-on.
Lol, i will just ignore you. Troll.
On top of is the as a part of. I build business applications on top of .NET, they don't become part of the framework.
Entity Framework is generally the preferred .NET way. There are a few different connectors to point it to a different DB engine than the default MSSQL. I am currently migrating a project with about 50 tables from EF6 to EF Core using this connector: [Pomelo.EntityFrameworkCore.MySql](https://github.com/PomeloFoundation/Pomelo.EntityFrameworkCore.MySql) Just make sure you omit this line from their sample code: &gt; context.Database.EnsureCreated();
Since you're using AD for authentication I'd assume your application is using Windows Integrated Authentication. If that is the case you could probably leverage the using System.DirectoryServices.AccountManagement namespace. Then you could write a method like this to check for a specific role which could be used to build your list of nav links: public static bool IsMember(string UserName, string GroupName) { try { UserPrincipal user = UserPrincipal.FindByIdentity( new PrincipalContext(ContextType.Domain), UserName); foreach (Principal result in user.GetAuthorizationGroups()) { if (string.Compare(result.Name, GroupName, true) == 0) return true; } return false; } catch (Exception E) { throw E; } }
This might be easy to add to Topshelf. 
To get started I would do the following: ``` $ dotnet new web ./path $ dotnet add package Pomelo.EntityFrameworkCore.MySql $ dotnet add package Microsoft.EntityFrameworkCore.Design $ dotnet add package Microsoft.EntityFrameworkCore.Tools.DotNet $ dotnet ef dbcontext scaffold [insert your connection string here] Pomelo.EntityFrameworkCore.MySql ``` That should set you up with all of the class files you need to get started working with the database. 
You can format it nicely in C#. I definitely wouldn't load in text files to run SQL queries. Maybe the traffic isn't enough to see it, but I have to imagine there's a performance hit doing that every single time. SQL inline is an option, stored procedures is an option... I would pick one of those.
How is your queue implemented? What service is that implemented with? Your queue either needs to be smart enough to be able to trigger an action when the message arrives or, you're going to have to poll it and pull on a continuing basis. AWS Simple Queue Services (SQS) allows you to trigger a Lambda function for each message as it arrives. So your workflow would be something like * Message sent * Message received * SQS calls Lambda function * Lambda function calls your API endpoint In the alternate scenario where you have to poll the queue and check for messages, I'd say you have a couple of options: * A separate service worker which triggers on a schedule (every minute?) to check the queue for a message then call an endpoint on your API for each message. * You could have an endpoint in your API which polls the queue and handles the new messages. Then setup a separate service worker to call that endpoint on a schedule (you could use third party tools which perform basic HTTP requests in a CRON job like fashion) * Similar to the point above, except you use Hangfire in your project to run a background job on your API every minute to check the queue and process your messages. I personally like the idea of de-coupling the API from the message queue worker so I definitely prefer those approaches over this. 
You could give Dapper a look, it's quite light-weight, and works with MySQL. Here's some sample code - https://github.com/StackExchange/Dapper/blob/master/Dapper.Tests/Providers/MySQLTests.cs
I would be quite interested to see your work I'd you get the time to upload it. I know how tough open sourcing even small bits of code can be!
Just install the mysql nuget package.
It is necessary, why make work for yourself when it's built in.
Strictly speaking it isnt. It is a feature of the framework. What ever suits you. Go build something.
Most SQL engines will also cache parameter query execution plans, which is what Dapper uses. As long as you aren't concatenating strings together manually to form a SQL query with params then you shouldn't be paying the plan tax.
Sure. By that logic LINQ isn't .net, neither are generics, or DataTable / DataSet. .Net is a collection of libraries that "do stuff" if you were to exclude everything outside of the System namespace and basic library then. Net would be useless. Define .Net as what is **is**: a set of libraries from Microsoft which make a framework. ASP.Net is a part of that, not some random thing that comes separately
To be fair, one of the biggest advantages IMO is data binding, both one and two-way. I would definitely make it your goal to learn how to use the data binding.
If you are looking for the second part of this, here it is: https://koukia.ca/a-microservices-implementation-journey-part-2-10c422a4d402
You can do .NET web development without ASP.Net, it's just the most popular framework.
Whew. I thought this was /r/shittyprogramming first.
how do I run a nancy app? 
I'm already using a build system: MSBuild. This was not even covered in this blog post.
Actually, I'm not really *covering* build systems there. It is an enumeration of benefits. Some of them do not apply to MSBuild as-is. For instance, I don't consider MSBuild to be easily extensible nor does it support control flow constructs in a convenient way. That's why I'm talking about *modern* build systems... but again: covering build systems was not the intent here... I wrote a different post about this some time ago, if you're interested.
I will add this asap... I see you also commented about the psake project build not actually using psake... do you know any representative psake script to link to? 
https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/implementing-basic-crud-functionality-with-the-entity-framework-in-asp-net-mvc-application
*chuckles*
Can't tell if this is a joke. .NET is a framework for creating Windows applications and applications for Windows phone. If you've heard of MONO, you've heard of .NET.
I've been programming .NET on a Mac for a few years now, programming without Visual Studio is perfectly feasible. You can use Vim, Emacs, VS Code and a host of others.
You can use all of the mono tools [from the command line.](http://www.mono-project.com/docs/tools+libraries/tools/) 
Thank you for adding *Invoke-Build* to the list, I am sure some people may find this tool useful. As for a representative *psake* script, I do not know. Here is their [Who is using psake?](http://psake.readthedocs.io/en/latest/who-is-using-psake/) with some links. 
Why Nancy/Mono? Why not just use .Net Core MVC?
The SmtpClient has been marked Obsolete so you should either choose your own library or use the MailKit as per Microsofts suggestion. https://www.infoq.com/news/2017/04/MailKit-MimeKit-Official
Literally the first line of the article: &gt; TL;DR: Yes. Yes you can.
Next time read the complete article, &gt; You probably should not use this in production unless you know exactly what you are doing. The original question was regarding a single property to know how a Windows service was configured to start. That is probably safe unless your code uses other libraries that use code compiled for a later framework version which does not take this hack into consideration. I would argue that most libraries does not :)
Ah then that is why you just posted the TL;DR part, I see.
I've been using .Net Core MVC and I like it very much but Nancy is something that I've been curious about. I just wanted to build a small application for fun.
Thanks, will give it a look.
You can use Nancy with .net core. We have several apps at work running on this stack and it works perfectly. You’re not required to use asp.net MVC with .net core btw. 
MVVM is very common in WPF. If you don't use it, you may have difficulty finding help, be it online or to have other people work on it.
If you don't execute methods that don't exist, then your code will work. It's a terrible idea to do this intentionally. Knowing that thus is possible is helpful for basically one thing: understanding what happened when a user comes to you with this strange missing method exception.
I mean, isn't this what ViewModels are for?
We've been using [this POCO generator](https://visualstudiomagazine.com/articles/2012/12/11/sqlqueryresults-code-generation.aspx) and it works great; modified it slightly to initialize strings to Empty and to make C# the default language and to pre-populate the connection string with our test database. 
This is *exactly* the kind of resource I was looking for last Friday. Thank you!
Something like this: &lt;?xml version="1.0" encoding="utf-8"?&gt; &lt;configuration&gt; &lt;apikeys&gt; &lt;/apikeys&gt; &lt;packageSources&gt; &lt;clear /&gt; &lt;!-- Custom package feed --&gt; &lt;add key="[Package Feed Name]" value="[Url to package feed]" /&gt; &lt;!-- Default package feed (NuGet) --&gt; &lt;add key="nuget.org" value="https://api.nuget.org/v3/index.json" protocolVersion="3" /&gt; &lt;/packageSources&gt; &lt;/configuration&gt;
This looks like a great list of various topics. Excited to check some of these out! 
Is this book better than the one provided here by Microsoft? https://www.microsoft.com/net/learn/architecture
If you are not using Razor views, you can use VSCode, because it doesn't support Razor templates intellisense (and it won't for a long time). Otherwise Rider is pretty good.
It's cool SaasKit can resolve the tenant for you, but is there a way to make the entity queries include the tenant filter by default?
Not mongo, and not raven. I Mingo has had historically bad concurrency issues, and as far as I remember raven was dropped my Microsoft's support long ago. Document DB is expensive, so don't use that. Really, there isn't a great one as far as I know. Either Microsoft directly or indirectly bad done a really good job of getting people to zero in on MSSQL
I've used both Postgresql and Mongodb and they both behave differently as document stores. I hate to admit that I've even used Sqlserver as a document store (not my choice and was pre-json support). Postgres with npgsql is a breeze so I'd say start with postres then try to solve the same problem with mongo. One big benefit of Mongo for me (if using the right driver) is the ability of the api to create collections on the fly, meaning no explicit table creation calls need to be made. We currently have been using mongo with C# for about two years with little problem.
Citus won't let you rebalance your shards w/o paying 1000s per month, keep that in mind.
RavenDb support has never been dropped because it has never been a Microsoft product. It's developed and supported by an independent company, Hibernating Rhinos. And they just released a .NET Core version. Compared to MongoDb I would just expect it to be much faster based on my experience with MongoDb compared to other RDBMS themselves. But I'd like to see benchmarks. An obvious difference might be the pricing model though.
If you are not yet ready to jump to a full document DB you might also play with https://github.com/sebastienros/yessql which provides a document DB layer above MySQL, PostgreSql, SQL Server or Sqlite. We use it in Orchard Core CMS with great success.
Can you say which drivers you have tried and which one you preferred?
I believe we're using an older version of the official [MongoDB driver for dotnet](https://www.nuget.org/packages/mongodb.driver). [Here are instructions](https://docs.mongodb.com/ecosystem/drivers/csharp/) on how to get started. The C# mongo api does have Linq extensions, but querying is a completely different beast than Entity Framework. [Npgsql](http://www.npgsql.org/efcore/index.html) is also a nuget package if you haven't already used it. Make sure you use the Npgsql.EntityFrameworkCore.PostgreSQL package for EF Core support. 
I was actually reacting to your comment on different drivers for MongoDb.
The one I linked to was the one I’ve used. I did a quick search for others and didn’t find any others for c# (not that I put much effort into the search). 
This is perfect, thank you. I've been looking for something like this for a while
I am not sure what a document database is but on my side: * Used Postgres, never had any issues, it smoother as it had been * If I need no-sql or indexing capabilities I use https://github.com/hhblaze/DBreeze
Glad people like the blog, enjoy working your way through all the linked posts!!
Thanks for your amazing contribution
Is NBitcoin just a bitcoin library, or can I make my own coin with it?
Awesome! I’ve been reading up on NBitcoin for a while now, getting ready to develop a new .Net application. Thanks for the contribution!
'Team City' *shutters*
Feel free to add suggestions on what I should use and why! 😜
Feel free to add suggestions about what I should use and why. 
It's a library. You can fork bitcoin from a website if you wanted to. 
Ok. That’s what I thought
I've found a book called ASP.NET WEB API 2, in which they are talking about the server and I have to modify the path for packages (install them into lib folder), but the book is kinda old and they are using an old syntax. I can;t figure that out. :(
Ah right, this is probably about all you need to create an up-to-date nuget.config: https://docs.microsoft.com/en-us/nuget/schema/nuget-config-file
I've really found AppVeyor to be the best be used CircleCI, Travis, Team City, and Jenkins and they all are great if your fine with have-what I thought to be- extremely complicated deployment, and build process, but AppVeyor just streamlined so much of the process and has so much support built in and great documentation that I have yet to find anything better. Also note I suppose that im. Microsoft Stack dev, so other disciplines will very on their opinion 
Does NBitcoin support Bitcoin Cash?
I feel like it’s partly my fault. I knew as soon as I bought in it would collapse. 😂
For something simple and local, I really like [LiteDB](https://github.com/mbdavid/LiteDB); I use it in my custom CMS I am still working on to store data. Fast and very user friendly from my POV. MongoDB is a great choice because of the widespread use, but was never as big of a fan, personal preference, but all things being equal will probably be your most reliable pick. I know RavenDB gets some hate but I have never found any of the issues some have mentioned. If it were not the price tag (which is actually not THAT high for lowest level, but doesn't beat free OSS) I would be using it much more. For me it was the easiest to understand and use, and pretty easy for development. Another worth looking into is couchbase. I am finally poking around with it, and a bit more verbose than some of the other options, but kind of dig it.
This seems really interesting actually... I might have to give this one a try over some other options. Have you found any big pain points vs the alternatives? EDIT: oh, you are the author. :)
If you want to mine you will need more than a virtualized language running on an x86 system. Most miners are pretty specialized these days. You can write the code for fun though and it will work, you just wont ever get anything. or you can code a miner against another type of coin that has not yet pushed people to customized GPU rigs. 
Been there done that. Luckily we use build scripts now. What a nightmare executing a Teamcity scripted build manually.
You might call MSBuild a *build system* and stuff like cake *build automation*. I don't know about the others, but cake does not evaluate input parameters and will happily execute all build steps even if it is not necessary.
I'll be damned, I didn't realize. Max 3 core, 6 gb, and 3 servers in cluster is not bad for free. I wish they would let us add more cores and memory, but I get having a bit of a carrot out there to entice more people to use it. 
Why not ASP.NET Core? Also, the Bootstrap version referenced is outdated.
 Was posted 3 days ago in this same subreddit :P https://www.reddit.com/r/dotnet/comments/7rnspx/a_sort_of_problem/?st=jcsahvnr&amp;sh=e19a7529
If people are interested in a block chain implementation entirely written in .NET they should probably check out Stratis https://stratisplatform.com/
Stratis Bitcoin is based on the NBitcoin project
Nice! I hadn't seen this new feature. I'll see about updating the example.
this looks like the one: https://www.hanselman.com/blog/LearningOpportunityNETTerrariumIsBack.aspx
There is a outdated branch for this. The nuget package can't sign their transaction. It supports litecoin though.
It already works with other Bitcoin fork (except BCH who changed the signature scheme).
Just a library. If you want to make your own you should check stratis Bitcoin full node.
Yes but it's quite difficult to get it to work with other bitcoin forks unless you know blockchain inside-out.
It depends on the application. If you only want say address parsing, this is fine. If you want proof of work check as well. Some currencies are so different though that lots of thing would not work (if transaction format is different) For litecoin or says dogecoin for example there is no issue.
I see. I was thinking specifically of PIVX forks, generating addresses, confirmation checking, checking wallet balance etc. There are so many PIVX forks now it would be very useful to be able to talk to their chains without running a local version of the wallet or talking to an external block explorer / API. On a separate note you didn't build NEthereum as well did you?
Haha I want to know more about this superbug now.
You don't have to use data binding. You could write backing logic for every user control. Or, you could use data binding, and have it just work automagically. Up to you.
Are you looking for “AntMe!”?
Now I’m getting nostalgic for old school delegates and ArrayList. 😍
The home of the game seems to have moved from CodePlex to Github https://github.com/terrariumapp
I did not built NEthereum. They reused some parts of NBitcoin code though. Checking wallet balance and etc.... is very messy business. I am doing this in a separate project [NBXplorer](https://github.com/dgarage/NBXplorer/). This is a lightweight block explorer, multi crypto, connecting to your node and exposing simple json/websocket. (Work on LTC and BTC now) This is not yet well documented though. I am using it in another well documented project called [BTCPayServer](https://github.com/btcpayserver).
Flag crossposts. I've already replied here: https://www.reddit.com/r/csharp/comments/7sms31/z/dt5vzne
Are you planning to keep support up to date? Or know someone who will?
Esent works very well as long as you stay on Windows hosting. RavenDB was originally built with it, but there is a tiny wrapper built for Esent called "Managed Esent". It gives you a nifty Dictionary container called PersistentDictionary. It behaves like a normal dictionary but it leaves a few db files behind in your working directory, thus you have a dictionary that saves itself to disk. Very nifty. You can build your own lightweight generic wrapper around it to throw any Type instance into it and it saves to disk. Running complex queries across multiple object types it can be slightly tricky at first, but you must just make a mindshift to how NoSQL db's work in general, then you will be fine. I've also successfully used it as a cache (a wrapped it in a class and instantiated it as a singleton with SimpleInjector) - this use case also worked really well. for me. Obviously once you trust Esent, you can move up to RavenDB, which is basically Esent with very nice management tools &amp; feature built around it. See: https://github.com/Microsoft/ManagedEsent
What do you use these days?
… Lambdas and generics?
As I recall, there was no mechanism by which bugs could communicate with each other. Everything was event-driven, you wrote code to handle events like "BugDiscovered" which fires when you see a bug, this event had an argument called OtherBug, which contained properties like the other bug's type, size, orientation, speed of travel, etc. Some guy figured out a way to get his bugs to communicate which gave him an enormous advantage. When he encountered other instances of his own bug he could communicate the locations of other nearby edible bugs or plants, and rally several of his own bugs together to attack larger bugs. He did this by developing his own "body language" using the only data available which was the bugs orientation and speed. When he saw another instance of his own bug, they would face each other and start rotating back and forth rapidly, for example rotating left-right-right-left meant a bug was nearby and the degrees of total rotation was a bearing on the other bug's location. He also developed the perfect companion plant which did nothing but fervently reproduce so when all the other bugs were gone, he could sustain his army on plants. Basically it got to the point where the whole Terrarium was just full of the Superbug and Superplant and anything else only lasted a few seconds before being eaten. 
So he basically made Africanized honey bees. Neat.
Some people have complained about performance when it gets rather large. Others about inconsistency in data. I have not found any of this so far, but perhaps I have not scaled large enough. I also sometimes wonder if the performance could be related to not designing correctly around a document DB vs a relational one, but never got a chance to get their input.
Thanks for this! I never heard of this and have been hackin w/.Net since 03.
Does not count for the new `ValueTuple&lt;&gt;`.
For simple apps, ClickOnce is definitely the way to go. The idea is that you'll publish your app in VS via ClickOnce and put the published artifacts somewhere reachable (like a URL) by all users that intend to receive updates. All new versions will continue to be published to the same location. However, ClickOnce apps cannot be ran as administrator AFAIK. If you use the ClickOnce publish wizard, there will be an option to set the "update location" for the app to check for updates. Once the end-users run the app, the app will check for updates in that location and, if it finds a newer version, ask the user if they want to update the app. I am developing an internal tool for my company right now that I plan on using ClickOnce to deploy. I will deploy it to a shared folder on our network and that is where the app will be installed from and where updates will be checked. ClickOnce is perfect for many internal tools at companies. Our main products aren't installed with ClickOnce -- we use a home-grown deployment system that takes care of the automatic updates. For more complicated apps or apps that need admin or apps that you want installed to Program Files x86, you'll have to use a different payload. I am more familiar with .msi installers as I have used both Installshield and WiX. Installshield has a great GUI designer for your installer but does too much for you and can be a pain to work with, especially within VS and with source control and with licensing. WiX is far better with VS integration and source control and it is free &amp; open source but has a *steep* learning curve. Only IS has an auto update feature but with a cost. If your app is simple, ClickOnce. If your app isn't simple, WiX + homegrown update code built into your app would be my suggestion but Here Be Dragons.
Is there any benefit over **object[]** type in the particular example?
If you have an on-premises domain controller, use Novell's .NET Core LDAP library. You may have to build an adaptor to shoehorn it into the DI container. If you are in Azure, use ASP.NET Identity. It should work out of the box.
[Wix](http://wixtoolset.org/) is the best option.
Sorry to revive an old topic, but I need to rant. I just found explicit evidence that DevExpress exposes unsupported properties. It's an old support topic, but it seems to still apply - the AllowHtmlDraw property is exposed for controls that don't support them (in this case, MemoEdit). I suspected there were properties that did nothing, but now my suspicions are confirmed. What kind of crap design is that? 
This is really nice. But... If anyone is starting a new project, don't use this! Just around the corner they'll be releasing a template which hooks up Angular cli with .NET Core apps. You can even download the preview now play with it. https://docs.microsoft.com/en-us/aspnet/core/spa/angular?tabs=visual-studio
dont remember it but its an old game and most languages have a version of it. I first remember writing on for a system called probots (i think) on turbo pascal. If you cant find the original I would not be surprised if there is a community of C# devs somewhere that run a game like this. 
whoah, this looks interesting. thanks!
While you can certainly databind directly to a DataTable, the whole point of the Model is that it should give a clearer idea of what the DataTable *represents*. A DataTable is not a Model, *it is the data*. So in your case you should make a POCO (Plain old CLR object) class called InventoryItem, or InventoryDescription if you only extract specific information from your data. public class InventoryDescription { public int ID { get; set; } public string Description { get; set; } } This is what I would call a Model. I'd recommend you make a more complete one, call it InventoryItem and only expose whatever relevant property by using View property bindings.
I've actually set mine up very similar, although slightly less separation. Just wanted to let people know as lots of people have asked for cli integration and come up with lots of solutions but the supported template is just round the corner thankfully.
We're getting there. But you should look into some tutorials on Entity Framework that can do this for you automatically. The concept is called O/RM (Object-Relational Mapping). The first thing I'd do is decouple your ViewModel from the data source by passing in an interface. The empty constructor is for convenience. private ObservableCollection&lt;InventoryItem&gt; _inventory; public ObservableCollection&lt;InventoryItem&gt; Inventory { get{ return _inventory; } set{ SetProperty(ref _inventory, value); } } private IRepository&lt;InventoryItem&gt; _repo; public ShellViewModel() :this(new InventoryRepository()) { } public ShellViewModel(IRepository repository) { _repo = repository; } RefreshInventory() { Inventory = new ObservableCollection&lt;InventoryItem&gt;(_repo.GetData()); } public interface IRepository&lt;T&gt; { IEnumerable&lt;T&gt; GetData(); } public class InventoryRepository : IRepository&lt;InventoryItem&gt; { public IEnumerable&lt;InventoryItem&gt; GetData() { // SqlLite logic here and convert rows to inventory items } } 
I absolutely understand your intentions here. However, if anyone is serious about using angular and .net core together, they would split the two into two separate projects, because it eliminates so many headaches.
Most of the article is about doing that. It describes how you can achieve that and deploy in IIS or Nginx.
What do you mean by serious? We've got multiple projects where they are pretty similar to the upcoming templates and they work fine together. You can update both independently, and if you view the code changes they've made to allow .NET Core to talk to the cli, it's been done in a very flexible manner, similar to having nginx as a proxy. I think it's over in the aspnet/templating project on GitHub if you'd like to see.
Yea except most of the angular/spa services stuff is 6+ months out of date...
In this case, not really. I think it was more for demonstration. As /u/AngularBeginner mentions below, ValueTuple is where we will see the most use probably. The case of being able to return multiple types/objects without making clumsy object arrays or parent objects with obscure properties is pretty useful. https://blogs.msdn.microsoft.com/mazhou/2017/05/26/c-7-series-part-1-value-tuples/ 
This is quite routine. In Models folder - Add item - Data - ADO.NET Entity Data Model - Name your DbContext class Code First From Database Select your database - New Connection - Server Name: (localdb)\MSSqlLocalDb - Name your connection string Select your tables or views - finish 
There are a multitude of different ways you could go about that. Two simple ways would be: public class InventoryItem { public string id { get; set; } public string desc { get; set; } public InventoryItem(DataRow row){ if(row == null) { throw new ArgumentNullException("row"); } id = dr[0].ToString(), desc = dr[1].ToString() } } dt.AsEnumerable().Select(p =&gt; new InventoryItem(p)); Or public class InventoryItem { public string id { get; set; } public string desc { get; set; } } dt.AsEnumerable().Select(p =&gt; new InventoryItem{ id = p[0].ToString(); desc = p[1].ToString() });
Don't forget that .NET has 'value types' that are (often) stored on the stack (http://jonskeet.uk/csharp/memory.html), so you can achieve similar behaviour if you want to
Look into ORM frameworks (object relationship mapper). Consider that your model in code might be completely different than the data in the database. It may not be a direct relationship between object and table. My two favorite are Entity Framework (Microsoft's fully featured ORM, this can generate tables, do updates, create classes from existing databases, use a GUI or code, etc.) and Dapper (hand written SQL queries mapped to objects). I tend to lean towards Dapper because I like having full control over my queries. 
If I wanted it to not be a 1 to 1 relationship models/controllers to actual tables and instead wanted to, for example, map a query to a model/controller, how would I do that? I have found ways to declare a table to be related to a model/controller, but I've not seen a way to make a query operate in the same way. 
Database first is going away. Better to focus on learning code first.
Well, you can always use some nuget packages like Automapper to map your query result to model. The reason why the view models are different from entities is to have a seperate layer and not to directly have the web access to the data layer.
If you don't want 1 to 1 mapping, you would be better off using something like Chain. https://github.com/docevaad/Chain It was designed to materialize queries, not tables.
no I don't. I am open for the BCH community to contribute to NBitcoin to make it simpler, but they never proposed a PR.
I would learn how to do it the normal way through entity framework with a 1 to 1 relationship first before you try to learn how to do it the advanced or custom way. This is because the more specialized your project, the less and less help you will be able to find. Better to learn the usual path and all of its strengths and weaknesses before you assume you need something different.
Thank you. That's useful. 
You should think of using SQL as "advanced". It's not any harder, just different.
Sucks you have to use strongly named assemblies. Might want to check out something like [Strong Namer](https://github.com/dsplaisted/strongnamer)
In the example in the blog he's using it with a ValueTuple.
That's new, it wasn't there before. I would be curious where he saw the implementation, I couldn't find it.
I'd it live somewhere where I can test it, or do I need to host my own? It looks really nice from what I can see on the repo.
I thought it was a web installer.... Thanks xD
This is actually super interesting!
I didn't set up a live version that's publicly available because I didn't want to go through the risk of hosting anybody's files/information. You can run it locally though just by running an executable if you're on windows 7+ or ubuntu 14.04/16.04 (otherwise you need .net core 2 runtime) The downloads can be found here: https://github.com/Beyhum/quickpaste/releases
Ah, that's understandable. I'll try it out later tonight/this week then :)
I'll definitely do!
How could adding a Finalizer help in the first case? As I understand it the Finalizer wouldn‘t be called as long as the Customer is in the Dictionary!?
Yeah it crashed when it reach 20 000 USD of altitude, but we are building the next generation shuttle.
I can answer a few questions. 1. Why the constructor in the generated model? Try this code using your model var model = new Blog(); if(model.Post.Any(){}//NULL Reference Exception Using a default constructor to initialize properties such as List is pretty common if you are not sure if that property will be populated 100% of the time before you access it. 2. Why are they partial? So you can extend them in your code from another file if you so wish (adding a calculated property or something) and if you regenerated the DB model, you do not lose your extra customization 3. I have no idea what you are getting at in EDIT 2
Re: the constructor, that makes sense. I didn’t really consider it because I never use the model in such a way, but then again my learning apps are simple as can be. As for edit 2, I should clarify. Before when I was making my models I was including both a navigation property and an id property for the relationship (in the case of a Comment model, it would contain both an integer PostId and a navigation property of type Post). The auto generated code seems to do this as well. However, when running Add-Migration on my model, the migration generated would have two PostId properties (PostId and PostId1). If I removed *int PostId* on the Comment model and just left the Post navigation property it would generate migrations the way you would expect; that is, it would only have a single PostId field. I hope that makes more sense.
I didn't find it in the source code.
I think I understand now. I *believe* that when you have a navigation property, the identifier of that object is encapsulated in it already. In your comment model, I assume that EF adds something like `public Post Post {get;set;}`. PostId is the primary key for the Post object, so EF is smart enough to know to use that as your foreign key on your child object. 
That's what .Net Standard is for. You write your library in .Net Standard, then your .Net Core / Net4.6.2 application can consume that library to do the framework specific logic.
That makes sense and is exactly what I thought was going on, thank you :). I’m still trying to get the hang of finding my way through the documentation, it’s really intimidating starting out.
Thanks for the response. Yes my libraries are .Net Standard. That isn't clear from what I wrote but it's set up that way. A few corners of my services call 3rd party nuget libraries that target net462. Once very nice thing about dotnet apps is that, even when running in netcoreapp2.0, my command line and unit test projects build and run great until I actually invoke one of these corner cases, at which point I get an error. I can combat this by targeting core or 4.6.2 depending on what I want to do. I was assuming (hoping) I could do something similar with the API: provide limited functionality cross platform, and full functionality by targeting net462 just like we can in console apps. It seems, however, that I'm out of luck on this front. And I'll need 2 APIs or a workaround. 
While `Microsoft.AspNetCore.All` is .NET Core only, most individual packages support .NET Standard which can be targeted by .NET and .NET Core applications. The `Microsoft.AspNetCore.All` package is a messy everything-in-one-package, I'd not use it at any time.
 What do you mean by "messy everything-in-one-package"? It's just like how angular does their node_modules, what part of it do you not like? Just curious, not flaming.
You can detect your runtime at... er, run time, and throw PlatformNotSupportedException.
Did you take a look what this package includes? Besides including ASP.NET Core MVC, it also includes the whole authentication framework (including Facebook, Google, Microsoft, OAuth, OpenIdConnect, Twitter implementations), Entity Framework Core (with both SQLite and SQL Server support), the configuration framework (with ini, XML, and JSON support) and the Caching framework (with in-memory, SQL Server and Redis support). It's simply an everything-we-have-in-one-package. It's for people who have no clue what they want. "Here, have **everything**!"
No problem! We all started somewhere (and at least you are RTFM! haha). Keep it up, you will get all this figured out in no time.
I was thinking the same .... there’s still a reference to the data !
While it's really everything at one package (or "metapackage", that's how they call it) - the real intention of Microsoft was to reduce nuget packages restore time. For me it sound pretty reasonable, even though I still don't use it, as packages are cached and restore time is decent
HAve you found any way to unit test and mock serverless functions?
Love seeing more core and vuejs. Its a powerful combination. For extremely small apps I've started to use nancy and vuejs
Heck yeah it is! We are wrapping up our first product launch using full Dotnet Core 2 and Vuetify as a single page application. It’s so awesome. 
 It's referenced from your local store, it's not actually downloading them, you could argue not taking up the space locally, but I do find it nice that you don't have to go through and reference most of the things you're going to use every time anyway, but to each their own I guess :P It's what angular does (to a more extreme), ionic and xamarin all do too :P
The question for me is why the consolidation necessitated a move to netcoreapp2.0 instead of netstandard2.0, which most of the inner dependencies seem to target. It seems to me that this `Microsoft.AspNetCore.All` package is kind of a flagship package from Microsoft, and making it non-standard is a big slap to the enterprise folks migrating from net462 .
It's not complexity, it's just unfamiliar. If you want talk added complexity, try teaching someone how EF does object caching and de-duplication at the context level. I've lost tract of the number of bugs caused by EF Core doing weird shit in relation to that feature. 
UWP will pay bigger role https://www.windowscentral.com/windows-core-polaris
I found this when I was looking up HashSet doing the the whole RTFM thing heh. Took me a while but I eventually got there.
Also, kudos to you for thinking "If mine isn't like theirs and theirs is better... why?" Only real code nazi's would fail you on a code review for your implementation so you would probably never have read half the things you just did without holding yourself to the higher standard. Bravo!
Yes, lucky I went there after that truly shitty title
What if you create a new template of a project?
Why in a PM? Maybe other people can learn from feedback that you receive 
Many project templates already do, start an ASP.NET Core project and it opens in the editor by default. Look at the solution that is scaffolded and see how they do it, copy it
Sounds like I’ll have to do that. Making two projects was something I was hoping to avoid. Thanks for the response 
This would be a target attached to one of the projects in the solution. MSBuild is the tool reference you want to understand the XML in the project files. 
[removed]
I mean any JS framework is equally as good. Not sure why Angular stands above the rest. It also depends on the usecase.
Oh! Back in V.1 or somewhere.
Can confirm the veracity of this answer.
It works quite well and is significantly more performant than full framework. Armchair general all you want, but the numbers don't lie. I observed 6x better application performance compared to full framework.
Netcoreapp dates back to the bad old days of PCLs. .NET Standard is new (&lt;1year) while netcoreapp is about 3 years old.
You **completely** missed the point.
does it need to start up when you open the solution/project? you could just attach a Readme.Doc word doc or whatever to your project, and just have it in there. 
My point was not about assembly loading or runtime performance. Not at all. So yes, you missed the point.
You seem to be confused as to why the libraries are bundled in the SDK rather than being installed in the GAC.
And it is not efficient for small number of items.
I don't know exactly what sort of advice you're looking for, but why not both? ASP.NET (Core if you want to be most up-to-date) server and JavaScript on the client? You can get as crazy as you want. Quite a few options out there. And if you decide ASP.NET Core is interesting, check out all the dotnet new templates that are out there. They have some that get you started with more advanced SPA frameworks.
It depends if you wish to spend more money to host your website. A simple HTML with some js can be hosted for free in some places, while having something that needs to be compiled (like a website in asp.Net) can be kite costly. Please note that I'm not an expert in that field and I can be wrong. I'm sharing this because I asked myself the same question a few years back, and I decided to do a simple html portfolio because it would have been too much expensive. 
Heroku provides a free tier that lets you host apps. My portfolio is an angular app. However the server that its on shutsdown after a period of no activity. So it usually slow when you access the page first time.
Put a readme.md in the root. Add it to the solution. I suspect this request stems from the fact that you already have one and are frustrated people aren't reading it. Forcing the file open won't fix that issue...
You can do the same with Azure, but the free tier usually do not allow you to have your own domain name. It usually gives you something like myproject.azurewebsites.net. On the other hand, you can use google firebase to host a one page site for free, no shutdowns, with your own domain name and a free https certificate.
That’s what CNAME records are for, assuming you already have a domain name you control.
I'm working on that right now, hoping I can piece-meal my dependencies so that I can target both camps. Notably the default MVC template from `dotnet new mvc` does not work on the full framework because of the aformentioned `Microsoft.AspCore.All` package. The console apps work fine.
I absolutely hate the fact that the SPA templates force TypeScript down your throat. Not everybody wants to use TypeScript for their js. Typical of MS to do that though. At least give me the option of it, what if I wanted to use Flow instead of TS?!
Core 2 + &lt;SPA /&gt; is a powerful combo. We've developed and launched 8 Angular2+ Core applications already.
"free https certificate" That's not a selling point anymore. LetsEncrypt certificates are free dependent of provider.
Hey! Looks interesting. The example in the readme is really deterrent though. Try to make the easiest example you can think of, avoid the controller stuff. It makes it easier to get a grasp what the project is about
Thanks! I'll try updating that to a much simpler example. :)
Looks like you're correct. TIL.
However, you may be able to create a "Pointer" with your domain registrar. Standard pointer will just redirect to the app, a stealth one will have a Frame pointing to your app. 
In my opinion, a free trial is useful to test the water. I would not try to publish something on it. But it's a good idea to try it to see how much it will cost you.
Instead, write a simple tool or a service to learn the concepts of Asp.net. I strongly advice Core version.
Yes I use Docker compose for all my dev environments. It’s very helpful when you have a service based architecture. Plus if you’re using swarm in production it’s as close to identical from the dev side so you get an added confidence that you’re going to work the same once code is released 
How does one use GUI applications in a docker compose?
Thank you.
No... you can share directories between host and your container. Keep your build/test tools in the container. Your files stay on the host, outside of the container (because containers should be treated as ephemeral). Then you just edit files like normal and pop into your container to do builds/testing/etc. Of course, if you're using Visual Studio, there's probably little to no benefit of doing this since you're probably building/testing via the VS UI... in which case Docker gets you almost nothing. I prefer VS Code as my "IDE", and I just do builds and tests from the command line. So anyone else can boot this container and be able to do a full build immediately... and still hack on the code using whatever editor they want.
EF takes a lot of the work out of CRUD operations, and can be incredibly convenient. It can be frustrating at times, but for the most part it allows you to spin something up in just a couple of minutes. LINQ is invaluable for working with datasets. Do you really do 100% of your querying on the DB side? To me, that would seem far too cumbersome to ever be enjoyable. Being able to query data without having to constantly reach back to a DB is something I guess I take for granted now. 
Well, whole articles have been written about this, but the one thing that stands out for me as an advantage is the syntactical correctness of the queries. In regular SQL, if you have a query: string query = "select Id, Name, Sales from SalesTable" you better hope that there is indeed a table with those fields. In Linq/EF, when you say... from s in SalesTable select new { s.Id, s.Name, s.Sales } the compiler will yell at you if that is not true. ( Code is for illustration purposes only and may not be technically correct )
I include information like that in custom exceptions as a rule. If someone has issues with exposing metadata like that in their apps, then global handlers are key (along with those custom exceptions!)
I would probably use both in different circumstances or maybe standardise on both if your application is predominantly fitting in one case. I would use stored procedures for a high frequent operations for updating data that is possibly atomic and has a stable execution plan. I would avoid putting any non trivial logic in the stored procedure (as you will have to do unit testing in both c# and sql domain - not impossible but another item to do). A good example would be an use case for a stored proc that upserts invoices with related line items. You may call it hundred times a second. The stored procedure will be in sql server cache and query plan cache. Passing the arguments to the stored procedure in one go reduces the number of round trips and hence the performance per thread of the calling process. In this case I would still use EF as is mechanism for calling the sp and passing parameters. This will eliminated possible mistakes in data type mismatches, sql injection, etc. You can use another ORM but principle is the same. If you are predominantly querying the data and have a complex query patterns, mapping complex structures (e.g. invoice + lines + nested objects) linq may be easier route than the traditional way. Again these are two use cases that come up immediately. Kind Regards Alex
You're still right. EF is over rated and wait for the day it breaks on you, and it will. SPs allow all kinds of changes without new builds. Linq is great, and you should learn it, it with EF does NOT replace SP, there are places for both
Do you have any specific examples in mind?
Was this a false assumption on my part?
Do you have more evidence (e.g benchmarks)? I am not doubting but would love to see quantitative data. On a separate note I am aware for a narrow use case (Disruptor and Disruptor .net port) and java implementation was faster than the .net port (as far as my memory serves me) and there was a quite a detail discussion in the difference between jvm and clr and how one of the difference played different on the performance. However Disruptor framework is a very specific and narrow use case.
I do not. I perhaps made a false assumption after looking at performance benchmarks for different JVM languages and assumed the case would be same for the CLR languages, since from a macro-perspective, the platforms are similar. 
* Some languages are more memory efficient and generate very little garbage * Some languages generate more temporary objects / garbage * Some languages have better standard libraries * Some languages favour simple and easy to read code at the expense of slower execution * Some language favour more terse but performant code
It's just difficult to make any generalized statements. Different languages may encourage different paradigms and solve similar problems in different ways with different performance characteristics (e.g. mutating an existing object vs. allocating a new immutable one), but you can't really make any factual comparisons without specific examples. Dynamic code is going to be measurably slower due to the binding and dispatching overhead and lack of optimizations like inlining, but you can write dynamic code in C# as well. If you compare apples to apples, you would generally expect identical performance.
I only dockerize the external dependencies (like databases etc...), so that tests and debug works without dev setting up 10+ dependencies.
I always go with EF for reasons mentioned by others, but depending on what you're doing, you may not be able to get completely away from writing stored procedures. You can at least call them via EF though and let it handle the monotonous parts for you. Some things simply aren't supported in EF such as xquery/xpath, query hints, full text search, etc. I realize you can extend EF and gain some of this, and you could also just have EF execute your raw SQL for you. But I still think some things just fit better left in SQL.
Lots and good points here. I'll add that EF adds another layer of code that you're refactoring tools can easily target.
EF gives you all the advantages of static typing and IDE support for your queries. If done correctly it can lead to a massive reduction on errors as they will become apparent at compile time. Also reduces the use of a lot constant strings which can be a nightmare to debug and update.
Not sure why your downvoted. EF is useful for CRUD. But any complex SQL I'd rather have control over. Plus EF still never learned to use table types. I have to write frameworks for adaptation of table types. I would call table types the ferarri of SQL bulk work if you are seeking any sort of round trip efficiency. any ORMs tend to be too chatty or inefficient.
ORM saves time for a lot of reasons that people have already mentioned. It seriously sucks having to map data to/from DataTables and DataReaders. However, save your battle for when they just use EF to query relational data and pass it on to the front end. Your application should live in the back end. Too many people forget that their database holds relational data, not object oriented models. The ORM gives you relational data. It’s up to you to map that back to OO models for proper use in an application. 
Just because all the code is compiled to the same medium does not mean that all compilers are created equal. The resulting bytecode from a compiled Boo application may not be the most efficient bytecode, for instance.
Is that even a real phrase?
I would disagree. That's the ORM's job. If you just use mapping POCO classes you might as well stick to SP's and DataSets. The whole point of EF (notice the word ENTITY) is to provide a transparent persistence of your object context. It's not just about mapping, which is a technicality. 
You must have observed this in some way, haven't you?
/r/digitalnomad
Here's a sneak peek of /r/digitalnomad using the [top posts](https://np.reddit.com/r/digitalnomad/top/?sort=top&amp;t=year) of the year! \#1: [My brother, Adeolu Ogunniyi (24) has been missing since September 10, 2017. He was backpacking in Central America and last seen at Laguna De Apoyo in Nicaragua. If you've seen him or heard anything PLEASE contact me. (more details in the description)](https://imgur.com/a/S4AlS) | [98 comments](https://np.reddit.com/r/digitalnomad/comments/75jzo6/my_brother_adeolu_ogunniyi_24_has_been_missing/) \#2: [SpaceX requests permission from US government to operate network of 4,425 satellites to provide high-speed, global internet coverage](https://www.theguardian.com/technology/2016/nov/17/elon-musk-satellites-internet-spacex) | [31 comments](https://np.reddit.com/r/digitalnomad/comments/7o1hiq/spacex_requests_permission_from_us_government_to/) \#3: [Real Digital Nomad](https://i.redd.it/8boo0ocap4401.png) | [27 comments](https://np.reddit.com/r/digitalnomad/comments/7k1jxt/real_digital_nomad/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
You’re misunderstanding. The point is to stop using just what the ORM gives you. It’s a horrible practice to simply pull relational data that you need, then change data and persist. It breaks encapsulation and spreads both your business logic and data access throughout your application. Use the ORM to get your data, but you still should be following OO principles. 
The point is to stop thinking about it as relational data. It's not. It's the core entity model of your business domain. Within certain limits (like direct N-N relationships) it's classes like any other. Your DataContext is what it says, a data context. It holds information about all instances objects (entities). The fact that it's persisted to a SQL database under the hood is just a technical detail. 
I've worked with the .net stack for 15 years, remote contract for the last 6. Two years ago gave up our permanent residence and been traveling with my wife and 3 kids. Airbnbs for a month or two at a time. Canada, New Zealand, Australia and the Pacific NW mostly. Getting old though, looking forward to settling back down soon. Primary source of work has been Toptal.com during that time.
Looking forward to the day that there's a built in option to include parameter values in the stack trace.
Ah, so remote workers. Got it.
not all remote workers embrace a nomadist way of life
Cool! Any chance your name's a reference to "Jugo de Lúcuma"?
You don't seem to have had a good answer to this one.... You absolutely **can** target multiple frameworks at a time. What you describe - you have some dependencies that target specific frameworks and not others - isn't uncommon and there are ways and means of dealing with this. Firstly, as you've already done, you should target netstandard and any netstandard packages should "just work" with any platform that implements that netstandard. That's the easiest route if you can do it. However, it's not always possible if some dependencies don't target netstandard. In this case, you can add a second target for that more specific platform, i.e. in your csproj file: &lt;TargetFramework&gt;netstandard1.4&lt;/TargetFramework&gt; becomes &lt;TargetFrameworks&gt;netstandard1.4;net46;&lt;/TargetFrameworks&gt; By doing this, you can then optionally import different references depending on the framework, i.e. &lt;!-- Conditionally obtain references for the .NET Framework 4.0 target --&gt; &lt;ItemGroup Condition=" '$(TargetFramework)' == 'net40' "&gt; &lt;Reference Include="System.Net" /&gt; &lt;/ItemGroup&gt; &lt;!-- Conditionally obtain references for the .NET Framework 4.5 target --&gt; &lt;ItemGroup Condition=" '$(TargetFramework)' == 'net45' "&gt; &lt;Reference Include="System.Net.Http" /&gt; &lt;Reference Include="System.Threading.Tasks" /&gt; &lt;/ItemGroup&gt; In your case, your netstandard code can pull in Microsoft.AspNetCore.All while your net462 conditional can pull in other libraries to fill the gaps it can't. Within your library's code, you can then use conditional code to change the behaviour depending on the platform: #if NET40 Console.WriteLine("Target framework: .NET Framework 4.0"); #elif NET45 Console.WriteLine("Target framework: .NET Framework 4.5"); #else Console.WriteLine("Target framework: .NET Standard 1.4"); #endif The net result of doing this is that you'll end up with a single nuget package that contains a library for netstandard apps and a library for full framework apps, consumers won't have to think about which is which, nuget will pick the library that's most relevant to them and your code should be pretty maintainable. As someone mentioned, you might want to throw a PlatformNotSupportedException in certain edge cases that just don't apply to a particular platform but everything is there for you to handle most if not all cases. Full information here: https://docs.microsoft.com/en-us/dotnet/standard/frameworks
Haha it is actually a reference to the fruit - Lucuma!
I did that for a while, but I felt that it was pretty awful. Ended up scaling to the paid tier. I have the dev subscription until August, then I'll probably downscale.
myself is declared as out and it must be assigned before exiting.
.net nomad here. .NET might be tough as a lot of the work is 'enterprise'. I haven't seen much .net remote work. I had been watching stackoverflow careers via RSS url for remote jobs when I found my current full-time job.
Do you mostly work for the same place? Find your own contracts? Or use a contacting firm to help land gigs?
How many years of experience?
working as intended. cookies have a default lifespan of 30min; you have to configure them to be persistent.
Thanks, I do have an on premise DC so I'll be looking into that once I squash some other issues.
I had a coworker recommend Open Id Connect, I ran into some other requirements that I'm currently running down but I had not read the article you linked. Thanks for the information!
Thanks a million for the detailed response. I ended up doing exactly what you suggested, decomposing Microsoft.AspNetCore.All into its individual packages that all targeted netstandard. Notably this decomposition works on netcoreapp and net462, so there's no reference to AspNetCore.All right now. What worries me slightly is why AspNetCore.All is netcore while its individual packages are netstandard. I'm sure there's a netcore only package in it somewhere, but I have a relatively robust API that doesn't seem to need whatever dependency can't be netstandard for some reason. What worries me more is if Microsoft will, over time, be moving these netstandard packages to netcore. In which case these efforts to run dual framework would be kind of a waste. I'm still pretty thrilled with netcoreapp as a whole. Can't believe how much of my full framework application code works as-is once targeting netcoreapp. Thanks again for your response. Cheers!
In that lat link of yours, there's a section named "Persistent Cookies". It's got all you need to get started in tweaking the expiration time of your cookies.
What you're attempting to do will be much simpler using JS from the front end, and posting a list back to the app. But it should be possible not using JS as well. Note I have never tried this. Try creating a button inside a form that does a post to an action that just adds a new empty element to an array of questions, then call View("whatever", array) and render accordingly. Hopefully this helps. 
I'm currently trying to do this, I've 8 years .net experience(2 companies) (not including uni years) &amp; a CS degree.. As others have pointed out .net is very much in the enterprise area. A large amount of people don't keep up with the MS ecosystem and are ignorant of .net core being cross platform which doesn't help. (I love .net core and hope it takes off but there are many reasons why its not the best choice for many things, I could make an entire article on that). I fucked up in not securing clients or building clients before i left my 9-5 job. As for getting remote work without this, its hard enough... Upwork and Freelancer are full of (**by and large**) low grade clients that want "build me *some software*" for $5 .. and low grade Indian/Pakistani developers that will ~~do~~ promise it for $2. This isn't always the case(on both sides) but I will stand my my comments as it being the *majority*. I think i will move into the TopTall System which although disliked by many (see any hacker news post comments about toptall).. may yield better clients. As for me I've actually started picking some work up on reddit of all places. Currently making a services website... to help push myself out there and away from job sites. Remote worker i think is the term your looking for. Digital Nomad is a marketing term really pushed by those wanting to sell to it. I think coined by blog and FBA(drop shipping) people who try to sell courses and talks to said group and attempt to push the "dream" to people that do not have actual(measurable) skills. 
Hey sorry to jump on this without being able to help, but what’s the difference between API and MVC controllers in core 2? Do they inherit from a different base?
You are in Chiang Mai after all. The home of the digital nomad. My favourite is people selling courses about how to make money selling courses. There are lots of full time .net jobs in Bangkok if you get tired of visa runs.
Fear not, the metapackage is netcoreapp by design, it's only meant to be used for net core **applications**. All of the various components are netstandard and there isn't really a good reason to change that as .net is more than just core and the framework these days - Xamarin, UWP, mono, Tizen, etc. all consume netstandard packages. They explain the decision a little bit here: https://github.com/aspnet/Announcements/issues/243 Glad you got it sorted though! 
[removed]
Automapper? DTO support? Standards syntax (odata/graphql)?
In addition to what k_charles said, you will be submitting a list of viewmodels to your webapi/mvc controller. Make sure that your endpoint can receive this list. The razor view/page will use a list model of your question viewmldel. Develop front-end javascript code that can extend your form. You can find many examples on stackoverflow on how to handle this.
This application often gets posted with a statement like that. It is actually an incredibly well designed ui.
no, that doesn't make sense. Use docker for services like redis, not desktop apps.
I am working remotely, but do not travel much while working.
Agreed, it does looks great. Do you know how many people worked on this?
Yup, just one (bbougot)
You could write a function that takes your route as a string without the v1, and adds the v1 to it. public string AddVersionToRoute(string originalRoute) { return string.Format("/v{0}{1}", config.apiVersion, originalRoute); } I have my own gripes with .net core like the number of differences between core 1 and core2 and the difficulty of finding helpful tutorials.
- Yes, that's typically the way it starts. Microsoft suggests creating extension methods so that you can group your dependencies together and just say `.AddMyStuff()` in Startup.cs. Other IoC containers I've used have the ability to auto-wire things so that, for example, any class named the same as an interface (minus the `I`) will be registered as its transient implementation. - I'm not a huge fan of doing end-to-end integration tests for complex web apps. There's just too much that could change too easily. Unit test everything important, and then make some smaller integration tests around sets of components that work together often. Testing that an end point writes out certain HTML is just incredibly fragile (unless it's dictated by contract, like with an API). You can use the same unit testing framework to implement integration tests like those. - It sounds like you want to [version the paths](https://github.com/Microsoft/aspnet-api-versioning/wiki/Versioning-via-the-URL-Path). That's absolutely supported (as are a few other methods). - Auth0 is incredible. After having reviewed a number of SSO options, I'm actually sad that we couldn't choose Auth0 for technical/business reasons. It's easy to set up, easy to use, and has so many options for different SSO connectors. I'd stick with it. - What makes you think that there's something wrong with your DTO classes? They look just about as I'd expect them to look, minus the "Dto" suffix everywhere (the namespace already contains DTO, so I wouldn't add it again). - The tests look like a good start. There are a few things in there that I would consider code smells (global test state for your mocks, which might be okay, depending on how they're used, and testing whether or not a method was called), but otherwise, from a quick glance, everything looks good.
For authentication alternative I personally like IdentityServer4. Very well made system especially useful for federated authentication.
&gt;Is it really practical to put all of injection code into the one Startup.cs file ? It's starting to become big and just doesn't seem right to me. Yes, and from an IOC standpoint it's quite important. Startup.cs is effectively your [Composition Root](http://blog.ploeh.dk/2011/07/28/CompositionRoot/) so if you're not setting up all of your injection code there, then you're likely having some other areas of your code that's doing things it shouldn't. As another has suggested, refactoring some of that logic into a separate method is fine, and in fact the preferred approach, but all of it should effectively be done in Startup.cs. &gt;Is there better way of doing my routes ? I've noticed repeating route names. Maybe there's some kind of way to input the v1 version into the routes, instead of hard coding ? I didn't see anything in your routes that looked out of place. As far as versioning goes, I don't know if you're just getting ahead of yourself, or simply getting familiar with it as something you feel you'll need to know. Either way, I think that's a loaded topic, and one that has several different approaches that can, and likely should, be supported. I personally like Troy Hunt's write up on [API versioning](https://www.troyhunt.com/your-api-versioning-is-wrong-which-is/) for a survey of the options. If you don't absolutely need it, then it might be worth just pushing it to the side for now.
Re: ioc startup: we have individual classes per assembly that contain all the default mappings for the assembly. The startup of each application entry point just calls to those classes or overrides the ones it needs to. Much more maintainable.
Regarding dependency injection, try creating an interface to represent a specific DI scope ( e.g. IService would represent transient scope of something) then auto-wire any classes which implement the interface to the DI container: https://github.com/ChuckkNorris/PredictableCoding/blob/master/src/Movie.Api/Startup.cs Some more random examples: https://chuckknorris.github.io/PredictableCoding/
More than 10 years experience, though not all of that .NET.
Thanks for the link. &gt; there isn't really a good reason to change that The one reason I can think of is the reason that prompted my original question. Running `dotnet new console`, `classlib`, and/or `test` creates projects or applications that immediately work on netcoreapp, docker, and net462. However, `dotnet new webapi`, which I could maybe argue is the most modern and accessible way of using the above components, is an outlier and it abandons net462 compatibility. Sure I can piece together an equivalent api using the individual packages, but the lack of an immediately accessible cross platform template raises the barrier of entry for folks like me who have migrated their in-house libraries to netstandard. Perhaps something along the lines of `dotnet new api -framework netstandard2.0;net462`. Of course this would create a huge and unmaintainable number of framework permutations, but I've got to think that folks wanting simply netcore and net462 compatibility would far outnumber those targeting UWP and Tizen. Cheers!
Would be interesting a port to UWP :P 
&gt; the lack of an immediately accessible cross platform template raises the barrier of entry for folks like me who have migrated their in-house libraries to netstandard. I'm not sure I follow what you mean by this. A netstandard targeting library will work on net462 as net462 implements the netstandard. A netstandard library is cross-platform, as is a netcore application - it's cross platform in that it'll run on Windows, Mac, Linux, etc. The only issue you'll have is if you try to consume a net462 library but even then, **this will work**, it'll just throw an exception if you try to do anything on a platform that doesn't have the full framework. If you create a new webapi application, it makes sense for this to be netcoreapp by default as you get all the benefits of .net core, including cross-platform deployments and efficiency gains. If you have a dependency on a net462 library, that's fine as well, the [compatibility shim](https://github.com/dotnet/standard/tree/master/docs/netstandard-20) will still let you consume it. There really isn't much of an argument for targeting net462 directly for a webapi project unless you're going to be coupled to something very framework specific (like Active Directory). 
Simple answer is that here is no API controller in .NET Core - scaffolded API and MVC controllers will both inherit from `Controller`. If you're after something more lightweight or performant, you can create your own API controller, either injecting `ActionContext` via an attribute, or inherit from `ControllerBase`.
Phew. I’ve built a few core services recently and I’m only doing API stuff, and it’s always a worry when something I need to do is in an “mvc” namespace. Thanks!
What makes it so polished? Does it implement MVVM in a remarkably good manner?
Sorry, that wasn't obvious to me. As someone who has written a handful of WPF apps, I found the MVVM implementation the hardest to get right.
To expand on u/Awful_poster's answer: API controllers have an attribute to control what data type they output: `[Produces("application/json")]`. They should also have Routes specified either on the controller, or in Startup.cs. MVC Route control is set up in Startup.cs: public void Configure(IApplicationBuilder app, IHostingEnvironment env) { app.UseMvc(routes =&gt; { routes.MapRoute( name: "default", template: "{controller=Home}/{action=Index}/{id?}"); }); }
If you are writing site b you can implement a wcf service or webapi. These endpoints will allow you to post data and then you will be able to save to your db.
u/arthurbarnhouse how did you go?
This could be useful, thanks :)
Thank you!
Thank you for saying this. I really don't understand how people confuse the two. 
I wouldn't say that its "better." I would just say its different. EF handles a lot of things that people traditionally have had issues in the past with. And to highlight a few... * SQL injection attacks * Keeping the domain models for your tables and your code in line * Automation with being able to complete the previously mentioned task * donr have to create a repository pattern, because EF is the repository pattern * Removing the requirement of having to know about T-SQL, with things like constraints and identity columns However all that tradeoff means that your app can't scale. EF is slow as all hell compared to use DataSets/Tables for sure, but as others have pointed out its going to save maybe a weeks worth of work in data modeling. 
You could easily do this with web api or anything that allows you to post server to server. That being said take a look at hang fire. You could have website one enqueue a job and website two would process it. Alternatively Service Stack would work really well in the scenario you describe with a gateway. 
Interesting, cheers!
&gt; Due of simplicity I've integrated auth0 authentication, however is there any better alternative and more microsoft environment friendly ? Check out the OSS auth provider list on the docs: https://docs.microsoft.com/en-us/aspnet/core/security/authentication/community 
I haven't tried using Entity Framework in .NET Core yet but in the ASP.NET MVC, you can install entity framework by entering "install-package entityframework" in the package manager console. Now, connecting to MySQL using an MS language can be a bit tricky. You must also install MySQL.Data.Entity in your project by entering "install-package MySQL.Data.Entity --version 9.10" (version is highly dependent on your system; I remember installing the latest version but it only gave me a hard time connecting my application to the database). There are a lot of tutorials on the web that could help you. Let me know how it goes so we can work on it together. 
everything is already installed and still doesn't work for me, do you think mssql is the way to go? 
Automapper has nothing to do with injections. Dto is not about own model, it is that mode properties being queried may not be the same as they are in the db. I guess you’re right, but snapping take &amp; skip on the query is easy enough... and I don’t see what this service gives me beyond that
So, I'm going to be totally honest here. I clicked this with general interest of how you were going to explain this with cats. But then I was asked if this random site I've never been to could send me notifications. So, I closed it and downvoted this thread instead. No random site, you may not spam me with notifications.
There are quite a few things in this article that are bad best practices :-/
stackify is no random site, but that does not give them excuse to send us notifications, like WTF? And the article is moot. This might be usefull for someone who doesn't know what the java &amp; dot net are but other than that it's pretty lame.
The usual way would be to add query parameters for sorting, filtering, and pagination to every GET action in all your controllers. Then handle the code for that in every action, resulting in lots of duplicate code that's frankly boring to write &amp; maintain in a project with many controllers. Sometimes you even end up writing a micro-DSL (ie. comma separated sort/filter values) for every project. Instead, you can let Sieve take care of all that in a semi-standardized way and write less code (check out Sieve's source).
Please follow this tutorial : https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/intro
I'm on my cellphone so I didn't get the notification. But even for me as a junior programmer this article is bad. It doesn't have depth and it highlights points anybody would know that has ever heard of the 2 mentioned languages. Can't recommending reading at all. Btw. There are not cat references just a few random car pictures that don't seem to elaborate with the text. 
Have a look at the free resources on the MS site first of all, see if you get on with them: https://www.microsoft.com/net/learn/architecture There's various subjects inc. ASP.NET Core.
&gt; Visual Studio is the one and only IDE for building .NET applications Not really. There's Rider and monodevelop.
Jesus Christ more web forms? 
Companies are still supporting legacy Web Forms applications. :)
That solved it. The example I saw online had them in the wrong order, something the last two solved it. Thank you!
This is a nice convenience, as someone who doesn't really use twitter thanks for sharing!
Thanks. This was good suggestion, I ended in heriting apicontroller and building few apis from that. Thanks! Still some work to do to get the best practices of ASP.NET
This is new? This has been used for years in ASP.NET Web API already.
I am basically trying to create a new user and add their information into my ‘ user ‘ table but when I click submit nothing happens and I get the following error “ Exception thrown: ‘System.Data.SqlClient.SqlException’ in System.Data.dll “ any help will be greatly appreciated. 
In addition, it should provide some kind of bootstrapper script that renames the folders and namespaces. Otherwise it is not a template, but an instance instead.
Try running that exact insert statement in SQL itself. I'm guessing one of the column names is wrong, or possibly there are other non-nullable columns that you have to specify. Basically, the exception is telling you that your database doesn't understand the SQL statement you're trying to run, but it's not passing enough information back to .Net. If you run it in SQL, you should get a more useful error message. SQL error messages are usually pretty descriptive.
You either used `public ActionResult` or `public Product`. If you used the latter you couldn't return a 404 using `return new NotFound()` as NotFound is not convertible to Product 
Didn't that require you do something like `return Response.CreateMessage(product)`
Depends on the implementation.
Had to do some debugging but it worked, changed the Table name to ‘ Users ‘ and also changed the Id field to include ‘ IDENTITY ‘. Thanks for your help ! 
You could set status codes using Exceptions. Imho the simplest way to do it.
Your not wrong about using exceptions as signal, but it does horrible things for performance. 
I guess the difference now is that middleware handles turning the success path into an ActionResult. Minor change, but I guess it's nice.
Hello, sorry for hijacking this but I work in a company with 3 encvironmenrs, dev, quality (UAT) and prod. Be pretty cool if each dev could have their own environment up in minutes (we use mongo and elasticsearch, we don’t have that many dependencies). Is this easy to achieve?
Yes, Docker compose is where I would start, you could also use minikube with a startup script. Depends on what prod looks like, if it’s aws ECS then I’d just use cloud formation and make an isolated dev deployment.
New?
What's the difference between this and returning IActionResult using Ok(new Product { SampleInt = productId.Value, }); instead?
Apppveyor, one yaml file, each VM is clean.
Seems highly useful. I love it when code like this helps me streamline my own code.
Can't move to appveyor any time soon. We just paid TeamCity licenses for another year or two and we can't use public repos for our code. Lastly, I never worked with custom images with appveyor.
I’d run the front end and backend on my machine tbh, I can run all of our backend services too. We do run on AWS but I don’t need to run a local environment with anything on EC2.
If you're returning an error code, performance is the least of your problems.
It's new in asp.net core. Why it wasn't there all along is beyond me.
this is what i do. everything is an iactionresult. you get access to the whole range, including StatusCode(), for when you're like me and so stupid things with api design sometimes.
Returning not found properly is highly performant
To add, the reason you need a *pluggable* gc (or pluggable anything) is that performance means different things to different audiences. a hyper aggressive gc may be better in more constrained environments, where kb matter. a lax gc may be better if you've got gbs of ram, and can afford to just ignore gcs until "some work" is done and then just throw a whole chunk away. one way or other, super cool to see the .net core folks paying attention to the nitty gritty details like the gc load interface. if this is indicative of other work going on (and I think it is, I've tried a couple of the dailies and have been impressed), super stoked for 2.1 and beyond.
you shouldn't be exposing exception information of any kind if security is a concern. even "key not found" could be considered a leak. security like that is too easy to get wrong (java applets and CAS) so it's a bad idea. so i agree, it's a no brainer. i suspect that they ignored it because helpful "ToString" overrides seem rare, and getting a keynotfound of a "Your.Company.BusinessObject" isn't helpful. However you'd start implementing helpful ToStrings real quick...
 What? you need to return error codes all the time, there is an entire list of HTTP error codes for a reason, that's not a problem. Someone manually types in /55 for a table that only has 1 item, do you expect it to return that one item? You should return a not found error. Returning error codes by using exceptions is an issue. Not returning error codes themselves.
would love this. parameter information is available in the debugger so we know the runtime can figure it out. might be hard for optimized code. wouldn't be helpful in some other clr language scenarios, like f# tail calls.
if you have a simple data model, it can be easy to confuse the two since it isn't so hard to write simple ado code. but you get a meaty data model, orms become more valuable Just had this experience. I was saying "just hand write the sql and use dapper". but their data model was complex, mine was just some simple non-relational book keeping stuff (but needed to be lightning fast). so orm worked better for them. another lesson in "use the right tool for the job".
Which can now be inferred from the actual signature without a superfluous attribute. Seems a bit more idiomatic in my opinion.
 You're still going to use the attribute for a lot of scenarios. What if you return a (Product) for 200, but a (ProductInclusive) for a 201?
Sure, except that was last week and now we're returning `ProductInfo`. That attribute has it's uses when things get tricky, but it shouldn't be used in the general case. 
Do you really care if their return takes an extra nanosecond?
True. Although for ease-of-access, a newer developer (or even just one who wants something quick) only needs the simple case of a 200 with Product, in your example. Advanced (or proper) APIs will need more, but that won't really go away.
 I don't believe you understood what I said, unless you're arguing throwing an exception for something that doesn't need an exception is ok.
Exactly currently it's one size fits all. You can now more easily write exactly what you need.
 True, it should simplify it in MOST use cases (assuming swagger actually implements that), just saying it's not actually going to get rid of the attribute :)
doesnt actually respond to the problem since i'm using MySql and that thing is much different than sqlserv
About seven months ago I started StreamDeckSharp (https://github.com/OpenStreamDeck/StreamDeckSharp). Recently I made a few changed and performance tweeks. There are still a few problems with playing videos on stream deck but it's just a proof of concept atm.
Neat. Implicit operators FTW. https://github.com/aspnet/Mvc/blob/a7cc243942c4bdcf8028ad76571432258b1b2533/src/Microsoft.AspNetCore.Mvc.Core/ActionResultOfT.cs
Yes, I could copy and paste that logic across every single item Get method. Or I could do it once in a filter. I like code reuse.
When I played around with my StreamDeck, I noticed that the device would flip back and forth between the last few 'frames' a few times if I updated them as fast as possible. The more frames I updated in a row, the longer it would happen after the last frame was written, while adding a 1ms delay after each button write completely fixed the problem. I wasn't sure whether I just did something wrong with the USB stuff, have you noticed anything like that?
Yes I did. If I stop the video playback the exact same thing happens. When did you notice that? I'm not sure if it's a bug in the latest firmware or if the old firmware had the same problem.
It was already the case on my non-updated device I got ~2 months ago. I was hoping the new firmware might fix it, but unfortunately it didn't.
True about Chiang Mai but there are people working too... a few crypto startups, App devs some Saas people... as well as the..mentioned others. Yes I'm totally tired of the visa runs. Is pay decent for .Net Devs in BBK ? 
I just wrote a library that internally uses HttpClient. Nothing special, I just extracted a base class I've been reusing few times in different projects into its own package. You can find it here https://github.com/Kralizek/HttpExtensions To write the unit tests I used a package called ’WorldDomination.HttpClient.Helpers’. it basically contains a message handler that you pass in the constructor of your HttpClient. It will intercept the http request and return a forged response according to the configuration. You can see how I use it here: https://github.com/Kralizek/HttpExtensions/blob/master/tests/Tests.Extensions.Http/HttpRestClientTests.cs Disclaimer: test coverage is far from done. Many cases still need to be properly handled.
Yes I know that specifying the generic type is redundant and implicitly derived, I did it more to demonstrate that it's actually there. Currently `Ok()` accepts an `object`, which is not strongly typed. 
Hey there, you might checkout this page here: https://fullstackmark.com/post/13/jwt-authentication-with-aspnet-core-2-web-api-angular-5-net-core-identity-and-facebook-login I implemented this pattern in a few smaller projects and it is working great so far. I don't think you want to rewrite to opt for angular, but it is just there to show you the idea of it.
And for a filter you don't need to use an exception, you can just use a filter or middleware that listens to a 404 result code. Exceptions are awfully for performance, and while you don't care if the 404-request some time longer, one surely cares about the general performance of the application. It's unnecessary load that can be avoided by just writing better and cleaner code.
Why not mock the HttpClient? 
depends on the company. Western companies like Allianz, IBM and Thomson Reuters are higher than a Thai company. Salaries are going up due to high demand. Companies like Agoda seem to pay a high western salary.
The trick is to actually do it with a multiple generic arguments `ActionResult&lt;T0, ..., TN&gt;`. I wrote a Formatter for [https://github.com/mcintyre321/OneOf](https://github.com/mcintyre321/OneOf) types for WebApi a while back, hopefully this will do the same thing.
Who is doing it? 
HttpClient is not meant to be mocked as it acts as a facade (not exactly a facade pattern) between your code and a HttpMessageHandler (typically HttpClientHandler). The package I use gives you an implementation of the HttpMessageHandler abstract class that is optimal for testing scenarios. Check how clean my tests are. I really like how they are simple and expressing exactly what I am working with.
I put together a super simple boilerplate for auth with ASP.NET Core ... https://github.com/matthewblott/simple_aspnet_auth Tutorial here ... https://coderscoffeehouse.com/tech/2017/09/05/simple-aspnet-auth.html
I guess I should have mentioned I'm not using .NET Core at all
It's not out quite yet, but I like [this guy's blog](https://andrewlock.net/), and I'm sure [the book](https://www.manning.com/books/asp-dot-net-core-in-action) will be good also.
Yeah, it seems to rely on .NET Core a lot which I'm not using and don't plan to anytime soon :(
Great job! Can you give some most popular use cases? 
One use case is tenant in subdomain, you can see example here https://github.com/mariuszkerl/AspNetCoreSubdomain/wiki/Multitenancy-in-subdomain
Your posts are also garbage.
Start here https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?tabs=visual-studio%2Caspnetcore2x
That static property is going to be a problem. I'd suggest using something thread safe and user scoped like session or a database keyed with a user id.
I'm actually glad to see you point out the static property because I had a feeling that was going to be an issue. Thanks, I'll look into sessions. I thought I might be able to pass the WizardViewModel along as I went but it seems like this is wrong.
You also get the added benefit of installing your own software. I can now run integration tests, because mysql and vagrant is installed on my private CI agent.
That is not true, and you know it.
After trying to plug away at this I've come up with jack squat. I don't suppose you could point me in the direction of an example? Most examples I've been able to find seem to be either not what I'm looking for, or so outdated as to be completely irrelevant.
Nice catch! Can't believe I missed the second one. Thanks!
It's generally a bad idea to have an unsafe default. If the caller doesn't provide a seed, consider using something other than the time to seed the Random object. 
Sessions work by storing an object in memory on the server that's tied back to the user via a cookie. You put a new object in session that will live across multiple requests. Update it by pulling the object out of session then pushing it back in. I recommend against sessions because it breaks the server-less architecture. Since sessions are tied to a single server. You are looking for a place to store the full object while the user works on parts of it. You can do this many ways. Another standard way besides session is using a database. When the user starts the form, you create a db record to keep track of progress. Another alternative is storing all the data client site. This can either be by posting the full objects as hidden fields, or as a full client-side javascript form builder. I go with the js way anymore because it makes the server code simpler, but adds js complexity.
Sessions don't have to be tied to a single server: https://docs.microsoft.com/en-us/azure/redis-cache/cache-aspnet-session-state-provider There are other examples using SQL server, gemstone etc
Is there a reason that you use lowercase namespace instead of Microsoft conventions regarding using uppercase/lowercase letters? (Based on the readme)
 I disagree with almost all of this. Great, you know high level concepts, you go to an interview for a C# position. What is the difference between an ArrayList and a List&lt;&gt;? Cool, you can explain to me what a data structure is. &gt;The short form? Businesses looking to hire you care only about how you are going to help them by way of increasing revenue or decreasing costs. &gt;In that light, being a JavaScript developer sounds more like a limitation than a feature. No, it's the exact opposite of that. If you apply for a JavaScript position, knowing "high level concepts" is not going to help you get that job - at all. Knowing JavaScript for a... JavaScript position... is a requirement, not a "you can know if it you want". &gt;Finally, if you’re about to start a new role and you know you’ll be using a programming language you aren’t familiar with, learn the language in the context of building something. &gt;Don’t spend time reading language syntax documents, line by line. Instead, pick something to build that’s related to the industry and use the language. You’ll without a doubt learn the most common features of the language in the process. Does anyone actually do that?... Who reads documentation line by line? Anyway, this is only applicable to the absolute beginner of jobs. If you don't know the language you're applying for, you're not going to get it unless it's actually a language agnostic job, like a startup, that doesn't care what you use, just want it done. Every language has very specific ways of doing this, and libraries to be used. You need to focus on a few languages, and expand your knowledge into how those concepts apply to other languages how you go along. This sounds like an article for a college, "Learn a bunch of concepts but have 0 idea how to apply them". This is literally the same flaw of why college is nearly useless for software development.
I used this tutorial as a starting point. It uses ASP.NET Core 2 https://medium.com/@lugrugzo/asp-net-core-2-0-webapi-jwt-authentication-with-identity-mysql-3698eeba6ff8
&gt; Sessions work by storing an object in memory on the server that's tied back to the user via a cookie. You put a new object in session that will live across multiple requests. Update it by pulling the object out of session then pushing it back in. Thanks for the explanation. I do know what sessions are I just haven't used them in ASP.NET yet (I'm a dirty PHP dev) so I'm wrestling with the API. I personally would go with the JS approach but I am trying to replicated *exactly* an existing web app as a learning exercise. The result is more server requests than I would prefer (more than one). I'm actually getting frustrated to the point where I may just say to hell with learning the way they did it and use my own way. 
It’s very possible they are using web forms but I checked the source first thing to look for hidden fields and they aren’t using any :(
Your code does not generate unique ids. If you generate enough of them, you will have duplicates.
Isn't Microsoft planning to phase out Access
One for example has users in the field entering data into local copies of Access. Weekly/Monthly they run an import macro that copies the data into a "Master"Access Application/Database. The Master then gets sent off somewhere else so the data can be exported. Every time there is an application change to be made, they send me the latest copy of the "Master" and everything is on hold until I send it back. I can see some obvious workflow issues that can be streamlined,and what I'm looking for are ELI5 reasons to use .NET over Access. 
There are many screenshots if you scroll down so I don't see the problem.
Don't worry... It was obvious to me what you were talking about. I would never assume polished meant *clean* code.
 List is generic, ArrayList is not. ArrayList stores everything as objects, so you incur constant boxing and unboxing fees, nor do you know the type when you go to use it. There's a bunch of other things, such as implementing IEnumerable, Linq, etc on it as well, but that would be a pretty big topic. ArrayList is old, you should always use List&lt;&gt; now :)
Most functional languages have something resembling looping, even if it's just (tail) recursion over sequences. Tail recursion is supported by the CLR, but C# unfortunately doesn't expose it (F# uses it, though) Bindings (and function parameters) are superficially similar to variables, but understanding the difference between bindings and mutable variables is very important.
Nobody has tried to ELI5, so here goes. Microsoft Access is to .NET as cellular telephones are to smartphones. What once seemed fantastic and easy to use, is now archaic because the replacement is so far, far better. Dot net will enable the business to expand more easily, have much less friction with the technology they use (everything can all be connected and updated, in real time, for everyone) and most importantly: reduce future cost. In every single way, it is simply better because it is meant to be. 
Are they using the forms stuff in Access or just entering data directly into tables? Try to identify pain points like each person only seeing their local data until it gets merged and everything being on hold when they need a change. Ask about the benefits they would expect from modernizing the system?
These clients are presumably using forms in Access as well, which would need to be replaced. Access provided an all in one database and app layer for power users.
Because the maximum size limit of an MS Access database in 2018 is 2GB. It's only a matter of time before their system stops working. If you figure out the current size of their data and the growth rate, you can extrapolate how much time they have left. 
Which is precisely why the OPs question makes no sense and why they're getting push back when they say it, undoubtedly.
What the hell does it even mean to "move a database into a .NET-based stack"? Like, honestly, what does that even mean? How is that "reasonable" to ask? You can't even explain it to a technical audience, let alone a non-technical one. &gt;The comment I replied to said they'll replace Access with [insert database of choice here] and use .NET to create an application layer I uh, I'm done here. If you honestly think that is somehow "nonsense", then we're not even remotely close to being on the same page. That's literally the only answer that makes any good sense for the question posed by the OP. BTW, I'm pretty sure you misread the comment that you replied to initially. They were commenting that you could build a frontend that talks to an MS Access backend, which is also not a terribly unreasonable thing to suggest given the OP's question.
Let's go over the facts rather than continue to escalate: * OP wants to replace Access with .NET. * The comment I replied to suggested this is nonsense and that you dont need to replace Access with .NET, you need to replace it with some other database (SQL Server, MySql, etc). * My intent was to explain that if one is nonsense, then so is the other. In both scenarios, you are replacing Access with both an application layer and a database. Access provides both. If you are going to be pedantic about OP's wording and say "Access to .NET" is incorrect, then you need to apply the same logic to the assertion that "Access to [database]" is correct.
Everyone else brought up good points regarding performance, scalability, and flexibility, but I'd like to throw in the fact that Access has no real security. Simply holding down the shift key while opening the database will bypass any startup macros, and I believe even the old security from .mdb Access versions (2003?) can be bypassed with external libraries. If your client has any sensitive information stored in the databases, it may as well be plain text.
The OP didn't mention the need to scale. "normally become corrupt", really? This is normal behavior?
Let's replace Excel and Word with .NET while we're at it. Makes about the same amount of sense.
I wish Microsoft had kept Access current with .NET technology and maybe even incorporated SQLite. The truth is, there’s still nothing out there that does that all-in-one job that Access did. They’ve tried offering several different products as the successor to Access, but they all miss the mark in my opinion. Having said that, Access is utterly useless by today’s standards, being incapable of using web APIs or even .NET assemblies without jumping through a bunch of hoops. VBA is missing basic features like regular expressions or serialization. The data type limitations and 2GB file size limit are often show stoppers. Obviously nobody should do any new development in Access, but if they are currently using a solution that isn’t hitting those hard limits, there’s no immediate pressing reason to rewrite it. But at some point there will be. 
OP didn't give full specs for the application either.
Thanks, I guess I'll do the same. 
Access and .NET are different things, as pointed out by /u/Eldorian and so this question is difficult to directly answer. Access stores data. That is it's primary function. It has some basic tools layered on top of it to make it easy for users to enter the data, view the data, and interact with the data, but at it's core it's meant to hold data. .NET is a programming language for application development. It can write applications that do the same things (and more) that Access does. It by itself cannot store the data. You would need to get a separate database package. Some examples are MySQL and Oracle. Why would someone switch from Access to .NET and a different data storage method? Power and flexibility. As someone who creates Access databases at work, I know how nice Access can seem. But I also have dabbled in .NET and have seen the light. Access has a tendency to get corrupted, security for it is relatively easy to bypass, and as you clarified elsewhere on this page, is not live (end of month updates only). .NET + a database application on a server has downsides too. Building something this way is often slower than building it in Access, but the flexibility you gain is enormous. Do your databases have VBA code? Imagine writing the same code in an easier to understand format in half as many lines. Do you wish you could customize the way your data is displayed more? Maybe throw some nice graphics / animations in there? .NET will let you do it. Does your VBA code run kind of slow? In Access the compiler is compiling one line at a time, .NET would be pre-compiled and faster. And the database / storage side of it? Access has some tools for querying and reporting, but often a full fledged database on a server will offer nicer tools that do more. Need 3 sequential queries in Access? Probably can do it in one with a server database. In my line of work Access is great for short run, non-essential databases. We collect temporary quality / audit information about manufactured product. But the things that actually make the business flow like taking orders from customers, releasing orders to the shop floor, timekeeping and payroll? Those all run via an Oracle database on a server that is backed up regularly and accessed via .NET applications. We can't risk someone closing their database weird or early and corrupting the Access database.
While this article may not be the most well written, I generally agree with it's sentiment. It may not be a popular opinion on a programming sub, but in general one more programming language is not really going to get you *that much* of a leg up in terms of job opportunities. What businesses need are problem solvers, and it just so happens that programming is a way to solve those problems. A specific language is more about the existing systems that need to be integrated with or the stack that's already in place. While learning Go might seem interesting to you (I've thought about it myself), if you can't articulate why that is meaningful and valuable to the business then it won't move the needle for them. What does move the needle is understanding how programming fits into the overall business and how you can add value to the business. Can you make people more efficient? Can you bring in more customers? Can you make data more accessible and/or facilitate better decision-making? Can you identify new products and bring them to market? Those are all things that are facilitated through programming, but ultimately one more language probably isn't going to help you all that much. Overall system design, architectural principles, and business analysis will likely get you much farther. Most programmers can learn a new language/framework/technology if asked, and while it'll take some time to ramp up, most of the time your ability to learn is more important than whether you already know the language/framework/technology. That being said, you won't convince someone that you're worth hiring in spite of not knowing that particular language/framework/technology without having a solid understanding of the fundamentals, strong problem solving skills, and an ability to understand the businesses needs. Those are the things that will distinguish you more to most employers than knowing the Go or Rust.
If they have a working access APP, migrate the backend to MSSQL and update the app they are using to connect to the DB server. Sure its an outdated UI but it does what they want I suspect.
I guess I don't understand the distinction, or am too tired to care right now. Sorry for my tone earlier.
At this point `ArrayList` is code smell (and has been since 2.0) - it says you don’t have a well defined object hierarchy if you need to deal with a group of objects with no common ancestor. 
 I work in a massive project that was written on the first release of .net, and it's still littered with ArrayLists. If it wasn't for my work I probably wouldn't of even known what they were either :)
Access is pretty much Rapid Development, you will have an interesting time convincing someone to get rid of Access development if it is already part of the workplace. I am working in this ecosystem now. My main duties are to migrate Access DBs to SQL. The thing is, you cannot get rid of users using Access DBs no matter what you do. The only thing you can do is evaluate each situation as it appears and determine if a specific database will be accessible by multiple users. If so, storing backend data in SQL would be a better solution than in an accdb/mdb file. Another thing to note, Transitioning from VBA to .NET is a huge undertaking but very possible. Best way I have convinced superiors to switch is based on use. If more than 10 users, SQL and .NET. If only a handful of users, Access front end and store data in SQL back end. If one user, Access. Good luck to you and your quest. 
To play devil's advocate, this copying of data can be done in Access as well. Access the master over any DB library (old ADO) and write there from an access macro on the local db, or the same for that final export. Is the problem that the export destination can't be accessed like that? If so, it's a question of network, which is kinda unrelated to Access vs. .NET.
[Access was added to Office365 not so long ago so probably not.](https://blogs.office.com/en-us/2016/11/04/microsoft-access-now-included-in-office-365-business-and-business-premium-with-new-enhancements/)
Access databases are prone to corruption and are not well suited to use by multiple users. While your individual field agents should be ok to continue using access, the "master" db should be migrated to a more robust technology *assuming* this is a business critical system. Additionally, once the master db is a proper server (eg one of: MySQL, postresql, mssqlserver... etc...). It starts to make a bit more sense to build out either a web interface so records are entered directly into the master database, or move away from using access as the local client. Though in all honesty... I wouldn't be too fussed about access in the field. If they sync daily, you're at most going to lose a handful of records at any given time. Access does do some things pretty well. I mean... at that point though, why not excel with a macro? Anyways, I don't know much about your system. So that's just an off the cuff answer.
As mentioned by some other posts here, most workplaces use Access because they're seeking a "no code" solution to rapidly develop business solutions. Developing fully customised solutions using a SQL database and a .NET stack and the requirement for software engineers and software development overhead to be involved is typically what they're attempting to avoid. My assumption is that you are asking this question as you have software engineering skills but are currently supporting a heap of legacy Access solutions (a pretty common scenario I and many others have been in). To answer your question, MS advises against continuing to create Access DBs particularly for distributed applications that are accessed by many users (as per this article https://support.office.com/en-us/article/access-services-in-sharepoint-roadmap-497fd86b-e982-43c4-8318-81e6d3e711e8 ). Hopefully your work place has or is planning to migrate to Office365. If that's the case my recommendation, which they suggest in that article, is to look at using PowerApps (unfortunately only available on Office365). PowerApps is fully "no code" but it can make calls to web services so a common solution stack uses PowerApps with Flow and Azure Functions. Good luck with finding a solution to your predicament.
Yeah, I can see that. I’ve never met anybody who took a look at genetics and thought, nah, I’ll just use boxing/unboxing. Those days seem so crazy not.
My company currently uses .NET which forces us to provision Windows servers at AWS. If we migrate to dotnetcore we can switch to Linux and cut our bill in half. That's like $50 grand a month at our current size (and we're growing fast).
You can have forms in access which have a SQL back-end though
Why not? I think it is really good and working really well for a backend api. Especially combined with EF Core.
 It's a rewrite of .net to be much *faster, open source, Cross Platform*(YAY no MS Server Lock-ins/License Fees). This Article Explains usage https://docs.microsoft.com/en-us/aspnet/core/ This one compares it to the OLD framework https://docs.microsoft.com/en-us/dotnet/standard/choosing-core-framework-server
Firtly: .net core and aspnet core are two different things. .net core is the base - with it you can build console apps &amp; libraries (that you can pack into nuget packages if you want). aspnet core is a web development framework - it is built ontop of dotnet core. With it you can build api's &amp; mvc/razor based applications. You may also plugin SignalR if you want "realtime" network comms. aspnet core is just a fancy console app that listens for requests and hooks those request up with your application code, usually in the form of SomeController + routing magic. aspnet core significance comes from that you can build api's with it quickly, it runs quickly, it can run on a wide ranges of server OS's and to top it off, you get to code in C#. Being able to run a small/mid business off of $10 to $20 hosting for almost your entire stack is amazing. The rest (email &amp; cloud storage can be done inhouse if you have static IP's). ~Most~ companies don't need "webscale", load balancers, 99.9999% uptime nor do they need their entire system to be microservices. So even for just starting out, you will have a good experience. For the other large companies that do need those kind of things, you can most definitely do it with aspnet core, but then your focus will shift more to process/devops flow and how everything gets glued together and the aspnet core choice is just one of many considerations, probably not even the most important decision (usually your host OS choice &amp; language choice &amp; licensing is the most important decision decidors (word?)). My currently workflow looks like this: I have a linux box with *only* the dotnet core runtime installed (not the sdk) and nginx. I develop the actual api's in visual studio (for obvious reasons) on my main machine (I use Fedora with Windows in a VM on an SSD - works perfectly), then when I want to see the magic running "like production", I deploy it to my test box. If it works there over the network, I can deploy the release build to the real VPS on Digital Ocean (Ubuntu is easiest to get started with). I havent figured out how to automate my build &amp; deployments yet. Maybe at some point in the future Microsoft will port Winform/WPF/UWP to the other platforms (not like the xamarin way - an actual native port) and will then be compatible with dotnet core on different platforms (personally, WPF + .netcore on Fedora would blow my mind). 
There's a lot less legacy code in Core and a lot of it was rewritten. The upshot is better performance.
If not more, coz with docker you can squeeze every last penny out of those servers
Short Answer: * Performance * Cross Platform * Earlier access to Azure specific features There is a write up on MS Docs detailing how to decide if Core or Framework is for you.
You do not have a filter on your insert statement, you select everything from shoes. You'll have to look at your sender object the event gets. Maybe there's a row in there where you can find the Id or add the Id to the hyperlink as an attribute?
Android? :(
Right now the best thing .net core is good for is asp.net core on Linux. 
Adding to the list: Side-by-side versions .NET Framework only allows a single version on a machine. If you have a server with many apps on it an one needs a newer version of the Framework upgrading it changes the version every application is running on. With .NET Core you can install a new version (e.g. 2.1) and the applications using 2.0 and 1.1 will happily keep using their version; so there is less disruption/upgrade fear.
Thanks for describing your workflow! That’s what I was looking for. Examples of asp/.net Core stacks
That’s definitely a good reason!
It has first-class support for Linux/Mac, which makes it inter-operate with other language/platforms/OS a lot better. For example, I had a text-image processing service that relied on Tesserect (OCR program developed by HP and Google), getting that configured correctly on Windows with .NET was pain, but it was trivial to setup with .NET Core on Linux. 
I would databind each button's CommandArgument value to be the ID or other primary key value for the shoe in its row. The CommandArgument gets passed to the click event, so you can then use it as a WHERE clause at the end of your SQL statement to choose which shoe gets add to the basket.
The word You needes was "factor".
I'm using ASP.NET Core because I can deploy it on Linux and certain native library I'm using has like triple the performance on Linux vs Windows. I've actually ported my project from full framework because it was unusably slow.
This is a very good point.
Xamarin
.Net Core for the backend and Xamarin/C# for the app
&gt; .NET Framework only allows a single version on a machine &gt; With .NET Core you can install a new version (e.g. 2.1) and the applications using 2.0 and 1.1 will happily keep using their version; so there is less disruption/upgrade fear. https://docs.microsoft.com/en-us/dotnet/framework/migration-guide/version-compatibility https://docs.microsoft.com/en-us/dotnet/framework/deployment/side-by-side-execution Unless I've really missed something all these years, I'm not really seeing how this is unique to Core. 
Can't upvote this enough. Xamarin is awesome, even if you don't use Xamarin.Forms. Tooling is ready made with Visual Studio, and it just works. VS needs a better layout editor tho.
Are you sure you specifically need Chromium? I used CefSharp for a project recently and not only did it run like arse, but it had a horrific memory leak as well that caused memory usage to go up to several GB over the course of a few hours. The WinForms version ran well, but still had the same memory leak issue. I ended up using GeckoFx instead, which worked fine for the most part (although console.log statements would cause an exception to be thrown).
I just thought that would be better instead emulating GeckoFX inside a WindowHostControl . Are you on WPF?
Access to linux ecosystem. We are using on premises solutions and managing windows servers, simply sucks, just try automating it. Microservices architecture is hard to monitor, orchestrate on Windows, networking capabilities are inferior. Since we switched to to .NET Core, Node.js, Docker and Kubernetes, it's way easier to manage and eveything works in more predictable way.
The application ended up shipping as WinForms so I don't remember if it worked with WPF or not, sorry.
that's a compiled file. you shouldn't need to have it open most of the time. if you build the project, that file gets replaced. If you make edits in that file, then build the project, your edits are tossed. Unless you need the g.cs open for something, just close them. 
I see, that file gets opened by itself while I had some XAML error. The error is fixed in XAML but I guess I forgot to close the g.cs file.
Okay so I ended up figuring it out a different way which included a deep dive into the MSDN basically what I did was cast a sender as a link button control and compare it to a set of other link buttons in the table and then just altered the sql statement to work based on the name field. Not the best way I realised but just needed a quick short term fix. Thanks guys, your ideas inspired it. 
Same here, also IIS isn't exactly quick
Understatement. IIS also lags behind its peers in ease of configuration, security, and resource requirements.
Framework example of this is the TLS 1.2 migration. .Net 4.0 applications did not have TLS 1.2 enabled by default via ServicePointManager. However, installing .Net 4.6 does enable TLS 1.2 by default, even in apps that were compiled for .Net 4.0. In that case for me, it was a lifesaver. However, this same mechanism means that installing new frameworks will put new, potentially unexpected code into your app dependencies regardless of the compilation target. This same "fix" would have removed SSL3 from default, so if you are relying on legacy services that have not been updated yet you are hosed. With framework you are utterly reliant on Microsoft to have 100% code coverage in their tests to ensure nothing breaks and their decision making on what functions to remove.
Every release of the framework from 4.0 -&gt; 4.7.2 has be an in-place upgrade
You can use docker with .net framework as well, on windows, so that's not super relevant.
I'm using it quite extensively both personally and professionally. At work I ported several APIs to dotnet core, which reduced a several day Windows/IIS setup process into one (okay two) lines of code: `git pull xyz &amp;&amp; dotnet run xyz` Also several of our APIs fit into docker containers now. Great for distribution of prototypes and SDKs. Personally I'm just happy to be able to script things out in C# on Linux and OSX. For example yesterday I had to reconfigure my gamelist.xml and image files for my retro gaming ROMs to work with a new front end (RecalBox). This involved xml reading/writing, and plenty of file/folder management. Did this all in VS Code. No sweat. Kind of like how multilingual speakers think in their primary language -- with technical endeavors I think in C# and the .Net framework, so now that I can apply to nearly everything (and not just enterprise apps on windows) thrills me.
Adding another layer won't make it faster.
.NET Core is meant for high performance web sites -- Microsoft's official stance
https://docs.microsoft.com/en-us/dotnet/standard/choosing-core-framework-server
That's not entirely accurate. While Access itself may not have security, it's built like any other Office app, it will obey the AD file system permissions. 
+1 Don't just chase shiny things. Dig in and analyze there data throughput, usage volume, and underlying business need. 
Check out how GitHub does versioning, it's the cleanest out there IMO. It uses the Accept/Content-Type headers. No idea how you'd implement it in ASP.NET Core, though. I suppose it should have support for content negotiation.
Microsofts newer .net solutions and most other newer technologies in other languages deviated from the approach you are writing this website in. Microsoft asp.net was an absolute clusterf--- from a browser support standpoint. The plugins that runat=server weren't supported very well by Microsoft (can't really blame them), didn't have great browser support and Microsoft has moved away from them. Asp.net tried to simplify web development by removing the client and server side code distinction, the problem with this is when the client threw errors you had to go through a rats nest of Microsoft's cryptic JavaScript to determine the cause and then write a JavaScript snippet of code to bandaid the page. Microsoft is moving in a great direction lately with .net core and MVC. MVC can start your project with the barebones of a rest API. Rather than interact with the backend by redirecting the user to pages expecting data and laborious lugging a user from page to page transmitting persistent data from page to page. You simply have the web controls on the page hit an http reference via ajax which returns you json or xml. That data is like an object and you can easily recurse through it's entities. Your API that you retrieve data from and post to interacts with your data, handles authentication, performs error control and feedback. It's also the central point for your documentation for everything that happens between the app and the database.
You will get this if anything outside visual studio modifies a file. It’s basically asking do you want to keep the changes in your editor which are not saved or throw them away and reload the file with change made externally. Also yep the debugger will open files to show where the exception happened in the code even if it a auto generate cs file
I would highly encourage you to look at pivotal cloud foundry. It's very easy to work with as developer, and they have a publicly hosted version (run.pivotal.io) or if your company is large enough consider spinning up your own paas instance. It's multicloud so can port it to any of the big 3 public or vmware / openstack if you want to do onprem. . Net is supported for both framework and core via Linux and Windows support (though public version hosted by pivotal only offers Linux for now, so you are limited to core). What makes it great is that you don't have to think about things like containers or docker. Just push your binaries, it will detect the app type, introduce the runtime and hook everything up. Containerazation is a function of the platform itself. It also offers great features like distributed tracing, marketplace model that injects things like connection strings directly into your app via env variables. If you use SteelToe packages (search nuget) which are now part of .net foundation, it makes development a breeze
Cross platform. Will allow us to easily cut out Azure bills by running our services on Linux/lightweight docker containers. Plus performance. New services are recommended to be built on Core 2, while older freamework 4 based services are slowly being ported.
Chrome user here; just curious: WPF is a MS tech, wouldn't an embedded MS browser be easier to integrate? 
Does the interface have multiple implementors? No, *mocks don't count*. At runtime, are there multiple implementations of the interface? *Then, and **only** then*, does it deserve to be an interface. You can always extract an interface later, if you need it.
Deploy your .net core app as an Azure app service, and I believe azure offers postgres dbs now.
It depends on the type of project. The library in currently working on will be consumed in many ways. So by interfacing everything, I don't have to volunteer for all of the little extensions and behavior changes that people didn't know they needed. If you're building a downstream project with no consumers, then sure, I don't see the big deal. On the other hand, it's so trivial to do ahead of time and to use inheritdoc, I also ask - why not? 
Oh my.
Sure, all solutions should start with "it depends...". Too many devs/leads out there don't ask this question though. 
Mocks count to me. It's way easier to mock interfaces than concrete classes.
Except windows has a licensing cost and requires beefier vms to run the os
It’s not about making it faster, it is about stuffing as much useful code as you can onto one virtual machine/server 
can you expand how interfaces make mocking easier than virtual?
I spent another day looking over sessions and this is the revised code. It has the same effect and looks a lot cleaner. Any chance I could get a quick glance? If not thanks for pointing me in the right direction nonetheless :) namespace DataBetweenControllers.Controllers { public class WizardController : Controller { const string WizardViewModelKey = "_WizardViewModel"; public IActionResult Index() { return View(); } [HttpPost] [ValidateAntiForgeryToken] public IActionResult Contact(PersonalInfoViewModel personalInfoViewModel) { if (!ModelState.IsValid) { return RedirectToAction("Index"); } HttpContext.Session.Set&lt;WizardViewModel&gt;(WizardViewModelKey, new WizardViewModel { PersonalInfo = personalInfoViewModel }); return View(); } [HttpPost] [ValidateAntiForgeryToken] public IActionResult Review(ContactViewModel contactViewModel) { if (!ModelState.IsValid) { return RedirectToAction("Index"); } var wvm = HttpContext.Session.Get&lt;WizardViewModel&gt;(WizardViewModelKey); wvm.ContactInfo = contactViewModel; return View(wvm); } // User should not be able to access any of the actions via GET [HttpGet] public IActionResult Contact() =&gt; RedirectToAction("Index"); [HttpGet] public IActionResult Review() =&gt; RedirectToAction("Index"); } }
but that's not getting more out of it because of docker, that's getting more out of it because of the OS, which is what they said. As long as Windows has Hyper-V (windows 10 professional/enterprise for instance), there will be no difference between linux/windows for docker.
I think it's important to *justify* your mocks. Lots and lots of mocks is an anti-pattern that shows there's too many coupled objects. And if you're doing IoC, it's easy to swap concrete implementations.
It fairly safe to check the box on that dialog. If you haven't changed the file in the editor, you probably don't care that it got updated. This file is probably re-generated each time you save HeroPage. If you are using VS 2017: I've noticed that it keeps prompting me about changed .sql files even after I close them. It seems to track them until I restart VS.
Do you mean making all the public members virtual on the class?
If you forget to mock a property/method in an interface, your code won't compile. If I add a new virtual property to my class, it's easy to forget to mock it and, by definition, it's *optional* to override. In a mock you should be required to override everything (or nothing) otherwise your unit tests will start having to work around your mocks. Also if I want to swap out functionality I don't have to inherit from a base class (that comes with its own baggage), I can just create a whole new class and implement the contract. If I need logic from the original class, I'll use *composition* instead of inheritance.
I agree. One reason why I favor interfaces for mocking over virtual methods is because I don’t want things to be capable of being overridden until it actually becomes necessary. I ultimately want to be able to look at a class, see virtual methods, and say “ok cool, these methods are virtual because something somewhere needs to override it.” If everything is marked as virtual then to me that muddies the water in terms of what the intent was for how that class was supposed to be used and worked with. Using an interface allows the design of the implementation to be explicit and clear while adhering to a contract with the added bonus of now being mockable. That being said, I don’t think interfaces need to be slapped onto everything. As always YMMV
I have a pretty strong tendency to define an interface for anything that I am putting into the DI container. Aside from that, if there aren't multiple implementations, there is no separate interface. 
I realize I replied to the wrong comment just now. Putting it here instead. One reason why I favor interfaces for mocking over virtual methods is because I don’t want things to be capable of being overridden until it actually becomes necessary. I ultimately want to be able to look at a class, see virtual methods, and say “ok cool, these methods are virtual because something somewhere needs to override it.” If everything is marked as virtual then to me that muddies the water in terms of what the intent was for how that class was supposed to be used and worked with. Using an interface allows the design of the implementation to be explicit and clear while adhering to a contract with the added bonus of now being mockable. That being said, I don’t think interfaces need to be slapped onto everything. As always YMMV
You know an easy port for system drawing?
That looks better. Only critique would be to avoid static contexts in favor of dependency injected interfaces (ex: direct access to HttpContext). That is more for unit testing than performance. Basically it is difficult to mock a static. The System.Web.Abstractions lib is a good place to start. Either that or look at the repository pattern. Others have pointed out that you may want to look at your session state provider if you have more than one server. That way you get distribution of the session data across the farm.
Good one ! Tune is stuck in head now ...
Hopefully it'll be apart of the compatibility pack soon: https://blogs.msdn.microsoft.com/dotnet/2017/11/16/announcing-the-windows-compatibility-pack-for-net-core/
I generally draw the line at unit testability. Can I reasonably unit test my class with the concrete instance? Keep it as is. If not, wrap it with an adapter and interface the adapter. As far as refactoring to abstractions if you have ~ three examples of an implementation and an abstraction can be derived that makes sense, generalize. I've also seen examples where a layer supertype can be beneficial to provide a facade to prevent a vendor library from leaking all over your code.
When mocking a concrete class with virtual members you still need to be able to provide its constructor with arguments which will typically be dependencies when using DI. This means you have to mock out all of the dependencies as well as the actual class (and if the dependencies are concrete classes with their own dependencies then the problem repeats) or you have to provide constructors that can instantiate the class in an invalid state purely for testing purposes (yuck). Interfaces can be mocked without having to deal with all that.
* Performance * Cross Platform * New Features * Consistent API across vendors
Good, it lost a long time ago.
I don’t know and I am a lead developer. But guess what? a google search reveals it should be essentially deprecated.
The overhead of windows within windows is still higher? 
&gt; The System.Web.Abstractions Just so I'm clear, if I want to use DI I would add a property and constructor to the WizardController class, correct? private readonly HttpContext _httpContext; public WizardController(HttpContext httpContext) { _httpContext = httpContext; } 
You said ‘you can use docker with .net framework on windows’. Full .net framework requires full windows runtime. Anyways, i feel this is not a productive discussion.
Dunno about ‘just works’.. had a range of issues trying to add custom obj c categories and then the fat that you need to have two computers and keep xamarin versions on sync is quite infurtiating
In theory, theory and practice are the same. In practice, they are not. -- Yogi Berra In theory the article is generally right. In practice you need to be able to think like the language. If working with a framework (like .net ) you need to know the secret handshakes that are necessary to get past those pesky tooling issues that can cost you a day or two. If you really want to know what separates the men from the boys ask yourself my favorite interview question: What do you do on weekends? If you write code in your spare time you are a dork and patterns and languages are just more interesting things for you to absorb. You are already at "the next level". If any of it is work you should focus on syntax. So if you are going to learn anything, learn to enjoy being a coder. 
You can run an ASP.NET Core website on a Debian or Redhat box and never touch Windows Server again.
I'm a fan of interfaces at topological boundaries (i.e., persistence library, configuration library, security, etc) for unit testing purposes, as well as some degree of swappable implementation. One of the advantages I've found in doing this is making it easier to unit test different scenarios (and therefore code paths). Another benefit is being able to swap out parts of the system more easily and divide labor. What's your take?
&gt; The only exception to that I can think of where you don't need pre-existing classes is e.g. making an interface for future developers, e.g. if you're releasing a library for other devs to use. In that case, you may not have a class to implement the interface on yet. For the love of all that is good I hope in this theoretical scenario they provide a reference implementation or at least unit tests.
App service spins up a vm per app. It's not container based paas so cost start adding up quickly if you have multiple env or have to scale out
My favorite example is for communicating with other services. Especially when the topology/protocol might change over time. IMO one of the best times to actually *use* things like abstract classes and interfaces. `interface IDoThingsWithDomainObject` I'm just sending and retrieving data. Maybe I'm doing it with a database. `class DbDoThingsWithDomainObject : IDoThingsWithDomainObject` Which is probably not going to look like a whole lot more than a normal 1-1 concrete implementation; doubly so since most Database classes tend to either be awkwardly abstracted with EF, or tightly coupled with any given ORM. But, then you run into someone who decides that it should be it's own REST service. You might then wind up writing a class to deal with that instead, nicely wrapping your web calls up: `public class HTTPDoThingsWithDomainObject : IDoThingsWithDomainObject` But then, over time, you do this more and more with OTHER classes too And you notice that a lot of your code feels very boilerplate. Things like logging HTTP calls uniformly, setting up your certificates. You decide to be a little more clever: `abstract class BaseHttpThingDoer' `class HTTPDoThingsWithDomainObject : BaseHttpThingDoer, IDoThingsWithDomainObject` Note the base class DOESN'T implement your specific interface, but rather would have your methods set up to perform your web calls however you want to do them. I used HTTP here as an example, but if you were dealing with another method of connection such as RabbitMQ, Akka.NET, GRPC, etc. it would be just as useful. Most of the time you cross service boundaries it is a good idea to have the way you cross those boundaries *uniform*, especially if you have the ability to keep it uniform. I've done this sort of abstraction pattern before, both WRT abstracting calls from Direct, to HTTP or Akka Microservices, and also for abstracting out the way data was uploaded/downloaded. In the latter case, we had a number of partners that received the same/similar data, but would require slight changes to the format, and/or wanted the data dropped different ways. This is when I learned the (somewhat hard) way to *avoid abstracting early unless you have a lot of experience.* While the API itself was very functional, I wound up having to 'do-over' a lot of the parts because it wasn't necessarily *friendly*. While it was super-freaking-cool that this thing could be switched over between Local Files/S3/FTP with minimal effort, developers had challenges writing the initial code to actually have it do what they expected. What revisions I was allowed to do helped me learn, but it was still an awkward lesson. You'll sometimes see this over-abstraction when people try to use the 'Generic EF Repository Pattern' across an application. I pick on it here because it's the worst of both worlds; The EF Repository pattern is fairly abstracted (**Within its feature set!**) but most implementations of `MyDomainObjectEFRepository` wind up being so bare that the dependency on EF *leaks* outside. A good abstraction means I can switch between what provides those domain objects, and only have to touch minimal, if any parts of the rest of the code. (IOW: If your Development effort to switch between EF-NHibernate involves more than your repository and Container/DI layer, it's probably leaking.)
Well said. Without interfaces DI and generics are much harder to utilize, if not impossible.
I forgot about the ControllerContext property on the base Controller class which is essentially the same thing you have, without the need for a external dependency. If you write a unit test, you'll need to mock it out which isn't too much of a problem. 
The compatibility pack is coming.
Until MS add it to the compatibility page try the [CoreCompat.System.Drawing.v2](https://www.nuget.org/packages/CoreCompat.System.Drawing.v2/) package. I've been using it with a .NET Core 2.0 web project without any problems.
That's what 'app service plans' are for.
That is so much easier, thank you! My biggest issue with learning ASP.NET right now is that things that I'm used to doing in PHP seem to take a lot more effort. Still, it seems a lot better than when I attempted to learn it back in the early 2000's...
Yeah we implemented a custom version of this in my shop ourselves. You can return Result.Fail&lt;MyClass&gt;("Error message here"); or return Result.Ok(instanceOfMyClass); We have more complex errors than just strings, too, so you can pass out an InvalidParameterError, (among others) with relevant metadata useful for debugging.
In asp.net Core the only main different is that MVC has a View and Web Api is accessed via url endpoints. You’re likely going to use web api. Mvc is for web apps that you access via a browser. 
It doesn’t, but do you know of other .net core specific alternatives for stuffing a server full of services?
Using ProcessStartInfo will execute it on your local machine. You can start the process on the remote machine with WMI. 
We're talking Android here. iPhone development sucks by default, even with Xamarin.
Guy, did you have any issue ? At work we are used to work with the .NET Framework but we tryed .NET Core for a new Application ! Result: no lazy loading, no windows authentication (with kestrel) , no performance profiler :s Our project with Angular + DotNetCore was a disaster 
Web API will be what you want, probably returning JSON data. Cordova/Ionic will give you a native app that can then hit them API endpoints. Without sounding condescending, maybe take another look at the differences between MVC and Web API, because this shouldn't really even be a question as it's so obvious :D
Yep, that’s close to my thinking. If you’re developing using OOP then you’re probably using DI extensively. Having all those “services” behind interface abstractions is the most sane approach to me in that situation. 
Bingo. Biggest problem unless your a monster and using empty constructors with mutable properties D-:
Do you have any cross-platform .NET Core development experience involving Pivotal?
Agreed, go with web api, and for frontend you can use whatever you want: Ionic, NativeScript, PWA (angular/vue) ...
At the risk of being a pendant, using inheritance to share common setups could be considered a risky design if the boilerplate crosses usage domains. This does not invalidate your design point so much as it should be warned. To elabourate, if using your helper base class for HTTP within different domains of the application, you may get into a scenario where you need different base classes and you end up forcing others to implement it. This is a classic case of inheritance vs composition woes. The biggest issue falls to when the new class needs to be tested and the base class is specific built to aim at a single service. Having an interface passed in that abstracts the http vs Rabbit vs Akka communications means your composed classes can change their behaviours for free assuming the contracts are held. Further, you can mock the shit out of an interface in tests. I'm sorry, this got longer than I wanted, I've just seen inheritance patterns like you suggest get unweildly and untestable. 
&gt; At the risk of being a pendant, using inheritance to share common setups could be considered a risky design if the boilerplate crosses usage domains. This does not invalidate your design point so much as it should be warned. Agreed, which is why people should be very careful about doing so. &gt; The biggest issue falls to when the new class needs to be tested and the base class is specific built to aim at a single service. If that's what you got out of it I apologize. The `BaseHttpThingDoer` in this example would have nothing but methods for performing actions like GET or POST, and not to any specific resource/endpoint. In this regard a `BaseAkkaThingDoer` is actually pretty boring in practice; It lets you define your behavior for things like ASK timeouts more uniformly (Because `IDoThingsWithDomainObject` in a good design shouldn't throw an HTTP related exception versus an Akka `AskTimeoutException`. It's another leak in the abstraction of the interface.) but otherwise there's not as much setup involved for a Tell/Ask versus an HTTP call.
Containers just allow you to achieve higher density per vm. One can argue that if your usebase is small, a single container is more cost effective then a vm of you are starting out. Think time share vs owning a cottage. With app service each app instance = 1vm,and that's what you will be paying for
I have never understood why you would want assertions that do not go into your release build. it's up to the front end to decide how to present the issue to the user, but for logging and diagnostic issues, I want that check in there. what am I missing?
Devil advocate: The OP specifically states "...project structures that mirror every concrete with an interface..". To me this implies even the private concretes of said library contain an Interface counterpart. I agree with your example in the case of the lib's public API, but this can be unnecessary overhead for private types. This is a case of invoking YAGNI.
https://en.wikipedia.org/wiki/Dependency_inversion_principle &gt; In many cases, thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema. 
I feel it is a bit of a personal preference. And I see reasons why you might want to have assertions enabled in release builds. The only thing, which you MUST do in such a case is to remove the DefaultTraceListener and implement your own TraceListener.Fail method. This Fail method should fail more gracefully than Debugger.Break (which is what DefaultTraceListener do) so the frontend could have anything to say to the user (Debugger.Break without a debugger attached will crash the app without any notice). 
for us, it was because we needed something self-hosted for a specialized web app. also, the fact that it is open source is amazing.
Do not use assertion in public methods. They should only be uselful for programmers to catch some development issue. For issues that may rise in release in public methods, use exceptions.
sadface.jpg
[sadface.jpg](http://i.imgur.com/Xo9GNok.jpg) --- ^(*Feedback welcome at /r/image_linker_bot* | )[^(Disable)](https://www.reddit.com/message/compose/?to=image_linker_bot&amp;subject=Ignore%20request&amp;message=ignore%20me)^( with "ignore me" via reply or PM) 
So while writing this I ended up looking up the particular bug you were describing and found precisely what you ran into. Pretty thorough description [here](http://www.reanmachine.com/net-4-5-wpf-selector-bug-with-objects-whos-hashcode-are-not-fixed/) (if I understand correctly). Wow - that must have been a bear to have to figure out! Not sure if it ever got fixed, but now I can recognize the potential utility in the side-by-side Core versions. I've been fortunate to have had no issues with framework compatibility modes, so I'd not considered how frustrating it would be if that didn't work. Thanks for sharing! ----------------------- I mostly work on back-end components, so my knowledge of WPF is admittedly thin, but I would not expect to see a behavior difference unless you'd re-compiled the application for 4.5(+). Microsoft even backs up my thinking here: https://docs.microsoft.com/en-us/dotnet/framework/migration-guide/application-compatibility &gt; "Runtime issues are those that arise when a new runtime is placed on a machine and the same binaries are run, but different behavior is seen. If a binary was compiled for .NET Framework 4.0 it will run in .NET Framework 4.0 compatibility mode on 4.5 or later versions. Many of the changes that affect 4.5 will not affect a binary compiled for 4.0."
I totally agree with you. However, I would add one qualification: In some projects abstracting everything by default is the most sensible thing to do. There is a trade off. Like I said in another post, abstracting a group of POCO objects is pretty nuts, but if your project is strictly implementing logic that carries a high testing burden then **abstract all the things** is immensely helpful.
Mocks do count. The OP gives a perfect example of this. OP mentions the *virtualize all things* approach to testing. This destroys communication of intent of an object. It's more reasonable to create an abstraction and then mock against the abstraction, rather than mocking the concrete directly via `virtual` usage. 
&gt; Mark Seemann has a good post about testing internals (essentially: don't) From my own experience I must totally disagree with this sentiment. On one hand, I agree that usually you don't need to go crazy and unit test the hell out of internals. However if an internal process/object is complex enough, testing only the public API only serves to complicate your tests. Like everything else, there's tradeoff that has to be made in making this decision in each individual project. A blanket rule like 'dont test internals' will only lead you to trouble. 
As with everything, common sense should prevail. Our rule on our current project is to do what makes sense and keeps things simple. A particularly complex class can be 'unit tested' if need be. That said, its inability to be tested simply through a public API may highlight the need for it to be broken out into its own assembly, thereby giving it its own public API through which it can be tested. It also means it's a nice shippable component should other parts of the business wish to use the business logic without need to add or update it. Definitely, I agree about tradeoffs: all software development, architecture etc. is about tradeoffs, for sure. 
Is there any sort of configurable proxy to create a single public API surface when there are several APIs behind the scenes?
Now if we could just get the remainder of Ben.Demistifier merged into Core...
Could you be a bit more concrete with code? Linq pops to my mind, but not sure without an example.
you could list1.AddRange(list2).Distinct() but it really depends on what it means for an item in list1 to be equivalent to an item in list2. 
Stackoverflow.com might be a better forum for these kind of questions. 
So at work we are developing several products - Company app - Company portal - Possibly a customer portal And some reporting stuff. I've got all the class libraries created in .NET Standard, and apps currently in .NET Core. This way they can easily be dockerized, and the libraries can be run on any runtime supporting standard 2. I've got GitLab setup as our VCS, CI, and Docker registry all in one using gitlab linux runners to both build the source and the container, then register it with it's registry. So far I'm loving the flexibility with .NET Core and standard. Plus if I ever need to write something windows specific, the bulk of our functionality is in standard libs so we can use them in that just as well.
var notInBoth = list1.Where (x =&gt; !list2.Contains(x)) Linq is you're friend
Without seeing much code, a quick and easy way with LINQ might be something like this: int serviceId = list1.Service1.ServiceId; Service found = list2.FirstOrDefault(s =&gt; s.ServiceId == serviceId); if (found != null) // skip this item; If you're iterating over one of the collections and have an object reference to a specific item, you could also use the List object's Contains() method with with an IEqualityComparer, something like this: public class ServiceComparer : IEqualityComparer&lt;Service&gt; { public bool Equals(Service s1, Service s2) { if (ReferenceEquals(s1, s2) return true; if (ReferenceEquals(s1, null) || ReferenceEquals(s2, null)) return false; return s1.ServiceId == s2.ServiceId; } public int GetHashCode(Service service) { if (ReferenceEquals(service, null)) return 0; return service.ServiceId * service.ServiceName.GetHashCode(); } } ServiceId and ServiceName are properties of the service object, if you've got different properties you're using to identify one service from another, use those instead. Code to do the comparison would just be list2.Contains(list1.Service1, new ServiceComparer());
Whatever happened to the feature pre-2.0 where changes were compiled in-memory? They removed it in 2.0 for some reason. You could make changes and see them on your page while debugging super fast without compiling entire site.
That was a feature of the pre 1.0 "DNX" stack, where the compilation story was shared between design-time and run-time. That said, the work in 2.1 to improve the project build time ("inner loop") will get as much closer to that experience, and we're continuing to invest there.
Have you tried Awesomium?
Any idea why it was taken away? There was never any announcement about it, and it was one of my favorite things in asp net core
Kudos on the full disclosure. So if I'm reading this correctly, we can continue developing locally on Windows, upload our .dlls (app and unit tests) to Pivotal and Pivotal takes care of the rest - CI/CD pipeline is included in the platform? Do you have any links beyond the Pivotal frontpage, preferably relating to .NET Core deployments?
Fast builds, thank fuck. 70s to 6.8s. That is quite literally life changing lol.
So if I'm reading you guys correctly, App Service is not a PaaS solution? I was under the impression it was something along the lines of Heroku. Does Azure have any Heroku-like offering?
&gt; [CLI] Global tools; replaces .NET CLI Tools (DotNetCliToolReference). I'm actually kind of excited about this.
Nothing platform provides, though apigee is an addon you can install if you are running your own pcf. I would recommend you look at spring cloud gateway. It's Java, but it's easy to use, infrastructure independent and free. You can map it to your .net microservices 
It is a PaaS as it abstracts out management infrastructure and lets you focus on thing that matters to your business - app and data, but it's not the most efficient PaaS as it does't make best use of raw infrastructure resources. Newer container based PaaS offerings allow you to put multiple apps per VM via container usage, achieving higher app density at same cost while offering same level of isolation. 
CI/CD is not part of the platform - you're still responsible for compiling your app and invoking "push" command to send it to pcf. But it makes running apps built in most languages as easy as going into your compiled app folder and running "cf push myappname" command from terminal. the platform will detect the type of app it is and hook everything up automatically. This detection and runtime hookup is done by packages called buildpacks (huroku uses same concept) https://docs.run.pivotal.io/buildpacks/dotnet-core/index.html This quickstart guide is tailored to ruby, but it work just the same for .net core http://docs.run.pivotal.io/buildpacks/ruby/sample-ror.html Instead of cloning their sample app, use your own. Run a "dotnet publish" command on your codebase. Then at the step where you deploy your app, make sure your terminal current directory is inside that publish folder where all the compiled stuff is and do the "cf push my-funky-dotnetapp". In about 30 seconds it should be up and running. run.pivotal.io is our hosted offering, and you can sign up for free trial credits so you can play with this for a good month or so before you run out of credits. It's pretty cheap after that. Coming back to CI/CD There are templates available for all major system for integration, including VSTS &amp; Jenkins. But because you're not concerned with building containers, packaging your app for paas is probably identical to what you do now. Good luck
You can actually change the default browser to a newer version. I read that somewhere not so long ago
Will it finally tell me the name of the variable that triggered the null reference exception?
LINQ has an Intersect method: https://msdn.microsoft.com/en-us/library/system.linq.enumerable.intersect%28v=vs.110%29.aspx?f=255&amp;MSPPError=-2147217396 If you do `list2.Contains(...)` on each item in list1 that others suggested, then you will end up with an O(N^2) solution.
That sounds pretty good. I run kestrel in production but have a webserver infront of it - that does all of my https stuff (redirection, decryption, etc). 
Yes the basic rules for package management are same
I've been using SonarQube together with visual studio online. Although I haven't managed to make it work together with .net Core. Does anyone know if there is a way to since Sonarqube seems to rely on MSBuild to analyze?
The application is exposed via Kestrel, a Web server. You will typically have IIS or nginx in front of that for public traffic.
Haven't tried personally but looks like its supported - https://stackoverflow.com/questions/45866552/sonarqube-build-failure-for-net-core-2-0 
I don't think you can build them both at the same time. I would recommend using something like VSTS(or any other) build system. With VSTS you would create a step for compiling each piece. Then build a release flow that pushes them out to their respective destinations(your web server and github). This will allow you to produce a build and optionally deploy it at the click of a button. No manual steps to remember. Then eventually you might configure the release to go to different stages like test then production. The final advantage is arguably the most valuable. The build is completely independent of your dev machine. It might take a few hours to setup the process in VSTS, but in the end you we'll save tons of time.
Okay, I'm gonna try that. Thanks!
This just can't come soon enough. Really excited about SignalR and the added EF Core functionality.
Should I look into Azure Container Service for a containerized environment? Which vendor would you recommend?
Except, unlike maven this isn’t done automatically when built. You’ll need to still do a night pack and then copy the npkg file to your local .nuget cache (or add a new feed with a folder). Maven does this seamlessly when night/visual studio takes manual steps. 
What is a "night pack" ?
Sorry, auto correct. 
You are talking about creating and uploading a new package version? In general, if I build on my local machine, I don't do that. You can do a `[nuget push](https://docs.microsoft.com/en-us/nuget/create-packages/publish-a-package)` from the command line, and you could make that part of a build script, but it's more commoin to do that on a CI server. [TeamCity has this built in](https://confluence.jetbrains.com/display/TCD10/NuGet#NuGet-UsingTeamCityasNuGetServer). You might want this to happen to every passing CI build to an internal feed (you called it a private repo, I think), or you might want it only when you decide to press the button on it - more common for the `nuget.org` public feed.
Not unless you want to learn how to work with container as unit of work. Azure container services are built on kubernetes, which have a learning curve. It's not as straightforward to get started - I would actually try app service before venturing into acs as it will significantly impact your workflow
Is this really any different from Jon Skeets edu-linq series?
I was really hoping they would add native ldap support for Linux. Also lazy loading was a huge problem in the beginning for me, but now I am good without it. I haven't used SignalR before but has anyone missed it not being available? If so, what is a use case for it? Trying to see if it is something that would add some extra functionality to my development. 
&gt; I haven't used SignalR before but has anyone missed it not being available? If so, what is a use case for it? Trying to see if it is something that would add some extra functionality to my development. Its useful. Its a messaging abstraction over top of WebSockets. Combined with a javascript front end, like Vue.js it means you can do push notifications to the client. For example, our backend service notifies our front end when order status changes are processed and the front end pull new data and sends it to the client using SignalR, allowing push notification of realtime status changes without requiring expensive polling of all orders for each client. If you aren't interested in realtime backend changes in the client though...then it probably isn't useful for your use cases.
Thank you for the interesting information. I could see this being useful for one of our main applications. We receive a lot of data from clients that is offloaded to a separate service. Depending on the amount of data it takes anywhere from a second to several minutes. Users have to go back to an import status view to see the progress and I can see SignalR make a better user experience. Is it safe to assume that users can go to different views and be able to receive push notifications? I am thinking of how to keep track of the intended recipient. 
It depends on how you implement it, but you could easily add SignalR client code to each page (perhaps through a shared layout) and show notifications to the client when a back end process ends. If you have more than several thousand users active at a time you will need to carefully benchmark the impact this will have on your server's RAM and connection limits, though in our case we found it fairly manageable (we only have a couple thousand users on our eCommerce B2B app).
Yes, you can tweak the register to emulate IE11 inside the WebControl. Sadly it is not that good either. I had to change my plans and go with UWP...
Asking the real questions
Very good point about RAM. I didn't think about it. I appreciate your help. 
I'm familiar with CircleCI. I think the main question at this point is, is it possible to run the WPF build in a Linux container, and if so, what do I need to install on the build container?
interesting possibilities for creating little tools that can be installed/versioned easily. should help a lot.
It's really neat, because I'm a big fan of languages that have useful CLI and package managers. (Python/PIP, JavaScript/NPM, etc) :)
The same. Using alpha2 version in production.
 let sum ns = List.reduce (+) ns let avg ns = sum ns / List.length ns &gt; […] No unexpected outcomes can occur. Oh, word. let result = avg List.empty&lt;int&gt; I guess that's unexpected, though? 
This is really nice, because it gets the development environment that much closer to production. We run HTTP kestrel behind a haproxy load balancer that does the HTTPS. Having the default setup more closely account for this is a major plus.
Are you guys considering using C# interpreter to improve the speed of dotnet watch?
I guess? But I don't really expect new tutorials on it...
thanks for raising the issues you have. you can send in a PR
Its just the namespace that is lowercase (to show the relationship to the NodeJs package that I ported) the class names and method names follow the MS conventions
Not a bad article. Not sure if you're the author or not but there's one thing I wanted to mention. The code sections are very hard to read. There is too little contrast between the foreground and background, it's like grey on grey. Updating the color scheme to something more readable might be a good idea. 
WPF is not supported on Mono.
Okay, I was probably confused from the last sentence. Are you talking about establishing a CI build? In that case I would recommend you https://github.com/nuke-build/nuke (disclaimer, I’m the author)... but there are other alternatives too
As a note to mods. I'd love to see something in the sidebar that states something along the lines of " I'm a beginner" and then just lists out the various supported fields like ASP.NET, UWP, and Xamarin. And just links to hoe to get started. 
Well, one way you and the business can decide together is what you need to support. Personally if I am working on a product for our company; an intranet app I go with UWP, and create a native app for windows. It saves a lot of memory usage, additional language support, I.e. No CSS, JS, and HTML, you'll just need to know XAML and C#. If its something that for whatever reason is client facing and needs to be supported via the browser because it cant or shouldn't be installed on someone's home machine then a ASP.NET app is the way to go. 
I know this is a serious question, but when I saw it I had the overwhelming urge to say “I guess I would start with an abstract base class” K, seriously: I would recommend picking up one of the “learning C#” books. Since you are already familiar with programming, those getting started books do a great job of going over the basics of the language, and you’ll gain more from it than someone coming at it fresh. All of the core CS concepts are pretty much the same and your mostly learning the intricacies of the new language. From there then you can gain more from a class on a framework that rides on top of that vs trying to jump right into the framework without a familiarity with the language.
In fairness I think a pile more posts about Vue and .Net Core would be great, the spa template app isn't the best. 
Hey. I didn't realize this was such common topic. I've tried to google Vue using .NET core but all templates and articles that I found were focusing either on TypeScript or pure JS. I could not find anything on JS [single file components](https://vuejs.org/v2/guide/single-file-components.html) (.vue files with vue-loader) and .NET core. I've spend some time setting this up so I thought I will share.
I appreciate articles like this there aren't **many** Vue and .net core article's
"Until now, EF Core could only map properties of types natively supported by the underlying database provider. Values were copied back an forth between columns and properties without any transformation. Starting with EF Core 2.1, value conversions can be applied to transform the values obtained from columns before they are applied to properties, and vice versa." Most people will focus on lazy loading, but from my point of view, this "value conversion" feature is much more important: even in a repository layer that does not leaks entities, it's not very safe to have raw database content without types giving proper guarantees. Also, microtyping database ids is an awesome thing to have. I loosed all interest in EF Core 1.0 after I noticed this is not supported and only now I'm willing to check it again.
&gt; They are similar to the fingerprint and just like two people cannot have same fingerprint, two different geographic locations cannot ever share common coordinates. Last I checked, nobody had scientifically validated the claim that fingerprints are truly unique. And it's true that two different geographic locations can't have the same coordinates, but interestingly, the converse is not true: a single geographic location can have multiple different coordinates. - The distance is zero between any two coordinates with latitude = 90º, and same with latitude = -90º. So there are as many ways to represent those two locations (the north and south poles) as there are ways to represent values between the range [-180º, 180º]. - The distance is also 0 between any location with longitude = 180º and the location with the same latitude but longitude = 180º. If you want to validate coordinates that will eventually be entered into a typical web mapping platform that uses [EPSG:3857](https://epsg.io/3857) (Google, Bing, and OpenStreetMap, for example), then you'll want to reject coordinate values with latitude not between +/- 85.05112878º (that lets them make the map a perfect square, so they can do some [really cool optimizations](https://msdn.microsoft.com/en-us/library/bb259689.aspx) to store and index the map tiles).
You need to start at the beginning. Why is it slow?
So wait you want to poll and API endpoint and stream the response from it to the clients?
&gt; - V3 - There are only two allocations now - one is our buffer from V1 and the other is final resulting string. Since we know that length is extremely likely going to be somewhat short, I've decided to put the buffer on stack using `stackalloc` to avoid the buffer allocation. In near future you'll be able to use `Span&lt;char&gt;` to avoid some of the unsafeness. You can also allocate the final string and write directly into it, to eliminate the copy at the end and without having to feel nervous about `StackOverflowException` caused by the user passing in a huge length value. using System; static class Program { static unsafe void Main() { string finalString = new string('\0', 10); fixed (char* p = finalString) { for (int i = 0; i &lt; 10; i++) { p[i] = unchecked((char)(i + '0')); } } Console.WriteLine(finalString); } } Really, though, I'm just confused about why we're discussing this when `Guid.NewGuid()` is not worse.
Interesting, didn't know one could do that - it certainly makes more sense here. This is really just an exercise, but there are legitimate reasons why generate own ids and not use `Guid` (or part of it). One might need custom character set to avoid similar character or to include more ids while keeping length short - think url shorteners instead of real ID, it's really more like "short tag".
The problem isn’t MSMQ. It’s plenty fast enough, in and of itself. That technology has been around since NT 4. Suggest taking a look at your architecture. Something in your environment is having a negative effect. Why is it being described as slow? What are the performance expectations? Was the system designed with throughout performance in mind? Is there a database in the mix? Are there indexes on the application tables? Are stats being updated regularly? 
What behavior would you expect? You can see by the definition that average must divide by the length of the list. You are telling it to divide by zero. That is...what IS the average of an empty list? Answer: an error. Give `List.average List.empty&lt;float&gt;` a shot.
Is there an up to date feature comparison with EF6?
&gt; **Query types:** An EF Core model can now include query types. Unlike entity types, query types do not have keys defined on them and cannot be inserted, deleted or updated (i.e. they are read-only), but they can be returned directly by queries. Some of the usage scenarios for query types are: &gt; - Mapping to views without primary keys &gt; - Mapping to tables without primary keys &gt; - Mapping to queries defined in the model &gt; - Serving as the return type for FromSql() queries About damn time.
What is processing the messages off the queue?
He didn't make a sub, though. It's a post on his profile. I don't know what profile posts are even for so I'm not sure if he did it right or wrong 🤷‍♂️ 
Some in house way of grabbing the message and processing it with some classes that gets some job done. That can be uploading a picrture to amazon, adding tracking to some service, sending email. Different “small” tasks.
I don't disagree. I don't really get wtf the new profiles are supposed to be used for. I guess so you can follow Reddit celebs? Idk I've never used it and probably never will. 
Are you using SignalIR?
It's more likely that the bottleneck is in your processor. Have you tried running multiple instances (if your data allows for parallel processing) of it to see if that speeds up your overall throughput? I'd also start instrumenting it to see if you have any operations that could be optimized. Another thing you could try is to create queues for each of those "small tasks". That way, you could have a processor pull messages off your existing queue, and route them to the new task queues where you could have dedicated task processors doing their thing. Of course, if 99% of your messages end up being routed to one type of task, this won't help much, but if you have a wide distribution of task types it would help a lot.
No. You're missing the point. Averaging an empty list is nonsensical. Just like dividing by zero is nonsensical. You should get an error when you give the computer nonsense. That is the expected result. Otherwise, what is the expected result? It's not the job of average to handle exceptions. This is why the F# List implementation `List.average List.empty&lt;float&gt;` throws an error. Incidentally, it throws the same error as the implementation above. ``` System.ArgumentException: The input list was empty. ``` ``` System.ArgumentException: The input sequence was empty. ``` Of course, you can wrap the computation in a context of possible failure. ``` type safeAverage = List&lt;int&gt; -&gt; Option&lt;int&gt; ``` But, that isn't the definition of the average of a list.
I think that you have to look at the overall architecture as was mentioned earlier. Unless you have some sort of partitioned processing, you might not see much of an improvement. What is being done with the data. You mentioned different "small" tasks. Does this mean that you have a single queue that you are dumping all work in and a process that is looking at what type of message is being pulled from the queue? If so, you need to split up your work, implement multiple queues, one for each function and split up your executable into specific processes. Instead of having 1 queue that you dump 10000 messages on, you might end up with 5 queues that get 2000 messages each.
I'd use the new SignalR client for this. But on the server side, you can return an IObservable from one of your hub methods that your clients can subscribe to. So you'd use a thread to poll the 3rd party API, ping a BehaviorSubject with the result, then return that subject from a hub methods as the IObservable. All clients that have called that hub methods can just hold on to the client side observable to listen for updates. I'm on mobile at the moment, so I won't attempts to write a sample here, but hopefully that made some sense.
&gt;I don't really get wtf the new profiles are supposed to be used for. Porn. It's supposed to be used for Porn. GW girls are the only people I've seen on Reddit with their own subs.
That makes a loooot of sense 
If you don't know that MSMQ is slow, it isn't. The code that processes messages is slow. Look there.
Why do you spam this subreddit with your really low quality posts? This is not impressive, nor is it the best way to handle this. Instead of spamming your daily scratch pad, why don't you spend some time refining your posts with some quality work. These posts you write provide very little help. It gives no explanation and are hard to comprehend. They are rarely ever the best approach to a problem. I'm not saying stop completely, I'm just saying improve your posts and cut down the spam. 
No websocket middleware in asp.net core
Every little task. E.g. Update user image, or send email, or upload image to amazon has it’s own queue. So we have like 40 different private queues on one server. Every queue has x threads running the task. Typical is 2 threads per queue, but some with more messages have 6-8 threads.
There is a lot of things in the mix. Have to look at it deeper. Thanks.
Thanks!
Why websockets though. If you want continuous data from client and server, you need a socket connection (think like multiplayer games). You'd just send and receive data over a port you decide via bytes. You'd get to be in charge of the encryption and data used.
There's been some good early work done on a multi-stage JIT, that interprets first, before doing moving on to compilations of increasing optimization (which are slower), but no firm plans for release.
Are the queues and processes running on the same server? Have you done any capacity or performance profiling on the server?
Yes same server. Have only done profiling if we had major performance problems.
I just love how everyone's talking about his profile post and noone is answering the question so far. Well, /u/bharad91, just try it out yourself. Let us know ;)
So you are saying no one should use websockets?
As others have suggested, look at the process(s) pulling work off the queue(s). Some easy things to try and spot would be: 1. Are you using one or multiple queues? 2. Are you using one or multiple consumers? 3. Are you using "fat" or large messages? If you find any of those... 1. Split work out into multiple queues. This would let "fast" queues, run independent of "slow" queues. 2. Run multiple instances of the consumer. Sometimes this isn't possible, but if you can process multiple messages in parallel, this is a great way to scale out. 3. "Fat" messages might include something like document or image manipulation. If you are currently including large binary content in the messages, see if you can save the binary data somewhere (disk) and have the message simply carry the location. 
&gt; even in a repository layer that does not leaks entities, it's not very safe to have raw database content without types giving proper guarantees. what do you mean by this? Unless you're going to e.g. convert a string from the db to a custom type which mimics a string with a given max length, or e.g. a decimal with proper precision/scale characteristics (the .NET decimal doesn't have those), everything has a type already. The conversions are more used for e.g. char(1) where Y/N are converted to bool, or ints to enums, or byte[] to image. Most full ORMs like LLBLGen Pro and NHibernate have these conversions since the beginning (which is over 15 years ago now). EF now gets them which is stunningly late. 
We are using a data project, where you have sql files to define your database/tables. You can also add sql scripts to insert data. There is a load database (one which contains the schema and 'default' data). And we have the real database. When there is an update, we use the data/schema compare in Visual studio. It works fine, because you have a separated project specifically for your data (which you can add to source control). 
You probably have to encabulate the XML parity bits.
Thank you for the reply, I have no idea how to do that. Any tips?
Second this OP, SSDT is definitely the way to go. Means you get a deployable artifact for your database, which can be versioned and managed independent of code. You can output delta/change scripts for manual deployment if you need to, or just run the command line tools using a dacpac (build artifact). Standing a database up from scratch is a matter of seconds/minutes and if you have all your pre/post scripts setup as merge statements you never need to worry about whether or not the base seed data is available or inconsistent. 
No?
AutoMapper is one of those things that sounds good in theory, but doesn't work nearly as well in practice. Mapping code is a lot of boiler plate with a few other pieces that handle the caveats. The question is, what's more important, the boiler plate or the caveats? For me the answer is the caveats. In my experience AutoMapper sacrifices the caveats in an effort to serve the boilerplate. This makes the complicated part of mapping even more complicated that it should be. It also now introduces this ticking time bomb when there's a change in the boilerplate code that is no longer handled appropriately by the mapper. Let's face it, while that mapping code isn't fun to write, it's also some of the most straight forward code to understand. Yes it'd be nice if i didn't have to write it, but the price is more complex code taking its place and a process that has a fair amount of obfuscation. For me, it wasn't worth the headache.
I would recommend looking into RoundhousE. We use it to synchronise schema and data between all our environments.
I can't see the problem because you didn't share the informationPOCo object so I can't see what Save actually does. From what you did link, it seems like it is just a data structure. I assume the problem is with the save function. 
I use a fair bit of Automapper because I quite like the clean and simple approach to mapping objects. Never really thought of anything bad about it until I read the tweet at the top of the article. &gt; If I want my code to throw errors which could be caught at compile time at run time, I'd write my app in Ruby or Javascript. And that did kind of hit home. Reasonably often (not that often to be fair) I come accross bugs in my apps where Automapper has failed because of type mis-matches, null references, things like that. Things that, when I used to do these things manually back in the olden days, would never have happened. Plus I end up trying to use it on objects which I probably shouldn't, casting values, enums, collections, conditional mappings, and end up with these really long and overly complicated single line if/linq definitions. But I really hate having an application mix/match both Automapper and manual mappings.
Here is the link to the PoCo Class. https://www.dropbox.com/s/egbp46w21egg0s2/InfoPoco?dl=0
The best thing about auto mapper are the projections from a entity framework model to custom classes, so that you don’t need to load everything from the database. Also when you use the same naming for your api as the database it is easy to use it with automatically generating maps.
It looks like you wrote a lowercase I instead of an uppercase I. This has happened 561 times on Reddit since the launch of this bot.
The problem is you are using OpenOrCreate, so what is happening is you are writing from the start of the file. The problem you are seeing are when your new save file is slightly smaller than the other thing saved. The fileIO you want is Append (which will create a new one is needed) https://msdn.microsoft.com/en-us/library/system.io.filemode(v=vs.110).aspx
Now it's some lib fault of improper usage and overall quality of code? "And again, if we rename or drop the field we silently break the system. We’ll get feedback only when we run some kind of test during the runtime." Who drops db field while app is running!? How is that even allowed and what mapping lib has to do with it? The whole article describes the team with really bad engineering practices, blaming some tool for bugs. 
Thank you very much for looking into this. I take a look at this link. Very helpful! :)
yeah, I am not sure what you are going for, but it seems like it is just writing your xml output over the existing file without clearing all the content. I think the other option is you would load the file into a xml dom object, update the values, then resave it. But the way you are doing it, by writing the new xml serialized object, you are most likely just overwriting existing data. So it would be best to just delete the file before writing / saving. Another way you can look into this, is look at your file stream before you write, and look at the position (it should be in how many bytes it is into the file). If I am right, the number will be zero, meaning it will begin writing at the start of the file, overwriting things. 
If it's something that can be done automatically then you don't need a mapped proxy object at all. The whole point of mapping is to do explicit transforms. If it can be done automatically, but requires a lot of setup, e.g. Mapper.Initialize(cfg =&gt; { cfg.CreateMap&lt;Source, Destination&gt;() .ForMember(dest =&gt; dest.Total, opt =&gt; opt.ResolveUsing&lt;CustomResolver, decimal&gt;(src =&gt; src.SubTotal)); cfg.CreateMap&lt;OtherSource, OtherDest&gt;() .ForMember(dest =&gt; dest.OtherTotal, opt =&gt; opt.ResolveUsing&lt;CustomResolver, decimal&gt;(src =&gt; src.OtherSubTotal)); }); You've got to ask yourself, would you rather maintain the mapping with the above meta relationships, or would you rather just use plain old C# and hand written mapping classes? 
AutoMapper would be better as a code snippet tool that spit out the boilerplate than a library you link with. 
It's all about tradeoffs. Like any tool, you have to evaluate, for your project and situation, what the possible benefits are against the limitations. I didn't feel like writing a bunch of dumb mapping code, AND the tests, so that's why I made AutoMapper. These days I try to stick to the LINQ projection feature, probably because you're least likely to get yourself in trouble. I avoid reverse mapping, too, because that imposes limitations. I agree with all the author's points. You lose visibility, debugability, etc etc. and it's up to you to decide if that's worth the tradeoff of not having to write and test the projection code. We still use it in our projects, but it's not ubiquitous inside them. It's only in places where the projection is so obvious that you don't need to debug.
my god, what a shitty bot
Yeah. For dead simple 1-1 object mappings, it’s fine. Beyond that (including nested type graphs) and you’re in to borderline uncharted territory and all sorts of dragons that you wouldn’t bump in to if you just wrote it yourself. Oh and bring your aspirins if you use inheritance at all. The problem that it solves is so trivial and the problems it causes so time-consuming that I stay clear of it altogether on principle now. 
I feel like you don't totally understand the article. It's not "dropping a field while the app is running." The author means: if you're using AutoMapper, you might delete a property/drop a field without realizing that it's being used somewhere, because static code analyzers might incorrectly flag it as being unused (because there's no reference to the property in code). With AutoMapper, you also lose static type checking and other compile-time code quality checks. It's opens the door to introducing errors that are only caught once the app is running. Now, granted, I'm not persuaded by the author. I think there *is* some tool blaming going on. However, I can see that AutoMapper trades succinct code for strong type checking. As someone who prefers AutoMapper, it's something I'll consider in the future.
I wonder if there's a market for a Roslyn/scriptcs-powered mapper-generator backed by Automapper, except it generates a mapping class rather than runtime magic.
My expectation would be that any errors will be caught in unit and/or regression testing. I could see the argument if you aren't writing any tests for your code, but if that's the case you've got far bigger issues than Automapper...
Like T4 templates? I find those harder to maintain. Recompiling the templates every model change is brutal. 
Which is what the author mentions when talking about T4 templates. The thing is that it's easier to make a NuGet package that's really easy to just drop into your project. It's a bit harder to build something that actually integrates with VS and the development process, which is what you'd probably need for a tool like that.
Ideally yea, but with mappings it is easy to get burned by edge cases.
That's why I don't use EF. AutoMapper is a decent bandaid for something that EF should offer out of the box. Why can't I just write `dataContext.Customers.From&lt;CustomerSummary&gt;()...`?
I looked at doing something like Roslyn to generate code too. Instead of T4, it'd be actual runtime codegen. That tradeoff would be losing quite a bit of cross-platform support. And since our teams use the LINQ stuff mostly, we wouldn't get much use for that kind of thing. I build stuff for my teams first and foremost since that's what pays the bills :)
Neither. Whenever I see mapping I consider it to be a code smell. Something is wrong with my model, library, or overall design. 
That “reason” doesn’t hold water. There are tons of patterns, approaches, and libraries where I will end up with code that doesn’t look like it’s being used, but is during run time (by a DI container, perhaps). You should not be removing code just because some analysis software says you don’t use it. It’s for finding potential problems, not acting as the judge. 
Unused code is a serious problem for larger projects. I've lost track of the amount of time I've wasted updating code to adhere to new feature requirements, only to find it was impossible to test because the code wasn't actually being used anymore. On one ASP project it was so bad that I had to write my own static code analyzer. Using it, I was literally removing thousands of lines of code every day for weeks. 
What's a better solution? We have a Converter class with about 1000 Convert() methods that go to and from dtos and entities. It's not pretty but it works I guess. 
Does anything support something like this? I'm not aware of anything that introspects DTOs to infer the projections.
How do you show data on a screen?
I think so. I avoid codegen because of the extra step but it looks like the EF Core team is doing with Roslyn to support seamless lazy loading that doesn't require the clunkiness of T4.
You could try [Scripty](https://github.com/daveaglick/Scripty). It's like T4, but running on a C# script rather than the weird T4 language.
What is it about my case then, that makes websockets the wrong solution?
I didn't say it was. What I said was "Why websockets though", because I was wondering if you need HTTP handshakes or not. I was not sure what type of data and purpose you are seeking here. WebSockets are just sockets with some framing to support some HTTP standards. Really, there's very little overhead and you can accomplish nearly the same thing, however depending on what kind of performance you need, perhaps you don't want TCP and you need the shotgun speed of UDP. Really, just saying "Best way to fetch and send continuous (key word here) data to all connected clients" I'd say if speed and lag is an issue, UDP all the way. If you need accuracy and the comfort of no data loss, TCP. I figured, if you're considering WebSockets, maybe perhaps depending on your needs, might not want the little extra HTTP/TCP support of the WebSocket. If you want/need TCP, websocket is going to be just fine.
Can you show me a small example of how you use LINQ projection? I'm curious what that looks like.
Use an ORM that doesn't require both entities and DTOs. I have one setup where the UI passes just a table/view name and a type parameter. Then at runtime I generate the necessary SQL. I hope to add GraphQL support eventually, but that's a hard API pattern to code for.
I have some interfaces that define the types of conversions i need to do (IDtoMerge, IDtoMap, IDtoMergeAsync, IDtoMapAsync). I have a number of files with the implementation of those for a particular DTO/Entity pair. They are all partial classes of the DtoMapper class.
My own ORM can handle filtering, sorting, and paging with projections. But in exchange for great performance, you don't get LINQ or deep object graphs.
An example would be: IEnumerable&lt;ClassView&gt; enumerableSource = new List&lt;ClassView&gt;(); IEnumerable&lt;ClassDto&gt; projectionResult = enumerableSource .Select(x =&gt; new ClassDto() { Id = x.Id, Name = x. Name, Price = x.TotalPrice });
Oh okay, pretty straight forward. Thanks!
Hi, This article containing the code snippet to validate latitude &amp; longitude only. This is well-tested quality code. I don't want to write a whole story on it as per your suggestion. The content I share enough to understand for any fresher or experienced developer. Thanks
Coming in effect core 2.1 with views support
That's what i understood. My point is, you need to know how to use the tool properly, meaning, cover all those mappings with unit tests. 
a short introduction into what is basically a monad and why I need it; would be super helpful.
About bloody time. WTF took them so long?
Yikes. You have to test the whole app to refactor this...
You can have vs do it on save for you
That works if you don’t switch db engines. Also, if you are good at writing views.
If you were updating code before checking if it is used, i don’t think any tool is to blame for that ;))
I second projection. It can be lightening fast if used correctly. 
Yeah. None answered my question. It goes off-topic. Lol.
It was just a projection example. Needless to say you wouldn’t use it like that in a real world app.
Not really. It's effectively just a select statement which doesn't return all the columns. It's no easier or harder to test than any other data access. 
please visit this website and get order :)
The idea behind websockets is that you don't have to repeatedly poll an API. The server can just push data to the client when something happens.
Yes, but the server may need to poll an API to get the data it will push to clients
Hand written.
I found this mostly neat for the `Microsoft.Extensions.CommandlineUtils`! TIL.
Arguing about lazy loading, I suppose
Courtesy of Brad Wilson: https://twitter.com/bradwilson/status/960588797732782080 and Channel 9 who also discusses the road map for 2.1: https://channel9.msdn.com/Shows/On-NET/NET-Core-21-Roadmap-PT1
[example](https://github.com/jbogard/ContosoUniversityCore/blob/master/src/ContosoUniversityCore/Features/Department/Index.cs#L42) So you project directly off an `IQueryable` like `await dbContext.Products.ProjectTo&lt;ProductIndexModel&gt;().ToListAsync();` That automatically converts ProjectTo into a Select LINQ expression to be evaluated by a query provider. Skips the entity model and goes directly from SQL to DTO.
Misleading title in yellow if you ask me. The client side part of the script does nothing to actually get the data into excel other than copy a value and regex replace things. I thought you had come up with a way to not use server side code all together. 
*UPDATE* So I tried Append, and it duplicated all the XML below it so that unfortunately did not work. :( Let me give a little more detail here on what I am trying to accomplish. Basically what this is, is an app for a Tabletop Game (D&amp;D). It is an active, ever-changing character app with a save file that will update every time the user uses it. There will also be a save file for each character created. As for your other suggestion, I am not really sure how to do that. I am new to C# and WPF so pardon my ignorance. I will see if I can google how to clear the XML before I overwrite it. But any advice is appreciated. -Abe
jQuery Datatables does excel and csv exports entirely client site. Pretty sure it just passes data into some excel js library that can be used independently.
"build performance"
Also web large means compiling orchard, their aspnetcore crm for anyone interested.
Oh that's really cool. Thank you!
Please use gist,pastebin or hastebin or something similar in the future. I don't want to download something to read simple lines of code. Especially not from Dropbox where it could be anything.
Will do, sorry. 
yeah mixed up the acronyms. i do CRM stuff sometimes so the term is more familiar to my fingers :D
Is the 2.1 tooling Released yet? I thought it was only going to be out in mid 2018?
fuck me, that's harsh.
Isn’t it funny that we still use terms that are no longer applicable? This is the floppy for save all over again. Will they be printing DVDs with .NET Core 2.1? 😀
It handles switching database engines just fine. In fact, it can generate much better SQL than EF including database specific code for things like * Upserts (atomic if the database supports it) * Update a row and then return the (original or new) record in one database call * Delete a row and return the original row in one database call But yes, you do have to know how to write good views to fully take advantage of it. The design often uses this pattern: view ==&gt; model/dto == &gt; table(s) For example: var team = ds.GetByKey("TeamSeasonView", new {teamId, seasonId}).ToObject&lt;TeamSeason&gt;().Execute(); //change columns ds.Update("Team", team).Execute(); ds.Update("TeamSeasonMap", team).Execute(); As you can see here, it's smart enough to update the appropriate columns in each table from one entity. And the only configuration it needs is the occasional `Column` or `NotMapped` attribute. 
So the append makes sense, as you are appending it. If we are truely replacing the values, then the best idea is to delete the first, then write the new file. https://msdn.microsoft.com/en-us/library/system.io.file.delete(v=vs.110).aspx Basically just call File.Delete(filepath) on the file first. Maybe do a File.Exists(filepath) to make sure you don't get an error on deleting the first time. This is a problem kind of unique to text files. If this was a database, then this wouldn't be a problem. Same if you loaded the file into an xmldocument and then modified the values. But that would require you to know things like xpath. Given this information, my best advise at this point is to: 1) check if file exists 2) if file exists, delete it 3) save character to file that will ensure that the files are always up to date. 
So you're saying that instead of wasting hours trying to test some code we changed that turned out to be unused... we should waste hours trying to test code that is unused before we change it. Ok, I'll grant you that could result in saving some time. But when you look at the big picture, yea its still a lot of wasted time because someone didn't remove the unused code.
Thank you, I will give it a shot. 
It looks like everything is now working! Granted it was intermittent, but it has not happened with 10 or so save files. Thank you everyone for your assistance. Here is what I ended with, I had to change a few things around. public void Save(string filename) { if (File.Exists(filename)) { File.Delete(filename); } using (var stream = new FileStream(filename, FileMode.OpenOrCreate, FileAccess.ReadWrite, FileShare.None)) { stream.SetLength(0); var xml = new XmlSerializer(typeof(InformationPoco)); xml.Serialize(stream, this); stream.Close(); } 
bad bot
Thank you celluj34 for voting on Yasuo\_Spelling\_Bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Exactly. Forgot about datatables. I was just wanting it out there than the OPs title was click bait. 
Err, aren't all web apps browser-based? 
So they're re-doing Silverlight??
I thought Orchard was on the full 4.5 framework? This post is about ASP.Net Core 2.1 RTM.
C# and Razor as webassembly rather than javascript
I've found ObjectMapper project which allows to generate mapping method using Roslyn API. I think it would be more convenient if it could generate pure methods instead of modifying existing object but it's a good starting point https://github.com/nejcskofic/ObjectMapper
Bad example with awful code. No one should even be trying to implement an export this way. There plenty of better solutions that don't involve hacky, unsafe code. 
Quick update. In case anyone is interested I've created branch where you can find template which doesn't use MVC views for index.html and routes. MVC is there only for WebAPI.
Webassembly is nowhere near Silverlight
I think some kind of compile-time code generation is much more suitable for implementing mapping between objects.
Okay so now i need to refactor my dto. The projection statement you have is in my Query handler (in cqrs), model (ddd) or repository (soa). And the output is in my controller (mvc) or svc (wcf). If i had a separate output (controller), dal and mapping classes i could test these (unit test) in isolation. But in this approach the dto is inseparable from dal. Now in half a decent app i re-use my code, and say 2-3 places use this query. I need to write unit tests that ensure my refactor won’t affect them because my projection that goes outside of the app is all the way down in the db access. (This is what i meant by test the whole app)
Time to open source and publish the framework?
Quick update. In case anyone is interested I've created branch where you can find template which doesn't use MVC views for index.html and routes. MVC is there only for WebAPI. https://github.com/MarczakIO/netcore-vuejs/tree/No-MVC-template
What are you running in a windows service? If you have an alternative there then you could run your .NET core app and MSSQL on an Ubuntu Server on digital ocean for less.
Picture it working just like typescript does for angular. Read this: http://blog.stevensanderson.com/2018/02/06/blazor-intro/ Picture Webforms. Minus the forms.
Web assembly is really exciting so I'll be interested to see where this project goes! 
AWS has a free tier you can try out.
They said it'll be before the midpoint of 2018, and they'll be doing monthly preview releases. Source: https://blogs.msdn.microsoft.com/dotnet/2018/02/02/net-core-2-1-roadmap/
You can use it now. https://github.com/docevaad/Chain/ I'm currently working on the MySQL implementation. 
Classes? Yea, those would have been nice. There might have been a handful. This is ASP on a very large platform. We're talking thousands of scripts that include one another, sometimes repeatedly. Add it all up, and a single page could literally require half a million or more lines of code. That code is, of course, freely mixed VBScript, HTML, and JavaScript. 
`ID` is capitalized in your model, but lowercase in your view.
This is a function which takes a collection and returns a list, no more, no less. You test it like any other. Expected inputs give expected outputs. The whole point of a DTO is that the receiving system doesn't know or care how you created it. Same with your MVC model or view model. If changing this means retesting the rest of your app you've done it wrong. 
For cheap and reliable vps or dedicated hosting i go to hetzner.de. Or some small instance on azure/aws/google cloud. They are all usually way below 30€/m
Windows box will cost around 15$ extra per month compared to linux. Why not hosting core app on some cloud linux vm, use hosted paas mssql, lambda/azure function to periodically trigger app URL to simulate Windows service, or just do some processing from that serverless code? 
Very interested in this project. It's refreshing to see as a seasoned .NET developer who spent all day looking at various JS front end frameworks to get a new web app off the ground. If my existing backend and desktop application skills would transfer to the web, I'd be so happy.
I would like to recommend to use the zetpdf.com its a very good and easy to use. 
How are the bundle sizes and render times under current Blazor?
I would like to recommend to try the zetpdf.com
DotNetRocks podcast covered this today, only got about half way thru but so far sounds very interesting
This is by far the most exciting thing I have seen today, and I watches the Space X launch... I would like to understand what Microsoft have invested in this. I sincerely hope they have hired the original developer and are putting some more devs on the project too. The original demo is mind blowing - do watch it if you haven't already. I'll be watching the project with interest and I already hate myself for the pre-alpha super early adopter frustration I am going to feel riding the bleeding edge on this one. Great move Microsoft :-) I'm betting that Blazor is going to change the Web forever.
are you trying to have them do the exact same thing or redirect? if you want them to do the exact same thing, just build them both as real routes, and have common logic between them.
if they ship this as a committed product it's going to be a game changer!
I concur. While me and Javascript go back a ways (heck, it was the first language I ever learned back in the late 90s), I feel like it's not fair how it had a strangle hold on the web. Blazor and Web Assembly as a whole are probably the most exciting things to have sprung up in the past couple of years to have gotten me truley excited about web development again.
hm. i assume the httpcontext object is available in that filter, can you just hard code some stuff in there that examines the route? maybe it could hang some flag off the httpcontext items property to signal the downstream controllers to behave differently? magic like that is terrible code smell. but staleholders designing routes is also code smell, so there's that. this does sound shitty. you have my condolences.
We'll be following this closely at work. I can only echo the sentiments here - I can't wait for the front end "cambrian explosion". This will be even more meaingful when WASM gets support for garbage collection and first-class DOM access.
Ooooooooooo sheeeeet. Anything to get me away from JavaScript is a welcome development. I just enjoy C# so much more, and the thought of being able to use it for web front end excites me in my pants
Resistance is futile. 
&gt; staleholders apropos typo
I can't seem to find any proposed design about it. Just links to PRs.
Because it's not obvious. Operators make since for primitive types, but not complex, multi-value types.
Yea, that sounds right.
This is... interesting. Looks like a legitimate and in-depth analysis. And screw all this LINQ crap! Hard coded SQL all the way!
Very interesting 
As someone who primarily uses webforms still, can you elaborate how this will be like webforms? I'm not really understanding how this works. I'm used to "put control on .aspx page, put click handler in matching code-behind page" and then "let the page postback or put in some ajax slop to mask the fact its doing a postback." From my understanding, webassembly effectively replaces (?) javascript and AJAX, right? ...And then this project, Blazor, effectively lets you write .NET code and translates it during compilation over to WASM, thereby letting you use things like HttpClient, nuget packages, and other .NET Standard shenanigans client-side without the need to actually hand-write WASM (or JS for that matter)?
Yes, it is.
I think it's great idea to let me use web apps while filling the car.
&gt; They said it'll be before the midpoint of 2018 I am well aware of what they said, and also how this tends to play out. I stand by "in mid 2018"
There are two methods in development one as you describe. The other is they compile mono in full to Web assembly and then execute normal assemblies against it.
What is the structure of central DB? Is it possible to apply ordering by some column? If yes, you can get data in batches using ORDER BY\LIMIT or TOP\OFFSET and increase OFFSET after successful inserting data to localDB. Another solution is to use something like cursoring pagination, so instead of OFFSET you save the ID of last received row and then request data with filter "WHERE id &gt; last_received_id". Play with LIMIT\TOP size to find an optimal value to balance batch size\requests count.
Yes, It's possible to order it by a certain column, However there can be 1000 of data. I want to take only the updated ones, for that, i've cross checking the central table with the local table every time to get the updated rows. And there's around 50 tables, when there's a internet drop, it goes from the beginning again for each table. It's messy and it's ugly. It could take up to 5-10 mins sometimes. I want to over-come these stupid issues. Cause someone else implemented this current scenario which is complicated as hell.
There are many other choices available, both in the full ORM field and in the micro-ORM field, which perform better and offer more features, both for dapper and EF (Core). Even if you have to pick one of the two provided in the article, it makes no sense to compare them, as they operate on a different level. Dapper forces you to write hardcoded sql queries, which is a step back from having compile time checked queries in a query language like Linq. Dapper is also an odd choice as for microORMs there are many available which offer compile-time checked query APIs and therefore don't force you to write hardcoded SQL strings. I assume Dapper was picked because a lot of people think it's the fastest (it's not) ORM available. 
... Well, you asked if it's been released yet, so I was just answering your question, that's all
Using web apps while karting with my friends/mortal enemies... now that's operating at peak efficiency!
I've also found that this is far faster as it can send your projection to T-SQL 
&gt; I sincerely hope they have hired the original developer and are putting some more devs on the project too. Steve Sanderson (original creator) already works for Microsoft in the ASP.NET team. He did a talk on this at NDC London and easily got the most applause out of all the talks I watched. Agreed on the rest though, I'm really excited for this and hope Microsoft put some decent resource into it as if they are one of the first to market with a proper WASM front end framework they will get a lot of adoption I reckon.
Nice article. Great for junior devs (and even some senior devs.) Side question: Has the `MoveNext` method always been part of the Iterator implementation or is it a recent addition?
Yeap. `foreach` was added in C# 2.0, which was contemporary with .Net 2.0. Iterators are mediated by the `IEnumerable` and `IEnumerable&lt;T&gt;` interfaces. Both [were added][1] in .Net 2.0. BTW, docs.microsoft.com [lies][2] and says that .Net 4.5 was the first to support `IEnumerable`; I suspect Microsoft cut off everything below 4.5 when they created docs.microsoft.com. [1]: https://msdn.microsoft.com/en-us/library/system.collections.ienumerable(v=vs.110).aspx#Anchor_5 [2]: https://docs.microsoft.com/en-us/dotnet/api/system.collections.ienumerable?view=netframework-4.7.1
So does webassembly even relate to something like AJAX? If you were using Blazor to write code/razor pages compiled to webassembly, would it be able to send/receive requests to a remote server without the need for AJAX or postbacks akin to how javascript and AJAX work? I watched a youtube video of one of the Blazor devs creating a little MVC project with it, but as I'm not super familiar with MVC (again, shitty webforms background) it didn't look like his example project was doing any kind of postbacks.
Wow. huh. Wish I'd known about `MoveNext` back in the day. I can think of a number of cases where I could have used it. (Documentation has been easier and easier to find as years have gone on...)
&gt; foreach doesn’t actually use IEnumerable, generic or otherwise, but instead is duck typed I've always loved this about C#. My day job has me coding in Python 3.5+ and its pretty good (all things considered), but I ran into an issue where I couldn't iterate over something and I said to myself, "C# would iterate over this..." ;)
You should give this a look: https://ritcsec.wordpress.com/2017/05/16/web-assembly-vs-javascript/
It's probably because `foreach` is older than generics, so you originally couldn't pass strongly typed values through a single interface. `foreach` doesn't *require* `IEnumerable`/`IEnumerable&lt;T&gt;`, but it does use them when no implicit implementation is available.
Or this one: https://www.sitepoint.com/future-programming-webassembly-life-after-javascript/
Very short, but may be a good start: https://github.com/MonacsLib/Monacs/blob/master/docs/Glossary.md#dont-fear-the-monad
Its not like it had a lot of methods :P https://msdn.microsoft.com/en-us/library/system.collections.ienumerator_methods(v=vs.71).aspx
Haha, touche.
You do not need to use the `.Include(x=&gt;x.Table)` and `ThenInclude(etc)` on queries when you are using projection.
Sure, but practically speaking, EF and Dapper are the two that you are most likely to encounter. They are popular, and battle proven, and that's why they get so much community attention. Not really that surprising at all. Popularity and the likelihood to find (a) experienced developers, (b) plenty of documentation, and (c) a lively community expanding and working with it are major factors in any technology choice.
You could try the following: using (var f = System.IO.File.OpenRead(model.FileToUpload.FileName)) { blockBlob.UploadFromStream(f); } Everything above that is allright for creating the Blob and using it. The issue is for sure I'm the statement I modified for you.
Well, since I have been on both side of this issue as both a student and a teacher, I would prefer the code first + seed approach. Then the bacpac. And only in last resort I would generate a sql script for the database schema **and the data**. If there is a lot of project to review, it's highly possible your lecturer is looking for an excuse. tldr : Change your lecturer. 
You don't get the choice to change :P. Yeah, well, from what I understand of it then -- he should not have added that in his brief as an alternative. That's pretty misleading. So on those grounds, I would say he should remark it. I don't think it's a good enough excuse.
It's not overkill. It's normal. I always make a Web API to go along with the client, and implement CORS, e.g. app.mydomain.com is the spa angular/react/vue/whatever client, api.mydomain.com is the REST api for it. That said, you should really try to leverage the client as much as possible. Imagine your map project takes off and gets 10 million hits a day. If you are proxying all of your google maps calls through your API just to work around CORS issues, you will have to scale your backend API to 10 million hits a day as well. If it were all implemented in the client using google REST apis directly, there would be no backend overhead for you. It's a trade-off.
The point is that your instructor should just be able to run your app and it should work. He can’t setup an environment for every student who submits. Here’s a couple options that could have solved the problem for you (in increasing complexity) - Use a provider like SQLite, where you don’t need a database engine set up. (If you were using EF this is pretty much free) - Don’t use a DB, just keep data in memory (it would only work for one run, and wipe after each run) but would work for his playing around purpose. - use a cloud provider for the database, like Azure and point your application at the database when you distribute it. - package your app into a container, and use something like docker compose to distribute the app along with it’s a dependencies (another container for the db)
Because Chain doesn't support Oracle. ;)
Are you sure he wasn't looking for an [MDF file](https://stackoverflow.com/questions/1175882/what-is-an-mdf-file)? That's what I would think of when talking about a "SQL Server database file". Never heard of a BACPAC but it looks like something cloud-related (or does your university do everything in Azure?)
No clue. I was shooting in the dark with what he wanted. I do not believe we have ever used Azure.
BACPAC is the new DB copying method for SQL Server 2016 and most especially SQL Server running on Azure. AFAIK, you can’t deal with a traditional backup file on Azure, you need to use BACPAC.
Love the bearer token service. 
So what happens of you have more complex business rules that need DB interaction? Are those still triggered by Model.IsValid? or will the user potentially have to submit more than once to see all errors?
`ModelState.IsValid` uses validation attributes as well as custom validation methods you set up. Check out the docs on model validation: https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation.
SQLite would be the way I’d go...database is a single file and the engine is a Nuget download.
He was expecting an mdf with a connection string to it. Obviously not a production solution, but perfectly fine for an assignment.
You can use SQLite. It’s a simple, file based database and also works with Entity Framework. When your API starts up, you can call EnsureCreated. This will create the file for the database and setup the tables. If you need to seed data after creation then you can do so after the method runs; it returns a bool that is true when it has to create the database. The included libraries are all that is required to read and write to the database. 
Thanks for the comment @Otis_Inf. Good points made about other potential ORMs and the lack of static typing in Dapper. As @i8beef pointed out - popularity is important when you are just 1 person in a team building products that need to be maintained for a long time
Shouldn’t you rethrow when it is not a DbUpdateException rather than calling next? You effectively swallowed all exceptions 🤔
Yup, thanks for spotting that! Just updated :)
Not sure if you are being sarcastic but I'll answer anyway. Try see the controllers as coordinators of the components of the underlying layer. Responsibilities of the controllers are * Validating the input * Coordinating the different services of the business layer * Formatting the output * Returning the right action result Thinner controllers are easier to test because you don't need to validate the output of business processing through a View result. Testing a controller action is mostly about making sure that incorrect input is detected, services are invoked and action results are of the proper type given a certain set of inputs.
As a enterprise developer, not a teacher, I agree with you he's giving excuses. Code first, database first, that's not the point. I do both depending on the job demand and database engine. I can see a teacher setting requirements and expecting you to follow them, this is how it is often times in the enterprise world. But this teacher isn't willing to compromise in the spirit of education because he doesn't want to install SSMS? Does he not know VS can work as SSMS? Excuses excuses....
Why even catch all exceptions? Just catch DbUpdateExceptions: try { await _next(httpContext); } catch (DbUpdateException ex) { await HandleExceptionAsync(httpContext, ex); }
Can't believe I didn't notice that. Feel like an idiot. Updated again :).
With a library that gives us control templates for data grids etc this could be super productive for UI heavy software. Great potential. I hope they keep it up
Did you find a replacement for the marker bar? I miss that so much :)
Most ORMs are battleproven, and often more battleproven than new frameworks like EF Core (e.g. LLBLGen Pro is on the market since 2003, NHibernate since 2004, Linq to sql since 2008). Any ORM that's used a lot (and there aren't many left, the ones that do are often used by thousands of users) comes with huge documentation sets (even NHibernate has its act together nowadays), examples, lots of articles how to use it etc. Experienced devs, sure, but using an ORM isn't the most complex thing in the world, at least it shouldn't be. Most support Linq in one way or the other (where the microORM Linq to DB is also by far the fastest ORM on .NET, see: https://weblogs.asp.net/fbouma/net-micro-orm-fetch-benchmark-results-and-the-fine-details) and as long as you understand what an ORM does and what a DB is (you have to, no matter what ORM you're using, the DB is a key part of your app, the ORM just allows you to work with it), learning a particular ORM isn't a problem, even if you haven't used that particular ORM at all. The main thing is that simply opting for 'what's popular' limits you greatly in choices and can hurt your app: e.g. using EF Core isn't a great choice now as it still lacks basic features like DB side group by. 
I just saw this: Deploy to an [Azure Web App for Containers](https://docs.microsoft.com/en-us/vsts/build-release/apps/cd/deploy-docker-webapp). If I'm reading this correctly, it's possible to deploy Docker Containers to Web App? Web App is an App Service app subtype of some sort, right?
Updated!
I've been looking for a lightweight, clean data access library for a while. Your library looks promising. Do you have plans to provide tests and/or samples?
I've never been a fan of IsValid. I always felt that was the responsibility of the business layer not controller. if you reuse the base logic in a non-web app, then you have to redo all those rules. also if you have a lot of Remote checks, you would be firing ajax calls off like crazy vs. just having 1 validate everything multi-threaded.
To me the job of IsValid is only to ensure that the shape of the data is valid (your model has a start/end date range, is that range valid or did the user swap the dates?). Any business rule validation happens further down the pipeline.
I'd suggest getting a vendor control like https://www.telerik.com/aspnet-core-ui/grid The amount of time you will spend on recreating and maintaining the same feature set will justify the cost. There are plenty of other controls on the market if you search "asp.net core grid control".
I’ll have a look! What do you think about the dynamic search form? Would it be possible with teleriks controls or is it not possible to remove/add?
What I meant is a search form where you can add/remove search fields. The data contains a lot of properties but the user is only interested in searching in a few of these properties but it needs to be completely dynamic thus no page reloads. 
There are a couple of demos. Most work off the columns. https://demos.telerik.com/aspnet-core/grid/filter-row https://demos.telerik.com/aspnet-core/grid/filter-menu-customization
&gt; To me the job of IsValid is only to ensure that the shape of the data is valid. Any business rule validation happens [in the model] Data validation happens *through the model*, where the business rules are supposed to be anyhow. Either through direct entries that decorate the model elements or via fluent validation that is hooked into the model as a whole.
lol
I ran into the same issue, so I tried this extension method someone had posted to Stack Overflow: public static IQueryable&lt;TSource&gt; WhereIf&lt;TSource&gt;(this IQueryable&lt;TSource&gt; source, bool condition, Expression&lt;Func&lt;TSource, bool&gt;&gt; predicate) { if (condition) return source.Where(predicate); else return source; } Unfortunately, it resulted in very slow queries. I ended up going back to the ugly way.
So if you support a set of filters on your site for drilling down into data, some of which require different sets of joins, do you manually code a SQL statement for every possible combination of filters for each query? Meaning you code thousands of manual SQL queries? Or do you resort to some crazy string concatenation scheme?
Interesting but my issue is not how to perform the query on the backend but rather how to display a dynamic search form. 
Did you try speaking to other students to see how they solved the database issue?
http://lmgtfy.com/?q=mdf+file+.net+core
Poe’s Law at it’s finest. Of course I’ll be using LINQ, no questions asked. Or rather, I won’t be working on a soul-crushing project with data-drilling grids or whatnot.
Smartass. 
I think most seeded. 
So other student were able to complete that part? It sounds like the lecturer is right. 
Don't thinking you've read the post entirely. 
Ah, gotcha. I thought you meant the mess of building up a query from many possible combinations of search parameters.
That part is all done ;-)
If you take extreme stances like screw LINQ and hard coded sql all the way you can expect a bit of Poe's Law heh. TBH Linq is nice but it's far from optimal. I'd love a type safe query builder DSL that was actually a direct representation of SQL itself (and not some monadic approximation). Linq translation is very heavy, and extremely hard to get right (as evidenced by the many ORMs that throw exceptions on lots of common query cases).
&gt; I would rather have a class dedicated to validating everything than scatter it through the layers Hence Fluid Validation rather than element decorators. FV allows you to define all validation in a separate class, keeping everything nice and tidy. I also despise decorating fields directly with validation attributes.
I'm not really a fan of fluent validation because it is very verbose and requires an expression even though you often need checks across multiple properties or database checks that you want to run multi threaded and async.
Controllers are very difficult to unit test because they are the glue layer. You either have to mock out a lot of dependencies, setup an in-memory or temporary database on disk, create fakes of classes, or all of the above. If you have complicated logic or data transformations in the controller, move them to methods that can be tested in unit tests without dependencies.
Personally, I hate all of the options listed in the blog post but usually favour some sort of interface that describes whatever functionality I need from the HttpClient. "Fake" super-classes make me feel queasy and using Moq makes for brittle code. I really wish Microsoft would provide a System.Net.Http.Abstractions package to make everyone's life easier. p.s. Good to see a fellow Norfolk'r on Reddit - also pretty sure I currently work with one of your former colleagues! 
Hey, thanks for the comment. I agree none of the methods are amazing but at least they are options for people. Part of the reason for the post was the amount of times I’ve heard developers say that you can’t test it. Which while the methods may not be elegant it is possible. I would personally use the interface approach as well. What I’m interested in is the upcoming HttpClientFactory in ASP.NET Core 2.1. Which may make things much better. I believe you currently work with a few of them! Hope all is going well, they’re a decent bunch. 
Are you asking about running: - Xamarin (rebadged as VS for Mac)? - JetBrains Rider? - VSCode and doing .NET Core? - Running VS Pro 2017 in a WinVM? 
Sorry - didn't want that to come across like I was criticising the blog post, I was more criticising the fact that there is no "perfect" solution for mocking the HttpClient. I hope they release some sort of pooling (or singleton), as most don't realise that HttpClient should only be initialised once and then re-used throughout the application! Going well thank you - hope all is well with you!
Check out this post on the new [HttpClientFactory](https://www.stevejgordon.co.uk/introduction-to-httpclientfactory-aspnetcore) I think you will like what you see. Well, hopefully :)
Don't roll your own authorisation system. Instead, take a more detailed look into the policy-based authorisation you mentioned (https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies).
You can also intercept the HTTP request and mock the response with this framework: https://github.com/justeat/httpclient-interception
&gt; The correct way is checking the SqlException error code, which is documented. I was trying to make a cross-db solution for the purposes of the article, but so far haven't found any standardized error codes. &gt; more logical to put in an extension method That's actually much better IMO. Just updated the article to use extension methods instead :)
Someone correct me if I'm wrong but isn't aspx Web forms? Asp.net MVC is completely different and if you can you should switch to that.
I completely agree. If they are giving him the option to rewrite and it is feasible he should. But all the factors you mentioned need to be considered 
Asp.net MVC has a couple of rendering engines available: aspx (webforms) or razor. It's not webforms in the sense of the actual asp.net webforms of old. There are no postbacks, it just uses the webforms syntax.
Would anyone know why vue is missing when I run: dotnet new --install Microsoft.AspNetCore.SpaTemplates::*
I am not sure what your talking about, why would you need to inject the repositories into the domain model?
custom policy based auth is pretty flexible (we use some pretty weird stuff). I'd go down that route before attempting to roll your own auth, rolling your own auth is a recipe for disaster.
As I'm pretty sure you are one of the team members for LLBLGen, your stance doesn't really surprise me ;-) Your points are based on technical merits though. From a PRACTICAL stand point, using a tech that devs are familiar with, especially for something so common as basic SQL data access, is a huge advantage for any dev team. You're not wrong and there's nothing wrong with advocating for your product, but it shouldn't really be surprising that the big two are the ones they focused on.
It sounds like you are not very experienced. A full rewrite of the app is probably doomed to failure. If their goal is a "responsive" app, you are on the correct path if you attempt to apply boostrap and restyle the existing application to their specifications.
As others have said look into policy. This policy server was recently released from the same guys who wrote identity server http://policyserver.io/
I'm on digital ocean running .net core also. I have a "service" (its just a simple application) that runs every 12h. Instead of having the application handle scheduling, I simply went with having cron handle it.
I used this before and it works great. using System.Web.Mvc; using System.Web.UI; namespace MvcApplication1.Controllers { public class UserController : Controller { [OutputCache(Duration=3600, VaryByParam="none", Location=OutputCacheLocation.Client, NoStore=true)] public string GetName() { return "Hi " + User.Identity.Name; } } }
Could you please provide an example of how exactly I can store and fetch the data in this example?
I personally do not dislike the approach taken for the HttpClient by Microsoft. The lack of an interface isn't an oversight but rather a statement on the fact that mocking its methods would be too cumbersome, think of how complex the HttpResponseMessage class is. Using a fake http handler like the one in the post or in one of the comments or a library I had mentioned in a previous thread lets you focus on the shape of request and response messages rather than worry about mocking every single property. Obviously I would never let any of my Devs do this in our source base just because we can't have the testing muscles Microsoft can put and the HttpClient strategy relies a lot on trusting that that class is optimally and thoroughly tested.
why can't you just save it as a session variable?
Sounds like you should just use a cookie.
@: is a shortcut to switch to &lt;text&gt; mode, maybe :@ is somehow related.
I am not attached to LLBLGen in any way but that product is amazing. ORM isn't rocket science in terms of usage. Any competent .NET developers can pick any up in a day or two. 
Actually no. EF was "popular". It lasted for three versions before being made legacy by EF Core. Microsoft simply has terrible track record on ORM.
The return would be your data access layer call. The data tag attributes above the method will handle telling the client to cache the website for 3600 seconds.
By wrapping Tags: in the &lt;span&gt;, you've now coded a proper explicit limited transaction. Without the HTML wrapper, or Razor &lt;text&gt; tag wrapper, errors can happen at runtime. Looks like that's what you were seeing. https://docs.microsoft.com/en-us/aspnet/core/mvc/views/razor#explicit-delimited-transition
Agreed. A client side cookie would work fine here
You created a label in c# called "Tags" that can be the target of a goto.
this is correct. are you using a client framework? (Angular, React, etc.) if so, there is probably already a mechanism for cookie data.
`Cache` in `HttpContext` is shared by everyone. `Session` in `HttpContext` unique to the user. These are both server side. That being said, you could also store it client site with JS using a `Cookie` or more preferably `localStorage`. `localStorage` is better because its not included with every request.
Sometimes Razor want an @ and I can't figure out why, other times it complains about and @ too many.
So basically what I said when I wrote: &gt; Do I need the WebClient to authenticate beforehand by reaching Azure AD directly, like the website, and thanks to OpenID Connect sharing its data, automagically being authenticated in the website? I'll check what I can found but I'm a bit lost. If you have a link to a tutorial or a GitHub example, it would be welcome. Thanks for your answer.
Like a lot of these questions you may be putting the cart before the horse. Are you sure you want client side caching? If, for instance, you are going to be putting a name in the nav bar once they login for the duration of them logging in, and its an SPA, then clientside caching has it's place. If you have a traditional MVC application, user session would make life easier for you most likely. If you tell us what the use case is a bit more, and what you are using to build it, we can put you on the right path.
&gt; Location=OutputCacheLocation.Client
Yeah it's a bummer :( But we did salvage the template so at least you can start there! https://github.com/Dev-Squared/DotnetCoreVueSPATemplate
Too bad it doesn't support Xamarin.Forms
I think I would prefer to get a napkin
&gt; &lt;img src="data:@ViewBag.Mime;base64,@ViewBag.Message" /&gt; This works without a problem. Not sure why your doesn't work.
I work with a team of .net developers and nobody uses the github extension for vs as their primary git client. We all use different clients and have never had a problem. Git bash, git extensions, smartgit, sourcetree, github client, etc all work fine together. He probably just had a bad experience and doesn't want to troubleshoot all the different git clients. It can be a pain to help someone when you are not familiar with their GIT client. They all do the same basic thing, just with a different UI.
Yea, forgot to say that troubleshooting part on my comment. Good you put it down. Sounds like the most plausible reason.
Totally agree with /u/neilg you shouldn't have any issues. The only thing I'd recommend if you're not using the git tools in Visual Studio and you're using .NET is to remember to include the [Visual Studio .gitignore](https://github.com/github/gitignore/blob/master/VisualStudio.gitignore) early on, otherwise you could end up adding stuff to course control you don't want. 
Please also note the claim types are often standard or "well known", so there is often a constant string or readonly string you can use to compare the claim type with if you're looking for a specific claim.
I use git bash for most git operations except for merge conflict resolution, which I like the visual studio extension for. 
I use babun
Git GUI’s are just kinda bad in general and if you want to use one the source control within Visual Studio is actually not that bad. I’d definitely recommend getting your head around Git Bash though as it’s so much better in every aspect other than being pretty you wouldn’t believe you even used a UI in the first place. 
The Prof is probably referring to issues with the default setting of core.autocrlf in VS; VS will try to commit CRLF, whereas most git clients, and this is normally the convention, will commit LF, even if it was CRLF in the work tree. If you mix VS and some other git client -- which you will need to because VS is a little limited in what it can do -- you will see strange things like an entire file appearing to have changed where you only absentmindedly hit CTRL+S once.
One of three ways it depends how lazy i am or how important it is 1. Client side validation thats calls an api. 2. Pre save in the repository, check to see if it already exists, return a validation message 3. Just chuck an exception, make sure the error is readable...
&gt; Support for ASP.NET Dependency Injection Finally. Bits and pieces of Core find their way here.
They are asking you to do something that is likely all HTML and CSS. You should be able to restructure the page markup and styles without touching code. It doesn't matter what stack the site runs on.
More like this if I'm understanding you right, looking at it in the context of the third party app. https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-code-samples#server-or-daemon-application-to-web-api You can either use a generated cert, or the client secret route. I've done both, secret is probably easier since you don't have to generate your own cert. 
I use git bash with asp.net core projects all the time and it works perfectly fine ... 
Interesting project, but I feel the person who wrote the article didn't have the fullest grasp as to what they were talking about. Not the most clueless author I've seen talking about dev stuff though lol.
Your professor is wrong, it's as simple as that. I have been using both for the past 6 months, and even for basic stuff prefer the command line. For me, it's quicker and more powerful than using the UI. Having said that the UI is much easier and simpler. Both are great. 
Not that it's suggested, but you can return anonymous types from your controllers. 
I recommend using F# records to define DTOs. It's so much terse that you probably will end up defining all of them in a single file. The bonus is that you will probably start exploring F# more if you haven't already. It's really an awesome language, ahead of C#.
Also, is a WebJob the best way for me to run a console application in the background to perform tasks?
Yup that is complete bullshit.
A rather useless post
This isn't specific to dotnet But I'm a dotnet developer and recently purchased these...can't wait to dive in. Im hoping I already know a lot of it! My reading order is clean coder, clean architecture, clean code, 97 things.
need more info ! a couple random links describing two popular approaches: * [WebAPI backend](https://www.codeproject.com/Articles/1206306/Integrating-ASP-Net-Core-and-Angular-project) * [SPA Template](https://www.hanselman.com/blog/ASPNETSinglePageApplicationsAngularReleaseCandidate.aspx) 
I want to create angular4,.net core project with vs2017
You'll end up going crazy trying to reuse the same entity models for input and output DTO as well. What you have is fine. You could use a library, e.g. AutoMapper to generate proxy objects, but as the mapping complexity grows it gets more painful than plain old DTOs. You could mark up all of your entities with [Attributes], various [JsonIgnore] and so on, so that you could use the same classes for everything, but that's worse than DTOs. DTOs are a part of the definition of your external API. You don't want that auto generated.
It’s maybe just my aversion against Martin, but I found the Clean code book to be very average aspecially in comparison with Code Complete. The 97 things you can probably skip as well. 
Careful with that serialization, though. 
Would be cool with an update of how it goes with these books, I consider reading similar books! 
Oh hell no :(
I think it's semantics but if you're making a purely REST API I think it's a misnomer to name it a view model and at that point it simply is a DTO, or that's my understanding. It doesn't help there's so much overlap especially when you then bring in other areas, eg. WPF ViewModels actually are sort of the controllers and have services injected etc, but a ViewModel in ASP.NET is literally a POCO passed from the controller to MVC View.
I agree with this.
The fact that you can doesn't mean you should! 😜
Yeah this is an interesting approach, I've never seen it done in practice though. I think if you're using AutoMapper you're going to potentially have a troublesome time though. I had a look and I think you'd need to use `ConstructUsing()` to new up the F# records from and pass in the properties from the C# model you're mapping from. There are plans for record types in C# at some point (you can find it on GitHub) so eventually you might be able to not need the F# part for this.
If it's running on Azure then sure this is the way to go. Of course, naturally anything running for a long time can be expensive so double check. Though if you happen to need a full VM on Azure then you can just run your console app as a Windows Service (dead easy, you just need to get TopShelf) and run it that way too. Recently they also announced Durable Functions that can run for as long as you want too. So a few options for you.