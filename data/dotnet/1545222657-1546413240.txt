:( no direct access to the internet either. Access to the internet goes through a secure citrix solution, that more or less is cut off from the rest of the computer and network...
I'm beginning to think that I have to try the DB solution posted here, and if that doesn't work, I might have to make something like 5 sec scans of an XML file to refresh
The font-icon suggestions are good, but it's worth noting that the MDL2 font isn't the same across Windows versions so you may find the icon you picked is available on 8.1/10 but not on 7, so if you want to support 7 then you should be cautious. How are your icons defined? Are they static images or are they defined in XAML, e.g. as a DrawingImage? If the latter then you could look at using an AttachedProperty for the brush and including bindings to that property from the definition of the DrawingImage.
No worries! dotnet is a great community. One thing I implemented recently - for list box / list view you can have different templates depending on the type. So for example, you can have an observablelist of some class/interface, and fill it with different types that derive or implement from that class, and using the difference templates they can be displayed differently in the list. 
Best of luck you're in a very limited environment when it comes to allowing multiple computers to communicate sadly :/
Could some kind of Azure service work here? Not sure which one, maybe the IoT ones. This way the server is actually Azure, which is pretty reliable, and all the client programs can get notified when ever something changes, or even just poll for updates. Note I have never done this with Azure but I did a simple php database thing once that did basically the same thing. 
Thanks buddy. I already made something in word vba that imports all kinds of stuff into a document, so if the DB thing doesnt work in my environment, I'll fiddle something together with a file, maybe XML. Afterall, the amount of data I have to store is somehow limited, and i can live with scan times
Hmm. Could be. I'll check up on it, thanks :-)
My thoughts: 1. Pick a simple project that will hold your interest 2. Choose the latest stable C#/XAML framework where possible, maybe UWP for desktop and Xamarin.Forms for mobile. For web use ASP.NET Core and Razor pages but maybe other people know better than me 3. Start with a basic template. For UWP, the Windows template library can get you up and running with an app immediately. I know there are similar templates for the web stuff. 4. Slowly modify the project to do what you want, levelling up your C#.NET google-fu as you go. 5. Spend the remainder of your programming life googling one error after another. 
BTW here is a bit of code for the converter from my app using System; using [System.Windows.Data](https://System.Windows.Data); &amp;#x200B; namespace WPFHelpers.Converters { public class IntegerToColourConverter : IValueConverter { public string\[\] Colours = { "#a6cee3", "#fb9a99", "#cab2d6", "#fdbf6f", "#ffff99" }; &amp;#x200B; public object Convert(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture) { int idx = System.Convert.ToInt32(value); return Colours\[idx % Colours.Length\]; } &amp;#x200B; public object ConvertBack(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture) { throw new NotImplementedException(); } } } &amp;#x200B; And you put it inyour ResourceDictionary e.g. &lt;converters:IntegerToColourConverter x:Key="IntegerToColourConverter" /&gt; and then use in XAML like (for example) &amp;#x200B; &lt;StackPanel Grid.Column="1" Background="{Binding [Sample.Id](https://Sample.Id), Converter={StaticResource IntegerToColourConverter}}"&gt;
Thank you so much! Great response!
We ran into issues quite quickly with CommandParser and I've soured on it. It failed to run on a server that didn't have 4.0 installed due to some interesting shims it was using to support calls on multiple platforms. I dug into the source to find some other interesting things such as the library pulling individual code files from another repo via paket. I'm soured on it due to being more complex than we needed, and were able to quickly spin up an alternative that does what we need without bring in a dependency we ended up not being comfortable with.
Yeah, it is a mystery why we tend to ignore basic principles. Why do you think that happens?
Thanks for the article. I found this really helpful. I am self-taught and have very plain POCOs. You called it exactly right, the models I have are simply containers and my services tend to handle validation/rules. 
Any examples of where this shines over doing it yourself?
Thanks for that link. I've been on NDesks.Options for years. It has some major drawbacks, I do like that it is a single file no installing or anything but it might be time to upgrade.
I just tried this. It is really awesome :O This is way better then finding the source all the time!
It also works for private nuget packages hosted on private source control. We use it with our systems. Making debugging private libraries a million times easier to debug ! Have fun. 
Yeah, I thought the same thing. Would like to see a sample of this working well on a larger app with more parameters and subcommands. 
Tab completion looks pretty cool. Never tried, but I imagine that would be a pain to implement from scratch. https://github.com/dotnet/command-line-api/wiki/Features-overview#Suggestions
It literally is a rebrand of Xamarin studio / MonoDevelop. 
If you're doing .net core the built-in [command line configuration binder](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-2.2#command-line-configuration-provider) works pretty well. It will set properties on your "app settings" right from the args collection. This also lets you set up layers of settings sources such as appsettings.json, environment variables, and command line.
It was probably the .Net 2.0-era Identity system, which had the tables named "aspnet_{whatever}" instead of the newer "AspNet{whatever}" ones. aspnet_Users has ApplicationId, UserId, UserName, LoweredUserName, MobileAlias, IsAnonymous, and LastActivityDate columns. AspNetUsers has Id, Email, EmailConfirmed, PasswordHash, SecurityStamp, PhoneNumber, PhoneNumberConfirmed, TwoFactorEnabled, LockoutEndDateUtc, LockoutEnabled, AccessFailedCount, and UserName columns (by default, but the new Identity system is EF code-first, so it's easy to add more).
Thank you! I couldn't find the columns of the old tables anywhere. You helped me alot!
You can also use dotPeek (free tool by JerBrains) to decompile assemblies and run a local symbols server. https://hmemcpy.com/2014/07/how-to-debug-anything-with-visual-studio-and-jetbrains-dotpeek-v1-2/
Very interesting but the documentation is awful.
It certainly is small compared to what I'm used to. 
&gt; But fine, don't use it. I'm not sure why you're asking about it then. People are telling me to use it because VS is slow and resharper is productive. I'm just challenging that because I don't think it's true.
Just one is enough for me not to be able to use it and there's one in particular that is very obvious (and identifiable so I won't tell :)) so I didn't even bother checking them all.
We use chaijs http://chaijs.com/
Thanks for the feedback. I'm actually using functions, specifically, in the case that caused my complaining lol When you create, for example, a new function app, it will create all the dependencies for you (app service, storage, etc.) which isn't "free". It's just annoying that when - let's say as an example - advertising functions as $0/month until a certain usage threshold is reached is not the whole story - since you end up having to pay for the other stuff. In the case, I didn't have the option to select a free tier of the app services when I created my functions app...
You can use this.ViewContext.HttpContext to get the HttpContext which has the request URL and any other information you might need. For example this.ViewContext.HttpContext.Request.PathBase returns the relative url from the server root your application runs on. For example if my app runs on http://www.example.com/ it's an empty string, but if I'm on http://www.example.com/someapp/ it returns "/someapp". .Request.Path returns the path relative to the application's url to the page the user requested. You probably want to use PathBase plus the application's path of the AJAX API to build the URL you want. I actually made something similar just today but I opted to keep it simple and just navigate the user to the search page running on the different URL or server. Here is how I did that: @{ if (!string.IsNullOrEmpty(Program.Settings.SearchPageUrl)) { &lt;form action="@(Program.Settings.SearchPageUrl)" method="GET"&gt; @{ if (!string.IsNullOrEmpty(Program.Settings.SearchExtraArg)) { &lt;input type="hidden" name="someUrlQueryArgument" value="@(Program.Settings.SearchExtraArg)" /&gt; } } &lt;div class="mdl-textfield mdl-js-textfield mdl-shadow--2dp"&gt; &lt;input class="mdl-textfield__input" type="search" id="search" name="q" /&gt; &lt;label class="mdl-textfield__label" for="search"&gt;Search&lt;/label&gt; &lt;label class="mdl-button mdl-js-button mdl-button--icon"&gt; &lt;input class="material-icons" type="submit" value="search" /&gt; &lt;/label&gt; &lt;/div&gt; &lt;/form&gt; } } In this case I have a configurable url that is probably pointing to an external site. But here is an example of using pathbase: &lt;link rel="stylesheet" href="@this.ViewContext.HttpContext.Request.PathBase/styles.css"&gt; http://www.example.com/somesite AND http://www.example.com/somesite/ both load my page so I have this set up to force urls to be absolute (relative to the server) so the page loads correctly (I probably should have made somesite without the slash redirect. But now you get an example!)
You can specify which controller/action or page the form posts to. So setup a form in the layout and set the controller/action or page to point to the place you run the search. https://docs.microsoft.com/en-us/aspnet/core/mvc/views/working-with-forms?view=aspnetcore-2.2#the-form-tag-helper
Ah. That sounds exactly like what I need. I'll give it a go. Thanks a lot.
What features do you miss, if I might ask? Maybe I just never dug into it too much due to the performance issues. I tried it on both my home PC and my laptop. It would just drag VS to a crawl. The worst issue was when I would type, and the cursor would start spazzing out with the "working" animation, typing would stop and delay, then continue. I tried all their performance tuning advice, my systems aren't under powered by any means, I just couldn't get it to not lag to hell and back :( So I just never really got into it. I find VS + some extensions cover everything I really want though. 
Either don't store that on the client-side, keep a cookie with a session Id on the client side and keep the session data on the server. Alternatively encrypt the data you keep in the cookie so only the server can read it and change it in a valid way.
In Tools &gt; Options &gt; Debugging there are some options that might interest you. Under General you can disable "Enable Just My Code" which will replace "External Code" in stack traces with the real stack from .NET or other sources while debugging. In my experience by default VS will download source code when it wants to display it (for example an exception in .NET code or you step into .NET code). "Enable .NET Framework source stepping" will make stepping open .NET source files if applicable rather than skipping over them. Under "Symbols" you can enable official sources for downloading symbol files for .NET. This will allow for accurate stack trace information for .NET. It's likely F12 does not download the source like the debugger does because it is a lot faster to display the metadata (which can be generated on demand) than to download the file, and typically the metadata is all you need anyway.
Cool, but interestingly the debugger will actually download the original source when it tries to display .NET code (for example if you disable Just My Code). I guess because for F12 you don't really need the EXACT code while the debugger has to be more picky since you're stepping through.
https://docs.microsoft.com/en-us/aspnet/core/fundamentals/app-state?view=aspnetcore-2.2 Use ASP.NET Core Sessions. This will automatically set up the cookie for you which is just a session id. The user can't just change the session id since they are randomly generated and I assume they are tied to the user's IP anyway. And then all you need to do is store/retrieve data from it as needed and the persistence is handled for you and the client cannot see the session data (unless you specifically show it to them).
I learned the fundamentals of c# first before going into .net as the framework is built on c#. If you don't understand basic things like c# generic collections, it would be hard to get into .net mvc as it relies heavily on that. You might already have a background in it but making sure to understand generics, interfaces, and inheritance will help make .net easier to work with.
how do they do that? The programm didn't run yet, didn't it? 
Make sure the documentation you follow is using ASPNET Membership , not ASPNET Identity. Sounds like you have Membership schema
Read it, it's interesting. You need to install another program, which looks for tabs being entered in a console and pipes that over
You can use encrypted cookie, but more normally you'll use sessions. I.e the cookie only contains a guid, and the rest of the data is stored on the server in some temporary cache keyed on the guid.
I'll look into that. My boss doesn't really know what all the subcontractor used. It's honestly like a black box at some points lol. Thank you!
I write tests for the args handler as there's variation in what flags and switches are supported. Everything else is part of a mature framework which has more integration tests than anything. Everything run that runs CLI conforms to a specific interface, so really there's one EXE which can be configured to run any one of n services. The services themselves can be unit tested easily.
Yes. We are approaching version 3 or dotnet core.
Hey Tdubeau just wanted to let you know I was able to work around my issue. Thank you for the assistance, it was GREATLY appreciated and welcomed. 
As the other people have mentioned use session for this. But if you intend to have actual user management on your site i would sugest looking up [Identity](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-2.2&amp;tabs=visual-studio) instead of doing a custom implementation.
yes but there are still issues with many project not being able to be ported to it because their dependencies are unmaintained and not supporting asp.net core. Probably wont be too much longer before these are widely replaced or updated by community but I still come across it occasionally.
Sounds like you may want a Razor view component to encapsulate the state of that form for the layout.
If you need your jquery to get access to sensitive information, but want to ensure the client doesn't tamper with it, then what you want is a browser or cookie stored JWT (JSON Web Token). * The server provides a payload (sensitive info) and a hash * The hash is generated with the payload and a secret that the server only has * When the JWT is provided back with requests, the server can hash the payload with the secret and if it matches the hash included in the payload, then the server knows the payload was generated by someone who knows the secret * [ASP.NET](https://ASP.NET) Core middleware exists to do most of this for you (you just have to provide the secret and some other configuration). You can also integrate with identify server to do the work for you as well * I am leaving out a few bits like refresh tokens and delegated authorization, but if you want to use this approach you'll have to read a bit to do it right anyways. Good luck!
I would make another site just on your own. Keep it nice and simple. Maybe implement some ajax in there. I would do the whole process and document through github. &amp;#x200B; I would make a requirement doc, wireframes and then the app. I think that would look really good if someone was looking to hire you. It seems like you had a good course. I have taken server side in school and we worked with MVC5. I had A in the class and learned very little of any value. Everything i picked up on my own through when needed at work, but sometimes its nice to learn with a book at first to avoid wasting time.
I would probably try to learn .net core moving forward if you are career oriented, although .net framework will be around for a good while. It's not much more difficult. There is of course going to be less documentation out there since it is much newer. Core will give you access to cross platform development, c#8, built in DI and is overall much more performant running snd building.
You didn't say whether you used ASP.net MVC or not. Given that you learned DataSources and GridViews, it sounds like you did not. So I would start learning that. It is a very different paradigm, but its emphasis on separation of concerns will give you a good foundation on actual software engineering ideas and principles.
A database doesn't really work well for keeping things in sync, it's not really made to instantly update all clients. You would really need a server, such as a ASP.NET/ASP.NET Core web server, and you could use something like WebSockets to send updates to clients whenever something changes.
I was browsing options for my desktop app project and I was just reading this projects' Github page a few hours ago. I want to do it in C# since that's what I'm going to use in my next job starting in two weeks, but also in cross-platform manner. This seems like a good fit.
...then don't use it. You're not challenging anything by just saying "I dont like it" without even trying it.
That’s awesome!!! Thanks!
What kind of List are we talking about? List&lt;T&gt; - what is T? If it's easy enough moving to a DataTable then that's not a bad idea. How much data's being returned? Do you have to consider things like paging?
I've tried to help with issues on that repo where I can, but the original author of CLP really got into the "functional programming" weeds with v2.0 and I don't like it much at all. I began writing my own alternative to CLP about 6 years ago and IMO it's much easier to work with. I tried to pre-validate many configuration issues like invalid option names or duplicates with actual, helpful error messages instead of junk like "Sequence contains no elements" https://github.com/nemec/clipr
sure thing, thanks for considering it! issues coming up!
completion is a separate module you install into your shell (e.g. bash or powershell) that listens for tab commands and then delegates to your program for completion suggestions.
It’s like one of those List&lt;MyClass&gt; with a few strings in it. Yeah, I think I’ll just start returning a DataTable for this. It’s not much data, just a few rows, so no need to worry about paging or performance overhead. Ever since I moved away from web forms, I’ve tried to stay away from DataTables. Yeah, after thinking about it... querying a table just to get column names doesn’t seem very efficient. Thanks for the help. I think my brain just associated DataTables with Web Forms for some reason.
Another issue that I have, with my company specifically, is that we have a lifetime license to use, let's say Aspose Words 4.0. that was built under .NET Framework. It is still being maintained, but would cost us something like $30,000 to purchase it with the new licensing model. That's a cost you have to consider "are we really saving that much money by moving to .net core?"
If you could offset the cost of a windows host by moving to linux? Then probably yes.
the ascii table is built in such a way that the decimal numbers are right before the uppercase letters, such that there is a fixed offset between the integer (ascii) value of each character and the number it represents. read the string right to left and convert each digit individually to binary.
&gt; b) The server it will run on doesn't have access to System.Numerics for BigInteger, otherwise that would be easy I don't follow that. Are you targeting .Net 3.5? You can even use [ILMerge](https://www.microsoft.com/en-us/download/details.aspx?displaylang=en&amp;id=17630) to put System.Numerics.dll into your applications assembly if you so wish (or in the same directory is normally enough). If you're doing this as an academic challenge, I'm sure there are much cleverer ways, but if you just want to get it done just add BigInteger. You can even copy/paste the raw source into your project if you need to. 
I picked up [NDesk.Options](http://www.ndesk.org/Options) years ago and never found any reason to bother switching to anything else.
I think it's been pretty much ported into Mono.Options?
You can get the source here: http://www.ndesk.org/Options It's old, but it does what it needs to do. Might be helpful to have netstandard targeted though.
Sorry, yes you are correct I did not us MVC. Okay I will definitely check that out, thanks! 
I'm definitely career oriented, I really didn't give ASP much thought until I took that class, but found I enjoy it. Plus out of the languages I've tried or taken classes in I enjoy C# the most. Say I just go with Core from this point forward, will (if need be) jumping back to .net framework be an easy transition?
That definitely sounds like a great idea. I have a GitHub portfolio, but it's pretty lackluster - so that would be a great addition to it. The class i took the professor actually didn't use a textbook. We just kind of dug into building a site using ASP components. 
You could also use reflection on MyObject to get the property names which would represent the column names here but using a data table should be easier.
It's not difficult going back and forth from a language perspective, they both use C#. In general the framework libraries are laid out the same, but there are some differences that you will come across - notably that net core doesn't support everything that is in net framework yet (and somethings never.) I don't have any issues supporting about 20 framework projects and about 5 net core frameworks at the moment. I would say net core is pretty stabilized now and production ready. It would be nice if EF core had some of the more interesting features that EF 6 has. I'd also look at Java, Javascript. I'm not as much of a fan of typescript or python but those are also good options. C# is my favorite language and environment by far.
You can break the string down a bit at a time. Check the last digit, if odd, then the bit is 1, if even then the bit is 0. Subtract 1 from the digit if it was odd. Then divide the string by 2 using long division. Repeat until the string is 0. Then put the bits together into the hex codes.
This essentially runs Roslyn inside the web browser, doesn't it? That is cool.
 That's good. So would you say more companies are going to be looking for people who know Core? Or is there already so many projects done in Framework that they look for people who know Framework? Also yeah, I've taken a Java Class in between two of my C# classes and I think I was already spoiled by C# and Visual Studio, because I just didn't like it that much haha.
There is a ton of code out there written in net framework. A lot of places probably won't stop using framework until they absolutely have to. Newer companies and projects would have a hard time justifying using framework over core, I think. But really they are very similar, if you know one you could learn the other easily. Java with Jetbrains intellij is pretty on par with visual studio. Java is just extra verbose and kind of tiring to work with imho (you NEED an IDE to be productive) - probably why Kotlin is becoming popular (a more terse language that runs on the JVM) Really you want to learn good programming concepts more than anything though and those transcend languages and the frameworks du jour. Just in the last 3 years I have done major projects using c#, c++, vb.net, java, javascript, tcl, r, sql all at the same job!
Write your own integer parser that accepts a string and outputs an array of bytes. The algorithm is fairly easy to adapt, and is widely available. Then use the byte array to output the hex digits, which is easy because one hex digit is exactly one byte. .. Or, you could just rip a bigint implementation from github, or from dotnet's own source, and be done with it. Because bigint does exactly what I've described above.
hint: just because it's an array of bits doesn't mean it has to be in base 2 You can use BCD.
If you use the encryption strategy, make sure you use authenticated encryption. If you take a small message, encrypt it, and then start modifying the encrypted bytes, you will eventually generate a sequence of encrypted bytes that decrypt to a valid message.
&gt; Then divide the string by 2 using long division The string cannot be converted to a numerical value per the OP, how do you expect to do that?
Cool! But be careful, I don't think it is ready yet for production use.
Say you have the string "364". Look at the left most character "3". Convert that to a number. Divide by 2, get 1 with a remainder of 1. Move to the next character "6". Combine that with the remainder to get "16", convert to a number, divide by 2, get 8 with no remainder. Move to the next character "4". No remainder so just convert it to a number. Divide by 2, get 2. Combine the results to get the string "182". Its the same as doing long division by hand. It would even be possible to do the long division by 16 instead of 2 to simplify the convert to hex.
It'll be, at least for starters for my personal use and it'll be a fairly simple app. Nothing crazy.
&gt;Then divide the string by 2 **using long division** By using [long division](https://en.wikipedia.org/wiki/Long_division), like you learned in elementary school.
**Long division** In arithmetic, long division is a standard division algorithm suitable for dividing multidigit numbers that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps. As in all division problems, one number, called the dividend, is divided by another, called the divisor, producing a result called the quotient. It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; I assume they are tied to the user's IP anyway. If they can it's not by default. Generally, session cookies are not tied to IP addresses because IP addresses can change for subsequent requests due to dynamic proxy setups. It's basically just a random number that you hope nobody but the intended user has during the lifetime of that cookie.
Thanks everyone for your assistance, just needed to shake a few cobwebs loose, C# isn't what I'm most familiar with. I ended up using the Long Division method, dividing by 16 (expanded the function out to be base-generic, so would divide by the base being converted to). Works perfectly, and don't need any additional libraries. &amp;#x200B;
Not the person you asked, however there are a few features of R# I miss (not enough to go back to the slowness, though): - the ability to quickly navigate up / down an inheritance and override chain (go to parent or go to children) - solution analysis for classes of problems (e.g. find all places that violate a certain rule, oh and fix them all) - single shortcuts for navigating to a type of item: for example `Ctrl-T` for type, `Ctrl-,` for methods in the *current* file, etc - the ability to LINQ-ify code/statements was nice and it would help some of my junior team mates; some of the other suggestions were nice too (use null-propagation operator, etc) At this point I miss more the navigation than the refactoring features.
https://www.nuget.org/packages/Morcatko.AspNetCore.JsonMergePatch/
That's the nuget I mentioned
Long division, short $MU
I've already jumped right to T# and I'm reading up on Z#. /Smug hipster coder.
I've gotten sucked into web development here outside of what i was just getting comfortable with WPF and C#. I'm not a fan. They basically want real apps, but in a web browser. And I'm like I can do this a lot faster if you'll just let me make it a real app. I don't like websites.
Yes, It is roslyn inside browser. 
Still doesn't explain how you plan to do it on non numerical value in a software where you can't divide the whole number because it doesn't fit in any given types.
Did not know that was called "long division" (non-native English here). I thought you were talking of a division by a `long` value.
https://github.com/videolan/libvlcsharp
Web apps are always longer to create, I'm not fan of it myself. Especially dealing with security. But I still use c# for web backend 😂 on other hand it's always good not to stay in your comfort zone too much, that's why I am moving slowly towards uwp
Haha yeah, i was like "if i'm doing web, i'm still using c# and i guess I'm learning asp.net core now". I really wanna find some ideas to start learning uwp myself. It's hard for me to just try learning something, I came into programming as a powershell guy who needed to make a UI, and it just ballooned from there, so I'm just winging it learning things but it works best when I have an actual thing I need to make. 
Before that, make sure your server is on Linux 😂 I made that stupid assumption once and at the end admin could only provide Windows server. Which meant that web app was needed to be redone (as well as services for database, But that's different story). Uwp has two bad aspects in my view which are basically killing it for me : 1. Capability restrictions 2. Forcing the store. If you don't care about it, you will find that uwp provide much better libraries for Windows and it is easier to do multi inputs. If you want you can join my "endless conflict with Microsoft support" project that is written in uwp 😂
I saw something in the Roslyn project issues about a NullReferenceException, the field is marked `private readonly`, has a single assignment in the entire CU, where the null case is checked. Turns out, the customer AV was doing the same crap outlined in this article (module injection).
OK I'm just going to ask. Which part of an LOB application "should" rather be done in F# as opposed to C#? Where does the functional paradigm really yield benefits in the old and boring enterprise world where big data is not an issue?
One thing not mentioned here that I'm very excited about is support for SIMD intrinsics. They've already done a ton of work on this, I've been using SSE, AVX, etc. in some of my hobby projects for a while now with the .NET Core daily builds.
Great read! Thanks for the report!
We really need widespread adoption of IPv6.
I hope you contacted both MS and the AV vendor about this
Really looking forward to working with .Net Core again. Sadly, I am stuck with a 10-year-old VB.NET project for 3 months to come.
Could someone just freaken clarify if winform/wpf etc will actually run on NON windows OS's? There is so much confusion on what they are saying. If this is a Windows only feature then wtf is the point of "Core" runs everywhere and now they are sticking things in .NET Core that wont, in fact run everywhere.
[This help](https://docs.microsoft.com/en-us/dotnet/core/migration/20-21) ? &gt; Removing the &lt;DotNetCliToolReference&gt; references for those tools from your project file fixes this issue. Shout if that works.
This particular article seems to make it pretty clear that the WinForms and WPF stuff supported in .NET Core 3.0 is only for Windows development. I don't know whether there is an effort planned to somehow make Winforms/WPF (or some variant of it) cross-platform or not but it seems like supporting GUIs for MacOS and also the various Linux desktop environments could be a rather complex issue to tackle.
It has been clarified many times, NO. The point for Core is to have cross-platform and modular .NET, that is has better performance than the old .NET. It doesnt't mean that a Windows specific GUI lib must run on other OS. Winform are for Windows, not a cross-platform GUI lib, but you can bet, that in the future there will be some sort of unified GUI library. .NET Core has some better performance options, so those Winform apps could perform better. In the meantime, the are open source projects that are meant to fill the GUI void. You should check them out if you need GUI for more than Windows
Is this in the .csproj file because it is not allowing me to edit it.
I deleted them from the .csproj file. Now It still doesn't build but also has no errors and warnings. :( 
It sounds like your colleague needed to work with big numbers and chose to work with strings rather than using BigNumber. He's painted himself into a corner and now wants you to help him out. It needs to be done properly with BigNumber. Add the nuget package to your project. https://www.nuget.org/packages/System.Runtime.Numerics/
The warning does not prevent your project from building or running. Whatever that issue is it is unrelated to the warning, which you successfully dealt with.
The author of this blog post also has an extremely comprehensive book on .net memory management that I’m going through now: https://books.google.com/books/about/Pro_NET_Memory_Management.html?id=pIx5DwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button Certainly not light reading; but I’m hoping next time I run into weird GC issues (or our ops team blindly labels as a GC issue) that it will give me a solid starting point to help debug. 
IIRC SQL always returns either a table or a scalar (single value). If you're taking those results and building a List&lt;MyClass&gt; in that process you have to be using the column names to match up the results with the properties (unless you assume the columns are in a specific order and populating by column number). (BTW Entity Framework does all this for you; it returns the MyClass objects as a result of the database query you issue.) If you have your MyClass objects you know what fields are in it. The question is what you mean by "dynamic table headers"? Are some of your objects subclasses of MyClass with additional fields? Does your stored procedure sometimes return a list of SubClassA and sometimes a list of SubClassB depending on how it's called? As stated you can use Reflection to iterate over the members of a class to figure out what ones you are interested it, get their names, and get the values of those members from the objects. First step is to get the type of the object with object.GetType(). In this case the object would be one of the list items (you probably have to iterate over all of them and check each one unless you know all the list items are the same type). Once you have the Type, you can call .GetMembers(), .GetFields(), .GetProperties(), or whichever appropriate function, to get the list of members/fields/properties ("members" covers ALL possible things you can declare on a class, others are more specific). You may need to use the call which accepts BindingFlags if you want non-public and/or static members. For each MemberInfo you have (which can be a FieldInfo, PropertyInfo, etc) you will likely need to cast it to its specific *Info type to get useful function calls on it (.MemberType has the type, or you can do a if (info is PropertyInfo propertyInfo) { etc. But each MemberInfo has a .Name which can be a column name. Then on FieldInfo and PropertyInfo you have .GetValue, where you can pass in the object and it will return the member value, which you can then cast to the appropriate type (.FieldType and .PropertyType can tell you the type of the value, though since you are just displaying them you can just .ToString() the return value to convert it to a displayable string). Of course the member names may not make good column names, so you can tag each member with a DisplayNameAttribute (from System.ComponentModel) and specify a better-looking name there. Then in the code, on the MemberInfo you can do info.GetCustomAttribute&lt;DisplayNameAttribute&gt;() to get the DisplayNameAttribute object you set on the member, where the .Name property is the string you specified.
There is no effort, there won't be, and they have went as far as saying they won't take pull requests that try because of the breaking nature of it. If someone wants to for it and do it they can, otherwise we have to wait for a new UI framework.
I think that's fair. I don't think there's an issue, in fact it's probably a good thing, to integrate WinForms/WPF into .NET Core to at least help provide a path for those who have Windows UI apps to integrate with .NET Core. That would hopefully provide a less painful way to migrate their Winforms/WPF .NET Core app into whatever potentially cross-platform UI framework comes along next.
I feel your pain, have an updoot
I just don't go anymore. They are all sales pitches for Azure. And there are thousands of people with typically a whole lot of walking. Not that walking is bad, but I don't want to start dripping sweat
Can't wait to dive into Core.... we're stuck doing Java Java Java because the Director is Java obsessed.
You can put \`&lt;TreatWarningsAsErrors&gt;true&lt;/TreatWarningsAsErrors&gt;\` in your csproj and it will not allow you to build or run with warnings (although it will specifically tell you that when you go to build), so it is possible warnings don't let you build. &amp;#x200B; Also, if you're using Visual Studio, it could simply be messing up and not saying he has errors, it's happened in older versions of visual studio, it just happens some times. I usually recommend deleting the .vs folder in your root directory and deleting the obj/ folder for each project. That usually fixes it.
Whatever the one that is hosted in the largest city closest to you is. Seriously, the local-run conferences are generally very good, with a fraction of the cost of the large ones. Some of them are even free. &amp;#x200B; Examples: Sandusky, OH - [CodeMash](https://www.codemash.org/) (8-11 Jan 2019) Miami, FL - [South Florida Code Camp](https://www.fladotnet.com/codecamp/Home.aspx) (2 March 2019) Knoxville, TN - [CodeStock](http://codestock.org/) (12 and 13 April 2019) Columbus, OH - [https://stirtrek.com/](https://stirtrek.com/) (26 April 2019) Virginia Beach, VA - [RevolutionConf](https://revolutionconf.com/) (6 and 7 June, 2019) Kansas City, MO - [KCDC](https://www.kcdc.info/) (17-19 July 2019) Phoenix, AZ - [Desert Code Camp](https://oct2018.desertcodecamp.com/home) Los Angeles, CA - [SoCal Code Camp](https://www.socalcodecamp.com/) St. Louis, MO - [DevUp Conference](https://www.devupconf.org/) &amp;#x200B; There's a bunch more out there, and they can be difficult to find. Try googling for "developer conference (city name)" for the nearest major city to you.
Here an article on SIMD intrinsics if anyone wants to learn more: https://blogs.msdn.microsoft.com/dotnet/2018/10/10/using-net-hardware-intrinsics-api-to-accelerate-machine-learning-scenarios/ 
^^^ THIS. Exactly this. 
What is he achieving ? I just use the daemon from Docker on Windows in the WSL. But it’s unnecessary just use powershel it’s the same thing. I just like the Linux console 
Those are two very distinct questions, since many of F#'s advantages are not related to functional programming. My answer to the first question: the business logic. Doing domain modeling in F# is addictive. You get to use more powerful types (like sum types), which makes your domain logic clearer and decreases the number of illegal states. Creating types is also much more convenient due to 1) minimalist syntax, and 2) conventions that support declaring types immediately above the code where you're going to use them. When programming in C#, I often do whatever I can to avoid creating a new type, because I don't want to have to create a new file (after I figure out where the hell it belongs), fix the usings, and type out all the boiler-plate (for immutability, for example). Second question: the aim of functional programming is to make code easier to reason about, leading to better productivity and fewer defects. That makes it highly relevant to all the LOB apps *I've* ever worked on. If you're skeptical of FP, be skeptical that it *works*, not whether it applies to the domain you're in.
The whole point of sessions is that the information is not visible to the user and especially cannot be manipulated (as such information may be sensitive, for example a flag which tracks whether the user is an administrator). You will want to store such information in cookies instead. JavaScript can access this information through `document.cookie`, while cookies can be read back in on the server side, and also set in the headers of the response from the server.
Well yes, but then you've set that up yourself so that's on you. Doing a Clean/Rebuild Solution/Project should also clean out bin/obj and do a rebuild from scratch (if you selected Rebuild).
All the big commercial ones are, for sure. Just content you can get off a blog or MSDN
1. Open up your database and delete everything out of it; verify the tables really don't exist. 2. If you use IDesignTimeDbContextFactory, make sure it is connecting to the right database. Update-Database uses the class which implement IDesignTimeDbContextFactory&lt;YourDbContext&gt; if one exists. 3. If you don't use IDesignTimeDbContextFactory, make sure the default constructor of your DbContext class will connect to the correct database (eg if it's constructed in a void without your entry point running). 4. Make sure you have one migration created with add-migration and that the CS files for it show up (by default a Migrations folder gets created in project root and a CS file for each migration, plus an extra for the current state, is dumped there). If something doesn't look right do Remove-Migration for each migration and Add-Migration once. If Remove-Migration files manually remove all the files from Migrations and then Add-Migration. 5. Update-Database is not necessary to run if you set your program up to automatically create/migrate your DB. Make sure .Database.Migrate() is called on your DbContext at some point before any queries are issued against it and the database schema will be created/updated as needed at that point.
Ah, well that's good to know. Thanks for all that (and everything else) Yeah, I've been told and read that once you really have the grasp of a language picking up another is really not too hard. I think part of my disdain for Java was because of the Prof and how he taught the class/language
First of all you should look up how to do syntax and maybe run through tutorials on simple programs. The best way to learn is to pick a project and simply google how to do the individual code bits you know you need to do in .NET. You'll learn the capabilities of .NET along the way (as they relate to your project). Visual Studio has Intellisense which is very powerful and useful for learning. Basically if you type in any code you get contextual help in tooltips. So I can type in System. and I get a whole list of everything in the System namespace. Just from that we can learn a lot about .NET. System.XML probably means .NET can work with XML files. System.Net probably has a lot of networking classes. System.Math has all the math functionality, System.Random can generate random numbers, System.Linq... ok that one's not clear :) System.Windows encompasses all the GUI stuff. etc. And you can keep selecting things to drill down and see more lists of .NET objects available to you. Other than that, here's some general purpose things I think every .NET developer should know: * Poke around all the debugger tools and learn how to use them, and master them. As in C++ ad other languages it's essential to know how to use the debugger well. * System.Collections.Generic has the best classes for working with collections of objects. In particular List&lt;T&gt; and Dictionary&lt;TKey, TValue&gt; are invaluable. Of course there's T[] (arrays) for fixed size collections. * Generics (the &lt;T&gt; stuff above) are extremely powerful. They work like templating in C and replace some of the functionality of C macros for that purpose. * Iterators are useful to learn (a function can return IEnumerable&lt;T&gt; and use yield return x; to return individual values in an enumeration. System.Linq does this internally. * System.Linq is very useful for manipulating enumerations (a iteration/stream of objects, includes lists, arrays, collections, etc) so it's worth learning at some point, though not essential. * System.Reflection can be used to dig into your own code and inspect classes, iterate through members, etc. It allows you to do things like set up a Settings class with properties for all your settings, and use Reflection to iterate through all properties and build an interface automatically to expose these settings to the user (I just did this today). * Asynchronous programming. async/await and System.Threading.Tasks namespace. Much easier way to do multi-threading without explicitly managing threads in many cases (unless you mix it with synchronous code).
And if you aren’t with a company it costs a fortune
I'm not seeing anything that immediately makes me thing you need or want ES. This is a bit of a cheat, but can you restate the question specifically in terms of what you think you get from ES as opposed to just CQRS. Once we knock off the ES thing we can see if you need CQRS either. You probably want it anyway, it's a nice way to reason about problems. 
Sorry man. I recently had a pleasure of going through some legacy VB code. One method was 1700+ lines, it was quite something.
I'm looking at ES as a way that I could go back later and add additional projections as far as tracking historical price data and availability. Also, its a new-to-me technology that I'd like to learn. Probably 50/50 either way.
&gt; Could someone just freaken clarify if winform/wpf etc will actually run on NON windows OS's? How fucking lazy do you have to be to not read the 20+ articles on that topic.
:) Certainly give it a go if you've got a bit of capacity to rectify stuff later. I only know stuff because of the things I've broken or fixed :) Anything that feels auditable is quite a good fit, so price history is good. And it feels like it would be a relatively small part of the overall system so a perfect place to give it a go.
Here's some more of the cheap/free .NET related conferences from around the Upper Midwest. * Minneapolis, MN - [Twin Cities Code Camp](https://twincitiescodecamp.com) * Milwaukee, WI - [Cream City Code](https://www.creamcitycode.com/) * Wisconsin Dells, WI - [THAT Conference](https://www.thatconference.com/) * Des Moines, IA - [Iowa Code Camp](http://iowacodecamp.com/) &amp;#x200B;
1 method, +1700 line, WTF!!!! 
Any news on corefx?
No joke. I was going to modify the application but it is just spaghetti and complete rewrite is required. 0 objects. 0 encapsulation. Just one class per view and massive methods with 0 maintainability in mind. Variable names like: variable1, variable2 UI elements with names like: TextBox1, Textbox2.
1. The database is new. No tables are or have been in it. 2. N/A 3. The default constructor is empty, but I did override the OnConfiguring method and that creates the configuration that points to the connection string in my appsettings.json. Could this be the issue? Because I've had my DbContext set up this way for months and have pushed many migrations without this being an issue. 4. Did that, same issue. 5. I just prefer to manually push migrations for right now. But if the automatic migrations would remedy this I can try that. 
Try hardcoding the connection string in .OnConfiguring to see if the issue is it not getting the right string.
Why are you concerned about the size of the aggregate? Are you concerned about the size because you currently are not using snapshots? Is the Catalog object going to be frequently loaded from memory (once a second)? Or is this more like a once a week process? My rule of thumb has usually been to snapshot around every 500-1000 events if the aggregate is used frequently.
&gt;we’re also porting EF 6 to work on .NET Core The most important part of the article.
That's a good idea, I'll give that a shot. 
Indian?
Years ago a friend of mine worked at a large energy company (not In US) and all their production, forecasting, market prediction stuff was formulas and routines in one giant Excel file. 
I see there's currently a PR open in the svg project which is supposed to implement .net standard 2.0... but the PR is super old. If you can hold off on releasing after merging this, I can probably take a look at it as soon as late friday as I will have lots of time on my hands again... I also intend to use this package quite soon so I'm especially interested to have this work nicely.
I wonder if webassembly inside a browser shell inside and app is the way it will happen
Nope. It straight up looks like outsourced pile of trash to the lowest bidder, but this guy was recommended and wrote complete management systems before lol. 
.NET servers run antivirus?
What a dick.
Yeah I would also be worried about the size of the aggregate. Even before making it event sourced, an aggregate with a collection that is expected to grow into the thousands doesn't sound ideal. Unless you really need invariants over the whole collection of products, I would change the aggregate. And then once you have event sourcing on top of that, yeah that would be a lot of events to replay to build the aggregate up in memory. We try to design our aggregates so that they don't end up with collections that can grow arbitrarily long and lifetimes/events that will likely grow to a large amount either. I would create an aggregate for Catalog that just has CatalogId and Name and whatever other properties. And then a Product aggregate that contains Name and ProductId and CatalogId etc. That way your catalog aggregate stays small - a given catalog will probably only change names/other properties a handful of times, even a few dozen isn't a problem. Products can come and go but they will have their own short list of events too. 
Antivirus cause more problems than viruses does.
Java isn't *too* bad, it's just like using C# from 12 years ago with broken generics and more boilerplate.
I came from a Java background, and I agree for the most part (except after using C# for so long, I could never go back to Java)
I'm glad I could help, good luck! Other good resources are the Slack channels listed on this page: https://fsharp.org/guides/slack/, some of them are fairly active.
That did it! Good call! &amp;#x200B; Now, what am I to do about this method? It's worked for so long, but now it does appear to be that the configuration builder is not capturing the connection string for some reason. 
There is probably no configuration to load, I am pretty sure Update-Package loads in your assembly and instantiates the DbContext, so no other code is run, so no configuration is loaded. You can set up a IDesignTimeDbContextFactory to instantiate the DbContext properly for Update-Package. You can then hardcode a connection string in there for the purposes of development. OR just let .Database.Migrate() do the updating in your actual application.
That Conference get's a HUGE thumbs up in my book. 
If there's no configuration to load, how was it working properly all the way until now? That's why I don't really understand. 
I don't know, maybe it's pulling a connection string from some other configuration. Configuration files are (by default using the default ASP.NET Core template) based on the ASPNETCORE_ENVIRONMENT variable, maybe that is impacting what configuration settings are getting loaded.
I’ll look into that as well. I haven’t changed the environmental variable but it’s worth checking. 
Well it's only a factor if your connection string is in the relevant configurawtion file. There's appsettings.json and appsettings.&lt;Environment&gt;.json I believe.
Whatever happened to the plan to incorporate Web Hooks into .NET Core? It looks like it's been abandoned and moved to AspLabs - https://github.com/aspnet/WebHooks.
Java is to C# as lobotomy is to ... not lobotomy.
Yes! Intrinsics support will be really nice to have
Are you planning on using the actual properties from the products within your Catalog? If not, you might just need ID pointers to them and leave the details of consuming the products to the various projections.
Came here to say this. Catalogs generally don't contain products though, they contain product groups in my experience? So a product catalog aggregate could contain pointers to product group aggregates (per language if you want that) You could then assign product agggregates to product group aggregates, which in turn would reference product aggregate IDs (you could argue you don't need to store product language here, because you already have it on the product group aggregate level) From Product Inventory Management (PIM) point of view, this then allows you to build very decoupled areas for management - product management, product group/hierachy management and catalog management. This also means you can build product group and product areas first and build catalog area later (since neither groups nor products reference catalogs themselves) making it open for extension. Same could be applied for building price, stock, discount etc. later.
Woah! Util.Diff and the db schema tricks are gonna change my life thanks!
DotNext.com is biggest one as I know, but it in russia. 
... Where did you get the term "gathering initializer" from? It's called the collection initializer.
Thank you! i will poke around, do some projects and maybe do an udemy course! I appreciate the time you took to give an so detailed answer :D 
what is this? teache me plz
The brutal truth is, Docker on Windows sux, even if you're running Windows 10 Pro (as required). Need for constant restarts, because something just hanged up etc.
No, you'll have to check at runtime just like the other serializers. 
[removed]
This is one of those cases where I'm 90% sure I'm misunderstanding the question. But I'm going to give a "wrong" answer to get better clarity on the question itself. The question hand waves away ISerializable as a way to "customize," but that's the typical way to only allow serializable objects. Take this code for example: interface MyInterface : ISerializable { } class Type1 : MyInterface { public void GetObjectData(SerializationInfo info, StreamingContext context) =&gt; throw new NotImplementedException(); } class Type2 : MyInterface { public void GetObjectData(SerializationInfo info, StreamingContext context) =&gt; throw new NotImplementedException(); } class Type3 {} class Program { static void Main(string[] args) { SerializableOnly(new Type1()); SerializableOnly(new Type2()); SerializableOnly(new Type3()); // Error (doesn't inherit from MyInterface which inherits from ISerializable) } static void SerializableOnly(MyInterface whatever) =&gt; throw new NotImplementedException(); }
That may be the case in most catalogs, but my concept of a catalog is already filtered down to the only product group I care about. My specific project is building an ammunition search engine/price comparison tool. I want to import the product catalog from various online retailers, match it up with data I've gathered from various manufacturers, and make it searchable. My current thought process is have the manufacturer data management (manufacturer, type, caliber, velocity, etc.) in one bounded context (may just be CRUD) and another bounded context for the store data, which is event sourced and could generate price history data as well as what is currently in stock. In this case, I'm taking the entire product catalog for Academy Sports and filtering it down to only the ammunition. After that, I still have something like 1500 items that they offer. When I go to do cabelas or similar, I would expect it to be roughly the same. Another thought I've had is comparing the products against a catalog projection rather than the catalog specifically. From there, I would only have to create the new products and load the individual products that have been dropped or updated. The catalog aggregate itsself would be nothing more than a collection of product ids
My knee-jerk reaction is that I hope they don't do something like that - at least not as the official cross-platform GUI solution. I'd rather see something that is more akin to a native solution for the environments they support.
Just this year I got to rewrite a 10yr old VB.Net app. That was satisfying
There are quite a few videos on pluralaight, and you can at least search content without paying to see if it's what you're looking for. 
Oh, I have pluralsight. Do you know the names of any of the tutorials? I've found a ton on react specifically, but couldn't find any that used the visual studio boilerplate.
I think Building a Website with React and ASP.NET Core does. 
Awesome, I'll check that out. Thank you very much!
Tutorials for React or for ASP.NET Core with React? Everything is the same as with regular ASP.NET Core MVC. There is no difference in setting up a db to the app.
It's pretty bad. If you just want to know how to use React, search for Stephen Grider or Maximillian Schwartzmullers classes on Udemy.
I really enjoy CodeMash, 2019 will be my 3rd year.
What's the use case for sourcelink here? Who would want to gather coverage for external libraries? I mean good on coverlet for gaining this feature, Im just interested in how I can abuse this :D
It is? I figured since it doesn't let you choose to add authentication in the setup wizard that there was a special way I had to go about doing it. Thank you!
Rip-off of Raymond Chen's [Nifty trick: Combining constructor with collection initializer](https://blogs.msdn.microsoft.com/oldnewthing/20181127-00/?p=100335)
Oh I love Stephen Griders courses. I was just curious if there ones out there showing it specifically with the setup Microsoft gives us. 
At first, I read it as "Google APIs" and I was like "WTF? This is the end of the world" And then I corrected myself and read it as "Google+ APIs" and I was like "Ok... btw Google+ is still alive?"
Yeah, my connection string is in the appsettings.json file. 
You could make an Roslyn analyzer, but that would be overkill. 
I would love a cross platform XAML-based solution. I’m slowly getting competent in WPF and I love the way you can create class dependant template and datatriggers etc. I wonder if anything more is happening with Xamarin.Forms MacOS 
click the link then you lazy pleb
It can build and run, but not debug, UWP apps. That makes it a no-go for us at the time being since we're working on a Xamarin project targeting iOS, Android, and UWP and, for the core code, it's much easier to do most of the debugging in UWP and then work out the device specific details. They're working on UWP debugging support.
I was really confused as to why this would break Google OAuth. I didn't think the main OAuth provider was actually Google+. Apparently, it uses Google+ to gather additional info: &gt; The Authentication.Google package implements OAuth2 with Google services. However, it uses Google+ to fetch additional user information. &amp;#x200B;
Instead of going through all of the trouble of building your own data table, you could use [dataTables.js](https://datatables.net/). It know it has server side functions, but I'm not a hundred percent sure it can load pages server side. Typically, I just load the data asynchronously and let it do all the heavy lifting for paging, sorting, filtering, etc. You'll get better performance when all of those things are done on the client, at the cost of the initial data loading a tiny bit slower.
I just learned recently you can get the user's email address and user ID from the contents of the IdToken field if you treat the IdToken field as JWT. That was not obvious to me. I was using the Google+ API to get the email address for the user. So now I don't have to use that call anymore. 
What does this have that Postman doesnt? just want to know before installing it :) Thank you
Would have thought typically all that data is in the QS or hidden form elements. Definately not cookies as they hang around longer than needed. 
I am not sure if this what you are looking for but I stumbled on this today. https://azure.microsoft.com/en-us/resources/designing-distributed-systems/ and it is "free" as you have to pay for it with your information..
You should look at this repo. There is all you need to learn with the book provided. https://github.com/dotnet-architecture/eShopOnContainers
On chapter 6 now. Great stuff. You can skip the first 2-3 chapters if you already know basic Docker.
You still have to add basic JWT authentication (plenty of tuts available) then persist the token manually in localStorage or using redux-persist. But everything else is the same, besides the csproj (for deploying) and the SpaService addition in Startup.
Also, quick tip. Delete the ClientApp folder. Run create-react-app to create a new ClientApp folder. This way you know React is up to date.
This repo along with the video that goes with it is amazing. https://github.com/EdwinVW/pitstop (Fair warning, this is a highly advanced subject with a lot of things you need to learn...API Gateways, Event Messaging, Eventual Consistency, Kubernetes)
Cool! Thank you!
Thanks, I'll do that!
Have you tried downloading the offline installer?
Can you provide a link? While searching for solutions I did see that suggested but Microsoft says they did not provide an offline installer for VS Community 2017. 
You can try [this](https://stackoverflow.com/a/46473422/1339826). Barring that, you may want to Google where the VS Installer stores its log files. There may be errors in there that could help you identify the issue.
That's a good point. These types of settings are perfect for model binding with hidden inputs.
I do use a lot of hidden input fields on forms (for pagination at least). All of my links for creating/editing objects are anchor tags though. To pass data between the requests, I am using tempdata. Should all of these be converted to forms instead? I'm not even sure if that is semantically correct 
This is the definitive free resource IMO. Read the book, fork the repo, and start playing around by adding/modifying features, etc. Just be ready to spend months on it. Microservices end up being lots of relatively simple concepts but there are just a ton of them.
I'm not terribly familiar with this space, but I'd look into domain driven design if you haven't yet. If I understand the space well, the architectural concepts used in micro services are basically a union of SOA, DDD, and distributed systems concepts and theory. If there's an expert that can expound on this, I'd personally appreciate it. 
I can't keep up.
This is awesome.
Building Microservices by Sam Newman.
Once you have done some reading and are ready to implement those microservices then you should consider checking out PostSharp. It has many different threading models like the Actor pattern implemented for you so that you can focus just on writing the microservices without so much boilerplate.
Been meaning to read this free ebook but haven't yet so can't say how good it is https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/
I know how you feel. My last gig was my first real exposure to microservices, and they were using Service Fabric. Took months to really make heads or tails of some things. Didn't help the guys who built large parts of it didn't really know what they were doing and massively over engineered lots of things. Intermediate services that didn't need to exist. Using RabbitMQ in the dumbest way possible. Ugh.
I’d love to hear any tips you have for learning. I get the technologies but this is my first time writing an enterprise app with micro services and I’m the only developer at my job. 
You try restarting your computer? /s
As the only developer at my job who is also trying to learn micro services, I second this request 
Your post has been removed. Self promotion posts are not allowed.
If you are the only developer at your job I find it hard to believe that your application would benefit from micro services. 
In think "tempdata" basically just saves a bunch of state to hidden fields doesn't it ? Similar to old .net forms view state? So not much different, just less accessible to the user or JavaScript. QS is fine for this sort of thing, I'm not sure what you mean by making the links ugly. Typically QS values can be seen as "filters" (in a REST model), you a filtering recordsets by page, so it's the appropriate place. It also means users can bookmark are particular page etc 
Consider looking at other existing UI setups. I use the telerik kendo UI and the values are defined in the view but when the page loads it creates an object in JavaScript that is for the control and the value is stored on that object. It can be accessed and changes via a method on the object after instantiation. The downfall of this pattern is that the page size is defined on the view; so we created an HTML helper that has many of the defaults set for us. We have some common methods that happen during certain events for the control that are setup in the helper. It would be simple to write a handler that retrieves the users setting from local storage and update it in the control object. Or whenever it's changed to update the setting in local storage. Local storage exists for a domain until you or the user clear it, you could also store it in the database when the value is changed in the UI so if their local storage is cleared you can repopulate that from the database. Think of the local storage as a client side cache of the database. User claims is also a place you can store things like this. It's built into .net identity though some others also support this. The great thing about this is that it's sent to the server inside of the auth cookie and loaded into the principal's claims so you can read it server side if needed without needing to query the database. The downfall is all claims are sent to the server on each request which can bloated the cookie size so there is some overhead. There are many options but I've realized some storage on the client is helpful but more resliliant data stores behind it are nice
Just try on Microsoft Orleans https://dotnet.github.io/orleans/. They solve a lot of things if you need micro services architecture. 
We use ~50 MB of memory and Postman uses much more. &gt;200MB in my experience. Postman currently has more features but we're aiming to match it as best we can. 
Take a look at [https://github.com/devmentors/DNC-DShop](https://github.com/devmentors/DNC-DShop) \- this is a solution that we've been working with a friend of mine, and recently, we've started recording videos [https://www.youtube.com/channel/UCc3apIciZhgTUw\_kk6C9EJQ](https://www.youtube.com/channel/UCc3apIciZhgTUw_kk6C9EJQ) \- many more episodes will be published in the near future :).
Nobody in their right minds would suggest the \*\*application\*\* would benefit from micro services with a team of one. But if you're in a company where IT is not the main focus, and the project lead loves buzz words, you end up in a situation where you can protest all you want, you're still implementing a micro services architecture.
Postman is made with Electron(which is once again, garbage and RAM eater) while this isn't and much more optimized. 
There is no viable editor for Android. What you can do is have a full machine running somewhere and then connect via remote control. Ether let a PC run at home, or host a machine somewhere (e.g. Azure).
Have you considered any kind of CLI integration (e.g. Powershell) to support continuous integration testing?
Doesn’t mean I shouldn’t learn it does it??
I've seen worse... and that was in C#. The kicker was that not only was that function +/- 2000 lines but that more or less the same function was duplicated *multiple* times (10 times). Confronted the no-longer "architect" who came up with that design and code and his response was similar to "this is fine". Yeah, +/2000 lines functions (x10!) with some if clauses changing here in there in random places in the function's body is fine...
Sorry, I wasn’t trying to discourage learning. Just makes me uneasy thinking about a single developer trying to break up a monolith and deal with all the complexities that go into maintaining micro services. 
DDD works quite well with microservices, cause once you start thinking how to split your monolith into smaller apps, the bounded context tends to be a natural boundary of a single microservice. On top of that, modeling techniques such as Event Storming are very helpful.
No it can't be considered as a Micro ORM
downloaded it just to support you and test it, added a simple get to an app i am running locally and it never finds it. Tried a POST and same issue, 
Sad they’ve chosen an obsolete framework that runs on a single platform.
Popcorn is pretty illegal so I’m not certain why someone would waste their time to build a client for it and then advertise that they’ve contributed to piracy. And why is this any better then the existing popcorn time clients that have been out for years, written by popcorn themselves? I can understand doing stuff to learn but i wouldn’t screw with piracy. 
Yeah this is totally weird not sure why they picked popcorn of all things to rewrite...
Well considering that there is .NET Core and the .NET standard, the supporting libs could easily be ported. Thee cloud based components don't need to change. The main UI is WPF and could be converted to a Xamarin based app or something similar. I'm also assuming that with the WinUI components being open sourced we will soon see some cross platform shim. All that being said, it is still targeting the most popular platform for the time being. 
All good. I know it’s not a good approach for what I’m doing but I’m using it as a learning experience and since I wrote the monolith I know the program well and I figured I could use this as a test project. 
If you want to query the same endpoint for different objects that are all subsets of a larger object, that's pretty much exactly the use case GraphQL exists for. The other being when you want to combine object graphs together (parent/child(ren)) into a single request. It doesn't really matter if it's being used publicly or you're the only consumer, if that's the use case.
Hmm interesting. Do you have postman or another rest client installed? I'm curious if they cannot reach your local server as well. 
A bigger argument for using DTO's is for protection from over-posting attacks. For that reason alone, a lot of projects I've worked on have mandated DTO's. The downside to this approach, in my experience, has been code explosion (even with Automapper, if all of your models and entities are not simple 1-1 mappings you will have a lot of extra code). So IMHO it's a trade off. I have projects where we use DTO's with AM as well as others where we just expose the entities (via OData). One last thing I'd like to point out from a security perspective: You are ALWAYS exposing a public API. If you have a web page that makes and http(s) request to an API, then it's public. If you don't believe me, open up a browser window to your site, watch the network traffic (F12 in any major browser). Any of those http(s) requests can be modified and resent from another browser window (or any program that can make http requests). It may seem pedantic, but, from a security standpoint, it should ALWAYS be taken into account.
Late answer, but I would investigate a quick SignalR app to do this. Th client and hub structure of SignalR is pretty much exactly this. [https://code.msdn.microsoft.com/windowsdesktop/Using-SignalR-in-WinForms-f1ec847b](https://code.msdn.microsoft.com/windowsdesktop/Using-SignalR-in-WinForms-f1ec847b)
Thanks for the suggestion. Unfortunately I made a simple c# wpf application to test it on my work pc. It wouldn't run it, as it tried to access some HP file on our shared network drive.. (I did the build and copied the bin files to run the .exe file)
Looks hot. Anyone used it in production anywhere?
Try looking at the code for youtube-dl. Using `youtube-dl -g [URL]` will output the direct link. I don't know about a C#-specific version, but it might exist.
To be fair to the project, it is an absolutely beautiful and robust WPF app, regardless of its legality. On the note of legality, its also kind of a grey area as the app itself isn't illegal, just what user's choose to do with it. You are allowed to "pirate" content you already own, and the app could easily be used for legal things... even though that probably only accounts for .1% of users.
I will certainly accept that this dude knows what he’s doing in the WPF space. Just not sure why I’d pick it out from the official ones. This thing only does one thing: play movies from torrents. The second you do that it’s illegal. Since it has only that one function, it’s certainly not something I’d want to put my name on.
postman has no problems at all, i extensively use postman but decided to give yours a try few notes fro me 1. Cant reach any local host stuff (local site is running through httpS if it matters) 2. You should start to make this free with premium features until its popular then you can charge for it, IMHO 3. I should be able to add a request directly to a collection with right clicking (easier)
Hot damn, that's a beautiful WPF ui. Great job.
Maybe he only knows .net? Maybe he’s trying to learn wpf? Maybe he just likes coding for windows ? The guy made something cool and is sharing it; there’s nothing sad about it.
I have some decent experience with high-traffic, multi-layer, distributed caching in .Net. A few things stand out right away. You want to be able to run your pub/sub on a different Redis instance than the one you actually store your data in. In my experience, pub/sub eats a lot of CPU. It will start to hold up your actual data requests from getting processed. This library does not provide any way to support that. Another small point on pub/sub - using JSON is needlessly wasteful. Cache is a backbone component. It is not needless premature optimization to simply pick a nearly functionally equivalent but significantly more efficient method from the start. The author has neglected to pass the cancellation token in to other async methods within his functions. That's a bug, not terrible, but obvious. My last point of contention is an artifact of implementing the `Microsoft.Extensions.Caching.Distributed.IDistributedCache` cache interface, but I think it's worth bringing up. It's dealing with everything in byte arrays which just kinda makes interfacing between this and your application code awkward, and introduces needless performance sinks. You're going to have to layer on top of this and convert everything else to and from byte arrays - int's, string's, Guid's, object's. That means every time you get a value, you're going to have to deserialize it. Now, that does give you built-in immutability on your cached data (and if you don't have immutable stored cache data you're gonna overwrite your shit and cause bugs), but it's a really wasteful way to do this. This caching library is really not doing much for you at all. Even if I wanted to use it, I would lift the parts out that were worthwhile and put it behind `Get&lt;T&gt;`, `Set&lt;T&gt;`, etc. and I'd store regular CLR objects in the memory cache instead of byte arrays. I'd also add other helpers like a `GetOrSet&lt;T&gt;` and `Remove(IEnumerable&lt;string&gt;)`. Maaayyyybe for objects - if you've used a highly performant serializer - I would do byte arrays in memory ([NFX Big Cache](http://nfxlib.com/book/caching/cache.html) does that, but I really don't like their serializer or the app framework they tied into everything - [MessagePack](https://github.com/neuecc/MessagePack-CSharp) is a better binary serializer). UTF8 that's going to get output directly onto a response stream I'd store as byte arrays. Images...
This guy caches! ^^^^ Thanks for detailed reply. 
.NET Core 3.0 includes WPF so I'm sure it was made with the future in mind
It would've saved me hours in the past week alone. Can't wait for it to become stable!!!
Torrents are not inherently illegal
The real tragedy is that resource hogging Electron apps have become ubiquitous and are posing as "desktop" apps.
 
Haha yes he does 
I guess I’m concerned because of inexperience more than anything. Each catalog will be loaded a few times a day at most, just when the catalog gets updated. I’m not familiar with what would be considered too large for a snapshot. Would deserializing 5000 items in a seldom run side process be cause for concern? Probably not. I think all of the user facing actions would be based off of projections which would be much more optimized for the purpose. 
My system serves about 1.5 billion requests and over 200 TB of content each month. Lots of highly dynamic data coming from 3rd party sources. It's not huge, but it isn't small. Presents plenty of interesting challenges :)
Feel sorry for your eyes! Not sure how you pull it off. But as an alternative - why don’t you get some micro pc (like inFocus kangaroo pc that run Windows) or stick pc and plug it in available screen (all you need hdmi port and a (any /usb / bt) keyboard. They would not be speed champions but should be enough for running and editing in local and of ours take sd cards so you can keep all data in local. Intel NUC units also can do the same. Maybe Amazon has old recondition laptop or win tablet that would do the trick as well if you really need it - I have seen old surface refurbished for decent price. You would go for i5 models etc. This was just an idea. Kind Regards Alex
Hi SingidunumAussie (love your username ;-) ), I think there are two layers of this - the logical architecture - how do design and then split by redesigning a monolith application into micro services. The second layer would be now that you know the future logical design how you implement it using available technologies (all good stuff mentioned in the post below. Keep in mind logically application can be implemented as a set of micro services but deployed as a monolithic deployment and still have some benefits for micro services and benefits of lower overhead because you don’t need cuz on the day one. However the way the market is going I can see jobs demanding more and more some of the Skills such as Docker or Rabbit MQ etc but this you can learn independently depending on the posts. Now the logical level how do I re design an app to be broken in micro services... Let’s start with a typical problem most of the applications are a 3 tier applications that are sharing a single database (or even masquerading as multiple web services but again coupled with the same dB and violating the autonomy principle for soa architecture). So on logical level you want to figure out how to break the existing monolythical application in a number sub applications (micro services) that are very loosely coupled. In my experience that is debatable is through Domain Driven Design, event Sourcing and CQRS (when combined often called distributed domain driven design) all of it Event Driven Architecture style. To me is the key in DDD. Bounded context helps you define the boundaries of your services. aggregate Roots gives you the units of your consistency (in plain English what would be the smallest subset of your tables in your per micro service dB to avoid gigantic dB with kitchen sink included). Event sourcing helps enormously with persistence design in some situation and in general events can be mechanism of keeping micro services together without ending up with a massive ball of tar of coupling. Having said above. In terms of resources for dddd - My favourite if you can find - the distributed systems podcast (Rinat Abdulin, Jonathan Oliver, etc) can’t find the link now. 
[removed]
Your post has been removed. Self promotion posts are not allowed.
I don't want to use claims to store this data. I do have some custom claims I introduced (I am using Auth0). I'll look at Kendo UI to verify if that is something I want to start using. Thanks for the help!
Sorry should have stated that these were links for creating/editing objects. For pagination I agree that QS is fine and I do make use of it. When I navigate to /meeting/create I don't want the QS to contain filters, page numbers, and page size. That is why I was using tempdata when navigating to those specific places.
&gt; This project and the distribution of this project is not illegal, nor does it violate any DMCA laws. The use of this project, however, may be illegal in your area. Check your local laws and regulations regarding the use of torrents to watch potentially copyrighted content. The maintainers of this project do not condone the use of this project for anything illegal, in any state, region, country, or planet. Please use at your own risk.
As someone who has a Raspberry Pi, I do like the prospect of a mini computer. That being said, I'm fairly poor so it's rather impractical. I like the way you think though!
Great suggestions. I'd add [NDC Minnesota](https://ndcminnesota.com/) (May 6-9) if you're looking for less Azure pitch from Microsoft and want to peek beyond the .NET stack. If you'd rather stay well within the .NET/.NET Core/Azure stack mainstream, then [DevIntersection](https://www.devintersection.com/) and [VS Live!](https://vslive.com/) are two popular multi-city conference series.
WPF will never be cross-platform, even on .NET Core 3.0
True but conferences serve the purpose of forcing you to allocate time to learn about things that you'd otherwise be reading about for months and years, between actual work and procrastination.
&gt; A bigger argument for using DTO's is for protection from over-posting attacks. For GraphQL you must define the exact mutations (POST/PUT) which gets handled on the backend. This doesn't allow overposting, due to the built-in schema validation and then your business validation on top.
We have two projects where the Backend API is using GraphQL for the client frontend and admin frontend. We could have easily gone with the normal REST approach with DTOs and AutoMapper, but we decided not to for some of the following reasons: - Mainly the frontend (team) has an easier time working with the API. For SPAs client libraries like Apollo with their built-in network layer allows each component to define the data they want. The network layer handles aggregating and Apollo itself caching and state management. This allows us to think even more in terms of "Components" and we don't have to make a GET Request and via some other state management share it to other components. - In the backend, you have one truth on how for example "User" looks like and don't have a "user(short/long) endpoint or provide a query param for "include/filter" properties etc. - The backend developers participating in this project all said that GraphQL is faster (with own abstractions) to develop with. - Apollo Codegen works well. However, * for the GraphQL .NET Lib we had to create lots of better abstractions and essentially wrapped the library to provide a better consuming API. * Apollo is riddled with bugs for even the most basic things (e.g. try the loading flag example from the docs in Angular or different network policies) where when you track these issues down in their repo, almost all of them are closed with active discussions. When you look at the pull request which supposedly fixed it you see no test coverage for that and the developer often bluntly states the this "should" fix the bug. I mean if you have no test coverage, maybe just ask the issue creator to verify it. * You have to use the data loader to batch the queries * Nullability by default is really an annoyance, we maybe create an opinionated version which inverts this behavior. (null | (null | item)[])
Is your postman settings set up to disable SSL validation? My postman cannot reach my local host unless I disable ssl validation. The reason I ask is because Nightingale currently defaults to enable ssl validation and I currently don't expose the option to disable it. If your postman has ssl validation turned off, I'm wondering if you can try turning it on and then try connecting your local host again. I'm curious to see if the connection goes through. 
I was just throwing claims out as an option, I actually wouldn't go down that road either. Kendo UI has a free version but also a a paid one and it may be the best solution for you. I was just suggesting if you write your own controls to look at another control framework to model yours. jQuery UI also has a free UI set you could use as inspiration in your design.
It isn't included. It is a separate library to be used for Windows apps written with .NET Core.
What kind of hardware is backing this ?
How exactly are you using it? Which Docker installation do you have in mind?
Just use sftp there are tons of examples online. Use python and automate it.
The thing is the the way my group member set up the raspberry pi cluster, he made it so to be able to connect to it externally we have to use the remot3 it service to get the external IP to connect to (say using winscp). That external address changes constantly making it hard to automate at this point 
By default postman has SSL Certificate verification off (I just installed it, no setting changes). Hope that helps u, i guess back to postman it is for me
Hi, author here! We are currently pacing it through performance testing with intent to release in January-February. For our specific case, we have IdentityServer4 spread across a few webservers. Cache reads are on the order of 5-20 per authorization request and deferring to Redis was very costly even just in terms of latency. The pub/sub is very lightly utilized, as evictions only occur several times a week in production. Lowers environments are another case though. However it is highly important that any eviction is as immediate as possible.
My overall intent with the library is accessibility. I really could not find something that did L1L2 at a basic or even introductory level to even prompt someone to consider it as a solution. As an inclusive solution that is both better than RedisCache and nudges someone in a direction that may solve their specific problems, I'd consider it effective. JSON serialization definitely could be better, but I initially preferred to defer any serialization work to see what advantages the serialization changes in Core 3.0 might provide in terms of both performance an accessibility. The performance of eviction messages was not a priority as in my case they were infrequent. Though there is always an opportunity for performance enhancements, the burden of knowledge associated is an important cost to consider. Based on caching needs, the performance gains from this over just a RedisCache can be substantial and were absolutely in my own case. 
I think Orlando FL has them too. At least I went to one there quite a few years ago during a Windows 8 conference
Out of curiosity, what was your take on [CacheManager](http://cachemanager.michaco.net/)? 
To add to the list, InfoQ has lot of good quality content about DDD. Check book by Vaughn Vernon and any talks by Greg Young (the inventor of cqrs pattern). Eric Evan’s book is good but if you read it, he said in his talks that he regret that he has not started from the bounded contexts first (grasp this concept first).
WPF isn't obsolete at all. 
WPF is open source, we dont if "never" is accurate
This link might help https://docs.microsoft.com/en-us/visualstudio/install/troubleshooting-installation-issues?view=vs-2017
Docker on Windows. WSL connects via the API on Windows. Docker does t run on WSL but you can install it to issue commands. But as I said it’s a long winded approach 
For me it's having the same feature set on Mac and Windows, I develop and switch to both platforms often, and VS for Mac is just atrocious. Also Code Vision. Same thing as CodeLens without the $1200 price tag.
Replace action="/action\_page.php" with runat="server" So: &lt;form class="modal-content animate" runat="server"&gt; Use the code from here (to get it working initially): [https://www.w3schools.com/howto/tryit.asp?filename=tryhow\_css\_login\_form\_modal](https://www.w3schools.com/howto/tryit.asp?filename=tryhow_css_login_form_modal) Make sure the 2 asp textboxes have unique ID's. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Without seeing your code I doubt anyone is going to have much insight into why it is misbehaving. I know it isn't what you asked about, but I'm curious why any school is teaching ASP.NET Web Forms at this point. That way of developing web applications on the .NET platform hasn't been popular for about 10 years now. To be blunt: if this was the most modern web development on someone's resume, I wouldn't hire them. Not because the technology isn't in vogue anymore, but because that entire model of thinking about web development isn't, either. The world has moved on. If you like .NET, go learn yourself some ASP.NET MVC 5 (technically out of date but still relevant) or ASP.NET Core. Stay as far away from anything `runat="server"` as much as possible. I'm having PTSD flashbacks just reading your question, lol.
Some simple things to check would be is the modal related Javascript reloading when the modal loads for some reason? Also does the page post back for any reason when the modal opens? Maybe the button that opens the modal is a runat server button and is posting back? If you could post some more of the pages code it might become more apparent. 
That tutorial is for PHP, not ASP.NET Webforms. Can you paste the code you have? 
Your modal button is probably of type submit and submitting the form. 
I know! I'm not a fan of Web Forms too but I don't really mind studying it now because it's just High School. Why do they still teach it? Good question!
https://pastebin.com/tAUTudMJ I've tried to convert the input to ASP.NET and I think it should work
https://pastebin.com/tAUTudMJ Here it is. I don't think anything makes the page post back but I'm not sure. As I said it only happens when I remove the "required" parameter from the textboxes
It is not
That helps a lot. Thank you. I actually have a new update almost ready for release. It contains option to turn off ssl validation. I'll let you know once it's available so you can try. Sorry for the inconvenience. 
We did something similar using dot net core, ef, angular front-end. Approximately same code base for each client. Each client gets their own solution with their own project and client specific code. Otherwise we have a “main” code base that gets any updates first before we add the updates to the clients. Each client gets their own database (in a container) and their own docker containers running their infrastructure. We manage the Servers, deployments, etc. Not sure this would work for your project but it seems to be working for us. 
Are you able to split the application into microservices e.g cms, front end, api. This may make it easier to scale inividual parts to meet demand. When you say the front end will be a spa for each client, is this off prebuilt templates or they are uploading code into the cms? Depending on your data model there may be some join issues at scale with a sql dB. It may be worth thinking about using cosmosDb.
This was my final 'solution' as well (more or less). If this could be automated, I don't really see any issues. The only downside is that the infrastructure will cost more than if you could share resources. 
I suppose that is possible. But isn't it raising issues when there are 1000s of clients? If you could scale a single client, it would be easier to manage that, right? If there are a lot of big clients, you will reach a limit in terms of scaling 1 microservice, no? Frontend will not always be a SPA. All client websites are made from scratch (or based on a template, but a low percentage of them). I guess that also depends on which data solution is chosen.
If all are made from scratch I would keep them separate. Why do they need to share a database? Is it just the cms code that is shared between them? You could create some powershell scripts to spin up the resources using ARM templates and pull the latest code from the repository and create a DB. 
There is no need to share the database. But it's a possibility (tenant_id in all tables). And yes, only the CMS code is shared. Yes, automation shouldn't be a problem. Thanks for your input. 
Sure. Poster above asked about using it in production, and I gave an opinion that goes beyond basic or introductory level, as production environments generally are. Frankly, lots of data doesn't even need the Redis layer and would be fine with just an in-memory cache. If you're deploying in the cloud, most other services from your cloud service provider - barring a complex query - are going to be just as fast as Redis. The network hop takes the majority of the time. This is likely where your performance gains came from, and depending on your source data you may be able to remove Redis with no ill effect. Watch out for that pub/sub CPU usage though. That will bite people with more frequently churning data. Also you should pass the cancellation token into all the other async methods you're calling.
We're on Azure and our main web service is three Standard_D13_v2 VM's - 8 core, 56 GB ram. Sometimes load brings us up to 4, and external service degradation can bring us up to 5. We also have a Standard_D12_v2 VM running recurring jobs. We use three Standard C4 13GB Redis instances for large data, two Standard C2 2.5GB instances for small data, and one Standard C2 instance for pub/sub. Premium P1 SQL DB for main data, Premium P2 SQL DB for log data. We also have an App Service Plan with three instances of Standard S3 - 4 core, 7GB ram that's using most of it's horsepower dynamically resizing/cropping/etc. images. It also runs our public web site and various internal web apps.
&gt; However it is highly important that any eviction is as immediate as possible. Keep in mind that Redis sub/pub does *not* guarantee delivery. It may not be appropriate for your described use case.
Take a look at tenant architecture for asp applications. Then read up on database sharding; it may be too much for what you have now, but it's a lot cheaper than hosting a database per client.
I've read about that here: https://docs.microsoft.com/en-us/azure/sql-database/saas-tenancy-app-design-patterns#g-multi-tenant-app-with-sharded-multi-tenant-databases This is more or less the idea that we have. Will investigate some more. Thanks! 
Nice we linked the same document haha. We're slowly transitioning to it as we build out our micro-service stack (1 multi-tenant sharded database per service) at work and it has made everything a lot easier to maintain and far cheaper infrastructure costs on azure. The learning curve is a bit steep to start, but it's definitely worthwhile in the long run. Good luck!
Sql sharding is fantastic. It really does make these situations way easier to maintain
Check out saaskit: https://github.com/saaskit/saaskit
What is probably happening is the textboxes being required may make them validated against, try to return false on your js like this answer : https://stackoverflow.com/questions/683746/how-to-disable-postback-on-an-asp-button
So if you have asp textboxes with required true it works? You can check if the page is posting back by putting a breakpoint in your code behind, you can also check if(isPostback) 
I lucked out. They taught me JavaScript in high school... well, probably before you were born… and it's still relevant. Most technologies don't enjoy that kind of staying power. Web Forms didn't.
Depends. If your project might be relevant to legacy projects then yes. New projects will most likely target newer versions of the framework.
Well do you have any projects that are stuck on .NET standard 1.3?
Sure, we have at least one. There's no stopping us from upgrading. If your project is using new technology like machine learning then it might be worth it. I'd just focus on 4.6.2 if not.
I have never cared about neither Standard nor Core. I do not want these *lowest common denominator* runtimes and will stick with the Framework.
I'm supporting that too. In fact, I currently write everything for .NET Framework 4.6 and then cut features until it fits into .NET Standard. But that won't last forever. With .NET Framework 4.8 not supporting the full version of C# 8, it's pretty clear that I will need to change focus in the future.
I'm confused. How can it be stuck on 1.3 if there's nothing preventing you from upgrading?
The .NET standard _is_ .NET framework (on windows). Why do you like the .NET Framework so much? Have you seen the improvements done in the .NET core lib? For instance: Span, Memory, and performance improvements throughout. How can you not like that? Not to mention cross-platform support.
I personally think that Core is the future: Much more rapid deployment, open source, cross platform, and generally a bigger interest from the community. The latest performance updates (in 2.1/2.2) are pretty mind-blowing as well. The .NET framework also has loads of legacy code, which can simply be "cut" in Core. You also have self-contained deployments, which require "no framework" to run at all (it's bundled).
Depends, what product are you working on? Do you think people will use it in environments not supporting .NET Framework 4.5+ or Core? Generally, I'd say, target .NET Standard 2.0, it has most desired goodies.
I'm really looking forward to single file WPF apps. Right now I'm shipping way too many files compared to what the tool does.
Lazyness and caution. It is a huge product and we can't test everything. Who knows what might break? Lazyness is probably the primary reason.
I'm working on a data modeling library (think MVVM but not tied to a UI), an actual MVVM library, an ORM, and some other random stuff. But I want to keep the discussion more general to see what people are actually using. 
Ok, that makes sense. 
&gt; Why do you like the .NET Framework so much? I wouldn't say I "love it", but I have been on projects where it would have been a lot easier to not be on Core site to missing libraries. I expect this to get better with time. 
I'd say it already is better, but I'm working almost exclusively in high performance applications, and web development. A year and a half ago, I'd say it would be a bit early to adopt it for (some) business purposes, but as it currently stands, I think it's very mature.
Yeah, that's the current caveat. I think cross platform IL-merging is in the works.
Often it is some random thing that trips me up. For example, bar codes. A solved problem in .NET Framework, I had to try out several broken libraries for .NET Core before I found one that worked reliably enough.
It's hard to keep the discussion "general" as different fields vary hugely. I.e. compare a library that has to be compatible with Windows forms, vs say a web app, vs IoT.
No
Does it really? I haven't seen anyone say that their field is stuck on an old version of .NET standard yet.
I'm building something very similar. Im using the microservice architecture. Each customer/website gets its own database (not each service, they share the same database but don't know about the other tables etc.). This databases also contains the login data etc. The user has to fill 3 inputs: access token (one alias for the database, for example the customers company name) username &amp; password. With this informaton i can build seperate connection strings for each request context. I also have a service for database managment. In this service i keep care of the database names, execute sql scripts for the database, manage aliases etc. &amp;#x200B; In the CMS i have a "Page Designer". With this page designer im able to build individual input pages for my customers. The designer can be extendend at your needs. In the database I store some information about the input type, of course the value(s), some properties about the input fields etc. The input information from this pages can be queried from the cms api by the customers website. Each input is able to specify a "Content-ID" which can be requested. All Inputs with the matched Content-ID will be build up to a single JSON response. With this i can query information across custom pages, aslong the inputs have the same content id. &amp;#x200B; All this features are designed so that any database is able to host the licensed microservices. My own database is just like a normal customer database, except the activated licenses are different. &amp;#x200B; All together is just one solution for every customers page which just need individual designed cms pages by the page designer.
Oh wow, I wasn't aware of the backplane component for Redis operating in a similar way. This is really cool! Very interested in the bulk message publishing/subscribing, would be pretty awesome to evict multiple entries with only one message. I'll set up a branch to fit in during performance testing.
Damn I didn't know wpf could look that good. I've worked with it for some small business applications, but never had a chance to make something good looking. 
Most SaaS don't do dedicated hardware. Makes no economic sense and is overall much harder to manage/ scale. A good middle ground is shared application / web tier, then dedicated database/ storage per client. (Note:database, not database server) So you basically end up with an autoscaling app/web tier, then pools of db servers housing 10 or 20 clients on each. Basically just means your app tiers data layer needs to have access to a Conn string per tenant. This also allow you to offer "dedicated storage" with minimal customisation if you get one client that demands more segregation. Basically just stick their database on a dedicated server.
Tenant_id column segregation in SQL db is bound to hit scaling issues eventually. It also makes it way to easy to stuff up a query and accidently show cross tenant data. Database per tenant scales better and is much safer/ more secure. (Database, not database server) 
Fine setup but this is a case where you should have looser coupling with the CMS system. Don't just share the code but have the CMS and data be a single but separately running service accessible via HTTP or whatever API interface you like. Then all of your client sites can talk to that API using JS, C# or whatever language. This is similar to using "headless" CMS systems like contentful where you just get an API for all the data and access it however you want. This makes upgrading easier since you only have a single backend service to worry about and can version the API if you have major changes, and client sites and apps can change on their own while using the API for all data.
Although it doesn't feature seperate websites per tenant, have a look at aspnetboilerplate.com
If a school is still teaching Web Forms in this day and age, I would seriously question the credentials of the school. Web Forms is dead tech. Useful to know if you need to maintain a legacy app, but no one with more than two functional neurons to rub together should be starting a greenfield project with it.
I'm not a big fan of electron either. Please do let me know what you think. It's still new so any feedback would be appreciated.
Haven't thought of that yet but that sounds like a cool idea. Out of curiosity, have you tried doing this with curl or another powershell/script-based http client? 
They probably still teach it because the still have the materials for it and no one has the time to write a completely new lesson plan. Ask the teacher if it is ok to use a newer technology? Maybe you can do the projects in something like people have been recommending. 
Unless someone else is preparing to make it cross-platform it is currently "never". It is the official position: [https://github.com/dotnet/wpf/issues/48](https://github.com/dotnet/wpf/issues/48) &amp;#x200B;
It might be worth keeping a .NET standard 1.6 target. While the tables show .NET 4.6 supporting .NET standard 2.0, it was done retroactively and not very successfully. Immo Landwerth on the .NET team has said .NET standard 1.6 is best for targeting .NET 4.6
Once again, client side evaluation is not necessarily a thing to be avoided. Working out what you should or shouldn't do in Sql vs the application is an intermediate to advanced topic, but telling beginners that they should never do it is not helpful. 
Client side evaluations are a code maintainability issue. Luckily EF Core gives us tools to detect and fix them during development. Take this line of code from the article for example: var contacts = await _context.Contacts .Where(c =&gt; String.Compare(c.FirstName, "D", StringComparison.Ordinal) &gt; 0) .ToListAsync(); With EF and EF Core by default this will silently client evaluate, but it wouldn't be at all obvious from reading the code (particularly if you don't know every single possible edge case, like which String.Compare() overloads aren't supported). Just to be clear there's two different issues here: - "Client side evaluations are bad!" which is simply not true (it is up to you to scope them) - "You're creating difficult to maintain code that mixes client/server evaluations" which is absolutely true In EF Core luckily you can configure it thusly: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { #if DEBUG optionsBuilder .ConfigureWarnings(w =&gt; w.Throw(RelationalEventId.QueryClientEvaluationWarning)); #endif } This will act as a Assert, throwing exceptions when you stumble into inadvertent client side evaluations. In production, due to performance and the fact that this exception is unhelpful, it isn't used. If client side evaluation was **intentional** simply refactor the code thusly: var contacts = await _context.Contacts.ToListAsync(); contacts = contacts.Where(c =&gt; String.Compare(c.FirstName, "D", StringComparison.Ordinal) &gt; 0) .ToList(); This won't throw, and makes your intentions clear for people maintaining the code. 
No, just create a data access class with methods for your sql queries. There's no point in including extra overhead like EF.
I like EF because it also allows you to version control your database changes via migrations, similar to Liquibase.
You can execute stored procs, it's not really it's best use case. Currently we use Dapper for reads and EF for writes. It simplifies transactional updates spanning numerous tables, you just give it the object graph and it sorts it out. We do use SP's and some hand written SQL too. It's whatever gets the job done well. Table joins are just object references. You have an Order object and a OrderLine object (and an Order table and an OrderLine table). You add OrderLines to the OrderLine collection on the Order and EF just sorts it out. There are 3 main versions of EF currently in the wild. The .NET Core one, 6 and 4. 4 hangs around because it was the last version included in the framework and 6 is the latest .NET version. I don't think it's getting much love any more, it is mature though. 
Thank you. I actually did this same thing after reading this, but did the configuring in my startup using a simple environment if statement instead of the debug pragma in my data context. Probably just personal preference.
I'm gonna say do learn it if you have the time and willingness to do so. It's another tool in the toolbox that's extremely indispensable given certain circumstances. When you want to do large scale projects with complex, expressive domain model, you are inevitably going to work with full featured ORMs. Also, knowing and using "big" ORMs can speed up your development cycle if you know their in and outs, even if you end up eventually using hand-wrritten queries instead. As a footnote - CQRS using Dapper/PetaPoco on read side and EF on write side is a nice "template" to slap at almost anything. 
[Insert Admiral Ackbar's It's a Trap here!] There's a handful of legitimate benefits to EF. - Unit of work pattern by default - Database agnostic (you aren't writing queries, you're asking database specific query generators to write them) - Compile time checking which can catch several error classes (you'd need unit tests to check raw SQL queries) - No concatenating raw queries which can be difficult to maintain (e.g. optional where clauses on search pages with multiple fields, etc) - Secure out of the box (even the idiot on your team cannot concat a client provided search string, everything is parameterized) - Benefit from IDE/code tools, like Refactor -&gt; Rename, to change a Table's name throughout the codebase (as opposed to search/replace for raw query strings). - Multiple hooks, such as logging where useful resources like trace recording, performance tracking, etc can be integrated. - You get to write LINQ. I really like Dapper and I have EF and Dapper in most solutions, they both serve a purpose. You can also re-use EF's connection string in Dapper since they're both ADO.NET backed, making mixing them super pleasant. 
EF Core simplifies a lot of the stuff that made old EF complicated. You can execute procs and custom scripts easily, data shaping and default mapping means you don’t have to write custom stuff. There’s a shallow learning curve. The upside: you can express a database in code that is exactly what you’d do in DDL, and use the migration tool to create it. You can express your logic in LINQ, so it’s all in one place. The queries generated are sensible. You can unit test your data layer, if you have anything more complex than crud. The biggest upside: you aren’t married to a database technology. EF has providers for SQL Server, Oracle, SQLite, pgsql, and numerous others. Now, if you are not a .Net guy and only really do database / etl / report development, pursuing EF would a bit outside of that particular career path.
You make some solid points, thanks. It sounds like I should learn it when I get time, but I shouldn't really make it a huge priority. 
It is 100% worth learning. If you can get your employer to pay for it pluralsight has great videos about it. You can use it with stored procedures and views.
While I was hired for database, ETL, and report development, I've created several .NET Core applications for facilitating automation of my job. Most recently, I created a web portal in ASP.NET Core to allow other employees to do a lot of my manual data tasks. Every day I seem to be using C# more than SQL in the spirit of automating my job and living my ideal lazy life. I haven't actually tried EF Core yet (last time I tried EF was before Core was a thing) so you've convinced me to give it another try (when I get some time) :) Thanks!
For some reason I cannot comprehend, my personal e-mail is attached to the MSDN account I had with my employer from three years ago and it looks like they're still renewing it, so I think I have access to a few months of that for free. That sounds like a great use for it.
Another vote for EF. I know ORMs aren't perfect, but for 95% of my use cases, they speed up my development and maintenance time significantly. I have used EF Core for dozens of enterprise apps over the past two years, and it has served me well. That being said, there is a significant learning curve to EF. You will find yourself frustrated with how objects are tracked, lazy vs eager loading, configurations, client side validation issues, etc... Just push through. I have had a few developers dabble with it briefly and give up in frustration. But there is a moment where it will click, and then you won't go back.
No. Don't waste your time. Dapper is much, much better.
Sometimes optimization is meaningless and there isn't a use case that demands custom queries over challenging schema or data set sizes. Sometimes you have a very straightforward CRUD problem and smacking it with EF, scaffolding out a page from it and calling it a day is a very effective way to deliver something extremely valuable. If you do not have such trivial problems I'd suggest you continue doing what you are doing. I've never questioned a potential software engineering applicant for not having an ORM on the resume, but I always question when it seems like an ORM is the only data access pattern they know how to use. 
&gt;It’s mature and stable, so moving to a new version is a smooth and efficient process. Complete opposite of my experience. I've had an incredibly frustrating time with Xamarin, with hours upon hours that were spent on mystery errors that then somehow got resolved once my pc was rebooted. To me it feels like I'm programming in the dark. Also the tooling is very, very lacking in comparison to for example Android Studio. I gave up after a few months because I simply got way too frustrated with the whole thing. However I still wanted to build a cross-platform app so I tried out Ionic, which I like a lot more. I'll admit I was already used to working in Angular, which lowered the entry barrier greatly. In my opinion Ionic is a lot easier to use once the initial setup is finished. Especially when you want to build an attractive interface with (for example) animations.
I like EF a lot but ORMs are slow if you are used to raw SQL queries. There are tricks to make EF performant but that comes with experience. You can also save some development time by being able to scaffold all your models in EF.
https://www.learnentityframeworkcore.com is a great resource for beginners. I am the opposite of you, I don't have a lot of SQL experience (just know some basic commands) so EF is great for someone like me who doesn't have the time.
Yeah I think EFCore is widely used and it's easy to learn.
You should be able to use the `Authorize` action filter with the `Roles` property set to `Admin` on the controller or controller action that you want to protect. No need to write a custom ActionResult (which you very rarely need to customize). Here is an example: https://docs.microsoft.com/en-us/aspnet/core/security/authorization/roles?view=aspnetcore-2.2#adding-role-checks The above is for core, bit the syntax is the same for full framework.
You all are right. Client-side evaluation isn't bad. It was a source of pain that prompted this post since it isn't clear when it is happening. Sorry if the post came across as it is always bad.
For my projects I'm still supporting 4.6 explicitly, but if I wasn't I would certainly do as you suggest. 
You can use both Dapper and EF in the same project. I have. EF is really good at keeping your models in sync with the DB schema. It is also great for inserting or updating a small set of entities/rows. Dapper is great at queries and SQL Server specific features. You can also let EF manage your DB connection: _efDbContext.Database.Connection.[DapperMethod]
Maybe this is a side effect of our ignorance and lack of experience with Xamarin, but when we used it at work it was a pain. The concept of Xamarin seems great, but in practicality my experience with it was: - Stuff would compile and test fine for me, I'd push it up, colleague would pull changes down and couldn't build. Same version of VS and the emulator. Ended up being seemingly random issues with the tooling and having to delete object folders. - I would find a lot of out-of-date or inconsistent information - I would find I needed to get third party plugins (or roll my own) for seemingly simple things you would think would be built into the framework (like linking to other apps, opening links to a website, network connectivity, etc.) - From the perspective of Xamarin.Forms, customization at the time just seemed... extremely lacking. Granted, I wasn't the one doing the actual UI, but the person doing, that was his main complaint. I mean it seems great, but it doesn't seem nearly as mature (at least from a Xamarin.Forms perspective) as this article makes it out to be. **That being said** I am hoping with Microsoft heading this up now, and their push for cross-platform, they will invest on improving Xamarin.Forms and the customization around it. I'd love to love Xamarin, but my first impressions of it sucked.
Learn everything you reasonable can. You have no idea what you'll need in your next gig. Learn on small throwaway or internal projects. Never learn on production code if at all possible, you're first three tries will suck. The next three will to just not as bad : )
EF + LINQ is very good for shopping cart databases. For any serious database work use it only if your customers have plenty of time or your database sever has poor SQL engine.
I keep seeing Database agnostic, but in my experience there are separate nuget packages for the different databases. How does that work? Wouldn't you need to compile against a specific database?
&gt;I gave up after a few months How long ago was this?
I'm not sure you understood my problem but maybe I'm just oblivious. It's not as simple as it sounds IMO, that's why I used Sql queries. I can't just admin authorize an index/edit or delete ActionResult(or the whole controller) because as you can see the table AspNetUsers that is used for register/login is separate from the AspNetRoles and AspNetUserRoles and it has totally different properties. You can see what I wanted to do in the Sql query and before it, the admin authorization syntax. I'll try to make it clearer, english isn't my first language. I wanted to show a list of users with a simple username/email/role columns for the admin to see and change. The thing is it needs to update in the AspNetUserRole table as well.
Didn’t the folks that invented dapper decide to move their stuff to EF Core?
Like I asked the other person how long ago was this? It has come a long way since Microsoft purchased it. Any experience prior to early 2018 should be considered out-of-date. 
About a year ago now.
Xamarin from a year ago compared to today are basically two different products. It has come a long way in the past year.
You would need to update the provider package and the connection data in Web.config. But that should be all. Whereas with Dapper, you need to review and possibly edit each and every one of your SQL queries as well. With EF the engine takes care of that for you. It’s probably more useful if your product needs to support multiple databases simultaneously rather than switching from one to the other, admittedly.
&gt; But that should be all. With EF the engine takes care of that for you. But it's not, and it doesn't always. This is one of the common places the ORM abstraction leaks. You cannot blindly assume your queries will work the same or as efficiently from one engine/provider to another.
No. They're using both tools simultaneously. https://blog.marcgravell.com/2018/09/monotoolism.html
No, they replaced their use of Linq2Sql with EF Core.
Linq2Db is as fast as dapper.
There are ORM like Linq2db that arent slow at all.
Really? I’ve been using it for 2 months now and i still get the same problems. Everything is a pain in the ass to do and I get cryptic build errors a lot.
Oh my fucking God why is there a damn big subscribe thing that takes 1/5 of your screen that you can't dismiss in mobile
Around Jan through May if this year. I have noticed they've kicked into high gear on the releases, so I am a bit optimistic about that. 
IQueryable doesn't implement IEnumerable, it inherits from it. Interfaces don't implement anything, that's what makes them interfaces. Classes implement interfaces. So, when a class implements IQueryable, it also has to implement IEnumerable.
Everyone I've talked to with more recent experiences agrees with my findings though.
No kidding. Absolutely horrible mobile experience. 
Anyone shitting on EF or EF Core without a good explanation why has clearly never tried it since RAW queries are fully supported. EF Core(2.0) and EF fully support compiled queries as well so in reality you'll never need 90% of the so called 'better' queries sql guruc claim to write. Most of the time they are wasting money and development time while prolonging any fixes that need to be made. If your performance isn't acceptable after fully leveraging compiled queries and of course setting up indexes and partitioning your db then you still have the option of writing that better, faster and more optimized query. But just skipping all that because you're too proud to try something different is just being stubborn because you're afraid your skills might be replaced. But again. Chances are you're wasting company money for a soluzion that excludes programmers that aren't skilled in SQL. Even if you have a high traffic enterprise solution. And this is from someone that loves sql queries. 
Rightly or wrongly, Functional programming styles have found favour over OO for a lot of problems. To be fair, I think previously the industry swung a little too hard towards OO as the solution for everything. Perhaps now people are swinging too hard away from it, but ultimately you want to find the right tool for the problem at hand.
It doesn't necessarily mean that you should architect your application to be *that* modular. It just allows for design choices in applications. There's a common API that's shared between each of the database providers (MySQL, PostgreSQL, SQL Server, SQLite, etc). Some features may not be enabled for certain providers, too.
6 is getting full support in the next version of .NET Core(most likely for windows only unless they actually manage to port the whole thing to net standard) . The net core will apparently be developed along it since they're going in a different direction(full support on different platforms). It still lacks some features of the net 6 version. Like 1+ many queries and grouping doesn't work as it should but it's slowly getting there. Thank god for faw query support until then. 
It's worth learning, like anything else, but better to know what it's doing under the hood so you know what you're doing when you replace it with a less bloated data layer. I've never seen a project where it was used properly without some form of headache, so I end up just removing it from the project entirely. If replacing it isn't an option, then yes, knowing EF will help you deal with it in the meantime.
Not OP, but instead of ignoring basic principles I would frame as people moving away from OO towards Functional programming. You probably aren't seeing anemic OO style objects as much as data transfer objects (often immutable ones at that). It is simply a different style.
God damn man, who hurt you
You're conflating the common case with an edge case. In the common case EF absolutely does handle it, with the potential for issues or edge cases remaining. It is still a massive effort saver. 
&gt; EF has providers for SQL Server, Oracle, SQLite, pgsql, and numerous others. I'm going to stop you and anybody who uses this as justification for any ORM, because it's such a stupid argument. Yes, it's technically true - but you tie yourself to the lowest common denominator of SQL functionality that everything shares in common, or you THINK you have created a portable abstraction but end up not realizing you've been really leaky with it. Pick a database, I honestly don't care what it is, and take advantage of it. I love PostgreSQL, it's got some amazingly powerful features that even Oracle and MSSQL lack, I'd be a fool to not take advantage of them just to remain "portable". On that note, MSSQL has things like temporal tables, indexed views, etc. that PostgreSQL just doesn't - and for now it still has more functional partitioning support as well (Pg is catching up, though). Maybe you do just have a really simple CRUD app that doesn't need the more specialized features of your database engine, but please don't use "portability" as an argument for ORM's - it does nothing but constrain you as a developer from actively utilizing the full breath of your storage layer.
Dapper users who were too lazy to back their claims up using bench marks. Needless to say by the time I went to prove them wrong they already had the 'well unfortunately the project is so far along now...' excuse ready. I'm all for performance but it has to blow everything out of the water to justify usage and time wasted, especially when you have people who aren't into sql, working on the same project. 
EF core sucks for stored procedures. The procs output has to match a n entity definition which is inconvenient.
And if you were using Dapper and SQL most of your queries would be standard SQL that would work across all engines without modification as well. Particularly if you put a modicum of forethought into it. Your SQL also has a better chance of being well organized and segregated into a single area, whereas EF makes it a lot easier to end up with IQueryable passed around and used everywhere. So I don't really see them as all that different.
Just curious, because my use case for creating the web portal I've been using Dapper for is not very typical. How does EF handle if, say, I wanted to display the latest customer that registered on a dashboard and include their contact information? In SQL, I'd take the MAX() of the create date on the Customer table and then join that result to the CustomerDetails table. Is that easily done in EF? What if say I wanted to display a bunch of aggregate details of a particular table? For example, let's say we have a "Receipt" table with a column called ReceiptType that indicates if the receipt type (let's say anywhere between 1-5). If I wanted to include a count of each of those and return them in one row, I could do something like... CountOfReceiptType1 = SUM(CASE WHEN ReceiptType = 1 THEN 1 END) ,CountOfReceiptType2 = SUM(CASE WHEN ReceiptType = 2 THEN 1 END) ETC. Is that type of operation also easily done in EF? Or would you have to return it as a GROUP BY on ReceiptType and deal with getting five rows instead of one?
I had nightmares with mysql EF when unfortunately dealing with DB first. If you can do code first for things like adding columns and tables it will go quite a bit better for you. DB first has some other things that make its just not as nice to deal with.
The short answer yes, it's well engineered especially in the latest version, and it save your time for the 80% of your work. but wait, there is still the hacky 20%, some advanced queries, union, window functions (row\_num over() ...) and so on, that EF is not designed to handle that. Mixing EF and Dapper is a good decision. &amp;#x200B; Personally I use [https://sqlkata.com/](https://sqlkata.com/) a query builder, flexible more than EF and more advanced than Dapper. PS. I am the author of this library.
Still "never" is not accurate. Also, the official way is not the only way to get ports. 
I haven't tried with these. I'm sure it's technically doable, would mostly just be a matter of how pleasant the scripting and test running process is. If you've already got a dashboard for making and running test collections, a CLI tool for running the same things would be handy, especially if it supports common auth schemes.
Use what works, and what you and your team know what works. Be it EF/core, LLBLGen Pro, Dapper, Linq2DB, plain [ADONET](https://ADO.NET)... &amp;#x200B; If you get shit done with Dapper, keep using it. If it limits you, use something else. Do know that Dapper is a low-level tool, it doesn't offer services full ORMs do. It might very well be a lot of code you're writing now can be replaced with simple calls to ORM methods or even left completely to the ORM. That's not to say you should switch, as I think people should use what fits their way of thinking/working. If that's plain SQL queries with a microORM, keep using it: your users don't care what you use, they only care what you can make for them. &amp;#x200B; (I write ORM tooling for a living for a LOOOOOOOOOOOOOOONG time now)
I find Prism works great with Xamarin Forms. These days I think the mystery problems aren’t as frequent, however upgrading an existing project to a newer version of Xamarin always seems to be a nightmare. Xamarin Forms is an excellent tool for C#/XAML devs, works with dotnet standard libraries as well, which is handy for sharing models between projects in a solution. 
Yes, EF is worth understanding. There’s too much nonsense around performance etc, it’s super useful, and people would pay for a more expensive database rather than paying for developer time. Pro tip: Don’t label yourself as a “database developer”, that’s like saying your a “API developer”. Nowadays companies are looking for people who can do stuff end to end.
https://stackoverflow.com/questions/23977036/asp-net-mvc-5-how-to-delete-a-user-and-its-related-data-in-identity-2-0
I mean... You *can* write fast EF code. I've done it before under EF6 (well, aside from the first query hit, which was unavoidable in that version) You have to have a certain level of discipline and diligence (no `.AsNoTracking()` on a Read query? Set PR to "Waiting for changes from author"...) to do so however.
You can just define an object for the result of that prox. Can be an entity or a value object or something in between. I don't really see that as much of a restriction. It's entirely possible that I'm not seeing it as one because my use of stored procs thus far has largely been solely for handling hierarchical tables and not much else. I know people use procs for a huge variety of reasons other than that, so I'd be interested in hearing why this comes across as a notable restriction for you. 
Have you run into any situations in live environments where the ability to downgrade migrations has been useful? I haven't worked on a production system yet, but I have run into plenty of reasons why I have to roll things backward on local. But very few times have I needed to do so to anything deployed (to test environments). 
&gt;How does EF handle if, say, I wanted to display the latest customer that registered on a dashboard and include their contact information? In SQL, I'd take the MAX() of the create date on the Customer table and then join that result to the CustomerDetails table. Is that easily done in EF? `context.Customers` `.Include(x =&gt; x.Details) //Join to the CustomerDetails table` `.OrderByDescending(x =&gt; x.CreatedDate)` `//can optionally use a .Select(x =&gt; new {}) and define only a subset of properties to pull` `.FirstOrDefault(); //can use First() if you want exceptions thrown for 0 results` What if say I wanted to display a bunch of aggregate details of a particular table? For example, let's say we have a "Receipt" table with a column called ReceiptType that indicates if the receipt type (let's say anywhere between 1-5). If I wanted to include a count of each of those and return them in one row, I could do something like... CountOfReceiptType1 = SUM(CASE WHEN ReceiptType = 1 THEN 1 END) ,CountOfReceiptType2 = SUM(CASE WHEN ReceiptType = 2 THEN 1 END) ETC. Is that type of operation also easily done in EF? Or would you have to return it as a GROUP BY on ReceiptType and deal with getting five rows instead of one? &amp;#x200B; There may be a better (read: more performant) method than this, but this is, I think, the simplest way: [`context.Receipts`](https://context.Receipts.Select) [`.Select`](https://context.Receipts.Select)`(x =&gt; new { CountOfReceiptType1 = context.Receipts.Count(y =&gt; y.ReceiptType == 1), CountOfReceiptType2 = context.Receipts.Count(y =&gt; y.ReceiptType == 1), //other types...}).ToList();` However, that query is not very maintainable, so I would argue that the GROUP BY version is better: `context.Receipts` `.GroupBy(x =&gt; x.ReceiptType)` `.Select(x =&gt; new { ReceiptType = x.Key, Count = x.Count() })` `.ToList()` &amp;#x200B; &amp;#x200B;
And then go out of business when you leave in a SQL injection vulnerability accidentally :).
EF has its downsides, some of which can be mitigated, but one of its strengths is moving your time away from data access work into business logic and/or UI work (depending on your role). Generally speaking, that is more valuable to spend your time on (from a business' perspective, at least). Depending on your organization, that could remove or lessen your need to have database programmers vs programmers working on the true business logic. Every situation is different, the pros and cons have to be weighed based on the problems that you are trying to solve. EF helps with some, other approaches help with others more/differently.
Not really, having them just be able to be upgraded to a certain migration is the most useful part. I agree that downgrading is more of a testing thing. If you truly needed to downgrade in prod then you’d want to create a new migration for it anyways so that your next migrations would be able to be added normally.
It is different in ef core. Which is the reason I dont like procs in core. I am not sure why they changed it in core. Something to do with populating the data. But ya, I have nostalgia for ef traditionally for this one thing lol.
Hi /u/houseme. Just wanted to let you know that a Nightingale v0.9.25 has just been published to the Store. It now has a toggle to turn off SSL validation in the settings (⚙) menu. If you'd like to try Nightingale again, try turning off the SSL validation setting and then connecting to your localhost. Please me know if it's working or not.
The link @hydraulic_IT_guy posted is a good start. Most of the code you need doesn't need to be created by hand, you can use the existing libraries to manage users and roles. The two bullet points you highlighted in your post can be achieved with the authorization attribute on the admin role. As to what code you write when the Authorization is granted, use the built in UserManager and RoleManager. They provide an abstraction over the tables so you don't need to write complicated queries to achieve user delete and role granting capabilities. 
Yes i'm looking at his link now, it looks promising /u/Hydraulic_IT_Guy thank you! I'm awfully sorry for these nooby questions and thank you for your time. 
As a C# developer that has to use other people's databases that are designed and developed without EF in mind, I'd beg you to learn EF just so you can understand naming conventions and readability from my perspective. I had a pretty good understanding of db design before learning EF, and when I tried to apply the traditional naming conventions, it just broke the readability. A. Great example is prefixing table names with tbl. If you do that, when you use C# and EF it'll look like you are trying to retrieve a table because the type name will have a tbl prefix, but you aren't. You're trying to get one item from a row. Sure, you can cope with it to make it more readable but that's extra work and setup, so either way you break either one of the two fundamental purposes of using EF (readability or reduced work for handling the database).
Judging from my experience working with clients for the past 5 years, I haven't seen a single code base that has a non-anemic domain layer. I believe that people are not spending enough time shaping the domain but instead focusing on other things but this is a topic of another discussion. I hope that moving to Functional helps with that. 
Absolutely, there is no tool that is universal. Using the right one is very important.
VS Code is bad solutulion for real. NET development. Try using Rider, it increase your performance up to 3x
I encourage my students to keep a SQL Server Profiler open while learning &amp; developing EF. It helps you catch nasty queries &amp; N+1 selects during development. It also gives you insight into how EF works. My policy: use EF for the straightforward stuff to benefit from the ease. Use Dapper for complicated queries and high load stuff.
looks interesting, but it looks like it would benefit from using expression trees like ``Where(x=&gt; x *2 &lt; 10)`` 
Get a license for https://www.llblgen.com/ and be done with this DB access thing. It's far superior to EF and vastly more mature and you now the next iteration isn't gonna break anything. 
Just use and contribute to https://github.com/OrchardCMS/OrchardCore/ It's multi-tenant and multi-all-the-good-stuffs. 
I spent 2 hours trying to get a stored procedure to execute, then ended up taking a different path because the parameters were not being passed in. That really should be much easier.
Actually a great idea! I will check this right now. 
Is it worth learning ... as a ...? The answer is generally always yes. The more you know the better you get at what you’re doing. And the higher your chances are to find something new to do (even within the same company) if you should not enjoy what you are doing at some point. If you already were in contact with applications using EF, it is apparently a technology used in your company and any technology used in your company is good to know. You work in tech. And tech evolves at crazy speed at the moment and we all have to keep up, even if EF is not en Vogue tomorrow, knowing the concept will help you master its successor some day. So don’t think twice, grab a book, ask your favorite Dev for some tutorials or use google and start learning. 
&gt; Compile time checking which can catch several error classes (you'd need unit tests to check raw SQL queries) While EF certainly helps catch type errors, you still need unit tests for every EF query. Especially when EF Core which breaks stuff with every release, including minor ones.
Go look at the performance of Entity framework vs Dapper. If you want to scale, don't use EF. We had a developer use it for one of our applications and he ended up having to rewrite it almost immediately because it used too many server resources per request. Our in house applications don't use any ORM, as we don't find that they save any significant amount of time.
UruIT? More like *U ruined it*, amirite? Sorry.
It is already being worked on: http://avaloniaui.net/
See http://avaloniaui.net/
EF never silently evaluates on the client but throws an exception, only EF Core.
I'm guessing that English isn't your native language? This is a bit painful to read - a lot of the words are like the longest synonym was found and thrown in place 
sounds like a resource issue (networking or disk or memory). are you sure you don't have any firewall or other network/internet safety item blocking your download? antivirus scanning and locking files while they are being downloaded? is your pc is up to spec enough? also check the installer logs.
Did you attempt to use https://localhost?
Not really. It's still riddled with stupid problems like the ones mentioned above. You get build errors for no reason, and no diagnostics on what is actually wrong and how to fix it. The xamarin team keeps pushing feature releases and are leaving loads of stability issues on the back burner. 
The cryptic build errors are still a major issue. Just with more features now!
If you mean with angular then yes. I get "Secure Connection Failed" in FF and "This site can’t be reached localhost unexpectedly closed the connection." in Chrome.
So let me tell you what I did. You can choose not to, or do as you planned. I deleted that crap. I don't know why they added that in as the default. Your microservice hosted on kestrel wherever it may be, should NEVER be responsible for https/hsts or anything of the sort. Microservices are designed to be lightweight portable, dependency light web apis. HSTS and TLS should be handled by a more experienced reverse proxy/web server that sits closer to the outside world. I think they added this for easier azure deployment, or the people who want to make a single webapi and just attempt to put the monstrosity in production on IIS just like the old dotnet days.
I’m trying to install VS on a clean install of windows with everything up to date. I7 16gb ram 500gb ssd. I solved the problem somehow. I had to use a vpn and change the vpn location to get the installer to download 10%. And pretty much rinse and repeat that until everything was downloaded. It would download at 25+ mbps but for only about 5 seconds at a time unless I changed the vpn location. Otherwise with or without a vpn it was downloading and stopping after a few seconds and would not progress even after letting it try all night. Thanks Microsoft. 
So you suggest to delete both of those lines? &gt;You have not mentioned where you plan on hosting the webservice. that will play a big part. Please follow up here. Maybe it's some miscommunication, but I mentioned in OP "My plan is to deploy on DO" (digital ocean if it's not clear). That's why I opened the thread in the first place. I would like to know do I have to edit something now, so once I deploy it and point my domain with SSL to it, I don't end up with non-working app due to http/https mismatch. I've never deployed app myself, so I'm not sure am I over-complicating it.
Sounds like an issue on your end not MS. I don’t know anyone ever having problems downloading or updating VS unless their company blocks the downloads. Hence why all the downvotes and such. Good you got it working though. Maybe like I said it’s some internet filter your router or pc or ISP has. Odd it would do it for a MS domain.
Sorry when i saw DO i thought you were thinking about azure dev ops. Infrastructure has many possible options from different hosting providers, do you want to do serverless? How easy do you want everything to be? Are you optimizing for cost? Is this for revenue generation? Is this for just learning? I will say that usually you want to have a more hardened webserver do your TLS and HSTS. Most people do take out those lines you mentioned. You have an additional problem though, you technically have two things you need to host. the angular app, and the webapi. Usually static files are delivered by a very serious static file server like apache httpd, nginx, or putting those files on a cdn (content delivery network) that will take care of your https as well. Your next issue is where you need to decide how you want to enable tls for your app. I would do some research what digital ocean provides in that regards, otherwise you will need to install the cert in the docker image, and expose both port 8080 and 443, along with any other steps that might be needed (since I have never done this myself)
Really odd issue. Especially since no download or install issues with any other programs. So from the surface you’d expect it to work, I agree prob my end but still very frustrating. I am not smart enough to troubleshoot very deep
Is this still true? Http 2 is a lot faster than http with sharing TCP connections and whatnot. Should we still be using a reverse proxy to terminate SSL? That’s what I’m doing now but I’ve been starting to wonder about it
It's just to put it on my resume, nothing more. I don't expect it to have visitors, except when people from different companies go through my resume. Currently it uses Sqlite and I'm thinking to switch to MySQL, but might just leave it as is. Instructor is deploying it to DO using SSH and in the end it works, but on http. I thought to dockerize it, if it's not too complicated, since it will be a nice addition on my resume that I used docker. My only concern is do I have to edit both projects in order to for them to work on https. It would be perfect if I could leave them as is and just point my domain with ssl to it and it works without showing that warning in the address bar. 
What I did was: - Commented those lines out. - Deployed as http. - Waited for the DNS to propagate since I was migrating an existing site from PHP .NET Core. - Setup the cert - LetsEncrypt in my case. - Uncommented those lines. - Redeployed as https. 
well, you may be right about the speed of http2, I never bothered figuring that out. One point about the reverse proxy effort is I have single point of entry for my whole infrastructure. I don't have to worry about managing multiple certs across containers. Hmm... let me know what you think. I'm using kubernetes and you basically can't get into my cluster without going through https. It delivers a 308. I have not set up hsts yet. Is it worth it? But to your point that http 2 is faster than http, if I reverse proxy in kubernetes its just as easy to do it over tcp. TCP has to be faster than http2 no?
So only those lines? Did you change anything on frontend side?
Again you are going to have to look into it for DO along with your provider since you purchased a cert. I know that some dns providers offer the service you mentioned in the last paragraph. But I'm not sure, I only know the way I did it, which was to make a kubernetes cluster, get a letsencrypt cert, and deploy my app through a container on kubernetes. You can see mine working at [www.iotcomet.com](https://www.iotcomet.com) :P As far as it being a resume piece, it depends what you are going for, if you are a new dev. just show some work and effort like you are doing, but showcase the api, and the angular piece. You are getting into the world of infrastructure and devops and architecture which is not expected to know every detail for a newbie. Since its just for a resume, I wouldn't worry about making it fully production ready and going down the kubernetes route. Find a guide for something like aspnet core tls azure tutorial. and follow that for getting it out there. :P
No front end changes - all my paths were relative so I can't think of a case where that would be needed.
Thanks for answering, I will see how it will go with namecheap and DO. I can always get cert from letsencrypt, if that's easier for DO. I'm not a new dev per se, I have 1 year of professional working experience. I know how to do it on azure, since I did it on my job, it's just that it was really simple, so I didn't even bother with https/ssl and so on. Senior took it over and did other necessary things.
Thanks! My paths are relative as well, so I assume it shouldn't make an issue. I just don't understand why angular currently in localhost can't retrieve data from webapi.
How are you deploying it? An web app on Azure will handle https for you (it proxies traffic, terminates SSL, and then hands it off to your app). It expects your app to listen for http on 80. There are other ways you can deploy though. If you’re doing IIS on a VM you need to handle it in your app.
Whether you're using a Docker or not, simply remove the Https redirection in the Startup.cs, install Nginx on the Ubuntu Server, and using a reverse proxy, redirect a traffic to the [ASP.NET](https://ASP.NET) Core app. Within the Nginx configuration, you can also configure the TLS and certificates [https://docs.microsoft.com/pl-pl/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-2.2](https://docs.microsoft.com/pl-pl/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-2.2). You can also give a try to e.g. [https://www.portainer.io](https://www.portainer.io/) which is a lightweight UI for managing your containers. Later on, you can play with Kubernetes or Rancher.
Thanks, will have to check that out a bit more detailed, since I never used Nginx.
Well, my first impression was to do it with docker container, but if I get stuck, I will just do what instructor did and that's with ssh, hosting it on the smallest droplet on digital ocean. I saw he got https automatically once he uploaded to azure, but I already bought DO and wouldn't want to spend money on azure now, while idling this droplet and another VPS.
It’s about reusing a TCP connection to deliver assets, removing the need for the browser to make another request. I don’t think internally using TCP would work if the SSL is terminated but I don’t really know enough about the subject. We are running IIS due to our IT being afraid of Linux. We use a Cisco load balancer with some nice scripting capabilities. On some external configurations, they run their own load balancer a that terminate SSL. We have had issues in the past forcing HTTPs on our server, which caused infinite redirect loops. I want to switch us to docker since we are asp net core with net core 2.2. I may end up adopting your configuration as that seems to be the industry standard or at least a well traveled path. With etcd, you can share configuration across large deployments. I’m just worried that Kestrel wouldn’t be safe for HTTPS quite yet, which gives me pause. So I suppose your question is the question of the day.. Does terminating SSL at your reverse proxy disable all the advantages of HTTP2?
I believe droplets are just small VMs so you would have to handle SSL yourself. It’s not just turning it on in code. You need the appropriate certificates and to configure a web server (something like nginx) and to use them proxy traffic to your app. Definitely doable but more work. Docker could increase the complexity even further. PaaS hosting is so much easier to deal with.
Yeah, I think you are correct, those are just small VMs. In that case, I will just do what instructor did plus that part for SSL in order to get https. After that I can update to docker once I get a grasp of it.
Same thing here. My favourite is when the android app would just silently disappear. It turned out it couldn't find some resource that was supposed to be packaged up. I learned so so much about the ADB over that two days, and the worst past is it got resolved after perhaps the third time of deleting the obj/bin folders, restarting the computer, and then building again. :tears:
That hasn't been our experience at all. Spent the entire day fighting with nuget and packages trying to get it to run with the right support V7 and Vwhatever libraries to all work. VS would completely freeze, and when it didn't freeze it would get build errors. Watching one of my staff trying to wok with tooling while we can't bill the client for it.... thats hard to watch. &amp;#x200B;
It's because of Apple's arrogance.
Yeah, like our work estimate time frame is terrible now I spent more days looking at coding issues, package issues etc
I used to be a devout VB.NET dev, anyway I'm never going back since I got used to C#. Does take a while to get used to.
1/5? More like 1/3 on my Note8....
In certain countries it is even legal to DOWNLOAD content you don't own, for personal use (ie. watching, backing up). But sharing it, making money off of it, etc, is not allowed.
UWP would have been a better choice IMO. The controls have better implementation there, and most users will be on Win10 any way.
Nice, I already used in my Web Api With .Net Framework, it’s very helpful!
What you can do on digital ocean is just deploy your droplet locally unsecured on something other than 443 or 80 and use nginx to handle the SSL layers. Redirect http requests to HTTPS on 443 and proxy the 443 requests to your droplet. Your app won't be available directly, only through nginx.
Firefox doesn't use the Windows certificate store by default, which is why you're getting the warning. Chrome and IE and edge do use the Windows certificate store. There's an about config setting in Firefox to tell it to use the Windows certificate store.
You can never go wrong with pure-ADO.NET. Use Dapper of ServiceStack.OrmLite. IMO, EF Core, even though it's more lightweight than it's predecessor, is still too much IMO. I don't like it's affinity to ambient state. There is just too much magic for what most people need.
IMO, batched queries aren't worth GraphQL. My advice is to always keep things simple.
yea, im not letting kestrel do anything external to the network. I'm sure it's fine for https if you want to continue it from upstream. Anyways, I did my research, and ssl termination is the best route, it is not only industry standard but recommended because certificate verification is quite cpu intensive, even if there are protocol benefits. Good luck on the adoption. You could POC a kubernetes cluster, it is extremely easy to work with, it has made infrastructure fun again, and far cheaper than many other solutions! 
it all depends on how much time you want to spend investing yourself in current production ready scenarios and technologies. From my understanding, DO allows you to create little container droplets so you wouldnt need the ubuntu server, just use an nginx reverse proxy container, that has firewall rules to your containers with your app (meaning your app can and only will talk to the nginx container, and your nginx container will be exposed to the outside world, and it can also deliver the static files at the same time that is your angular app.
I'm definitely going to go that route soon. We just migrated our main application over to web. The cost of running 10 servers per client is a bit much. By using docker, we'd be able to save significantly in server costs. Nice chatting with you :P
VS code is not fist-class editor for . NET. It doesn't have many time-economy features like Rider or VS + Resharper. Your performance is not so high
Nice demonstration of handling the easy ask, but this would hit the database twice which I strongly avoid. Also, sometimes the queries become more elaborate. For example, as a common request, I have to generate complex reports, including counts and sums from data spanning multiple tables. They're usually for a specific date range, so often I'll grab all the data from the date range and then use linq to narrow the selection or perform aggregation on the set. You can also use .Include to get related entities so that you only have to make one database call which is pretty handy. Sorry, don't have time to give examples but I might come back to it. You seem experienced; maybe you have a better way of doing it than I. 
Nice and effective demonstration :) 
I would recommend letting your web server handle http -&gt; https redirects. If you deploy on a windows server, there is [a great tool](https://github.com/PKISharp/win-acme) for installing and setting up let’s encrypt certificates automatically.
Pretty sure it's been Google Translated. Third party -&gt; third gathering ward in the opening paragraph is a bit of a giveaway
I'm pretty sure the recommendation is to use NSwag rather than Swagger now, as it supports Swagger and OAS 3
&gt;I think that you still need to deploy a typical VM and install Docker on your own (there are some built-in VM images but not too many), however, the overall solution is pretty straightforward.
What are you trying to do with it? If you just want a history of the changes (and you're on SQL2016 or later) [Temporal Tables](https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017) might be a simpler option. 
True, Resharper isn't there for people who want it, but pure performance of the editor balances it out against the monolith what Visual Studio is.
Isn't a forum a lot better suited than a Discord chat? I mean, I just joined the server and I have no idea where to begin with, it's just chaos, everybody is chatting about different things. In a forum you have threads. And to add, if I help someone on your Discord server, nobody else can profit from that who is not from your Discord server. With a forum, you can type your problem in google and eventually find the forum thread to get help.
Yes and no. A forum tends to stay cold and the community aspect of it is distant. Also, most questions have been asked an can be googled, when you want to understand stuff better, it's nice have have live conversation. Lastly, a forum is a bit inconvenient, compared to discord, as it requires more steps to simply post and check for reply. We aim to be more than just a help desk. 
Yeah but. Forums are like, so early 2000s, man. Pro tip: for tech discussions there is no better approach than stack overflow.
Again yes and no. Stackoverflow is a bit intimidating to ask a question, because it is quite often that they boo you for asking same question or it gets ignored if question is poorly formulated. Professionalism wise, they rock. 
&gt; they boo you for asking same question or it gets ignored if question is poorly formulated. That’s... what makes it great.
That's what makes it not so beginner friendly. Great too, but it does not look that encouraging to ask a question you are unsure about. If you have found an answer but you don't understand it, you cannot ask there. You will be booed or ignored. Directed to read about it instead of getting an explanation. It is both good and bad. We encourage beginners asking even the so called "stupid questions". 
&gt; A forum tends to stay cold and the community aspect of it is distant. No, that has nothing to do with a forum. That has something to do with the people. It depends on how the forum is administrated. While StackOverflow is very nice for concrete questions and answers, discussions are very limited. As you already said, for newbies it's just frustrating. But why use Discord instead of a nice platform that is well structured? You could create extra subs for beginners, so also beginners could help beginners. In german, we have a .Net forum [https://www.vb-paradise.de/](https://www.vb-paradise.de/). That's where I started with programming, it's very beginner friendly, easy to use and it's often the first search result if you type german programming questions in Google. There's a place to present projects ("Show Room"), a place for providing source code, a place for tutorials, tips and tricks and of course all facets of programming (WPF, Xamarin, Networking, Database, Games, Hardware, etc.) are covered. So when I'm a Hardware expert, I just check once a day the sub for new questions and answer them. Can I do the same with Discord?
No. 
And that's why forums are better suited for looking for info and Q and A. But I think they are worse suited for a live discussion. 
&gt;Discord Yeah, no thanks :P Might be interesting if you ever set up a proper forum though
It's awful if you're new and don't know the terms of what you're looking for. In those cases, a discord channel is awesome. Discord in turn is awful at in-depth explanations, as it's difficult to search, so every question has to be explained in full every time. Both have different uses, but both are definitely useful.
Work in progress🙂
I can share with you what I do with [asp.net](https://asp.net) core apps in docker. I use an nginx reverse proxy to do my web handling. This way, I can accept multiple requests on the same ports with different hosts and proxy them to the correct backend port running whichever container is responsible for that host. I was doing this manually for a few years, but then I found [docker-gen](https://github.com/jwilder/docker-gen). With your compose file and docker-gen, it will create your nginx conf file based on your container's settings. This way, my dev port and production port are the same. I don't have to mess with that. I run it with a [letsencrypt helper](https://github.com/JrCs/docker-letsencrypt-nginx-proxy-companion) to automatically configure my certs, but you don't have to. The documentation provides all the info you need for generating nginx confs based a few additions to your compose file. I currently have 4 sites running on the same Linode box with this setup, all but a ghost blog are [asp.net](https://asp.net) core. Works well for a single server setup.
Post closed due to being a duplicate of another post. That's why SO sucks.
I think the problem is that people assume they know how to ask a question. 9 times out of 10 the right way to ask a question on SO is to not fucking ask if, and use the search features. Pretend that posting a question costs $300. This is not the way the minds of most newcomers work; we’re taught that asking questions is good and natural.... but for most STEM fields, we’re all walking heavily trodden paths, so it’s really not. I’d go so far as to say that success in programming isn’t about learning code or algorithms - it’s about learning how to not ask questions, via tactical google searches etc. But hey we all gotta start somewhere and there’s no harm in taking those first steps while chatting with other people. 
It's what makes it good and bad, depending on the person and the situation. Discord is a perfectly viable solution, and better than SO in many aspects. I know because I'm part of a C# guild with close to 5000 members now, and a lot of people are able to ask questions there and engage in real-time discussions with other people there.
&gt; for tech discussions there is no better approach than stack overflow. Stack overflow does not allow discussion and it is very unfortunate. StackOverflow tries to box everything into black and white questions and answers. Not every question and answer can be reduced to a quantitative expression. The "why" is as important as the "how" in many cases. Some of the best answers are based on qualified opinion and experience and confirmation and perspective from multiple users adds credibility. StackOverflow would benefit greatly if they allowed and encouraged discussion and opinion. They need to allow users to upvote based on how much value a comment adds to the discussion. They also need to identify downvoters and make them specify a reason. 
they are actually changing this behavior lately
Most of the questions ARE duplicates. Sort by new with no answers, 4/5 would be a question that I can google just by copy pasting the post title. The problem with StackOverflow is that, it's supposed to be a knowledge repository, not a Q&amp;A site. I.e. you should only ask questions if you are certain that question has never been asked. That point isn't made clear to newbies and that turns people off.
Have you inspected the result too? If there was an issue, it'd get logged there.
Yeah. Await that CreateAsync method then look at the result. If an error occurred, it’ll show up in the result. 
thanks. await did it. maybe I should move away from using var too.. but it's handy! :)
thanks! await did it!
WOW never heard of it. Does sound very interesting, will definitely check it out! Thanks for sharing!
SSDT database projects offer the same advantage, though, so this is not really an advantage of one over the other.
I prefer when I can to simply do it in code. If I need to warm up caches for example, I'll call code to make the appropriate query calls from Startup. Some services I write such that they have an explicit initialize method. Those are resolved from the container and called from Startup as well. I also have my CI/CD system hit urls to ensure that results are valid before putting the freshly deployed instance into production. That of course can also serve to warm up the app.
https://www.udemy.com/build-an-app-with-aspnet-core-and-angular-from-scratch This is the best one I've found so far. Ignore the Angular parts if you don't care about Angular.
Strange that performance dropped so much. I also don’t fully understand why so much code needs to be rewritten. A text templating engine seems like something to me where you read in some files (which you can do async), do the templating stuff purely without IO side effects (which should be most code and where async does not make sense) and then write to files again with IO
Check youtube. Plenty of tutorials to create a SQL Server.
Woww 200++ vídeos, i see a big course, Thank's dude.
Signing up [here](https://visualstudio.microsoft.com/dev-essentials/) (free) gives you three months access to [pluralsight](https://www.pluralsight.com/search?q=.net%20core) which is pretty good stuff, probably the most recommended. Edx.org has some general C# introductory material, free as well.
Same here. The garbage collection isn't reliable either. I always have to IDispose managed objects and set fields to null. Just suck it up and go native, you'll thank me later.
https://github.com/qmlnet/qmlnet
https://github.com/qmlnet/qmlnet
https://github.com/qmlnet/qmlnet
It's up to date as well, so you don't need to feel like you're missing out on anything.
For your first sentence, I’m pretty sure putting an async/await operator in a method will generate a bunch more code with accompanying overhead. Async is, as far as I understand, most useful for operations that run for a long time, such as a network call or something CPU heavy. Definitely not for every method. Hopefully someone more knowledgeable than me can explain.
No
Not more knowledgeable, but I did a little bit of work with UWP and it seems like most of the built in libraries are now geared towards using async for everything. The little bit of cost is worth it because it improves user experience when the UI never freezes or stutters.
CPU heavy tasks are not meant to be used with async. You mixed up pararellism and asychronous programming 
In general async/await was designed to release the calling thread while waiting for some IO/network operation to finish. So when you are waiting for some data from disk/db/net/etc, thread is idling and your call is usually blocked during this operation. Therefore you can release it to do something else while waiting. It has good application in pipelining, data parallelism, parallel processing or native UI apps, where you don't block the UI rendering thread while data is loading. Not really the best idea to put it everywhere, especially in some easy CPU-bound tasks, where it might be faster to just run the calculation synchronously. Starting many CPU-bound tasks in parallel to execute some in-memory calculations might take more time than just execute them synchronously, because the resource allocation, context switching and threads opening is expensive operation and it might just slow you down.
Go ahead and schedule the 70-480 for June to show the effort and get experience passing an MCSA. Get the book and study guide for 70-486 now then go through it and eliminate topics you feel comfortable with. Take the list of remaining topics and then practice implementing every concept on that list. After you pass the first one, assess where you're at and then discuss with your mgr. Got my ZCE in about 4 months with that strategy. It works. Now I should do it again with these, you inspired me to do it in 2019.
Let's do it together for 2019 and share any progress, no matter how small or big the progress is.
There are exam dumps that has all the questions they ask. Some of which have answers. I've never been forced to get a certificate, but from knowing people that have, their company usually strongly hint at them using brain dumps to help study for the material. Seems like an industry wide thing to do. **BUT it's considered cheating so use them at your own risk** On the other hand it seems like Microsoft acknowledge these sites exist and don't really crack down on them: https://www.quora.com/Why-Microsoft-does-not-ban-certification-dumps-available-on-websites 
Ive never heard/seeb of mcsa in any developmental resume so i question its value. 
It's pretty common for people to iteratively improve their code as their experience grows and situations change.
I've held a MCSD certification for the past 6 years, so I have taken these exams many times. You said you are not interested in brain dumps and you lack experience in the stack covered by the exam, so I'll cut right to the bad news. It will take an absolute miracle for you to pass. The MVC exams are a mix of general concepts and very specific framework implementation details. The 3 - 5 year number comes from that being the average amount of time it takes for a developer to have worked with those various concepts enough to be somewhat competent in them. However, the exams also target whatever new bits of functionality that Microsoft is pushing, and if you're job doesn't require you to use those new bits, you won't know anything about them. For example, one of my times taking the exam they were pushing some new Azure hybrid federated security something or other. I don't remember what it was, and it certainly wasn't something my job required. I'm pretty sure I bombed all of those questions. If I wouldn't have had such a solid foundation in the other areas covered by the exam, I would have failed. I don't think 7 months is enough time to prepare unless you're going to work at studying like a part time job. It's not the quantity of questions. It's the scope. If you can't push your timeline out anymore, I suggest keeping an eye out for some of the deals Microsoft has for taking their exams. They usually have an offer sometimes for a free second chance if you fail the first time. They also have a 3 for 1 deal at times. See if you can find one of these and take the exam asap. You will fail but it'll let you see real questions used on the exam. After that, you'll be better equipped to make a decision about how to navigate this situation with your employer.
thank's dude, you're amazing
What could they do to crack down on them? I don't see a way to stop them.
Do you have any idea how long it takes new tech to get into the exams? like if i were to take the MVC exam should i expect it to have questions related to dotnet core? 
Microsoft Silverlight / gold partner status is its value now
I passed my MCSA Web Apps a couple years back, the 70-486 exam. They're a corporate tick in the box. Personally I'd rather hire someone with a guthub account of sort, over someone with these exams. In fact I've worked with someone who is exceptional at passing these exams, yet useless when it comes writing good clean code. YouTube videos, blog posts and some old exam papers are a good way to prep.
I'll be honest and say that you would need to study pretty damn hard to pass the 70-486. The HTML/CSS/JS one is pretty easy if you've done web development for a little while. You can buy the 70-480 book from Microsoft, ready it cover-to-cover, and it contains about 95% of the exam content, so if you understand 90% of it passing should be no problem. The exam book for 70-486 does not at all cover all of the exam content. Your better bet is looking up the exam spec for this one and going through each topic looking up docs, tutorials, blog posts, and giving each topic a go yourself in a sandbox project. If you have an MSDN licence (assume so) then you can use your free Azure credits to have a play around. An awful lot of people have these certs because they used braindumps. Bear in mind that this is against the exam Ts&amp;Cs and Microsoft can take your certs off you if you get caught. The combination of braindump availability and the way a lot of the exams require memorising specific framework method names and implementations mean I don't consider much, if any, value in the MS certs. In fact when I've been interviewing recently, the candidates with MS certs haven't been very successful at all. There is also the tendency for people to continue to list their MS certs on their résumés even though they are 10+ years old and not current anymore. But hey, Silver/Gold MS Partner status.
Just a note: month if you don't have msdn/VSpro, table is [here](https://docs.microsoft.com/en-us/visualstudio/subscriptions/vs-pluralsight).
I also have the same problem, first API request takes A LOT. Deployments itself is quick, just the first request is a problem. Didn't manage to find a solution to this.
How do you write a async cpu bound operation? There's nothing for the calling thread to "wait" so it's not async
70-480 and 70-486 are easy exams imo, there's only like 100-150 questions, just memorize the exam dumps and don't do any thinking. The jump start and plural sight courses for these exams are good and you learn lots doing them and would be good to do for your job but they have not got much correlation to the exams. &amp;#x200B; There's so many questions badly written with ambiguities and downright errors that for these exams i wouldn't even bother correlating passing the exam with learning the course. There's legit 10-20 questions not even mentioned in the official course book. &amp;#x200B; 480 i learned all the course material and had a quick look at exam dumps, 486 i learned about half the course and smashed the exam dumps and got 10% higher. I'm supposed to do 70-487 too the azure one but it is the most nonsensical out of date joke of an exam ever with more than half the course material literally obsolete, as in literally the technology has been discontinued you can't actually do the demos.
Merry Christmas, Lloyd🎅 Hope you're doing fine 🙂
yes definitely, took the exam in september 2018 and there were a lot of questions about core and azure in there
I knew I read this before: [The future of JSON in .NET Core 3.0](https://github.com/dotnet/corefx/issues/33115).
I recently passed the 70-486 exam while having just 2 months of experience. I had about a year of experience with PHP web development and a few years of c# programming experience. I started learning for the exam with pluralsight for the basic asp.net framework functionality and implemented it in test projects. I did this full-time for like a month. I then started learning about the specifics, like deployment stuff and more advanced topics. After a little while I started getting the feeling of naming conventions and stuff and had a good chance in guessing the answers of questions about specifics I haven't seen before. I also did this full-time for a month and read some books and just searched the web for whole days. I must say it is a pretty hard exam and I was really lucky and passed it with exactly 700 points, even though I guessed a big part of the exam. I also had some questions about Ajax which I already knew from earlier webdevelopment with PHP. 
They publicize whenever they refresh the exams along with what is covered in the updates. 
The use case for EF is for prototyping or end-user applications and it works well at it. It's not a production server framework. In the case of the developer I mention, he was already well versed at SQL and was looking for what he'd perceived to be a shortcut.
Did you measure it? No way to know for sure without some metrics.
Well, that's basically what this article is about :)
lol
486 is very easy to learn IMHO. I failed it twice with 2 weeks of study, got the book, did everything in it and passed the exam on my third take. The book really helped to understand the scope of the exam and its questions which I didn't want to believe before and thought it would be better to just compile a bunch of info myself. And don't sweat about the 2-3 years of experience thing. There are some people who are certified as fuck but who have no idea what they are actually doing once you put them in front of an IDE. Just do a couple of tutorials to get a very vague idea about MVC, then get the book and do everything in it at least once. This should take about 1-2 months at most so you should have more than enough time before your mid-year evaluation. Provided you pass 480 immediately, that is.
There’s a hundred of these already. Lol. That’s how I set mine up. Ok 👍 
I don't think explicity typing out Task&lt;IdentityResult&gt; result = _userManager.CreateAsync(...) vs using var would help you to find out you need to await much sooner. The issue is result just wasn't used at all. With either way if you tried to do \`result.Succeeded\` or used result at all you would have found out you need to await it.
Bummer :[ Thanks for the heads up.
I don't think you'll find something open source, my company ended up developing a solution fron scratch a couple of years ago because we couldn't find any web based BI that was cheap/free. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Recommendations for good free\/opensource asp.net business intelligence offerings?](https://www.reddit.com/r/csharp/comments/aa1t0p/recommendations_for_good_freeopensource_aspnet/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
If your job makes you do them, do them. As a hiring prospect? In Europe? Don't waste your time. They're like all the worst interviews baked into one - the trivial implementation details, tested, like you wouldn't just Google them.
I'm going to disagree that it's "not a production server framework". Microsoft even makes effort to guide users on production performance considerations, and I'd recommend anyone that has questions about EF performance or is new to EF put in the time to thoroughly go over their [EF Performance considerations whitepaper](https://docs.microsoft.com/en-us/ef/ef6/fundamentals/performance/perf-whitepaper).
[Here's some actual benchmarks,](https://exceptionnotfound.net/dapper-vs-entity-framework-vs-ado-net-performance-benchmarking/) instead of "performance considerations". It's been that way consistently, and it goes back to what I said before: you really don't save any significant development time with most ORMs vs just writing the SQL or a stored proc. You don't simply because SQL isn't very hard to pick up, you'll end up trying to find your way around your ORM solution when it can't do what you want it to do, and you have to spend a bunch of time profiling something you wouldn't had you just gone with straight SQL to begin with.
A limited version of SSRS is included in the free SQL Server Express edition. I believe the largest missing feature is scheduled reports. 
It's not written in .net, but metabase might be your best bet: [https://www.metabase.com/](https://www.metabase.com/)
FYI, angular 6 does not have ivy render engine. Neither does angular 7 currently. 
This right here. I've taken several exams in the past and those years of experience are the only way to truly understand the content they're asking. 
SQL Server Reporting Services is just a suite to allow you have a common area to store RDL's, Datasets, Datasources. Even the "integrated" versions of SSRS for sharepoint and sharepoint online have even moved away from the SSRS service and to a runtime install/client plugin. If you could find a "reporting suite" that supports RDL's you could essentially embed your data sources and datasets in your reports and the reporting suite could simply use the [Microsoft Report Viewer 2015 Runtime](https://www.microsoft.com/en-us/download/details.aspx?id=45496) to achieve the same results. The reporting suite would just allow access and facilitate execution against the runtime. As far as where a reporting suite is that is open source and uses the runtime, I have no idea. I just remind people of this when they have 5 reports they want to include in their app and they come to me interested in integrating a full blown SSRS instance just to add 5 reports to their app. What are you all using for your company collaboration software?
it's meant to be used for a call center, so that program leads can generate some common reports, based on agent statistics and whatnot, but then for client by client/program by program, running custom reports against a certain campaign, and literally whatever else they can think of. essentially it needs a drag and drop ui from whatever table's and columns they'd have access to, to build stuff on the fly, serialize and persist the template, and then run it whenever they want, or let other people have access to it. But, your approach with MS Report Viewer, is something im going to look into. Thank you.
PowerBI in o365 comes with a small package I think. Also it may be free for DevOps Free accounts. Can’t remember exactly but you’ll struggle with open source. 
So just to be sure I made it clear, the runtime is just a runtime. Installing it simply installs and registers the runtime and library support into your windows machine global assembly cache. From there, if you have an asp.net or any .net application you can include the libraries in your .cs or .vb code. Your code can then point to an RDL file for the report definition and execute it with parameters. This will return you binary which you then spit to a directory or back to the via an http service. You could write an application or process in your current application to generate and email reports to users by passing in parameters to the runtime, return the binary and construct an SMTP call that would email the reports to your recipients. If you have a web application internally that you've already created and you want to implement some reports in it and you like using microsoft's report builder, this is a great purpose for the libary. Many languages even outside the .net family have support for integrating with this runtime because a lot of people like report builder's simplicity.
I use Metabase as a way of giving my boss the ability to run pre-written queries. It also makes a good place to store your queries.
Check it www.testking.org if you want to check out all of the answers ahead of time.
Got my MCSA Web Applications Certifications a few months back. I believe they were just changing the test at the time so it was quite difficult identifying what was on the test. I studied about 3-4 days per exam and passed them both the 1st time. I bought some specialized reference books but the test exams on us.mindhub.com were the real star. I wouldn't recommended taking the test without going through practice exams. 
By default when hosting on IIS, the ASP.NET Core app runs in its own process with its own HTTP server, and IIS acts as an HTTP proxy server between the user and the ASP.NET process. As the name implies, with the new in-process model the ASP.NET Core app runs directly inside the IIS process, without the additional HTTP proxying inbetween. This has possible performance benefits, but also has other side effects (which you can find the docs).
I enjoyed the article but from the title and imagery, I expected more than only the S in SOLID
I think "formaction" is one of those MVC helpers. I like to avoid all that stuff in favour of common front end technologies, like AJAX. AJAX isn't complicated. You're just making a web request, you get a result, and you do something with it, but in JavaScript. You would wire up a click event handler for the button, and make a request to your controller with AJAX, and do whatever you need to do. [https://api.jquery.com/jQuery.ajax/](https://api.jquery.com/jQuery.ajax/) This video should help you: [https://www.youtube.com/watch?v=kvlBmon98xg&amp;index=60&amp;list=PL6n9fhu94yhVDV697uvHpavA3K\_eWGQap](https://www.youtube.com/watch?v=kvlBmon98xg&amp;index=60&amp;list=PL6n9fhu94yhVDV697uvHpavA3K_eWGQap) If you don't understand it, maybe you would find it helpful to start from video 53, where he explains the basics of AJAX. &amp;#x200B;
At last someone talks about the downsides of religiously and blinding applying SRP.
Honestly at the end of the day everyone can simply use their better judgement. If something has to change often, it's probably best to be it's own class. If your Class is 5k lines, then there is a good chance it needs to be broken out. However if your class is perfectly readable, and the functionality inside is cohesive, there is no need to make every "slightly different" responsibility its own class. Personally I'd rather look at one class than 25 no line classes and try to assemble a picture in my head.
This was helpful and got me pointed in the right direction, thanks! I've got my clicks all wired up now and found some other sites that had some docs that made a little more sense now. Only issue left is now my loading div isn't hidden on page load and hiding / unhiding properly, but my ajax button IS doing the proper button action, so that's cool.
I don’t see a better alternative that allows me to write cross-platform applications, from a single codebase.
BI tools are a big category. Are you talking about running BI queries and analysis? PowerBI is from Microsoft and very cheap. Free open-source options include [Redash](https://redash.io/) (which can connect to just about anything), [Metabase](https://www.metabase.com/) and [Superset](https://superset.incubator.apache.org/). None of those are built in .NET, what is your reason for that? If it's because you want BI charts, tables and query builders to embed into another program then there really aren't any good options, but lots of commercial ones from Telerik, DevExpress, SyncFusion, etc.
That's not true. *Async is about not blocking thread execution* while parallelism is about doing multiple things at the same time. The common example of not blocking the UI thread by doing the work (even if CPU-bound) on another thread is about being asynchronous, not parallel.
That's a common misconception. *Async is about not blocking thread execution.* There are plenty of times where you want the current thread to not block even if the work is CPU bound and you can use an async method to move it to a different thread. This is a very common example used to keep the UI thread responsive in desktop apps while doing processing an another thread using async.
I feel like a lot of programming advice is rooted in reality but expressed in terms that are simplified to the point of losing meaning, and I think Single Responsibility and imaginary line limits on classes and methods are two of the biggest culprits. Whether violations of these "rules" are harmful is entirely contextual and that fact is usually not expressed when someone brings them up.
TLS certificates come in two pieces: a private key and a public cert. Both of these files need to be hosted by whatever server is out front and talking to your web visitors browser. This is the server that you point to with DNS. [ASP.NET](https://ASP.NET) Core uses Kestrel for the built-in HTTP server and it can handle HTTPS but you need to provide the certificate files and configuration. `dotnet dev-certs` is just a shortcut for when you're developing on your local computer so that you can test secure connections on [https://localhost](https://localhost) in your browser without having to buy and install a real certificate, but it will not help you with a public website. If you don't want to use [ASP.NET](https://ASP.NET) Core's Kestrel server, you can use other web servers like Nginx or Caddy which will handle the secure connections and then forward traffic over HTTP to your [asp.net](https://asp.net) core app. You still have to provide your certificate files and setup configuration so it's really just increasing the work involved if you want to use your own certificates. Another option is to use a CDN or load balancer that has TLS certificates built-in or easy to upload. Lots of options here and then you can have it point to your website on the backend over HTTP. This is a security risk since that backend traffic is not encrypted but it might be fine for your site. If you still want to host the webserver yourself, then look at using Caddy which automates the certificate process with LetsEncrypt once you setup the DNS. It means you won't be using the certs you bought but that's probably not a big deal. &amp;#x200B; &amp;#x200B; &amp;#x200B;
It almost always is, and in that case it's better to have a single post with all the answers instead of several copies all over the site. The only issue is that it can be unfriendly to new users but that's more of a training, UX and general behavior issue.
So it's now going to behave just like a traditional ASP.NET MVC/WebForms/Etc.? Or is there more to it than that?
I hate it
Toggle the CSS on the div, give it an id. display:block shows, display:hide hides
That's about the lamest thing I've seen or anyone had used to try to convince me. This stupid comparison of a situation where no attempt at all was made to configure the tool used for justification of tools to be used in a production environment was torn apart by the people commenting on it. The author lacked knowledge of EF and his pathetic attempt at benchmarking totally exposed his ignorance, which is entirely why I shared the link about performance considerations. It's like he said plastic hammers are better because when he put a wooden one in his hand it was backwards and he was too stupid to consider turning it around. You did however do a great job proving my point. Thanks. 
So far this seems like the simplest method. Turns out part of my problem is my ajax click event is apparently wired wrong. I've got the div panel showing now with CSS, so just gotta figure out the wiring. Thanks!
Is it served over IIS? I don't think it is starting the process before the first request. I think you can adjust that... I will try to remember to look tomorrow.
Probably two different ports, making it cross origin. If so, you need to enable CORs, but it'll be best to serve it all on one port.
&gt; If your Class is 5k lines, then there is a good chance it needs to be broken out. [Is this one ok?](https://raw.githubusercontent.com/dotnet/coreclr/master/src/gc/gc.cpp)
That's garbage code anyways!
When you use another thread not to block the UI one it is called offloading. And this is clearly not async. Async means you use same thread. In fact there is no thread in async.
Yeah, look at those braces
[Clear candidate for a refactor](https://xkcd.com/221/).
isn't there a mistake in the code example when saving the schedule object for the first time? looks like it's saving the report instance twice
I run on linux with kestrel behind nginx. Does this effect me at all?
do you hate file sprawl of the idea of solid?
Not sure about Linux, but when I migrated to inprocess Kestrel on Windows , the "Assembly.GetExecutingAssembly().Location" for Website's assembly has changed. It might not affect your site, but it worth making efforts to test your app after migration. Regarding performance impact, can't tell for sure because my site is running on low load.
That’s one thing I appreciated about the Pluralsight course for SOLID principles: part of each principle was a “beware of the cost of implementing this method. Just write our code first, and if you find yourself encountering X problem more than once, then this class might be a good candidate for refactoring and implementing this principle.” As you get more familiar with programming in general, and pattern recognition specifically, you’ll be able to do more and more planning before you code, and will start to recognize where these principles will be needed before you write the code and run into problems that these principles help address. Until you’re familiar enough to recognize them ahead of time, blindly adhering to them can cause more overhead and complications than they initially help solve, especially in one-person projects. 
No, you just proved mine: you don't save any time with ORM, because apparently there's something else you have to figure out and do. And you've yet to do the simplest thing: find an independent benchmark that shows EF running at the same speed as SQL plus coded mapping classes. This of course just works.
Add by T, but remove by int32 index? That is a leaky abstraction.
Start with Visual Studio installer projects. https://marketplace.visualstudio.com/items?itemName=visualstudioclient.MicrosoftVisualStudio2017InstallerProjects
And then imediately dump it, because you cannot build those with msbuild. If you don‘t need a automated buildjob, this would be the easiest solution I guess.
Thank you for your recommendation but I think this would fall in line with 3rd party extensions, be it microsoft created or not. The current installer is just using base classes shipped in 4.0 in a winforms application. At least I think this is how its getting the job done now. The two libraries i believe running the install is system.diagnostics and [system.net](https://system.net). It appears its pulling files from the web and running them via Process class in diagnostics. Not sure if this is best practices, I would assume not as most folks are using MSIs for installs.
Absolutely need an automated build job. Life would be painful without it.
Why they hell would they force you to roll you own solution for something that has solid 3rd party support and that isn't a core feature of your product?
I don't know. I was puzzled about this as well. Oh well, good experience I guess ¯\\\_(ツ)\_/¯ 
Yeah, this is pretty silly unless they just want you to add a folder, icon and some registry entries. I used innosetup for a time for several projects and switched to wix. Or just get installshield and go that route. Installations are easy, upgrades are not. You have to account for bitness, os level, whether frameworks are installed, user data, user input, logging, existing installations, rollbacks etc. It's one of the dark unappealing and unappreciated parts of development.
Yeah this is pretty much the setup. Pull our files down from the web onto a directory we create. Check for pre-reqs, check os, install correct pre-reqs via .bats, log it, twist it, bop it. Gonna be fun.
Sounds like someone reinvented wix and installshield. Sorry you have to go through that!
&gt; Installations are easy, upgrades are not. The happy path of installations are easy. Dealing with the literally hundreds of things that can go wrong is not.
WiX is used by Microsoft for some of their products. I think the Office installer is WiX. Poke around the Wix site and see what you can learn. Rob Mensching has been on the dotnet rocks podcast several times, listen to one of those. Then have a chat with the Dev lead about why and explain what you have learned. [http://robmensching.com/](http://robmensching.com/)
Yeah WiX is by far my favorite installer tool but I can't use it here. I had worked on an installer for my last company. I've already gestured it would be wise to move to WiX but he dismissed that rather quickly. Not a hill I want to die on, still new at this job.
I like the cut of your jib. Usually better to go along and get along even if it's not the 'best' way.
I'm sure you've tried, but it's definitely worth pushing back on that requirement. Maybe show this thread for support. I saw someone else mention the installer project and that it doesn't support msbuild. You can, on a builder, call devenv.exe (visual studio exe) to build if that helps. 
We use Wix\# which is basically a C\# wrapper for WiX. It's really great, you can even make custom UIs if the default MSI is looking too boring.
Seconding this... It doesn't make sense to avoid using industry standard installers. Installers are rabbit holes that can burn hundreds of hours on a project, not to mention figuring out MSI to give a true "installer" experience is substantially more complicated than figuring out wix. Sounds like the team lead had a bad experience with wix and thinks installers are easy... They aren't. 
Out of process will have better "whacky error" tolerance. Meaning if you do weird crap (COM objects, calling unmanaged code...) and there are problems the crash will be better handled. It'll just be the worker process that crashes and then IIS will restart a new one. How many users are you expecting? What kind of hardware? Small internal web app, out of process should be fine performance wise. Writing the next stack overflow... My advice is to start with out of process and then switch if needed. Errors will be more likely in the early days and out of process will be a bit easier to handle them.
Yea, no need to be squeaky wheel too soon. Build automation will be tricky. Others mention the studio plugin, that revives the old system that used to be in studio be default. They're right that MSBuild won't handle that project type but I suspect that you could install studio on the build machine and then call that from a batch file. Not pretty and you'll have whacky dependency issues on the build machine but it'll work. Keep asking questions and good luck!
Your manager sound like an idiot. The entire world uses WIX to create installers on windows. Is there any requirement to have a cross-platform installation (on mac, linux etc)?
I'd start by trying to understand the reasons you're being asked to do this without using external tools... There may be valid concerns for this specific project, or there may be concerns that warrant debate. Seek to understand why. That being said, in my experience, the more you roll your own stuff, the harder it will become in the future to properly handle upgrades. Unless your app can work with a simple XCOPY or unzipping an archive somewhere (no registry, no setup scripts), the project will benefit from using something standard like windows installer in both the near term and in the future. Windows Installer definitely has problems and complexities, but it brings a lot of useful functionality and the ability for a non-admin to install software that might require admin permissions (or just basic UAC elevation). Just try to stay away from custom actions where possible (in my experience, they make ongoing maintenance harder, especially for admins that may be installing your software). For them, the ability to use tools like Chocolatey to ease installation, or Windows Installer transforms to customize an installation, may be very important. Windows Installer started with Microsoft Office installers, which guided some of its complex design decisions. If I remember correctly, the original WiX tools came from the internal MS tools used to build those initial MS Office installers. That toolset has a long history. If you must forge forward with a custom installer, ask if it even needs a UI at all. Would a simple zip file or NuGet module and a PowerShell script work too?
I use var until I can't or if the right hand side doesn't easily imply the type - code is much more legible imho.
I use that in several places, thanks for the heads up.
It's made by Microsoft, just distributed through the market place.
A windows installer (ala .MSI, aka 'windows installer') requires the use of an .MSI file. Without using windows installer you don't get an entry in "add/remove programs". Technically you could create a .MSI without any third party tools... but only by building your own tools for it. The .MSI file format I believe is the JET db engine. Wix is an excellent open source tool (ala Entity Framework, MVC, .Net Core). Windows installer is a funny beast. It is a weird programming language, which has only a few entry points (Install UI, Install, Uninstall UI, Uninstall, and some of the advertising/admin sequences... oh and rollback phases if something went wrong). The "operands" (essentially the languages keywords) also have some interdependencies (ie. can't call X until you've called Y). "Custom Actions" can also be a giant pain in the ass. These work "best" if implemented as a C dll... though Wix includes a bridge that lets you write custom actions in C# **2.0**. Passing information to/from a custom action is also a pain in the ass (and varies depending on whether your custom action is called during the install UI or the install phase... oh and install ui occurs in the context of the launching user's process, while the actual installation occurs in the context of a different process). Doing a fully customized UI is possible, but that has to be implemented as a custom action, and you have to everything yourself (i.e. window, buttons, etc). On whole windows installer is a complicate beast. Mostly because it tries very very hard to make installation (and upgrade, repair, remove) ACID transactional (either the installation completely succeeded otherwise no system changes where made). If your custom actions make system changes (eg. configures IIS) then getting that custom action to be "transactional" is pretty tricky. Some things that appear to simple can actually be pretty tricky. A good, but old, example is COM registration of an ActiveX component. Often these component's dll offered a RegisterActiveX entry point (iirc name correct) which would make the requisite entries for you (this is how regsvr32 worked... just called that entry point and the dll did its thing). Unfortunately this is tricky for windows installer... because of that transactional goal. Windows installer can easily create registry keys, and can handle rollback/undo, but when calling a custom action it has no clue as to what the custom action has done. If the custom action dies part way through, or something else dies later, getting that whole rollback/undo bit working is a giant pain (and might not be possible... leaving the system in a partially installed state). The side effect of this is that developers who aren't windows installer aware pass off to the installer developer a .DLL that works fine if installed with regsvr32 but requires a bunch of work to extract the actual registry keys being used (wix has a tool to help with this... but some developers used regsv32 to more than just register COM classes). Long story short it's good that you're doing .NET as this avoids a bunch of things... well... unless you need to register with the GAC). For a complicated application developing a windows installer based installer can be challenging and time consuming (even more so when if you're just learning the windows installer "language"). If I were you, and I got approval to use windows installer, I would focus on getting the installer working (install, uninstall, install rollback, uninstall rollback, upgrade, upgrade rollback) flawless first. Then I would focus on a fancy custom UI. I would budget 3 to 6 weeks effort (but I already know windows installer... maybe add a learning curve adjustment of 50%). Note for a simple installer (I have a few... one for an rest based .net API client that's as COM accessible) and it took me a few days to knock out. Oh... other challenges you might face: 1. Package as a self extracting .EXE. You'd think this would be simple... but if you want your branding to show and not the self extracting exe vendor's then this is a pain. Why would you need this... cause the boss doesn't want to deploy a .MSI (hehehe... but in reality if you need stuff to run outside of the .MSI then you need a self extracting .EXE). Also... if you want to run the .MSI with a debug log enabled (highly highly highly highly recommended) then you need a .EXE that calls the installer with a debug flag set (eg. "msiexec /l* C:\install.log /i MyInstaller.msi"). Always producing the debug log (and removing on success) has made support of my complicated installer much easier. 2. A better prerequistes checker. Windows installer can do prerequistes but the UI for failures is pretty horrible. I wanted a richtext display with clickable links. Note that this means a C/C++ application with a UI (WTL? MFC?) and if you want it display a nice message when run on older unsupported OS's (eg. winXP) then that's a bit challenging. Best avoid this if at all possible. 3. .NET version installation. Wix 4.0 does do this now (ala burn) but that's not released yet (been in beta for a long time now... the wix project moves pretty slowly). I still using Visual Studio's bootsrapper task which works pretty well. 4. God help you if you want to include a 3rd party installer as part of your setup. Getting something like SQL Express installed as part of your main installer is tricky and I wouldn't attempt it without using Wix 4.0's Bundle feature. Good luck!
Thanks this explains it well. My site calls unmanaged code so I will keep with out of process. Not really sure how I set it either way yet though tbh.
This uses wix under the hood but I really like Squirrel. https://github.com/Squirrel/Squirrel.Windows
Is it just because the want it to look sexy? WiX Toolset + a WPF BootStrapper Application is the way to go.
Without knowing what specific errors you're having, what your project looks like, or what commands your running, I can say that this series of commands builds for me (from scratch): \`\`\`dotnet new console -o test-efcore cd test-efcore dotnet add test-efcore.csproj package Microsoft.EntityFrameworkCore dotnet build\`\`\` I would suggest breaking your project down to get it to build, then slowly adding things back in and recompiling along the way to isolate the problem.
Strange that they're paying your to re-invent the wheel, but so long as they're paying you... ;)
You could look into new MSIX-based installers, but even then it's easier to use a 3rd-party package to create the installer (unless your app is very xcopyable).
You should really ask why as it may give some insight into what exactly they're looking for that they may not have articulated. Or give an opportunity to correct a wrong impression of 3rd party tools available.
Might be just what I am after. I looked at your website, as well as git - do you have a live demo up and running anywhere?
You should fix your pathological queries before you worry about microservices and horizontally scaling SQL Server. &gt; 2 tables, each table has about several thousands rows That's nothing. Absolutely nothing. If you're having performance issues, you're either running sql server on a smart watch or your doing something horribly wrong. EF can make it a bit easier to end up with a horribly wrong query, but that's still on you.
[removed]
&gt; Not sure if this is best practices, I would assume not as most folks are using MSIs for installs. It definitely isn't. It's probably an obstacle with techno-savvy clients, too, as most people don't like running web-hosted scripts willy-nilly. It's a major security risk if your site ever gets hacked. And it also means your "installation" can't be managed by an admin, but has to be run on each machine. Sysadmins hate that. But it seems like they don't really care, which is why they gave you those requirements.
Yeah, WiX would handle this well. Unfortunately cant use it. Hes a good dude, but yeah don't think hes up to date on installers.
Thanks for the suggestions! I'll check them out.
I think the problem you had was that the package was cached in the global NuGet cache (.nuget in your user directory on Windows) and it was referencing that one. I had a similar issue with a library that was updated, projects using an older version conflicted with it.
&gt; It'll just be the worker process that crashes and then IIS will restart a new one. That's what happens with w3wp.exe, so what exactly is different?
The new [MSIX installers](https://github.com/Microsoft/msix-packaging) might have something usable for NIH management assclowns. I've been digging around to see if they support deployment of windows services, since they seem aimed more at desktop/store app deployment and it's unknown if they've already fleshed out everything. It's supposed to combine all the installation technologies into a superset implemented in open-source code. It was announced in March of this year, so I'm *sure* it's got all the bugs ironed out already. And has useful samples and libraries. Most links I've seen just direct me to API pages. &gt; He wants a nice modern looking installer. What does this even mean to him? Most people go with simple vanilla-looking installers because people are *comfortable* with them. I *guess* you could build a WPF installer. You could also pretend you're a shitty antivirus company or that you're shipping an MMO and build something using [Sciter](https://sciter.com/). But that's still a toolkit. In fact, at some point you *will* use a toolkit - even if you're building it yourself. And some of them will have installation dependencies you'll have to manage. That could be Visual C++ runtimes, .NET dependencies, Unity, etc. You'll save yourself massive amounts of time with a toolkit. Ask him to point out an installer that's modern. Ask if you can just use whatever they use. Then rip it open and see what they're using. There are tools for decomposing MSIs. Though if it's "modern" it might be an executable and pulling it apart will be harder. As I mentioned before, clean MSI installers make people *comfortable*. They're easier for antivirus products to scan and system administrators can make deployment packages for their organizations easily. If you're directly working with MSI, the standard windows installation format for over a decade, you're going to have a really bad time creating those without a toolkit. The MSI format is a flat file DB, and the it's got years of nasty warts on it even if you're just dumping files and registry entries. You'll burn hours on messing with upgrades, versioning and learning how all the stupid tables interact with each other. Only an ignorant fool would recommend you do this manually. *Everyone* who's actually built MSI installers knows how obnoxious it was and doesn't want to go back to the dark ages when we had to clutch our rosaries and pray the installer actually installed. And uninstalled. &gt; The lead said I cannot use third party tools like WiX, Advanced Installer, etc. We've got opposable thumbs so that we can use tools. It seems like your manager is expecting you to whittle your own computer out of balsa wood and have the end result work flawlessly on client computers. In reality, you're going to burn countless hours up front to end up with what is likely a mediocre, fragile product that will be a major cost-center in your company for years to come. MSI installers have so many edge cases that it used to be its own full-time position before these tools were created. As a third option, you might want to look into a crappy, XCOPY-style "installer" in .NET that just extracts files and gracefully handles UAC. Honestly, this seems like the lead's expectation, since they're playing luddite and ignoring the reality of windows installation. You're still going to end up using libraries - whether it's WiX, or Nuget libraries with WPF controls or MSBuild tasks, runtimes like with Qt or VC++, or post-compilation linkers to pack all your DLL dependencies into one obnoxious package. It sounds like your current "installer" is a simple deployment package like this, so unless you want this project to be your next 6 months, it might be worth putting some new lipstick on the pig by revamping the UI and refactoring the old code. Unless the target OS is new, you won't be able to use bridged UWP components. You might have to design some WPF controls or themes. Your best bet at a "quick win" might be this. Your lead has absolutely no idea what he's talking about and should probably learn to not micromanage outside his comfort zone. As long as the end result is maintainable and documented, he should trust his team to implement it with their own experience. Especially when it's a difference of hundreds of hours. Good luck explaining that to him, though. If he's the type of person to recommend you waste hundreds of hours just to avoid industry standards that he doesn't like, he's probably the type to pretend like he's the techno-messiah and be really paranoid about people being more knowledgeable or useful than him. If you just want to dodge the conflict and wage-slave through it, it's understandable. Being the voice of reason isn't profitable nor stress-free.
Theres some by Mosh Hamedani (I think I spelt that right) that I liked.
WiX supports wpf. Do it in wix and the idiot would never know.
Yeah pretty much make it sexy, plus need to go through some pre-reqs and update things a bit. WiX Bootstrapper combo would be great. Unfortunately, cant use that. 
Ill take a look, thanks for the recommendation!
Thank you very much for this thorough and extensive feedback on the processes / technologies used in installing software. I'll have to look more into the details when I get home tonight cause this is a lot to digest! Thanks!!
I don't exactly know why. When I asked why I couldnt use WiX. I was told, "We like to do hot fixes by copying dlls to a specific clients computer. Later on a full version build would be created semi-anually and rolled out from that point on." Although I don't understand this reasoning because those could be minor builds in common installer tools. However, I'm not trying to argue with our lead about this. Yeah it seems rather complex the current setup they have. Users have to be using admin accounts but I guess its been doing the job for 7+ years at this company so oh well. Thank you for your input though on this problem. Greatly appreciate it!
&gt;The slowest part, are when developer use for-loop to fill data for calculated field, which cannot be calculated with LinQ to SQL query. The results of this is unacceptable slow (I'm talking about 10sec for a complex data query) I know Entity Framework is slow, but is it really this bad? :( This sounds like you are making a query within a for loop in your C# code. If so, the network overhead is what is killing you. Figure out a way to get all of the data you need in a constant number (preferably one) of queries.
I did this exact thing with React and Bartender software. Pretty simple. 
Use WiX and mask it with a style. What a dumbass
Nice!
Better head for pluralsight
You should definitely ask why. It's a pretty large project to do for no reason.
It sounds like he's been burned by "Advertising" self repair in the past and thinks it's something that can't be turned off.
I am working on updating the docs and putting up examples. This should all start appearing over the next week. 
WiX is not a third party solution by the way. It was created at Microsoft like 20 years ago internally before it went open source. It's also what Microsoft use to author installers, and there's support for you to use WiX and drive it through a WPF rich UI like the previous visual studio installers ie. 2012-2015. Those were made with WiX. If you want to make an installer for Windows, WiX is literally top dog. Your lead is an idiot, but if you dig into the history of WiX and show him, he may change his mind.
Is the lead an idiot or actually a decent guy, what's the deal here? Is he dismissing the best option based on incomplete information? It would seem like if you already have some experience here with WiX &amp; using a managed bootstrapped UI, he just needs a nice shiny proof of concept throwing together, make it just install a single file with a plush UI, and then when he says he loves it, tell him it's WiX. Then lay all the reasons to use it on him. Clear up his fears.
When i try to load a sln file the app crashes :/
The logging is a bit better when it’s out of process. Yea you are right that things will / can restart automatically. But out of process will restart faster and with better logging of what happened. Better as in a little better, those kinds of crashes are always a pain at first. 
I (not the OP) have learnt a lot from this thread. Thanks a lot everyone!
Thank you for doing this series
Your problem is that you're querying the database in a loop. It's got nothing to do with sql server or entity framework, any language will have the same problem. Google DB Indexes, then check that the database design is normalized, data types are correct, indexed are added, foreign keys added, constraints added and query plans examined. Complex queries convert to stored procs, but honestly I doubt that's necessary. 
MSI supported a ExternalUI which can give your manager the modern feel he is looking for. Your installer logic (files, services, iis, etc) can be WiX but then it uses your UI that you code into your app, showing a wizard or whatever, and then you handle the install progress messages to display the progress bar however you like. 
If all of those various apps share that one database, and effectively use it as your point of integration between them, then this will confirm my suspicion that your problems extend far beyond how well or poorly you are using the ORM... You have an architectural problem. Your app / service decomposition does not match your business. I know this is going to sound like a haughty dismissal, but it is not meant to be. Hire an experienced architect. Otherwise, you'll likely end up chasing the irreducible number or bugs / performance problems fallacy, where you fix for one performance issue introduces a new one.
Nuget packages can be referenced using a combination of `Reference` elements (with explicit `HintPath`s) and a `package.config` file (old style) or in the main project file (`.csproj` for C#) as a `PackageReference` element (the new SDK project style). By default there's not massive room for error here, but there's a few ways to do it. Older, non-SDK Projects (using the `Reference`elements) also had a UI setting for "Reference Paths" where you could add multiple directories that the IDE and MSBuild would check for dependencies. Some teams would use this for when different developers had massively different directory setups for their systems and when the paths were absolute. These reference paths weren't stored in the main project file on purpose, but instead in the user settings (`.suo` or `.user` files, I think) that generally didn't go into source control. This kind of thing got a lot less common, though, with Nuget and git repositories. But if you've got an older project, there's a chance that your reference paths can change what's being referenced. Sometimes the IDE would even look in the output directory for DLLs when they were set to copy local. Usually it wasn't a big concern for Nuget packages, but I've seen it muck with COM references. Like if you're using the .NET Crystal install from the SAP site.
Hi, it hasnt been updated yet to support VS 2017 solution files and the udpated csproj files. If you want to give it a swing, you can still load up the compiled libraries directly.
No problem. I just tried to remember the pain points I encountered putting together an installer :)
This sounds like a great solution. Really cool to have it working in the browser. Would you mind elaborating on what bartender is used for? 
Your boss is a moron but fun project I guess. Good luck
If you're new to programming and want to learn it via C#, then [this is a very way to go](http://www.introprogramming.info/english-intro-csharp-book/) This is how I got my first job as a C# developer.
I think that in-process is better because you can use the debugging facilities available in IIS for years, for instance launching a debugger or other command once the process goes haywire.
It doesn't make sense to do this from scratch to me, unless the company you are working for is going to be selling an installer solution or similar. WIX is a great flexible tool for creating installers. If he want's something that is a "nice modern looking installer", can't they just get a graphic designer to come with modern looking designs for forms / images etc? Just my 2 cents.
I was thinking the same...
Perhaps if it has been in place that long, there are deeper reasons it is done that way. You probably know, but shipping a few good features is a good way to build trust and reputation with a new team. Take lots of notes as you build the manual installer, and any changes it needs over a few releases. They may come in handy if you find time on the side to build a proof of concept or demo WiX installer. Sometimes a demo goes farther than words (but don't be surprised if they say no even after a demo, and don't let a side project get in the way of your primary tasks). Good luck!
Bartender is a software used for designing barcode labels and automating print jobs. Basically, you just set up SQL tables and Bartender watches the tables and for every row that is inserted it prints them in order. We do about 7000-8000 a day using this.
I second Squirrel. Does it still count as "third party" to your boss if you just copy all the code into an installer project? ;)
What is the license? Do you have copyright, since this was produced for a company?
I am not looking to do print jobs at this point, but that is good to know you can use for this. I am going to be sending the ZPL to the printer which I will retrieve from an API. 
I have just added an MIT license now. I was the sole director of the company and have 100% full ownership of the code. Thanks for your interest!
Invest the time learning how msbuild works (what everything in a proj file does). There are plauralsite videos on msbuild. It is well worth it and will save you a lot of time in the long run. It's really not that complicated if you find a good source. 
!RemindMe 1 week
I will be messaging you on [**2019-01-05 01:56:55 UTC**](http://www.wolframalpha.com/input/?i=2019-01-05 01:56:55 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/aac8l7/live_documenter_now_open_source_on_github_net/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/aac8l7/live_documenter_now_open_source_on_github_net/]%0A%0ARemindMe! 1 week) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
i agree w the comments, it seems your data size is negligible and your diagnosis might be wrong... please share more about your tables, database, joins etc... if it's really in the thousands only i really doubt sql server being the issue...
Can you get some issues created for what help you'd like with migration? I want to get something going related to creating a docs site for my org, and this seems like a good candidate. 
Crystal reports fucked you on the update at the beginning of 2018. They changed the assembly version. There are a huge bunch of assembly mappings that can be added to your web.config to make code compiled for older versions work with the new assemblies. It’s a pain in the ass. Most references aren’t as difficult as crystal. Crystal Reports sucks and it needs to burn in hell and die. I’m also currently stuck with it.
I recommend you read [Stephen Clearey’s blog](https://blog.stephencleary.com/2010/08/various-implementations-of-asynchronous.html) on this as well as he has comparisons between background worker and tasks in performance / coding complexity. Good luck
Just to make sure. The in-process hosting only relates to IIS (Internet Information Server). IIS only runs on Windows. Compared to your setup, IIS would be equivalent to nginx. The only way it'd make a difference is if you run on Windows and opt in to in-process hosting. 
Watching Introduction to MSBuild on Pluralsight now. Looks like a lot of great stuff! Thanks!
I think it’s the principles of solid. I value some flexibility in organizing code by class. In this example, i would never prefer to have two tiny classes (stored in separate files, crowding the same namespace with similar names). I enjoy having a class that can group together a set of similar needs. I would rather look for related methods in a single class than offload the organization to namespaces and filters, and have to search for separate classes. I don’t consider the downside addressed in the link to be a significant cost- not compared to organizing ideas and concepts in a way that I see fitting. Put another way, if my project has 10,000 Foo()’s, I’d rather have like 1,000 classes than 10,000 classes
What kind of business buy-in do you have (i.e. are your bosses/colleagues on-board)? That might seem like an irrelevant question, but it is actually the most relevant question. You're asking us technical questions the result of which go well beyond this current two month deadline. If your colleagues/boss wish to see change then I'd argue you sit down and hammer out what the future looks like, and talk about how to meet the deadline while also meeting the long term goals for change. It is also absolutely acceptable to tell the business that you cannot figure out how to meet the two month deadline solo. That isn't the end of the conversation, that is the start. They'll either have to provide additional resources, including an actual plan which multiple people can implement, or live with it taking longer. The No.3 on your list makes me quite concerned for the existing code quality: &gt; Avoiding the original line of business code entirely, no inheritance, no composition, just new classes, new storage, sort of like DDD bounded context. It lives in the same codebase, shares some entities, but needs a lot of new UI and new tables. The fact that a new LOB class would require "a lot of" new UI, tables, storage, etc makes me wonder just how spaghetti it is. Even basic system layering should have mitigated some of these issues. 
Have you thought about interfaces? Make a ILoanQuote interface that defines the logic/properties that both lines of business share. This way you can make a brand new class for the new business line. You might also want to look into using a facade either to make the old class work with the new APIs or the new class to work with the old APIs. You might not a bit of both into here. You might even use a mixture of both. I think however mostly what you need is a solid plan of your goals and how you will accomplish them. Sit down and map out all the changes that need to be made and see if will work. Also is the deadline when you need to stop coding, or is that just when something needs to be in place to support the new business line? Perhaps what you need to plan for instead is quick and easy changes to get it working and then a step by step plan of how to merge the two business lines after launch. 
Good points. Part of it is me thinking I'm on my own. I have told my team about it, and they will go with whatever plan I choose since I am the most skilled (which is not ideal, I wish I had a mentor on the team). I don't trust the other dev on the team at all to be able to figure it out. I told them it could take a few weeks to refactor it properly to support the new line. I also suggested we go greenfield and jump on the aspnet core bandwagon. I'm confident we can handle it because the new line is still mostly a CRUD. Needing the new UI is ultimately because it's an old webforms app so the pages and codebehind all reflect the original line of business, and the entities are like active records so they save/load themselves. Option 3 I think will be the easiest but it's not as integrated as I'd hoped. 
Thanks for a advice. I think a plan would be a good idea, I've just been haphazardly trying different refactorings and side branches hoping one of them works elegantly. 
Read your response again and I think what you said about talking with my team about it possibly taking longer is really what I need to do. I think I'm overworking myself assuming I need to solve it alone at any cost.
If the “properties, processes, and calculations are very different”, then you should be fine keeping the new stuff separate from the old stuff. Just because both lines of business use the terms “loan quote” and “loan account” doesn’t mean they have to have be the the same types (or even share base types, interfaces, or database tables) in your code. Especially with the short deadline, I’d stay far away from the old code with this. And I’m someone who seriously LOVES to refactor badly written code. I could do it all day everyday. But from what you said, I don’t think it’s the right choice in this situation.
You're welcome :).
A couple of links that may clarify things: https://docs.microsoft.com/en-us/dotnet/standard/choosing-core-framework-server https://docs.microsoft.com/en-us/aspnet/core/fundamentals/choose-aspnet-framework?view=aspnetcore-2.2 Microsoft's long term strategy is with .net core and asp.net core. Because core supports Windows, Mac and Linux you can focus on Windows apps like you want without limiting your future options.
ILoanQuote still works similarly with his number 2. Although if he went with route two, interfaces is a much better plan.
The table on this page might help explain things: https://docs.microsoft.com/en-us/dotnet/standard/net-standard Net Standard is a standard that dot net core or dot net framework supports. So for example dot net standard 2.0 has been implemented in dot net framework 4.7, and also in dot net core 2.0. 
[asp.net](https://asp.net) core looks to be good for both front end and back end, correct? aka MS's answer to js/jsnode it seems. Then I also see this stuff on Razer Pages lol. What's up with that? :) &amp;#x200B; &amp;#x200B;
Listen to this answer. Trying to solve this on your own in two months is going to be an absolute nightmare based on what you’ve told us. I’ve learned this the hard way and I can’t stress enough how important it is to communicate with the project owners/stake holders how unrealistic that goal is. 
Quick aside on this point, unless your job title is "Dev in Charge of all Decisions because everyone else is incompetent" and being the sole point of responsibility is in your job description , I feel you may be setting yourself up for a lot of unnecessary liability. Absolutely look for direction/support from the team and make sure there is a paper trail with your bosses sign-off on it. The last thing you want is for a potential catastrophic fail to happen and then everyone points the finger at you because they said you knew what you were doing or something like that. Even if the collective makes a decision that you may not agree with at least you're covered. I know that sounds mercenary but unfortunately companies tend to eat people and move on without a second thought, it's business, regardless of how long you've been there or how many nights and weekends you spent gave up to make it work
Makes sense there would be some type of crossover. 
Razor is just Microsoft’s template engine, you can create a template for how you want a page to be rendered then when data is sent back to the browser it runs through the template engine which spits out an html file. This is useful because you can dynamically generate content. 
Core currently only supports back end coding up to when the HTML page leaves the server. Razor is a templating language that addresses HTML rendering while giving you a .net experience. There is work being done on blazor which will use web assembly to render client side content and interact with the browser object model. Until that is released, Angular and similar frameworks are your best bet for now.
Ah that's pretty cool. 
If you’re familiar with nodejs/expressjs then it’s like pug/jade. 
Ok good points. I'll keep that in mind. We keep good slack backups and so far the decisions have been "team decided" if you were to look back at the logged conversations. I try to make sure my suggestions come from a "what do you guys think about these options?" standpoint.
You probably aren't going to like what I'm going to say. In fact I'll get down-voted most likely. But I hope some truth gets through with what I'm about to say... I used to do a lot of contract work, and would come in, learn new businesses and their code very quickly. A lot of the time, I come in, see the disaster of code that is absolutely not maintainable. I had a few options at my disposal. A. Recommend they pretty much just start over from scratch. B. Find all areas where the code is refactorable, and refactor it. There better be some damn good testing in place for this one. This is close to your number 1 and 2 together. C. Add a few more bandaids. Find pivot points where I'm pretty sure I am not introducing new bugs, and have at the new logic, itll make the mess worse, but if you properly separate it out, people should at least be able to understand whats happening. If you can't reasonably guarantee that you won't INTRODUCE bugs, the shortest amount of development is probably work in the new logic with option C, and I say that with experience. Spending 2 days going through extremely complex piles of garbage of a stored procedure (or whatever), is actually easier than starting over. It doesn't feel good...but it is faster. I have gotten a few employers try to hire me after the contract was up, the one where I knew I wanted to stay, was when they went with option A. If i do a regex search through your codebase and find 30,000 if else statements. Your app isn't remotely maintainable, it sure as shit isn't object oriented. It's one giant procedural horrible state machine. Every minor detail of something changed will break the entire flow of those if else statements. I didn't even count the number of try catchs, even though they were essentially being used as state flow as well. You really wouldn't believe it with this code base it was so bad. So with that being said I don't think you should do your 3rd option. Iff you are even talking about refactoring, and adding base classes etc, it's not at that point yet. Here is the part you don't want to hear. The whole point of object oriented programming is to make refactoring easy, but if its not already easy to add new code, make objects extensible, you dont have an OOP code base. Do not feel bad that you won't have base classes, or fancy Interfaces. Just find some places that would feel good and easy to refactor/add what you can to the existing codebase. If they are only giving you two months, they already don't care about their codebase, or don't understand technical debt. It's either up to you to explain this, in order to get a bit more time, or you gotta work with what ya got.
This is really how I see it and may be the route I take. I hear you on the pleasure cleaning bad code up, I feel somewhat obsessed with trying to extract a nice base class, but it feels a little like a house of cards so that's why I'm reaching out on Reddit.
!RemindMe 2 week
From what you said and then others, it does sound like I need to really have a talk with my team so they understand the situation. They need to know that there is either a VERY long refactoring period needed to try to save it from ball-of-mud god object status. Or we build the new line on new tech. Otherwise we are band-aiding it in and adding more debt. There are unit tests but our team inherited them and i don't totally trust them. This codebase is actually what taught me the value of tests. You really can't have any confidence in changing something so big without tests. Thanks for your suggestions. I'm going to talk with my team and go very slow. 
I will do. I will also (perhaps in a seperate repository) release some code showing how the API library was used to create a documentation website that was writing the doumentation on demand. Having something like that in the manner of an API will perhaps be really useful for everyone.
It looks like there may be various avenues to look at: 1) Re-engineer your solution 2) Beef up you production platform. For 1) depending how down the line you are in dev you have various options. Just a splitting on micro services and bounded contexts per micro service may help but other things can help. Specifically you may find that cqrs pattern may help with your current performance issues. Idea is that you separate your read side of the system through materialized view of the data - think like maintaining a cached result what your slow queries are querying and hence these query run fast (and can run against in memory or special optimized dB). If you are building an erp system research REAL methodology. If I am not mistaken this is how workday is built. If I had to build an ERP system from scratch I would use REAL methodology and DDD ES and CQRS as very good fit to each other. Some other readers may not share the whole sentiment but would suggest that you research and make mind for yourself. But strategically what you want in 1 is you want you to shrink the boundary of consistency in your case due to performance issues. Because of ACID properties of rel dbs which is by design your throughput is driven by how many people can both modify and query the data at the same time within the same boundary of consistency your dB without stepping on each toes. So one strategy is to splitting your boundary of your consistency (say you have on dB per each tenant or start having one dB per each subsystem etc). Or you can have eventually consistent subsystem (say read model in cqrs) or slightly lagging Redis cache for your queries etc. For high throughput systems you may find that you still can sit on top of a single dB instance but because you use DDD design principles and your AR is pretty good boundary of consistency and you end up with low lock contentions on the dB as a consequence of the granularity of ARs according to DDD methodology. That are only my personal experiences. 2) Beefing up prod - environment. Sometimes hardware upgrades can help without massive re-engineering work. Not sure what is you current setup but adding faster IO or Ram can be reasonably priced. I do no know where you are now bottlenecking now. Any chance that you share SQL monitor snapshots - Waits, latencies, batch rates, your worst queries, etc... SQL server Always On potentially can help - you can create a secondary replica of your prod dB and then query that replica. So that you can spread the load across multiple servers. Again I am not sure where are your bottlenecks (CPUS, query plans etc). 
Thank you, LD looks very useful! Will it be possible to make it more F# friendly, starting with adding FSharp to languages? At this point, are you interested in some F# related issues opened in your repo?
Getting feedback for F# language support and any other enhancements would all be greatly appreciated at the moment! In terms of the work to support F#, it is certainly possible, but time to implement will be down to the amount of support/help the project receives. Thanks for your interest.
Why do you need “new tech”? I’ve noticed you’ve mentioned this several times in your OP and in various responses (aspnetcore bandwagon etc.). Unless there is a compelling business and technical case to switch technology I’d be very cautious about pursuing this given the deadline you’re operating to. You could very easily find yourself facing two major problems when at the moment you face one. 
To me it sounds as if what you are dealing with is technical debt, which seems to be huge. If you try to push new features into that code smell you will end up with something with even higher interest. Paying back the technical debt is crucial for sustainability in the long term. There is no way around it, the debt has to be paid sometime. Have been doing contract coding 20y+ and see this kind of stuff now and then. 1: Try to understand/extract complex business rules that you need (reuse?) in the new line of business, from the old system. Not necessarily code, flowcharts or whatever is ok. 2: Start fresh on the architecture on the new system with some DDD and context boundaries. Design with modularity/microservices with the goal of later incorporating the old system into the new modern architecture. And design the new system totally separated from the old one (resources/servers/DB and so on). Oh, and you need to talk to project management about this. Seems hard to get it done in 2 months. Sorry...
Just wanted to voice a bit of caution with developing a new system while maintaining the old. Make sure you have a plan, and buy-in, for migrating or winding down the old system, and follow through with it. Otherwise you’ll just be maintaining two systems for god knows how long. A company I worked at tried to develop a new system to replace a 10 year old application. This was attempted twice over the course of a few years (large applications) and both, for the most part, failed (for various reasons). We found ourselves maintaining three separate systems for many years. Whatever you decide, good luck!
We definitely don't need it, but it's compelling because the new line of business is different enough and the requirements aren't much more than a glorified CRUD app. I hear you though. The reason I'm asking on here is because ideally we don't have to go that route. I just don't know if it's better to isolate the new line or try to do some kind of hierarchy despite it not being in the best shape for it.
Yes fair points, thanks. Right now there's only a few staff members managing the new line of business, so if we did a new system it would just be with a handful of logins. But we would hopefully migrate over.. that really could take a long time though...
Thank you. That's a fair analysis. Digging into it has opened my eyes to the reality of the system and how it is sort of teetering on the edge of ball-of-mud status. It feels like a make or break moment where this decision could accelerate thing either very bad or to a more sustainable future.
PLEASE just use .msi, nobody wants to look at their installer all we do is installer /s /noninteractive /noreboot /dontYouDareShowAnyFuckingUI anyway and .msi is the industry standard, official first-party installer format for Windows that is manageable, deployable, upgradable and easily uninstallable not to mention has decades of bug-fixing and worldwide testing under its Nelly Tell your boss it's a funny idea but in the real world an installer only needs to be two things: 1. silently and non-interactively deployable 2. work without bugs on every Computer CHOOSE MSI
It honestly sounds like his intention is to get you to quit
If it's an utterly awful code base and you have any option for a b.a. to draw out the features and behaviour I'd just Greenfield it
I really like [Jamie King's](https://www.youtube.com/playlist?list=PLRwVmtr-pp06rfSgNYu_oBg40DkwXiRHt) introduction to assembly/csc basics to understand where the compiler looks for dlls
We have started a project over an older application a few months back. First decision i made was to design tests around the old behavior so that we gained insight into the system and we have a safety net for changing future behavior. It was not easy but it has paid us back in triple evrey step along the way. So i would really start there. Understand the system first and then start the changes. I am not talking about unit tests either and i am not talking about changing any behavior for a start. You can loose some encapsulation so that you are able to test better, but those are all tradeofs. The next step would be to engineer a way in to consolidate the code, but the tests might give you some ideas on how to do it without messing the rest up. At least this was how i did it. First created the tests and understood the system and then we were in a better position to make decisions about the architecture. 
Microsoft is past the crossroads point. The ".NET Framework" is coming to an end as version 4.8 will be the last one. 
Most likely if you're using some of the other Azure services such as Azure Functions, Azure Active Directory, Azure Storage, Azure App Service, Azure Analytics, Power BI, IoT, Cognitive Services, Machine Learning, and more. [Azure SignalR](https://azure.microsoft.com/en-us/services/signalr-service/).
I am talking out of my ass here, but it probably has to do with billing structure. Running the web app requires constant CPU and resource allocation, and you will be billed based on that and network. The signalr service bills by message and concurrent connections.
I used it for work in a project were we would need to send millions of messages per day to thousands of users. Could only do that with Azure signalR
Did you start with a self-hosted signalR service?
Yeah we did, but because of the amount of connections and the amount of messages we had to change.
Probably better to ask this on stack overflow. 
I think someone would need a really compelling reason to not use Task / Task&lt;T&gt; - it's the only implementation that does not significantly stray away from the synchronous programming models, and is likely the most efficient when throughout increases. If you do have a compelling reason, I would even question if it can be handled using a Task based implementation anyways.
If you're confident you'll have time to refactor after the 2 month mark, here's one way to hack up a prototype and meet the deadline while leaving yourself in a good position to streamline the design. Write new code for the new line of business (similar to your option #3), but try to keep its structure as similar to the original line of business code as possible. You could even start by copy-pasting the old code into a new folder/namespace and editing it from there. Once the new LOB is implemented, compare the two code bases side by side. Notice which parts you had to change and which stayed the same, and refactor to get rid of the redundancy. Forget the philosophical stuff about entities and OO design; your goal is simply to enforce DRY and make sure each bit of logic is only implemented once. For the parts that stayed the same, extract base classes or components you can use in composition. For the parts that changed, extract interfaces or let the class hierarchies diverge if that's reasonable. *tl;dr: clone the old code, modify the clone, then refactor as you merge them back together.*
The Signal R service isn’t just for heavy loads. If you check out the [pricing structure](https://azure.microsoft.com/en-us/pricing/details/signalr-service/) you’ll note the first 20,000 messages/unit/day are free. One benefit comes at its scaling abilities. You immediately have access to its massive scalability. Compare that to a web app where scaling out and in takes time and can result in performance problems while scaling is taking place, of not done right. Another benefit is simplicity. It’s a bit easier to maintain this than to maintain a web app. More info from Microsoft’s documentation can be found [here](https://docs.microsoft.com/en-us/azure/azure-signalr/signalr-overview-scale-aspnet-core#why-not-deploy-signalr-myself) 
Agreed. Much less chance of beeaking existing stuff that currently works. It should end up easier to QA as a result.
download something called fiddler4 and you can watch every URL call individually. one of them will have a `Cookie` header. that's where it's bound. none of the code you write is involved. you'd need to use a decompiler like dotPeek or justDecompile on the authentication DLL to look for that same `Cookie` header being applied to see exactly when it happens.
It’s done via the [SignInManager](https://github.com/aspnet/AspNetCore/blob/master/src/Identity/src/Identity/SignInManager.cs). Check the SignInAsync method. You’ll see it calls the `SignInAsync` extension method on the HttpContext. In this method, your authentication service is looked up and it’s `SignInAsync` method is called. Eventually you’ll get to the [CookieAuthenticationHandler](https://github.com/aspnet/AspNetCore/blob/master/src/Security/Authentication/Cookies/src/CookieAuthenticationHandler.cs) where you’ll see it create the cookie. You will also note that this method makes use of your LoginPath that you specified in ConfigureServices. I did all this code lookup on my phone so I may be crossing versions but the gist is similar between v1 and v2. I did a deep dive in ASP.NET and ASP.NET Core Authentication once they were open-sourced. I would recommend anyone do the same. Clone the project, add it to your solution and reference it, and then step through all the managers required to auth a user. Once comfortable, you’ll be able to roll your own. Hope this helps, let me know if you have any follow questions.
I sometimes look at open source projects to get a feel for how others solved the problem in my same domain. I found an open source loan management system. Here's their Loan type. Maybe you'll garner some ideas on how they run the core engine through loan policies. So you could just have a set of Loan Policies and Quote Policies with their own custom rules but use the same LoanProduct. [https://github.com/PavelBastov/opencbs/blob/master/Src/OpenCBS.CoreDomain/Products/LoanProduct.cs](https://github.com/PavelBastov/opencbs/blob/master/Src/OpenCBS.CoreDomain/Products/LoanProduct.cs)
There are a few cross platform GUI frameworks available, but all under development, and none by Microsoft. Avalonia and Qml.net spring to mind. 
Hey, In the Login method, I’m using SignInManager. PasswordSignAsync and not SignInManager.SignInAsync. I think they are different?
Xamarin supports macOS and is developed (now) by Microsoft and provides native access to the Mac APIs from C sharp For cross platform, Xamarin Forms can target macOS and UWP and is maintained by Microsoft. There is also a community effort to support WPF.
So you can develop desktop applications with Xamarin? 
Check the source, here’s what the stack will be: PasswordSignInAsync =&gt; SignInOrTwoFactorAsync =&gt; SignInAsync
I think so(based on what people are saying) But I think that the big issue is that Xamarin's tooling is so bad and buggy so development with it wouldn't be so good, I guess.
I've had good luck with [Eto.Forms](https://github.com/picoe/Eto), but I've only tested on macOS and only for a toy app. It seems pretty solid and provides a wrapper over native controls (using Mono). I was able to tweak some of the native bits pretty easily and I've been happy with it.
Yes, using Xamarin.Forms or using AppKit directly (just like with Objective-C or Swift). Visual Studio for Mac is a thing now (was previously Xamarin Studio) and it's not nearly as good as Visual Studio for Windows but it works if you want to write C#/F#.
Dude, i wanted to come back and thank you...we actually have SSRS, but our "senior" DBA is kind of a retard. anyhoo, http://gotreportviewer.com/ TOTALLY hits the spot. works beautifully! thank you *so* much!
What I find a bit confusing is that the SignalR service doesn't actually host the code upon connecting or sending/receiving messages, right? 
Anything for Mac is shit because Apple wants you to use their own garbage, mono works surprisingly well on Mac.
this.. every xamarin update always break something. And Xamarin Forms is slow
Although I see where you are coming from, they are improving. This year end’s tools are a far cry from last January as far as build speed and deployment are concerned, for example.
FWIW, Rider lets you make cross-platform Xamarin apps for iOS, Android, Mac, tvOS, and watchOS. And it beats the pants off Visual Studio for Mac too.
Slow to develop on or slow as in, final apps are slow? Also... Do you have any numbers to back this up? 
Visual Studio Code [Download ](https://code.visualstudio.com/download) 
I've had good luck with these Gtk3 bindings: https://github.com/GtkSharp/GtkSharp For OS X, you'll need to grab the native gtk3 libs via homebrew.
Look into .Include (and .ThenInclude). 
I really want a cross platform WinForms. I miss the days when I could knock up a professional looking app in an afternoon, compile, and ship it. 
Thanks. Include did the trick.
Really not trying to give the most optimal/ most native solution, but if somebody wants to see more options, then here ya go: - Electron.Net (works with .Net Core) - ReactNative.Net (haven’t tried myself yet) 
I think Xamarin is the closest you're gonna get. "Cross-platform" now means more than just "across Windows, Mac and Linux," it means across desktop, mobile and TVs. Xamarin is quirky, but you're writing code that could run across a Mac desktop, an Xbox, and an Android phone.
That's everybody! So it seems like this is still a big bag of hurt huh? A bunch of third party solutions that are probably not awesome and Xamarin is still as buggy... No wonder Electron is a thing. Thanks again everybody!
Check out TallComponents as well
ooh good idea, thank you!
Thats true. I've wondered about code-consistency with this. It's a webforms monolith with active record style entities but there isn't anything stopping me from creating an MVC "Area" in it, using MVC, repositories, DI, etc. It would be quite a different architecture and appearance, but could act as a model framework if we ever were to try to rebuild.. There is a lot of freedom allowed with the decision.
Given time we’ll see WinForms and WPF on Mac and Linux bit that isn’t now. Wait until about a year after dotnet core 3 is released. 
Yep - monoliths can be improved over time, and your new code should lay the groundwork for future improvements. I’ve done a lot of work converting WCF services to web-api for newer clients using web technologies. Migrated some EF6 stuff to Dapper along the way. Much more satisfying to leave a project (arguably) better off than it is to just do things the old way because the old way is already there.
Why not .net core?
Careful - Microsoft's choice of names has caused some confusion in the community. Razor Pages make use of the Razor template engine, but they are not the same thing. Razor Pages are an alternative to MVC. The default routing matches the folder/file structure under the project's Pages folder. Instead of being routed through a separate controller, the actions are handled through the OnGet and OnPost methods in a Razor Page's code behind file. Anyway, there are three main approaches with ASP.NET Core: - Web API: https://docs.microsoft.com/en-us/aspnet/core/tutorials/first-web-api?view=aspnetcore-2.2&amp;tabs=visual-studio - MVC: https://docs.microsoft.com/en-us/aspnet/core/tutorials/first-mvc-app/?view=aspnetcore-2.2 - Razor Pages: https://docs.microsoft.com/en-us/aspnet/core/tutorials/razor-pages/?view=aspnetcore-2.2
dotnet core only supports web and console apps (in the LTS version anyway)
That's not an inherent limitation. There are third-party desktop UI libraries for .NET Core, like Avalonia, and WPF and WinForms are being ported to it (though will still be Windows-only).
Do you think they can decouple so many parts of Win32 from WinForms? I don’t think Microsoft will.
Miguel de Icaza (developer of Xamarin and Mono) has some suggestions here. https://www.reddit.com/r/csharp/comments/9za988/crossplatform_gui_tools_for_mono_c/ea8naoh
final apps slower than native.. but my experienced has been with Android and IOS apps developed using Xamarin Forms, not desktop apps.
Agree Microsoft unlikely but with WinForms and WPF both being open sourced if the community wants it it can be attempted by the community 
I suppose if OP is developing on Mac. They didn't say that though. If you don't want to install Java and run an IDE built in Java to make .Net applications, stay away from Rider.
Does it matter what the IDE was written in? Rider is a full .Net IDE. Sure it's not quite on the same level as Visual Studio, but it has all the tools you need. I use it on Windows for the rafactoring and code analysis (or when I just want to mix things up and get away from VS), and on my Ubuntu laptop when I'm away from my main dev machine
That's a very strange definition. Async does not mean you use the same thread (how would that even work?) and has nothing to do with threads. It's about preventing blocking execution by scheduling the work elsewhere and resuming once it's ready, regardless of what that work actually is. Offloading is not really a technical concept, and much higher-level than threads and async.
to be fair, beating the pants off visual studio for mac isn't exactly a difficult feat
SignalR can be confusing because it is both a library and a system for real-time communications. Basically you have clients that are connected to "hubs" and they can send messages back and forth. A common example is a Javascript client running in your browser that's talking over websockets to your backend server which runs the hub. This works fine if you have a single server but what do you do when you have multiple servers hosting your site? Clients connected to server A won't be able to talk to server B, and each server will hosting isolated hubs. This is solved with something called a backplane, which is basically a messaging system that connects all the hubs together and allows for any client or hub to talk to each other even as you scale up your front-end servers. The problem is that the backplane is just another thing you have to run, usually Redis or something else that supports pub/sub, and eventually you have to scale that too. Azure SignalR service is a managed backplane that you connect to and get a scalable messaging system for all your hubs. If you have a larger site or want to be able to have persistent connections that don't get reset with server changes then it's a good choice, and pretty generous with the free tier.
But the hub code wouldn't be hosted in the Azure SignalR service?
No, the hub code is in your app. It use the backplane to connect multiple hubs so the whole system works across servers rather than being isolated within each server. You can read more here: [https://docs.microsoft.com/en-us/aspnet/core/signalr/scale?view=aspnetcore-2.2](https://docs.microsoft.com/en-us/aspnet/core/signalr/scale?view=aspnetcore-2.2) &amp;#x200B;
It really just depends on personal preference. There are some people who really can't have the Java ecosystem on their dev machine for specific reasons and knowing this is important.
Only Avalonia is multi platform and it is still a work in progress. Microsoft really lost an opportunity here.
Avalonia is decent, although it lacks documentation and is somewhat buggy 
Just wanted to comment as well because the other commenter seemed to think k you meant Razor when you said Razor Pages. Razor is indeed the templating language, uses to transform data being sent in an HTTP response into HTML for the browser. But Razor Pages is something new where instead of the MVC approach to web development (which describes three layers you program) you have a more old school style of web development where each route gets its own source code file. If the route is a GET, there's one method in the source code file. If you want to cover other HTTP verbs you have more methods in that file. It's an approach similar to PHP development without frameworks, where you just had separate PHP source code files for each route you had and you did everything in those files without splitting your code up into lots of layers. MVC and Razor Pages are the two styles of web development there are in the ASP.NET world now, and Microsoft plans to support both of them.
From a great distance I initial thought that the key aspects for a UI platform should be integration with a good toolkit, and being responsive and dynamic so you can quickly see and experiment. And that C# on the Mac is zero for both. It is a compiled language largely unrelated to the native Mac UI platform. Isn't Swift a language designed for the problem of Mac UI applications? 
They just open sourced WPF, so it shouldn't be long before full windows experiences can be ported.
[Good news!](https://blogs.windows.com/buildingapps/2018/12/04/announcing-open-source-of-wpf-windows-forms-and-winui-at-microsoft-connect-2018/)
Not likely. WPF is built on DirectX. That would need to be ported first, which would be a huge undertaking since it would mean introducing a completely new graphics API to OSX. Possible, yes. Probable, no.
They already have it. They own Mono and it has a version of WinForms that doesn’t require Win32. I should say, it has It’s own replacement for Win32 obtained from the Wine project. It’s already open source. The community just needs to put them together. As far as WPF, I think it’s just a matter of hooking it up to another renderer. Some of this work has already been done with Xamarin since it has a version of the XAML with it. When Microsoft purchased Xamarin they knew full well what they were doing. 
That doesn't mean cross platform at all. As they have said before, it's windows only and they will never work on porting it to any other platform.
Starting with 3.0, the will support WPF as part of .NET Core. That could open the possibility of swapping the low level graphics access from DirectX to a plaform independent abstraction layer. Also applications will be standalone executables, so you can use even SDL to create windows and stuff
Also while Visual Studio has many more features, I find Rider's editor and analysis tools far superior to those of VS, not to mention the look and feel (although this last point is, of course, subjective)
Yeah, that's what I meant. Sorry for the confusion.
I hear ya, I started my journey thinking on the Python/Django route. Poking around in VS2017 under [asp.net](https://asp.net) core seems Reacts/ and Angular are options. But as my thoughts were coming together I felt C# is my best bet as I wouldn't mind attempts at small apps. Joe W
Your post has been removed. Self promotion posts are not allowed.
Dear god. Forms and professional? We're 2018 already.
If the old system works within specifications then don’t rewrite or improve it. Take a look at the code you need to reuse and try to isolate it. For example there’s a part of the class that calculates interest. It might grab a few parameters from inside the class so it needs to be made into a pure function first that takes everything as a parameter and does not touch anything inside the class anymore. This function can easily be reused or moved to a different part of your code. Separate out all of the stuff that works and needs to be reused in this way. Don’t fix bugs, don’t add features. If the function is largeish then break it up in smaller functions. Probably you’ll add new classes once patterns emerge like LoanSaver or InterestCalculator but never ever improve or change code. Make sure QA keeps testing the old system while you do this. Build your new system around the reusable bits and add anything you need for the new.
Not exactly, you connect to the hub in your app/function on your first call but on the `negotiate` call, the response actually points the client to your SignalR service. The service acts as a proxy to maintain those connections but will use your app to invoke the methods you defined in your app. This is why functions are big deal, you can use serverless to call this code while maintaining constant connections via your SignalRService. Some helpful flows can be found here: https://azure.microsoft.com/en-us/services/signalr-service/
I doubt it. I think way more effort will be put in Avalonia for cross platform desktop development.
Yeah GTK# works extremely well on Linux, haven't tried macOS.
Are there tools you use but don't do things the way you want, or you think you can do better? Make your own version. You don't need a totally original unique idea.
It doesn't really matter, when I am looking for someone I just want to see how the code etc is structured. I don't really care what it does. Doing a simple thing well can be better than a half baked complex thing. If it is something you will actually use then all the better (eg a blog engine with a blog detailing the tech choices hosted in the engine itself!) 
People aren’t going to hire you for your big ideas. They are looking for problem solving and programming skills. You can probably just contribute to a lot of open source projects. Or build your own utility library. It doesn’t have to be earth shattering, it should display the quality of work. 
What's important is not your idea because it's the client's job to come up with that. What you, as a dev, need to be able to do is write clean, structured, extensible code that's bug-free. Just pick something like a blog and then plan it. Do a bunch of diagrams and apply OOP principles to them. Write Unit Tests. Create mockups. Do everything you would be required to do in the open field. Then maybe create a repository where you upload everything and paste the link in your application. Maybe set up a working demo so people can see what you have done in action. Stuff like that would impress me far more than if you had an awesome idea but nothing really to show for it.
\&gt; Like as a software, all of the ideas are taken and done like great apps &amp;#x200B; Originality is overrated. &amp;#x200B; If you were going to open a restaurant - nobody would expect every dish to be original, right? If you opened a pizza place, nobody would say, 'Umm, didn't Pizza Hut already do this in the 50s?' &amp;#x200B; If you are building a portfolio, the goal is to show off your ability to write code - not design and launch the most original piece of software in the world. &amp;#x200B; Besides, almost all of the most successful software we are familiar isn't original at all. Imagine if the world stopped writing word processing software after WordStar in 1979 because, 'it wasn't original'. Six Degrees is generally recognized as the first real social network site and most people haven't even heard of it. Everything that came later wasn't original, but they were successful. Pokemon Go was almost a direct clone of some other game they'd already written, just rebranded - not original. Find something you are passionate about. At least a little bit. Pick that. If you like classic RPG games, write a classic RPG game that isn't very original but is a good game. If you like to workout, write an app that tracks your workouts. There are 50 others out there, but yours will be yours. And when you go to your job interview - you'll be able to explain it in great detail all of the technical/design choices you made. They won't care what the app does, they'll care that you wrote and can speak intelligently about how and why you did what you did.
Create things that you use, that way you have a reason to create them besides for a portfolio. When asked why you created it; "Because I needed it and use it" is a very good answer
Hey Joe, I think at this point, learning [ASP.NET](https://ASP.NET) Core MVC seems like a logical path for you. With [ASP.NET](https://ASP.NET) Core MS have simplified a lot of things (around configuration etc) which means, for the most part, the framework should keep out of your way, freeing you up to figure out how to build your features. Along the way you'll learn a lot of the fundamental concepts surrounding developing for the web (including handling/routing HTTP GET/POST requests etc.) which then leaves you in a good place to move on to other options such as returning data via Web API for consumption by a frontend (SPA) framework (think React, Angular etc) if you so wish. The good news is that a lot of what you'll learn in order to get [ASP.NET](https://ASP.NET) Core MVC up and running transfers very easily to using Web API. Essentially you just go from returning views plus data like this... ``` csharp public IActionResult Index(){ var data = _someService.GetData(); return View(data); } ``` To returning just the data... ```csharp public IActionResult Index(){ var data = _someService.GetData(); return Ok(data); } ``` Where the data can then be consumed by a separate front-end (React etc.)
Something I need and haven't been able to effectively implement myself is looking up a movie or TV show on IMDB. Given [title] [year], e.g. "The Incredibles II (2018)", look up the video and return the URL for the page. My method involves searching IMDB itself. It's only about 95% effective, and it needs to know whether the video is a movie or TV show. It would be great if your version didn't need to know whether it's a movie vs TV show. I saw a Python implementation that uses the Google API. I haven't explored that yet. Just an idea. Please PM me when you post your solution to GitHub. :)
You mean, we're **almost 2019.**
Appreciate the info sir. Curious, I know I mentioned in my initial post about staying in the windows world. My main work system is Windows 10, but I've actually been using Fedora for a while now, when I was learning Python/Django. As from my understanding and research since we're in a cross platform area now it looks like there's no reason why I can stick with Fedora...Pros/Cons?
I'd say work on your grammar first. 
You can hack together a Xamarin.Forms app with mono develop on Linux: https://stackoverflow.com/questions/48693220/can-i-build-and-compile-cross-platform-xamarin-apps-on-linux
If you have access to a Win10 machine you would have a much better time using that. With WSL you could use the ‘nix command line tools you’re used to using but also have access to a XamL editor and much more supported tooling.
Sorry if english is not my native language. If you want we can both speak my native language and then see who was to work on the grammar
Yes, with Visual Studio Code you can. VS Code is crossplatform and open source. It supports all Xamarin libraries.
Personally, I view having side projects at all as a good sign when hiring someone. So many developers don’t learn outside of their jobs, and that’s where you’re able to tinker, play, and learn the best. Tech changes so quickly, and side projects (even if it’s a simple to do list app) help you learn and keep with up with new things.
Don't controller names have to end in "Controller"?
Could be, instructor named it like that so I followed, but I don't think that's an issue, since I'm hitting the controller with Postman.
Take a look at something like PuppeteerSharp: https://github.com/kblok/puppeteer-sharp. It’s a wrapper around headless chrome and can convert pages to PDFs: http://www.puppeteersharp.com/api/index.html#generate-pdf-files
Yeah, the problem with that though is people have lives and responsibilities outside of programming and don't always have time to dedicate to side projects. Not to mention, a lot of people would rather not be stuck at their computers doing work with no compensation. It's not really fair to those sorts to ding them for not maintaining any side projects.
Aren't you missing the following statement? app.UseSpaStaticFiles(); I'm also pretty sure you can define the spa.options.sourcepath to the folder where the Angular app is built. This can be done inside the Use Spa statement. Normally it looks some thing like this: app.UseSpa(spa =&gt; { spa.Options.SourcePath = "ClientApp"; if (env.IsDevelopment()) { spa.UseAngularCliServer(npmScript: "start"); } }); This will use npm start in debug and use the build inside folder clientapp in release/production. 
I guess I do, but only in Docker containers. Haven't really messed with installing the framework on an actual system. Also note I don't think you can run .NET Core on a Pi Zero.
If you're wondering why it's because the Zero doesn't support some ARM instructions the CLR requires. It's very unfortunate.
Tried it last year with Mono, but the performance was not great. .NET Core might be better, however I decided to use Go for this purpose to minimize overhead on the rpi.
I have built some apps for fun and learning, targeting Rasbian and using Unosquare.Raspberry.IO (Nuget) to communicate with the hardware, like GPIO and SPI. I even contributed code for using an RFID reader module. Also been experimenting with ASP.NET Core. You‘ll find my projects on GitHub: http://www.github.com/robertsundstrom
I am using dot net core for an API with MySQL Entite Framework. I don't know about specific performance but so far I haven't noticed anything that was slow.
&gt;Aren't you missing the following statement? &gt;app.UseSpaStaticFiles(); Honestly, I don't know. I follow the course and the instructor didn't add that. He was getting 404 until he added "MapSpaFallbackRoute()" after that everything worked normally. I'm trying to somehow send a token on refresh, but I'm not sure will that solve the issue at all.
Has anyone used a Pi for the entire workflow? As in using something VS Code in a Raspian desktop environment to create the code, then run it on a Pi in production? Side note: it's a crazy world that we can do this now, all on a Raspberry Pi for £30!
How do you use it, is it just to try out new stuff or do you use it in production? And why do you use the Pi over a "regular" computer?
What sort of application were you working on? I've used the official packages on a Raspberry Pi and the performance seems OK, but wasn't doing heavy db lookups or computation.
I use the Orange Pi Zero instead, £5 for a board, absolutely tiny and has a full size USB and ethernet port to boot.
I use it for everything I write for my home automation system, so nothing heavy weight or anything, mostly performing translation / integration between various services / device protocols and MQTT. I like the footprint. I use Pis for a couple of different one off things. Their biggest issue is they really should have about twice the memory for my purposes at least, and that SD cards are terrible storage mechanisms for an OS doing constant writes, as it kills the card over time. I'm actually just starting to upgrade my primary home automation Pi to an i5 Intel NUC though. It's an x86 architecture, I can slap 16 Gbs of RAM in it and it'll handle Plex transcoding / RTSP restreaming MUCH better, and can use a better suited M2 SSD for a system drive that I don't have to worry about SD corruption with. The Pi was nice to start with, but its time to upgrade.
OpenCVSharp on a Pi can be surprisingly performant! I've done some face detection and recognition, also a cool project where my Pi + a webcam pointed at my bird feeder would try to classify bird species and collect data about which birds were coming and when and post it to a web service on Azure. 
Oh cool! I run Home Assistant on a x86 Ubuntu Server, but spent a while playing with rolling my own home automation, I could never find a good way of interfacing with my IKEA lights though so abandoned the project for HA
Check out the [Meadow Board](https://www.kickstarter.com/projects/meadow/meadow-full-stack-net-standard-iot-platform) for an IOT board with full .NET Standard support.
That's pretty cool, have you got any good links for using OpenCV? It's something I've always thought about trying to use but never gotten round to.
How had I not heard of this!? Thanks!
I was gonna write my own as well until I found node-red and MQTT. MQTT is the core of my system. All UI, processing, automations, integrations, etc. must ultimately speak MQTT. Most of my current stuff is just a bridge between SOMETHING and MQTT. That lets me cut out HASS, which wasn't my cup of tea, and run pretty much all the interesting stuff directly in node-red where I have a lot more control.
I'm using .NET Framework with Mono on the Pi a lot.. mostly for reading sensors, talking to databases, simple Web UIs and computer vision tasks. I'd have preferred to use .NET Core but it didn't support serial ports on Linux at the time (not sure if it does now..). &amp;#x200B;
Not sure if it supports serial either, but v2 bought a lot of good stuff in that was missing, might be worth another look.
I tried running .NET Core-based projects both "raw" and Dockerised within Raspberry Pi 3's a year or so back. They worked, but I suffered performance problems and was moved on to other projects before I got a chance to do much more. 
&gt;If i do a regex search through your codebase and find 30,000 if else statements. Your app isn't remotely maintainable, it sure as shit isn't object oriented. It's one giant procedural horrible state machine. It's like you can see into my nightmares.
Yeah I've been using .NET on Pi for a few years. It reads from a [DS18B20 1Wire temperature sensor] https://datasheets.maximintegrated.com/en/ds/DS18B20.pdf The Linux kernel already implements the drivers for this, so I simply have to read from the exported virtual file system for it. (and then take into account a bunch of bugs and assumptions and incorrect failure paths the kernel drivers don't take into account - I should probable try fix the issue in the kernel driver and get it merged upstream)
I started down the dotnet core path for an app I was creating for RPi. It basically took data from an Arduino telling me what coins were given to a coin return and I was playing a video in response. I found that dotnet core doesn't have good serial port support on Linux unless you download 3rd party libraries but Mono has great serial port support using classes from the .NET Framework (SerialPort). I ended up loading Mono on my RPi and using that. Well, until we ran into a show stopping bug in OMXPlayer but that's another story. 
To expand upon your pizza example, which I really liked, if I were to higher a chef (coder) to implement *my* idea, as nobody is hiring you to implement *your* idea (I guess Angel/VCs would “hire” you for that), I’d be interested in how well they can execute on staples, and then maybe more advanced cooking concepts, not on how creative and original their own creations are. Up one level, how well they can run a kitchen (project), if they are personable (work with team mates), if they can manage the line process efficiently (SDLC), if they know standard food hygiene practices (code standards), etc All of those can be shown within the confines of public repos and how you interact with other people.
The SDK doesn’t run on the Pi (yet) only the runtime. I develop and build for Linux-arm on Windows and macOS then SCP to the Pi. It’s slow to startup an app but once it’s running its plenty fast. I’m using it in production though we’re still in early betas. 
The SDK has run on ARM for a while now, since the release candidate for v2.1 I believe... [https://dotnet.microsoft.com/download/dotnet-core/2.2](https://dotnet.microsoft.com/download/dotnet-core/2.2) &amp;#x200B;
How are you running the application, using the dotnet run command or precompiling it specifically for the target platform?
Oh nice! I haven’t checked in a while. I’ll definitely update in the new year. 
Pre-compiling (dotnet publish -c release -r linux-arm)
Yeah they have nice packages in a proper repo now too so the process is super easy... [https://dotnet.microsoft.com/download/linux-package-manager/ubuntu18-04/sdk-current](https://dotnet.microsoft.com/download/linux-package-manager/ubuntu18-04/sdk-current) &amp;#x200B;
for a while? 2.1 was literally the latest release as of a few weeks ago.
Break Spoon-Web up into two packages - call the second one Spoon-Core (with your DTOs and interfaces that Spoon-Postgres needs to return to Spoon-Web). Spoon-Postgres should depend on this new package, and not know about Spoon-Web at all. The end user will install Spoon-Web and their choice of Spoon-DatabaseDriver in their solution.
Release candidate for 2.1 SDK on ARM was available on 17th May..... so..... in the context of a conversation with someone who didn't think it was available at all, yeah, roughly 7 months, a while. Not sure what you're getting at?
Conceptually I get that this would work, but is this the norm? Seems to have a lot of drawbacks in terms of the client having to managing package dependencies on their own instead of the library doing it for them and ensuring everything is good to go. Are there other OSS projects out there that do this, with them trusting the user to install a corollary package and which version to install?
Dude, that sounds cool. I like birds. How did you get on with recognizing/classifying bird species? That sounds crazy complicated but fun. I'd like to look into it myself!
I don't think the client ends up having to do all that much additional work. If you follow SemVer and set appropriate version ranges, they now have to manage dependences on two packages (Web and their choice of database) insetad of one. They don't need to do anything to manage the versioning for Core; NuGet manages that for them. (e.g., you specify that Spoon-Postgres 2.0 can work with any version of Spoon-Core from 2.0 to 2.999. Once they upgrade Spoon-Web to 2.1 that depends on Spoon-Core 2.1, NuGet will automatically upgrade Core for them.) Entity Framework Core follows a similar model - the project with the client Context will reference the core EF packages; the specific provider packages will only be referenced in the end application project (usually InMemory for the unit test project, and then SQL Server or another real implementation in the front end project).
The bird recognition was done by loading someone else's pre-trained caffe models (VGG-16 i think) OpenCVSharp has an easy set of classes for that. The detection bit was a combination of background subtraction and FAST or SURF (may have been both?) Algorithms. One of the most tedious parts of the project was compiling opencv and opencvsharp on the pi. Also setting up https://github.com/unosquare/sshdeploy saved a lot of time on doing deployments easier and more frequently. 
In this paradigm, just to make sure I understand... A user wants to use Spoon with Postgres, so they download the nuget package for Spoon-Postgres. They then would have to manually download the package for Spoon-Web, by reading some instructions somewhere. Then updates, can happen to each package independently, but they would not be totally away of each other? How would Semver come in there and where would I set it? 
My point is that in the big scheme of things, 7 months is not long at all.
It involved processing near realtime financial data through a websocket connection. The calculations made could make the rpi really slow from time to time. The Go equivalent used less RAM and was a little faster.
AFAIK VS Code doesn't support mobile stack. At least officially. 
What about the netduino? 
As far as .net goes, rn I use an rpi to host discord bots written in c#
I'm currently implementing JWT authentication into my own web service, and found this to be helpful, thanks.
Glad that you found it helpful. Thanks for the feedback!
I think a recently read that net core 3 will support ARM.
Looks like you are adding Authorization to all controllers by default. So Fallback requires authorization. &amp;#x200B; Maybe the instructor has \[AllowAnonymous\] attribute applied to Fallback controller? Could you check that?
You don't have to install java, Jetbrains IDEs come with their own runtime
Good write up. Thanks!
And again you copy some other guys blog post and post it like it is written by you! [https://www.reddit.com/r/dotnet/comments/a9422k/entity\_framework\_core\_string\_filter\_tips/](https://www.reddit.com/r/dotnet/comments/a9422k/entity_framework_core_string_filter_tips/) [Entity Framework Core: String Filter Tips](https://itnext.io/entity-framework-core-string-filter-tips-768139b55ffd)
Yep that's right. Linux hosting is often cheaper so it's useful to be able to deploy to either Windows or Linux. For development, you can use Visual Studio Code to develop asp.net applications so you may find it more comfortable / familiar to use that and stick with Fedora. If you're keen to use visual studio though you'll need to switch to Windows. I've never actually tried to develop on Linux using vs code myself but do know a few people who do and find it productive! 
Yeah I hear ya, pretty happy with Rider on Fedora. Worst case I run into issues I'll just move it to my windows workstation. What do you do in your career? 
Talk to myself I guess :)
You could try DCoder. I wrote a few uni assignments uwing it and it went relatively fine. Other than that, you can install Linux on your phone, install .NET Core, Vim or Nano, and go with that.
&gt; The slowest part, are when developer use for-loop to fill data for calculated field, which cannot be calculated with LinQ to SQL query. You could just use EF's `.FromSql()`. You could also use a micro ORM like Dapper. But don't discount the cost of bringing in an additional dependency. &gt; But something like I said wonder me that is it really better if I try to optimizing query performance if I still using this SQL Server design with Entity Framework or just move to a completely approach like Dapper would be smarter choice? I've written some pretty hairy queries against EF and been surprised at how well they perform, but there is a fair amount of SQL you just can't produce with LINQ. That said, you're probably going to save time dropping into SQL, and likely will find it easier to maintain. You should be able to do that easily with EF, and I doubt it's necessary to bring in another DB library.
Nice work, thanks!
Node red and mqtt make it so intuitive to maintain a home automation setup
If someone knows enough to know that they want to use spoon, I'm assuming that they'd be looking at instructions on your site or wherever that would tell them they need to download both packages (spoon-web and spoon-pick-your-database-driver). It's not the most discoverable, but I don't think this kind of package was ever something that someone would find through browsing NuGet.org *anyways*. In your docs, though, I'd probably approach it the other way. Start with saying you need spoon-web, and then you need to pick one of these X database driver packages (or create your own). SemVer would come into play when you're numbering your packages. https://semver.org gives a good overview. Basically, it provides a good way to know whether two versions of something are compatible with each other. If you follow it and construct your NuGet packages correctly, then NuGet will make sure that all of the users' packages are using compatible versions automatically.
If you were biased towards .NET you would have written a third of the words and communicated twice as much. I'm being a bitch, this is the one night there are literally no rules. Or is that a movie I saw. Who cares, Big hugs :D
I could have written just the last two sentences to be fair, but then I would have given no context. :P
Tell me, is the reason you're so big on Node.js so you can write the same programming language on the both client and server sides? The big breakthrough in ASP.NET Core is going to be [Blazor](https://blazor.net/). It leverages WebAssembly to run .NET Core in the browser on the client side, which means that you can write your server and client code all in C#.
Identity paired with identity server 4 has a lot of boilerplate but it's super powerful and flexible. I love entity framework, but most of this sub seems to not care for it and opts for Dapper. I would recommend at least trying it, however, but really read on how it works and best practices. It has a few gotchas. Other than that I don't think it's too hidden, it's kind of like Angular, a lot of indie devs will be using node but plenty of enterprises ran on framework / moving to core if they haven't already. It's a great time to be a C# dev
I messed with and have done research on Blazor too, watched the main panels about it and read docs. Seems pretty cool too, I think at the moment they are mainly looking to reduce size as it is a bit large? Mainly I was interested in front-end development and learned Vanilla JS, React, common templating languages etc, but I felt like I wanted to build my own API's as it felt really limited only being able to create front-ends... So I learned some backend too and naturally chose Node you see. &amp;#x200B; As my main language though was JS (though I did do Java at Uni as a mature student) I wanted to improve my general OOP skills and have a more general language, C# seemed interesting to me. It seems pretty decent so far along with [ASP.N](https://ASP.Net)ET Core.
The thing you seem to be missing is that Microsoft has already had MVC for a long time and a ton of people are weary of Microsoft (rightfully so). I think [asp.net](https://asp.net) would be far more popular if people didn't know the history of microsoft.
Oh no I am aware of this. however, Core has seen many performance improvements and cross platform. This is the reason for why I say it's perhaps a bit of a hidden gem in plain sight. People are aware of [ASP.NET](https://ASP.NET) MVC, it's all been around a long time. It seems however that many are very unaware of all the improvements that seem to have been made with Core and the C# language in general.
Entity framework worked alright when you had a monolithic app. Set it up once (correctly), and you're in pretty good shape. If you're moving your data access to microservices, setting up entity framework time and again for each database for each microservice gets to be a pita. Easier to just go Docker in that case.
How would you say Entity Core is in comparison to Dapper with regards to ease of development and performance? I am guessing Entity Framework Core is a lot more performant than Entity Framework... Saw that a lot of people disliked Entity Framework and basically just say use Dapper or rip. :P
The move to cloud and the complete embracing of .NET core by Microsoft is going to catch alot of more senior devs and architects off guard I think. Until .NET core Microsoft wasn't huge on embracing cross platform development - and alot of legacy monolithic solutions are going to be stuck without a clear vision and path forward for transitioning to the cloud / .NET core.
Setting up EF...? You can run a single command to generate everything.
Use entity framework. Have proper logging/monitoring/performance profiling/etc in place. If it's holding you back, use dapper as needed. People make a big deal about it for no reason IMO.
The last time I remember working with it - it seemed like there was a lot more set up code/glue than a single line that auto initialized everything.
&gt;is going to catch alot of more senior devs and architects off guard I think It already has. We have a lot of senior devs and architects that aren't used to having to move as fast as Core is moving. That and they let their skills get stale by never learning anything outside of the .NET/IIS/Windows ecosystem and are going to pay the price for that now.
Only if you're doing code-first. I can't really see an upside to doing code first. Treat your database as a database and regenerate your models when it changes.
&gt; If someone knows enough to know that they want to use spoon, I'm assuming that they'd be looking at instructions on your site or wherever that would tell them they need to download both packages (spoon-web and spoon-pick-your-database-driver). &gt; It's not the most discoverable, but I don't think this kind of package was ever something that someone would find through browsing NuGet.org anyways. You aren't wrong, but I still would like to do this right, so when I do have some that people would discover on there own, I have it done correctly. Spoon is just an example. So if I had something like this in my CSProj file for SpoonCMS.SpoonPostgres, would the client adding SpoonCMS.SpoonPostgres automatically add SpoonCMS.SpoonCore, and then get updates if they initially installed 2.0, and I released 2.1? Or would it only pull the latest when they added it through nuget, and forget about it after? &lt;PackageReference Include="SpoonCMS.SpoonCore" Version="2.*" /&gt;
It's insane how much they have advanced. We have so many different versions of Dotnet Core at our current job because of this. We have had no problems upgrading and maintaining any of them so far.
Personally, I've always thought code first was for neophytes who think "real" database work is too scary. I'm sure there are exceptions, but that's been my impression so far. Regardless, I've never worked on a project where a code-first database in any language would have been acceptable. Data has always been sacrosanct in the organizations I've worked with, so it gets first class TLC by actual DBAs. 
From what I've seen, everyone already using .NET is excited and moving on .NET Core as humanly fast as they can; regardless of whether they'll deploy cross-platform. It will be very nice to finally break free of the traditional .NET Framework installation model where just upgrading the .NET installation on a server could partially break the supported applications. Add to that the general push towards FOSS, a general renaissance in development tools (Visual Studio, VS Code, and Rider have all contributed huge QOL improvements), devops, etc. and things are moving VERY fast in all the shops I've had contact with over the last few years. At this point, the only folks that will be surprised like this OP are people like you who are relatively new to this ecosystem. Anyway, jump right in! The community is generally very professional and friendly. You're sure to find a home in here if you want it. Just repeat after me here: "static typing is the way to go" and you'll get along with most others just fine. ;)
Agreed. Code first is fine for certain projects though. It is pretty handy for a personal project to do code first and have it create a simple sqlite database for you.
I would use puppeteer if I were you, the pdf rendering engine is great
When it comes to programming, you should always come to your own conclusions. This (and every other programming sub) sub is a massive example of why. Not trying to be offensive... But people parrot hard, or don't try to stay up to date, or don't know why something is or isn't good... Etc. Most developers stay at a very low level (I don't mean at a company) forever and whatever their current company uses is the best thing ever. End of rant, but hopefully you get the idea. Always try something, see if you like it. For whatever my opinion is worth, the performance difference between Dapper and EF is almost never going to be substantial enough to ever be note worthy (there are a few cases, such as bulk operations that should be looked at, however). However, the type safety, readability and model structures are great enough benefits to overlook most of it's shortcomings, and it's being constantly updated to add everything everyone wants. As for EF vs EFCore, I would say stick with EFCore. Is EF currently more feature rich? Sure. But you're intentionally putting yourself to be behind in the future by doing so, and there aren't major features missing anyway.
Lol, :) 
I've been with .net since the beginning and have experienced it's ups and downs both technically and in the community. I've never been more excited about what they are doing than now. I think it's great that Microsoft decided to "redo" dotnet with core. One thing I like is how normal it is to use asp.net core on Linux. It works well for me.
I work as a Rails dev now, and while I've come to like Rails, I still miss working with .Net on a daily basis and am rather bummed to be missing out on such an exciting time to be working with .Net on a professional level. That said, my current employer's interview process is language agnostic, so it was wonderful to write everything in .Net and all the devs who reviewed my code had to do was install the SDK on their Macs and they could run everything I wrote. I think one of the devs who handled my in-person, where they have you modify the code you submitted, hadn't seen my code beforehand. When I told him I'd be coding in .Net his immediate response was, "Oh, so you're using Mono?" Nope, actual .Net running on my Mac. "How's that possible?" You download the SDK from Microsoft and run the installer.
Two jobs ago the lead developer, now an architect for the company, didn't care about .Net Core because it, "Didn't support WCF," which is what the bulk of the code base he'd written was using. My general reaction has always been, "Yeeeeeeah, there's a reason it doesn't." :P Like he legit thought (maybe still thinks) .Net Core was absurd and had little to no future just because it lacked WCF support.
If you are learning ASP.NET Core, check out my project https://github.com/dodyg/practical-aspnetcore 
I pretty much agree with your whole post. I'm a Node dev professionally and I'm really impressed with .NET Core. It's like the modularity and open source aspects of Node world merged with great design patterns from legacy .NET Framework minus all the fluff we didn't need from it. I can't wait til more of the world starts to embrace it.
Not everybody is writing simple CRUD apps, you know? As soon as your queries become more complicated, the whole equation changes massively. Type safety is nice, but in this particular area I gladly take the risk of a runtime error due to a typo that will be caught rather quickly over the risk of accidentally introducing a n+1 time bomb. Set operations like ``INSERT INTO x (...) SELECT ... FROM y`` or ``UPDATE ... WHERE EXISTS(...) are very common, at least in my code, and they are very easy to fuck up in EF.
I mean, what is he supposed to do? Throw away all of the company's WCF code? Not porting WCF is a dick move that destroys those people, who, I might add, have been told to use WC, WCF and a lot of WCF on top of WCF. This might very well happen to you some day. Be very careful about embracing MS frameworks. 
Have a look at jsreport. You define your reports in HTML and CSS, and then pass your data in as json and it returns PDF/XLS/whatever. You can use jsreport for free for 2 or 3 reports. There is a jsreport.aspnetcore nuget package that provides middleware for rendering MVC views as reports as well, but in my project I just stored the HTML templates on the server and passed the data to it, and got a PDF file stream as a response.
I'm experienced with ASP.NET Core and still found your repo super useful. Cheers man
Wow, thanks a lot man! You were correct, [AllowAnonymous] attribute solved the issue. Instructor didn't have it tho, but the changes he did after publishing were for Identity, so he probably had the same issue, just didn't republish I guess. Regardless, thanks once again!
I work as a contractor and code-first is really nice when getting access to client’s database can often take much longer than it should. Then two simple commands and I have everything ready to go once I get that access
Now switch C# with F# and it'll blow your mind.
Glad I found your thread :) I'm new lol
haha I like your little anecdote. I think it's good to be language agnostic for sure. Strong fundamentals and adaptability is where it's at.
I’m not sure many more enterprises will be moving to Core. Now that it’s no longer possible to run ASP.NET Core on the full framework, migration becomes all-or-nothing. It will be a much bigger task going forward, which makes it less likely to happen.
My issue was more the fact that he had no desire to even entertain using .Net Core for anything simply because it lacked support for WCF, not even for newer stuff that didn't really need something like WCF. We're also talking about a developer who had extreme "Not Invented Here" syndrome. He wrote his own logging solution and JSON parser because, "We're writing enterprise software so we need to be in control of all our code." Oh, cool, so rather than use libraries for things that companies massively bigger than us use to take care of these things, which are well documented, we're going to write our own stuff which also needs to be maintained and updated in addition to all the other work we need to do, and all knowledge is tribal to the point of we're fucked if you leave or get hit by a bus. Cool story, bro.
I'm surprised core still doesn't have stored procedure mapping.
Give it a couple years. Ef core is still very very new, and and the article acknowledges what is needed for ef core to catch up.
Thanks for sharing this. Looks interesting! I'm not quite sure if it's what I'm looking for, but I'll download and play around with it.
Yea, I still don't understand that. Assuming only one result set, it's trivial to implement.
Umm. Should this be posted on a Linux Reddit instead ? We already know 
I saw someone asking for a guide a couple of days ago so I guess *somebody* needed to see it
MicroORM can call stored procedures and it is literally 160 lives of code. Granted it uses dynamic types, but still this isn't a hard problem and yet EF 3.0 won't support it. 
I didn't know it was possible. Last I heard we were stuck on. NET Micro. (Or was that arduino?)
.NET Micro ? Wow. Core ran on Linux when it was in Alpha.(it had a stupid code name I can’t remember now ) You Could host MVC6 sites with Kestrel and nginx. Clearly I was wrong about the fact that we already know. Welcome to 2019 🤩
If you're looking for a quick dataset binder on arbitrary queries (like dapper or your microOrm), entity framework is not the correct technology. Use dapper or your microOrm. If you're referring to binding models to CRUD stored procedures, please remember that Ef is designed to be a full orm, there's a lot to consider, design and develop to continue is lightweight approach while having full ornament functionality. Which are you referring to?
It's been possible to run .NET on Raspberry Pi since it first came out. There is even an IO library. You also might want to take a look at this. https://www.kickstarter.com/projects/meadow/meadow-full-stack-net-standard-iot-platform
Is there a way to directly interface with the Pi?
Dotnet on raspberry pi....dotnet subreddit. Hmmm, nope seems to fit here pretty well. 
EF doesn't just support SQL server. There are several other databases they would need to support before they could really roll this out. Plus there are probably a million little edge cases that we have no idea about from our level. 
Yea I ordered the Meadow Pack. You could run on Pi using Mono but the ARM version was way out of date and you had to compile the new one your self or trust some other random packages. Compiling took 8 hours and took up tons of space. Core SDK is like 170mb about and runs natively on all supported OS’s which also delivers the speed optimisations in NET that were mitigated by going the Mono route. Since Microsoft has been going open source especially last 3 years Mono has been aggressively fixed up (after they bought it out) to deliver better performance mainly fir Mobile platforms. But I think Core can be side loaded into Android and IOS now too )jailbroken( so until they find a way to run UI Native specific NET apps on mobile we still stuck with Mono on that side. 
I just left mono to compile overnight. Took a copy of the SD card afterwards.
Go CQRS. Use dapper for performant reads and EF core for writes. This way you have the best of both worlds.
 Why would you use dapper for "performant reads" instead of just using FromSql? I don't understand why you would want to mix technologies like that.
CQRS is great but recommending it without knowing what problem is being solved is bad advice. That’s a lot of unnecessary complexity for a lot of apps.
They just need to provide an interface and framework and let the individual implementations figure out how best to implement for their platform (or choose to implement a subset or not at all). It doesn't need to be all or nothing.
If you find CQRS is complex then check out MediatR.
Doh, yes I forgot about Rider. I've not tried it myself but know a lot of people really like it. I work primarily on full-stack applications for an agency. They've adopted various technologies over the years so it's not unusual to spend one day working on WebForms only to spend the next working on a full-stack ASP.NET Core and React application! Certainly keeps things interesting/challenging... I put up with the WebForms stuff but find ASP.NET Core and React much more enjoyable (which is probably why I spend more time [blogging about Core/React](https://jonhilton.net/) than anything else :-)). 
Not sure if I'm understanding your question entirely.. you can hook up a monitor/keyboard/mouse to your Pi or run it headless and SSH in 
Programmatic interface in .NET Core sorry
You're missing the point entirely. EF Core already can perform arbitrary data binding. The reason it doesn't have first class stored procedure support is... well that's the thing. No one knows why and their explanations don't make any sense. 
Stored procedures are already supported at the System.Data level. It's just a matter of passing in the procedure name, parameters, and a flag that says "I'm a proc call, not a SQL statement". Whatever reason they have for no stored procs, it's clearly not because of database differences. 
Does it support compilation of F# on Pi yet? 
In short, yes. Didn't read OP article or this one fully but I think you'll find what you're looking for here: https://msdn.microsoft.com/en-us/magazine/mt808503.aspx
[Looks like it's possible...](https://stackoverflow.com/q/48211291)
Can it be installed with apt?
It's always been possible. Before we had "query types" we'd create fake tables in the DBContext that would define the shape of the result set. And we can certainly perform the tedious work of defining the inline SQL for the stored procedure call and parameters. So again, why don't they define the pattern for first class stored procedures in an EF Context? Then again, why did it take until version 2.2 to get support for views? And why aren't views included in code generation until version 3? None of this makes sense to me.
Yas 
I think there are some libs that can be adapted for it. You can create them too by connecting to available .so libs. 
Last I looked it still didn’t have dB first so we won’t be using it anytime soon. 
DB First? As in generating classes from tables?
It does support an existing database. We are doing it now, but potentially we are going to let entity scaffold the models, but then use dapper to make the calls. Entity framework has so many idiocyncracies we can't figure them all out. I'm having problems with it, granted its mostly lack of knowledge, but I would just rather build the sql and use dapper.
Honestly stick with this if that's what works for you. Main reason for EF in this scenario would be to easily migrate database changes. SQL projects do exist, but they never really felt 100% supported
I think I read that .net core 3 is going to have new apis for accessing serial comms and the gpio. Nope, was wrong. GPIO is supported in 2.1. just serial ports in core 3. Info [here](https://blogs.msdn.microsoft.com/dotnet/2018/12/04/announcing-net-core-3-preview-1-and-open-sourcing-windows-desktop-frameworks/).
This
I’ve used CsvHelper a bunch. It’s good
Unfortunately, this is the only way to install on Rasbian. All the apt-get methods on the .NET Core download page work only for x86 platforms.
Why use identity server at all then, though?
We're using it to send 'realtime' updates to Users using web page from Azure Function that receives events(from service bus queue) from actual API. When someone adds something new in a Project you belong to, you'll get a red counter in the top, without annyoing permission request for notifications. Mobile apps can/are handled by regular notifications.