You haven't yet? WCF Data Services is more of an Entity Framework project than a proper part of WCF, so that wasn't really what I meant. But given that the WCF shop was opened on the promenade of the SOAP Titanic, I won't really contradict the suggestion.
Single it's a member of a singleton it shouldn't be garabage collected unless if you reassign that field to another value, which then the old value would be garbage collected. If you've implemented it properly that should also be shared across the application, which means you need to make sure it's thread safe.
From what I've read default is 60 seconds for output caching, default session length is about 20 minutes so that's another option. Probably can define cache length as a setting in web.config or machine.config. Google that might give you better results.
This, Singleton in Asp.Net environnement will be garbage collected when the Application Pool is recycled. Which is 20 minutes without any activity by default.
This + Resharper make VS the sexiest IDE on the planet.
This + Visual Assist X for me, but otherwise agreed.
If your database has a unique table name and the servers have browser turned on , you could write a connection isolation shim that effectively selects from sys.tables joined to sys.databases where tablename == unique name and write the connection string to the app.config at run time.... 
Thanks so much for your help. The ConcurrentDictionary is especially helpful. Part of the confusion regarding my original question is that we're using the dependency injection tool Ninject with an "addon" or "extension" for singleton scope, and frankly I'm not convinced it's doing what it's supposed to do. There is a method attached to this class that provides the ability to "reset" these dictionaries I've given a small example of to null. So, in order to "reset the cache", I can call this method via a controller, and it works for me. The strange thing is that it does not work for a different user. So, in other words, I can clear my cache for these variables, but it doesn't work for the guy sitting next to me requesting the page from the same IIS server. This leads me to believe that this class is in fact not being used as a singelton, because if it were, resetting those objects would not only work for me, it would work for everyone. Thanks again for your help. 
I'm more than a little confused regarding your post, but perhaps it's just a matter of arguing semantics. Let me see if I can figure out what you're trying to communicate. When you say "individual fields are never garbage collected, only entire objects", I'm confused on your definition of "entire objects". Naturally .NET classes inherit from system.object, so I'm going to assume you mean that instantiated classes are garbage collected, not the fields within those classes. Is this accurate? 
Thanks dude :)
I'm currently looking into Open Source projects for some insight on this subject, but I've yet to find a project using webforms that is current, which makes things difficult. Another element that would be helpful is a reliable messaging/event system. There's the built in event system used to communicate between user controls, but it leaves a lot to be desired. Has anyone had experience with [Sasa](http://higherlogics.blogspot.ca/2013/04/sasaevents-type-safe-null-safe-thread.html)? I've yet to dig deeply into ServiceStack, but I bet there are some gems in there as well.
That's correct. In .NET, "instantiated class" and "object" are more or less synonymous.
Have you used tuples with linq before? Linq only understands object types that it has references for in the library. So passing in a user object won't work and neither will extension methods. It could be simply that Linq does not support tuples. I ran into a similar problem when I extended the Enum class. I added a .ToDescriptor() method which returned a string version of the enum. But when I tried to do: var c = from m in TaxingEntity where m == enumType.ToDescriptor() Linq threw a similar error message. Essentially when linq gets your expression and tries to parse/execute, it comes across the Tuple.Create() method to which it has to reference(library) for. 
You cannot pass that statement along to your database engine. Try doing .ToList().Select(...). There might also be something in EntityFunctions that could help you accomplish what you are trying to do, though: http://msdn.microsoft.com/en-us/library/system.data.objects.entityfunctions(v=vs.110).aspx 
Curios, why using the CMS? As for logging: log4net I am some what of a purist when it come to JavaScript so jQuery only for me. What are you looking for In ways of exception handling?
Man I really wish I knew more about asp and web forms to give you a hand. I guess my personal preference is to avoid having to many techs with overlap. For instance I worked at a place that had an inhouse control library, windows controls and two 3rd party controls. I came into the work place about 4 years into the project and I found that no one control set was used everywhere. Instead we had a mix n match of all the controls. This caused HELL when we tried to change the theme of the application (ie colors, style ect). I can see this same thing happening if you bring in ServiceStack while using web API and someone else chose MVC. So I guess my only advice for chosing tech is to pick one, learn how to use it right, use it in its entirety, don't let anyone use anything else if it is supported in your tech. I know that does not exactly answer your question but I think it kind of dances around it.
Kentico was chosen by the powers that be as the ideal platform for our business. In other words it is cheap and .net. If you are a JavaScript purist there it's a new video on pluralsight that demonstrates how you can minimize or eliminate jquery all together. I found it interesting. Thanks for your help!
I think that's great advice. We do have the option to throw in some mvc with Kentico but have not done so yet. When I referred to service stack I meant only to imply is steal their architecture ideas not implement their technology
Your asax can handle errors for your site. Or Action filters too.
Just be sure to note that .ToList().Select() pulls the entire table into memory. So yeah, not always the best idea.
That depends on the expression before .ToList(). ToList() pulls the entire __expression__ into memory, which is a huge difference.
Your right, I should have been more explicit in my suggestion.
Op is doing two groupings then the unsupported select, so in this particular case it is pulling the entire table. 
Thanks for the advice -- all is well. Got to sit down with everyone who is making my life difficult and told them how things were going to be moving forward. No more changing names!
I use .NET and SSIS quite often. I was happy when SSIS 2008 allow C# as an option for script tasks. Most of the .NET I use is written in C#, so it was easier for me to not have to switch mental context. I'd personally say use whichever is used most often in your organization. Without knowing much about what you'd it's hard to give you any more than general advice. For learning .NET, I'd suggest investing in something like Plural Sight. Personally, I really enjoy using screencasts as a method of learning. If I recall correctly, there are a few caveats for using external .NET code with SSIS, so I'd also suggest becoming familiar with the GAC (Global Assembly Cache) and deploying DLLs there. I believe that's the sanest way to use external libraries with SSIS. Good luck!
Thank you for your advice. I'm basically just looking to learn how to write a script in SSIS to, say, create a new data output pipeline based on a condition. For example, I've been getting a lot of ragged right files lately and I'd like to be able to create a custom conditional split that can send data to different pipelines based on the number of delimiters in a record. That's just one example. But I'd like to be able to properly learn the terminology and how to properly write a script from the ground up. I know C#/VB are a big world. I'd like to focus my learning so that it is catered towards things I can do in SSIS...if that even makes sense? 
Others have addressed your issue, can I just say: Please, please, please stop doing database queries in your page templates. If you must do server-side HTML generation (client-side templating is usually best), put that crap in a controller and pass a model object with your data to the template. 
Yeah mate also saw this, but thanks anyways. I'm looking more for some good video tutorials, or maybe even a book.
From personal experience, I'd say most .NET jobs are web dev now. If you're with a dinosaur company, they'll have you working on Web Forms applications and if it's a younger company, you'll be doing MVC. To be on the safe side, sharpen your skills all across the stack. If you don't have DVCS experience yet, learn Git and SVN. As with any web dev position, make sure your Javascript skills are pretty decent.
You can use web forms, web API and mvc in the one web application. It's all part of the new one asp.net mentality. It works quite well. We have some projects that were webforms which I started to use webapi for the rest web services. Now I'm slowly moving pages to mvc. 
Thanks for the explanation!
There are third party components that extend SSIS to do a lot of work that you would normally have to write script task to do. Task factory from Pragmatic Works has 40 some components. And it is relatively cheap. You can download a trial from PragmaticWorks.com
I don't, I quite often however make a test page to prototype ideas before I shift them into my DAL.
It's a test view, I use it to test ideas before I put into the proper place, saves reformatting code in my real classes / controllers for stuf that may or may not work.
CodeRush for the win.
That's not Xamarin's doing, it's a limitation of the Visual Studio Express editions (they don't allow add-ins/extensions).
I have ultimate edition (provided by work) so it has nothing to do with my edition of VS.
Could you tell more about the benefits of your lib compared to plain old sockets? And include it on your github page? It looks like you are doing multiplexing and connection pooling etc but dont explain the benefits tot your potential new users :)
At the moment I'm using [ELMAH](http://code.google.com/p/elmah/) for logging unhandled exceptions, which is working well for me. I hear log4net is very good too but I haven't used it. For error handling, I often redirect the user to an error page which explains what has happened by using the custom errors section in the web.config. For user controls which are non-critical I might display an error message inside the control explaining that the component failed to load, however my method of handling the exception will depend on the content and context of the control. I'd be very interested in hearing about your previous approaches to the cross cutting concerns you mentioned.
I gotta ask: Why would you want this? It seems like a big appeal of C# was the removal of pointers and memory management. Now you want pointers back? In my mind, one of the best reasons for not using references is that it prevents programmers from making mistakes. The whole point of OOP is to ensure good encapsulation. Why would you want to break that by allowing methods to throw around references to objects? That is just asking for unknown side effects (aka bugs). &gt; First-class references enable you to decouple programs using simple get/set semantics from the underlying representation being modified. In other words, give the programmer the ability to modify the contents of objects regardless of whether or not its a good idea? Maybe I'm missing the point...
When have you ever needed to do this? It seems like such a bad idea to modify the passed in reference via a function side effect. 
I'm probably being dumb here, but if you wanted to expose an inner value on a class you own you'd just create a property with appropriate get (and set) methods. If you want to access something private to an object you don't own then given you're not supposed to do that anyway I don't see a need to make it very easy; a helper method or 2 using reflection would do. Perhaps a more realistic use case might be useful; I'm just not seeing the use case this makes easy.
I understand that, but enabling you to manipulate the underlying reference opens up a whole new class of problems. I don't doubt that maybe it could be useful, but i would like to be shown an example where I'd want to do this
&gt; I'm probably being dumb here, but if you wanted to expose an inner value on a class you own you'd just create a property with appropriate get (and set) methods. And how would you do this with an array slot? How would you do this when there are multiple fields or properties you wish to expose, because you can't implement the same interface more than once?
&gt; I understand that, but enabling you to manipulate the underlying reference opens up a whole new class of problems. Like what? The reference directly invokes the get/set methods of the underlying property, so there's no additional unsafety there. If you expose a direct field, then direct field access without mitigating sanity checking code in a property is then part of the object contract. Ditto for arrays. Where is the alleged unsafety?
Well it looks like stuff still doesn't work in 2012: https://connect.microsoft.com/VisualStudio/feedback/details/806348/razor-intellisense-does-not-work-in-visual-studio-2012-after-upgrading-to-mvc-5-razor-3 Thanks Microsoft. One year old product is getting the shaft. 
If I am not mistaken I think the upgrade purchase is only available with an MSDN subscription. Which implies that you must have your 2012 tied to your MSDN account and that is how it knows. A second thought would be that when you install the 2013 upgrade it looks for a valid 2012 CD Key in the registry. Both of these are just best guesses though.
I am hunting for a job right now with 5 years of c# experience. Here is what the tech field looks like as of now. 1) ASP .Net. (as apposed to ADO .Net.. learn the difference). 2) restFUL services in any of the following Web API, Service Stack, MCF 3) Some type of ORM (such as entity framework). 4) SQL - I find most places do NOT have a DBA and expect their devs to have at least some knowledge of views, functions, joins, stored procs. 5) Some type of source code repository such as TFS or GIT. 6) I have seen a lot of places using nuget for building their projects. This is more of a bonus then a need. At least now you know what techs to look into.
I forgot to mention that I only have problems with `Mode=TwoWay` **and** `UpdateSourceTrigger=PropertyChanged` (which is quite common when you want UI that *feels* responsive). Example: - I want to write `2.5` - I select the `0` and press `2` - I then press `.` - The UI makes the textbox content just be `2` because `2.` is equivalent to `2` - ??? - Go to a corner and cry. Try it yourself. My point is: why would this be default behavior? Did they not think through how their properties would interact? Every time I have to work around this issue I feel stupid. I'm like "It can't be that hard to do this; this is basic interaction. I must be doing something wrong." Maybe I am doing something wrong and there is a really simple solution to this. I am yet to find it. 
If you meant vs 2012 update 4: http://www.visualstudio.com/en-us/downloads/download-visual-studio-vs#DownloadFamilies_3
Yeah, no easy answer there...a workaround has to go somewhere, either in the WPF system libraries, or in the app; I'd probably put it in the ViewModel or write a converter - I know that's not the ideal answer, though. I'm probably gonna run into this on an upcoming project, so this will be relevant (to me soon).
Resharper makes VS the slowest IDE on the planet. Yuck.
It looks like other NuGet packages are [now unrestricted](http://blogs.msdn.com/b/dotnet/archive/2013/11/13/pcl-and-net-nuget-libraries-are-now-enabled-for-xamarin.aspx). This is a big step in the right direction for Microsoft.
I've been using it for years without updating it, it's worked fine. I'd say so long as it has the features you need then there's no reason to avoid it.
We used Screwturn but felt very limited by its built-in capabilities. Especially after Screwturn was declared dead, plugins was a drain to find, so we felt stuck. Today we're using Atlassian's Confluence which is only $10, if you can stick to under 10 uniquely authorized users on it. If you need more than that and you're looking for something OSS/Cheap, Confluence isn't what you're looking for.
Also using the 10 user license of confluence. I just paid for it with my own money
You'd be fine with that load.
Not that I know of. We're using it as a wiki today, running the latest version. We've got several "spaces", some public, some internal. Public sample: http://docs.ipaper.dk
What are you trying to accomplish with the wiki? There might be other products better suited.
To be honest you are ok without a formal education. Your desire to continue to learn and improve is far more important! Topics: * IoC (this will sort of force you in to greater familiarity with interfaces) * for deployment configuration, app configs, web.configs, config sections, settings files, msbuild/msdeploy * Services, building succinct encapsulated services that can stand alone and be shared between applications or make up a larger application. WCF (hate it personally but used a lot), Web API (REST) * If you intend to do web stuff and work on the front end, learn JavaScript, cobbling together some stackoverflow jquery answers doesn't count. I loved both JavaScript the good parts and Secrets of the JavaScript Ninja Resources: * dot net rocks podcast * hanselminutes podcast * clean code by Robert Martin (he also does a supposedly great C# book Agile Principles, Patterns, and Practices in C#, but I haven't read it yet) * JavaScript the good parts Douglas Crockford and Secrets of the JavaScript Ninja John Resig 
This is usually the kind of ignorant nonsense you get from non-CS degree holders. You don't really know what a recent CS grad knows until you've done it. But if parroting anecdotal evidence you've encountered about the merits (or lack thereof) of a CS degree makes you feel better about not choosing to actually go to college and find out yourself - more power to you I guess?
Then it's really going to make you mad when you learn that I'll, on average, make 23 - 30% more money in my lifetime than you. But to each their own.
If you can spare the cash (or can wrestle it out of your employer) definitely look into a Pluralsight license. Thousands of hours of mostly very-well-done videos about everything you could ever want to know, and they have (well, had, they are slowly broadening) a particular focus on .NET. I was in a similar boat as you just recently - bachelor's in sysadmin/networking hired as a sysadmin and slowly transitioning to a more dev-oriented position over the past 3 years. I could figure most things out well enough but I didn't have any of those CS classes to really know...well, looking back, much of anything. I talked my employer into getting me a Pluralsight license and it has made a huge difference. IoC, design patterns, RESTful API design, proper javascript, proper unit testing, proper use of ORMs. Basically just proper development. The amount I have learned over the past 12 months of working on my current project is crazy, but the amount I have learned in my last 4 months of having a Pluralsight license has just been absurd.
This man speaks the truth. Pluralsight is an insanely good resource!
There was an effort to continue the project: https://stw.codeplex.com There's a few recent post to the mailing list (not many). Can't see much evidence of ongoing development though could be wrong. Screwturn has served me well over 5 years and helped stave off Sharepoint :-) I hope it comes back to life.
Which PluralSight classes have you found the most beneficial? 
I have good news and bad news. The bad news: You are missing the important stuff. Object Oriented Programming (which you should know like the back of your hand), all of the C# language features (what they do, how they work, and their performance characteristics), and enough of the .net framework to be able to navigate documentation to find what you're looking for. Without those I would not hire you for any programming position. Without these basics, everything you write will be cargo cult. The good news: It is easier than ever to learn these. Documentation has come a long way since the release of .net. You really only need one book to get you properly started; C# 5 unleashed by Bart de Smet. It covers everything from insultingly beginner topics to the intimidatingly complex. It is meticulously well written and understandable. This is the only resource I recommend other than the MSDN library, and reflector+courage. Qualifications for this post: 12 years of experience with .net (I was part of the beta), former sr. {architect, developer, team lead} (now CEO), spent 3.5 years architecting/developing core features for one of the sites in Alexa's top 150 (for the US).
As far as I know the only fantastic programmer in that list is Bill Gates.
What is unit of work, anyhow? Keep seeing that around. If I were you, I'd look into continuous integration/automated build and deploy, as well as unit testing with Visual Studio's built-in tools.
Hijacking top comment here, I've got some "One week *unlimited* Pluralsight trial licences" here from a conference. First 5 to respond get a key. Plus one for OP if they want. *EDIT: And I'm out. Thanks for playing everyone!*
It all depends on what you're doing and where your knowledge gaps are. Personally, I got a lot out of the mocking with moq, unit testing, and inversion of control classes because designing for testability (and writing tests) were a huge blind spot for me. I do a lot of web app development with aspmvc, entity framework, and backbone at work and mono/servicestack, nhibernate, and backbone at home so the various related courses were pretty helpful. Except backbone fundamentals was surprisingly terrible - I went to FrontendMasters for most of my JavaScript learning. I also poke around the design patterns library class now and then when I'm doing some class architecting.
PM'd
Please! I will DEVOUR some Pluralsight .
Gates dropped out of *Harvard* and fell into the start of the most lucrative industry to ever exist and became the richest man in the world. If this guy really thinks he can relate himself to Bill Gates I have something to tell him.
This is the only line of your reasoning I can agree with. A degree != success. 
Sure thing, as soon as I get back to the office tomorrow, forgot them at my desk.
I keep hearing about these book things. From what I've been told, I can just buy and read them. Do you think any of them contain this magical knowledge you mentioned, or is it specifically reserved for those who give up industry head-starts to go to university? I'll give you that most self taught programmers in my experience don't bother to learn theory, but comments like this make you look ignorantly pompous to those of us who have. Op can learn everything you have at a fraction of the cost of your education if he chooses to, all while earning a salary and gaining experience during the day. You don't get a monopoly on understanding just because some old man graded your assignments.
Unfortunately the programmer earnings cap is the great equalizer in this regard.
Way more about what - everything? What do you suppose the curriculum is for computer science? Sitting around for four years playing video games?
If you still have them, I'd love one. Thanks! 
Hell yes I will take one. Upvotes for you! Thank you Mechakoopa.
For those wondering what was said: http://i.imgur.com/RYOMmJP.png
This is the main reason I asked this question. I want to fill those gaps. Luckily, I did stumble onto topics and examples of things like IoC and have even successfully used Unity for dependency injection (its why I even know what an interface is, and why I use IDbSet in EF instead of DbSet lol). I also want to get into those things around .NET development that aren't necessarily just code related, such as proper app deployment, security and authentication, IIS, app pools, load balancing... all those things other people go "Man... I don't wanna fuck with that." I figure that's where the good $$$ is. Plus, I enjoy it all. Win win! Edit: I r fix speling.
I can't agree with you more, but I'm lucky enough to be clever in a big company, so they pay me to develop and learn at the same time because the stuff I come up with tends to be pretty useful. Of course I still have to produce quality products that can be maintained by a support team after the development cycle is done, and its easy to see that the formal education is key, especially since there are so many college-trained developers. I do notice that I tend to be up on newer technologies than a lot of the more experienced developers. A lot of them know primarily webforms, but now that we're building apps that target mobile, tablet, and desktop, there's a huge push for MVC apps. I find so many of the "classically-trained" devs struggling with all the conventions used. Its hard for them to not write everything on their own and trust MVC to know what they meant. I always remind them they can override default behavior if they can find the right thing to override.
Sure you can certainly read all those books and teach yourself about compilers, assemblers, memory management. I have no doubt that lots of people have done just that. There are probably uncountable non paper holding individuals who are better programmers. Shit I used to work with this guy Robbie who I respect the hell out of because what he learned and how he programs is who I model myself after. But its a simple fact that for the *majority* of people who enter the field without a piece of paper they will never understand the wealth of knowledge a formal education in Computer *Science* bestows on you. I don't dis-regard programmers who didn't go the college route. That would be asinine. I have met tons of great self taught programmers some better then me some not. However, the largest group of terrible programmers I have come across (by a large margin) are those who never got that piece of paper. Can you be a killer outstanding awesome programmer without some white paper? Sure as shit you can. Can you be a terrible sucky ugly code writing hack *with* a piece of paper? Damn fucking straight. The point is in the margins. 
Well, here's a couple ofthe articles I bookmarked in my early learning: * [Generically Implementing the Unit of Work &amp; Repository Pattern with Entity Framework in MVC &amp; Simplifying Entity Graphs](http://blog.longle.net/2013/05/11/genericizing-the-unit-of-work-pattern-repository-pattern-with-entity-framework-in-mvc/) * [The Unit Of Work Pattern And Persistence Ignorance](http://msdn.microsoft.com/en-us/magazine/dd882510.aspx) Mostly I learned it because everything I read said it was common practice. After trying it out--and the headaches that came with learning it on my own--I really didn't come to appreciate it until I started working with Dependency Injection.
I can see where you're coming from, and I agree on your ratio. It's a touchy subject. Sorry for getting defensive.
Yes. The emulator is itself a Hyper-V VM, and you can't run nested Hyper-V instances.
If you still have a key left I'd LOVE to have one, I'm in a spot where I need to freshen up my CS skills to get some good arguments to why (and how) we must restructure our system at work. Today almost EVERYTHING is a Singleton.
He speaks the truth. I was developing a mobile app and researched this. There is documentation out there that clearly states this is not possible. Of course, this didn't stop our IT guy from wasting 3 days trying to prove me wrong.
Most of the stuff that one gets out of undergrad that you don't get in the real world is heavy theory and novel algorithmic problem solving that 99.9% of programmers have 0 reason to do in their day jobs. Like others said, you could follow along in a bunch of books, but you probably won't, and I've yet to see any coalesced repository of knowledge online that covers undergrad course work like discrete math, computational theory, compiler implementation, complexity analysis/algorithm design, data structures, and a host of other advanced concepts that only a fraction of devs actually need, but are useful to know because of the new ways one must think to reason about them.
Ok bad example...
 I purchased a Windows 8 home license to dabble in Windows Phone development but found out (after the purchase) that I have to professional. This and the Hyper-V turn me off writing any software for the platform and I shall stick with iOS. 
I once had this long argument about how many hours someone spends programming does not mean they are somehow any more dedicated to the craft of programmer then someone who doesn't spend 60 hour work weeks. It went on for days. But it was stupid because it was petty and the only thing that really matters is how dedicated *you* are to the craft. Not some anonymous pricks opinion of your dedication and skill set. So don't sweat it. I don't want to me that anonymous prick either.
I have never felt the need to get an actual cert. I mean, I can get all of the actual learning I need done without spending the time or effort to get the actual piece of paper. Who knows, if/when I leave my current job I may regret not having all of those nice letters to put on my resume, but it doesn't really seem worth it to me now.
I'm just confused why it wouldn't work like in the tutorial. I checked the used packages, use the same vs2012 and templates and it still threw the error.
Another reason for me to finally try nodejs ... Nice
I would say don't use any of the built in security, it's extremely restrictive.
The `ExtendedMembershipProvider` is a hold-over from WebMatrix framework. Then M$ decided to also use it for MVC. It is included in `Microsoft.AspNet.WebPages.WebDataData` package. (I'm nearly positive this should be included when you start an MVC 4 project, but if not, you can get it from Nuget). If you're starting a greenfield project, you can probably use the provider out of the box, but if you have existing userdata, you will need to extend and override methods from `ExtendedMembershipProvider` into a custom provider, and then load that class thru the web.config.
WCF already does this as part of the service if configured correctly. Consumers should be able to point a browser directly at a WCF endpoint and get automatically generated usage documentation with example messages. If for whatever reason you need to turn this off on the services box itself I think there is a way to extract that HTML, even if not officially supported you could always automate it as part of your build/test process to spin up a copy of the service, do an HTTP GET on the endpoint and save the HTML off to wherever you need it to be.
Thanks. I really appreciate your input. I'll be looking into it. 
http://blog.aumcode.com/2013/08/what-is-nfx-what-is-unistack.html That provides WCF replacement and 100 times easier
Looks interesting but. Um... yeah. Going to need a lot of traction otherwise it may be a waste of time. Are they all PCL?
Do you have anyone that actually uses this in the real world? Is there even a public release of it yet? I couldn't find any information on it.
Really cool stuff. I agree about bloat, and that simpler implementations could work just as well, especially if bundled together. 
The point I was making was the to develop for Windows Phone, you currently have to jump over a number of hurdles. Lets be frank, most native mobile development today is done on a Mac, it would be wise to make it as easy as possible for these developers to target Windows Phone. This is something I had wanted to do, hence why I purchased a copy of Windows 8 so I could dabble in Windows Phone development. I currently earn all my money from iOS and Android development but haven't earned a penny from Windows Phone yet. I've never had a client ask me for a Windows Phone App so from a purely business point of view, Windows Phone isn't worth much to me in terms of initial outlay. The license fee I paid for Windows 8 was purely to port some of my free iOS apps to Windows Phone as a fun weekend project. Having already spent £100 on this fun project, I was a little upset to find out that I needed to spend more money to get the Pro version. This limitation is crazy and I think MS should make Windows Phone development work on all versions of Windows. I looked into upgrading my copy to Pro but then found out that I could develop for Windows Phone using a VM. Thats now 2 rather important hurdles for me. More money spent on a hobby than I wanted and then having ti partition my MacBooks hdd which isn't huge to start with. Every time I wanted to write some Windows Phone code, I would need to restart my computer into an OS which doesn't have any of my files, my music, my bookmarks. It becomes a real core and not something that I would consider fun at this point. If I were to use a VM, my workflow wouldn't be interrupted so much. In any case, the reason I am here is I use Xamarin for my mobile development. That means I'm using C# and I think this sub is a good place to hang out when I want to interact with other .Net developers. 
If nothing else I could see it helping with RabbitMQ management.
Talking is cheap, show me the code...
We are opening this on GitHub within 1 month. This uses only around 15 classes from very BCL: List, Dictionary, Thread, Task, Process, Stopwatch, XMLDocument. It does not use: MS configuration, ASP.NET, MVC, EntLib, any JSON etc, DataSets/Tables, the code is used currently in production HFT applications 
Cylons, we will be adding more details to our blog http://blog.aumcode.com Right now working with lawyers to have a proper open source license that would allow everyone to use and contribute to NFX. As far as benchmarks, I can only provide my own at this point here, as the end-customer applications are proprietary. Here is a list of notable performance # that I can share at this point: -------------------------------------------------------------------- * **NFX.Serialization.Slim** - replaces MS BinaryFormatter and DataContractSerializer family - uses dynamically built expression trees for fast type-safe serialization. Yields about 5x better performance that BinaryFormatter, time-wise, and 10x space wise due to built-in TypeRegistry compression * **NFX.Templatization** - replaces Razor, ASPX et al. on average 20% faster than ASPX and 15-40% faster than Razor. Also supports all kinds of templates - image templates, text templates (not web-only concept) * **NFX.Log** - provides all kinds of logs with failover, SLAs, SMTP, CSV, debug etc. handles 100,000 msg/sec easily, sync/async options * **NFX.Erlang** - 100% managed Erlang support in CLR, types, OTP, sockets, mailboxes, pattern matches * **NFX.CodeAnalysis** - foundation for static syntax text parsing, JSON/Laconic serializers are based on it. Provides Lexer, Parser abstractions * **NFX.Serialization.JSON** - 4x faster than any MS technologies. "{a: 2, b: 'yes'}".ToDynamic().b=="yes".... ISODates options, ASCII/UTF text encoding options * **NFX.DataAccess.CRUD** - I am using it now for receiving 50,000 rows/sec from Erlang servers over 3-4 sockets on Core 2 QUAD with 4 GB ram, users filter/sort 300,000+ row client datasets in ram 
We are planning very tight Erlang&lt;-&gt;CLR support so we can build fancy UI for existing Erlang back-ends. One of the apps that we did in 2012 was just that - massive Erlang backend and windows/CLR rich client that worked directly over OTP. 
Because you specifically mention cross-cutting concerns it made me think of this https://github.com/guy-murphy/Conclave From the looks of it the framework isn't really finished enough, but there might be some ideas there for you.
Sounds like a bad case of Not Invented Here Syndrome. 
Waiting the release...
Anyone who's been around has heard this story before and knows how it usually ends...
Here is some. http://blog.aumcode.com/2013/11/working-with-json-data-in-nfx.html http://blog.aumcode.com/2013/11/aum-cluster-application-remote-terminals.html
How does it end? And actually I have been around for 20+ years and have not heard the story. Let me give an example around just 1 thing: .NET configuration. I have stopped using MSFT configuration back in 2005. I have never regretted this and used this approach in many applications since. Sorry but .NET config just sucks. There are 100+ classes that do not do basic things like: local variables, navigation, configuration in DB(not only file), mutable configuration, env variables.. it is just unusable per my standard. And there are many more things like that, i.e. JSON serialization is another one, logging - not there, dependency injection is not there. So I decided to open-source it. If you see no value in this - dont use it. 
Very well aware of the syndrom. It is not the case. The case is this: someone comes to you with $25K and asks to build an app for nursing home. It turns out that there are 250 data entry screens, some of which are complex (a la gov IRS forms). If you tell me how can I do this for this budget, then I will agree that I have wasted much time building NFX. With NFX 1 COMPETENT developer may build such system in 1 months and that would include security, scalability (i.e. sharded databases with billions of rows are supported out-of-the box), so your app will contain &lt; 5,000 LOC and will not have any 3rd party dlls 
&gt; it is just unusable per my standard. key words there...YOUR standard; everyone uses different pieces of these libraries in different ways (MVC considered bloated? that's a first...), and then others have to work around the holes created by the homegrown/new libraries, which when replacing core libraries, are often pretty big, which lead to even worse bloat than just using the first-party DLLs. I've seen this firsthand several times, so that's why I'm skeptical.
"everyone uses different pieces..in different ways" - that's exactly what the problem is. Those libraries are big, yet they do not solve basic things like security. MVC does not provide any means of usable security but some primitive role checks pass/fail. I currently support at least 5 large scale web applications that require 1000+ permissions each. Everyone that I know who uses MVC for business apps (not a Twitter feed reader), they all have tons of code just to make their screens secure - i.e how does administrator assign permissions etc.. This is just one of the aspects. Logging - is another problem. It is not uncommon to see 2-3 different logging frameworks used in a single corporate app. And lastly, have you ever maintained an ASP.NET application on 50 servers? How do you configure those servers? How do you maintain DEV vs PROD vs TEST etc. sorry, I worked in corporate environments for a long time to learn better
If the rainbow perentheses were an extension by itself, I'd use that. I find that when I install extensions that do too much stuff, they conflict with each other and cause me a lot of pain and suffering.
See my comment. Rainbow parentheses along would be fine. Perhaps the licence permits this and you can just take out the parts you don't need.
This, this this a thousand times.
Try the menu links at the top of the page.
Tried it out -- looks cool but seems to be creating some serious delay when I'm typing. (i.e. I hit a key and it takes about a second to process that key stroke).
I didn't know about this, thanks! Seems like it will be especially helpful for the "selected" or "checked" attributes.
Yes, this is a good tip. Mainly because if you search the web for stuff like this, you get the MVC3 syntax.
Holy crap, I had no idea about this. Nice find!
I dig the rainbow braces and I like being able to customize keywords like if/else/null individually. I think I will keep it.
When did *cool* jobs ever use .NET? .NET has always been for the enterprise, and the enterprise is anything but cool. &gt; I spent too much time with .NET I don't want to learn ruby! Languages are languages. If you aren't trying out at least one new language every year, you really should be. Ruby is fun, but not as much fun as Erlang. Why aren't more cool jobs using Erlang? It's awesome!
Having been around since before the beginning of MS, I think there might have been a brief period in there when MS was cool, but that may have been just that I was far far from the cutting edge (whereas now I'm simply one "far" away). Having said that, I don't think that you'll starve any time soon being a .NET dev. Things mostly work (on Windows) with .NET and MS puts lots of effort into making sure of that. In ten years, you'll still be [PC Guy](http://blogs.houstonpress.com/hairballs/apple-pc-mac-people.jpg), you'll just be PC Guy on Linux.
I saw someone say this online: "The code will tell you how. The comments should tell you why." 
Who knows why the majority are lame. But some good ones are out there. I work at a .net shop now where the environment is cool. Yoga Wednesdays, Gym down stairs, front entry way is a local art museum display. I ran tough mudder with a few of the owners and we smoked pot and drank whisky the night before. The problem? Its not a software company. Its an accounting payroll company who just happen to have an IT department which produces in house software. So... maybe that is a difference?
All those ruby weenies can build are shitty web apps that don't have half the features or usability of good native apps. That's not cool. With .NET you can build native apps for any platform. Pickup Xamarin or Unity and you can get a cool job using .NET where they actually build good software, not some hipster garbage.
Preach it! What do you want? A pieced together house which uses all the latest tools that change so fast that most the workers hardly understand the new power majigger and how it was designed to work with the hizzlehaut? This will cause a house to have all the latest features like a new 6 nozzle shower head and granite counters with an open layout. But because that one product is no longer support your whole bathroom can't be used and when the hurricane of 'next months new tech' hits, well, the house has to be rebuilt from the bones. Or do you want a house built with standard but optimized tools ontop of a firm foundation and steel bones. Then when the 'next months tech' hurricane comes through it leaves your house in *better* shape?
Hey Viasfora guy. How can I change the colors / font? I don't like the visibility modifier colors.
&gt;That's not cool. You know whats cool? A billion dollars.
Learn to make mobile apps then with Xamarin! Big community growing and growing...
A billion dollars *is* cool.
Sorry. It is completely unrelated to the topic, it is a line from Justin Timberlake in Social Network.
Alright. You're alright. You can fuck my sister.
All the cool kids might look down at people working at some enterpri$e using micro$oft tools. But chances are, if a company is willing to invest money into the licenses and tools necessary for a .NET/Windows application environment, they will also be willing to invest in their developers which means more $$$ in your pocket. Salary reports for my area consistently have higher compensation for .NET stack technologies, with free frameworks/languages like Ruby, PHP and Java at the bottom.
As also a .Net dev in Washington DC we have a few cool 'startup' like companies using .Net. They pay decent but not what the enterprise is currently dishing out. My suggestion, keep on with .net and start learning F# and JavaScript/TypeScript. 
I think there is a major cultural issue affecting the Microsoft ecosystem. I recently went to a Windows Azure training camp. I'm an independent dev, so I take free training when I can get it. The first thing that I noticed was that I'm in my late 30s, and I'm one of the youngest people in the audience. Later, the presenter starts talking about NoSQL, and there were a lot of blank stares. When the presenter explained what NoSQL was, there were a lot of laughs in the audience ("No relationships? No keys? HAHAHAHA. People actually think they can use this in production?"). Then it was explained that the largest sites in the world use this stuff, then there were more confused looks. Moral of the story, the Microsoft devs there cared nothing about the latest technology, they just wanted training on the products their managers and executives were expecting them to use. They don't think outside of the box because, in the Enterprise world, that isn't a safe thing to do for job security. While I believe the .NET stack can go up against any other stack used in startups -- the culture aspect isn't there to attract the interest from startups. Startup culture is focused on upsetting the status quo. Microsoft technologies, with their corporate culture, represent the status quo. Microsoft's technologies and tool ecosystems are incredibly good, but the cultural aspects of Microsoft in the startup community have a hugely negative connotation. 
You're looking for culture and that isn't defined by language. I've worked at many companies that were hip as fuck. My team at Microsoft fit that stereotype and my team at a major stock exchange is pretty close. I've never worn a tie to work. You should consider a city with more startups. Those are going to have the culture you're looking for and DC isn't exactly known for tech startups. That said, it's my advice to start learning more languages than C#. If If you want to stay within the Microsoft ecosystem, I'd get super familiar with C++ and JavaScript. Those will ensure you're hirable for, likely, the rest of your career. I'd consider NYC though. Still on the east coast and there are tons of tech startups, many of which use C#. Lastly, ruby is a waste of time imo. Popularity is declining and pay is generally lower for those positions.
&gt; Ruby is fun, but not as much fun as Erlang ????
I didn't know livingsocial was out of DC! That's neat
that made my 11:22pm.
NoVA technically. They're out in Fairfax. 
Or, you know, develop in the language that fits each of those platforms the best. When all you have is a hammer...
Very insightful post. I believe it's cultural as well. Individuals that I have known that use what are considered to be really "cool" technologies often are well versed in white papers that have been provided by companies such as Google and Amazon on approaches like NoSQL. Naturally those approaches are built on Linux based servers, so it also follows that using languages such as PHP or Ruby or for heavier lifting C/C++ is the way to go. Most of the systems seem "hackish" to me in some ways but that actually works for teams that focus on methodologies that enable rapid refactoring or new build outs. The .NET stack is also very large. So many moving parts that developers that have been working in it for over a decade have a hard time keeping up just due to atrophy from constantly re-learning approaches to adopt the best practices. That is in part because Microsoft has not really been a trend setter in many areas. Four years ago I was using ASP.NET AJAX and SOAP based web services and felt like I was doing something good at the time because those were things that MS had a hand in. Then technologies like JSON and JQuery really re-wrote the approach and Microsoft just got around to building nice tooling support with VS 2013. 
I'm not sure how mono vm compares to .net or java vm in terms of speed and reliability.
&gt; I'm not sure how mono vm compares to .net or java vm in terms of speed and reliability. Its awesome now for a lot of backend stuff its actually faster than traditional .net The xamarin &lt;S partnerships have brought a bunch of items in the last few months like PCL that make it feel closer to the normal experience. With all that said selenium driver for firefox in mono freaking sucks and I have been fixing it the whole day. 
There are plenty of passionate .NET folks out there - and we have a very large body of production quality open-source projects on github, Codeplex and bitbucket etc. to prove it, IMHO. Not that I'm saying your experience is wrong - I have the same experience and I agree that it's likely that fewer 'cool' projects are started in .NET and many MS devs are not likely to be starting those sorts of projects. And like you, I've seen the meetups tend to self-select towards being passionate Linux folks. But I think people like us can be the exception - we can be the passionate .NET people among the Ruby, Go, Clojure, Erlang etc. people. And we certainly don't have to limit ourselves, as others have pointed out. I've never known my knowledge of another language to make my C# coding worse (except for really long periods of getting out of practice, perhaps!). I'm trying (very slowly) to re-learn C and learn Clojure now. Anyway, there are lots of us here who love what we do in .NET - hey why not post about something you find cool, or start an open-source project and post it? :)
.NET was introduces as a "Bussiness applications" framework, with a lot of dependency on Visual studio. Bussiness developers are the least innovative in my opinion, creating boring software for boring businesses. Also, many .NET developers are directed by Microsoft's vision and marketing. .NET can be used in many "cool" applications, but this needs a paradigm shift from the developer towards things Microsoft doesn't talk about. I have built a news aggregator that binds same story's coverage from different news sources, sorts news based on priority, display customized news aggreagation for every user based on his preferred news sources and categories. In other words,it is google news with better interface,better mobile support and better support for Arabic (google news' Arabic version is awful with many bugs in news aggregation). I built it using .NET for the crawler (desktop application) and the web application (MVC4), but I used Cassandra and ElasticSearch as back ends. If I worked using the Microsoft directed mentality, I would have stuck with Sql server or their cloud based solutions which has nothing to provide the functionality provided by ElasticSearch. Sql server is the most respectful relational database in my opinion (in terms of performance) but the relational model itself is limited for such application's requirements. That said, I don't know a single developer in Egypt (where I live) who used a non relational database once, and I don't know 5 developers who understand the concept, since the corporate software is the only type of software built in Egypt, either using Java or .NET or php. It is not a problem of the platform, it is a problem of the mentality using this platform.
We're cool, our little software house has always used .Net, and we make cool stuff like www.staffsquared.com - which, in my opinion, it's pretty freakin' cool :)
Honestly, I've never had a problem with build orders. But that might be because I'm usually using some form of IOC/DI to avoid truckloads of references.
It's cause you are in the DC area. I too am a dev in dc working in .net and just went through looking for a new job. I had the same experience, dc is just a shitty tech town, with not a lot of interesting jobs. However, look at NYC or San Fran or Seattle and there are a ton of cool .net jobs. I interviewed at some neat places like fog creek, olo, practice fusion, zocdoc, who are all doing really cool stuff in .net. In the end I chose to get familiar with another stack just to be well balanced so I took a job with amazon in Seattle, but the point is that dc blows. That said, I'm hosting an f# meetup this Thursday so feel free to come! It's at my current job spot in dupont, a small cool company that used .net (who is going to be looking for a new dev now that I'm leaving soon :)) http://www.meetup.com/DC-fsharp
The counter argument is that businesses the use free tools have more money available to pay their devs.
&gt; Why? First, because each language teaches you to think about programming in a different way- it brings a new perspective to how you solve problems. Even if you don't use the language in practice, it will make you a better programmer. Second, programming languages are just tools. They're like hammers and screwdrivers. Just because you *can* hammer a screw in doesn't mean you shouldn't be using a screwdriver. Third, the language you favor today will one day be obsolete (or change dramatically). Unless you intend to go obsolete with it, or have the deep business knowledge to remain on the fringes where it's still used, you're going to have to learn a new one. Learning is a skill itself, and being able to quickly learn new things takes *practice*. Fourth, technical mastery is the least important skill a programmer can develop. Far more important is deep knowledge about the business processes you're solving. If you have decent programming skills and a deep knowledge of, say, accounting, then you can program accounting software in *any* language. If you only know .NET, and not accounting, then we're going to have to spend a lot more time explaining the requirements to you. You've become a liability.
No extra container frameworks, no build order problems for me either. I don't want to use absolutes and say "you should never need this", but if you can't reason about your solution and definitively say that project C relies on project B which in turn relies on project A, you have bigger problems waiting for you.
I think languages like C# are far away from becoming obsolete. It's going to be around for more that one year so I don't see myself having to learn "a new language every year" for *that* reason. Again with C#, it's more of a multi-tool than a screwdriver. It can do desktop apps, web apps, services, all sorts. You can even program embedded applications with it. And on your fourth point, wouldn't that mean that spending time gaining business knowledge would be a better investment than learning a new language?
Yes, but time spent isn't a zero-sum. I can do both by using a new language to solve a business problem that I don't understand as well as I should. And the point of playing with a new language every year isn't because you're going to ever *need* to learn a new language every year- it's because you *need* to learn new things all the time- and keeping your "learning skills" sharp means that you'll have an easy time learning new things. I think people should *constantly* be learning new things- a breadth of knowledge supports a depth of knowledge- your experience should be like a pyramid, with a wide base of experiences supporting more specialized knowledge. If you're not learning something entirely new, you're just getting old.
I agree learning new things is great, I just don't think it needs to be a new language - at least not one per year.
Thing is, if you're in the habit, it's a few tens of hours out of the entire year. Note that I've been very careful with my wording: "trying out", "playing with". I'm not talking about learning a language so deeply that you're going to throw it on your resume and claim to have experience. Now, I will admit- part of my background on this comes from working in the training industry early in my career. I had *no* experience, and they threw classes at me- "Hey, do you know Java?" "Nope." "Great, you're teaching it next week." "Okay. Do I get a book?" "Nope. Go on Amazon and pick something." I got *really* good at cramming.
In their defense, they simply have no reason to know that for their jobs. I am not a lazy developer like that though, I like to stay on top of the tech scene and on top of my career. I may not use those things, but I certainly try to be aware of what they are and what's trending.
There is so much to learn in the .NET world I can't spread myself any thinner learning whole new stacks... I am not a code ninja so I need to set priorities on what my brain can learn.. I feel like I would be almost starting over if I applied and got some ruby job.. all that command line stuff :(
its more than a language..its the api. .the enviorment... Id love to learn every language but I just don't have the brain power or time.. I am hanging on to my hope that there is some .NET job in DC with some cool people... All I want is to work with people who find coding interesting and don't wanna just maintain something and get their paycheck.. but most .net work seems like this.. especially the H1b visa types in my exp of about 12 different jobs
Does it really cost much ? They are almost giving away .net dev tools at this point
I do some of that CLI stuff in .NET. So much of Visual Studio sucks so hard…
I think you're selling yourself short. It's really not hard, and one thing you quickly learn is that it's pretty much all the same. Oh, the syntax changes, the document structures vary, but it's all based on the same principles.
Totally agree. 
Very true. Coming from a .net developer in the "corporate" world myself...
well if you know of any let me know. punkouter AT outlook DOT com
Im stuck in Dc.. I did live in santa monica for a year.. they are way more fun over there :(
there was a unity meetup in DC 2 weeks ago I went to. I am using that now to mess around.. it is c# atleast
"WPF Controls Unleashed" gives you a good sense of what's going on under the covers, and a good preview of writing custom controls. I'd recommend having it for when things get outside of common practice (a scenario that WPF excels in). For a general overview, I'm going to suggest an old one. "Applications = Code + Markup" by Charles Petzold. It doesn't cover the new features (it is circa .net framework 3), but it is the best WPF book I've read. It will give you a broad understanding, then you'll have to find something to cover the new stuff (you can find most of that on the internet). There is a lot of new stuff. For ASP.Net that's going to have to be someone else's recommendation. They are really good about documenting new features and technologies online, so I never feel I need to pick up a book to gain incremental understanding. MSDN is fantastic; but there isn't a lot of "why", or "also see". I'd suggest you spend a lot of time there (f1 in visual studio will bring up the MSDN documentation on whatever class or method your cursor is on). Books are great for helping you navigate the huge .net framework to figure out what parts you need to learn now, and what parts you can put off. MSDN should be your go-to when you need depth and specifics.
&gt; For ASP.Net that's going to have to be someone else's recommendation. I guess you meant *documentation*?
Cool! I remember you. This weeks meetup should be fun, and b-line is a cool place to work if you are looking for a new spot 
&gt; enterprise scale &gt; enterprise scale &gt; enterprise scale &gt; enterprise scale 
dito
... What year is this? Multi Solution -&gt; continuous integration server -&gt; NuGet for dep management Looks like this was released by a guy on the HP Quality Software team - horrible, horrible software.
"Enterprise" and "shitty" are largely interchangeable terms, but they don't have to be.
Someone else is going to need to recommend a good ASP.net book to you. I read an asp.net book in the early 2000s, and have been keeping current using online resources since. I'm no use to you when it comes to ASP.net book recommendations, as I haven't read an ASP.net book in a decade.
Unfortunately I have seen much 3rd party .NET code used in 100+ enterprise grade systems in the past 10 years. Telerik, Infragistics, NSpring, Obout, MSEntLib, and tens of home grown solutions. Those systems are not rocket science (insurance, medical, banking, HFT) but are extremely complex in terms of change management/depl. There good 90% of complexity is not from business logic, but from the fact that there is no unified architecture. I have seen Telerik used with Infragistics on the same ASPX page, where backend is built with Unity that also uses ninject in some cases:) Also, majority of web applications I come across these days use JSON.Net + MS JavaScript serializer, + DataContacts for SOME cases. The code becomes impossible to manage. The way NFX was born - I built it for my personal consulting needs.
I &lt;3 Rx
Now serving as a constant reminder of why I'm still on windows 7.
For those of you who have made the switch to 8/8.1... Does it ever get better? The one time I tried it, I couldn't even find the run button. The run button is the second most used button, right after the Start button.
Yeah, it's just fine. You can get to the run dialog by right clicking in the list left corner and choosing run or by pressing Windows+R. Most of the time you can just hit the Windows key and start typing the program you want and Windows will figure it out.
Right up until the search catalogue for the Start screen stops working, then you just get "This application can't be searched" no matter what you type. Joyful.
Going to try to avoid the "LEL windows 8 LE OMG" train and say that regardless of it's purposeful similarities to the metro (which I do like), I think the theme is well done. 
Reflection Emit allows you to, through C# (or any other .net language; but who are we kidding), emit and compile your own IL code **AT RUN TIME** to be executed by your existing code. There is a high learning curve to emitting IL (what your C# code compiles down to), but if you learn to use reflection emit properly you'll be able to pull off optimization magic that will confuse and anger your architect. You probably shouldn't use it in projects you are being paid for unless you **REALLY** know what you're doing with it. You will also learn valuable lessons about what your C# code compiles down to that may affect the way you write C# (and make you a better programmer).
Apparently the Stackoverflow guys use this in certain hot paths.
I used Emit to create an OpenGL wrapper using interfaces. It allowed me to add throw on OpenGL errors which could be disabled in run-time without any penalty. Sine the result was an interface, the code could be tested as well. No static functions.
Yes, I've come to prefer it over the old way. You need to spend a little time organizing the start screen though. Personally, I remove all the metro apps from there and just put the programs I use in an order I can quickly navigate. But mostly I just type something and use the search.
Can you add /r/windowsazure to the "other subreddits you may like" list? You linked to the dead one...
Very nice; best subreddit theme on the site, IMO.
Stardock's start8 &amp; modernmix make windows 8 feel really great. I shake my head that Microsoft didn't take this as at least an interim approach. 
Ya, I'm not to impressed. Its the exact look and boxy feel that I try to avoid with windows and windows apps. 
Razor templates are converted to c# and compiled at runtime as well, you can use reflection to use them outside of web projects so I wrote a system to use it for a simple templating service for automated emails so marketing and legal can change wording and layout of emails sent by the system without having to bug us to update and redeploy. The templates are stored as blobs in a database, and my lovely front end wizards slapped on an editor with preview functionality.
You should also check out [Expression trees](http://msdn.microsoft.com/en-us/library/bb397951.aspx), they are a friendlier wrapper over Reflection.Emit.
Is there a way to generate anything but static functions with them? My understanding is no, but I would be very happy if there is. I agree, they are MUCH friendlier.
such awesome. much wow
Partial classes are one of those "completely changed my life" things.
Neat! I use Linqpad for stuff like this but the browser approach is cool. 
I agree. I think it's a nice "browser-based" supplement to a tool like Linqpad and while it may not be at that level yet, it's certainly an approach in the right direction (*especially with regards to the collaboration aspects*).
Yup, actually before I left the .net job I got some F# into production code (though when I left I heard the team rewrote it in c#). There's a lot to love about f#, but it seems to me that the community isn't as rich, the tooling and adoption aren't quite there, and if I had to choose to start a project between scala and f# I'd go with scala. I do miss Hindley-Milner type inference, though.
In my experience, there can be a lot of Partial Actions associated with a single public controller action. I do this a lot to refactor from complex Views or support a dynamic ajax experience. So if you have 5 or 6 public "ViewResult" controller actions, if you have use Partial Actions (either via client ajax or Razor Html.Actions) then you have might have 20 or 30 methods. 
Concept looks great, it just doesn't seem to be working at the moment, I'm getting execution timeouts just running the Console WriteLine example: http://i.imgur.com/r0rJrZo.png
Are you still encountering issues? I received a few e-mails of people that had read the article that were experiencing the same issue. I just [checked it locally](http://i.imgur.com/fFoUBZk.png) and it appeared to be fine. I know that the developers were working on it earlier today and applied some updates. It's possible that it could be a maintenance issue.
It appears to be working now, first load it timed out again but a refresh seems to have fixed it. Cheers!
True, but many confuse .NET/Ms software bloat and CLR/VM + C#. CLR and C# are fantastic tools, but the libraries around them are very "fat" and mostly target FORTUNE-500 companies. The reason why many startups choose Ruby or NodeJS or even PHP because Microsoft software stack became HUGE, yet lacking basic things like transparent performance counters (that are stored in db), centralized configuration , logging (not built-in cluster-centric logging with SLAs, cascading) etc. and there is nothing for scalability and cluster-programming, i.e. there is no "Cluster" object in your app, there is no concept of tiered cyber centers, caching, VM spawning etc. So there is a reason why people use Hadoop , Chef, Puppet. Mesos and OpenStack - those functionalities do not exist in .NET paradigm. And this is a HUGE miscalculation on the Microsoft side as their "enterprise" stack (COM etc..) is 20 years old. Lastly, I think it is time to change that. Microsoft is trying - they are trying to port Hadoop and have NodeJS on Azure, but this is not enough. In spite of all of this, we have selected CLR/C# as a primary platform for our startup, but that is because we have completely abdicated from all .NET ecosystem 8 years ago. We use CLR/BCL(very base classes), and C# on top. We can also migrate to Java easily. Here is my blog post on the subject: http://blog.aumcode.com/2013/02/nfx-aum-cluster-philosophy.html 
Good point, speaking of "No keys HAHAHA" - that's exactly what I keep hearing daily from corporate developers, yet Google is built on top of BigTable that has no classical RDBMS "sql server-like" model. It is all about balance. I would never store financial data in MongoDB, maybe a de-normalized view-only snapshot for lookup speed. On the other side, why would anyone pay for SqlServer/Oracle(or the like) to store 10 billion comments? As far as "MS's negative connotation in startup commune" - there is a reason. A friend of mine is an independent consultant doing a web sites in NodeJS which take him 2-3 days of work to set up not using any Droopal-like CMss. Those sites are with custom business logic. He used ASp.NET in 2003-2008, then tried MVC but never regretted once his conversion to NodeJS. Unfortunately MS tool stack became a monster, ASP.NET is HUGE yet extremely painfull to work with. NodeJS is primitive compared to .NET, but in practice that is why many devs can quickly create awesome dynamic sites in a week. Lastly, I recommend all .NET developers to look in Ruby, NodeJS, Erlang, Python paradigms. It will really infuse some fresh blood and raise awareness.
Exactly right! But now, there are not only more languages on top of .NET framework, but more than one framework on top of .NET :) We are open sourcing the result of our 8 year internal project. "Use CLR+C#+(very)BCL", everything else is written from scratch in C#: http://blog.aumcode.com/2013/02/nfx-aum-cluster-philosophy.html 
Really? I still think there an ugly hack for code generation, and haven't actually seen one since MVC/WPF came out (maybe a little in WPF).
Oh dear god no. Partial classes are a maintenance nightmare. "Which file is that method in again?" They are a crappy way of dealing with code generation. If you can avoid them, do so. It is a feature generally meant for a compiler, not a user. I am speaking from painful experience here. I am maintaining a legacy project that is a single class split across 15 6k lines of code files. It is a nightmare to make any change. Controllers should be super clean. They should handle I/O only. Just in that example, I saw two methods that should be in the View and one in the model. If your controller is complex, your organization is bad. Your view and model should never be in your controller. Your ViewModel should really never be in your controller. Seriously. Read up on MVC. No, don't separate based on HTTP type. Separate logically. If I want to edit something having to do with editing users, I want to open the user controller. I don't want to have to open four controllers and then find the methods pertaining to the user. Give me all the user controllers at once. What is a DTO? Searching showed a possibility of it being "Data Transfer Object"? If so, again separate. They should be in your external interfaces. Heck most places make them a separate DLL. Interfaces are be definition decoupled from their implementation. So, no, don't place them in the same file. &gt; By reducing the size of files you are working on, and keeping them organized by subject, it is less likely that you will have conflicts with other developers. Or, you may only have them in the .csproj file, where it is fairly trivial and safe to resolve. What the hell? What does the author mean only have them in the project file? Organize your code for the ability for a coder to grok it. If you are organizing to reduce merge conflicts you are coding wrong, organizing wrong, and managing the project wrong. Ditto with organizing for CR. Organize via logical differentiations. If you put your models and views with the controller you are violating the whole purpose of MVC and the separation of the Model, View, and Controller.
&gt; He means the project files that visual studio uses, which are a pretty common source of merge conflicts. Any sane build system would include wildcard paths. But MSbuild/VS have to have a list of every file in the project. You can. VS just defaults it to explicitly listing every file. And it is pretty easy to manually merge those. Include="&lt;rootdir&gt;\**\*.cs" http://msdn.microsoft.com/en-us/library/vstudio/ms171454.aspx
This is a fantastic idea.
I did that once and it took VS several minutes to load thr project. That was a database project though. Not sure if the same would occure in a normal one.
Sorry for the delay but its' added now. You should think about merging /r/windowsazure and /r/azure.
I get that it can increase throughput, although no profiling information is provided so who knows, but: &gt;The Taskmatics Scheduler makes judicious use of the new async and await keywords in the administration website. In order to ensure that the UI is responsive when retrieving a lot of data I don't see how. If it's a normal html request then it will still take just as long but it will free up the thread while it's waiting. If it's an ajax request then it won't be affecting UI responsiveness anyway. &gt;we make data retrieval calls asynchronously so that we can retrieve more data concurrently and therefore display the screen to the users as fast as possible. I would love to see some performance benchmarks on multiple asynchronous database calls and batching database calls. My monies on the batching. Publishing to a service bus/message queue would get you most of the responsiveness in addition to a lot of fault tolerance. Look at the following: &gt;//collect the payment information orderData.PaymentDetails.AuthorizationCode = await _paymentProcessor.ProcessPaymentAsync(orderData.PaymentDetails); //save the order to the database orderData.OrderId = await _orderRepository.SaveAsync(orderData); //generate and send a confirmation email. await _emailGenerator.SendConfirmationEmailAsync(orderData); If the payment processor is down (black friday) the order doesn't get saved. If the DB times out etc (black friday remember) the payment is processed but the order isn't saved. If the mail server is down (still black friday) the customer doesn't get a confirmation email.
Hi /u/flukus, thanks for your feedback. I appreciate that you took the time to read and critique it. &gt; Unless you want the services to return the models. I tend to follow [this philosophy](http://lostechies.com/jimmybogard/2009/06/30/how-we-do-mvc-view-models/) for strongly typed view models, and keep them distinct from DTOs. &gt; Terrible idea. Concrete classes with virtual methods are just as testable as interfaces. Sounds like you are saying that you think using interfaces is a terrible idea for services? Can you clarify what you mean? &gt; Haven't really explained the need for DTO's. Yeah, that's a whole other article. This article is aimed at people who already use DTOs or don't philosophically object to them. &gt; Your using IOC wrong. Was I using Kernel.Get? If so, that's a mistake in my code. Not sure where that is.
Hi /u/cadoc7, Thank you for taking the time to critique my blog entry. A few comments inline... &gt; I am maintaining a legacy project that is a single class split across 15 6k lines of code files. It is a nightmare to make any change. Curious to know what happened there. Sounds like a class that should be refactored. Is this a problem because of partials, or because the class is too big? &gt; If I want to edit something having to do with editing users, I want to open the user controller. My issue with this philosophy is that what if you are editing a "UserNote". Should it be in the User Controller or Note controller? I tend to organize my controllers by routes, and I don't mind if they get larger. Otherwise, developers start to make sort of random decisions about which controller to place new functionality in. &gt; What does the author mean only have them in the project file? I mean that the merge conflict will be in the .csproj file itself, as opposed to in the .cs files with actual code. &gt; If you put your models and views with the controller you are violating the whole purpose of MVC and the separation of the Model, View, and Controller. I always thought of these as logical separations. I use strongly typed view models though (coupled to a particular view), which I find makes more testable controller code.
&gt; Curious to know what happened there. Sounds like a class that should be refactored. Is this a problem because of partials, or because the class is too big? I would love to know too. That is the thing though. It is a *single* class. That is what partials do. They split the code for a single class across files and the compiler just slaps them all together. I would love to slap the genius who thought it was a good architecture. As far as I can tell, it was meant to be a stop-gap measure that turned into a major feature. The pain comes from how everything turns into spaghetti. You are essentially just using GOTO everywhere. Finding a specific method is a major pain. The method drop down in Visual Studio has hundreds if not thousands of methods on the class. Good luck finding out which you need. It is equivalent to maintaining a single 90k line file except you have no idea which file the method is in. Its essentially random. Hell we have constructors in different files. The partial classes in the project I'm maintaining, as far as I can tell, started the split to reduce merge conflicts. It was not a smart decision. We are in the middle of rebuilding the feature from the ground up so we can junk the old stuff (and move it to Azure). A partial really only should ever be used for generating stubs, and even then a better approach is an abstract base class. &gt; My issue with this philosophy is that what if you are editing a "UserNote". Should it be in the User Controller or Note controller? I tend to organize my controllers by routes, and I don't mind if they get larger. Otherwise, developers start to make sort of random decisions about which controller to place new functionality in. What is a UserNote? Is it a note? Is it a user? Is it a note tied to a user? Is it a property on the user? It sounds like the the note is the primary entity. So my routes would start like: /api/user/&lt;user_id&gt; /api/note/&lt;note_id&gt; And maintain any user/note mapping in my back-end. So anything having to go with a Note goes in the NoteController and anything with the User has to go with the UserController. If you don't have this clear separation, you should probably refactor. You should never need much more than: /api/&lt;controller&gt;/&lt;id&gt;/&lt;action&gt;/&lt;id&gt; For instance, to link two issues in an issue tracker: POST /api/issue/1234/link/4321 If you need more, really think what you are doing through. It is not passing the sniff test. &gt; I mean that the merge conflict will be in the .csproj file itself, as opposed to in the .cs files with actual code. My point about how you shouldn't organize to reduce merge conflicts stands. It will become a maintenance nightmare. Especially when new people need to read the code. &gt; I always thought of these as logical separations. I use strongly typed view models though (coupled to a particular view), which I find makes more testable controller code. They are logical separations, but they make no sense to be in the same file. Good C# code will never have more than a single class in the same file with minimal exceptions for strongly-coupled components (mementos and their model are the only ones that come to mind right now). In a perfect world, the Models, Views, and Controllers should be able to separated into different DLLs. You should be able to re-use the same View in multiple projects. And the same Models. For instance, the new version I am working on uses the same Views for the help and examples page. The content is different, but we use the same exact View code. And we re-use the Models in a related Powershell environment without any changes. We just link the DLL. While your M, V, and C may be separate classes, having them in the same file makes as much sense as putting your GUI, business logic, and data access layer together in a desktop application.
Unfortunate, but I think, very true. Despite really cool things happening on the .NET stack, it (along with Java) has the patina of "Enterprise" all over it. Also, of course, is the cost of entry for the pro-grade tooling. Say what you want about how capable the Express versions of VS are, to really get into .NET one has to make a significant $$ investment. Lastly, there is the whole thing where MS pulls the rug out from under devs after they invest time and effort learning the latest MS (Silverlight, anyone?). I love me some C#, and VS 2013 is an awesome toolset. But I have also been branching out into other ecosystems. 
&gt; Lastly, there is the whole thing where MS pulls the rug out from under devs after they invest time and effort learning the latest MS (Silverlight, anyone?). That exists on both ends. Open Source has the "flavor of the month", and platforms and toolkits that never quite made it prime time. .NET developers have the reassurance that the core of their skills carry on to the next API Microsoft releases. C# features change, but Microsoft is never going to say "Hey guys, drop your C#, because it's PYTHON this month!" The problem with Open Source is that the language AND the APIs are subject to community whim. Microsoft throws new APIs at us with regularity. Fortunately / unfortunately, some of them compete with themselves -- for example, Microsoft's data strategy, you have ADO.NET (legacy), LINQ and Entity Framework to deal with. Are you going to invest your time in all 3 technologies, or just focus on one and move on? For me, WPF is essential to the work that I do. It is hardware accelerated, XAML-based, and very powerful. I chose that instead of Silverlight, even though they do 90% of the same thing (but slightly differently!) I thought I had made a wise choice, but then both WPF and Silverlight are set out to pasture, and replaced with something new that's somewhat different than both of them for Windows 8. &gt; I love me some C#, and VS 2013 is an awesome toolset. But I have also been branching out into other ecosystems. I'm 100% VB.net these days because my client codes part time, he doesn't see the need to use C#, doesn't want to have to learn C#, and he needs to be able to go into my code and make changes himself if I happen to fall off the face of the Earth. VB.net is kind of C#'s cousin that went to prison. VB is still treated like a family member, but nobody talks about him or his past. I knew C# v.3, and I'm not intimidated by the new language features of C#. Language syntax is something I have no trouble adapting to, but if C# used an entirely different framework library, I would be useless. Fortunately, I know the .NET framework, so my skills translate seamlessly. I try to avoid the things about VB that don't translate easily. For example, VB has language syntax that gets the length of a string: LEN(firstName) is equivalent to the framework String method of: firstName.Length I NEVER use LEN(). I make a conscious effort to not use VB's language shortcuts that replace framework calls so that I don't get my brain stuck in VB land. The language features are almost at par, so I don't feel like I'm missing out on anything. 
&gt; If you only have one implementation, a logical assumption if your declaring the interface in the same file, then an interface is just more code for no good reason. Are you using service in the generic sense (naming shit is hard) or is it a real web service (which is a real WTF)? I am not using it with web services. This is just a services namespace in my MVC project. The value for me of interfaces is it makes it easier to mock in test code. I am aware that you can mock classes as well, but if you are using DI, the constructors for these classes may contain many dependencies, which can be tedious when you are writing lots of tests. Ie, I only have to mock one thing, instead of the thing plus all its dependencies. &gt; If your trying to keep your project organised and understandable (the goal of the article), then one of the best things to do is ask "Is this abstraction doing anything for me?" ;) I find that DTOs reduce coupling between the service layer and the controller. They also enforce a cleaner interface for services (if you use a Request/Response pattern) 
I dig that idea. I just wish my company didn't ignore the 800 warnings we already have about unused variables &amp; hidden methods.
I found this a few years ago and I love it. There's a few limitations, but for the most part it's great.
Thanks op, I implemented this after I saw your post. 
Thank YOU for reading, and taking the time to comment. Cheers!
As an F# + TypeScript dev, this soothes my ears
Best practices evolve with time, and stacks are mostly static/backwards-compatible so they get stuck in time. Never stop learning, or ten years from now all you'll know how to do is supporting legacy apps.
Methinks your joy has less to do with the switch from .NET to JVM and more with the switch from imperative to functional. F# is also an amazing language, in my opinion more so than Scala.
I'm primarily an F# coder, the biggest thing I miss about Scala is Akka. F# has actor, but ownership and management is a lot less refined.
I've been primarily using 8/8.1 and I wouldn't go back. Windows 8 is to Windows 7 as Windows 7 is to XP; I'll always remain fond of XP, but I'm more efficient with 7.
As a hopefully transitioning to more F# &amp; TypeScript dev, this soothes my eyes.
&gt; I only have to mock one thing, instead of the thing plus all its dependencies. It's a mock, you can just pass in null parameters instead of mocking them. They should never get called.
You may get more help if you post error messages, what you tried, what documentation you are using and define the specific problem you are experiencing. Also, stackoverflow.
&gt; In my code, i couple a Controller Action -&gt; ViewModel -&gt; View (since it's strongly typed) These 3 items are coupled and not reusable. I also couple my Request/Response DTOs to a service method. I understand that. I am saying that is not maintainable. &gt; How do you reuse a view in multiple projects? I've never really seen that before. You re-use by designing right. A View corresponds to an element on a page. So you can use it again. In the same way that a TextBox can be re-used across GUI applications. &gt; As far as reusing the service layer (how you are referring to "Model" here), I agree. That is data access code, and should be reusable from other projects. But a service layer is not really part of the MVC framework. Perhaps that's a blog entry for another day... The model is the data access, business logic, and anything else that does not have to do with accepting a request and sending/rendering a response. Note I build far more REST API services than Websites. The UI elements are typically help and example pages. &gt; Perhaps you are misreading me? I never suggested putting Models, Views or Controllers in the same file (I don't think this is possible even as far as views) I am reading this: &gt; Store ViewModels in the same file as controller methods
This couldn't have come at a better time. 
take a look at NPOI. very fast and easy to use
does it handles date correctly? I had nightmares regarding date in Excel. When suddenly I have several customers born in 1900.
yes it does, i just tried that, here is the corresponding snippet https://db.tt/kD9QViyi 
If you're this concerned about language you must be a shitty programmer. 
It's not free, but this is [what I use at work](http://www.aspose.com/.net/excel-component.aspx). 
&gt; Sorry for the delay but its' added now. Hey thanks. I added you to my sidebar as well. &gt; You should think about merging /r/windowsazure and /r/azure. I dunno how to do that.
Seconded. I use it for exporting reports better than rdlc can.
In addition to Aspose, [SpreadsheetGear](https://www.spreadsheetgear.com/downloads/purchase.aspx) is supposedly fast. However, it is expensive.
EPPlus is easy to learn, stable, fast and well... The price is right. Not sure what specific date functionality you're referring to, but it's likely there.
Aspose.cells if you have the money.
I see. how easy is it to use compared with the others?
I don't think they costs too much. I think they are in the range of most enterprise prices. It's just that most startups think it's a waste with this amazing new technologies like ruby etc... They start with that to create an MVP and than get stack with it and prefer to spend thousands of dollars to buy Aeron chairs.
I pulled up LinqToExcel for my needs. It's open source and on the public Nuget library.
What are you looking to do? If you want to create a file, add some worksheets, throw some data in cells format them as tables, and autosize them thats real easy. I never tried changing the fonts and such. Regardless of what library you use, if you are doing advance formatting, I'd suggest you store a template xlsx file as an embedded resource and append data to that. That lets non-programmers edit the template.
my flow would be like this "XLS with a certain format in &gt; read the file &gt; do some calculation &gt; put calc result along with some pretty graphs in the xls/xlsl file" About font or colour, I do not really care about it.
[NPOI](http://npoi.codeplex.com/) or [ExcelLibrary](http://code.google.com/p/excellibrary/) if you have to use the old .xls format, [EPPlus](http://epplus.codeplex.com/) for .xlsx.
So you want to parse the C# file, do some calculations that you can't (or don't find practical to) do in excel, and spit out the results into another spreadsheet? You render charts with Epplus, but I'd still advise you make a template output file and append the input and output data to that, and overwrite the original file for performance and maintainability. But do what you find most convenient. Out of curiosity, why not write a C# dll that does the calculations you need and expose the functions as COM objects to call through VBA? Do you want to obscure the calculations from the end user?
Spreadsheetlight. Wrapper around open xml
I use Syncfusion XLSIO at work, which is a component that comes with a larger package of .NET components for ASP.Net, WinForms, etc.. Every once in a while I'll run into some strange bugs, but for the most part it works well and supports all excel file types (including 2003 if you're working with legacy spreadsheets)
Partial classes == you're doing OOP wrong. SOLID ftw
Ah, thanks, I've tried posting in the "Forum" section but I didn't see this specific section in Projects. 
Wasting my time when I could be learning something that's going to be making me money.
Reach out to the other mod and see if he/she is interested in doing that. Then depending on which one you want to keep alive, you just setup a redirect from that sub to the other one. 
NuGet packages is the top item in User Voice by far. We have started implementation and it should be released in 1-2 weeks. There is a limit of 200KB of memory usage per single execution. Internally we use multiple Worker processes that handle compilation an execution. There are some issues with AppDomain and Roslyn that result in memory leaks and NuGet will add to that. What we do is check memory used by Worker after each execution and if it reaches certain limit, we just restart it.
I work for Xamarin, so am clearly biased, but here are some thoughts... First, I'll just say the obviously-biased thing that at Xamarin we think there are pretty insanely-cool things being done by our clients, where insanely-cool ranges from iOS games like Bastion to awesome Mac programs like Calca to enterprise solutions that achieve 80% code reuse between iOS and Android, and (etc...) Second, I think a big part of the issue is that "cool stuff" often originates in the domain more than the technology stack. For instance, it's becoming pretty clear that Python is becoming the language of choice in many parts of the science community. So you see "cool stuff" having to do with numerics and data analysis and visualization in the Python community. Or in the Web community, a few years ago Rails truly delivered a competitive advantage and the Ruby community, for awhile, was "the place for cool Web stuff." Today, maybe that's moved on to Node and the JavaScript community. Third, I'll move on to my main interest, which is language geekery. And here, I'm going to again be self-serving and separate the CLR languages from MSFT-the-corporation. C# and F# are, from a language-geek perspective, two very good languages. C# is the best-designed of the "mainstream high-productivity" languages. It's evolved faster than Java and with a more coherent "mainstreaming of functional programming concepts" vision. Java's now gaining some of those facilities but if you look at, for instance, C#'s evolution towards async/await, you see the payoff of a longer arc. And now mainstream C# programmers are dealing with futures and CPS and higher-level functions and most aren't even realizing that they're getting these benefits because of some fairly "neck-beard-y" stuff. F# is a *sweet* language. (I don't think that it's as clear a next-step for the CLR world that Scala is for the JVM world, but that's only because I'm quite certain that C# will continue to evolve.) F# seems to be gaining some traction in some of the same "cool" areas as Python (Big Data and analysis, machine learning, etc.). It hasn't "crossed the chasm" in the way that Python has, but OTOH, the F# community is in that great time when it's small enough to still feel connected but successful enough to be throwing off interesting libraries and projects (and some amount of jobs). I think you get similar passion and excitement you get in, say, the Erlang and Clojure worlds in the F# community. I program iOS and Android apps in F#: that's not worn-down territory. But I'm going to go back to JavaScript to make my final point, which is that when languages are mainstream, the "cool" stuff is harder to see, even if the absolute number of cool projects/jobs/community is greater than in some of the emerging languages. The vast majority of JS jobs are going to be horrible front-end Web jobs where you'll be thankful if people have heard of jQuery. But on the other hand, you have Node.js-based startups with 24-hour-in-office-drone-delivery. Similarly with .NET and the CLR. There's a ton of enterprise-y business jobs out there. (Protip: That's not a bad thing if life throws you a curveball.) But there are also startups, mobile entrepreneurs, data science, visualization, hard-algorithm domains, etc. They just don't jump out quite as much. P.S. Xamarin is hiring http://xamarin.com/jobs 
Learning new languages isn't the only thing that leads to being a 'better programmer'...
Uh.. Because... I don't know any better? :/
We use spreadsheet gear at work. Its fast and easy to use. Documentation and support are pretty good as well. I highly recommend it.
That's what my company uses; used to generate ~1,000 different reports from very simple, to fairly complex (graphs, pivot tables, complex formatting, etc.). Can be a pain at times, but have no regrets about picking them.
I have a large application that uses SpreadsheetGear to display loads of data. It took a while to learn all of the nuances with formulas and how to properly work with the control, but it certainly does its job. If you need to do a lot of advanced excel functionality programmatically within .net, of if you need to offer a gui to your users for an Excel like feel, there really isn't anything that compares (I've tried out a number of controls).
Great tool, Rion. Some developer coworkers and I had fun trying out the collaboration this week. One question: When you're collaborating and hover over the top of the dock, you see a "Move the dock" tooltip, but we couldn't figure out how to move it. Is that a future feature&gt;
I believe so. I tried it out as well and experienced the same issue. There are quite a few things that seem to still be in the works, so this might be one of those issues. The developers have been pretty quick to respond to any issues and I'm sure they will catch wind of this and hopefully throw a fix together.
Theme looks great, don't worry about it :)
No totally, but it's one thing. Saying you're not going to learn a new programming language because there are other, better things you can learn is equivalent of saying you don't do cardio, only weights.
I think that's a bad analogy. Weight training and cardio are different types of training, and both are somewhat required for a fit and healthy body. Learning a new language and learning say, how to run a small business are different types of learning, but I wouldn't say that both are required. It depends upon the situation. Right now, for me, learning about running a small business is more worthwhile to me than learning Ruby (for example) because I'm about to start running my own business, but I don't see myself using Ruby at all. It might be a good idea for others, however. I just disagree with the assertion that 'everybody should be learning at least one new language every year'.
Bingo. If you have this kind of dependency nightmare to deal with, then you should setup an internal NuGet server. It solves all of this in a much more elegant way.
Those benchmarks could have been so much better... They are basically testing JSON serialization performance across almost every single one of those benchmarks. Even the database ones, etc., are serializing the data before output. I remember seeing these a few months ago and looking at the source code, and was kind of surprised... essentially if it was slow at JSON serialization, it would be low in all scores.
No, there isn't - I looked into it a while back and static is the only option, you can see some more info [here](https://dlr.codeplex.com/workitem/1378)
This is truly a cool tool, Nice work! Any plans to open source? 
Crosspost from r/csharp
I think all us redditors and /r/dotnet subscribers understand the why the change was made. But some of us just don't like the look in general. I actually go fairly far out of my way to avoid having a Windows look to any product I develop. However, I won't rally for you to change it back because its just as easy for me to turn the theme off :)
True, but that's not entirely the point of the article. I wanted to address the fact what people think what's fast might not be as fast, and what are the upper limits of what one can achieve in raw speed. There's a tremendous amount of difference between EF and e.g. my own framework (LLBLGen Pro). There's however not a lot of feature difference between these, that one can say "EF has to do more, so it's slower". They simply didn't engineer for it. They said to fix it in 6.1 though, so we'll see :) btw: &gt; To select something with a Micro-ORM such as Dapper, it can just execute a query from a string and is a convert to an object. Doing the same with nhibernate or the entity framework requires them to use all kinds of magic, usually reflection, to get attributes such as a table attribute and a column attribute, dump everything into a query builder, and generate a query at run time. this isn't true. EF and NH know exactly what's expected in the query, how the object looks like and how to put the data into the object at runtime. You see that in the EF without change tracking test, which is very fast: it still materializes objects, it just doesn't do change tracking logging etc. so it bypasses a lot. But what it doesn't bypass is precisely what you describe above :)
Have you tried doing a 'clean' then 'rebuild-all' then try to use the SafeFileName property? 
Yes, no luck there. I also went back into the one non Microsoft resource and changed it to 4.0 as well.
Tried removing the reference and just replaced the occurrences with Object. Clean &amp; Rebuild and I'm still getting the same error. 
Oh Dude, I just re-read your post and I think I figured it out. Are you using the reference: * [System.Windows.Forms.FileDialog](http://msdn.microsoft.com/en-us/library/system.windows.forms.filedialog.aspx) Or are you using the reference: * [Microsoft.Win32](http://msdn.microsoft.com/en-us/library/Microsoft.Win32.FileDialog.aspx) Because the Win32 class is the one with SafeFileName property. The other one does not have it. I have no idea what the difference is between the two libs. 
Thank you so much. I was using the first one. Now I don't feel so crazy.
What isn't true? That EF and NH have to pull tricks to generate queries? When compared to Micro-ORMs such as Dapper, they do. When Micro-ORMs can just execute queries from strings directly, thats pretty hard to compete with... I haven't used *LLBLGen Pro* myself, but I don't doubt its fast, if thats what you designed it to be. Maybe for a full speed comparison, you should compare the other CRUD operations too, to ensure its not just a one trick pony, good at selecting things.
What ultimately matters in your Stored Procs are the select statements. I'm assuming your stored procs only returns 1 DataSet with the columns described in your SalesSummary class. var sale = cnn.Query&lt;SaleSummary&gt;("spGetSalesSummary", new {startdate = "", enddate = "endDate", login = "login"}, commandType: CommandType.StoredProcedure).First();}}} This will return the first row of your Sales Summary Stored Proc. Make sure the column names matches your SalesSummary parameters name
I just did some playing around with it and the difference is almost entirely in how the various frameworks handle relationships. Delete the relations mappings for nhibernate and it was as fast as the micro orms. Considering that micro orm's don't handle relationships I think that's a fair comparison.
petapoco had a great test
That kind of performance on NHibernate makes me think there might be a N+1 SELECT issue going on. I`m not interested in putting in the time to read the code and check, but the symptoms look right.
The article states &gt; 10000 times 3 entities gives you the same performance degradation But there is no test in the repository that convinces me this is true at all. I have seen many home-grown db frameworks that become slower as O( 2n ) or O( n^1.5 ) and I wouldn't be surprised if EF and NHibernate have the same issues, especially with relationships (my uninformed guess is an issue with trying to find the child entity in some level of cache). If that's the case, then this benchmark really is completely meaningless to the vast majority of readers. If the performance of all of the ORMs really is linear with regards to result count, then this can be taken as seriously as ORM framework benchmarks deserve to be taken. 
I've included a screenshot of a profile I did on the NH code, to see why it was so slow. When I include 1 entity, no relationships, the NH code is hovering around 1500ms, EF around 1100ms. The slowness is coming from having more relationships in the model, but that's precisely the point: it's unnecessary to have that influence the fetch of a set of a single type: the other relationships are not at play, the ORM _knows_ that. The slowness isn't coming from the size of the set, but of the size of the model: more relationships, slower fetches. What you refer to is having a dumb ORM store more and more entities in a set which gets slower and slower because it does a list.Contains(toAdd) before adding an entity to avoid duplicates (wild guess), which gets slower the more entities are stored. That's not the case here: the more relationships in the model, the slower they get. That is precisely why I picked this entity and this database: if an ORM has sloppy code so it can't deal with normal sized models, it will show. 
When I had 1 entity in the model, no relationships, it was still slower (around 1500ms): http://pastebin.com/AdsKitr3 This is from earlier this year. EF5 is faster too, as it has just 1 entity too (all have just 1 entity, except llblgen pro, as it isn't affected by having many relationships in a model, as it stores information about the model differently) The thing is: it shouldn't have *any* difference whether there are a lot of relationships in the model or not: the fetch is about a single entity, the ORM knows that, still it has to spend a lot of time doing completely irrelevant things it seems. It's not as if the ORM thinks 'oh my, I get a set of rows back, maybe this is a projection of 3 entities (flattened graph) and I have to project them back to 3 entities and merge them efficiently', as the query clearly states: single entity, nothing special. 
Why the hell would EF use reflection? It knows at compile time everything there is to know about the classes involved. I'm not saying I know how EF works internally, but it's pretty damn clear how it shouldn't work.
I'm all the way over in Europe, so I can't give you a lot of insight on those camps or the market in your region. But I want to just tell you that as a junior programmer, your attitude will be everything. My company hires people based on their attitude, regardless of their skill level. Because in the programming world as it is, a lot of the knowledge that's relevant today won't be in 5 years. So an eagerness to learn and a genuine passion for technology will get you a job sooner or later. Good luck!
Once again ORMs prove that they can be fast as hand-rolled code... if you spend copious amounts of time tweaking all of the settings and disabling the stuff that makes it an ORM.
By the time you are done deleting all of the relationships, disabled the change tracking, and have replaced the auto-generated SQL with hand-written SQL you'll almost have good performance out of the ORM. Or you can save a lot of effort and start with good performance by not using an ORM in the first place. Not a hard choice for me to make. But then again, as a consultant I get paid by the hour.
&gt; When I had 1 entity in the model, no relationships, it was still slower (around 1500ms) When I tried it with no relationships it took it from 4500ms down to 1000ms, not great but not terrible either, less than twice the others. &gt;The thing is: it shouldn't have any difference whether there are a lot of relationships in the model or not: the fetch is about a single entity, the ORM knows that, still it has to spend a lot of time doing completely irrelevant things it seems. Because it does have to do a fair bit. It has to generate a placeholder class for all those mappings, when you profile you will see the constructor called. Otherwise if 2 entities lazy load the same relationship they could end up with different instances, instead of one, which could be bad for data integrity and performance. 
&gt; By the time you are done deleting all of the relationships, disabled the change tracking, and have replaced the auto-generated SQL with hand-written SQL you'll almost have good performance out of the ORM. This is a manufactured scenario, great for testing and optimizing but it says nothing about real world performance. &gt;Or you can save a lot of effort and start with good performance by not using an ORM in the first place. That ends up with several times the effort and quite often poorer performance IME.
&gt; to let people see how fast the frameworks they use really are It doesn't, it's not a real world scenario by any means and says nothing about how fast the frameworks are. &gt;This wasn't always the case though. 6-8 years ago it wasn't fast at all, as I hadn't done any performance tests and was confident that it was fast enough, like EF and NH teams still do today. Hopefully nhibernate will too, it does seem like performance could probably be improved, it's just a matter of if it's worth while or not. There might be better uses for the time.
You apparently glanced over details you didn't want to see. E.g. linq to sql and also LLBLGen Pro didn't require you to do anything special, yet were 5+ times faster than EF or NHibernate. 
In some scenario's it might indeed, however with that logic, it must be unclear to you why people have written micro-ORMs like dapper, as they apparently serve no purpose (not that I want to promote micro-ORMs, I just use it as an example that for some people there was a requirement to do faster fetches as it did affect their applications) Not to flame, just interested in how you see that. :)
I thought the point of micro ORM's was that they are quick (setup, not necessarily performance) and simple with no mapping configurations etc. As far as I know they aren't actually ORM's, they are just data set mappers with some basic querying. To the best of my knowledge their use case is relatively simple websites (not that there's anything wrong with that).
Interesting :) (considering that dapper was written solely because linq to sql was too slow for stackexchange) Thanks for your thoughts though. The test was a spotlight on 1 single feature, not everyone finds it important (for various reasons, valid or not). 
&gt; (considering that dapper was written solely because linq to sql was too slow for stackexchange) I didn't know that. Was it specifically hydrating objects that made it too slow for them? Your tests indicate it's not that different. But I don't know if that's changed over the years, etc. &gt;The test was a spotlight on 1 single feature, not everyone finds it important (for various reasons, valid or not). I think it's interesting, and that it was a worthwhile thing to do. Hopefully the nHibernate maintainers see it and can improve it. 
It's still 10% or more, which might be important if every cycle counts. But linq 2 sql has some quirks, not all queries are fast so it was necessary for them to look elsewhere.
What tweaking? I didn't tweak anything at all. 
Because, as a simple example, you can make something like this: [Table("Customers")] public class Customer { [Column("Full_Name") public string Name { get; set; } } Which usually requires reflection to read those attributes.
WebApiThrottle is open sourced and MIT licensed, the project is hosted on GitHub at https://github.com/stefanprodan/WebApiThrottle, for questions regarding throttling or any problems you’ve encounter please submit an issue on GitHub. 
Reflection to read the attributes, then runtime code gen to create a function that copies the data into it. The warm up costs will be higher, but you only pay it once per class.
Wanted to say thank you for your help! This is the nudge I needed!
Yes, there are. I recently got that certificate too. There is this Jump Start video which is really good, and covers pretty much everything you need to know at Microsoft Virtual Academy: http://www.microsoftvirtualacademy.com/training-courses/learn-html5-with-javascript-css3-jumpstart-training which is free. If you feel like you need to know more about specific topics, I found the pluralsight videos very useful too. This one about HTML5 for example: http://pluralsight.com/training/Courses/TableOfContents/html5-fundamentals-2e Microsoft Virtual Academy is free, pluralsight has a free trial, but I think it requires you to give them a creditcard
I pay for PluralSight because it is excellent, but I was planning on going through CodeAcademy afterwards, to pick up any random things that MS really wants you to know, before I test.
Did you take it? Did you use any prep books or other from MS or do you think CodeAcademy covers it?
* Are you certified? No * In what are you certified? No * Do you find the certificate useful for obtaining a job? No * Do you find it useful for obtaining higher pay? No * Do you find the knowledge useful for your work? No 
I haven't done a cert in a long time. I am a MCP and MCAD. Both were done in 2003/2004, so that was when .Net was new.
I've taken a few tests, but it was always paid for by an employer. Being certified might have a marginal effect on your ability to get hired, but don't count on it acting as justification for getting a raise. The Microsoft tests evaluate your ability to memorize random facts about the particular technology you're testing for, so no, the knowledge you gain isn't useful. If I were a hiring manager, certifications would be near the bottom of my list in terms of importance.
third
Yes SharePoint, Web Development, many more Yes Yes Yes Source: We're a microsoft partner so maintaining gold/silver partner status means certain partner programs and perks are available to us. Generally we have a culture of certifications within the organization because of it and everyone embraces it. The training and exam are company paid.
Fourth, And as an owner that writes code every day, I'll take real experience (and pay for it) over a cert only. 
Reading the certification books are infinitely more useful than actually getting the certification. When I see the certification on a resume, I just ignore it. It doesn't sway me one way or the other.
I took the test before (failed it.. it was free and I never took a test before so wanted to try it before it expires). A lot of the material was targeted toward the MS stack. You will see a lot of windows 8 layout stuff covered in the MS virtual academy class. What got me was the javascript. I never wrote a web worker before and questions related to JS calls and functions killed my result.
This is my blog spam, which I'm not sure is allowed here or not. Being a quiet sub I figured it would be ok but let me know otherwise. I also just put up [part two](http://proactivelylazy.blogspot.com.au/2013/12/orm-vs-sql-part-2-ordering.html) which looks at ordering. nHibernate also begins to spank sql here (10ms for nHibernate vs 8500ms for sql). I'm also very curious if anyone can improve on the sql side of things. I'm far from confident that I have the optimal solution, just the simplest one that works.
Yes, I have some certifications and no, I don't think they matter much. And I was seriously pissed off when I found out how easy it was to cheat. The first one required quite a bit of studying.
Good to know, thanks
&gt; Are you certified? Yes. My company paid for it and I did it while I had some downtime anyway. It's part of the Gold Partnership stuff. &gt; In what are you certified? Currently: HTML5/JS/CSS, MVC4, Azure/Web Services. Three exams that results in the title Certified Solutions Developer. &gt; Do you find the certificate useful for obtaining a job? Marginally. Hard to say. Our sales people says it helps. &gt; Do you find it useful for obtaining higher pay? See above. &gt; Do you find the knowledge useful for your work? Well the knowledge areas tested are what I use daily at work and in hobby projects. It's my career and passion. Of course it's useful! Or do you refer to *knowledge gained while studying* for the certs? Then no, I didn't learn anything new that I find useful. ----- Also, I felt the current tests are much better than what I took 4 years ago. There are less questions like "what is the namespace of the BinaryFormatter type?", and more that actually tries your programming skills.
&gt; Are you certified? Yes &gt; In what are you certified? 70-515 and whatever else is along that topic, I did 2 exams 2 years ago, purely for the points to become a MS gold partner at work. Now its useful in this job because we needed a technical contact as part of our MS profile and they required someone to be a MCP. &gt; Do you find the certificate useful for obtaining a job? No, although one job I did apply for said its a step up towards getting their Gold certification with MS. &gt; Do you find it useful for obtaining higher pay? No &gt; Do you find the knowledge useful for your work? No By the way, depending on how you look at things, I kind of "cheated" to get my certs - I went to http://www.examcollection.com/, downloaded the questions for the topic on Monday, learned all the answers by heart, then on Thursday went to the Exam and got 90%+ in 40mins. All the questions are multiple choice, and I simply memorized "For question 'What is LINQ good for', the answer is C" - I got to the point where I didn't even bother reading the question, just looked for keywords in the question and picked the right answer. According to the examining place I went to (in London, UK), quite a few people do it this way too.
* Are you certified? No (I once got an MCSE but that was a big mistake) * In what are you certified? B.sc. in computer science. * Do you find the knowledge useful for your work? The degree: absolutely. What I've seen of ms certification, I don't think they're useful: they don't teach knowledge which leads to wisdom, they teach tricks how to do a certain action one can also learn from a manual. If you are a hiring manager: * Do you consider certificates? No * How much do they weight your opinion? Zero. An MS certificate would tell me nothing about what the person can do, how the person can apply what's been taught in practice, how the person solves problems and how the software written looks like and of what quality it is. TL;DR: don't invest money in them, take a traditional course in computer science, it will be helpful your entire career. 
The sql in the ordering isn't OK. You can simply use this pattern: WITH __actualSet AS ( SELECT *, ROW_NUMBER() OVER (ORDER BY CURRENT_TIMESTAMP) AS __rowcnt FROM ( -- the query to page here ) AS _tmpSet ) SELECT * FROM __actualSet WHERE [__rowcnt] &gt; @rownumberLastRowOfPreviousPage AND [__rowcnt] &lt;= @rownumberLastRowOfPage ORDER BY [__rowcnt] ASC See: http://weblogs.asp.net/fbouma/archive/2007/06/05/sqlserver-2005-paging-there-is-a-generic-wrapper-query-possible.aspx The query can then be anything, without any paging directives, but WITH the ordering. You simply wrap it with this paging wrapper. I use this in my ORM (LLBLGen Pro), works great. Additionally, creating a temptable for paging (with identity column which is used to select the page) isn't so bad either. Perf tests (that's me again ;)) with very large sets and temptable based paging suggests it takes a very large set (over 200K rows) before it really starts to show it's slower. Btw, paging without ordering AND without distinct filtering is actually quite useless: you might run into duplicates or pages with the same data. NHibernate doesn't apply any distinct filtering in a query, you have to do that manually, and without it, the paging query is not useful. btw2: calling MySQL elegant made me chuckle :) about the only thing elegant about mysql is perhaps its paging query syntax, which is actually quite equal to postgres/firebird. 
&gt; Are you certified? Yes. &gt; In what are you certified? MCTS 70-516 (Data Acces in .NET 4.0). &gt; Do you find the certificate useful for obtaining a job? No experience so far. &gt; Do you find it useful for obtaining higher pay? No. &gt; Do you find the knowledge useful for your work? No. The knowledge required to pass the tests are to specific to MS products. If you use a mixture of tools from different companies, you won't benefit from it. But i have to say, i took the exam 3 years ago. It probably changed in the meantime.
Oh I have a CS degree and a minor in Mathmatics. I am only asking because the whole idea of certs is interesting to me. I am a career oriented guy (ie I Like Money) so I wanted to know if they would boost my career. It seems like the answer is a resounding no.
Interesting response. I never thought of it as a way for the sales team to sell. I can see that as a positive. "We have certified Microsoft experts working on our software. We are a Microsoft Gold partner." On the other hand... That only works for a SaaS model :)
Are you certified? Yes In what are you certified? I have a MCPD Web 4 Certificate That includes: MCTS: .NET Framework 4, Data Access MCTS: .NET Framework 4, Web Applications MCTS: .NET Framework 4, Service Communication Applications Do you find the certificate useful for obtaining a job? No, not really. Do you find it useful for obtaining higher pay? Maybe, but I haven't gotten higher pay explicitly because of the certs. Do you find the knowledge useful for your work? No, most of the stuff the certificates cover is how Microsoft, at the time of creating the test, thought stuff should be done. It's not best practice, and most of the stuff in the Data certification is stuff I rarely do. It may different in different countries, but here (Sweden) most professionals I know think certificates are a necessary evil, clients/customers tend to value them (probably because they have no idea what they mean), and in some cases it could mean that last little detail that puts you above some other candidate. Also, the sales people think its an easier sell than a non-certified developer. It's all smoke and mirrors though.
I'm not certified because I've found that it's not that useful from a career standpoint. The only places that care about it are the gold/silver/whatever Microsoft partners. But no other company really cares. I'm also a hiring manager where I work and can say that we don't consider certificates at all. They get completely ignored by us. In fact we've found a negative correlation between certificates and our interest in a candidate. Generally anyone that applies with us who even comes close to what we're looking for, we do a phone screening. And on the phone screening we do a coding test using some screen sharing app (can't remember the name of it at the moment). In our experience people who are big on the certificates can't do the coding test (which is something on par with FizzBuzz). Might just be the candidates in the local market though.
You mean visual studio?
&gt; It seems like the answer is a resounding no. Correct. In the early days they were a good sign that the person indeed had done his/her homework and knew a thing or two about the product, but nowadays it's easy to pass the tests with braindumps and the tests themselves are actually quite silly: there's emphasis on little things, not on 'why' and general good skills like problem solving. your degree will bring you further than any certificate will.
MZ Tools. Been with me since VS 98
For something a little different: Search code like you search google: * [Sando](http://visualstudiogallery.msdn.microsoft.com/06f39a31-20ce-408c-afee-8a02b484db1c) Automatically commit saved changes to a local git repository: * [autogit](http://visualstudiogallery.msdn.microsoft.com/45a3d62b-955e-43cb-9d91-255a837d5f35) Automatically generate a summary and time-ordered diffs from your coding history: * [automark](http://visualstudiogallery.msdn.microsoft.com/078d00b7-dfbd-4cfa-97f9-8be08bb510ee) Better TODOs, highlight TODOs, optionally attach them to editor viewport: * [attachables](http://visualstudiogallery.msdn.microsoft.com/850937ba-ff0b-43cb-badd-4e273b508c32) Auto-insert popular code snippets from the web/Stack Overflow e.g. `//? Encode string in base64` * [Flow](http://www.flowextension.com) A collection of nice highlights, editor improvements: * [Viasfora](http://viasfora.com/)
1. [Productivity Power Tools](http://visualstudiogallery.msdn.microsoft.com/d0d33361-18e2-46c0-8ff2-4adea1e34fef) * Multie level tabs. * Tripple click a line selects the whole line of code. * Ctrl + CLick Go T oDefinition * Enhaced Scroll Bar - By far my favorite! Your scroll bar turns into a mini view of your code file. You can't read it but it has indications on red squiggles &amp; break points, shows the indentation of your lines so you can quickly jump back n forth. 2. PowerCommands * Close All * Copy Class * Collapse Projects (Another favorite! It collapses all the projects in your solution.) * Open Containing Folder - In case you can't find a file on disk. * Remove &amp; Sort Usings - Awesome if you are OCD about using getting rid of using that are not referenced and like them sorted. I have had this bite me though. Something about a using that was not explicitly referenced but was needed during reflection. 2. Viasfora * Colors different keywords differently. * Rainbow Races (currently turned off for 
ReSharper is pretty handy for automating some mundane code tasks and code cleanup, among other things. NCrunch is the bee's knees if you're into TDD.
I would suggest trying another forum unless you are planning to show source. A discussion about your program without code is somewhat useless in the scope of a .NET forum. You would be better of asking for people to beta your work in a gaming forum. FWIW I am extremely suspicious (as many are) of a random compiled binary posted with someone asking me to "go ahead and run this code". There is no way for me to tell what the payload is without de-compiling or running in an isolated system. EDIT: RED FLAG WARNING. OP is a NEW user as of today and this thread is the user's first and only post. Chances of this being malware just jumped from 50% to nearly 80-90% in my mind.
&gt; No (I once got an MCSE but that was a big mistake) A big mistake? How so? I can imagine that certificates may not be useful. But a big mistake?
Not integrated into Visual Studio, but this looks like a nice up and coming contender for code search: https://sourcegraph.com/ Also, the Flow extension I mentioned lets you search code, by typing an english comment and inserting a code snippet.
pricey and requires some time to really get the keyboard shortcuts down, but I &lt;3 Resharper
I just pulled down Viasfora and am on the fence about it's choice of colors. Red typically is a panic color, so I'm not too keen on it's choice of that for simple "if" blocks
&gt; In what are you certified? I'm certified in HTML5/JS/CSS, ASP.NET, WCF, ADO.NET, Core Foundation &gt; Do you find the certificate useful for obtaining a job? Not sure. I was recruited on linkedin and still get messaged by recruiters on a weekly basis. But my colleagues who are not certified experience the same thing. I would assume they help keyword based search triggers of recruiters go off... &gt; Do you find it useful for obtaining higher pay? Yes. On a yearly basis we make a plan on how we are going to improve ourselves where I work, which is used to determine how well you're performing. Just saying "I learned HTML5" is a little weak and difficult to make into something SMART. Getting a certificate is a SMART goal and proof for a manager that you actually acquired something. &gt; Do you find the knowledge useful for your work? Yes. What I learned for the .Net Core Foundation certificate is very useful for example. When I was just a self-taught .net programmer, I did not know *why* half the things worked they way they did. After looking it up I could usually figure out how to do most things, but without deeper understanding of the framework I didn't know why. ADO.NET has been useless for me. ASP.NET and WCF have been to use. Enabled me and colleagues to quickly solve certain problems, without having to look it up. I only recently got HTML5/JS/CSS, and haven't been to much use so far, but that's probably because of the field I'm in, usually writing back-end code 
Anyone here using [VSCommands](http://visualstudiogallery.msdn.microsoft.com/a83505c6-77b3-44a6-b53b-73d77cba84c8) ?
I am surprised that as a back-end coder you found ASP.NET more useful then ADO.NET.
Thats because its an old certificate. 70-561, ADO.NET 3.5 from 2008 Newer certificates such as 70-516 are not pure ADO.NET certificates, but do include entity framework things. Those are probably useful. EF is build on top of ADO.NET, but when it first got released in 2008, its usability was questionable, so I hardly used it until the 4.0 release. By then my ADO knowledge was pretty much faded away
prior to 2012, this wasn't a native feature. I remember writing my own macro to do this in 2008, and using an extension to do it in 2010
The best feature in 2013 (might have been in 2012) is the sync with document thing. 
If you consider the time savings, it is well worth it (for both). Even if it only saves you 2 minutes a day, over a year that is over 8 hours a year. That x your wages is less than the cost. Plus add in the extra sanity from having that stuff automated :)
Yeah the holiday sale is the only reason I have my license.
You forgot the most important part of ReSharper: it analyzes your code and finds common programming errors like possible null reference exceptions, loops that can be converted to faster LINQ queries, redundant statements, propped programming practices, and much more and it does all of this on the fly. Honestly I wouldn't write any C# without it.
Having looked at your wrapper a bit more. Performance seems to be about on par with the solution I showed, although it does give you a bit more freedom with the queries. It's the dynamic ordering that slows things down dramatically.
VsComnands (I think, not at my computer now) gives you an Attach to IIS button.
I'm still on the fence about it. I'm going to give it the full shake on the next phase of the project I'm on before I give it a formal shake. If it works as well as it *looks* like it could work with the level of integration NCrunch provides? I'll be countering with, "You have time to rebuild and reload IIS Express each time you change code? And fix bugs in code you wrote last year because of a bug you introduced in your last patch?" It does seem utopian. But the last programming conference I went to and saw these two tools in action together convinced me to give it a fair shake. We'll see!
NCrunch is AMAZING; my #2 paid extension, hands down. Actually, I'd prefer to code without Resharper than without NCrunch.
FxCop is nice for code reviews.
Don't get me wrong, Resharper is spectacular - but missing it means (largely) just typing extra code. NCrunch streamlines testing so well that it has become an integral part of my workflow. Or, my not-workflow. It's so good, I don't notice it when it's there, but I wouldn't know what to do without it. My code would be uglier without Resharper, but my code would be much less tested without NCrunch.
One of my favorite benefits of well covered code: fearless refactoring!
you win!
ReSharper and StyleCop are my two goto's here. Takes a bit of time to setup right and share the settings files with everyone, but now all my developers essentially have an automatic wrist slapper on all projects when they do something that doesn't fit the coding standard. I do realize that FxCop has some slightly different uses.
That's just a formatting bug. Remove and replace your closing brace and Visual Studio will fix that right up for you. ;)
The ordering is placed inside the wrapper, which should work OK. Paging inside sql server is a delegate matter, there's always some performance trade off. You could also try to create a temptable with all the columns of the normal query and one extra which is an identity column, then do an insert ... select TOP (@skip+@take) * FROM (real query)) into that temp table and select the rows of the page to select. With a properly setup temp table it should perform OK. The dynamic ordering you're showing is slow because the query plan has to be recompiled with every execution. The same happens with if statements in sql. Typically people using procs will write multiple ones, based on usage scenarios. The thing is that in theory one should be able to sort on all fields, but in practice the only thing that counts is what's used: if that's just 2 different sort methods, create two procs. :) (yes, I'm playing devil's advocate here. Dyn. sql has many advantages, one is indeed having optimal sql for the situation at hand.)
Turned off for what?
Yes, extremely handy
I found using NCrunch to be quite transformative as far as testing goes. There's nothing like having your code coverage status in your face *all the time* (but not intrusively so, unlike CodeLens) to keep you on top of your test suite. 
oops! ...for a couple reasons. 1. One of the braces was red which didn't mesh well when it was near a "string". 2. I don't program very deep methods so there are relatively few braces for me to match up anyway. I can do most with just glancing. 3. In visual studio you can change the color of the Brace Matching Rectangle. I changed mine to neon green. So when I put my cursor on the back side of a brace VS will draw a neon green rectangle around it and its counterpart. This is enough visual indication for me. 4. Lastly, I found it to add the ever so slightest delay to my typing.
So with Style Cop. Who the fuck decided that using statements should be inside the namespace declaration!
On second thought. I am actually branching visafora to my local repository and I'm going to add a color selection dialog under general.
Does this affect CodeFirst?
I don't think so, only projects that use the EDMX seem to be, but I haven't tested it.
that would be nice. you'd think that a plugin whose meat &amp; potatoes is colors, there would be a color picker.
After looking at the code the project seems young and there is only 1 dev. So it is actually well written and does a good amount for the given timeframe.
local builds fart and give you the "N successful builds since the last fart" in the status bar. this is gold.
&gt; PSA: Avoid using Entity Framework in Production Fixed that for you.
One is for WinForms, the other WPF. I'm not sure what that means in real terms.
EF is pretty good nowadays
The point is that this LINQ query was entirely generated by resharper. The code I'm actually using is (multiple) foreach loops + extension methods which is a lot cleaner than what RS suggested.
It's ok, but for beyond prototyping. No.
Lol. You found the most reviled rule in the whole set for me. There is an edge case where you might want that. There's a stack overflow question on it somewhere. StyleCop and resharper both need to be setup by the lead and then the configs shared. Both have some weird defaults that once fixed make the pair incredible.
I use PetaPoco and Nhibernate. Hell even Lightspeed is better in my opinion. Just for speed and the SQL generated. For a nice lightweight alternative you should check out https://github.com/linq2db/linq2db
We use a combination of EF and Dapper. EF for basic CRUD stuff but then Dapper where speed matters.
Yeah, we've been exclusively on code first, and as I said haven't had any issues. With that said our model is not at the sort of scale that they are talking about here. So it might still be a problem, it just doesn't impact us.
Ours is at over 200 entities and to say it's murdering our speed is an understatement. Home page load went from ~300ms to ~1000ms on average right now. Some pages are clocking in over a second and a half load times, all tied to EF.
Our entities are pretty basic, very few FK properties to cause a bunch of associations and nearly everything is a view anyway. Not to mention our performance hit happened the moment it got updated to EF 6.0.1 from 5. 
The query worked as expected and it took about a second to complete. I think it might be more of the code that is parsing the data than the query itself but I don't really have access to the code.
If that's the case, it doesn't make sense why the SQL server is maxed out. Try putting breakpoints in your code to see when the query is executed and when the results are returned. 
How does that even happen? Is it parsing the xml every time an entity is loaded? Does it happen in release builds as well? Even rudimentary performance testing should of found this.
~~Same exact templates with 5.0 and 6.0.1?~~ ~~We're using 6.0 with a pretty large, complex model and we haven't seen any of those issues with the upgrade.~~ Ah, now I see what you're talking about. I think "don't use EF6" is an overstatement and very specific to this test case, though, because it's not going to be an issue unless you're loading a massive number of records and you don't care about those associations at all. So for this specific case, yeah, a revert to 5.0 is probably in order. But for paged lists of 10-20 records like you'd see in a typical CRUD app, it's not going to be a problem. Edit: ~~Do you happen to know where to find the AdventureWorks database that the ticket is using? The one I have (2012) has a different schema than the one used in the repro.~~ Never mind, it was 2005 version
Shocked they didn't make it version 9 and charge for an upgrade from 8.0. I'm kidding, but it did feel Jetbrains was releasing make versions at increasingly alarming rate 
&gt;but I don't really have access to the code How exactly did you inherit the application then? If you run the profiler (the tools menu in management studio) it will show the queries with their parameters etc.
I have been doing the exact same thing lol. I also break up my models into smaller domains, although I understand that might not always be possible.
Ha, nope. But it definitely beats EF hands down on performance. But not everything needs to squeeze out every millisecond possible. We mostly use EF for MVC and WebAPI projects. But we have some workers that pull work from a queue and it needs to churn through them as fast as possible. I like the flexibility of LINQ/EF for the web stuff since that is always morphing and evolving based on new requirements or UI changes. I'm also very against CodeFirst. I'm all about DB first. And the EF designer stuff lets us manage that better too.
Also, AsNoTracking gets it to ~315ms, which doesn't excuse the bug, but is probably what you'd want in a view situation anyway.
Had a lot of success with the profiling tools from hibernating rhinos http://www.hibernatingrhinos.com/products/EFProf
It happens in our release build. And I agree, sloppy for this to have been missed is an massive understatement.
tbh i wasn't around before 7, but I feel like they added a substantial amount of features between 7 and 8 to warrant the version upgrade.
Solution Explorer Collapse And Sync - http://visualstudiogallery.msdn.microsoft.com/291836cc-febe-44ad-86b0-b745485110e7 A very neat one that add "sync with active document" button to solution explorer, instead of all on or off.
I was the one who found it, I first reported it here: https://entityframework.codeplex.com/workitem/1781. Rolling back to v5 is of no use, it is also affected. The related workitem 1781 has a screen shot of the dottrace profile I did. I found it while doing benchmarks of ORMs, I blogged about that a few days ago: http://www.reddit.com/r/dotnet/comments/1sgwjm/fetch_performance_of_various_net_orm_dataaccess/ It surprised me a lot. At first I found it when moving my tests from v5 to v6 and while doing that I had included all entities in AdventureWorks (which are around 67 of them, so not that big at all). It looked like it was introduced in v6, but my tests with v5 had just one entity, so it didn't ran into the bug. Microsoft's testing of this with v5 revealed it had the same issue. I reported this late October 2013, they have done some testing but it's now quiet. They moved the debate over to this new issue which is now scheduled for 6.1, so it's not fixed soon. Profiling EF v6 to see why it has this problem revealed that it does a lot of work under the hood for the relationships of this single entity type, apparently every time a new instance is fetched (even in a set). I write ORMs for a living, I know what they have to do at runtime to make them do their job. Looking at the EF6 profile, I think it will be a tough job for them to fix this, as it doesn't look like it is a simple bug where they call slow code, it's actually slow _design_. That they haven't found this during testing is likely also the reason why NHibernate is even slower than EF6 (they apparently don't do perf testing as well), also related to associations in the model. ORMs don't need to slow down because of the associations / relationships of entities, that's static information, setup an in-memory data construct which provides the information for the engine and it's fast. 
No it doesn't generate an edmx, but it generates the same elements which are also generated by the edmx reader, which are then used to produce the in-memory views. 
The adventure works db I tested when I reported this issue to MS (I found this bug) is 67 entities big. AW does have some dense subgraphs (some entities have more than 2 associations) but to me it's not a big model at all. I've seen models with 2000 entities (albeit not using EF ;))
&gt; But for paged lists of 10-20 records like you'd see in a typical CRUD app, it's not going to be a problem. That's not necessarily true. Fetching 10000 times 20 entities is still 200000 entities fetched, which is as slow or slower than fetching 200000 in one go. In a situation when there's plenty of memory and cpu power left on the server, it's not a big deal, but it never is in that case. Things go wrong when your site is under high load. Then every performance drop counts. If you're pissing away the performance because you use a slow sub-system, overall your site is then performing sub-optimal, which could mean your site will go belly up.
If you're gonna cover web.config, make sure you cover; Splitting it up into different files, using the configSource-attribute, it will help you organize you configuration files into smaller pieces, increasing readability. There's alseo the file-attribute, that is similiar to configSource, but differs in that it enables you to merge configuration, where configSource is a single-source of config. You'll also need to cover the classes ConfigurationSection and ConfigurationElement, since that is essentially what you're working with when creating custom configuration sections. It's not pretty though, and you might be better off considering another approach to configuring your applications.
It is not that complex even if you go deep -- but in reality custom sections etc are not needed that often. Aside from the obvious, here are two more things I think are important: * hierarchy of configs (machine config, web configs in subfolders) * binding redirects And some conceptual things: * alternatives and when it makes sense to use web.config * how to manage environment and deployments (e.g. transforms and whether you use them or something custom) * how do you unit-test classes that depend on configuration
What I found very useful with web.config files was to use [transformations](http://msdn.microsoft.com/en-us/library/dd465326%28v=vs.110%29.aspx) for different environments. Doing that by hand can be a complicated task, so maybe you want to look into [SlowCheetah](http://visualstudiogallery.msdn.microsoft.com/69023d00-a4f9-4a34-a6cd-7e854ba318b5). The teams I've worked with all found that really useful. A good example of where you'd use transformations is connection strings. You'd add transformation so when you deploy to test, the test connection is transformed into the web.config
We have a consultant developer who writes the code. If and when there are performance issues it is my job to figure it out. Yay!
But AsNoTracking prevents the whole issue and is created for situations like these where you don't care about associations. The extra time "wasted" is presumably spent setting up change tracking/fixup for those associations. Basically, even if you don't load the navigation properties, the context needs to be set up in case it needs to fixup new items. If you don't want that, No Tracking is there for that kind of scenario. I'm not saying "who cares about the wasted time", I'm saying most of the time you're going to want the functionality that comes with those associations, so it's not actually wasted. That's probably the reason you see ~17 votes on that ticket and not 17,000. I have a feeling this may not be a bug per se, but more aggressive fixup that is causing the difference in load times between versions.
I would touch on [customerrors](http://msdn.microsoft.com/en-us/library/h0hfz6fc\(v=vs.85\).aspx), and how you can use it to redirect based on HTML status codes. Helpful if you don't want to have to configure IIS with your custom pages if you have to delete files when deploying code.
&gt; But AsNoTracking prevents the whole issue and is created for situations like these where you don't care about associations. The extra time "wasted" is presumably spent setting up change tracking/fixup for those associations. AsNoTracking means the entity object doesn't do nor can be used for change tracking scenario's, so is effectively a readonly object. So that's something else than what you say :) I.o.w.: if you want to alter the values of the entity and save it later, don't use asnotracking.
Rephrasing for clarity: AsNoTracking prevents the whole issue and is created for situations like these where you don't care about associations. **If you don't use AsNoTracking**, the extra time "wasted" is presumably spent setting up change tracking/fixup for those associations. So if you're in an edit context, you (implicitly or explicitly) want those associations to be tracked, and you incur the fixup hit as a result. If you're in a view context, use AsNoTracking to prevent the fixup. It almost sounds like your complaint is that tracked entities are slower than untracked ones, which they obviously always will be. 
Interesting, thanks for the info.
&gt; What's a brown-bag? A short technical learning lesson given by one of the team over the lunch break where people bring their own lunches in a "brown bag".
I've used it here and there and it's worked well for the simple things I've done. What shortcomings are you dealing with?
Try llblgen and maybe you'll change your mind. At least in EF the queries are readable. Or maybe that's just the way they work with llblgen at my current project.
I don't really like EF much either, though we have an uneasy truce and I will use it when maintaining projects that already do. If left to my own devices, I will opt for a micro ORM such as ServiceStack.OrmLite for CRUD plus stored procedures for heavy lifting.
&gt; simple things That would probably be why your experience with it has been pleasant. If you have complex entities and mutate state, *bring your aspirins*. I've lost days/weeks trying to wrap my head around the impressively opaque change tracker and why it decides that it can't update shit I didn't realize I had even touched. EF code first is so very nice to get a project running quickly-ish with, so nightmarish to get it to stop hammering data unexpectedly.
I've been considering dapper lately.
EF is getting better but it's actually better to mix and match multiple ORMs, depending on the use case/performance requirements for each. In certain cases, it makes sense to use a micro ORM and in others, raw SQL is a better option. And a framework like EF is better for other use cases. 
I started with Database-first. It was extremely productive, and the basic CRUD stuff was done in less than an hour. However, it started getting painful when I had to change the models. Auto-generated code only syncs one way, and it's just a mess to play with. A good practice is to use extension classes over DTOs. That way, the DTO code remains untouched (and can be overwritten by EF when the database change), and your logic exists in the extension classes. It's not perfect, but it worked well. You can even configure EF to use existing stored procedures. Code first is fun to use. You create your classes, and the database is generated dynamically from them. It can be painful at first to keep updating the generated database based on the models, but I believe the compromise is worth it. You don't actually have to think about your database, and simply think about your code. Persistance is done for you. But you lose some control. I think I prefer Code-first, but that's highly subjective. I don't have enough EF experience to make objective claims about which one is better. I guess "it depends".
That's exactly why I prefer Code-first.
I use EF in my Repository patterns, and love it! Are you using repos?
I dropped EF for this project and did something a bit different. I didn't use an ORM but I used F# instead for my data layer. F# has something called type providers. Basically at compile time they generate types for existing data sources. So there's a SQL type provider which takes in your connection string and generates types for your database at compile time. You don't write any code. I found it to be faster than most of the micro ORMs. I'll admit you don't get features like caching but for me it's been brilliant. More readable code and as long as your project structure is alright, it's easy to slot in with an extra DLL. It's as simple as type DBSchema = SqlDataConnection&lt;"conn string"&gt; let DB = DBSchema.GetContext() Then you can do stuff like query { for row in DB.Users select row } This is all assuming that your DB already exists
MicroORMs all the way!
yes, this! if only for "Locate in TFS"
I agree. EF is, for me, a great example of many Microsoft undertakings. Late to the party and not as good as the alternatives. EF has always felt overly complicated to me. I still like (Fluent) NHibernate. It's really not difficult to use, regardless of how bad the documentation can be. If I need something more lightweight, I've had good luck with Dapper and Massive. Just my opinion, EF advocates can power down their flame throwers.
What do you consider slow? Are you having issues with the initial execution (context does take a few seconds to materialize) or the several milliseconds overhead per call? Give me an example of something simple that is hard to implement? I'll admit I do not use domain driven design (I know it's all the rage) so my entities rarely contain business logic, I have manager classes that perform business logic. I understand that if you have a requirement of accessing the context from within the entities it could get hairy. I really don't see how EF is that much different or more complicated than other ORMs. I've always used database first, I prefer building my databases old school in SQL.
I've switched exclusively to dapper. 
We dropped EF in a small project that was connecting to a small database and saw several orders of magnitude performance increase, and we no longer had to code around EFs oddities in several places. I will never work with EF again if I can help it.
Yea i dumped EF years ago, switched over to Linq and NEVER looked back...
To be fair, people often stick to what they are comfortable with. If you've used other ORMs (NH) before you probably not going to want to jump ship let alone embrace it. EF never felt overly complicated to me, not after I read a book on it and learned how to it actually works.
Ah, ok thanks for that. So to clarify, does thus impact code first as well?
I've had to make deep clones of objects and null out navigation properties to keep things from going haywire. Mutating state of numbers of objects had led to plenty of pain. 
I've used a variety of data access technologies. From stored procs and my own data layer, to code generated techniques, linq to sql, nhibernate, micro-orms, etc. I just find NHibernate to be the tool that fits the bill most of the time. I've also found the way EF does things to be clunky and unintuitive. I've read up on it, I've done projects with it and I just can't stand using it. Some people like pepsi, some people like coke. 
Certainly. I don't have much experience with NH but when I did use it I remember having a hell of a time creating the XML mapping files. With EF all I had to do is point it to my existing database and let the magic happen. I am sure there are tools for NH that probably do the same thing. Anyway it's all good, that's why there are options.
I favor the stored procedure route. Separation of concerns and compiled query execution plans. In a large company, it let's your DBAs control accesses to the database and your developers write code to consume the data. They can also do a much better job managing performance. Also, you can count on stored procs being there if it's a Microsoft/SQL shop. Learning frameworks really only help you if the company you are at uses that framework. Kinda like why Linux folks use 'vi'....you can count on it being there. LINQ is great for small projects where you own the entire stack. But for a large distributed system, I'd almost always go with stored procs. But Im with you on EF. More trouble than benefit.
If you don't mind me asking, what is your issue with EF exactly? Earlier versions were less flexible and suffered from performance issues, but EF 6 is a big improvement. You can choose several methods of implementation: DB first, model first, code first, or code second (code first on top of an existing schema). You can use EF's built-in conventions or define your own, or you can configure your entities independently. In addition, you can do all of your entity config using the fluent API so that your domain objects are truly persistence ignorant. There is a lot more as well, but I don't want to create a text wall. Of course, no ORM is perfect, so there are definitely areas where EF can be a pain. However, you can easily run ad hoc queries that return strongly typed result sets to get around most of those issues. Finally, in the best interests of all who may read this, I am a co-author of Apress's Entity Framework 6 Recipes. However, I do not work for either Apress or Microsoft, so I am not a shill. 
Sorry for the late reply. I'm lucky enough (or do I'm led to believe) to be mostly doing greenfield projects in Code First. Mostly stand-alone LOB web apps in MVC. Trying to do the 'right thing' - layered architecture etc. I used to be more of a 'build your data model first' developer, but after reading more Larman, Wirfs-Brock, Fowler, Evans etc, I moved towards a more domain driven approach. I'm no dogmatist though. I used to care about my ER diagrams, but then was doing probably too much validation through constraints and triggers and too much logic in the SPs. At first, Code First EF seemed like a revolution - just objects! No more DB concerns! But, the more 'business logic' you build into the domain model through the behaviours and responsibilities of your classes, the more pain you experience. While in-memory unit tests of the model work fine, persisting to the DB through EF is increasingly time consuming. I probably spend more time in SSMS trying to work out what EF has done than VS. More specifically, but in no particular order: * You have to constantly balance the huge performance hit of reconstituting more than a simple graph, versus wanting to bring a REASONABLE amount of the domain model back into memory to do work where it should be. * You end up doing less objectOne.Equals(objectTwo), and more objectOne.primaryKey == objectTwo.primaryKey. * Composite keys are a pain. The lure of pointless surrogate keys is built into the convention. * The way state management works verges on random. Saving Disconnected objects do unexpected things to other objects. Updating many to many tables is a convoluted nightmare. * Annotations riddle your classes with DB concerns, fluent API becomes a big un-manageable list of unintuitive configs. * The impedance mismatch is surely no justification for the lunatic queries generated sometimes. * Classes littered with public auto properties (with a private set at least) * empty parameterless constructors We have other teams in-house and they are doing WPF stuff and finding similar issues. 
Thanks for this - very interesting. Maybe I'll just bite the bullet and build the DB the old fashioned way and refactor it as I go along ambler style. 
Hi, as my reply above, I mainly work in code first. That said, we only have legacy EF5 and 4.3 projects right now (partners in France are on older systems). Is EF6 less infuriating to use as well as faster?
How do you handle mapping then? In my experience those who don't use an ORM either reinvent one or end up having very procedural style code. Stored Procs and ORMs are not solving the same problem - not at all. You also get a few big downsides going the stored proc route, such as: - Losing version control, which could make deployment, code reviews, and rollbacks difficult. - There are things that are naturally done better in code (complicated search queries for example), so you still have some sql that has to reside in your code. Compiled query execution plans are not unique to sprocs - these come from prepared statements as well. Separation of concerns is easy to attain with some discipline. Using the Repository pattern achieves the same separation, except you have better code support, queries are easier to debug, and you get version control. 
FYI the drag gesture behavior on your site makes it unreadable on a mobile device. If the font is too small to read, but zooming in requires scrolling, which is disabled because it is interpreted as "go to next page."
SqlDataReader 4 life
FWIW - I agree, the xml mappings are a pain in the ass. That's why I use Fluent NHibernate. I can map with C# code in a way that makes sense. If I'm starting from scratch, or if my existing database follows a set of conventions, I can define those conventions (such as PK is called Id or Table Name + Id) and the Fluent NHibernate automapper will take care of it for me. There are some things you can do with the hbm files that you can't do with Fluent, but in those rare cases, you can use multiple map types. There is definitely a learning curve and unfortunately a lot of blog posts make it harder than it needs to be.
Don't switch to NHibernate! All these ORMs try to make you feel comfortable and hide the SQL from you. That's all very nice, until it's too slow and you find yourself writing SQL through the ORM's query generator. Go with a micro-ORM next time.
Attach a screen shot of the errors. If that doesn't work you will need create a VM w/ win7 installed on it.
Damnit if MS doesn't force me to re-invent the data layer every 2 years. If DAO/RDO was good enough for me then it is good enough for you whipper-snappers!
Petapoco or linq2db
Windows 8 should come with .Net 4.5 installed, and if you have 4.5 you can run .Net 4.0 apps. To run .Net 2.0 apps, you should install .Net 3.5. You can do this by going to Control Panel, then Programs, then "Turn Windows Features On or Off". There should be a box for 4.5 that is already checked, and a box for 3.5 that you can check to install it.
If all else fails, you could install an older windows on a virtual machine. I find virtualbox works. Check support for those products first.
yep
Then why not move to something which works better? There are more ORMs on the planet than just EF. I see you use POCO classes, then your set is a bit limited though. 
Micro-ORMs force you to do a lot of work when persisting data, as there's no change tracking, no graph sorting and therefore no insert/update query generation from that. They can be used to replace the used ORM to overcome slow fetch queries of course.
Try the following: Go to Control Panel &gt; Programs and Features &gt; Turn Windows features on or off and pick the older .NET versions. A reboot is required.
code first is actually odd. The thing is that O/R mapping is about an abstract entity definition which is realized in both a class and a table/view in such a way that there is a mapping definable between the two so instances of the abstract entity definition (the data!) can flow between instances of the two definitions: from a table row to an entity class instance and back. The work needed to perform that flow of entity instances is done by an o/r mapper. Starting with code is equally odd as starting with a table: they both require reverse engineering to the abstract entity definition to create the element 'on the other side' (so reverse engineer the class to the definition to create a table and the mappings is equal to reverse engineering a table to a class and create a mapping.) I know the whole idea comes from the fact developers want to write code and think in code and want to persist objects to the database, but that's not what happens: you don't persist objects to a database, you persist their contents, which are the entity instances. They can be stored in other objects, and it would mean the same entity instance. In theory the best way to look at this is to start with an abstract entity model and create _both_ sides from that, both code and tables/views. A change in that model (which reflects the entities and the associations in the domain) then correctly ripples through to classes and tables and thus the mappings. It's then rather odd they completely abandoned the model first approach which is close to this, and went all-in with code first, which is IMHO a bit of a silly thing to do, as not everyone works code first, nor does everyone think they're persisting .net objects to a database. Another negative side effect about code first is that it's code. It is readable and changeable easily but to get an overview what's going on, a text editor isn't sufficient anymore, tooling is needed to get proper overview how the model looks like, what the associations are, which inheritance relationships are present and over which fields (wrt entities). So it works with a small model, but if you have to work with a model with 50 or 100+ entities, it's not going to be easy at all. Code first also hides the other side of the picture, or better: a construct in code-first might have devastating results for the other side. E.g. some people have the urge to create a common base class / entity for all their entities in which they define fields like 'modifiedby', 'createdon' etc. so all entities will have these fields. Though inheritance in memory with objects is cheap, it's expensive in a relational database, as with every query, joins will be added for all super/subtypes, if the inheritance is in the form of Target per entity. Starting from the DB of course also hides the other side from the picture, however choices in the DB don't have these kind of effects in code, most of the time. Changing a 1:n relationship between A and B to a 1:1 relationship should take little effort. In code first (and db first) this is more work than you might think. In a model, it's as easy as changing the relationship type. All changes required on both sides follow from that and are produced by the system used to create the model. 
What would you recommend?
We usually include the proc definitions within the source code tree (when we write them). In a previous company, the developers didnt even write them - they requested them from the DBAs. We would ask them for a stored proc with the following inputs, and outputs. That gave them complete control over the inner workings of the stored procedure. Basically, we let the DBAs manage all aspects of the SQL server, and the coders would focus on the application. From a management perspective, it was easier to find resources that understood ADO.NET than to find folks who know this ORM or that one. But you do raise some good points. From my experience though, overall, keeping the design simple and keeping employees focused on what they do best really works well.
Non poco: llblgen pro (can also do ef/nh, which solves your code first issues) (I wrote it, so I'm biased towards this of course) Poco: openaccess (from telerik), lightspeed (from mindscape) 
Thank you. I will trial llblgen pro and post my feedback.
if you have any questions, PM me. 
F# type provider just uses linq to SQL under the bonnet AFAIK.
How do you unit test your stored procedures?
Actually if you use no tracking in EF then its not that much slower than Dapper. Its the change tracking stuff that slows things down a lot.
Yeah it does. I find its a lot nicer to use than straight linq to sql though but that's just my preference.
Why does anyone think code first is good for anything beyond a simple tutorial application?
To your points: - don't use lazy loading. - objectOne.Equals(objectTwo) and objectOne.primaryKey == objectTwo.primaryKey have different meanings. Modified objects with the same primary key are certainly not equal. - don't use composite keys. You can always add a simple key which encompasses both. Composite keys are like segmented memory in a flat memory model world. Why do I need two IDs to address an individual row in some tables, but not others? It's just bad design. Add a third ID which encompasses both, and make that the PK. Composite keys *should* be a pain. - I've never had a problem with state management, but the context is managed using a [repository pattern](http://msdn.microsoft.com/en-us/library/ff649690.aspx). - I won't argue with the generated output (it's gross, but it works!) &gt;I used to be more of a 'build your data model first' developer, but after reading more Larman, Wirfs-Brock, Fowler, Evans etc, I moved towards a more domain driven approach. I'm no dogmatist though. They are all wrong. Code first is wrong. Build your data model. EF is fine if you use what works and skip what doesn't. I went through the same thing about 8 years ago before coming back to the data model first approach. Those people write more books than code, IMO.
&gt;They are all wrong I have a hard time taking people who are dealing in absolutes seriously. You are not even giving any reasoning as to why.
XML and serialization? Let me know how it goes. Also: despise. 
Ok, they are not entirely all wrong. Domain driven design is the right idea. The restrictions of a domain in a domain driven approach are best described using a domain specific language. Your programming language of choice isn't it. It's no doubt a general purpose language, describing a general domain. SQL better models relationships. This is why there is such a disconnect between programming languages and databases when using ORMs. They describe different things. It is a better language, more specific to the domain of describing business entity relationships, than a general purpose programming language is. It is the better tool for the job when coming up with a design. You then design your software around the model, not the other way around. C, C++, C# , Java, etc are all good at describing structures, recursion, algorithms, but not so good at describing one-to-many optional relationship with check constraint. In SQL it's just a table description, in the other languages you have to write code to make that happen. It's very verbose and not very helpful at seeing the big picture, obscuring the design process.
I'm personally favorable to stored procedures, but F#'s SQL type provider has the flexibility and expressive power you're looking for.
http://imgur.com/VcBktjC
tl/dr: If you do not know how a database works, you will experience some pain. But that's true even if you do not use an ORM at all.
You might want to look seriously at LLBLgen Pro then. The prefetch feature would likely do you good, assuming you know in advance which parts of the graph you need. It can be complex to build a proper prefetch for a complex graph, but it will then load and reconstitute that graph with a minimum number of queries, and so just about as fast as it can be done. EF's .Include() works quite differently and is totally unsuitable for exactly the cases that LLBL's prefetch shines. In fact for a complex object graph, I would hand-code it the way LLBL works. .Include() is only useful for very simple object graphs without descending into cartesian product hell. Of course, another option is to store the graph rather than deconstruct/reconstruct it using a non-SQL database.
I should have said multiple result sets. If you create a stored procedure and then use EF to generate code from that SP it can't.
So 6.0.2 was just release which is supposed to fix some of these issues - can you give us an update when you get a chance?
Is this now resolved with 6.0.2?
.Net is not restricted to just Windows, you already mentioned mono above. There a ECMA standard for the CLR. http://msdn.microsoft.com/en-us/vstudio/aa569283.aspx Also the F# compiler is completely open source. I think Java has more of the traction it does because it's a bit older, larger community and better support on Linux. I think VS is a superior dev environment to pretty much all other IDEs. If you are a VI/Emacs person that probably doesn't matter as much. But it's also somewhat a crutch because the tools that are not Visual Studio for developing .Net apps are not as good and it's difficult to do with just a text editor. EDIT: added bits about ide Disclaimer I've been a .Net dev for about 10 years and I now work for MS.
While .Net is not restricted to just Windows, the direction that C# and .Net takes is definitely controlled by Microsoft. Off topic, I'm a die hard Vim coder, I cannot code or even edit text without Vim commands. That said, I worked on one .net project in VS2012 using the Vim plugin and it wasn't too terrible except when I had to do anything that wasn't coding, which seemed to be half of what you do in VS. Really though, the only thing I liked about it was C#, I think it's a fantastic language as well as F#. It also seems to be impossible to get projects to work with newer versions as MS comes out with them. As long as it's not impossible to create server projects or even desktop projects with just command line tools then I don't think that's a downside. I'm not sure if you've ever tried to run Clojure on .Net, but it's definitely a great language and if it isn't too hard to port applications between the JVM and .Net then I may do that, but I still would like to know more of the major pains of both platforms. Thanks for the comment!!
- the jvm is faster. Not orders of magnitude faster, but it's definitely faster. - but the jvm needs more ram. - the tools for java are a better IMHO, but both are excellent - there are way more high quality open source libraries and frameworks for the JVM - The .net standard library feels more straightforward, with less byzantine inheritance hierarchies. - .net's P/Invoke (interop with c code) works great. I have no experience with JNI, but I haven't heard nice words about it. 
Neither ECMA standards (for C#, CLR etc.) nor Mono change the fact that they only cover portions of .Net. There are large numbers of libraries/tools in the .Net platform and ecosystem that *only* work on windows. I'm highly dubious about claims of .Net being "cross platform" when: - Mono does not (and probably never will) offer 100% compatibility with the MS .Net implementation - Microsoft has no economic interest in making .Net actually work cross platform. They want to sell windows servers / desktops with .Net. Sure they don't mind people contributing open source libraries for free and doing hobby work in Mono, but the last thing they want is a serious commercial .Net alternative on Linux. - Microsoft has complete control of .Net, in terms of the evolution of the platform, and probably in the form of patents / other legal means that they could use at any time against someone competing with them via a .Net implementation. The cynic in me observes that MS likes the positive PR of claiming that .Net is cross platform, but then perpetuates just enough incompatibilities and pain points so that any business trying out Mono on Linux will at some point come to the conclusion that it is just easier to switch to Windows. 
I believe it's easier and more attractive to implement compilers for new languages on the JVM. New languages seem to be much more vibrant on the JVM than the CLR (see: Clojure, Scala, JRuby, Groovy etc..) I think this is for a few reasons: - From a technical perspective, CLR has a much more complex type system in the runtime itself (in order to support C# reified generics, among other things). This is harder for language implementers to support, and restricts you to designing your object model around the C# view of the world. By being less sophisticated in the regard, I think the JVM is actually more flexible (Scala's type system for example is a thing of crazy beauty...) - Langauge designers probably prefer the cross platform, open source ecosystem on the JVM. This makes it easier to build up the core libraries and you can run on your favourite platform (be it Windows, Mac, Linux etc.) - Businesses investing in alternative languages understandably don't want to be dependent on Microsoft : they haven't exactly proved to be a trustworthy partner in the past. The more open JVM ecosystem (especially since the OpenJDK) is much more attractive. This is an interesting thread with more insights on the topic: https://groups.google.com/forum/#!topic/clojure/DnbOstrvhzU
&gt; the jvm is faster. Not orders of magnitude faster, but it's definitely faster. [Here is a counter example where .NET runs 17x faster than Java](http://fsharpnews.blogspot.co.uk/2010/05/java-vs-f.html). &gt; .net's P/Invoke (interop with c code) works great. I have no experience with JNI, but I haven't heard nice words about it. I've heard that PInvoke on .NET is also an order of magnitude faster than JNI on the JVM. 
I'm curious, if you are a die-hard VIMmer, why bother with VS at all? You can build code for any .NET language from the command line - you don't *need* VS.
I'm not sure what tools you are referring to...I use visual studio and eclipse at work and VS2013 is worlds better for every project type that I work with.
&gt; Porting applications between platforms is cumbersome as compared to the JVM (You can't just run a .NET app on Linux using Mono and expect it to work, it almost always won't) This isn't true anymore (except UI driven apps perhaps which use control libraries which do win32/gdi stuff under the hood). It's often the case that you can grab a compiled .net assembly and use it on mono without recompiling anything.
Tooling : I'm thinking about Package Management, Continuous Integration and stuff like that. But I agree that Visual Studio beats everything that's available for JVM stuff. 
Yea, the fact that the JVM doesn't have value types really shows in that benchmark. But for more complex stuff the JVM usually beats .net.
Configure a logging provider and NHibernate will show you all of the SQL it generates along with the parameters it is using. You can cut and paste it directly into Query Analyzer and see what's going on, see if you need indexes, etc. 
I'm kind of surprised that the "persisted object graph" crowd hasn't mentioned any of the NoSql options yet. While still immature and completely awful from a tooling perspective, RavenDB works really great when all I want to do is store a persisted object graph and get it back quickly. 
Any chance the parameters being sent to the query are returning a result set much larger than your test? When you were in management studio, did you have it show you the query plan? Did it recommend any indexes? 
That is what I assumed, and I think it would make an interesting blog post as well. I honestly can't stand the VS tools, thanks for the info!!
http://www.asp.net/mvc/tutorials/mvc-music-store/mvc-music-store-part-1 Lots of great tutorials on the official site 
There are some counterpoints to offer.. &gt; restricts you to designing your object model around the C# view of the world. F# has a very nice type system. IronRuby isn't so bad either. &gt; don't want to be dependent on Microsoft The same can be said about Oracle.
Microsoft has partnered with Xamarin lately, so I think Mono is "good enough" at this point wrt legal constraints.
&gt; Yea, the fact that the JVM doesn't have stack allocated types really shows in that benchmark. Value types and reified generics kick ass. &gt; But for more complex stuff the JVM usually beats .net. Can you cite some benchmarks to back that up? I've not seen anything that made me think that... 
I have previously used csc.exe to compile single files (proxy for a web service). How would one go about compiling a complete ASP.MVC 4 application this way? 
I've never tried it, but [someone outlined the process as an answer to this stackoverflow question](http://stackoverflow.com/questions/2451138/how-can-i-implement-a-site-with-asp-net-mvc-without-using-visual-studio).
Don't be masochist. Even MonoDevelop sucks.
&gt; Is EF6 less infuriating to use as well as faster? Less infuriating? Yes. Faster - it depends. There are some issues currently where depending on the size/shape of your model, perf is pretty terribad. 
RavenDB is great, but it comes with it's own headaches.
Yup. If you have visual studio just use if! It's hands down the worlds best IDE
Face it. If you're on .NET, you're a Visual Studio developer. Why do you think Xamarin works hard on integration with VS? Because XS is a distant second. 
There's [SharpDevelop](http://www.icsharpcode.net/OpenSource/SD/Default.aspx) and [Compilr](https://compilr.com/). Compilr is an online IDE though. I haven't used either though, so I can't speak for their quality. I haven't used anything else because.. why? Even the Express versions of VS are better than anything else out there. It is in MS's best interest to keep VS good so developers continue to use their platform, so it should stay good for a while.
A few years ago I worked at a firm that developed using Delphi.NET (a .NET lanuguage with Delphi syntax). The IDE that we used, Delphi 8, was slow and, most importantly, terribly buggy and would crash regulary, loosing work that you had done. Must of my collegues just accepted it, but I decided to find a solutution around this. After spending much time trying to figure out why the IDE crashed, I ended up compiling from the command line and using Notepad++ as the editor. This was a better solution than using Delphi 8, but I did miss out on common features of any IDE (e.g. realtime compiler error and warnings). The whole experience taught me more about MS build process and the .NET framework (knowing which class is in which dll), but I did exhaust more effort initially trying to set up my environment. Overall, I do think this made me more knowledgeable about .NET. I would recomment this if you want to learn more about what an IDE does for you.
I use vb for Android as an alternative for making Android apps its nice to use a stripped back ide for a change 
For a few years I used MacVim for all my coding/typing, and had a VM with Windows 7 + Visual Studio on it. I then had folder/file monitors setup in the VM to kick off automated builds as files changed, and I'd go to VS to get the latest errors/warnings. Any live debugging I'd do I'd do straight from Visual Studio. This solution wasn't perfect, but I felt more focused and felt my memory improve when I couldn't rely on autocomplete. It was an experiment that I was pretty happy with. I got to use the command line in Mac OSX for git, grep, etc, and the pop into Windows to debug or run the website.
Have you used Visual Studio?
For the sake of curiosity, why do you want to do this? If you just want to learn to build from the command line, you can still do this and write code in VS. I do this for our build system, I have a series of msbuild scripts (or whatever you call them) that handle the building, config transformations, etc for our various environments. I find that very useful, but I can't imagine why I'd want to code in just a text editor, so I'm curious as to your reasons - maybe you've got a better imagination than I do! 
Shouldn't you know ahead of time if your collection is read only?
You built a repository pattern on top of a repository pattern.
&gt; Both really - but particularly slow on reconstituting big graphs. What do you mean by big? Your not using it for reporting or anything are you? 
wonder if they're following Windows' numbering now
Which mobile device? On my phone (samsung galaxy) the font size is plenty big enough and it doesn't even allow zooming. It's a standard blogspot skin.
There is no motive other than learning how to do it. It's just a quest for knowledge.
This is exactly the answer I am looking for. I feel there is a lot of stuff hidden away from me when I use VS. For everyday work that is fine since it probably boosts ny productivity. But I feel that there is more to it, and I want to know the details.
I will look into it. Might write a script to template my solution aswell. 
MonoDevelop still isn't all to bad. It beats Eclipse by at least two orders of magnitude in my book.
Sorry to say it but this does not sound very productive. Did you see a big difference in productivity?
Could it be worse? Yes. Can it compete with VS? Not even close.
Perhaps different deployments/user context would have an effect? Probably best to know those conditions ahead of time, and if you can't, you probably want to throw an exception up the call stack that you can handle gracefully...
http://stackoverflow.com/questions/125319/should-usings-be-inside-or-outside-the-namespace
You missed an important part: debugging. You can use sublime or notepad++ to edit, compile on the command line with csc or even your own sln/csproj files with msbuild, the things is: if you want to debug your application, you're out of luck. You might think you're good, but no-one is that good that s/he never needs a debugger. The debugger is what makes/breaks a development environment. Any editor will do and any make/compile set up is sufficient after a while, the thing that lacks is the debugger. The debugger experience in vs.net is excellent, so it's hard to get rid of that. If you want to get rid of vs.net reliance, I fear you have to move to another language/platform.
Composite keys ensure you don't have duplicate rows for the same relationship between two objects. Adding a surrogate primary key is just overhead. You still need the FKs and the unique index to eliminate duplicates. *** And no, your proposal doesn't make it so that you only need one key to refer to the row. Instead you now need three.
This made me laugh.
It's a matter of education not ditching VS. I love VS but feel that I should experience the platform from a different light. Edit: I am fully aware of the lack of a debugger. I want to see how much I would miss working without it.
If you insist on developing C# without visual studio, then I would have to recommend [ScriptCS](http://scriptcs.net/). It uses a Microsoft compiler, but offers you the ability to develop in something like Sublime. [Here is a video to give you the basics.](http://www.youtube.com/watch?v=q2hFK8oK_bQ) I can't say that it will work specifically for asp.net development. I can say that there are example of it being used to develop winforms and wpf apps. While, that isn't the same I would be surprised if asp.net support is available.
Cool, just curious. I hope you are successful in your quest!
Inexperience.
Can some one explain to me how much time you actually save automating freaking mapping code? Given the lack of control, I'll stick with stored procs and a basic repository pattern.
&gt; And don't compare it with female genital mutilation, you'll always lose the argument when you do. i'm on an iphone, ios 6.
YES! I've been in my good old text editor since the '80s. Using a macro to run the compile/link steps, and a separate macro to run the executable. Where needed I've employed hand-coded batch files to help things along. I'm a command-line person. I've developed this way in Pascal / C / C++ / assembler / and for a long while in perl / html / js / sql / css. Lately I've embarked on the dotnet experience and am still in the learning phase. So this may change. But for now I intend to be writing only web handlers rather than full-blown apps. I turn off syntax highlighting and prefer to memorize a few core APIs rather than rely on context-sensitive help. Those things get distracting to me and slow me down. Preferring command-line tools I use git for version control. I can't stand it when something writes code for me, so to extend it I have to then dissect another programmer's style and patterns. Or there's a bug and I have to re-write it anyway. Why do I do it? Because the amount of fine-grained control it allows. And the incredible "nuts and bolts" insight into how things work lets me quickly fix problems. I also work in I.T. support so knowing the nuts and bolts gives me insight into how the computer runs as a whole. Those two disciplines are very complementary. The 'corporate' developers (I work in a satellite location) consult with me when they encounter a problem outside their silo of understanding or influence. I tend to also not use a lot of canned libraries either, preferring to write my own for specific purposes. I get the features I need this way, as well as more stability. I've run into brick walls (lack of features or outright bugs) many times using third-party libraries. So I 'roll my own' a.la. the early days of Winamp. Using any IDE after understanding the nuts and bolts gets a lot more intuitive. Is it painful? Yes!!! Especially in the learning phase. Though once I get the 'environment' settled in things work well. Will I never use VS? I can't say that. It does afford a level of productivity I can't match. But I don't work in a high, volume shop.
Wow this really got me hooked. In particular the super simple web.api implementation. I am going to explore this further. Thanks again!
Very informative. Do I understand it correctly if you build different batch scripts for each project?
To be more precise, I've built a repository *abstraction* over a repository pattern. This way, I can change it whenever I like.
I saw a demo of this a few months back at Microsoft. It looks like a pretty awesome project. While I don't really have a use for it, there are certainly a lot of cool things you can do with it.
I am a linux user in the non proffesional IT-world. So the fact that there is work going on for a mono port is fenomenal. I was blow away by the web.api demo. I can really se myself using that. Write up a library that handles database IO in VS with a test suite. And reference it from the web.api. that way I can script my end point and use a rigorous IDE to do the heavy lifting. I have to check out the async/await progress though, love that sugar..
And how many times have you done that?
All de time! YOU DON'T KNOW ME! ok, but for srs, if you must know I have a custom template (like, a project already setup with DI, all the NugetPackages I use for every project, membership/role providers etc) so oftentimes I do swap out for different POCOs, and then it works ALMOST AUTOMAGICLY!!!! (yay!) Plus (and probably more importantly), without doing this, it becomes really, really hard to mock for testing (this is essentially 'change it whenever i like' [Even at runtime!]). GET TOLD SON! ;) ^^it ^^ok, ^^im ^^just ^^playing ^^^^don't ^^^^get ^^^^mad
No news here actually: we're releasing 1 major upgrade a year + several minors
We do but not for all kinds of licenses. Commercial licenses can still be purchased on a per-major-version basis, with free upgrades only to the current major version.
Haha, no that's just a coincidence. We're sure to come up with v9 before MS does.
Are you a dev?
Try Notepad++ with CS-Script plug-in: https://csscriptnpp.codeplex.com/
Windows Forms works well though, it just looks out of place.
&gt; when I had to do anything that wasn't coding I hear people say this, but with the exception of Windows Forms and Model-First entity framework, everything in VS is done through the code editor. In my daily work i NEVER touch stuff that isn't code in VS so I don't understand where you are coming from. It mostly seems like a complaint from people that don't use VS normally and think it's like Delphi, VB6 or DreamWeaver.
If you are starting from complete scratch, I would recommend starting with the [C# Fundamentals](http://pluralsight.com/training/Courses/TableOfContents/csharp-fundamentals) course on Pluralsight. The website is $29 a month, so two months will put you back about the same amount that one of those fat .NET books cost. Once you finish that course, you can move on to one of the many other .NET tutorials they offer. They also have an app so you can watch videos on your phone/ipad if you need something portable. 
I second this because books are often bloated with extra words (some writers gets payed by word count) and don't teach best practices in my experience 
We used this book at my school: Beginning ASP.NET 4.5: In C# and VB by Imar Spaanjaars Not only will this expose you to the main programming languages, but it will also show their application in ASP.NET.
[Try Head First C#](http://www.amazon.com/Head-First-C-Jennifer-Greene/dp/1449343503/ref=sr_1_1?ie=UTF8&amp;qid=1387296069&amp;sr=8-1&amp;keywords=head+first+c%23) These books look huge, but there are tons of diagrams and pictures, end of chapter quizzes, stuff like that. The way they're written, the writer sets up a problem and then uses C# to fix that problem. You feel like you're actually accomplishing something instead of just trying to jam facts and syntax into your head. ...that being said, the Pluralsight stuff it really good too.
check nopcommerce. Serious software, (very) serious devs, huge userbase (developers), even huger customerbase, a shitload of code, extensibility : mvc and code first. My own experience with code first is it's amazing and is becoming mature. I only blame EF on the inheritance issues, which are very annoying.
After trying LinqToSql, EntityFrameWork, NHibernate I've settled on Dapper.net. It's a micro orm that's faster and simpler to use. The bigger ORMs always felt like a really complicated way to make things easier, if that makes sense. I found myself jumping through hoops to do relatively simple things, going back and changing a dbml or object mapping always felt like way more work than it should be. Dapper is just a nice object wrapper around straight SQL / stored proc calls.
May I ask why you're learning VB.NET specifically? C# is widely used now so if there is no special requirement, I suggest you learn C#.
I almost wonder if starting on 4.5 is too much abstraction and people are being exposed to language features with no clue on the inner workings of said features. It's all logical to me at this point as I've been doing it since 1.1 but I can't imagine starting into 10 years of accumulated language features without the appreciation of why these features exist and what they translate to at a lower level.
Sorry if you're using code first, then you are inherently not using the full power of your database engine for developer convenience. That's the compromise.
Last couple of times I uses ORMs it was to bulk loading test data into a complex set of tables. But I'm hesitant to do even that again because the performance was so horrible. By default each time you add new object to the collection it requires an O(N) or maybe even O(N^2) operation in EF. So you have to find the magic go-fast setting or drain the collection (i.e. hit the database with your inserts) after a hundred rows or so.
If you use ORMs correctly instead of just blindly calling SELECT * on every table then you lose all those nice things like change tracking as well.
ok sorry, mister masm32.exe
The differences are extremely noticeable and well documented and not on the same level of writing C code vs hand rolling some ASM. Comparing that with attempting to shove an object graph into a DB is foolish.
I wanted to start with VB.NET and move onto C#. I've been coding in Java for a long time, so I think I can catch up with C# quite easily.
Well yea, that's what I use them for. To ensure that each pair of rows from tables A and B have only one mapping row.
.NET is just the framework. What do you wish to learn? There's ADO.NET, ASP.NET, VB.NET, C#, F#, Silverlight, LightSwitch, etc. You'll have to research what your needs are before deciding. Saying you want to learn .NET is a bit vague.
Seconded! I would recommend starting out with 2.0 to learn the core features, then moving forward as you start to see the pain points and learn how they were addressed in later releases. E.g - what problems of WinForms were solved by WPF? ASMX -&gt; WCF? ADO.NET -&gt; LinqToSql/EF? IAsyncHandler -&gt; async/await? WebForms -&gt; MVC? WCF -&gt; WebAPI? The list goes on, and a lot of the framework improvements were accompanied by language improvements as well, so I think it can be very fruitful to follow the history instead of just learning the latest stuff.
I don't know of any good VB.NET books, though there are plenty of great C# books like Head First C# which has a lot of project based learning. Both languages are pretty interchangeable for the most part so you should be OK learning from a C# book with a syntax reference like http://www.harding.edu/fmccown/vbnet_csharp_comparison.html Also, I noticed the website I linked to previously also has a Java to C# comparison which may be useful to you. http://www.harding.edu/fmccown/java_csharp_comparison.html
But why would you want to learn VB.NET at all? Unless you have a legacy codebase to maintain, there is absolutely no sane reason to learn VB.NET.
Resize the media panel that you want to make fullscreen to fill out the entire form. Then do what is described on your codeproject-comment link. Maximize, and eliminate borders. 
If you're familiar with Java, then just jump right into C#. The only reasons anyone would want to learn VB: * Maintaining (low quality) legacy code * Crying * Time travel to 2001
Thanks, that did it. One thing (among others) that was hanging me up was that I couldn't seem to make the media player stream take the top level and cover everything else, it turns out I misunderstood the topmost property (it's a forms property not for panels, doh). I finally set everything else to visible=false and resized the screen and that did a great job. Edit: Turns out, I didn't need to set anything else to visible = false, just needed to set panel1 (or whatever panel the user chooses) to panel1.BringToFront();
What event is this code executing in? I suspect that you're trying to access your drop down before it's available.
Interesting thinking, but you are not going to gain anything learing VB.NET and then C#. What's important is the frameworks and the tools available. Knowing Java, will mean you have a head start with C# and can get onto the cool stuff, ASP.NET MCV, Entity Framework, WCF, WPF, to name a few.
So I have put those code in the load of the entire page. The grid within the page will not show or use the dropdown until it is in edit mode so its not even visible prior to that. Where should I be putting the databound for the grid if not in the load of the page?
It should be in the Page_Load event, yes. Obviously, *something* isn't initialized when you're running this code. My first guess is the drop down. My second guess is something inside of your GetSetLookupTable function. The only way to find out is to step through the code in the debugger and use some watches.
Please give an example :)
Consider this projection SELECT Customer.CustomerKey, Customer.FullName, Customer.TypeKey, Type.TypeName FROM Customer INNER JOIN Type on Customer.TypeKey = Type.TypeKey I've never met a ORM developer in real life who would actually create a class to represent this. In fact I'm not sure that I've met one that even know how to do us in an updatable manner. Instead they all just use two classes, giving us this query: SELECT Customer.*, Type.* FROM Customer INNER JOIN Type on Customer.TypeKey = Type.TypeKey And that's assuming they aren't being lazy, in which case it becomes a 1+N query.
I'm assuming you're rendering the grid once for each grid row, right? If memory serves me correctly, you can't refer directly to a control (your dropdownlist in this case) that's inside your (grid's?) item/edit template by *id*. You have to traverse the elements inside your grid row by row. 
What are your environments? (dev/test/live/etc?) You can always put stuff in the web.config file and edit it on the fly without having to rebuild or redeploy. You can even set it up so different build targets have different values in the web.config
Here's a GREAT C# tutorial, and it moves at a pretty good pace, while still actually explaining every concept very well. http://rbwhitaker.wikidot.com/c-sharp-tutorials
Is it one SP or several?
It makes sense. I'd rather have to hit update again for a new dependency package than have it auto update and potentially have breaking changes and have no clue as to why or that it was even added It would be nice if we could easily limit the version through the UI. I hate having to lookup the code to limit jquery to before 2.0 each time I include it in a project
I hope you get the answer you are looking for but please with on your titling. If this was stack overflow it'd have been deleted, here you just take advantage of sleepy mods.
I hope you are not doing this on your employer dime. Before tackling this issue I would spend time learning all the keyboard shortcuts for vs studio. Making sure I have 80% code coverage, continuous builds, automated deployment to qa and production environments. 
&gt; I've never met a ORM developer in real life who would actually create a class to represent this. Why not? If it's used a lot, one can decide to use a separate class for it and write a simple linq query to fetch it in 1 go. It's read only data, I presume? You only give a sql snippet, without intend into what it should be stored in or used for. If the fetch is 2 entities in a graph, a join is not really useful in a lot of cases (could lead to duplicates, which means lots of extra data to consume). In my orm (LLBLGen Pro) one could define a path which results in 2 queries if this is a graph fetch (one for each node), and with each node you can specify which fields to fetch (or to exclude, e.g. you don't want a blob field in that query as it's not used by the caller). If Type is a lookup, you could also decide to store that in a cache so fetch of that set is pulled from the cache, so you effectively hit the db for just customer and just the fields you specified. And it's stored in a change tracked entity, so you can update it in 1 line of code. And I doubt it's much slower or slower at all. The thing with comparing a sql query with an orm query is that it's apples and oranges: you can't execute the sql query, you have to write extra code to run it, create parameters, or use a lowlevel lib for that, but it also gives you the burden to do your own change tracking for example, persist the data in the right order so fk's are not violated, the data is validated, so required fields do have a valid value etc. 
Waiting for someone to write my sql would suck. But I guess it would force people to think out their solution.
My brain hurts.
Obviously not! That would a very good way to send my productivity to hell. This is a purely private venture. After a bit of fiddling I have concluded that in almost all cases this is not good enough for production. 
This is very helpful, thank you.
noice!!
Could you elaborate on the reason for the growth the operation big O notation size in EF?
No. I don't know the cause, my information comes just from the timings. And I don't have access to that source code any more so I couldn't even say which is the magic flag to make it go fast.
Well you could say that about lots of things. Lambda functions are not totally necessary. They're fucking useful though. 
Since .NET replaced VB6 and VC++ for programming desktop apps people have been complaining that .NET makes threading too easy. Maybe we should go back to ATL and MFC for building Windows apps, that way only "real" programmers who know what they're doing will be coding and will building things the right way, right?
Why a .net encoder? I know this is a .net subreddit - but ffmpeg is really powerful. The command line is certainly a barrier to entry, but it should do what you need. You could most definitely invoke ffmpeg via .net using System.Diagnostics.Process and read the output to do whatever you like to update a UI or whatever it might be. It also looks like there's a .NET wrapper for it :) http://www.intuitive.sk/fflib/
Ive seen that but I was unsure of how it well it works. Basically I need to be able to convert from any format to something HTML5 streamable. Have you ever used it before? How well does it work with queuing/background processes? I know the product I mentioned uses ffmpeg, I am curious if pay for the 3rd party licence would cut out a significant amount of dev time. 
I've used ffmpeg a lot - and it will give you the flexibility you need. What you want to do is transcode your media from whatever format it might be in now into MPEG-4/webm/ogg. If it's already in one of those formats, you should be able to play it using the HTML5 video tag. EDIT: I should add that you'll need to use .NET to do any queuing and process prioritization. Transcoding media will be very CPU intensive. EDIT 2: I haven't used the .net wrapper for ffmpeg - I've always just used ffmpeg directly. 
Can you point me to any guides that show how to use ffmpeg from .net? 
Thats why you came here :)
I went with AWS CloudFront to host a 30MB image and timed it via Stopwatch and WebClient.DownloadFile, Cheers.
Thats a great idea. :) Be sure to check the stats from the network adapter too. Just grab the bytes before the download and after the download. As you will want to check uploads as well in the future.
That's what's so nice about async/await. Be default it doesn't create new threads in GUI apps, so they are less likely to mess that up.
There is a stackoverflow thread on C# wrappers here: http://stackoverflow.com/questions/2163036/solid-ffmpeg-wrapper-for-c-net I haven't tried those. I use ffmpeg quite a bit from node.js, by having node.js code run the command line and monitor the output. You can try the same approachfrom C#, using Process.Start(). Here is some utility code for running external apps with process.start and monitoring output that you may find useful: http://stackoverflow.com/questions/2163036/solid-ffmpeg-wrapper-for-c-net 
Without considering the .net wrapper, you can use it through .net by using Process.Start: http://msdn.microsoft.com/en-us/library/system.diagnostics.process.start(v=vs.110).aspx This guide should get you up to speed on using ffmpeg: http://en.wikibooks.org/wiki/FFMPEG_An_Intermediate_Guide#Get_started_with_FFMPEG_transcoding using ffmpeg can be as easy as providing 2 file names, and as complicated as you want it to be. The complexity, if you want to call it that, of using ffmpeg and even transcoding video in general depends entirely on your requirements. For example, are you displaying video on mobile devices or PCs? What are the dimensions of your player? Will you allow full screen viewing? Are your videos powerpoint presentations, movies, or sporting events? How much bandwidth do you have on your server? I feel like it's also worth adding that properly transcoding a video requires that you plan for most of these things. You will waste a lot of bandwidth, disk space, and cpu time transcoding a video of a powerpoint presentation at 5Mbps and 60fps. It's all about finding a good balance. I don't want to overwhelm you, since it sounds like you're just getting started. But the encoding profiles you choose will make the difference between needlessly devouring resources and getting exactly what you need. 
Thanks much for your reply, I'm going to read those links and the others everyone else suggested. As for the profiles, I'm aiming for either 360p or 480p around 700kbps h264, not much, really just for streaming on the web, I know video transcoding is resource intensive hence why I think setting up the proper queuing will be the most challenging. 
Word - let me know if you need any other help. 
This isn't true. What happens is that tasks get spawned up on the Threadpool then it gets to decide how many threads to create to to handle a bunch of things. If you create a bunch of tasks and then do something like await Task.WhenAll(tasks) and those functions are io/or CPU bound the threadpool is doing to try to execute them all in parallel. There are nuances but it can have an added benefit of making things mutli-threaded in the sense that threadpool can execute a bunch of things. This isn't the same though as writing an algorithm that does parallel computation to maximize CPU output like a matrix multiplication or something like that. Edit: tasks aren't always spawned off of the initial threadpool but they do use the threadpool when trying to run multiple tasks, which appropriately removes the burden of having to create your own threads. Edit 2: OP has a better explanation in the article. :/
You can, but that's not really what it's made for. And if you are a library writer they explicitly tell you not to spawn threads from async methods. 
Do you have different sets of test data to stress each condition in your SP - do u clear down DB and insert the appropriate data on each test? How do u ensure devs downloading code for the first time can run these unit tests? Do you not get perf issues on DB tests?
ffmpeg is GPL so there may be reasons why you may not want to use that. 
Expression Encoder is end of life and will not be supported, I think, as of next year.
If I get performance issues in my database tests then I'm going to get performance issues in production. I honestly don't understand why people prefer fast tests over more accurate tests. Or why they assume a slow test is a test problem rather than an application problem.
Most of my database tests are "dirty tests". I don't clear the database, I just use what I find. That means a lot of tests exercise insert, select, and update procs all at the same time. Usually I test through the service layer. When procs are written for a specific application or service, I mostly care how it behaves with that service. If its a generic proc used in multiple places then I'll run a real integration test.
There's App builder rewards build.windowsstore.com/rewards for Windows 8 and there's DVLUP dvlup.com for WP8 (and some Windows 8)
Thanks for the links!
No, that includes using threadpool threads. If you are writing a library and expose a function that returns a Task and has Async in its name then you are supposed to actually perform the work asynchronously. That means you don't get to use a thread at all, not even one coming from a thread pool. (Well technically you do get a thread from the IO Completion Port's pool, but that only runs long enough to signal that the operation is complete.) Here is a summary and links to more info: http://www.infoq.com/articles/Async-API-Design 
Cast it into an instance of ICollection, then you can read the property.
Right. But maybe there is nothing to do. Look at how TaskCompletionSource works. It is surprisingly flexible. 
Read the links under "related articles". It's a series.
Not sure if this is what you mean (and I know I'm far from expert) but here is what I typically do: In the aspx, I only show the drop down when a user is editing, or in the footer for when they are adding a new record. Of course you can change this as you see fit. &lt;asp:TemplateField HeaderText = "SKU"&gt; &lt;ItemTemplate&gt; &lt;asp:Label ID="lblSKU" runat="server" Text='&lt;%# Eval("SKU")%&gt;'&gt;&lt;/asp:Label&gt; &lt;/ItemTemplate&gt; &lt;EditItemTemplate&gt; &lt;asp:DropDownList ID="ddlSKU" runat="server" DataValueField="itemId" &gt;&lt;/asp:DropDownList&gt; &lt;/EditItemTemplate&gt; &lt;FooterTemplate&gt; &lt;asp:DropDownList ID="ddlSKU" runat="server" DataValueField="itemId" &gt;&lt;/asp:DropDownList&gt; &lt;/FooterTemplate&gt; &lt;/asp:TemplateField&gt; Then, in the code behind, in the RowDataBound of the gridview, I bind it to a dataTable that contains all the index/value pairs, and set the selected the value for the row. If e.Row.RowType = DataControlRowType.DataRow Or e.Row.RowType = DataControlRowType.Footer Then If ((e.Row.RowState And DataControlRowState.Edit) &gt; 0) Or e.Row.RowType = DataControlRowType.Footer Then Dim ddlSKU As New DropDownList ddlSKU = e.Row.FindControl("ddlSKU") ddlSKU.DataSource = dtItems ddlSKU.DataTextField = "SKU" ddlSKU.DataValueField = "ItemId" ddlSKU.DataBind() If e.Row.RowType &lt;&gt; DataControlRowType.Footer Then ddlSKU.SelectedValue = DataBinder.Eval(e.Row.DataItem, "ItemId") End If End If End If 
That is an interesting approach and will try it out. Your method also seems more efficient than preloading all of the dropdowns in every row of the grid at load time. Thanks much!
Cool, I hope it works for you. I load the dtItems datatable in the Page_Load, so it's only hitting the database once per page for all the rows.
Not to echo everyone but I learnt VB.NET before C# because I was coming from vb6. I soon changed over to C# and never looked back. It's more concise and readable IMO. I also knew Java so the syntax was familiar. There is no reason to learn unless you must maintain a vb.net codebase. 
You want IIS Application Request Routing. It adds a reverse proxy feature and URL rewriting to IIS. http://www.iis.net/learn/extensions/planning-for-arr/using-the-application-request-routing-module
It does. This walkthrough should get you there: http://www.iis.net/learn/extensions/url-rewrite-module/reverse-proxy-with-url-rewrite-v2-and-application-request-routing
[TFS](http://msdn.microsoft.com/en-us/vstudio/ff637362.aspx) is basically the successor to VSS. Plenty of debate about what's best, but it gets the job done. For now, for &lt;=5 users, it's free to use the online version. Much easier when MS hosts the server and does the maintenance. [VSOnline](http://www.visualstudio.com/products/visual-studio-online-overview-vs)
I tend to prefer Git for both small and large projects. There is an extension on Visual Studio Gallery that provides GUI tooling if you prefer that.
[Github's client](https://github.com/) is pretty nice. [Perforce](http://www.perforce.com/) is nice too, and free for up to 20 users. And there is always TFS. 
Good god, man. You could take your code, print it out, set it on fire, and *still* be better off than using VSS.
And with only one user you shouldn't even have any serious problems.
You're all crazy people. Subversion all the way.
I would recommend either [GitHub for Windows](http://windows.github.com/) (GH4W) or [SourceTree](http://www.sourcetreeapp.com/). GH4W is geared towards git repositories on GitHub, but it does support other remote repositories. It's awesome in its simplicity; syncing changes between your local repo and origin can be done with a single click. The only drawback is that because it hides a lot of the git commands, if a something does go wrong (e.g. a merge conflict happens), all it tells you is to fix it by opening the console. I personally prefer SourceTree because it doesn't hide anything and allows me the all the flexibility that git provides. It's easier to shoot myself in the foot if I'm not careful, but I rarely need to open up a console to fix issues that come up. Plus it provides a nice branch graph; I don't need to run `gitk --all` from the console.
Using git from the command-line has more of a learning curve, but in the long run I think thats the best option.
I second SourceTree, we use it exclusively at work with multiple remote and in house repos and it's powerful enough to keep me out of the console but simple enough our designer can do pulls and commits. 
Mercurial (Hg) with TortoiseHg is my go to source-control system. Very similar to Git but easier to use in my opinion. TFS is terrible and should be avoided.
Microsoft has an official git source control provider in the VS extensions gallery. Even if you stick to the command line it's nice to have it for diffs or to look at the git log with a nicer UI. I detailed my command line setup [on my lousy blog](http://ventajou.com/all-in-one-command-prompt-for-windows-developers), as a way to remember how to do it. A bit annoying to setup but pretty useful in the end.
I've been using hg with Bitbucket at home for a little while now and I haven't figured out why everyone's all over git but no one ever talks about hg.
Mercurial, Git, Subversion, TFS, take your pick...almost anything will be better than VSS.
I forgot VSS even existed .... those were some dark days.....
If you are already familiar with Git, might as well stick to it. There are a ton of clients for it (GitHub for Windows, SourceTree, Git Extensions, PoshGit, etc.) and multiple hosts that support Git projects (VS Online, Bitbucket, GitHub, etc.).
Subversion works nicely, and if you're familiar with conventional source control systems it won't have a learning curve like git. VisualSVN Server + Tortoise SVN (windows explorer integration) + Ankh SVN (visual studio integration) will be you all setup nicely.
You can also choose to use git with TFS if you want. It will prompt you which source control system you want to use when creating new projects.
I prefer TFS, however git and mercurial both work fine with VS and asp.net projects. i used to use bitbucket a lot.
The first rule of VSS is stop using VSS. If you want to enable your career begin your transition to GIT.
Gotta love HG. There's a small learning curve coming from VSS since HG uses sub-repositories, but once you adapt its great.
This, also TFS is so much more than just source control
What are the early 2000s like?
Both were readily available on my mac, but at the time, I was seeing a lot of complaints about git being clunky on Windows while hg was smooth. Also, I tend to have a thing for underdogs :-) 
We updated to 6.0.2 and for our use (straight reading without updating any data) it did drastically fix our performance issues. 
Oh god, the flashbacks.
How does that handle major refactorings within Visual Studio?
Team Foundation Server is probably your best bet, as you can use both the internal source control module and git. If you just want a quick replacement for VSS, SourceGear Vault (https://sourcegear.com/vault/) can quickly get you off of VSS. I've used it for years without any issues, though I moved to TFS because of the integrated build process.
Just fine, though if there is a particular problem you anticipate I might be able to clarify. The only gotcha is to make sure you "save all" before committing if you have changes to the project or solution file, as sometimes Visual Studio doesn't save them (thats bitten me with TFS too). I use the "git gui" command to review each modified file as I include it in the commit. I haven't tried the newer VS/git integration, but given all the bad experiences I've had with TFS/VS integration I'd prefer not to try it. One nice thing about being comfortable with using git from the command line is that I can use the same approach for non-VS coding projects. 
What about with file renames &amp; moves? Svn gets pissy if you don't tell it about those, making an ide plugin almost mandatory. 
If you already know how to program stuff, then I'd recommend c# in a nutshell. It's a very well written reference that covers the language as well as the framework. http://www.albahari.com/threading/ is a free chapter about threading in .net. 
I did and it's not slow. I even went so far as to telnet to the SMTP server and create an email with multiple recipients. I had no problem whatsoever. It seems to be only when sending via mvcmailer with multiple recipients.
Drop MS deploy, it gives you absolutely no flexibility. If any one knows otherwise on this I'd love to know more. Use nant (or msbuild if tour sadistic). In a build directory recreate the file layout you want and copy everything to the staging site. This is generally good enough for CI. If your needs are modest you can create a zip file to send to production. If they are more involved then look into the nullsoft installer.
Don't use githibs client, it sucks for even basic things. Git extensions or Source Tree are both way better.
What does your deployment step resemble, are you copying just this project files to the corresponding directories on a running instance of the site? Or are you deploying to a new instance of the site and switching to it?
The big question is, does it matter? Are you sending a large volume of email? If your sending them from the IIS process directly, don't.
Hi there, As Flukus said, MSDEPLOY doesnt give you this flexibility because its a deployment method (like FTP), not a multiproject consolidator tool. The problem you have is unrelated to msdeploy, and more that you are trying to deploy multiple "sites" into one "site". I assume this is an Umbraco or Kentico site? Neither were really designed with CI in mind. I see this being possible a few different ways: - consolidate all of your projects into one, but keep your files separated at the file system level (project file includes) This allows you to blow it all away for an upgrade and quickly redrop your files, but allows the entire site to be deployed easily "as one" the way Microsoft designed. Your need, while frustrating because you feel that something is "broken" is not really how Microsoft designed the Visual Studio web development workflow. Having been there I know how frustrating it can be. - use msbuild to consolidate your project outputs after Publish before running msdeploy to package the consolidated output. - if you don't need a package you could actually deploy each "site" separately using msdeploy direct to your host but with different paths, which can be done by calling msdeploy using the "contentPath" provider and placing "siteName/path/path" as your contentPath for deployment. This method of deployment splits your separate areas of your project if it came to wanting to deploy as an atomic unit but you could remotely call msdeploy backup against the root site after deploying the last folder to get the whole consolidated atomic deployment package for restore.
It absolutely does matter. The emails are sent as the result of a user clicking a button (notifications to other users). 
So it's being sent from the IIS process directly which is a terrible idea. Pick your messaging poison and handle it in a background service. 
Mvcmailer doesn't really give you a choice. You create an instance and do .send() and off it goes. 
I didn't realize MVC Mailer had a dependency on the MVC framework, this is a terrible design decision. There are a few other frameworks that allow you to use razor to generate the templates though. I'd look into them.
I agree but it's popular which is why I decided to use it. Do you know of any of the other frameworks off the top of your head? 
Awesome. Thank you for the info. Thankfully I'm on vacation for a few days but I'll try them out when I'm back at work. 
And I don't feel bad about that.
What happens if you reimplement just the basics and test? For instance, just make a similar message and send through Smtp the normal way and see what you get. My thought would be a foreach(toAddress) send an email type of situation somewhere, but I'd have to dig through their code real quick to see what they do in that situation.
Inedo build master. Free for up to 3 users. Seriously. 
I don't get why he does this: $('#process').click(function () { $('#myform').submit(); }); And then moves the button outside the form element. Wouldn't leaving the button in its place and avoiding the code above just work?
&gt; use msbuild to consolidate your project outputs after Publish before running msdeploy to package the consolidated output That's what I would like to do. &gt; using the "contentPath" provider and placing "siteName/path/path" as your contentPath for deployment. I tried doing this, but it doesn't seem to have any effect on the placement of the deployed files: everything still ends up in the root folder
Thanks I will take a look into this.
Well, no, actually, it did not "just work" as was explained in the article. I have not been able to figure out WHY it didn't work (you would think it would, right?), and I would love some feeedback as to what I may have missed. Because as you indicate, the way I ended up doing it feels like an unnecessary hack. Essentially, no matter what I did, so long as the button element was within the form, neither the preventDefault() method, not simply "return false" prevented the default submit action from firing. I would love to hear I was missing something dumb (which is a very distinct possibility, and one reason I post stuff here), and would promptly correct the article . . . Thanks for reading . . .
Having re-visited the issue, I believe I corrected whatever the problem was in some other way, and merely thought I was fixing it by moving the button. I went ahead and futzed with it, and almost immediately found it now just works, the way you describe. Must have done something stupid after all . . . off to update the post now . . . Thanks!
Yeah, inputs of type submit inside a form element already trigger the onsubmit event attached to the form when clicked. However you added to the default behavior the .submit() method call on click, which doesn't fire the form's onsubmit event but directly submits it. By the way another approach could be using Ajax.BeginForm with UpdateTargetId and OnBegin / OnComplete functions.
Indeed. One of the problems I was having figuring it all out was that there are so many different ways to do it, with minor variations. I'm sure I'll get better at it over time . . . :-)
I like the jQuery BlockUi library for these type of operations. 
using Config; using System.Text.RegularExpressions; public class SomeController{ private SqlService _sqlService = null; public ActionResult SomeAction(String data){ if(true == String.IsNullOrEmpty(data)){ return Content( "Invalid data"); } List&lt;MyItem&gt; myData = Deserialize(data); foreach(var item in items) { var col_id = item.column.PregReplace('/[^\d\s]/', ''); var widget_id = item.id.PregReplace('/[^\d\s]/', ''); var sql=" @UPDATE widgets SET column_id='$col_id', sort_no='"+item.order+"', collapsed='"+item.collapsed."' WHERE id="'"+$widget_id+"'"; //mysql_query($sql) or die('Error updating widget DB'); _sqlService.Execute(sql); } return Content("success"); } } public static class ExtensionMethods { public static String PregReplace(this String input, string[] pattern, string[] replacements) { if (replacements.Length != pattern.Length) throw new ArgumentException("Replacement and Pattern Arrays must be balanced"); for (var i = 0; i &lt; pattern.Length; i++) { input = Regex.Replace(input, pattern[i], replacements[i]); } return input; } }
Notes: In .net you'll probably want to use mvc.net so you'll need to create a controller with actions. This example uses a sample controller with a sample action that you should name or place in what ever fits best for you. You'll also need to write a deserialize method that maps your json to an object. This is pretty simple you can User the native [JavascriptSerialiazer](http://stackoverflow.com/questions/7895105/json-deserialize-c-sharp) or the popular [json.net](http://json.codeplex.com/). I didn't specify any specific sql service but there are a verity of options for executing simple sql statements. Obviously if you are going to be doing a lot with a database it may be in your interest to earn Linq to sql or Entity framework but if you jus want to keep t simple look into the native SqlClient. Also I was a little lazy with the regex replacements. Infact i bet you wont even need them if you have the json map to integers.
For the sql you could do something simple like this: using (var conn = new SqlConnection(MyConnectionString)) { var cmd = new SqlCommand("UPDATE widgets SET column....", conn); cmd.Parameters.AddWithValue("@bar", 17); conn.Open(); cmd.ExecuteNonQuery(); }
No, I did it much like you indicate (except, I was originally trying to catch the form.submit event, then use e.preventDefault()). I must have been doing something else wrong, but not had enough coffee to see it, or something. 
I actually found this: [RenderViewAsString](http://www.codemag.com/Article/1312081) and I'm going to give that a try on Friday. It's probably what Mvcmailer does behind the scenes.
Caffeine is magical. 
Uh, I didn't know that, so let me get this straight: * &lt;input type="button" /&gt; : sometimes submits * &lt;input type="submit" /&gt; : always submits * &lt;button type="button"&gt;&lt;/button&gt; : never submits * &lt;button type="submit"&gt;&lt;/button&gt; : always submits Right?
VS2013 comes with built-in support for Git now. But yes the Git Source Control extension is more feature complete. I still prefer to run the excellent Git Extension app on the side as it lets me see better where I am and do all the branch switching/merging kung-fu one would possibly want.
VS2013 is ~~not~~ now supported.
I tried it but to no avail. I will confide my IDE-free dev-work to node.
Is this statement ironic? Considering what I know about the architecture of early Facebook, this seems ironic.
I don't know... just posting what Justin Timberlake said in the movie, didn't think about it much. : )
I implemented one for my company that we did nearly 160k through in roughly 5 months. We've had zero issues with it. [http://www.nopcommerce.com/](http://www.nopcommerce.com/) I even loaded their source and it compiled first try! One of the biggest benefits is that out of the box it has a HUGE amount of functionality so our authorize.net account *just works* along with shipping estimates (although we have a discounted account and unfortunately I'd need to modify the default behavior to support it)
Came here to recommend nopcommerce as well. Great system, frequently updated.
-1 for choosing µtorrent as bt client +10 for nice utilities I didn't know about
I know some people have had luck with [episerver ecommerce](http://www.episerver.com/) I personally like [siteFinity](http://www.sitefinity.com/ecommerce) from Telerik. (note: this is not free) 
How many complex Java applications compile the first time? This is one of the great things about VS and .NET.
I was really intrigued by nopCommerce when I found it a few months ago, but the problem is that there is essentially no 3rd party support for it. Especially when it comes to themes, all of the big sites ignore them (TemplateMonster, ThemeForest.net, etc). I think their limited selection of themes are way too basic for most companies, and this is what ultimately led me away from them and to Magento (shudder)...
But now you can run it in a background process. MSMQ/RabbitMQ/NServiceBus, anything along those lines. [EasyNetQ](http://easynetq.com/) is simple and does what you want. I don't know what the rest of the app entails though. It could be worth your time investigating other options.
If the API is the same, there is no reason not to consolidate. You will gain significant benefits in terms of caching and manageability. Have a *stateless* REST service that all the clients can call. Consolidating does not mean one machine, nor should it. Your database should have a backup or two that are geo-redundant. Depending on your requirements, you can even have the replicas serve read traffic if you want. For the service itself, you have a group of servers any of whom can handle any call (aka stateless). Preferably, these servers are also geo-distributed. So it looks this: * Client 1 calls service.foo.com/api/blah * Server 5 receives request and queries database.foo.com * Server 5 returns the response * Client 1 needs more info and makes another call to service.foo.com/api/blah2 * Server 3 receives the request and queries database.foo.com * Server 3 returns the response. Server 3 and Server 5 can be in different datacenters and a replica database could have taken over being write master in the intervening time. I would also insert memcached chunks between the client and the server and the server and the database too, but that is a perf optimization. And the best part is that this architecture can scale up or down to pretty much anything you want. It will work with 1 server. It will work with 10,000 servers. The key is to assume that any machine can go down at any moment for any reason. You are consolidated into one logical entity, but you have plenty of resiliency built in.
Service Bus, App fabric for mem cache and Wcf hosting, application request routing
But you're aware of the fact that both languages are doing the same stuff? C# has 2 or 3 things that VB.Net can't do. Everything else is the same. You can convert IL code to any .Net language you want to. In this particular case OP is already familiar with the (i think ugly) syntax of those c/java like languages, so C# should be easier because of that. But feature wise there's no difference for a beginner. I would even say the VS code editor is better for VB.Net than it's for C#, which helps beginners.
You, sir, are a unicorn. Until now, I have never in my life heard someone say they prefer VB for the syntax.
Well, the Visual Studio code editor is helping me out a lot. If I had to type every keyword myself, including all the end if/using/try/for/etc statements, I would go insane too. But with VS you only press enter and everything gets done automatically :) Regarding keywords I usually type 3 letters and then press space, trusting intellisense to be able to recognize the right thing. And t+space becomes true and f+space becomes false. I feel lazy but efficient.
This is the most complete list I found this year. Awesome tools on that list, and a lot of new tools for me to explore. 
There is always windbg for debugging your app. That will give you an understanding of CLR internals.
I'm a WebApi convert. No more heavy WCF proxy client generation. Maybe related; if SOA isn't warranted, or you otherwise want to solve these, or similar, problems with local assembly references, referencing your central shared components in your own NuGet package is a snap and eases the "one solution to rule them all" problem.
+1 for AppFab(ulous). But it's not a magic bullet. It comes with it's own set of problems. Optimizing the throughput isn't a slamdunk.
Very true... But very easy to use from a code implementation perspective. 
The only problem is going to be managed private binpaths, which are configured in the config file of the original executable. Since several different executables can have different configurations, this can lead to the use of different versions of the same referenced assembly, etc. There is really no way to handle that correctly, though, without starting with an executable or pointing out a config file or similar.
Yes, OfficeLoc would be the location. The company I work for has many different locations. The upper managment like to keep the idea we are all one company but we feel more like a conglomerate. Hence me wanting to differentiate from the other locations. In the example I used this class will be used in multiple products/utilities. This is why I decided to keep a close resemblance to the origin namespace.
For general purpose stuff it's just `[OrgName].[GeneralArea].{[MoreSpecificAreaIfNeeded]...}`. For something with specific business rules that in my organization are generally scoped to a contract, we generally don't use the OrgName and replace it with the contract name instead. I would like to still have the org name but this convention predates me. We have a core library with a bunch of stuff in it. Everything in this assembly and a small number of optional feature assemblies is named like `[OrgName].Core...` We also have another layer of "apps" that work like plugins in to our base product that can be included and extended as needed. Those are in a namespace like this `[OrgName].[ProductName].Apps.[AppName]`. Our solutions end up with 3 tiers, the `Core` which is rarely touched, just used as is, the `Apps` which will sometimes get stuff added if it's general purpose enough (then pushed back up the branches to be available in other contracts) and the contract specific stuff which is where most of the churn happens. The namespace hierarchy we use ends up working pretty well for this. I do not like the idea of having `System` in my user created namespaces but I don't have a very good argument for why other than it feels wrong. Edit: To add, if it were me I wouldn't include the `OfficeLoc` in the namespace unless that code will never be used anywhere else. The only scenario where I see this making sense is if the functions of each office is so dramatically different that it would be unlikely there would be any reuse between offices. If some one above you is insisting on this to stake out ownership of parts of the codebase, I'd be very worried about the broken structure of the company. If that's the concern, handle it in source control. Otherwise it's mostly just going to needlessly fragment your codebase and encourage code duplication.
For internal stuff, we use OrgName.Division.Product as a base for everything. Seems to be fairly standard. We do it this way because our agency is big enough that the divisions are autonomous units to a point, so I may do development work for two different units. If we have a shared product, Division goes away, e.g. we make extensions for a product called Sitefinity fairly often, and two of our divisions have sites running on it, so: 1. Agency.DivisionA.Sitefinity.Web - Sitefinity project for DivisionA 2. Agency.DivisionB.Sitefinity.Web - Sitefinity project for DivisionB 3. Agency.Sitefinity.Modules.Video - Shared video player module that both use For your specific case, it sounds like this is a persistence facade, yes? In which case I'd probably use the naming conventions of "Repository" or similar. 
dice.com?
In have some coworkers who specialize in Dynamics CRM. They too have had a hard time finding resources for 2013. The SDK was released a few weeks ago (http://www.microsoft.com/en-us/download/details.aspx?id=40321) . Otherwise our best bet is your Microsoft rep (TAM, TSP, etc.) Should have some access to partner and customer resources for you to use. 
I think the suggestion was for you to look for work. 
Tons of great jobs in the marketplace that you could be at. Doing MS CRM is a niche that, much like Sharepoint, will ruin your career. Consulting agencies and off-shore talent is where you should be steering your manager.
Fair enough, sorry then. I'm kinda stuck with this one for now, it's complicated. 
Gotcha. Have to keep this for a another year or so and it's usually a cool place to work at. And no, I am not planning on specializing on CRM or sharepoint.
Thanks a lot! Ok, so it's not just me. I'll dig deeper through the SDK then,
Sack off any abbreviations. They only lead to confusion and saving yourself all of a few keystrokes is pointless.
DNN is by far and away the worst CMS system I have ever had to work with by several orders of magnitude. It is terrible. For open source, Umbraco is in the lead IMO. For closed source I'm not so sure.
You might get confused because it does a lot of other things but at the core Sharepoint is really a Content Repository. [Sharepoint Foundation](http://www.microsoft.com/en-us/download/details.aspx?id=30345) is also free (as in beer).
I am not looking for something like SharePoint. I am not developing a document management system. More or less, I need a CMS that is flexible.. that allows its skin to be completely re-done in addition to custom modules that will be integrated into the site. The website is a mix between a commerce site with the ability to "build" products. For example, say a user wants a custom T-Shirt with their own logo/text on it. The site would go through a "builder process", allowing the user to customize their product and then "review" their product and purchase it. Everything is done through custom modules that I build. Over time, these builders and modules need to be updated.. so I updated the modules and redeploy them. The notion of encapsulated modules that can be deployed is crucial, since I am moving away from the traditional "rebuild the whole website" and "push the whole website" when changes are made.
If you are an ASP.NET developer, and are comfortable writing custom code then you might take a look at [Umbraco](http://umbraco.com/). I implemented this CMS for our company and have largely felt it was perfect for our needs. They make it clear that it is a CMS for developers. You won't find tons of functionality out of the box as you do DNN but then again the system isn't nearly as complex or bloated (in my opinion). Instead you learn some powerful concepts such as their document type system, razor API, macros (how to load user custom user controls) etc. If you've considered writing your own I'd definitely encourage you to check out Umbraco. Edit: Didn't notice DaRKoN already recommended but I'll leave my post since I provided a little more information
We use DNN at my employer and whenever I have to actually use it I want to gouge my eyes out. Even more fun than using DNN is trying to upgrade between DNN versions, because that's what you'll inevitably have to do.
Do you have any recommendations of an open source CMS that is lightweight, which would allow me to extend and tweak to needs? All I really want is a CMS system that allows deployment of modules. As mentioned before, I no longer want to go through the process of rebuilding the website and pushing the site when I make a hotfix or upgrade. I will def take a look at Umbraco. I looked at it before, but was turned off when the CMS was upgraded and the upgrade broke the CMS's session state. This issue affected tons of people using the CMS. Obviously, the Umbraco company fixed this issue right away... but it was a real turn off. Anyway, I will take a look again and give it another chance. But I would really like to find a simple CMS that I can customize to fit my needs. Obviously DotNetNuke isn't doing the trick.
I don't know what people do to their DNN setups to cause this... It's pretty easy and straightforward if you ask me. 
I want to agree with some of the people below: 1. Never use DotNetNuke unless you are forced to. 2. If money isn't an issue, then use Kentico 3. If you want free, then use Orchard. I have written modules for all three, and by far my best experience was with Kentico. The free version is a little lacking, though. Orchard is really nice for free software.
Namespaces should give you context into what the code is doing. Anything that isn't specific to the code is usually useless. Org name, office location, division, etc. doesn't help you describe what the code is doing. So in your example above, the only useful info is System.IO. However since system is usually a bad spot to put things, go with AssemblyName.IO. I mean if you really want your code separated from the other locations in your company, why not just have separate repositories?
Do not listen to this! You will want to kill yourself within 6 months if you take this route. You are recommending SharePoint, really?... Are you a Microsoft salesman?
If you want a simple and extendable CMS I would recommend using N2CMS. It will easily plug into existing sites and you only have to use the features that you choose or customize.
I'm both Sitecore and Umbraco certified, and either is fine for enterprise dev. Sitecore only really comes into its own when you're dealing with huge websites.
**SiteCore**: I have used several times and i also have a cert about it, it's a really advanced system and mostly it was easy and efficent to develop with. Well i guess thats why the license costs that much. You can find many open and closed module to it also. The developer forum is not great at all, I would say - these problems are beyond the basics, of course the training material is clear and detailed enough. **Umbraco** : Umbraco is a free CMS, I used Umbraco 5 first, which was a dead end, no wonder why, i felt it pretty soon. I heard good news about the newer versions and i developed with the version 6 in a smaller project. I would say it's just fine for fast projects, you can use your own logic easily to extend the CMS, but don't hope anything fancy. **Orchard**: I don't have that much experience with this CMS myself, rather than i implemented some fixes to previous sites written with this tool. As i saw, Orchard has a large community and it's easy to find examples and help at the dev forum to implement your moduls. I would say Umbraco is a bit more complex compared to Orchard, but by far simpler than SiteCore. As of your requirements, i think all of them fulfill these goals, however the debuging one is mostly based on your code design.
Out of curiosity is this for a company based out ABQ? Your example described exactly what a company I interviewed for does. 
Unit testing. The ability to mock dependencies is pretty much the only reason I do it, but that alone makes it well worth the effort.
I prefer to make my components testable so that mocking isn't required.
I'm not sure I follow. The reason you'd want to inject a dependency for testing is not because that dependency isn't testable, but because you don't care about it's implementation unless that's the class you're testing. Let's say you pass a config class into the actual class you're testing and you want to verify that the class handles the config values appropriately. In this case, you don't care at all how the config is created or what it's doing under the hood, you only care about what happens when config value = x. Normally you would have to create a concrete instance of the config class and set it up in a specific way to ensure that the main class receives the values you want. With a mocked config, you can just mock the value you want and not worry about any implementation details. This is great because it allows you to focus on the class you're actually testing and not on the config class. You would still have separate tests for the config class to handle concrete implementations, but you wouldn't use the tests for the main class above to test whether your config works correctly.
Lets get a few things straight. *** Dependency injection just means you give a class its dependencies. The alternatives to dependency injection are... * The class creates its dependencies. This is called "Encapsulation". DI and encapsulation are at opposite ends of a sliding scale. * The class finds it dependencies by reading some global property or function. This is often referred to as the "Service Locator Anti-pattern". Dependency injection does not require using abstract interfaces, but some choose to do that for mocking purposes. *** It is my opinion that well designed classes don't need to be mocked. Services can be mocked, but that occurs outside of the application boundary. *** Dependency injection frameworks, in theory, make passing in dependencies to classes easier when the application starts up. That's the key point, DI Frameworks should only be active when creating an Application object (WPF) or Controller (ASP.NET MVC controllers). In practice I've never seen a DI Framework take less effort than manually wiring up dependencies, but that's a personal choice. *** Inversion of Control just means class A used to control class B and now B controls A. You can apply IoC twice and return to your starting point. It isn't a design pattern, it is a technique. *** An IoC Container is completely unrelated to Inversion of Control. It is a way to apply the Service Locator (Anti-)Pattern, usually by (miss)using a Dependency Injection Framework. In my experience the use of an IoC Container results in applications quickly degenerating into a big ball of mud. 
&gt; Having working on a wide variety of apps, I think updating a tightly coupled app is much easier and less time consuming. I agree. In my experience "loosely coupled" just results in spaghetti code. But don't call it "tightly coupled", call it "well encapsulated". It means the same thing but has a better connotation. 
There are several reasons. It's important to remember that IoC and DI is just a means to an end to achieve things that you could do by hand, but would take lots of manual effort. Firstly, and IoC container means that you as a client of a class don't need to be worried about the dependencies that the class has - the container will do that for you. So if you add a new dependency into a class, all clients of the class are unaffected because they delegate the responsibility to the container to create that class. Secondly, testability. If you want to unit test, you'll probably need to start using interfaces soon (either that, or subclassing, or using something like Typemock) to break the coupling between your class under test and its dependencies. Again, an IoC container can automatically map the interface to the concrete so that at runtime you get the "real" dependency, but in your unit tests you can put a fake in instead e.g. instead of "really" sending an email, you just have an in-memory fake. Thirdly, you can do very powerful chain-of-responsibility style, compositional patterns that mean you can e.g. tag a method with an attribute e.g. [Transaction] to automatically wrap up a method call in a using transaction block etc. etc..
What if you had a two classes, one of which called the second. The first did some very simple logic that you wanted to test. The second did some complex calculations, called a web service etc. etc. - why would you want to couple testing the first class to the implementation of the second? The first has nothing to do with calling a web service, but without a shim in between the two, you would essentially be unable to test the first one. Or what if you wanted to test something that went to a DB. You'd have to ensure that every developer that wanted to run unit tests against that class had a DB where the connection string matched, that they had correct data for every test etc. etc.. - I've done it, and its painful.
&gt; The first has nothing to do with calling a web service, but without a shim in between the two, you would essentially be unable to test the first one. Inversion of control. It's hard to believe, but that phrase actually does have a meaning. Change your design so that the simple class doesn't depend on the complex class. *** Now if for some reason you can't change the design then you should mock the web service. Not the web service call, the actual web service itself. Your application doesn't change, you simply point it to a different end point. 
Unit tests and integration tests are two completely different things. It sounds like you either don't know or don't care about the distinction.
&gt; well encapsulated I may have to use that, thank you :)
&gt; IoC and DI is just a means to an end to achieve things that you could do by hand, but would take lots of manual effort. Could you perhaps give an example, or link to a blog post that might illustrate this? All the examples I've seen look like extra steps to me. &gt; don't need to be worried about the dependencies This is one of my hang ups, I like to be worried about them. I don't really like black box, especially when a problem arises. I want to be able to track something down, or even better, give the problem to a junior level programmer to track down. This may just be my opinion, but I find it easier to track down problems if the dependencies are more explicitly laid out. While using an attribute to chain responsibilities together is cool, I can see another programmer coming behind me easily missing that in a troubleshooting scenario. 
Thanks for clearing up the terminologies. I agree with your points to, glad I'm not the only one.
I am partially persuaded, but I think you need to test both of them separately before you can test them together; the ability to test one without the other is a sign of good coding practices. Do you test your controllers in isolation from the data access code they use? I do think people get a little cargo-cult in the way they think about isolated unit tests. It's such a beautiful abstracted principle that people don't always ask if it's *useful*, or if it's the best way to invest the scarce resource that is their time.
Historically speaking integration tests are about testing components written by different teams. The integration phase tends to be high risk as each team may have a different understanding of the agreed upon conventions. Integration tests, again historically speaking, aren't just "duhr, I called a database". But they could be "I called a stored proc which may or may not do what I asked the asshole of a DBA for". **** Unit tests test a unit of functionality, but the size of that unit is variable. Though the definition has changed to "testing more than one method is scary", there used to be an understanding that larger units were built up from smaller units and both need to be tested. *** At the end of the day I don't care about arbitrary definitions and categories, I care about writing good tests. And that usually means not dogmatically obsessing about single method tests to the exclusion of all others.
Integration tests and unit tests are different things. I prefer to do both. You'll never see me write a unit test that requires a DB.
My work heavily involves automation of network equipment and provisioning of virtual machines. we need to test for a variety of failure scenarios and can't afford to run unit tests on physical hardware. You are doing integration testing, which we also have, but without a solid DI architecture our testing would be impossible or incredibly cumbersome. We gain a tremendous amount by abstracting out almost every bit for testing.
&gt; In practice I've never seen a DI Framework take less effort than manually wiring up dependencies, but that's a personal choice. If your tests run automatically, like on a build server, it can be very helpful to run mocked-up unit tests alongside integration tests in the "live" environment. This helps you understand immediately, when a test fails, whether it's an environment issue, a data access layer issue, or some broken controller logic. That's how I see it, anyway. It takes maybe 20 minutes to throw an adapter around a data access class and set up some Moq code.
Now that I will agree with. I even will consider mocking a database if the database is owned by another team. It is the "I've got to mock the configuration class" shit that really pisses me off. 
(full disclosure, I'm talking about my own toolset) I wrote a fully-serviced IoC container after looking at what was out there and not liking either the API of other containers or having consistent problems with them. Our QA team at our company is able to use the container to override registrations in our codebase without the need of developer interference and without having to have their test code in our codebase. Basically, they reference DLLs from our code, write their own implementations, then drop their DLLs in the bin folder with a small .plugin file that the container reads and uses to override registrations. As long as they implement the proper interfaces, its good. This allows them to mock external calls and have greater control over the inputs and outputs in our system, and we don't ever run the risk of test code being deployed because its completely external to our build and deployment process. It's not even in the same solution/repository. Coupled with automation, its very powerful for validating lots and lots of use cases. Their implementation is data driven, so their automation sets up the configuration for their mock, runs the test, our system loads their mock and uses it in place of the real 3rd party integration, and validates all the results. It works for us very well, but it may not apply to everyone.
Generally no. I make my controllers so incredibly lightweight that there is no reason to test them without the database. In fact my goal is to make every controller method look like a copy and paste job, just with different method calls. I use rich model objects and rules engines to pull this off. They, not the controllers or database access layer, contain the vast majority of the business logic. To ensure they remain pure they are usually in a separate project so that they can't access the database even if they want to. 
Absolutely understand. Like I said, your example was spot on for the company I interviewed for. Thanks for responding though!
I use Assert.Inconclusive to indicate environment failures separate from test failures. That said, I'm not against using mocking the database if you also have database tests and you do it in a way that doesn't make it hard to understand the normal code paths. That means external mocks or tools like Microsoft Fakes that don't require creating otherwise unnecessary interfaces.
I'm not interested in dogma, either, but in practice it boils down to * Unit tests: Does my code work? * Integration tests: Does my system work as a whole? As you said, unit tests are for developers and integration tests are for testers, which could incidentally be the same in some cases. But a tester shouldn't ever have to worry about how the system works, but that it works correctly. They only have to know that when they enter x, y should be returned. I would argue that testers shouldn't be aware of how the system works at all from a technical perspective. Using your stored proc example, the value of DI is being able to test the code in isolation. So you'd test the result of your stored procedure in its own tests, then you never have to worry about them again. When you're trying to verify that your controller returns 10 results, you don't need to set up that proc again because the testing for it is done. You can mock that to give you 10 results, then focus on ensuring that the controller does what it's supposed to with them. So you're never duplicating anything. Otherwise you're spending most of your time and effort focused on the underlying data implementation instead of the controller's behavior, which is the thing you really care about. For a more concrete example, let's say any action on a system requires authentication. You could recreate the entire authentication process for every test, or you could mock a user authentication flag and accomplish the same goal. What you care about here is what the method does based on that flag, not how that flag was set. Your authentication tests would test the actual implementation of that property.
In the end that is what all of this is, opinion, I understand the sense of religiosity and my way is better than your way, but at the end of the day all that matters is the maintainability and utility of the code you generate. You can write great code without IoC/DI some of us just find that we feel we write better code with IoC/DI. So, IoC, for me, is simply because it made sense for me from a conceptual sense and I found it made my team's code cleaner. DI is just leveraging the power of IoC and the ease of use. With most frameworks I can wire up a DI container in less than 5 minutes and either never or very rarely have to touch it again and it just works. I almost never have to worry about newing up anything. I also have to disagree with /u/grauenwolf as I feel DI and IoC reduce the complexity and therefore the risk of your project becoming a big ball of mud. 
&gt;IoC and DI is just a means to an end to achieve things that you could do by hand, but would take lots of manual effort. &gt;&gt;Could you perhaps give an example, or link to a blog post that might illustrate this? All the examples I've seen look like extra steps to me. I use StructureMap a good bit. It has a convention over configuration approach where it automatically wires an implementation with a corresponding interface (eg. Calculator implementing ICalculator). Its not a ton of code to write and could potentially save you a ton of new operators [example gist](https://gist.github.com/alexcurtis/2867214#file-structuremapcontainer-cs) While I practice DI frequently (mostly because I test well with it), and work with DI frameworks on a regular basis, I agree that they aren't always beneficial. In fact, a DI framework managed poorly is the worst to work with. I've seen projects with references to multiple DI frameworks and other messy situations. I think it all boils down to KISS, depending on your situation.
In my experience you won't effectively get full test coverage without decoupling your classes. You tend to end up with either unit tests that test 60-70% of your code, or full coverage but at the cost of massive test classes that are hard to maintain.
Note that I didn't say DI and IoC were necessarily problems. I said IoC Containers, and the misuse of DI Frameworks, are a problem. Huge difference. 
&gt; When you're trying to verify that your controller returns 10 results Why do I want to do that? I can't think of any occasion where I've written a test that requires exactly ten items were returned by a controller. I have written tests where I ensure that at least one record is returned, if I know that such a record must exist. And I verify all of the fields on that record to ensure that nothing is null when it shouldn't be or has a value that I've filtered out. And that I don't receive two copies of the same record. But I don't check the number of records unless I've just inserted said records as part of the test. Doing so tightly couples my test to the contents of the database, making the test unnecessarily fragile. 
There's a baby and some bathwater here. Don't mistake one for the other. DI is a good idea. There's essentially no way to abide by the Single Responsibility Principle without it, because a class has to both perform its primary actions while locating and instantiating its dependencies. A class designed with DI in mind is unobjectionable in every way: it accepts everything it needs to live in its constructor. So far so good. The alternative, in which everything is riddled with singletons, is too grim to contemplate. But there's a problem here. If class A depends on class B, then in order to construct an instance of A you need an instance of B. This hasn't solved anything, it has just moved the problem to the caller. And of course, class B might have its own dependencies... Enter the DI container. A container can let you build a class without baking in knowledge of how to resolve all of those dependencies. There's a single point of registration, so these details aren't repeated throughout the code base. Every usage of class A doesn't involve a lot of irrelevant details about B, C, etc. Like anything, this can be misused, and there's a bad history of cargo-cult programming with DI containers. XML configuration was always a terrible idea, but you can choose libraries which provide fluent, conventional, or reflection-based registration. Most DI containers have too many features, which leads to drift within an organization. Something like SimpleInjector is fast and small, and has one right way to do everything. Ideally, the DI container is never used directly in your code. If you just have a static Unity container that you're resolving everything from, you're doing it wrong, as that's just a service locator. The container itself should mostly disappear, outside of a few root factories. There are two ways to avoid DI. Non-GC environments have to track ownership in ways that make RAII far more sensible, since no one can be truly oblivious about resource ownership. Functional languages often force extreme locality (i.e. a function sees no references beyond its arguments) and have more powerful module systems, and thus can achieve the goals of DI without requiring library support. I'd suggest reading through the SimpleInjector documentation, as it makes an eloquent case for the value of minimalistic DI in promoting SOLID principles. As does this [architectural discussion](http://www.cuttingedge.it/blogs/steven/pivot/entry.php?id=91).
You're talking about integration tests, everyone else is talking about unit tests. Unit tests are always isolating everything except the functionality they want to test, which makes refactoring safer because if you break a test you don't have to trace back the entire execution stack to determine which part you broke assumptions in.
If all your logic can be broken down in to data model rules then that's all fine and dandy, but you're going to have more edge cases than you know what to do with that need to be handled in code if you're dealing with more complex use cases, like end to end finance systems or anything to do with banks. You ever write and test your own serialization service?
Either that or that 40% is too much effort to test so one doesn't bother with it. Then when one refactors they unwittingly introduce a bug that goes undetected. Slightly OT now. But without full test coverage you cannot be truly fearless and ruthless about refactoring.
That was just an example, but it still sounds like you're trying to bundle everything up in one huge test. The key point is that there are no records returned. There is a data set that you create completely independently of any external data store. You know exactly what is in that data set. You know how many records there are. All you need to know is that your controller is doing the right thing with that data. Any validation/shaping/etc. on the data itself would be tested in its specific area. Again, your controller shouldn't care where the data came from, just that it's there and it's handled correctly. You can do something like this with Moq: Mock&lt;MyDataProvider&gt; mockProvider = new Mock&lt;MyDataProvider&gt;(); mockProvider.Setup(p =&gt; p.GetRecords()).Returns(new List&lt;MyDataRecord&gt;() { myDataRecord1, myDataRecord2, ... }); MyController controller = new MyController(mockProvider.Object); controller.DoSomething(); //Assert/verify controller functionality mockProvider.Verify(p =&gt; p.GetRecords(), Times.Once()); So basically you've just replaced an enormous amount of tightly-coupled setup code with a more semantic, flexible alternative. 
Nonsense. The first thing I do when adopting an old code base is ruthlessly refactor it. Usually don't bother writing automated tests until I get it into a condition I'm happy with. 
&gt; You ever write and test your own serialization service? Often. I spent five years working in the financial sector and even built my own automated trading engine for the bond market. One of the things I learned is that integration tests are by far the most important thing you are going to have to deal with. I didn't even bother writing unit tests for the trading engine because it wouldn't have helped. All of the serious bugs I had to deal with were only visible when interacting with ATEs at the other companies. 
Integration tests can take an incredibly long time to run (hours and hours for semi-large systems). You can't honestly say that is a feasible way to test code, can you? I see your points in this argument but you're simply wrong. You must be used to working with tiny systems or untested systems.
If you have to trace through the execution stack to determine which assumption was broken then you haven't properly encapsulated your components. While the use of dependency injection doesn't have to lead to poorly encapsulated components, they often go hand in hand. So regardless of everything else we're talking about, make sure your code is self-validating. That means not doing things like exposing a required dependency as a settable property. Make it part of the constructor so that if it is missing you find out immediately. Likewise ensure that every parameter and property has appropriate null and range checks. Don't let the bad data in and you don't have to worry about what happens when it comes back out.
You can't honestly say that you are willing to skip over important tests, and possibly risks thousands if not millions of dollars, because you didn't want to run a test over night? I understand that novice developers need that instant feed back that comes from running their tests every 30 seconds. But when you've got ten to twenty years of experience you should be measuring your time between errors in hours or even days. But perhaps I'm a bit biased because I worked in an industry where live testing was the norm and mistakes had real world consequences. 
So how do you know that you haven't inadvertently changed the existing behaviour? Or do you simply never make mistakes? Or - once you've written some code, how do you refactor it to take into account new requirements if you don't have full test coverage?
&gt; So how do you know that you haven't inadvertently changed the existing behaviour? I don't, but I don't know the existing behavior was correct either. I do, however, heavily rely on bench testing, manual testing, compiler type checks, and static code analysis. &gt; Or do you simply never make mistakes? While I certainly do make mistakes, when it comes to refactoring they tend to be very rare and very obvious. If I don't refactor the code to make it more understandable and more testable first then I tend to make a lot of mistakes that cost a lot of time. That's why I'm such a fan of early and aggressive refactoring. *** Something to keep in mind is that I rarely get put on projects with a solid code base. My forte is dealing with code that has already become an utter mess.
This is pretty much the major reason right here. There are others, but unit testing is the big advantage. Loose coupling is useful with specific things (e.g. unit testing), but the way it's always demonstrated ("You can switch out components for other components!") almost never happens (e.g. switching put ORM, logging framework, etc.). Well, I've done it on several projects as components like PDF libraries and such had to be removed, but I guess it doesn't happen to most people just maintaining legacy code. However, there are other things DI is good for. For example, I have a library that I forked and maintain that I'm currently in the process of rewriting several components to allow for injecting the configuration for the library. The big boon here is, again, unit testing, but also for reuse by other individuals. Before everything called a singleton to get the config from the app.config. Now it get's passed into components from the entry points instead and it's much cleaner. Note this has nothing to do with IoC containers, it's just bare DI. I've also kind of abused IoC containers for wiring up messaging services before (e.g. SomeMessage get's autowired in the MessagePublisher by asking the IoC container for all MessageHandler&lt;SomeMessage&gt;). In the end most IoC containers are just service locators. Getting real familiar with SOLID would be a good starting point for why one would want to do this. As for the unit testing religious war that grauenwolf frequently starts in such threads (*ducks*), I disagree that integration tests are the only meaningful test, but do agree most unit tests are useless as they usually end up just testing language features and glue code. Very few of us are really writing algorithms or processing code that unit testing would be good for. However, I absolutely *love* them for regression tests, bug tests, etc.
We went through a review of most of the top contenders two years ago. 1. Sitefinity - Our winner because the content authors are non-technicals. By far the best in terms of user experience that we looked at. Development isn't BAD and there are lots of extension points, but at times it does seem over engineered (Typical of Telerik). 2. Orchard - Speaks to me on a developer level. All the new development toys included (MVC, modular, data structure is extremely extensible, etc.) but not really "web site" friendly, more content friendly. Still, on my short list of things I want to be using. 3. Umbraco - We looked at the doomed 5.x line where they tried to go MVC and failed apparently? Fairly well regarded, though pretty bare bones when we tried it. 4. DNN - Oh god the humanity. Developed sites on it for three years. Hated it. 5. Sitecore - Expensive, but well regarded. Looked more complicated even from an admin perspective than we wanted to put our content authors through. 6. Kinetico - I forget why, but this was knocked off our list very quickly... 7. N2CMS - It's more of a framework for adding content editing on top of an existing site... developer friendly, but sort of requires you to build everything except the basics needed for your own CMS... My top pick would be Orchard for free stuff... admittedly because it's designed surprisingly close to the way I would have done it. It is also one of the only MVC based CMS's out there that is fairly well designed... but it's definitely still in the pre-UI optimization stages. The core is great, but there's not much there interface wise that is easy to work with. I just wish they would focus on that for one of these releases... If the people editing your site don't know HTML but want the most power laying things out as they go, I really think Sitefinity is the best here interface-wise, which is why we chose it. Of course, it's commercial... also Telerik's support is actually really good surprisingly with developer questions... I think Umbraco or Kinetico are probably easier interface wise, but I don't agree with some of the engineering decisions they made for data storage, etc.
Read "Working effectively with legacy code" by Michael C. Feathers.
&gt; In my experience the use of an IoC Container results in applications quickly degenerating into a big ball of mud. My god, 1000 times this. My current codebase suffers from going with Unity to make things more testable but what we've ended up with is just people resolving dependencies willy nilly and completely ignoring every tons of principles of programming just because the IOC container lets them. I loved it for a bit but at this point if I get the chance to rearchitect things, DI will be handled manually and Unity will be on its way out. Note that there's nothing wrong with Ioc Containers inherently, it's just that in practice they give devs too much flexibilty. The whole point of them is that they enable you to decouple what you do but all the sudden now to use a library you need to bootstrap like 1000 interfaces to use the simplest api.
With regards to managing object 'life cycle' - e.g. choosing whether an object is Singleton, transient, per web request etc., I agree this is an important and sometimes overlooked advantage of using an IoC container.
I'm curious what you don't care for as far as the Umbraco db model? Unless you were referring to their caching scheme. I've never dug deeply into the Umbraco model, but I do favor abstraction which I believe they achieve with their concept of a "document type" along with the extendable "value types"
I'll bite - why would I want to use a DI/IoC container when I could just use a mocking framework?
It was heavily XML based from what I remember. That wasn't going to fly with non technicals.
Using a container (or to give the more accurate name: Service Locator) is not DI nor IoC. It is certainly related. DI/IoC just means don't construct your own dependencies, simply demand them as parameters (in constructor or method, whichever is appropriate) As evidenced by this thread, too many get this distinction confused. 
Using a container != DI. DI means demand dependencies, don't construct them. There's nothing about DI that mandates the use of a container. The container does nothing in your scenario that a Factory wouldn't fix with a fraction of the configuration/code. With a service locator, you'd still have architecture woes only they will remain hidden until runtime.
You'd have to use a mocking framework (or fake classes) in either scenario. The container adds nothing to this.
That's got nothing to do with using a container, only using mocks.
This is what I was looking for, thanks!
I am sorry, I guess I disagree at even the fundamental level of your argument. DI is intrinsically tied to having a container. Demanding dependencies, is itself IoC, which is separate from DI. When you have a class that accepts a limited number of contracts, essentially demanding its dependencies, that in and of itself is simply IoC. However, when you have an object graph that is composed entirely by a container and the dependencies are injected as needed by said container only then do you have DI. Also, you're right to a point, that the answer to not using a DI container while still utilizing IoC is that the class accepts(read: demands) factories that compose the dependencies. Factories are still an often used pattern even when using DI containers. However, without DI, this stands the potential to become unwieldy especially in the hands of less experienced developers because at the end of the day you're still responsible for the creation of the object graph. Often you will ultimately see an inordinate number of factory classes to support the graph creation when all of that could have been abstracted away in a container. This is where my argument that DI often lends itself to cleaner code that adheres more strictly to SRP ultimately lies. 
Using a factory is (also) DI. You do not **need** a container like StructureMap or Castle.Windsor to have DI. Abstracting it away doesn't happen, you just move the logic of construction about, it never goes away. It moves further away from the stuff that uses it, adding confusion when you revisit in the future. Keeping construction separate from usage is good, but not at the "distance" that containers keep them. Finding usages/dependencies becomes that little bit more complex with no payoff. RE: Factories, I'm not advocating the factories be injected - I'm arguing the factory is used to create your dependency that is to be injected. Subtle difference, but significant IMO.
A DI container is completely different from a mocking framework. Mocking frameworks are used in *unit tests* to *make fake proxy classes that implement the inherent interface of the dependency*. A DI container is used by the running application as *a repository that knows how to build instances of classes*. Essentially, a service locator. A mocking framework *makes a fake object* when asked for one. A DI container *makes a real instance of an object* when asked for one. Just different contexts of usage (mocking frameworks for unit tests, DI containers for applications).
+1 to SimpleInjector. Great library with fantastic documentation and a very responsive developer that seems to really know his stuff. Very fast and works on Mono, too. 
I've swapped out components in the past (swapped the Jurassic JavaScript for the IE one, Chakra). 
Are you still looking? Check out nVLC. 
I feel like a code example would be useful here, lets start with what code looks like with static references: public class FooController { public FooViewModel GetView() { var data = FooRepository.GetData(); var output = new FooViewModel(); - Do some processing on data here and fill output object - return output; } } public static class FooRepository { public static FooData GetData() { - Get data from database - return data; } } static void main() { var controller = new FooController(); var view = controller.GetView(); } Pretty quick to write but it pevents you from doing certain things easily; How do you unit test the logic that GetView() applies to the data? All you can do is a full-stack (integration) test which requires reading data from an actual database. What if the database you use on UAT/Production systems isn't suitable for use as single instances on developers computers? There's no easy way to switch databases for different environments. If you do it using a DI pattern you end up with this: public class FooController : IFooController { public FooController(IFooRepository fooRepository) { this.FooRepository = fooRepository; } public FooViewModel GetView() { var data = this.FooRepository.GetData(); var output = new FooViewModel(); - Do some processing on data here and fill output object - return output; } } public class FooDatabaseRepository : IFooRepository { public FooData GetData() { - Get data from database - return data; } } static void main() { var controller = IoC.Resolve&lt;IFooController&gt;(); // The IoC container will fill in the constructors automatically var view = controller.GetView(); } You need to create more interfaces, but unit testing is as simple as: public class FooControllerTests { public void TestGetView() { // Arrange var fooRepository = new MockFooRepository(); // Mock class that returns hardcoded values var subject = new FooController(fooRepository); // Act var view = subject.GetView(); // Assert Assert.AreEqual(view.SomeValue, "Expected Value"); Assert.AreEqual(view.AnotherValue, "Etc"); } } Likewise if you use different storage systems on local developer computers you can have several implementations of IFooRepository and configure different environments to use different classes
You need to be using DI to be able to use a mocking framework properly (this is separate from an IoC container) This is how you normally use mocking frameworks: public void TestFooController_GetView() { // Arrange var mockFooRepository = new Mock&lt;IFooRepository&gt;(); mockFooRepository.Setup(x =&gt; x.SomeMethod).Returns("Expected Value"); var mockFooService = new Mock&lt;IFooService&gt;(); mockFooService.Setup(x =&gt; x.AnotherMethod).Returns("Etc"); // All the service classes that FooController uses are passed in the constructor // This is what makes it DI var subject = new FooController(mockFooRepository.Object, mockFooService.Object); // Act var view = subject.GetView(); // Assert Assert.AreEqual(view.SomeValue, "Expected Value"); Assert.AreEqual(view.AnotherValue, "Etc"); } If you have any static class references in FooController then you can't mock them, unless the mocking framework you use has some nasty hacks to override static references. Of course even then if you add a new static reference the mocking framework won't know they're there and the test will run extra live code instead of a mock. With DI if you add a new reference the unit test won't compile until you've mocked this too. What an IoC container does is (if you're doing it right) simply fill in the constructors of all the service classes when you reference them, eg. IoC.Resolve&lt;IFooController&gt;(); would fill in the constructor with instances of the correct classes (and if those classes themselves have references they would be filled in recursively).
It's not exclusive. They did expose the content tree so that you could use XPath, but that was one of I believe 3 different completely separate ways of navigating the CMS programatically. 
Works for amd. Com.. 
The old xml/xslt way of templating content was never for non-technicals, that was for the developers when implementing the site. Razor is now the preferred templating method now however.
I'd suggest paying for a month of the training videos over at umbraco.tv. I'm fairly new to umbraco and they helped me to get up and going quickly
If you're resolving dependencies in your code rather than having them bootstrapped, you're not using the container properly.
It's all set up around constructor based injection if that's what you're attempting to get at. My bigger concern is that people end up doing lazy things like introducing dependencies on HttpContextBase in a DAL because Unity handles that for them.
Gotta do some MEF first. 
Maybe I am not understanding something here but if you're accessing things like the HttpContext in the DAL, I'd be a little concerned as concerns are being blurred.
ITT: People offering up scenarios to use their crappy IoC frameworks that ain't ever really gonna be needed. 
Looks interesting and useful. My question with any VS extension/add-in is: does it get along with other add-ins like ReSharper? 
ILDASM is a standalone application that examines the CIL assembly, it doesn't plug into VS
What you describe is way cool but OP is confused about the "blank line" part I think.
Is that option new? If not maybe it is default enabled to be backwards compatible with earlier VS versions.
Oops. I'd meant to reply to a [different programming thread](http://www.reddit.com/r/csharp/comments/1u801p/codemaid_an_open_source_visual_studio_extension/). My comment above probably doesn't make sense in this context.
I just opened VS 2008, and the exact same option is there. Still Checked. I don't have anything older though.
This is a misleading article. You say to never use an async method call without awaiting it. Often times that's exactly what you want to do. Take the following for example: var input = new List&lt;int&gt;{1,2,3,4,5,6,7,8,9,10}; var tasks = new List&lt;Task&lt;int&gt;&gt;(); foreach (var i in input) { tasks.Add(DoSomethingAsync(i)); } await Tasks.WhenAll(tasks); //Now I can check my tasks for exceptions Making a blanket statement that they should always be awaited is not right. edit: Also, AFAIK, async void is not a valid signature. If you are not returning anything your signature would be: async Task DoSomething (){ ... }
async void is a valid signature. See: http://www.jaylee.org/post/2012/07/08/c-sharp-async-tips-and-tricks-part-2-async-void.aspx. They're meant to be used for event handlers where you'd have code that uses await but doesn't need to be waited on. It's the equivalent of calling Task.Run without awaiting it. 
The web filter at my work blocked the site completely, stating: "This Websense category is filtered: Potentially Damaging Content. Sites in this category may pose a security threat to network resources or private information, and are blocked by your organization." Be careful.
I use shift+delete to delete whole lines I don't need. I found out recently that this option is what allows that. I also found out that when I shift+delete a line, it copies it to the clipboard (not always what I want). 
That is absolutely the problem. It's much easier to enforce when dependencies have to be passed around, however with a unity container people do stupid stuff as shortcuts because they can and accrue tech debt at a faster rate than before. IOC Containers make it easy to write testable code but on the other side of that they also make it extraordinarily easy to write very bad code. For this reason I'd shy away from using an IOC Container in the future even though it does give me a lot of wins.
Yeah, it's probably fine. Websence is pretty shit, just thought it best to warn people. Looking at the site on my phone it does look very..... Minimalist. Which is cool. 
My guess is that some people like to delete blank lines with ctrl-x.
It's there in VS 2005. I just unchecked it. 
The completion code for a task generally asks the currently active synchronization context to schedule the follow up code. The default sync context calls (Unsafe)QueueWorkItem. When he code was called from e.g. Win forms or WPF, follow up code is posted to a central event loop.
Is there an article somewhere on what async/await unrolls into in IL? Seeing the actual source there would help to visualize what's going on.
You may need to give the DefaultAppPool entity modify access to your website folder in wwwroot. The old method of giving the Network Services entity the required access was removed in IIS 7.5 or there abouts. 
Nice :), any complete list with Resharper as well? ;)
Well yea, shift-delete is the universal Cut command in Windows.
I don't know the actual reason for the option, but my wild guess is that some people disable it so they can quickly delete a block of code and have it stop at the next blank line. I use a lot of blank lines, so this wouldn't be very useful...
Thanks I will check this out. I tried running it in a server 2008 R2 with IIS 7 server and it had the same behavior. Is there something I need to do to get the MVC routes to get recognized? if I enable directory browsing it shows the contents of the directories...so I think I am not getting the MVC portion working correctly.
True, it does the same thing in Visual Studio, just not universally in Windows.
No. This should just work. 
Aspnet_regiis - i
he ment thst ctrl-x and shift-del is the same thing in windows. Therefore VS will interpret it in the same way
This will most definitely be a permissions issue on the folder. Try adding access to IUSR.
Good idea and thanks for the effort, but you write asif you are already bored with the subject, and assume your readership to be too clever. Case in point - For Java EE, you write "enterprisy stuff". And you roll your eyes together with the reader at how lame this is, instead of calmly explaining what you mean, and if I need it or not. Then it's not clear if JavaFX is a just a library or another flavor of JDK. If you are explaining something that you identify as (and I agree is) inherently ambiguous - try to be more clear. 
WPF and [demo](http://blogs.msdn.com/b/text/archive/2006/06/14/631136.aspx)
You can access the post data by using: var data = Request.Params("data");
Often DI smells like speculative generality. Be careful to use it only when the opportunity for change realistically exists. Otherwise its another over engineering technique. 
Just a sanity check, you are using the published version of the site right? Not just copying whats in your Visual Studio directory?
Handy post. I've written a fair amount of Java back 8 years ago but I found it counter-intuitive compared to .net. For instance, identical strings are not overloaded to equality. They'd have to be the same instance, if you want to compare them, you'd have to call some static method. I think a good programming framework makes the most commonly used scenario the easiest to access (read ==). Because really, how many times do we care that string1 is the same instance as string2 vs checking for identical values?
I think the new theme looks great. I like that the other MS subreddits share the same look and feel. Surface and Phone subreddits should do the same.
I've upgraded recently because of the offline-viewing capabilties. However, after upgrading I found the source code also very helpfull. So yes, I can recommend it, of course, if you're a heavy user of PS. I'm watching at least one course per week. 
When I had the plus subscription (I have the basic subscription now), I never used the exercise and assessment features. However, I did love the feature of being able to download video files and watch them offline. I was able to watch videos whist commuting. When my annual basic subscription expires I will go back to using the plus subscription.
Wait. So Razor code lacks intellisense by default in Visual Studio 2013? This kind of stuff is why I don't upgrade to new technologies very quickly anymore.
No. The person is trying to put MVC views into a class library project. Not sure why. 
Ah. So it's more like, things don't work the way you hope if you try to use them in a way other than intended.
It's a good way to reuse components. At my previous workplace we had a few separate websites that shared a lot of core infrastructure. With web forms, we'd share components the same way too (put them in a class library). 
You're fraying into a religious debate. Really, from my perspective, you're tightly coupling your IoC container to your code which is a code smell. It is acceptable to wrap up your dependencies within a few factories that call the containers resolve method decoupling your code while still adhering to good design patterns. My team has a loose rule of thumb that if there are more than 3 dependencies required that we wrap those up into a factory. This keeps the code quite clean and maintainable and you avoid having to work with a container in your unit tests, which I would also argue is another code smell.
[StackOverflow]( http://stackoverflow.com/questions/557742/dependency-injection-vs-factory-pattern ) That will cover a lot of the basics... A DI container is essentially a generic configurable factory... Or service locator. They are kind of complementary. My argument would be constructor injection plus DI framework is simpler than maintaining a suite of factories.
Why is using an IoC container in a unit test "code smell"?
A ctor is a contract on what is required to use the class. A class should (almost) never go out and grab what it wants. It should be told. If someone has to read your code to figure out what dependencies are needed, and if you change them and do not get compilation errors informing you that the classes "contract" has changed, then you'll run into problems. IMO, ctor params are the way to go, falling back on other methodologies for stuff like attributes and other un-injectable things. 
Because the whole point of unit testing is isolating a small chunk of code from all of the rest of the code and making assertions about that chunk. When you introduce an IoC container into the mix you are by its very nature testing more than a single chunk of code, you're by proxy testing that the IoC container works as well with every unit test. You're using StructureMap based on your syntax, I believe, which is what I use as well, great framework and very well tested. Your chances of it causing some quirky issue is minute which is why I only consider it a smell rather than something that should never be done but it is non-zero which breaks the spirit of unit testing from a TDD purist perspective(which I am not a purist).
well... it is used as intended. but i give you that it is an edge case and therefore the mvc dev probably did not take it into account.
There are a couple of scenarios where dependencies can be legitimately passed as a parameter, but they're generally to do with unit-of-work handling. eg: Request-Specific caches, database or transaction contexts, etc. The calling code though shouldn't need to know how to call your IoC framework to get an instance though - it should be calling some factory that returns the instance. Some IoC frameworks do this for you automatically. The rationale is, like /u/joshlrogers says - you don't want to be testing your IoC framework. Passing dependencies via a method to reduce the number of dependencies in the constructor is a pretty good indicator that you need to reduce the amount of work that class is doing. 
This statement seems to contradict IoC and single-responsibility. Defining the container factory rules then relying on the container to resolve the instances as needed seems cleaner than forcing all callers to know about the dependencies the called method needs.
Maybe, but there are a handful of assumptions about referenced assemblies from the testing assembly itself to the mocking framework. Say I had an integration test that exercised my EF or WCF connections; I'd have a lot of trust on external assemblies that I'm not asserting against.
I'll agree with you to an extent, tightly coupling your IoC container to your unit tests makes swapping it out in the future a giant pain in the ass. *However*, I don't think you should be manually initializing the container in any of your test cases, there should be a facade around your IoC container that provides the functionality your application needs so that all you need to do is modify it once and both your application and test suite use the new container. Personally, any application I've used DI with has used the same IoC setup with my unit tests, whether that's been CDI with Java EE or MEF with my Caliburn.Micro apps, there's no reason to not take advantage of it if you already use it.
What is the benefit you get from calling GetInstance everywhere? E.g. public void Constructor(IFoo foo, IBar bar) { _foo = foo; _bar = bar } vs: public void Constructor() { _foo = ObjectFactory.GetInstance&lt;IFoo&gt;(); _bar = ObjectFactory.GetInstance&lt;IBar&gt;(); } E.g. in ASP.NET MVC/WebApi application you only need ControllerFactory (or similar) that creates the Controller using GetInstance method. I have never run into a situation where I have to call it in other places. One problem (that I have seen myself before) you might get is a case where you have configured some object as singleton but you need to get a new instance of it for some special case. If you are calling GetInstance yourself then you probably need some if-statement into the class. This if-statement will most likely affect lot of unit tests. The end result is that your DI logic has leaked into your code. Instead of your code happily using the objects it gets it now needs to know the lifecycle of those objects. **tl;dr: Calling GetInstance() manually everywhere will probably work just fine (I have seen lot of projects to that) but once you get your first exception to the rule (usually related to scope/lifetime of the specific class) things get messy.** 
If I understand you correctly you're debating constructor injection vs service location? There's a good criticism of service location [here](http://blog.ploeh.dk/2010/02/03/ServiceLocatorisanAnti-Pattern). Imagine if I write a class that uses a service locator. Now you also want to use this class but you have the following potential problems: 1. You have no way of knowing what dependencies my class has without reading through my code. 2. If a dependency isn't registered the compiler won't warn you so you're going to get run-time errors (the worst kind) instead of compile-time errors. 3. If I resolve lazily the lack of a dependency might only cause errors under specific conditions. You could end up with live code that seemingly fails at random.
Is it a licensing issue?
No, just a matter of gathering experiences. Edit: Options are good. Read your comment out of context.
It's not bad all the time, but you should try to minimize explicit use of any library, including IoC containers. I have experience from a team that did the same thing. The biggest issue with this is that there is no simple way to know what dependencies a class or method has except reviewing the code, and depending on how it is written, you may still encounter weirdness. On one occasion, it turned out that the SMS library was not properly mocked, and a proper gateway was defined for the unit-test envronment for some reason. Since the test auto-generated phone numbers there would now and then occur that someone who wasn't even a client would call support saying they recieved confirmation of a name change or whatever the test did. This was due to their IoC pattern, coding standards and how they handled mocking. The default would be a working implementation. So if you missed a dependencies you could get weird timeout errors or the result would be that a client didn't exist or whatever. Of course I'm not claiming you write code like this, but my issue was not always knowing what dependencies a method would require. Method arguments or constructor dependency injection is easier to handle in my experience. I would also claim that using the IoC container inside the callee is pseudo inversion of control at best since you delegate the dependency resolution to another dependency.
It's not contradicting Inversion of Control. IoC is enforcing what class does by telling it how to behave. It doesn't really make any difference if you're injecting the raw implementation or factories which generate them, you're still enforcing behaviour. If you do inject factories however and within them you are doing stuff like Container.GetInstance&lt;T&gt;() then you are tying your codebase to your container which is a code smell. There is a perpetual debate as to how much that matters with a IoC container as you don't often swap out your container but a smell it remains nonetheless.
No worries, we have lots of developers using VS and have no probs, I quite like it. But do let me know if you find an alternate you prefer. Nice to have options!
Exactly this. I think people's first instincts when learning about IoC containers are that they are to be used as some magical service locator. It's easier to think of it more as a framework to facilitate constructor injection. This article explains the way that me and my team make use of these principles -http://blog.ploeh.dk/2010/09/29/TheRegisterResolveReleasepattern/ As others have said, you should'nt be using IoC containers in your unit tests, but I would almost always have integration tests to verify that all the dependencies have been registered correctly and that you can resolve your very top level component.
Check you 4.5 framework installed. Create an application pool for this site specifically assigning it the 4.5 framework, try also to test between integrated and classic pipelines. Assign the app pool to the website. Hopefully this may help.
Thank you. I am wondering if there is a good site for Webforms developers like myself who can ask, "this is how I do it with webforms. How the hell can I do this in MVC"? GridView is a big thing for me. I need to find out quick how to make it happen in MVC.
To me, the IoC container is a tool I use along with a mock util (Moq, Rhino). I consider it no different than if I were resolving classes under test with my own rolled Factory. I have varying degrees of strict "Unit" coverage. But I can assert on my IoC'ed operations disconnected. 
Razor is a view engine, not an MVC view engine. 
I just got this forwarded.. thought it might be of some interest to people here: &gt;MembershipReboot is a user identity management and authentication library. It has nothing to do with the ASP.NET Membership Provider, but was inspired by it due to frustrations with the built-in ASP.NET Membership system. The goals are to improve upon and provide missing features from ASP.NET Membership. It is designed to encapsulate the important security logic while leaving most of the other aspects of account management either configurable or extensible for application developers to customize as needed.
So I decided to setup my test web server (Windows 2008 Server) as an SMTP server and to relay all mail to our real SMTP server. It sends in a couple of seconds for multiple recipients instead of 10+ for just 2 recipients. I guess a variety of reasons could cause this (different VLAN for my dev box -&gt; SMTP) but who knows
So I decided to setup my test web server (Windows 2008 Server) as an SMTP server and to relay all mail to our real SMTP server. It sends in a couple of seconds for multiple recipients instead of 10+ for just 2 recipients. I guess a variety of reasons could cause this (different VLAN for my dev box -&gt; SMTP) but who knows
So I decided to setup my test web server (Windows 2008 Server) as an SMTP server and to relay all mail to our real SMTP server. It sends in a couple of seconds for multiple recipients instead of 10+ for just 2 recipients. I guess a variety of reasons could cause this (different VLAN for my dev box -&gt; SMTP) but who knows
You can take a look to angularjs and firebase. You can also use WebAPI with your app.
If I understand your solution structure correctly, you can do it similarly without the `#if` blocks if you wish. Instead, declare the `CreatePlatformObject()` method as a [`partial`](http://msdn.microsoft.com/en-us/library/6b0scde8.aspx) method with no implementation (and the `PlatformFetcher` as `abstract` as well, naturally). The platform specific (or dummy) projects, can now link the file _and_ add their own second `partial PlatformFetcher` file which implements the `CreatePlatformObject()` method returning the platform-specific class. So essentially, in your `PCLTestLibrary` assembly you might have: public partial class PlatformFetcher { public IBaseFunctionality FetchPlatformUnique { get { var r = CreatePlatformObject(); if (r == null) throw new NotImplementedException( "The platform version of the PCLTest library was not linked in!"); return r; } } static partial IBaseFunctionality CreatePlatformObject(); } Then in your `PCLTestDesktopLibrary`, link the above file, and add a new file/class: public partial class PlatformFetcher { static partial IBaseFunctionality CreatePlatformObject() { return new PCLTestDesktopLibrary.DesktopBaseFunctionality(); } } This has three key benefits: 1. ~~Forces a compile-time error for a platform where a developer forgot to include an implementation (even if that implementation essentially throws a `NotSupportedException`) rather than waiting for runtime for it to blow up.~~ (EDIT: Nope, I'm wrong on this, if an implementation isn't supplied then it would be `null` by default assuming you use the `ref` workaround I mentioned in my below edit) 2. No longer requires the compiler directives (which is nice for the custom "FILE_SYSTEM" one you had to add manually) 3. Removes the monolithic `#if/#elif#endif` block which essentially has knowledge (even if it's `#if`'d out) about how each platform builds/constructs/serves its specific implementation. EDIT: Disregard me. I forgot `partial` methods _require_ it be a `void` return type. Only way around this is to change the signature to use a `ref` variable to be assigned which might be a bit awkward, but doable. I'll update this later with a _working_ implementation, but right now my lunch break is ending at work. :)
Do you have a lot of items in it? You might want to look into virtualization with the itemscontrol. Posting from my phone - so apologize for the brevity! When I get to my computer I can post more info if you're still stuck!
As someone who is a big fan of ServiceStack, this looks very interesting. There are some actual benchmarks in another post on his blog: http://forcedtoadmin.blogspot.com/2013/12/servicestack-performance-in-mono-p3.html
Is this in a win store app? Wpf? Silverlight?
WPF. I'll throw together a cut down example later. Windows Store app ListView touch panning seemed fine. But the WPF ListView sucks. I went to an ItemsControl in a ScrollViewer, which I'm sure is little different, but it behaved exactly the same way.
I use the Surface 2 runtimes in my project, but I have to register some of the DLLs because I don't get an opportunity to have them install the runtime as my application is sort of "enduserless". I hate to tell you this but I don't think MS has *any* intention of improving WPF now that they've moved on. Lucky to get any bug fixes, even. I wish they'd fix the MediaElement (it's so unstable! memory leaks, unrecoverable if the display driver crashes) but that isn't going to happen. They are almost forcing my hand to move to a Windows Store App because those controls are just *so* much better. As far as your issue you described.. well, I had that problem too. I wrote a virtual keyboard that suited my needs. 
CRUD? Don't you mean PGPD (Put, Get, Post, Delete)? :p
Do you have the "Copy to output directory" property on your generic files set to "Copy if newer"? That should help them get into the build pipeline.
No. Actually the project doesn't even compile if referenced by default. I had to make changes to make it build in the first place.
Are you using TFS?
It depresses me when I see people continuing to use WCF for REST when the community is racing forward with much better tools like [webmachine](https://github.com/basho/webmachine/wiki). It is time to move on and modernize!
Just dump your "generic" files into a directory above the project directory called, say, my_files. Then in the .csproj files of the projects you want to copy the generic files in, add this at the bottom: &lt;ItemGroup&gt; &lt;MyStuff Include="..\my_files\**\*" /&gt; &lt;/ItemGroup&gt; &lt;Target Name="CopyMyStuff"&gt; &lt;Copy SourceFiles="@(MyStuff)" DestinationFolder="$(TargetDir)" /&gt; &lt;/Target&gt; &lt;PropertyGroup&gt; &lt;BuildDependsOn&gt; $(BuildDependsOn); CopyMyStuff &lt;/BuildDependsOn&gt; &lt;/PropertyGroup&gt; That will copy all files in the my_stuff directory in the directory above the project directory to the build output. It will not preserve directory structure inside my_stuff. That is, if you have my_stuff/another_dir/foo.txt, it will get copied to bin/Debug/foo.txt instead of bin/Debug/another_dir/foo.txt. You can do that with the Microsoft toolset with &lt;Copy SourceFiles="@(MyStuff)" DestinationFiles="@(MyStuff-&gt;'$(TargetDir)%(RecursiveDir)%(Filename)%(Extension)')" /&gt; but Mono's xbuild lacks support for many MSBuild features so that doesn't work.
Typically I will use the AfterBuild target to handle a copy operation like this. This way I don't have to include all the files in the project if I don't want to, I can even pull files from outside the solution if desired. More info on [Overriding Build Targets](http://msdn.microsoft.com/en-us/library/ms366724.aspx)
Doubtful, if he's using MonoDevelop.
Uhhh
Actually the last example works, and MonoDevelop uses mdtools and not xbuild (yet). I know this is an option, I was just wondering if I can get the target directory without changing the referencing project, just the referenced one. I know this is not necessary for anything, it's just how I'd like to see it done. Thanks anyways :)
I'd look for any i5 processor and 8gb of RAM. SSD would be ideal.
Honestly, a laptop is a laptop is a laptop. Start with one that feels right ergonomically, then look at specs. That being said a few specifics: * Pixels are more important than screen size IMHO. I can't picture developing at less than 1920x1080 for extended periods of time. That probably means a big laptop that would be clumsy to use on a plane, bus or train. I use such a big laptop on a plane and I fly coach, and to take the Hudson Bergen Light Rail. * If you care about parralization, you probably want to optomize for cores and get an i7. If not, the i5s are ok. Personally I spend a great deal of time parallizing all my code. * 4 gigs of ram is enough for Visual Studio and SQL Server. I think 8 gigs will make you comfortable. Tune max memory, etc on SQL Server to be low. If you are going to need to run multiple db servers at once (e.g two sql server instances, or sql and mongodb, your going to want more ram. * It sounds like you don't have room for a second monitor. That's unfortunate, and increases the need for a large monitor. You might want to look at [fences](http://www.stardock.com/products/fences/) (which I don't use personally)
The Amish look at technology on a case by case basis to decide if it would bring the community closer together or not. For instance you can get Amish power tools (they use compressed air). You can actually buy "Amish approved" computers also. They're battery powered and about as powerful as something from the 90s, but they work. Actually a lot of the tech they adopt/convert is pretty cool.
A basic development machine would be about the same specs as the orignal surface pro: * Core i5 processor * 4gb ram * 128gb storage SSD If you want to be able to stretch your legs and have better performance all you really need to do is add another 4gb of ram to that total. Now if you are looking to do multi-tier development (say SQL server, with a web app and maybe some backend services) then you will want to beef that ram up to about 12-16gb and push the SSD to 256gb. The next performance bump to the system would be to get a Corei7 processor for faster compile and loading times. For mega work, where you need to simulate multiple servers in hyper-v maybe do some cross platform development between say Java and .NET and simulating an integration backend for testing then you will want to beef up more. If you want to be able to do everything and the kitchen sink with room to spare and with ultra high performance you may want to copy the specs of my biggest dev box: * Core i7 Processor * 32gb Ram * 2 256gb SSDs in RAID0 configuration 
I use HP folio 13
I would say use windows 7, 4gb ram, 500Gb hard disk. Dual core @ 2ghz as a minimum.
While I agree with everyone about memory (.net applications are greedy) I think it's important to note that VS is a 32 bit app so there's an upper limit around how much it will ever be able to allocate. 4 gigs is probably fine, assuming you're not building applications with millions of lines of code I don't think you'll really have an issue with anything you pick up for a stock setup. Once you start usings some VS addins like resharper though you may run into some performance concerns. My advice would be to bootcamp a macbook pro, at that point you'd have the ability to develop pretty much anything at your leisure. 
Whatever you get. Make sure it has more then 4GB of RAM. The only applications I have open right now are VS2013, Chrome, and Powerpoint and my computer is idling at using 4.8GB of ram. If I run SQL Server express it jumps to 5.3GB. Get 6GB minimum, but if you can get a laptop with 8GB, there are plenty that have it. I recommend an Ultrabook with an i5/i7 and 8GB of RAM. Long battery life, SSDs, small profile, and they can pack quite a punch.
4GB of ram just doesn't cut it. I have VS2013 open with Chrome and Powerpoint and I'm sitting at 4.8GB/6GB in use on my pc.
4GB of ram just doesn't cut it. I have VS2013 open with Chrome and Powerpoint and I'm sitting at 4.8GB/6GB in use on my pc. 
Just a year ago, at the ripe age of 23. I was actually working on the Fortran compiler team of my company. Fortran doesn't require you to be old :)
I'm running VS2010 on a 1ghz atom (dual core) 1 gb ram, netbook running xp... However, my work machine is 8gb quad core i5, with 2 SSD, running VS2012. I also have a macbook with win 7 4Gb, with VS2012 as well. Millage varies...
I do a lot of .NET development on my Eee PC with 2GB of RAM. I bought a £180 model with 1GB RAM for my 5 year old son and it cannot run VS well enough to do development.
I'm a VS 2012 developer and my work box is a Dell Latitude E6430. Windows 7 reports 8GB RAM, and an i5-3340M CPU @ 2.7GHz. 240GB SSD. Also running SQL Server, Oracle SQL Developer. Main piece of code is a 100+ project solution. The box runs everything just fine. Seriously, ensure whatever you get has an SSD. EDIT: Forgot about screens. I have two 1680*1050 22" monitors at work so it's not an issue for me, but at home I have another lappy (Toshiba u920 something or other) which I love with a passion but the screen is only 1366*768 which isn't big enough for VS. The Dell screen is 1600*900 which is OK but I probably wouldn't use my customary Consolas 9 font ..
Projects. It's just so much easier to step into to find a grief problem that inevitably arises, as well as handle multi-layer changes in source control. It provides better documentation for checkins as well, since the changes to all layers are normally committed to all projects. I often turn off logging by setting the debug compile flag, so it's also nice to not be spamming people with a release DLL. I can see the argument that having the layers that accessible is a bad temptation, but having worked only with R&amp;D groups where a project can change on the whim of a scientist or engineer has conditioned me to make everything as flexible as I can.
Initially you will want projects for rapid development. As your application/team matures, you will want to use more DLLs. Advantages of DLLs: More controlled/fewer changes. Faster compile time for the solution. Advantages of project references: Faster development. Easier to change. Easier to debug.
Another vote for starting with projects and moving to the DLL when / if a particular library gets very static (not changing for majority of releases). 
I base the choice on build frequency. If two things are often built together and both are built often I try to keep them in the same solution and use project references. If I am working on reusable code or if development workflows get out of sync I move on to package references (just binary references using NuGet). I keep my packages and source versioned and tagged and distribute the packages in a simple shared network folder or on nuget.org if it is public. It has worked out well for me so far.
Small individual projects and NuGet for dependency resolution. If you need to debug a dependency just pull the code and drop the debug build. Errors in dependencies are a lot less common than errors in the project you are currently working on. Small projects also facilitates shorter build and integration testing times. NuGet makes sure that you don't actually check in binaries which is a bad practice for a number of reasons. My typical solutions has 2 projects: main assembly and test assembly. Also not really related, but also try to make sure that projects stay within their domain. I constantly fight against changes that add NHibernate or StructureMap dependencies to the Plugin API project or MVC toolkit project. When come production dependency issues are a constant hazard, and the best solution to this is keep domains specific. Edit: sorry for spelling errors. Using my phone. Edit 2: Fix spelling.
Also if you decide to use normal binary references be sure to set your output paths first. I have had issues in the past with hint paths being generated incorrectly as a result.
Referencing a project means it has to be loaded into the same solution. This can be time consuming if you reference lots of projects: vs.net might take longer to load the solution, and compiling at initial load might lead to longer compile times. Advantage is that you can make changes to the referenced code in the same solution as the code referencing it, it makes it easier to make changes across the board. But be careful: because it's easy it also makes it easier to create dependencies you might not realize, as you can change code in both code bases at once, everything is right there... Referencing a dll makes it harder to make changes in the code referenced, but that can also be a blessing. It also makes it faster to load the solution with the referencing code as you don't have to load/compile the referenced project(s). To illustrate what I use: I have split up my project (LLBLGen Pro) into a couple of solutions, all are focusing on a group of systems: one for the designer, one for the code generators (which are loaded at runtime by the designer so are not directly coupled), one for the runtime, one for the DB drivers (which are loaded at runtime as well), and a couple for the unit tests. This way, the solutions are manageable (load times/ compile times, IDE speed) and also defines the borders between the subsystems which are coupled, so it's harder to cross them (to add a method to a code generator engine and use it at runtime, I have to define it in an interface in the designer solution, then implement it in the code generator engine it was meant for, meaning I have to cross solutions, which makes it harder and forces me to take a step back and think. The main unit-test solution contains 44 projects, all project referenced, and it's slowing the IDE down a lot when loading/compiling (to illustrate the downside of having them all in 1 solution). You mention resharper: be aware that the larger the solution, the more code resharper will work on and the slower it all gets. If you just want to work on a subset of it, why bring all the rest into the ide as well as it slows things down?
If all layers are actively developed, projects. In lieu of referencing assemblies, a private nuget server and host your cross concerns and infrequently changing libraries. Reference the nuget packages and make your build service repackage and publish on change. Works very well. 
Projects! If the library is stable, convert project to nuget and host on private server
Close all the porno first.
No, that's not how it actually works. We often teach programming that way because it lets you explode out the thought process, look at each step in isolation, and reveal how the various steps in the process work together. Source: I spent many years as a technical trainer
&gt; Unless someone else is implementing your interfaces too - why do you need to externalise them? I've worked in environments where we've used configuration driven DI before, and we have had parties other than the project developers implementing modules before. But also, from a testing standpoint, I don't like really having to reference the project containing the implementation of a dependency just to get a the interface it implements. 
The presenter is trying to show you how it all ties together by jumping back and forth so you can understand how the MVC pattern works. In the real world, you'll focus on a specific layer before moving around. Most likely laying out the business logic before you even start working on the presentation or data layers.
You are presented with all the stages of an integration in a simple way because that is the level of complexity you can probably grasp at this point. When you understand the whole system completely, you will write one layer at a time, with other techniques you have learned over time.
To say something different then the others: If you have an MSDN license and a fast Internet connection at home then you could use a Virtual PC : [Scott Hanselman's set up guide](http://www.hanselman.com/blog/UsingASurface2RTARMToGetActualWorkDoneRemoteDesktopVisualStudioAzure.aspx) I tried it with an older laptop and it work pretty good and responsive. On the other hand, if your manager will give you a new laptop then you better take it!
Must register to view
Sorry about that - fixed it :D
[Los Techies](http://lostechies.com/)
Read everything on stackoverflow.com - not a blog, but there are links there to various sites and those are awesome. Also, almost every question has a really sensible answer that explains everything. And every important question for a newbie has been asked on SO. http://stackoverflow.com/questions/tagged/c%23 http://stackoverflow.com/questions/tagged/asp.net Try this for example: http://stackoverflow.com/questions/tagged/asp.net?sort=frequent&amp;pageSize=15 Learn to make extensive use of the tags, filters, and tabs on the top that say - featured, newest, top, frequent, etc Also see SO user's profile pages - people with 5k+ points who have blogs are generally knowledgeable, and their blogs are useful. Also codeproject.com for tutorials, codeplex.com for code. Hope that helps. 
Scott Hanselman
Thanks a lot, very helpful comment.
So this is a wpf non-windows store desktop app? There might not be a lot you can do if the scroller wasn't optimized for touch performance. The windows 8 platform scrollers were designed to be great with touch.
SO is an invaluable tool for learning. When I want to better myself on a topic, I browse that tag and try to answer questions for myself, then I go back and check the accepted answer to see if I was close or way off. Then I start actually answering questions, sometimes they're accepted, sometimes a more complete solution is presented and I learn even more. No matter what you're looking at, it's an actual real world problem that somebody is trying to solve. It's the type of thing you can't really learn in school or a lecture. It's real people with real projects solving real problems. I try to visit the site whenever I have downtime. 
Is there a lot of crap that gets posted? Not trying to sound threatening towards our favorite software giant, just curious. Sometimes with blog feeds that kind of stuff slips through the cracks.
What is the magic number (4)? What is the use case for something like that that is not covered by "cmd.exe /k ipconfig"
Because meh.
If you have MSDN, get the free [MSDN Magazine](https://msdn.microsoft.com/subscriptions/manage/), it comes in print or PDF. Also check out the [DotNetCurry (DNC) Magazine](http://www.dotnetcurry.com/magazine/) There is also [Visual Studio Magazine](http://visualstudiomagazine.com/Home.aspx) There is always http://blogs.msdn.com as well
Then why ask the OP question? 
Do the labs on the virtual machine work as well on your own VS2013 Ultimate desktop machine? Or is this a 'special' installation. I am wondering what the point of the virtual machine is? Do some people not have That VS version on their machines and still want to learn about it? (I don't know I have an educational version but not sure what happens in industry) I found the programs installed on it, they look typical. http://blogs.msdn.com/b/briankel/archive/2013/08/02/visual-studio-2013-application-lifecycle-management-virtual-machine-and-hands-on-labs-demo-scripts.aspx
It displays the IP address of additional adapters.
Also, that is just example code that the user can use if they wanted to insert such a feature into their application.
The purpose of asking questions is to save yourself the effort of doing the research.
http://www.alvinashcraft.com/ This website aggregates news/articles every day.
Corey House http://www.bitnative.com/ 
Before I purchased the annual subscription I would keep the cheaper subscription monthly then upgrade for a single month if I really wanted source code. With the annual plan you can't do this unfortunately.
damn i JUST completeled 70-480 :/
try using await instead of just waiting for it. See if the delay still persists.
Working on a similar topic. This link for me was a good source of inspiration, hope it helps you too: http://damienbod.wordpress.com/2013/11/01/signalr-messaging-with-console-server-and-client-web-client-wpf-client/
This method isn't called via a property getter by any chance is it? WPF can really pound on the get method of the properties you bind to. You don't want to do anything with side affects in any property getter for a property that you're going to bind to for this reason. Without seeing the rest of your code, my guess is you want to open this connection somewhere else (either when it's first needed or when the application starts, whichever makes more sense for your scenario) and then hold on to it until you know you're done with it. Also, you probably want to do the SignalR calls asynchronously and perhaps disable the control until it's done. Doing it synchronously is going to make your UI laggy and unresponsive while the UI thread is busy waiting for the relatively slow SignalR call to return.
This kind of post is better suited to stackoverflow.com. Don't worry, they're very helpful over there!
You might find Zurb's interchange javascript library interesting for what you are trying to accomplish. Done client side instead of server side, but it solves the problem you are looking at http://foundation.zurb.com/docs/components/interchange.html
This looks pretty promising, thank you.
You might want to look at the mobi 51 framework. It detects incoming mobile requests and serves up mobile views. Thus, you can have a whole different view for mobile. Other then some sort of mobile detection framework, I do not know any easy ways of hiding partial views (based on a client type). http://51degrees.codeplex.com/ However, I am also going to plug Bootstrap, which is what I have been using for mobile. 
I you are deploying to iis you can just put a App_Offline.htm page in the site root
Bootstrap has been pretty good for this. Has most of the resolutions covered. You can define custom break points if these aren't working for you. VS 2013 has Bootstrap as default now as well.
Go away spambot.
I'm not sure if it's exactly what you're looking but you can create mobile views in MVC 4 and switch between the desktop and mobile views. http://www.hanselman.com/blog/MakingASwitchableDesktopAndMobileSiteWithASPNETMVC4AndJQueryMobile.aspx
Twitter Bootstrap and other projects. This is largely a "solved" problem for 80% of people's use cases.
I guess it depends on what tool you're using to do deploys; I'm not really aware of any that delete all files in a given directory and then extract the new ones.
Gotcha. Just for shits I left app_offline.htm and did a deploy and it did NOT delete the file. So this looks like it'll work for me. Thanks for the info.
For releases we rename App_Offline.htm.bak to App_Offline.htm, then rename it back when we are done.
Gotcha. This looks like it'll just do the trick for me. Thanks.