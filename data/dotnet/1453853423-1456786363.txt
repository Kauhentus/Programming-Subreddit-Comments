Nice article, I've started something similar myself but using typescript, and I have to say it has been a joy to work in. It was a little frustrating to get setup at first but I blame that mostly on all the tutorials/articles referencing old depreciated interfaces and methods. The new .net core name should help immensely in that area I hope. Webpack is awesome especially once you get hot module reloading setup, really speeds up development. Edit: HMR was a little tricky to setup when mixing with asp.net but you already have the base setup you just have to specify the full URL with the webpack port when in dev and in prod you use the relative paths for the scripts.
I am a 10 year user and a big fan of DevEx's components (mostly their WinForm stuff but I've dabbled in some of their other things) IMHO their support is excellent, they have consistently added features at an impressive rate, and the value you get for a subscription is astronomical compared to what it would take you to roll similar stuff yourself (especially when you consider the bug fix/enhancement portion of the arrangement) If you are worried about future proofing, DevExpress has been around for ages, has a massive customer base, and has shown no signs of either slowing down or going away.
It's been a while, but I think the package needs to contain a folder called "app" in order to be treated as a command. I know both Kestrel and Entity Framework 7 contribute runnable commands when you install their packages; perhaps open those packages to see their structure? Edit: Just remembered - I'm reasonably confident that your tool's assembly doesn't need to be a .exe - if you look at how the "dnx ef" command from EF7 is implemented, it's just a .dll that gets loaded by the DNX application host when it's run. Have a look at "%UserProfile%\\.dnx\packages\EntityFramework.Commands\7.0.0-rc1-final\app\ef.cmd": @dnx --appbase "%~dp0." Microsoft.Dnx.ApplicationHost ef %* In general, it looks like DNX gets around the which-runtime-to-use problem by only having .dlls (in packages) and the runtime you wind up using is the one that provides the "dnx" command.
You could use [Duality](http://duality.adamslair.net), which is an open-source 2D game engine written in C#. I'm pretty sure you can achieve what you want with Duality. It's easy to get started, and the engine itself is modular (you can build plugins depending on your needs). Give it a try!
DevExpress customer for eleven years here. Overall I'm happy with them, although some of their design decisions makes me cringe. There's a few places where you may experience exception swallowing. If you report it, you'll be told it's by design. Yes. Exception swallowing by design. That aside, it remains remarkable value for your money.
At the bare minimum, you can write some PowerShell scripts to do this for you. More robust than batch files
I don't use DevExpress, I hate so much their ASP.NET MVC implementation because it's very confusing. Use [DevExtreme](http://js.devexpress.com/) instead, it's their Javascript implementation for web apps. To me, is the best library for UI widgets. Also, don't use Knockout or Angular widgets, they are very buggy in performance. Use their jQuery approach.
Hey man, you may want to take some tips from these slides: https://speakerdeck.com/toddmotto/angularjs-the-performance-parts Around slide 36, you can see the actual tips for more performant Angular code. Hope it helps!
Old &amp; failed: http://kck.st/MT1LWT
We're on version12. 
A full instance of IIS is definitely overboard to play with javascript / jquery. You can create an html file on your desktop and include jquery in your &lt;head&gt; like this: &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"&gt;&lt;/script&gt; then just double click the file to open it in your default browser.
Every tool should only be used when appropriate.
I've used DevExpress on some Webforms applications for about four years. -Their support is really good. You'll get a quick response to any questions you have. -Their controls are really easy to use, with the exception of the gridview. I've found that to be pretty difficult. Although, through time I've become decent at using it. That said, if I was in a position to start a greenfield app, I would prefer an Angular/WebAPI option.
First off, authorization is hard so I'm going to try and boil it down to the essentials. In general, there are five factors at play * the resource being secured (URL, field, etc) * the subject requesting access (user) * the right they are requesting (read/write) * the policy that governs access * the action to take (deny, permit, show, hide) You can factor other things in as well, like time or some other context, but I'll skip those. So, your approach seems to take these all into account. Field groups are really just collections of resources that you control with a higher level policy. A single field is a field group of 1. User specific policies may not scale well, so you may want to look into attribute based identity and write your policies to the attributes instead. (A role is an attribute with no value). 
Consultancies love certifications. Otherwise, it can be more of a detriment. MCSDs teach one how to solve MS problems by their book on their latest platform. Rarely do these conditions exist in the real world. More often than not, you'll be faced with legacy code that follows whatever (anti) patterns that developer left steaming on the clients chest; or you've got to integrate with other 3rd party services/APIs that care very little for MS's certified nature. Hell I'd bet there are few at MS that consider an MCSD a high benchmark. 
MCSD certified here (from back in 2005). I've been a MS consultant for 18 years, 10 years .NET exclusively, now in a full-time permanent role. I've both been hired and hired people with certs. I live in the midwest. It may or may not help, but it doesn't hurt. In my case, I happen to have considerable experience in addition to my MCSD. I took a bootcamp and passed the tests in order to transition from classic ASP to .NET, because I couldn't find a role where they let me learn on the job. It was extremely helpful to my career. I've worked several places where certifications were encouraged and paid for by the company. If a job candidate ONLY has a certification and zero experience, that's also obviously better than just having zero experience. Not better than building your own project using the tech in question, but better than nothing. You will find that there are a small number of very loud zealots out there who are rabidly anti-certification, insisting that that certifications are 100% useless and no 'real' programmer would ever get one. Not surprisingly, those folks generally aren't the ones interviewing candidates for jobs where certifications would be applicable. Even if they were, I would never want to work for someone with such a knee-jerk hatred of 'book learnin'. Any hiring manager who throws out a resume based on the sole fact that the candidate took the initiative to study and pass a certification test is an idiot. There's simply no denying the fact that lots and lots of successful, excellent .NET programmers have certifications, for a wide variety of reasons. 
Yep, here's one of them now. If you were to ask candidates why they got certified instead of making assumptions, you would quickly discover that a lot of developers with certs do not fit into the broad generalizations you mentioned. Learning about a technology and then being tested on that knowledge is simply never a bad thing in and of itself. Your various concerns are all regarding *why* the person got certified, and that's something you simply won't know until you ask. Your assumptions regarding a certification somehow teaching someone bad habits would need more explanation. A lot more. Obviously, no one should blindly hire someone based on a certification. Poor problem solvers can always pass a test if they work hard enough. But it's equally as ridiculous to throw out a resume simply because you've assumed a certification has somehow completely stripped a developer of the ability to solve problems and write acceptable code. Both approaches are simplistic and naive, and are guaranteed to significantly reduce your pool of qualified candidates. Your loss.
Anywhere in Canada? Nobody cares. I took it just for the fun of it, but when I see a candidate that has it I check very carefully if he's not just a certificate guy and actually knows stuff.
You're getting that error because you're selecting am Id from the UserClubTag table. So you can only use the int? In your where clause. I believe what you're trying to do will require you to use joins to accomplish. Take a look here for some examples: http://stackoverflow.com/a/5010568
I agree, it would be very helpful if OP could separate the various permission models into roles so you can ask, "is user in HR? they get X permissions" instead of making sure each new user is assigned the right permissions (and they're kept up to date). This also helps for auditing, as each role will explain *why* a user needs a certain permission. Say one user transitions from Finance to HR, how do you make sure that he doesn't have permissions that he shouldn't? Keep in mind that HR and Finance may have overlapping permissions and the user may have additional permissions from side projects that he keeps during the transition.
Role based was talked about. After a meeting we just had it sounds like we can go with the approach of "Deny X" roles and deny certain users from a grouping of data. So a new role of "Deny Job Data" would deny users in that group from seeing "Salary, Start Date, etc" But one of the things HR wanted original was 3 users of HR staff. User A is in charge of handling Job information so she needs to see and edit Salary. User B is in charge of the phone list... They need to be able to edit the phone number AND ONLY THEM, BECAUSE DAMMIT PHIL WILL YOU STOP CHANGING YOUR PHONE NUMBER TO 1800MIXALOT. User C... you can't edit anything... but you need to be able to see most data... it might change tomorrow. But short of it is, we were able to talk with HR and discuss a plan of adding roles. Because of auditing, and other things.
MVP != Certification. You have to be very VERY active (and correct) in forum answers and blogs to earn an MVP. Sure there's a Venn intersection of certs &amp; MVPs, but they are separate things entirely.
In one sentence you affirm correlation then defend it as causation in the next. you sure told me. Guess I lost at internet today.
In my own job search for a senior level .NET programmer in the midwest about a year ago, I'd estimate that I saw mentions of MS certifications about 1/3 of the time. This is almost always under the "Desirable skills" category with a bunch of other things, but it was not uncommon to see it included in the actual job posting.
I'm a freelancer (hate offices) and MCSD certificate helps me a lot to find customers. People just trust any official papers and ready to pay more for that. I can't blame them :)
&gt; Guess what, they're a lot tougher than I expected. If you're taking an ASP.NET exam, for example, you have to know in detail and in order the various steps in the request/response pipeline. On a WCF exam, you have to know how to set up a binary binding. So you have to memorize a bunch of shit that can be looked up in seconds. That's the definition of completely useless to me.
FYI opening on chrome may give an error due to local script execution (i.e. you're executing code in a local file). If it doesn't play nice you can download nodejs which comes with NPM and then run "npm install -g lr-http-server", this will install a tiny http server with live reload(i.e. it'll update on changes to the source of any file in the folder". Then from the command prompt switch to the directory with index.html, and run "lr-http-server" nd it will tell you something like "serving on http://localhost:8080" go to that in the browser and you're set 
Certs may get you ast an inexperienced HR person (or just an HR person), so it can't really hurt, but wouldn't really invest a ton into it. Your biggest thing is to ace the whiteboard test, that's what makes or breaks most people
Detroit area here, Sr. System Architect, and I do hiring. In general, certs mean almost nothing. Oh, and university degrees mean just about as much. &gt;&gt;So are you saying you came out of your mother's womb knowing enterprise patterns?? &gt;Everyone needs to start somewhere, and everyone needs someone at some time to old their hand. We all need and can be taught something by someone No, but some people are autodidacts and once they learn to read... they're done having their hand held. Once in a great while they'll need to ask a question, but that's it. Give me a good book -- to skim or read as necessary -- and I'm good to go. Yeah, yeah, books are outdated so: good *written* documentation, *written* tutorials, with an example or two and I've got it. I can't do classroom learning or videos: 90% of it seems to be filler and the rate of information uptake is slow enough I fall asleep. Plus, they're not searchable. Your exercises *bore* me to tears and are not relevant. I wish I could pay (or make my company pay) for a book on a topic and then ask questions from an expert occasionally on subtleties I may have missed. No classroom, no "teaching".
you're not wrong, but it's not totally useless... lots of legacy code out there to maintain.
I agree as far as certs and degrees go, some people ( myself included ) have a genuine desire to learn, and explore. I do hiring as well and if a person exhibits those qualities, I will hire them on the spot. I know a bunch of languages, architectures and frameworks from self learning. However because of that I also know how little I know. I also like to ask questions though and have people teach me the way they do things (I'm not talking a 5000 dollar course that gets me same benefit as pluralsight, I mean my peers, even less experienced than me). I've learned over the years also, that everyone's brain works in a different way and there is no black and white / right and wrong. What works for me, won't work for other, and the perfect thing might be a hybrid. Some people try to learn to program and get overwhelmed and need guidance where to go. Great example is some suggestion online for people to learn algorithm and data structures as their first intro to programming. Those are great to know but as your first exposure will likely make you puke and never want to touch a keyboard again or utter the word variable or pointer My point is, that while some people naturally are curious and prone to learning, others need structure. As seniors, we are as much guides/teachers/mentors (I would actually dare to say that this the most important thing) than we are engineers. We must push less experienced toward learning and bettering themselves, yes some will fail and you will waste time, but others that mentoring can mean a difference between them herding along or becoming a great engineer
I wish I could have read what he said because of your response
MCPD exams are not a way to learn IMO. The books aren't real-world at all. Really, pluralsight is the best. Taught be real people, giving real-world advice and examples.
It's because you're selecting out only the ClubTagId. int does not has a property called UserId. Your query will still fail because you can't equate a list to a single id.
Yeap... Agree... You can consider them as they have many good reviews too. Good luck
&gt;Of course this requires a lot of planning and we are pushing back on them to go with less control and really do permissions on field sets instead. But I do need to plan for the case that we need do attribute level permissions on a number of fields. Go even further, they are different views entirely. You'll get more views but drastically simpler code.
I do love partial views... but this application is currently in webforms... and the page this control will mostly effect is just one page... a really really long coded page... (I have been cleaning up the code as I go along separating out code, consolidating repeated code, etc) but I can't just do a complete overhaul yet.
thanks! will try this and update
In the latest [ASP.NET Community Standup ](http://live.asp.net) Damian Edwards said that they will look at it on a week by week basis.
`Select()` just transforms a collection into another collection. As other commenters mention, it will return a list of ints in the case of your code, which is why the .Where() is failing to compile -- your code is trying to access the `UserID` property on the int class which does not have such a property. Is User &lt;-&gt; ClubTag a 1 to many (or many-to-many) relationship? You should be able to configure this in your Entity Framework configuration class instead of stringing together a complicated chain of calls. Look into `HasRequired()` and `WithMany()`: https://msdn.microsoft.com/en-us/data/jj591620.aspx
This is madness
Change the Where to: Where(i =&gt; i.UserID.HasValue &amp;&amp; i.UserID.Value == id)
I have read the statement "The November release candidate (RC1) will be a supported and production ready cross-platform release. Depending on feedback from RC1 we will ship additional release candidates as necessary." but the statement doesn't seems to be true , specially after all the renaming stuff (Asp.net 5 to Asp.Net Core 1.0). They are also renaming package (assembly) names. How can it be production ready
Thanks for alleviating the confusion I had, it has become clear now after a night of sleep and helpful context everyone gave me here. 
&gt; Where(i =&gt; i.UserID.HasValue &amp;&amp; i.UserID.Value == id) Same problem int? does not contain a definition for UserID and no extension method UserID accepting a first argument of type int? 
Indeed, a lot of companies are looking to train up new COBOL programmers as it's cheaper to do this for the maintenance tasks than rewriting a large legacy system. I read somewhere not too long ago that most of the world's business transactions are done with software using COBOL. I'm not sure if that's true or not but there are a lot of organisations in government and industry that are still using systems from 40 years ago and are quite happy continuing to do so. I imagine COBOL will still be with us 20 (maybe 40) years from now.
I looked at getting certification a few years ago, started reading through one of the ASP.NET books and then realised I'd be learning stuff I'd never use, had no interested in using and more importantly nor did anyone else (remember ASP.NET skins?). I'm not knocking MCSD certification but I think it's for certain people. I work a lot with OSS and outside the MS environment a lot so it's not for me but if you want to stay in that world (and many do and there's nothing wrong with that) then they certainly have their value.
I worked at a company that was training scores of fresh graduates to not only maintain but also write new code in COBOL. For one of the biggest banks in Europe. 
&gt; but the statement doesn't seems to be true , specially after all the renaming stuff (Asp.net 5 to Asp.Net Core 1.0). They are also renaming package (assembly) names. How can it be production ready Renaming something doesn't mean it wasn't production ready. It just means the name has changed. A ball ache for sure, but it's pretty much just a case of Ctrl+f/Ctrl+R 
What do you mean? Do you want to use VB.NET to control your Duality game?
Can you post the entire new query?
No doubt google cdn hosts a lot of OS projects. there is also https://cdnjs.com/ and I believe maxcdn does a lot. Good places to check if you need to quickly get a jump with a library without dl-ing it. Also another interesting tidbit is you can link directly to a github file, when you're on github looking at the source click raw and copy the url (just don't use it in production :))
I was at a talk about ASP.Net this month. (Jan 2016). The phrase "this has changed since RC1" came up. A lot.
The official page for it is here: https://github.com/aspnet/Home/wiki/Roadmap So it is ... "TBD 2016" and "Depending on feedback from RC1 we will ship additional release candidates as necessary."
Nope, I ask all of them. Haven't hear a good answer from one yet. I never said that certs teach bad habits. Avoiding people with your self-important attitude is hardly a loss.
&gt; ASP.NET Core is currently RC1, RC2 will follow in February and the RTM version will become available in March. According to [this](https://ultimatecode.wordpress.com/2016/01/20/asp-net-core-nuclear-option/)
Read: "Shit's broken, and we have no clue how to fix it all".
Why do you want to translate a whole view? You would be duplicating markup. Your second post seems to have the solution, in any case. At my work, we have a custom key-value translation system backed by a database.
Tried it, didn't work sadly.
I agree with the situation but not the cause. I am not so sure it is developers that lack the will here. IMO the situation is: 1. Entrenched ideas as the article says 2. Complete lack of desire by team management In the last 8 years I have many times attempted to bring an OSS project into a shop. Like most OSS tools there is inevitably a couple minor things that you would like to change to make it work perfectly for your business. When bringing this concept up to management it has been consistently rebuffed, these are the "reasons" I keep getting: * We are not an X shop, we should not spend any time at all on X even if spending time on X would make our own internal practices more efficient. * I am afraid of IP, copyright and other issues and I refuse to believe the developer that it is legit to do even after they walk me through what OSS licenses are and how they work. Most managers I talk to honestly believe that if you modify and OSS project and use it in your company's application that you must OSS all of your company's code. I have had hours long arguments with managers and legal depts in multiple companies, same damn story, every damn time. * Touching code that did not come from this company just plain scares them and they don't think anyone on the team can handle that * And the worst of all, why should we care to support that? Someone else will want it enough eventually to do it and then we can take advantage of it. Until then just keep doing that time wasting or technically poor workaround forever. To support open source at all the .NET based companies I have seen would require the developer to "go rogue" and do it on their own. While supporting OSS is a laudable goal I don't see many people risking their jobs over it.
It seems management are dinosaurs here. &gt; Most managers I talk to honestly believe that if you modify and OSS project and use it in your company's application that you must OSS all of your company's code. You can thank GPL for that and Microsoft's long time opposition to it. MIT/BSD/Apache licenses are only getting noticed lately by the ye ole Microsoft shops and it is these licenses that should be pushed forward (including LGPL).
&gt; Are you trying to support multiple translations of your page's content or your page's UI? Both. Translating the keywords ("home", "contact", etc) is quite easy, but having multiple paragraphs (with maybe different html formatting?) is more challenging. &gt; If you're publishing content like a blog post in multiple languages then you'll want to key that content with the culture code and treat it like data i.e. store it in a db and pass it to your view in the model. Yes, I guess I could store it in a db. I was trying to avoid any solution that added complexity in maintaining the code, and in this regard I though it would be logic to have multiple .cshtml (one for each language) and have the system choose which one corresponds to the current culture. My guess would be that with the db solution it would be difficult to have the advantages to easily edit the html with Visual Studio, but I might be wrong since I don't really know what strategy I would follow to serve the content via db. Thanks for the input, I'll try to think about it.
That's true you wouldn't be able to edit the html stored in a db easily from within Visual Studio. But you can write an admin page and empower an admin user to edit it rather than require a programmer. If you go that route I'd recommend using something like TinyMCE (https://www.tinymce.com) to give them a nice editor for the html. That will help keep them from ruining your layout if they miss a closing tag. 
&gt; N paragraphs in the text would require N * M entries in the database (being M the number of languages served). This could easily get out of hand... How is having N*M entries in a highly optimised data lookup system like a database better than M views in a dynamic unoptimised view lookup? Just make all of your strings method calls that find the right thing... Don't try to optimise for performance in a system that doesn't need it and then make a massive mess of your codebase.
If the N paragraphs are part of something like an article body, the whole body was translated as one text. Also, a lot of the views are cached whole so lookup from cache/database is kept low. You shouldn't worry too much about how many rows are in a database table. As long as you have proper indexes, lookups will stay fast.
I've seen that too. e.g. Why use TFS when Git is better? "Well, we're a Microsoft shop". 
So is Microsoft, and they're moving everything to GitHub :)
We have, I think, about 100 pages and about 7 (?) languages to deal with. Once we did the initial conversion (from hardcoded English to using keys for the text), it was not bad at all. We wrote a link to a pop-up window into the master page in our development environment where we add the keys and the text. We have a class file that goes and extracts the text from the database and returns it as (I think) a collection. So the code behind just has things like TextClass MyText = new TextClass(); MyLabel.Text = MyText["Paragraph1"]; MyButton.Text = MyText["Submit"]; The class file know what page you are working on and what language you are using. (We set that as a session variable at log in.) Once it was all set up, it is not that much extra work. Instead of typing text into the markup page, you just type it into the pop-up window.
I would start with the asp.net [IMembershipProvider](https://msdn.microsoft.com/en-us/library/system.web.security.membershipprovider(v=vs.110\).aspx)
Thank you for suggestions. I've used MembershipProvider before and didn't like it at all. Plus, it's a little bit dated and less customizable. I would prefer to use ExtendedMembershipProvider instead of this one
ASP.Net Identity + Claims defining what users have access to. I have used Dapper with Identity before, it's pretty easy. You need to implement a [custom IUserStore](http://www.asp.net/identity/overview/extensibility/overview-of-custom-storage-providers-for-aspnet-identity) (+ any additional stores you want) for basic user CRUD/lookups. "Stores" for identity are essentially repositories that the identity user manager uses to persist and lookup user information.
You can also use git source control with tfs on visual studio.com. That's been supported for at least 2 years now.
&gt; What sets BCrypt apart is that instead of the more typical SHA-* algorithm, it leverages the Blowfish algorithm, which has the advantage of being much slower in parallel. Since users log in one-at-a-time, this makes it much harder for attackers, who will test numerous passwords, to beat the algorithm I didn't get this.. can anyone explain what author is trying to tell?
Seems like a weird "benefit" to be a slow process. But from the wiki page about Blowfish &gt; Bruce Schneier, Blowfish's creator, is quoted in 2007 as saying "At this point, though, I'm amazed it's still being used. If people ask, I recommend Twofish instead." https://en.wikipedia.org/wiki/Blowfish_(cipher)
He's right in that many other domains have far better OSS ecosystems. Java and the JVM, for example, have many amazing OSS libraries in domains that .NET doesn't really touch. There are ports being made for some of them, like [Netty](https://github.com/Azure/DotNetty), but [few of these types of projects are started in .NET](http://www.aaronstannard.com/the-profound-weakness-of-the-net-oss-ecosystem/). But there's more to the story: - All the other OSS environments have open core languages and runtimes, which facilitated their open growth. .NET's been living with a black box for years. - It's only recently that Microsoft itself has been in support of the OSS paradigm shift. - Nuget has only been around for five years, and popular for less than half of that. - Mono being maintained by a third-party kept a lot of people from using it (as did the single-platform status). Mono was also in uncertain legal status for a while, which made many people avoid it completely. Microsoft's litigation practices haven't helped this much. - Most .NET developers use Visual studio, and VS Git support is relatively recent (2012). Decentralized SCM has made a huge impact on the ability to do a drive-by commit. No one who's used one really wants to go back. - Visual Studio Community is a recent-ish thing. Express editions existed before, but it really wasn't the same experience. - Microsoft courted the commercial component market for many years. We're still feeling the effects of that. - IIS being the only way for HTTP held us back for a long time. Microsoft's involvement in this sets a precedent that cannot be understated. A while back, I submitted an issue on VS 2015 and got an email update from a developer that he'd fixed the issue. He even linked me to the Github issue for it. It's very different from previous Microsoft engagements, where you couldn't talk with a developer without opening an issue and accepting the possibility of billing. This type of thing will start to change the minds of people used to the idea that code can't be shared. We're in the process of clearing out the final obstacles. As .NET Core reaches feature parity with RTM .NET and it starts to proliferate both on Windows and other platforms, the landscape will be more disposed to OSS .NET. We'll draw more OSS developers for the amazing tools and language. More closed source developers will get comfortable with the idea of open source. And people will start to fill in the holes. It's ramping up, and we're not at full swing yet.
From a consumer (of the framework) perspective, there isn't that much difference between .NET Framework and .NET Core. 
I could not agree more. While what the author wrote is not at all inaccurate, the tides are turning quickly. As a (primarily) web developer working in the .NET space at my day job, but being a hobbyist in more (until recently) open web technologies, what ASP.NET Core is bringing to the table has me tickled. I know a lot of other people are excited about it as well. I suspect the ecosystem will be quite a bit different in even just a year or two. The simple fact is it is going to take some time to undue decades of a closed off approach. The good news is we seem to be well underway. 
I'd argue against using BCrypt in .NET, because there is no cryptographically verified implementation. Sure, most likely there is nothing wrong with it, but there are no guarantees. The recommended .NET password hashing algorithm is PBKDF2 with high iteration count, which should be good enough for now.
&gt; Doesn't that mean your entire server will keep slowing down for every legit user while the attack is going on? Conceivably, if you fail to implement a login/retry rate limit. Which you should be doing anyway. If you don't have one, it's still entirely possible for them to bring your system to its knees with a distributed attack even with plaintext passwords. DOS and DDOS attacks are already a known risk with any internet-visible service. Most of the hashing algorithms have work parameters that allow you to fine-tune your hashing. You could have hashing that runs in a few milliseconds instead of microseconds, which doesn't make an appreciable difference to an individual user but increases the effort required to compromise a secret by orders of magnitude. This can help buy you time to deal with an intrusion. It sounds crazy, but with liability concerns, having an unusable system can be better for a business than a compromised system. &gt; Is it like, people have a dedicated vm for just hashing-involved-routes so that other routes work fine under hack load? Well, that's a possibility if you have really complex hashes. I'm of the opinion that it'd have to be a *really* heavy hash to justify it. More likely you'd run it on the same machine as the rest of your application and scale it horizontally as needed with load balancers.
You make some interesting points and I wrote about the lack of variety on the .NET platform quite recently ... https://www.reddit.com/r/dotnet/comments/3oqxqu/the_net_framework_needs_more_languages
I created some real-time graphs using D3 and signal-r. It works great.
I recently created a small wrapper for blob storage very similar to yours. One thing you may want to consider is moving the StorageAccount logic into your constructor. This will allow that configuration logic to be run through only once when it's been injected instead of every time SaveAsync() is called, including any future methods you might add (DeleteAsync, etc.). 
True, thanks for tip. I'll leave it for now as it is, since I'm 100% sure I'm using it max one time per request.
You seem to ask similar questions regularly, and post them in a bunch of subreddits.
Meh, C#, Java, and Javascript are all different technologies and none of them are going to replace each other. Javascript is primarily for front end web development. Yes, Node allows for Javascript on the backend, but Node is designed for a particular use case, one where the web server is just an intermediary to the database, and does virtually no computation. Javascript is slow compared to Java and C#. Java and C# are popular for back end development. They can also be used to make desktop applications. They're also good for server side processes. Xamarin is popular. It's pretty good if you want to do cross-platform, mobile development. It costs a good chunk of cash, so I don't recommend it for personal projects. It's great for companies that have deep pockets. I can't speak to the Ottawa market, but most major cities have jobs for Java, C#, and Javascript developers.
Yep, D3 and SignalR are really awesome together. Takes a bit to wrap your head around D3 (it did for me, mainly the enter/exit and transitions) but once it clicks, you'll be golden.
If you want to do full stack web development, you need to know Javascript and SQL, along with whatever server side languages are used at your company. These will usually be C# and Java in more established companies. Startups mainly use Ruby, and a few use Node. There are plenty of benchmarks out there, but C# is compiled, while Javascript is interpreted. Because C# is statically typed, there are a lot of optimizations that can be done at compile time. Javascript can't do that, because it doesn't know the type of any object until runtime. The type may not even be defined until runtime. The same can be said in Java vs C#. The word "overkill" is usually bullshit when discussing an entire programming language. Java is a good language. Both Sun and Oracle have let it grow a bit stale, and I don't know when it will catch up to other languages in terms of features. But if you want a strong, statically typed language that has good performance and has a large development community and lots of options for tools, you're not going to beat Java. The Play framework is a newer hotness. Spring MVC is popular. Is it possible to have a career with Javascript, HTML5, and CSS3? If you want to restrict yourself to front end development, you can do that. I recommend learning jQuery and Angular too, since those are commonly used in front end development. I recommend doing full stack, since you will have more options for employment.
Xamarin's $25/month is per platform, so $50 if you want iOS and Android. That doesn't include the Visual Studio plugin. Virtually every C# developer uses Visual Studio.
Google is your friend: http://www.codeproject.com/Articles/18102/Howto-Almost-Everything-In-Active-Directory-via-C
If you're using bundles, sometimes that causes a problem. Certain publish profiles will only included minified css/javascript in a bundle, so if you're usingt the wrong profile and/or don't have both versions, the file will not be included in the bundle. It's tricky, because it doesn't 404 or anything.. there's nothing to see with like firebug or other client side debugging tools, because it's simply excluded from the bundle, so the client doesn't even try to load it.
Double bundle it sounds like to me, as well.
Make sure the files are in your deployed copy. I had a couple projects where fonts and custom CSS weren't marked for publish and needed to be manually copied over.
Yes, SignalR with RxJS is a very good approach her for management on the clientside. I'd also think about throttling this a bit. or buffering the data on how it are displayed. If you're polling every quarter of a second for each graph, you're going to kill your DBs performance. If I might suggest buffer the data and use RxJS to display the set of data, then have either the server push or the client poll every 20 seconds or so. 
so basicaly you recommend me to be a expert in C# with Asp.net MVC and Javascript, Html5, Css3 all of them + some Sql for database and windows form and wpf etc ? isn't that a bit much if you consider networking too like WCF Ado etc or the windows phone framework, microsoft seem to have their own way to create their front end html with razor and the rest of their tool xaml etc doesn't seem to work like the open source stack... right now I am learning Html5 Css3 and Javascript for fun with Webstorm ... was thinking of going to Node and Angular once the Basics are under the belt in 2-3 months I can study like 10hr's a day or more if I want etc so I have some fun 
Websites that override the scrolling behavior are fucking retarded.
"Expert" For CSS you use a framework like Bootstrap via Less or Sass. That's like 2/3 days learning curve to set up the toolchain and then it's done. For Javascript you have to know how to make really basic jQuery functions to initialize objects. For SQL you have to understand basic relational databases. For C# you have to know stuff properly as there's a lot more depth to the MVC library than to what you need to know about Javascript/CSS when you're using a framework like MVC.
Which of the plenty ASP.NET frameworks are you using?
I have an Android client. I am calling this service's endpoints from client. I just need to add one that will be able to receive an image from my Android client. Programming client side is not the problem, I just need to know how to program the ASP.NET to be able to receive the image. Base64 was just an idea. I might as well do something else. If you've got any advice, I'll appreciate it a lot :)
This is what multipart requests are for: http://stackoverflow.com/a/19746825/2001966 Alternatively you can URI Encode the Base64 (to remove the slashes) - but don't do that. GET's shouldn't result in any side effects, a POST or a PUT would be better. 
I think it's still too early to say that. Mono isn't good enough, not even close. It's an unofficial bastard child of .NET where things may or may not work, and that's been our best fucking bet for cross-platform .NET development. No wonder it's not "picking up". Fortunately, that's by now obviously not the end of that story. Now you might say that open source != cross-platform and that's not the topic here, but they are closely linked because a reason to go open source is often to support multiple platforms or at least let other people work on a codebase to make it happen. I think .NET Core with .NET Native compilation is the future for the "cross-platform .NET" story. It's official. It's building on the recent (too recently for it to be judged as a success or not) official open implementation of the .NET base libraries. And it's a Go competitor in that the binaries can become standalone for easy 'xcopy' deployment. So, I think... Come again in 2018. :) I think the interesting question is if it'll be too late. I think people often throw that argument around without thinking it through, and I use to ask "too late for what?" But I think Go is quickly picking up steam here, rallying especially Python, Ruby developers but also interest from other groups. And it's already where I think .NET Core wants to be -- today. Now imagine where it will be in two more years? It's been moving ahead quickly. Sure, the language itself is spartan, but that can also be a benefit as it's quick and easy to adopt. Its standard library is already exhaustive and constantly praised. It has strong support for web backend developement like ASP.NET Core 1.0, but it doesn't stop there, unlike .NET Native which doesn't really work today. And it must be compared to .NET Native, because native is what Go is, and it wouldn't be nearly what it was without that pillar.
This is a good answer, however I would add that breaking changes in your dependency can be more complicated. So if your library depends on JSON.NET in this example, and the newer versions (major versions if they are following semver) contain breaking changes then you may need to either: 1. Restrict your package to a range of versions not containing the newer breaking changes 2. Just upgrade to the newer package and drop support for the old dependency version (in severe cases, this could be considered a breaking change in your package) 3. Publish/maintain two versions of your package (say the 1.x versions of your library will be maintained against JSON.NET 7.x and the 2.x versions against JSON.NET 8.x) In a perfect world you would test your package against all versions of the dependency (JSON.NET) to determine which versions really are compatible. Hope that is clear, this can be a complicated topic.
My bet is on the content type for woff2 not being defined too. 
Use a POST or a PUT (add/update). You'll want to at least set the mime-type header so you can tell on the server side what the image was (png, jpg, ...). You will also need to set the content length header to match the size of the base64 encoded string so the server itself (iis) knows how large a body to expect. For the post body you can do something as simple as imageName=&lt;image name&gt;&amp;image=&lt;base64 encoded image data here&gt; alternatively you can include the image name as part of the URL and only send the image data as the body. You can also omit the image= part if your endpoint knows that the only thing to expect is image data and you have direct access in your server code to the post body. I personally would recommend using name=value pairs because most frameworks will split that out into some kind of dictionary or give it to you as a parameter to your endpoint (mvc, webapi) 
PBKDF2 is what OWASP recommends. Source: https://www.owasp.org/index.php/.NET_Security_Cheat_Sheet
have you guys worked with Phonegap or any mobile dev framework in Javascript ? You can build full app with that framework and sell them in the store in a package ? 
Base64 is just a string, so create your POST endpoint that receives a string. Then, convert that from Base64 to Binary and save it. http://stackoverflow.com/questions/8645088/how-to-decode-a-string-of-text-from-a-base64-to-a-byte-array-and-the-get-the-st
Talk to this guy: https://twitter.com/terrajobst/status/691740807125037056
Thanks for the heads up, I will!
Well the thing is, it's not *my* library and I don't claim it to be. Someone else wrote it originally and I think setting up a full site and all might be going a bit far? I don't know.
Are you doing Build or Rebuild? Rebuild only compiles the projects that have changed. 
I think it's the reverse, rebuild is clean + build and builds everything. Build builds incrementally. http://stackoverflow.com/questions/3095901/difference-between-build-solution-rebuild-solution-and-clean-solution-in-visua
https://github.com/udelblue/PeoplePicker
Both answers are helpful, thanks!
I just did the same thing and just wrote a bit about what I found, http://www.lavinski.me/porting-microbus-to-dotnetcore/.
Is this really easier than IdentityServer3?
Blogging and tweeting about it and getting covered by the big league bloggers (like Scott Hanselman) is probably the best way. Since you're not the original author of the library, posting a short, friendly announcement in various topical subreddits is also appropriate, IMO.
something like..... .ForeignKey("dbo.Worker", t =&gt; t.WorkerId, cascadeDelete: true)
Yes, but the cascade delete property is an enum. MigrationBuilder.table.ForeignKey( // stuff //onDelete: ReferentialAction.Cascade onDelete: ReferentialAction.NoAction); You need to switch the ReferentialAction property. (I just looked at the code I was doing this in a couple days ago).
I found this after checking @terrajobst twitter, it was a good read! I might write a similar post myself.
Interesting. what namespace does the referentialaction enum live in? 
Not sure if I get your question. But I just rebuild and hit run after I update my model.
Why is this downvoted? This sub is just terrible if I'm honest. Seems to get no attention at all.
No really. I should write something about it.
I fixed it actually, it's weird because SQL server thinks i will delete some records but i won't so i did some stuff with fluent api, it is fixed now like you said.
I'm writing up a detailed post of all the steps I took on my own personal blog now. I think that's probably a good half-way point, I'll post it here when I am done and hopefully that'll help people in future.
I want .Net core for very simple reasons: I do a lot of data apis, hosted on AWS, and on .Net core we can start them up on AWS on lower-spec VMs, because ASP.Net core is much lighter weight; and start up much faster, because AWS Linux machines start up much faster than windows ones. Or maybe there will be an equivalent for windows soon. [Yep](https://technet.microsoft.com/en-GB/library/mt126167.aspx) and [it's also .Net core only](http://blogs.technet.com/b/windowsserver/archive/2015/11/16/moving-to-nano-server-the-new-deployment-option-in-windows-server-2016.aspx). So that's a win that even cost-concious tech-ignorant managers can understand. So it's very likely to happen.
You could go with Umbraco and Merchello. Although, making calls to a third party API has nothing to do with the software you choose being open source. 
&gt; Look guys, it's clear that MS cares about Azure and only Azure. And that's why they completely rewrote the .net Framework, execution engine and compiler to be cross-platform? That's why the open-sourced everything, even the "old" stuff?
I've used [nopCommerce](http://www.nopcommerce.com/) for a couple sites in the last few years and it worked out pretty well.
Good comment. I posed the question a while back if this is all a bit late in the day for .NET and it wasn't well received but I still think it needs to be asked.
I agree these benchmarks are useless in theory, but in practice many developers don't know that, and use those kinds of numbers to evaluate trendy technologies. Optimizing for this won't have much of an effect on app performance, but it may change developer mindset. As usual, the customer is always right, even if they're objectively wrong.
I'd start to look into a caching solution on the web server side, especially if your serving relativity static data. I'm building a backend at work that collects lots of data from a lot of remote sources, normalizes it and stores it in a SQL database. Off topic but since a lot of that data collection is I/O bound I really try to take advantage of async/await and I've been able to keep 100+ remote sources synchronized with each job averaging 5-15 seconds and running every minute - 6 hours That data can then be served via Web API. Pretty soon I'm going to start implementing [caching](https://github.com/aliostad/CacheCow) on the server side since about 98% of my data is immutable once it has been collected and stored. Some of my other data is updated every 60 seconds but this is only a small subset of the overall dataset. I may implement API functionality that can bypass the cache if the right parameter is passed. 
Used nopCommerce as well. Somewhat of a learning curve, but easy to customize. Connects well with most online payment processors. 
I really hope it was obvious to everyone that getting 1.2M requests for their specific plain text bench mark was not going mean you were going to be able to handle 1.2M requests for a normal web application.
But can it web scale?
Completely agree with you. Currently going through this. It sucks
It will only mean nothing to you if you cache nothing for any request and get everything from a database including your javascript and images
I was a major contributor during the re-write from webforms. I might be doing the same for ASP.NET core as well.
SmartStore is in legal battles with nopCommerce, because it is a rip-off with no attribution. They have forked and continuously copy over features implemented upstream from nopCommerce. Just stick with nopCommerce. SmartStore has a nice UI, but nopCommerce is improving in that regard.
I think I saw Scott Hanselman himself, say, on a Dot Net Rocks show that "MS wants to sell you Azure, XBox and Office 365". But I don't see that as a bad thing. Every business has got to make money. Windows isn't gonna generate the revenues any more so they are morphing. Is that a crime? And, it's worked for me, I've developed some Node.js sites, but I ended up pushing them to Azure. I'm not locked in, that's just the choice I made.
All true. You can do a lot with [the simple built-in MemoryCache](https://msdn.microsoft.com/en-us/library/system.runtime.caching.memorycache(v=vs.110\).aspx). As it's simple and in-memory - it doesn't sync between servers and it's emptied if you restart the app. But it's actually very useful and performant. If that doesn't meet your needs, then Redis and memcached are popular choices.
Yes, but it's worse than that: there's no fixed ratio between the performance of plain text benchmark and "a normal web application". Further speeding up of the framework will have essentially zero effect on a normal web application.
I agree, of course Microsoft is going to try to sell and push their services. That doesn't make them bad or evil at all. The point is, they're doing it in a way that doesn't lock you in. You can still take your .net apps elsewhere if you want. I think now is a great time to be a .net developer.
Can you please suggest some solution that you like?
I did a bit of hunting around for that name and could only find info about the orchestrator pattern by itself, is this what you meant? https://www.simple-talk.com/dotnet/asp.net/never-mind-the-controller,-here-is-the-orchestrator/ If so, I'm in two minds about it. Adding more onion layers to a project is always annoying, but the testing benefits look damn nice. I'm working on a fairly large vanilla MVC project at the moment, and I've found that quite often multiple view actions can trigger the same bit of business logic; the orchestrator would be perfect to handle this.
Can't you just right click the project you changed and press build? It will only build that project and it's dependencies 
I think that what takes the most is when I start debugging.
The orchestrator portion is correct, creating a separate layer just for business logic I've found has helped out in the long run. Right now we are in the middle of converting a webforms project to MVC and patting ourselves on the back for keeping the logic in the business layer. Plus, as you mentioned, testing is extremely easy. The commander is similar but at a lower layer. It also lives in it's own later, with it's primary purpose being inserting only correct, valid data into the data store. It has no knowledge of other entities or how they interact, that is the orchestrator's job. How you build it is similar to the generic command pattern, but the gist is that all of the data gets validated before actual persistence, and the only way to insert data is through the commander.
Agree. Living in the .NET world I thought it necessary to use one of its solutions but Wordpress and Joomla are way ahead of anything .NET has to offer. And if you want to get something up today you can use Shopify, I have a customer using it and it took me about an hour to get them a slick site up and running.
Im going to go against the grain and say *nothing* about Magento (with plugins) is *efficient*. Its kind of like Wordpress, becomes a ball of mess after plugins that are poorly made and people end up paying a small fortune for "experts" to resolve. I'm not saying Magento or Wordpress are bad, but if you know .NET you probably shouldn't be using these types of solutions. For every good Magento install there are a thousand terrible ones.
I'm just importing the Google fonts by means of an html link and then in the css. What file would I want to copy to output directory in this situation? edit: steps 3 and 4: https://www.google.com/fonts#UsePlace:use
I remember seeing woff2 files - is this the one I should attempt to include?
I'll check that out, thanks
I'm not sure there are any font files I'm using. I'm just utilizing steps 3 and 4 here: https://www.google.com/fonts#UsePlace:use 
&lt;configuration&gt; &lt;system.webServer&gt; &lt;staticContent&gt; &lt;mimeMap fileExtension=".woff" mimeType="application/font-woff" /&gt; &lt;mimeMap fileExtension=".woff2" mimeType="application/font-woff2" /&gt; &lt;/staticContent&gt; &lt;/system.webServer&gt; &lt;/configuration&gt; those are the two entries you want. 
Use Dapper. ;)
Thought you might say that, in that case create a ContractResolver See: http://stackoverflow.com/questions/13588022/exclude-property-from-serialization-via-custom-attribute-json-net
I know it's not exactly what you're asking, but I'd highly recommend creating some business objects and not returning your data entities from your controllers. Not only is it better separation of concerns (which will make later work so much easier) it will save you a ton of headaches like this.
ASP.Net core isn't feature-complete yet. If you want a head start on future-proofing, go with core. If you want a full-featured server immediately and have no interest in (potentially) rewriting your app when Core changes, go with Web API.
That is pretty much what I figured. Just making sure the dev community knew something I didn't know.
Understand wanting to keep it simple, but honestly, a "service class" or "business rules" layer *is* pretty common practice in these situations, especially if you want to do any sort of unit testing. What I'm saying about the model is essentially to make sure that your DOMAIN model fits your *problem domain*. I'm questioning if you are fully understanding your problem domain, or if you are looking at your problem domain through a constrained view point that means you are designing to fit a single need rather than the actual problem. Maybe you really are just designing a system to track overtime hours only, but given the rest of your design, that didn't seem to be what you were working with. Sounded more like you were dealing with a time tracking system, and for something like that I don't think I would segregate the two "types" of hours and handle them separately. Working for one month = (presumably) 160 regular hours, while overtime is just hours over that that are marked and calculated as such. This allows you to use the same model for both, and not duplicate a lot of the same handling logic to handle two different classes of hours. Of course there are reasons to maybe make both a subclass of "Hour" and have "RegularHour" and "OvertimeHour", and that's ok, but I tend to lean away from inheritance nowadays unless it's specifically called for. Just my two cents. It's easy to get tunnel vision on a problem you are trying to solve and then realize that you failed to grasp the actual problem, just your view of the problem. I've found over the years it's better to start with a model that matches the problem as closely as possible or you are going to write a bunch of spaghetti. Of course that's what refactoring is for. :-)
would you store the DTOs in a seperate assembly? Current the project structure looks like this: * Database (EF Model) * Api (Web Api) * Client 
I dont think you need to, but I do. Also, if you are returning json a quick fix could be: GlobalConfiguration.Configuration.Formatters.JsonFormatter .SerializerSettings.ReferenceLoopHandling = Newtonsoft.Json.ReferenceLoopHandling.Ignore; Read somewhere that this isn't advised, so I would go the business object route the others seem to suggest.
well - it's not really a clean approach but you could put everything in your main window and control the flow via bindings of visibility properties. are you familiar with the MVVM pattern?
MVVM? That's new for me haha. I'm reading something about it right now. I thought to do this playing with the visibility of the control but i must create a too much controls and would be confusing
Thanks, i don't track working times actually. I'm trying to make salary tracking/planning software. Salaries are known, every month we need to see how much we need to pay to who. But people can work overtime so we need to record that too. So basically it will be like total "salary for this month = (monthly salary + (overtime salary per hour * total overtime amount))". Like you said it seems best way to have both repository and service layer. But i'm not sure how i seperate them. For example, in overtime service layer, should i call accounting service layers create method? If i do that how can i handle database transactions, should all repository classes use the same Db context? Edit: I actually found this great article about what i'm asking, maybe it can help someone: http://mehdi.me/ambient-dbcontext-in-ef6/
Create a `nuspec` file and define your license in it: https://docs.nuget.org/create/nuspec-reference
What platform are you planning on hosting the final / production web api app on? If Windows/IIS, there's not really a reason to go with ASP.NET Core right now.
Thanks! I'll do it :)
I have been using Core in production with IIS/Windows Auth for 8 months (yes, I know I'm insane). I see no reason to not go with Core. I have never had a problem. You may have to refactor some code come RC2/Release, but I have run into 0 issues.
Let's say that your solution contains 4 projects: * Foo * Bar * FooBar * BarFoo Let's say that the code in Foo requires the use of a class in Bar, that's called a dependency. In that instance, a change to Bar might mean that Foo needs to be rebuilt (but not always). If we extend this a little and say that Bar is dependant on some interface in FooBar, then changing the interface might mean a change in how Bar works, which would mean that Foo has to call the new class in Bar. Meaning that if you change something in FooBar, then it could trickle all the way down to Foo. What i think /u/cptrootbeer was asking you is how is your solution mapped out? If your models are used in a lot of places, across many projects, then each time you change and rebuild them the compiler has to update the references elsewhere and rebuild all of the code which depends on your (now changed) model.
For fun, yes. For work, no.
Is IIS important to you?
My team is currently building a new set of APIs and we chose ASP.NET Core 1.0 (wow do I hate that name). The learning curve really has not been that steep and everything has been stable.
My experience has been the same. I've been using Core on personal projects since beta5, and for professional projects since beta8. I have never had a problem, either. It's such a pleasure to work with.
This is a good analysis of what it really means https://stevedesmond.ca/blog/performance-is-paramount
Thanks I'll check that out!
Yep, with a combination of ContentControl, DataTemplates, ViewModels and UserControls you will be able to do exactly what you want.
There is general caching available https://github.com/aspnet/Caching however output caching is not available yet https://github.com/aspnet/ResponseCaching
Yep, thanks. The Memory cache that I linked to is [in the first repo](https://github.com/aspnet/Caching/blob/dev/src/Microsoft.Extensions.Caching.Memory/MemoryCache.cs)
No, they are retarded.
Ah, yes. Great point. 
Thanks for that information. 
I'm going to be using Azure so either is an option.
It'll be Azure. Yeah after a bunch of looking around, I think I'm going with WebAPI and keeping an eye on .NET Core.
Question 1: Do you have a budget? A budget will help a firm decide what can be included or not, but also shows you're serious in this endeavor. Question 2: Do you want a fixed-fee project? Fixed-fee means you should know final cost upfront (not always 100%), but you better make sure your requirements are fully detailed and are what you truly need. Don't expect a little change here, or there. Those little changes add up over time and sometimes what you think is a little change ends up being quite big. This type of project will require detailed requirements and usually ends up with multiple change orders during or after the initial project due to finding more use-cases or needing to change your initial assumptions. Question 3: Do you want a time and material project? Time/Material means you don't know final cost, but with the right firm you can get a solution that is a better fit as usually during development use-cases and needs will change. Usually Time/Material will require more of your time during development as you start off with grand-vision of what is to be accomplished, but the details of each feature will be researched/designed/developed in small (2-4 week) iterations with a usable solution ready for you to test. Based on short iterations, you can prioritize most important needs upfront and if going longer/costing more than you like, you can end the project but still end up with something of use. It might not have all of the features you want, but you get something instead of nothing. You can start out by reading up on [User Stories](https://www.mountaingoatsoftware.com/agile/user-stories) P.S. Don't ask for a programmer to estimate the cost, and then turn around and make that a commitment. Estimate does not equal Commitment. TLDR; The more detailed the better.
Where is SignalR 3 on myget? I found Beta5 in nuget though https://www.nuget.org/packages/Microsoft.AspNet.SignalR.Server/3.0.0-beta5 is there newer version? 
I'll say that getting ASP.NET 5 working in IIS is hit or miss and isn't easy. Sometimes getting the HttpPlatformHandler to hand off the request doesn't work for security or path reasons.
Interesting. I ended up settling on Web API 2.2 simply because the migration to .NET Core looks to be promising, and everything is pretty well stable. My next project I'll probably start .NET Core however.
If I had to guess, that CC cleaner mucked some things it shouldn't have. My recommendation would be to reinstall that component if possible (repair from Add/Remove programs maybe?) and let the installer do the under the hood repairs to the configuration of the system. If that is not possible, then a good, hard look at all of the touch points between the .NET engine, IIS, SQL and ReportViewer would need to be looked at to determine what configuration setting is not like the others.... It also could be something as simple as CC cleaner deleting a registry key that had a defined value, Windows detecting that it was missing recreated it with a default, but incorrect value. There are seriously a lot of things this could be.
ReportViewer is nothing but a ASP.Net web control, there is no component installed on his machine (not that I'm aware of at least). It's just a web page he's trying to view - I'm really stumped on why this message would appear on his machine only, indicating a server problem (web.config issue)
Wow. That's totally messed up. I don't think that CCleaner is the issue though since nothing should be installed on the client.. 
Perhaps his browser is now in a configuration where the AJAX components on the ReportViewer are making requests to a different end point that your other users. Is that line in your web.config as the error message suggests?
You can use Core while using standard .NET 4.5 libraries, as long as you target net451 platform.
Under project.json, use any framework that doesn't mention "core", such as dnx451, or net451.
Were you using the preview version (6.2.2-preview) of the Azure Storage library? https://www.nuget.org/packages/WindowsAzure.Storage/6.2.2-preview The current stable release of the library does not support it, but the preview version is supposed to and has Core rc1 listed as one of their target frameworks on the project's page. https://github.com/Azure/azure-storage-net/tree/master#target-frameworks
Could other users actually being loading from cache instead? I know CCleaner does quite a bit of browser cache cleanup and he may be the only user doing a full request so far. Have you checked the server logs and event viewer to see if this error is being logged as a server issue?
I don't know why people expect anything from .Net Core right now. If this was the "old Microsoft" you wouldn't have even *heard* of it until the product was finished and shipped. We're simply getting a look at how the sausage is made.
Core is not there yet and probably will not be there for at least 2 more releases. Problem is not with the framework itself but the surrounding ecosystem. When core was announced, I got excited and started with a POC and midway I realized there is no way I can finish it with the existing set of compatible libraries and features. Only very simple projects with very few or no external dependencies can be done right now.. But I have high hopes that it will get there in the next couple of years
Yeah! I was kind of gobsmacked by the thread yesterday, where everybody was saying *I've been using it for months now in production*. I tried it on a project as well, and ran into various show stopping bugs. In regards to Azure storage, there is a prerelease version which is *supposed* to work, but we still had trouble with it. I'm keeping core on my radar, but sticking with ASP.Net 4.5 for the time being! If you are hosting on IIS only, and developing on Windows, I don't see any massive advantages to core at the moment. Also client side, you can still use npm/gulp/bower etc in 4.5, there's nothing stopping you, and tools such as the task runner in VS2015 work just as well in 4.5. Don't get me wrong, there is a tonne of stuff I like in the new and shiny, but I can wait a year while they sort out the kinks.
I tried this on a project and then still ran into some show-stopping issues.
Pre-release? According to their roadmap, RC1 (which is out) is: &gt;The focus for RC1 will be on polishing existing features, responding to customer feedback and improving performance and reliability. The goal is for RC1 to be a stable and production ready release. 
Use 4.5.1 as your target platform, should really be using this until more package authors start making their stuff fully compatible. With that said, it's in RC1 with a fairly major change coming in RC2 (DNX is getting replaced by dotnet CLI). So it's not like you're working with a finished product right now.
Fair enough, but I was more responding to the "complete" comment. The final beta was literally titled "Feature Complete".
https://support.microsoft.com/en-us/kb/3086251 4.6 has been patched. You can download the update there.
Yes, production ready - Release Candidate. Version 0.99. Good for you and good for Microsoft. I'll wait for Core 1.5 or 1.1 or 2.0 or 3, whatever next release will be called with amazing Microsoft naming conventions, before starting to use for real projects. This thread is about "common sense VS what Microsoft said".
Why not 4.6 or even 4.5.2?
I hear you there. The transparency is great. And thinking an entire (well, even a partial) port of the .NET runtime and BCL to Linux isn't a one year project. Or even two or three. Ask Mono, they've been at it for over a decade? And the developers who are already kicking the tires and seeing what it can do are great. They're helping along the way. I expect nothing but more awesome down the road. #ifdef NETCORE Oh, and a lot more of these. #endif 
That's great, thank you 
Still pretty outrageous on license fees. Looks like I'll be sticking with http://layercake-generator.net/ or https://www.my2ndgeneration.com
Samba is the closest you will get. You need an entire AD infrastructure: Kerberos, LDAP, replication. Samba 4 can mostly get this working, but it's pretty buggy.
2 years? Really?
Care to share? At that point, the only difference is the DNX assembly resolver, which shouldn't be a big deal.
[Read this](http://www.hanselman.com/blog/PublishingAnASPNET5AppToDockerOnLinuxWithVisualStudio.aspx) for one possibility. Another reference of interest is [Windows Server Containers and Hyper-V Containers](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/manage_docker). Note that containers are not yet cross platform, if they will ever be (e.g. Linux + Windows), unless universal containers come in some future version.
But as someone who creates web sites, APIs, excel plugins... What does it change? I understand Azure and the benefit of publishing to Azure ... Is docker more for when you have many pieces of software running together on an OS ? I'm confused still afte rlsitening to podcasts about it
I got my own machine at home with VS Community / code, Free Dev Essentials program (https://www.visualstudio.com/en-us/products/visual-studio-dev-essentials-vs.aspx), Free azure subscription. Git hosting on Github (if opensource), for other private stuff I host my own Gitlab instance on a dedicated linux machine at leaseweb (30$/month, but also used for LAMP software / teamspeak server). For continuous integration/testing/building -&gt; travis and appveyor (if opensource and hosted on github). 
I used this framework extensively on my last project in a heavily customized fashion. I found it largely very awesome, and the guy who runs the project is very responsive to bugs and questions. The real interesting thing is there are many automated QA tools that consume swagger that allow your QA Team to define their test cases directly from the definitions.
Incase anyone was curious - I needed to remove bootstrap.min.css file - not sure how they got there or what they do.
Incase anyone was curious - I needed to remove bootstrap.min.css file - not sure how they got there or what they do.
As a DevOps guy, docker is a nightmare, not "big". The whole "point" of docker is to bundle your dependencies (this includes the whole OS libraries you require) with your application in a docker image. Something has a security vulnerability, you need to remember to rebuild your container and redeploy it. Docker is awful from a security perspective, package your software properly - if you really want the isolation docker provides to run multiple instances or something then fine, but I'd still package your software properly and just have the container automatically install system updates and your software on launch.
Docker is more aimed at web applications. You can containerize various desktop processes, but it's aimed more at server applications.
Wow, I totally missed the point of your real question. Real answer now: There's different strategies you can go with modelling a complete problem domain. With entityframework nowadays, you often see that the data model (layer) is also the business layer. Which is a design flaw in my opinion. It's impossible to create a proper data model which is also at the same time exposed as your business layer. So. in case of a simple asp.net mvc app. In your mvc controller, when talking with your business layer, you don't even have access to a dbContext; only your domain (business) model. The public aggregates and entities in a domain model often don't even have any public accessible properties, because they are not needed. You don't QUERY on your domain model, you act on it. And public properties are usually also a big NO. (exposing a public 'decimal Balance' on 'Account', that's a flaw). In your example. You would have a readonly (entityframework) model of the data in the db, available to users, used by the mvc app for querying/displaying data. For your domain model, you could call into a service e.g. timeRegService.RegisterOverTime(userId, overTimeParams). It's the job of this service that all the 'business logic' get executes. Read data from db, calculate, save to db, call into other services, send out notifications, etc. Try to model your domain as closely to the real world scenario as humanly possible. In this case, the 'act' of the employee registering overtime in the application, should trigger the creation of a corresponding accounting record. So in this case: - A client application to register overtime - Something to 'act' on (overtime service and/or the employee) All the rest are business rules applied after the fact (overtime has been registered). Other ways to implement this are using an even more real world model, using 'Employee' as the aggregate root: emp = repository.Get&lt;Employee&gt;(id); emp.RegisterOverTime(params); In this case, Employee is NOT something that is directly stored in the database, but a business class your application exposes as a public API to use and consume. In our example, it only has one method, no properties. It all really depends on the overall domain and application / system you are creating. If it's a simple domain, and using common tools (mvc/ef, migrations / data annotations) I would go the 'services' way. Hope that helps (again) &lt;3 --- Previous answer There is a lot of material on these subjects, so I won't go in depth. But one thing to remember is that the **M(odel) from MVC is NOT the Business Model**. Actually, the MVC pattern doesn't even concern itself with business logic, because it lives within a different architectural layer: the User Interface (or presentation layer). A basic system has 3 layers: - Presentation - Business - Data Within those layers, depending on use-case, platform, etc. can exist a complete own subsystem, with it's own models and patterns. MVC is all about orchestrating how and when (controller) some data (the model) is to be presented to the user (view). the pattern is often used in web applications, to clearly separate the view (ui: html/js), from the controller getting and transforming the data (model). If you really want a clear separation between your presentation layer and your business logic layer, you should view it as such: a complete and separate layer from the rest of your system. Strictly speaking this means, that you cannot put any (bussiness logic) code in the models you create for entityframework/MVC app. Personally, around when MS released the asp.net mvc framework (the MVC pattern is actually way, waaaay older), I have started to call the 'model' from mvc, the 'viewmodel'. So it's, view &lt;-&gt; viewmodel &lt;-&gt; controller. This is, to clearly state that it's the model (data) shared with the view presented to the user. It is NOT the thing which is actually stored the database. In most cases even, the interface shows data in a completely different way then it is actually stored. Therefore, you often see the use of mappers (AutoMapper) to map database (or business) classes, the business model, to the actual (view)model classes. For example: - CrmService.Data.Customer (retrieved and stored in db with entityframework) - CrmService.Customer (business object representing a customer) - CrmService.CustomerService (create, and act on customers) - WebApp.Data.Customer (readonly from database with entityframework) - WebApp.CustomerController - WebApp.CustomerOverviewModel (for index.aspx) - WebApp.CustomerDetailModel (for view.aspx) from your CustomerController (in WebApp) you can: - present data to the user, by directly querying from the database (query on WebApp.Data.Customer and others, but NEVER save) - Call your business logic (whatever that may be). E.g. Create customers, change address information, placing orders, etc. From within your business layer, you use 'CrmService.Data.Customer' which is read/write, so you can actually save the changes you make in your business logic code. You could even force yourself by creating a read-only user in your db and use two different connection strings. One (read/write) for the DbContext in Business.Data.* and one (read-only) for use in WebApp.Data.* I know, MS makes it really easy to get started with mvc, entityframework and all, which is a good thing actually. But when diving into topics as architectural design and patterns, you should really be careful not to try and mix those with the "getting started" guides on those frameworks. E.g. you can actually have multiple DbContext live within one web application (to support above scenario's). I don't know how well EF migrations handles this, but that's the cost of good architectural design; scaffolding / tooling is not going to be of too much help here. Hope this helps &lt;3
 As an example the Entity Framework is an implementation of the Unit of Work and Repository pattern, so why would someone create another repository layer over Entity? That is a point that is constantly argued amongst dotnet developers as well. Some like the extra abstraction layer, others don't. So you don't need to if you don't want to. Another item of confusion coming from JPA/Hibernate where you have to save the foreign key reference (the actual object, just not the id), however with Entity it appears you can just set the ID for the FK. You can do the same with EF. You can assign it the actual object, or the ID. Another design preference.
So as someone who creates web sites and publishes them to the company IIS server or to Azure there is nothing there for me ? 
Ok so for example I could be working on some app on my local machine and maybe I have to install some various pieces (services) that also work together with the web site.. So in this case rather than try to take a new machine and perform all the steps the DOCKER would help ? Or I could just take a snapshot of my machine ? Maybe that is what confuses me... In the scenario above what does DOCKER do that taking a snapshot and making a VM not do ?
That question gave me an immediate headache in my left eye. For most PMs and developers, every project is different. So... what are you asking?
You cant use ToListAsync if you remove AsEnumerable either?
To be honest I hate products that don't list their prices upfront. All that means is that different people get different prices. There is no reason for that and in my opinion is very shady. Just list your prices and don't put a stupid write us to get a quote nonsense. 
Glad someone agrees. I got all hyped about docker when it came out but the security nightmare makes it not worth it. If you really want lightweight VM's (read: no additional kernel running) to isolate your applications OpenVZ does a great job and you can manage them like a standard box, meaning puppet/chef/ansible/salt whatever can do the heavy lifting to configure and install your application - and most importantly KEEP EVERYTHING UPDATED.
So if my web app relies on RabbitMQ which in turn relies on Erlang, bundling them together in a docker file would result in faster deployment, quicker scale out and better isolation? Or would I put my web app in one container and RabbitMQ + Erlang in another and then host them both on the same hardware?
What about using Task.Run()?
Not sure honestly, I didn't think it had full windows support but I could be mistaken there. From using azure and the deployment options provided out-of-box with VS I get a lot of the kinds of deployment management docker would do it is just a very different way of looking at the problem. I would say the advantage might be that if you like the work flow and tooling or you already are leveraging that tooling you could extend it to all of your infrastructure.
I'd like someone to explain to me what the point of containers is at all. The claimed performance gains don't exist. "VMs on x86" was the Holy Grail of computing for decades and I see containers as generally a huge step backwards. Running on bare metal fucking sucks and VMs saved sysads endless hours of fighting with hardware. I have no idea why anyone wants to go back to this. 
&gt; Docker is awful from a security perspective, package your software properly **THIS.** The reason why people think Docker is "good" is because DevOps guys think they can download random containers off the Internet, deploy them in production, and imagine they have no security or scalability problems at all. Then they get their first security audit and run into 1000 security vulnerabilities they have no idea how to fix. I have literally seen this exact scenario play out multiple times as an auditor. 
Agreed. I found a few bugs from the PVS alpha/whatever they released a couple months back, but when I then went to go find the price, it wasn't listed, so I decided I wasn't interested.
The thing that irritates me the most is they are literally trading one packaging issue for another. Look, making an RPM or a DEB or MSI package isn't that hard. Just do it instead of spewing a bunch of crap into the filesystem outside the control of your package manager. You need to vendor your dependencies because you don't want to package the 20 additional libraries you are using, okay, well since no-one else is packaging them I guess it's not the end of the world - but docker is solving a deployment problem nobody should HAVE in 2016.
So, Azure is fine, but it is a Microsoft-only cloud technology. If you want to deploy to Azure, they are the only game in town and if you have made your application dependent on Azure, then they are your only option. Docker however is a standard way to package up your application image and use it in *any* cloud environment that supports Docker. You're not stuck with a specific cloud provider. You can use your image with a number of providers, all at once if you wish for the sake of redundancy. Do you see the possibilities there? Now, just to clarify - Your Docker image must contain all the resources you need for your application. If you also want it to work with a database, or other technologies, then your cloud environment will need to support those as well. So, if you make your image dependent on SQL Server or Oracle or the like, then your cloud provider will need to provide those as well. Caveat emptor....
How are you building/invoking Roslyn? How big is the solution? If anything, I've noticed a modest performance improvement for Roslyn builds, but I can imagine there are situations where it might be marginally worse. I wouldn't expect a 2X performance drop, that sounds like there's definitely some problem.
"But maintaining a package is HARD!" The whole point of Docker is being lazy and sloppy. 
The main reason it's good to add an extra abstraction layer around Entity Framework is to isolate your Data Access Layer. Imagine after your first release there are huge performance problems with EF, and you have to swap it out for another approach. You'd just write another repository implementation and swap it in - the rest of your code would be unaffected. Some people hate that idea though, I'm sure someone will show up below to explain why.
The fuck they haven't. Part of the whole pitch of containers over VMs is that they have performance advantages over VMs because they're running on the bare metal. 
I've heard it as cheap potential performance - e.g. load is increasing, now you can choose to create more small instances quickly and at-will, allowing for a cheap, scalable, performant application. They aren't better-performing, but cheaper and changable.
It's not. That said, Docker allows for substantially higher efficiency than VMWare or any other VM system because Docker allows multiple containers to run on an actual machine. This avoids the double whammy overhead of a) running VM calls through a VM execution process into the host OS and b) having to host a separate copy of the OS with all the related overhead for each container. So, while Docker images are sandboxed containers, they do not each get their own VM. This is one of the primary reasons they are worth anyone's time.
Which makes sense if you want to run a bunch of the same thing, like you want to run 100 Apache instances on your set of hardware. That's something that a SaaS provider or something like that would do and while I can see that use case for containers (basically wanting to squeeze all the performance you can, and only needing to run one app), containers are sold as something for general computing and I just don't see the advantages they have over VMs for anything but a handful of edge cases. 
&gt; Docker allows for substantially higher efficiency than VMWare or any other VM system because Docker allows multiple containers to run on an actual machine. First, that performance hit is minimal (1%) on everything but I/O. Basically only databases are affected. This is based on very extensive experience. I've only had one job, which involved real-time audio recording, where that hit was enough to pull the databases out of VMWare. Second, even for databases, this is largely meaningless. Hardware is cheap, staff costs money. Running on bare metal will mean your expensive staff will spend more time fucking with hardware and will result in a net loss. This is why every large company ran screaming to VMWare. As I said in another post, I can see containers in the niche where if you want to run a lot of identical instances of an app (say 100 Apache instances), and you need to keep them security-separated as in SaaS. If you don't need that security separation, why not just run a bunch of copies of Apache? 
&gt; They're not even in the same realm. Over 50% of .NET development will target Linux within 3 years. I'd bet my life on it. As soon as our document management software stops using arch-specific libraries I could port 100% of our software off Windows. And I will do it, because while DSC makes things slightly more tolerable managing IIS is a joke - we pay a couple thousand a year for Octopus Deploy because trying to package ASP.Net applications as MSI's is a giant nightmare - RPM specs are much easier, and puppet feels substantially less janky of *nix boxes.
No, Samba 4 can get pretty close to re-implementing active directory but it's not really there yet. If you want a unix-native directory solution look into FreeIPA, you can establish a trust with an AD directory that allows your FreeIPA users to be treated like any normal AD user, and your Windows application servers can just be joined to that AD domain. Not sure if that helps, let me know if there's something specific you are trying to accomplish.
I'll look into it. Not enough time to dip into everything new....
I am curious what the caller is doing here. Return a Task&lt;JsonResult&gt; and await upstream or return async tasks all the way to Http request (assuming this services an AJAX call). You may not be able to call SQL asynchronously, but you can just .ToList() and make the wrapping function async.
"Microsoft .NET Framework 4.6" appears 37 times on that page. In particular, at the end of the page: &gt; Article ID: 3086251 - Last Review: 12/03/2015 11:40:00 - Revision: 3.0 &gt; **Applies to Microsoft .NET Framework 4.6** That *is* the official statement.
But RyuJIT and Tail call optimizations appear zero times. I know that this is **a** patch for .NET 4.6, but the KB does not flat out say that it fixes the RyuJIT bug (in true MS fashion). 
&gt;so why would someone create another repository layer over Entity? Ignorance 
A lot of people are focused on the performance of containers over VMs or on their small size. Although those things are cool, they aren't really what make containers cool (not by themselves, anyways). What makes them great are the advantages they give you in deployments, portability, and scaling. Right now you probably have a VM for every app/service that you host, and you have to manage each VM. You have to install your app's dependencies on each one, including monitoring, logging solutions, etc. You have to start and stop servers. Each one probably has slightly different procedures that you have to setup in your CI system or perform manually. These procedures often make it so that you can't run multiple services on one VM, so you end up with at least one VM for each service. Then when you need to scale, you have to scale entire VMs. What you end up with is a ton of VMs to manage, each with slightly different configurations, and tons of different CI workflows. What docker gives you is a deployable container as a build artifact. That container will run exactly the same in dev, test, prod, etc., and it is deployed in exactly the same way as all of your other containers. All of your service's dependencies, not including external dependencies like databases, are included in the container. There is virtually no setup required to run a container. You just have to make sure external dependencies like databases are available (either in another container or via cloud services). When using an orchestration tool such as Kubernetes, you can simply configure a cluster of VMs and then deploy your containers to Kubernetes, which will take care of running the containers on a VM in the cluster with some spare capacity. Now scaling becomes very simple. If one of your services isn't handling the load, then simply instruct Kubernetes to add another one. If your entire VM cluster is overloaded, then fire up a new VM and it will register itself with Kubernetes and start running containers. Think of it as a separation of concerns between how you build your apps and how you run your apps. You could build 5 different containers on 5 different languages and frameworks, even different versions of Linux, and they would all still be deployed and run exactly the same way. You build ONE CI workflow and ONE deploy workflow and that same workflow is shared across all your apps. I hope that helps.
Whatever your distribution of choices package manager is - THAT is the standard for deploying your application to the cloud. Make an RPM or a DEB, anywhere you can run a VM you can run your application. If your cloud provider insists on using a docker container then go install your package when building your image instead of spewing crap all over the file system - and then have fun keeping your containers updated with security updates.
It just feels like an incremental improvement over virtualization. Yes, easier to orchestrate and scale than VMs, but so is a micro service running on a PaaS resource that scales automatically on demand. The platform provider certainly stands to gain from compute density and tenant isolation, but more and more the machine the container represents is becoming irrelevant to the developer. The advantage that containers bring to the typical .NET developer is trying to answer abstraction at a level like arguing HD DVD vs. Blueray, where the clear winner is streaming media. Sure, a few hipsters cling to the brief nostalgia and collectible quality of Blueray, but the vast majority are already moving on. Containers almost seem more relevant at the client end where OS config disparity and update deployment at massive scale is a real concern, but for hosting a web service or app? Who is really going to be watching to instruct Kubernetes to do anything when peak load is hit? Is there a simple API and management backend to handle triggering scale up/down based on container performance counter aggregates? I'll pay for automated scale by the drink and avoid dealing with extraneous, non value added details of container infrastructure (which ultimately requires managing owned hardware also) to the platform I'm building on. I say the .NET ecosystem in a couple years will basically demand you pick a cloud.
They're different worlds but it's probably more comfortable to learn .NET in general rather than restrict yourself to .NET Core (the linux libraries). .NET Core is still very new where as .NET on windows is a very well flushed out technology. 
Imagine running a VM, the OS always incur's a certain level of overhead and licensing costs. In a container world, you only incur that once and then have multiple containers on top of that original image that all seem like independent environments or virtual machines but aren't incurring the penalty of a full operating system running on each. It's like virtualization but more efficient. From a dev standpoint imagine having a base machine setup and then you do all the configuration and package that up as a container. Now all you need to do is ship the layer (image) which is overlayed on top of the previous image. This means that rather than a full blown VM for every app configuration everything is scoped only to the immediate needs of the app.
Imagine you have a bare metal box that you've installed 10 VMs on. Each one of those VMs needs an OS and incurs some penalty in running that OS. A container acts as a scoped environment on a single OS so it's similar to running 10 different machines that seem like unique isolated environments but are actually only incurring the overhead of a single operating system install. It's not a perfect technology by any means but it does have a lot of potential. 
&gt;I say the .NET ecosystem in a couple years will basically demand you pick a cloud. And in a couple years that cloud infrastructure will likely all be container based instead of VM based. Just because it doesn't directly impact your life writing code doesn't mean you should be ignorant of the technology. 
Try adding a space in front of the dot. I can't remember why but there's a spec out there that says something about that and I've had that work for me.
Yes. No can do. Says the type of *response* does not have any extension ToListAsync. Also, how does using AsEnumerable() magically allow me to perform ToString() and such inside the linq?
I could do that but does it serve the purpose? I mean if that was the way then we wouldn't have extensions like SingleOrAsync() or ToListAsync, etc. But I am no expert though. Feel free to correct me.
I am sorry but I did not quite get you. Could you please explain again?
I changed my operation above to look something like this now: public async Task&lt;JsonResult&gt; GetProjectData(int projectId = 0, string fromDate = "", string toDate = "", int employeeId = 0) { //removed for brevity var response = await shifts.Select(s =&gt; new { s.ProjectId, s.StartDate, s.EndDate, s.StartTime, s.EndTime, ProjectName = s.Project.Name, ShiftName = s.Name, ShiftId = s.Id, Employees = s.EmployeeShiftAssignments.Select(c =&gt; new { id = c.Employee.Id, name = c.Employee.Name, isAccepted = c.IsAccepted, isChangeLocked = c.IsStatusChangeLocked }) }).ToListAsync(); return Json(response.Select(d =&gt; new { start = d.StartDate.ToString("yyyy-MM-dd") + "T" + string.Format("{0}:{1}:00", d.StartTime.Hours.ToString().PadLeft(2, '0'), d.StartTime.Minutes.ToString().PadLeft(2, '0')), end = d.EndDate.ToString("yyyy-MM-dd") + "T" + string.Format("{0}:{1}:00", d.EndTime.Hours.ToString().PadLeft(2, '0'), d.EndTime.Minutes.ToString().PadLeft(2, '0')), title = d.ProjectName + " - " + d.ShiftName, color = clrHelper.GetColor(d.ProjectId), shiftId = d.ShiftId, employees = d.Employees }), JsonRequestBehavior.AllowGet); } Is this appropriate? Basically, it wasn't letting me do all the ToStrings and stuff inside the linq so I got the data from database asyncly and then did my ToStrings in another projection to reply to the client.
I came across this a while ago, might be overkill for what you need or might just be exactly what you're looking for http://aspnetboilerplate.com/ . I've not had chance to use it personally yet but it looks good.
The major issues have been solved, and 4.6 has been superseded by 4.6.1. https://blogs.msdn.microsoft.com/dotnet/2015/11/30/net-framework-4-6-1-is-now-available/
https://code.msdn.microsoft.com/
Well I'm already about 2 years comfortable with .NET MVC so I'm at that point to expand my knowledge. What I've been learning lately are better CSS techniques since I've never done much FED work (web development) and wanted to learn better javascript as well, but I think .NET Core sounds pretty interesting.
This does seem reasonable, but I wouldn't be surprised if all the overhead of the async methods and such actually made it all slower. In my experience, it seems like (especially over the web) there seems to be a pretty decent sized critical mass that has to be hit before all the async stuff will actually work out to be a performance increase. But I'm assuming you've profiled and this is the right approach, so I would probably implement things in a similar style to this personally. My personal opinion is it's a better solution than trying to make the ORM perform operations it's designed to not perform. It also more clearly identifies it's doing that join, which I like because it gives nice hints where to look in case things get nasty at the database level some theoretical point in the future. Plus, if necessary, you could refactor this out to a view without having to spend a ton of time fighting with this. The other thing is if you want to output a sortable DateTime, you can more easily just do DateTime.ToString("s") or, if you need 8601 format you can use the "O" format. Right now, dependent on the nature of the DateTime kind you could actually be incorrectly omitting the time-zone specifier to indicate a local-time offset (or that you're UTC), which could confuse a consumer and introduce latent/small bugs :)
But in the new world with Azure PaSS if I am using Azure to publish my pieces to then none of this matter to me right? 
Yes. I do use my own code now. But I want to find code that is written at a high level and emulate that when I can instead of doing things my dumbed down way 
Correct usage of `async` will always be slower (usually by a difference not noticeable by people, but perhaps noticeable by tools that measure it) in the use cases where the environment has enough power to do the entire thing synchronously. 
It depends... I don't know which 300 free credit offer you're talking about. For MSDN subscriptions they get $50-$150 a month in free credits, but they're explicitly for DEV/TEST environments only ([See FAQ](https://azure.microsoft.com/en-us/pricing/member-offers/msdn-benefits-details/))
Take a look at [this](https://github.com/martijnboland/VSReact). It is really cool, helping you understand webpack, react (javascript components) and babel. I've managed to get this to work inside an .NET MVC project. I like this approach more than the heavy-handed React.NET codebase. I have more examples if you are interested.
&gt; .NET core is a limited subset of the framework. It is only limited related to web server hosting. No UWP apps with it. Yeah, a few namespaces are missing. I'm sure they will build on .NET Core with future versions (you can't do everything in a 1.0 release).
Windows once supported POSIX subsystem, NTFS still does (but is being slowly deprecated). It wouldn't be toooo hard for Microsoft to do that again.
&gt;But yes, I'm willing to put that money where my mouth is, but I will clarify that I meant "50% of new .NET development". Obviously I don't think &gt;50% of existing .NET will be ported. That would be fucking absurd. :) How would you measure "50% of new .NET development"? Can you show me what the current "50% of new .NET development" is today, empirically? 
Conflicts have nothing to do with "who" made the change. Only that the same line(s) have changed in two different versions. Are you trying to merge branches and seeing this? This is the only time that should happen (if you are always working on the same branch as the same user on the same machine this shouldn't happen). If you are using multiple machines, make a change on machine 1 and check it, then make changes on machine 2 WITHOUT getting latest first you'll probably also get a conflict when you try to check that in. Another possibility is that you have some kind of agent that does something to the files in source control.. 
The purpose are creating micro-instances that are way "cheaper" than VMs. While providing the same isolation and security (in theory). Deploying VMs is a very slow and time consuming effort. You can think of containers as being able to deploy the app&amp;host parts without the majority of the OS parts. Mostly these changes will lead to better deployment stories and better hosting stories as a hoster. But if you're not a commercial hoster and you don't do high capacity computing across giant clusters the containers won't likely offer you anything immediately. With one of their earlier notices about Windows Nano Servers, they stated that the VHD size was 93% smaller, saw 92% fewer critical security bulletins, and 80% fewer reboots. Yay for less patches needed.
RoboCopy/XDeploy is a kludge when you have more than one system to worry about. Octopus Deploy is proprietary, unfortunately.
I have also seen if you keep certain types of files in source control like user VS settings, resharper cache objects, etc, they can really wreck havoc on your commits since they change all the time and typically the same lines. It is for that reason as well as they are only really specific to my machine, I exclude settings files from source control. 
&gt; This security update resolves vulnerabilities in the Microsoft .NET Framework... see Microsoft Security Bulletin MS15-092. This patch addresses bulletin MS15-092, which says: &gt; The security update addresses the vulnerabilities by correcting RyuJIT compiler optimization for .NET Framework. For more information about the vulnerabilities, see the Vulnerability Information section. Addresses [CVE-2015-2479](http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-2479), [CVE-2015-2480](http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-2480), and [CVE-2015-2481](http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-2481).
Sounds like something isn't right with your TFS setup? Do you run it in-house or are you using the MSDN hosted one? You might try checking everything in, and creating a NEW branch from the existing one and see if it keeps happening there. Otherwise I don't know. Personally I do not like TFS and only use it for one personal project because it was free with my MSDN. I figured I might as well give it a try so I would know what I was talking about and I am not impressed with it. 
Are you getting the latest version of the branch before you checkin?
Isn't that what this is doing too, just in a more round about way? &lt;form action="" method="post"&gt; &lt;input type="submit" value="Save" name="action:Save" /&gt; &lt;input type="submit" value="Edit" name="action:Edit" /&gt; VS &lt;form action="" method="post"&gt; &lt;input type="submit" value="Save" formaction="MultipleSubmit/Save" /&gt; &lt;input type="submit" value="Edit" formaction="MultipleSubmit/Edit" /&gt; Or am I missing some added value?
Throw everything at me. I am definitely interested. Will take a look through everything when i can. Im commit5ing qn hour and a half every day to learning right now. 
Sometimes yes, sometimes no, it doesn't seem to matter.
Do you have multiple workspaces set up for the same working directory? That would cause it.
Yeah we have a strict policy to keep anything that's not absolutely required for the build out of source control. Anything generated by resharper and .usersettings are excluded for that exact reason. We actually also configure resharper to keep caches in %PROGRAMDATA% rather than in the same folder as the solution.
Make sure that you are saving files with the same encoding as what's checked in. It's possible that it recognized the file initially as UTF-8 but you VS is configured to do ASCII or something like that. We use Perforce in-house and sometimes this happens. It's easy to fix in Perforce though.
No, just one 
You can use IdentityDbContext for user management and one (or multiple) other DbContext for the rest of your application. EF Migrations also supports multiple different DbContexts and migrations, living in different directories. (although, when using the powershell cmdlets, you WILL have to use the parameters to specify which Configuration to use, else migrations won't know which configuration / dbcontext to use)
Bad advice. async is not about threading. You don't want to have Task.Run in any kind of library / service layer. That's for UI/application layer only.
The latency will be higher, yes. But it allows for more throughput as there are more threads available to service new requests. Which, depending on the use-case, can be perceived by the user as having more 'performance'.
It's a setting in Resharper somewhere. You can configure where it stores those caches to keep it from polluting your workspace. Makes it much nicer. 
Are you using the lock file on edit feature? That can cause issues.
Derive your context from the identity context and you have one context for both. Like this: &amp;nbsp;&amp;nbsp; public class ApplicationDbContext : IdentityDbContext&lt;ApplicationUser&gt; { public ApplicationDbContext() : base("DefaultConnection") {} public DbSet&lt;Product&gt; Products { get; set; } public DbSet&lt;Payment&gt; Payments { get; set; } }
Running on that bare metal server you've created a lovely single point of failure as opposed to that VM running in vSphere. Containers make single point failures even worse, because now you have this teetering pile of containers all resting on a single piece of hardware that if it dies, everything collapses in a heap. With vMotion, one of your servers dies and *nothing happens* because you can seamlessly shift the VM to a different server. **This is the fucking Holy Grail.** I used to spend half my time fucking with hardware and now I haven't *touched* a piece of hardware in years due to the glory that is VMWare. &gt; Each one of those VMs needs an OS and incurs some penalty in running that OS. It's 1%. Hardware is cheap, so I don't give a crap about OS overhead. Disk space is cheap, so I don't care about the 32 GB or whatever the OS uses. You know what is expensive? My time. What I care about is my time spent fixing systems that keel over and that's vastly easier with VMs. And that's not my *opinion*, that's an objective fact. Staff costs are typically 10X (if not more) the cost of hardware. &gt; A container acts as a scoped environment on a single OS Which is [chroot from 1979](https://en.wikipedia.org/wiki/Chroot), a workaround for problems with the security model in Unix. Huge step backwards. 
So I figured out what was wrong. When logging in (and setting the cookie), I was sending a post request to a different domain than the one I was currently on (profile.teknik.io/Login). This for some reason was not setting the proper cookie, so no auth was occurring. Once I moved the login to the parent domain, the auth works correctly across subdomains.
Great input, thank you. Sorry for the slow follow up, got hit with the flu. This initial scope is really to get the work quoted out, so I wanted to be as upfront and clear as possible as the quote will determine if it will get approved or not, so your feedback is perfect. 
I'm absolutely in the discovery page. &gt;but try to avoid thinking of implementation (i.e. user interface). This is tough - kind of, because the entire goal for the project is a user interface. &gt;You'll then want to target a technology platform. Since you're posting here I'll assume .NET but is this a web application or a windows app? I have basic experience with .Net, so I'd like it to be developed in that, since I can maybe help support it after the fact. But I'm really open. &gt;Then keep in mind when you do hire a developer. Thank you for this, I think for the layman looking to develop something, this is overlooked. I've had some bad experiences with developers in the past due to people not providing the right expectations to start with. 
Yeah, don't do that unless you have parameterized things for multiple systems and functions.
&gt; MSDN hosted one When you say hosted do you mean visual studio online? Is so, do you know you can use git with TFS there? All these problems are TFS Version Control related. 
It's a very vague thing I remember causing an issue several years ago. I can't remember the details, I generally avoid shops with TFS.
Yea, that's what I did. issue was cross site requests not saving the cookie.
That's focusing on preparing the database. A few tutorials later actually talks about deployment. http://www.asp.net/mvc/overview/deployment/visual-studio-web-deployment/project-properties
Totally agree here. I could see creating a generic repository that abstracts multiple DbContexts that relate to the same concern - like IRepository&lt;User&gt; where you rely on the identity context and a custom extended properties database. Could be smoother in the long run than extending the schemas for Identity context directly.
Honestly, the difference is probably going to be negligible so use what you feel the most comfortable with. I personally prefer XML or JSON serialization for smaller files (especially for settings). 
Pluralsight.com
What you're doing is known as Cargo Cult Programming. You don't know why things work or don't work, you're just doing it because you've seen other people do it. That's a bad way to go about it. If you had spent those 12 hours working through beginner ASP MVC and EF tutorials and reading the documentation you would have a working solution and the understanding of why things are done the way they are. As it is, you're no better off now than when you started. You don't have a working solution and you haven't learned anything. Start at the beginning, not in the middle. Here's a hint: In SQL Server, the default starting value for a database-generated identity int column is 1, not 0. You also have an issue with your automapper config, where you deviated wildly from the tutorial you were following.
Agreed. 12 hours of youtube or reading and he would be much more advanced by now. Reading blog posts and poking around will only get you so far. Read this if you're serious http://download.microsoft.com/download/0/f/b/0fbfaa46-2bfd-478f-8e56-7bf3c672df9d/getting%20started%20with%20entity%20framework%206%20code%20first%20using%20mvc%205.pdf
DocumentDb and Search Services... Microsoft desperately needs to come up with more consumption-based price tiers for these products in particular.
Your updated method is what I had in mind. Looks like it may be working out for you. 
What binary serialization formats are out there? I only know of protobuf.
You separate domain models from data models when your domain is sufficiently more complex then data. Data tends to be very reductionary of the business domain and sometimes your business logic needs more complex data models to reduce continues rebuilding of that organized and related data. One area that comes to mind is reports. Usually they are a complex combination of different data tables that are joined manipulated calculated and displayed. But you don't write a report table aka an entity framework entities. You write a domain model that represents that information as its traversing thruogh the business layer getting built. At least that would be my argument for separating domain and data models and where each comes into play in a sufficiently complex system.
If we don't start off in MVC 6, the project will probably remain in MVC 5 forever. I imagine the big MVC 6 bugs will be ironed out by the end of the year?
By the end of the year it will likely be ironed out. I just want to say that the current stuff is not as fledged out as Microsoft wants to show it. Realistically projects written in MVC5 are fine for years to come. It basically boils down to what kind of path you want to go: - The cutting edge one, then go with MVC6. You **will** encounter issues, bugs and breaking changes on the way and a heavy lack of documentation. Especially now as it is still in a preview state. Development will be slower, but you will be more *future proof*. - The solid business route, you work with what is supported, is already there and good documented with plenty of resources available. Development here will definitely go faster, and you're less likely to rip out your hairs in frustration.
Everything that touches Entity Framework will throw that exception, because when EF spins up it will detect that you have two tables for the same type, which for some reason I never understood is a big no-no. The solution is to remove the duplication. I can't tell how you set up you DbContext, but it sounds like your reference of users are causing it. If you post you DbContext, it will probably be easier to spot. In any case, once that is fixed, your site should come back to life. :) 
have a look at MVA https://mva.microsoft.com/ (Microsoft Virtual Academy) nothing better than learning from the makers :). And if you want, have a look at this section https://mva.microsoft.com/training-topics/web-development#!lang=1033 
The main query starts with *Shift* entity which is linked with *EmployeeShiftAssignments* which in turn has a link to *Employee* so, a query for those is being generated also but the query is one big query as per the profiler.
MVC6 will definitely take more effort to get to a working, stable product. Especially so if your App is fairly large and complex. Consider that once ASP.NET Core and MCV6 are RTM many libraries and resources will be upgraded in short order. The side effect will be that while MVC5 will run for years to come, innovation will essentially stop. No one is going to invest equal effort into the old and the new, so you could miss out on years of great technology and efficiency in the long run.
Its true. I've never heard of Cargo Cult Programming before, but that's exactly how I've learned in every job I've done. And it works well if you know what you're after, which I do. Thanks for the article, I've got some reading for the weekend :)
Thank you for the insights! I hadn't heard of Cargo Cult Programming until you mentioned it. Yep. That's exactly what I was doing. What I'm attempting to implement/learn is MUCH larger than what is in the program I have up on git. Also, I hadn't touched .Net in over 3 years, the last time being on an ancient 2.0 version. So my tactics seem to be working at least a bit :). Thank you so much for the hint!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/develeire] [InfoQ is looking for authors and reporters](https://np.reddit.com/r/DevelEire/comments/44bm2z/infoq_is_looking_for_authors_and_reporters/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Nope! I'd avoid it if I was you. Sorry to say!
Why don't you implement a basic caching strategy on the user/role relationship so that it is not a ping to the DB on each request. You can do this within the application or probably through some Oracle configuration settings (if MSSQL offers this, I am certain Oracle does as well). This could be as simple as reading in the entire table to a global variable on application start. Or storing a Dictionary&lt;userId, roles&gt; in Http Cache. Or, setup your database to enable caching of stale data so that it is actually performing the cache. That seems like it would be a) easiest to implement and b) doesn't require maintaining multiple databases. 
The old stuff (FormsAuthentication) and the new stuff (ASP.NET Identity) are pretty incompatible. If he is using Identity, then the old FormsAuthentication based providers SQL scripts are going to make a mess of things.
It's not necessary if you don't need to scale up.
I'm leaving the async method in the repository and have the two controller actions next to one another (one commented out of course). Would you personally use the Async approach here? I'm only performing GET requests, no POST actions. Also, since you have experience with the async approach, I am curious if I wrote it correctly when using a respository? Thanks!
Would you be opposed to storing a users' role list in Session when they login? That would not be a ton of data, specific to each user, and keeps the DB hits to a minimum? I could help you if you thought that might be something worthwhile. I am not against implementing Identity, I just feel its overkill for what you want to accomplish.
This, by the way, doesn't require the `async` and `await` keywords: public async Task&lt;Card&gt; FindCardByUrlAsync(string url) { return await context.Cards.FirstOrDefaultAsync(c =&gt; c.Url == url); } It's more efficient to just return the `Task` directly, and let your caller `await` it when they need it.
It's not even an optimization. Sync is faster than async, sometimes significantly so. But I only use sync when I have to because of the scalability repercussions.
Here are some choice bookmarks I've grabbed in the last 2 months trying to figure out the latest in front end development with a goal to use it in an MVC website. Books / Lessons [Build Web Apps with React JS and Flux - Udemy](https://www.udemy.com/learn-and-understand-reactjs/) [Introduction to Webpack - Udemy](https://www.udemy.com/introduction-to-webpack/) [Pro React book](http://www.pro-react.com/) [React DnD](http://gaearon.github.io/react-dnd/docs-tutorial.html) [React Fundamentals - Course by @joemaddalone @eggheadio](https://egghead.io/series/react-fundamentals) [React Lessons - Screencast Video Tutorials @eggheadio](https://egghead.io/technologies/react) [React Native: Up and Running - React Video Tutorial #free @eggheadio](https://egghead.io/lessons/react-react-native-up-and-running) [React Tooling and Development Workflow in Action](https://www.udemy.com/react-js-development-workflow/) [React: Learn with Expert Online Training | Codementor](https://www.codementor.io/reactjs) [SurviveJS](http://survivejs.com/webpack_react/introduction/) [Using Modern JavaScript Today - Udemy](https://www.udemy.com/using-modern-javascript/) ES6+ / Babel&lt; [Babel 6: configuring ES6 standard library and helpers](http://www.2ality.com/2015/12/babel6-helpersstandard-library.html) [Babel stages](http://babeljs.io/blog/2015/03/31/5.0.0/#tc39-process) [ericdouglas/ES6-Learning](https://github.com/ericdouglas/ES6-Learning) [How to Use Classes and Sleep at Night — Medium](https://medium.com/@dan_abramov/how-to-use-classes-and-sleep-at-night-9af8de78ccb4#.sh4fkg14t) [JavaScript ES7 Function Bind Syntax – Jeremy Fairbank Blog](http://blog.jeremyfairbank.com/javascript/javascript-es7-function-bind-syntax/) [Learn ES2015 · Babel](http://babeljs.io/docs/learn-es2015/#modules) [Modules in JavaScript Circa 2015](http://odetocode.com/blogs/scott/archive/2015/10/07/modules-in-javascript-circa-2015.aspx) [Quick guide: how to update Babel 5.x -&amp;gt; 6.x — Medium](https://medium.com/@malyw/how-to-update-babel-5-x-6-x-d828c230ec53#.vghmoe8ob) [React on ES6+ · Babel](http://babeljs.io/blog/2015/06/07/react-on-es6-plus/) [RyanCavanaugh/jsx-intro](https://github.com/RyanCavanaugh/jsx-intro) [The Six Things You Need To Know About Babel 6](http://jamesknelson.com/the-six-things-you-need-to-know-about-babel-6/) [The struggles of publishing a JavaScript library | Read the Tea Leaves](http://nolanlawson.com/2015/10/19/the-struggles-of-publishing-a-javascript-library/) [thejameskyle/babel-handbook · GitHub](https://github.com/thejameskyle/babel-handbook) [Using ES6 and ES7 in the Browser, with Babel 6 and Webpack](http://jamesknelson.com/using-es6-in-the-browser-with-babel-6-and-webpack/) node.js / npm [./node_modules/.bin is not in the path / Suggestions / Discussion Area - AppVeyor Support](http://help.appveyor.com/discussions/suggestions/637-node_modulesbin-is-not-in-the-path) [Expose Node.js on an IIS Server by Reverse Proxying With ARR](http://adamtuttle.codes/add-node-to-existing-iis-server/) [Getting started with iisnode: node.js on Windows - Joe Kampschmidt&amp;#39;s Code](http://www.jokecamp.com/blog/getting-started-with-iisnode/) [Give Grunt the Boot! A Guide to Using npm as a Build Tool](http://www.sitepoint.com/guide-to-npm-as-a-build-tool/) [How to Use npm as a Build Tool](http://blog.keithcirkel.co.uk/how-to-use-npm-as-a-build-tool/) [Node.js installation in Windows 2008 R2 Server | Blog](http://admin-ahead.com/blog/node-js-installation-windows-2008-r2-server/) [Node.js Manual &amp;amp; Documentation](https://nodejs.org/api/) [tjanczuk/iisnode](https://github.com/tjanczuk/iisnode) [What You Should Know about Node.js v5 and More | Node.js](https://nodejs.org/en/blog/community/node-v5/) [Why I Left Gulp and Grunt for npm Scripts — Medium](https://medium.com/@housecor/why-i-left-gulp-and-grunt-for-npm-scripts-3d6853dd22b8) 
If you are looking for something purely for small hobby projects, I wouldn't be too concerned with the future of Web Forms. It may technically have reached the end of it's useful lifetime, but it's not going anywhere anytime soon. It is still widely used and will be supported (if not necessarily updated) for many years. It is not quite as dead as it might seem! I also prefer the Razor syntax, but in this case I would go with the most common platform. Examples and availability of learning material counts for a lot! And as far as availability and support goes, there are really only two options: Web Forms or MVC. 
Thanks, now that it's explained it seems stupidly obvious regarding the controller vs action part. Edit: Was able to answer my own follow up question with a couple quick lookups.
Agreed. Async is not for speeding up an individual request, but rather to use far fewer resources as the number of requests in the system goes up.
Dependency injection has to do with injecting dependencies at __object creation__. The first example can be dependency injected because it's in the constructor. The second cannot because the ObjectController has already been created at the time the Admin() method is called.
I don't know about your user visits because I don't know what one user visit entails. If it's a public website of any type though, you'd better do async. I only use ASP.NET for private, Intranet sites (that sometimes run on the Internet too, but for a private set of users) and I have a single core server running 2 custom ASP.NET CRM apps wherein I use Identity Framework + Dapper. Most of my calls are synchronous. Some of them are async because they had to be (to upload large files). There are only ever about ~20 simultaneous users hitting that server (at most), so I didn't need to do everything asynchronously.
Thanks! This looks like what I want to accomplish so appreciate the answer. Looks like the comments suggest this is a bad design though. I understand the argument but seems like every controller needs DBContext so it's logical that alL controllers inherit it by default IMO. EF handles the switching of DAL for me so it seems safe to do..
I also have an API that gives the same data that will be consumed by 4 other websites. Would the above, coupled with the API requests deem an async approach necessary for both API and MVC controller actions?
I wouldn't say so. If your API consumed 4 other API's that's where you would get the benefit. You would be able to make all 4 calls at the same time. The other time it might be worthwhile is if your controller has a long running action associated with it. Using Async frees up the webserver to reuse the thread that your connection came in on to respond to others. This is really only necessary if you're dealing high volumes of long running transactions. All that being said, the TPL is a great thing to learn and MS seems to be pushing an async first attitude these days. As long as you await everything properly, you shouldn't deadlock and there's no real harm in doing it.
https://github.com/aspnet/MusicStore/tree/dev/src/MusicStore Probably a good place to start 
The default Dependency Injection mechanism in ASP Core is constructor only. If you want to use dependency injection in other methods, you would have to replace the default with a fancier implementation, like Ninject or Autofac. This is really easy to do, here's [the docs](https://docs.asp.net/en/latest/fundamentals/dependency-injection.html?highlight=dependency%20injection#replacing-the-default-services-container). In my opinion the default is good enough for the vast majority of use cases. Constructor only DI might seem limiting but it forces you to structure your code well.
Yeah, OP, do you have a Migrations folder in your project? If not, go to the nuget console and type: enable-migrations. If successful, then type: update-database to reflect any changes you've made to your models.
Also, when you do stuff asynchronously you have to guard against re-entry. So if you have a button click event handler that starts an async database operation, you probably want to set a flag of some sort like this: void myButton_clicked(object sender, EventArgs e) { if (_busy) return; _busy = true; // Do the database query and set _busy = false; when finished (or if there is an error). } Other than that - if you start the database call from the UI thread, the response will automatically come back to the same thread. If you're using .NET 4.5 you should also be using async/await for your async calls.
This is absolutely the wrong way to do this. This works if you are using Invoke to queue the operation for execution on the main thread but it is totally broken and not thread safe for actual async calls or situations where the operation has been run in some kind of background thread. The easiest solution for the existing codebase is to simply move the data access calls in to a threaded call using the ThreadPool. When the operation completes use Invoke to safely call a method that updates the UI. I would also suggest looking at the BackgroundWorker class. It can handle most of this work for you. The suggestion of moving to Dapper makes no sense because that isn't the problem. The introduction of .net 4.5 style async calls will just complicate the shit out of your project for no good reason. Protecting calls from reentrancy via bool flags on potentially multithreaded access is a recipe for disaster and a huge antipattern. 
&gt; Protecting calls from reentrancy via bool flags on potentially multithreaded access is a recipe for disaster and a huge antipattern. I need more information! Why is this bad? Doesn't the flag get correctly?
Lastly, I would like to point out that Dapper is simply a bunch of extensions and addons for the `System.Data` classes (aka ADO.NET). It doesn't interfere with your existing ADO.NET code one bit. You can add it to your project and keep all of your old code the same while gradually converting it to use all the nice time saving Dapper methods.
Too much, too early. None of these bits are officially released yet - and the shift from dnx to dotnet cli is causing a lot of churn. The libraries replanted on top of dot net cli still don't exist on nuget yet (they are on myget if you're keen). My suggestion is to wait for the next milestone release (which might be called rc2) which should complete the transition to dot net cli. Don't expect VS Code to be ready at that time either. I know a lot of work is still going on in Omnisharp to support that.
Okay. That's discouraging, but at least it's definite. Thanks for the info.
I had the same experience when it was first released about a year ago. I figured it was more stable than it actually was. Even now, I'm seeing regular, bigger than you'd expect changes, so I've made the decision to step away from the [bleeding edge](https://en.wikipedia.org/wiki/Bleeding_edge_technology) for a bit longer.
As a programmer yourself you should have known relying on your codes months before finalization is a bad idea, **chumptard**. Sorry for calling you chumptard, in bold, but you were on your knees begging for it.
Oh, the shame! However will I go on? 
Yeah, it's really frustrating, because I feel like I could improve our products so much if I had the ability to fit them into our newer platform (which is all Linux-based). Oh well. I can wait. I just can't do so graciously. 
&gt; months before finalization Microsoft always made it appear to be more stable than it actually is, especially by providing a go-live license.
I've spent a bit of time with the RC, and it really does feel very incomplete. I'm giving it a year and then revisiting, I'm sure it will be great in time, but to use it for anything serious at the moment you would have to be a total maniac!
This is in addition to free Pluralsight Plus for 6 months! Impressive!
Technically dnx rc2 is already a thing - it was just minor because that project's ended. But yeah, I'm waiting for a dotnet rc. It's very early to move - from what I can tell, xunit r# support and castle core aren't compatible yet. It's worthwhile to note that on github, dnx has moved from aspnet's organization to dotnet/cli, which might explain the rapid shifting.
Before DNX there was all of the 'K' stuff, KRE etc. Between the K stuff, DNX, and now 'dotnet', it's been a bit frustrating to follow. If history proves anything it's that .net core won't be production ready until a few more *major* version numbers are released. I'm not sure it can be avoided considering the development pace. They are doing a great job and I've loved everything that's come out of the open source push. 
&gt; First question: should I still be using dnx, dnu, and dnvm? If you want to make a start **now**, then yes. &gt; I've seen one blog post suggesting that dotnet renders all of that moot It does but it's not done yet. The next update (RC2) should make everything consistent. &gt; and two other saying that it's only for ASP.NET use going forward. The actual .NET Core documentation doesn't make that particularly obvious, either. That's because it's completely wrong. This is not just for ASP.net. The project was originally started and built by the ASP.net team, however it was never intended to be just for ASP.net. This is why some of the confusion over the naming has come about, because it's only relatively recently that the actual .net team have taken over from the ASP.net team, hence the name change to "dotnet". As others have said, your frustration is coming from the fact that the project has changed names a few times and you've no doubt been reading around looking for answers but picking up bits from out of date or old blogs. My advice to you is to just wait for RC2. However, you can absolutely make a start now if you want, just be aware that the project.json structure is going to change a little and yes those dnx commands are going away. You can read this blog post by Marc Gravel for a good, in-depth understanding of how you port an existing library to .net core: http://blog.marcgravell.com/2015/11/the-road-to-dnx-part-1.html It's also worth noting that the project.json/xproj format supports multiple targets, so you can actually just move your existing .net Framework library over to the new project structure, **then** port it to .net core.
&gt;months before finalization Except they explicitly stated Beta 8 was feature complete and RC1 was production ready and stable...
To piggyback on this, the OP should check out the MVP pattern, which is a pretty effective way of making a Windows Forms application more composable. However, I have to quibble a bit with the part at the end. It seems like you are suggesting that the UI thread stuff is irrelevant; it isn't. The application seems like it's died because without using asychrony these I/O ops are locking the UI thread.
&gt;Async/await doesn't use threads; I had no idea! Thanks - for all the info.
I see, thanks.
If you need to ask the question it's unlikely to be an issue IMO.
You should check this [article](http://markheath.net/post/maintainable-winforms) for windows form bast practices.
Thank you dude. I've found a tutorial in youtube. should i follow this? https://www.youtube.com/watch?v=DqjIQiZ_ql4 
I was at uni having Java drilled into me when the original .NET Framework and Visual Studio .NET was in the works. How does this hoo-hah with .NET Core compare with how it was back then? How did people cope with the transition to VB6 and classic ASP over to VB.NET and ASP.NET relative to this new tech?
Any request. When the thread hits the await it goes back to the threadpool and can be used to process any other request. The catch is that whatever you are *await*ing on must use an asynchronous mechanism like i/o completion ports in order to get an actual benefit. If it simply spins another thread using Task.Run for example then you are freeing one thread just to use a different one, which does not help with throughput. As others mentioned asynchronous processing does not make the thing you are doing faster (quite the opposite actually) however, when implemented properly, it helps you parallelize better which results in better scaling.
Look out for my new code, I used async and await that I've found in some sites. but I am having a runtime error which is cross threaded stuffs. Look out: http://pastebin.com/hdAEfgpS
Look out for my new code, I used async and await that I've found in some sites. but I am having a runtime error which is cross threaded stuffs. Look out: http://pastebin.com/hdAEfgpS
Well, I'm up to my neck in WebForms because the owner (60-ish) is still "in love" (his words) with ASP Classic (vbscript) and doesn't see any advantage in all these objects everywhere and compilation, fuck that, we need to be able to modify pages in production, and "back then" I too battled changing tech, but it's all the same, and COBOL, and then IBM and punch cards... I've tried abandoning ship, but the job market is shitty where I'm at (northern Italy).. Being almost 32, 9 years experience , I suspect my salary is apparently too high for the few sane companies around where I live.. I'd gladly take a smaller pay, but sadly after that one interview I had they wouldn't share why I wasn't offered the job... I am going to be so sorry when I hit 40 and discover I've been practically obsolete from the start of my career.. 
That was a complete swap, and it caused a ton of (ongoing), pain... But that was when the net was young, and MS was their whole ecosystem. Now webapps and real world web dev are undermining that ecosystem powerfully. This is their attempt to stay relevant, and will be a paralell step for .net devs when its RTM version comes out. Bleeding edge MS has costs associated. They've made the right, but short term painful, choice. Things are going in a good direction, but platform segregation and a loose relationship to backwards compatability are issues for 'real' development... Nothing new for .net though ;)
Being 32 with ~10 years experience I never had problems with job hunting neither in Latvia nor when I moved to Spain. On the oposite, I'm tired of turning down LinkedIn offers. Either you should invest in your skill/tool set or situation over there sucks big time. 
Maybe try to dedicate some time for some side project on your own? I really was under the impression that .NET developer (especially with some front-end skills) have no problems to find good paying job. Good luck, man! :)
I already do.. Thank you for your advice and good luck to you too!
async methods should always return a task instead of void, read up on this poision. Also Task.Run is newer and slightly better than Task.Factory.StartNew. Avoid Thread.Sleep and use await Task.Delay instead.
Thanks for the clarification!
Or, if setting up an MTA is not available --DKIM and SPF, oh God-- message queuing is another good alternative. [First result off DuckDuckGo](http://stackoverflow.com/questions/10994176/net-message-queuing-solutions-that-are-not-msmq)
okay thanks for the tips
Azure also fully supports SendGrid...
If you really want to write your own then consider using a simple Queue model (Azure supports this) and then a WebJob running off that queue which will send the email. Very simple without bringing huge amounts of code + database into play. WebJobs also scale, so you can scale up if your web sites start generating too many emails.
Am I the only one who's getting bugged when the author refers to Macs as MACs? (For the record, I don't even own a Mac.)
yes
I began by designing and constructing the database, then I built the project seperately. Things were working fine until I added the answers table (AssociateAppraisalQuestionAnswer). Following the same patterns I was before, I added the model, configuration, repository and services. From what I understand, everything should be working.......those tables and IDs are spelled EXACTLY as they should be.... This is why I don't understand what I did wrong. I think there is something Entity is doing using some conventions I don't understand. Also, Entity is only installed on my web project, not any of the others in the solution. The database connections were all designed manually - this was SUPPOSED to HELP me understand how all this works.....
&gt; MailGun thanks!
Thanks!
Look's good thanks :)
Thanks!
Well, no, I think the whole thing is just a column name (the underscore thing is usually an auto-generated foreign key property from Entity). Anyway, confirm your database actually matches. Try adding a migration and seeing if it finds any changes if you don't see anything obviously wrong just looking at the table.
Sounds about right. Estimates are estimates, could be more, could be less. Configuring, implementing, QA, then deploying.. there's lots of steps professionals do (with good reason) that you may not be aware of.
Thanks for the answer. It is crazy that it takes this long to do in ASP.NET Storefront, we will definitely be moving our store off of this backend. Edit: why would this be down voted? Based off of how the Dotnet developers act on this subreddit it is pretty apparent that this is the right decision.
It won't be less on other backends.
It sounds like you're essentially planning to rewrite [Hangfire](http://hangfire.io/), just making it specific to your application's emails.
Figured it out. It was, as I thought, a result of not properly decorating my model classes with foreign key and notmapped annotations. Your suggestion of adding a migration is exactly what I needed. Thank you!!!
Agree with this. If you're already in Azure, best to make use of what they provide and not reinvent the wheel.
Sure, but you're probably paying for PM overhead with the agency too. Quote for work is rarely just the development time involved.
/r/programming reposts, really?
A Facebook tracking pixel? Yes. 9.5 has been around for over 1 year, what is new about it?
We have about 7 different templates. Individual pages would be about 10,000. I am assuming they would just be adding the code to the templates?
Depending on the templates, it's likely that it would need to just be added to just those templates.
If it was from anywhere besides Facebook I would say that is a fair rate. My rates go to 150% of normal once the word "Facebook" appears in the spec. I'd call it a good deal.
You simply cannot, Core libraries are designed for compatibility across hardware and operating systems. Most of .NET"s big frameworks cannot be loaded through Core libraries at all. In short .NET libraries can use any lib, including core. However Core libraries can only be referenced by other core libraries.
Why reinvent the wheel, just use send grid, mail gun or postmark. I have been using postmark for years now, you can use the library or use the good old SMTP
Are you sure? I can still use WPF types or non-portable packages like `Microsoft.Win32.Registry`, so I'm sure there must be a way to do it for UWP.
It seems a bit high. Have you gotten a second quote? That'd be the best way to go forward 
You could ask for an itemized breakout. Ideally they already have google tag manager (or something similar) installed and can just add it there, which might be 2-3 hrs including testing. If they are adding it to 7 physical templates, I would think 10-15 minutes per template plus testing &amp; deployment. I agree with you that 10 hours seems high. I have added it to sites in the past, so I'm familiar with the steps. 
I haven't gotten a second quote yet. We pay this agency a retainer ~$3,000 a month (I have no idea why, I am newer at this company) and they charge for any dev work on top of that. 
i've just seen a talk about dapper :-)
That's you, you're hiring a whole company filled expenses with non billable folks, rent, electricity, etc. Can you get that task dinner cheaper? Sure, but don't expect an agency to think it's worth their time to take on such a small project.
Fairly sure, I could be wrong but I don't think Core is meant to work with outside libs. That does not mean you cannot, just that its usage would be unstable and untested.
Core supports pInvoke, so you could always do it the hard way. 
Pretty sure this isn't possible - things like WPF aren't intended to go cross platform afaik (WPF's tied heavily to D3D, which isn't cross-platform), though you can look into projects like Perspex if they are of interest.
Not sure why this is getting down voted. We built a system like this to aggregate all the mail across our web sites and servers. Worked great! Eventually we swapped out a local MTA for AWS. 
You certainly can use roles. I've used various approaches. A custom AuthorizeAttribute being the latest.
What exactly do you mean by not normalized? Do you mean the tables are not in 3NF out do you mean there is no one convention being followed?
Make sure to look into Dapper for raw sql if you haven't already! 
I think Entity Framework would be your way to go, at least at the time when you create your API. Is it possible to alter the Database? Perhaps you could "normalize" the database by creating some new views that order the information you need, that way, you can just go through Entity Framework in order to access the information, and go from there.
Just one point, you don't need ASP.Net core to use node/npm/gulp/bower etc. They all work perfectly fine with 4.6, you can even configure the task runner to run gulp tasks on build/clean the same way you can with Core.
I have not had that exact fault you've got but in the past I have had various versions of Visual Studio consistently crash on startup, or even more frustratingly, permanently hide files I'm working on when I debug the app until I close out and come back in again. The only way I've been able to fix some of these problems is to do a system restore to a day or two before the crashes started happening. In your case, having just upgraded to 10, you might be out of luck on that option. All I can say is try to uninstall Visual Studio completely (all addins, everything) and try to reinstall. It's a hail-mary pass, but it might be all you can do.
I never upgraded any windows version. Clean installs only since I had my first PC for 98SE ;)
 Utility.csproj ├─ \Extensions\ ├─ \Helpers\ └─ \etc\ A collection of helpful extensions, static methods, objects, and whatnot that *are not* specific to the application or company. From here on, "user" is a generic example [Company].Core.csproj ├─ \Models\Users\User.cs ├─ \Contracts\ │ ├─\Logic\IUserLogic.cs │ └─\DataAccess\IUserDataAccess.cs ├─ \Extensions\ └─ \Helpers\ Defines the core of the application. * Core should be 99% scaffolding, with very few implementation details at all. Implementation comes later. * Models is all POCOs and defines the objects that are passed through the different layers of the system either as parameters or at return values * Contracts are the interfaces that your layers will use to inject and communicate with each other. * Extensions and helpers are just that - helpful extensions and static methods that are application-specific. * In this example, we've defined a basic user logic and user data access framework * `IUserDataAccess` will have methods like: * `IList&lt;User&gt; FindUsers( UserSearch search );` * ^^line ^^break * `IUserLogic` will have methods like the following: * `User GetUser( int userId );` * `IList&lt;User&gt; FindUsers( UserSearch search );` ` [Company].DataAccess.csproj └─ UserDataAccess.cs Defines the implementation of the data access. It does not matter whether you want to use ADO.Net|SQL or EntityFramework or Dapper or NoSQL in your implementation, as long as that implementation obeys the contract [Company].Logic.csproj └─ UserLogic.cs Defines the implementation of the business logic layer. Generally the logic will use the data access contracts and resolve the implementation through injection (I prefer constructor injection, but that's up to you). See my reply for an example. [Company].[Product].csproj └─ yadda.yadda.yadda.cs Your WebApi. The WebApi should make use of the logic contracts and resolve the logic implementation through injection. The WebApi is pretty much responsible for output formatting, authorization checks, and initial uploaded file handling. Put all of these `.csproj`s together in one solution and you'll be pretty well set. Research what tools you want to use for dependency injection. I like Unity since it has extensions that will implicitly resolve dependencies for your WebApi controllers, but there are a lot of good ones out there. After that start creating Unit Test projects, one per "basic" project. And you're golden. ---------------- Tips: * Lots of unit testing. Define your workflow and code `Core` to fit, then write your tests as soon as possible. * Keep it clean. If your data access is the only layer that needs an object then *don't* put it in core. Define it and keep it within that layer. This happens a lot if your database doesn't exactly match up with your ideal models. Best practice here is to define a transition class to map back and forth between what is needed by the database and your ideal model. * As far as logic is concerned your API shouldn't be much more than an authorization check and a pass-through to the logic layer. That way if the project ever needs to be converted to something else you aren't tied to a WebApi-specific implementation.
I upgraded my desktop from 7 to 10 and haven't had any issues at all. I mostly just game on it, but I do use it when I record development screencasts and both VS 2013 and 2015 work just fine after the upgrade. Just one bit of anecdata.
Find it in Control Panel -&gt; Programs and Features and select change/remove. You'll have an option to repair or reinstall. Also, wtf is this? &gt;Microsoft.VisualStudio.Services.Experimentation.ShippedFlightsClient
Check the config file for anything about .net 4.6 Make sure your app doesn't have any dependecies (dll / nuget package) that requieres .net 4.6
Copy the DLL locally and reference it (I like to put a DLLs folder inside the App Code folder). Right-click the DLL in Windows Explorer, go to Properties, and click "Unblock" if the option is there. Remove all references to the network path.
I've upgraded from 7 to 10 and have had no issues whatsoever. Haven't had a new operating system work this well, maybe ever. 
Found it .. should have found it sooner .. sorry to bug you all :) http://i.imgur.com/2iFgmTI.png
If you are comfortable using F#, or don't mind learning the basics for it, the SQL type provider would allow you to access the database in a type save way without having to do any work yourself. [Here is a walk-through of how to do it.](https://msdn.microsoft.com/en-us/library/hh361033.aspx) You could write a thin layer just for database access in F# in a separate library and invoke it from C# code. 
Thanks for sharing. I don't think I've ever seen that screen before. 
It has nothing to do with performance and everything to do with good design. By putting business logic into your ui, you couple your business logic to the ui. This means it's not easily testable, it's a poor separation of concern and means changing your ui could break something. For your simple calculation, you want to create a viewmodel and do the calculation on that. Then you just bind the properties to your UI and you get the same results, but your ui has no business logic and is just a view of the viewmodel. 
As seen by all the other comments there are a lot of different ideas on how to build API:s and applications in general. I tend to favor separating the API into two types of interactions: commands and queries. This approach is known as CQRS, which stands for Command-Query Responsibility Segregation (Googling it may turn up a lot of blogs with intimidating things such as event sourcing, denormalizers, projections, etc. none of which are really needed; CQRS merely enables them). Commands are operations which updates state in some way and they should always signal the intent of the user, e.g. CancelSubscription, AddItemToCart, etc. The commands are handled by the domain model (or business/logic layer if you are more used to such naming conventions) which loads all the needed entities from repositories, validates the desired change, applies and saves it. Commands should usually return nothing at all, except for the outcome of the transaction: success or failure (and perhaps some identifier, if need be). Queries, on the other hand, are all about how information should be presented to the user. They are read-only (e.g. GET-requests) and defined by a completely separate model called a read model. So why bother with keeping this distinction? Doesn't having two models mean that you need to maintain a lot more code? In my experience, no. In general, using a shared model often involves a lot of added complexity not needed with the segregated approach: * Mapper classes translating domain entities to and from DTOs. * Bloated domain entities always containing more state than necessary because they just *might* take part in a query. * Huge repository interfaces with a lot of different queries, not always returning an entity (e.g. `bool CustomerExists(Guid id)`). * Frankenstein-DTOs, stitched together from different entities which had to be loaded and mapped from different repositories. In trying to use the same model for both reads and writes, the result will often be optimized for neither case. When using a dedicated write-model, the repositories need only load the amount of data actually needed to actually validate the state transition. Also, since there is no need for any mapper to read the state, it is often possible to keep most of your domain models' state private, ensuring that the data is properly encapsulated and protected from the horrors of the [Anemic Domain Model](http://www.martinfowler.com/bliki/AnemicDomainModel.html). Likewise, queries are no longer constrained by the shape of your domain model. There is no need to load a `Customer`, `Order` and `Product` through their respective repositories and then map them into a `ShoppingCartDto`. The read-model can go straight to the database and fetch only the data which is needed to assemble the `ShoppingCart`. In my experience, queries are often the first in need of some kind of optimization (e.g. a page with a list of a lot of objects which takes forever to load) and having a separate model to read from means that you can optimize your SQL-queries without the constraints imposed by the write-side of things. Finally, I would like to join /u/redditcow and give my love to [Dapper](https://github.com/StackExchange/dapper-dot-net). It takes away the pain from dealing with ADO.NET, while keeping me in control of what queries I make (unlike NHibernate and EF).
[removed]
Yep, it is absolutely ridiculous. I guess this company has had this set up for years. I am getting rid of the agency in the next couple of months...
That doesn't even make sense...
Which is why I said "as far as the schema is concerned". There are relationships just not through foreign key constraints. The application took it upon itself to do the integrity checks.
We pay a retainer... This is on top of that.
Are you trying to execute a fire-and-forget operation? If so, you might be interested in [`HostingEnvironment.QueueBackgroundWorkItem`](https://msdn.microsoft.com/en-us/library/dn636893\(v=vs.110\).aspx) (requires .NET 4.5.2). In some cases you might get away with simply using [`Task.Run`](https://msdn.microsoft.com/en-us/library/system.threading.tasks.task.run\(v=vs.110\).aspx). For more durable options, I recommends Scott Hanselmans' excellent write up on [how to do background processing in ASP.NET](http://www.hanselman.com/blog/HowToRunBackgroundTasksInASPNET.aspx).
Then you're being taken for a ride
It's their product, they can do that. My first thought when I saw the title here (before I clicked) was "they've got over a decade of brand recognition here, why throw it away?" And that's pretty much what the response from MSFT was.
Yes! I'm exactly looking for a fire and forget operation. I'll look into that method.
What is "the install"? Have you by any chance added a Windows Installer project and have you checked if .Net 4.6 is a prerequisite in that project?
Yes it is their call to make, however a lot of people have bemoaned the continuation of the name (the uservoice site doesn't do justice to the opposition). The [ASP.NET 5 is dead - Introducing ASP.NET Core 1.0 and .NET Core 1.0](http://www.hanselman.com/blog/ASPNET5IsDeadIntroducingASPNETCore10AndNETCore10.aspx) post from Scott Hanselman anouncing ASP.NET Core 1.0 instead of ASP.NET 5.0 had a lot of calls for a name change. 
And a lot of people prefer them keeping the name. So what?
Some people want a name change, others don't. The 'so' is that the decision has been made, some people will be happy, and others won't.
You didn't notice the foreign key missing from your tables?
I'd paid to see that, popcorn and all.
In this case? Basically none. But this is just a simple example to illustrate how DI can work with the architecture.
&gt;It's their product, they can do that. And I can consider them misleading dicks because of it. &gt;"they've got over a decade of brand recognition here, why throw it away?" But it's not good recognition. ASP makes people thing of webforms and active server pages. Modern MVC is fantastic, and they are heading in the right direction by open sourcing it, but it won't pick up any momentum in the wider development community because of the legacy of ASP.
Is the agency familiar with storefront? If not then you have to pay for the time it takes them to familiarize with it. Making the change probably takes 2 seconds, the time is spent knowing where to make the change.
Kinda wonder why they didn't just go with "Microsoft Core". ASP sounds like active server pages, .NET sounds like it was inspired by dotcom 20 years ago. But I guess they are just names.
Sorry don't have an answer for you but I can strongly recommend not using Entity Framework with MySQL as the Mysql provided drivers are rubbish. Dapper is a good lightweight alternative.
Can you elaborate? I have used MySQL before and it works just fine. Even code first generation works.
I didn't quite understand how annotations worked. I have a much better understanding of how Entity maps objects now. I have since completely re-architected my model to proper code-first form.
* 6 month turnaround on a pretty simple bug https://bugs.mysql.com/bug.php?id=74918 introduced when upgrading to EF6 * cryptic error messages when using unsigned * cryptic error messages for timeout/long running transactions Simple uses are good but if have a complex implementation becomes hard to manage fast and the support is terrible.
The good thing about mistakes like this is you now understand it much better and won't do the same thing again. :)
Yes, of course. But beware, any tasks queued up using either `ThreadPool.QueueUserWorkItem` or `Task.Run` will be cancelled if IIS recycles the application pool. If you need to guarantee that the task will be executed, see the article I posted (and yes, /u/awesme, my favorite is also [Hangfire](http://hangfire.io). :)
Speaking of irrational things, I really have never liked PHP for the sole reason that it is what all the armatures use. At the end of the day, they all can do the same things so it does not really matter what you use. And I guess my natural curiosity is leading me down the path to learn something new. The ironic thing here is, I am worried because I think Web Forms are dead but what I am proposing to learn, ASP.NET web pages is a very niche technology which will probably always be much smaller than Web Forms.
Depends on how far you want to go, but personally I prefer to use the absolute bare minimum of Identity, which is described fantastically here: http://www.khalidabuhakmeh.com/asp-net-mvc-5-authentication-breakdown-part-deux (his part 1 is also worth reading). Using this method you don't have to implement *any* of the Identity components and can build your auth logic however you want. For your scenario, you'd implement this code, swap out the "input.HasValidUsernameAndPassword" with a call to your service layer (or whatever) to validate the user, then another call to pull out their roles+profile information, which you'd then stick in claims and you're off to the races.
If you enjoyed PetaPoco, take a look at NPoco which has become its spiritual successor. Tons of additional functionality (https://github.com/schotime/NPoco/wiki) along with support for basic things that PetaPoco was lacking (like multi-column primary keys). And for fun, here's an article I just found while Googling that could have been written by you...in the future! http://nathantreid.com/why-i-ditched-entity-framework/
Haven't used it myself,but maybe you're looking for https://geteventstore.com Made by Greg Young, basically the guy who "invented" CQRS. 
Woah, I was just looking around for alternatives and hadn't seen this one. Looks great, adds a good number of added features that I was looking for when I moved away from PetaPoco. Great post.
Sounds like you think the way I do. Really digging this tutorial, thank you so much for bringing it to my attention. I really did not like all the extra bloat involved with the full scale version of Identity and I don't believe this project calls for anything more than the basics he is describing here. 
As a side note, with NPoco what would be the correct way of handling dynamic Where clauses? Would I do that with straight code or is there a "framework way" of doing it. I use iBatis at work and with that you can specify a conditional and a block of sql which will be inserted depending on the outcome of the conditional.
Nice app! But consider signing it, now I get messages from Windows about the app being unsafe.
Call me stupid but I feel like the database is the wrong place to do access controls like these.
IMO the retainer you left out of the OP is pretty important in answering your question. However, I'd guess (hope) that besides just installing the script they'd go back and make sure it doesn't adversely affect anything currently working, go through test before applying to production, etc. And you're paying the agency overhead too. 
&gt; The cutting edge one, then go with MVC6. You will encounter issues, bugs and breaking changes on the way and a heavy lack of documentation. Especially now as it is still in a preview state. Development will be slower, but you will be more future proof. Will he? I'd say no, if there are going to be major breaking changes still.
glad to see you're maintaining it and not leaving it to wither in the alpha wastelands like my hobby projects do.
I have the terrible feeling it will keep be busy for a very long time! Only joking, it has been a joy to work on and I can guarantee there is loads more to come
Sql Templating functionality is the framework way to handle that (https://github.com/schotime/NPoco/wiki/Sql-Templating). You can do some really complex stuff with this, especially if you take a little time to write some helpers specific to your use case. As an example of how powerful this can be (with a little glue), the codebase I'm currently working on allows a developer to write a new SQL query (say for a brand new sales report that has to be done yesterday) and then with a few lines of code have that query turn into an HTML table with individual and multiple column-level searching, AJAX updates, paging and sorting, the ability to dump the entire report directly to Excel, and some other goodies - all in about 15 minutes. This was all accomplished by writing a couple helper attributes that we apply to the POCO that help us map the object directly to the query (i.e. [SqlColumnOptions(Name = "UserId", Alias = "u", Searchable = false, Sortable = false, Exportable = false)]). Then we have an interface to identify these special POCOs, a few helper classes that use lots of reflection to pull all of the attributes off, convert them to SQL, and call SqlBuilder methods appropriately. Another helper converts these POCOs to a standardized JSON glob which gives us the table structure (column names, types, sortable values, etc. just Javascriptified). Then all you've got to do is call magicalTableHelper.bind('.classOfNewTable', 'NameOfPoco') and a table appears. There are certainly some limitations (magic strings everywhere that will break if you refactor/typo/etc.), but we've found that this *rarely* happens as backend reports generally don't change once they've been designed. There is also significant reflection going on too, but with ~50 users we don't see a noticeable performance hit on a little 2 core 3.5GB box. Point is: you can do a lot with Sql Templating.
You can do that. Check out the search options - the cog symbol
Yeah I've seen that... managing Linux IaaS VM's in Azure is a non-starter for me. I'd just as soon implement something simple myself using Azure Table Storage and save on the ops overhead. If he ever gets a Marketplace offering up and going I'd consider it. Thanks for the link though.
Sorry didn't read about the Paas you wanted. RavenDb is in the Azure Marketplace. Completely different beasts but from what I heard it also performs better than RDMS, and supports ACID transactions. Beware if you want IO performance azure storage is in general pretty bad in terms of IO, except premium storage which it's kinda ok. 
It's pretty much the only place to do row-level access controls. Sure you can secure an API and filter results based on claims, but what about multiple callers to that dataset? This level of security is used when sensitive information is in play, shared among many actors and many systems. You can either pass the user context universally and let the source transparently return what is allowed or hope every web application, ETL and BI tool implements an identical set of authorization rules. 
I use hang fire religiously 
LOL. Had a feeling it was spam, but was hoping it was some new technology named Olga.
Another sockpuppet post promoting the Viva64 shit.
Using generics is generally considered an API anti-pattern. Either use an array or create a concrete dedicated type like 'UsersCollection'
I haven't heard that one before. What's your source? The only times I've seen custom collections be popular is for things created before LINQ. Arrays implement `IList`, so the underlying type could be an array and it would still work. Beyond that I chose to not use an array because creating a list is slightly more efficient than an array. More often than not I'll actually use `IEnumerable&lt;x&gt;` for the return type since it will fit with any collection and not imply the ability to modify it.
Love this app- thanks for all the work!
Hello, I'm not trying to blatantly promote my YouTube series, but I'd be interested whether anyone would like to follow it and try and offer my advice over the course of the series. This is my main aim with the YouTube series, get feedback to improve my skills. 
If I run it and drag the Log4Net log file to it then it gets stuck in a loop with a gradually increasing log file size aha
We have tools like sccm and powers hell dsc which can accomplish this.
I tried the webpack runner but switch to NPM based tasks now .. NPM scripts do 99% of what I need most the time (simple web setup).
It's in the dot net framework design guidelines book written by one of the key architects (name escapes me) that's why you see so many XXXCollection classes even post generics. For my internal usage I always use IEnumerable&lt;&gt; but for public APIs it's a no no, I remember being surprised at the time but the reasoning was sufficient.
Post an issue to the EntityFramework repository. EF7 is **preview** software, it's not gonna run perfectly. Bugs are to be expected.
I like it. You point out where information is incomplete and mention where to find more information. This is a very nice point. You might want to provide further information about the `MarshalByRefObject` base class and why it is required, as well as the pitfalls about having multiple app domains (all data that goes across appdomains needs to be serialized, loading a type in another appdomain will load the assembly there too, etc etc). Given the hype around .NET Core you might also want to spend some time with it and write a blog post about that system, since AppDomains and assembly unloading is not available in .NET Core.
Write your own extension method that doesn't use Inner Join?
I don't know if it changed in EF7 compared to EF6 - but EF6 always used inner join by default. To get a left outer join, you had to do something like: from s in _context.Study from p in s.Project.DefaultIfEmpty() select new { Test = p == null ? "" : p.ProjectDesc }; Completely untested and unsure about whether EF7 works the same way or not.
EF7 (EFCore now) is not production ready. However much one could iterate RC, it's not RC. It's beta. A lot of important features are still missing, and a lot of optimizations and bugs need squashed. Don't use EFCore if you can help it for now.
Hey Angular, thanks for the feedback! &gt;You might want to provide further information about the MarshalByRefObject base class and why it is required One of the things I've struggled with when trying to write is to stay on point. This is a good suggestion though and I will revise. &gt;, as well as the pitfalls about having multiple app domains (all data that goes across appdomains needs to be serialized, loading a type in another appdomain will load the assembly there too, etc etc). Same as above. &gt;Given the hype around .NET Core you might also want to spend some time with it and write a blog post about that system, since AppDomains and assembly unloading is not available in .NET Core. I'm hyped about .NET core as well, however, it's too volatile of a tech right now for me to write about. I have given it thought, however. Thanks again! 
Drive by comment (I am waiting for a 2 minute deployment to finish). I haven't had a chance to read your comment, but after my quick dip in and out of your site, I wanted to say that I found your font hard to read for some reason (size, spacing, color, content main pane width, something - I'm not sure exactly what it is as I am not a designer.?). For comparison, Hanselman's blog is pretty easy to read - http://www.hanselman.com/blog/. This was just my 30 second snippet. Congrats on getting a blog up and going, its always nice to see what others are doing with the framework!
I think this is a good article about loading and unloading via AppDomains, but I think you could talk a bit beyond a single target class. Like what happens in your CustomPlugin class if you have a method that returns an instance of a type that's defined in the plugin assembly and serializable? What if that returned type is MBRO instead of serializable? FWIW, I tried this separate app-domain loading and felt it was inadequate for long-running services. With a loaded AppDomain, it's possible that the code in the AppDomain misbehaves in a way that makes it impossible to unload the AppDomain. In the end, it was worth it for me to use an external process to load the custom plugins (like IIS and w3wp). That gives you a control mechanism to terminate a plugin by killing the process if the plugin caused the AppDomain unload to fail. Your situation may be different, and if you have more control over the plugins that get loaded and confidence that they're well-behaving, then it may not be worth the overhead, but figured I'd call it out.
If you are a polyglot then why not select the appropriate tool for the job rather than try to cram a square peg into .NET shaped hole? Of course it will be a nightmare.
That was sort of what I was trying to get at. How much of a nightmare will it be to do a new production ASP.NET 5 app on linux in the year 2016 :)
Honestly, it's looking like it will be good on release but I feel that it's not quite there yet. I haven't used it on Linux but I have on Windows, and it's just suffering from too much change at the moment. That said, it's certainly an option worth exploring if you need an enterprise stack that has corporate backing. My question, though, would be this: why not node.js? You mentioned other developers on the team not having as much web knowledge and I can certainly understand that node is a far leap from Java or even .NET for that matter, so that's a perfectly valid answer, but it's become a very valid application platform with many fairly unique advantages (electron, easy horizontal scalability, single language stack to name a few), and it's much more supported and mature on Unix systems. On a related note, if you're going to have any OOP-style developers learning front end or node code, I'd definitely go the TypeScript route. I don't know if you're familiar with it, but building applications in JS on either end of the stack is significantly less hacky feeling with it, and it'll compile down to javascript supported on clients that are about as legacy as you can imagine, it's a very powerful tool.
I have other node projects I've done entirely in node and I love it when I need speed and simplicity. I know that we're at a place where I can probably deploy node in the enterprise and it will be fine, but I'm a little anxious about a project this large in node with multiple developers handling it that would be very new to node. For me personally, on the largest projects where performance is less important and consistency/maintainability is more importent, I tend to go for something OO over Node for the core/backend because I can setup a fairly rigid set of rules and valdiations and most importantly I understand how to secure this stuff very well. When I was using something like meteor and serializing objects I got from a browser -- I really wasn't sure what I had to look for from a security perspective. I definitely need to cut 1-2 languages out of my life, and Node is appealing in that regard! Typescript is actually a really good idea for some of the OO folks, and angular 2 supports it inherently, right? Most of my users are not actually going to be using the web front end, so I'm completely OK with going a little newer/riskier there. 
&gt; Node Honestly, the more I go down this, the node route sounds better and better. Just trying to decipher on what is going to be least-worse between EE7, Spring, Play ,etc... Node seems awful cut and dry :)
&gt; I vastly prefer .NET to Java for development, and was considering targeting ASP.NET 5 for the app. Seriously think about why ASP.NET 5 over something else, something more proven and production ready. What is your motivation for this particular technology? 
If you are planning to deploy in the next few months, i.e. you still have to build this thing, yes, it will likely be a nightmare not only in production but likely in development. I was just offering on a project where the customer is fairly forward thinking and wanted to try ASP.NET Core (this is what it's called now, not ASP.NET 5) but quickly abandoned the idea due to how much time we'd likely spend fighting changes in the RCs, tooling, dependencies, and lack of mature library support rather than working on the business case. I appreciate forward thinking, but I also have to manage risk. For what it's worth, the initial idea was to deploy self-hosted in Kestrel reverse proxied behind NGINX, but there is not yet any official recommendation or guidance from Microsoft how to deploy ASP.NET Core to production on Linux. Edit: You can see some recent discussion on this [here](https://github.com/aspnet/KestrelHttpServer/issues/612). In their case the deployment target was flexible so ASP.NET 4.6 on IIS was a very attractive fallback and with the proper architecture we can migrate to ASP.NET Core at some point in the future once it settles down and the ecosystem catches up without too much work effort. If you know Linux is the deployment platform I'd stick with Java for for your use case and time schedule. It's annoying how they ignore the language warts, but it's a reliable platform. 
&gt; In their case the deployment target was flexible so ASP.NET 4.6 on IIS was a very attractive fallback and with the proper architecture we can migrate to ASP.NET Core at some point in the future once it settles down and the ecosystem catches up without too much work effort. Actually after going back with the stakeholders, they are OK with deploying to windows, and bonus if it runs on linux down the road. The plan is to get it to build in a standard 4.6 (and host in IIS) but try to keep it Core-compatible so that we can move at a later date. On the plus side it may just work perfectly OK in mono with apache or etc. 
Mostly on Source Control using TFVC.
At this point I think the mods should consider it spam. 
Download [TFS Sidekicks](http://www.attrice.info/cm/tfs/) and clean-up the old/bad work spaces.
Use git.
I'm on mobile so I'm going to keep this brief. First off, Microsoft stack is not "yesterdays" technology, it's quite relevant today and they are gradually open sourcing their products. I wouldn't get caught up in "what language is best", you should focus on learning how to code in general as opposed to how to code in a specific language. I myself started writing in VB, then I move to C# and SQL, and then it was WPF and SQLite, and then it was ASP.NET, and then back to WPF, then JavaScript (MEAN stack). I don't think anyone can say they are an expert, there's always something more you can learn. The point is the language you code in will more than likely change over and over during the course of your career so don't focus on becoming an guru in a specific stack, just code and you can adapt to any language you need to get the job done. I wouldn't expect everyone to follow the same philosophy but that's just my two cents. **Edit: Looks like I wrote this all for nothing.**
Agree. Also on mobile. These days I think it is about the right tool for the job. I'm currently using the new ASP.NET bits and I like where they're heading, but I've historically been a C# developer. 
[Don't feed the troll.](https://www.reddit.com/user/kevinmarenger/submitted/) 
You seriously have the balls to call your own articles the 25 best on the web? 
Actually looking through here, they seem to have ripped off every single article from other sites, and reworded them in an attempt to call them their own. Article #1 comes from: http://www.codeguru.com/csharp/.net/net_asp/mvc/understanding-dependency-injection.htm http://imgur.com/a/4S15I Article #2 comes from: http://www.webdevelopmenthelp.net/2015/04/understand-application-lifecycle-in-asp-net-mvc.html https://imgur.com/a/ZPfmC You get the idea.
Worthless spam.
Whoa. This is one bored guy.
Wow. That's much worse.
If you need the application life cycle management features of TFS, then use it. Otherwise.. use something else. TFS is quite cumbersome and it's "workspaces" paradigm is kind of annoying.
I'd recommend using tfs with git over tfsvc. That said your problem is easy to fix if you just map the original / base path to a folder instead of making per project mapping
So you need to realize validation should consist of 2 parts: 1. Input validation and 2. Business validation The input validation should be done at the controller. Tools like FluentValidation integrated with MVC are great for that. Even builtin data annotations and model state validation are good enough for input validation. The purpose of input validation is to prevent a malformed or malicious client from ever communication with your back end. Malformed requests should be rejected with Http 400 bad request. Things being null that aren't allowed to be null is an obvious example. Business logic. A mvc controller should look similar to [POST] ActionResult DoSomething(MyModel myModel) { if(ModelState.IsValid == false) return new HttpStatusCodeResult(HttpStatus.BadRequest) var response = _myService.DoThing(myModel) if(response.Success) return new HttpStatusCodeResult(HttpStatus.Ok) return new HttpStatusCodeResult(HttpStatus.Forbidden, response.ValidationFailure) } MyService should only return validation errors (that are allowed to be publically displayed). If the server errors, that should never be caught by MyService or MyController. That should be handled by your global error handling. Your global error handler should log the error and return http 500. Business validation errors should result in http 403 forbidden. There is nothing you can ever do to allow this request to work. Your validation should never touch the database, ever. Your input validation should never touch an external dependency, only the model &amp; model's type meta data/attributes. Your business validation can be dependent on the database but it should never invoke db calls. All db calls should be governed by MyService talking to MyDataService, so you need to check for a dupe email you **pass in** that information out of the db into the validation, not allow the validation to **call out**. In a rare situtation like double submission and the email doesn't exist but results in an unique constraint violation, that's not a validation error but an application unhandled error.
I've had a lot of success with [Durandal](http://durandaljs.com/) and ASP.NET WebAPI 
Purely use MVC for all http connectivity. There is **exactly 1 reason** to use WebApi for self-hosting in a console/windows app. Webforms would be a nightmare with knockout. Related, i would recommend looking at Aurelia over knockout. In 2016 i probably would never use knockout again on new development. 
 &gt; I tended to return stuff like this, though I don't know if it's for practice or not: &gt; return new HttpResponseMessage(HttpStatusCode.OK) { Content = new ObjectResponse(resultobject.GetType(), resultobject, new JsonMediaFormatter() }; Should've just used MVC, you're not using anything WebApi provides. 
We use fluent validation at my company(we rolled our own) and we all love it.
I see that in their reference links in footer.
Knockout provides the framework you would need. It can take care of your routing, controllers and view logic while all your data is coming from your web api with Ajax calls. With the right headers on your calls you have security through tokens acquired from a login page or what have you
I just did a WebAPI and Aurelia project. My first real go with .NET MVC and Aurelia both. I really enjoyed Aurelia. If you're in the US and have been to a Target since September and noticed a BB-8 display, then this is relevant: http://thebb8resistance.com - front end source is on GitHub linked from the About section. Some things are bugging me with WebAPI. Multipart form upload POSTs are not very straight forward. I wanted a form to upload an image along with a plain input field. I could not for the life of me get it working. Ended up being easier to do two POSTs via JavaScript. One for the file, one for the text. What many of these types of posts seem to miss is that it really shouldn't matter what the back-end stack is like for a JS framework. You should be able to rip out one or the other. As long as your endpoints of the API return the same data, it shouldn't matter. At work, we have a front-end team who works with Angular. Our backend happens to be WebAPI2 but it really doesn't matter. We could replace half the endpoints with Node or something else and even keep the addresses exactly the same and have a reverse proxy server send requests to one backend service or another and the front-end simply wouldn't care or know a difference. 
Yeah actually I was trying to integrate that but I cannot get my stuff get injected into the validators. Here is the conversation : https://github.com/JeremySkinner/FluentValidation/issues/204
I've been working with knockout for like a year. I've never seen it do routing. How are you getting that?
Jeremy is saying is that fluent validation doesn't know where to find 'ApplicationUserManager userManager' required in the CustomerCreateValidator validation constructor. You can solve that by creating a validation factory for fluent validation where you instantiate CustomerCreateValidator using your own IoC container and let it resolve any constructors. An easier way could be to remove the ApplicationUserManager parameter from the constructor and resolve the user manager inline with something like: _userMan = YourIoC.GetInstance&lt;ApplicationUserManager&gt;(); 
Hmm. That could possibly work. I'll have to look at that. Right now I have the file going in as part of a JS FormData object.
&gt; Also using inline would hurt the validator's testability I think. I'm not sure it would matter much as long as you get the instance from the same IoC container as the validation factory gets the instance from, but the validation factory setup is definitely the proper route.
&gt; In 2016 i probably would never use knockout again on new development. why?
Lol that thumbnail.
Funniest sub today.
great for valentines day
This is such a shitty answer, sure if you like spending hours typing arcane and inconsistent commands for EVERYTHING instead of it being integrated in VS then sure.
Where I work its badly intergrated
Although you can manage access controls in the application layer, a system that manages sensitive information or a multi-tenant database where data needs to be restricted is best done with a defense in depth approach with access control set at both the application layer and within the database.
I'm new and still learning, but we use SQL for CRUD operations on a database, right? So what's with entity framework, and all that? 
entity framework (or any ORM) eliminates the need to write SQL because the ORM writes the SQL for you. This allows you to use the same code base for your data calls. You can also write raw sql queries with ORMs as well.
There's better tools. For straight forward tasks there's intercoolerjs. There's better more full tools like aurelia / angular. There's react and react-like riotjs and preact. And maybe you can use babylon to build in ES# observables directly.
If we're to take your word for it, so is TFS.
What makes these tools better?
I'm using [AutoHotKey](https://autohotkey.com/). This little script does what you want and it will also work on any other window. LAlt &amp; T:: Winset, Alwaysontop, , A exit First line defines the mapping. I set mine to Left alt + T key (press it a second time to disable the topmost state). I tested it with Visual Studio 2015 and I can pin individual tabs.
They're either simpler or more complex. Knockout gets really complicated really fast, way faster than these newer tools. The more complex tools might have a steeper learning curve out of the box but allow you to manage a larger project better
Yeah, that's a scenario that would make sense. 
As far as i know https you are good. Just require ssl for your site (ie don't allow http, you can apply the requirehttps attribute). Web config should be safe, if you are concerned about things like passwords or encryption keys, take a look at azure key vault service. You can store files using blob storage. I would encrypt the contents in app before saving. Hope that helps!
Hmm. That's the first time I've heard anyone call Knockout complex. It's just a data binding library that implements a simple observer pattern. I think it's still a good choice if all you want is 2 way data binding.
There are lots of opinions about using EF for data access. The approach I would take is to use it to get started quickly, but do make time to learn it more in the near future. There's nothing wrong with going the raw SQL route, just know that it will take more time up front. Both paths have tradeoffs. I personally use EF for 90% of my data access code and the rest stored procs for more specialized queries. 
No i didn't say knockout is complex. I said the complexity increases too rapidly the larger your application is. It also provides next to no guidance for structure. The more advanced your usages get the more you get into having to mess with prototypical inheritance. You also get into knockoutmapping vs without mapping. The application starts to slow down the more and more observables you have.
Don't learn webapi learn MVC. Everything you need to make "rest" services exists in MVC and is superior for accomplishing them.
Use Dapper.Net It sounds much more like it would be valuable to you.
What makes you say that? 
One of the very very very few things webapi provides that MVC doesn't out of the box is content negotiation. You immediately throw ConnNeg out the window by invoking the json serializer like that.
As someone trying to pick it up and learn, its a bit annoying that the controller hooks are different. Different decorators, different .methods available for handling the request. It's bizarre to me. The pure API approach is very appealing and desired so we can develop a client side web ui and mobile apps. File uploads have been around as long as there have been &lt;form&gt; tags. Seems weird how convoluted seemingly trivial stuff can be. Taking a file in and sending to Azure blob storage should be dead simple. As much as MS borrows from other frameworks like Rails, they need to improve on reducing the significant boiler plate to do stuff like this. I come from mostly a ColdFusion/CFML background. It's amazing how easy it has been for 10+ years to capture form data, including files and write it to disk. In fact, it's as little as one line.
Probably because it's more lightweight / performant than EF and if OP can't use any of the time saving features of EF and has to use sp's anyway, they might as well use dapper for the performance benefits alone.
I would look into a lightweight ORM like dapper instead of EF or raw ado. It's just faster (way faster than EF, on par with raw) and easier(way easier than both). 
&gt;Absolutely no SQL commands can be made on the app side. I know it doesn't make sense but There are actually a lot of benefits to encapsulating the SQL in the database and using the price as an API. I used to work at a place where we would radically change the table structure with zero changes to the dozens of applications that use it. 
no benefits at all. tired of this argument
&gt;As someone trying to pick it up and learn, its a bit annoying that the controller hooks are different. Different decorators, different .methods available for handling the request. It's bizarre to me. Which is why it was all unified back to MVC. It was pointless to fracture in the first place, it was all caused by Microsoft's left hand not knowing what the right is doing and they wouldn't just scrap the work they did on WCF Rest. &gt;The pure API approach is very appealing and desired so we can develop a client side web ui and mobile apps. Which is really easy to acheive with MVC. With the biggest added advantage of 100% supported razor. Sending back html in ajax tends to be far better than forcing the client to do all the work of creating it. &gt;File uploads have been around as long as there have been &lt;form&gt; tags. Seems weird how convoluted seemingly trivial stuff can be. Taking a file in and sending to Azure blob storage should be dead simple. As much as MS borrows from other frameworks like Rails, they need to improve on reducing the significant boiler plate to do stuff like this. /shrug i feel like multipart forms were just an after thought 
Probably because you don't understand it. Learn some real SQL some time and you may change your mind.
This looks great, thanks for sharing 
If you have some great DBAs, a good process and don't require too many changes having SPs COULD be beneficial, especially performance wise. That said, I'd take any ORM over SPs any day.
Thanks
I like this, never occurred to me there was a way to do it for the whole site. The require attribute i suggested only applies to the controller requests and not all resources. 
Nice content. Thanks for sharing.
More blogspam stolen from various places. Also maybe don't reuse your [sockpuppets](https://www.reddit.com/user/sophia_Mcleon) so obviously.
Also, you do not need to construct additional pylons
When you say it was brought all back to MVC, isn't WebAPI relatively new? Its still in ASP NET Core which is the the next evolution, isn't it? Again, would really prefer my front-end to deal with rendering, be it web, native mobile, other internal services just over HTTP, etc.
So, a couple weeks ago, I built a simple proof of concept app using 3 different stacks for a client. 1. The new .net core stack mvc + EF7 + postgresql. 2. A traditional IIS mvc 6 with EF7 + postgresql. 3. A clojure + postgresql app. A couple issues I ran into.. EF 7 &amp; postgres driver is also very alpha. Forms authentication is gone. Porting from mvc 5 to mvc core 1.0 is quite a bit of work, many things have been moved around. So yeah, prepare for moving target and much pain if you choose to build on top of the .net core over the next couple of months, especially if you are trying to bring some developers up to speed who aren't already familiar with .net. I think I would only attempt this with a very skilled .net team that was up for a big adventure. As for .net core deployment on linux, I don't think there's even a state of affairs yet, given they are in the process of a complete reorganizing dnx/etx under the dotnet command line tools, none of which have been released yet. My suggestion for an immediate need if you must use .net/linux would be to build on mvc 5 and run on mono for the next 6 months or so. Then port to .net core after the dust clears, which will be significant work. Personally, I would pick an alternative JVM language + postgres. Many solid options there, probably best suited to your immediate need and long term stability. 
Blog spam nonsense
&gt;When you say it was brought all back to MVC, isn't WebAPI relatively new? Yes, didn't survive to version 3. &gt; Its still in ASP NET Core which is the the next evolution, isn't it? No. They have shims for backwards compatibility. &gt;Again, would really prefer my front-end to deal with rendering, be it web, native mobile, other internal services just over HTTP, etc. Servers can render 100s or 1,000s of times faster than clients can. Go look at the ajax responses that twitter sends back. You'll see.
Yes. With dapper, you still write your t-sql (OP's t-sql will be sproc calls), it's just that with dapper, getting from t-sql to poco is painless.
LibertyIsNotFree is wrong in this case, WebAPI is what your want in that use case.
You realize that EF can project data into a different structure than it's stored in the database, right? Either way, you have to change code. A properly factored application can deal with data changes without affecting the front end code. *Edit*: Downvoters don't think stored procedures are code.
You're getting down voted, but you're right. The generated SQL from EF can't compete, performance-wise, with fine tuned SQL SPs/queries.
SELECT, INSERT, UPDATE, DELETE with INNER JOINS. Really anemic stuff for a transactional database. get real.
Yeah the action fires, but then just continues to fire after returning. Will install postman.
Others have hit upon the correct points, but for visualization, here is some comparisons between ADO.NET (hand written SQL essentially), Dapper and Entity Framework 6 (I found some for EF7, but it is still beta and therefore probably not optimized). Basically, performance-wise, Dapper runs on par with straight SQL queries (no ORM). http://www.exceptionnotfound.net/dapper-vs-entity-framework-vs-ado-net-performance-benchmarking/ Tl;Dr; EF Averages over 10 runs ( 0.77ms|3.57ms|113.45ms) Dapper Averages over 10 runs (0.047ms|1.01ms|7.94ms) ADO.NET Averages over 10 runs (0.013ms|1.03ms|8.84ms)
Hmm. Make sure you don't have any threaded operations pending that is returning directly to the result of the action ie: entity framework query that has yet to fire (the dbset being directly assigned without an interim ToList() or First()). As for the multiple hits, that is messed up yo :p
pylons?
&gt; pylons I assume he meant POCOs or other type of model class. Having used Dapper, I know that you can basically call stored procedures that return one or more result-set. Dapper can cast each row of each result-set to a class object or not.
LOL - that makes more sense. I figured it might be a typo, but for the life of me, I couldn't translate in my head :)
Yeah, watching a presentation recently made me realize how early/small it is now. I think they are definately moving in the right direction and I'm super excited to see them start to "do right" by alot of the OSS guys. I've been a huge fan forever, but yeah, its not quite there yet. Next year may be an entirely different story
Dapper will be a better fit. You can call stored procs with EF but you'll have to play a few more games with the api to make it work.
all depends on how you set it up. we run everything as a non-privledge user. Typically the only problem I see is when people have web projects configured to use IIS rather than IIS Express. Others have gotten more creative in how they have their environment set up so everyone's mileage will vary. If you kept things relatively sane you shouldn't even notice a change.
&gt; Others have gotten more creative in how they have their environment set up VirtualBox for the win! We actually had to do this when we couldn't install BIDS 2008 for some reason known only to global IT. 
Agreed. I've ran all of my dev work out of a VM for the past four or five years. Wouldn't do it any other way if I got the hardware.
The first thing I'm finding is that running as a different user is really messing with visual studio profile settings. Ugh
&gt; You realize that EF can project data into a different structure than it's stored in the database, right? Yes, but it's much more difficult to do it at the EF level and you tend to lose the ability to use EF's advantages such as being able to save a whole object graph at once. 
Here's a real example: When the database was first created, all messages were stored in one small `message` table. Over time that table got wider, longer, and increasing complex to the point where it was bottlenecking the rest of the system. So I moved the most annoying message type to a separate table. That message type has a few columns that only it used, so those were moved too. There were no application changes. All of the changes were encapsulated by the stored procedures so we didn't even have to stop the applications during deployment. 
These days I use procs to reduce the amount of time it takes to deliver code. (Or at least I try. Our customer is stuck on an old version of SQL Server so I keep running into cases where a simple lead/lag query would solve the problem in a one-liner, but instead I have to write a bunch of C# code to simulate it.)
&gt; Such as dynamic SQL (e.g. dynamically constructing WHERE clauses, etc.), or XML parsing. In general I agree. However, I have known people who use XML for complex batch insert/update operations. (Though I suspect that TVPs would have been even better.)
I'm waiting to find out how many registered pieces of software are going to crap out on licensing when they're run under alternate credentials
Dapper is a very, very thin layer on top of ADO.NET. Basically it just makes mapping result sets to object collections really fast, as good as the code you would write yourself.
&gt; Don't use Ajax mvc controls. It's a helper method from Web forms era Web development Ajax.BeginForm is not from web forms. It was made for MVC. 
**With the limited information you give me, I'd probably advise you to go another route: use the repository pattern and use either raw ADO.Net or any micro ORM of your choosing (Dapper, ORMLite, etc.)** But to answer your question a bit more in-depth ... I'll tell you a story: Once upon a time, we were in similar situation like you (mind you this was in the EF4 -EF5 days). No direct access to the tables allowed, so we ended up mapping everything with other database objects. All of our reads became views, all of our manipulation became stored procedures. And we ended up mapping everything in EF to entities of our own. The advantages: * Our entities did not have to correlate to our database tables * EF gave us relation navigation for 'free' between entities (once mapped correctly). * You can have lazy loading. The disadvantages: * It was hell to maintain (manual EF mapping, stored proc and view maintenance, etc.) * Performance was bad (not the fault of EF, I'll explain below) So ... what happened in this scenario, is something I've seen in a lot of projects that involve an ORM like EF. The developers think EF is this magic tool that will solve all their data access problems and that they needn't worry any more about DB queries. So they just write up something in EF that works. Often-occurring problems (mainly due to developer laziness): * N+1 selects (The worst I've seen was an 2N*M + 1) * Full objects loaded where only 1 scalar field would have sufficed * Complex queries in LINQ that were slow, where a query that achieved the same result in pure SQL was more readable and a couple orders of magnitude faster. In conclusion: I think the better part of the projects that I've seen EF used on would have been better off without it, or at least with a second data access strategy next to it. If you force developers to think about what happens in the DB, they will generally write better data access, but it also requires more time. *So ... to come back to your question. If your situation requires you to map dozens of interconnected entities that already exist in the DB, EF can help you get up to speed a bit faster. But for high load stuff you will need profiling and a backup strategy like Dapper or raw SQL (you can execute raw SQL on an EF Context if you wish). If you start building entities from scratch, and your developers are not against writing/tuning SQL, I guess repositories with a Micro ORM would be the direction I would push for.*
At least they gave you a priv account. I'd create probably a hundred support tickets each day if not... If like to know which licensed fail.. Do you use intelli j?
Setup your app pools to run as the same user as Visual Studio and you will not have any problems running under IIS.
It's not from Web forms. It's from that era of development. When ms felt that there must be a server side control to every client side action
I had to deal with a generic csv to table routine that was choking on largish datasets. Changed it to use STUFF and FOR XML and it hummed along.
But some people have told me to check out MVC first because a lot of concepts carry over to Web Api, which may be missed on a pure Web Api focused tutorial. Is that true?
Fairly typical problem. If your machine is *not* used for anything other than development, you don't need rights other than on your own machine. So "local admin" should be good enough for your account -- and it doesn't trigger anything in a security audit. (Trust me, we go through dozens a year as a Large Financial Company.) The domain admin needs to set that locally on your machine, and everything will be fine. You can't do anything stupid (like shut off antivirus), auditors will be happy, it's still check-able on the security clipboard of stupidity your admins have to put up with. 
This. My company puts the worst antivirus on our dev machines doubling our already slow compiling times, then they delay Windows updates for years. Then they give marketing macs. They then say Hipaa only requires Windows machines to have antivirus and to be domain controlled. 
Check out BI Cube. Works with .NET, you build SSIS packages behind it. On the front end you hook into data "cubes" you've built, and the users works in Excel. Great stuff. We're migrating all our reporting over to it. It's a Godsend. It saves a ton of hours from my developers building reports for users, and users feel empowered by building their own reports. If you prefer open source, check out Pentaho. Same concept, but users build their reports in a browser. Good luck. 
I would agree, but if they won't let the devs write SQL I agree that they probably won't let them use EF migrations. I personally use EF to build it first and then replace with dapper where I need performance. I really like the convenience of migrations.
I don't have local admin with the account I'm supposed to work in. I have a second account with local admin that I'm supposed to use to "run as" any programs that require admin rights. Running Visual Studio as the elevated user from a session as the unprivileged user triggered a profile creation for Visual Studio that appeared to work properly on initial inspection. I've looked and I can find no scenario listed by Microsoft or elsewhere where a developer runs an unprivileged session and must use a separate account with admin rights instead of "running as admin" with the initial account. Is this a "thing"? 
I've typically worked for places that exclude iis and other folders from the av.
Cool! Thanks for the code example. I'll take a closer look and see what I can come up with. I was actually going to rewrite the backend in Node to get a bit more comfortable on that end. Mainly because for such a simple app, Azure seems to be quite expensive for some reason. 
Thanks for following up! The design wizard is too cumbersome unfortunately. We're only interested in letting people create tabular reports, we don't want to give users a complicated Microsoft Access style report designer.
So.... Why not just properly configure your path variable?
The whole purpose behind model binding is to take a request and turn it into a well-formed object, either validating or failing it. Show me an endpoint that requires dynamic data and I'll show you an endpoint that is doing it wrong. Also: &gt; Even though the dynamic type seems to be everywhere these days I'll need a source for that. And the blog looks horrible on mobile
Hi friend, Thanks for your comment, I am not doing anything related to marketing, i have shared which i learn and posted. Please understood before making comment 
Do you have an external keyboard option? 
I do not use an external keyboard. But I could just easily buy one and plug it into the docking station, so it's ready to use when the notebook is put into the station.
It is true that a lot of the concepts carry over, and you could make your MVC just return status codes. That said, there is such a small different that you will quickly learn anything MVC only. (if/when etc) Personally I would stick what would makes most sense and then keep it simple.
Some time ago we used [i18next](http://i18next.com) with DurandalJS. We had a db with translations, packed them up as json in the format this lib accepted them and sent the payload down to local storage on first connect. The html had special markup for that lib so texts, placeholders were injected nicely. Granted we only had to support English + 1 at any given time so the payload's size was not an issue. We determined the user's language based on previous settings (cookie or storage flag), the browser's language and if all else failed defaulted to English. With this, we initialized i18next with the appropriate language setting. The lib's page and doc looks a lot different since I used it so I don't know what changed since then.
Resource language files work quite well, however you may want a more robust data store for the words.
Which cloud provider are you using? Azure builds this in with tons of docs for their Azure AD service.
In a comment, the author equates it to the `ViewBag` property. Even though they accomplish *very* different things, and `ViewBag` is a code smell of its own. &gt; And the blog looks horrible. Amazingly, it actually looks better on mobile than it does in Chrome on a desktop.
Reddit has very specific rules around self promotion and Spam, and AngularBeginner is correct - You can't continue to just post content from your website. Please stop or you'll be issued a ban from the subreddit. &gt; Feel free to post links to your own content (within reason). But if that's all you ever post, or it always seems to get voted down, take a good hard look in the mirror — you just might be a spammer. A widely used rule of thumb is the 9:1 ratio, i.e. only 1 out of every 10 of your submissions should be your own content. https://www.reddit.com/wiki/reddiquette
Yeah, SSO is never easy but it sounds like that may be a bit of a pain, you may not have much choice however. You could potentially run AD out of azure and use it as the auth provider for your AWS app though that seems like even more overkill. SSO is always way more complex than you think it should be. Best bet is to pick a method and commit to it. Setup will be a bear but once it works you can pretty much ignore it for quite a while.
If your using mono, your fine. The only difference from that point is just how the assemblies are found and loaded into the application. You can reference any mono assembly in the project.json as a "Framework Reference".
Dapper.Net yes, this is exactly my train of thought. Or even PetaPoco, which adds a few nice features. 
Doesn't newing up the fixture in each test defeat the purpose?
Are you only doing SSO with active directory? I suggest you also consider Json Web Tokens. It's now an industry standard and lets you do SSO with virtually anything. http://jwt.io/
I wouldn't use the location based authorization for an mvc app. Routes make it fairly trivial to circumvent and the AuthorizeAttribute is the standard approach. [This](http://stackoverflow.com/questions/1712167/asp-net-mvc-localization-route) stack overflow post details how to add a route to capture the culture info. The top few Google results for " mvc localization" provide other options using route handlers.
I've thought about it, but it's really a big system, and I really would preffer to keep it organized, using my location at url path, or subdomain. But, when I read about it, it just seens easier to use the location at url path. I really wouldn't use it as query string.
Anyway, I have the culture built in the url, this is my route: routes.MapRoute( name: "DefaultLocalized", url: "{lang}/{controller}/{action}/{id}", constraints: new { lang = @"(\w{2})|(\w{2}-\w{2})" }, // pt or pt-BR defaults: new { controller = "Home", action = "Index", id = UrlParameter.Optional } );
Have you ever used angular before? How familiar are you with C# and EF? I've been doing development in all three for about 2 years, and reading your question tells me you're fairly new to this. Like, maybe just finished the angular tutorial on their site or just finished a university course on c#/EF kind of new to this. My suggestion is, if my assessment is fair and you are new, is to not use angular at all and just do this through MVC in .NET. It'll be easier, cleaner, and less likely to make you quit out of frustration. If I'm wrong and you aren't new, this question would be so basic to you that you would have found it on stack overflow already. Try lynda.com or pluralsight if you're into video tutorials, or look up some basic .NET MVC tutorials on google.
I did that after I posted the question and that didn't fix it
At least in their repositories there are packages that contain "rc3". It's not been announced yet, but I expect (and hope for) it.
The problem is that the tooling is still a mess. You will spend too much time beating your head against a wall because not only is it poorly documented, there's lots of old inaccurate blog posts applicable to previous betas. It's going to waste your time now, and the experience you gain with these tools may be irrelevant when the next beta shakes things up again. There's better things to spend your time on to learn.
You mean the On .NET Podcost from 4th February? They just announced they will release **something** at build - not that it is RTM. At least I hope they won't be so stupid and try to release a RTM version on build - it's just not ready and not well enough tested. Especially given the huge rewrite and refactoring that comes now between **RELEASE candidate** 1 and 2.
Exactly my point. Setting yourself a deadline and having many unforeseen obstacles is more of a realistic software development experience. I regularly ask candidates to code a reverse polish notation calculator because it weeds out the people who can't adapt quickly. It almost always gets to printing RPN only in the time allotted (which again is like real development :p ).
I am still deciding between PetaPoco and NPoco. Seems the latter fixes issues faster, has multiple result set support, etc... Any thoughts?
This guy took the words right outta my mouth
Not me. 
So, you think I should stop using Form Authentication? Do you think the problem is probably this? My main concern from migrating from Form Authentication, is cause we already have the system running on it, and lots of users. I hope it's not that hard to migrate.
Authentication and Authorization are different mechanisms. You can use forms authentication with attribute, resource or policy based authorization.
Check out the inbuilt .NET charting engine. Its quite good. It can do both WinForms and Web. https://msdn.microsoft.com/en-us/library/system.web.ui.datavisualization.charting%28v=vs.110%29.aspx https://msdn.microsoft.com/en-us/library/system.windows.forms.datavisualization.charting%28v=vs.110%29.aspx Quick example: http://tomaszrabinski.pl/wordpress/2010/01/04/gantt-chart-why-not-2/
Hah, wonder what we were doing wrong! Oh well, everything works now :-)
I've not heard of NPoco, so can't really comment. As it's a fork of PetaPoco that's a good start though.
Check the "awesome .Net" list on GitHub.
I work with POS systems and I've found the [Dapper](https://github.com/StackExchange/dapper-dot-net) library works great with retail database structures.
Yeah, like the others are saying-- that's a file share, not a URL of a service being hosted on IIS. The good news is that this might mean you have access to the raw source of the service, rather than just an endpoint. You can then examine it and run it locally as necessary. Once you do get it running somewhere, you should be able to just add a web reference to it in Visual Studio. Generally ASMX web services will generate WSDL for you if you put ?wsdl on the end of the URL. E.g.: http://someurl/service.asmx?wsdl This is what Visual Studio is doing when you set a web reference-- it's downloading the WSDL and then using it to generate a proxy class(es) that you can use to access the WS.
Second this completely. Avoid the siren song of EF.
I'm using node and angular as my gradual escape from the MS ecosystem. It's always a breath of fresh air coming home on an evening and having my own machines run Arch.
Have a look at the result from https://github.com/RehanSaeed/ASP.NET-MVC-Boilerplate, it uses allot of the concepts from the article you posted in a real world application. I have used this on a couple of projects and it is a good starting point.
just downloaded! i'll try! Thanks!
The only problem is that it is using exceptions for control flow.
There is a whole subset of the .net framework for dealing with this: [code contracts](https://msdn.microsoft.com/en-us/library/dd264808%28v=vs.110%29.aspx)
Fluent validation is definitely useful. We use it for all model validation. Very flexible. 
They're right. Most of what the webapi project does for you is behind the scenes. You'll be using M~~V~~C. Once you have MVC understood, webapi won't really require more lessons.
we use this library and are quite happy with it. we have it tied into our request pipeline so the calls to validate are done automatically and the dev basically just needs to build up the proper AbstractValidator&lt;Whatever&gt; and it all ties together quite nicely.
Have to admit, that was TLDR. But a similar issue came up a few months ago on this sub. Best thing I saw was IValidatableObject. 
If you turn your parameters into a model you could use this to validate the model (and any others) its pretty feature rich, well written, extensible and very useful! https://github.com/JeremySkinner/FluentValidation 
I'm partial to generic validation classes that make use of Linq, as presented in this article and in Scott Allen's "Linq Fundamentals" video series on Pluralsight. https://allangagnon.nettryx.net/2011/06/12/businessrules-using-linq/
Do you not want to report multiple validation errors at once? var allErrors = new List&lt;string&gt;(); if (someViolation) allErrors.Add(description); ... return allErrors();
Painful: yes. Did it work well: kinda. Data driven apps are usually fine but once you want to use sensors or non-http protocols or unusual transport layers you end up Implementing per platform or building/using native libs. Also, their IDE is shit and the VS plugin isn't really any better. 
He wrote it as a string extension method, so no it won't.
Disagree - You should query and manually return the 400. e.g. return BadRequest(); Exceptions hurt performance of a web service and unnecessarily reduce req/s -- scalability. 
Hard to say. I'm familiar with both .NET and Android and while being awfully bad with Objective C I kinda like swift. That's why I prefer implementing non-standard apps native. If you're working in a team with a strong .NET background it probably makes sense to use xamarin. 
I know I'm in a minority here, but I absolutely hate the fluent style. Although this particular example isn't that bad, usually it quickly degenerates to an *unreadable* mess. I just don't think it carries much benefit over and above a few simple method calls. And extension methods IMO should be used more sparingly.
I've been developing a Xamarin iOS application for the past nine months with a team of about five. As C# developers, already on a C# stack, Xamarin suited us perfectly. We built a suite of RESTful APIs and then a Xamarin client to consume them. So far, I would say that Xamarin has served us well. The only major challenges have been licensing and configuring a dedicated build server. Recently www.bitrise.io began offering Xamarin build servers for subscribers; with a little bit of effort, they work quite well. One interesting bit of our application is that we have a central component which we elected to build entirely in Objective-C and then bind with Xamarin. This component is an SVG viewer with a lot of intricate business requirements and Xamarin was _not_ the right tool to use for this part of the application. Binding to the iOS library was not a problem and the solution works quite well. We're planning to move onto Xamarin Android development soon. As we've implemented our application so far, we will be able to reuse significant segments of our shared PCL code; therein lies the "promise of Xamarin". As with the iOS version, we will be building the central SVG component entirely in native Android. We'll have to see how that goes.
Yeah, RubyMotion is free, NativeScript is free and ReactNative is free.
My beef with Xamarin (I have a license): 1. Pricing (obviously - RubyMotion has a free version, React Native and NativeScript are also free options for developing native apps). 2. The editor is so so - it does the job but it's buggy and you have to work around this. 3. It seems they're targeting enterprise customers. I think the reason for my point 2 above is because their focus is on Visual Studio, all the tutorials are for VS and all the effort seems to be on making a VS experience easier, each release seems to be trying to decouple the need for having a Mac. 4. Related to point 3 - I don't use Windows and don't find it a great experience using Xamarin Studio. I'd liken it to using Eclipse.
And ionic
Care to explain why? I built my first application in EF towards late last year and found it pretty interesting. 
Deploying ASP.NET applications on a Debian vps box would be awesome. It will greatly help attract more people to ASP.net. 
Voltage/current isolation to protect the usb port on the computer
A future with out IIS hosting is a good world. 
I'm adding my code to an existing web service and it already had a method that did validation by using lots of if statements and stopping when encountering the first error. I based my enhancements on that model however you make a very good point, I'll modify my pattern to return all errors at once.
Until you put code in your site. I bet most IoC containers alone would shit all over these gains.
Yes there are many others but I was comparing with other frameworks for building native apps.
ASP.NET Core already uses an has DI/IoC built in so it is part of these measurements
https://msdn.microsoft.com/en-us/library/jj662724.aspx
tried what you wrote but it didnt work. What struck me as odd is that i got my code to workt by manualy inserting a index number, [0] and it started to work atleast. someservice.add(@Model.GraphData[0].GetValue()); Not the prettiest when id like to iterate like 10 objects but heck it works :P Still curious on how to get it to work "propperly" 
Well I think I should clarify. You should save your model in the JavaScript object like I mentioned above, but then you'll also want to modify the code in your loop to use the JavaScript object instead of referencing your model like you have been. Change this : @Model.GraphData[0].GetValue() to something like var model = @Html.Raw(Json.Encode(Model)); ...Then in your code: for(var index = 0; index &lt; model.Length; index++) { someservice.add(model[i]); } If you inspect your script in the browser, you should see that your JavaScript object model is an array containing whatever elements your C# model contained. This is assuming your original C# model was some kind of collection. 
The list is kinda long... File read and writes. Importing. Adding extensions. I believe even nugget has to be under admin. My question is: how does your IT group not know this.. And also why don't they know that. And also, why can't they Google it? 
That's a nice point.
Is there a place like github where people can store documents and best practices and rationales for developers? A repository of some kind as a reference? 
Or, yanno, you could realize that it may have nothing to do with trust and everything to do with minimizing attack vectors, especially on a machine that has frequent interactions with their product source code...
&gt; Practically that means easier deployment Webdeploy is best deployment system I have come across &gt; better resource utilization Maybe &gt; and much lower response times. This is very much app related. 
Man.... you're a developer, and if you have to justify needing admin access, then you may need to start looking for a new job. I'm an admin on my local machine, and admin on every server needed run/test/develop my app.
Sounds like you want to go nano server (option of Server 2016)...
Having shipped things in node and python i am super eager for Windows Containers to go live. Low overhead and ceremony services are great. 
I'm not sure you know how to Reddit.
Form experience working in IT and moving to dev, most .net devs seem pretty clueless at basic IT tasks
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/webdev] [NancyFx - Are there any medium to large projects using it? (x-post from \/r\/dotnet)](https://np.reddit.com/r/webdev/comments/46na7e/nancyfx_are_there_any_medium_to_large_projects/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
My rationale would be i am admin to my PC or i quit. I don't get paid enough to deal with your IT bullshit. Edit: converse question to the IT guy "why do you need admin access to anything?"
Stackoverflow.com?
So, you're a liar. You claimed that **you personally** had started 1000 containers in a second when you've obviously never done that. And you didn't actually cite a source. [This is a citation](https://github.com/docker/containerd/blob/master/README.md). And it's not valid. Because the README says nothing about the hardware involved, "126-140 containers per second" is just a random guess. What the hell does "starting a bunch of containers quickly" have to do with how the applications in those containers perform anyway? I could start a bunch of VMs really quickly too. You can't start 1000 containers in less than a second and apps in containers do not perform better. You are totally and completely wrong and you've clearly never actually used these tools in a work environment. 
I've never looked at voat.co, however I will suggest you should probably be looking at Server 2012 R2. It's the standard for deployments where you don't have vendor defined OS dependencies. 
What does "full" mean, exactly? Kestrel **is** an http server. I don't plan to ever expose it to the Internet. The aspnet team suggests that you run it behind nginx or IIS or such.
index is a JavaScript variable that you're trying to use in c#code. The c# code is executed on the server before the page is sent to the browser. 
Nothing public but I know of a few internal projects that are using Nancy. The ramp up time to get started with Nancy is pretty small and for the most part, once people used it, they liked it well enough to keep using it. It helps that they have a fairly active community on GitHub and Jabbr too.
Recycling an earlier comment: &gt; The appeal of these drop-in ORMs is that they optimize for the problem at hand - they speed up development by either creating your database or your POCO classes. The illusion of rapid progress is very attractive, especially if you're trying to sell something to management. &gt; The problem is that, in the long run, this is a terrible optimization. What you should optimize for is the performance of your application and your ability to maintain it. For this, simpler wrappers like Dapper and PetaPoco are ideal. However, if you do Code First stuff with EF, it might be worth keeping around for its ability to manage database migrations extremely well.
It's really very good. 
I would recommend looking into TPL (Task Parallelism Library)
I think you should look into something like this: [How to: Make Multiple Web Requests in Parallel](https://msdn.microsoft.com/en-us/library/hh696703.aspx). I would make one small change from example in the link. Instead of awaiting each request individually, I would keep an array of Tasks and then use await Task.WhenAll. If you need a primer on tasks, see here [Task Parallelism](https://msdn.microsoft.com/en-us/library/dd537609). Good luck.
The answer is yes, do concurrent calls. Look into using Tasks and async/await calls, like httpclient.getasync. Create tasks for all 25, add them to a list, then Tasks.WhenAll(list).Wait().
What's the contract like for after the free period? 
How about this? RuleFor(s =&gt; s.ProposalDetail.AgeMin ?? 0) .InclusiveBetween(1, 99) .WithMessage("Minimum Age entry is required and must range from 1 to 99 years."); This solution just falls back to 0 if min age is null. I'm not familiar with fluent validation, but I'd be surprised if you couldn't concatenate rules. It'd be more succinct and you wouldn't be repeating yourself or conditionally changing the value being validated as I am above. Edit: looks like you're looking for RuleSets https://github.com/JeremySkinner/FluentValidation/wiki/b.-Creating-a-Validator#rulesets
Azure hosting and Visual Studio licenses are our only expenses related to ASP.NET. ASP.NET Core is going to change the game here, allowing ASP.NET hosting on Linux.
It's all good until biz spark runs out. Stackoverflow experienced this recently with sql server 
Personally, I'd setup an offline task that updates a local data store. You'd have to know the acceptable window for data "freshness", but that way things are a lot more manageable. If you're using Azure, it'd be through a separate WebJob project that could save to DocumentDb/SQL Server/MySql; then your front end would simply hit that. If you're not using Azure, so long as you're on a recent version of the .NET framework (I believe 4.5.2 or 4.6.x) there's similar functionality now baked in, although I forget its name offhand. In that case it'd be within your web project itself, but the concept is the same.
I just wish it was stable enough to start developing with now. It's going to change my life!
I can't believe Microsoft is doing this. Their whole business model depends on Windows. They want developers to write software for Windows. So now they open it up where we can write dotnet software for Linux. Visual Studio Community is very good, and free. And they're giving away Windows 10. How are they going to make any money?
Please elaborate? 
But the requests are based on a user action, I mean I can't make the calls without the user giving input and then clicking a button, if you wouldn't do it on a user action how else would you accomplish that?
&gt; How are they going to make any money? 1. Enterprise sales 2. Microsoft Office 3. Mobile devices (Surface, Windows phones) 4. ...
It does make sense. To be honest I wanted to avoid saving comments (When you said 'save' I immediately thought store in a db, apologies if I'm misunderstanding that). As for how they're going to be present it's just in a collapsible panel. The application itself is going to be just showing a User information about a Reddit User and displaying it through visualizations (graphs ect.) similar to http://snoopsnoo.com/ but my hook being that I'll be performing Sentiment analysis just to see how someone being positive/negative impacts on someones 'Score' on here. It's just something that I thought would be fun/interesting to see. So when someone comes onto the site they're just going to input a name and get displayed the results, I want to avoid storing any information in a database with regards to comments. I have a feeling that I may have misunderstood your comments though so if I am please correct me :p Thanks for the help btw buddy.
Wa? no way they would qualify for bizspark. 
I currently have a little script that executes and shows a spinner so that they can see something is going on:) thanks for the help anyway.
We run .NET on an azure VM. Each server costs us around 300/m at the current sizing. Which is way more than we need currently. (5000sessions/day). Everything runs on a single VM. API, Web, DB, redis, rabbitmq and wordpress. We get maybe 5% utilization tops. We opted to run Postgres instead of getting vendor lock in with SqlServer. This saved us a bunch on licensing and lets us run ubuntu down the road for part of the infrastructure. Azure apps hosting is also very nice and we considered it but we have more time than we do money so we opted for a VM. The reality is .NET is pretty cheap to run once you're making money but could be considered expensive in early stages.
When the BizSpark program ends for you, you have to start paying for licenses. Not much more to it than that, nothing special about SO's case. 
That's kind of the point. They were a BizSpark startup, really Microsoft's poster child for BizSpark. Then they got big, stopped qualifying for BizSpark, and the reality of SQL licensing was about to hit them hard. They were considering porting everything to Postgres, because it was far cheaper to pay for developers and engineers to port over than it was to pay the insane SQL licensing rates. The founder talked openly about it on Twitter, a MS rep got involved, and then everyone got real quiet. Assumption is that Microsoft cut them a HUGE discount to stay on SQL. Edit: just realized I kept calling it BizTalk... Too much working on BizTalk servers lately, I guess. 
Isn't that restrictive or did they open that up to let you do everything? I don't think you can even use plugins with it last i checked
That was the Express edition. Community edition is basically the same as "Pro", can use plugins, etc.
The only real license you need to pay for us Visual Studio, but I would recommend an MSDN subscription if you can swing it. 2500 a year for the enterprise edition gets you all kinds of tools, office licenses, etc. You also get free azure dev credits to try it out. Personally I find azure a little expensive for my small stuff and host everything on an $5/month package until I actually need what it provides. Azure is very nice though, and provides a lot of value for the money. That said if the app is developed right you can drop it on azure pretty easily even if you don't start there. 
You have to start paying for additional licenses. You can continue using what you're already using at no cost.
You could try developing with NancyFx or ServiceStack in the meantime, if you want to play around with hosting on Linux.
The free version is limited in who can use it (e.g. megacorps can't use it to save some money), but it's not really limited in what it can do.
They only have two SQL servers, how can it be that expensive?
Running something like 24 cores per, with per core licensing, I'm guessing that price tag came close to $200k per year... Actually assuming enterprise edition, closer to $300k. Standard would run somewhere around $75k I believe with that many cores.
I'll take a look. We're starting a greenfield project on Monday and have to choose a framework, so maybe...
How have you found Azure support? Do you get a special hotline or do you have to submit slow support tickets to India like everyone else?
SQL Server is the killer though. Porting to .net core would only eliminate 1/3 of our licensing costs. I really hope MS look closer at the affordability of SQL Server. There seems to be a divergence in the way ASP.NET and SQL Server are going. Perhaps MS are trying to steer users onto Azure, but Azure is still prohibitively expensive to get anywhere near the performance you can get running off your own servers.
So we do have contacts that we can reach out to, but that's mainly for help on utilizing Azure to the fullest. When there are outages or issues, I submit a ticket like anyone else. I don't find there to be much trouble with their support system. I really only do so when there are outages and I'm getting responses within an hour. And really, every time there's an outage it's widespread and not something specific to my configuration anyway so it's always a wait and see situation. In our case, every time we've experienced an outage we've learned from it and made our system more redundant. I'm actually looking forward to the next time app services in US East go down, so I can see my servers in the Central/West pick up the slack. :)
Are you sure about that? It seems like StackOverflow wouldn't have been affected if that were the case.
This does indeed sound like an interesting project. I cannot think of anything similar, but I can say a framework like this could be quite useful in my shop. We've been looking for a better way to KPI. If you posted up a git repo I'd likely be inclined to poke around and contribute where I could.
You may want to look into using SignalR to communicate with a page hosted in IIS that does the UI work and querying back to the service. Use IIS to handle any of your intranet security concerns and do what it does best and serve the pages/JS. Let your service do what it does best and leave the UI to a real website. You could even go one further and use SignalR on the client (browser) and just use the website as a message hub.
Make sure that in the azure portal the application settings of your webapp has an App Setting "Hosting:Environment" with value "Production". Because if you take a look at the Configure() method in Startup.cs there is a check what the environment is and migrations are only applied if its not Development
Not sure why your'e downvoted. &gt;Any individual developer can use Visual Studio Community to create their own free or paid apps. &gt; An unlimited number of users within an organization can use Visual Studio Community for the following scenarios: in a classroom learning environment, for academic research, or for contributing to open source projects. &gt; For all other usage scenarios: In non-enterprise organizations, up to five users can use Visual Studio Community. In enterprise organizations ... no use is permitted beyond the open source, academic research, and classroom learning environment scenarios described above.
I can't seem to find this setting on Azure though: http://i.imgur.com/qE3UUPE.gifv Am I doing something wrong here? This is my Configure(): public void Configure(IApplicationBuilder app) { // Use different settings depending on debug or production builds if (config.Get&lt;bool&gt;("debug")) { // Register the Microsoft default Error Handler app.UseDeveloperExceptionPage(); // Register the ASP.NET Runtime Info Page app.UseRuntimeInfoPage(); } else { // Use a custom user-friendly error page if not app.UseExceptionHandler("/Home/Error"); } // Register MVC Middleware AND specify the routing format app.UseMvc(routes =&gt; routes.MapRoute("Default", "{controller=Home}/{action=Index}/{id?}")); // Register the File Server Middleware app.UseFileServer(); } I did a check based on a setting in my config.json to set the error handlers but I think that doesn't seem to be working properly either as the hosted site seems to be using config.dev.json instead where I have debug set to true.
This doesn't add any value to the discussion. Next.
You can look into the ServiceController api that comes with the Windows Service. If you build the service to be contacted outside for either managing, querying, etc. Now I haven't personally used it but now it seems quite useful. Then you can build a MVC project to run on that local IIS or even a separate server that contacts that service. Edit: spelling
I suspect we're in very different segments of the market. But even if that's true for yours, devs make up a very small percentage of the user base. What you develop on doesn't matter nearly as much as where you're deploying to, and what your users are on. And for good measure, [there's this](http://time.com/3975115/microsoft-windows-10-share/).
I wrote a log4net appended that used signalr to write to a web page hosted using Nancy and owin. Pretty simple actually. I can try and find the code somewhere if you'd be interested. Got it down to a simple nuget install for most of it if I remember rightly. We have a load of 'services' at work that are either console apps or have winforms ui for just the reason you state.
&gt; I believe those cookies are being set by WordPress with a path. For safety reasons they're inaccessible from other non-descendant paths. Agreed, but shouldn't CookieContainer at least store them for future use? E.g., I'd think that it would store the cookie with the path `/wp-admin`, then use it in the request for `/wp-admin/` in step 3. That seems to be what the browser does, anyway.
To pile on the "use an existing logger" train, Serilog + Seq would give you exactly what you are looking for. Seq 3.0 has some pretty cool looking query support too
I don't think you'd be able to do a build with a HyperV. AFAIK OSX isn't the issue in the build process, it has something to do with needing an ID of some sort that is only available on Mac Hardware. AFAIK you used to be able to do builds on a Hackintosh, but Aple caught on a few years ago. Mac mini is your best bet, or Craigslist an old MBP you can find the for like 200 bucks. Even if you can get a build running, you won't be able to sign, thus pretty much rendering the thing a pile of useless code You can also get a cloud build box like http://www.macincloud.com/ or http://xcloud.me/ this way it's something you don't need to manage. You may even be able to hook this up to your CI I'm actually really surprised Xamarin themselves don't have something like this, considering they are targetting more or less Windows/MS devs
Btw you can access those cookies (and parse them yourself) if you get the raw response.
Not sure that's the case as the fourth cookie, which is `httponly`, is correctly sent to `/wp-admin/` in step 3 above. But Fiddler shows that the third cookie (`path=/wp-admin`) is missing.
I've done hosted web inside a service before, though never explicitly for log viewing (there are way better ways to do that other commenters have mentioned). I've only ever used it for hosting WCF endpoints for middle tier services, or configuration and management portals so you can make changes to your services on the fly without having to cycle them to reload the config data.
If you watch the requests going across the line, the browser doesn't even send those cookies to the server so ASP.NET and CookieContainer will never see them. That path is the same thing that makes sure you can't ask for someone's Gmail authentication cookies from anywhere. 
&gt; If you watch the requests going across the line, the browser doesn't even send those cookies to the server No, the browser does correctly send the cookie in question. Only `CookieContainer` doesn't. And this is when the cookie's path matches the URL's. Hence my question whether there might be a bug. When I request `wp-login.php`, whether in a browser or with an `HttpWebRequest`, the server returns the 4 `Set-Cookie` headers in my original post. I can verify in Fiddler that this is happening. So far so good. Now, when I submit the login form in the browser to `wp-login.php`, WordPress returns a 302 redirect to `/wp-admin/`, the browser follows it, and cookies **1, 3, and 4** in my original post are sent to the server (that is, the two cookies where the path is `/`, as well as the cookie where the path is `/wp-admin`). However, when submitting through an `HttpWebRequest` with the same `CookieContainer`, only cookies **1 and 4** are posted. That's fine for cookie 2—we're headed to `/wp-admin/`, so a cookie for `/wp-content/plugins` should be excluded. But the cookie whose path is `/wp-admin` is missing, even though the path matches.
I guess I'm unsure then. 
Definitely check out web API. It's very easy to set up
That's pretty-much exactly what webAPI does What you have described is an API.
Thanks.
Same here -- thanks for the extra set of eyes in any case. It seems like odd behavior, and affects newer offerings that use `CookieContainer` as well, like `HttpClient`, so maybe I'll get in touch with MS about it to see if a fix is planned or if it's by design.
Yes, I know. The question is, is the server enabled for version 4.6?
You can't upgrade 3.5 to 4.6. These two versions are installed alongside, not as a replacement.
I always thought that was the CLR version and not the framework version. 
Thank you for the answer. &gt;This really isn't possible once a REST API is out there on the web it's out there for everyone. Yeah I understood that with my searchs, I badly worded my question. I want app A to respond with data only to authenticated users from app B and refuse it to others, but you are on spot. After reading the article it seems that Oauth2 is rather hard to fully understand and needs time to master. As I'm a bit in a hurry, I guess I'll try to use JWT alone. I'll go and try to find useful articles on google. If someone know of one, please link it! Thanks again !
If the web services are only ever going to be accessed by "application b" then this is definitely something best suited for network rules as you mentioned. Even if you plan on making the services public in the future, restricting it to localhost access only for now is your quickest bet.
I'm new to webapi and tokens also. I built a JWT system and incorporated it into my already created user database. The best resources I found were these two. http://bitoftech.net/2014/09/24/decouple-owin-authorization-server-resource-server-oauth-2-0-web-api/ and http://bitoftech.net/2014/10/27/json-web-token-asp-net-web-api-2-jwt-owin-authorization-server/ I ignored the audience part as I only have one app sending tokens (1 audience). It took about two days to implement and learn it. Edit: Forgetting one thing. The gotcha of building his spec, is that you can't revoke tokens since you never verify. I overrode another function to include a datetime of the last time the password was changed, and if it's older than the current DB time, it invalidates. It checks every time. 
https://msdn.microsoft.com/en-us/library/dd286726.aspx
Nope, automated UI testing is expensive. But honestly, if you can't afford Visual Studio Enterprise then you probably can't afford the labor costs to actually implement automated UI testing. No matter what product you use, it's a ridiculous amount of work compared to the payoff. *** My recommendation is to structure your code to use "pure models" and unit test those. Then use integration tests against your view models. http://www.infoq.com/articles/View-Model-Definition
those aren't unit testing test cases. Those are integration tests. 
Look at the MVVM pattern. It allows you to test view models easily through code. http://www.codeproject.com/Articles/165368/WPF-MVVM-Quick-Start-Tutorial
You've got some ASP.NET MVC and Razor stuff, but then say no CSS or HTML. Are those courses pre-reqs for yours? If not, MVC and Razor might force you to dig into HTML. And I know you said no T-SQL but If it were me, I'd go for Dapper/Micro-ORMs before EF. I think it's easier to layer EF on top of some understanding of Micro-ORMs, especially in a classroom setting and teaching bare-minimum T-SQL seems easier to me than explaining relationships and cascades and other things for EF. If you're really opposed to teaching T-SQL, I'd say RavenDB or another NoSQL database. 
Unit test testing meaning no external dependencies is a misconception. How big the "unit" is, depends on context. Experience helps a lot. Abusing of mocks and stubs often leads to less mantainable code, because the tests are more coupled to the internals of the production code, making it harder to change. Sources: - http://martinfowler.com/bliki/UnitTest.html - http://martinfowler.com/articles/mocksArentStubs.html - Hundreds of tests using both approaches 
Hours :) each course is a couple hours total. Which is why the content per course is super limited.
3 places that helped me immensely: [Official reference \(shitty but covers the very basics\)](http://nhibernate.info/doc/nhibernate-reference/) [Ayende's blog](https://ayende.com/blog/tags/nhibernate) The best for last: [NHibernate pitfalls](http://weblogs.asp.net/ricardoperes/nhibernate-pitfalls-index) by Ricardo Peres. That saved me hours. 
Yes, if it were me I would proxy the requests from client to app a through app b. Basically, the API that your client calls is a gateway to the rest of your infrastructure. This will allow you to secure things at the one layer, and the rest of your infrastructure can treat that gateway as a trusted client. Id still secure app B to app A with a simple api key though. Only reason I'd go to oauth is really if I needed to propogate caller identity down through the infrastructure, as it gets super complicated at that point.
That's not a useful comment, it pedantry. Also its probably wrong. An integration test is when you try to plug in your website to Facebook's authentication system, not when you happen to hit your own database.
We've been playing with TopShelf Windows services that run a Hangfire instance for the service event loop and exposes the Hangfire dashboard on a port for status. Works good for most of what we want.
The term "integration test" has a historic meaning when you actually bring together separately designed and built components, often created by people who have never met before. If you read computer science books from the 70's and 80's they spend quite a bit of time talking about the issues. Actual integration testing is quite complex and challenging. Sadly these days we've got script kiddies churning out crap code and are utterly amazed be people who can figure out how to stand up a local database. Seriously, if you can't figure out how to run your local DB "regardless of environment" you really should try another career. Maybe digging ditches or blogging. 
[removed]
Ok cool a connection string entry named "Data:DefaultDatabase:ConnectionString" now appears in Azure, however, it isn't one that is auto generated, it took what was in my config.json and added it into the string. That string was a LocalDB that I used locally. This is my config.json: { "debug": false, "InMemoryDB": false } I also have a config.dev.json here: { "debug": true, "Data": { "DefaultDatabase": { "ConnectionString": "Server=(LocalDb)\\MSSQLLocalDb;Database=HighScoreDB"} } }
You need a demo
And/or a significantly better readme.md
Go with asphostportal.com. It only cost for around $4.00-$6.00/month, fairly cheap. Beware with free hosting!
Spam.
The common usage of "integration test" is that you're testing the integration of different software modules, not necessarily written by different teams.
Ignoring that this is spam... Where do people put things like: * Model binders * HTML Helpers * Extension functions
I usually put them into Infrastructure folder on applicaiton root, separate them a bit, so Infrastructure\ValidationAttributes\ Infrastructure\HtmlHelpers\ Infrastructure\ModelBinders\
No one goes and looks at all of your source files. I actually started looking and after about 8 files i saw nothing interesting and closed it out. You need to provide a strong elevator pitch for why i should care in your readme
This is the only answer, and it's really part of the point of implementing MVVM to begin with. Put all of your actual logic in the viewmodel, and keep the code-behind and XAML stuff strictly for UI-specific things. (animation, tweaking control properties if necessary, handling, etc.). You can then run unit tests directly against ViewModels and have them behave as you would expect. You don't have to fake out clicking buttons, you just call the methods the control actions are hooked up to appropriately. I've always found automated UI testing to be a real chore for a bunch of reasons, but 90%+ can be now be tested easily (the really important stuff anyway) by properly implementing MVVM. 
The windows event log schema is part of the Windows SDK. You should be able create a DataSet from the xsd and read the XML file. From there it's all ADO.NET. https://msdn.microsoft.com/en-us/library/windows/desktop/aa385201(v=vs.85).aspx
You talking about the FREB files? Best bet would be to have a script shipping those off to a central location off of the IIS server. They are just xml files so as long as you have their little XSLT that's all you need. There's a chance you could read them right from the freb log folder location and just display them, but if they have things locked down I don't believe that'll be possible.
I would strongly recommend wrapping the counter in a partial with a cache expiration. Even if it's 1 minute it will offload busy servers.
Well I was thinking about building a quick admin page to add to our portal for viewing them, but I really like what you mentioned about shipping them off the server. That would really help. Then I could just read them onto our admin portal via iframe or something. Really doesn't matter how it looks. Just admins looking at it trying to pull stack trace, etc...
Meant FREB files sorry. My bad. But I might look into this too. Thx for pointing in the right direction!
I use combination of custom JWT, CORS, authorization features. Additionally restrict IPs, network related settings on IIS are like Icing on cake. But Web API should be secured esp in SPA kind of project.
There are none. Write one yourself or wait until the platform matured.
Great article and I wish I'd read it a few weeks back when I ported a class library. Actually the biggest issues I had were working with the new xproj project format in VS for multi targeting. It seems that all this is heading to a good place, but unlike previous Microsoft releases, the internet is filled with details of pre-RTM versions, much of which is misleading and confusing. Still, I'd rather have all this evolution happening out in the open. It's been truly fascinating to watch and be a part of.
It looks like [a lot of work](http://download.microsoft.com/download/B/E/1/BE1AABB3-6ED8-4C3C-AF91-448AB733B1AF/Report%20Definition.xps). Especially VB expression parsing.
Nhibernate predates EF by many years, it was far ahead in features and performance for a long time (and maybe still is in some respects). It would also be a defacto choice for people coming from Java background.
So the extra tools arent additional layers of code, they are helpers. Fluent allows you to map in more code focused way without having to mess around with the XML mapping files. Mapping generator allows you to quickly generate the mapping whether you are using fluent, hbs or whatever. They just make your life easier, no synchronization required. Codesmith and PLinqo aren't only for NHibernate.
Ask yourself, do you need the Identity framework? Do you need the implemented Datastore for the Identity framework?
&gt; glue between a object database and something like C#. No - an object database is a very specific concept, and not used all that much. You're thinking of a relational database (basically all the SQLs). NHibernate, EF and others are what you usually call an ORM - an object-relational mapper, because they "translate" the concepts between a relational way of thinking and an object-oriented one.
You use NHibernate as an OOP model of your DB. NHibernate doesn't have the Visual Studio integrated tooling like EF does to help you generate that model. The NHMG is the tool you use to query the database, pull down the structure, and create the models and mappings.
Thanks that explains a lot to me.
lol.. Desktop apps aren't even covered. How great of a percentage of GUI apps are using UWP today vs Windows Forms or WPF? Just dismissed as "not supported". I can understand WinForms, but WPF hurts given the very strong similarities to the UWP platform. If there is any scenario that needs porting guidance, it's this sort of thing. And if UWP is the way to go for future apps, and .NET Core is supposed to be here for cross-platform, why on earth do Microsoft not have any plans at all to support UWP on the .NET Core platform? I'm as confused as usual with Microsoft the past few years.
Oops, sorry about that, what I understood from the tutorial was that upon deploying to Azure, it would automatically generate a SQL database and in the application settings on Azure, it would state DefaultDatabase with the connection string already filled in with the auto-generated SQL database. When I did it, however, it simply used the connection string I specified in my config.json instead. If what the tutorial did isn't possible, may I ask how would I add it in manually?
It's not clutter, it's capability ;) NHibernate is quite modular, so you can swap out large portions of it if you need advanced caching or prefer another method of model generation. That's why it has an ecosystem of "important" modules. It also lets the community run ahead with what's "right" while the core project moves conservatively with what's proven - it's a bit more of an Enterprise approach. In practice that modularity has provided easy solutions to hairy problems. I'll take a little complexity in my NuGet packages to save a lot of complexity in my app logic.
Work - win7 Home - win10
The orthodox way would be to have a user class and assign each instance of that class a set of roles. You'd have a customer role and a seller role and a one-to-many relationship between users and roles.
Primary dev is in Win 10 VMs. That's for ASP.NET and Windows Services going to '12 servers, and tons of desktop (WPF, Winforms, Services) deploying to 7, 8, and 10.
Hope their pricing gets a bit better for the hobby programmer.
Ah ok. I'm not familiar with the additional tools. That sounds very cluttered. 
It probably will. Plus I imagine MS will add Xamarin to their BizSpark/DreamSpark programs to make it more enticing for individuals and startups. 
Asking the right questions. My biggest fear is they cannibalize the dev resources on mono and move them onto core or something.
This should get you going... I put in a few comments to explain: Public Class Form1 Private Sub Form1_Load(sender As Object, e As EventArgs) Handles MyBase.Load cms1.Items.Clear() ' This is your ContextMenuStrip control. ' The key is casting the result as "ToolStripMenuItem" ' and not keeping the default "ToolStripItem" Dim jobs As ToolStripMenuItem = cms1.Items.Add("Jobs") ' We can now add a sublevel to that "jobs" item Dim test As ToolStripMenuItem = jobs.DropDownItems.Add("test") ' Add the final level and event handlers to grab the clicks Dim x = test.DropDownItems.Add("ex1") AddHandler x.Click, AddressOf subItemClick x = test.DropDownItems.Add("ex2") AddHandler x.Click, AddressOf subItemClick ' Another sublevel from the top "Jobs" menu... Dim test2 As ToolStripMenuItem = jobs.DropDownItems.Add("test2") x = test2.DropDownItems.Add("exA") AddHandler x.Click, AddressOf subItemClick x = test2.DropDownItems.Add("exB") AddHandler x.Click, AddressOf subItemClick End Sub Private Sub subItemClick(sender As Object, e As EventArgs) Dim item = DirectCast(sender, ToolStripItem) MsgBox(item.Text &amp; " was clicked.") End Sub End Class 
Can confirm!
I'm hoping it will be free in Community Ed. too. I'd like to be able to make quick little utility apps for Android. Just not enough to learn the Java ecosystem or pay $1k/yr for Xamarin's VS integration.
consensus in our office is that they'll likely offer the tools for free.
Thanks for the response. I did ask Nat Friedman earlier on Twitter if Xamarin Studio would still be supported and he replied 'yes' although no more than that. I guess with JetBrains' Rider on the horizon Microsoft now has some genuine competition in the IDE market so investing in Xamarin Studio makes sense.
[The signup page](https://www.microsoft.com/bizspark/signup/default.aspx) says BizSpark applications are manually reviewed, how does this work? (honest question)
Pretty much. As long as you indicate that you are a developer of some sort.
Like /u/db92 mentioned, I think the tooling is actually going to improve quite a bit. Currently, there is no way MS is going to spend the time or effort required to port Visual Studio to Mac/Linux and VS Code is never going to be an full fledged IDE. But with Xamarin Studio, they now have a stable platform that they can start building on. 
There's still a hell of a lot of legacy code out there. It's worth knowing. Generally it's paired in its most ancient form with Access, or in a more present form with SQL Server. It's not that far off from PHP, honestly, in terms of methodology.
Here is hoping they can do another round in the grinder and keep Mono alive. I have some trust in Miguel Deicaza as he went into and out of Oracle successfully. But still the .NET community will take a blow from having just one .NET leader(M$) with little alternative for the time being. JetBrains IDE - Project Rider http://blog.jetbrains.com/dotnet/2016/01/13/project-rider-a-csharp-ide/
&gt;My biggest fear is they cannibalize the dev resources on mono and move them onto core or something. Can you elaborate why that's so bad, if they're both open source and both fulfilling the same purpose?
I was mainly referring to the fact that seemingly a very large % of the contributors to mono and monodevelop are on the xamarin payroll and those people do a ton to keep those projects moving along. If MS owns xamarin and are less enthusiastic about those projects it might slow down the pace of development on them just from lack of manpower.
Windows Server 2012 R2 at work and at home.
Try out apache cordova. Its a website wrapped in an app. Much easier to push updates, since its just a container running a site with hardware api support
Consider React Native before RubyMotion.
Yes it does, porting VS to *nix based systems is a non starter - it would be a complete rewrite.
Yes there are others - NativeScript as well but I have an aversion to JavaScript :-)
Proper naming mostly. Maybe a few lines about the class if it's not obvious. Also splitting code into classes makes it easier to navigate, thus easier to understand. Comments for hacks both where they are performed, and where they affect. Comments for "magical" or complicated logic and calculation. And of course comments explain what and why, not how.
Yup, and I kinda doubt MS cares about how well Monodevelop runs on linux and mac.
&gt;explain what and why, not how. this. p.s. I prefer to introduce an explaining variable than to add a comment explaining it.
When I first found out about Microsoft working with/purchasing Xamarin I was hoping that Microsoft would have a universal API for making desktop apps across Windows, Linux and Mac. So far I've been very disappointed. 
WINE ha implemented many of the Win32 APIs.
[Bridge.net](http://bridge.net/) and [JSIL](http://jsil.org/) can help a bit. I tried Xamarin to write an iOS app and found just using swift directly was much less burden. Same with javascript in the browser. This is coming from someone who has been using C# for 15 years.
Yeah, i agree with that - and get creative with the metaphor that you're using. Want to grab the products that you want to purchase but not sure if you actually want purchase them yet? You could totally GetProduct and it would make sense. But you could also PreparePurchase and then every time you have to use the function it'll be a step shorter that your brain has to reach, because you don't have to translate what you want to do into how it's done in the code.
This.
I'm not sure I completely agree with you, in this specific example. It hearkens back to the .NET naming conventions for actions. `Get` is the convention to return a specific item from a collection. If the method only searches and returns the product then I would absolutely name it `Get` (though it would preferably have validation and then it would be `TryGet`). However if the method does anything beyond simply returning the requested object then `PrepareProduct` would be a good name to use. 
For the most part I don't like having comments in my code at all, code gets updated, comments generally don't - you can't trust comments. There are the occasional exceptions to this rule though where it makes sense. If I'm building an API I do like having XML comments on my controllers because then I can use a tool like Swagger to easily create the documentation. With that said, as long as you properly name your methods, variables, etc. very little documentation should really be needed. If you have good unit tests with good coverage those could really end up being all the documentation you need.
Just a question- what do you not have an aversion to? Asking because I love pure JS but despise the over complexity of C++ - in most cases, not all. And by pure I mean minus all of the attempts to make it what it's not- like jQuery, React, Angular etc... essentially the Javafication of JS makes me cringe and have, maybe, the same aversion that you do.
No. Not a good time. Their internal builds still break, and there is no Visual Studio support. Even the Visual Studio Code is hacky as best, because it still relies on DNX. There's also no way to upgrade your installed version - you have to download and execute the installer again.
Most of my "commenting" is done by making my code extremely readable. I have some developers who like to make syntactic riddles - a lot of their stuff is really hard to understand simply because it's hard to see what's happening. In my code, I try to make it as obvious as possible what I'm doing, with naming, with whitespace, and by breaking out complex lines of code into less complex but more readable lines. If there's stuff that isn't exactly self-explanator or that might be confusing, I'll add a comment line or two. 
I've read Douglas Crockford's 'Good Parts' and made serious attempts to like JavaScript but failed each time. It's not the dynamic feature - I thought it might be but then I started coding with Ruby and Python and found I really like both. Possibly it's the context switching after working with server side languages - I find the endless callbacks in code weird to work with. But the biggest problem (which you allude to in your comments) is the attempt to force a classic OO paradigm onto a language that has the very different prototypical inheritance. The result of this is with a big project you end up with a mixture of the two and it's hard to follow. Angular 2 and TypeScript are trying to make it easier for those coming from server side C languages and at first it doesn't seem so bad but as soon as you hit a problem and are looking at a third party library you'll be working with idiomatic JavaScript and it becomes very confusing.
&gt; with a big project you end up with a mixture of the two and it's hard to follow Yes, super extra all of that yes. And double for the terrible third party libraries. Doug's book is great but he has plenty of follow up to that book which shows some really artful ways of using the language. John Ressig's name is used sometimes- I actually like his thinking a lot... but he did try to implement Class. I generally use RequireJS and the module pattern to do everything. I would say if you read and get anything about the language, check out that pattern. It is remarkably simple and elegant, gives you encapsulation and intuitive code management. And I love Python, too.
* Learning vNext (or whatever it's called this week) anticipating a good complementary skill in my market * Using for a small-scale personal project MVP Also, given the timeframe of Final being released within a year, this is fine for both points.
Yeah... I tried going down the .Net Core and ASP.Net Core route for some of my side projects with RC1. Didn't go that well. I'm not sure why they called it RC1 to be honest and not sure why they're calling the next one RC2 because I would still consider these betas at this point with the number of breaking changes, etc.
.NET is an open standard developed much like HTML and done under collaboration from the top language developers/people(Java,C++,Lisp,...) It was intended as an any language in &amp; any platform out, IL, adaptable, forward compatible platform. The controversy and maybe not true: Then when it was nearing release M$ jumped the gun and pushed .NET from M$ This is what I heard from the guys when I got into .NET, from inside the development &amp; beta process of .NET And backed up by some community documents.
Do you have a source on that? Because it's literally the first time I hear someone say that .NET was open source since the beginning.
www.dotnetexperts.com/ecma/ Is a ok place to start, but you can google "ecma .NET" It had ratification into an ISO standard in 2001, worth a read If your a .NET dev, it had heavy M$ people from the Java team (they built Java to 1.3) then moved into the core of .NET M$ lost the right to dev Java to Oracle, so the people pored tech into .NET That is why .NET is a super set of JAVA at the byte code(IL) level... Rant done
&gt;I have just found that you can type ''' and it brings up some xml This information will popup in the intellisense when you call a method which makes it very useful.
Its .NET Core 1.0, not .NET 5, correct?
One thing noone has mentioned is that often the best comments aren't related to your code that much, more to do with business rules, external protocol logic etc.. Even if your code is squeaky clean, a comment about why you've done things the way you have can save heaps of time down the track. Especially when dealing with external code/protocols. Or strange businnes rules.
There are so many breaking changes in rc2. If you want to deal with them now, then do it. And wait until it's actually released before you production it.
If you are considering .net core 1.0 then be sure you understand what limitations it has that will not be solved. For instance binary serialization and some reflection libraries will never be ported.
I don't remember who exactly said that, but there is a rule of thumb I always tend to stick with - if method name contains word "And" it probably needs to be split into several methods.
Omg. You're going to love [Object Serialization](https://goo.gl/gykQjS) and [this little "secret"](http://blog.codeinside.eu/2014/09/08/Visual-Studio-2013-Paste-Special-JSON-And-Xml/). One of my favorites.
The only addition I'd add is when writing APIs add the summary comments. Some times what's cleat to those of us writing the API means nothing to the person consuming it. Otherwise I agree, your code should explain itself barring the rare hackery that needs a comment, etc
As a rule of thumb, yes. But you need to be careful when applying it. I've worked in a company that to the book "Clean Code" to the extreme, and let me tell you, it wasn't pretty. They ended up with a mess of small disjoint methods, most of them used only once (and my personal favorite: One line methods to calculate Boolean conditions). For me, when you use a method, you're hiding code, and I only have your word for what it does. A method should preform one *business* action. No more, no less. And that's the difference between naming the method for *how* it works and *what* it does. So yes, as a role of thumb you shouldn't have "and" in your method name. The question is, what are you naming them for?
Wow. That's a great idea. I've been suffering with DOM manipulation for years. This makes XML viable again. Does serialization introduce performance hits or caching issues?
You always have to be careful when applying rules of thumb, sure. I could argue about value of small methods - there are quite a few occasions, when I prefer one-liner private method, just because it's name describes action. And then those one-liners (and not only) are "collected" into one (or few) *business* methods, which are very easy to read. You usually arrive to it by starting your *business* method as something similar to pseudocode by using method calls as placeholders for *logical* things to build logic and then filling in actual actions in methods afterwards. I'm not a big fan of religious following of "Clean code", but this is one of ideas which (caveat - correctly applied) can greatly improve readability of code and reduce necessity for coments. P.S. I think method should perform *logical* and somewhat *atomic* action. The granularity depends on "layer" of class and/or method, but it's not necessarily mapping to *business* actions.
Yes, I still can't believe they made such a drastic change AFTER releasing RC1. It's maddening.
FAIL
"Comments are for when we fail to explain in code". Comments should be reserved for things that just don't make sense without specific knowledge about why the commented code exists. I also use comments to make large code blocks more readable. Today is used comments to break up 12 object initialisations with slightly different parameters. Every 3 cell objects is commented with // Cell 1 ... // Cell 2 etc.
I actually thought you just failed at contributing to a discussion by instead focusing on a minor detail and suggesting that everyone is an idiot because the week-old rename wasn't referenced. It's been called .NET 5 for two years. People will still call it this until the various _official_ websites and documentation is updated to the new change. And if you'd followed my post (where I try to contribute to the project by highlighting an important problem with new users), you would see that I have indeed taken the time to understand the state of the project. I was asking for opinions from those who are using RC2 bits now. But I'm unlikely to even consider your opinion because you come across so badly.
Just realized you made an account JUST for this. Now _I'm_ laughing? What the fuck dude? haha I'm just gonna assume you had a bad day, I'm always here if you need someone to shit on buddy. xxx
[removed]
Which version of visual studio are you using?
I definitely agree with this. If you're building software for a company, the extra time it takes other devs to figure out your unreadable code is costing the company money. Unless there's some other good reason for writing it that way, just make it readable. This also helps even if you're not working on a team; it'll make it so much easier to figure stuff out when you come back to maintain the project after 6 months of not looking at it.
No... no one dropped the ASP term. **ASP.NET Core 1.0** is built on top of **.NET Core 1.0** They are not the same thing.
Why spend time in something that will be obsolete in the near future? It's a waste of time. It's early preview software. Everyone spending time with this should know it, and should know that there are fluctuations and obsolete/incomplete information available. I'd rather have them concentrate on developing the product and writing documentations that won't be obsolete right away.
If a name is not self explanatory, it should be renamed.
Could not agree more.
Ohh, that makes more sense. My bad.
Never use cookie for sensitive data. Sessions is perfect for you. They save reference id to user data and not actual data. And it's super easy to work with them. Sessions is not the old way to do it, it's only way to do it. Of Course you can wrap up sessions in models and etc. for more easy use or direct mapping for example User model from Entity to session persistent data.
In general you *do* want to open new child windows for windows apps, it's the windows way. But if you really want the other style, for an easy solution, look into WPF Frames and Pages. For something a bit more complicated, check out http://www.codeproject.com/Articles/72724/Beginning-a-WPF-MVVM-application-Navigating-betwee
But that's the thing. On the server side I don't need/want to save anything. Page A, is really just shopping cart contents, B is personal info to be collected and immediately sent away. No database and jazz needed, that's just gonna get more complicated than it needs to be (unless I absolutely have to)
I was looking for where the commenting would actually appear and I did finally see it there. Is it only the &lt;summary&gt; tag that gets displayed or do I have it limited on my end?
Can I do something where the form on page A, posts back to itself then redirects to page/action B? Would that be: [httpget] Public iactionresult pageA() { return View(); } [httppost] Public iactionresult pageA() { //set cookie stuff and some kind of redirect to pageB }
RC2 is a shit show, it's more like alpha 10, they aren't even a state that can be called beta. I think they need a least 1 more year before it's even remotely "ready" and even then, the whole concept of ASP.NET Core is so terribad, you might as well just switch to NodeJS now which is actually battle tested. Basically prepare to spend more than half your time just dealing with mysterious bullshit issues with thousands of dependencies and trying to figure out what the fuck is wrong all the time with cryptic error messages... The whole exercise is mental masturbation of the worst kind.
No worries. I had to live through the same a few weeks back. But I have a client waiting for a finished product :D No complaining though. My fault for doing production grade work with a platform i have no experience with. If you cant get it to work hit me up and send me the whole solution, so i can try to get it to work. One little trick: if everything else fails you can take the azure connection string and hardcode it in startup.cs then you can single out the culprit. If that works move it to your config.json, and so on
Xamarin would probably be the better option tbh since it allows you to create native applications, whereas Phonegap and Cordova are basically just a 'webview' wrapper in the platform of choice around a normal web app.
I thought that the runtime was CoreCLR (because of the strings used in packages/samples), and that that blog referred to .NET as in the framework. I now understand that the rename is from ASP.NET 5 to ASP.NET Core 1.0, containing/requiring the .NET Core 1.0 and .NET Native 1.0 runtimes.
The thing to think about is that serialization isn't really any different to you serializing yourself... either you have to create the XML, or the serialisation library you're using does. There's little difference. I'd agree with /u/fightingpirates that JSON is generally preferred now, and that Json.Net from Newtonsoft is a great library - much faster than any other JSON or XML library I've used
Why do you think the overall concept of ASP.NET Core is so bad? 
You could start working on your actual app and business logic in 5 minutes with previous versions... Now you spend all your time trying to figure out why nothing is working. It's not even related to alpha/beta/RC - it's simply the way it is with Core - a lot of shit simply "doesn't work" and won't for years (if ever). It's almost comical to see no less than 3 package managers running and needing to install 15 packages (with THOUSANDS of dependencies) for a simple Hello World app. 
No problem. Just ignore the other guy. I think he is having a mental breakdown. 
That's odd. It should come with VS2015 community. All I can suggest is that you try reinstalling VS, or IIS Express separately. You can also just use full IIS, which requires you to install it as a windows feature (you can google how to do this Apps and Features),
Since we're being pedantic here, there's actually no such thing as ASP.Net 5. Stop being a self righteous prick, and people will like you more.
Stop. Do not pass Go. Do not collect £200. It has **NEVER** been called "Visual Studios". It's Visual Studio. When errors like that come up it's not usually worth the effort of trying to fix it, and just reinstall VS usually. Although that itself is never that reliable, and sometimes a Windows reinstall is better than spending hours searching for esoteric registry keys, fucking around with 50 config files, posting to MSDN connect or whatever it's called and getting a unhelpful generic "Did you try turning it on and off again" from someone who claims to be a MVP. Have you tried opening the solution in VS on another PC?
Having a model != using a database. Pass the model from one action to the next via RedirectToAction.
Goggle how to add a website to IIS and then google how to attach to IIS in your project configuration 
The IDE (devenv.exe) and the .NET build system (MSbuild.exe with csc.exe) are actually different beasts in how they handle compilation. What command line switches are you using? Have you tried multi-threaded builds? 
Sure, you can do it! For a simple school demo / homework, the best it to make each page (card / panel) as a separate user control. On the main page, you create the menu and a large content control. For every click on the menu, you set the content control's content to the page / card / panel you want to show. Done :)
Yeah, I know what it's called. My bad on typing it wrong, geez. Also I have already tried reinstalling but it has the same effect.
So apparently I left out a whole paragraph. Yes the file is missing and reinstalling visual studios did not put the file in. How do I get it to install the file? Google searches results in fixing .csproj but that isnt the issue for me.
Try waiting for a release. Also be aware that DNU is discontinued. In the future it will be NuGet3.
Thank you!
Are more files missing if the intellisense doesn't work? None of the shortcuts bring up intellisense either
use Roslyn directly
That suggests your install is broken for some reason(s). I would suggest uninstall first and then do a clean install.
Thanks, I'll look into that.
Thanks for the pointers.
You know, I may well go JSON now that I think about it.
For me it is a Server 2012 R2 because i need the possibility of a domain controller
I'll look in, I actually rarely need to worry about the building side of things 
Thank you, I'll look into it
Awesome, thank you
You can also write JSON and then convert it to a VB or C# class. [JSON Utils](http://jsonutils.com/)
Interesting, sadly we can't wait for release as there isn't a release. It's a very odd use case, it's for education. So the project we're compiling isn't important it's accepting user provided code, and getting unit test results. (education is weird sometimes)
I tried (and not sure what exactly you instructed me to do) but that doesn't sound like what I'm trying to do (ASP.NET Web Application, Web Application, MVC, C#, )
I use Rx extensively in production across a number of products. It's a mature product so there's not a huge amount to do - but that doesn't mean it's dead. The .NET version is still being actively worked on - check out their Github repo. Also, Rx is getting a lot of traction in the Java and JS communities.
&gt; ASP.NET5 runs on the CoreCLR 1.0 That has never been true. ASP.NET 5.0 ran on .NET Core 5. When .NET Core 5 was renamed .NET Core 1, ASP.NET 5.0 was also renamed. 
I'll let you know after I get hired.
Honestly I can't think of a reason to use it. TPL Dataflow is a much better fit for any workload I've ever seen. That said, there is no reason to avoid it if you have a legitimate use case. 
One app uses Rx to process a few different streams of data coming from some custom hardware sensors. A couple of others are UI-heavy desktop apps that use Rx extensively in their UI layers to make things quicker/easier to write and to reason about.
Thanks found it. Same directory structure but different drive where half of VS installation went to.
Thank you for the helpful info. 
Thank you for the answer; I'll be heading for the SQL Server Express now.
I want to get data from db, save it to a variable and use this for further calculations. I'm also wondering if I could extend User.Identity and save there more data about currently logged in user
Do you have a db installed? There are several ways to get the data you're looking for but do you have it to begin with? edit: http://stackoverflow.com/questions/20925822/asp-mvc5-identity-how-to-get-current-applicationuser
I've had to solve a problem very similar to this. We have the same data stored locally to a web application and remotely on a client's system. It's not very clear from your post whether or not you're trying to create an abstraction over data access or an abstraction over the services themselves. The way I'd solve this is to first identify the layers you want or need. For a desktop application, you often only need two layers (UI and Services) because there's a single point of data access for any given service. To make a two-layer system like this more testable, you'll often see the data access abstraction dependency injected into the services. If you have multiple data sources with similar APIs for retrieving and modifying data, it can make sense to logically layer your data access below a services layer. Next, you'll need to decide how to contextually resolve the types needed for your services. The simplest approach would be to use dependency injection. In this scenario, your desktop application would construct the data object, then construct the service object by passing the data object into its constructor. There are obviously issues with this approach. I'd say the next option is to use a service locator. You'll see mixed feelings about service locators-- some people love them, some people consider them an anti-pattern. If you're not familiar with the pattern, check out [MSDN](https://msdn.microsoft.com/en-us/library/ff648968.aspx) and be sure to follow the related links to Fowler's site. If your service locator implementation is the only dependency injected into your service, your service can then request any types it may need. This makes your services more testable when you mock calls to the service locator. The last option I'd say is Inversion of Control (IoC) using a container like Unity or Castle Windsor. The reason I think this would be the last option is because it can be a difficult concept for some mid-level developers to pick up. It involves configuring which types get constructed whenever a given interface (or interface + options) is requested from the container (the container is essentially a service locator). Almost every popular IoC framework supports both runtime and XML-based configuration of type registrations. However, since the container is working some magic through reflection, it often enforces conventions you may not otherwise choose and always requires discipline. With little or no unit testing in place there's an indication that you're working on both convention and discipline. From there, I would stick closely to SOLID principles. I've found that if I write code not for myself today but for myself 6 months from now, I usually get better and more testable code. Two books I'd recommend are [Adaptive Code via C#](http://www.amazon.com/Adaptive-Code-via-principles-Developer/dp/0735683204) and [Microsoft .NET - Architecting Applications for the Enterprise](http://www.amazon.com/Microsoft-NET-Architecting-Applications-Enterprise/dp/0735685355/).
I have only ever seen it removed from production code due to performance problems.
If you are using the example project it should have generated an ApplicationDbContext class for you along with an ApplicationUser. Under the ApplicationUser class there should be a method called GenerateUserIdentityAsync. This method is where you can add custom claims to a user. public class ApplicationUser : IdentityUser { public string CustomColumn { get; set; } public async Task&lt;ClaimsIdentity&gt; GenerateUserIdentityAsync(UserManager&lt;ApplicationUser&gt; manager) { // Note the authenticationType must match the one defined in CookieAuthenticationOptions.AuthenticationType var userIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); // Add custom user claims here userIdentity.AddClaim(new Claim("CustomClaim", this.CustomColumn)); return userIdentity; } } You can then access the custom claims on the User.Identity object by casting the User.Identity to ClaimsIdentity. You wont need to pull the info from the db because the UserManager should have retreived it for you.
Thanks for the detailed reply, I'll definitely have to get copies of those books and go through them! Ever since I was introduced to this code base I have always had this feeling that the data access (as an overall feature) is just very poor. It seems very bad to have a class that consists solely of about 30-40 static methods (which I know are terrible for testing) just to send and retrieve data from our main application (this main application is not .net based by the way - it's built in something called OmnisStudio). I also quite dislike that we have three different methods of getting data, so I guess my goal would be to create some form of encapsulation around all three types of access to provide a common access point. It definitely seems that the service locator idea is something that would satisfy this goal.
You have to extend the ApplicationUser model. Inherit from the base class in your class. Then everything that is using the User Manager must be using your inherited model. 
I've been playing with Reactiveui for a couple of side-projects - love it. 
To expand, Visual Studio installs the tools required to connect to an instance of Sql server but doesn't install an instance directly. 
It's kind of unusual to declare your models inside your controller. Typically, they're in their own files, but it shouldn't cause a problem. Possible problems: * Your original inputs don't have names * Your model property names don't match your original input names * Your controller method doesn't have a matching parameter for your model * The model accepted by the controller action isn't used * The second view doesn't have a model * The second model (if there is one) isn't passed to View() in the controller action * The second view doesn't use any model properties * The second view has inputs with names that don't match the model properties I could probably give you a list of over a 100 possibilities, but there's really no way for me to know what the problem is.
Sorry, I meant a new instance of that model. It lives in the "Models" folder. The model contains 2 classes that are supposed to have their values filled by 2 forms. Cart details collected, then payment details are collected. When I get to the summary page (third action) only the payment data has made it EDIT: Wait, controller variables don't hold data between requests?
Thanks for the advice. I have installed HeidiSQL already, and that seems to work quite well (at least for now). But I am familiar with MS SQL Server Management Studio and that's where I'll go, if Heidi turns out not to be up to it.
Why aren't you using the models though? Is there a reason for not using models, because that's what models are exactly for. A simple model binder and a ViewModel will get you exactly what you want. 
This is a port of a nice python animation library: http://zulko.github.io/blog/2014/09/20/vector-animations-with-python/ This is an example of what can be done: https://www.youtube.com/watch?v=Q_a-STbdDYU 
I did something kinda similar. It uses WPF to render animations. https://github.com/pauldotknopf/WPFAnimationEncoding [Here](https://youtu.be/N75Znma3zq0) is an example rendering of it. I can imagine a nice video animation editor, entirely in WPF. Then, on "export", encodes each WPF frame to a video/gif/etc.
I have this class in project indeed. Can I declare there some objects that are already in database? I mean: what if I wanted to add to User.Identity variable that contains user ID? Do I need to simply add new variable called "ID" to ApplicationUser class and it will cointain this ID from database, or do I need to do something more? And what is GenerateUserIdentityAsync? When is it called? Is it called at all, or maybe I have to call it somewhere myself?
Thanks for the warning, but I don't plan on using MySQL. I have now installed SQL Server Express, and presumably that will work quite well.
My problem with VS is that it's windows only, and I have very little interest in working on a windows machine. 
I'd recommend using a Windows VM and visual studio. The Linux/OSX development platform is improving considerably, but is still light-years behind VS on Windows. 
Thanks a bunch.
I don't think it's quite ready for prime time, as it doesn't have all of the useful features with deployment from mainline VS, but you should definitely toy with it. Code gets better all the time and I think it's really only a matter of time before they get as serious as the rest of VS.
So I have to create new variable in ApplicationUser class? What if I want to use already existing data? Like ID?
I've been trying to get some mvc4 site running and failed miserably, I guess I'll keep researching on web api, we have some projects that we may use on linux
Thx for the good work... I'll check back with feedback when i got the new version on a spin... i don't work on the weekend
I've actually just grabbed a Windows partition and cloned it to my External HDD. It runs fine. A little slow to boot. If it starts sucking, I'll get an External SSD. But for now this will be fine.
Since you are inheriting from IdentityUser that information will still be available by the UserManager
FYI all of the image links are broken.
I work on .NET for Linux (Mono) full time. I have left .NET Core alone for the time being because I found it is a moving target. But here are my observations from working with Mono: 1. Mono code runs very well. Performance is similar to what you'll get on Windows but with the added benefit of using an OS better suited to servers. 2. ASP.NET also runs pretty well and I have production code using both Web Forms and MVC. I've had no performance issues or maintenance issues so far but it's a bit more involved setting things up as opposed to doing stuff with IIS on Windows. Here's a tutorial I wrote for setting up a production site on Linux ... http://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html 3. Most of the .NET framework works on Linux but some libraries don't and you'll need to be aware of this. If you take an existing application and try and run it on Linux you may hit problems. For example (and correct me if I'm wrong) async doesn't work with ASP.NET on Mono. 4. Tutorials and help is thin on the ground and this has been my biggest pain point. Part of the reason I've been putting things on my blog is because there simply isn't the info out there. 5. The Mono REPL is great and beats anything on Windows. I love it and use it all the time and it's come in super handy a couple of times changing scripts on production - I can just open a terminal and immediately be coding in VIM with syntax highlighting and code completion. This is definitely (at least for me) an advantage over Windows / VS. 6. Xamarin Studio (and to some extent Mono) is a second class citizen in .NET land. Xamarin are focused on making the Visual Studio experience great for their users and are less interested in their users who aren't using Windows. You could of course work with Visual Studio and deploy to Linux but I'd advise against this (at least with Mono) as I tried it and it didn't work out for me. Xamarin Studio is a competent IDE but it lacks polish and the debugger isn't in the same league as VS. Be prepared for this. I'm really hoping JetBrains Rider - their new C# IDE - fills this gap. I hope this helps. I also hope you can join us (few) .NET guys on Linux - we need to build the community :-)
If it's any help here's a tutorial I wrote for setting up an ASP.NET site for production on Linux http://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html
This is a great tool, I just wish it worked on mono/*nix too.
Thank you for the input, one of the other Devs figured out a way to bring the build time way down. Still not sure the details, he said "monobuild" helped, which as far as I can tell is not a thing, that said... He is getting results so... (haven't had a chance to actually have him show me what he's done) 
Ok now i tested it... Great update! I'll ditch BareTail now. What is great: * dark mode * open files from command line * performance * layout * search speed Feature Requests (some are hightly unrelevant and nitpicky): * Option to Highlight the whole line if a keyword is found * Option to save Searches/Hightlights as Profiles and apply them automatically to certain fileendings * e.g. i call my webserver log foo.machine4log open it and have automatically all login request log lines highlighted red * tone down the green title bar in dark mode * donate link? :-) * margins could be a little smaller everywhere, * especially the tab bar feels too fat, i am used to vs2015 tab sizes * also the space between lines feels a little to wastefull * the row that contains the search bar could have a slightly differnt background color to separate it from the actual log thx for this awesome tool!
I believe the connection string is malformed. provider = "Provider=Microsoft.Jet.OLEDB.4.0;Data Source=C:\Users\Krishnan\Documents\MoviesNet.mdb" dataFile = "C:\Users\Krishnan\Documents\MoviesNet.mdb" connString = provider &amp; dataFile `connString` will be `"Provider=Microsoft.Jet.OLEDB.4.0;Data Source=C:\Users\Krishnan\Documents\MoviesNet.mdbC:\Users\Krishnan\Documents\MoviesNet.mdb"`. That doesn't make a lot of sense. You should probably write something like this: provider = "Provider=Microsoft.Jet.OLEDB.4.0;Data Source=" dataFile = "C:\Users\Krishnan\Documents\MoviesNet.mdb" connString = provider &amp; dataFile By the way, any reason why you're using VS 2008? There is a free Visual Studio 2015 Community Edition.
Thanks for the positive feedback and well considerd suggestions. I will bear them in mind. You will be pleased to know that saving more settings is already a work in progress. I also have been thinking about a highlight entire row option so now you have asked, I will do it.
I agree that it would be good to get it working in mono. This has already been requested. However what has held me back is I love the material design look and feel which is based achieved using a toolkit which is only available for windows. In this context, to get the same look and feel in mono amp would be a large effort. I am also keeping an eye on perspex (OS framework) and if that comes good maybe base a cross platform version on that. 
There's nothing out there. Every book you'll read is Windows centric and centred around Visual Studio, it's very frustrating. I was going to do a tutorial on using Xamarin Studio but concentrated on putting something out there showing how to deploy a website as I found this was the hardest thing to do. Getting an app running on OSX I found pretty straightforward but you've now incentivised me to a tutorial on the getting started part :-)
The [ASP.NET MVC site](http://www.asp.net/mvc) is the place to look
I would love it if this could somehow connect to an ELK stack. But I imagine that'd be too much for this tool.
I was going to upvote in support of this comment, but your username is rubbish. :P Seriously, the .NET site has tutorials on MVC, API, EF, Identity (and more). Its an excellent starting point for any developer new to .NET or MVC/WebApi.
That would be great. Honestly asp.net mvc seems pleasantly simple and I'll get along just fine. It's just that u learn much quicker with a solid introductory tutorial and the ones I've found are all heavily focused on tools and not the framework. 
[A possible alternative](http://www.codeproject.com/Articles/848111/Multi-Tenancy-System-With-Separate-Databases-in-MV)
Have you looked into aspect oriented programming like query interceptors? I googled around and found this article: http://xabikos.com/multitenant/application%20design/software%20as%20a%20service/2014/11/18/create-a-multitenant-application-with-entity-framework-code-first---part-2.html Basically the author creates a interceptor that adds the current tenant context to the query where clause.
If you're using EF you can try and implement [Dynamic Filters](https://www.nuget.org/packages/EntityFramework.DynamicFilters/) by adding a TenantId to all your entities and have your queries automatically apply a filter to the query with current TenantId stored in the Identity session.
I used monodevelop with the default repos from my ubuntu 15.10 
Xamarin Studio is the same as Monodevelop but I'm not sure if the version with Ubuntu repos are a behind (although in my experience the packages are usually pretty up to date).
Oh wow, they don't even minify their CSS, let alone bundle it.
Is it really that good? I checked it out a few months ago and it seemed more like a Sublime Text alternative. I'll give it a shot. Thanks
THERE AS SO MANY QUESTIONS THAT WE NEED ANSWERED! WE CANNOT HELP!
Find the multi tenancy series of blog posts of Ayende, which are great.
The original post simply provides no information at all. It's as if you are asking: &gt; Something doesn't work. What am I doing wrong?
Querying the database repeatedly is not a problem, until it becomes a problem.