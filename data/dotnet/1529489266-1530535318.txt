You can read series CQRS of this guy for a simpler version of eShopContainer. [http://foreverframe.net/tag/cqrs/](http://foreverframe.net/tag/cqrs/)
[Routing for MVC](http://johnatten.com/2013/08/21/customizing-routes-in-asp-net-mvc/)
Well first one is reason why I'm learning c#
Things like the UpdateSqlGenerator might produce better SQL at runtime than without it.
Reason 7 hits home for me, been doing Java for years but recently picked up C# for reason 1 and found out that it's a fuckload easier to get shit done in C#. If only I'd known that earlier!
I'm surprised that xamarin is absent, developing cross platform apps is awesome 
Agreed! That's why I mentioned Xamarin in the precursor to this article. You can find it [here](https://fluxmatix.com/en/blog/10-reasons-why-csharp-is-alive-and-kicking-in-2018) :-)
Sorry didn't read the previous article 
No worries my friend!
[it's a principle of 12-factor apps](https://12factor.net/logs) to do exactly this.
Why do we need a list like this? Who is saying C# is dead and why do we care about what they think?
Perception of C# in the developer community outside of C# developers is worse than it should be. It was labeled for most of it's life as a Windows specific tech (even though that wasn't true even before .NET Core, because of Mono, Xamarin, and Unity). Developers were calling it dead when Windows desktop application development started to struggle, and Microsoft's web development technologies were starting to show age. Even now the .NET ecosystem is still behind in open source machine learning and distributed computing libraries and systems. Anyone paying attention can see that has gotten much better for .NET in the last year or two, but it still has ground to catch up on and public perception from those that aren't already using .NET lags (in some cases it has significant lag). We care what they think because most of those developers don't realize that C# + .NET Core is probably one of the best cross platform open source stacks, and increasing developer community size is the easiest way to maintain a high level of maintenance and velocity for an open source platform (it keeps giving us good patches and modern libraries).
i have another reason for you. C# can be made unsafe so that it can work with c++ code. 
I was thinking the same thing. C++ isn't "going anywhere", why would c#? Microsoft is heavily devoted to it.
Nail. Hamer. Head.
I believe it doesn't need to be unsafe. P/Invoke power to the rescue! 
it does if oyu are passing pointers or changing how a struct is laid out. .net lays out a struct in memory that is the most efficient but in c/c++ you have the ability to change that. I was working with a library that required the struct to me sequential. needed to go unsafe.
At least in Lithuania, I wish that c# was more used outside of enterprise environments... Loving the language, tooling, etc, but the only choices in the market are 9-5 office rat jobs
If people actually dont know if c# is worth learning I would love to see what alternatives they would propose.
I believe the marshalling does have support for common pointer scenarios so that you do not need to use unsafe code. Some things can be passed as `IntPtr`, some pointers can be `ref` or `out` parameters instead, and some things can be marshalled as arrays or strings as far as I know.
C# is far from dead, but it *is* beginning to show its age in a few areas when compared to other modern languages (Swift, Kotlin, Rust, etc.)
Javascript based platforms will kill it.
Yes stop writing to files. Working in the corporate world, I found it way easier to consider the web server as read-only. With no ability to write any data locally, we passed our internal pen testing every time. Remote syslog server is your friend here. 
You can change struct layout kind, size, packing and charset through an attribute and even use FieldOffset to mimick a union. You can also use IntPtr which is a safe alternative if you have to deal with int*
I had a similar problem but with the error 405 and hosted in an IIS. WebDAVModule which seems to set PUT and DELETE request methods disabled by default. If this is your problem you can try the workaround and add the following in the web.config: &lt;system.webServer&gt; &lt;modules runAllManagedModulesForAllRequests="false"&gt; &lt;remove name="WebDAVModule" /&gt; &lt;/modules&gt; &lt;/system.webServer&gt; 
What is wrong with working from 9 to 5?
No "Avoiding Oracle?" Yes, there are other popular languages besides Java, but C# and Java are the most obvious and likely direct competitors, and I'd argue Oracle are a boat anchor around Java's neck. - Suing Google for Java usage - [Auditing Java users](http://www.theregister.co.uk/2016/12/16/oracle_targets_java_users_non_compliance/) for accidental usage of "commercial" features - Incompatibilities between the OpenJDK and Oracle's Java runtime(s) - Trademark issues ("Java" usage e.g. on training sites/videos, makes it harder to find resources). Back when Sun owned Java, Java was seen quite correctly as "Open Source with commercial extensions." These days I'd describe it as "Open Source with expensive Oracle booby traps." Microsoft are far from perfect, but their management of the .Net/Open Source ecosystem has been up there with how Sun did it back in Java heyday. 
If you want something to compete with Swift and Kotlin, why wouldn't you just use F#?
Explain why please. Ps:that's not a provocation, I'm just curious 
Well actually there is nothing wrong with that. What I have in mind is that the 9-5 is an umbrella term for all the bloated enterprise crap - you're just a resource and nothing else more. Maybe it is just my experience as I have only worked in companies that outsource the developers to the companies located in the Europe. I was unable to find any job ads from small to medium companies that target EU market. Local sector IT companies are lagging behind for at least 20 years and they only participate in public procurements that are ridden with corruption. 
We use console logging and awslogs to cloudwatch 
Helping my friend with his Java assignment for uni with only C# experience was painful. Had to read a file line by line. I had to pass a stream to another stream because for some reason File.ReadAllLines() doesn’t do what it FUCKING SAYS IT DOES. 
I disagree. C# has always jumped to new grounds and always made a safe landing. Vala, Swift and kotlin are very similar to C# and bring nothing new but different names and an uglier syntax. C# always grew through watching cool useful stuff that other programming languages did and packing it in a well designed manner in one language. e.g kotlin has data classes and haskell has records so now we will have records in C#8. Another major example is F# which MS is treating like a testing ground for C# functional features. This always keeps C# ahead of all other languages by steps and miles. C#'s age hint maturity and lively development and not lack of 'new' features which C# is always adding.
&gt; it is beginning to show its age in a few areas when compared to other modern languages (Swift, Kotlin, Rust, etc.) That's a rather generic argument without any supporting evidence provided. Can you elaborate?
Because Blazor based platforms will kill JavaScript :^)
I started on C# before learning Java. For me the biggest annoyances with Java is having to write a method for every getter and setter. I was so thankful when I was able to go back to using C#.
*Close the project. *Remove references to ninject from your packages.config, WebApplication.csproj, your packages folder, and possibly your $(HOME)/.nuget/Packages folder. *Re-open the project and add the reference to ninject again.
Having to take Java classes for CS felt like stepping back in time about 100 years. It was painful. 
Yikes!
C# needs to die and make way for glorious F# ... Can F# please be THE .NET language now MS? :(
He already did that. 
Use something like NLog (probably log4net and others too) that lets you easily change your logging target(s) via configuration. Personally I like having log files when running locally, they're just easier to deal with. Deployments to any real environment should be sending logs to a db or something other than files.
Unlikely, that's like saying C# will kill Java.
No, because most people prefer OOP to functional. 
Nothing wrong with F#, but lol no. 
It was that easy for me until the site tried to connect to mysql. The third party provider blows up on EF 2.1, so I had to remove all the meta packages and figure out what to add back.
Something something, logging library problems :)
[StructLayout](https://msdn.microsoft.com/en-us/library/system.runtime.interopservices.structlayoutattribute(v=vs.110).aspx)
[System.Runtime.InteropServices.StructLayoutAttribute](https://msdn.microsoft.com/en-us/library/system.runtime.interopservices.structlayoutattribute(v=vs.110).aspx)
Grammar my friend! Real life isn’t a LINQ statement ;
If by "cross-platform" you mean "Windows, macOS, Ubuntu, Fedora or RHEL", then yes. On other Linux distros you're on your own and you'll be futzing around with Mono and XBuild and dotnetcore-runtime and _wtf is this shit?_.
For circlejerking, mostly. 
"I have a problem with my custom logging framework. Should I use log4net to log in my logging framework"-kinda problem.
I prefer to use languages with an elegant design. F# has to bend over backwards too much to be ML with some tacked on OO functionality.
The most obvious one is that C# will never have optionals as first class citizens. There is no CLR support so it’s always going to be restricted to warnings rather than errors and runtime checks will remain necessary for public API’s. It’s also behind when it comes to modern features like sum types, traits, and the generic type system is limited.
Keeps C# ahead? It is already behind today when it comes to language features. The C# team’s capability of extending the language without making it a bloated mess has so far been impressive, but it’s reaching the limits of its type system.
&gt; Kotlin I would love to see a K#
Before anyone wastes their time: [OP's entire shtick](https://www.reddit.com/user/lizaard64/submitted/) is to make reddit posts about his own weird ass language/framework, including convoluted and homegrown "security" measures which inevitably contain numerous massive flaws. [When these obvious flaws are pointed out to him, OP refuses to acknowledge them and argues about them instead.](https://www.reddit.com/r/programming/comments/8oove7/please_hack_my_phosphorus_five_server_i_have/) He isn't merely ignorant about security and cryptography, but actively hostile to anything that could be considered best practice or even just common sense.
You are leaving out Debian, SLES, openSUSE, Alpine, and Raspian with ARM support. To use the phrase 'cross platform' only requires support for two platforms and Microsoft publishes downloads for more than 15. No one claims it supports all Linux variants, nor does it need to.
F# is a bit different than Kotlin/Swift. It's a nice language though. But more comparable to Elixir. Great type system!
&gt; Swift and kotlin are very similar to C# and bring nothing new but different names and an uglier syntax. You're insane. Null-safety would be reason alone. Pattern matching is top-notch. Ever tried the enums in Swift? Spectacular. They can hold different types per value, so you can do something like this: enum PlayerType { case footSolder case driver(vehicle: Vehicle) case pilot(plane: Plane) } How would you do this in C#? Optional values in a function call? One base type and then sniff and cast to the real type you expect? And what if you add a different type of player? The Swift compiler _will_ warn you if you forget to implement it somewhere. Powerful.
Before anyone actually believes what this guys says, every single concern pointed out in my code has now been implemented, and is due to be released in the next version ...
Xaml has been treated like a second class citizen for a long time and is stagnating. Javascript frameworks are exploding in popularity and they are also starting to tackle the app problem. That said im just a filthy casual, ive mostly dodged the app life.
&gt; .NET Core 2.0 was released on August 14, 2017. As a non-LTS release, it is supported for 3 months after the next release. .NET Core 2.1 was released on May 30th, 2018. As a result, .NET Core 2.0 will be supported until September 1, 2018
Indeed the type system is not as strong as that of a functional programing language like haskell but the functional part of C# was never rudimentary. Even so, it seems the current focus is to fix that by adding functional-ish features like no null by default, more pattern matching in C#7 and typeclasses (a C#8 candidate). C# not having these from the start is because it was designed as an OO language pandering for LOB where functional programming languages don't shine.
I know, and they're doing good work and it's a great language, but I don't expect it to remain my language of choice in five years.
React Native sure exploded out of airbnb. Xaml is everywhere in WPF, UWP and Xamarin. It is not going anywhere.
How do you setup a default page for asp. Net? Github loaded the project as a project. Not as a website. 
`async`/`await`, for one.
You know what, I was actually tempted to defend myself against this, but thought. Naah, why give you the pleasure ...
Google is already leaving Java... Kotlin is coming to replace it, at least in their sector, Android !
C# is that language in that space. It’s constantly evolving new features and is one of the most advanced and well designed OO/imperative languages available. It’s not a new kid on the block, so it doesn’t seem exciting, but it’s arguably as advanced and feature full as all of the languages you mentioned (though Rust really sits in a separate area).
Kotlin is nothing more than a shitton of sugar over Java. Specifically looked into it today - did not found anything what could drive me from c#. BTW it really looks like typescript on java
Is it a packagereference in the csproj or do you use the packages.config?
start here: https://stackoverflow.com/questions/102558/biggest-advantage-to-using-asp-net-mvc-vs-web-forms
Oddly enough, .Net being a MS technology is still a common argument for not using it. Some how the threat of MS suing is worse than actually getting served with one by Oracle. AFAIK MS hasn't sued anyone related to .Net.
not true https://www.nuget.org/packages/Microsoft.Bcl.Async
packages.config
hi. Which nuget package do i need to install to get IdentityUser in the Domain class library?
I'll give it a try, it's not just ninject. This is happening for a lot of our nuget packages. That was just an example to illustrate my problem.
Probably a dumb question, but is there a reason why the CLR can’t be update to support optionals? Is it just too difficult to do, or actually impossible?
I just tried using the tar.gz version of .Net Core SDK on a Gentoo machine. And at least hello world works fine.
It was released in May, but it's still only available in Preview currently. Does this mean that come September 1st, we can expect 2.1 to be available outside of Preview?
Adding that doesn't need a completely new language though and [it is being considered](https://github.com/dotnet/csharplang/issues/113) (though likely won't be in C# 8.0).
The preview releases were earlier in the year. It was RTM 5/30/2018. [https://blogs.msdn.microsoft.com/dotnet/2018/05/30/announcing-net-core-2-1/](https://blogs.msdn.microsoft.com/dotnet/2018/05/30/announcing-net-core-2-1/)
No, it‘s already RTM.
It's not a dumb question! I think there is some chance that the CLR will be updated at some point in the future to support optionals natively, but the ship will have sailed for C#. Turning warnings into errors is a breaking change and MS isn't going to do that unless it's unavoidable.
Huh, looks like I missed that. Thank you! It can feel like I'm try to grab hold of the wall in a water slide sometimes with all these updates haha
It would be great to see it in C#. It really makes your code a lot more clear in a lot of places. Also nice for callbacks where you have a Result type that's either Error or Value, just switch on that result instead of two callbacks or ambiguous values that might be set or not.
Okay, I see where I got confused. Clicking though on that link has another link to download the SDK: [https://www.microsoft.com/net/download/dotnet-core/sdk-2.1.300](https://www.microsoft.com/net/download/dotnet-core/sdk-2.1.300) Which has a message at the top saying: &gt;To use .NET Core 2.1 with Visual Studio, you'll need Visual Studio 2017 15.7 Preview 1 or newer. Make sure you've got the [latest Visual Studio 2017 Preview](https://www.visualstudio.com/vs/preview/). But going to the .NET Core download page: [https://www.microsoft.com/net/download/dotnet-core](https://www.microsoft.com/net/download/dotnet-core) makes no mention of Visual Studio Preview and installs alongside my existing VS 2017 just fine Thank you for pointing out that it's in RTM though!
You could just fix your hintpath in the csproj then. Check on another machine if that fixes it. Although clearing nuget locals could be useful too.
 From site: &gt;[.NET Core 2.0](https://blogs.msdn.microsoft.com/dotnet/2017/08/14/announcing-net-core-2-0/) was released on August 14, 2017. As a non-LTS release, it is supported for 3 months after the next release. [.NET Core 2.1](https://blogs.msdn.microsoft.com/dotnet/2018/05/30/announcing-net-core-2-1/) was released on May 30th, 2018. As a result, .NET Core 2.0 will be supported until September 1, 2018. &gt; &gt;After that time, .NET Core patch updates will no longer include updated packages of container images for .NET Core 2.0. You should plan your upgrade from .NET Core 2.0 to 2.1 now.
I didn't know 2.1 was going to become an LTS version. That's good to know. All my apps are 2.0. I guess I should upgrade.
Somehow I feel people will be confusing the Core version number’s EOL with .NET Standard 2.0
There are some great improvements in 2.1, like way faster builds. It's easy to upgrade and super worth it.
2.1 is not LTS. Neither is 2.0. https://www.microsoft.com/net/support/policy
From the comments at the bottom of the link: ".NET Core 2.1 will be a long-term support (LTS) release. This means that it is supported for three years. We recommend that you make .NET Core 2.1 your new standard for .NET Core development." They haven't made it LTS just yet and are waiting for a patch or two before slapping that label on it.
Ah thanks. 
AWS better add lambda support. 
Your link says this for 2.1 support &gt;May 30, 2021 or 3 months after next Current release or 12 months after next LTS release, whichever is shorter.
RTM = GA
God damnit
The builds improvements are part of the sdk, you can get those even if you are using the 2.0 runtime.
*Beep boop* I am a bot that sniffs out spammers, and this smells like spam. At least 100.0% out of the 4 submissions from /u/Leonardorxs appear to be for Udemy affiliate links. Don't let spam take over Reddit! Throw it out! *Bee bop*
Did anyone else have a hell of a time upgrading from 2.0 to 2.1? I was struggling with it all day today. I must've had some weird configuration setup in my solution, but I kept running into dependency mismatches, version conflicts, problems with packages not upgrading to 2.1.1, etc. Took forever to get it all sorted.
Maybe when someone changed the target frameworks from 2.1rc or 2.1preview to 2.1, they screwed up some other settings in the project files. Do you have any npm or JavaScript front-end CLI commands or other MSBuild commands in your project files? Are you certain you upgraded your installed .NET Core SDK to 2.1? Are you seeing the slowed builds in VS, or on your build server?
Had this, luckily just started the project. Created new API from scratch and moved the files from the old project.
I posted this on SO, but will also post here: The 2.1.1 (preview) SDK is available on their GitHub. You can download directly here: https://dotnetfeed.blob.core.windows.net/orchestrated-release-2-1/20180605-09/final/assets/Sdk/2.1.301-preview-008906/dotnet-sdk-2.1.301-win-x64.exe See: https://github.com/dotnet/versions/tree/7a833dddfddc27f2074b755b94234a25b9757637/build-info/dotnet/product/cli/release/2.1 We are still waiting on the official SDK... Edit: If you are having trouble building, add the following to your .csproj &lt;PropertyGroup&gt; &lt;TargetLatestAspNetCoreRuntimePatch&gt;true&lt;/TargetLatestAspNetCoreRuntimePatch&gt; &lt;/PropertyGroup&gt;
I set up a docker oneclickapp on DigitalOcean in may. Worked great, simple deployment. Is that no longer an option? 
I am just looking into it now and learning about docker
https://www.microsoft.com/net/download/linux &lt;-- is this what you need?
Isn't the solution to add .ConfigureAwait(false) on the GetStringAsync ?
That's not how it works. Both the "message pump" and `Task` are just normal piece of C# code, they don't have special world stopping/changing powers (such as the GC does). The only way to "unblock" a synchronous call like `Task.Result` is to return from the method call, which will then continue executing the method it was called from with all its side effects, return to the parent caller, and repeat that all the way up the call chain until control comes back to the message pump which caused the button click event. There is no way to gracefully jump into the waiting thread, pause the `Task.Result` call, jump to the message pump and let it do its thing, and then jump back in the middle of `Button1_Click` and resume the `Task.Result` call as if nothing happened. It's just not something that is possible in synchronous code. This is exactly what `async`/`await` are made for. If you want this kind of behavior where a thread is unblocked while waiting for the result of an operation, then use `await` instead of `Result`.
How would that work? You can't read result until the Task has completed, so it is by its very nature blocking. Now they could have the compiler see a call to Result and turn it into an async call using await, but a) that would break all legacy code where people expect blocking and b) it's trivial to use await yourself. Your code doesn't work as listed as you're returning a JObject from result and trying to assign that to a .Text member which is a string, so I don't know why you ? But you could just make Button1_Click async void and await the result of GetJsonAsync and then assign it to the textbox (after you've solved the returning of a JObject of course). 
I've written a blog post on how you can look to do this, if you like :) http://www.karam.io/2018/Deploying-a-NET-Core-application-to-a-5-digitalocean-droplet-using-dokku/
I had a lecture about it in Internet services at uni. I can send you the slides if you want to 😀 
It wouldn't be simply unblocking itself, because there is still a message in flight (the mouse click in your example) on the main message pump, and that message needs to complete before handling any other messages, because sequential consistency must be maintained. Your secondary message pump must not be allowed to run anything that touches or observes any state that relates to the currently in-flight message, because that would break all sorts of things. On the other hand, it *must* process all the Task completions that the Task (that the main message pump is blocked on) depends on. You can't actually satisfy these two constraints at the same time, as dependencies and interactions can be arbitrarily complex.
It's not really the case that you can't unblock `Task.Result`. It just waits on a wait handle. You could create a code path inside `Task.Result` that, if the wait handle is signalled and some specific flag is set, calls into a message pump instead of resuming normal operations. The problem is that would break all kinds of guarantees, not that you can't *do* it.
Historical reasons (Task was originally for 'asynchronous' code, not the async/await keywords). You should make your top level async void in this case, you might need .ConfigureAwait(false). In the general case, you can also use .GetAwaiter().GetResult() as the ultimate "Run this async code synchronously".
Look personally I loved WPF when it first came out. In comparison to winforms it was amazing... But Microsoft seem to be embracing the javascript frameworks while XAML has just stagnated over the last decade. I wish I was wrong.
It doesn't always "have to result in a deadlock" , that's just a side effect of calling it in the synchronisation context of a windows form application. In other applications like ASP.NET and Console, it's fine because the inner async task continuation is allowed to complete on a threadpool thread, because the synchronisation context is different. There are a few reasons why ConfigureAwait(false) isn't the default, and the main one is that this would cause your code to potentially hop threads upon continuation when you're not expecting it - yes there are memory barriers inserted by the compiler, but in a forms application this could cause cross thread calls to UI elements and break things. ConfigureAwait(false) should only be used if you know that you need it. I disagree with the sentiment that it should be used constantly and out of habbit.
So you want runtime to detect you are in a message pump and literally rewrite the entire calling stack to be async on the fly?
 public async void Button1_Click(...) { var text = await GetJsonAsync(...); textBox1.Text = text; }
Yes.
Microsoft.AspNetCore.All I think. It should be easier. But if you want, you can install only the ones needed: - Microsoft.AspNetCore.Identity - Microsoft.AspNetCore.Identity.EntityFrameworkCore - Microsoft.EntityFrameworkCore - Microsoft.NETCore.App I think you should be good with these 
Awesome! Thanks!
I did, but I think I upgraded too quickly, before some third-party things were ready. If I had waited a few days and had looked at the migration documentation right away to note the small tweaks to the host builder, it would have been painless for my project, I think.
I really wish they had built `ConfigureAwait` in at the language level instead of making it a very non descriptive method call. Something like await nocontext MyAsyncMethod(); await context MyAsyncMethod(); // nocontext implied by default await MyAsyncMethod(); It'll never happen because backwards compatibility, but I can dream. 
Youre either gonna have a to learn python, ruby ,or some other framework or move. Come to NYC if you think youre up for it. Plenty of C# jobs.
Async all the way or don't. Don't mix async and TPL code. // My "top-level" method. public async void Button1_Click(...) { textBox1.Text = await GetJsonAsync(...); } 
&gt; `.ConfigureAwait(false)` should only be used if you know that you need it. I disagree with the sentiment that it should be used constantly and out of habit . If you're writing async methods **that don't have any reason to care about `SynchronizationContext.Current`** (e.g., a general-purpose software library), then I advocate for either applying `.ConfigureAwait(false)` religiously on every `await` or using some helper to achieve the same effect. It is often redundant or irrelevant, but in the other cases, I believe it to be overwhelmingly more likely that `.ConfigureAwait(false)` will make things better than that it will make things worse, and in the cases where it makes things worse, the system should tend to fail more loudly and immediately to help you find your way to the correct solution than what happens when you use the default behavior when you should have done the other thing. These are opinions, of course, and they are based on my personal experience with a WPF application.
Ya, I think the default should probably have been false. 
&gt;It'll never happen because backwards compatibility, but I can dream. Actually, this is something that we can get really close to. If a syntax alternative is developed as optional (leaving the existing syntax around for non-`Task` awaitables and legacy code), a diagnostic could be written that fires whenever you use today's `await` on something where the new-style alternative is legal. Then existing code could work, and projects could opt into getting compile-time errors when they do the other thing. The main argument I can come up with for why this shouldn't happen is that it increases the complexity of an already complex part of the language, and for questionable benefit given that `CA2007` from [Microsoft.FxCop.Analyzers](https://www.nuget.org/packages/Microsoft.CodeAnalysis.FxCopAnalyzers/) offers most of the benefits with today's syntax.
Which message pump though? Maybe you could get access to the one in WinForms or WPF, but it wouldn't solve the same deadlock in ASP.NET.
&gt; In other applications like ASP.NET Oh no, it will happily deadlock ASP.NET as well. (Though not ASP.NET Core.)
You could also use `Task.Run` to ensure `GetStringAsync` is on a different thread.
That doesn't matter for what I was saying
&gt;Async all the way or don't. Don't mix async and TPL code. Im not sure I understand this. Task and Task&lt;T&gt; are TPL classes. 
Is there any whitespace in the username on the machines you tested? We have a bug [https://github.com/aspnet/Razor/issues/2406](https://github.com/aspnet/Razor/issues/2406) that affects those. The fix for that is already in and it's being considered for the next patch release. In the meantime, adding the property `&lt;UseRazorBuildServer&gt;false&lt;/UseRazorBuildServer&gt;` to your csproj should unblock you from the slow build times.
You are right, it's poorly worded and after all any async method awaits a Task&lt;T&gt;. I mean if it's async it should be from the topmost caller down to the last, and getting a .Result is usually a code smell. I was referring to [this](https://stackoverflow.com/questions/29808915/why-use-async-await-all-the-way-down#29809054)
Typescript integration isn’t all that stellar.
No. Or, at least, [not necessarily](https://blog.stephencleary.com/2012/07/dont-block-on-async-code.html). &gt; Using ConfigureAwait(false) to avoid deadlocks is a dangerous practice. You would have to use ConfigureAwait(false) for every await in the transitive closure of all methods called by the blocking code, including all third- and second-party code. Using ConfigureAwait(false) to avoid deadlock is at best just a hack). &gt; As the title of this post points out, the better solution is “Don’t block on async code”
&gt; It doesn't always "have to result in a deadlock" , that's just a side effect of calling it in the synchronisation context of a windows form application. In other applications like ASP.NET and Console, it's fine because the inner async task continuation is allowed to complete on a threadpool thread, because the synchronisation context is different. No, it's [certainly possible](https://blog.stephencleary.com/2012/07/dont-block-on-async-code.html) to get a deadlock in an ASP.NET application. &gt; This example is very similar; we have a library method that performs a REST call, only this time it’s used in an ASP.NET context (Web API in this case, but the same principles apply to any ASP.NET application): &gt; ... &gt; This code will also deadlock. For the same reason.
So it's a possible solution, but the await on top is the best way. In this case, the ConfigureAwait might be a decent idea aswell because the continuation doesn't need to be on the same synchronisation context.
The reason it deadlocks is you are "asking" the compiler to create a deadlock by using the .Result method. This issued has existed before .Net was around in the VB days with "DoEvents()". We just forgot the rules. You could do async programming before async/await existed, it was just more difficult and more boilerplate code. The rule with winforms is to never block the UI. I also wouldn't trust any Library code to configure my context in the correct way. Had an ActiveX component try to pop up an alert box running under IIS, never again. Code below is from " **Progress Reporting with UI Updates"** [https://msdn.microsoft.com/en-us/magazine/gg598924.aspx](https://msdn.microsoft.com/en-us/magazine/gg598924.aspx) public void Button1\_Click(...) { // This TaskScheduler captures SynchronizationContext.Current. TaskScheduler taskScheduler = TaskScheduler.FromCurrentSynchronizationContext(); // Start a new task (this uses the default TaskScheduler, // so it will run on a ThreadPool thread). Task.Factory.StartNew(() =&gt; { var jsonTask = GetJsonAsync(...); var result = jsonTask.Result; // We are running on a ThreadPool thread here. ; // Do some work. // Report progress to the UI. Task reportProgressTask = Task.Factory.StartNew(() =&gt; { // We are running on the UI thread here. textBox1.Text = result; }, CancellationToken.None, TaskCreationOptions.None, taskScheduler); reportProgressTask.Wait(); }); }
Do you have specific examples of what guarantees would be broken?
You’d just make one. It would be a specific code path inside of `Task.Result` which waits for a continuation for that synchronization context to be available. It has nothing to do with WinForms, WPF, ASP.NET, console apps, etc; it’s intended to be an agnostic bridge between synchronous and asynchronous code.
You’re getting hung up on the example. The desire is to have a bridge between synchronous and asynchronous code with no pitfalls: no chance of deadlock, and no need to sprinkle every callee with `.ConfigureAwait(false)`.
So it's a browser window, minus all controls, and still requires a webhost?
Events, message bus, reactive code. There are a ton of ways to do this. It sounds like you might want to spend some time studying design patterns.
OVH. 3.35$ for 1 vCore (2.4GHz), 2GB RAM, 20GB SSD...can't beat that.
Yeah I mean that is what I am trying to do... Thanks for the help?
You’re getting hung up on the example. I don’t care that this is what you "should" do; of course it is. I want to know why it HAS to be this way (aside from missing out on many of the benefits of async code). This whole mess makes it overly difficult to, for example, implement a library as async and use it from both sync and async code.
You have not provided enough information on what you are doing to provide a concrete pattern. This is why I suggested 3 actual technical features you could google for and find more on and then suggested reading more on patterns. I suggest digging into GoF patterns to start with and just start reading. FYI, Im already done helping you, you are they type that does not want help, you want a predone example and you dont even know what you are asking for. 
Yes I believe you can do this with Azure Event Grid to trigger a logic app or Azure Function. Event Grid should be able to detect the row being inserted into your DB and then trigger an Azure Function or Logic App to execute a method in your .NET code such as a RestAPI endpoint. https://azure.microsoft.com/en-us/services/event-grid/ 
The webhost is not required. The webhost is used to connect your asp.net website to the browser window. It serves the same purpose as Electron.NET (cross platform desktop apps using html+css+C#) but replaces electron with webview. Webview on osx/linux is based on WebKit, and on windows it uses IE/mshtml. This results in a much smaller binary. The runtime memory usage compared to Electron.NET is much smaller too. 
I’ve edited my post with some clarifications. &gt;Your secondary message pump must not be allowed to run anything that touches or observes any state that relates to the currently in-flight message, because that would break all sorts of things. The secondary pump wouldn’t be running literal Windows messages, it’s just waiting for the synchronization context to have a continuation ready to run.
But why? What if I’m implementing a library as async and want to use it from both async and legacy synchronous code?
You'd be overlaying a weird psuedo-synchronization context on top of the real one. My head hurts just thinking about all of the problems that would cause.
I agree, that making something both sync and async is hard - I don't think anyone would disagree. The answer you seek really is down to "You do have to do it this way, else you'll have the very same problems you are experiencing." There is a hacky workaround to forcefully run async as sync but it's not pretty. You store the current global synchronisation context in a temp variable, then replace it with a (custom) synchronous (i.e. single threaded) implementation, execute the "async" Task with it, then restore the previous global synchronisation context from the temp variable. 
What problems, specifically? This is what I haven’t been able to find any information on. I just don’t know what would break.
No? See my edits.
Start by studying the internals of the Win32 message pump. Then read everything you can about COM. That will give me three to six years to have the next reading assignment ready. 
If you are implementing it from scratch I would say, implement both synchronous and asynchronous methods. But that's just me.
Can you give an example of a failing scenario?
I wasn't joking. What you are asking for requires a deep understanding of the very core of Windows. It's not something that I'm qualified to explain. And given how arrogant I usually am on this forum, that's saying a lot.
But that means either duplicating your code or adding lots of boilerplate to your library that makes assumptions about your caller.
&gt;The answer you seek really is down to "You do have to do it this way, else you'll have the very same problems you are experiencing." But I want to know why! Why is it not possible for the framework to contain/hide whatever solution is necessary?
Ahh I was thinking of ASP.NET Core. Didn't know that ASP.NET Classic has a sync context for the request.
You can use a trigger (https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql?view=sql-server-2017) with a CLR user defined function (https://docs.microsoft.com/en-us/sql/relational-databases/clr-integration-database-objects-user-defined-functions/clr-user-defined-functions?view=sql-server-2017)
The issue only occurs when async code is awaited synchronously, in a synchronisation context in which awaiting things synchronously will block. Yes, adding `.ConfigureAwait(false)` to everything at the library level will fix this, but then again the consumer *shouldn't ever be blocking* when they have a synchronisation context that will cause this behavior in the first place. Using `ConfigureAwait(false)` to fix this is a kludge. The library *can* concern itself with this as a convenience to the user, but whether it should actually be the libraries concern is a more philosophical question. Ultimately, I would argue that it's not the libraries concern to dictate whether tasks it runs resume on the threadpool or not. If the user of the library desires this behavior, they should run the library's task with `SynchronizationContext.Current` set correctly. 
Another option is to use the SqlDepency class (https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqldependency.aspx) or may be change tracking (https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-2017) because you don't provide information about the context it's difficult to provide the 'right' choice.
I will take a look at both of these solutions. Basically I have some .NET code that currently handles printing of reports using the Crystal Reports runtime and I would like to access it through a REST API from mobile devices in my business. My thought was make a simple javascript web app that users can enter appropriate information in and have that app write a record to the database. The .NET code is a server software that runs 24/7 and handles scheduled reports currently. This was my attempt at making it also handle "triggered" reports. 
Indeed, but what happens if, upon starting your secondary pump, the continuation then starts calling UI operations. Normally you are required to do things like Control.Invoke so that different UI operations do not interleave and cause havoc. Here you are already on the UI thread, but you *are* interleaving separate operations beacuse you still have a half-processed Windows message on the main message pump. Who knows what kind of demons you will unleash. What happens if in the middle of the click event you were processing, you reorganize your Control hierarchy? 
&gt; that makes assumptions about your caller. That's what I thought when I read &gt; What if I’m implementing a library as async and want to use it from both async and legacy synchronous code? You either write it fully async and let the caller manage it if it wants to use it synchronously, or vice versa, or you both expose sync and async methods (like for example Linq with .any() and .AnyAsync() ). If you are the only consumer of your library, why bother and not write it to suit your needs in any scenario? 
Just out of curiosity what are you doing that can't be done in sql?
With the first solution (trigger+CLR function) you need an admin access to the sql server instance not only the database to register the function. It's powerfull and may be instructive for a simple case like yours to discover what you can do with CLR functions. SqlDependency works like a cache dependency, so if you are familar with the .net cache it could be a better choice.
There is a useful invariant here: the call stack is at the same point as when the async method was called, so there is no "unrelated" code running. This means that any interleaving that might occur must already be anticipated by the caller, the same as if the callee were a synchronous helper method on the Form. If the caller failed to anticipate any UI changes the callee might make, that would be a problem for the dev to solve anyway, regardless of whether async code were involved (since we're basically converting it to synchronous).
That you are on the same thread and all the thread state remains the same. To get it to work you'd need to have the 1MB of thread stack be transferable between threads; however, even that wouldn't work in this situation as you are on the UIs message pump and its that which you are blocking and all the UI controls and updates are tied to that thread.
&gt; write it fully async and let the caller manage it if it wants to use it synchronously I just want this management to be less burdensome on the caller. &gt; If you are the only consumer of your library, why bother and not write it to suit your needs in any scenario? Because duplicate code is the devil! And how would you even write unit tests guaranteeing identical behavior if the tests themselves have to be duplicated?
Why would the stack need to be moved between threads? You would wake up the UI thread and run the continuation on it. It wouldn't be doing anything continuations don't already do.
I don't think you should be trying to merge that file. When everybody has applied their changes to whatever your main database is that's when you would regenerate the model. That could be a step that happens after anybody merges changes to the database. 
`.Result` has just blocked the UI thread at that point; the continuation has been told it can only run on the UI thread, so its been added to the UI event queue; but the UI event queue will never proceed as its stuck at `.Result` waiting for the continuation to run.
&gt; Basically I have some .NET code that currently handles printing of reports using the Crystal Reports runtime and I would like to access it through a REST API from mobile devices in my business. My thought was make a simple javascript web app that users can enter appropriate information in and have that app write a record to the database. The .NET code is a server software that runs 24/7 and handles scheduled reports currently. This was my attempt at making it also handle "triggered" reports. 
This is super cool! Does it work well?
check this out: https://github.com/christiandelbianco/monitor-table-change-with-sqltabledependency 
What if .Result waited on two events, one being the event it already waits on (and causes a deadlock), and the other being an event that's signaled when it needs to run continutations on the current thread (because it is the UI thread). It could then run whatever continuations were needed and thus make progress.
Abandon edmx. If you want to to database-first, use the wizard to generated code first from an existing database. In the image with "EF Designer from database" its the option "Code-first from database" 
I think coreclr sort of does that; if the thing blocking the thread is also waiting for the thing that wants to run, it will run it on the thread. However if its several awaits deep it won't detect that. Something to try on .NET Core 3 when it gets the Windows UI layer https://blogs.msdn.microsoft.com/dotnet/2018/05/07/net-core-3-and-support-for-windows-desktop-applications/
Why can’t there be a mechanism that goes "oh hey, there’s a thread blocking on my context, let’s resume from there instead of going to the event queue"? It could even work if multiple threads are blocking on it by pseudorandomly choosing one to run on (though obviously this isn’t applicable to the UI example).
Are you trying to figure out how to get it hosted? I had a similar issue with Azure and ended up putting it into a docker container and hosting it on Heroku and used their PostgreSQL db.
You could try CLR Triggers [https://msdn.microsoft.com/en-us/library/ms131093.aspx?f=255&amp;MSPPError=-2147217396](https://msdn.microsoft.com/en-us/library/ms131093.aspx?f=255&amp;MSPPError=-2147217396) We use this in place of a stored procedure.. don't do that because you cant debug these programs.. Unless you can now then please someone comment because we had some devs that couldn't write sql scripts so they did it in .Net (Arrrrgggg). If you have thoughts on this solution please speak up because @\_Migals you problem is something that we come across and we use scheduled tasks to run and see if anything has changed.. which has its overhead for sure
I'm not sure I get you. The call stack in the middle of a `task.Result` call and immediately after `await task` are vastly different. In the former, you're in the middle of a Windows message being processed, with all its implications. In the latter, the message has already completely finished processing and you're now in a new message that was scheduled specifically for the continuation.
https://github.com/dotnet/corefx/issues/11463#issuecomment-244950904 &gt; Task.Result/Wait already do try to do a bit of additional smarts to alleviate some of the issue here: if you wait on a task backed by a delegate (e.g. Task.Run(someDelegate)), and that task hasn't started executing yet, then the wait operation will ask the scheduler if it's ok for the wait to "inline" the execution, i.e. rather than simply blocking and waiting for some other thread to come along and execute the task, since this thread is going to be blocked anyway, it may as well run the task rather than blocking waiting for it. But that only helps in some situations, and it doesn't help if there's nothing to actually run, e.g. the task returned from an async method is just a promise... there's nothing associated with it to actually "run", so waiting on it will block. We explicitly chose not to pump arbitrary work items in this case, in part as doing so could cause the wait to take much longer than it otherwise would have (the pumped work item may be completely unrelated).
&gt; check this out: https://github.com/christiandelbianco/monitor-table-change-with-sqltabledependency Right now I am looking into setting this up. It looks promising.
Async code [doesn't actually](https://stackoverflow.com/questions/35464468/async-recursion-where-is-my-memory-actually-going) use the stack [in the usual way](https://blogs.msdn.microsoft.com/seteplia/2017/11/30/dissecting-the-async-methods-in-c/). It creates a state object which initially lives on the stack, but can be moved to the heap when needed (such as when awaiting/switching contexts).
I am well aware of how async code works, thanks. 
This may be even better than Electron.NET, because Electron.NET still goes Browser -&gt; Node.js -&gt; Bridge -&gt; .NET (despite .NET being able to communicate over HTTP). Cutting out node on the backend entirely saves a lot I'm sure.
Then why the concern about the async code's stack? It can literally be pushed onto whatever stack it needs to run on. I'm envisioning something like this: 1. UI code invokes `task.Result`. 2. `.Result` runs the task synchronously. 3. The task awaits a resource which runs asynchronously. It is now waiting for a result from that resource. At this point, there is no longer a thread or a call stack for this task, because it's async all the way up to #1. 4. `.Result` blocks on the task. 5. Task gets its result and is now in a runnable state. 6. Instead of sending a message to the UI thread, identify that the thread which invoked the task is blocking on it (is this possible? what code should do this?) and unblock it (maybe also set a flag or something to indicate that a continuation is ready to run). 7. `.Result` continues running the task. 8. Repeat #3-7 as many times as necessary. 9. Return result to the UI code.
DB first version control exists, all about the RedGate tools, really awesome and well worth a check out! That lets you create your database, commit changes to version control, and manage those changes. Furthermore, even handles static data!
And as other people have said, use edmx with defaults to do merge etc. 
I used DO for a while but they constantly had some sort of issue with my data center. I eventually switched to Vultr and got cheaper service for more RAM and less downtime
I've seen a fair number comments about Chromium being slow, how does the speed compare?
Currently the app doesn't work with live streaming or recording but uses prerecorded video from the phone's camera roll or gallery. Do you know of any examples I could follow to see if this is reasonable or possible for us?
I've never had issues like this, but I have had my eye on Vultr for a while! How did you get cheaper service? What's the package? :)
Vultr offers The following package for $5: Looking now, it looks like DO has dropped their prices. Previously Vultr offered a bit more memory and storage for their $5 package than DO did, but it looks like DO has caught up to them
Let me know if you have any questions or would like more information! Nish is a colleague and friend of mine and I can help connect him with any questions you have!
It's painful, but great advice. Ditch EDMX. 
Ugh, that comes so close to answering my question that I have the programming equivalent of blue balls. &gt; But that only helps in some situations, and it doesn't help if there's nothing to actually run, e.g. the task returned from an async method is just a promise... there's nothing associated with it to actually "run", **so waiting on it will block.** WELL *YEAH*, that's the point! What I want to know is why anyone thought deadlock-by-design would be a better outcome than blocking. There's got to be a better underlying reason, and this is not answered in that issue's discussion. &gt; We explicitly chose not to pump arbitrary work items in this case, in part as doing so could cause the wait to take much longer than it otherwise would have (the pumped work item may be completely unrelated). Pumping arbitrary work items would be disastrous...
[This is another good issue to read through.](https://github.com/dotnet/corefx/issues/2454)
I wasted a bunch of time getting an old edmx project to work; even for edmx it was badly done. (A crashed system that we tried to bring back up for someone) At one point I created a new code-first DC from the database and removed the db-first stuff. I thought it was going to be arduous, but it only took a couple hours, far less time than I had spent trying to get the edmx and its odd connection strings working. Since then I recommend dropping edmx
It's funny, I've had the opposite experience. Vultr servers which just randomly go down, so I switched to DO (recently, after price drop)
That's not completely accurate. You only need `ConfigureAwait(false)` on all of the awaits directly touched by the UI thread. Anything that happens inside an task that is already on a separate thread can't jump back onto the UI thread.
That's not necessarily true. .Result doesn't always "block" the UI thread. It could simply be using that thread inside the Task. In that case, and if we only were using WinForms, then the Task that's holding the UI thread hostage could, in theory, go back and pump Win32 messages. But there's no way to generalize this for all cases. And we really, really don't want `Task` to be tied to WinForms, WPF, etc.
That really depends on the UI framework. For WinForms (and the older VB6) you could pump messages while handling another message using DoEvents.
I assume you’re talking about Vue, the typescript integration will be much better in 3 as it’s being rewritten in TS.
Yeah I heard, looking forward to it. Also why the hell so people in this subreddit downvote for stupid reasons like someone saying the TS integration needs improvement?
I agree that you could in theory design code to be safe even in the face of processing arbitrary messages halfway through another message. But I don't think it would be wise for the TPL developers to assume this to be the case!
Agreed. 
Probably because Vue isn't mentioned at all in the article, it was only shown in the thumbnail I saw on mobile.
Oh fair I was on mobile and didn't actually click the link. I was confused why you needed to assume I was talking about Vue, but now I understand why (and the downvote too).
It's picked up a meme from another Vue post I have on my blog.
"Code-first from database" I think is the way to go. The database-first, EDMX approach maybe saves time initially, but you'll spend way more time fighting with it after the initial creation. After you get used to it, the code-first method is pretty simple to use and easy make changes to. Also, the source control aspect then becomes simpler.
"Code-first from database" I think is the way to go. The database-first, EDMX approach maybe saves time initially, but you'll spend way more time fighting with it after the initial creation. After you get used to it, the code-first method is pretty simple to use and easy make changes to. Also, the source control aspect then becomes simpler. 
When my Azure free subscription ended I went to sign up for a paid subscription. I discovered that as part of a paid subscription I must elect to receive "offers" i.e. spam from Microsoft and their affiliates. I declined the subscription at that point as a matter of principal although it made me really sad to do so. I really want to learn Azure and like it. But reading about how easy to deploy to DigitalOcean makes me glad I stuck to my guns. Thanks for the great article.
&gt;The issue only occurs when async code is awaited synchronously, in a synchronisation context in which awaiting things synchronously will block. [...] Using `ConfigureAwait(false)` to fix this is a kludge. True, but deadlock is not the only reason I advocate this (though it would be sufficient even if it were). If you have multiple independent tasks in flight that all have to funnel through the same original sync context, then you have potentially created a bottleneck which `ConfigureAwait(false)` will improve even in a situation where there would be no deadlock. As with the deadlock situation, whether or not it ever makes a difference is entirely up to how your caller invokes the method.
Damn they have a lot of interesting products, thanks.
Seems like that's the way to go, thanks.
This seems like the best option. I will be looking into this in more detail over the weekend, thanks for your help.
Try to installl any nuget with async/await support.
The EDMX should be in its own project. Generate the thing and check it into source control. Make one person or team responsible for it and everyone else keeps their hands out of it.
Ditch it, move to FluentMigrator. We like to put all the migrations in its own project within the solution. More for cleanliness. It's also possible to setup an automated test (via xUnit, etc.) to make sure that your migrations work and will work when you deploy.
Then you are limited to make everything complaint to ie8 or worse on older OSes. Making development painful.
Yeah you're absolutely right. As an aside, is there a way to set the sync context within a scope, so it could be used inside a `using()` block? It's thread static, so the scope could set it and then reset it on dispose, but I"m not sure the the context would get propagated down the async stack.
Yes, it is a limitation of the underlying webview that the .net core libraries use. For windows it is [IWebBrowser2](https://docs.microsoft.com/en-us/dotnet/api/shdocvw.iwebbrowser2) I'm looking at options now. 
Actually, you exclude all the generated files form source control. The only file that needs to be in source control is the .edm file. Exclude all the generated files. When you merge, Visual Studio will likely resolve all conflicts. It actually really easy. Your only problem is that you are keeping the generated files in Source Control and you should not. Generated code files should be excluded.
I think your assumption was that I wanted the blocking thread to handle async work for code from any context. That is not the case, because *of course* such an approach is full of race condition-esque perils. I said before: &gt; which waits for a continuation for **that** synchronization context to be available. Synchronization context is probably the wrong term to use here. Execution context would be more accurate. If that is the source of our miscommunication, my apologies. I want the blocking thread to run continuations that it is responsible for creating and is now waiting for a result from. Any continuation that runs on the blocking thread should be able to be thought of as being in the call stack of that thread had the code been written as regular synchronous code.
The question is usually why. What problem are you trying to solve and similarily what problem where the people building the other .net compression library trying to solve?
Definitely ditch the edmx files as others have suggested. I've found a combination of, SSDT for Schema management and deployment, and T4 templates to generate POCO's to be relatively lightweight and pretty easy to manage. It is a little manual in the workflow though, make changes to schema, deploy schema changes, run T4 to generate POCO. If you need to elaborate further let me know.
I HATE Azure “mandatory notices” that are actually marketing emails in disguise. I love Azure products but FUCK YOU azure marketing interns. You are going to get sued someday for your spam. For several hundred dollars a month you’d thing they’d have the decency to respect your inbox. 
We still have issues. Someone will update from the DB and accidentally update the schema of a table that was changed (for some reason). Why the fck it ALWAYS updates entites instead of giving you the option to JUST add a table is beyond me. So there's a mandatory visual check of the EDMX before checking in which no one does. And then there's the source control merge issues., made worse by the random shuffling of sections around depending which version of VS the person was using and other random issues. It's horrible. We have a related tech we're trying to ditch that relies on a specifically old version, so we're a bit screwed there too. It will die though.
I successfully used roselyn for a rate engine implementation. It was quite easy.
Yeah I liked Azure as well and naturally how easy it was to deploy to it given the whole Microsoft eco system. But ultimately it just ends up costing more than it's probably worth and the UI is still more complicated I feel than it needs to be. This is obviously my opinion; but I'd rather just remote on to my DO droplet than use the Azure UI lol, I find I get stuff done faster :D
watch?v=3Z9yK3sMDUU
Hey, what you said not only was offending, but also was inaccurate. You could share your opinion without offending anyone. And what do you mean ? are you even confident about what you are saying ? Let me clarify you. 1- FIRST HOW DOES LINQ DO ITS JOB : LINQ in its core does its job with a set of extension methods which use predicates(as lambda expresions) to carry out operations on the underlying layer of data. CHECK THIS PLURAL SIGHT COURSE from Scott Allen where he explains it very deeply and from scratch (check module 3 of the course). https://www.pluralsight.com/courses/linq-fundamentals taking this into consideration, how do you think one will implement these LINQ queries in his api just like RX did ? answer yourself. 2- IF You are still not convinced about what is clearly stated in that course, Check this course on RX and how Dan Sullivan explains that RX EXTENDS LINQ. https://www.pluralsight.com/courses/reactive-extensions 3- What does the award on one's blog have to do with the fact that you like or dislike the content ? Just by looking at your profile, it stated you created it a few days ago, but you have several hundreds of COMMENT Karma points and 4 POST KARMA POINTS. which clearly means you comment (Obviously criticize) far more than you share knowledge. 
Legacy apps written in Forms have no way to be migrated. I hate to say this, but form made thinks like grids and Ajax input way simpler than pure MVC. Migrating to MVC such app is just impossible. That said, I don't want to touch forms anymore.
I hit regressions with entity framework postgres on my side. Nothing bad, I could workaround it... But still.
&gt;is there a way to set the sync context within a scope, so it could be used inside a `using()` block? sync context current is thread static, so the scope could set it and then reset it on dispose, but I"m not sure the that current context would get propagated down the async stack. That's tricky, and one of those kinds of questions where it might be better to try to address the underlying question than this one directly. For the most part, as far as I know, if your method is called by a framework method with the current sync context set to a particular non-`null` value, then you have the responsibility to set the sync context back to that same non-`null` value upon returning from the method. `async` methods make this complicated, though, since the generated CLR method returns at the first `await` of a not-yet-completed awaitable, which is when your requirement kicks in. So if you're just trying to flow (or suppress the flow of) the sync context across `await` boundaries without `ConfigureAwait(false)` on every `await`, then you could try [Ben Williams's `SynchronizationContextRemover`](https://blogs.msdn.microsoft.com/benwilli/2017/02/09/an-alternative-to-configureawaitfalse-everywhere) (disclaimer: I've never used it... I've only seen the code and written something similar that's worse). Conceptually, `await new SynchronizationContextRemover()` is equivalent to awaiting a task with `.ConfigureAwait(false)`, with two big advantages that I can see: 1. It guarantees that everything that follows it in the `async` method will execute without an ambient sync context (whereas awaiting a task with `.ConfigureAwait(false)` will only guarantee that if the task is not yet completed at the time of the await), and 2. It doesn't force the rest of your `async` method to get rescheduled if the sync context is already `null`.
Visual studio has one built in
&gt;Async all the way or don't. Don't mix async and ~~TPL~~ synchronous code. I've violated this guideline, intentionally, knowing about it and why it exists, to great effect, in one of the oldest parts of a subsystem of a highly interactive WPF application that was about a decade old or so when I did it. Even as recently as this year, I did some of this mixing in a way that I thought was sure to be safe, only to find that a subtle interaction was causing a really important (non-UI) thread to block on something that it definitely should not have blocked on (talking about a potentially minute-long CPU-bound callback that prevented other mostly-independent potentially longer callbacks from starting in parallel). Not fun. At some point, I should probably learn TPL Dataflow, but I digress. I still completely endorse this guideline in general. Deviating from it is expensive and will more likely than not involve many trips to blog posts by Stephen Cleary that will remind you that you signed up for this by violating that guideline as they give you the information you went there for. That said, it did wonders for my understanding of how this all works.
&gt; how would you even write unit tests guaranteeing identical behavior if the tests themselves have to be duplicated? var result1 = await sut.GetValueAsync(); var result2 = sut.GetValue(); Assert.AreEqual(result1, result2);
What does "depends on it being running" mean? How does a class "run"? Can you serialize and deserialize the object? You can store the object in session state (often unwise, can cause problems with multiple tabs open, but might work for what you're doing). You could make a static collection to hold the object in memory and reference it by some identifier (but understand about thread safety before you attempt this). Try describing what it is you expect the class to do, and why you can't use persistence, and maybe I can give you a better answer.
Hi Cracky7, My first response would be: in general, look into dependency injection if that is available to you. You can then instantiate the object when your app starts and registering is as a dependency or service (depending on the DI framework that you will be using). If you want the object to the shared to be exactly the same instance, be sure to register it in the appropriate way after you instantiate it, during app boot. Most DI frameworks call this registering an object as a "singleton". Happy to get a bit deeper into this if you provide some more details. Cheers!
&gt; involve many trips to blog posts by Stephen Cleary This made me laugh loud because I go back and forth on [this article](https://msdn.microsoft.com/en-us/magazine/jj991977.aspx) so much that to conserve bandwidth I have it saved on my desktop :D &gt; Not fun. At some point, I should probably learn TPL Dataflow This is exactly my thought at the moment. But yeah, async/await is a powerful tool and as such it must be understood in its entirety. Glad to see that someone has shared my same learning path. I'll forever be glad to great people like Stephen Cleary and [Bar Arnon](http://blog.i3arnon.com/) for their invaluable teaching. 
I figured it out. Dependency injection and AddSimpleton did the trick. Thanks.
I figured it out. Dependency injection and AddSimpleton did the trick. Thanks.
Awesome :-)
DbSchema is good. Not free, though 
That may not be correct, depending on what your class does. A Singleton (I assume Simpleton was a typo) means the same object is used for ALL requests. So if User A interacts with your object, and User B makes a request, they interact with the same object. This has implications for thread safety (Say user A adds an item to a collection in the object at the same time as user B tries to read from the collection), as well as security (if you're storing user relevant data in the object)
Yeah, Singleton, I don't know what made me type simpleton. I do understand the problems, and while it wouldn't be the solution if I was aiming to serve the project to the public, it's perfect for what I need (basically a server to use as a demo that won't be accessed by more than 1 user at the same time).
I didn't know this! How does it work? I'm using [ASP.NET](https://ASP.NET) Core with PostgreSQL.
I thought Azure was similar in that you could create a Linux VM and remote into it? I used to create Windows VMs a couple years ago and usually just RDP'd into them. I hardly ever used the web interface.
Yeah but I still personally prefer the DO UI to create th VM, that's what I was referring to. Also the cheaper prices are a factor.
I highly recommend anyone reading this article to take a looking at what is offered by Pivotal Web Services (powered by cloud foundry). From a .NET core perspective, it would be as simple as going into your compiled bin folder and executing "cf push myappname" command. It uses the same model as Heroku with buildpacks, will auto-detect that it's a .net core app, introduce the runtime, hookup monitoring, log capture etc. It will also allow you to on-demand spin up any services (such as db) and bind it to your app, which will inject connection string into your app via an environmental variable. You can parse it out yourself or use any of the steeltoe.io libraries to map it into the standard IConfiguration and register it into the container. For a single app instance, I think it's like $10 a month. I like this model because as a developer I don't have to build containers as developer. That's extra overhead that is managed for me by the platform. The other advantage is that when they update the buildpack with patched versions, I can reintroduce the new runtime just by telling platform to restage the app (rebuild the container). Take it for a spin - they give I think $70 of trial credits when you sign up which will last you for a while. https://run.pivotal.io
That's not very helpful. This should be titled how to read what type of hardware and connected devices exist.
I love articles like this that show how to do things with aspnetcore. Is a Linux VM the way to go for aspnetcore hosting? I wonder what percent of aspnetcore developers use Linux versus Windows for hosting these days. I like that Microsoft opened dotnet to these kinds of scenarios. You could do this on Windows servers, but not at Linux prices.
I can't run the code, maybe a using is missing? 
Yeah whenever DO has maintenance, I get emails in advance.
Nope, I know how LINQ works, I know how RX works. This is why I said they do not share the same codebase but the same DNA. Where I was wrong, and I corrected my post with an edit: was that the RX operators are in the Linq namespace, but under RX. Namespace usings just magically appear for me as I use tools, but I concede I made a mistake. I'll give you "RX extends LINQ" if it makes you happy. It's not a million miles from what I described using DNA in my example. The same patterns that enable LINQ, extension methods and expressions enable RX and they are in the Reactive.LINQ namespace. It's not something I'm going to fight over. I assume as you've made this your cause célèbre, you concede the other points I made? And don't pick on my internet points, I've only been back on reddit for a couple of weeks, who gives a damn? I don't play for points. 
Not that I’m aware (to some relief) - but there is scaffolding (both from vs which I haven’t used and the cli which I have used via aspnet-codegenerator). Not sure what it’s status is in 2.1 but works fine in 2.0.
Sounds like you are trying to use a sql table to do a message/task queue work. I really suggest to use some kind of a task queue that you can produce a message to from your app and consume from the .Net server code. Take a look at rabbit mq .. 
Oh wow this does look nice, I showed this to my lead developer. Although we cant use project, the service broker is probably our number 1 pick for replacing a scheduled task! Thanks! Let me know how it goes if you remember.
Thanks for this.
I managed to get that working today and it is quite interesting! Still working out some kinks but basically I have a way now to execute code when a record is entered into a specific table.
8PM EDT happens when this comment is 5 hours and 13 minutes old. You can find the live countdown here: https://countle.com/211864JMe5 --- I'm a bot, if you want to send feedback, please comment below or send a PM.
I'll be using Phosphorus Five, my opponent I am not sure about. There's a donation thing, where the money goes to some good cause chosen by the mods at /r/shittyprogramming ...
You should reference the System.Management assembly in your project, I forgot to mention it, and I'll update the blog post consequently. 
I figured it out finally textbox2..Foreground = (System.Windows.Media.Brush)Application.Current.FindResource("TemplateFont");
Just recompress until the file size is zero :) Seriously though, if you want a sane method (i.e not CMIX) then use brotli or bz2.
&gt; We were extremely lucky in this situation as the byte[] arrays that made it onto the LOH had two sizes; 131,186 and 131,096. This means that as old objects of either size died and were collected, newly allocated objects were just the right size to slot right into the empty space. You were not lucky, this is by design. The LOH isn't there to punish you for allocating large arrays, although LOH fragmentation can become a serious problem when you do naive IO like `byte[] responseBytes = new byte[response.ContentLength]`. It's an optimization strategy that relies on two assumptions: 1. Moving large objects around is very time consuming, and 2. Large objects get allocated in fixed sizes (buffers!) If those assumptions are true, everything is fine and nothing leaks. Trying to outsmart the GC by allocating arrays that are just a little bit smaller then 85kb is not a good idea. Well, yes, they do not show up as LOH allocations anymore, but OTOH, the GC has to move around lots of data constantly. Using presigned urls + HttpClient instead of S3Client.PutObject is still a good idea, though. 
It can work, but it's not very simple. Is it always the same format?
Awesome article! Thanks!
I'll check it out, thanks! :)
I don't think there's really a right or wrong answer! But generally speaking, unless you have a very specific requirement for a Windows server, why not prefer Linux for the prices? After all this was one of the reasons many people were excited that .NET Core brought cross compatibility!
For some reason, with the .NET Core Console App project, it does not include the "Include Docker Support" on the project wizard. But it appears you can add it in after the fact. Might just be an oversight from the Visual Studio team.
I got it. Yeah, I guess the UI services like Azure are like that because they offer so many granular services. Amazon and GCloud are no better. Also understand about prices. I think an equivalent Azure VM is around $10.30 if you add a disk. I'm looking for a cloud Linux VM host. Not sure whether to go with Azure, GCloud, Digital Ocean, Linode or another yet.
I agree for your example, but those types of functions can be useful in a pointfree style. Lodash doesn’t really make this clear with their examples.
When is this shit going to get a gridview control?
They do that a lot. It's easy to miss something like that with VS being broken up into so many small pieces. 
It’s got the class diagrams or EF could allow you to do it, depends how abstract you need your model to be I guess. If it’s basic then just add a new class diagram file to your project, if it’s more complicated then EF can do it. Do you need just a diagram, then Visio can do it or a plethora of online tools, if you need it to also translate to code then EF may be an easy choice 
/tmp and /var/tmp are the global temp directories. /tmp is often used for extremely temporary files. For example, a web browser might use /tmp to write data. /var/tmp is for more persistent temp data and will stay through reboots and other purges. For the specific users though, people will often do different things and handle things a bit manually, maybe putting a ".temp" file in the /home/user ,for example, for permission reasons. Of course you'd have to delete that temp programmatically as well.
Use Path.GetTempFileName and write to that. 
What happened?
Came here to say this. 
I kind of like the *functionality* of StarUML, but the v3 *usability* is absolute shit. Like, absolute complete stink-to-high-heaven dogshit. Zero keyboard ability to move (tab) between items, having to *double-click* (!) on a form field (!!) to focus on it, mis-focusing of the cursor when you don’t double-click precisely, completely unexpected behaviour, the works. And those are just the rage-inducing things that float at the top of my mind weeks after I last touched it. It’s like they built a really awesome product from a functionality standpoint, but fired their only person with any usability experience whatsoever before anything got started. Trust me… use anything else that is at least as good. You don’t want to use StarUML unless it has a feature you absolutely require and cannot do without.
On windows, you can modify browser emulation through the registry to force IE11/Edge mode. But that's only on windows 10.
I use iWebBrowser2 in my project and it emulates Edge. You have to tell Windows which browser emulation mode to use. Per app or system wide. Google how to change browser emulation in Windows registry. You can have an app ensure it's always set to IE11/Edge mode and you get all of Edge's rendering engine.
Telerik, Devexpress, datatables. None of these work for you?
Have you tried Brotli? [https://blogs.msdn.microsoft.com/dotnet/2017/07/27/introducing-support-for-brotli-compression/](https://blogs.msdn.microsoft.com/dotnet/2017/07/27/introducing-support-for-brotli-compression/)
I'm in the same boat somewhat. I came from Windows and made the switch about 4 years ago. Getting up and running isn't too hard but it's difficult to know what the best practices are. I think there's an opening for someone to write a book '.NET NOW!' (that is: Not On Windows).
I'd advise you to be careful about drawing conclusions from his articles. Similar to what he did in the last one, he takes an arbitrary metric like LOH-allocations and optimizes the crap out of it, without giving much thought at all about the trade-offs that such an action implies. By "patching" AmazonS3Client, he moves roughly 25mb from the LOH (where it does *not* cause fragmentation!) to the normal heap where it will be constantly allocated, deallocated and moved around. I'm not really sure that this is a good idea. 
https://docs.microsoft.com/en-us/windows/uwp/design/shell/tiles-and-notifications/send-local-toast
You don't get edge, only "edge" - like - latest IE webView compatible. You force the system out of compatibility mode for ie8 and let it use latest known technology (for IE)
Well, I can't speak for the other guy, not being acquainted with his programming language - But [here's what I implemented](https://github.com/polterguy/programming-contest). We were told to basically implement _"the IT systems for a Jurassic Park'ish type of theme park"_. The spec was monstrous, with tons of features and such, so obviously none of us finished. But I am pretty sure I covered the _"most ground"_, which I guess was as expected, since the other guy was using a low level programming language, while some referred to what I was doing as _"6th generation develop mentplatform"_ - Which I'll not argue with in fact, since I largely agree with that ... Here's a screenshot showing on of the comments ... https://phosphorusfive.files.wordpress.com/2018/06/img_5827.jpg I was able to have login/logout, a public front end for purchasing tickets through PayPal to exhibitions, an administrative backend for creating exhebitions with Markdown support for creating exhebitions, a couple of datagrids, a simple MySQL database, etc, etc, etc - I got stuck for an hour with the database schema though, and lost a lot of time partially due to being nervous, and partially due to being interrupted by the mods constantly - Which I guess is the same situation my opponent had though, so I'm not complaining though. Obviously I was able to cover way more ground than the other guy, who was using a low level programming language though. But we both had fun :) However, I find the project interesting in general terms, so I'll probably clean up its code over the next couple of days, implement better structure for it, and actually finish it - Though _without_ focusing so much on the problems _"Jurassic Park"_ had, and more create something like a public kiosk ticket/exhebition system in general for theme parks ... It's a nice use case in fact, and an example of a _"real project"_, kind of right down the alley of where my own tools happens to shine I think ... :) It was very stressing, far from optimal conditions, and at the middle of the night for me (3AM-6AM in the morning) - But **definitely** fun :D I hope CalSosta will release video of the whole thing. Most who were viewing kept on viewing for the entire show, and there was comments shooting in every 20 second or something, for 3 hours straight ... :)
Why? private void WriteObject(ExcelWorksheet worksheet, object contextObject, Type type) { var properties = type.GetProperties(); int skipper = 0; for (var i = 1; i &lt;= properties.Length; i++) { worksheet.Cells[i, 1].Value = properties.Skip(skipper).Take(1).SingleOrDefault().Name; worksheet.Cells[i, 2].Value = properties.Skip(skipper).Take(1).Select(v =&gt; v.GetValue(contextObject).ToString()); skipper++; } } 
Not specialized to text compression, but did you tried SharpZipLib (https://icsharpcode.github.io/SharpZipLib/) or SharpCompress (https://github.com/adamhathcock/sharpcompress)
Make a ASP.NET Core app then remove the ASP.NET bits. Dumb work around but I bet it works.
Not to be rude but it would be better if you debug the code and find it out yourself.
I would also add `new FileStream(Path.GetTempFileName(),..., FileOptions.DeleteOnClose)` which deletes the file after you've disposed of the file stream.
Thanks. But as mentioned in another comment, I discovered that for a Console App, you can add in Docker support after the project has been made. It's strange that they didn't just include it on project creation, but I guess it was an oversight.
This is really cool! I'll definitely be using this later.
Use HttpClientFactory if you're going to use it hardcore in production. https://aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/ Yeah, it's a mess, but HttpClient pools and re-uses websockets so if you're doing several requests per second you should share either a static HttpClient or use HttpClientFactory to prevent production outages.
Be careful with http client in using blocks though: https://medium.com/@nuno.caneco/c-httpclient-should-not-be-disposed-or-should-it-45d2a8f568bc
I don't have to debug it to see that this method is considerably more complex than it needs to be. It is as if you completely forgot what an indexer is in the face of extension methods. The method also crashes. But it would be better if you had some tests and find that out yourself. Still, why is that method better than: private void WriteObject(ExcelWorksheet worksheet, object contextObject, Type type) { var properties = type.GetProperties(); int i = 1; foreach (var property in Properties) { worksheet.Cells[i, 1].Value = property.Name; worksheet.Cells[i, 2].Value = string.Format("{0}", property.GetValue(contextObject)); i++; } } If you have a reason, you should have a comment. Without comments explaining absurd code like that and without tests and without a license this code is worthless. 
it's a very simple function. It only fires when you've a single value instead of a list. And the main difference is it shows property names in 1st column and the respective value in the second one. With that being said, what kind of license do you need. THIS IS NOT A PRODUCTION READY CODE.
So just in case anyone comes across this, the way I solved it was: 1. `&lt;RuntimeIdentifier&gt;win-x64&lt;/RuntimeIdentifier&gt;` added to csproj (This will go away once AWS adds .NET Core 2.1) 2. Add `&lt;DotNetCliToolReference Include="Amazon.ElasticBeanstalk.Tools" Version="1.0.0" /&gt;` to csproj 3. Add `aws-beanstalk-tools-defaults.json` to project directory with aws defaults. 4. Use `dotnet eb deploy-environment` instead of `eb deploy` (I got used to using eb deploy for Node applications) AWS defaults example: { "comment": "This file is used to help set default values when using the dotnet CLI extension Amazon.ElasticBeanstalk.Tools. For more information run \"dotnet eb --help\" from the project root.", "profile": "eb-cli", "region": "us-east-1", "application": "myapp", "environment": "myapp-dev", "cname": "myapp-dev", "solution-stack": "64bit Windows Server 2016 v1.2.0 running IIS 10.0", "environment-type": "SingleInstance", "instance-profile": "aws-elasticbeanstalk-ec2-role", "service-role": "aws-elasticbeanstalk-service-role", "health-check-url": "/", "instance-type": "t2.micro", "key-pair": "MyEC2KeyPair", "iis-website": "Default Web Site", "app-path": "/", "enable-xray": false }
That seems too easy! Didn't even know about that one. Thanks!
Nice!
I think you glossed over what I wrote.
Can someone give me use case for this? is there any benefit to writing to file instead of memory when you immediately delete the file again?
I used this once in order to pass a stream coming from a webapi request, save it to a file, and pass the file name to a comand line utility (FFMPEG in this case), and take another temp file which contained the result from the command line utility and pass it back as the result of the api request. Because of FileOptions.DeleteOnClose I never had to worry about cleaning up the temp files, it just worked perfectly.
Well there you have it. Thanks, that makes perfect sense.
I understand what it does and why you have it instead of the version where you have multiple values. I don't understand the linq ejaculate. With the first column: `properties.Skip(skipper)`, assuming this is running against the latest version of core this will force an allocation (`new ListPartition`) and a `O(1)` algorithm; on earlier versions it may be a virtual call, an allocation and a `O(n)` algorithm. `.Take(1)` will be a virtual call (`IPartition.Take(1)`) and another allocation (`new ListPartition`) again assuming the latest version . `.SingleOrDefault()` will be several virtual calls (`IEnumerable.GetEnumerator()`, 2 `IEnumerator.MoveNext()` and 1 `IEnumerator.Current`) as well as pointless branches and checks for cases of having 0 elements and more than one element. All together that is a stupendously expensive way to index into the properties array. Then the next column: `properties.Skip(skipper).Take(1)` again... `.Select(v =&gt; ` produces a lazy IEnumerable state machine `v.GetValue(contextObject)` can return null `.ToString())` can throw an `ObjectReferenceException` and this relies on some type inference in the setter for `.Value` to determine that the value is an IEnumerable with one value and that value is a string so it is used in the assignment. --- As to a license, MIT is always good but that is really up to you. You might get a pull request here or there (probably not because there are no tests, but who knows). And even if it never becomes a library of production ready code on its own, it could be useful as sample code and would (again more likely if there were tests) wind up getting copypasta-ed piecemeal into other code bases. As it stands, right now the license is "forget you ever saw it and avoid writing anything that could potentially be construed as a derivative work." Adding a license in GitHub is pretty easy, follow the instructions at https://help.github.com/articles/adding-a-license-to-a-repository/ and it will be recognized by GH and your project will show up to more people searching for stuff and you may get contributors.
Is there a easier way? That looks like way too much code for something as simple as displaying a message.
Stupid questions: - Issuer matches on the cert? - Not sure how IdentityServer is using the token here, but is this a situation where the Identity the Web app pool is running under needs *full permissions* on the cert? i.e. if the validation procedure requires use of the private key, this is a must.
Your next best bet is finding a library that handles everything that's shown in the tutorial above.
The application identity has full access to the private key. One thing I see is Comodo root certificate "This certificate is not valid for the selected purpose" never seen that before
If you're in a curl state of mind, that's on-the-fly or stateless scripting. C# implies a stateful runtime like an app or site... I've never thought to convert curl to something an app S/b executing. I usually translate curl (and the dependency on it's executable) commands to PS Invoke-WebRequest/RestMethod scripts for Windows environments.
Er.... So is the root certificate the one with that error? How was this cert made? It's possible the cert wasn't made properly (i.e. it doesn't have the right Key Usage flags) and that's causing this issue. There MAY be some awkward ways to get the cert to work, but I'm not sure if one can do that with IdentityServer.
I wish this was more well known. 
You basically need a CryptoTransform, CryptoStream and a GzipStream. Create a CT, initialize CS with an underlying GS and a newly created CT.
Open PGP supports compression in the same operation. It's internal to the standard. MimeKit for .Net supports PGP crypto easily, allowing you to create MIME messages. If you don't want mime, or PGP, you could use CSharpZipLib which supports AES ...
Thanks for mentioning about the connections limit. I'm aware of this limitation and agree that you shouldn't use the code if you are going to make many HTTP requests. Probably I should add a warning about this issue. The using statements are intentional behavior, and the idea was to produce minimum working code which you can copy to LinqPad and execute. Any suggestions are welcome.
You can see a demo of my contribution here - https://www.youtube.com/watch?v=zo1-5ETRTaw And there are 2 minutes of video (from the 3 hour long session) here - https://www.twitch.tv/sh1ttyprogrammingleague/clips It was darn fun, and the admins managed to keep most of the 300-400 viewers in the channel for the whole nine yard :)
You may want to tune down the database. It’s not initially obvious how to, but play with the options in the calculator. I don’t remember exactly how sorry. But yes they have cheaper options 
Ahh I didnt see the second drop down on single database. Cheapest is £3.65 which is a small difference lol
Yeah it’s definitely a UI mishap on their part
I really like this writeup! However I teach web students who are all point and click, and no command line. Is there any way to go from VS to Github, to DigitalOcean? So just pushing to github refreshes DO? This is a totally foreign land for them and I and so I want to find the simplest, method possible. 
I think I can delete the certificate and recreate it. Thanks for your help. 
Look, kudos and everything for actually going through with this - I don’t think competitions of this type prove anything valuable and winning/losing is meaningless, but it still takes some stones to actually do it - but this is the most boring thing I’ve ever seen. You’re asked to implement Jurassic Park and you choose to do a vanilla ticketing ecommerce app? Where’s the raptor feeding schedule? The park power reboot process? The airlock controls for the T-Rex compound? The meeting room calendar for the DNA scientists? The controller software that monitors the weather and controls the front gate? Where, I ask you, is the 3D filesystem? There’s like a million things here more interesting than selling tickets. You’re a fraction of the man Nedry was. 
Thanks Anton, I do GZip and recently changed to Brotli compression prior to encryption but to me it feels like there should be a lib that offers specific benefits of both compressing and encrypting. Google hasn't yeilded much so I guess that is just a dream.
Thanks, I experimented with the standard GZip, and then incorperated XZ and finally ended up with Brotli (which in my cases helps a good 15&amp;#37; over GZip) my feeling was that there was going to be an encryption system that takes into consideration encryption. Thinking along the line of what something encrypted looks like - totally random poop, and the various hacks related to people being able to decrypt (I.e. [https://en.wikipedia.org/wiki/CRIME](https://en.wikipedia.org/wiki/CRIME)) I had felt there would be a library to specifically address this situation. 
**CRIME** CRIME (Compression Ratio Info-leak Made Easy) is a security exploit against secret web cookies over connections using the HTTPS and SPDY protocols that also use data compression. When used to recover the content of secret authentication cookies, it allows an attacker to perform session hijacking on an authenticated web session, allowing the launching of further attacks. CRIME was assigned CVE-2012-4929. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Unless you absolutely need the full framework, use .NET Core. It's faster, kept up to date more often, and is Microsoft's platform of the future. .NET Framework should be relegated to legacy applications, or those that need features not found in Core. As far as web hosting goes, it really depends on what your building. If it's a small website, shared hosting on a cheap provider is more cost effective than something like Azure. If it's a complex service, you need the features of Azure--or a private server, which is likely more expensive.
**Strengths of .NET Framework for SaaS (Compared to .NET Core):** * Better support for Window's legacy components, libraries and languages (e.g. COM, and VB.NET) * Preinstalled on Windows Servers * Uses standard Window's administration tools to install and configure * Better support for most Enterprise and third party Window's based systems and tooling (today, anyways) **Strengths of .NET Core for SaaS (Compared to .NET Framework):** * Better performance and scalability * Side by side installs * Self contained installs * Cross platform * Gets new features first (e.g. Span) * Source code and issue support is available on GitHub * Most new blogs, articles, documentation and books are being written for .NET Core In general, for a new project you should be using .NET Core unless you are using technologies vital to your project that are only available when using .NET Framework. The question of cloud based or web based deployment and hosting is kind of asinine to ask about before you have even decided what the goal of your software is. Its something that should be informed by system architectural choices. Trying to choose it up is much more likely to get you to arrive at the wrong system composition. If you are asking when to use each architectural style, you could try reading some documentation on the approaches. Microsoft publishes [architectural guidelines and books](https://www.microsoft.com/net/learn/architecture) that are a decent introduction.
I think my problem has to donwith sessionstate and loadbalancing... grumble
Chrome and Firefox dev tools have a copy to curl feature when you right-click network requests. This is the main use case I see for something like this.
Oh alright cool, thanks for your response. I'm looking to build something like, for example, [hunter.io](https://hunter.io) : Where user creates account, pays a small fee, gains access to their API/query x number of emails per month. So basically a company with around 100000 active monthly users. For this scenario would you suggest hosting on a cheap provider, cloud such as Azure or private server? And when you say complex service, can you provide an example or features of a complex service?
Compress first, then encrypt.
I built exactly what you're talking about with Bubble.is and .NET Core recently. I use Lambda so I only pay for the API requests and nothing more. Here is a blog post about how I did it. I let AWS's API Gateway handle all the key tracking for me. https://medium.com/@mendoza.paul/sigparser-creation-story-bubble-is-net-core-aws-lambda-api-gateway-1667213bf0cb
Yes, but you might have to use the pre-release version. I've been using it for the last five months on .NET Core 2.0 and it has been good for me. 
just curious what ORM you would use, EFCore is still seriously lacking features and is not on parity with EF or nhibernate. what ORM would you use if not EFCore?
I would not start a new project in .net framework. Go with Core or go somewhere else. --- starting a project in .NET desktop framework leaves you with 5 choices: 1. WebForms - a leaky abstraction that worked alright for intranet applications where you don't really need to worry about scaling too much and full page refreshes are fine; the abstraction falls apart as you dive into ajax requests and try to do things like work outside of viewstate, session and postback events. The last significant update brought http2 support and injected the roslyn compiler to compile pages and fixed a missed feature with async. I think it is important to note however that this update is not documented anywhere on the asp.net site. The documentation there ends in 2014. 2. MVC - I am convinced that MS chose the name for AspNetCore MVC to be intentionally as confusing as possible for people looking for documentation on MVC. Asp.Net MVC appears to be an end of life product with no path forward if you are stuck there and for some reason cannot migrate to Core. On the bright side, the migration path to ANC 1.0 and then 2.0 on full framework isn't that bad as long as you weren't depending on some of the older cruft inherited from WebForms. Unfortunately since the .net framework team decided `Span` isn't coming any time soon, your migration would be stuck there. Since you are going to eventually want to migrate this way anyway, there is little point in not starting new projects in ANC 2.1 in the first place. 3. WebAPI - if you are going with WebAPI, you are likely in a place where you will have a very small web layer anyway. As good as WebAPI is, ANC 2.1 is better and completely a superset of the functionality. WebAPI also appears to be a dead project and the migration path forward is to rewrite in ANC 2.1. On the bright side, people looking to do that migration can do most of it with copy+paste and a sprinkling of attributes and maybe a small search and replace here and there. 4. WebPages - I don't really know much about them. Apparently you can use an old version of Razor syntax as if you were writing classic asp style pages backed by what appears to be MVC 4. The spiritual successor to this appears to be ANC Razor Pages though I have no clue what a migration would look like there. You might as well start there. 5. ANC 1.0/2.0 on desktop framework - the only reason to do this is because you have a compat restriction preventing you from running on Core. A new project should avoid the various legacy techs that have this restriction and IMO should publicly shame the companies that continue to drag their feet or are otherwise unresponsive (on that note: do not use SAP ASE or the official MySql driver from Oracle as they still do not properly support ADO.NET 4.5 non-blocking IO). Oh and btw, `Span` (a type that ANC 2.1 depends on) will not (as of today) be in desktop framework 4.8 coming in 2019 sometime (probably) and ANC 2.0 will not be supported as of October 1, 2018 which means if say a vulnerability is found in ANC 2.0 after that date it may not be fixed (I suspect if that happens, MS will backtrack and patch it... but "officially" the end of support for ANC 2.0 is 2018-10-01 which means there will be people in this weird limbo land). In any case, asp.net in the full framework looks pretty bleak. As far as I can tell all the devs left to work on ANC with some vision of being closer to the cloud team at MS and handed the full framework off to the Windows team where they continue to work on XAML, WinForms and other stuff that deals with WinAPI. Also I am not sure if the versions of MVC, WebAPI or Web Pages in as far as they exist as assemblies outside of the BCL are currently supported. WebForms has a somewhat privileged status in that it is inside the BCL. I wonder what would happen if someone today published a vulnerability against MVC 5 in System.Web.Mvc.dll. --- Starting in Core though is pretty bright. You have roughly 3 options and they interoperate well: 1. ANC MVC - you have routes which invoke methods on controllers that interact with models, passing them on to views for rendering; the strict conventions tend to land you in something that is pretty testable. 2. ANC Razor Pages are a pretty straightforward way to model an application with a bunch of different pages. It turns out most applications utilizing MVC tend to have 1 main action per view and occasionally several secondary actions. The conventions that make the application generally testable are lessened somewhat, but you still don't have the heavy dependencies you had in desktop framework so it is still reasonable. And if you are pretty vigilant on the patterns you practice, testability remains as nice as MVC (because underneath the facade, it is). 3. ANC API backed SPA - ANC API is really just ANC MVC except you don't have views (by choice). That means there is a sliding scale between an ANC MVC app and an ANC API app and it is extremely likely you have some API calls in whatever MVC app you write. I view this design as more of a strict separation of concerns compared to a normal MVC app. In a pure API/SPA app, the API contains your proprietary logic and does some content type negotiation at the request/response level, but crucially it contains none of your application html or JavaScript. All of that is in the (poorly monikered) "Single Page Application" and is commonly using a framework such as Angular, React or Vue. --- I personally feel an API backed SPA is absolutely the way to go and my preference for a SPA framework is Angular. It is my experience that modeling a web application as anything other than a client application written in html+js and a completely separate server application that interoperate over a well specified API is a recipe for eventual frustration and possible failure due to the underlying impedance mismatch between the "logical application" and the various underlying technologies.
To be honest my main experience is in .NET Framework and Java. Which is why I was contemplating on sticking to Framework instead. But I'm sure the alternatives available would not render me completely unable to perform the desired functions.
Great thank you, will give it a look.
I wouldn't actually use an ORM. I'd use raw ADO.NET (my go-to), but I've also had some decent experiences with Dapper.
 Dapper works fine. If more is needed... NH 5.1+ supports Core (minus a couple of features you can work around https://github.com/nhibernate/nhibernate-core/blob/master/releasenotes.txt#L81). LLBLGen supports Core with basically the same restrictions: https://www.llblgen.com/Documentation/5.4/LLBLGen%20Pro%20RTF/NetFullvsNetstandard.htm#features-not-supported-in-the-.net-standard-build 
Great! Thanks for your detailed explanation.
&gt; but this is the most boring thing I’ve ever seen Well, roughly 400 people openly disagreed with you there. &gt; You’re asked to implement Jurassic Park and you choose to do a vanilla ticketing ecommerce app? Do you want to meet me in a similar competition? I'm pretty certain the admins would be happy to arrange another contest in the same ballpark ...
&gt; Where’s the raptor feeding schedule? The park power reboot process? The airlock controls for the T-Rex compound? The meeting room calendar for the DNA scientists? The controller software that monitors the weather and controls the front gate? Where, I ask you, is the 3D filesystem? None of this was in the specification we were handed for the record ...
Thanks for the warning. What I liked about this article was that it deals with non trivial (for me at least) technical issues. We currently have a consultant analysing memory dumps from our our servers and this kind of post helps to grasp what he is doing. I am completely ignorant of the tooling used for this kind of analysis. This helps to start understanding how some tools fits together for this task.
Private server more expensive? Maybe if you aren’t using it’s full capacity I suppose. Every 4 years I need to re-evaluate cloud platforms and every time Azure / AWS comes out at 2x the price for the same performance
You also can try to use **linq2db**. [https://github.com/linq2db/linq2db](https://github.com/linq2db/linq2db). It's nice ORM with some interesting features like CTE, LeftJoins\\RightJion as FluentApi. You can generate you DataBase scheme via .tt models or use code first approach. If you choose this orm, don't forget to explicitly choose database version. In defalut this library generate slq 2008. `public DataManager(string connectionString) : base( new SqlServerDataProvider(ProviderName.SqlServer2012, SqlServerVersion.v2012) , connectionString) { }`
There are web hooks and things that can pick up changes from GitHub and deploy to Digital Ocean for you. Unfortunately that is slightly out of the scope of this tutorial and I've never used them. One option would be to teach them what CI/CD is and set up a pipeline using something like http://buddy.works/ and configure it to listen to a GitHub repo and then based on a condition (eg if unit tests pass) then configure a git deployment to DO. This would be what I would teach them. The EASIEST approach would be for them to use SourceTree instead of command line and have two remotes to deploy to. GitHub and DO. Then just commit and push using the UI to both.
It's because of the partial view the model binding doesn't occur. It's generating this as the input: `&lt;input type="text" name="name" /&gt;` What it should be generating is this: `&lt;input type="text" name="osoba.name" /&gt;` You can accomplish this by rendering an editor for: `@Html.EditorFor(model =&gt; model.osoba, "ViewName")` The view name must exist in Shared\EditorTemplates and can be in a directory under there. If you want to continue using the partial view you need to pass in the Model (not Model.osoba) and update your asp-for binding. -- If you're using a partial to be re-used in other views you can create an interface e.g. IHasOsaba which just has the single property in it. That way your partial view can be called from anyone that has the interface. Does that make sense? 
Is ADO.NET what people refer to when they use System.Data.SqlClient directly? 
/me golf claps A fellow fan of Dapper. Nice to meet you sir!
I'm sharing this with my team at work! This looks great! Thanks for sharing/writing.
When I edit the partial view to use the "osoba" prefix, it sends it correctly ("osoba.name", ...) but the result object still doesn't contain the information. When I access Request.Form["osoba.name"], it prints it, so it is passed into constructor, but it isn't in the result object. 
Rest in pieces Core 2.0
I’m surprised no one mentioned this, but you shouldn’t really care. ASP.Net core can choose either .NET Framework or .NET core. Start with .NET core and move to .NET framework if there are missing features. The main difference is that you’ll have bundled installs with Core and Core is faster but it probably won’t affect your app. 
Core You don't want to incur costs and underperformance related to Windows servers, especially with a start-up. Deploy on Linux with an open db and you're solid
Yes. 
To start with, EF Core is a productivity booster (unless you hit some edge cases or "lacking features" scenario ). You'll be able to deliver quickly. However, just abstract those EF query (LINQ) in such way that it's easier to replace that with Dapper as you get traction.
We've been using EF Core in production without a problem. Since 2.1 lazy loading (which is a bad idea in the first place) and group-by to SQL translation were added, which are the two bigger features missing most people will probably use. Everything else thats not there from EF6 is mostly very specific stuff only needed in edge cases. So what are you missing?
So what testing library does this project use?
Oh, absolutely. Cloud costs are insane if you're actually using it 24x7x365, but for a startup, I don't see that being an issue. The pay-as-you-need-it style is probably better aligned with the development of something brand new with no users. As it ramps up, as you do, continually evaluate whether that's still the best option. I strongly discourage developers at my office from tying themselves too closely to any one cloud provider for that reason (among others).
Hmm, looking at nuget and through various repositories on github, it appears that aspnetcore 2.1 targets standard 2.0 still and not netcoreapp2. That should mean you can update to 2.1 if you have an ANC 2.0 project on desktop framework. I am not sure where I got the notion that you couldn't from. Presumably it will be faster if you are targeting Core. The documentation on updating to 2.1 actually says: &gt; If you're targetting the .NET Framework: &gt; &gt; * Add individual package references instead of a meta package reference. &gt; * Update each package reference to 2.1. Because https://www.nuget.org/packages/Microsoft.AspNetCore.App/2.1.1 only targets netcoreapp2.1 The full list of ANC 2.1 packages that have dependencies outside of netstandard2.0 is: * https://www.nuget.org/packages/Microsoft.AspNet.WebApi.Client/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Http.Connections/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Mvc.Razor.Extensions/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Mvc.Razor.ViewCompilation/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Mvc.Razor/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Razor.Language/ * https://www.nuget.org/packages/Microsoft.AspNetCore.ResponseCompression/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Server.Kestrel.Core/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Server.Kestrel.Https/ * https://www.nuget.org/packages/Microsoft.AspNetCore.Server.Kestrel.Transport.Sockets/ * https://www.nuget.org/packages/Microsoft.AspNetCore.SignalR.Common/ * https://www.nuget.org/packages/Microsoft.CodeAnalysis.Razor/ * https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.Design/ * https://www.nuget.org/packages/Microsoft.Extensions.DependencyInjection/ * https://www.nuget.org/packages/Microsoft.Extensions.DiagnosticAdapter/ 
You can run framework on non windows servers. 
It looks like Webview already does this and it's not an issue. whatismybrowser says edge11.
Seems like it uses some static set, so wonder if it would break for test methods running in parallel against the same instance 
Very nice! Usefull quick and dirty tools.
Than you're good 👍
.NET Core = Linux compatibility, .NET Framework = Windows constraint. For a startup it's a no brainer.
SMO is sql server management objects, so no, there is no dB agnostic version as its built by Microsoft for Microsoft. AFAIK there is no alternative for Postgres, because SMO was built for sql server management studio. You can do everything you can with SMO using actual sql in postgres. What's your use case for needing SMO? 
Only with Mono, meaning youl be behind the newest version and generally will have more issues than if you were on a Windows server. I wouldn't recommend using mono for production apps when. Net core is out, and if you insist on framework I recommend Windows servers. 
We use this a lot, can't recommend it enough
Here is the ticketing back end, the guided tours based upon speech synthesis, automatically translating to any language before starting the tour, generating QR code that links directly to tours, etc. I spent some 5 additional hours coding on the system today. See a demonstration of these features in the video below ... https://www.youtube.com/watch?v=uTeok28QFVo If you want a 3D file system, I'll have to disappoint you though, since I'll highly likely never implement that unfortunately, since I am trying to prioritize features I believe potential users might find beneficial though ... Total amount of work ==&gt; roughly 10-12 hours ...
Was there any specific advantage of using [Bubble.is](https://Bubble.is) as opposed to something really popular such as 'Wix'?
Thanks for that I am just looking at doing the same thing :-)
Do those attirbutes actually change the .NET memory layout or just hint to the marshaller how it should be handled?
As far as hosting, check out Amazon Lightsail. It would allow you to get started on the cheap, and scale up if need be. AWS pricing models can be tricky, when the time comes, might be worth hiring an AWS DevOps guy on a freelance site for a few hours, savings can add up. I am not sure what your hosting needs will be, but from experience developing sites from other clients where there were regulatory concerns such as HIPPA, PCI... I would have gone with Core if it had been a viable option. The cost on AWS for Windows images was a lot higher than for Linux. You can now run SQL Server on AWS linux instances now. If you abstract the code well enough, your business logic could be moved to standard in minutes. I still prototype projects in standard, because I am so proficient with Entity Framework, and during prototyping, don't really want to worry about docker and such, and there seems to be less configuration.
It should. If typing are good. And I checked it and they are. I created simple project: tsc --init yarn add lodash @types/lodash and then added simple *test.ts* file: var objNul!: { foo: string; } | null; if (_.isNull(objNul)) { objNul.foo = 'a'; } else { objNul.foo = 'a'; } if (!_.isNull(objNul)) { objNul.foo = 'a'; } else { objNul.foo = 'a'; } VS code shows errors, and running `tsc` also shows error: test.ts:6:3 - error TS2531: Object is possibly 'null'. 6 objNul.foo = 'a'; ~~~~~~ test.ts:14:3 - error TS2531: Object is possibly 'null'. 14 objNul.foo = 'a'; ~~~~~~ So you must be having some wrong type definitions or don't use _strictNullCheckc_ at all.
If only I knew what Telldus Core was...
Yeah, I’m not interested. The competition is over. 
Thank you!
&gt; lazy loading (which is a bad idea in the first place Interesting comment: Might you elaborate?
You are welcome.
&gt; The competition is over. OK, and assuming the task was not to implement the _"most shitty"_ system, who do you think won ...? ;) PS, I'd love to meet you next. I am certain /u/calsosta would be happy to arrange another contest between me and you a month from now ... ;)
Possibly you missed it, but I already made my view clear on this sort of thing: "I don’t think competitions of this type prove anything valuable and winning/losing is meaningless". I haven't seen the whole thing, so I don't know who 'won', and have somewhat less than zero interest in investing any of my own time into such a thing. Throwing together a half-assed system in the shortest time possible is not a skillset I wish to optimise for.
&gt; Throwing together a half-assed system I could easily already at this point, 10-12 hours of coding that is, suggest this system as already roughly 50% of the necessary implementation for any small _"Theme Parks"_ out there. Of course, now I probably get a couple of _"superlatives"_ from /u/Othis - Which already for reasons unknown seems to have a horn in my side - However, there are others here reading our _"discussion"_ too I believe. For those, feel free to watch what 10-12 hours of P5 coding can do here ... https://www.youtube.com/watch?v=uTeok28QFVo &gt; have somewhat less than zero interest in investing any of my own time into such a thing Why doesn't that surprise me ... ;)
&gt; Of course, now I probably get a couple of "superlatives" from /u/Othis I already gave you credit for actually going through with this, and even lightheartedly compared you to Dennis Nedry (the programmer of the Jurassic Park software in the film) and, as ever, you took it with no grace whatsoever. I don't see much reason to continue.
Did you mean to link to something?
That was interesting. Is the next one Azure? The Google Cloud version seemed a lot more easier than the Amazon one.
Same here :-)
In short, because it's almost always bad for performance. In web applications you will always know which properties to load beforehand, so just include them in the first place. This will help to only make one instead of multiple requests and in most cases increase performance a good bit. This article probably explains it a bit better than me: [https://ardalis.com/avoid-lazy-loading-entities-in-asp-net-applications](https://ardalis.com/avoid-lazy-loading-entities-in-asp-net-applications)
&gt; you took it with no grace whatsoever Well, then I apologize. However, you kind of destroyed your own compliment afterwards, by asking stuff such as _"where is the 3D file system"_, +++ - In a kind of negative way too may I add ... I think I told you thank you for your nice words though, and your compliments. The contest was **for fun** mostly, and I admit it **was fun**. However, I also must confess that after the amount of resistance I found after registering here at Reddit, I also had other _"ulterior motives"_ for going through with this. And I'm pretty certain few will be able to repeat the words of my opponent, at least with a straight face - Who started the whole thing with the following sentence ... &gt; Hyperlambda is an amalgamation of buss and hypewords I assume we can all agree with that I have scientifically proven the OP's original statement to be 100% perfectly false now ...
&gt; Well, then I apologize. However, you kind of destroyed your own compliment afterwards, by asking stuff such as "where is the 3D file system", +++ - In a kind of negative way too may I add ... Have you even seen the film? The 3D file system has been a target of good-natured ridicule for 25 _years_ now. While I maintain that selling tickets is the least-interesting piece of Jurassic Park software imaginable, I was not seriously suggesting you implement a 3D file system.
I remember the 3D file system yes. If I am not entirely wrong, this is where the guy utters; _"Awesome, Unix"_ - At which point all _real_ hackers watching the viddi starts laughing ... ;) &gt; selling tickets is the least-interesting piece of Jurassic Park software imaginable Well, it's also obviously the most important part, assuming the theme park is run by a corporation interested in revenue. Besides, I wanted to create something that was actually useful, and since I don't see raptors starting to roam our streets anytime soon, I thought I'd implement a system that could be customized to work for _any_ Theme Park out there ...
Added it now :)
If I've understood your request, this is what you're looking for: [https://msdn.microsoft.com/en-us/library/cc668201.aspx](https://msdn.microsoft.com/en-us/library/cc668201.aspx)
I've been working on a .NET Core project that uses DllImport/PInvoke with native libraries for different platforms (win,linux,osx) and architectures (x64, arm). It all seems to work fine. What is the issue exactly? That the libraries have different names? I'm importing "webview". This is webview.dll on windows, libwebview.so on linux, and libwebview.dylib on osx. It all works as long as the native libraries are in the bin folder along with the app. If the issue is all the libraries are different, e.g. "glibc" vs "msvcrt" or "vcp140" I would recommend following a similar design - write a native wrapper library that exposes the OS specific calls, and DllImport the wrapper library. 
I'm using a wrapper library for SDL2, it loads the SDL2 native library. It can't locate the right library, as noted in this issue: https://github.com/flibitijibibo/SDL2-CS/issues/117 I'm also planning to use other libraries that reference SDL2 and potentially other native libraries. I haven't found anything that indicates that Core can handle finding the right library beyond automatically appending extensions if you don't specify one. SDL2 in particular does not have matching names on different platforms which causes problems when using this automatic resolution. I know symlinks could be used to help with that, but it won't solve the problem for existing libraries that don't work. Writing a wrapper for something as large as SDL2 is a lot of effort, a better solution would be to just allow specifying the native library name for different platforms.
Async ...? Context switches (thread jumps) have a high cost, and often tends to drain the app. Make sure you don't use more threads than the number of CPU cores you've got ... You can also implement something similar to this (having one _"master"_ machine, delivering code to other servers, to take advantage of scaling your solution out) https://gaiasoul.com/2018/06/22/invoking-a-pgp-encrypted-lambda-web-service-with-5-lines-of-code/ If you can somehow parametrize invocations to such processes, you could probably distribute the workload across multiple server easily ...
Looks more like he's asking for code to _shorten_ his existing URLs. If I'm wrong, here's how I do my routing though ... 1. [Rewrite URL](https://github.com/polterguy/phosphorusfive/blob/master/core/p5.webapp/Global.asax.cs#L147) 2. [Retrieve actual URL](https://github.com/polterguy/phosphorusfive/blob/master/core/p5.webapp/Default.aspx.cs#L57) There are some Phosphorus Five specific code in there, but I'm sure OP is able to clean those parts away to get _"the ghist"_ of the C# code ... The key is to retrieve the real URL in your _"Default.aspx.cs"_ for then to rewrite the URL at the beginning of your request in _"Global.asax.cs"_ - Assuming he's using ASP.NET ...
Start by **not** using `Task.Run()` and dedicated threads. That will improve performance already. Make sure you use `ConfigureAwait(false)` to not carry unnecessary synchronization contexts. And after that: Profile your application.
I am not managing threads manually. I am doing along the lines of this: var semaphore = new SemaphoreSlim(100); foreach (var domain in domains) { semaphore.Wait(); Task.Run(() =&gt; { try { CheckDomain(domain); } catch(Exception ex) { Console.WriteLine(ex.Message); } semaphore.Release(); }); } Thanks for the "Lambda Webservice" article. That seems like it is what I need. One command server in cloud and then a series of bare metal servers set up as slaves that the command server executes code on.
Profile the code, that will help you figure out what is slow. Guessing is futile. Jetbrains or RedGate have tools, VS has too if you have the enterprise version. I'd be surprised if context switching was the problem. Scaling out is usually easier than scaling up, but costs more in the short term ($ cost, not effort cost). There's nothing messy about using a central store. This is literally how every website that manages sessions does it. You are interested in writing more than reading and certainly not a mix of both, so you can stream stuff into it with good performance. By "it" I mean which ever platform you choose. This is a big topic. If you're processing data over 30 days I imagine you already have a store of some sort. There are lots of tools for distributed processing. Try something like AKKA.NET for example. I think that will fit with your requirements better than some more opinionated and expensive tools. But to reiterate, I doubt if threading is your problem here, profile stuff. If you can't afford the tools (you can afford the ones that come with resharper and Redgate do a trial) then break down the code into units and measure their performance with something like benchmark.net, or a Stopwatch (class, not an actual stopwatch :)) 
so you have to make http GET request: [https://www.googleapis.com/customsearch/v1](https://www.googleapis.com/customsearch/v1) with 3 mandatory query params cx - which is your ID key - your key q - phrase to look for example: [https://www.googleapis.com/customsearch/v1?cs=1234&amp;key=4321&amp;q=how+to+suicide](https://www.googleapis.com/customsearch/v1?cs=1234&amp;key=4321&amp;q=how+to+suicide) full params list: [https://developers.google.com/custom-search/json-api/v1/reference/cse/list](https://developers.google.com/custom-search/json-api/v1/reference/cse/list)
what is the point of task in here? Oh, I will write this piece asynchronously but i'll make it synchronous
Check this out - https://stackoverflow.com/questions/9543715/generating-human-readable-usable-short-but-unique-ids
Thanks!
When I look at that project, it looks like they have the extension hardcoded as ".dll". It might work if you rebuild the project without the hardcoded extension. Each import has a variable 'nativeLibName'. It's set to "SDL2.dll". Change it to "SDL2" (no extension) and rebuild. Also change the other imports for image, mixer, ttf, etc. SDL2 is SDL2.dll on windows, libSDL2.so on Linux, and I'm assuming libSDL2.dylib on OSX. Importing 'SDL2' should work on all three platforms. 
When scaling out, have you considered using some kind of message queue? i.e. one work item per message, multiple consumers taking work items off the queue, and potentially putting new ones on the queue. Yes, you most likely will also need to share a database. That's the way it is, it's not more "messy" than necessary.
Isn't greatly utilising async so you may be suffering from threadpool starvation. Try increasing the min threadpool count to check ThreadPool.GetMinThreads(out int workers, out int ioThreads); ThreadPool.SetMinThreads(200, ioThreads); 
Are you running these domains one at a time? That seems to be what your semaphore is doing, locking it down so each item waits for the previous to finish. Yeah, it will be faster in parallel. Also, the code would be the same without the " Task.Run(() =&gt; {" so why did you bother with that?
Agreed, the scaling issue sounds like it needs a simple queuing system. /u/Mr_Nice_, If you're going down the Azure route then Service Bus can easily be used for this sort of thing. What I'm not understanding is how something can take nearly 30 days to run when it's just checking stats? The snippet you've posted in this thread doesn't really explain anything because even if you've threaded badly you're not going to see massive performance issues like this. If it's definitely CPU, I'd suggest getting a profiler hooked up. Again if you're going down to a hosted solution, you could use App Insights to do traces, there's Prefix for local development which might help and is free.
&gt; One command server in cloud and then a series of bare metal servers set up as slaves that the command server executes code on Sound much more scalable yes. Depending upon how you access these servers from your command server (lan/wan), PGP encrypting it, and cryptographically signing MIME envelopes, might be overkill. But has additional advantages too, such as compressing requests and responses. My tools use MimeKit internally, which is a kick-ass lib to create MIME/PGP requests. You're [free to use my tools since they're open sauce](https://github.com/polterguy/phosphorusfive) - Not sure how you'd go about it 100% natively, if you're to transfer code from the master server to the clients though with a more _"pure"_ C# solution - But I assume you'll figure it out :)
It's messy prototype code atm I put together to grab some insights and it ended up making some money so now I actually have to make something on a production level. Right now it's just in localdb on my dev box and I am manually triggering reports so free to move in any direction at this point. I will check out AKKA.NET, thank you for the lead.
Check out some profilers first, Akka.NET is great, but if you don't have a heads up on where your bottlenecks are it won't help you because you don't know what the problem is. Sorry if that sounds blunt, it's a surprisingly hard message to convey to people, so I find just sticking it in there is the best way :) And good luck, I hope the project goes well :)
Try this: https://hashids.org and a sample implementation: https://pastebin.com/0fjHPp9g
A long time ago I had to build an app to process 500mb files every minute with a runtime of 1 minute to 1:30. These were not CSV files, but some proprietary mailbox format files for emails. Taking a look at Task Manager while it was running, mid-level CPU, low IO requirements. Eventually I created a file system buffer and that shot the CPU way up, but processing was done in like 15 seconds. Also look at Kernel Times versus User Time in Task Manager. You could have a different context switch from Kernel to User land and not know it. Profile the code is a good first step or just add some metric solution like StatsD/AppMetrics to the mix to track it down. Profiling is hard work, follow the scientific method and find a problem, try a soltution, remeasure, and if it didn't improve, toss the code. The posted example code with the SemaphoreSlim lock, I would recommend researching more about how .Net handles threads and locking. Locking and Threads are hard, logging will help, but make sure to toss a Guid in the log for each domain transaction so it makes more sense. With optimized code, you would be surprised what one computer can do. I've maxed out 1GB network links before the .Net code could max out the CPU. I would optimize first before spending the time trying to make it distributed because then you have two problems.
ah yeah good point. This part I copied is running in proxies and I only have 100 proxies atm so the idea was to have only 1 proxy active at any one time. I shouldn't have copied it as a snippet I wasn't thinking. I have another process where I send batches and handle each batch like this: foreach (var domain in toCheck) { semaphore.Wait(); taskList.Add( Task.Run(async () =&gt; { try { await RunFullFilteredProcess(domain.DomainName, pm); } catch (Exception ex) { Console.WriteLine("GENERAL UNCAUGHT ERROR: " + ex.Message); } semaphore.Release(); }) ); } await Task.WhenAll(taskList); Not sure if that is any better. I was initially doing everything synchronously and then just played about with things until they went faster. All I can say is doing it this way made it go a lot faster so if its all still running synchronously I must have got speed boost from something else ¯\\\_(ツ)\_/¯ I used threads before once a long time ago, it's not something I usually have to do building basic CRUD sites. I never intended anyone but myself to see the code but now I have to do it properly.
Yeah I have explained this really badly sorry about that. I shouldn't have posted that snippet because it is releasing the semaphore in same block and not inside the called method. The 30 days was because of the http connections in a different process. I monitored my cpu and bandwidth and my bandwidth was only slightly used but my CPU was 70%+ on all cores. That code is pretty convoluted but it runs fast and I can see its using the CPU so I guess I got something in there right.. maybe? I wholly agree my code sucks, just trying to figure out the right direction to move in.
You might want to use TPL DataFlow for this.
Every major Java IDE has the ability to auto generate the getters and setters. Often times it takes seconds to create them.
Thanks, I will check this out.
Thanks this is a good point. My current network usage is low and all cores hover around 70% so my code is probably really bad. I will start with optimizing it before I try to distribute it.
See how .NET Core itself uses DllImports https://github.com/dotnet/corefx/search?utf8=✓&amp;q=dllimport&amp;type=
That's fine mate, don't worry about it. I can really only echo others here saying profiler is the best bet here. Being stuck at 70% all the time can be OK but if it's that intensive then something is doing something it shouldn't be or you're going to need to start doing some intricate performance tweaks. If you can replicate the behaviour locally, I'd recommend Prefix as it's really nice and also free.
Can't. ef6 isn't written in .Net Standard so u have to use core.
So, from the looks of it, you have a large amount of data processed separately by threads (or tasks running in threads). Just from this idea, I see already two big problems. If you run this, for example on a multi-core computer, with let's say 8 cores, that would mean that the optimal number of concurrent threads should be between 8-16 threads. Anything more and you're not gonna achieve a speedup but actually slow down as instancing more threads includes processing more overhead. Secondly, I see that you're running separate threads for each calculation, which again, if you want to use threads would mean that you're spending more time starting the thread than actually using it. Thus, I can propose two solutions for you. The first one is the simplest I would say, and could be implemented very easily by just a few lines of code. Use a ThreadPool. You set the maximum number of concurrent threads and you submit tasks to the threadpool. The threadpool doesn't close the threads it runs, but keeps them inactive when they have no work. Using it will eliminate the overhead of creating threads and will massively speed up your code. I recommend setting again the number of concurrent threads to a reasonable number in respect to the degree of parallelism of your computer (eg. 8 threads for 8 cores). DotNet has an implementation of ThreadPool ready if you want to use it. A second solution is, if you want to use separate threads would be to divide the work data in multiple threads. Like, for example you have 8 threads created and ready to work. Then you divide all data across those 8 threads and you run them separately. This should probably be the fastest option, as you don't create separate tasks for it and remove the overhead of creating the tasks (in comparison to ThreadPool), but this assumes you know the size of the work to be distributed. If you want to use this implementation, i suggest determining first the size of a chunk to be processed by a thread, and modifying the foreach to a for which creates the number of threads necessary, and another for in the thread which processes only the data for this thread. In conclusion, after looking at your code, I would suggest you use Threadpool as the amount you have to modify is minimal. Although, if you want the best solution, you should search for some beginner examples of how to multi-thread big amounts of data. Those examples should provide you with all the info you need to split your data across the threads. Anyway, whichever solution you use, please don't use more threads than available cores, cause it will slow your process down more than using it in a single thread.
Not a tutorial - but probably a good reference: https://developer.microsoft.com/en-us/windows/apps/design
We will be running ASP.NET core on .NET framework initially. 
Convert to EF Core first. There are some differences in Identity that you should take care of first
Well, a lot of it depends on what you're trying to build your UI with. WinForms, WPF, UWP...? &amp;nbsp; The [MahApps.Metro](https://mahapps.com/) WPF control library has a lot of really great Metro-styled controls. [MahApp.Metro GitHub repo](https://github.com/MahApps/MahApps.Metro). It's not *Metro*, strictly speaking, but another really good modern WPF control library is the [Material Design in XAML](https://github.com/ButchersBoy/MaterialDesignInXamlToolkit/) tool kit. &amp;nbsp; If ^(^god ^help ^you) you're using WinForms, the [MetroModernUI](https://github.com/dennismagno/metroframework-modern-ui) library is as close to Metro you're going to get. 
This doesn't make sense. ASP.net Core is a runtime just like .Net Framework. You can't use a .Net runtime "on" another runtime. ASP.net Core is designed to work with EF Core. 
Check the design talks from MSBUILD since 2014 at Channel 9, there are a few of them.
This is not correct. ASP.NET Core is a library/framework and it can run on either .NET Framework or .NET Core. Microsoft chose a naming scheme that is confusing.
Use ef core. Version 2.1 is solid enough.
You can have a look at CoreRT or Mono AOT compiler as well. Why aren't you using C++ then? Native everywhere, very mature set of tools with 30 years of battle field experience, and you can make use of C++/CLI on Windows for accessing .NET APIs. Anything else you will probably regret later given lack of libraries and tooling.
You're right. My bad.
To shed more insight on the linked issue: `[DllImport]` on .NET Core cannot be used to target a native library that has different names on different platforms. The only flexibility it allows is: * Prepending "lib" to the name. * Appending "dll", ".so" or ".dylib" to the name So, for example, all of the following would be matched by `[DllImport("example")]`: * example * example.dll * example.so * libmyexample.so * example.dylib * libmyexample.dylib * etc. Most cross-platform libraries were relying on Mono's "DllMap" feature, which allows that runtime to redirect native calls from one name to another. .NET Core does not support that feature, so those libraries don't work on .NET Core. To help deal with this, I [built a library](https://github.com/mellinoe/nativelibraryloader), aptly called "NativeLibraryLoader". I use it in my own SDL2 bindings ([click](https://github.com/mellinoe/veldrid/blob/master/src/Veldrid.SDL2/Sdl2.cs#L10-L41)) to locate and load SDL2 on various platforms. The native library for Vulkan also has different names on different platforms, so I do similar probing in [my bindings](https://github.com/mellinoe/vk/blob/master/src/vk/Commands.cs#L18-L50) for that as well. By the way, I suggested [this improvement](https://github.com/dotnet/corefx/issues/17135) to .NET Core a long time ago, and it is supposed to be on track for some kind of inclusion soon. That would remove the need for NativeLibraryLoader, but not for manually loading function pointers.
Not sure what platform you are using or what your budget is- but DevExpress's controls make it very easy to copy the look and feel of Microsoft's products.
What kind of stats are you getting. Could you pull all your data into some sort of nosql database and use that for the stats generation? I've been doing a lot of work with elasticsearch recently ingesting billions of records. My thinking is if the hold up is the processing then use a tool design to process data. Happy to give pointers on ES if you think it may help. 
Don't forget for .NET 2.1 you also have to add &lt;RuntimeIdentifier&gt;win-x64&lt;/RuntimeIdentifier&gt; to your csproj file. 
Actually, Petabridge has a Sample that is a [Clustered Webcrawler](https://github.com/petabridge/akkadotnet-code-samples/tree/master/Cluster.WebCrawler). That example may be some sort of jump-start if you go that route.
You were a riot last Friday.
Thx m8 :D I assume that's a compliment :)
Bitly already has a service that can shorten url's and is free, at least for 10k requests per month. You only need to call an API with your security token and url that you want to shorten.
Not a direct answer to your question but here it is anyway. Check out [Writing High-Performance .NET Code](https://www.writinghighperf.net/) by Ben Watson. It is full of methods and advice on how to improve your codes performance and you performance analysis skills.
I made this little snippet a long time ago to process a list in parallel with a given threads. It might help you. It can probably be optimized a bit. If you are waiting for a lot of http calls. Make sure to check ServicePointManager and other .net setting for how to increase the number of connections you can have active at the same time. // Demo class Program { static async Task Main(string[] args) { var count = 10000; var list = new List&lt;int&gt;(count); for (var i = 1; i &lt; count; i++) { list.Add(i); } var result = await list.ForEachAsync(1000, async (item) =&gt; { // Your code ... var delay = 5000 + (item % 5000); await Task.Delay(delay).ConfigureAwait(false); Console.WriteLine($"{item}: \t{delay}"); return delay; }); Console.WriteLine("Done"); } } public static class TaskExtensions { public static async Task&lt;List&lt;T2&gt;&gt; ForEachAsync&lt;T, T2&gt;(this IEnumerable&lt;T&gt; source, int degreeOfParallelism, Func&lt;T, Task&lt;T2&gt;&gt; body) { var sourceList = source.ToList(); var result = new T2[sourceList.Count]; var tasks = new List&lt;Task&gt;(); using (var throttler = new SemaphoreSlim(degreeOfParallelism)) { for (var i = 0; i &lt; sourceList.Count; i++) { var index = i; var localThrottler = throttler; var element = sourceList[index]; await localThrottler.WaitAsync(); tasks.Add(Task.Factory.StartNew(async () =&gt; { try { result[index] = await body(element).ConfigureAwait(false); } finally { localThrottler.Release(); } }, TaskCreationOptions.LongRunning)); } await Task.WhenAll(tasks); return result.ToList(); } } } 
Thanks, this is a neat extension method. I am doing something similar in part of the app but I got conscious of the task list having millions of completed tasks on it so I made it clear all completed tasks from list on each loop. Not sure if that is a good thing to do or not, it probably reduced performance.
Had a quick scan of the Github link. Looks like a very close parallel to what I am doing. I will run through it in more detail once I get some time. Thanks. 
why not just do var jsonTask = GetJsonAsync(...); jsonTask.ContinueWith( /* use UI thread sync context and do UI stuff /* );
If you are building for the web, there is a helpful UI framework you can use. [https://metroui.org.ua/examples.html](https://metroui.org.ua/examples.html)
Are the views all set to Content (so that they get published),?
why not do continue with? public void Button1_Click(...) { var jsonTask = GetJsonAsync(...); jsonTask.ContinueWith(res =&gt; { this.textBox1.Text = res.Result; }, TaskScheduler.FromCurrentSynchronizationContext()); }
Perhaps a backhanded one. I think you're absolutely bonkers. On the other hand I've recently started working with a 10+ year old project that is written in VB6 and uses something not at all unlike Active Events. And I'll be the first to admit it's *amazingly* maintainable for how archaic it is. The only problem is that only half the team gets the concept. For whatever reason, It seems like people who have worked with weakly typed languages get the idea/concept better. (Except, somehow, those who's only weakly typed language is javascript... somehow? People who have dealt with VB6 variants get it. People who have done Lisp seem to get it). 
Have an Upvote, no clue why you were at 0. I *hate* ORMs. But Linq2Db is an exception. Yes, the code is absurdly ugly. *but* it's the only ORM I've found that is: - Free - Handles different RDBMS like a garbage disposal - Has a beautiful, single-trip `Update` syntax OOTB (You can write one for EF, but by the time you have...) There are some things that are annoying however: - If you want something as a Parameter, It really has to be a parameter in your Linq statement. The Engine otherwise has a good chance of turning it into a **safe** string/number/date literal. So no SQL injection risk, but it will pollute the query cache if you do things like `Where(q=&gt;q.Status =="New")`; - This impacted me more when I was an Oracle Refugee, but some of the type mappings were counterintuitive in how you were expected to have a DB Type map up. - The T4 Generation isn't helpful if you want to deal with smaller domain data contexts.
The service that will "speak" with all devices created to support this "standard". Like turning on and off power outlets or lights, or read temperatues around your home. https://telldus.com/
Ah. Interesting.
Interesting library. Why are you exposing raw pointers in those methods? For example: [UnmanagedFunctionPointer(CallingConvention.Cdecl)] private delegate void SDL_GetWindowSize_t(SDL_Window SDL2Window, int* w, int* h); private static SDL_GetWindowSize_t s_getWindowSize = LoadFunction&lt;SDL_GetWindowSize_t&gt;("SDL_GetWindowSize"); public static void SDL_GetWindowSize(SDL_Window Sdl2Window, int* w, int* h) =&gt; s_getWindowSize(Sdl2Window, w, h); could be: [UnmanagedFunctionPointer(CallingConvention.Cdecl)] private delegate void SDL_GetWindowSize_t(SDL_Window SDL2Window, out int w, out int h); private static SDL_GetWindowSize_t s_getWindowSize = LoadFunction&lt;SDL_GetWindowSize_t&gt;("SDL_GetWindowSize"); public static void SDL_GetWindowSize(SDL_Window Sdl2Window, out int w, out int h) =&gt; s_getWindowSize(Sdl2Window, out w, out h); couldn't it? Or at least the public method signature could be?
Have a look at Microsofts [fluent design](https://fluent.microsoft.com) principles. There are various example and demo apps on the page (look at the toolkit). 
Hi guys, it's more of a shortening url like bitly. But I'm not sure if bitly is customizable for my needs such as decorating the short url like the sample I've included in my post.
Thank you for this. Will consider this as one of my options.
I was under the impression that bitly might not be fully customizable such as what I used as a sample in my post since it prefixes the \`bit.ly\`. But fortunately the client just told me it doesn't matter, just make the url shorter so this would probably be the easier option I can find.
We changed the default behavior to not redirect to HTTPS if no port was specified. We made this change due to the default scenario without setting up an HTTPS cert is broken. [https://github.com/aspnet/Announcements/issues/301](https://github.com/aspnet/Announcements/issues/301). You probably need to explicitly set the redirection port to 443 in the HttpRedirectionOptions.
Sure, you can marshal the parameters however you’d like. Personally, I prefer to define them as close to the native version as possible. ref/out versions can be implemented on top of the pointer versions if you want that.
Be careful with material design in XAML. My experience with it is that the performance is terrible.
I got it up and running in a few hours and it looked good and worked well. Then I found out that when you update a site the entire instance gets wiped and the updated one is run. As a result sqlite if totally lost. So then I had to create storage for 2 sqlite dbs. This was a pain in the neck, and after an hour or 2 of working through tutorials, and I had not even got to the connection string stage I gave up. AWS is a complicated, jigsaw of services that are only loosely connected together and more time to set up than its worth.
Look at AWS SDK for .NET
AWS is cheap and you get a free SSL when you use load balancers. 
You can use this https://github.com/mrahhal/Migrator.EF6/blob/master/README.md He also has a library for Identity 3 and Ef6. Works well.
The load balancers in aws will add a header to each request called `X-Forwarded-Proto`. This contains the original protocol of the client. You can use this in your app to determine whether the original request was http, and if it was then perform the necessary redirects. More information can be found here: https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/proxy-load-balancer?view=aspnetcore-2.1#iisiis-express-and-aspnet-core-module Using the forwarded headers middleware will also ensure any redirects in https will return the location with https and not http, as it will update the protocol in the HttpContext class with the values from the forwarded headers. Hope this helps. 
No , technology come and go :) may be it for better One lang for every side of the stack is pretty , lovely Not focusing on the syntax is a dream 
Good article. I like the fact they describe such things as correlation analysis. Its most important to understand the methodology of working with data, language and frameworks are secondary.
&gt; And I'll be the first to admit it's amazingly maintainable for how archaic it is The constructs we have invented since the days of Simula to create more maintainable code, are the equivalent of making wheels rounder by applying more circles to them. If you take a look at my contribution to the programming contest here - https://github.com/polterguy/programming-contest And especially the _"imp"_ folder, which contains the system's implementation, I am confident in that you could understand its _"OO model"_ in seconds, even though it has no _"OO model"_. For instance, here I've separated the UI from the model using folder structures. https://github.com/polterguy/programming-contest/tree/master/imp/frontend/tickets The files are evaluated as _"functions"_, as you can see here - https://github.com/polterguy/programming-contest/blob/master/imp/frontend/tickets/ui/purchase-tickets.hl#L32 I could explain the code to a kid in 5 minutes, who's never done any programming, and he'd probably _"get it"_. If I had tried to do the same with a traditional OO solution, I could easily have wasted years trying to explain it - Without success. Code is communication. Once we have to use half the Latin vocabulary to communicate what we need to communicate, we loose 98% of the crowd, and our ability to communicate suffers. I am not surprised you say what you say above. If you had an open mind, you would realize that the _"amazing maintainability"_ is far more important than those 45+ different design patters, and astronaut architecture implemented in a _"real"_ solution - Which I kind of guess you do at some level ... &gt; People who have dealt with VB6 variants get it. People who have done Lisp seem to get it. Reminds me of a blog I once wrote called _"This is your brain on OOP"_, with a picture of an egg in a frying pan ... ;) OOP seriously damages our brain's ability to think clearly ...
&gt; 2) Do you feel threatened by JS in the backend because you can use 1 language everywhere node etc When they start supporting multi-threading maybe we'll talk. Until then its just a toy.
Absolutely. I’m currently teaching myself JavaScript, and volunteering for every JS assignment I can. My front end skills are mostly WPF and SilverLight. One of those has already been killed and I’m not sure that having WPF as my primary UI skill is a good idea anymore. The backend feels ‘safer’, but still, take a look at the job market - many .Net jobs have some JS in the requirements. I don’t think we can safely ignore JS anymore unfortunately. 
I’ve actually asked similar questions here before, mostly because I had a team lead who was a front end dev who hated .net with a passion and wanted to switch everything over to node for seemingly silly reasons. I found a new job. The thing about development is technologies come and go over time, but nothing really disappears overnight. You see people on Quora asking is ‘PHP dead’ because they don’t realise this. Node doesn’t threaten me because there is a ton of .NET jobs. But let’s say that all disappears and it’s all node jobs. Know what I’ll do? Learn me some Node. All these skills are transferable and if you stick with 1 stack and become proficient I guarantee you’ll be able to learn another easy. It’s also worth noting that just because there is a noisy bunch of people online who hate anything that isn’t JS doesn’t mean that accurately represents the real world. Good luck!
1. I don't feel threatened by JS itself because it's only a programming language. One of many. You can just pick it up if you need it. It takes frontend skills and way of thinking about the problems to work on the frontend. The language is only a way of writing things down. The same goes for the backend, you have got a completely different set of problems to worry about. 2. The argument of having everything in a single language looks good on paper and you do have TypeScript which is pretty much C# for the browser. Where this argument can break is when you have clear FE/BE separation of responsibilities in a team and people are able to pick what they like. 3. In terms of speed and cutting edge, I believe JS is the one that's behind when you look at the backend side of things. 4. I use C#, python, JavaScript and others daily. Its only important to pick the right tool for the task, or be able to learn the one you're forced to, eg. 99&amp;#37; of things for Deep Learning are in python. 
[How would you like to travel today Sir](https://gaiasoul.com/2018/06/26/encapsulating-your-system-through-the-file-system/) Written explicitly just now, inspired by your comment ... :)
&gt; The only problem is that only half the team gets the concept. Psst, maybe you only _need_ half of your team ...? ... just sayin' ... ;)
If anything JS should feel threatened by other languages on the frontend as soon as webassembly becomes popular.
I would have worried circa 3 years ago if .NET has continued to stagnate. But .the development of NET Core is running all cylinders and it's exciting. 
&gt;// My "top-level" method. public void Button1\_Click(...) { var jsonTask = GetJsonAsync(...); textBox1.Text = jsonTask.Result; &gt; &gt; MessageBox.Show("Done"); } How then do you propose to schedule the call to the message box \_after\_ the Result is done? That literally means \_rewrite\_ the handler to support async properly. And what about other contexts besides the UI? Like [ASP.NET](https://ASP.NET), console, games...what to do in that case?
More like until the tooling stops threatening itself
how big a deal http/2 is? Most people would have an http/2 capable proxy in front of kestrel anyway
\&gt; API client generation (C# &amp; TypeScript) I'm very hyped for this, currently I'm using NSwag to autogenerate my typescript angular API services, but it would really be nice to do this out of the box.
&gt; One of those has already been killed and I’m not sure that having WPF as my primary UI skill is a good idea anymore. I am 100% confident in that it's not in fact. I'd definitely brush up my skillset here if I were you ... ;) &gt; I don’t think we can safely ignore JS anymore unfortunately create-widget element:button innerValue:Click me! onclick create-widgets micro.widgets.modal widgets h3 innerValue:Header p innerValue:This is a paragraph ... not so sure about that one though Sir ... ;) 
Yeah, I’m on the case. Trying to pick up WebAssemby, TypeScript, and JS. As well as common platforms / frameworks. My day job is .NET still, but I’m working with some really talented JS devs on a BlockChain project, so making some good progress I think. 
There's this sniper he's freaking accurate. Last time he uses a M-16, and then comes the Armalite, afterwards the Barrett, but no matter what rifles he uses, he's still freaking accurate.
I think they are working towards making Kestrel a full fledged web server so we don't need proxy anymore.
How do you know if someone uses Dapper? They'll tell you ;-) 
I hate code generators... i might make an exception for an api.
Still no visual designer in sight. :(
Not really sure how to fix the issue here. I've been looking all over but no luck finding a solution for him. He only asked me because he's stuck but I've never worked with ASP or MS ACCESS Any sort of help would be greatly appreciated. On his site the links in articles and blogs are broken and I can find a way to redirect them to get them to open the PDF documents. 
I usually agree, but it's just such a pain in the ass to keep your Server and Client model and services classes synchronized and always write the same boilerplate code.
AWS ELB use a random IP in your VPC subnet and by default the aspnet modules only trust [127.0.0.1](https://127.0.0.1) by default so you may still have issues. So you can either configure your app to trust the whole subnet and firewall off everything, or clear knownproxies and firewall off everything. I stumbled into this, and the docs don't bold it enough in my opinion and had to answer my own question. [https://stackoverflow.com/questions/43749236/net-core-x-forwarded-proto-not-working/44390593#44390593](https://stackoverflow.com/questions/43749236/net-core-x-forwarded-proto-not-working/44390593#44390593)
I use JS only for some additional front-end work (e.g. Aurelia or Angular framework). On the other hand, I don't feel like I will ever feel threatened by a language that has been poorly designed from the beginning. It's a nice language to work with, especially once you start using ES6/2017 or Typescript, but not a good choice for some serious back-end development.
This is a big deal for performance across distributed systems that use asp.net for internal services.
1. Decent in Javascript. Can hack me some Angular 2+, jQuery, Knockout, etc. front ends well enough. 2. No, not threatened by JS because JS is single-threaded. While Node hides this well with async-everywhere, it's still single-threaded. The future is more cores and more threads, so seems like a fatal flaw for JS if you ask me. 3. .NET is way ahead in everything not-web-front-end: desktop, mobile, games, back-end, and soon, it'll take over the front-end with Blazor. :) Typescript is basically C# for the front-end already. .NET Core runs on everything. 4. Java is dying at Oracle's inept hands. C/C++ is old school and useful when you need to be close to the metal. But C#'s Span&lt;T&gt; and other continual improvements are giving these old stalwarts a run for their money. If anything, the C# rocket is taking off using the open source fuel of .NET and will dominate. C# makes programming fun again. The widespread use of Java in big data (Hadoop/Spark) is curious considering C#/.NET has so many of the basic constructs ready to go without needing Scala. No doubt it was aided by the closed source nature of .NET and the more relatively open Java, but now that .NET is open source and Oracle is squeezing end users for cash, I think a sea change is on the horizon. I'm working on a C# big data solution myself, and I think it's promising.
Check out Cosmos DB if you're trying to find an API which exposes databases 
It would be nice if they could help the third party EF providers to get MySql working on 2.1
It will be helpful to read the asp.net core docs. They have tutorials on how to do closely related things.
Using the command line to do things makes me feel like I have gone back in time.
Eh, I prefer it a lot because I get a really good sense of what is going on in my project. It forces you to know more.
I have found scaffolding generally better to work with compared to continuous code generation (assuming that is the case here).
And here I made my own client generator... Nice they are adding an official one, maybe they can add updated vsts clients while they are at it...
repo?
He wants to do it with web forms so it wouldn't really help him. Not saying web forms is a good idea though.
Good point
Have anyone figure it out how to inject a ViewComponent scripts and styles into the layout sections?
What is the error for the broken links?
Good luck :)
You want to learn **Angular**, which is basically version 2 or higher. AngularJS is pretty much in a [maintenance-only period now](https://blog.angular.io/stable-angularjs-and-long-term-support-7e077635ee9c), hence why the resources you see on it are older. Here's a good place to read up on the newest Angular: [https://www.reddit.com/r/Angular2/](https://www.reddit.com/r/Angular2/)
Doesn't http/2 have features that you specifically need to program for? I'm pretty sure the connections reduction doesn't automatically work through proxying.
Aspnet core one is a Work Repo, can't share it here. Was making one for Blazor that will be public, should have that out before the weekend,if you care about that.
Yup, i am tending to agree. I still feel dirty doin it
HTTP error 500
this could be any one of a dozen things; a 500 error is a server-side error. If you've moved to a new provider, this is possibly due to some permissions not being set server-side (assuming that the code used to work, of course). If you share the code for ViewArticle.asp, we might be able to better assist. Incidentally, the .asp extension means that this is technically not a .NET web. : )
That means the server is hiding the error. He needs to hire someone to fix this bug. I don't think people appreciate getting asked for free work on a business site.
Thanks for trying to help me out. I’ll have a better look into asp I’m just a bit confused with the way this site is setup. I don’t think he wants me to share the files etc. 
Oh no it’s not like that I told him I’d do it for him cause I’m doing a bit of web design at university. I think it’s just more complicated than I thought. I was just going for the teach yourself method. 
&gt; OOP seriously damages our brain's ability to think clearly ... It really can and does. I had a lot of schooling that was around OOP, but when my first programming opportunity came along, it was nothing but (auto)Lisp. It was a hard crash-course in remembering how to think about things in a much more simple way. When we ported parts of the lisp functionality to .NET (We started using a drafting package that didn't have LISP built in, but had COM Bindings all over the place) the amount of code required to do the same thing (even ignoring the COM part of it) was certainly larger. I guess I'm still on the fence about OOP. It can be useful for a *certain* subsets of functionality. For instance, if I'm writing something with database storage, it *might* make sense for me to have an abstract class that does normal ANSI SQL for methods, and perhaps leaves certain specific methods to an implementation (ever done a Top or Skip-Take query in Oracle before 12C? It sucks.) Or perhaps for some Crypto Libraries where you're Really, truly reusing common functionality. But those really do seem more like edge cases than the norm. But, in general the .NET community seems to have fear of moving closer to KISS. Oftentimes we lie to ourselves and attach libraries that compile to the size of Megabytes (EF6) in the name of 'simplicity'. And yet in the corporate world I've never seen (at any shop I've worked at, anyway) an EF implementation that didn't have a repository layer so complex/convoluted you wonder why they bothered in the first place. Keep things *simple*. 
Why of course!
I don't really think a fully fledged web server deployed in process is a particularly great idea, and I doubt that's their plan.
I'd love to see ur blazorz
&gt; I've never seen (at any shop I've worked at, anyway) an EF implementation that didn't have a repository layer so complex/convoluted you wonder why they bothered in the first place An unnamed company I used to work for had 1.5 million LOC in a project they used to call _"acme common"_. Everybody referred to it as _"acme garbage"_. I doubt there was a single line of code I would even consider using from that thing. I believe this is a general pattern out there, and I doubt this company was special in these regards. The biggest problem is fear of throwing away things you've invested so much in, since this is the equivalent of accepting _"loss"_. So they keep their anchors at the seabed, believing they'll somehow magically move faster - Even though everybody who knows anything about literally anything can clearly see that the _"King is naked"_. Complexity is the problem, never the solution ... There is much wisdom in your comment ...
1) I'm 100% sure somebody who spends all their time in Javascript is more capable with it than I am. 2) No. If JS was all together better than C#, I'd focus more on JS. IMO nodejs is fine for smaller, simpler, and more focused services but I don't think it's as suitable for building large systems compared to .NET. 3) I'm certainly a bit biased but I believe .NET is the best technology for building applications. 4) I could easily see myself changing technologies if I felt something else was better. 
&gt; It can be useful for a certain subsets of functionality Psst, I am not of the opinion that OOP is always inferior. I agree with what you say, that it is _sometimes_ beneficial. However, it's clearly seen much more action in our industry than it deserves ...
The big data products started in the Java world, and I'm guessing that most of the projects are probably somewhat derivative of each other.
yeah i don't think the ASP.net core team wants to put kestrel through all the auditing required for them to feel comfortable to say it doesn't have to be reverse proxied for one 
Honestly, this is still my preferred mvc-vm pattern. mix in the server pre-compiled views &amp; release mode (cache-safe &amp; minified) resource bundling and I don't see what Angular2(6) is improving.
Have you looked into EPPlus? I know it can parse Excel spreadsheets.
u might want to rethink angular, vue is gaining more traction, on parity with react or even surpassing it in new github repos. but yeah, the learning curve is steep. u have to learn typescript plus angular. ah u want to learn angularjs, i would avoid this. go with angular 2+, ideally 6, if you want to learn angular. otherwise consider vue.
Check out Vue.js. Can be used very similar to Angular 1.
Even if they did, does it even make sense to do that? Do we want that much overhead on every single service?
Thanks for your help you helped a lot! One more question, when I try to search for a barcode (I work as intern to a company) like "5705831010727", I get no results from the API, but I get some results from normal googling. Do you know any way I could fix that? PS: how+to+suicide is not a joke
Don't use Task Run. Use Async Await all the way down, so you do not block threads when doing http requests, and do not need locks. 
Model-first was messy so I understand they wouldn't want to focus on that.
Might be, but there are lots of enterprise shops that were we need to integrate DB designs done by the DBA team. I surely don't want to write boilerplate by hand, so we keep using EF 6 in such cases. If I need to write boilerplate by hand then I just use Dapper.
Azure is next. Amazon was by far the hardest of the big 3 to get up and running.
So, maybe try being more specific? Some random number is confusing. Try asking for: **q=barcode+5705831010727** or if you are developing some app for checking barcodes, look for API to check barcodes. Googling them might not be the best
What kind of view you think about? Is it editable excel like - Excel Online (MS) or Googles' something? Or is it just a grid with values? What about charts, cell style, other sheets? What do you exactly need?
Hands on table looks good. [handsontable.com](https://handsontable.com/) 
I enjoy VSCode+CLI. There is something that feels simple and fast about it. Knowing the CLI also came in handy when setting up CI/CD.
We are using [https://github.com/reinforced/Reinforced.Typings](https://github.com/reinforced/Reinforced.Typings) in multiple projects, works nicely, but it is hard to customize.
Have you considered a full ORM product like LLBLGen?
May be you could have a look at this project: https://github.com/Shazwazza/Smidge
unfortunately it won't work. We are talking about components here. Sometimes you just want to have a localized style for the component and some simple javascript to localize it. That means putting the style at the head and javascript at the bottom of the HTML layout. It's not doable using ViewComponent right now.
Comments like this are why I hate javascript frameworks haha
I'd also recommend Vue (or react, but I prefer Vue) over ng.
Are you being bottlenecked any of the stuff mentioned in this artle? https://support.microsoft.com/en-us/help/821268/contention-poor-performance-and-deadlocks-when-you-make-calls-to-web-s The maxconnections setting bit me one time only allowing 2 connections to a service/url at a time
Serious question: why use this over system.configuration that’s there in .net standard 2.0? 
Interesting. To evaluate Intellicode and Roslynator - is enough to disable R# code analysis or must R# be uninstalled? We have been struggling with perf due to extreme numbers of solutions / files in our solution files. Most devs are not using R# as a result, code is not clean :(
You can read Excel with JET and Oledb. I did something but the other way around, I was writing from datatable to excel, check out this url https://dejanstojanovic.net/aspnet/2018/february/export-dataset-and-datatable-to-excel-with-c/ it might give you an idea how to do it
Sorry for getting back to you so late, been out of office carrying only a MacBook with Parallels which is always slow haha. Anyway even on this one after upgrading to .NET Core 2.1.1 I've seen a huge performance increase and build times are now back to what they were on 2.0 or better (hard to tell on this device). I dont know if the patch was in the 2.1.1 release, or if upgrading just everything solved the problem. Nevertheless performance is great again, thank you!
Vue is great, and I hate js, but I love Vue
Many times (especially when working on legacy projects) you have to read and even write existing to .ini, .cfg, .conf, .cnf files Example: my addins for MonoDevelop / Xamarin Studio / VS for Mac / Unity 3D / Notepad++ Vertical AutoCAD/Revit applications (for reading revit.ini) Fallout / TES mod tools and mods (configs are ini) etc Obviously that for new apps one should use JSON/Yaml/.env files etc
You can profile the results, in fact you should profile the results and the code before so you have a baseline. That said, I'm looking into Span/Memory at the moment for some of our code, and even Buffer of old, but I'm still figuring out the best use cases too. 
&gt; When is it better to simply use an array instead? As far as I can see, it's better to use an array when you want to copy the data. If you don't want to copy the data, span makes much more sense. However, this is all performance. There's still the issue of optimising too early. If your API or whatever returns in 20ms, switching to span might shave off a couple more ms but is that worth it? Does that affect you? I'd say the biggest pitfall is simply getting bogged down with using them for the sake of them.
Yeah, why would you commit changes for performance without measuring?
No, because EF6 designer is free with Visual Studio. 
i hate js with a passion, FE guys dont understand. its got a shitty type system, and the prototype way of building classes just fucking sucks to do it right. at least there is Typescript to save the day. the generated output, i would never dream of coding by hand, yuk.
The microsoft.extensions.configuration does not have persistence yet (or maybe ever https://github.com/aspnet/Configuration/issues/385), so yours does provide a way to save new config values over the standard. Not sure about the other file types but there is an official .ini provider for reading those types of files though https://www.nuget.org/packages/Microsoft.Extensions.Configuration.Ini/
One use is for slicing arrays. If you write an API that consumes a bunch of values and the parameters you take are `int[] source, int startIndex, int length` then not only can someone pass in bogus arguments (out of bounds indices and such), there is also nothing preventing you from accessing parts of the array that you weren't supposed to touch. If you take a `Span&lt;int&gt;` instead, these concerns go away. You don't have to check that the indices are valid, and you cannot access any part of the array that the caller doesn't want you to. Additionally, with a `ReadOnlySpan`, you can't even *modify* the data you get access to. This means that the caller doesn't have to do any defensive copies of his array to make sure that nothing modifies it. Another use is for safely passing pointers to memory that is otherwise unsafe. If you `stackalloc` a buffer or get a pointer to unmanaged memory, then you can't use any methods that accept arrays as inputs with this memory. However, any method that accepts a `Span` instead *could* operate on this memory, because you can create a `Span` that wraps it. https://msdn.microsoft.com/en-us/magazine/mt814808.aspx Is a nice article that elaborates on why Span exists and how it works. 
Exactly + my implementation does its best to preserve original formatting (indentation and such) It supports arrays, advanced boolean handling etc. In fact I'm planning to implement Microsoft.Extensions.Configuration abstractions, so devs can drop it in easily with dependency injection
As the saying goes: First make it work, then make it right, THEN make it fast. Still, always good to understand the differences
If the devs are not using R# and haven't replaced it, the quality is going to be low. I'd advise having half the team mandated to use R# and half Roslynator and Intellicode. See the envy kick in, and allow people to swap sides after 2 days.
I see your point, but I have to ask an obvious question. Since you control both ends of the API, why not create a batch endpoint that can serve your needs with a single request and single database query?
I would be very interested in what kind of best practices you follow while doing JavaScript. I would hire you if your JS code would look clean and maintainable. Testing. Tools. If you learned how to write great JS you can learn to do it in another language too.
Thanks for the reply. That sounds pretty reasonable. 
And try to have a general idea of what the differences between C# and JS are, what .Net is. Don't try to be smart but let them know you've put some time into understanding what you're about to do. Do a Hello World in Visual Studio Express.
Pomelo has an RC package released for 2.1 last time I checked
Well I think what I'm realizing is that using Span and ReadOnlySpan as returns from protected/private arrays is a great way to go. BUT, I think it's futile to return Span or ReadOnlySpan if the method you are calling is generating an array for consumption. Best to just return the array if nothing else has access to it. I was going a bit overboard in trying to use ReadOnlySpans for everything when it simply isn't necessary. 
Good practices aren’t exclusive to any particular language. Good, clean, easy to read and maintain code is what’s important. A languages documentation and syntax is readily available whereas good practices are something you apply. So long as your coding style is good it shouldn’t matter too much if you’re unfamiliar with .net so long as you’re willing to learn and you’re upfront about your current knowledge on .net. I made an error in the beginning of my career by studying the low level workings of .net (the clr) and how everything works behind the scenes, only later did I move onto clean code and the latest/best practices (which mind you always evolve because there will always be a better, cleaner solution to any problem). I find best practices has helped me way more than any understanding on how the .net clr works. You’ll do well so long as you understand good practices, have strong refactoring skills and are eager to learn. The one thing that is amazing with the developer community is that we all share a love for development, there are other higher paying jobs but we do this because we enjoy it. Convey your passion for development and you’ll be fine.
Setting that to int32.max feels pretty dangerous. You could chew through the servers resources if things got out of hand there.
Agreed, if you’re passing an array somewhere with the intent of transferring ownership of it, just pass the array rather than a Span. Spans are for giving temporary access to a bit of memory.
What other people said is good. Also, find out what frameworks they're using in .NET and try getting somewhat familiar. If it's web development, it could be ASP.NET MVC, Web Forms, or Web Pages. Or ASP.NET Core MVC or Razor Pages. Soon it could also be Blazor. 😁 MVC is the most typical in .NET web development, so it'd probably be worth reading some tutorials on that. Download Visual Studio Community Edition (free) and fire up an MVC project template that comes with it to get a quick overview of how it's structured. If they're doing desktop development as well, they're likely using WinForms, WPF, or console. UWP is a thing, but not many companies use it.
 I'm getting really tired of these low quality medium articles... alright, let's do a simple test: API: `[HttpGet]` `public ActionResult&lt;IEnumerable&lt;User&gt;&gt; Get()` `{` `return new User[0];` `}` Console: `using (HttpClient client = new HttpClient())` `{` `var tasks = new Task[1000];` `for (int i = 0; i &lt; tasks.Length; i++)` `tasks[i] = Task.Run(() =&gt; client.GetAsync(new Uri("http://localhost:5000/api/values")).ConfigureAwait(false));` `Stopwatch watch = new Stopwatch();` `watch.Start();` `await Task.WhenAll(tasks);` `watch.Stop();` `Console.WriteLine($"Watch Elapsed: {watch.ElapsedMilliseconds}");` `}` A thousand API calls returned in `Watch Elapsed: 214` 150 returned in `Watch Elapsed: 48` There's something else wrong with your code. After applying your changes, I actually saw an increase to: `Watch Elapsed: 386` and `Watch Elapsed: 56`
Tests like this against Localhost are not always the best for measuring real world changes (unless real world changes really do involve looback). 
It's just asking to be DDOSed. There is a healthy medium between '10' and '2 billion'.
So there's some stuff here that's right, and stuff here that's wrong. `ServicePointManager.UseNagleAlgorithm = false;` This is actually somewhat dependent on the size/type of your requests. If you're doing primarily smaller requests, it's a good thing. If you're doing terrifying things like directly feeding streams through (say a file server API) I would be a lot more careful; lots of stalls on the input end will cause extra chatter/overhead on the network. `ServicePointManager.Expect100Continue = false;` Torn here. If my API is somehow choking out this could just make things worse all over the network. `ServicePointManager.DefaultConnectionLimit = int.MaxValue;` LOLWUT? No. No. There's a whole bunch of reasons this number doesn't make sense, going all the way down to the thread pool. For what it's worth, the default values **are** absolute shit on a console app, doubly so when you think about how SSL can amplify this number behind the scenes. In my real world experience, the optimum number is going to be around 12-16. This is a generalized number for requests that were subsecond on the backend, and would return a smaller (&lt;128kb) set of data. `ServicePointManager.EnableDnsRoundRobin = true;` This MIGHT be a good idea, or it might throw whatever load balancer you have in place for a loop. I'd be a little careful, not to say it's outright a bad idea. `ServicePointManager.ReusePort = true;` Ummmm I think this can be a double-edged sword. On one hand it will save claiming ports. OTOH it can lead to having to wait for the shared port to be open. (IIRC. I've never tried this one much.)
Another pitfall is currently it makes your code harder to port to dotnet standard, dotnet framework or xamarin. 
 Put it on a $5 linux box on other side of US, change the uri to `new Uri("http://http://206.189.169.73/api/values")` and it's the same thing. `235`, `74` without, `271`, `89` with his settings. I'll leave it up for today 
I used to do hiring for entry/junior level developers that would primarily be on the MS stack; but we didn't care if they had experience with .NET As long as you've been honest about it, I wouldn't worry about trying to cram in a bunch of .Net facts or anything. We'd ask you a bunch of questions about Node.js or whatever you feel the most comfortable with, ask you to talk us through a few basic problems in whatever language you want or even pseudo-code. The only .Net/C#/VB.Net/whatever questions we'd ask would be to make sure you aren't against the idea of working with them. &gt; "So have you done much C# development?" No, I haven't had a chance to do much of anything with it, but ever since I heard of NET Core I've had it on the back of my to-do list to learn more. If you can show us that you are open to the idea of learning .net (bonus points if you seem a little excited about it) and that you've done even the smallest amount of research. It doesn't have to be core either, just anything about .net or C# in general would be enough for us to check the 'Seems like they'd be cool working with dotnet' and as long as the rest of your stuff is solid you'd get the offer. Obviously, every place is different, but that's what we did. Best of luck on your interview. 
It's still not a good real test. (Note that I'm not saying the Medium article has great settings.) - Your API is returning a static call. No DB or real world logic is involved. This can impact benchmarks of HTTP by a decent amount, especially when things like `async` are involved because a lot of the extra machinery that winds up being involved in parallel web-calls doesn't get hit... - Nit-pick here, but per above and my other comment the Threadpool is going to be a limit one runs into with the absolutely stupid settings they gave. - Another Nit-Pick, your call is not HTTPS. This causes additional connections/overhead versus an HTTP Call, and `DefaultConnectionLimit` is more likely to cause you issues in this use case. Optimizing this stuff can be tough. In a separate reply I gave settings that I've found are a 'best-all-around' that came from optimizing a number of parallel WebAPI calls between two separate servers as well as a localhost server. But those were done after measuring *our average use cases* as well as *real world testing*, and are almost certainly not perfect/ideal for every scenario. 
* That's exactly what you're supposed to do when testing the speeds between two different things. You limit all outside factors that can cause discrepancies, which is why localhost is nearly always the best choice for speed tests in terms of connections. * Threadpool has a limit of 1000 i/o threads... it's not going to be a blocker here. * It's a single expensive connection and then nearly negligible on this amount of requests... and again, as the first point, you're adding more over-head to what he is trying to test against. You ALWAYS want the least amount of moving parts when testing efficiency between two things.
&gt; 1) How good you guys considered yourself to be in Javascript compared a frontend JS engineer who do just that ? Meh. I'm probably not on par, with, say, a JS guru. I'm able to maintain javascript libraries and add functionality with no problem, but there's plenty I don't know. &gt; 2) Do you feel threatened by JS in the backend because you can use 1 language everywhere node etc I can do just about everything in [C# or F#](https://websharper.com/) as well. Heck you'd be surprised what can be done with custom ViewEngines and Razor. &gt; 3) Do you think .Net still can compete or its "behind the curve" compared all the JS frameworks ? cutting-edge, features, speed etc that kind of things Okay, now I know you're either fishing for marketing material, trolling, or just are absolutely clueless. I don't know what in the JS world would be 'cutting-edge' compared to C#, but I can think of plenty in C# that is still cutting edge after 10 years compared to JS. That goes for in general as well as features. Speed is somewhat use specific, but for most things .NET is fast enough if not faster than Node. &gt; 4) Do you guys see yourself programming different things in 5-10 years in another field outside of webdev like database or using another language like Java, Js or even going native with C++ ? I only 'webdev' because it's part of the job description. Normally I am a backend arcanist. I can write enough C++ to know I probably shouldn't, but at the same time .NET is going to be far more accessible to people who will have to look at my code after I'm gone. I *can* write Java (lol, fixing "The Java Devs' code" at my last job) but would rather not. C# is a good compromise of expressiveness with functionality. Javascript has been so painful on the frontend that we have 20 different ways to hide from it. There's TypeScript which compiles to JS. There's WebSharper which essentially transpiles C#/F# to JS. I'm sure there's a Go to JS compiler (but, lol no generics). If a language wasn't utter shit, why would so many people be trying to get away from it?
I feel like being contrarian. &gt; 3) I'm certainly a bit biased but I believe .NET is the best technology for building applications. Microsoft pours a ton of resources into it, and it shows. If you follow this you will eventually fail painfully. .NET is an amazing platform for building a LOT of applications. But you'll have to be sure to keep your toolbox open. And *never* depend on Microsoft for the Next Big Thing
I'd say the biggest pitfall is the stack-only nature of `Span&lt;T&gt;`. It means what you can do with it is significantly limited: You can't store it in a field of a `class` or a `struct`(only in a `ref struct`, which has the same limitations). You can't use it in an `async` method. You can't easily use it from a lambda. And so on.
Interesting that you pick apart answer #3 and totally ignore point #4.
Would HttpClientFactory help?
The authentication handling looks pretty rough. You could try using the identity framework helpers to get a jump start.
I get what you are saying, but a more accurate test for your dummy service would be have random thread sleeps between 100 and 2000 ms. This would over complicate the test.
why is compiling to native on mac/linux a requirement? perhaps they are worried about dealing with mono? (is this what you mean by "hard to install requirements?) if you're writing for net core, you can target the platforms directly, includes the runtime and everything. called self contained deployments. it outputs something you can just run. i dont know how packaging on osx works, but for linux you can literally just copy the directory in and it just works. (the caveat is that if you're doing anything with ui you're going to have a hard time doing xplat with. net. stuff is out there, but not particulalry mature)
I'm assuming the file is already gzipped and is sitting in S3. This reads a csv file and gzips it first, then reads the gzipped file and processes the output line by line. Instead of using the `using(var input = File.OpenRead("test.csv.gz"))` line, you would read from your response stream. using System; using System.IO; using System.IO.Compression; using System.Text; namespace GZipTest { class Program { static void Main(string[] args) { var buffer = new byte[1024]; // create the gzip archive using (var input = File.OpenRead("test.csv")) { using (var output = File.Open("test.csv.gz", FileMode.Create)) { using (var gzip = new GZipStream(output, CompressionMode.Compress)) { var bytesRead = 0; do { bytesRead = input.Read(buffer, 0, buffer.Length); gzip.Write(buffer, 0, bytesRead); } while (bytesRead &gt; 0); } } } // read the gzip stream and process the result line by line using (var input = File.OpenRead("test.csv.gz")) { using (var gzip = new GZipStream(input, CompressionMode.Decompress)) { var bytesRead = 0; var stringBuilder = new StringBuilder(); do { bytesRead = gzip.Read(buffer, 0, buffer.Length); for (var i = 0; i &lt; bytesRead; i++) { switch (buffer[i]) { case (int)'\r': break; case (int)'\n': // process the line here or get fancy and do yield return var line = stringBuilder.ToString(); stringBuilder.Clear(); break; default: stringBuilder.Append((char)buffer[i]); break; } } } while (bytesRead &gt; 0); } } } } } 
you don't save money on cloud because it is cheaper per cycle or per gb of ram. you save money because you don't need a person (people) to manage your physical infrastructure. for ultra small places, this may not be a big deal, but still, you have reliability, scaling (scaling down is a real thing too), and your time can be spent making yoyr product, not duct taping over cords to make sure they don't come unplugged. 
We’re still in a data center where they take care of the hardware, it’s just that the hardware is dedicated to us 
.net core works great everywhere. we use aws via lambdas and linux containers (hello alpine in 2.1!). just evaluate what works for you. these days aws and azure are very mature, so you really can't go wrong either way. since net core is ms, you can be sure azure will support it. but aws has great support for net core, and their sdks target netstandard of some flavor, so compat is great. I'm actually surprised at how good the aws sdk for .net is. they also have support for net core 2.1 on lambda coming soon too. aws supporting net core is a pretty big deal, it means they have enough demand that it is worth supporting for the long term. the xplatness of net core is so awesome.
don't underestimate how nice it being open and on github is. don't understand something? "ClassName github" on google and bam that's how it works. i use this all the time. 
mono perf isnt great either. if you use framework, i agree, use windows server. core is xplat as a principle, so perf is good everywhere. i think it is still better on windows, but they are being addressed, like the new managed httpclienthandler and dropping the deoenedency on libuuid. 
they are two different operations on a stream of bytes. one that compresses, one that encrypts. conceptually they are different, and they algorithms they use are disjoint. just do one and then the other one, and the opposite in reverse. 
agreed. two kinds of transforms, so two steps. 
at that point it is a sunk cost, but i suspect it would be cheaper to run hosted. transition costs can be very large though, so that can be a limiting factor.
Why did you do ```tasks[i] = Task.Run(() =&gt; client.GetAsync(...)``` and not just ```tasks[i] = client.GetAsync(...)```?
Ado.net works, too just need to use .net standard/core 1.1 + It's not terribly difficult to use. This repo https://github.com/jhealy/aspdotnetcore has some examples. Just return the data table to your view and format it as an html table.
You really will want Core2.0 for ADO, the 1.1 version is missing a huge amount of the System.Data classes.
That's what I'm saying. How did this not play a factor here?
I pass in a singular client, so without ConfigureAwait it will run using the same context as the main thread, basically making the code run synchronously, and because I use ConfigureAwait it is a ConfiguredTaskAwaitable, not just a task, so I have to tell WhenAll that this is indeed a task that needs to be ran, so it will create threads for all of them.
You could give Dapper a try, which is basically ADO.Net on steroids with a mapper... https://github.com/StackExchange/Dapper
&gt;”may be security risks” Its more than a maybe, you’re essentially allowing users full access to your database. 
This article is more up to date, it's worth reading both. https://www.codemag.com/Article/1807051/Introducing-.NET-Core-2.1-Flagship-Types-Span-%5BT%5D-and-Memory-%5BT%5D
Better idea: Create a service that doesn't require 15 requests to be made to get the data you want. If that is that tightly coupled you should consider breaking that up into multiple services so you can scale up whichever one is causing the performance hit.
I'm more referring to why you are explicitly running these truly```async``` IO calls on the ```ThreadPool```. You don't need to run ```client.GetAsync``` on another thread, i.e. ```Task.Run()``` is really wasteful to run async IO call. You can still do ```task[i] = client.GetAsync(...)``` and then when you _actually_ `await` them, then you ```ConfigureAwait()```. So it should rather be: using (HttpClient client = new HttpClient()) { var tasks = new Task[1000]; for (int i = 0; i &lt; tasks.Length; i++) tasks[i] = client.GetAsync(new Uri("http://localhost:5000/api/values")); Stopwatch watch = new Stopwatch(); watch.Start(); await Task.WhenAll(tasks).ConfigureAwait(false); watch.Stop(); Console.WriteLine($"Watch Elapsed: {watch.ElapsedMilliseconds}"); } 
Google HttpClientFactory.
Is there a resource that teaches unit testing with this philosophy? So far I only found the 'mock the database because unit testing' approach and I always gave up on it because it resulted in brittle tests that didn't really test anything anyway.
Depends on the permission for the user used to make the connection.
https://docs.microsoft.com/en-us/ef/core/querying/raw-sql Or you can get familiar with expression trees and build the LINQ query in code and pass that to EF.
 What you're doing is completely different, run them, yours will take almost 4\* longer to run
FYI, I recommend a 64KB+ buffer for 2018 datacenters. 
 No it wouldn't, that's even worse. One, you're testing efficency of API calls for integration tests using httpclient on the client side, that completely skews the numbers in terms of benchmarks.
Using `ServicePointManager` I'd guess its full framework; also from the code at the end `System.Web.` its not aspnetcore
I just did. I'm not sure where you are getting your 4 * longer stats from but that is utterly ridiculous. My tests hitting `new Uri("http://206.189.169.73/api/values")` runs quicker than your code. My code is doing **exactly** the right thing and that is the right way to make those calls. Trying to run a 1000 Tasks on the ThreadPool is shocking and if you had that in web code, your scaling will be a joke.
I gave a solution in the comments. There is a package on Nuget for ASP.NET Core MVC to do that, which we made for the Orchard Core Framework to let any part of the application add scripts to the main view. Documentation: https://orchardcore.readthedocs.io/en/latest/OrchardCore.Modules/OrchardCore.Resources/README/ Video: https://youtu.be/-CX98nfXfo0
You're right, yours isn't 4 \* longer, it gets MUCH worse the more tasks you add, at 100, it does create a context specifically for it. At 1000, it puts them on a single context. [https://imgur.com/c04cb9ab-bd4e-48bd-a4f7-4e2db7024eb4](https://imgur.com/c04cb9ab-bd4e-48bd-a4f7-4e2db7024eb4) You clearly didn't even test the code lol.
HttpClient has a DNS cache and other caches within, therefore you should partition the instances by the context in which you are executing. For example, if you're talking to an HTTPS endpoint, to avoid the handshake each time you make a request, you should reuse the instance.
HttpClientFactory if you’re using 2.1 Also I’d recommend making your Post method async and then await httpClient.PostAsync, that or use httpClient.PostAsync.GetAwaiter().GetResult(). https://github.com/aspnet/Security/issues/59
ServicePointManager and System.Web are both in .net core as well, it probably still is framework, but it doesn't prove otherwise.
 You should always reuse the instance, regardless of what you're doing. There's a lot more than just DNS, but yes, please always use a single instance or httpclientfactory.
Thanks for clearing this up. I was focusing on the HttpClient, not the factory.
Nah mate, I have [tested](https://imgur.com/a/pPRnU9F) it. Seems like we can go back and forth all day long. Your code does not scale and makes no sense. Why on earth would you run an async operation on a background thread, it makes no sense. Anyway, you do you.
I've no idea what a batch endpoint is, can you please explain?
 Out of curiosity, can you do the test again and wrap your entire httpclient with the stopwatch. When I run your code in Visual Studio, instead of directly through dotnet run, it kicks off most threads on the actual client.GetAsync() creation. Not that I'm doubting your picture, mine shows even more significant differences: [https://i.imgur.com/HzkaaGQ.png](https://i.imgur.com/HzkaaGQ.png) However, wrapping the entire client in the wrapper showed mine to be faster... only partially, and I know it's because of debugging (compared to straight dotnet run) but it significantly slows down both of our solutions. Mine also creates 208 threads, while yours only created 5 However, kestrel reports no failed requests, and it didn't even get close to bandwidth or CPU limits. It didn't crash because of the server: [https://i.imgur.com/esxqNZm.png](https://i.imgur.com/esxqNZm.png)
I think it does public class WebApiApplication : System.Web.HttpApplication { /// &lt;summary&gt; /// Application start /// &lt;/summary&gt; protected void Application_Start() { AreaRegistration.RegisterAllAreas(); GlobalConfiguration.Configure(WebApiConfig.Register); FilterConfig.RegisterGlobalFilters(GlobalFilters.Filters); RouteConfig.RegisterRoutes(RouteTable.Routes); BundleConfig.RegisterBundles(BundleTable.Bundles); XmlConfigurator.Configure(); // ServicePointManager setup ServicePointManager.UseNagleAlgorithm = false; ServicePointManager.Expect100Continue = false; ServicePointManager.DefaultConnectionLimit = int.MaxValue; ServicePointManager.EnableDnsRoundRobin = true; ServicePointManager.ReusePort = true; } }
Oh cheers, I used the user claims system from MS's documentation. Would you mind explaining why it's not ideal please?
They mean, rather have an endpoint that works on an individual entity, you can post multiple entities at once within a single request. For example, if you want to add 10 books at once, rather than call a post request to `/api/books` 10 times, you could put the 10 books into a single request and call it once.
Thank you for the explanation. From the article, that one initial request seems to generate 10 to 15 other requests. So not sure how passing 10 at a time would solve this? Those 10 would still have to generate 150 other requests. I guess, a restructuring of the app is needed. Maybe that's where it is going wrong. It shouldn't generate all them other requests?
Added better person authentication and now saves tasks/accounts to db.
Either that; or you pass a `Span` into the method producing the data and have it fill in the Span instead of producing an array. That way it doesn't have to know about any buffering/whatever is done to help speed things up, it just fills in data.
I totally agree with this comment. Make sure they see that you understand concepts and good practices. Don't worry too much about not knowing C# and the .net eco-system (yet) and be transparant about this fact. Like others mentioned, show them some of your node work. Especially it's neatly organised and maintainable. I would be impressed if I saw such a project! It means you are able to create order from chaos and that's important. 
Hi drum445, I noticed a few things: * Why do you hard code data access code in your repo's, instead of using something like EF Core? Not that's it's wrong, just curious about your reasoning. * Looking at your controllers, I see that you are instantiating the repos in the controller's constructor. Why are you not using asp.net core's built-in dependency injection? Nothing "wrong" here. Just some stuff to think about :-). Press on!
I'd recommend this, by Maximilian Schwarzmüller https://www.udemy.com/the-complete-guide-to-angular-2/learn/v4/overview Really good instructor and up to date course.
Thanks for the advice, I will have to look into EF core I'm just too used to putting SQL in code I guess. How does the dependency injection work sir, not sure how I would go about that?
Up to a point. For example, even with no permissions on any object, I can still grind up your server by running a bunch of exponential queries on numeric literals.
This is good place to start: https://joonasw.net/view/aspnet-core-di-deep-dive Also, google is your friend ;-)
No offense, but this is only one metric. GitHub Stars: Vue &gt; React &gt; Angular NPM Downloads: React &gt; Angular &gt; Vue Stack Overflow: Angular &gt; React &gt; Vue Jobs: React &gt; Angular &gt; Vue
Instead of client side generation they should push OData imo. [https://www.nuget.org/packages/Microsoft.AspNetCore.OData/](https://www.nuget.org/packages/Microsoft.AspNetCore.OData/) A nice addition would be multi-database tenancy support in EF core.
One thing: on client disconnect, you don't need to manually remove a connection from the group, that is done automatically.
Not that I know of. I should have written one myself years ago.
Thank you for pointing this out! I updated the blog post and code accordingly.
Uses dapper
Interesting. I'll give this a shot. So I'm guessing that with a zip I'm unable to use the ReadLine() of the stream since it's compressed it probably can't find the end of the line on it's own?
[Introduction to Razor Pages in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/razor-pages/?view=aspnetcore-2.1&amp;tabs=visual-studio) and [ASP.NET Core Razor Pages with EF Core](https://docs.microsoft.com/en-us/aspnet/core/data/ef-rp/?view=aspnetcore-2.1)
Fuget is an amazing thing
Thanks!
Wow this is awesome. Microsoft should buy this immediately and roll it into nuget.org. 
You're not quite getting the point. If you control both ends of the API, you can accomplish the entire task in a single request, no matter what it is. At most, you would have two requests. One to get the initial set of data to maybe do some logic on it and the other execute the operation you want. Breaking that up into 10, 15, or 150 requests is going to bring ANY application to a grinding halt. That's not how you design systems that scale. Yet, the blog was written to help assist with scaling issues. My take away is that all the author did was to alleviate a symptom of a deeper problem while ignoring that deeper problem.
How do you like Coravel compared to Hangfire?
You could give it a shot. Now that I look at it again I can't see why that wouldn't work
I'd love if this were rolled into the nuget client in VS too. Such a good idea :)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sqlserver] [Are there any ways to tell if a website might be trying to connect to multiple databases?](https://www.reddit.com/r/SQLServer/comments/8uky67/are_there_any_ways_to_tell_if_a_website_might_be/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Is this your website or are you hacking into something? What technology stack? 
If your app runs on a VM that you control, you can install traffic analyzers to see which IP addresses it is trying to connect to and on what ports. From a code perspective you can review every database connection to ensure the correct connection string parameters are being passed (might not be feasible for a large application or an app that should have many different databases to connect to). If its an app you aren't very familiar with, make sure it isn't just showing cached data when it fails to query data...it might not be succeeding to connect to your database at all.
It's a custom ASP.NET site on .NET 2.0 that utilities libraries from Microsoft Commerce Server 2.0. The queries are coming from the MSCS libraries.
Too bad
If you have the code search for SqlConnection. 
If you have access to the server you can install Wireshark a d look at the traffic. 
That wouldn't tell me the DB though, would it? It would just show the connections to the DB server. I ran TCPView and that's pretty much what it showed.
lol buy
Wow! This is the same guy who created [Ooui](https://github.com/praeclarum/Ooui). You should check out his [recent episode](https://www.dotnetrocks.com/default.aspx?ShowNum=1540) on .NET Rocks. Amazing stuff. 
Great site, I'll be using it a lot! What do the icons next to the types represent? What is the difference between aa filled start and an open star? Why are some types and namespaces faded? 
I already ran TCP View, but it didn't show me anything I didn't already expect to see. It's not trying to connect to a different server. It's hitting the same server, I just can't tell all the DBs it's trying to hit.
There still seem to be lots of articles about it and debates about what is best: https://www.reddit.com/r/dotnet/comments/8ud3do/reusing_httpclient_didnt_solve_all_my_problems/e1epjug/ I haven't yet seen a simple "pit-of-success"
Did you change the profiles connection string that is embedded in the profiles definition. https://visionsincode.files.wordpress.com/2018/02/partitions1.png You can also run some queries against the MSCS Admin database to see if there are odd connection strings in it 
I had updated them manually in the DB. But I checked just now in CS Manager and they are good. They test successfully.
An easy way to explore packages? And an API diff?! fugetaboutit
I think the one embedded in the profile partition might be stored in the profiles db rather than MSCS Admin. Can you tell what sort of operation is happening when the error occurs?
There isn't a profiles DB. After I had changed it in the MSCS Admin DB, it appeared correct in the CS Manager tool. What's so weird about this is that I didn't miss a connection string. If I had, then the SQL account name that says it doesn't have access, would be the old name. But instead, the new name appears. I just mass updated the strings in the admin database. So maybe it's trying to hit another DB that doesn't exist anymore. But it would be really weird if it was.
&gt; There isn't a profiles DB. Is everything for the CSSite in one database?
Yeah. The main config stuff is MSCS_Admin and the site DB has everything else. There's a source definition in the site DB that has a connection string that I've updated as well.
good start, a few minor things: 1. Your auth code is vulnerable to timing attacks leaking usernames. 2. I'm not really a fan of inheritance trees like you have for your repository pattern implementation, I think it would be better to have a connection factory that you injected via constructor injection: public class MySqlConnectionFactory : IFactory&lt;DbConnection&gt; { public DbConnection Create() =&gt; new MySqlConnection { ConnectionString = "..." }; } public class PersonRepo { readonly IFactory&lt;DbConnection&gt; _connections; public PersonRepo(IFactory&lt;DbConnection&gt; connections) =&gt; _connections = connections; ... then in startup: services.AddTransient&lt;IFactory&lt;DbConnection&gt;, MySqlConnectionFactory&gt;(); services.AddTransient&lt;PersonRepo&gt;(); and then in your controller: [Route("api/[controller]")] public class PersonController : Controller { readonly PersonRepo _repo; public PersonController(PersonRepo repo) =&gt; _repo = repo; 3. I dislike how `Person.Password` is being used for 2 things: the hashed password (with a poor salt) and the plaintext password. Password hashing where you need to do it should be done with `Microsoft.AspNetCore.Identity.PasswordHasher&lt;TUser&gt;`; ideally you can work with the identity package for your user management (the nuget package `Microsoft.AspNetCore.Identity`) but if you wanted to directly use `PasswordHasher&lt;T&gt;` you can use the package `Microsoft.Extensions.Identity.Core`. 4. `/backend` should be `/src/Web` or some other name inside a `/src` directory; this convention will make it easier to identify future additions and for other people to come to your project and find things where they might expect 
I decompiled the MSCS library and it's seems like it's trying to pull up the OLEDB connection string and that's where it's failing. This is what the OLEDB connection string looks like: https://i.imgur.com/Q7e1Rch.png The actual ODBC connection works just fine, but I'm wondering if there's something wrong with using an instance name on this new server in the connection string. I can't imagine that would be a problem, but this connection string appears to be the issue and that's the only thing different from the old server.
Really? I've been impressed with the performance, relative to any other WPF controls. WPF really bogs down when you add drop shadows, which material design has a lot of. From looking at the code, Material Design in XAML does what it can to be as efficient as WPF allows it to be.
Why is `.GetAwaiter().GetResult()` better than `.Result`? It isn't. You shouldn't recommend or use it.
Before you continue to learn more about `HttpClient` and `HttpClientFactory`, take a minute to teach yourself some basic concepts about async/await. I see code like this daily and the `.Result` is usually a red flag. I even see it in MSDN code examples which really makes me cringe. Here is a post about the new HttpClientFactory that correctly sums up the reasons to use this class. It also briefly talks about the problem that HttpClient has when you don't use it as a static object. [https://www.stevejgordon.co.uk/introduction-to-httpclientfactory-aspnetcore](https://www.stevejgordon.co.uk/introduction-to-httpclientfactory-aspnetcore) For a more technical / in-depth explanation read this:[https://aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/](https://aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/) This article is a good resource too:[https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-requests?view=aspnetcore-2.1](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-requests?view=aspnetcore-2.1)
The CSOLEDB provider is specific to CS. I can't remember exactly where you must use it instead of oledb. Probably in the partitions conn string. Its been a long time since I worked on a CS project, but I remember the partitions connection string was on a list of manual interventions when deploying.
https://stackoverflow.com/a/38530225/564755
Read the link I posted.
So uh... how do I browse the packages without searching? Don't see any posibility to do that.
this is awesome. well done!
From exploring a library I made, the open star represent interfaces, abstract and static classes. Filled stars are for instanciable classes.
And in almost every use case, you are not going to need to micromanage your performance down to this level, because you will almost never have code where you would get any benefit out of this method. I spent years working on a very optimized system that you have almost definitely used at some point, written in C# and had to communicate from a store to the server and back, doing some heavy logic, in a few milliseconds, millions of times a day from all over the country. Never once used a goto.
Performance claims require performance evidence. Sad to not see any metrics.
Is this competition for browsing source packages in Vs?
I started searching the nightly .NET Core CLR code. I found goto heavily used in .CPP and .C files, but only in a few .CS files: P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\Interop\\Unix\\Interop.IOErrors.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Convert.Base64.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Marvin.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\MemoryExtensions.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Number.Formatting.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\SpanHelpers.Byte.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\SpanHelpers.Char.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\SpanHelpers.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\SpanHelpers.T.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\String.Comparison.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\String.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\String.Manipulation.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Diagnostics\\Tracing\\EventProvider.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Diagnostics\\Tracing\\EventSource.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Diagnostics\\Tracing\\TraceLogging\\ConcurrentSet.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Globalization\\CompareInfo.Unix.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Globalization\\DateTimeParse.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Globalization\\TimeSpanFormat.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Globalization\\UmAlQuraCalendar.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\IO\\Win32Marshal.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Text\\UnicodeEncoding.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Text\\UTF32Encoding.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Text\\UTF8Encoding.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\shared\\System\\Threading\\ReaderWriterLockSlim.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\src\\Microsoft\\Win32\\RegistryKey.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\src\\System\\Buffer.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\src\\System\\StubHelpers.cs P:\\github\\coreclr-master\\src\\System.Private.CoreLib\\src\\System\\Reflection\\Emit\\ModuleBuilder.cs
I don't think Coravel is quite ready to use in production apps yet (as the author states in the repo's readme). But it does have a lot of potential as an alternative to Hangfire.
He is micro optimizing the reading of binary data for a custom made storage engine called Voron. If all your library does is read fast, then micro-optimizing the hot read loops might be worthwhile.
Like I said, not impossible that it will be useful but it's so rare that almost anyone reading this and considering using goto as optimization will be misled into thinking it's a good practice. 
Whoda thunk it?
I clicked this because the name is hilarious but I am genuinely and need to add XML comments to my packages now lol
`ConfigureAwait` does exactly what it claims. There's no need to call it except at the point at which `await` is applied. Putting these operations on their own threads with `Task.Run` [may not be helpful](https://stackoverflow.com/a/18015586). Waiting for an HTTP response is not bound by the CPU. The Task Parallel Library might be the best choice here, as you can instruct it to invoke only _n_ at a time, which will avoid overloading a remote service (or the client's own code, even). 
They’ve released multithreading recent lyrics 
^The linked tweet was tweeted by [@lucasmeijer](https://twitter.com/lucasmeijer) on Jun 27, 2018 21:21:41 UTC (22 Retweets | 149 Favorites) ------------------------------------------------- Loving our hackweek project: "C\# without dotnet". running language feature rich c\# program in 4kb compressed, including runtime &amp;amp; gc. ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
I assume you've got a collection of objects your page displays in a table?So you're doing something like this in cshtml: &lt;ul&gt; @foreach(var item in Model.MyCollection) { &lt;li&gt;@item&lt;/li&gt; } &lt;/ul&gt; And somewhere youre populating `Model.MyCollection` from your database. To export your data as text, you can make a different action in your controller (or modify the existing one with a parameter to return as plaintext instead of html): public IActionResult GetMyView() { var myCollection = . . . // Get MyCollection from the database var entityStrings = myCollection.Select(entity =&gt; entity.ToString()); // Or transform your c# object to the string you're looking for string contentText = string.Join(Environment.NewLine, entityStrings); // Separate each entity's string representation with a newline. You can specify whatever delimeter you want. return Content(contentText); // default return type "text/plain" }
Hey, why did I get a downvote?
I guess you could browse on nuget.org and then change the "n" in the URL to "f" to lookup something on fuget.
A good thought! I've actually used a Span&lt;T&gt; just today in a way I think it was really intended. Because it allows for 'ref' on indexes, I was able to to use Interlocked.CompareExchange(ref span[index], newValue, oldValue); Which is why they mention it being good for ObjectPools... (longer story)
I hate NuGet
Thanks for the response, could you please explain how the auth code is vulnerable? I will look into the factory method. I don't think the salt is week being a UUID but will look at other options, couldn't find a simple bcrypt lib for .net core. I'll have to research the identity package as it looks interesting. Cheers
If you're using ASP.NET Core, it should be pretty easy to just hook up the identity providers you need; https://docs.microsoft.com/en-us/aspnet/core/security/authentication/social/other-logins?view=aspnetcore-2.1#multiple-authentication-providers. I guess you want to look at the [WS-Federation](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/ws-federation?view=aspnetcore-2.1) and [AzureAD OpenID Connect](https://azure.microsoft.com/en-us/resources/samples/active-directory-dotnet-webapp-openidconnect-aspnetcore/).
why?
Well - you are doing something wrong. You are not posting what is going wrong. In that case no one will be able to help you. If you want help, give enough information so people can help. If not, don't post at all. 
You just saved my life 
I run docker and .net core for all my side hustles and I have to say, I love how you build a build image and then build your deployable image from there. So neat. I recently moved from a remote Teamcity instance to VSTS for my CD and I'm happy MS supports Docker just as well as Teamcity, made the transition easy. Shout out to [canister.io](https://canister.io) too, they provide free private container repos.
Buy buy buy 
Thank you! I changed the text to make it clear what I am trying to achive.
Keep us posted. 
Come to the light!
If at this point your just learning mvc? No offence but your fucked. You have cruised for way too long in your career and its time to buy a coffee shop or a franchise lawn mowing business.
Just curious why ? I'm a relatively new dev and used what I guess is considered MVC in building applications with JavaFX. But I work in a .net environment that does not use MVC. Is it more than just an organization of files into folders?
If people new to the .NET ecosystem can learn MVC why should it be any different for someone who has been building sites using WebForms? I'm aware of many companies (big and small) who have working software written in WebForms which needs to be maintained. You could easily be a dev relatively early in your programming career whose only experience of ASP.NET so far is WebForms (because you work for one of those companies). Are you implying they've cruised too long as well? Maybe it's just me, but I feel that wild generalisations don't really help anyone (no offence :-)).
The WebForms abstraction makes programming with WebForms unlike programming for many other web development frameworks. That abstraction means you're often one step removed from the "reality" of what you're building (for example, controls which render html rather than writing the html yourself, buttons which magically post back to the server when in reality they're submitting a form). On the other hand, MVC leaves you in control of your html, forms, GET/POST requests etc. This also carries the advantage that knowledge of those underlying mechanics transfers nicely to building apps using different approaches e.g. front-end SPAs (like Angular) where you build the front-end logic using javascript but still end up making GET/POST requests to your API. Does that make sense? 
It's well worth your time to learn, but if you've been relying on a library of server controls, it's going to take a lot more work than this article implies. Still though, take the plunge, come over to the light side.
Agreed. Hmm, didn't intend to imply that there isn't more hard work ahead, just wanted to strip away some of the WebForms magic to reveal the underlying fundamentals :-)
Yes, I understand that. Working in an environment that isn't MVC friendly though is there a possibility I may find myself at a disadvantage in the future? One of my first tasks here was to dismantle a website that was supposedly built using MVC and reconstruct it. I wasn't given any access to the original files, I had to do it all manually by getting whatever I could get from the website. 
Hard to say. At my work I literally swap between WebForms, MVC and React projects every week! That you were able to reconstruct that MVC site is a good indication (to me at least) that you're keen to learn new things (and apply what you've learned) which is a valuable skill in today's fast-changing industry and will serve you well going forward :-) 
Just store the pictures somewhere else, and store the path to the picture in your database.
 Update! Hey everyone, I ended up being able to precompile our web application. We ran into an issue where we still needed our pages to be updateable, so there's a flag in PrecompileApp.config that I changed from "updateable=false" to "updateable=true". This then allows the web application's pages to be updateable. Thank you everyone for your help! I greatly appreciate it.
I'd highly recommend this episode! I've listened to several hundred of their episodes, and this is in the top 3 of those, for me.
File blobs don't belong in the database. You just need a file storage location (folder on the server, network attached storage, Amazon S3, etc.) and store the full path to said image in the DB. When you pull the locker record, go fetch the image with regular file IO (or webservice call if stored in the cloud).
Good point but I don't control all the end points. It's hypothetical example but in reality I have to make few parallel and the few sequential calls after receiving response from parallel calls. Over all the number comes out to be fifteen for a very specific operation in the system. This post was all about fidgeting with some base settings to achieve desired performance level. I do have support for Batch for one of the workflows where we control both api's similar to what you suggested. 
if you _really_ must store the image in the database, convert it to a base64 string and store that.
don't know how i did all those server side button click events with viewstate. and it was super exciting when the AJAX control was created so I can just wrap controls into using it. just a shit show in general. i think that is when i started to hate web development
It's my favourite blogger again. No explanation of what global mouse hooks are, or how to do them in a dozen lines of code without a nuget, or why that would work. You can do the same thing in WPF, it's the same hooks basically. So why the heck would you say you need winforms? Your rosetta and other fake blog awards still annoy me, and as you didn't have the decency to come to an accord with our "RX is not fucking Linq" argument, I retract my offer of letting it slide. Read the Rx homepage and tell me where it mentions Linq. Seriously, I like the enthusiasm, but you need to correlate that with stuff that's correct. 
Why the everloving fuck would you go from one dead technology (webforms) to another (MVC) when you could just go straight to Core?
MVC exists in Core too you know. It's just an architectural pattern and isn't going away any time soon.
Didn’t SQL add a filestream option which kind of handles the whole reference to a physical file? 
This. Azure Blob storage is cheap and easy. The Storage Manager app makes viewing and changing files easy for an admin (you). Trying to manually go through base64 strings would be a pain in the ass.
News probably next week as author stated on tweeted thread.
Has anyone tried self hosting this? I like the features but don't want to burden his hosting environment.
And you did that nicely! I was just remembering a contract job I did recently where they had a team doing Webforms and they had dozens of purchased server controls. That team desperately wanted to do some new stuff, but their company felt that they had paid all that money for controls and subscriptions that they couldn't justify the switch. It's sad to see that kind of lock in. 
Why? `varbinary(max)` exists, and its purpose is to store byte arrays.
That's still a "can vs. should" conversation, imo.
Poor quality post. Any explanation, use case - anything? Or “just go figure out yourself”
When it redirects to /connect/authorize/callback, does it contain a query string that looks something like this: `/connect/authorize/callback?client_id=&lt;CLIENT-ID&gt;&amp;response_mode=form_post&amp;response_type=id_token&amp;scope=...&lt;BLAH-BLAH&gt;...&amp;redirect_uri=http%3A%2F%2F&lt;YOUR-CLIENT-APP&gt;` That redirect\_uri (which is sent by your client) should do the trick, as long as it is a whitelisted URL in your Client config. IdentityServer should redirect to that redirect\_uri with the id\_token in tow. We use signInManager.PasswordSignInAsync(...), and after it returns Succeeded == true we redirect to the passed returnUrl (which is what you see above). We just used the code from the IdentityServer4 Quick Start.
I worked with WebForms and various web services things on ASP.Net for several years. Not a stranger to IHttpHandler. It had been at least 2 or 3 years since I did any WebForms before I started doing MVC 2 years ago. I think it helped that I started by adding features to a existing site; I found the transition pretty easy. 
It might help if you break down your problem into pieces. Here are some of the tasks you will need to look into: * How do you upload images from a web page using web forms? * How do you access the images from your code behind? * How do you store the images? What image format do they use? Is the database the best place to store them? How about the file system? * What are the security implications of allowing users to upload files, and then serving them again? (hint: there are lots, and they vary depending on how you store/serve the images) * How do you retrieve the images from where you stored them? * How do you display those images to the user? Hopefully this gives you a starting point, so you can start figuring out what you need to do next. Please ask if you need more direction :)
Can you explain further? I’ve never used it but from what I recall it saves the file to disk and a reference to the db which is exactly what people are recommending the OP to do but manually. Also If they use filestream they get included in your backups / replication optionally. 
Because &lt;img&gt; in html supports rendering images from base64, without odd asp controls, and fiddling with memory streams. I vote to store the image in database as base64, it'll be the quickest way to get it going, without fighting with network drives, permissions, and backups. I wouldn't do it in a production application, but if it's a simple project for yourself I say go for it. You can always change later.
/u/xoofx wrote this; there is probably a blog post coming on http://xoofx.com/ It looks like a prototype of a feature to enable code generation extensions (https://github.com/dotnet/csharplang/issues/107). Basically you add a package reference to his compiler(a fork of Roslyn) and then create an analyzer for your project like you would for Roslyn but inherit from CompilationRewriter instead and can inject syntax nodes wherever you want right before the compilation pass. 
You are very funny, Lol. I'll answer you if that is what you are waiting for. 1- I explained what are mouse hooks briefly in the first paragraph of the blog post (Paragraph 1 line 3). 2- Who even said something about WPF ??? The Library I used for the demo app uses some classes present only in Windows.Forms (This is it's github repo Go and check for yourself https://github.com/gmamaladze/globalmousekeyhook ) And I mention in the blog post that System.Windows.Forms should be referenced for the app to work and it is ABSOLUTELY TRUE (YOU ARE FREE TO TRY the code is on github and the link is on my blog) 3- Have you gone through the RX documentation before talking ? Read this : http://www.introtorx.com/Content/v1.0.10621.0/01_WhyRx.html at Paragraph 3 you will find what I told you about RX and LinQ I back the examples I describe in my blog posts with functional code which can be downloaded on github at https://github.com/coolc0ders , I'm open to criticism and and I help those who contact me when they have an issue with the examples I post, I also update my posts when a reader shares with me something which will be profitable to my audience. Also, thanks for paying attention to each post I make. At least it proves I have your attention. 
I had a similar issue in the past which was caused by mixing http and https. May want to check that. 
&gt;Can you explain further? I’ve never used it but from what I recall it saves the file to disk and a reference to the db which is exactly what people are recommending the OP to do but manually. Also If they use filestream they get included in your backups / replication optionally. &gt; &gt;edit: my only concern would be is if the web and sql are different servers you might not want that increase in bandwidth between them and would implement a cache on the web server Exactly that last concern. Generally, file IO is a bottleneck, so being able to push that into anything other than the DB is preferable. It is orders of magnitude cheaper to scale out web servers or file shares vs. a sql server. Also, with regard to backups, I haven't looked into that, but I imagine you would really inflate your backups (and thus, your restore time) if it's chocked full of images. 
And then they learn it, then someone tells them to learn react and functional programming...I hate our industry sometimes, we're so ADD :D.
This is the thing, You don't need Windows.Forms for this, and you didn't mention WPF, but if you went for the more deeply understood solution it would have just worked there too. See what I mean? IntrotoRx is not the project's site, but it is good :) Can we not fight? You're not a moron and I like that, I get frustrated when you waste your talent and encourage people to think likewise. LINQ is not EF. We agree? So LINQ is not RX, but LINQ operators (specifically written for RX) work on RX primitives? Agree? If I can use RX primitives without using LINQ operators then RX is not LINQ. Agree? I feel the need to say agree again. This may be a problem in the future... I have the strangest urge to buy you a bunny right now ;p Agree? 
Personally I found it easier to move from WebForms right to JavaScript + WebAPI rather than worrying about throwing out everything I know to understand MVC. Its nice because I could continue to make .ASPX webform pages and then just slowly move functions from code-behind in forms to functions/actions in Controllers. It was a smooth transition since some newbie libraries like jQuery make POST/GET with JSON easy and WebAPI defaults to serializing and returning objects as JSON.
 When you say that, you don't have two separate build files correct? Docker introduced multi-build files, you just have to declare two FROMs, such as [https://raw.githubusercontent.com/Dispersia/SurvivorQuest/master/src/SurvivorQuest.API/Dockerfile.prod](https://raw.githubusercontent.com/Dispersia/SurvivorQuest/master/src/SurvivorQuest.API/Dockerfile.prod) it's a small thing, but for some things it's incredibly useful (my angular app has 6 build steps, so thankfully I don't need to run 6 commands to get it to work)
What the point of this service? You could already deploy static websites in App Services.
Static site only served off the Storage Account. No need for App Service. The only down side is no server side rendering. If you host a simple site with no dynamic content... things just became real cheap for you.
So difference is just the price? Apps services have a free tier while storage does not, so it's not cheaper in every case either.
Free tier doesn't have custom domains and has a quota. It's all about giving people options.
Have you checked [LimeLM](https://wyday.com/limelm/) ?
Thanks for pointing the differences.
 What kind of licensing are you looking for? For the most part, this is just an account, and an extra property saying if they have access to it or not. If you don't want accounts, and just want keys, you could just as easily just have a table of uniqueidentifiers with Guid.NewGuid() used to create keys.
I agree that this is the right approach. Not to say that MVC is dead, but WebAPI + JS is so much more powerful and flexible.
Angular2+ doesn't play well with MVC applications. You generally just treat them as static resources, and call webapi resources from them. Angular is a full framework on it's own, merging with MVC soon proves to be more trouble than it's worth. React and Vue are view libraries and can fairly easily be used to replace or compliment Razor.
MVC isn't even a Microsoft thing. It's just like you said, it's a pattern.
The wmp control probably doesn't support that kind of stream. Try using the vlc control
Yeah, actually i am saying that. if you have been programming for 2 years and your working on a crusty old webforms from 2006 and you havnt learnt a modern framework in your spare time. And then decide to start with MVC... I hope your not alergic to grass seeds or coffee because programming isnt for you. I didnt say anyone should learn MVC. Personally I would be looking at React, Anuglar2 or Vue. Then possibly MVC just for giggles. 
You could also host static files in blob storage too... 
We started migrating over to MVC 2 years ago. What I wasn't aware of initially was that WebForms and MVC will happily co-exist. We went for all the low hanging fruit at the beginning (Simple WebForms, with minimal postbacks and few server controls). In 1 month we managed to migrate ~ 70% of the app. The remaining 30% we tackled in-between projects. On top of all this we also had to move from VB.NET to C#, there was a hell of a lot of QA testing.
Is it worth asking the author to target .net stages instead of .net core? That would keep you both happy but might not have much of an impact on them.
&gt;*ServicePointManager.UseNagleAlgorithm = false;* &gt; &gt;*This is actually somewhat dependent on the size/type of your requests. If you're doing primarily smaller requests, it's a good thing. If you're doing terrifying things like directly feeding streams through (say a file server API) I would be a lot more careful; lots of stalls on the input end will cause extra chatter/overhead on the network.* Author - Point well taken but I'm not streaming any huge blob of data. These are small json objects. &gt;ServicePointManager.Expect100Continue = false; &gt; &gt;Torn here. If my API is somehow choking out this could just make things worse all over the network. Author - Why would you not adopt a circuit breaker pattern if your api is choking in taking headers and body together? Also I'm pretty sure when we build api's we don't really realize that there is latency introduced by default networking stack due to this setting turned on by default. &gt;ServicePointManager.DefaultConnectionLimit = int.MaxValue; &gt; &gt;LOLWUT? No. No. There's a whole bunch of reasons this number doesn't make sense, going all the way down to the thread pool. &gt; &gt;For what it's worth, the default values **are** absolute shit on a console app, doubly so when you think about how SSL can amplify this number behind the scenes. &gt; &gt;In my real world experience, the optimum number is going to be around 12-16. This is a generalized number for requests that were subsecond on the backend, and would return a smaller (&lt;128kb) set of data. Author- .Net 4.0 auto config is already set for DefaultConnectionLimit set to 12 \* no of cpu cores. Here is the deal I have not had any problems under sustained load with this configuration on in prod. If you had an instance when you had issues I would be happy to know what scaling effects you had and learn from it. As I have put in the article as well the source code comment of .net framework where it limits the persistent connections to 2 for non asp requests. There are few other folks who had the same problem and came down to similar resolution. [https://blogs.msdn.microsoft.com/tmarq/2007/07/20/asp-net-thread-usage-on-iis-7-5-iis-7-0-and-iis-6-0/](https://blogs.msdn.microsoft.com/tmarq/2007/07/20/asp-net-thread-usage-on-iis-7-5-iis-7-0-and-iis-6-0/) &gt;ServicePointManager.EnableDnsRoundRobin = true; &gt; &gt;This MIGHT be a good idea, or it might throw whatever load balancer you have in place for a loop. I'd be a little careful, not to say it's outright a bad idea. I'm not sure I understand the concern here. I will be cycling through the IPs which are resolved for my tcp connections by evenly distributing across all the resolutions. But this IP will be of reverse proxy which can internally balance the requests. &gt;ServicePointManager.ReusePort = true; &gt; &gt;Ummmm I think this can be a double-edged sword. On one hand it will save claiming ports. OTOH it can lead to having to wait for the shared port to be open. (IIRC. I've never tried this one much.) I have not been to instrument this change as well I have mentioned in the post. I don't have any empirical evidence to prove it. 
Is it possible to serve static websites through CDN service, too?
Totally! Works flawlessly with Cloud Flare! Allows you to map your domain and get free SSL! 
You can read more here : [https://blogs.msdn.microsoft.com/tmarq/2007/07/20/asp-net-thread-usage-on-iis-7-5-iis-7-0-and-iis-6-0/](https://blogs.msdn.microsoft.com/tmarq/2007/07/20/asp-net-thread-usage-on-iis-7-5-iis-7-0-and-iis-6-0/) and here how corefx got rid of this problem completely. [https://github.com/dotnet/corefx/issues/2332](https://github.com/dotnet/corefx/issues/2332)
The x64 installer is the one you want.
You guys will love [Blazor](https://github.com/aspnet/Blazor). (just assuming you don't know about it)
Think of it this way: you are asking us whether or not you should fill up with 87 octane gas or 96 octane gas when your mechanic is working on your car. The mechanic has nothing to do with what fuel is needed in your car. The mechanic only works on the vehicle, he doesn’t drink the fuel. Go for the 64-bit SDK. It contains both the 64-bit code as well as the 32-bit code needed to develop your project; settings within the project itself will determine which are chosen when you run/publish your project. Only go with the 32-bit SDK if your entire operating system is 32-bit, as you will be unable to test a 32-bit build.
You need System.Windows.Forms actually, Else it won't run. Did you read that article well ? read it again and go through the source code. I NEVER said LINQ is EF or LINQ is RX. do you remember what I said ? 
Uggh, Reddit ate my reply when I edited my post. Some of my information was a bit too biased to the use case of a console app. I edited slightly in the name of accuracy. &gt; Author - Why would you not adopt a circuit breaker pattern if your api is choking in taking headers and body together? Also I'm pretty sure when we build api's we don't really realize that there is latency introduced by default networking stack due to this setting turned on by default. Well, this is more in relation to *large* requests. if I'm sending small Gets and Posts, no worries. My concern is someone may think it's a good way to speed up an API that's having to gobble down MBs of binary data (happens more often than one would hope...) &gt; Author- .Net 4.0 auto config is already set for DefaultConnectionLimit set to 12 * no of cpu cores. Here is the deal I have not had any problems under sustained load with this configuration on in prod. If you had an instance when you had issues I would be happy to know what scaling effects you had and learn from it. As I have put in the article as well the source code comment of .net framework where it limits the persistent connections to 2 for non asp requests. There are few other folks who had the same problem and came down to similar resolution. Yeah, I was a bit off here. all my numbers were related to *console* applications. in ASP.NET I've honestly never had to tweak the connection limit, even under some very chatty scenarios. The *Thread pool* on the other hand... ASP.NET has pretty low Thread pool minimums by default. You can see by looking at `System.Threading.ThreadPool.GetMinThreads`. We bumped these numbers up (12-15 on both ends) and that was enough we didn't even think about the connection limit. I'd probably argue that after a certain point of upping the connection limit, are you better off either refactoring your API calls to be a little less chatty, or perhaps you should consider something for your backend like AMQP, GRPC, or Akka (uses a long-lived TCP socket from point to point). &gt; I'm not sure I understand the concern here. I will be cycling through the IPs which are resolved for my tcp connections by evenly distributing across all the resolutions. But this IP will be of reverse proxy which can internally balance the requests. That statement was a bit of paranoia: I've seen a lot of not-quite-right load balancer configurations cause problems. Usually this was a perfect storm (where the back-end was a lot more stateful than claimed!).
Breaking up in to multiple services is the reason why I have to make 15 api calls :( . All these discreet apis can scale independently but when you get to implement a workflow with different entity apis you end up in this situation. Yes architecture can be made better by adopting an asynchronous mechanism (message queue or actor programming model) of doing this workflow but that is a cost and benefit analysis depending on the maturity and adoption of the product. We are currently working on that aspect too but to fund that asynchronous architecture you need to earn money and customer adoption. I never said its a problem with HttpClient. HttpClient is a library but it's upto the consumer to make the best use of it. ServicePointManager setting changes should be done if you are not happy with your performance levels and you want get more juice out of the framework or library. If you are happy with perf numbers then don't bother.
I believe this could lay some groundwork for macros that operate on the AST rather than just text replace.
I doubt you can make this work. The while idea of a container is you can blow it away and your data is safe in a database somewhere else. You could run an ec2 instance which is what elastic beanstalk sits on top of but you will lose the easy deployment. 
I'm using full framework not asp.Net core
Stages? Did you mean Standard?
Thanks for that :-) 
The part you describe is the key for the user account. But the heavy lifting of doing .NET client side licensing stuff requires some sort of library that does a lot of stuff under the hood, such as date rollback detection, trial time period, etc. That library works off of a propriety key file to set all those parameters. It's this key that is generated server side to send to the customer to activate the software. 
First of all you shouldn't be disposing of httpclient after every api call. I have stated that in my previous post why and here is another person confirming the reason why [https://blogs.msdn.microsoft.com/alazarev/2017/12/29/disposable-finalizers-and-httpclient/](https://blogs.msdn.microsoft.com/alazarev/2017/12/29/disposable-finalizers-and-httpclient/) Second, i don't want to offend but I don't understand why would you spin off a thread pool thread when the actual api invocation would be done async IO thread. You are increasing your overhead for no reason. You can monitor your thread usage with this method [https://msdn.microsoft.com/en-us/library/system.threading.threadpool.getavailablethreads&amp;#37;28v=vs.110&amp;#37;29.aspx](https://msdn.microsoft.com/en-us/library/system.threading.threadpool.getavailablethreads%28v=vs.110%29.aspx) Third, The real value of your tests will show up when you put some latency in your api to mimic a real world scenario of network latency and some computation on an api which can't be guaranteed to be constant always. Fourth, any kind of load test should be performed for a sustained load for longer time. Standard strategy is to use Jmeter and hit your api for 24 hours to see the stability of your api. I would be interested in knowing the results of your test harness. After you have done all of this mentioned I would be happy to take down my post and say my findings and optimizations suck. PS: I'm using full framework not aspnet core. 
And instead of an ec2 instance have a look at RDS offerings as well. Not as cheap, (but I think there’s a free option) and it removes all of the overhead of running a server, like patching and maintenance and stuff like that. 
 Everything is being done on the server, have a column for end trial time, and server dictates if the account is still "active". There is no such thing such as things like date rollback when it's networked. The server knows what time it is, you should only care about that (at UTC time for both the current and end times for trials). 
I used to recommend LimeLM too, but in light of [recent events](https://wyday.com/forum/viewtopic.php?f=3&amp;t=27998), I'd warn against using their product. They recently changed an API without warning, leading to many of their customers losing days worth of sales, having to invest significant development effort in working around the changes, etc. Their handling of the issue has been extremely poor, and instead of working with their customers to come up with a fix, they've spent their time making defensive/misleading posts on their forum and suggesting that their customers are just incompetent, etc. (For context: My company are customers of LimeLM, and we weren't really impacted by this change, but the way LimeLM handled this has really concerned me). 
There are many situations where some desktop software needs to be able to operate without an internet connection. Traditional licensing systems like what OP is referring to will support this, whereas a simple "guid and datetime" server won't. And once you've gone down the path of supporting offline software, you then need to consider rollback detection, key sharing detection, etc.
I think you're misunderstanding what I'm doing. I'm deploying .NET WinForm apps to client computers. Those apps need a licensing component to prevent unauthorized use. It's the server component that generates license keys for those apps that I'm trying to move to .NET core.
Use Amazon RDS. It's super easy. Just open the port to your security group on your EB instance. 
Thank you. You seem to be the only one that was able to comprehend what I was talking about. Geesh
Yes I would agree get away from sqlite. I was only recommending ec2 if you really wanted to keep using sqlite. You will be much happier with a normal database like Postgres. 
A guid generated by `Guid.NewGuid()` is "weak" because it is explicitly not designed to be cryptographically strong. Instead it is designed to be generated fast and to minimize the probability that it is duplicated. On Core, `Guid.NewGuid()` returns values in the form: `{xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx}` where `y` is 8, 9, a or b (binary `10xx`) and other bits `x` are to be set by a (pseudo)random number generator as specified in https://www.ietf.org/rfc/rfc4122.txt A common pattern for storing password hashes is (presumably, I'd hope most use a mechanism somewhat like this) 1 byte for a discriminator, 4 bytes for the number of iterations, N bytes for salt and M bytes for a hash stored in a single varbinary field or base64 encoded. The first byte is used by the program to decode the rest of the data according to your designs. Alternatively a system may store this information as delimited string data. This is done so that you can improve the algorithm over time. Identity4 currently stores passwords by default in the format: {0x01}{int32}{int32}{int32}{salt}{hash} The first `int` is the enum `KeyDerivationPrf`, the second is the number of iterations and the third is the size of the salt. When validating a hash, it uses the values here and if they do not match the current configuration it returns a code saying the password needs to be rehashed. -- To prevent timing attacks your authentication code should be something roughly like this: public Task&lt;ActionResult&gt; Login(string login, string password) { var delay = Task.Delay(_enforcedDuration); //must be longer than normal logins; can be some sort of exponential backoff based on UserHostAddress repeated access var login = Task.Run(() =&gt; { //actual login function return success; }); await delay; if(login.HasCompleted &amp;&amp; login.Result) return LoginUser(...); return Error(); } Of course you can get fancier with a task completion result. If you want a backoff you can have something like: static ConcurrentDictionary&lt;(int, string), int&gt; backoffs = new...(); static int Backoff(string ip) { var hour = DateTime.UtcNow().Hour; int result = 1000; //whatever default result backoffs.TryRemove(((key+1)%24, ip)); backoffs.TryRemove(((key+2)%24, ip)); ... backoffs.TryRemove(((key+22)%24, ip)); // or other reset strategies... if (backoffs.TryGetValue(((key+23)%24, ip), out var previous) { result = previous * 2; } return backoffs.AddOrUpdate((key, ip), result, (_, p) =&gt; p * 2); }
Thanks.
True, but also a Microsoft library. Unfortunately named. 
Thanks. The gas / mechanic analogy didn't work for me, but I woke up from a nap having come to the same conclusion as your last paragraph (I have exciting dreams).
Then you login and activate, and save your time stamp. There is no way to do clock rollback detection, OR keysharing without internet. The only thing I can find that records time changes is the windows event log... which can be cleared. Key sharing wouldn't be possible if it's offline only, how would you know if one computer activated it if you never tell anyone that its been activated? (unless they send you some machine id, which case, you can still change your machine id to match theirs).
It's not about stopping dedicated attackers who are willing to reverse engineer your assemblies - it's just about keeping your honest customers honest, and stopping casual piracy. The same reason I lock the front door of my house even though someone COULD get a tank and just drive through my wall.
 And that's exactly what having exactly what I said doing server side will do, just much, much easier, and without having to pay some third party for something that can be implemented in 5 minutes.
The gas analogy isn’t clear. OP go for x64
And what you've suggested is a fine solution for online-only software, but OP has indicated that he/she wants a fully-offline solution that provides date rollback detection, trials, etc. There are completely reasonably requirements in many environments, and there are plenty of products that offer them. As an aside, I'm not sure why you're downvoting my posts, specially since the OP has explained his/her use case and it's in line with what I'm saying.
 How? What would you expect them to name it? Microsoft.AspNetCore.MicrosoftDidNotCreateMVC.MVC.Core?
I don’t know. There are plenty of other MVC-based frameworks that aren’t called “MVC”.
Nothing to see here 😊
Blazor is a really cool prospect but isn't it more geared to replace JS with WASM rather than solve the Webform -&gt; MVC issue?
How did you manage to get them running side-by-side? We're in the same situation where we have a legacy VB.NET WebForms application and would like to migrate to C# MVC overtime. I've looked at this in the past but saw many contradictions whether it's possible or not. The other proposed route is JS + Web API and sticking with WebForms.
I’ll admit: I totally misinterpreted SQLite as SQL Server Express. Point still stands though. A web app would be better served by a “proper” database unless it’s small and trivial enough. 
We had to roll our own :/ 
Can you explain your setup here? Looks promising 
No, YOU need Windows.Forms to make YOUR code run. It's not required to make global hooks work. Look I don't care, I find your blog posts annoying, so I'll just ignore you from now on. All the best.
give Rhino Licensing a try: [https://hibernatingrhinos.com/oss/rhino-licensing](https://hibernatingrhinos.com/oss/rhino-licensing)
Thanks again mate, I shall look at Identity4 and make sure the Login method takes the same time regardless of whether the user exists in the db
**ALWAYS x64.** VS is an x86 app for legacy reasons.
We had to do a full migration from VB.net to c# first. Only then were we able to start the gradual MVC migration. We used a tool to convert the app to c#, I forget the name (although I can find it for you). It does a reasonable job, although it doesn't handle everything. You need to then build the app, and work through the errors until it finally compiles. Find, replace is your friend here.
You don't need to use a CMS at all for really basic websites. Just write HTML directly and serve static files.
Throw in Mediatr CQRS, Unit of Work, and a myriad of JavaScript frameworks screens and then you are approaching where we are. All of these things are pretty standard architecture features if you are trying to stay robust and SOLID. Is this a large or small project? That can make a difference in the answers you receive.
1. Add your domain name do Cloud Flare. 2. They will copy your DNS setup or set it yourself 3. Copy the Cloud Flare nameservers on your DNS provider records 4. Enable SSL 5. Enjoy
It sounds like you are after a static site generator. Google them, there are many available. It gives you much of the functionality of a cms but without the complexity.
Happened all the time where I worked until a few of the culprits retired or moved to a different product group. We're still dealing with places where it's difficult to modify small things because the over complicated design gets in the way. The biggest problem is that the designers are they way they are in order to solve problems we "might get in the future"... yeah those problems never happened. I'm heavily in the YAGNI group with all the designs I propose...
Missing /s?
&gt; The initial intent was to eliminate the need of changing/adding multiple code pieces when adding a new project specific class. It sounds like your abstractions are leaky and your classes break the single responsibility principle. This whole registering constructor thing (either version) sounds like a terrible code smell with overhead at both development and run time. I'd be looking at your code base and start refactoring to maintain the SRP
BlogEngine use XML as first storage and optionally can use SQL Server https://blogengine.io/
I run into this with some of our older stuff. Supposed to be a basic processing job or something like that but is overrun with useless enums, reflection, database controllers, etc. like you said and ends up being a pain to deal with. It might make sense if most of it was reused in the future, but of course it’s not and it just becomes frustrating.
Yes, but it mixes all the nice things of MVC and the nice things of WebAPI + JS. And, more importantly, ditches JS (or tries to).
[Static site generator](https://www.staticgen.com). You don't really need a server at all.
why are you writing code by voting? if you have time to add all this crazy shit, you're all obviously not being kept busy enough. this is a management problem. talk to management about it. become management, if necessary, to avoid shit like this ever happening again. and until then, just don't use their crazy crap. you'll get your work done faster than them and they'll feel stupid and start joining you, one by one.
Yeah it is more confusing than anything lol.
For my personal blog I use Hugo with Netlify. Source code is checked into GitHub and deploys automatically when new commits are pushed. You can read the following articles to get you started: * [Hugo Quickstart](http://gohugo.io/getting-started/quick-start/) * [Host Hugo on Netlify](http://gohugo.io/hosting-and-deployment/hosting-on-netlify/) Hugo is free, as is the hosting on Netlify.
I find poor designs often developers incorrectly assess where most of their time is spent or due to politics/power struggles. When in doubt, go with the solution that is quicker to understand which is almost always the simpler of the two.
I don't understand what problem you were trying to solve. What does "changing/adding multiple code pieces when adding a new project specific class" I never thought of having to write a little code as a problem, I just consider that my job. I'll take writing a few lines of easy follow code over some nonsense reflection DI patter that doesn't do anything... What the hell are they doing at your work that you are solving made up problems and not adding value to your company?
Ah! Simpler than I though. So DNS, SSL, CDN -&gt; cloudflare and static hosting -&gt; Azure Blob Storage? For some reason I thought one had to combine the azure CDN with cloudflare DNS
Not even. In fact, you don't even need to bind your domain name to your Azure Storage.
You tend to go through an experience curve, start off simple , make things more and more complex, get horribly un-manageable code, and then decide to make things simple. Sounds like your ahead of the other on your journey . Don't worry they'll catch up :)
Uh? Really not even a single test in the project on \[github\]([https://github.com/praeclarum/FuGetGallery](https://github.com/praeclarum/FuGetGallery))?
We try to not impose anyone's opinion on the others by force. We also try to do some research, quick POCs before coding for production. Even so, feels that biased opinions find a way to survive. :)
Hehe. Thanks! Some of them are ahead of me on other aspects. 
It is a medium size project. Do you think keeping it simple is detrimental to SOLID?
A little more concretely, we needed to define a new enum value somewhere and new methods to process it, and a new case for a switch statement. It was a little all over the place, and for no good rational reason, just really fast development with a dead-line in mind.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/oop] [Fresh witness of useless complexity and over engineering. x-post from r\/dotnet](https://www.reddit.com/r/OOP/comments/8v318o/fresh_witness_of_useless_complexity_and_over/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
&gt; Even so, feels that biased opinions find a way to survive. :) because the opinion of the least knowledgeable person counts as much as the most knowledgeable. you're engaging in politics, not architecture.
But who decided who is the most knowledgeable? 
if you don't figure that out in the hiring process, that might be your root problem. are you hiring people to fill chairs and sit around at X times the average hourly rate of the entire programming team and try to figure out how to do .NET effectively as a group exercise, or can you contract with a more knowledgeable architect than anyone currently working there for less than what a few days of that would cost the company? for the cost of the technical debt going down a bad road will cost long-term, you could hire that architect full time.
This happens a lot with contractors. They want to come in and build a really fancy project that they don't have to worry a out maintaining after they're gone. 
same here
Why do you need to build this using an expression tree? Do the grouping and/or select parameters need to be build at runtime? Also, you will not be able to use anon types with expression trees (at least not very easily at all unless you want to dive into the mire of building types at runtime). Anon types are defined at compile time, so you don't know about them at design time to be able to reference them in an expression. Instead, you would need to define concrete classes (class GroupingParameters { public int PersonId {get; set; } })
Yep, I've been keeping tabs on it. I'm super excited to play with it when it's gets to a more stable version!
I think you need to get that count outside of this query after you materialize it. I think total would be g.Count() then.
Why not select the composite key for your label first and then group on that? This should also give you the number of items per key (i.e. group.Count)
If you look at how to use [`MultipartFormDataContent`](https://docs.microsoft.com/en-us/dotnet/api/system.net.http.multipartformdatacontent?view=netstandard-2.0), I think you'll find that its API matches the raw request you have extracted. 
Take a step back, explain what you need to accomplish and give us some sample data, and maybe we can help.
What I'm deriving from this: - having time to plan and discuss software is a bad thing. Ask management to pull out the whip. - Ignore what everyone else is doing. Just duplicate functionality by adding your preferred method. - A real man, when confronted with incompetence, will just smite those losers, take over the ship and sail it by himself. You, sir, are an idiot.
Ive spent the last two years deleting code. I could write a book about KISS and YAGNI. Just stupid stuff, we model and organisation, company structure, they all get contacts, addresses, notes, grouping mechanisms all of which are never used. Then you throw in a service bus that does nothing. Why use one database, lets use 15... 
At least they are using enums and not just passing everything around in strings.
I just got moved from my mostly backend position on one team to a full front end position on another. The over engineering in this js framework application is beyond out of control. Even the style sheets are so abstracted that it takes half an hour to simply change a font size. The idea was that it would be inner-sourced too (meaning any other team in the company can make pull requests to contribute) but there is zero chance they will ever understand the overly complex code base without having worked in it day in and day out for at least 2 months. 
That's the sort of thing I like...checking it out...thank you kindly, looks promising...
Netlify looks pretty awesome, not 100% sold on Hugo yet. I'm a bit suss on how netlify do it for free tho on the low end, but here's hoping it stays that way :)....
Thank you. Looking at them. Only DocFX is dotnet I think. But something there hopefully...very useful, thank you heaps.
Thank you. That looks like almost exactly what I want. I am however intreaged by the static site generator methods the other guys have posted. I like the idea of publishing via a git repository so I can just do my stuff in visual studio...
It really doesn't matter. You won't need to write backend code. It's really just a question of the template language.
Go with static engine generator. https://wyam.io/ is extensible as hell. 
Nice project, I did not know him. I will give it a try. Thanks!
What is interesting about all these projects is that they are Open Source and allow to test quietly and compare. :)
&gt; What I'm deriving from this: &gt; You, sir, are an idiot. then stop deriving idiotic things from reasonable, constructive criticism. i didn't say any of that, you did.
&gt; explicitly resolving the dependencies using the DI container By this I assume you mean the service locator anti-pattern? If so, sigh.
Thanks. Just regular .net framework here, unfortunately.
This is called not being a team player and is a great way to reduce project velocity by introducing competing patterns and undermining your colleagues, get ostracized by your team because of your ego, and fired because nobody wants to work with you and you slow down the team.
Getting the config ready to paste here I relised that I had this line commented out: builder.AddAspNetIdentity&lt;MyUser&gt;(); When I uncommented that it worked perfectly.
Probably with a profiler.
AppDynamics. It will help profile your services and long executing sql calls. From there the senior devs can walk through the code to figure out the performance issues. If Winforms, in my past experience a lot of people misused the IsPostBack and didn’t use SessionCache properly. 
It isn't unfortunately. We're lucky it's even WinForms (interop with Vb6 legacy code) :(.
With Linq2sql, make sure the IQueryable queries are taking advantage of the delayed queries properly. Like don’t call .ToList too early if you’re still building the IQueryable. This will result in multiple trips to the database. So try to work with the IQueryable object up until the result is needed for the response. 
Either Redgate ANTs or Jetbrains dotTrace profilers will tell you everything you need to know. From there you can start digging into the bottlenecks you'll find.
Definitely start with a profiler (eg ANT). I was once tasked with improving the performance of a data processing tool that took hours or potentially days to run if you weren't careful with initial parameters. I was able to speed up a particular section of code by around 10k (not a typo) because a central dictionary was way overused and the continual lookups was killing the application. I can't remember the element count but it was huge. Partitioning the dictionary, implementing paged lookups and a few other tweaks took the run time down to about 90 seconds.
Sure. An expression can just be built grammatically. .GroupBy takes an \`Expression&lt;Func&lt;TItem, TKey&gt;&gt;\`. So make one. \`Expression.Lambda\`
ANTS is probably where you want to go but if you aren't even sure where the code is ##DEBUG and Stopwatch are a brute force way to get you in the ballpark of offending code. 
I feel like the code needs to be SOLID. It doesn't meet the minimum level of quality in my book if it's not. Having said that, if it's a small thing that you can keep the whole app in your head with no problems then there can be exceptions. 
ANTS performance and memory profilers are freakin' amazing. There is a SQL add-in that times your queries as well. Visual Studio also has some built in performance tracing tools, and there is a free one I have used called Eqatec.
Use Fiddler with this plugin http://www.chadsowald.com/software/fiddler-extension-request-to-code
I want to be able to supply a function with object properties in the format of a string and generate something to the above 
I would like to build a report We can Currently group by one property Say location But we want to be able to create a dynamic query to group by department and location 
I can change it to g.count no problem 
Another thing to look at is caching if you're not already doing it. I've seen lots of applications have performance issues when they don't cache large amounts of static data.
There are build events you can use in the project properties to copy the configs based on the build profile. I'm on mobile, so I can't give an example right now. But it's a common thing you can Google.
You can use an environment variable if it was using .Net Core. Another way is to use another appSetting and based on that, pull in the proper config file. Use a powershell script to set the proper appSetting
Thanks. I wondered if that was the best option. I’ve never used them, so obviously don’t know much about them. I’ll add it to the google list!
https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.SlowCheetah-XMLTransforms
Since no-one else has mentioned it, I'll throw Azure Application Insights into the mix. App Insights can [automatically instrument your app](https://github.com/Huachao/azure-content/blob/master/articles/application-insights/app-insights-windows-desktop.md) and you can run it in production and get all kind of great telemetry from it, including timings for external dependencies like SQL. It's a good thing to have on top of the debug-style profiling others are suggesting with ANTS or the Visual Studio profiling tools.
Sure. I might be able to give you an example later tonight. But essentially, you just create a folder with your configs and name them whatever. Say, dev.config, stage.config, prod.config, or whatever else. Then you create build profiles with those base names: Dev, Stage, and Prod. In the build event, you write a single line telllng it to copy the file to the build folder, overwriting the default app config. You keep the build config name the same as the config file so you can go "copy &lt;buildName&gt;.config &gt;&gt; oldfile.config." There might be other ways to doing it, but that's pretty simple and has worked for me for years.
Transforms are the way to go
Why would you do that instead of just using the transform syntax already built in?
I'm not saying you would. This is just the only way I've ever done it or know how to do it.
It sounds like the code is violating multiple SOLID principles. Single responsibility, open closed, and dependency inversion. If you are questioning the value added, I would suggest researching the value of following these principles. I interview a lot of developers and in my experience most of them can rattle off what these principles are but don’t really understand the benefits. The argument I usually see is that it’s too complicated and not needed. It’s a small project, what’s the harm right? Most of the really large projects I’ve worked on were a complete mess because they started as small projects that became big ones. People cut corners to save time not realizing the impacts it will have 5+ years later. You mentioned spending a half hour on changes. Is a half hour really that big of a cost? In theory you should now never have to change that class again. Test the crap out of it and appreciate the investment you just made. 
Ants or Jetbrains dottrace profiler. It's worth it. Take a couple very obvious problem areas where the system noticeably lags, attach profiler, use the app, review the results. You'll probably find some very clear culprits, and once you address some of the more obvious ones, often it will clear up the issue in many other spots (For example, if you implement a repository with some intelligent caching instead of querying every thing every time). I've been through this process many times with web apps and taken a completely database driven design from seconds per page load to tenths of a second in a matter of a couple weeks just by tweaking with dotTrace. Can't recommend this approach enough. If yo udon't want to pay for dotTrace, I believe the newer visual studios have performance profilers built in that can probably help you out a bit, but I think dotTrace does the job better.
[perfview](https://github.com/Microsoft/perfview) is a good choice for analyzing overused/abused code paths that are candidates for refactoring or different choice of data structure / algorithm. It is developed by Microsoft and used to track down performance problems in many kinds of .NET applications. It won't help you much with bad SQL queries though (identifying that you're calling SQL in a tight loop, maybe). A commercial alternative is the dotTrace (which is a bit more intuitive to use, but based on my extremely limited usage, does not support the same breadth of features and analyses as perfview).
Just a heads up, AppDynamics helps with winforms (and any other .net application) in addition to web applications! The only difference is that you need to manually define transaction entry points (via class/method names). Not sure it's the best fit for you, as you mentioned preferring open source solutions, and it's also primarily focused on production monitoring (with secondary benefits like code optimization), but it's still the best .net monitoring tool out there IMO. Disclaimer: I work for AppDynamics, but I spent 10 years prior as a .net developer using a variety of other tools prior to my current gig. 
Just to add on to everyone else... Figure out where the bottlenecks are and hit the big stuff first. Since its a client app you may want to investigate async/await add a means to moving processing code off of the main ui thread so it doesn't block. Usually you won't have to get to into some of the advanced CLR options as a first step. Using a tool is great if you can get one but you can also just use your own stopwatches to isolate the slow parts yourself. The first step in any optimization is finding the problem. Don't just start optimizing everything because you may spend a lot of time on something that doesn't net any noticeable gain. 
This is what my company uses. Pretty easy to use too 
You're right on the explanation but it seems that we're talking about a very small app here with low throughput. Storing in the database would work fine. It has the upside of minimizing the number of infrastructure components.
I had this same problem and couldn't find anything that would do everything I wanted so I made https://github.com/codenesium/ConfigDragon
By multiple I mean multiple parts of the object, I.e. location and department vs just location 
For faster load times, see if the app can be [NGEN'ed](https://docs.microsoft.com/en-us/dotnet/framework/tools/ngen-exe-native-image-generator) 
Config transforms are your friend here but obviously you need to test the transforms carefully to make sure that things are changed as you expect. Otherwise a deployment tool like octopus can replace environment specific values in a config file at deployment time.
Check out [Writing High-Performance .NET Code](https://www.writinghighperf.net/) by Ben Watson. Disclaimer: I am not afiliated with this book or Author, I do however sown a copy of the first edition.
Yes. 
You can definitely set up profiles on debug, release build modes to publish to different locations. In order for the setting change to take effect you need to reload the solution. 
&gt; But we want to be able to create a dynamic query to group by department and location How you go about this depends on how you're pulling your data. If you're working with an in-memory collection, you can use basic reflection. However, if you're working with an ORM like EF, it's going to be more difficult.
Thanks for the link, I had not come across this yet. It looks like transforms only apply to web projects... is that right? If so it looks like that rules out another suggestion of using SlowCheetah.
Why don't you do the list on the ListaSugestoes page instead of getting the result then redirect? Which means you can pass the string urljson to ListaSugestoes then do the *Produto.Rootobject Produtos = JsonConvert.DeserializeObject&lt;Produto.Rootobject&gt;(resultado);* there. Or you can use a custom global class and do Dependency Injection.
Awesome, this looks to have great potential. However, I seem to be having trouble with POSTs over HTTPS. GETs work fine. Toying around with it now. Will post my results soon.
I've never used VS to deploy, but if you're on a tight budget, and want something quick/hacky, you could write a powershell script that will copy your build artefacts to both servers. It's probably better to use cake (C# make), or fake (F# make). If you have time+money, a much better idea is to set up a proper build system that will generate packages from your repository, and a deployment tool like octopus deploy that will push it to your servers. 
We use an open-source tool called [BenchmarkDotNet](https://benchmarkdotnet.org/) to profile code.
Temporarily, until they optimize this in the compiler 
This is amazing. It generates the code flawlessly. Just what I was looking for, though I am going to convert the generated code to use HttpClient.
Good article! :)
They'd be better off reporting this rather than writing bizarre code.
Glad it worked out
Hummm may be I'm dumb, but require an ORM compatible with EntityFramework (another ORM) it's like ask: I search for a car to put on my pickup! :) Seriously, you should use another ORM if you are not satisfied by EF. I looked at all micro ORM a few years ago for a project and found two nice one: - Insight.Database https://github.com/jonwagner/Insight.Database - Simple.Data https://github.com/markrendle/Simple.Data Both have advantages and disadvantages. Keep in mind that all this Magic is done using reflection and have a cost!
Damn, guess this is the weird behaviour compiler designers and developers have to deal with everyday.
Surprised to see no mentions of [MiniProfiler](https://miniprofiler.com/) or [Glimpse](https://github.com/Glimpse/Glimpse) (though Glimpse isn't really active at the moment but since you are using WinForms, you should be able to use the old releases).
&gt;They'd be better off reporting this rather than writing bizarre code. It was reported 3 years ago: https://github.com/dotnet/coreclr/issues/914 In the meantime, .NET Core developers choose to work around it, and I am happy that they do.
I created a global class a I am using it as the model when I redirect to other page. However, your comments is making me think if I got the right concept of this. Thanks! I will study more the Dependency Injection part of the docs.
mind blow
This is great advice. As he said, it's seemingly obvious advice, but it's not something I've considered doing before. Thanks for sharing this! 
You could use basic auth. But then your user is going to transmit username and password in every request. That is not good either.
This one weird tricky compilers don’t want you to know. 
Just an example from jwt.io. eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c { "sub": "1234567890", "name": "John Doe", "iat": 1516239022 } That's really not that much data. Are you seeing something larger than that?
I cannot make the Inline code formatting work. Not sure what I'm doing wrong.
I am not sure what you mean by using 2 ORM, that is not what I am asking. I am looking at way to automate filtering/sorting on public DTO so I don't have to keep manually writing it out for every endpoint on every property that can be filtered/sorted. AutoQuery is part of OrmLite which I am using as an example of what I mean. I want to see if there is any library to implement this functionality in EF. This is entirely possible, I have wrote my own generic sorting/filtering classes using dynamics library that go some of the way to provide it but ideally I want a more well developed robust solution that I don't have to fully support myself. Also I am sure there are other coders who would come up with much better solution than myself. The fact this feature exists in other ORM is besides the point. I am using OrmLite on my current project but I would love to have this on EF project too. My current solution for EF generates the where clauses automatically for any query property declared on the DTO. It is a clunky solution and is causing me time refactoring it as I go. I am looking for something already built I can use. Maybe it doesn't exist, I am asking to find out.
My original question was slightly confusing, i edited it slightly to show I want this to work with EF.
Ic :) I figured that out a little bit later on. I am playing with HUGO at the moment. Looks pretty good but a few hiccups I need to get over. What would be REALLY nice would be able to have netlify run the generater. I don't suppose if you know if that is possible? Thank you heaps for your thoughts and know how.
I haven’t had the time to fully understand what your problems are or what you’re trying to accomplish, but I think GraphQl and EF is probably the way to go. https://hackernoon.com/how-to-implement-generic-queries-by-combining-entityframework-core-and-graphql-net-77ac8faf4a22
You can use something like this https://marketplace.visualstudio.com/items?itemName=GolanAvraham.ConfigurationTransform
Looks like a temporary fix was merged into master for core clr in April. Unsure if it has been released yet.
Not too familiar with Netlify, I'm using Firebase. My code is in GitLab and I'm using their CI pipeline to build and deploy to Firebase. Would probably be very similar with Netlify assuming they have some command line API. I've done similar setups with Bitbucket pipeline. I assume GitHub can do it also if that's your jam.
Cab you give me an example of reflection?
I'm not sure what you mean this is about the jit and not the compiler. and this isn't a particularly weird situation. 
*compiler/managed runtime designers
 public class Person { public int PersonId { get; set; } public string Car { get; set; } public string Location { get; set; } } var persons = new[] { new Person { Car = "car1", Location = "somewhere", PersonId = 1 }, new Person { Car = "car1", Location = "somewhere", PersonId = 2 }, new Person { Car = "car2", Location = "overthere", PersonId = 3 }, new Person { Car = "car3", Location = "overhere", PersonId = 4 } }; static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params Expression&lt;Func&lt;T, object&gt;&gt;[] properties) { var pis = properties.Select(x =&gt; (PropertyInfo)((MemberExpression)x.Body).Member).ToArray(); DynamicGroupBy(entities, pis); } static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params string[] propertyNames) { var typeOfT = typeof(T); var pis = propertyNames.Select(x =&gt; typeOfT.GetProperty(x)).ToArray(); DynamicGroupBy(entities, pis); } static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params PropertyInfo[] propertyInfos) { var groups = entities .GroupBy(e =&gt; string.Join("_", propertyInfos.Select(pi =&gt; pi.GetValue(e)?.ToString()).ToArray())) .Select(x =&gt; new { label = x.Key, total = x.Count() }) .ToArray(); } Use it like this: DynamicGroupBy(persons, x =&gt; x.Car, x =&gt; x.Location); or DynamicGroupBy(persons, "Car", "Location"); Note, this will not work with EF or any other LINQ2SQL ORM. You will need to pull back your entire dataset into your application and work with it there.
 public class Person { public int PersonId { get; set; } public string Car { get; set; } public string Location { get; set; } } var persons = new[] { new Person { Car = "car1", Location = "somewhere", PersonId = 1 }, new Person { Car = "car1", Location = "somewhere", PersonId = 2 }, new Person { Car = "car2", Location = "overthere", PersonId = 3 }, new Person { Car = "car3", Location = "overhere", PersonId = 4 } }; static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params Expression&lt;Func&lt;T, object&gt;&gt;[] properties) { var pis = properties.Select(x =&gt; (PropertyInfo)((MemberExpression)x.Body).Member).ToArray(); DynamicGroupBy(entities, pis); } static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params string[] propertyNames) { var typeOfT = typeof(T); var pis = propertyNames.Select(x =&gt; typeOfT.GetProperty(x)).ToArray(); DynamicGroupBy(entities, pis); } static void DynamicGroupBy&lt;T&gt;(IEnumerable&lt;T&gt; entities, params PropertyInfo[] propertyInfos) { var groups = entities .GroupBy(e =&gt; string.Join("_", propertyInfos.Select(pi =&gt; pi.GetValue(e)?.ToString()).ToArray())) .Select(x =&gt; new { label = x.Key, total = x.Count() }) .ToArray(); } Use it like this: DynamicGroupBy(persons, x =&gt; x.Car, x =&gt; x.Location); or DynamicGroupBy(persons, "Car", "Location"); Note, this will not work with EF or any other LINQ2SQL ORM. You will need to pull back your entire dataset into your application and work with it there.
Brief, but good advice. My team has started tagging both the unit test and the commit message with the ticket that's tracking the bug. It helps ensure that code changes to prevent bugs is tested before committing. One of our systems is a real PITA to unit test easily. Static classes that are hard to mock, logic buried in nested constructors, and methods that modify parameters instead of a return object, to name a few. I make our more junior engineers work on it a little just to get a feel for identifying bad code smells, and how to write their own code in a more future-engineer-friendly manner.
Nice. Thank you for response. Good way will be to show them how to make code better and maybe adapt it little by little for testing. Here you will get 2 benefits: get your code better and get you juniors better!
Why are you happy that they do? Pre-mature optimization is stupid.
&gt;Why are you happy that they do? Pre-mature optimization is stupid. It's not premature. .NET Core 1.0 was released over 2 years ago. In many cases, the code has its origins in .NET Framework code. .NET 1.0 was released over 16 years ago. These optimizations tend to show up in some of the most foundational types in the library (the blog post OP linked includes a link to an optimization on `System.Boolean`). For a framework used by (probably) millions, small safe enhancements to commonly used types that give incremental improvements to performance can add up across the whole ecosystem to significant savings in terms of energy consumption. I'm glad that the developers are doing what they do to make my code run faster than it previously did.
I'm not sure what you mean. In the web publish dialog there is a drop-down to chose the build config, and that is used during publish. There is no need for reload the solution. 
SlowCheetah is great! I'm a bit biased though. 
This is how web projects used to work before transform support was added. The issue is that often someone will update a single config file and not the others. Then users would find out after publish. We added transforms to web projects to solve this exact problem. 
There will always be edge cases like this as long as there is a reason to perform optimizations within a time limit. In another example there was a pretty long time when returning inside a nested loop was slower than a goto + return at the end of the method: foreach (...) { foreach (...) { if (...) return x; ... } } return y; vs foreach (...) { foreach (...) { if (...) goto x; ... } } return y; x: return x; For that reason you will still see code like that in some places like roslyn and corefx.
If the original bug came from a customer reported issue, it's always prudent to verify after fixing that the original scenario is actually resolved. I can see situations where just getting the unit test to pass doesn't solve their issue completely (and yes, this would imply insufficient unit tests).
Definitely, there's no silver bullet. Anyway after fox Ng bug you should try reproduce scenario and see that bug is fixed.
You don't even need octopus. Tools like Jenkins and teamcity are just as capable of deploying as they are building. Octopus is better, but expensive when starting out. Teamcity and Jenkins are still 1000x better than deploying manually. 
How would you tackle the ORM problem ? We use ORMlite with servicestack 
JIT is a compiler
Nope, same thing as what you’re describing. Here’s a dockerfile that I basically cut and paste from project to project: https://github.com/mikeruhl/Cronos/blob/master/Dockerfile
IEnumerable vs IList on the base type. http://www.claudiobernasconi.ch/2013/07/22/when-to-use-ienumerable-icollection-ilist-and-list/ could be a good read. Basically, IEnumerable for interacting over a read only collection, and IList if you want the collection modifiable
One is a private method that returns a new list reference and two new customer objects references each time it is called. Meaning, if you had an if block checking the length and then a for loop, both using the private method, you would actually be using two different lists. The second one does not have an access modifier so it depends where this assignment is located, but it only instantiates a single list. Using the sane example as before, you would reference the same list object and contents every time you used this variable. To put it differently, think of a cell phone: one lets you redial a number while the other requires you to enter the digits every time.
IEnumerable is read-only. Use IEnumerable if you only care about iterating over a collection and don't care about it's position in the collection. It will be more performant. If you need to change the collection or you care about the position of an item in the collection, use IList.
JIT would just refer to "Just In Time". When you say the JIT, you don't mean "Just In Time". JIT refers to either JIT compilation or JIT compiler. Roslyn compiles C# to IL, and RyuJIT compiles IL to native assembly. Both are compilers, and the .NET team also uses the same terminology. See [this](https://github.com/dotnet/corert/blob/master/Documentation/intro-to-corert.md) document, for example. 
I just have never seen the appeal of docket as someone who uses azure. What uses are people finding for it?
I know nothing about ORMlite or servicestack, so I can't be of assistance there. Even if it were EF which I am very familiar with, what you want to do would be difficult without extensive hacks and workarounds. You really should try StackOverflow. That site helped me immensely during my career. Lots of skilled professionals there to help, but you need to be more descriptive with your issues there. https://stackoverflow.com/search?q=ormlite
We use Serilog with a Splunk sink. No physical log files on our servers because the messages go directly to Splunk. 
I am using [stackify](https://stackify.com/) with pretty good results and also Visual studio 2017 performance profiler is pretty neat as well 
Chat is the most boring application of SignalR you can use for. The framework so amazing because it is so easy to use. I use it for all my SPA applications. 
Single page application applications?
ASP.NET SignalR is not new. SignalR on ASP.NET Core might be new, but SignalR is not new.
Don't. It's bad design on mobile. Also google HTML tables, float and flexbox. 
using LINQ
You can do this in SQL, if you are using SQL Server. SELECT ID,Key,Value,Version FROM ( SELECT ID,Key,Value,Version, ROW_NUMBER() OVER (PARTITION BY Key ORDER BY Version DESC) as RowNumber FROM Table)x WHERE RowNumber = 1
I second linq 
I third LINQ
C# is not Ruby or Python, let's not try to make C# into those.
If DC can get away with DC Comics (Detective Comics Comics) than this guy can get away with SPA applications IMHO Opinion.
Adding a dedicated `patch` keyword into the C# language itself is a crazy and scary idea... And likely not even possible with the CLR.
Group by the key, of each group you select a new object, as part of the selection you order by the version and only take the first (of each group).
This is exactly why things like proxt patterns should be used when working with external dependencies 
.OrderBy(x =&gt; x.Version).GroupBy(x =&gt; new { x.Key, x.Value}).Select(x =&gt; x.First())
Yeah that's a no from me, dawg.
- How would this even work? I could imagine some VMT overwriting magic for virtual method calls, but what would happen for early bound methods? Overwrite the code in memory? That seems nuts, and actually dangerous too. - What are the semantics? As long as assembly "Patch" is loaded in memory, the patched assembly redirects calls to the patch? I could imagine this could lead to some headaches with app domains. - What about versioning? This could go awry real fast. - Can you patch system types too? I am not even getting into the potential for abuse by developers in all sorts of cases, which is admittedlya frequent argument against pretty much any new language feature. In this case, this potential seems much much greater than ever before. Is it just me, or making a custom version of the library is not that bad at all? We have done that on multiple ocasions, I would even consider this possibility to be a major advantage of using open source libraries.
How much data do you have?
Been using this for ages, my fav email library :)
Thank you. I'm in with bitbucket, but never played with anything beyond the basics of gitting and issue tracking. Some things for me to checkout, bit it looks like another rabbit warren :) Thank you!
Hi immendorff, Your question is not so much dotnet related but more general about html and css. I don't think this subreddit will get you far. I'd recommend asking people over at https://www.reddit.com/r/css/, https://www.reddit.com/r/HTML/ or https://www.reddit.com/r/webdev/ Press on :-)
Jumping on the static site generator bandwagon here. I'd recommend Jekyll. Used it a lot and can highly recommend it.