[Not at all](https://docs.efproject.net/en/latest/efcore-vs-ef6/features.html#features-not-in-ef-core)
You're right. I posted this very quickly late last night. It's a lazy post, sorry. But I've long thought that most people haven't noticed this comment in the DbContext comments and just had an impulse to share. I've made other comments here that go over the reasons I don't like the Generic Repository pattern. But to me it's all about testing. I strongly believe that data access code must be integration tested with a live database in order to prove your tests are correct. It's too easy to write unit tests that pass using LINQ-to-Objects, only to find that your code fails at runtime because LINQ-to-Entities doesn't work the same way. Implementing a Generic Repository usually means you're returning IQueryable from your methods. This allows your data access logic to spill into any consumer of your repository. And thus the footprint of things needing to be tested with a live DB gets bigger and bigger, or grows out of control. It's best to limit the scope of things needing a live DB to test. Unit tests are billion times faster, and speed is crucial when doing TDD. But the real point of the post is simply that, the DbContext is already a generic repository. So why wrap it in another one?
as far as I understand, they're getting more logic into the sql queries in order to save memory and IO in the database. 
Right. But in the mean time we need to know what isn't being done in SQL so that we can write correct queries given the current limitations. 
I didn't say code reuse across applications was bad. But I tend more towards application-specific databases. I find it much easier to have versioned service contracts, than to have different versions of my data access assemblies floating around. What happens when you change the database? Don't all of your applications need to be brought up to date with the latest code now? I'd rather have a service as a single point to handle that than to worry about updating and redeploying all of the apps that depend on the data access code that also has to be updated. I'm not suggesting code reuse across multiple applications is bad. That's batshit crazy. But when you design your applications in smaller, business-specific slices (i.e. bounded contexts) you wind up with easier deployments, easier versioning, and better tested code. But feel free to keep flaming me and telling me what year it is. Because clearly you are the one stuck in the 1960s.
I guess you could trace your DB and see what EF is doing, but to be honest I'd either work with SPs in the meantime or wait for EF Core 1.1 or 1.2. To be honest EF Core 1.0 feels like EF 1.x back in 2008
https://docs.efproject.net/en/latest/querying/client-eval.html#disabling-client-evaluation
Because I'd have to do that manually for every model creating massive repetition of code vs doing it once for a generic repo and having all my concrete repos as essentially one liners.
I can see your point, yeah. But in this case you can safely keep your generic repository as an internal implementation detail in your DAL.
i was also wondering why i should return Iqueryable instead of IEnumerable, i thought that what was returned was not actually accessed by the db until it was used, so e.g. service layer does a .ToList on the returned object
You can have entity-specific repositories that don't include business logic. Get the data you require from the database and materialise it. In a manager you can perform your business logic. It's an extra call but in reality you can use this to get multiple different objects from different repositories (if you so wish to), perform some business logic and then return it. There's no need to be so sharp tongued, though.
Right. Create the sln in VS and then open the root folder VS Code.
How do you set the flag? Sounds very useful
&gt; But using Linux as your development environment means giving up Visual Studio you have to give up WPF, Registry System.Drawing (good bye image manipulation) net core is fun but there are many rough edges, if you're not going full web dev stay away from it
While I have always developed from a database-first paradigm where what you're saying is true, I think that the code-first paradigm that EF supports should not require that much upfront design and thus OP's question is valid in itself. I don't have a technical answer to the question, but it I might have a helpful alternate perspective: It sounds like that while you're developing you should maintain a way to create test data that produces a database filled for use in manual or automated testing. That way you can simply recreate the database rather than upgrading it from one small change to another.
[Hell No](https://www.youtube.com/watch?v=PB4Nby2Ai-g)
https://docs.efproject.net/en/latest/querying/client-eval.html#disabling-client-evaluation
This article is a week old.
Honestly, I never liked migrations because they aren't source control friendly and overlay an imperative model on what should be a declarative one. I prefer to model the DB with SSDT and DacPacs and then map that to the EF model. Sure you have to make changes in two places, but you don't have those sync issues.
Visual Studio Code doesn't do solutions like Visual Studio does. Basically you'll want a root folder that has all your project folders in it. You'll also have to emit the entry point in the project settings for any project you want to reference in another project (so if you have project A and B, but project B references classes in project A). I have a boilerplate project that has a multi-project set up for reference. https://github.com/mzrimsek/dotnet-core-postgresql-boilerplate 
Thank you! That's a valid point about cross plat, completely eluded me.. I've built a few APIs with node I will do some further reading into creating a pub/sub service. Perhaps I need to look at a different data architecture rather than creating technical debt with additional services. Thanks again 
With a generic repo you're pretty much forced to return IQueryable. Otherwise doing something like _repo.Find(x =&gt; x.Name.Contains("Bob")) is going to return every entity that has "Bob" in the name before you can do anything else. This might be what you want, but most often not. Usually you'd keep composing the query wherever you're using the repo. You'd wind up with something more like var bobs = _repo.Find(x =&gt; x.Name.Contains("Bob")); var oldBobs = from user in bobs where user.Age &gt; 45 select user; If you return IEnumerable from Find then the user.Age &gt; 45 query gets done in memory. If you return IQueryable it gets done in the database when you do a .ToList() or some other action that iterates the results (like a foreach). This is the beauty of IQueryable. But with great power comes great responsibility. Check out /u/k0te's answer above on how he keeps his generic repo inside the DAL (make the classes internal instead of public). Then create concrete repos to contain specific queries. I imagine in your case this will mean pulling things out of controllers during your refactor. Again. Good luck!
Threw this together. public interface IEntity { long Id { get; set; } } internal class Repository&lt;TEntity&gt; where TEntity : IEntity { private DbContext _db; private DbSet&lt;TEntity&gt; _set; public Repository(DbContext db) { _db = db; _set = db.Set&lt;TEntity&gt;(); } public IQueryable&lt;TEntity&gt; All() { return _set; } public IQueryable&lt;TEntity&gt; Find(Expression&lt;Func&lt;TEntity, bool&gt;&gt; pred) { return _set.Where(pred); } }
You should take a look at SQL CLR, I'm on mobile or else I would send you a link. I've used CLR procedures in past projects combined with table triggers to essentially create web hooks from the SQL server to notify when data has changed. It's a little more involved but makes real-time updates from the database language/platform independent. 
Or by micro examples https://github.com/dodyg/practical-aspnetcore
Sounds similar to what I did, except in my case a stored proc pushed a message to MSMQ.
[removed]
Ouch - no geometries. Sounds like they have a nice toy right now.
Fuck off.
Will that then work on VS 2015 as a solution?
I've written a full linq provider and linq groupby was one of the biggest problems to get right. It's such a pain, because the scope of the linq groupby is different from SQL's group by, the placement of the aggregates is different, especially if you have multiple aggregates, and on top of that, Linq's group by is not the same as SQL's group by as linq's groupby is actually a group by + a join: it returns a set of sets, not a single set like SQL's groupby. 
As far as I'm concerned, no version of EF, core or not, can ever be considered production ready as it's a stunningly inefficient way of interfacing with any data store. Dapper for life!
So, it looks like it depends on what you want to do. If you want specifically JSON to return, then you can ("should"?) use: public JsonResult SomeController() { return Json(variable); } ObjectResult however implements content negotiation. So, if you set it up... it will return what the client requested. It looks like it also has some helper methods to return different statuses, etc... https://docs.asp.net/en/latest/mvc/models/formatting.html 
Doesn't work on my Classic ASP site I manage. I get an IIS 404 error. Maybe it was an IIS exploit? Or early ASP.NET like you mention.
If you want it to work in Visual Studio 2015, then I would create it there first. Then open it up in your other environments. Here's how I would do it without worrying about VS 2105 support. mkdir SolutionName cd SolutionName mkdir ProjectOneName cd ProjectOneName dotnet new cd .. mkdir ProjectTwoName cd ProjectTwoName dotnet new [Repeat for your projects] cd .. dotnet restore code . Also, for the `dotnet new` command, checkout https://docs.microsoft.com/en-us/dotnet/articles/core/tools/dotnet-new You can specify what type of project and it will load up some stuff for you depending on how much you want to be given out of the gates. For example: `dotnet new -t web`
It's been looooong since patched, so it won't work on any live site nowadays unless you're running an ancient system with no updates
I'm new to all of this... But, in my project I had created a IRepository... then a Repository. Then I called on my IRepository in all my controllers with the Repository injected in. I didn't really see the need for all of this. But I did it because that's what everything said to do. And the idea that I could just pass in a different repository. Anyway, through some of Jimmy Bogard's I've dropped the repository and have been following his mediator pattern using his MediatR and AutoMapper libraries. So far, so good. He says to not use a repository until you need one. I'm still trying to figure out when I would need one. He states this pattern and building his apps this way is what allowed him to change out his ORM slice by slice instead of having one huge layer that relied on the same thing. [Tackling cross-cutting concerns with a mediator pipeline - Blog post](https://lostechies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concerns-with-a-mediator-pipeline/) [CQRS with MediatR and AutoMapper - Blog post](https://lostechies.com/jimmybogard/2015/05/05/cqrs-with-mediatr-and-automapper/) [SOLID ASP.NET Core - Video conference talk](https://vimeo.com/180160537) [SOLID Architecture in Slices not Layers - Jimmy Bogard - Video conference talk](https://vimeo.com/131633177)
I did a little (very little) with getting PostgreSQL up and running. This was a while back. But, it seemed to work. I'm going to be looking at it again. Otherwise (same project) we are using the MS SQL provider.
How much effort have you put into researching the answers yourself?, because everything you need to know is online with a few simple searches.
https://www.asp.net/mvc
No disagreement there, but it's what MS is doing at the moment. I'd wait for netstandard20: a lot of libs will be coming to netcore by then, and things have settled down a bit. It's currently 'early adopter' status. 
Have you thought about just using webapi? You can dump the pages somewhere in your root, and the only .net thing then is to build the api, which js could call via ajax
it was by appending ::$DATA to the end of a .asp script in the URL. eg. www.domain.com/page.asp::$DATA This would output the source of that asp file. Found this link: https://books.google.com.au/books?id=Fr9UOKzOjsAC&amp;pg=PA59&amp;lpg=PA59&amp;dq=see+asp+source+code+hack+::$DATA&amp;source=bl&amp;ots=YJX3eGvQBU&amp;sig=v75dEtSkZuL9iodWx8cavvMfPW0&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjsgLj4zeTPAhVJ62MKHb_9DksQ6AEISjAI#v=onepage&amp;q=see%20asp%20source%20code%20hack%20%3A%3A%24DATA&amp;f=false and http://www.iss.net/security_center/reference/vulntemp/HTTP_IIS$DATA.htm 
People just expect free stuff for document databases (clustering, etc). I used to use RavenDB pre 1.0 (during the godawful switch to Silverlight console). I prefer RethinkDB but heck RavenDB manages to stay in business. So that's a big plus.
This should be the way to go. There's no need for rewriting everything.
Thought about it but I am *forced* to use ASP.NET MVC
Love Jimmy Bogard. I use AutoMapper on pretty much every project. I've used MediatR once before, but not as much as I'd like. Thanks for the links. I don't think I've read those last two yet.
The guy was asking for an example. So I gave him one. But yes, it does cause the DAL to leak. I don't recommend it at all. 
I've been using RavenDb since 1.0. It's first big flaw in .NET-land is that it isn't a Microsoft product with massive native visual studio support. From the ops side it is a bit funky compared to other DBs -- it is loads better now but the only way we got it into production was that we happened to own production at the time. Finally, see https://jeremydmiller.com/2013/05/13/would-i-use-ravendb-again/ for some interesting feedback.
I'm pretty sure it is the price tag. And also (correct me if I'm wrong) the Windows dependency. For most businesses paying for something that only runs on Windows when there are free cross platform alternatives is a pretty hard sell.
Yeah, check it out. It's interesting... and seems to make sense. The last two are videos and go through his approach. The first one talks about making filters for validation and DB Context SaveAll. Thus taking out the same code you see on every POST. He also shows using AutoMapper for Projections on your queries. The last video really covers the mediator pattern using MediatR. I'm employing all of this currently in a refactoring of my project.
Yeah the price tag is bad, but since everything I do outside of work is open source I use Raven indiscriminate of the price tag. 
This workaround looks really nice and it was what I was trying to do! I'm more inclined in creating on VS2015 first however. By the way, I was using Yeoman which could help for this workaround.
Where is your application deployed? When I've been working on applications deployed through Azure I've used the secrets manager to mange secrets when developing and then added the secrets to app settings within Azure. 
Unless you're in a Windows centric shop and then it might as well come with a free cake.
I actually moved away from MySQL to PostgreSQL last night and the Npgsql driver worked perfectly first try. I don't know how solid it is in full usage, but for basic migrations, queries, adding and updating data it's worked perfectly so far.
I'll take a look. Thanks a lot! ;)
https://github.com/AvaloniaUI/Avalonia
Yes true but most people don't use Windows for deployment.
The thing is, do you really want a generic repository? Probably not. I've yet to see an IRepository interface that actually made sense.
Web forms have been around for 15 years and will be around at least another 5. It is I guess considered outdated. However if you are wanting to get the latest and greatest then learn Aspnet MVC on your own time. MVC is more than something that comes from Microsoft. It's a design pattern that is tried and true.
Interesting. I think it perfectly suits most modern web considering it is very fast it comes to parent-&gt;child relationships that goes several layers deep. I actually haven't used anything else in years. To each their own!
Nothing wrong with web forms for beginners, but you will get to the point where it feels like you have training wheels on. When you start fighting against the framework, learn MVC. And really, even MVC is outdated for web apps. Web sites are great with MVC, but for actual applications you'll want to learn Angular or another JavaScript framework. (Again, wait until you hit the limits of MVC.)
Totally lost me, explain? Perhaps an example?
Depends on your ORM. If you are using Chain, updates from projections are no different than from full entities. With EF, yea it's a pain in the ass.
I enjoy EF. I'm glad you seem to have a tool/workflow you enjoy too!
Yeah, but the actual tables and such would still have a parent child relationship, right? Or even a many to many relationship for employees and departments, right? You projection is just an abstracted layer on top of the data structure to save in transmitting data. That isn't really dependent on the DB, is it? 
Yes, there are still separate tables. But if you design it correctly, the application doesn't need to know that. Document style database are bad at projections. They can do it, but it is very inefficient. Whereas in a relational database, projections are fast. Conversely, queries that return whole object graphs such as EF or Hibernate are efficient for document databases (if stored in exactly the same way as the query) but slow for relational databases. 
The first time I benchmarked EF 6 code, I was so surprised it was so slow so I profiled it and debugged it to see where all the performance went. The thing is that it's so over-engineered, it needs a tremendous amount of time to get things done as call chains are very long, spanning many many objects. It's very fragmented. (same is true for NHibernate btw). 'Porting' it over would have been a massive problem, as they would have to cut a lot of code to make it work on .net core, but making cuts in a code base with a lot of dependencies between tiny classes is hard. I also think another reason is that writing a linq provider is an endless project: their linq provider was OK, but also generated a lot of problematic SQL (way too big). They went for a 3rd party lib (relinq) to rewrite the linq expression trees first for their new provider. What surprises me is that they didn't have time to finish that, as relinq itself comes with a library that does sql generation for almost all linq methods. Additionally, they already had written a linq provider, the knowledge to do things and work around quirks was already there. (besides, their system outsources a lot of the problematic things with Linq providers to the ADO.NET providers, so that they didn't have time is even more odd). If you look at their list of commits, the speed in which they write software is really really slow. Some devs write 1-2 lines a week. I always like to compare OSS projects with RxJava and how fast that project goes. It's night and day. The EF team is either a group of coders writing stuff in their spare time or they're spending the majority of their time doing something else. 
Well I think we are talking about moving complexity for data transformation to the database (which is good at managing data) from the application (which is not as good at it). All of that is fine, but that's where architecting your documents in the documentDB is important so there is a much reduced need for transformation and combination. For instance, the classic example is a blog. A user has blogs, blogs have posts, posts have comments. In a traditional, somewhat normalized RDBS, to load up a blog you would select against the BLOGS table to find the one you want, pull that data. Then go into the POSTS table, and (even indexed) have a relatively slow lookup of all posts that match that blogID. And if you are so inclined to show comments on page load, you now have to go into the COMMENTS table and find all comments that match all of the PostID. The idea for a document database would be to have a Blog document which has posts, which has comments. Much like an RDBS, there are a bunch of ways you could do this (I would potentially separate Posts from Blogs document wise) but the idea is that the document would closely align with your view so you are making one call with no data lookups. At most, you would change the order or only grab a subset, of your lists (posts and/or comments). The funny thing is you CAN build your document store just like an RDBS, with documents being the same as table rows with relationships to other documents, and it performs about the same. But there is no reason to do that. The timing on it is much much faster especially for reads, but the architecture of it takes a bit more thought. Once I got in the habit though, it became much more simple.
Stay away from EDMX at all cost. You end up with all your entities and relationships defined in a huge XML file. When you update your schema, you need to update this XML file. Doing it manually can be tedious and when you let VS do it for you, it will basically re-write the file so doing a diff becomes impossible. EDMX is for those who don't want to write code or create tables.
The message is hilarious - oh we just kill this new thingie we introduced a few years ago, no worries, now we have new shiny PowerApps. 
I use AI to monitor my website, but I've never used it for local debugging. Looks interesting! 
It's not about affordability. It's a question of whether that money is being wisely spent. Maybe you are lucky and have only ever worked for hipster startups with deep pockets who give you what you want whenever you want but I've worked in many different industries big and small and none of them would ever sign off 700 dollars without questioning it.
Yes that's about right. I did have a look at Lightswitch a while back with a view to using it for an in house production system that badly needs upgrading. I feel like I dodged a bullet now! I just had a look at PowerApps but I think proprietary software from MS has a health warning for me now.
I personally use a Macbook Pro for all my programming now (especially now that you can code in .NET Core on a Mac). For Windows stuff I just have a VM for that. Other than that, any windows laptop with 16+ GB of Ram should be sufficient. 
Thanks for the response. I like the sound of that but I think a Macbook Pro is out of the question as things stand but might be something to look at in the future. So I'll need something a bit better than 8gb then. Would you imagine something like [this](http://www.laptopsdirect.co.uk/aspire-v3-574t-core-i5-5257u-16gb-8gb-ssd-1000-gb-hybrid-hdd-15.6-inch-fh-nx.g1sek.020/version.asp) would be more than capable?
Clever. I'd love to hear more about how they take advantage of SIMD!
Thanks, I had previously been considering 8GB but based on responses it seems 16GB is gonna be better all round. Probably not too important for the old projects.
The problem with big heavy laptop is that it does not last very. Heavy laptops get trashed around as you carry them .
At work I'm assigned an HP Elitebook 940 with 16 GB RAM, a 120 GB SSD (although I'd prefer more space - 250 to 500 should be more than sufficient), and an i7-4600U CPU @ 2.10 ghz. I don't know if you really need the touch screen aspect. My work laptop above does everything I throw at it and it's still fairly light weight. As for monitor size, to me that's negligible - just get a docking station for it and hook up a couple large monitors to the laptop and use it that way if you're at a desk :)
I'll run that by them tomorrow and see what they have to say. Hopefully it'll get the green light and I can have a look for something a bit better than I have been looking at.
We're part way there already. I used a framework called Cappuccino back in 2009. It was basically a port of OSX's Cocas to the web. You could even use the OSX Interface Builder to create your app's UI. The huge drawback was performance - it was slow in 2009-era browsers. The framework is still around, and performs much better now - partly because of improvements to the framework and its tools, and partly because the JavaScript VMs in browsers have improved so much. There's also Xojo - formerly known as RealBasic. It lets you target both desktop (OSX, Linux, and Windows) and the web. The IDE and language feel a lot like an evolved VB6. I don't have direct experience developing with the web version of it, but the demos seem to look and function quite nicely. [CSHTML5](http://cshtml5.com/) also appears to be rolling along quite nicely. It looks like it has a good chance of becoming a way to create web apps using XAML and C#. I'm not sure how much web assembly will help - not because it won't be useful, but because we've already got the tools we need to create a VB6/Delphi type of environment for creating web apps, and we've had it for a long time. None of the ones we've seen so far have become popular, though. It might be partly cultural - one of the people who worked on Cappuccino wrote a thoughtful rant along those lines called [I'm done with the web](http://randyluecke.tumblr.com/post/45915323813/im-done-with-the-web).
Does it need to be a laptop? You can (usually) get a fairly decent PC for cheaper than a similar spec laptop. Plus you get more options for future upgrades, more monitors, bigger monitors, etc. We just got in some Fujitsu D556's at work, and they're great for the price. http://www.ebuyer.com/store/Computer/cat/Desktop-PC/Fujitsu
Unfortunately it does need to be a laptop otherwise I'd have happily gone with a desktop. 9 times out of 10 it'll be hooked up to a monitor in the office but working out on site or from home will be required too. Thanks though. 
Author should use darker text. It's hard to read.
I hope we never see one of these frameworks again, frameworks need to be powerful to do whatever you need to do, giving a simple cut down framework jsut to do some CRUD stuff with a bunch of grids and not much else isnt going anywhere.
I do, visual studio is great, i use it to write typescript, angular apps and all my .net code, it does everything i need + with resharper I see a lot of devs using mac books, i suppose that makes sense for not .net work, but in this .net sub, im curious as to why someone would not use visual studio in windows to write .net code. also say you are writing everything thats not .net, i still find visual studio a great IDE and would use it to write other apps that were not .net, that is only for front end e.g. a purely angular, nodejs, typescript project. im curious why a mac would be bettter here, what are the tools available that make it better. the obvious one is that if you are deving for linux then this makes sense, jsut curious what dev tools are on there that make it a preference
learn webforms, but after youre done with the class, learn angular + webapi, keep mvc in mind after. webforms is an abstraction over the stateless http to make it feel like youre creating a windows app that works over the internet, state is maintained in viewstate (not always the case). The whole idea was to keep web dev simple. i feel like learning webforms only won't teach you how writing internet applications really works on the web, how is state maintained over a stateless http protocol for example
Why do you want to edit files direct on the VM or remote location? I know there were some articles of working in VS and direct publishing (possibly editing) to Docker containers. Maybe same principle could apply.
As long as the return value can be cast to an IActionResult I don't think it matters. Alternatively you can make your function async and return a Task and skip using ActionResults altogether, and write directly to the response stream. But that's a completely different way of doing it.
There really shouldn't be anything to configure on the server side besides having set up the proper Application Server and Web Server roles. The only thing we had to do for our project was make sure the following references had "Copy Local" set to true: * System.Web.Helpers * System.Web.Mvc * System.Web.Razor * System.Web.WebPages * System.Web.WebPages.Deployment * System.Web.WebPages.Razor Nuget was used to install ASP.NET MVC 5 (Microsoft.AspNet.Mvc) to our project.
Yeah, the article doesn't really make much sense. Step 1. Install Serilog Step 2. Install Serilog.Sinks.Elasticsearch Step 3. Pass your ES url to the elasticsearch sink constructor and you're done. No queue necessary.
I'm worried about the dependency on a laptop for projects. Push the code into a version control system! (Also, should be relatively easy to open in vs2015) As for the laptop: look @ microsoft signature editions
Great question. The main reason I went with RabbitMQ is to remain agnostic to the log storage mechanism. This was a requirement of the project I'm working on and may not be necessary for most people. It will also allow me to stay flexible and add any additional log tooling I may need without editing my applications.
So, just to clarify, we went from .csproj -&gt; .xproj -&gt; project.json -&gt; .csproj? Circle of life, Simba
That's a valid concern. For my purposes, I am already using RabbitMQ to connect the services which makes this quite simple. However, with the goal of deploying immutable micro services, I think this is a good solution. It allows me to create services that dispatch logs to a message broker (RabbitMQ) and then I can handle logging however I want. I can build alarming pipelines, conditional procedures, and even entirely replace the log storage mechanism without ever touching the code in my services. On a small scale this may not be a big deal, but with each added service it becomes an increasingly difficult task. You bring up good points and this architecture is not the right answer for every situation.
Does the IIS server communicate through a proxy? You might need to set a default proxy to fix the issue (https://msdn.microsoft.com/en-us/library/kd3cf2ex(v=vs.110).aspx)
I don't think so. I'll have to check. But I can get the same thing to work in a console project (same method, models). So i know its not the API that is rejecting it. Its happening in my MVC project Controller when using System.Net.Http (HttpClient) when using the GetAsync Method to try and retrieve a resource. 
Seems a bit hard to tell from just these responses. Is there some code you can share? (like on pastebin or if this is hosted with some git provider?)
&gt; Is there a way I can develop inside a native Windows 10 app on my desktop (VS2015, or VS Core) Open the project in VS2015 or VS Core on windows.... done. &gt; and have it give me the right Intellisense/code hints for .NET Core on a VM or a remote server The VM or remote server has nothing to do with Intellisense, .NET Core is the same across Windows/Mac/Linux so the intellisense is the same. &gt; I mean, I could set up a file share if I just need file access and I can tell Code/VS2015 to use Core in its IntelliBrains. You don't need to tell VS2015 or VSCode to do anything this is all handled by the project when you create it.
This is my controller public async Task&lt;ActionResult&gt; Index() { var client = HelperHttpClient.GetClient(); var model = new MyModel(); HttpResponseMessage response = await client.GetAsync("api/activecustomer"); if (response.IsSuccessStatusCode) { string content = await response.Content.ReadAsStringAsync(); var active = JSON.Deserialize&lt;IEnumerable&lt;ActiveDto&gt;&gt;(content); model.Active = active; } else { return Content("An error occurred."); } return View(model); } The code fails on HttpResponseMessage
I confirmed that my API is returning the correct response but for some reason my controller in MVC isn't liking it. I think its header based... I'll update with my findings.
It can. You just need to install the .NET Core tooling for Visual Studio. It's the second link on [this page](https://www.microsoft.com/net/core#windows). That will give you the ability to create .NET Core projects from within Visual Studio, and also open .NET Core projects created elsewhere. You could use VS Code on Windows, of course, but you probably won't find it to be any faster than on Linux. It's not a 'native' app anywhere - it's a wrapper around a web app no matter what OS you run it on. 
&gt; So VS2015 can open Core project files? Works fine for me, make sure you have Update 3 installed and the latest Core sdk. &gt; Maybe it's only Code that works with Core project files right now? I've only used VS2015 so that is definitely not the case. EDIT: Looks like the core sdk will get you working with VSCode but in order for VS2015 to work you need the .net core VS2015 Tooling Preview available [Here](https://go.microsoft.com/fwlink/?LinkID=827546).
Thanks! VS'15 is definitely faster on my desktop than Code is in the VM, and VS is a lot heavier. 
Great, thanks!
We never went from .xproj to project.json. Those two went together. Combining the two into the new .csproj format makes much more sense and is what they should have done from the beginning.
I would think your web forms class would touch on html, css, javascript, C#, SQL, data modeling and various other web concepts. Those are going to be around for a good long while. Focus on concepts and the shared tech and MVC will be easy to pick up.
You can do the same thing with an elasticsearch watcher
Your trying to run newer code on VS 2008? The only way I know is to just copy/paste into a new VS 2015 solution. Visual Studio 2015 community is free, I recommend getting that if you don't need VS 2008.
I think you have that backward - the code was written in VS2008, so it is being finicky about upgrading. Code from '08 isn't likely to be set up with NuGet, Bower, etc, so finding any dependencies that weren't committed might be a challenge
I would avoid .NET Core right now. At least half the Core-specific stuff you learn is going to be wrong and obsolete a year from now. And really, most of the things you learn from studying .NET Framework (a.k.a. the original) will carry over to .NET Core. The main differences are in how projects and library dependencies are handled. 
Does this help? https://docs.asp.net/en/latest/data/entity-framework-6.html
The language is the same on both, and .Net Core is a subset of .Net, so it doesn't really matter. Core is still relatively immature though, especially regarding the tooling, that is going to change soon. So you might be better off going with the "classic" .Net...
I'm conducting a study to get opinions and feedback about a product for .NET developers. You will *not* have to download or sign up for anything! I will only ask you to view a website and tell me what you think. Immediately following the call I will send you a $50 Amazon gift card as a thank-you for your time.
Nope it doesn't. I did follow that guideline already and still got the error message when trying to add a reference of entity framework project to asp.net core web project.
Is dotnetnuke still a thing?
You need to target framework 4.6 in project.json and you will only be able to host on Windows.
Just curious, what's the format? Open discussion, or structured/recorded questions?
It's a throwback to .net 2.0. The app_code folder acts as an uncompiled application folder which gets compiled at runtime. Any classes inside that can be accessible throughout the application, so it acts like the bin folder, but instead of storing DLLs and other resources, it stores a flat structure (I don't think you can have sub-folders in it) of updatable code. I don't recommend this approach, but it works, as I'm sure you can see. The app_code folder is hidden by IIS so is inaccessible to external requests by default.
If .NET is a path you want to take long-term and a web/core/mobile path isn't set in stone, I'd recommend following the [.NET Standard](https://blogs.msdn.microsoft.com/dotnet/2016/09/26/introducing-net-standard/) matrix as a guiding light. Right now full .NET is by and large the main distro set of targets but Microsoft has been pushing hard for cross-platform (a driving force behind *Core) for years, and Standard is their roadmap; the various .NET distros will continue to merge in the near future. If you're into lower-level stuff like building compilers then Roslyn is a sure bet.
So if I create a base site in iis and drop in an app code folder, it will execute the cs code within by default if the site is hit by the uri specified in the binding? I'm migrating this site, which looks to be an existing attempt at a quickie redirect attempt by a predecessor to respond to calls to the uri/thing. Thanks! 
Although I haven't tried this, I believe there are some unofficial nuget packages that contain the binaries to do something like this. I'm not sure which version you are using, but maybe these links can help? [WiX Toolset](https://www.nuget.org/packages/WiX.Toolset) and [Windows SDK](https://www.nuget.org/packages/Box.V2/)
Cool thanks! I don't know why I didn't think to check nuget for these packages. This is a step in the right direction at least.
Because JavaScript is THE language, donchaknow.
This. Why learn A language, when you can learn THE language. Clearly someone doesn't see sharp. 
There is a .net core version of EF. EF 6+ are for the full .net framework. With .net standard and portable class libraries being more and more prevalent I wouldn't be surprised if eventually there is just one version of EF for everything, but I think that's several years away.
This helps... https://code.msdn.microsoft.com/101-LINQ-Samples-3fb9811b
For reproducible builds, you really need a build server. We have a virtual machine with vs and all dependencies and use teamcity to create the builds. Dependencies are checked into version control so all dwvs can have the same version.
I use both. VSTS for private repositories (free for less than 5 users) and github for anything public. Git is the same and makes it easy to move between the two if you need to for some reason.
I would stick with ASP.NET MVC 5. It's not going to become obsolete any time soon. I don't really see it being plausible to upgrade from MVC 5 to Core. 
I'd like to see Silverlight back under web assembly.
&gt; So if I create a base site in iis and drop in an app code folder, it will execute the cs code within by default if the site is hit by the uri specified in the binding? Basically, yes. The benefits of this approach is that you can edit it and see the changes pretty quickly, as you'd expect with something uncompiled. I would migrate this, but over the years, I've come to accept that sometimes, it might not be worth the effort to change code which is small, infrequently maintained and *just works*. **Edit:** Also, the web.config has nothing special inside it which allows the application to work any differently from a normal application.
Ah, well in that case the issue is it tries to take table names from the model name and not the DbSet name, as described here: https://docs.efproject.net/en/latest/modeling/relational/tables.html
We tend to make one event source per major sub-system. In the end, it's about making the event streams as focused as possible given their target audience (at least for channels like the Admin channel which are meant to contain only actionable events). ETW's main disadvantage is that it's largely local-only; you'll want some kind of log-shipper like the semantic logging service or winlogbeat to forward the events to a central service such as ElasticSearch, Seq, or even just a database (being able to see events across multiple machines is invaluable if your system is distributed). BTW, although we use ETW for a lot of our stuff, we also sometimes use Serilog (also structured logging, but less dynamically-configurable unless you hook it up to a log server such as Seq). Serilog's advantage is that it's more suited to the speculative stage of development; sometimes you don't know what events you want to raise until you have at least a rough idea of how your design hangs together.
Cool, but... whats the difference between just making a Windows 10 VM and installing the tooling and making a snapshot or template of the VM? Sure maybe it takes a bit of time to setup, but what's really different?
I use that WiX toolset nuget package and it works fine.
So you can use ASP.NET Core and EF 6, but only if your ASP.NET Core site is targeting the .NET Framework (e.g. net462 instead of netcoreapp.) That way you'll be able to use ASP.NET Core, though you'll be locked into developing / hosting on Windows. edit: A quick after-thought, I doubt there's an implementation of the new ASP.NET Core identity stuff that uses EF6, so if you want to use identity you'll have to roll you own.
That (or a version of it) is the most next logical things to do for Microsoft. 
Regular .net. Core is not ready at all 
Full ORMs: - NHibernate (oss, free. On the market for 13 years) - LLBLGen Pro (commercial. On the market for 13 years. Like NHibernate, highly mature, battle hardened, massive amount of features. (disclaimer: I wrote it) MicroORMs: - Linq to DB (oss, free. fastest (micro)ORM on .net: http://pastebin.com/SNRYPEcp) - Dapper (oss, free. not the fastest anymore, but decent amount of features) - PetaPoco (oss, free. fast, decent amount of features) - ServiceStack OrmLite (commercial/oss. same as petapoco) there's enough choice. EF core is pretty fragile, it's very easy to create a query which crashes at runtime or doesn't work at all. Mapping is very limited. EF6 has decent amount of features, but is terrible slow (although NHibernate is slow too) and a dead end. 
You do realize you can target the .NET Fraework 4.6.x in an ASP.NET Core project, right?
Yeah you can't. You'll have to use the new version of EF because the old versions don't target the same cross-platform API
You get a +1 from me for mentioning dependencies and version control. This is absolute sense and the right way. 
Except the OP literally asked about learning C# the language, not about desktop applications.
It is, but is somewhat painful to do by hand. You end up having to create a .NET Core project which targets .NET Framework first to get onto the .NET Core tooling, then in-place upgrade the ASP.Net version; there are a lot of base-class changes you will end up doing, and a number of general breaking changes, so the larger you project, the more painful it is.
Yes. That's what I meant. I guess the code review from these is as good as it gets. Which I know should be enough. But as I said it's still hard for me to look at just the code to see how it should run. 
I'm sorry what is the right way? He doesn't suggest an alternative solution to adding the Binaries to source control. You cannot rely on the dependencies top be hosted online, ref leftpad
To be honest, I've never successfully written a linq style group by without it. (Which is especially embarrassing because I live and breathe SQL.)
Fair warning, a LINQ group by is nothing like a SQL group by.
isn't the project.json file going back away in the next version of visual studio ? 
You can set up a folder for nuget packages on a network share or something if you want something "good enough" and simple. It's something that can be a bit time consuming to maintain, though. And another big issue with that is that you'll notice nuget will slow down as the # of packages grows in your network share. A real hosted nuget server (something like http://inedo.com/proget or http://www.hanselman.com/blog/HowToHostYourOwnNuGetServerAndPackageFeed.aspx ) will work better than that, but it's kind of up to you to analyze and see whether you're going to have a lot of trouble with a regular folder (which truly depends on the # of dependencies you have to replicate). Edit: But basically, the order of "bestness" (from my experience) is: 1. Hosted private nuget server close to (same network as) dev/build environment - Disadvantage is monetary costs 2. Public nuget server - Disadvantage is online connectivity/ possible temporary loss of packages 3. Simple nuget packages folder - Disadvantage is a relatively large amount of manual management and has a "soft" limit on # of packages you'll be able to host easily. 4. Avoid dependencies, NIH rulez - Disadvantages are numerous and obvious. 5. Dependencies added to repository (with a caveat that you will be keeping up with several dependencies over a long time, it doesn't become a real problem until you check in a new/updated library every other week) - Basically, repositories balloon and everything becomes slower and more difficult to put up with, especially onboarding/reloading projects (with git especially). If you have git and you have been putting 100s of MBs worth of libraries into your repository over time, this can become so painful that it starts to become an "acceptable" idea to nuke the repository's history and start fresh, which is a good indication that something is horrifically wrong with this approach. 6. Dependencies installed per-machine - Disadvantages are also numerous ... basically becomes a management nightmare on all fronts over time. "Works on my machine" You could probably argue 5/6 can be ordered either way depending on how much pain you've experienced in the past. That said, you _can_ add a dependency in the repository without as long as you do it _extremely_ sparingly. I'm certain someone will say "I have checked in this one library one time ever and it's not a big deal," and you're right. In fact, I've done that too (a C++ dll that will never need to be updated and is only used for the one tiny project, the overhead of coming up with a way to deal with it vastly outweighs any positives in that case). The issue happens when that's your main strategy as opposed to a true one-off.
&gt; DbSet name I like using Data Annontations as much as possible -- it makes API documentation and validation so much easier. Using [Table] is understandable.
That's exactly why I am worried. My project is still in early development now, and it happened to be at the time when Microsoft is making such a huge breaking change to its core library/framework. I am facing a tough dilemma. If I go with .NET core right now I have to use EF core which still lacks essential features and will make it painful for me to develop. But if I stick with .NET framework 4.6, it is an old technology incompatible with .NET core, so future upgrade will be difficult, especially as the size of my application grows. This is why I am looking to develop most components with .NET core including ASP.NET core, but to keep the data access/infrastructure layer that uses EF at the old version 6.1.3. It's a good workaround if it works but the question is how to get it work. 
I see! Developing on windows isn't a problem for me, but ideally I want a Linux hosting package, since windows is less mature, less secured and is missing the vast collection of tools available on Linux only.
As far as i know its ignored on non-iis environments
Or check out the reference source at source.dot.net.
I think the fact we're now need to uss strings to look up localizations as opposed to strong typed helpers is a step backwards.
When you create the ASP.NET core project, go into project.json (while you are on Tooling Preview 2), and change the "frameworks" node, to look like: "frameworks": { "net452": {} } This will put you on .NET Core Tooling, but with the library base of .NET Framework. You can still write your own code to target .NET Standard, which future .NET Core-only projects will run on, so you can switch more easily later. At that point you use NuGet as normal.
They are working on a .NET Core version though.
You didn't read his comments then.
That you can copy texts from Message box with Ctrl+C :)
Writing C++/CLI by definition requires Visual C++. Although Microsoft proposed as an official C++ extension, no one ever bothered doing it. The Mono folks tried for a while but it wasn't 100% compatible and as far as I am aware no one works on it In any case, from the description you got a job in a Windows shop, so better embrace how Windows developers work, fighting the standard workflows will only increase work. Having said this, you can of course call C++/CLI from the command line, and there are Vi and Emacs versions for Windows. 
Thanks for the summary. I think I will look into a hosted nuget server. We are already using GitHub.com for version control so I'm not bothered by having the server in the internet.
[Eric Lippert's blog is probably the best resource for this](https://blogs.msdn.microsoft.com/ericlippert/2010/09/30/the-truth-about-value-types/)
I have a company provided Lenovo T60 and haven't had any issues. Currently using VS2015.
I've used Asus Zenbooks for the last 6 years or so and am quite happy with them. I think that, as long as you have a good amount of ram (12gb+) decent processor power, and an SSD drive to install VS on, your coder should be a happy guy/gal.
I thought the same thing... Though its my first job and its an internship so I guess its worth.
Which monitors do you have? I've been looking for advice on monitors that work well with that configuration. 
We've switched to the Dell XPS 15 9550 for all of our developers. Works great, as you can spin up plenty of vms for testing, run several visual studio instances, and never really run out of resources. 
Two acer 24", not sure of the exact model. I think the biggest challenge people have is the mini display to hdmi adapters. I got the wrong ones at first, swapped them out for the 'correct' ones and it worked fine. I think you need active ones instead of passive if I remember correctly.
The XPS 15 9550 is a developer dream machine. 4k display, 32GB RAM, 1TB SSD, decent quadcore and a discrete GPU. I am on the verge of ordering one myself after I saw my colleague with this.
Dell 7559. Cheap. Also has dedicated graphic card if you are into Unity.
Are native docks a requirement?
Would not recommend a laptop for .net development. Visual Studio is a resource hog and mobile class CPUs are garbage -- I have a 4 year old i5 desktop machine that performs better than my 2016 Dell XPS laptop at work (which has a mobile i7) For the reason above, management allowed all developers to switch to custom desktop machines (roughly $2000 gets you a machine with an 8 core i7 and 32 GB of ram, which is still less than the cost of a MacBook) and use laptops for meetings.
I've been very happy with my HP Spectre x360. I got the smaller (screen size) model with 16gb RAM, 512gb nvme SSD, and a 6th gen Core i7 processor. I run Ubuntu bare metal and virtualize windows 7 for my Visual Studio environment. Something else I would strongly consider if I was still shopping, Razer's newest Razorblade Stealth. It is super thin and now finally comes with 16gb of RAM (a minimum requirement for my workspace).
So so. About 3-4 hrs average. But it's a desktop replacement, not really going to get great battery with a 4k screen, i7, 32gb ddr4, 500gb ssd and a nvidia 970m.
If your a .net fan (and Microsoft technologies in general), the Surface Book would be an excellent laptop for .net dev work, it's resolution alone is worth it. 
We use it as a glue between main .net project and 3rd party native dll that has C++ headers. 
I see, I understand this. I have another question though. If my entity framework project needs to reference yet another project, such as the Domain/Model project. Can Domain/Model project that will be referenced by Entity Framework project use .NET core/standard? Or it must also use .NET framework 4.6? 
One of the problems I have with CSHTML5 is the one-year limitation for updates. If I was to buy into it, I would want a lifetime package, and would gladly pay a two-digit-plus yearly subscription for anything that truly delivered on its promise. But $500/yr? Not going to get many evangelists punting the product at this price.
I know. It was just out curiosity so I can automate some stuff with bash/python. Always good to know.
You also can look into WSL (or "Bash on Ubuntu on Windows"), it exists since Windows 10 anniversary update. You can run native Linux binaries in windows. 
It depends. The issue is understanding which projects need to reference assemblies that require more than .NET Standard. Let's say you have Project A and Project B. You also have .NET Framework-requiring assembly E. If only Project B needs E, then you can set up A to target both netcoreapp1 (or whatever netcore flavour your assembly needs - best way to figure out the moniker you need is create a .NET Core project via the VS templates or scaffold a project using the CLI) as well as net452 - this is what will allow A to reference B, but also have the compiler keep you honest about only referencing pure .NET Standard assemblies in A - while ProjectB will have frameworks set up as I showed you above. Basically, think of the list of frameworks in the project.json file as the list of targeted frameworks when building a portable assembly. The more you have there, the more frameworks the assembly output from that project will be able to target. But any assemblies that project references must be able to run on all of those frameworks. (That's why you have to switch netcoreapp / netstandard to net452 in order to reference EF: that only runs on .NET Framework, rather than .NET Standard)
When I have to go to the office, I tend to use my Surface Pro 4. Great machine for developing on!
I find that Surface Books (and therefore SP4) aren't as powerful as I'd like. My next laptop is definitely going to be a quad core one again, and 16GB RAM instead of 8GB.
You probably have the XPS 13, which is dual core. The 15 is quad core. I find it helps quite a bit.
Agreed. I have two Zenbooks I've been using for years, and they're brilliant dev machines if you're on-the-go a lot. A lot of the Microsoft guys we work with use them too. You also can't go wrong with the Lenovo X1 range if you have the money.
I've been using a Surface Pro 3 since release day. The damn thing is awesome. I'll bet the SP4 is better. 
The lack of a touchscreen makes me nuts. Honestly, I didn't think it would matter, but once I started using a Surface Pro I found myself using the touchscreen naturally without thinking about it. Scrolling and pressing buttons is especially great. I hate MacBooks now because they lack a touchscreen. 
I routinely run 2-4 separate instances of VS2015 enterprise without issue on my Surface Pro 3 with 8gb ram. I have a beefier desktop that I use once in a while for games, VR dev. 
How is the battery life on the 9550? The 5510 is horrible. 
How many VS instances can you have open at the same time? 
Not great, but it's a desktop replacement, not an ultra book. 4 hours while doing dev related work is pretty average. Not pushing it and doing basic web browsing, you can probably push it to 6.
Nothing. They are fantastic windows machines. Apple haters are blind to the fact that it's easy to get Windows running natively on any Mac since 2006.
doesnt tell me how good you are with Javascript compared a guy who do JS all day with framework like React, Angular and the open source stack outside Visual Studio etc etc
It's €2400 here in Belgium. Cheaper than a weaker spec MacBook Pro. Edit: and if you are paying a developer upwards of €400 per day (realistic figure for a payroll developer), it doesn't matter if the machine that makes him productive for the next 2 years costs €1000 more than a mainstream laptop. Edit 2: Turns out it's [€2449](http://www.dell.com/be/p/xps-15-9550-laptop/pd?oc=cnx5534&amp;model_id=xps-15-9550-laptop)
The best developer presentation I saw was by The Gu (Scott Guthrie). He kept it concise and knew his stuff. His code demos were screenshots which surprised me but it suited his purpose. The worst demo I saw was when the presenters had their demos on virtual machines. They BSODed the VMs (easy to do with VirtualPC) and couldn't get them back. The room emptied in about 10 minutes.
like what ?
i'd say we know as much javascript as we need to. if the project requires more of it we just learn more. but i'm not comparable to a frontender that's always up to date with the latest frameworks and thinks in terms of those frameworks. that said, i did use angular, a bit of backbone and (of course), jquery
that's what I mean but some JS programmers told me that C# or JAVA guy are crappy web dev because they dont use the latest JS framework or the right "practice" think emascript standard etc I was wondering if I get back into programming since im a network sysadmins guy, should I learn C# or JS because it seem there is a lot going on in the frondend JS world but it seem to change a lot too.. prefer the C# way with compile and it can target a lot of platform now tell me about it a bit and how JS from you differ from JS from a frontender like xaml and all theses things aren't included in the open source stack they dont even use visual studio etc sublime atom or vs code rarely
[Upvoted.](https://i.sli.mg/QoKDIS.jpg)
3-4 hrs under load, but for simpler stuff like editing documents and browsing the web I've had 6 hours
Sounds about right. I'm also not a fan of SPA websites so prefer to do MVC/Razor pages with Vue.js where needed. Because of this I'll never go the whole hog as such and get much experience with the full-on js stack. I have dabbled in angular and it seemed it seemed pretty cool, but just... Another way of doing the same thing along with it's own set of problems :)
It's a meaningless question. I've written .NET apps that have no JavaScript and I've written ones that are literally angular frontends. Personally I kind of like a mix. JavaScript tooling is absolute shit, especially if you need to use different configurations in different deployment environments. You can, and I have, made gulp to it, but it's a lot of hoops to jump through compared to just having your IDE do it automatically, so it's nice to use some features that 'pollute' the markup. Also, you don't really need to know very much JS at all to use stuff like angular and react, so it's a pretty stupid comparison.
Is it really that bad? I am considering buying one for WPF development using Parallels. Can someone please share their experience with that?
Thank you! :) Why didn't they agree with it? Perhaps we can change things for the future to make your life easier :) (If they only want things from MS, then that's of course not changeable, but then their EF inflicted pain is what they asked for ;)). 
Parallels is the fastest VM I've tried, by quite a bit, but I still find it quite a bit slower than normal Windows. It does get rid of some of the trackpad issues, but brings in further issues with Parallels screwing up keyboard shortcuts. Also, expect to cut your battery life to 1/3 of running OSX when using Paralells. :P On both my mid-2012 rMBP and Skylake i7 retina iMac, I generally prefer to reboot into Windows if I'm going to be doing Windows development for better performance, rather than using Parallels. It's acceptable performance, especially on the iMac, but not as good as I'd like. Also consider that every time you switch from Parallels to Bootcamp you'll have to reactivate a lot of software (such as Linqpad).
These days, the big difference between a backend dev and a frontend is not how much raw JS you know or write, it's how familiar you are with the JS ecosystem - language version, transpilers, dependency managers, frameworks etc.
Pretty decent, but I honestly don't use it unplugged for now than a couple hours very often.
Not sure where you are getting those options but it's only [$2600.](http://imgur.com/a/uBOoM) This was a pre-configured option in the business section of dell.com. The consumer version with windows home is $50 cheaper.
I agree. I have enough experience that I can comfortably work in and even improve the JS code that front-end devs write, but I don't have the knowledge of the ecosystem that it takes to get the front-end setup and running.
My JavaScript and C# skills are typical for someone of my experience. The difference for me is that I'm good with C# and I enjoy coding with it while I'm good with JavaScript but I don't enjoy coding with it. What can you do? JS forces your hand in the frontend, regardless if you like it or not.
First point is not valid, because we're talking hardware, not software. I have been using macbook pro with windows installed for over a year for work, and haven't had any issues with Bootcamp. Everything works as expected. I used HP Zbook 14 at my previous place, I had much worse experience than with my macbook. Random reboots, loss of wifi after waking up from sleep, other colleagues complained about overheating. Perhaps you can do better value for money, but I love the quality of finish, 8hours on battery, retina display, and the best touch pad I ever used.
I just ended up changing my Program.cs to the following to enable command line database seeding which allows me to run Database Seeder classes by running (like https://laravel.com/docs/5.2/seeding#running-seeders) dotnet run seed SeederName - public class Program { public static void Main(string[] args) { var host = new WebHostBuilder() .UseKestrel() .UseContentRoot(Directory.GetCurrentDirectory()) .UseIISIntegration() .UseStartup&lt;Startup&gt;() .Build(); if((args.Length == 2) &amp;&amp; (args[0] == "seed")) { var seederType = Type.GetType("WebApplication.Seeders." + args[1]); var seeder = (ISeeder) Activator.CreateInstance(seederType); seeder.Seed(); } else host.Run(); } }
just spec it with 8GB (1x8GB) initially. It's always much cheaper to buy RAM elsewhere after. You can get 2x16GB for less than $200
I don't even know JsonAPI was a thing. Neat.
Hell, I can't even figure best module for X on vanilla .net. doesn't help when Microsoft has a dozen old libs that are zombies but nobody replaces them because there's a de facto standard ms implementation (or 5 - xml serialization, I'm looking at you)
Newtonsoft has a Json thing that's nice and neat and useful. In project.json it's something like: `"Newtonsoft.Json": "9.0.1"`
I came here to suggest this. Beat me to it 
Thank you for your suggestion. Newtonsoft.Json is an all purpose Json serializer. I am talking about a serializer that follows the http://jsonapi.org spec. Checkout the links on my post to see what I mean.
It's not super clear why I'd use this over stock .NET Core Web API stuff. TLDR?
Don't think too much into it. As the old saying goes, "Whatever you set your mind to...... you're probably right." Relax. Let go.
From their website: http://jsonapi.org/implementations/#client-libraries-net https://github.com/scott-mcdonald/JsonApiFramework 
Right , the 'core' name is really confusing because it is really a technical term but was used as a name IMHO
$exception in the immediate window or a watch window will display the properties if there is one on the stack.
https://msdn.microsoft.com/en-us/library/system.threading.tasks.paralleloptions.maxdegreeofparallelism(v=vs.110).aspx
Parallels works well. Performance is fine (doing web dev with heavy-ish SQL load). I normally work with two external monitors, one is Windows with Visual Studio and the other is MacOS. But even working from the laptop alone is fairly decent. The biggest drawback is keyboard shortcut mapping - it adds a fair amount of cognitive load to keep everything straight even with Parallels doing remapping for you. It takes time to adjust.
That does work. But the object doesn't necessarily have the same consistent property names. I want to use a dependency property on the main component that sets a variable binding.
Can you run it always in parallel but on very low priority? In such case it will utilize maximum available resources and will not interfere with user experience. 
Lesson to learn, never postpone the catch block code. 
How to use tasks and you can start a new one with little effort with Task.Factory.StartNew Also, you have to wait for your tasks to finish either with the more proper way, waitall, or just Thread.Sleep(long time) (just good for basic testing!!!) or the processo will exit. 
I have used that a lot. 
News to me.
Web forms allow you to hit the ground running faster with server side development, but it is almost impossible to implement TDD and write unit tests for, and its maintainability is inversely proportional to the project’s size. Even when very well planned, it accrues technical debt at an alarming rate. MVC needs more planning, more up-front work, but is far easier to maintain and extend down the road. As long as you keep to best practices, you can avoid most technical debt relatively easily. Plus, its higher degree of abstraction can make it a joy to refactor under many circumstances.
.net 4.5, 4.6, and core support added. Your Help will be appreciated... Also first stable release published...
Are you talking about entity framework core?
Why did they choose a name that is used and copyrighted in some countries? https://en.wikipedia.org/wiki/Yakari 
I was able to achieve what I was trying to do with a custom Behavior. It wasn't a standard binding issue, I should have presented more code to be clear.
And it's going away. Enjoy.
I don't understand MS guys these days.
My understanding was that the command was "Add-Migration". I'm not familiar with "Scaffold-DbContext". Changes to the models then require subsequent "Add-Migration" and "Update-Database" calls to scaffold the changes. https://docs.efproject.net/en/latest/platforms/aspnetcore/new-db.html#id6 Are we talking about the same thing?
Oh I agree and you can follow this on GitHub: https://github.com/dotnet/roslyn-project-system But MSBUILD lives on and the reason is backward compatibility. They can't kill project.json without alienating all the Microsoft MVPs that saw this as"the end of XML hell". They have to bring the good stuff. Eventually, maybe, in a few decades, they will kill MSBUILD. Until then, who knows.
also worth mentioning: gogs. much less bloat, but still quite functional. 
No thanks - Ionic 2 for me...
Would love to hear your thoughts on how they compare!
I don't know about that... the MSBuild Target hierarchy for building a web app for example is extremely complicated and there's way too much inter-dependency. The whole process is poorly / not-at-all documented, and very touchy to setting a variable trying to override one behavior, only to find that it breaks something in a seemingly unrelated area. I was _thrilled_ that they were moving to project.json largely for the break from MSBuild and I'm really disappointed they're moving back to it.
When does the tooling come out of preview?
It's near the end: &gt;We intend to ship the final 1.0 version of the .NET Core Tools next year. This situation should get better. It will enable us to ship a `1.0.0-sdk` release, with no `preview` string. 
&gt; Moreover, there wasn't a way to have always the LATEST version. Isn't that, like a GOOD thing? Moving to a newer version of any package should be a conscious decision, right?
That's how ef6 works.....
Right, my response is to the OP, trying to figure out if we are talking about the same thing.
&gt; 1380 APIs were added in this release.
As tempting as that sounds, I'm hoping not to have to reinvent the wheel. I'm not sure if I have the time for it, either.
Some people would love to both run the blog and customise it using the language they love.
[removed]
Wilderblog: build failing
Blogs tend to cover almost all basic functionality you would use as a web developer (databases, dynamic content, etc). They're great to pick through when trying to move beyond "For Dummies" levels when learning a language.
Ignore me, I'm being dumb and not storing each connected socket in a list to iterate across. 
Come on. Where's your sense of adventure?
I really wish you'd port the old System.Data. I've got a lot of scripts that use the DataTable with Mono that I'd like to move over.
[removed]
 var host = new WebHostBuilder() .UseKestrel(cfg =&gt; cfg.UseHttps(cert)) Is there a way to configure the certificate on a per-request base? e.g. if you want to host on different domains at the same time.
Hell, I don't even know a well known regular .net blogging software...
On the web projects I worked on, we always did it via IIS.
People shit on repos, but honestly I haven't seen anything better.
Are you talking about enforcing SSL requirements? If so, how would you conditionally enforce it (if not all your routes should have SSL) at the web server level?
I'd add that you might want to permanently trust the self-signed IIS Express cert. Here's an article on how to do that: [https://blogs.msdn.microsoft.com/robert_mcmurray/2013/11/15/how-to-trust-the-iis-express-self-signed-certificate/](https://blogs.msdn.microsoft.com/robert_mcmurray/2013/11/15/how-to-trust-the-iis-express-self-signed-certificate/)
Meh. If they're using semantic versioning, then it means that they only updated the patch version, which technically shouldn't have any nasty side effect. My complain was more about letting the package manager update something from, say, version 3.0.0 to 4.0.0, where things will definitely break.
The app I just deployed is via termination through NGNIX.
There are some blogging/cms options here. http://reddit.com/r/csharp/comments/59gji9/a_collection_of_awesome_net_core_libraries_tools/ 
For console apps, ASP.NET / web apps, or UWP apps, .NET Core seems to definitely be the future. I've been able to upgrade console apps and SPA + Web API apps without issue. If you're making old school WinForms apps, those don't seem to even be on the radar for .NET Core. I don't see the need to port those over to UWP / Xamarin / etc if all you want to target is Windows.
Hey great job with the awsome list!
The NotImplementedException is a very good idea. 
That is for ValueTuple and deconstruction, both planned features of C# vNext. https://github.com/dotnet/roslyn/blob/master/docs/Language%20Feature%20Status.md specifically https://github.com/dotnet/roslyn/issues/347 (`(int sum, int count)` construct is sugar over a `ValueTuple&lt;int,int&gt;` struct type defined here and used in these extension methods; tuple deconstruction is discussed in this link as well) I am more excited about [`AsyncValueTaskMethodBuilder&lt;TResult&gt;`](https://github.com/dotnet/core/blob/master/release-notes/1.1/1.0-1.1-api-diff/1.0-1.1-api-diff_System.Runtime.CompilerServices.md) (see [arbitrary async returns](https://github.com/ljw1004/roslyn/blob/features/async-return/docs/specs/feature%20-%20arbitrary%20async%20returns.md)) 
But why T1 ... T21? Is there a reason for that besides "we had to draw the line somewhere"?
With a reverse proxy like Nginx you just expose SSL (or rewrite) to the outside while HTTP is used inside. You are correct about condition routes, but I can't think of many cases where you wouldn't just want to use https everywhere if you had to use it anywhere these days.
I think the main thing to concentrate on is that .NET standard is (hopefully) the future, so if you want stability for your development I would go with .NET 4.6 and once .NET standard 2 comes out you will already be supporting that. As .NET core is matures you can switch over components without too much pain if you want. .NET Standard will allow your .NET 4.6 apps to consume .NET Core components like EF Core. https://blogs.msdn.microsoft.com/dotnet/2016/09/26/introducing-net-standard/
Thanks for showing some love for WinForms. It is popular to say "obviously you should use WPF instead of WinForms" but I find that WinForms is great for certain problems.
That's pretty much it I'd guess. The compiler might work with user defined versions with more parameters (for the `Func&lt;...&gt;` delegates for example you can create however many you need: http://stackoverflow.com/a/7564120). There is this proposal: https://github.com/dotnet/roslyn/issues/5058 to help eliminate this problem in the future, but it looks unlikely to happen anytime soon to me.
How about creating your own yeoman generator? http://yeoman.io/authoring/
For me personally, I stopped writing a lot of desktop GUI apps when VB6 died: I switched to heavy back-end web development, and until recently, didn't do any desktop apps. WinForms is an easy and natural successor, but it clearly shows its age (and not just aesthetically). WPF is amazing and beautiful, but sometimes I just have to get my prototype and its functionality in someone's hands. And, of course, I don't keep my "models" and "controllers" in WinForms controls: I put those in libraries. So, for me, WinForms is just a very thin wrapper around the business logic/algorithm/etc. Really, though, I am spending a lot more time on WPF these days. I realize my love of WinForms is like that of a grognard proclaiming the high-water mark of D&amp;D was wither AD&amp;D or AD&amp;D 2nd Edition. That is: most modern players of D&amp;D would disagree, yet some of them play Pathfinder (Essentially, D&amp;D v3.75 after the Open Gaming License was released by Wizards of the Coast). Anyway, this analogy is ridiculous enough without further commentary on my part.
Out of curiosity, what part of EntityFrameworkCore are you seeing as not production ready? I ask because I'm in the midst of a similar migration myself and have not yet seen a reason why EF7 cant be used in a production environment. Migration is working, and it seems the most common attributes are supported. Edit: My opinion on your scenario is such. If youre not going to keep it up to date, Id go with the framework, not core. If you had posed the question 6 months ago, youd really only have one option as EF wasnt even released as final yet. The power of Core is going to come with Standard and open source contributions. As youve stated you dont have intentions of keeping it up to date, both of those strengths are negligible.
EF core doesn't support data lifecycle events
&gt; You can target any version of the .NET framework from 4.5.1 up https://jonhilton.net/2016/09/07/using-asp-net-core-against-net-4-6/
.NET Core is the way to go for console apps, UWP and contained webapps. Once .NET Standard 2 (https://blogs.msdn.microsoft.com/dotnet/2016/09/26/introducing-net-standard/) becomes available you'll be able to target both platforms... 
You might want to use a development cert: https://github.com/Eun/test.bi
WPF starts falling down when your number of bindings get above 10k. This can happen quite easily when dealing with Grids. I work on a risk management app for an investment bank. Our whole app is WPF except for the grid which is WinForms. Every WPF grid chokes on 100 columns and 100k rows. The scrolling becomes jittery or you loose the ability to scrub.
Well, the built in DataGrid is pretty bad at virtualization. That being said, I have not had an issue and everything I do is WPF at the moment. While you are learning you do need to spend time tweaking but after that issues like this are easier to solve.
Yeah, I was going to say, this is a problem I ran into years ago when we started ported WinForms apps but it was very low effort to solve once I figured out how control virtualization works. There's no good reason you would need to have the controls instantiated for 100k rows at once. Later we started using Telerik controls that are basically set up to do all of that stuff for you right out of the box.
WCF isn't old fashioned at all. It's incredibly versatile and that comes at the cost of complexity. I do wish you had more visibility into what's going on sometimes though, especially when trying to configure a new service and it's not working.
I wonder if you could decompile them and use them in your .NET Core software.
&gt; WCF isn't old fashioned at all. I wasn't saying that it __is__ but that I __get that feeling__. I mean, you do can transport objects more easily but, like you said too, sometimes there are way too obscure configuration that you need. &gt; I do wish you had more visibility into what's going on sometimes though, especially when trying to configure a new service and it's not working. That was what I was talking about when I said that it was way to complex.
So one thing that's fantastic about WCF is you can expose the same services on multiple endpoints. I recently did a big project where part of the API was exposed publicly via HTTP. The same functionality was also needed internally by different parts of the system, though with a much heavier load than would be coming from the public side. I was able to expose the same service as HTTP on the public internet and then a completely binary version over TCP on our internal network. There was a big performance bump, which was sorely needed, by not doing all the intra-server communications over HTTP. Since both endpoints ran the same methods, it allowed me to deal with a bunch of concurrency stuff in one place, which was a big issue in certain key places (this particular app was very time sensitive down to the single millisecond scale). Since this was a pattern that we are going to be seeing a lot, I built the configuration stuff into a base class. This way if you needed to make a new service you just derive from that base class and implement the service logic. The configuration was already done for you. It takes all of 5 seconds to get a new service up and running correctly. It worked extremely well and I'll probably be expanding it to include a SOAP endpoint for communicating with other servers outside of our domain. This was the first time I tried doing the config in code and I'm never going back. I made it so it pulls some of the configuration stuff from the database (IPs, security settings and things like that that are environment dependent). This way when it's deployed to our different environments (Dev, QA, Staging, Production) it automatically sets itself up for the environment you've deployed it to. Before that you had to be very careful that you made the right changes to the .config file or you could end up with your QA service running against the production database. Now it's basically idiot proof. You deploy the code, it figures out where it's running from and configures itself appropriately. The .config is the same in every environment.
Yeah, UI virualization in my experience means trading loading time for scrolling time. So lets say you have at trader with a 32" 2K display with your app maximized, he can see about 60 rows and 30 columns at once, with about 30 bindings per cell, (data, styles, etc), that means about 54,000 bindings to redo if you use UI virtualization and the trader scrolls. We've tried driving the grid with a fixed size data set that represents the visible part of the screen and we just shift values left/right or up/down as the user scrolls. It works better, but it is horrible to work with from an architectural stand point. Or, I just use the WinForms DataGridView, turn on double buffering and data virtualization, and it just works.
Probably intentional since writing raw ADO.NET using DataTables and DataSets is almost never a good idea, and was a bad idea 10 years ago.
Happy to answer questions. More on Windows Containers and .NET / ASP.NET here: http://blog.alexellis.io/tag/windows/
When you're dealing with an Application Server in another language, without any connection to a database from .NET, DataSets make a simple transport mechanism. We don't use ADO.Net connectors.
Do you use SQL Server? Just curious as I was really hoping with Entity Framework core it would play just as nice with other databases as it does for SQL Server. 
DataSet/DataTable will come back with netstandard2.0. For now, you can use DataTable-like alternatives from 3rd party libraries (like RecordSet from https://github.com/nreco/data ).
Besides the possibility to host many of them at the same time with a configuration change (have you tried in code configuration?), id also consider the possibility to use named pipes, queues (under certain conditions), relays, pub/sub patterns... Everything in a way that doesn't affect the logic of your service nor its definition by the slightest. Also I would add to the list the support for features like transactions (try let a transaction flow over multiple hops on a rest api) and service discovery, routing (therefore load balancing). Last but not least, if you dont like xml and your services are locked in a local scope, you can simply replace the datacontractserializer for something of your liking (json or protobuf) All things you can definitely implement in rest, but why reinvent the wheel?
[Community server](http://adopenstatic.com/cs/)
WCF feels like black magic, it's the first only part of the Fx that instill struggle with past the basics. Reading the wcf book made we want to gouge out my eyes 
https://github.com/dodyg/practical-aspnetcore/tree/master/configuration-ini-options This is a new example with nested ini key and option object.
Looks cool. Thanks.
MEF
MEF doesn't handle versioning, just composition.
So no new tooling, no .NET Standard 2... I'll wait for those two. Until then I'll use the full framework for production.
Query object pattern - if thats the name - is a good alternative.
I'm not even targeting Core for my libraries. I figure .NET Standard covers it for 95% of the use cases.
EF Core barely has a SQL generator, meaning that you can end up accidentally sucking the whole table into RAM.
DataTable is bloated and slow, but it is also how ADO exposes schema information. Losing it is like losing Reflection.
You can pass your where clauses as lambdas and use includes to eager load the joined tables. I don't know of a gui to help you but I would be interested to check it out if you find one. Good luck! 
You should be able to create Multi SelectLists (that show the columns of a table) where only the most fundamental one is default (only one always on the page). From here, you should be able to "add" different tables (via relationships from the base table), and have them appear dynamically (whatever dynamic method you prefer). Once you have all tables in, you should be able to select the columns and press a button to have them appear in a section below. From there you can provide inputs to manipulate the fields and a submit button to create the LINQ statement that builds the report. On mobile, will try to update with a more concrete example. This is my 10k foot overview. --- **Edit 1:** Okay, a deeper look at this on a proper desktop leads me to a bit of a fork in the road: - If this project will be growing and changing down the road, it might be best to [pull your information directly from the database itself](https://stackoverflow.com/questions/32704787/display-the-columns-of-a-sql-server-table-in-asp-net-mvc-c-sharp). This will be hard, but will make your feature adaptable and resistant to changes (you won’t have to update it if the rest of the database changes). - If you have a stable set of database tables, and you know what they are and what fields they have, then it would probably be best to build a set of “query tables” that hold: 1. Your table names 2. Their relationships to other tables 3. The fields in each table Basically, you will be getting really meta here, by making tables to hold information about your tables. The upside is that you should be able to build queries from this information that you can run against the actual tables, and have everything surprisingly simple to build. The downside is if your overall database changes in any way, these “query tables” would have to be updated by hand at the same time. Any structural deviation between their contents and the tables those contents represent *will* break your feature in some way. More to come.
Thanks. I think its the deserialization that's messing with us here. I edited the post with another update. I may try a really simple example to see if I can repro it in a simpler way.
Automappers UseAsDataSource feature is fantastic for pages like this. You set up a DTO that combines data from multiple tables, and set up the appropriate mappings so that Automapper knows how to populate the table in a single query using joins. Once that is set up, you can call .UseAsDataSource().For&lt;MyDTO&gt;(), which gives you an IQueryable&lt;MyDTO&gt;(). You can apply linq operations to that queryable that will be translated into the SQL query (as long as they are supported by linq to entities, of course). 
&gt; Each ORM is presented as its own xUnit test project. &gt; To ensure each ORM is "playing by the rules", a shared set of tests will be used. Pretty neat, will make for some good comparisons.
You don't have to mock functionality you aren't testing. Among all the good reasons unit testing isn't one of them. Like most arguments for maintainability and readability, the better solution is the Service. 
Thanks for this. I liked your original article comparing the three. While you're here, if you have a moment, what's your opinion on the stance "Entity Framework ALREADY is a repository". Is it still worth abstracting the technology?
Cool! I was just clicking run test after every change in mstest. Glad to know there's an easier way. 
Having gone through two migrations to different ORMs and about to start a third my answer would be yes, absolutely! If you use EF as the repository and don't abstract you have to bring in EF dependencies into every project that consumes the DAL. It also makes a move away or even just a disruptive update like EF core an onerous task.
Thanks Josh, I agree with you on this. If you don't mind telling, why the multiple migrations?
Oh no, I was still adding features to a VB 6 application in 2010. But the 4.6 upgrade is painless, so everything gets upgraded whenever it is touched for a new feature.
I used to think that the EF data context was a repository, but I don't believe that now. Simply far too much boilerplate is pushed to the caller. Look at the code examples for Update and Delete. Is that the kind of API you would have written if you were making an ORM from scratch? Even Chain, my pet project, doesn't quite fit the bill because you need to deal with implementation details such as table names.
Would you be interested in contributing a LLBLgen version to the cookbook?
Needless? Look at this code: using (var context = new OrmCookbook()) { context.Entry(classification).State = EntityState.Modified; context.SaveChanges(); } Is this the API you would create if you were writing a repository from scratch? Really, how many people even know that setting `context.Entry(classification).State` is the correct way to update a record? Or how about deleting a record by its primary key: using (var context = new OrmCookbook()) { context.Database.ExecuteSqlCommand("DELETE FROM HR.EmployeeClassification WHERE EmployeeClassificationKey = @p0", employeeClassificationKey); } What you see above is the correct way to delete a record by its key using Entity Framework. When I think "repository", this is not what I have in mind. 
This is where the Repository model falls down. Why are you dealing with the change tracker? Because you are abstracting your DB layer. If you deal directly with EF then its proxies capture changes automatically. Deleting isn't hard either, it's one line: DbSet&lt;T&gt;.Remove(DbSet&lt;T&gt;.Find(id)). The biggest problem with the EF Repository pattern is that your abstractions are hiding its true potential. 
&gt; Deleting isn't hard either, it's one line: DbSet&lt;T&gt;.Remove(DbSet&lt;T&gt;.Find(id)). That's bad code. You are making an unnecessary extra trip to the database. &gt; The biggest problem with the EF Repository pattern is that your abstractions are hiding its true potential. Agreed. But that's not necessarily a problem, as you are also preventing clients from writing truly bad code.
I love ncrunch. I have a license at my job, but not at home :(
Ah, there's the problem. Your models should have as much business logic as you can cram into them. However, they should not have access to service classes. That way they are naturally unit testable, no mocks required. Once you do that, your service layer becomes quite thin and you can cover it using just integration tests. *** If you need to test the UI without its database, then mock the entire Service object. But build mock services that are actually usable, not the crap that you get from some cheap ass mocking library. That way you can actually deploy your server with a "Mock or Real" flag. 
Needless abstraction that prevents you from using all but the simple CRUD functionality of your tool at the service layer, especially with regard to relationship loading. That forces a method explosion on your repository where you have 10 different ways to load the same collection. First you think code sharing is a good idea then one use case needs more data and a clumsy change now has performance critical paths loading half the database. So then you start splitting by usr case, and once you go there you might as well drop the repository and work with query objects. It's just not a problem to mock a DBSet. 
I think you are confusing service layer with an API service. Service is a ridiculously overloaded term in software. In what I'm describing models or services are two types of business logic classes that might have access to interfaces over data access. It's a pretty common pattern so I don't think its fair to dismiss it as wrong. I'm sure what you're describing is great too. There's many ways to skin a cat and different applications lend to different solutions.
&gt; That forces a method explosion on your repository where you have 10 different ways to load the same collection. Yes, but knowing what those ten methods are allow you to do important things like analyze index usage. I'll even go so far as to mention the method names in the documentation for the index or vise versa.
Agreed
For what it's worth, every property needs to be virtual so it can handle lazy loading (only loading the object once you access a property for the first time). 
&gt;That's bad code. You are making an unnecessary extra trip to the database. You're opening the connection anyway, which is most of the overhead. As long as they're in the same context, the impact is minimal.
&gt;All I can gather is that it somehow turns the rest of your code after the await call into a callback. Technically, it transforms the method into a state machine, where every `await` separates another step. If you're curious, you can take a look at the transformed output of an `async` method with a decompiler (e.g. [this](http://tryroslyn.azurewebsites.net/#f:r/K4Zwlgdg5gBAygTxAFwKYFsDcAoUlaIoYB0AKgBYBOqAhgCb5k0gDWIOADsAEYA2YAYxgDezEDABiAeykwA3thhKYXPoJjMEEIaWYsYAWQAUASnmLllgMJSIIKb1TEA6pTBoAMpFRGARADMwShRfExxLSxoAdxp3GF1WYgARVFEEIwBGAAYssIsImBs7BydXd1QvCB9fEFQBWzpQ8OUAX2wWoA==)) &gt;When using await Task.WhenAll(...), do all of the tasks run in parallel using multiple threads? `Task.WhenAll()` does not affect how the tasks are run at all, it simply waits for all of them to finish. Whether or not they run in parallel depends on the kind of tasks and how they are started (as well as `ConfigureAwait()`). &gt;When, if at all, does async/await take advantage of multithreading? When it's time to continue a method (after an `await`ed task is completed), the method may be executed in parallel on a different thread pool thread if continuation on the original synchronization context is explicitly disabled (i.e. `ConfigureAwait(false)`), or if there is no synchronization context in the first place (e.g. a console application). &gt;Another coworker claimed that we should always set .ConfigureAwait(false) when making async calls in a web application. It sort of makes sense to me since (from my understanding) this means "don't care about which thread started the async call, resume execution on any available threadpool thread" What tasks concern themselves with are technically not threads but synchronization contexts. In Windows Forms and WPF application this is generally synonymous with the single UI thread, but in web applications the synchronization context will already pick an arbitrary thread from the thread pool to continue the execution, but only ever use one at a time per request. You should use `ConfigureAwait(false)` when you do not care about your `async` methods being executed at the same time as others. This isn't always the case in web applications, you wouldn't want multiple threads to write overlapping HTML or access non-thread-safe data structures at the same time etc. &gt;Also, I selfishly find it a bit annoying to have to type the extra .ConfigureAwait() call on every await statement Technically you only need it on the first awaited call in each method, once the method is continued without the original context, all further `await`s are as well. The only caveat is that an `await` that doesn't need to be continued (because the awaited task is already completed) does not trigger this.
Even counting sp_reset, you are going from 2 calls to 3. It's a totally unnecessary performance tax that you can avoid with a trivial amount of effort.
&gt; And you cannot do this in any other way? It's a heck of a lot harder when you have to search the whole code base instead of looking in one place.
Just adding to the already good replies here- It's important to understand the conceptual difference between multi-threading, concurrency and asynchronous development. The goal of async is not to do things "faster" or "in parallel"; it focuses on "doing more with less" by unblocking resources while your task is idle. Async/Await is a mechanism for telling the compiler at which points in your code you will be waiting for something to happen; those are places in which the compiler may (optionally) want to "swap over" to some other task until your code is ready to continue. Whether that actually happens, and whether your code continues on the same thread it started with or on a different one - those are (in theory at least) implementation details. Async/Await isn't inherently faster than synchronous coding - actually is slower due to the overhead of managing the whole mess - but it does let you squeeze alot more work out of any given set of resources. [There are many articles explaining this in much more detail.](https://www.google.co.il/search?q=async+await+vs+multi-threading&amp;gws_rd=cr&amp;ei=OhgVWJvhAsyeaObQmqgN#q=asynchronous+programming+vs+multithreading) 
Not internal, vsixgallery has been around for quiet some time.
For nHibernate I always prefer `QueryOver` over `CreateCriteria`, it gets rid of the magic string in any restrictions. var foo = session .CreateCriteria&lt;EmployeeClassification&gt;() .Add(Restrictions.Eq("EmployeeClassificationName", employeeClassificationName)) .List&lt;EmployeeClassification&gt;() .SingleOrDefault(); var bar = session .QueryOver&lt;EmployeeClassification&gt;() .Where(x =&gt; x.EmployeeClassificationName == employeeClassificationName) .List&lt;EmployeeClassification&gt;() .SingleOrDefault();
Try running a decompiler on it so that you see what the real code looks like.
Is it beneficial to use async for all db calls by default? Or is there some kind of overhead where it's perhaps not going to be a benefit and may actually decrease performance for otherwise fast calls? If that's the case, how slow do things need to be to make async a benefit?
Believe it or not, we have written a number of *new* WinForms app in our shop this year. Can't say it's "the right" choice, but it's still supported and junior devs seem to have an easy time with the GUI tools in VS.
AFAIK there's no CLR concept of await or async, so technically you could write the same functionality without them.
For EF6 I implement a Repository and UoW Pattern. For EF Core I skip both. EF Core got much faster and many things are done on the client side now. I don't want the lowest common denominator just for persistence ignorance (actually the Repository Pattern for EF is in reality more like a Fascade Pattern).
Imagine you have to make 4 requests to external services, each request takes a second to return its result. If you call each of them and wait for it to return, it takes 4 seconds. If you start all of them at once then you only wait about 1 second to get everything done. 
&gt;Now you can async almost all of your database calls (ef) For Entity Framework, check out the QueryableExtensions. When converting your Queryables to IEnumerables (i.e., actually fetching the data), you can use their async methods. https://msdn.microsoft.com/en-us/library/system.data.entity.queryableextensions(v=vs.113).aspx I tend to have a mix of EF and Dapper ORMs. If you use Dapper, check out their async methods (e.g., QueryAsync) https://github.com/StackExchange/dapper-dot-net/blob/master/Dapper/SqlMapper.Async.cs 
This is making more sense for me, but won't there need to be a listener thread anyway to wait for the process to complete in the DB example? Perhaps this example could help explain my sticking point... Lets say 10 requests all happen at the same time to the same webapp. All it does is grab the top 10 names from a DB table. There would be a thread for each request, and on that thread it would use a DB connection, wait for the operation to complete, and the return the results. Are you saying in your scenario, that a thread would spin up on the request, make the DB call, RELEASE the thread, and then spin up a new thread when the DB operation is complete to return the results? I am a little lost logistically on what would be listening for the completion if not the original thread, but I feel like it's a step forward in my understanding if thats right.
Well yes, that's simple multi-threading. Async/Await seems to be a whole different paradigm then JUST that.
The framework will reuse threads as much as possible. In your example above, yes the thread is released to do other work and that other work is going to occur on some other thread (I/O background thread or thread pool thread) and it's possible that all the code that occurs after the await is ran on that thread (`ConfigureAwait(false)`). There's not necessarily a new thread that needs to be created or even a need to marshal back to the original thread when the callback is invoked.
Saying it won't do any of things is not exactly true. It definitely can make your app better and faster by consuming less resources and using the right resources (CPU thread vs I/O thread). But of course like most programming questions it depends. That said, you are somewhat correct that one of the biggest uses is scalability. At my job we write a lot of windows apps, but they all run on citrix farms with lots of simultaneous instances of the app running. Using as few resources and the right resources makes a big difference in the size and number of machines needed in the farm.
Wha? Ohhhh....... wow....... the last exception yeah? So it'll save the stack state in the exception too, not just show where we are now?
A lot depends on how you are accessing the database. If you are using EF, then you typically spend more time burning CPU cycles than waiting on the database. 
Reference please.
do you know what's the difference between async/await and the old ways of doing that pre c# 5.0? I remember it was a pain in the ass
It looks like JavaScript, with long chains of calls to `Task.ContinueWith(...)` and hard to understand error handling. 
I'm having flashbacks from doing process syncronization in C for college
Oh thank fuck for the performance improvement. Updating packages in our 77 project solution takes half an hour even with R# disabled.
I'm still struggling to understand it. Can you give a few examples of this? You mention async db calls Lets say you're loading a page which loads a bunch of stuff from the database. A user has specifically clicked on that page and whats to view that stuff now, so how does that fit into: "By switching to async you're telling .Net "I don't need this thread right now, the context is waiting for something else to happen"" I do a bit of ecommerce development, and I can't think of much things that don't need to happen "now". It's all based off the user doing something and wanting to see an instant result. The only things I can think of are background tasks, sending emails off, logging perhaps. 
It's good because while user 1 is requesting something from the database, the calling thread can serve user 2 while the callee thread from user 1's request can be fetched.
What's your personal opinion on why WPF is superior to UWP?
I will have a look at pluralsight. I wonder why didn't stumble upon it until yesterday. Is it popular among Windows devs?
It started out with mostly windows dev content until they pretty much covered everything that was worth making a video about. Now the spectrum is more diverse. I'm still subscribed, but haven't watched much content in the last year or so. They introduced Paths a while ago that should ease the pain of finding relevant courses. They probably still offer a free trial so I don't think you have anything to lose. They used to show a toplist of videos but don't know if they still do on this "new" design. From the top of my head I can highly recommend Scott Allen fundamentals courses.
This is excellent advice. 
UWP is the "new" platform, it's what you should _probably_ target assuming there isn't some specific API's you're missing. It will rule you out of &lt; Win 10 support however. People rage because it's locked down and sandboxed, but as a consumer, there are a lot of benefits. Visual Studio _is_ complex, but it's also massively capable, there is a learning curve because it can do a lot. Forget about PCL's (portable class libraries) unless you're writing libraries targeting different .Net frameworks (and even still, it's being replaced by .Net Standard). A "solution" is just a VS term for a collection of projects. Forget "Visual C#", it's referred to as C# only. F# isn't as mainstream as C#, so it doesn't get the same airplay.
&gt; So instead of having a thread wait for a potentially long running operation on a remote db server to complete This is the bit that confuses me the most. I would assume there's a cut off from where this becomes an actual benefit, or does the await only kick in over x ms? Say for example I have an existing app, all synchronous. I think its fairly obvious to change any 100ms+ calls to async but what about all the 5 or 10ms calls? maybe the async benefit of those adds up when you have many requests simultaneously awaiting, so overall it free's up a lot of processes? 
Its multitasking and might be multi-threading depending on the code. I think the paradigm shift is about async and await as language features making it simple enough to write and understand that everyone can do it. 
Temp data is only persisted between requests. You have a post - redirect - post flow and the temp data is cleared after the redirect. Can you use ModelState.AddModelError instead to pass errors? 
I do this. I turn all of my ef6 db calls into awaits or I run into the chance of the db running slow and moving unto next task without the first db results. Especially true during debugging.
Yes, that sounds right 
Great post. It has the info I needed to start wit windows containers. Thanks!
Ok but what if i presenet some of errors via ModelBinding and some of them via business logic. I want to separate them. With the ModelState i got a list of both errors. How can i do that?
Coming from someone who has been out of the loop on this whole docker thing, what's the point of doing this vs using IIS? 
Why does EF use Where.SingleOrDefault instead of Find? My understanding is that EF checks the local cache if you use Find, although it only supports the primary key. Neat project though!
Also true! But if you are trying to run microservices on-premise then Docker gives you a free and easy way to go...
I can work on it. I have a LLLblgen 5 license.
because its the latest "mongo is it webscale" and "microservices" type meme
What with VB devs?
Late I know, had this open in an old tab - if you go to Options -&gt; Build and run have you got parallel builds active? Apparently setting it to 1 can speed things up.
Cool, thanks.
Have you tried adding the shoeid foreign key in the run class?
You can put string.Empty for the property parameter and the error becomes part of the summary. You can also dot operate through properties to get deeper validation level. Other than that, what distinction would you need?
/u/colemickens, not sure why you deleted your post. I'll speak to what you said anyway (which boiled down to: "you're dumb, and you're locking yourself into one cloud provider") since I'm sure someone else had the same thoughts. I'm aware that it's not all that difficult to work with containers and that they allow you to move to any cloud you want. However: 1. AWS is actually quite good and rarely goes down or has issues for me. I personally do not see a reason to move away from it, so I can tolerate using proprietary services even if it locks me in. Yes, it's possible that some day AWS will be shit and I'll be stuck, but that's a risk I'm willing to take given past experience and the fact that a large percentage of the tech startups in the US have bet on AWS at this point. 2. There are significant benefits to be had by using said proprietary services offered by Azure/AWS: logging/diagnostics, deployment, SSL integration, geo redundancy, an API gateway if you want one, etc. All without any extra work needed on your end. You would otherwise need to set up all of these things yourself by writing scripts to generate said containers. By all means, if you want to do this or you expect to move clouds every 5 minutes/need a very generic infrastructure, go do it. As for me, I don't have such requirements and at the risk of sounding like an uppity snob, I want to spend my time writing software, not doing glorified IT work.
With docker you create an image of your app. You can create one monolithic image or more smaller images containing the components of your app. E.g. one image containing Redis, another containing Postgres and another containing ASP.NET Core. Think of Docker as app-virtualization. The huge benefit of this is that you create the image with your software and configuration and then easily ship it to whatever environment ("run a container from an image"). You can create the image on your dev linux box running CentOS or Ubuntu and then deploy it on a Redhat staging environment. With docker you don't have to worry about package versions and configurations. They're all the same within your Docker container. This is actually a game changer in terms of application deployment: Isolated software packages and configurations. All the other stuff like "ultra fast", "VM on steroids", "microservices" a besides the point - IMO.
I did *not* say you were dumb. If you're happy writing to proprietary APIs, knock yourself out. &gt;the fact that a large percentage of the tech startups in the US have bet on AWS at this point. And tons of startups are choosing to use Kubernetes/Mesos/etc because it gives them free portability across all major clouds, and/or a path to a private DC if they want that eventually, or for additional redundancy. Not that many startups are opting for the managed high level PaaS Cloud services like the ones you alluded to... because of lock-in, pricing, flexibility, portability and redundancy. &gt;You would otherwise need to set up all of these things yourself by writing scripts to generate said containers. That's just not true. I can deploy a load-balanced ASP.NET app with automatic. free SSL to a Kubernetes cluster of 200 machines in less than 5 minutes (I'm being generous, it's usually much lower) with 3 commands total. One additional command will do a rolling update to all of those instances. (And there are no "scripts" in sight...) Further, again with a single `kubectl create` command, I can start elasticsearch, kibana, and a log collector that is guaranteed to run on all of my machines and aggregate all of my logs for me and make them instantly searchable. Same with grafana/heapster displaying stats collected by cadvisor automatically. These things are made trivially easy by running an orchestrator. There's a hell of a lot more going on in container-land than just Docker and running containers by hand. Might want to brush up. (And yes, it can create L4 LoadBalancers and AppGateways for you automatically as well... that's part of the point the higher level orchestration layers) --- I will almost guarantee I spend a considerable less time on "ops" than anyone who SSHs into VMs, or RDPs into Windows boxes, or ever manually copies bits around to machines to deploy their software.
The initial setup of a cluster isn't that hard anymore. There are dozens of Ansible playbooks for various clouds, chef recipes, ARM templates for Azure, CloudFormation templates for AWS, etc. It's just not that hard of a problem anymore. &gt;plus several dockerfiles to define packages that need to be installed for your application/database/cache/gateway nodes. The latter takes a while to get right -- sometimes you forget to include certain dependencies, things don't work as you expect Dunno what this means. If you didn't install the right things in the container, then it's not going to work in your container. If you didn't install the right things on your VMs, then your application isn't going to work. Writing a Dockerfile and getting it right is much easier than managing a mix of your various application bits on your VMs... There's nothing you mentioned in those paragraphs that don't apply to normal deployments to VMs... and it still applies to PaaS-y Services too. For example... you still have to write configs for Application Gateway. And I will tell you, writing the config for Kubernetes Ingress is much simpler than configuring an AppGateway by hand :) . That goes for everything you listed, to be honest. The container mindset though means, again, I never SSH into my nodes. I never have to worry about installing some package on every machine because my app needs it. I never have to worry about having separate cache nodes and application nodes because the versions of python or ruby conflict with each other. And that's just it... I don't have "cache" nodes. I don't have "gateway" nodes. I just have normal Deployment objects that describe containers I want and how many of them I want running and Kubernetes makes it so.
Indeed and the author addresses this point in the article. But then lots of .NET developers have only ever worked with MSSQL and don't think anything else exists.
Looks about right just looking at those two properties.
Oh yay! Now I have another thing to look through before deciding on what to do about mobile apps. For simple DB CRUB apps do I want this, Xamarin, or Cordova? If this uses XML for the UI definition, I wonder if it would be possible to scaffold a quick UI using [T4 toolbox](https://github.com/olegsych/T4Toolbox) / etc. Hmmmm.....
Xamarin has the advantage of being C#, which is great if you are already a .NET dev as the overhead of learning new languages goes away. For simple apps any of the options will be fine, as it gets more complex, they all have disadvantages. Xamarin iOS and Xamarin Android are better for this than Xamarin Forms as you are just writing native apps (using C#) with shared libraries. From the experience of using it, and from the conference I went to hosted by Xamarin themselves, Forms is not ready for large complex apps but is great for simple ones.
It wouldn't be to hard, but I would recommend using another container for the db server and a volume for the actual data. For the hosting on Azure I think you'll need a docker enabled VM, use Azure Container Service or have access to the Beta of Azure Docker Sevice: https://blog.docker.com/2016/06/azure-aws-beta/ which is also available for AWS
"we felt that existing CMS's were too limiting and got in the way of building a great product. " so says everyone about every product. :) Some CMS want to take over and be your whole site and limit what you can with the underlying platform. You end up developing on "CMS Platform" instead of developing on "technology platform" Often, I would just like to be able to plug some editable content into a site without losing control of all my technological choices. Have you looked at EpiServer? I had good experiences with it several years ago. 
The way I am reading the docs and website suggests I can only use TypeScript if I go the Angular 2 route, is this true? I sure hope it isn't.
I think a large proportion of developers would prefer developing on "technology platform" and bringing in content management features as needed, so I guess that it what I'm trying to validate. I think allowing a developer to use their technology choices is important too, often the most productive tool to use is the one you know. I haven't looked at EpiServer myself, but Sitefinity is a similar one we've used which I think is in a similar class, but I could be wrong, you've got to go through the whole sales process before you can evaluate them. 
Thank you very much, I will be looking into this. I am a beginner, though, so we'll see how this goes... 
I build lots of B2{B/C} applications where a full CMS isn't justified but we almost always need the ability to give clients edit privileges to certain globs of content. We usually throw in some basic CKEditor/TinyMCE admin functionality for these and call it a day. So personally, I'd like to see: * No Web Forms, or if you must implement it, MVC needs to be a first class citizen and not an afterthought. In that same vein, .NET Core compatible would be ideal since we'll all start porting over "In The Near Future"™ * Good documentation and examples. The gold standard for me is probably IDS: [docs](https://identityserver.github.io/Documentation/docsv2/) and [examples](https://github.com/IdentityServer/IdentityServer3.Samples/tree/master/source). I've done a ton of complex stuff with IDS, and I almost never had to resort to asking for help because the documentation + examples were so solid * Drop in functionality that treats my application as the primary tenant. I want to be able to run an Install-Package and not have your install scripts fill my application with unexplained garbage, e.g. I don't want "Install-Package UmbracoCms". I understand having a package that installs all of the required assets for when you're starting from scratch, so maybe what I'm asking for is a YourMvc.Core package as an alternative. Using IDS as an example again, I can install it and essentially 'namespace' it to its own URL with something as simple as: **app.Map("/identity-server", identiyServerApp =&gt; { identiyServerApp.UseIdentityServer(options); });** * Assuming you use SQL, a way to override any automatic migrations you run and a clearly labeled process that says "here is the SQL for version x." We don't use EF or NH - we use SSDT to manage our schema and NPoco to do work. It's one thing to introduce an ORM, it's another to hide the SQL or require us to install a package, run migrations, diff the schema, dump that back into our SSDT, etc. Not fun. * Either give us a way to override your dependency resolver, hook into it or expose critical functions so that we can override them. Example: https://identityserver.github.io/Documentation/docsv2/advanced/di.html * Awareness of existing routes from the primary application in the CMS admin when creating content for end-users. If my MVC application has already registered "~/marketing" and an end user tries to create a page with the same route, I want my CMS to be able to tell them that's a no-go. * Extensibility (UI, theming, plugins). Good luck on this one, I think it's one of those genuinely "hard problems" to implement *well*. In fact I've never seen it implemented well. Drupal, Wordpress, Joomla, DNN, Umbraco, Sitecore...you name the CMS, I'm sure we can find something to complain about in the extensibility department, but it needs to be there. As a side note, the features list of PiranhaCMS did catch my eye a while back and might be good reference for ideas (https://piranhacms.org/docs/extend/page-post-types) but I never got the chance to use it and am unsure of its development state / stability.
All we want to know is; are the millions of bugs fixed? I've been a dev for 19 years and I have never suffered at the hands of such an unfinished toolset as much as I have with Xamarin. 
If you're sending over entire entities, one might skip the second layer in a "three tier architecture" altogether. 
Kestrel and WebListener are not recommended to be facing internet directly, so one might do SSL overloading via the reverse proxy as well.
Well, they have to wrap Android right? Which is a clusterfuck with a terrible API, it probably isn't easy.
Just jquery ui in the default template. You can add whatever you want, though. I like using bootstrap and telerik, but you may find that other frameworks suit your needs better.
Question: why MVC 4? Granted, Core v1 is not yet production-ready by many metrics, but MVC 5 (as long as you target DotNet 4.5.2-4.6.1 and C# 6) is probably your most modern bet.
I feel like ASP.NET Core is production ready. sure, there were growing pains, and at some point we're migrating back to a simplified csproj instead of project.json, but now that we're 1.0 it's very rock solid. heck, even rc2 was rock solid. EF Core on the other hand is definitely missing some things that may be a deal-breaker, but there are other ORMs to choose from, and you can even use the old battle-tested EF6. If you need specific third party libraries that don't play nice, that's a whole other issue.
I was searching for a tutorial on writing a REST based application and I found the link mentioned in the post. I think that was the first one I got in Google search results
Thank you, I think this is making a little more sense. Do you have an example I can look to?
Bootstrap 3 and jQuery are the defaults for MVC projects, and will work well enough for server-generated pages. Of course, you could use something else.
Depends on what kind of CMS you are building, I would love to see something that is not 1-on-1 webpages. Make the CMS very modular where the core contains only code for API, retrieving objects from db and routing/displaying webpages. If this is done nicely and it works with dotnet core, I am game.
Sounds great, keep me posted!
Up to 5 users can use visual studio team services for free. If you have MSDN you get tfs for free (for each licensed user) and can host unlimited stake holders. It has really come a long way since the old tfs build system. * Everything is task based * If you can do it in powershell, bash or CMD you can do it in the build/release. It is easy to extend. * There are templates for the most common use cases, and a marketplace for extending functionality. * Octopus Deploy, Jenkins and GitHub all integrate into the platform In general we have been very happy with it and it has been great at scaling to enterprise load 
Awesome, thank you!
Microsoft is on the right track and all this would be unthinkable under Steve Ballmer (imagine where MS would be now if he'd gone earlier) but there's still some way to go. OSS still doesn't feel quite right using the MS stack and I think that's the Windows factor. I'm hoping Core will change this as companies feel more comfortable deploying to Linux with the MS seal of approval than they would have done using Mono (I use Mono and .NET is definitely a second class citizen in the Linux world). Only when this happens will the vision become a reality.
Don't worry. We're all confused. I've been in .NET for 10 years and I have no idea what they are actually trying to do anymore.
Thank you for your answer. It is a reasonably serious idea. I certainly want to set up a business to exploit the idea but obviously I have no idea if the idea is going to be successful or not. Either way I'd like to be able to write something production ready at the start and keep improving it over time. So from what you are saying I should be using ASP.NET MVC 5 then? OK I'll have a look for a decent book on that subject then. Real time notifications certainly sounds like something I would like to implement in the future. Thank you for your help.
Are you involved with the TE benchmarks at all (besides the many perf enhancements you have to corefx/aspnetcore)? It looks like they still haven't published round 13 and based on the merge: https://github.com/TechEmpower/FrameworkBenchmarks/pull/2078 I am not sure if this stuff will be in 13 or 14.
My honest advice now, don't start learning on "core" anything. Let them sort it out while you work against the stuff they actually have to support. That would be the full .NET framework on Windows and ASP.NET MVC 5 as far as I know.
It's been a couple of years since I used it, but I remember the lack of documentation and data migrations being a pain in the ass as the main reasons we hated that CMS
Teamcity here
IMO, there is no rush to convert projects at this point, but it would be irresponsible to not consider asp.net core 1.0 for new projects, if not just to avoid rework later. The project.json vs csproj and msbuild changes are going to be minor and probably handled by visual studio itself.
TC for running build. Octopus for deploy. 
ASP.Net MVC Core is the new version of ASP.Net built with support for .NET Core. .NET Core is a new, light weight, multiplatform .NET Framework. You do not have to use MVC Core with the core framework. Unless you need multiplatform support I recommend MVC Core using the full .NET Framework. 
If anyone's interested in learning more about the ideas behind Serilog, I've been working on a pretty extensive series of posts on structured logging concepts in .NET. On the surface, Serilog and _Microsoft.Extensions.Logging_ appear quite like earlier tools such as log4net. The mindset required for getting the best out of them is quite different though! The series starts at: https://nblumhardt.com/2016/06/structured-logging-concepts-in-net-series-1/ - would love to have your comments and feedback. (Oh, and woohoo!! for seeing Serilog featured on the .NET Blog :-))
I'm certainly interested. Does it have a CGI type component?
Yeah, 3rd party libraries are also seriously lacking. 2 of the most popular mocking libraries (NSubstitute and Moq) only have preview releases available. There are various other things that I use such as MassTransit that also don't have .net core builds yet.
Serilog is seriously solid. Possibly the best logging library I've ever used on any platform. Side note: I was forced to use log4net (xml configuration garbage galore!) at a previous job and now I wonder how I used it for so long. 
Not that I'm aware of... but you could use apache to proxy pass it
People always say this, but it's wrong. First off Bill and Steve still own enough voting stock that anything they don't want to happen won't. Second, the first steps of this, like open sourcing .NET, happened within the first couple months of Nadella's tenure. In a company as big as Microsoft just running a change like that past legal is at least a six month change. This vision began under Balmer and goes back at least as far as his 'developers, developers, developers' rant. You can argue, probably with some accuracy, that Balmer's antagonistic history with open source meant that the change wouldn't have worked well under Balmer, but the idea that he personally was against such changes is demonstrably false.
I'm not excusing Balmer for the years of stupid. I'm giving him the credit he's due for the change of direction after the stupid. The last couple years are the first time I've seen Microsoft raring for a real fight in decades. It's a good thing, and Balmer deserves some credit for it.
I love that microsoft is trying to do this and all the effort they are putting towards these things, however if they want developers to actually start using the new .net core platform, they need to work on developer tools more. In windows we have Visual Studio and everything is great but trying to develop a .net core app on mac or linux is a nightmare. VSCode is a great text editor however it has so many bugs and lacks ton of essential things for .net core development. There is no proper intellisense support for razor, and there are bugs all over the place like intellisense popup freezing, not being able to click out of the intellisense popup and ton more. Since everything is in its early stage I am really hoping microsoft does something about this.
I'd say 50ms in the critical path of a user interaction is significant. Most of the micro-optimizations I see people proposing are somewhere between 0 and 1ms, skewing towards the 0, such as using `string.Empty` instead of `""`. (those produce identical MSIL) Anyway, I agree with the conclusion that you should demonstrate a bottle-neck *and* improvement with real testing before applying anything like this.
You can use environmet variables for that. If you are using configuration builder in your aplication you can call: builder.AddEnvironmentVariables(); What that will do is replace any configuration found in your config files with the values specified on your env variables. So you can deploy the same binaries and config to every env and just add env variables in each server or container to replace those you need to change such as connection strings.
Trust me on this as I've deployed a service fabric cluster to production and regretted it: use an App Service (Web Site) for microservices and don't bother with service fabric unless you have a very specific use case. Microsoft's description of Service Fabric is misleading. They sell it as a general purpose microservice framework, but Service Fabric is very...low level and IMO is only useful if you want to play around with stateful services (I.e. no external data store, no DB, all data for your app is managed through "reliable collections" which are distributed across multiple application nodes). You'll run into headaches if you want to write a classic microservice where you expose http endpoints and use SSL with service fabric. This is because SF basically just gives you access to VM scale sets and the idea is for you to use KeyVault -- which was in preview and horribly broken last time I used it -- or manually install certs on all the nodes in your cluster, which is even more of a headache. There are also lots of missing things that are needed for a managed microservice solution: auto scaling, diagnostics, user authentication, etc. App service has all of these things while SF does not, so use it ;)
You can put all your app services on one app service plan if they don't produce much load. You can also use azure functions (which, AFAIK is a fork of the codebase that powers app services) if you only want to pay for execution time and not worry about scaling. Either of the two options is better than manually managing VMs with the tooling available with service fabric. Also consider that service fabric has no free tier while app service does (so you'll save money on your test environment)
D3.js is a fantastic tool if you have the time to learn it. There are tons of examples out there for every kind of visualization you could imagine, and it gives you complete control to customize. Since it's a front-end tool, you can make your visualizations interactive as well. The downside is that it has a pretty steep learning curve. My first app with D3.js was a clusterfuck. It worked well, but I'd hate to have to change it. I don't know of any .Net tools that do the same thing.
D3.js is great! Echarts (http://echarts.baidu.com/) is also an option. and this easier than D3.js 
Anything MVC3+ should work just fine in MVC5. Just be aware of the [improvements in C# 6](http://developer.telerik.com/featured/essential-c-6-features-you-need-to-know/) and you should be fine.
You'll have a json appsettings with the dev values. Then in each environment or server you'll define real environment variables (not through a file) if you are running windows you can run: sysdm.cpl then click Advanced Tab -&gt; Environment Variables and you can add your values there. These values will replace those from the json file in runtime. Hope it helps! 
If you are building a dashboard from scratch, it might be worth looking into existing frameworks that have most of the basic functionality already. [Droptiles](https://github.com/oazabir/Droptiles) is/was fairly popular at one point but it looks like it hasn't been updated in a while. It might be less work overall to get it updated to the latest .NET bits than it is to code something new. There are probably others on GitHub that you could find too. But if you do want to build something new, use a wrapper library for D3. It's incredibly powerful once you learn it but it does have a steep learning curve. 
Is it possible to use ASP.NET core without kestrel/weblistener?
In theory yes. You just have to provide an alternative web server implementation. 
~~I wouldn't say ASP.NET core is not tied to .NET core. Yes, you can target different .NET frameworks but all core applications run in the CoreCLR.~~ Edit: I'm stupid
It appears you are correct. All this new terminology is confusing me, I need to do some more reading.
This is where it gets very complex. You have to deal with the situation ranging from field rename to refactored data structure. It seems to me Event Source is essentially turning a DB system into a NoSQL system. 
Check out www.msdn.com (Microsoft Developer Network) and the forums there. I post my development questions (Microsoft Technologies) there. Get lot of answers, including official Microsoft staff.
I'm actually not using service fabric anymore. Migrated everything to app service. The real deal breaker for me was the fact that I couldn't scale horizontally at all -- service fabric is backed by VM scale sets as I said, so you need to do a resource deployment to scale. This appeared to work at first glance, but after the new VMs were done being provisioned, I was no longer able to access my cluster through the management UI and it seems like whatever was linking the nodes together didn't recognize the new nodes as part of the cluster. Contacted MS support -- they didn't help and as such I had no choice but to delete my SF cluster. Also, it's difficult to manage the cluster once it's online -- you need to use (not sure if still true) certificates for auth, so each dev needs to have the cert installed on their machine in order to see stop/restart app instances. I also had some weird issues with my process being killed by the SF runtime. Was doing some long running background operations with threads, and my threads would sometimes magically die, which was a little terrifying. Have no such issues on a normal app service environment, and app service has great diagnostics in the form of live log streaming, so it's easy to see what's going on, whereas I was never able to fully diagnose the problem on service fabric. Hopefully this makes sense.
Holy sh_t that's a ton of references! I'm guessing a lot of this Microsoft.* stuff hasn't updated for .NET Core yet?
That is correct. You would have to implement one or more of the [request features](https://docs.asp.net/en/latest/fundamentals/request-features.html?highlight=features). At least `IHttpRequestFeature` and `IHttpResponseFeature`. In that sense it works a bit like Java EE and its application servers. I think it shouldn't be impossible to host ASP.NET Core directly in w3wp.exe as via a native IIS module.
Not anymore 😂
Try setting WindowChrome.IsHitTestVisibleInChrome to true on your text box. More info: https://msdn.microsoft.com/en-us/library/system.windows.shell.windowchrome(v=vs.110).aspx
This is pretty cool. I was just recently looking for something like this for a project since the SignalR sink seems to be dead.
[I think this is the link you're looking for.](https://dotnetcore.gaprogman.com/2016/11/03/what-is-net-core-and-why-you-should-be-excited-about-it/) 
I closed the tab right after I saw DNN...
Take the current mileage, add += the run distance, add the run to the shoe runs collection and call save changes. 
I'm using stateless to handle the state of some entities in our domain. I really like the concept of it! 
Hey, looks nice! Any plans to allow the formatter to be plugged in? I noticed the sink is using the default `JsonFormatter`, which is great but a bit verbose. Depending on the destination `CompactJsonFormatter` from https://github.com/serilog/serilog-formatting-compact can be a more efficient option.
Is there a recommended way of hosting workflows built using stateless? Also, do you have any guidance for HA and DR scenarios? I end up using WWF quite a bit, but the editor is horrible for larger projects. Compiling takes minutes at a time, and the editor somehow lags on high end machines. I'm definitely looking to try alternatives.
No, Stateless is developer-facing. Cheers!
Hi, The idea behind Stateless is that it's embedded into whatever kind of application you're already building. For state machine workflows, the state machine generally ends up a part of an EF entity. To mainipulate it, the app loads the entity and calls whatever methods correspond to the actions being taken by the users/external systems. Afterwards the normal ORM persistence cycle writes the entity back to the database. The question of HA from this angle just ends up with the same answers as EF high-availability in general: database replication and so-on. Hope this helps, Nick
Did you even try to Google it? https://docs.microsoft.com/en-us/dotnet/articles/standard/library Platform Name | Alias | text | | | | | | | | ---|---|---|---|---|---|---|---|---|---- NET Standard | netstandard | 1.0 | 1.1 |1.2 |1.3 |1.4 | 1.5 |1.6 | **2.0** .NET Core | netcoreapp | → | → | → | → | → | → | 1.0 | **vNext** .NET Framework | net | → | 4.5 | 4.5.1 | 4.6 | 4.6.1 | 4.6.2 | vNext | **4.6.1** Mono/Xamarin Platforms | | → | → | → | → | → | → | → | **vNext** Universal Windows Platform | uap | → | → | → | → | 10.0 | → | → | **vNext** Only the .Net Framework 4.6.1 has implemented the 2.0 standard. The rest will come in the next version (of .Net Core)
One of my major clients is using Stateless in more than 45 different applications and they are really happy with the library specially after the introduction of DOT graph visualization. The fact that it now supports .NET Core will help them with the new microservices they have in mind. Great work!
It's really strange to say that GetType behaves like a virtual. Object is the base class for every object, and it implements an accessor, GetType, to read the type information from Object's private data (the object header). The type information is just stored when the object is created. No "virtual-like" behavior needed. So this article could be shortened to the above paragraph and a summary of your observation: when there is no object header because we're dealing with a struct, we rely on the compiler to inject type information into the IL in the form of a box instruction.
Funny, I *just* started looking for an alternative to Windows Workflow Foundation. Looks good!
1.7.1 "recovery" build disables the automatic JavaScript typings file acquisition
No Sitecore? 
Recurs.On (like a play on "recursion") DateNET (play on "dot net") Recurrent (I like pithy names obviously) 
Synchronicity Or some variation, I just like the word. 
I'm not sure this is great advice. It catches a lot of pointless exceptions that you have very little control over.
I always struggle with names (I think many of us do). [Here's](http://www.dcode.fr/words-containing) an interesting tool I found for creating names. You put in some letters and it creates words based on them. So I could pick the first letters from the following dry sounding project name ... Matt's Super Fancy Cool Library (MSFCL) I put them in the 'Letters' text box and click the 'Search Words' button and it comes up with some suggestions like ... MESOFURCAL 
You're correct on the APIs, and then in the API endpoints themselves, you'd serve or take in data and update the databases accordingly. Normally, to interface with the database, you'd use something like linq to sql or entity framework, but if you're really trying to learn, I'd say go the old school route of just using SqlClient methods, so you actually have to write sql, to really get a grasp of how you update databases. Once you're comfortable with that, you can move on to linq to sql or entity framework, which essentially create's classes and methods for you automatically, to update your database. You mentioned you didn't want code written for you though, which is why I recommend the first option. I would literally tell you to start here. Build a simple form, and then try to take data from it to insert and edit stuff in a database. Figure out how that works and you can go from there. I'll even look over your code when you're done and give you pointers if you want. 
Great name
Yeth, obviouthly.
&gt; I always struggle with names (I think many of us do). I'm suddenly reminded of a fairly famous programming quote: &gt; There are two hard things in computer science: cache invalidation, naming things, and off-by-one errors. :)
Regarding EF, take a look at Jamie King's playlist on .NET Entity Framework [1]. They're fast paced and get to the meat of things quickly. Plus he goes into how to create your models for one-to-many and many-to-many relationships. He uses SQL Server Management Studio to demonstrate what happens when you change your models, but if you're using SQLite and want to follow along, then there is a SQLite Manager plug-in for Firefox that's pretty polished [2]. [1] https://www.youtube.com/user/1kingja/playlists [2] https://addons.mozilla.org/en-US/firefox/addon/sqlite-manager/
 DateManager /s
This article seems to be outdated and provides nothing new on ASP.NET MVC routing that hasn't already been covered elsewhere in perhaps greater detail. Examples of more interesting functionality include constraints and the use of methods. Also, I'm not sure why the author chose to write the tutorial against MVC 5, which has been around for three years, rather than ASP.NET Core MVC which went live in August - surely that would have made a better subject matter?
Pretty good tutorial on dapper and asp.net core [here](http://www.talkingdotnet.com/use-dapper-orm-with-asp-net-core/)
Name it something descriptive. We're not JavaScript here
The question is if ASP.NET Core has matured enough.
You might be interested in this article: https://technet.microsoft.com/nl-nl/itpro/windows/manage/lock-down-windows-10 and Windows 10 'Kiosk mode'
MVC 5 is just as current as Core MVC 1. Just two different variants, both supported.
Thanks for the reply!
The other replies here make me think they just sprinkle try catches in instead of avoiding the exceptions. I run catch all exceptions in a large enterprise system and don't have the problem of it stopping all over the place.
Yes, that's why I'm asking.
The only one I've heard of is Build. If it's on the decline, then maybe I'll keep looking. It's not until recently that I've been in a position to attend any so I haven't really been following what's out there. Thanks for the insight.
That's true for me as well. When I'm debugging, I'm intentionally going very slow in the area of interest and I want to know EVERYTHING that's happening related to exceptions.
Can you give a few examples? I haven't run into enough of them to warrant the same comment.
According to the release notes for preview 5: "Tooling for .NET Core is not included in Preview 5"
If you have msdn... just get Visual studio 2015 update 3. 
Now go ahead and check if ASP.NET MVC 5 is also supported.
Thank you - I think I needed someone to tell me that windows embedded is definitely not the answer. I'll explore the group policy and registry options to see that we can take care of all of our requirements.
You don't need EF. In fact, I would strongly recommend learning plain ADO.NET first. People who jump directly into EF rarely learn how to use it correctly.
I wish I could, but ASP.NET and EF are so craptactular that I can't debug them for all of the internally thrown and caught exceptions.
Hopefully someone makes a Java port and call it JaRule.
Maybe the Nginx would give you new experience if you look for hosting.
I don't need to. It still is. So is v4. What is your point exactly?
Yeah, but how do I configure the host to host both? I want to move my api to an API folder and my website to its own folder. I assume the startup and program file will need to be updated but I'm not sure with what. 
Ah i see. So basically what i want to do is write code against only the apis inside .net standard 2, so in the near future when .net core catches up, it will compile on that. how do i go about setting up visual studio to only allow me to use .net standard 2 apis, and give errors on anything outside of it? thanks.
I think you'll want to create a new .net core project (which will give you a project.json) and then put this as the framework: "frameworks": { "net461": { } },
I've been using .NET Core with Kubernetes and it's been fine. (OpenShift builds on top of Kubernetes). Are you using OpenShift's source-to-image build process? Is that where you're getting hung up?
They've recently announced the next version of the tooling so I'd expect it to be present in the next release. I think it's basically been delayed while they work to ditch tge whole package.json debacle. DotNet core doesn't play well with other projects yet so there isn't a whole lot of point in doing it in VS at the moment anyway.
Nice! So it sounds like I don't even need the chtml file. I didn't realize the static files bit would end up using a wwwroot direction. Thanks! I have it working. 
Create a WCF service, start it locally. Exceptions from the get-go.
I'm going to edit this post to include the 'Just My Code' stuff. It seems like that is going to be the most useful. Thanks /u/CoderHawk.
Neat. Looks useful for game hacks ;)
FIXED: For any poor soul that has the same issue that I had and is searching for a fix, I can only suggest that you reinstall IIS from scratch, then when you deploy your website to IIS, double check your application's web.config to make sure that SQL server its trying to use has the same name as the server that's running. To check, launch CMD as administrator and type *" SQLLocalDB info "* Your database should be named *MSSQLLocalDB.* Then check to see if its running with *" SQLLocalDB info MSSQLLocalDB "* If you don't have the database, delete other databases that are not in use with *" SQLLocalDB delete [database name] "* then create the correct one with *" SQLLocalDB create MSSQLLocalDB "*. And finally run it with *" SQLLocalDB start MSSQLLocalDB "* finally restart your website in IIS. If you don't have access to the SQLLocalDB command, you don't have LocalDB installed correctly.
You should have Server Explorer side-toolbar that will also show you all running instances of db servers on the system. 
CORS isn't huge. I was more concerned about hosting in two different containers of sorts, domains, etc. That'll save me on costs. One-click publish was very attractive with Azure as well.
I'll look into it soon thanks :) Web development isn't really my thing, I just wanted a simple place to post updates and information on my other projects. I'm pretty sure if any one of you where too look at my blog engine you'd claw your eyes out XD all my blog entries are currently stored in a single .json file. I plan on improving the website as time goes on.
[Image](http://imgs.xkcd.com/comics/old_days.png) [Mobile](https://m.xkcd.com/1755/) **Title:** Old Days **Title-text:** Lot of drama in those days, including constant efforts to force the "Reflections on Trusting Trust" guy into retirement so we could stop being so paranoid about compilers\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1755#Explanation) **Stats:** This comic has been referenced 8 times, representing 0.0060% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d9plz1e)
Windows 10 home ok for local development?
Don't think it should be an issue. If you aren't going to be building big applications, it should be alright.
Visual studio 2015 is a great software for developing .NET web applications. I use it myself and as long as your machine fulfills the minimum requirements for the program, you should be good. My personal suggestion for you it to go with a machine that offers SSD (if possible). It makes a big difference in performance.
I'm on a tight budget for now...it looks like it can handle visual studio laptop: 4GB, 1600MHz, DDR3L; 00GB 5400 rpm Hard Drive visual studio requirements: Hardware Requirements 1.6 GHz or faster processor 1 GB of RAM (1.5 GB if running on a virtual machine) 4 GB of available hard disk space5400 RPM hard disk drive DirectX 9-capable video card (1024 x 768 or higher resolution) sql sever 2016 express just 1ghz... 
Yes, I totally understand about the tight budget. SSD options are not that expensive now a days. You don't have to go with more storage, a minimum of 256GB SSD should be good and use an external hard drive for your storage.
I personally use windows 10 for .NET development and I don't see any issues. I personally prefer windows 10 over windows 7 now.
Thanks for the response. Yes, #1 on your list is golden. Good defensive programming is key to avoid exceptions in the first place :) 
That was certainly the reference I was going for ;)
Get at least one with SSD. Try the lenovo b50's or ideapads.
You don't strictly need Visual Studio. You can install ASP.NET Core and Visual Studio Code which is a lightweight IDE. With those two you can code on pretty much anything.
ASP.NET Core routing is quite elaborate. I am working on more micro samples on them.
one single .json file MUHAHAHAHA
We used #region and we liked it.
For nicknames, I'm afraid you are going to have to stick with lookup tables. For approximate string matching, I've found [this library](https://fuzzystring.codeplex.com/). EDIT: For nicknames, I've found [this project on GitHub](https://github.com/carltonnorthern/nickname-and-diminutive-names-lookup).
Page_Init
[ASP.NET Ajax Control Toolkit Demos](https://ajaxcontroltoolkit.devexpress.com/)
I really like the rounded corners demo! I still get flash backs about repeaters and update panels.
Yeah there is some confusion about `Expression&lt;Func&lt;T&gt;&gt;` and `Func&lt;T&gt;`. You only need to specify `Expression&lt;Func&lt;T&gt;&gt;` if you want to get information out of the delegate. One you call `.Compile` it'll turn into a `Func&lt;T&gt;` after that, you can only execute the delegate. This [**answer**](http://stackoverflow.com/a/2664845) on StackOverflow was key for me.
Several features don't work in `Expression` that do work in `Func` delegates. E.g. null propagation operator `?.` Expression&lt;Func&lt;string, bool&gt;&gt; expression = aString =&gt; aString?.StartsWith("foo"); That won't compile. You'll need to use the ternary operator with a null check. You're also at the mercy of whatever your chosen library's query engine is capable of. As an example NHibernate can't parse `Queryable.Contains&lt;T&gt;(this IQueryable&lt;T&gt; source, T item)`, nor can it parse `Queryable.Any&lt;T&gt;(this IQueryable&lt;T&gt; source, Expression&lt;Func&lt;T,bool&gt;&gt; func)` but it *can* parse `Queryable.Any&lt;T&gt;(this IQueryable&lt;T&gt; source)`. e: Another feature that doesn't work in `Expression` is string interpolation such as `$"Lorem Ipsum {someVariable} dolum moret"`
You're certainly right! As I mentioned writing a query inside Expression is way more complicated. Null propagation operator is one of the reason here.
It really feels like Expression could have used some more distinct syntax, like a special keyword or character or something - it looks like just another generic delegate, not some special operation that tells the system "Store this function as an expression-tree, don't fully compile it".
@sniper_fox - I have just checked in code to allow using this on .NET Core (it's a separate class library and example website).
Thanks @sniper_fox
Nice write-up, you've made me just a bit smarter - thanks!
I have a mac running bootcamp and I pretty much exclusively boot to windows. Occasionally I'll have a safari bug that needs worked out so I'll have to reboot into OSX and load the bootcamp drive in VMWare so I can use both.
To be fair, it's not the language's fault. It's library authors (including MS who writes the BCL) that decide not to support or just ignore a language feature.
I like building expression trees manually; instead of: Expression&lt;Func&lt;int, int&gt;&gt; powExpression = arg =&gt; arg * arg; I am tempted to write: using static System.Linq.Expressions.Expression; ... var arg = Parameter(typeof(int)); var powExpression = Lambda&lt;Func&lt;int, int&gt;&gt;(Multiply(arg, arg), arg); This may seem nonsensical for such a simple example, but most of the expression trees I write are either going to be dynamically generated via enumeration over results from reflection or contain objects that cannot be represented in the current builtin syntax (such as statements). Writing the simple ones this way keeps me comfortable with the api when I do something complex ([example](https://www.reddit.com/r/adventofcode/comments/3vr4m4/day_7_solutions/cxqr0w2/)). My point is that there is a syntax available for expression trees that looks distinctly different than lambda functions. This syntax even has advantages over the far more concise one. Unfortunately it is unwieldy. 
I've done it as a long time .net dev using windows/mac/linux quite regularly. I simply prefer windows because every tool and and integration point and troubleshooting process with Visual Studio is a degree or two easier to configure/setup/debug when 10 other people have done it and written about it on their site/knowledgebase. That's really the only reason for me. 
For expressions and LINQ, yes this is true. There are other aspects of the language that are pretty darned arbitrary. Two examples that bother me: - No static method variables. VB.net supports them. C# does not because, as one of the developers has stated, "[we] can't see a reason why anyone would use them." Paraphrasing, of course. I'll try to find the link later. - No `with new(...)` using multiple arguments as constraints in generics. This has been a [long-standing issue](http://stackoverflow.com/questions/9741211/restricting-a-generic-type-parameters-to-have-a-specific-constructor/9741812#9741812). The argument made in the link is that *there are costs*, but I don't buy it-- one can implement work arounds to accomplish the same thing, but we shouldn't have to. There are more.. These are the first two that come to mind.
Yes, absolutely. You can even use SQLExpress. It can be embedded into your executable, or at the very installed with your app. Also look at SQL Compact or SQL Lite (they can even work with mobile :)
I've already added a database with tables using the SQLEXPRESS server. Would you be able to point me to literature about that final process of the app development involving creating an executable with an embedded database? 
Where I work, we have a mix of Mac and PC. Everyone uses virtual machines to do the actual work. I have a separate VM per client plus a VM for experimenting with betas of VS/SQL Server.
Probably yes. I switched because I didn't enjoy the experience every time I stepped outside .NET land. Try coding in Python, Ruby, Node, PHP (or pretty much anything for that matter) and you'll find the experience on a Mac is much smoother. There's good Windows support for most of the main frameworks but you'll soon encounter friction when you start using third party libraries. I'd often find some cool new thing I'd want to look at and would spend an inordinate amount of time trying to just get a simple dev environment going. I have no hate towards Windows but I'm happy in POSIX land now and don't see any reason to leave.
What version of Visual Studio and .NET platform are you using? 
Makes sense. Some of our devs just run a Linux VM for that. (I don't know what their base OS is.)
I'll have to get back to you on the .NET version, but VS Community 2015
https://blogs.msdn.microsoft.com/user_ed/2012/12/09/how-to-embed-sql-server-express-in-an-application/ good thread http://stackoverflow.com/questions/30016652/include-sql-server-express-in-setup-project 
Thank you. I will consider switching. Which would you suggest is easier / plays nicer with a Windows Forms app?
SQLite is very easy to use. Call some API functions to open a file, call some API functions to make queries. It's even pretty darn fast. 
Given that I spend my time exclusively in windows, when using a mac the main annoyance for me was quirks with the keyboard layout compared to what I am used to. I also had a driver issue with the brightness sensor. I say stick with the hardware you're comfortable with, the software should mostly work fine. I went back to a PC after a few years with a mac, I was hoping to do some xamarin development at the time but I didn't get round to it and haven't missed the mac at all.
TLDR;?
This is what sqlite is for. 
This sounds like a job for SQLite.
I use various versions of visual studio all day long in three virtual machines running on a Linux host (Xubuntu). But it's a 12-core desktop with SSD RAID and ludicrous amounts of RAM. Not something I'd try on most notebooks.
It's also deprecated now. Microsoft recommends using SQLite too. 
Whose hardware is more to your liking? 
And use dapper
When I was in college few years back, I remember I used a Microsoft Access .accdb database file that I could easily copy-paste along with my exe, and also packed with setup of my application. I used *System.Data.OleDb* for executing SQL statements on it. I haven't used SQLite, but it was easy to enter data in MS-Access (it'll be somewhat like Excel spreadsheet). You can try SQLite or MS-Access. Won't really matter in a college project.
I am going to suggest if you can to use LiteDB. Its very simple to use and it can go anywhere your app can go. http://www.litedb.org/
https://en.m.wikipedia.org/wiki/SQL_Server_Compact#Deprecation
Depending on the exact nature of the settings, an SQLite or LocalDB database is lightweight enough to work quite well, while being a little more flexible than a simple flat file (and taking a lot of the work out of locking/unlocking the file for editing) The downside is that only one process can write to the DB at a time (for SQLite, I don't really use LocalDB): that's unlikely to be an issue with config data.
 MOV AH, 2 MOV DL, "@" INT 21H
Wow! Lucky to read this because this didn't seem well advertised at all! With .NET 4 support in the latest release I thought all was fine and dandy. Damn... MS has supported embedded systems with these sorts of standalone databases in one way or another for ages.
Interesting! Haha, I'm learning so much from this topic! First MS abandoning SQL CE and now this. Here's a .NET wrapper too, or maybe rather two in one: https://managedesent.codeplex.com One for convenience and basic uses (PersistentDictionary) and one (EsentInterop) for advanced and more complete support. Apparently the Esent API itself is old and awkward but this ought to clean it up. It's apparently used by MS even at least in Windows 8, so probably plenty of life left in it, being used for Active Directory and all.
Write everything down in a giant google docs or microsoft word online.
That's quite a broad question. What sort of things are you looking to monitor?
I'm using a Mac for my everyday work. Developing C# (Xamarin, .NET Core) and several Web projects using ASP Core and PHP. I don't really like this overloaded and IMHO buggy VS IDE (had a lot of trouble with it while dotnet Core was still in Beta). Pretty much prefer Xamarin Studio and vim. However, there are situations where I'm forced to use Windows. Thus I do have Parallels installed. If you decide to also go that way, make sure to get the Pro version of Paralles as of better virtualization options.
only service health.
what about app insights? https://azure.microsoft.com/en-us/services/application-insights/ I have only briefly used it, but it might be something that you are looking for?
Who's Amy?
Azure AD and AD in general is useful when you want to tie in with existing windows accounts inside an org. You can use .net identity which also has roles. The multi-tenancy is up to you how you implement it and what level of isolation do you need. Do you need separate servers &amp; databases for each "client" ? Or just that one client doesn't see/edit other clients' info ? 1 &amp; 2 have the same effect but vastly different implementations. 2) sounds more like what you need. I don't think azure AD (or AD in general) will solve neither the first or the second way of implementation. 
SQLite is pretty ridiculously lightweight
I agree yeah . . . but adding support for a database engine at all would be extra at this point, plus it's just overkill for basically 5 string values
The problem with Identity is that it is tightly coupled to EF. If you want to implement a proper Repository pattern, you have to slice-and-dice Identitiy (and partially redefine it) in order to abstract it out. But the bonus is that you can promote the primary keys from the rather lame GUIDs-as-strings to full GUIDs all the way through. I just found a blog that walks you through this process (although it doesn’t show how to build migrations capabilities), will update when I can.
As someone called Amy I was very confused when I saw the title
Smaller, simpler or “viably uncertain” projects can do without higher-end things like Dependency Injection, Automappers and Repository Pattern. I just finished off a small(ish) site (≈48 tables, 60+ pages) and did just fine without these. But anything larger, more complex or viably certain should have the time put into it for these things.
As someone not called Amy except when the customer pays extra, I was also confused. 
Setting up Autofac or other DI container is quick. I don't see any reason to omit it, unless project is very small and it certainly won't be developed further (and if we code professionally, we can't ever be sure that it won't be developed further). If we use EF, we don't need to reimplement generic repository/unit of work, we already have it.
If you just need per user settings, you can save data in HKCU without admin. If you want global settings for all users, you will need administrator privileges to write to that hive. &gt; BTW, I'm not downvoting, I welcome any and all suggestions :) Oh sorry, was not meaning to imply that you did it. I just wanted to point out that if someone disagrees, they should explain themselves and be helpful rather than simply pressing the downvote button.
EF already impliments the repository and unit of work patterns. And whats wrong with a GUID, it takes up a little more space but is like 99.996% unique, or something close to that
What's the alternative?
There is an example: https://github.com/OData/WebApi/tree/vNext/vNext/samples/ODataSample.Web As I understand, the OData version for Asp.NET Core is not ready, yet. The latest stable is on the previous version. BTW: Web API does not exist on ASP.NET Core. It has been merged into MVC.
VSTS might be easier to use for new users than git. 
It is dependent on the OS. If this information was released, it is buried somewhere in the OS release notes. You might also be interested in knowing that the constructor behavior for `CultureInfo` changed pretty dramatically. In Windows 8.1 &amp; Server 2012 R2 and before, `CultureInfo`'s string constructor threw an exception if the culture was not recognized. In Windows 10 and Sever 2016, CultureInfo does NOT throw if the language code is in BCP-47 format (e.g. aa-AA format). Windows instead creates an "Unknown language" with the "UserDefined" tag. It really screwed with some of our code when we found that one.
I've oddly enough never used TFS before. Some of my coworkers use it in their projects and they are happy with it, but I have never once touched it.
I can't find anything production ready for .net core webapi. Any hints? 
Ok, thanks for the suggestion. I will look into this. I've just been reading up on interview questions and I also stumbled across https://www.testdome.com/. See what you think of it.
graphql-dotnet , Hudl is currently running it in production , joe told me he spoke to them on the graphql summit , and joe is working towards using it on production as well on where he works . Here was his statement "We are unfortunately not in production with it. There are a few reasons for that (including that RavenDB-&gt;Marten transition), though if I can get a pretty good demo going and prove that it works well with Marten, we have plans to put it in production next year. Hudl is currently running it in production and has been for quite a while - since the beginning of the year I believe. I spoke to them at the GraphQL summit last week."
That's not enough for me to bet my job on it. But I'll look forward to it in the future! Thanks 
An alternatives, plz suggest ?
Cool, I'll look into that! I'm just trying to figure out the best way for paging/sorting this system. I feel if I build it from scratch, it may take longer to build than I have. I'm worried about adding/removing the elements too but I think regular JQuery would be useful for that.
&gt;Let the EF team know if you like this… Fuck. Yes.
if you need something simple for dynamic queries with web API , take a look to [NReco.Data library and its relex parser](https://github.com/nreco/data/wiki/Relex). Simple [web api example](https://github.com/nreco/data/tree/master/examples/SqliteDemo.WebApi) can be used as starting point.
Thanks a lot... I will try for it.
I did talk to the developers of the OData library a few months ago. They told me to go with the previous full framework version, as their ASP NET Core version is not quite ready yet. So far I didn't bother to look at it. What I do see is that there are not many commits to it recently, so I'm not sure they are actively developing it at the moment. It would be a shame, because it's really a great spec. Last thing I heard was they'd become an ISO standard soon.
I love this approach. In past there have been times we had to reach for internal classes using reflection, which of course would break on runtime instead of on build if there was a change and it’s was a nightmare. I wish everyone would adopt this approach so that just the `Internal` part in namespaces by convention indicates that these are non-public APIs and can change without notice. Thank you for pushing this idea to EF.
well. using System.ServiceProcess; ServiceController curServiceController; bool curStatus; curStatus = true; try { curServiceController = new ServiceController(inService.Servicename); if(curServiceController.Status != ServiceControllerStatus.Running) { curStatus = false; } } catch(Exception) { curStatus = false; } Could to the trick.
Basically, Microsoft changed their minds.
Still better than what we're doing now.
Yup, to be honest it works just as well as a dock - give or take a MagSafe adapter. And the new USB-C MacBooks should be even easier since the dock will provide power too: although we're yet to find a good dock/hub thing for those.
Web Forms stuff is much more hassle to convert, compared to MVC The nature of MVC is that the View (HTML page/Razor View) is by definition separated from the Model and Controller (ie the main logic of the application): that makes it a lot easier to have a separate business process to develop it. Web forms... it can be done, but it involves more backend work or frontend expertise. At the very least, you could learn Razor and take some of the workload, while encouraging them to move to Razor (and I promise you that if you take a chunk of their work away, the backend guys will be delighted to move to Razor with new projects to let you take away even more...)
Haha ok :D
With WebForms, you kind of need to be both back-end and front-end. You're right it does seem like a waste of work to create something that ultimately is just converted into something else. You might just try to position yourself closer to the back-end team. It's tricky that you have both webforms and razor. Your ideal situation would be like what /u/audigex mentioned, where the back-end promises to provide a data model to you and you just build the front end with mock data until they provide it.
Try searching for "cross compile". http://forums.dotnetfoundation.org/t/cross-compile-dotnet-core-from-windows-to-linux/1859 You may need to explicitly indicate which runtime when calling `dotnet build`.
I'd personally advocate for MVC. You get a model, a view, and a controller (hence, MVC). A View is your display Page and contains your frontend. It should contain minimal logic (if and for loops for example). The model is the data Sent to the front end. A model should not contain logic either - it's a data model, not to be treated as business logic. Lastly, theres the controller, and the controller can contain a bit more logic, but only for requests handling. Neither model, view, or controller should contain business logic, that should be seperated to a different layer. MVC is preferred because controllers dont know nothing about their views - meaning that front end and backend are loosely coupled. You can easily reuse Views or replace them without breaking your controllers or business logic. A webforms project has designer files and codebehind files. The designer file can be considered a View and codebehind file is your controller. The bad part is that they're tightly ingrained together - you cant easily replace your designer without altering the codebehind too, and the codebehind often alters elements on the designer. This is highly discouraged - logic and presentation should be seperated. The major advantage is that it can also greatly reduce code written, since logic and presentation are tightly interwoven.
Seconded! The job market for MVC is wide open (not to mention it is MUCH better to work with) and WebForms is a dying art.
Thank you. This cleared a lot of doubts I had!
Assuming you're referring to datatables.net, they have an example showing the type of functionality you're looking for: https://datatables.net/examples/server_side/row_details.html It's fairly straightforward to set up. Seeing your other comment on paging/sorting, that's going to require a bit more work. You need to map the datatables request object (described here https://datatables.net/manual/server-side) to a class so it can be bound. Then you can pass it / manipulate it / turn it into whatever query you want. Here's a (very dated) article on what that could look like: http://www.woodcp.com/2012/02/server-side-paging-with-petapoco-and-datatables/ This uses the old datatables version (they got rid of the hungarian API and cleaned the library up in general), but the concept is the same.
I usually try to avoid that situation in the first place. It's incredibly hard to get right, as one stalled client can prevent the message queue from draining. When I do need to broadcast, it's easy enough to just have a list of targets registered with the sending service directly.
I second the Fuck. Yes.
cos nservice bus is fundamentally flawed "NServiceBus Supports Horizontal Scaling", yes, but only if you have a single point of failure distributor. "Oh, just use windows clustering then". Great, thanks for the help.
So that's a problem with, specifically, MSMQ. But NServiceBus is just an abstraction and works really well with centralized queues and brokers (RabbitMQ, MSSQL Service Broker, or even SQL table-based queues). You can even used PaaS offerings like Azure Queues/ServiceBus or Amazon SQS.
[Serenity](https://github.com/volkanceylan/Serenity) - I'm a beginner as well, lots of documentation.
First of all, if you can at least upgrade to VS 2013, if you can go straight to 2015. Now to your question. I would go with start by creating a new "ASP.net Web Application". I can't remember if that's the exact wording but it's the one where you can choose MVC, Web API, and maybe something else. That gives you a lot of flexibility because Web API is amazing and so transparent as far as you can just throw plain old HTTP requests at it and it gives you this wonderfully rich way of handling everything. You should absolutely use Entity Framework 6 for you Data Access tier. It's a breath of fresh air as compared to the old dataset-based ado.net. You will also have SignalR at your disposal which is awesome and is really simple to set up and program against. Now this next one is my opinion so take it with a grain of salt. If you really want to super-charge your web app you could use AngularJS for all your client-side work. I personally think most .net web developers would enjoy using AngularJS for all the client side stuff. It works with Web API SO SO WELL! 
Reading the requirements I came up with 4 weeks. So a little under yours but not by much and I've been doing this for 10 years. I'd wager you were right to ditch webforms, on the surface they're simple but spiral into ugly complexity remarkably rapidly if you have to deviate from the status quo one bit. 
Isn't that basically saying "why do I need this tool when I've reimplemented it myself"? Also, the message queue not draining is a feature not a bug.
Thanks for the reply, I actually looked into Telerik for the Kendo UI and it looks really nice to work with. Unfortunately, my work isn't going to fork out the money for it. 
Interesting. I'm going to have to look into that.
Between the two, SQLite is easier to deal with. Access offers better support for real data types and concurrent users, but the SQL is so neutered that it's frustrating as hell.
That actually sounds about right to me. If you give someone with no experience a task like that then you should expect some start-stop and rework. They do not have the experience to choose the correct tool and know how to apply it. 
Why use that abstraction over just WCF?
Thanks, looks promising. I also like some other features like Rainbow Braces. Will give it a try on my sandbox VS2015.
Each client should have its own queue to prevent race conditions. If you had 3 subscribers and one message then the first one would grab it or you could say well all 3 of them need the message but how will the 3rd one know it was the last one in line? A message queue not draining is in fact a feature, you can do a new deploy of your service and it will start working again. You can easily scale up the part that is draining e.g. add more workers.
I would probably check whether the user already exists before you add a new one, or you'll end up with a lot of test users.
You could have told him that if they had stuck with the Excel sheet, it would not take any time at all :-)
I have no experience with it but syncfusion might be good for you if you're working for a smaller company (it's free if your organization make less than 1 million a year)
Yeah - WebForms were an attempt to make the web work like the Visual Basic GUI programming model. Turns out the web works like the web.
I here NServiceBus is coming to .NET Core. It has SAGA support.
Are you calling .Wait() or .Result on your call to CreateCampus? It sounds like your console application is exiting before the result of the asynchronous method call is returned. This is handled for you by ASP.NET MVC for async controller actions.
I wasn't. I was under the impression await did that, but I guess not? What I'm noticing is that it'll close right after I make the call to Azure, so it doesn't even finish out the method. Is that the behavior I can expect without .wait()?
This is the stack I used on my latest project and Ill agree web api, signal r, and angular work very well together. One of my favorite setups.
I'm going to try this tonight and let you know. I haven't dealt with too many console apps so this makes sense. 
Scott Hanselman just posted a blog yesterday about a project called Stateless that runs on .Net Core - http://www.hanselman.com/blog/Stateless30AStateMachineLibraryForNETCore.aspx You can find it on GitHub at https://github.com/dotnet-state-machine/stateless Their GitHub page states "Create state machines and lightweight state machine-based workflows directly in .NET code". As long as you are making a state machine-based workflow (most I've done meet this requirement), Stateless could work for you.
[Just say the name if you are going to make it so obvious](https://en.wikipedia.org/wiki/Packard_Bell).
ChangeBannerAsync() you mean 
Looks very cool! Thanks!
Two weeks.
Currently working on this project now, I can see why some of the estimations are low the more I work with it. 
You have a few options. Plain ol' HTTP, Service Fabric, and Akka.net to name a few.
What are the industries using for communication between services? 
except its obviously dell
Can /r/csharp finally get a nice theme too?
Is this your way to say "Soon"?
Any of [these](https://www.google.com/search?q=c%23+logo&amp;source=lnms&amp;tbm=isch) images would make a better logo.
Wow thanks! I've just merged it. I appreciate your contribution :-)
I can recommend it, it's great.
I always found the Reddit response to be overly verbose and the corresponding classes to be overkill. Typically in that kind of situation, I cheat and use `dynamic`. var client = new HttpClient(); var result = await client.GetAsync("https://www.reddit.com/r/EarthPorn.json?limit=1"); var body = await result.Content.ReadAsAsync&lt;dynamic&gt;(); Console.WriteLine(body.data.children[0].data.url);
Nice, that's nice. It never occurred to me to use dynamic, that's a clever solution.
Could there be any issues with using the .NET Foundation logo where it might look like we are associated with the .NET Foundation? 
Nice job with the app. I've set the built-in windows scheduler to run this once every day at 8 PM :)
Sharpffmpeg is kind of a pain in the ass. I've done streaming video in C# professionally, but not at the codec level. But I've had success with two things: 1. Using a C++/CLI wrapper around x264. C++/CLI is a way of writing .NET assemblies in C++. The C++ code can call x264 to compress frames, while presenting a standard .NET class to your C# app. 2. Using MediaFoundation.NET (mfnet) in C#. This is somewhat easier than #1, but you may not get h.264 out of it. You can definitely get WMV out of it. \#1 is the easier approach if you're comfortable with C++, and it's probably what I would choose if I were writing this from scratch today. #2 is the easier approach if you want to stick with fully managed code, but keep in mind your code will still resemble C code in terms of error handling and such. Lots of `if (hr == SUCCESS)` and making sure to clean up non-managed resources carefully when errors do occur.
Just to check: You're aware there are a whole bunch of free (and very good)options to record gameplay, right? I'm guessing you're wanting to do this as an academic/learning exercise, rather than because you can't find a way to do it, but figured it was worth asking
This is basically the old MonoDevelop shell (which is what Xamarin Studio is) but repackaged with an MS badge stuck on. I'm not surprised by this and it's not a bad move - Visual Studio is a strong brand name recognised and admired by developers outside the .NET stack. An IDE with the VS stamp will have more pull than something tagged with Xamarin or Mono and I'm sure the MS marketing people have looked at this (and it doesn't make sense to have different brands). I was all set to purchase Rider when JetBrains finally release 1.0 but the lack of F# support (with no plans to add this anytime soon) is a turn off and VS for Mac with its F# support is a big win. Exciting times for us .NET people that aren't on Windows!
Oh shit. I'd finally convinced myself that I don't actually want to pay for the new more-expensive MacBook Pro, and they pull this out? This seems like a great step for .NET though, at least for my use cases, with the drive toward Core. Not being able to develop on my MacBook (my "on the move" machine) has always been a big part of the reason I avoid .NET on many hobby and personal projects, but this should lift that barrier Of course, Xamarin and VS Code have both existed, but neither have really been anything close to a proper Visual Studio replacement
I couldn't see any either - I checked the Twitter account and it doesn't even mention it which is a bit odd.
This might have been published early, Connect() doesn't start until the 16th.
Here's an archive link http://archive.is/konMc
https://www.reddit.com/r/dotnet/comments/5cv2qz/introducing_visual_studio_for_mac/
https://www.reddit.com/r/dotnet/comments/5cv2qz/introducing_visual_studio_for_mac/
&gt; http://archive.is/konMc That's just an archive of the page above - there's no link there either.
Ah, sorry I thought you were providing a link to download the program. Yeah, they've removed the link now - it's clear this was announced before it was supposed to be.
Oh yeah sorry, I couldn't find any download even before the article got taken down. I misunderstood what OP was asking for.
Bingo. Everyone be patient - a mistake was def made. This will be out probably next week :)
Um - yay! I've really enjoyed Monodevelop/Xamarin. Here's hoping they keep it similar looking but add all the good stuff.
Yeah so far I've just gone with vsCode for my .net core learning because my laptop is running Kubuntu so it just makes sense to get more comfortable with the text editor. 
Why not use programs like dxstory, fraps or shadowplay (nvidia)? Shadowplay is great because it encodes the video on the gpu.
One thing to check is that your server is allowing the necessary HTTP method that the service requires. I think cross-domain errors have a specific error that explicitly mentions CORS. There is a way to add a handler in the web.config and enable the necessary HTTP method. And sometimes it can be that another handler is blocking the one you need. Did you see this? https://www.asp.net/web-api/overview/testing-and-debugging/troubleshooting-http-405-errors-after-publishing-web-api-applications
"I'm using Visual Studio". Great. Which one? "Visual Studio"? Or "Visual Studio Code"? Or "Visual Studio for Mac"? Re-using the name like this will just add to confusion.
Microsoft *loves* confusion. Remember all of the different editions of Windows Vista and 7? I am surprised they didn't rename notepad to "*Visual Studio Lite*"
They suck at marketing in general. Always have.
.Net Core works really well in Visual Studio - 2015 anyway. I haven't dared try in 2012 yet (I'm not sure if it's even supported) But yeah, 2015 "just works" once you install the tools: and you get to dodge back out of the command line again: I just hit run and it does everything for me like with "regular" .NET
So create your own observable collection and copy the objects into it.
Thanks for sharing! Currently I'm learning C# and was stuck at loading external files from a web server (.xml &amp; .json). Do you recommend HttpWebRequest for these kind of tasks or there are another APIs that do the job better?
Featuring visual studio on Mac: "The visual studio on Mac"
I hoped they'd change the name 'Dot Net' with the new open source version, I think it's a terrible 'design by committee' name. Yet with 'Dot Net Core' they somehow managed to come up with something even worse!
Also the [Surface](http://www.geekculture.com/joyoftech/joyimages/1775.gif).
'Visual Studio Core'. It's so confusing that I can see them doing it now.
They are always trying to leverage the goodwill of existing brands to other things. Remember ".Net Servers" ? 
Have you checked if the web app that is running as expected has the "always on" feature enabled and the other one has the feature disabled?
So for complex classes just make the models? 
http://www.hanselman.com/ That guy is good. There are many others, but he's good to start with.
Google cache has it
they both have always on of course. i am assuming the lack of resources in second app, because of huge amount of test data and when this happens caching stops working..
Yeah I've just had time to properly read it and it's just Xamarin Develop, which I've always found pretty underwhelming: what a shame. I assumed it was based on Xamarin Develop, but with some major updates to bring it to at least something approximating VS 2015, but apparently not. Hopefully this means some more effort will be put into it though, and it can become a "real" VS replacement/alternative
I have now implemented a fluent mapping api on the project (its in a branch at the moment) check it out: https://github.com/JamesStuddart/debonair/tree/FluentApi
I've heard of shadowplay but wouldn't we need a video card that supports it? All three of us are using some kind of old hardware. It's just Minecraft after all. ;)
If you're interested in large scale operations done in .net, definitely check out the stack overflow blog. They have a really interesting approach to making that site hum. https://stackoverflow.blog/engineering/ 
http://wasteaguid.info/
https://blog.uship.com/shippingcode/ Fair warning: I'm a contributor. [My posts](https://blog.uship.com/author/ivanmartinvalle/) usually focus on the .NET portions of our stack.
http://easynetq.com/ 
Please use WPF instead of WinForms. Please make sure you do not have "individual DPI per screen" setting enabled in your OS.
Can powershell a one-liner: `PS&gt; [Guid]::NewGuid()`
dude, easynetq is faaaar away from supporting dot.net core. It targets the .NET Framework 4.6 (possibly higher), but is no way near to target the platform agnostic runtimes that people refer to when talking about dot.net core apps.
I don't know a lot about grpc, but... Serious actors in the industry try to avoid the RPC pattern when communicating over a message broker. This is because it creates a hard dependency between the clients, and if the responder fails to send the reply it affects the caller (either crash or blocking call never returned). If you want to write something large and resilient, I would suggest following a event driven pub/sub pattern
&gt; Web Forms Is this… a *new* project? If so, dear lord *why‽* I mean, I know some classes can be a bit behind the curve with respect to subject matter, but any webdev course dealing with Microsoft tech should have switched over to MVC at least a half decade ago, if not more. You’re dealing with some seriously obsolete tech with Web Forms.
Try doing a clean and rebuild before the VS restart. It usually works and is quicker. But there are times when a VS restart is the only thing that works. 
&gt;In present scenario, targeting Smartphone users has become mandatory. Having mobile friendly website and applications allows a business owner to stay connected with a good number of people. Make sure the company is familiar with mobile platform-supporting applications. lol http://i.imgur.com/EwBUpXn.jpg http://i.imgur.com/UWawEga.jpg
The reason this was created was two-fold. First, I needed to generate 100s of GUIDs for testing purposes in JavaScript. I didn't want to import another library into my app and I just needed some hard-coded values. So, adding C# formatting onto what I had was simple. Second, I wanted to experiment with [Vue.js](https://vuejs.org/). This little app provided that opportunity.
tl;dr: &gt; Rewrite the bloody thing. That’s probably the worst decision you can make. &gt; Create a separate project and implement new stuff in it. In my opinion, that is the best way to go. Personally what I've been trying to do is a mix between both: keep extracting pieces little by little and putting them into maintainable separate projects. Of course this happens mainly when a major modification is needed in the legacy code, but sometimes it has been a preventive extraction as well. It has been years, and there is still a long way to go... but IMO the strategy has been quite successful so far. If anyone has more articles / infos about how to improve legacy apps that are the core of your business value, I would be very interested in reading them.
I am the author of RawRabbit, so I'm a bit partial. But... it all depends on what role you want the communication mechanism to play in your application. Akka.Net, MassTransit and others are huge frameworks that will have a big impact on how you structure your application. You have to align to their way of thought, implement actors etc. Sometimes, that is great! For the larger scale applications I've been working on, I didn't want to get locked in to a specific framework. And that is why RawRabbit is a good options for me. It is similar to how you would use a http client or any other client, it is just that it communicates over RabbitMq which allows for a lot of interesting aspects, like higher performance, multiple recipients etc. If not RabbitMq/RawRabbit then I would have a look at ZeroMq, I haven't tried it myself but I heard good things about it.
I suspect you have the correct approach. As for additional resources, Hanselman has done a few podcasts (hanselminutes.com) on the topic of working with legacy code. You might look them up if you want more perspectives.
This is probably one of the best blog posts I've read about doing something about the legacy code. I've dealt with it in a similar fashion the author did. The author is correct in not rewriting the bloody thing. The approach of adding a new project and slowly transitioning to the newer architecture/design is probably the best way to go. 
To add to this, I was almost always asked how to implement sorting algorithms (although in C# not VB) and literally always asked about OOP. Brush up hard on OOP concepts and why they're useful.
It's not a web development course, it's a Certificate IV in Programming and we're doing it because we're doing databases. That said, my lecturer is a little behind the times and more concerned with how it's going to work in a business. Seriously, I built this beautiful program for database management and he marked me down because I didn't order by the date in the SQL.
The way .NET Core works is it runs its own web server. When running under IIS it still does this, but a few environment variables instruct it to only accept connections from IIS, which acts as a reverse proxy. If there isn't a module for apache there will probably be one soon. It's pretty simple the way it's set up.
My advice is to be honest and talk lots about your final project.
I'd agree with this, and add (assuming database work is involved) having a basic understanding of the types of joins/unions and where/why you'd use them. "What is a left join?" is a useful interview question, because it very quickly establishes a basic level of competence, or at least understanding that suggests competence.
This is worth a read too: http://www.goodreads.com/book/show/44919.Working_Effectively_with_Legacy_Code
Being able to confidently describe the usage/differences between the more common keywords/attributes/modifiers: private, public, override, static, etc is also probably a good idea. 
Always and I repeat always make sure you acknowledge the fact you want to learn more. Employers look at this as you're going to put in a lot of effort to grow as a programmer and gain experience. 
All databases more powerful that SQLite support stored procedures. The variant of SQL they use to express them vary, but the calling convention is the same. Thus every decent ORM should be able to call stored procedures. (Not that I think EF is a decent ORM, but even it should support it.)
EF 6 definitely supports stored procs, I just haven't looked at EF Core lately. Also, fair warning. EF is slow. As in it will typically spend more time burning CPUs cycles on your web server than waiting for a response from the database. https://www.infoq.com/articles/repository-implementation-strategies And here's a cookbook we're starting to put together to compare other ORMs. https://github.com/Grauenwolf/DotNet-ORM-Cookbook 
I don't understand why interviews *still* ask about things like implementing sorts. I get that it's important to know when particular sorts should and shouldn't be used. I don't see where a programmer gains anything by knowing how to reinvent a well rounded wheel.
AWS STILL doesn't support .NET on Lambda. This might be enough to get me to switch to Azure by itself.
I'm hoping this puts a bit of fire under Amazons butt... 
&gt; Be honest, if you don't know, you don't know. This idea seems to have done well for me. If you say you don't know much about it, then talk about what you do know and what you think the answer might be it shows what you *do* know and some problem solving ability. If you claim to know about it then demonstrate a misunderstanding of it, they'll begin to doubt everything else you've said about yourself.
&gt;he marked me down because I didn't order by the date in the SQL. Seriously? I mean, output can be pretty clear as to what needs to have the default sort to make sense, but default sorts can also be as much about business rules as well. Was sorting by date an explicit requirement?
I've got to be honest, even with a CS degree I just don't remember the exact workings of most of the common comparison sorts. There are tons of implementation libraries out there written by far smarter people than me. They have done the leg work hundreds of times over and perfected it. I just don't see where it benefits anyone to commit the exact mechanisms to memory rather than just knowing that "X-sort works best by doing Y in Z conditions" and "A-sort works best by doing B in C conditions." 
Magic is just technology we don't understand
I think the simplest way of looking at it is thinking of an IoC container as a Dictionary that maps Interfaces to Constructors.
Manning's (publisher) [Dependency Injection in .NET](https://www.manning.com/books/dependency-injection-in-dot-net) by Mark Seeman is an excellent book on the subject, that covers both the underlying patterns and various IoC containers.
~~So .NET Core ships with a built-in Dependency Injection Framework.~~ So ASP.NET Core ships with a built-in Dependency Injection Framework. DI Frameworks give you a way of **setting up all your dependencies in one place** so that you can use them anywhere you like. The most common way to then pull them into your app is via constructors. So for example, if you create a IUserStore which has methods for retrieving users, you can then pass that as an argument to your constructor. public HelloController(IUserStore userStore){} And the DI framework will attempt to resolve that dependency when the constructor is invoked. In the case of controller constructors this usually happens per request. **Manual Configuration** Now you can manually configure the DI framework and tell it exactly which class to use when. For example, you could explicitly tell the .NET Core DI Framework to return an instance of SQLUserStore when attempting to resolve the IUserStore dependency. public void ConfigureServices(IServiceCollection services) { services.AddTransient&lt;IUserStore, SQLUserStore&gt; } Breaking this down, we're in the ConfigureServices method inside Startup class. We're telling the Core DI Framework to instantiate a new instance of SQLUserStore every time it's requested (that's what Transient means). Transient is a type of Lifetime and you can specify different lifetimes e.g. Singleton (created the first time they are requested and then re-used for every subsequent request). **Auto Registration** Finally if you don't want to manually "wire everything up" most of the DI containers have a mechanism to **auto-register** dependencies. This typically works by reflecting over the types in your app and applying rules about what should be mapped to what. For example, most of them would spot if you had an interface and only one implementation in your project (like our IUserStore example) and automatically wire up the only concrete instance (SQLUserStore) to the interface (IUserStore) to save you manually configuring it. Hope that helps!
Well that really depends on the application. If it is as bad as he says, then rewriting it can be cheaper than fixing it. Now I won't say that total rewrites are my first choice, but I have worked on projects where a total rewrite was estimated at less than 1/3rd of the cost to repair.
Some developers do understand magic though but they're more commonly known as wizards.
For all languages!!
It does not support cancellation.
There's an official image already available on Docker Hub registry (was surprised in the morning when my service suddenly stopped working after redeployment due to the new version of the framework).
"and please, beloved commenters—try to refrain from "embrace, extend, extinguish" posts, as the very concept is preposterous when it comes to Linux" This dank maymay really needs to die.
Fs I hate the installer for that. Asked me where I want to install so chose my data drive, then it continues to install most the software onto my C drive anyway, most of which I don't need!
Oh. I was already planning on a wipe + reinstall. It would be nice to not have to. 
Makes me smile a bit that Microsoft is at a higher level of membership than Google / Facebook / SUSE.
Anyone else unable to open .NET Core xproj projects? At first it wanted to convert it, which failed, now it just errors: The imported project "C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\Microsoft\VisualStudio\v15.0\DotNet\Microsoft.DotNet.Props"
I thought this was interesting... &gt; Google is joining the .NET Foundation and will be part of the Steering Group. .NET is still a major part of corporate development, and it would likely serve the company well to improve .NET support in Google Cloud Platform, if nothing else. Samsung is already a member of the .NET Foundation. The company is releasing a preview of Visual Studio Tools for Tizen, allowing developers to build .NET applications for Samsung's non-Android smart devices, such as TVs, wearables, and Internet of Things gizmos. ...as if they were each leaving room for successful tech to emerge from other companies, but trying to establish their ability to succeed whether that happens or not.
Wasn't MS also a gold sponsor for BSD?
IoC containers generally go one of three routes, the first is very explicit. Basically you have a generic function that takes a Func. You then map the generic type to the Func. Any time you want something of that type, call the function and return the result. This is less IoC and more service location but some people call it IoC. The second approach is reflection. With reflection the app takes the type that you ask for, gets a list of constructors for the type, creates any needed objects for the constructor, and then calls the constructor and returns the results. The third approach is basically reflection with code generation. Instead of doing the reflection lookup each time, you do code generation at startup to speed it up. As far as things to look at there are something like 200 IoC containers in .Net at the moment. The simplest is probably [TinyIoC](https://github.com/grumpydev/TinyIoC). [SimpleInjector](https://simpleinjector.org/index.html) is a good example of one that does code generation. Or atleast it did, I haven't looked at it in a while. But if you look at the code for TinyIoC you can get a general idea of what is going on. Or if you want, you can look at the [MVC Core's code itself](https://github.com/aspnet/DependencyInjection) for dependency injection/IoC.
That is our ambition. Some of the things we've done in this release: * We no longer install VS components into the GAC, we keep them private to VS * We have moved almost all our settings out of the system registry and now keep them with the installation folder * We install far less by default - the minimum install is one-tenth the size of the minimum install of VS2015 * We have switched away from MSI as the installation format for our core tooling - almost everything in VS2017 is essentially an extension to the base module. It takes a while to turn around a ship as large as Visual Studio, but we think we're getting there. This is just the Release Candidate release, so there may still be a few bugs, but we'd love your feedback on how we're doing. Best wishes, Tim Sneath | Visual Studio Team
So happy to see a "2017" instead of 15. Excited to dive into this release. 
Thanks, I am curious to learn more.
Log4net has a database appender that should do what you want. The other question is why are you trawling through that many log files? There is an email appender that can send you errors, or it can be setup to only log errors, or log errors to a different file etc. Rather than throwing out log4net you can probably use it more effectively.
Are you familiar with reflection? Basically you can inspect all the types in the system, create objects from them, call methods on them, etc. I'd recommend learning reflection and writing your own IOC container. You can knock one together rather quickly. Not for production use, just as a learning experiment.
I don't actually want to migrate as I would like to keep VS 15 compatibility. Apparently you can specify the tools version in globals.json to prevent it from migrating but I still get the above error. 
Why didn't they think of it before?
Or they joined because they offer C# Asp on their cloud platform to compete with Azure. But yours is mich more likely!
I can't find any socket libs that support Core either. :( Having a hard time.
SQLite sink came in recently, IIRC. It's published at: https://www.nuget.org/packages/Serilog.Sinks.SQLite, project site is: https://github.com/saleem-mirza/serilog-sinks-sqlite. Keen to hear how you go! :-)
Who is embracing, extending and extinguishing what? MS can't EEE .Net, they invented it. Google maybe could EEE .Net, but if that's what you're trying to say, I think you need much more than just that link to explain yourself.
I think they were being sarcastic because that's posted every time Microsoft joins a foundation.
Looks cool so far! :) Some issues though * I can't find the modules view after I installed it. Where is it located? * I would like to be able to select "web development", and then deselect some of the individual stuff without the whole "web development" getting deselected.
It's the better server option by far for large enterprise projects as well :-)
Sounds like [Human Music](https://www.youtube.com/watch?v=oDJ-DP31qAo). Personally, I like [Hans Zimmer soundtracks](https://www.youtube.com/watch?v=cdgUhz2F0Mg) to code by, although I usually just prefer silence.
We're still on 2012 at the multi-billion dollar company where I work. Our MSDN's are in budget limbo. This is so infuriating. Sometimes I forget and I try to use string interpolation and then I feel sad.
Great news: Visual Studio 2017 supports exactly that feature. You can now do File / Open Folder and work with a directory without creating a solution. 
Awww yea
There's also brain.fm, which is similar but has a wider selection [of elevator music].
yeah.. that and their awful sense of humor... I just goto the 10 minute mark.. I am not interested in their small talk and like when I can learn something... personalities are too corny for my tastes
Upvote for SOMAFM. My guilty pleasure is the Secret Agent Channel. 
I listen to hypnotic electronic or ambient music when I code. That or background noise generators like https://mynoise.net/
Inherit your DbContext from IdentityDbConext using the generic overloads that let you specify the user, role, user role, etc. classes. Use your existing custom classes for these. 
Thanks for the positive feedback. Forgive me for being stupid, but what do you mean by the "modules view"? Can you be more specific for me? As for the web development comment - that is our goal. Internally, our manifest is represented as a tree of components and packages. We have done a lot of work in this release to reduce the number of component-to-component dependencies and reduce the size of components, and as a result, more individual components can be deselected than in previous releases. But there are still more dependencies than we'd like. We'll continue to tweak and refactor - because we agree that you should be able to do what you're asking. Thanks again, Tim Sneath | Visual Studio Team
"modules view" as in where to add/remove components to visual studio. I found out that I could run the setup again to find it. But I kinda expected to find it somewhere inside vs.
&gt; You can just configure IIS to forward it to the actually hosted process. Out of curiosity, have you tried this yet? This has come up as an option for us to move more projects to core, but it's such a different paradigm for us, people are a bit hesitant.
LPT: put auth in a separate DB from the app.
We are planning to support in-place upgrade from RC to the released version, yes.
Oh no! Hey - if you wouldn't mind sending me a private message, I'd love to know if there are any logs captured so we can figure out what's going on. Thanks, Tim Sneath | Visual Studio Team
As someone who's implementing OAuth (bearer and refresh tokens) right now I can confirm that it's a fucking nightmare.
I used this series of blog posts to get me started with implementing identity in an existing project: http://bitoftech.net/2014/06/01/token-based-authentication-asp-net-web-api-2-owin-asp-net-identity/
Honestly, just use Visual Studio Code for now. VS for mac, isn't a port of the windows version, it's Xamarin studio rebranded. It does have support for .Net Core apps, so you can use it for web stuff, but it seems the preview release is quite clunky.
This is true, but you can do MVC Core in .Net 4.6.x mode if you don't have a real need for cross-platform _true_ Core. All nuget packages should work fine there.
well someone must be buying it ? 
I make music.. and you can have it all for free because its just music .. and you can even code to it if you want!! https://homelessrevolution.bandcamp.com/track/lunch-line-2
yeah I just use white noise.. any kind of music distracts me 
No idea. Just coz it's out there doesn't mean people are buying it. Carl Franklin is a musician and no doubt puts the music together for fun - anything he sells is a bonus.
It's a moving target. No doubt people will tell me this isn't the case but I've made several attempts over the past 18 months to start working with it then something changes. So I'll get it another 6 months or so before I start diving in again.
Mechanically it is a lot easier to treat it as a black box. From a content security perspective you don't need to ship the auth DB to the dev team to work on data issues or the app in general. Foreign Keys probably don't work for non-trivial cases, you want that sort of security in the code and almost always there is a but-if scenario where you need to override something that makes the FK worthless. Nevermind having a FK to the MS provided schema feels real dirty. On the data side we typically had much richer profiles than the MS provided fields. Moreover, we wanted to test stuff and we didn't want to be beholden to MS' identity stack to do so. Those kinds of things.
Doesn't look like it. Looks like it is only usable by Windows Store Apps.
I was recently forced to make the same decision. I went for CORE but referenced the full 4.6.1 framework, since there were a few libraries we needed and were not yet ported. Learning path was a little bumpy, especially around globalization and localization. But to sum up, I didn't regret my decision. I like the new structure and tools a lot more :) 
If you were intending on using EF and spatial data types, that's not an option in core yet.
Yeah - didn't notice that, thanks! Seems a bit weird to me that Node would outperform Core on every test except that though.
Damn. Thanks. 
ReportViewer.
I would say go for it. There's a lot of info out there. I've been using it for the last couple months.
Came here to recommend Serilog.
Pretty cool actually
Me too, for SharePoint development. It's a bitch to install and patching it up can be tricky, so having always a fresh one is superb. Also can make it 100% mirrored with client's in versions, hotfixes, etc.
Hey man, thanks for the kind words. Really glad you enjoyed it :)
Done. I sent a video as well.
You'll need both mono and monodevelop. http://www.monodevelop.com/ http://www.mono-project.com/
http://i.imgur.com/kwOtRs1.png &gt; Great news *puts down pitchfork*
that's fine.. hes free to do it... If I could sell my songs for $4 each id do it too .. I'm poor
I did use it on Mint and it worked just fine. Had one issue, though, during the installation process - some unresolved dependency but I Googled it, installed and got .NET Core platform working. I've recently switched to Elementary OS and it also works without any problems :). Here's the proof http://piotrgankiewicz.com/2016/10/17/net-on-linux-bye-windows-10/.
Can't deploy it to Azure, but everything else looks good!
So don't use EF, it's a shity ORM anyways.
Need a bit more information, what dependencies are missing?
Visual Studios if you require compilation. VS Code if you don't. 
Well one of the primary reasons would be what type of app do you intend to develop? If you are looking to do an Xamarin app then use VSM. If you just need to do a console app then you can use either VSM or VSC. Having said that I would stick to using Xamarin Studio if you need to do an iOS or Android project. VSM is only in preview and after having played around with it a bit I can say that it's a bit buggy.
Yeah, unfortunately.... The knowledge from older projects is there, and lots of older framework code as well. So we have to stick with it, for now. 
Skip npm and use yarn
Brooooooooooooo So happy you managed to get some real use out this broseph! I'm really surprised to hear you had problems with putting together the react side of things, I was pretty sure I took that one a cold run before I released to make sure it all worked. Maybe I was a bit "too familiar" with the code when I did it. With daylight between having written the article and now maybe I should go back and just make sure that is actually the case, thanks for the heads up. The original idea was to make a real time client that allowed you to hi5 back and forth between two clients but when we thought about the finer details of implementing that we decided that the scope would be too big to fit into a reasonably sized blog post so we scrapped it. You're right though, it would have been awesome! Well done on the interview too dude, anyone willing to do that much prep work for an interview obviously has their head screwed on right, I'm sure you'll do fine with that attitude ;) Cheers Zac
Yes. It's still better for pure DDD and there's still more control than EF6.
It's still useful for non standard cases like fk references to non pk columns (I didn't make it, but I had to work with it)
The 1.1 packages still start with "dotnet-dev-1.0.0-preview2" for whatever reason.
This… is possible? A freestanding, application-specific Kestrel instance? News to me. But then I have yet to really dig into DotNet Core. I would have just built the solution as a [WPF](https://en.wikipedia.org/wiki/Windows_Presentation_Foundation) app. Unless you need to serve the content across the network, why build a website?
With users installing an average of 0 apps per month. Is UWP really worth the effort? Most desktop applications are probably going to be line of business apps which are much better suited to WPF.
&gt; This… is possible? A freestanding, application-specific Kestrel instance? News to me. This is actually the **only** way now. Kestrel is just a webserver hosted within a plain old regular console application.
The hello world app is just that. You can easily run it side by something else. Just check Program.cs when you make a new .net core web app.
Hmm, I'm not sure then. I've not had the issue you're having but I use C# not VB. 
This. The membership provider in ASP.NET is extremely capable and does the job for probably 95% of cases. You have quite a bit of flexibility as well, including specifying a regex pattern for passwords and the ability to specify security questions for self-service resetting of passwords. You'd have to write your own system for "forgot my password" stuff, but the API gives you what you need for locking/unlocking accounts and changing passwords and stuff.
Your singleton initialization is not thred safe. Consider using Lazy&lt;T&gt; or better.. the built-in dependency container.
I'd go one step further and have a base repository class that implements the basic CRUD actions. 
 You can readcomplete deteails about vitual event in c# http://stackoverflow.com/questions/714893/how-virtual-events-work-in-c
I like that idea. Entity Framework was pretty quick to get going with and I had never used it.
Java? Because it's verbose as hell and is a decade behind on programming language features. Not to mention Oracle's penchant for litigation. If that's not what you meant, your question was too vague to parse.
Yup. Confirmed. That got it. Or at least, I found a set that worked. Just need the .deb file. http://hudosvibe.net/post/install-.net-core-on-mint-18-or-elementary-os So now the windows VM goes bye bye.
You should pretty much never follow any of the tutorials from Microsoft in terms of putting that code or something based on it in production. It's almost always terrible practice. EPP is decent though.
You can... look at Xamarin.
Still not native, unfortunately. 
I do exactly that in vscode, I have one web project and 5-6 libraries. 
I didn't say it would be a bad thing?
It's as native as you'll probably every get.
The lack of async makes it a non option for me.
Why would anyone think sharing a session across threads would be safe? 
Yeah Java has its good points. It's awesome for availability of libraries and solidity of code, and it's got some really performant enterprise-level stuff. I just really like C# for its design, and I like what Microsoft is doing with the Core initiative, open source, etc. Playing with Core right now is kind of painful, though.
Having a base CRUD repository class is one of the most common mistakes and anti-patterns. Repositories are meant bo be unique per Aggregate Root in the world of DDD and should expose only these methods that are valuable for the particular entity.
&gt; should expose only these methods that are valuable for the particular entity. What type of entities do you have that CRUD operations aren't useful for them?
you can cast generics to the types mapped to your tables public class BarReposiltory : GenericRepository&lt;FooBarEntities, Bar&gt;, IBarRepository { public Bar GetSingle(int barId) { var query = Context.Bars.FirstOrDefault(x =&gt; x.BarId == barId); return query; } } , or is that not required?
Easy to remember domain: https://get.asp.net Then you click on "Download" and select ".NET Core" on the left side. There you have all downloads.
Thanks for reply. I did follow same steps, but it didn't work. Should I uninstall existing?
No idea if you need to uninstall the existing. Did you make sure you downloaded 1.1, and now 1.0.1? Did you download and install the SDK **and** the runtime? You should also note that there is no dedicated 1.1 tooling support for Visual Studio 2015.
While you download the .net core 1.1 runtime. You still need to update your nuget dependencies to 1.1. I think the templates still uses 1.0.1 as they are not updated yet for some reason.
Thanks.
That's nice to hear.
I've just did this for my last project. I really enjoyed being able to quickly refactor things and not worry about storage until it was feature-complete. Plus now I have mockups for unit-testing.
Tutorials show in the simplest way how to get started on something. They rarely show best practices.
Hi there! Really anything in the TODO section of the README is up for grabs. https://github.com/Phrohdoh/CoreFire#todo One thing that isn't listed there but that I'd love to include is a WebSockets connection, this is where firebase's "realtime" database shines. Any libraries brought in must be compatible with .NET Core (and as a result run on macOS, Windows, and GNU/Linux). If I can clarify anything here just let me know (comment, PM, whatever). Thank you!
I always used Tesseract. There's a guy on Github who maintains a .NET wrapper.
I pointed you to the exact same page. :-\
Remote work question aside, I too am stuck in the webforms limbo... No experience with MVC, no work to get experience from... 
But does it support Raspberry Pi 1? I know most of it runs on Pi2...
I've been working as a contractor for about 8 years now, with about half of the time being remote. My current gig, they literally just found me on linked in, so I lucked out. (It wasn't a headhunter, it was the actual company.) I wasn't completely lucky, as i had to wait about 9 months from the time they contacted me to the time I got my contract, because the original job they wanted me for fell through. In general, I've found most of my remote work through local recruiters/headhunters. I've kept up a network of them throughout the years, and send out feelers when my contract is ending soon. They know that I'm primarily looking for remote work. I would recommend connecting with these types of companies, as they are in business to hook you up with clients. Which is great for me, because I would hate having to do the work to find new clients. FYI, I was like you in that I had tons of webforms experience, but no MVC. I ended up learning and using MVC for a few side jobs (small clients that I have had for a long time), so that's where I got my experience. If you don't have opportunities exactly like that, you could create MVC project just to do one, or find an open source project to contribute to. I hope this helps! I'm happy to help any further if you have more questions.
I will step up my linked in profile. I've been ignoring a lot of recruiters just because I get so much spam. I started replying to them lately with a simple question "does this position have a remote options" and they don't even bother writing back. Also, I've done two real projects with MVC, so that's no an issue. It's all the front end stuff that seems to be in demand is where I lack. 
I simply changed my dependencies and did a dotnet restore. Am I missing something?
&gt; web forms application Whelp, that’s your problem right there. Unless they’re looking for a maintenance plumber to keep some creaky must-have app alive and breathing for another year or two, web forms are essentially a dead field. Don’t get me wrong, it’s great for quickly prototyping small-scale features, but it rapidly gets very ugly and unmaintainable when trying to scale it up to enterprise class levels. I’ve recently had the pleasure of completely rewriting several web forms sites into MVC and the maintainability and clarity of the code is absolutely like night and day. Plus, despite my lack of experience when compared to the prior programmer who did the web forms sites (even now I’m only at about half his level at most) I was still able to pump out more features (overall) and more feature-complete parts of the site in less time (and wonderfully clean code to boot) with MVC than he did for the same project in Web Forms. And I had to backtrack numerous times to get out of corners that I had painted myself into.
This is what keeps me from seriously considering leaving my current job. I've worked for my employer for about 5 years now and for the past 2 years or so, I've had the freedom to work from home and flex my schedule as needed. I love the culture, but as with any job, things tend to stagnate after a while (and frankly, switching companies is often the only way to see a substantial pay increase). Seeing how difficult it is to find other remote positions is very disconcerting. Now as far as showing off new technologies, I'd recommend building proof-of-concept apps or simple side projects to flex your newfound knowledge and have something concrete to show a prospective employer. I've always struggled with this because if I'm not compromising performance at my day job, I have little free time to build "toy" apps or participate in open source projects. But they have proven to be successful in getting job offers and even some part-time contract work as needed. As someone that doesn't have a college degree, I've found most employers worth working for value tangible results most, not what your resume says you can do. Good luck with your search!
I need to edit my post. I have MVC experience, just not in the last few years. I easily dive back into doing MVC with just a week or two of brushing up. 
Exactly, plus who is to say private projects have any weight during candidate reviews? I've been looking to jump ship for more than a year now, but no go.. During this period the situation got compounded by the fact that my employer got acquired by another company that uses a completely different stack: jruby, rails, different rdbms.. Now I'm truly clueless on what to do next, as my career clock is ticking tirelessly at 32 years, 10 at the same, first employer, MS stack company.. But surely it's all about the local/regional job market. Mine's not one of the best, northeastern Italy, still to exit the 2007 recession... 
I sort of assumed that I was. Well, maybe not hated, but certainly envied. I only lived 15 minutes away from the office, and yet was only there once a week. And I worked with people who lived an hour away and commuted daily. I just got lucky. I know I will have better luck with smaller companies, but I am applying for anything I can find. How would blogging help? I keep thinking about just writing up random things. And thanks for those links, I'll look there. 
I wouldn't mind writing up some things for the world to see. Like the annoyances I am going through setting up a home network. 
The code is indeed out of the controller but there are always implicit dependencies with change tracking ORMs. For example, what happens when you have to deal with several entities and need a database transaction? Implicit or explicit, the knowledge about the underlying ORM will remain at the application layer, even if the classes are hidden behind an interface.
I think I would really like being a contractor or consultant. Those roles fit my skill set pretty well. The problem is I tend to be very conservative with my career decisions. The thought of a temporary project ending and having a gap in available work is scary to me. I'd love to get over that mental block though and see what is out there.
1.0.x is the LTS branch and will remain the default, you need to opt into the "current" release. Hopefully they make this easier when tooling matures.
DBContext should already offer those. 
https://www.infoq.com/articles/repository-advanced
can you give me a good example of where that is used? articles etc, im all for avoiding using unnecessary layers over DBContext, i hear you dont need this but i've never seen a good example of how to actually do this.
hmm i had a quick read of this, the article centers mostly on auditing, not what i needed but ok, then i saw this public IList&lt;Employee&gt; GetAll() { return Source.From(TableName).ToCollection&lt;Employee&gt;().CacheAllItems((Employee x) =&gt; CacheKey(x), policy: Policy).ReadOrCache(AllCacheKey, policy: Policy).Execute(); } this looks terrible, you can't pass in functions to refine the get all, you'll end up returning the entire table, which is the problem i had in the beginning. If the GetAll accepted a function to optionally refine the query e.g. with a where, that would be one thing, But im looking at returning IQuerable so i can build an expression tree in the service layer and perform execution there. I also saw in his prior article this public Employee Get(int employeeKey) { using (var context = new CodeFirstModels()) { return context.Employees.Where(e =&gt; e.EmployeeKey == employeeKey).First(); } } opening up a context every time you want to do something in a concrete repository is unnecessarily, he could have also made a lot less code duplication by using generics
As for opening up a new context, that's debatable. If you open a new context, then you can make your repository thread safe and dramatically reduce the amount of DI churn. Plus the context gets slower as you use it. Sometimes I'll throw the context away after every N inserts just to improve performance (though if I really cared about performance I would avoid EF entirely). 
Extension methods
That which is unresponsive to change is destined to quickly become extinct in a world that is based entirely on change. I suspect your shop, like many that worked with - and refused to move from - traditional ASP pages, is destined to find its opportunities and fortunes shrink rapidly and terminally over the next few years. I wish you all the best in your inevitably early retirement.
Resharper + VS is definitely a massive resource hog, but the Jetbrains IDEs are pretty good on that front - I usually have 3 or 4 separate Java projects open in IntelliJ but can only manage 2 solutions max in Resharper + VS on my 16GB machine. The refactoring and UX are something else in Jetbrains world too - I can't wait for VS to catch up there 
I would strongly recommend it. Especially with Entity Framework 6, it has become a real pleasure to work with. Like, seriously toe-curling pleasure; and this coming from a dev that approached Web Forms with almost more dread and loathing than Javascript. If you aren’t working with multiple solutions working from the same database, your code-first project doesn’t even have to implement a full repository pattern -- EF6 does it *adequately* as long as you implement eager loading and other best practices. The big thing is to not leak iQueryable so that your view models are robust and you keep db access to a minimum. Good luck!
I've heard that too but they specifically said they will not be doing that.
I just got a book on asp.net core. I really like what I'm seeing in it. 
Have they announced pricing for this yet? I'm assuming it's going to be the normal jetbrains sky high annual licensing stuff. I'll stick with VS Code or VS Community for now. 
Not announced yet.
&gt; ConcreteRepo.GetAll().where(x=&gt;blah....) It's not that though. Let's say you write this code: using (var concreteRepo = [...]){ return concreteRepo.GetAll().where(x=&gt;blah....); } //bye-bye repo That would fail. You absolutely need to add `.ToList()` before the repository is disposed or you'll get an object disposed exception when the result is finally used. The problem is that it isn't obvious from the API that `.ToList()` is needed. The only hint is if the developer happens to notice that it is returning `IQueryable` instead of `IEnumerable`. This kind of pitfall is bad API design, which is why people often hide the DBContext inside a separate repository.
Nope, it's getting close, but "as powerful" is wrong.
We can't reproduce this behavior but we might be able to provide a potential fix based on logs that we have reported. If you have any additional hints on how to repro this, please leave a comment under https://youtrack.jetbrains.com/issue/RIDER-3283 Thanks
The problem with implementing the repository pattern with EF is that EF already implements the repository pattern (and unit of work) so it is kind of pointless. I would create a 'TransactionService' that implements a TransactionService interface which in turn has the db context injected into it. The mapping code could also be abstracted to this level so your controller code becomes even more simple. This way the controller remains ignorant, it simply calls a method on the transaction service. This also means that a new service can be written (in an entirely different way - could even post out to an external service) and all you have to do is change the mapping for the DI container. Put it this, way, lets say your requirements change and you now need to push the new data onto a message bus/queue rather than saving it to a db directly. You cannot do it with your implementation without ripping apart your controller and changing lots of code. If you implement the service pattern you simply write a new a service, you do not change any code (thus adhering to the open/closed principle).
Shouldn't the dbcontext variables be wrapped in using statements? My understanding is that EF Database Contexts should always be wrapped in using statements so that you don't leak any connections if an exception occurs.
&gt; That was because Microsoft presented a new version of the .NET called Core. It's called .NET Core, not Core.
What do they have that vs does not already?
At the moment Windows Phone looks pretty dead... Yes go the Xamarin Route. Read up on Xamain Forms. They are limiting in regards of look an feel. But have a far more managable learning curve as Xamarin with native layouts. 
&gt;can I install it on Windows 7? Yes
Seriously? If it was called .NET 5 what would the version be? That's right, 5. Arguing semantics is silly, especially when there's nothing wrong with the way he used the name.
I've been using a Macbook Pro as my workstation for .Net work daily for the past 3 years. I've got to run my stuff in a VM since I have server dependencies that won't run on OS X yet. The only issues I've had so far are more a result of corporate IS policies than anything with the setup.
And a click bait title! 
&gt; I see, so you're saying that if an API is returning IQuerable, and ToList is not called, you'll get an object disposed exception? If the DBContext is disposed *before* you call `ToList`, yes. &gt; my issue here is the whole table is returned, and my site is slow Well yes, that is bad too. &gt; got any good repositories i could look at? Honestly, I don't really use them much anymore. I just use Tortuga Chain, which serves the role of both the repository and the ORM layer. Say you want to search for all employees with the last name "Tom". And you just need their phone numbers. var employees = dataSource.From("dbo.Employee", new {LastName="Tom"}).ToCollection&lt;EmployeePhone&gt;().Execute(); `dataSource` is thread safe so you don't need to worry about disposing it, just create one per database for the lifetime of the application. All queries are closed when you call `.Execute` or `.ExecuteAsync` so no fear of dangling connections. It's just easier all around (if you aren't working with deep object graphs). https://docevaad.github.io/Chain/
Okay, so first thing of look for is squiffy data, such as a user name with an apostrophe in it (that could look like an extra field potentially) Second thing of look for is null data, is you've submitted with nothing selected, as you don't seen to validate against that. Probably stuff you've already tried, so sorry of not helpful.
I'm not the author. I even didn't notice the click bait, but you are 100% right. Still it looks useful
Xenko looks amazing
Okay so I used to use the base-class-CRUD approach as well, so I can offer insight as to why it's a code smell in disguise. A repository, a proper repository, should abstract away any concept of a database or underlying data layer, otherwise you lose any benefit from removing it from your controllers. If you're JUST adding/removing/updating/deleting a single type of entity at a time then you can get away with it, but often you're doing more than that and that's where repositories are useful and that's why you don't want to go down the CRUD route. Databases are meant to be relational, so you're going to end up with situations where you're updating multiple entities in multiple tables. If you only have your CRUD class sitting on top of your DBSet, then you'll end up calling several operations (Say an add and two updates) - with multiple calls to SaveChanges as a result. Now what happens if there's an issue after 2 of those operations? Say your connection blips or something and the 1st and 3rd calls went through, but the 2nd didn't (yes, this does happen in the real world). Suddenly you have dangling data in your database. You want your transactions to be atomic, you either want all or nothing being written to the database. One call to SaveChanges(). Not only that, but your controller ends up with needless complexity. Your controller should not care about the underlying data structure, so even though you're not calling EF directly, you're still having to call multiple methods in the right order and hoping nothing goes wrong. The repository should have a single method that does all of those db calls in one go. Rather than calling AddOrder() once and UpdateCustomer() twice, you end up with something like AddCustomerOrders(). Your repository might end up growing to have a lot of methods, but that's ok, at least your controller is lean, your DB calls are completely transactional and you can mock a single interface for your entire application. 
This looks awesome! Definitely gonna give it a try.
Seems good for that situation. 
Isn't that the same as f12? Or alt + f12?
I've been using it for a week on my Mac as my main editor for an asp.net core project, working a charm. Must be something with your install / config specifically?
This is the part that I struggle with. I have only once dealt with a project where we changed the underlying database and that was a vb3 application using access2 to oracle. Every other project I have worked on since than has always had a fixed database. I can see with net core that we will see a projects being developed with Sql Server and being deployed to MySQL, but even then I wonder how often that would happen. I am curious how often people had to change out the database in a business application.
I saw this change when I skimmed over the release notes but didn't look too much into it. Now I'm really annoyed. I never had to deal much with raw csproj files in the past because I started .NET development on an established codebase. I started a new .NET Core project recently from scratch, and working with project.json has been fantastic. It's so easy to use and the Intellisense is great. I _really_ don't want to go back to clunky XML; those files look disgusting.
This is rather disappointing ಠ_ಠ 
What was the rationalization for this 
The **whole** eco system is based on MSBuild. Only the ASP.NET Core guys wanted an extra sausage and created project.json - which does not play well with everything else. When the choice is between adjusting all other projects, and third-party projects, or simply extend an existing and proven system.. The choice is easy.
barebones json or crufty xml, somethings probably still going to break your build and send you down a rabbit hole of whacking away at this file until you give in and revert.
Hm, I feel dumb, but how did you do that -&gt; var body = MailBody .CreateBody() .Paragraph("Please confirm your email address by clicking the link below.") I was under the impression that there is no "With" block equivalent in C#?
No problem I was just saying :-) but yeah cool stuff. Chocolatey helped me out a lot. 
I got this https://github.com/dotnet/core/issues/354
Proper old Microsoft thinking here. Forcing people to use their big heavy weight tooling. I have been working with .Net Core and Visual Studio Code (on various platforms) for a while now and the experience has been great. So wonderfully lightweight and yet so powerful. Well, it looks like this times are over. MSBuild, Visual Studio 2017, gonna have to be working in Windows all the time now...such fun!
Each method probably returns the modified MailBody so the next method can be called.
I'm not sure how this is related to anything I said.
It is not just about databases. It is having the flexibility to do whatever is necessary. You might want to change your application so that instead of writing to a db directly it sticks the data on a message queue/bus for some other process to pick up. This, in turn, may do more than saving to a db (e.g. save to elasticsearch or redis etc.). It might not occur very often (probably more often than you think) but the code should be written in a way that allows you to swap out implementations with ease because if the time comes where you do need to do something different, you will be thanking yourself that you wrote it in a way that is easy to change. 
Same here, all the above are store links :)
C# doesn't care about whitespace. A '.' does not need to be connected to its preceding object. Also if a method call returns 'this' then you can chain up calls against an object. This is what's called a fluent api.
They did. The comment "Yes the packages are resolved at build time and the hintpaths are not needed." from Srivatsn Narayanan found here https://blogs.msdn.microsoft.com/dotnet/2016/10/19/net-core-tooling-in-visual-studio-15/ states this clearly.
&gt; And the heavily simplified CSPROJ format is really easy to use. You wrote this. I am using the `dotnet migrate` tool to convert project.json csproj and it fails at the first simplest project.json.
As I said, I've had these issues with the last couple of installs as well so I don't see it as being an 'install' problem (one good thing about a POSIX systems is you don't have the horrible registry issues you get on Windows). Maybe a config file is doing something, although I'm pretty sure I removed everything last time.
Do you understand what "alpha" means? It's still a very early version. Once everything is in place, it will work. The `project.json` had so incredible many quirks too during the development (and still has). Besides, whether the **tooling** currently works properly is undefined with how cleaned up the format is.
The tooling might be alpha - but msbuild has been around forever. In this case, the error generated by the mis-configured csproj is opaque. 
The tooling involves the migration tool and the targets file, which cause this error.
Yes, but i'm guessing it won't be free. VS Community is a free edition for Windows.
It's not just going back to the old ways. The file is heavily improved, and tooling support has been added to make what little you really have to do in the file easier. The json version was completely incompatible with the entire rest of the ecosystem.
Llblgen 5.
I just looked it up actually and apparently it IS scoped to the request by default :)
the difference is that .net core projects include the package Microsoft.NET.SDK - https://www.nuget.org/packages/Microsoft.NET.Sdk/1.0.0-alpha-20161104-2 this package contains tasks and targets which allow for simpler project files and integration with the new vs2017 features. if you're using the new sdk then you can indeed simply switch from, say, &lt;TargetFramework&gt;netcoreapp1.0&lt;/TargetFramework&gt; to &lt;TargetFramework&gt;net461&lt;/TargetFramework&gt; - or &lt;TargetFrameworks&gt;netcoreapp1.0;net461&lt;/TargetFrameworks&gt; to build for both.
Read into it properly, it may be XML but it's not the same XML/CSproj you've used in the past. It's really the best of both worlds.
Woohoo!
This is a great explanation, thank you. Inserting that Payee alongside the Transaction in my example seems to reflect what you are talking about at the end of your second paragraph. It would have been not-so-good if I actually HAD separated the payee insert into a different call (multiple SaveChanges would then be needed).
Damn that's a shame, but I can understand the reasoning. project.json is just so much easier to read and looks much cleaner.
It doesn't seem to support Oracle. 
I liked the part where files were maintained implicitly and not because you remembered or forgot to add them into the giant XML list. Also that list tends to cause merge collisions when working with other people on different branches. They're supposed to add wildcard paths into msbuild to help that, though.
Most of these can be replaced with Power Tools.
&gt; clunky XML When you realize that XML is essentially strongly typed JSON with comments, you realize that both have their place.
It does https://www.llblgen.com/Pages/requirements.aspx
- ReSharper in the IntelliJ key layout, with Color Identifiers set to on Less is more.
MSBuild already has wildcard paths, it's VS that didn't support it. However, you'll be pleased to know that VS2017 has support for it and this new CSProj format does not list all the included files in one giant list. So you have the same behaviour as project.json and as a brucey bonus, you can adopt that behaviour for your existing projects (even something like Winforms or other full framework projects).
* [ReAttach](https://marketplace.visualstudio.com/items?itemName=ErlandR.ReAttach) for quickly debugging running websites. * [Typewriter](https://marketplace.visualstudio.com/items?itemName=frhagn.Typewriter) for generating Typescript definitions and scripts from model classes and WebAPI controllers. * [Code Health](https://marketplace.visualstudio.com/items?itemName=Jean-MarcPrieur.MicrosoftCodeLensCodeHealthIndicator) for when refactoring. More of just a fun to have.
ReSharper - Refactoring, quick navigation, file structure pane, unit testing and in general a lot of QOL features. AnkhSvn - We still have to work with SVN sometimes... [Hide Main Menu](https://marketplace.visualstudio.com/items?itemName=MatthewJohnsonMSFT.HideMainMenu) - save 30 pixels of precious vertical space (you can display the menu with ALT at any time).
Just use [dia](https://wiki.gnome.org/Apps/Dia/Screenshots) for free and forget vs altogether. From my perspective any ability to generate code from diagrams is of minuscule (if any) value to anybody but an architecture astronaut.
Might sound weird but, spell checker. I don't remember the actual name but it has saved me in js so many times. Also good for those exception messages.
I may do that. I have such a sour taste from the performance issues that it's hard to give the other tools a shot.
VsVim - the only thing I need.
My team starts off every new developer with these ~~four~~ five tools: * [Productivity Power Tools](https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.ProductivityPowerTools2015) * [VS Spell Checker](https://marketplace.visualstudio.com/items?itemName=EWoodruff.VisualStudioSpellChecker) * [I Hate #Regions](https://marketplace.visualstudio.com/items?itemName=Shanewho.IHateRegions) * [NUnit Test Adapter](https://marketplace.visualstudio.com/items?itemName=NUnitDevelopers.NUnitTestAdapter) * [ReSharper Ultimate](https://www.jetbrains.com/dotnet/) 
Would this work? https://github.com/pierre3/PlantUmlClassDiagramGenerator
Alt+Shift+Enter is perfect for me.
We just end up not using a strongly typed library for our current project. We probably won't use it anytime soon but it's good to know about it. 
I can definitely appreciate that. Even with top of the line hardware, the other features are just too damn slow. I still use them for the most part, but it can be very painful at times. 
Why not move version tag into a version attribute. Would cut down the number of lines dramatically if you can use self-closing tags most of the time.
Is this supposed to be an off by one error joke?
What do you like about that key layout?
I use the full screen mode only occasionally when I don't need any other programs.
In addition to a lot of the other ones that have been listed, I always install [SmartPaster](https://marketplace.visualstudio.com/items?itemName=martinw.SmartPaster2013). It basically allows you to paste text you've copied from somewhere else as a comment, as a literal string, as a StringBuilder, etc. It's fantastically useful when you're copying code from somewhere else that you need to put into an array or something. 
Looking to drop it when VS2017 comes out. Live unit testing etc.
I used it when I first started out. A ThoughtWorks employee who was giving me some training set it up, and I've used it ever since. It also means I can transition to any JetBrains IDE very easily, which is useful as I use WebStorm and PHPStorm. I'm not sure which version of ReSharper I started on, it was maybe v2, and they might also have had limited choices of schema back then
good riddance
Well, that's good news. I'm mostly a fan of XML, but it's easy to make it too verbose. This isn't the fault of XML, but of a schema design that didn't prioritize human readability. It would be nice if more devs understood this: XML can be easy to read and write!
QA is a stepping stone. One rung on the ladder, foot in the door. Typical pathway for a person starting out after training. Software authors love to have quality testers.
Hi, This is another way you can create your email: var body = MailBody.CreateBody(); body.Paragraph("Hi,") .Paragraph("First paragraph.."); // Your code body.Button("https://www.example.com/", "First button"); body.Paragraph("Another paragraph.."); // Your code body.Button("https://www.example.com/", "Second button") .Paragraph("— [Insert company name here]"); var htmlBody = body.ToString(); I hope you find it useful.
Hi, it is possible to override the template. You can check this example: https://github.com/doxakis/MailBody#custom-theme--raw-html You can change template.Body for your template. Ex: template.Body = @"&lt;html&gt;&lt;body&gt; ... {0} ... {1} ... &lt;/body&gt;&lt;/html&gt;" {0} : the content {1} : the footer If you want to see the current template, go to the function GetDefaultTemplate (file: MailBody.cs, class: MailBodyTemplate) Thanks.
Thanks god
&gt; Simplicity, JSON is much more naturally readable/scannable JSON doesn't support comments.
Just about anything that has the name "Mads Kristensen" on it. They're easy to find because if you sort by popular most of them are at the top of the list. 
Turning off solution-wide analysis helps.
Thanks for mentioning my jQuery Code Snippets tools. Glad they have been so well received. :)
resharper has an integrated nunit adapter, also seems to have a large overlap with the productivity tools. which features made you stick with these?
Never thought I would hear resharper described as the 'less' option. 
About F# books assuming you already know C# ... let me actually plug something quite different from the standard F# book: [Prof. Dan Grossman's Coursera MOOC](https://www.coursera.org/learn/programming-languages/), 'Programming Languages, Part A'. It's a thorough introduction to the SML language, which is an ancestor (but a close one) of F#, with much of the same syntax, semantics, and idioms. The great things about the course is that it doesn't talk about .Net at all--it's purely focused on doing functional programming with SML, which you can adapt quite easily to F#.
* Productivity Power Tools * Syntactic Line Compression = ON * Fix Mixed Tabs = OFF * Custom Document Well = ON * Turn off well tab colors * Set tab width to a min=150, max=150 so all tabs are same size * Turn on close button (X) at the end of the tab well so you can quickly close a bunch of tabs * I Hate #Regions (because regions imply shitty code) * NUnit Test Adapter if you use NUnit * C# outline 2015 * This gives you collapsible outlines for all types of code blocks such as if-statements, loops, etc. Super handy * GitExtensions Plugin * Gives you the ability to see git history in a useful way instead of the crap visual studio provides * Scrollbar set to Map mode (not an actual extension but a feature) * By far the most useful "feature" is that you can highlight a word in the code and it will make a blue mark on the scrollbar map so you can easily see where that word or highlighted text is used within the file. I seriously use this daily if not hourly
My team is working on adding Oracle to Chain, but it won't be ready until after the holidays at the earliest.
Didn't work in RC for me haha
Crutch? Personally I feel it helps do the grunt work when I'm coding so I can focus on more high level, important ideas. Resharper handles creating fields, extracting variables, extracting methods, renaming files, getting rid of unnecessary usings. It's not writing the code for you, it's just handling the unimportant things that you shouldn't have to waste your time thinking about. That said Resharper can be a bit...liberal with Linq statements. My only real gripe with Resharper that I come across on a daily basis is it's pressing insistence that all for loops should be Linq statements, only to convert it into some really gross, unintelligible mess. I love seeing what the code turns into, but 95% of the time undo it and leave it the way it was (there are times when I think the Linq code is just as readable, but they're rare). I'm genuinely curious as to why you see it as a crutch though. I've made my position on it pretty clear...if you don't mind explaining...I've seen several people call it a crutch but never had anyone give any explanation for why they think that.
Thanks for the response. I had never neard about Llblgen before so I just googled it and found the front page. Didn't see anything on Oracle there. Then looked at the "What's new" page and it mostly mentioned MS SQL Server and mySQL. That was the extent of my research, which resulted in the wrong conclusion. :) 
I'm getting a lot of usage out of [OpenCover.UI](https://marketplace.visualstudio.com/items?itemName=jamdagni86.OpenCoverUI), which allows me to see code coverage for my unit tests. I use OpenCover with Appveyor already and this saves me having to push the repo to see the coverage. A recent pull request added Xunit support but there's no corresponding release so I forked the repo and created an [unoffical](https://github.com/jpdillingham/OpenCover.UI/releases) one just so I wouldn't have to compile it again at work.
something like this: https://nexnet.files.wordpress.com/2013/03/031313_2127_startjtreem2.png?w=700 (statsvn)
&gt; MaxDegreeOfParallelism Can you provide an example of how you typically do this? 
I didn't know that I needed this until now.
[PVS-Studio](https://marketplace.visualstudio.com/items?itemName=EvgeniyRyzhkov.PVS-Studio) is a powerful static source code analysis solution for bug detection in C, C++, and C# projects.
And when dealing with async in IIS propagate it all the way up to respnse of the controller otherwise you will end up with locks
Thank you for the suggestion. There is logging for the request parameters, and response as well as any exceptions. However, when the service crashes, there is just no longer any logging, and all subsequent requests timeout.
Absolutely.
Crazy, never seen anything like that before! Looks pretty similar to Action&lt;T&gt;.
LLBLGen has the best LINQ implementation of any ORM out there but it also has other myriad ways you can build up your queries, etc. The tool has been around forever and the great thing is for every new version you can just upgrade and your old code still just work and usually with faster performance. I have been using the tool for almost 10 years now and watch microsoft ORM come and go. ORM should be boring and stable and just work.
&gt; e! Looks pretty similar to Actio TPL dataflow is a bit forgotten library, but I've bean processing terabytes with it
Search for ASP.NET MVC Core documentation. ASP.NET Core docs are low level building block for making web app. There is nothing changed as far as ASP.NET MVC Core is concerned. It looks very similar to ASP.NET MVC 5.0. The underlying tech thought is vastly different. This is how asp.net core code looks like https://github.com/dodyg/practical-aspnetcore
Contribute to or start a new open source project. 
2017 RC relies on the preview 3 SDK that has alpha support for msbuild for .net core. That tooling is truly alpha.
Try Xamarin
I don't see a question in your post...? You might want to try in r/cscareerquestions.
I'm not sure where you live, but this seems like an absolutely horrible opportunity. First, I've never heard of of a legitimate 2 year unpaid internship, and I can't imagine it being entirely legal, at least in the US. Additionally, for a developer with 2 years experience, most places would pay considerably more than 25k-35k a year. Secondly, no matter what someone says there is no such thing as a "guaranteed job two years from now". I don't have all the details, but this sounds like someone trying to exploit you. If you know what you are doing, you can probably land a .net developer job by passing some Microsoft certifications and looking for a Microsoft partner company in your area. Based on your age, it sounds like you haven't finished or even started university. My recommendation: finish university. If you have aptitude and are willing to work, I'm sure you will find something without a degree, but you will never have a time in your life with as much time and freedom to finish a degree than you have now. FYI: my company is always looking for people and we do a lot of .net / azure / and web dev work. 